[
    {
        "link": "https://github.com/cakeslice/Outline-Effect",
        "document": "HDR support (useful for bloom, etc...)\n\nMerged outline between renderers (or you can add outlines between different colors)\n\nErase option to keep outlines behind specific renderers\n\nPer renderer color support (up to three different colors)\n\nOption #1 (Recommended): Use OpenUPM or add the package using the Package Manager: (Add from git URL)\n\nOption #3: Download the files and place them anywhere in the Assets folder of your Unity project"
    },
    {
        "link": "https://docs.unity3d.com/2023.2/Documentation/Manual/script-Outline.html",
        "document": "The Outline component adds a simple outline effect to graphic components such as Text or Image. It must be on the same GameObjectThe fundamental object in Unity scenes, which can represent characters, props, scenery, cameras, waypoints, and more. A GameObject’s functionality is defined by the Components attached to it. More info\n\nSee in Glossary as the graphic component."
    },
    {
        "link": "https://docs.unity3d.com/560/Documentation/ScriptReference/UI.Outline.html",
        "document": "Select your preferred scripting language. All code snippets will be displayed in this language."
    },
    {
        "link": "https://discussions.unity.com/t/outline-highlight-objects/717320",
        "document": ""
    },
    {
        "link": "https://docs.unity.com",
        "document": "Docs and guides to work with the Unity ecosystem."
    },
    {
        "link": "https://docs.unity3d.com/Manual/class-InputManager.html",
        "document": "The Input Manager window allows you to define input axes and their associated actions for your Project. To access it, from Unity’s main menu, go to Edit > Project Settings, then select Input Manager from the navigation on the right.\n\nThe Input Manager uses the following types of controls:\n• Key refers to any key on a physical keyboard, such as W, Shift, or the space bar.\n• Button refers to any button on a physical controller (for example, gamepads), such as the X button on a remote control.\n• A virtual axis (plural: axes) is mapped to a control, such as a button or a key. When the user activates the control, the axis receives a value in the range of [–1..1]. You can use this value in your scripts \n\n A piece of code that allows you to create your own Components, trigger game events, modify Component properties over time and respond to user input in any way you like. More info .\n\nThe Physical keys option allows you to map key codes to the physical keyboard layout, rather than to the language-specific layout that may vary between users in different regions.\n\nFor example, on some keyboards the first row of letters reads “QWERTY”, and on others it reads “AZERTY”. This means if you scripted specific controls to use the well known “WASD” keys for movement, they would not be in the correct physical arrangement (like the arrow-key arrangement) on an AZERTY-layout keyboard.\n\nWith Physical Keys enabled, Unity uses a generic ANSI/ISO “Qwerty” layout to represent the physical location of the keys regardless of the user’s actual layout. This means if you specify the “Q” key, it will always be the left-most letter on the first row of letter keys, even if the user’s keyboard has a different letter in that position.\n\nNote, you should not read key input for in-game text input, because this will not allow users to enter non-Latin characters. Instead, use .\n\nEvery Project you create has a number of input axes created by default. These axes enable you to use keyboard, mouse, and joystick input in your Project straight away.\n\nTo see more about these axes, open the Input Manager window, and click the arrow next to any axis name to expand its properties.\n\nEach input axis has the following properties:\n• Between –1 and 1 for joystick and keyboard input. The neutral position for these axes is 0. Some types of controls, such as buttons on a keyboard, aren’t sensitive to input intensity, so they can’t produce values other than –1, 0, or 1.\n• Mouse delta (how much the mouse has moved during the last frame) for mouse input. The values for mouse input axes can be larger than 1 or smaller than –1 when the user moves the mouse quickly.\n\nTo add a virtual axis, increase the number in the Size field. This creates a new axis at the bottom of the list. The new axis copies the properties of the previous axis in the list.\n\nTo remove a virtual axis, you can either:\n• Decrease the number in the Size field. This removes the last axis in the list.\n• Right-click any axis, and select Delete Array Element.\n\n Note: You can’t undo this action.\n\nTo copy a virtual axis, right-click it and select Duplicate Array Element.\n\nTo map a key or button to an axis, enter its name in the Positive Button or Negative Button property in the Input Manager.\n\nMouse buttons are named and so on.\n\nYou can also query input for a specific key or button with and the naming conventions specified above. For example:\n\nAnother way to access keys is to use the enumeration.\n\nTo access virtual axes from scripts, you can use the axis name.\n\nFor example, to query the current value of the Horizontal axis and store it in a variable, you can use like this:\n\nFor axes that describe an event rather than a movement (for example, firing a weapon in a game), use instead.\n\nIf two or more axes have the same name, the query returns the axis with the largest absolute value. This makes it possible to assign more than one input device to an axis name.\n\nFor example, you can create two axes named Horizontal and assign one to keyboard input and the other to joystick input. If the user is using the joystick, input comes from the joystick and keyboard input is null. Otherwise, input comes from the keyboard and joystick input is null. This enables you to write a single script that covers input from multiple controllers.\n\nYou can use input from the Horizontal and Vertical axes and the method to move a GameObjectThe fundamental object in Unity scenes, which can represent characters, props, scenery, cameras, waypoints, and more. A GameObject’s functionality is defined by the Components attached to it. More info\n\nSee in Glossary in XZ space (forward, back, left, or right). Add the following code to the method on a script attached to the GameObject you want to move:\n\nrepresents the time that passed since the last frame. Multiplying the variable by ensures that the GameObject moves at a constant speed every frame."
    },
    {
        "link": "https://onewheelstudio.com/blog/2022/4/15/input-event-handlers-or-adding-juice-the-easy-way",
        "document": "Need to add a little juice to your UI? Maybe your player needs to interact with scene objects? Or maybe you want to create a dynamic or customizable UI? This can feel hard or just plain confusing to add to your project. Not to mention a lot of the solutions out there are more complex than they need to be! Using Unity’s Event Handlers can simplify and clean up your code while offering better functionality than other solutions. I’ve seen a lot of solutions out there to move scene objects, create inventory UI, or make draggable UI. Many or maybe most of those solutions are overly complicated because they don’t make full use of Unity’s event handlers (or the Pointer Event Data class). Did I mention these handlers work with both the “new” and the “old” input systems. So learn them once and use them with either system.So let’s take a look at what they can do! If you just want to see the example code, you can find it here on GitHub. Event handlers are added by including using UnityEngine.EventSystems and then implementing one or more of the interfaces. For example, IPointerEnterHandler will require an OnPointerEnter function to be added. No surprise - this function will then get called when the point enters (the rect transform of) the UI element. The interfaces and corresponding functions work on scene objects. But! The scene will need a camera with a physics raycaster and more on that as we move along. Below are the supported events (out of the box) from Unity:\n• None IPointerDownHandler - OnPointerDown - Called when a pointer is pressed on the object\n• None IPointerUpHandler- OnPointerUp - Called when a pointer is released (called on the GameObject that the pointer is clicking)\n• None IPointerClickHandler - OnPointerClick - Called when a pointer is pressed and released on the same object\n• None IInitializePotentialDragHandler - OnInitializePotentialDrag - Called when a drag target is found, can be used to initialize values\n• None IBeginDragHandler - OnBeginDrag - Called on the drag object when dragging is about to begin\n• None IDragHandler - OnDrag - Called on the drag object when a drag is happening\n• None IEndDragHandler - OnEndDrag - Called on the drag object when a drag finishes\n• None IDropHandler - OnDrop - Called on the object where a drag finishes\n• None IUpdateSelectedHandler - OnUpdateSelected - Called on the selected object each tick\n• None ISelectHandler - OnSelect - Called when the object becomes the selected object\n• None IDeselectHandler - OnDeselect - Called on the selected object becomes deselected\n• None IMoveHandler - OnMove - Called when a move event occurs (left, right, up, down)\n• None ISubmitHandler - OnSubmit - Called when the submit button is pressed\n• None ICancelHandler - OnCancel - Called when the cancel button is pressed\n\nA simple use case of the event handlers is a UI popup to show the player information about an object that the pointer is hovering over. This can be accomplished by using the IPointerEnter and IPointerExit interfaces. For my example, I choose to invoke a static event when the pointer enters the object (to open a popup) and when the pointer exits (to close the popup). Using events has the added benefit that other systems beyond the popup menu can also be aware of the event/action - which is huge and can allow more polish and juice to be added. It also means that information about the event and the object can be passed with the event. In my particular case, the popup UI element is listening to these events and since the PointerEventData is being passed with the event, the popup UI element can appear on-screen near the object. In my case rather than place the popup window at the same location as the pointer I’m using a small offset.\n\nFirst, when the pointer is down on the UI element a new prefab instance is created and the “objectBeingPlaced” variable is set. Setting this variable allows us to track and manipulate the object that is being placed. Then when the pointer comes up objectBeingPlaced is set to null to effectively place the object. But the real magic here is in the OnUpdateSelected function. This is called “every tick” - effectively working as an update function. To my understanding, this is only called while the object is selected - so this is no longer called once the pointer is up or at the very least when the next object is selected. I haven’t done any testing, but I’d guess there are slight performance gains using this approach vs. an update function on each button. Not to mention this just feels a whole lot cleaner. Inside the OnUpdateSelected function, we check if objectBeingPlaced is null, if it’s not then we want to move the object. To move it we’re going to do some raycasting. To keep things simple, I’ll create a plane and raycast against it. This limits the movement to the plane, but I think that’ll cover most use cases. This is SO much simpler and cleaner than what I’ve done in the past. If you haven’t seen the Plane class, I just discovered it a few weeks back, the plane is defined by a normal vector and a point on the plane. It also has a built in raycast function which is much simpler to use than the physics raycaster - albeit also more limited in functionality.\n\nHow about a double click? There are a LOT of solutions out there that are way more complex than what appears to be needed. All kinds of coroutines, updates, variables…. You just don’t need it. Unity gives us a built-in way to register click count. So let’s make use of it. The real star of the show in the code is the OnPointerClick function and the PointerEventData that is passed into the function. here all we need to do is check if eventData.clickCount is equal to 2. If it is then there was a double click. In addition, this should work with UI and scene objects (need a physics raycaster) equally well. The rest of the code presented just adds a bit of juice and some player feedback. We cache the scale of the object in the Start function. Then when the pointer enters the object we tween the scale up and likewise when the pointer exits we tween the scale back down to its original size. As a side note registering the double click did not work for me with the new input system version 1.0.2. An update to 1.3 fixed the issue. There was no issue with the “old input system.”\n\nOkay, so what if you want to move an object around in the scene, but that object is already in the scene? This is very similar to the example above, however (in my experience) we need an extra step. We need to set the selected gameObject - without doing this the OnUpdateSelected function will not get called as the event system doesn’t seem to automatically set a scene object as selected. Setting the selected object needs to happen in the OnPointerDown function. Then in the OnPointerUp function, the selected object gets set to null - this prevents any unwanted interactions from the object still being the “selected” object. The other bit that I’ve added is the OnCancel function (and interface). This gets invoked when the player presses the cancel button - which by default is set as the escape key. If this is pressed I return the gameObject to its starting location and again set the selected object to null. This is a “nice to have” and really easy to add. Who doesn’t like a draggable window? Once again these are easy to create using a handful of event handlers.\n\nhierarchyLet’s get right to the star of the show, which is the OnBeginDrag and OnDrag functions. When the drag begins we want to calculate an offset between the pointer and the location of the object. This prevents the object from “snapping onto the pointer” which doesn’t feel great doubly so if the object is large. Next, we need to set the object to be the last sibling. Since UI objects are drawn in the order that they are in the hierarchy this helps to ensure the object being dragged is on top. If you have a more complex UI structure you may need to get more clever with this and change the parent transform as well (we do this a bit in the next example). In the OnDrag function, we simply we simply set the position (excuse the typo - no need for the double transform call) to the position of the pointer minus the offset. And that’s all it takes to drag a UI object. But! I did add a bit more juice. The OnPointEnter and OnPointer Exit functions tween the scale of the object to give a little extra feedback. Then in OnEndDrag I play a simple SFX to give yet a bit more polish.\n\nOkay. Now the more complicated bit. The inventory tile itself. The big idea here is we want to drag the tile around, keep it visible (last sibling) and we need to toggle off the raycast target while dragging so that the inventory slot can register the OnDrop event. Also, if the player stops dragging the item and it’s not on top of an inventory slot then we’re going to send the item back to its starting slot. At the top, there are two variables. The first tracks the offset between the item and the pointer, just like in the previous example. The second will track which slot (parent) the item started in. Then OnBeginDrag, we set the starting slot variable, set the parent to the root object (canvas) and set this object to the last sibling. These last two steps help to keep the item visible and dragging above other UI objects. We then cache the offset and set the raycast target to false. This needs to be set to false to ensure that OnDrop is called consistently on the inventory slot - i.e. it only gets called if the raycast can hit the slot and isn’t blocked by the object being dragged. An important note on the raycast target: RaycastTarget needs to be set to false for all child objects too. In my case, I turned this off manually in the text object - but if you have a more complex object a Canvas Group component can be used to toggle this property for all child objects. Moving on to the OnDrag function, this looks just like the example above, where we set the position of the object to the pointer position minus the offset. Finally, the OnEndDrag function is where we need to toggle the raycastTarget back on so that we can move it again later. Also now that the dragging has ended we want to see if the current parent of the item is an inventory slot. If it is - it’s all good - if not we want to set the parent back to the starting slot. Because of the vertical layout group setting the parent will snap the position of the item back to it’s starting position. It’s worth noting that OnEndDrag (item) gets called after OnDrop (slot) which is why this works. Note: I also added a SFX to the OnEndDrag. This is optional and can be done in a lot of different ways. I had hoped to go into a bit more detail on the Pointer Event Data class, but this post is already feeling a bit long. That said there is a ton of functionality in that class that can make adding functionality to Event Handlers so much easier. I’d also argue that a lot of the properties are mostly self explanatory. So I’ll cut and paste the basic documentation with a link to the page here. button The InputButton for this event. clickTime The last time a click event was sent. dragging Determines whether the user is dragging the mouse or trackpad. enterEventCamera The camera associated with the last OnPointerEnter event. hovered List of objects in the hover stack. lastPress The GameObject for the last press event. pointerCurrentRaycast RaycastResult associated with the current event. pointerDrag The object that is receiving OnDrag. pointerPress The GameObject that received the OnPointerDown. pointerPressRaycast Returns the RaycastResult associated with a mouse click, gamepad button press or screen touch. pressEventCamera The camera associated with the last OnPointerPress event. pressPosition The screen space coordinates of the last pointer click. rawPointerPress The object that the press happened on even if it can not handle the press event. scrollDelta The amount of scroll since the last update.u seDragThreshold Should a drag threshold be used? IsScrollingIs scroll being used on the input device. currentInputModule A reference to the BaseInputModule that sent this event. selectedObject The object currently considered selected by the EventSystem."
    },
    {
        "link": "https://discussions.unity.com/t/different-ways-to-use-the-input-system/1586728",
        "document": ""
    },
    {
        "link": "https://docs.unity3d.com/2023.2/Documentation/Manual/class-InputManager.html",
        "document": "The Input Manager window allows you to define input axes and their associated actions for your Project. To access it, from Unity’s main menu, go to Edit > Project Settings, then select Input Manager from the navigation on the right.\n\nThe Input Manager uses the following types of controls:\n• Key refers to any key on a physical keyboard, such as W, Shift, or the space bar.\n• Button refers to any button on a physical controller (for example, gamepads), such as the X button on a remote control.\n• A virtual axis (plural: axes) is mapped to a control, such as a button or a key. When the user activates the control, the axis receives a value in the range of [–1..1]. You can use this value in your scripts \n\n A piece of code that allows you to create your own Components, trigger game events, modify Component properties over time and respond to user input in any way you like. More info .\n\nThe Physical keys option allows you to map key codes to the physical keyboard layout, rather than to the language-specific layout that may vary between users in different regions.\n\nFor example, on some keyboards the first row of letters reads “QWERTY”, and on others it reads “AZERTY”. This means if you scripted specific controls to use the well known “WASD” keys for movement, they would not be in the correct physical arrangement (like the arrow-key arrangement) on an AZERTY-layout keyboard.\n\nWith Physical Keys enabled, Unity uses a generic ANSI/ISO “Qwerty” layout to represent the physical location of the keys regardless of the user’s actual layout. This means if you specify the “Q” key, it will always be the left-most letter on the first row of letter keys, even if the user’s keyboard has a different letter in that position.\n\nNote, you should not read key input for in-game text input, because this will not allow users to enter non-Latin characters. Instead, use .\n\nEvery Project you create has a number of input axes created by default. These axes enable you to use keyboard, mouse, and joystick input in your Project straight away.\n\nTo see more about these axes, open the Input Manager window, and click the arrow next to any axis name to expand its properties.\n\nEach input axis has the following properties:\n• Between –1 and 1 for joystick and keyboard input. The neutral position for these axes is 0. Some types of controls, such as buttons on a keyboard, aren’t sensitive to input intensity, so they can’t produce values other than –1, 0, or 1.\n• Mouse delta (how much the mouse has moved during the last frame) for mouse input. The values for mouse input axes can be larger than 1 or smaller than –1 when the user moves the mouse quickly.\n\nTo add a virtual axis, increase the number in the Size field. This creates a new axis at the bottom of the list. The new axis copies the properties of the previous axis in the list.\n\nTo remove a virtual axis, you can either:\n• Decrease the number in the Size field. This removes the last axis in the list.\n• Right-click any axis, and select Delete Array Element.\n\n Note: You can’t undo this action.\n\nTo copy a virtual axis, right-click it and select Duplicate Array Element.\n\nTo map a key or button to an axis, enter its name in the Positive Button or Negative Button property in the Input Manager.\n\nMouse buttons are named and so on.\n\nYou can also query input for a specific key or button with and the naming conventions specified above. For example:\n\nAnother way to access keys is to use the enumeration.\n\nTo access virtual axes from scripts, you can use the axis name.\n\nFor example, to query the current value of the Horizontal axis and store it in a variable, you can use like this:\n\nFor axes that describe an event rather than a movement (for example, firing a weapon in a game), use instead.\n\nIf two or more axes have the same name, the query returns the axis with the largest absolute value. This makes it possible to assign more than one input device to an axis name.\n\nFor example, you can create two axes named Horizontal and assign one to keyboard input and the other to joystick input. If the user is using the joystick, input comes from the joystick and keyboard input is null. Otherwise, input comes from the keyboard and joystick input is null. This enables you to write a single script that covers input from multiple controllers.\n\nYou can use input from the Horizontal and Vertical axes and the method to move a GameObjectThe fundamental object in Unity scenes, which can represent characters, props, scenery, cameras, waypoints, and more. A GameObject’s functionality is defined by the Components attached to it. More info\n\nSee in Glossary in XZ space (forward, back, left, or right). Add the following code to the method on a script attached to the GameObject you want to move:\n\nrepresents the time that passed since the last frame. Multiplying the variable by ensures that the GameObject moves at a constant speed every frame."
    },
    {
        "link": "https://gamedevbeginner.com/input-in-unity-made-easy-complete-guide-to-the-new-system",
        "document": "Unity’s new Input System is now out of preview and is an official part of Unity.\n\nBuilt as an alternative to the old Input Manager, the new Input System is fully featured, highly customisable…\n\nWhile the new Input System can be more intuitive than the old Input Manager, it also has many more settings, options and moving parts.\n\nSome of which you might need, but some of which you might not.\n\nAnd while it offers many new and sought-after features, such as dynamic rebinding, local multiplayer and built-in support for modern controllers, it can be a little complicated to get to grips with at first.\n\nSo is it worth it?\n\nOr is Unity’s old system, the Input Manager enough?\n\nOr maybe an Asset Store plugin such as Rewired is a better option than either of Unity’s built-in systems?\n\nIf you’re not sure what the best option is for you, don’t worry.\n\nBecause in this in-depth guide you’ll learn everything you need to know about getting started with Unity’s new Input System, how it compares to the old system and whether or not you’d be better off using one of the Unity Asset Store’s purpose-built solutions instead.\n\nThere’s a lot to cover, so let’s get started!\n\nHow to manage input in Unity\n\nWhat is the best way to handle input in Unity?\n\nDo you need to use an input management system at all?\n\nAfter all, it’s possible to listen for and detect key inputs using just the Input Class.\n\nWhile this method does work, there’s a problem.\n\nThe input trigger, in this case, the mouse click or the Spacebar, is directly linked to the function it performs.\n\nThis means that, if you ever want to change which button, key or input triggers that function, you’ll have to go back into the script to do it.\n\nWhich could easily become difficult for you to manage later on as your project grows.\n\nBut that’s not the biggest problem…\n\nDirectly connecting the input trigger to the script in this way basically makes it impossible for the player to change.\n\nFor many players, not being able to modify the controls might mean they enjoy it less but, for some players, it may make it impossible to play at all.\n\nHow can you make an input system that’s easy for you to manage and comfortable for your players to use?\n\nThe answer is to use a modular input system.\n\nMeaning that the input that triggers an action is kept separate from the script that actually makes it happen.\n\nFor example, instead of having a script listen for an input trigger, such as the player pressing a specific key, the script listens for an action that is, in turn, bound to one, or more, real buttons, keys or other input devices.\n\nThe script doesn’t need to know what triggered the action and the input isn’t directly attached to the script.\n\nWhich means that you can easily change the binding that triggers an action without needing to change the script that implements it in the game.\n\nThat’s the theory, but how can you actually do it?\n\nLet’s start with how things used to work in Unity…\n\nThe old Input Manager in Unity\n\nIf you’ve done anything with input at all in Unity, you’ve probably already been using a modular input management system: Unity’s old input system, the Input Manager.\n\nThe Input Manager is the original input system for Unity and, at the time of writing, is still the default method for managing input.\n\nWhile it’s basic, it still works in a modular way, by assigning real keypresses to virtual inputs which can then be detected in scripting.\n\nWhich is good, as it means that you wouldn’t need to change every script that responds to an to input, even if you changed the button that triggered it.\n\nWhat’s more, the Input Manager is simple to use and fast to set up, making it easy to add basic controls quickly.\n\nBut should you still be using it?\n\nIs the old Input Manager still a good option?\n\nThe old Input Manager is ideal for simple projects or prototypes, as it allows you to set up basic controls very, very easily.\n\nBut, if you want to support newer control devices, re-mappable controls or manage input for different types of gameplay, you will probably be better off using a more advanced plugin or Unity’s new Input System.\n\nUsing the new Input System in Unity\n\nUnity’s new Input System, which has been in preview up until recently, is now an official part of Unity.\n\nThe new system, which is offered as an alternative to the legacy Input Manager, was created to address many of the old system’s shortcomings.\n\nIssues like compatibility across platforms, not being able to easily customise and remap controls and limited extensibility are all problems that the new Input System aims to solve.\n\nAlthough the new Input System is definitely much more capable than the old Input Manager; at first glance, it can seem like it’s much more difficult to set up and use.\n\nBut it doesn’t have to be…\n\nWhile there are many different ways to set up, use and modify the new Input System (which is a deliberate move on Unity’s part to make the new System as transparent and customisable as possible), you don’t need to know how to use all of them to still get the most out of it.\n\nIn fact… setting up basic controls can be very straightforward.\n\nSo… how does it work?\n\nHow to set up an input using the new Input System in Unity\n\nAt the moment, the new Input System is not the default method of handling input in Unity.\n\nSo, before you do anything, you’ll need to install it from the Package Manager.\n\nHow to install the new Input System\n\nYou’ll need to make sure you’re using Unity 2019.1 or newer. You can then download and install the Input System using the Package Manager.\n• If it’s not already selected, choose Unity Registry:\n• You’ll then see a warning asking you to switch Input handling from the old system to the new one\n\nSwitching to the new Input System disables the old Input Manager and Input Class. This means that anything that uses the Input Class, which includes functions that get Raycasts with the mouse position, won’t work anymore.\n\nIf you want to be sure of which input system you’re using, go to Edit > Project Settings then Player > Other Settings and look for the ‘Active Input Handling’ setting.\n\nYou can also use this option to switch back to the old system or enable both systems at the same time.\n\nHow the new Input System works\n\nThere are a number of different parts that make up the new Input System and, while they’re all useful in their own way, you don’t necessarily need all of them.\n\nIn fact, just as it was previously possible to use the old system to get input directly from a device (e.g. with Get Key Down), you can also get device input directly from the current keyboard, mouse or gamepad using the new Input System, without setting up any Actions or Control Schemes.\n\nEssentially this is the new Input System Equivalent of Get Key, Get Button and Input.mousePosition and is an easy way to get the new Input System working quickly.\n\nWhile this method of adding input is fast and convenient, which certainly has its uses, all it does is emulate the way the old Input Manager used to work.\n\nWhich means, if you plan to use the new Input System in this way across your entire project, you may as well use the old system.\n\nUnity’s new Input System can seem a little complex at times, as it includes many different moving parts, each with a designed purpose.\n\nHowever, understanding what each part is designed to do and which parts you actually need will help you to get the most out of the new system without overcomplicating things.\n\nSo what makes up the new Input System?\n\nTypically, there are six main parts to the new Input System:\n• Input Actions Assets – which store sets of Control Schemes, Action Maps, Actions and their Bindings.\n• Control Schemes – which define device combinations that the player can use (e.g. keyboard and mouse, gamepad etc.).\n• Action Maps – which separate different types of activity (e.g. Menu Controls, Gameplay, Driving).\n• Actions – single events that will trigger functions in the scene (e.g. Jump, Fire, Move)\n• Bindings – the physical controls, specific to a Control Scheme, that will trigger Actions (e.g. spacebar, left stick etc.).\n• Player Input Component – which can be used to connect all of the above to a player object\n\nBefore you can do anything, however, you’ll need to create an Input Actions Asset.\n\nWhile it’s possible to create Actions in code, or embed actions directly into Monobehaviours, the Input Actions Asset editor is a convenient way to manage all of your games input from a central location.\n\nDifferent to the Input Manager, the Input Actions editor isn’t a menu accessible from the Project Settings (although, confusingly, you will find an Input System Package menu item in the Project Settings, but those settings are optional and you can ignore them for now).\n\nInstead, Input Action Assets, which can contain an entire set of inputs and controls, sit in your project as an asset.\n\nSo, before you can get started you’ll need to create one.\n\nTo do that, right-click in the Project window and click Create > Input Actions, or select Assets > Create > Input Actions from the menu.\n\nOnce it’s created, double click, or select edit in the Inspector to open the Input Actions editor.\n\nOnce the Input Actions editor is open, you’ll probably also want to select Auto Save at the top of the window.\n\nWithout Auto Save checked, you’ll need to manually save the asset to apply any changes which, as someone who’s new to the Input System, I found easy to forget.\n\nWith the Input Actions Asset open, it’s time to start adding controls, starting with a Control Scheme…\n\nControl Schemes define the different types of device, or combination of devices, that people might use to control your game.\n\nSetting up different Control Schemes allows you to keep one set of controls separate from another.\n\nAnd, later, as you create Actions and add device bindings, you’ll be able to decide which Bindings belong to which Control Schemes.\n\nYou don’t, technically, need to create separate Control Schemes to support different devices.\n\nIn fact, you don’t need to create a Control Scheme at all. You can just add all of your Actions, with Bindings from all of the types of device you want to support, without adding an explicit Control Scheme.\n\nHowever, there are benefits to keeping different device types separate.\n\nSo when would you use separate Control Schemes and why?\n\nWhen to use Controls Schemes with the new Input System in Unity?\n\nOne benefit of using different Control Schemes in the new Input System is to explicitly separate different device setups from one another.\n\nThere are a couple of reasons you might want to do this.\n\nFor example, this is useful when you want to know that the player is either using one set of controls or another.\n\nYou can use this information to display in-game prompts correctly, for example:\n\nYou get the idea.\n\nBy knowing which Control Scheme is currently in use, you can give the player the correct prompt for contextual actions at any given time.\n\nBeing able to differentiate between Control Schemes is also useful for local multiplayer.\n\nIf Unity can understand what makes up a set of controls, it can assign devices to players automatically based on the devices that are connected.\n\nLet’s say, for example, you want to set up a game with player 1 using the keyboard and player 2 using a gamepad.\n\nIf you add both keyboard and gamepad controls to a single Control Scheme, Unity won’t know that one player is only likely to use one device to control the game.\n\nBoth devices are considered to be part of one player’s control set up and won’t be made available to player 2.\n\nSeparating them out means that Unity knows that, while player 1 uses the keyboard, player 2 can use the gamepad, and assigns them automatically.\n\nKeeping the Control Schemes separate helps to define which devices go together so, even if they’re not immediately useful, setting them up now might help you to keep your controls organised later on.\n\nTo create a new Control Scheme, click the Control Scheme dropdown in the top left-hand corner of the Input Actions editor.\n\nIf you haven’t created a Control Scheme yet, it’ll read No Control Schemes, otherwise, it will show the current selection.\n\nNext, you’ll be able to add devices to the Control Scheme.\n\nFor example a Keyboard and a Mouse:\n\nWhen creating a Control Scheme, you’ll need to specify what types of devices it should accept input from.\n\nSetting a device as required means that a device of that type will need to be connected and available for the Control Scheme to be usable.\n\nFor example, it would be impossible to use a Keyboard Control Scheme without a keyboard, and likewise a Gamepad Control Scheme without a gamepad of some sort.\n\nYou might add an optional device to a Control Scheme if you want to allow an additional device to be used, but not be required for the Control Scheme to work.\n\nFor example, using the keyboard and, optionally the mouse as well, where the keyboard is required to play but the mouse isn’t.\n\nIn that scenario, the Control Scheme would still be usable with just a keyboard.\n\nGenerally, if you’re using Control Schemes to define device configurations you’ll want to set devices as required.\n\nHowever… be careful to not create two Control Schemes in the same asset that each require the same type of device.\n\nIf you do, Unity will expect two devices of that type to be connected in order to satisfy the requirements of either of the Control Schemes.\n\nSo, if you want to include the same type of device in multiple Control Schemes, make it optional.\n\nLater, as you add Bindings to Actions, you’ll be able to assign them to the Control Schemes you create.\n\nMost bindings will be specific to only a particular Control Scheme however there are some general bindings that can be used across multiple Control Schemes.\n\nBefore you can assign a Binding to an Action, however, you need to create an Action.\n\nAnd before you can do that, you’ll need to create at least one Action Map.\n\nSo let’s do that next.\n\nAn Action Map contains a set of related Actions.\n\nClick the + icon at the top of the Action Map column to create a new one.\n\nIf you’re anything like me, when you first create an Action Map, you might struggle to think of a sensible name for it.\n\nThis happened to me because, when I first made one, I didn’t know what it was meant to be for.\n\nWhy make different Action Maps? What are they for?\n\nWhat are Action Maps used for?\n\nAction Maps hold all of the Actions for a certain type of behaviour.\n\nFor example, you might place all of your Character’s general Actions, such as movement, jumping, firing weapons etc, in one Action Map called Player or Gameplay. whatever makes most sense to you.\n\nThat Action Map, and the Actions you set up in it, will then be used to trigger all the gameplay functions in the game.\n\nSo why bother making multiple Action Maps if you can put everything in one?\n\nShould you be splitting gameplay actions across different Action Maps?\n\nAnd if you do, how should you split it up.\n\nTurns out, the reason why you’d use multiple Action Maps, and how you can decide what goes where is actually pretty simple.\n\nWhen to use multiple Action Maps\n\nSo when should you use multiple Action Maps in Unity’s new Input System?\n\nLet’s say, for example, that you’ve made a first-person game that involves a lot of shooting but also some driving.\n\nWhile you might use one Action Map to handle all of the player’s Actions (such as running around and firing etc.) you might want to use another, different Action Map, to handle driving controls.\n\nIt makes sense to place all of the actions that control the player in a single Action Map, as it’s highly likely that they will all be used together.\n\nY Button jumps, Right Trigger fires and pressing B slides you along the floor.\n\nUntil the player gets in a car.\n\nBecause now Right Trigger needs to accelerate, B uses the Handbrake and, instead of jumping, Y rolls you out of the car.\n\nAfter all, you could simply map those existing actions (Fire, Slide & Jump) to the car’s controls and that’ll work.\n\nHowever… one reason that you might want to keep them separate is so that the player can rebind the driving controls without affecting the player controls.\n\nLater, when you come to connect the Input Actions with the player object in your game, you’ll be able to switch between different Action Maps at runtime, allowing you to enable and disable an entire set of controls.\n\nFor now though, all you need to remember is that controls that go together should typically be added to a single Action Map.\n\nSuch as separating gameplay and menu controls, for example.\n\nThe Input System UI Module already includes a Default Input Actions asset that handles basic button and menu navigation for you.\n\nHowever, if you decide to set up menu controls manually, giving them their own Action Map, separate from the game’s controls, makes a lot of sense.\n\nIn Unity’s new Input System, Actions connect the physical inputs of a control device with something that happens in the game.\n\nIt’s how you make certain buttons, do certain things.\n\nBasically, anything that the person playing the game can do in the game.\n\nActions are specific to Action Maps, while the Bindings that you apply to Actions are specific to individual Control Schemes.\n\nHow to add a new Action\n\nTo add a new Action, simply click the + symbol at the top of the column.\n\nOnce you’ve created an Action, you’ll need to set the Action Type, which determines how an Action should be triggered.\n\nThere are three different types:\n• Button – The default setting, for one-shot actions triggered by a button or key press.\n• Value – Which provides continuous state changes as they happen, switching to whichever one is most dominant if there are multiple inputs set up. Use this for analogue controls, like movement.\n• Pass Through – This works in the same way as Value except that the disambiguation process, where the most dominant device is selected, is ignored.\n\nWhich Action Type should you use?\n\nGenerally, pick Button whenever you need to trigger an Action once with a control.\n\nValue and Pass Through are suitable for continuous analogue controls, such as movement or accelerator controls.\n\nWhich one you choose depends on how the active input source should be selected. Value will prioritise the strongest value, while Pass Through prioritises the most recent regardless of how strong it is.\n\nWhen using the Value or Pass Through Action Types, you’ll see an additional option to set the Control Type.\n\nThis allows you to specify what type of input you’re expecting to get from the input and affects which Bindings are available.\n\nIn many cases, you can leave it set to Any. If, however, the Binding you want to assign is greyed out, it may help to change this setting to match the Binding you’re targeting (for example, a Thumbstick typically outputs a Vector 2 value). Setting this value can also be helpful when rebinding controls, as the Dynamic Rebinding Operation uses the Control Type to decide what type of input controls can be assigned to it.\n\nOnce you’ve created an Action and set the Action Type, it’s time to actually connect it to a real input device by adding a Binding.\n\nInput Bindings are the specific keys, buttons and directional controls that your player will use to physically play the game.\n\nThe basic type, simply called a Binding and added by default when creating a new Action, connects a single control via its Binding Path.\n\nThe Binding Path could be a button, a keyboard key or an analogue trigger, or it could be a more general control, such as the left stick of a controller.\n\nOr it could be a composite of several bindings to create composite directional movement, such as for WASD keys.\n\nTo add a new Binding, right-click the Action you want to assign it to or click the + symbol to the right of the Action name.\n\nTo set the Binding Path, find the control you want to use from the list or click the Listen Button and use the control you want to assign to find it quickly.\n\nButton Action Types generally accept single inputs, such as a specific key or one direction of a thumbstick, whereas Value and Pass Through Action Types can accept a general control, such as a thumbstick or the d-pad.\n\nThis is useful for quickly creating directional controls without manually setting up a Composite Binding.\n\nAs well as specific, device-based controls, it’s possible to assign a standardised Usage to an Action.\n\nWhen selecting a Binding Path you’ll see an additional menu marked Usages.\n\nThe Usages list defines a number of commonly used commands that are typically bound to specific buttons on different devices.\n\nFor example, the Back command is commonly the B Button and Circle Button on Xbox and Playstation controllers while, on Keyboard, it’s the Escape key.\n\nOr the Primary Action, which is typically the A Button or Cross Button on Xbox and Playstation controllers respectively.\n\nOr, on a Keyboard, the Submit command, which is the Return key.\n\nThese common Usages allow you to specify a control without knowing what will actually trigger it.\n\nBut why use this feature? And when is it useful?\n\nOne example would be the buttons that are used to move forward and backwards through menus.\n\nFor example, you might be tempted to assign a menu select button to the bottom button, the South button, for controllers (A on Xbox, Cross on PlayStation).\n\nExcept, not on Switch…\n\nAs you probably already know, the A Button on Switch is the Rightmost button on the gamepad, the East button, not the South like with Xbox and PlayStation.\n\nThis would make your in-game navigation inconsistent with OS navigation on that platform.\n\nUsing Usages, however, you can assign Menu Select to the Primary Action and Menu Back to Back, which would keep menu navigation consistent with whatever platform the game is on while keeping gameplay controls consistent across different platforms.\n\nYou can see examples of some common Usages in the Default Input Actions asset that is created with the UI Input Module.\n• What is the UI Input Module?\n\nWhile you’ll be able to use general control paths for some inputs, you will often need to create custom Bindings yourself.\n\nExamples of when you’ll need to do this include when adding WASD movement, creating an Axis across two buttons or when setting up key combinations.\n\nLuckily, there are Binding types that you can use to do exactly that.\n\nA One Dimensional Composite Axis basically charts a point between two bindings, one positive and one negative.\n\nAn example of a 1D Axis would be forward and back on a keyboard, bound to the W and S keys.\n\nAs a Composite Input, the input values are combined, meaning if you were to press both at the same time, they will cancel each other out (unless you specify otherwise).\n\nIt’s also possible to change the minimum and maximum values of the Axis from the default of -1 to 1.\n\nHowever changing the maximum value doesn’t change what the input device delivers, which is typically a value between 0 and 1, or 0 and -1 when assigned to the negative.\n\nFor example, if you change the maximum value to 2, the W Key will still, by default, deliver a value of 1, although you can use a Scale Processor to change that.\n\nAlso, it’s worth noting that uneven values, for example, an Axis between -1 to 2, even if the positive value has been scaled up to meet the maximum value, will still cancel out at zero, and that the progression along the axis scale between the middle and the two extremes will be the same.\n\nPut simply, the middle will always be the middle.\n\nA Two Dimensional Composite Axis will provide a Vector 2 value from two axes.\n\nThis is ideal for movement controls and similar types of input.\n\nHowever… the value you get from a 2D Composite can vary depending on the Composite Mode.\n\nThe Composite Mode changes how the two axes affect each other.\n\nWhich one you select depends on the type of response you want from the control.\n\nFor example, Digital provides a square response. So if you push Up and Right, you’ll get a Vector 2 value of 1,1.\n\nUsing this method is fine if the vertical and horizontal axes aren’t linked.\n\nFor example, tank movement, where left and right turns while up and down moves, wouldn’t necessarily require the two axes to affect each other although you might bind them together to a single control, such as a thumbstick, for convenience.\n\nOne example of when not to use the Digital Mode, however, would be when setting up eight-way directional movement.\n\nThis is because the magnitude, the length, of a Digital Mode Vector with a value of 1,1 (or up and right pressed at the same time) would be around 1.4, which means that your player will move faster diagonally than in a single direction where the magnitude would be 1.\n• Why diagonal movement is faster in Unity (and how to fix it)\n\nHow can you fix this?\n\nThe solution is to normalise the vector.\n\nDigital Normalized, which is the default mode, works in the same way as Digital except that the Vector is normalised, producing a magnitude of one in all directions.\n\nUnlike the Digital Mode, where pressing two adjacent directional buttons (i.e diagonal movement) produces a Vector 2 value of 1,1 and magnitude of around 1.4, using Digital Normalized produces a Vector 2 value of around 0.7,0.7 to maintain a magnitude of 1 in all directions.\n\nIf you’re not sure what I mean by a vector’s magnitude, try Unity’s video on vector maths for more information.\n\nThis means that, when creating eight-way directional controls, your player will move at the same speed diagonally as in straight directions when using the Digital Normalized Mode.\n\nYou can also use Digital Normalized to snap an Analogue Control, such as a thumbstick to eight directions.\n\nSo, for example, you could create retro 8-way controls with an analogue thumbstick by using this mode.\n\nThe Analogue Mode, provides an even circle response in all directions, as you’d get from a thumbstick or joystick.\n\nJust like with Digital Normalized the magnitude is 1 in all directions, just in a complete circle instead of a diamond.\n\nBut, this only works when using analogue controls, such as an analogue stick.\n\nWhile testing for this article, I found that using this setting with a composite of digital buttons, such as the WASD keys or the directions of a d-pad, resulted in a vector value that wasn’t normalised, just like when using the Digital Mode.\n\nSo, for best results, it makes sense to only use the Analogue Mode with analogue controls, such as the left or right sticks of a controller.\n\nHow to recognise two buttons being pressed at once (Button with a Modifier Composite)\n\nSometimes, you may only want to only register an input while the player is pressing two (or more) keys or buttons at once.\n\nYou’ve used commands like this before, such as CTRL+Z to undo, or CMD-S to save in an application or, in games, multi-button moves in a fighting game.\n\nUnity’s input system also allows you to create bindings that require one or two modifier keys using the Button with One Modifier Composite or Button with Two Modifiers Composite bindings.\n\nThese allow you to add a Button binding, with up to two Modifiers that will gate the Action from firing until you press them in combination.\n\nThere is, however, a downside…\n\nThere’s currently a known limitation of the Unity Input System that one Action cannot pre-empt the input of another.\n\nThis means that if you assign a Jump action to the B Button, and a Dive action to a combination of Left Trigger + B, both the Dive and Jump Actions will be triggered when the Left Trigger and B Button are pressed.\n\nObviously, this isn’t helpful if you planned to use modifier buttons to extend the controls of a gamepad.\n\nSo how can you fix it?\n\nHow to avoid triggering both actions when using a Modifier Composite\n\nUntil this issue is officially fixed by Unity (it’s understood to be high priority problem), if you want to use buttons with modifiers, you’ll need to use a workaround.\n\nOne simple solution is to separate the modifier and button elements into two different Actions\n\nThe Button, which is just a normal button, and the Modifier, which will be used to report if the modifier is true or false.\n\nThen in your script, when the Button is triggered, you can simply check to see if the Modifier was also held down, triggering one Action if it was and a different Action if it wasn’t.\n\nThis basic workaround works in a similar way to the original, intended, functionality, in that it gates the trigger button’s secondary function until the Modifier is pressed.\n\nIn this case, I’m using it to monitor when the Modifier is pressed or released, storing the result in a Bool.\n\nFor this to work, however, you’ll need to trigger the Action when the Modifier is Pressed and when it’s Released.\n\nBy default, the Button Action only triggers when the button is pressed (The equivalent of Get Key Down in the old system).\n\nTo register both the Press and Release, you can change the behaviour of the button with an Interaction, in this case, the Press Interaction.\n\nInteractions, along with Processors, are properties that can be added to an Action or a Binding to change how it is recognised or to modify the resulting value.\n\nWhen setting up your game’s controls, Interactions and Processors can be used to change how input is interpreted.\n\nFor example, tapping is not the same as holding a button down.\n\nYou may wish to make a gun trigger more, or less sensitive.\n\nOr, if you’re anything like me, you will absolutely have to enable invert look.\n\nI honestly can’t play a first-person game any other way.\n\nHelpfully, all of that functionality, and more, can be created by adding Properties to an Action or directly to the Binding itself.\n\nProperties include Interactions, Processors and for Actions, the Action Type, which you will have already set when creating the Action.\n\nInteractions change what is required for an input to trigger an Action, while Processors modify the value that’s received.\n\nLet’s look at Interactions first.\n\nInteractions change what the player needs to do in order for an input to trigger.\n\nFor example, there are different ways that you can press a button.\n\nYou can press a command quickly for a light attack, or longer for a heavy one.\n\nYou can press and release, triggering an action when you let go of a button.\n\nOr you can double-tap it quickly.\n\nTo add an Interaction, select an Action or Binding and click the + symbol beside Interactions in the Properties Panel.\n\nThe Hold interaction modifier won’t trigger until the button has been held down for a minimum amount of time.\n\nWhile Press allows you to specify if you want the Action to be triggered on the press or release of a button or both.\n\nYou’ll also find Interactions designed for touch devices, such as Tap, which will only fire after a quick press and release, Slow Tap, which works in a similar way, except over a longer duration.\n\nAt first glance, Slow Tap appears to work in the same way as Hold.\n\nExcept, while Hold triggers after a set duration, Slow Tap triggers on release after a set duration.\n\nUntil you release it, it won’t do anything.\n\nLastly, there’s Multi-Tap which is for measuring repeated taps, or clicks, within an amount of time.\n\nSuch as a double click for example, Or for pressing a button twice, quickly, to perform a dodge.\n\nWhat to do if Multi-Tap isn’t working\n\nThere’s currently a known issue with Multi-Tap which, if you don’t know about it, could make you think that it just doesn’t work at all.\n\nMulti-Tap won’t work unless it is on the only Binding assigned to an Action.\n\nSo if you, as I did, assign both a regular button press and a double-tap press to an Action the Multi-Tap won’t work.\n\nIf you need to trigger the Action from two bindings in the same Control Scheme, and one of them is a Multi-Tap, then one workaround is to duplicate the Action and simply have one trigger the other to achieve the same result.\n\nJust like with Modifier Composites, however, there’s currently no way to have one input prevent the firing of another so, even with this workaround, a double press may still trigger any single press Actions from the same button.\n\nAt least until Unity modifies the Input System to allow one input to override another.\n\nProcessors modify the input value after it is received, for example, to change how sensitive a control is, define its limits, or to completely reverse it.\n\nJust like Interactions, they can be applied to Bindings or to entire Actions.\n\nInvert, as you might expect, flips the value from a control or, technically, multiplies it by -1.\n\nOne example of how you might use the Invert Processor is to invert the direction of vertical or horizontal controls so that they work backwards.\n\nThis is ideal for settings like invert look, or for reversing camera controls.\n\nYou can also apply the Invert Processor to 2D Vectors, which allows you to Invert the X and Y axes independently or together.\n\nClamp limits the value to a fixed range while Scale multiplies the value by a set factor, which can be useful for adjusting the sensitivity of a control.\n\nYou’ll only see some Processors, such as Invert Vector 2, on Action Types and Bindings that support them.\n\nAnd not all Processors will work with all Actions and Bindings, so you’ll more than likely need to experiment a little to get the results you want.\n\nHow to modify Processors at runtime\n\nIf you add an Invert Processor, for example, to reverse look or camera controls in a game, or a Scale Processor to adjust a control’s sensitivity, it’s likely that you will also want to offer the option to change those settings from the game’s menu.\n\nSo, how can you edit a Processor from a script?\n\nRight now, there isn’t a convenient way to modify Processor settings at runtime.\n\nLuckily, however, Unity are already planning a way to dynamically set Processor parameters from scripts so expect this to be added as a feature in the future.\n\nIn the meantime it’s still possible to change a Processor at runtime, you’ll just need to apply an override to do it.\n\nFirst, get a reference to the Action that you want to apply the Processor to,\n\nNote that I’m using an Input Action Reference variable type here to get a reference to an existing Action from the Action Editor, as opposed to an Input Action variable which allows you to embed an Action into the script.\n\nThen I can simply apply an Override to add a new Processor to the Action.\n\nOr to Invert the look of the camera:\n\nSee the Unity documentation for a full list of Processors and the values they can accept.\n\nThe Processor doesn’t need to already exist for this method to work so you won’t need to add a Processor before you use an Override.\n\nAlso, Binding Overrides don’t stack up, which means that to add a new configuration you don’t need to worry about removing the old Override, it’s simply replaced.\n\nWhich is great, as it means you won’t accidentally add multiple Overrides evertime you change the setting.\n\nHowever… this does mean that if you add multiple Processors separately, such as one Processor to invert the look and another to adjust the look sensitivity, only one of them will actually be applied.\n\nMultiple Processors can still be added to the same Action, they just need to be added in one Override via a comma separated list.\n\nThis means that if you apply an override to set additional control options, you’ll need to manually load and apply any Processors you set the next time you start the game.\n\nProcessors are ideal for manipulating input values before they get used in your game.\n\nSo… now that you have Input Actions, that are bound to devices, which are modified by Processors…\n\nHow do you actually use those in your game?\n\nFor example, how do you connect a jump action to the function that actually makes your player jump?\n\nHow do you pass values from the controller to scripts in your Scene?\n\nAnd, in multiplayer, how do you assign different devices to different players?\n\nLet’s do that next.\n\nHow to connect Input Actions to game objects with the Player Input component\n\nOnce you’ve created Input Actions and the controls that will trigger them, it’s time to connect them to the objects in your game.\n\nUnity’s aim for the Input System was to provide flexibility and extensibility, so it won’t surprise you that there are lots of different ways to do this.\n\nHowever, one of the most straightforward options and, in my opinion, the easiest way to get started with the Input System, is by using the Player Input Component.\n\nIt works by attaching the Player Input Component to a player object and setting the Actions field to match your Input Actions asset.\n\nThen, input action functions will be called in scripts on the object and, optionally, its children as well, using one of four different communication methods, including Unity Events, C Sharp Events and, in my opinion, the easiest method, Send Messages.\n\nThe Player Input Component is an easy way to connect all of your Input Actions to all of your player object scripts using a single component.\n\nIt provides a ready-made solution for handling the connection between controls and the game functions they trigger, without needing to write any extra code.\n\nYou simply add it to your player object, configure it to work with your scripts and you’re done.\n\nShould you use Unity’s new Input System?\n\nHopefully, by now, you’ve got a good idea of how Unity’s new Input System works and how to use it.\n\nBut should you?\n\nIt’s definitely an improvement over the old Input Manager but, then again, Unity’s old system has had issues for some time.\n\nThe question is, is the new Input System good enough now, to replace the old Input Manager, or is there an alternative that works better?\n\nSuch as Rewired, an extremely popular input manager asset available on the Unity Asset Store.\n\nAnd, importantly, an asset that has been extensively used, updated and tested since 2014.\n\nSo, while Unity’s new Input System is definitely good, and definitely an improvement, is it the best option? and just how does it compare to the most popular alternative system, Rewired.\n\nRewired is an advanced input management system for Unity that’s built on top of the old Input Manager.\n\nUnlike the Input Manager, however, it’s highly customisable and flexible.\n\nRewired is centred around defined players and, in a similar way to the Input System, uses Actions to connect scripts to inputs from one of its supported devices.\n\nHaving spent some time with the Input Manager and the new Input System, I was surprised at how easy Rewired was to use.\n\nWhile the Editor can be a a little confusing, it didn’t take long to get to grips with and, to me, it felt like a mix between the old Input Manager and the new Input System, which might be exactly what you’re looking for."
    },
    {
        "link": "https://docs.unity3d.com/Manual/class-Transform.html",
        "document": "The Transform stores a GameObjectThe fundamental object in Unity scenes, which can represent characters, props, scenery, cameras, waypoints, and more. A GameObject’s functionality is defined by the Components attached to it. More info\n\nSee in Glossary’s Position, Rotation, Scale and parenting state. A GameObject always has a Transform component attached: you can’t remove a Transform or create a GameObject without a Transform component.\n\nThe Transform component determines the Position, Rotation, and Scale of each GameObject in the sceneA Scene contains the environments and menus of your game. Think of each unique Scene file as a unique level. In each Scene, you place your environments, obstacles, and decorations, essentially designing and building your game in pieces. More info\n\nSee in Glossary. Every GameObject has a Transform.\n\nTip: You can change the colors of the Transform axes (and other UI elements) (Menu: Unity > Preferences and then select the Colors & keys panel).\n\nUnity measures the Position, Rotation and Scale values of a Transform relative to the Transform’s parent. If the Transform has no parent, Unity measures the properties in world space.\n\nIn 2D space, you can manipulate Transforms on the x-axis or the y-axis only. In 3D space, you can manipulate Transforms on the x-axis, y-axis, and z-axis. In Unity, these axes are represented by the colors red, green, and blue respectively.\n\nThere are three primary ways you can edit a Transform’s properties:\n• In the Scene view \n\n An interactive view into the world you are creating. You use the Scene View to select and position scenery, characters, cameras, lights, and all other types of Game Object. More info .\n• In your C# scripts.\n\nIn the Scene view, you can use the Move, Rotate and Scale tools to modify Transforms. These tools are located in the upper left-hand corner of the Unity Editor.\n\nYou can use the Transform tools on any GameObject in a scene. When you select a GameObject, the tool GizmoA graphic overlay associated with a GameObject in a Scene, and displayed in the Scene View. Built-in scene tools such as the move tool are Gizmos, and you can create custom Gizmos using textures or scripting. Some Gizmos are only drawn when the GameObject is selected, while other Gizmos are drawn by the Editor regardless of which GameObjects are selected. More info\n\nSee in Glossary appears within it. The appearance of the Gizmo depends on which tool you select.\n\nWhen you click and drag on one of the three Gizmo axes, the axis’s color changes to yellow. While you drag the mouse, the GameObject moves, rotates, or scales along the selected axis. When you release the mouse button, the axis remains selected\n\nWhile moving the GameObject, you can lock movement to a particular plane (that is, change two of the axes and keep the third unchanged). To activate the lock for each plane, select the three small coloured squares around the center of the Move Gizmo. The colors correspond to the axis that locks when you select the square (for example, select the blue square to lock the z-axis).\n\nIn the InspectorA Unity window that displays information about the currently selected GameObject, asset or project settings, allowing you to inspect and edit the values. More info\n\nSee in Glossary window, you can use the Transform component to edit the Transform properties of a selected GameObject. There are two ways to edit the Transform property values in the component:\n• Enter values into the property value fields manually. This is useful for very specific adjustments.\n• Click a value field and drag up or down to increase or decrease the value. This is useful for less specific adjustments.\n\nUse the API to edit the Transform of a GameObject through script.\n\nIn Unity, you can group GameObjects into parent-child hierarchies:\n• A parent GameObject has other GameObjects connected to it that take on its Transform properties.\n• A child GameObject is connected to another GameObject, and takes on that GameObject’s Transform properties.\n\nIn the Hierarchy window, child GameObjects appear directly underneath parent GameObjects and are indented in the list. You can select the fold-out icon to hide or reveal a parent GameObject’s child GameObjects.\n\nA child GameObject moves, rotates, and scales exactly as its parent does. Child GameObjects can also have child GameObjects of their own. A GameObject can have multiple child GameObjects, but only one parent GameObject.\n\nThese multiple levels of parent-child relationships between GameObjects form a Transform hierarchy. The GameObject at the top of a hierarchy (that is, the only GameObject in the hierarchy that doesn’t have a parent) is known as the root GameObject.\n\nTo create a parent GameObject, drag any GameObject in the Hierarchy window onto another. This creates a parent-child relationship between the two GameObjects.\n\nYou can group GameObjects into parent-child hierarchies.\n\nThe Transform values for any child GameObject are displayed relative to the parent GameObject’s Transform values. These values are called local coordinates. For scene construction, it is usually sufficient to work with local coordinates for child GameObjects. In gameplay, it is often useful to find their global coordinates or their exact position in world space. The scripting API for the Transform component has separate properties for local and global Position, Rotation and Scale, and lets you convert between local and global coordinates.\n\nTip: When you parent Transforms, it is useful to set the parent’s location to <0,0,0> before you add the child Transform. This means that the local coordinates for the child Transform will be the same as the global coordinates, which makes it easier to ensure the child Transform is in the right position.\n\nThe Scale of the Transform determines the difference between the size of a meshThe main graphics primitive of Unity. Meshes make up a large part of your 3D worlds. Unity supports triangulated or Quadrangulated polygon meshes. Nurbs, Nurms, Subdiv surfaces must be converted to polygons. More info\n\nSee in Glossary in your modeling application and the size of that mesh in Unity. The mesh’s size in Unity (and therefore the Transform’s Scale) is important, especially during physics simulation. By default, the physics engineA system that simulates aspects of physical systems so that objects can accelerate correctly and be affected by collisions, gravity and other forces. More info\n\nSee in Glossary assumes that one unit in world space corresponds to one meter. If a GameObject is very large, it can appear to fall in “slow motion”; the simulation is correct because you are watching a very large GameObject fall a great distance.\n\nThree factors affect the Scale of your GameObject:\n• The size of your mesh in your 3D modeling application.\n• The Mesh Scale Factor setting in the GameObject’s Import Settings.\n• The Scale values of your Transform Component.\n\nDon’t adjust the Scale of your GameObject in the Transform component. If you create your models at real-life scale, you won’t have to change your Transform’s Scale. You can also adjust the scale at which your mesh is imported because some optimizations occur based on the import size. Do this in the Import settings for your individual mesh. Instantiating a GameObject that has an adjusted Scale value can decrease performance.\n\nNote: Changing the Scale affects the position of child Transforms. For example, scaling the parent Transform to (0,0,0) positions all child Transforms at (0,0,0) relative to the parent Transform.\n\nNon-uniform scaling is when the Scale in a Transform has different values for x, y, and z; for example (2, 4, 2). In contrast, uniform scaling has the same value for x, y, and z; for example (3, 3, 3). Non-uniform scaling can be useful in a few specific cases but it behaves differently to uniform scaling:\n• Some components don’t fully support non-uniform scaling. For example, some components have a circular or spherical element defined by a Radius property, such as Sphere Collider \n\n An invisible shape that is used to handle physical collisions for an object. A collider doesn’t need to be exactly the same shape as the object’s mesh - a rough approximation is often more efficient and indistinguishable in gameplay. More info , Capsule Collider \n\n A capsule-shaped collider component that handles collisions for GameObjects like barrels and character limbs. More info , Light and Audio Source \n\n A component which plays back an Audio Clip in the scene to an audio listener or through an audio mixer. More info . This means the circular shape remains circular under non-uniform scaling instead of elliptical.\n• If a child GameObject has a non-uniformly scaled parent GameObject and is rotated relative to that parent GameObject, it might appear skewed or “sheared”. There are components that support simple non-uniform scaling but that don’t work correctly when skewed like this. For example, a skewed Box Collider \n\n A cube-shaped collider component that handles collisions for GameObjects like dice and ice cubes. More info does not match the shape of the rendered mesh accurately.\n• A child GameObject of a non-uniformly scaled parent GameObject does not have its scale automatically updated when it rotates. As a result, the child GameObject’s shape might appear to change abruptly when you eventually update the scale, for example, if the child GameObject is detached from the parent GameObject."
    },
    {
        "link": "https://docs.unity3d.com/Manual/PositioningGameObjects.html",
        "document": "To alter the Transform component of the GameObjectThe fundamental object in Unity scenes, which can represent characters, props, scenery, cameras, waypoints, and more. A GameObject’s functionality is defined by the Components attached to it. More info\n\nSee in Glossary, use the mouse to manipulate any GizmoA graphic overlay associated with a GameObject in a Scene, and displayed in the Scene View. Built-in scene tools such as the move tool are Gizmos, and you can create custom Gizmos using textures or scripting. Some Gizmos are only drawn when the GameObject is selected, while other Gizmos are drawn by the Editor regardless of which GameObjects are selected. More info\n\nSee in Glossary axis, or type values directly into the number fields of the Transform component in the InspectorA Unity window that displays information about the currently selected GameObject, asset or project settings, allowing you to inspect and edit the values. More info\n\nSee in Glossary.\n\nAlternatively, you can select each of the five Transform modes from the SceneA Scene contains the environments and menus of your game. Think of each unique Scene file as a unique level. In each Scene, you place your environments, obstacles, and decorations, essentially designing and building your game in pieces. More info\n\nSee in Glossary view’s Tools Overlay or with a hotkey:\n\nAt the center of the Move Gizmo, there are three small squares you can use to drag the GameObject within a single plane (meaning you can move two axes at once while the third keeps still).\n\nIf you hold shift while clicking and dragging in the center of the Move Gizmo, the center of the Gizmo changes to a flat square. The flat square indicates that you can move the GameObject around on a plane relative to the direction the Scene viewAn interactive view into the world you are creating. You use the Scene View to select and position scenery, characters, cameras, lights, and all other types of Game Object. More info\n\nSee in Glossary CameraA component which creates an image of a particular viewpoint in your scene. The output is either drawn to the screen or captured as a texture. More info\n\nSee in Glossary is facing.\n\nWith the Rotate tool selected, change the GameObject’s rotation by clicking and dragging the axes of the wireframe sphere Gizmo that appears around it. As with the Move Gizmo, the last axis you changed will be colored yellow. Think of the red, green and blue circles as performing rotation around the red, green and blue axes that appear in the Move mode (red is the x-axis, green in the y-axis, and blue is the z-axis). Finally, use the outermost circle to rotate the GameObject around the Scene view z-axis. Think of this as rotating in screen space.\n\nThe Scale tool lets you rescale the GameObject evenly on all axes at once by clicking and dragging on the cube at the center of the Gizmo. You can also scale the axes individually, but you should take care if you do this when there are child GameObjects, because the effect can look quite strange.\n\nThe RectTransform is commonly used for positioning 2D elements such as SpritesA 2D graphic objects. If you are used to working in 3D, Sprites are essentially just standard textures but there are special techniques for combining and managing sprite textures for efficiency and convenience during development. More info\n\nSee in Glossary or UI elements, but it can also be useful for manipulating 3D GameObjects. It combines moving, scaling and rotation into a single Gizmo:\n• Click and drag within the rectangular Gizmo to move the GameObject.\n• Click and drag any corner or edge of the rectangular Gizmo to scale the GameObject.\n• Drag an edge to scale the GameObject along one axis.\n• Drag a corner to scale the GameObject on two axes.\n• To rotate the GameObject, position your cursor just beyond a corner of the rectangle. The cursor changes to display a rotation icon. Click and drag from this area to rotate the GameObject.\n\nNote that in 2D mode, you can’t change the z-axis in the Scene using the Gizmos. However, it is useful for certain scripting techniques to use the z-axis for other purposes, so you can still set the z-axis using the Transform component in the Inspector.\n\nFor more information on transforming GameObjects, see documentation on the Transform ComponentA Transform component determines the Position, Rotation, and Scale of each object in the scene. Every GameObject has a Transform. More info\n\nSee in Glossary.\n\nThe Transform tool combines the Move, Rotate and Scale tools. Its Gizmo provides handles for movement and rotation. When the Tool Handle Rotation is set to Local (see below), the Transform tool also provides handles for scaling the selected GameObject.\n\nThe Gizmo handle position toggles found in the Tool Settings Overlay are used to define the location of any Transform tool Gizmo, and the handles used to manipulate the Gizmo itself.\n\nUse the dropdown menu to switch between Pivot and Center.\n• Pivot positions the Gizmo at the actual pivot point of the GameObject, as defined by the Transform component.\n• Center positions the Gizmo at a center position based on the selected GameObjects.\n\nUse the dropdown menu to switch between Local and Global.\n• Local keeps the Gizmo’s rotation relative to the GameObject’s.\n\nUnity provides three types of snapping:\n• World grid snapping: Snap a GameObject to a grid projected along the X, Y, or Z axes or transform a GameObject in increments along the X, Y, or Z axes. This is only available while using the World, or Global, handle orientation.\n• Surface snapping: Snap the GameObject to the intersection of any Collider \n\n An invisible shape that is used to handle physical collisions for an object. A collider doesn’t need to be exactly the same shape as the object’s mesh - a rough approximation is often more efficient and indistinguishable in gameplay. More info .\n• Vertex snapping: Snap any vertex from a given Mesh \n\n The main graphics primitive of Unity. Meshes make up a large part of your 3D worlds. Unity supports triangulated or Quadrangulated polygon meshes. Nurbs, Nurms, Subdiv surfaces must be converted to polygons. More info to the position of another Mesh’s vertex or surface. You can snap vertex to vertex, vertex to surface, and pivot to vertex.\n\nWhen you drag a PrefabAn asset type that allows you to store a GameObject complete with components and properties. The prefab acts as a template from which you can create new object instances in the scene. More info\n\nSee in Glossary into a scene, Unity places them at the cursor position by default. You can preserve any offsets in the Prefab in relation to the cursor position by holding the Alt key while dragging a Prefab.\n\nTo snap a GameObject to the intersection of a Collider, do the following:\n• Make sure the Move tool is active.\n• Drag the GameObject on to another GameObject with a Collider.\n\nUse vertex snapping to quickly assemble your Scenes: take any vertex from a given Mesh and place that vertex in the same position as any vertex from any other Mesh you choose. For example, use vertex snapping to align road sections precisely in a racing game, or to position power-up items at the vertices of a Mesh.\n\nFollow the steps below to use vertex snapping:\n• Select the Mesh you want to manipulate and make sure the Move tool is active.\n• Press and hold the V key to activate the vertex snapping mode.\n• Move your cursor over the vertex on your Mesh that you want to use as the pivot point.\n• Hold down the left mouse button once your cursor is over the vertex you want and drag your Mesh next to any other vertex on another Mesh.\n• To snap a vertex to a surface on another Mesh, add and hold Shift+Ctrl (macOS: Shift+Command) while you move over the surface you want to snap to.\n• To snap the pivot to a vertex on another Mesh, add and hold Ctrl (macOS: Command) while you move the cursor to the vertex you want to snap to.\n• Release the mouse button and the V key when you are happy with the results (Shift+V acts as a toggle of this functionality).\n\nUse look-at rotation to rotate a GameObject towards a point on the surface of a collider. This is useful to orient GameObjects towards a target, such as a camera or player.\n• In the Scene view, select the GameObject you want to rotate.\n• In the Tools overlay, select the Rotate tool or press E.\n• Click the rotate Gizmo handle to select it.\n• Hold Shift+Ctrl (macOS: Shift+Command) and move your mouse over the surface of a collider you want to rotate the selected GameObject towards.\n\nWhile using the Transform tool, hold down the Shift key to enable Screen Space mode. This mode allows you to move, rotate and scale GameObjects as they appear on the screen, rather than in the Scene."
    },
    {
        "link": "https://discussions.unity.com/t/how-does-unity-access-an-object-transform-position-so-that-it-can-correctly-render-it-in-world-space/677167",
        "document": ""
    },
    {
        "link": "https://medium.com/@scottrlooney/unity-tips-the-transform-9f8f96bb319d",
        "document": "Greetings folks and welcome to another Unity tips installment. Today we’re going to look at how to move a player object using its Transform.\n\nThe Transform is one of the most common classes that is available in Unity and it is also a way to determine the unique identifier of a game object. Put simply, the transform is the three-dimensional position, rotation, and scale of that particular game object.\n\nYou can find the Transform component at the top of every inspector on a game object. Just click on the game object and look at the inspector near the top.\n\nEvery Game Object in a Unity scene will have a transform component added to it. So even if you can’t see the game object, it will always have a transform.\n\nClick on the player object or create one, and be sure to send its location at zero. This means the position is set to 0 for all three axes as shown here:\n\nNow, to move the Game object, press the W key or select the Translate option from the toolbar in the upper left — it looks like arrows. In the scene you’ll see the gizmo in the Editor.\n\nNow you can move, or translate, the position of the game object in the X (red), Y(green), or Z(blue) directions by grabbing each of the colored arrows and dragging it with your mouse. Here I’m just going to grab the red arrow representing the X, and drag it.\n\nNotice what what happens when I do this. The X value in the position changes in either a positive or negative direction depending if I’m on the right or left side of the origin point of 0. Look what happens the Game View when I do a similar action:\n\nIn this case the Scene camera is positioned in a diagonal manner so we can see the cube looking 3D. But our Game View shows what’s really happening.\n\nIn Unity the axes of movement are:\n• Y — Up and Down movement\n\nHopefully this helped a bit for you understand the Transform and how to move in the Editor. In the next article we’ll look at how to automatically move or Translate an object to get our character moving under keyboard control. See you then!"
    },
    {
        "link": "https://discussions.unity.com/t/use-of-transform-properties-and-other-components/749255",
        "document": ""
    }
]