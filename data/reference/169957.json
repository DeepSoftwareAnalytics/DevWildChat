[
    {
        "link": "https://github.com/rajeshidumalla/K-Means-PCA-the-Breast-Cancer-Wisconsin-dataset",
        "document": "Clustering is an unsupervised machine learning algorithm and it recognizes patterns without specific labels and clusters the data according to the features. In our case, we will see if a clustering algorithm (k-means) can find a pattern between different images of the apparel in f-MNIST without the labels (y).\n\nA gif illustrating how K-means works. Each red dot is a centroid and each different color represents a different cluster. Every frame is an iteration where the centroid is relocated.\n\nK-means clustering works by assigning a number of centroids based on the number of clusters given. Each data point is assigned to the cluster whose centroid is nearest to it. The algorithm aims to minimize the squared Euclidean distances between the observation and the centroid of cluster to which it belongs.\n\nPrincipal Component Analysis or PCA is a method of reducing the dimensions of the given dataset while still retaining most of its variance. Wikipedia defines it as, “PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.\n\nPCA visualisation. The best PC (black moving line) is when the total length of those red lines are minimum. It will be used instead of the horizontal and vertical components.\n\nBasically PCA reduces the dimensions of the dataset while conserving most of the information. For e.g. if a data-set has 500 features, it gets reduced to 200 features depending on the specified amount of variance retained. Higher the variance retained,more information is conserved, but more the resulting dimensions will be.\n\nLess dimensions means less time to train and test the model. In some cases models which use data-set with PCA perform better than the original dataset. The concept of PCA and the changes it causes on images by changing the retained variance is shown brilliantly here.\n\nNow we import some of the libraries usually needed by our workload.\n\nI can easily check the current version and get the link of the web interface. In the Spark UI, I can monitor the progress of my job and debug the performance bottlenecks.\n\nIn this Notebook, rather than downloading a file from some where, I am using a famous machine learning dataset, the Breast Cancer Wisconsin dataset, using the datasets loader.\n\nFor convenience, given that the dataset is small, I will first construct a Pandas dataframe, tune the schema, and then convert it into a Spark dataframe.\n\nWith the next cell, I am going build the two datastructures that we will be using throughout this Notebook:\n• , a dataframe of Dense vectors, containing all the original features in the dataset;\n• , a series of binary labels indicating if the corresponding set of features belongs to a subject with breast cancer, or not.\n\nNow I am ready to cluster the data with the K-means algorithm included in MLlib (Spark's Machine Learning library). Also, I am setting the parameter to 2, fit the model, and the compute the Silhouette score (i.e., a measure of quality of the obtained clustering).\n\nIMPORTANT: I am using the MLlib implementation of the Silhouette score (via ).\n\nNext, I will take the predictions produced by K-means, and compare them with the variable (i.e., the ground truth from our dataset).\n\nThen, I will compute how many data points in the dataset have been clustered correctly (i.e., positive cases in one cluster, negative cases in the other).\n\nI am using to quickly compute the element-wise comparison of two series.\n\nIMPORTANT: K-means is a clustering algorithm, so it will not output a label for each data point, but just a cluster identifier! As such, label does not necessarily match the cluster identifier .\n\nNow I am performing dimensionality reduction on the using the PCA statistical procedure, available as well in MLlib.\n\nSetting the parameter to 2, effectively reducing the dataset size of a 15X factor.\n\nNow running K-means with the same parameters as above, but on the produced by the PCA reduction that I just executed.\n\nI am also computing the Silhouette score, as well as the number of data points that have been clustered correctly."
    },
    {
        "link": "https://github.com/tarunkolla/K-Means",
        "document": "breast-cancer-wisconsin.data or you can go to find it on GitHub here\n\nThe data set contains 11 columns, separated by comma. The first column is the example id, and has been ignored. The second to tenth columns are the 9 features, based on which K-means algorithm works. The last column is the class label, and has been ignored as well.\n\nK-Means algorithm performs clustering on the above dataset with K = 2, 3, 4, 5, 6, 7, 8. For each K value, the algorithm is first run and then the potential function is computed as follows:\n\nwhere m is the number of examples, xj denotes the feature vector for j th example and µC(j) refers to the centroid of the cluster that xj belongs to.\n\nEmpty clusters in a certain iteration have been droped and randomly the largest cluster is split into two clusters to maintain the total number of clusters at K.\n\nA graph is plot for the values of k and L(K)."
    },
    {
        "link": "https://kaggle.com/code/vishwaparekh/cluster-analysis-of-breast-cancer-dataset",
        "document": ""
    },
    {
        "link": "https://chegg.com/homework-help/questions-and-answers/objective-implement-k-means-algorithm-data-set-using-python-use-k-means-clustering-analyze-q117216458",
        "document": "The objective is to implement k-means algorithm for a data set using Python. Use k-means clustering to analyze cancer data and classify patients into two different groups, one with benign and the other one with malign cells. You will implement k-means clustering program in Python and test it on the Cancer Data.\n\n-Load dataset into Python. Hint: you may use pandas library to load dataset. You may want to use option na_values=”?” to replace missing values entered to column A7 as “?” with NaN.\n\n-Downloaded data is in file “breast-cancer-wisconsin.data” (without the names of the columns). The following statements read the data into data frame df, replace missing values with “?” and insert the names of the columns to the data frame: col = [\"Scn\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\", \"A8\", \"A9\", \"A10\", \"Class\"] df = pd.read_csv('breast-cancer-wisconsin.data', na_values = '?', names = col)\n\n\n\n-Impute missing value to column A7. The missing values will be either “?” or NaN. You may replace missing values using techniques like mean, median, mode imputation or any other methods of your choice. Example: If you want to impute missing values by ‘mean’ imputation method, you first need to compute the mean of column ‘A7’ without considering ‘?’ data. Once you compute the mean, replace ‘?’ value with computed mean value.\n\n\n\n-Find the mean, median, standard deviation and variance of each of the attributes A2 to A10. So you will have total of nine mean, median, variance and standard deviation values. Print all results rounded to 1 decimal place.\n\n\n\n-Plot histograms with 10 bins for attributes A2 to A10 (nine histograms). Hint: A histogram is a kind of bar plot that gives a discretized display of value frequency. The data points are split into discrete, evenly spaced bins, and the number of data points in each bin is plotted. Use Matplotlib and “hist” method on the Series to plot a histogram. For example, if s is a Series, the following statement plots to subplot sp a histogram with 10 blue bins and opacity 0.5: sp.hist(s, bins=10, color = \"blue\", alpha = 0.5)\n\nNote: There is no constraint on how many functions, classes and Python scripts you write to perform this task. However, you should have an executable Python script named ‘main.py’ that itself will be able to run your entire code by directly executing it."
    },
    {
        "link": "https://kaggle.com/code/mennamostafa3/breast-cancer-with-k-means-edition",
        "document": ""
    },
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html",
        "document": "Read more in the User Guide.\n\nThe number of clusters to form as well as the number of centroids to generate. For an example of how to choose an optimal value for refer to Selecting the number of clusters with silhouette analysis on KMeans clustering.\n• None ‘k-means++’ : selects initial cluster centroids using sampling based on an empirical probability distribution of the points’ contribution to the overall inertia. This technique speeds up convergence. The algorithm implemented is “greedy k-means++”. It differs from the vanilla k-means++ by making several trials at each sampling step and choosing the best centroid among them.\n• None ‘random’: choose observations (rows) at random from data for the initial centroids.\n• None If an array is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.\n• None If a callable is passed, it should take arguments X, n_clusters and a random state and return an initialization. For an example of how to use the different strategies, see A demo of K-Means clustering on the handwritten digits data. For an evaluation of the impact of initialization, see the example Empirical evaluation of the impact of k-means initialization. Number of times the k-means algorithm is run with different centroid seeds. The final results is the best output of consecutive runs in terms of inertia. Several runs are recommended for sparse high-dimensional problems (see Clustering sparse data with k-means). When , the number of runs depends on the value of init: 10 if using or is a callable; 1 if using or is an array-like. Changed in version 1.4: Default value for changed to . Maximum number of iterations of the k-means algorithm for a single run. Relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence. Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See Glossary. When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean. Note that if the original data is not C-contiguous, a copy will be made even if copy_x is False. If the original data is sparse, but not in CSR format, a copy will be made even if copy_x is False. K-means algorithm to use. The classical EM-style algorithm is . The variation can be more efficient on some datasets with well-defined clusters, by using the triangle inequality. However it’s more memory intensive due to the allocation of an extra array of shape . Changed in version 1.1: Renamed “full” to “lloyd”, and deprecated “auto” and “full”. Changed “auto” to use “lloyd” instead of “elkan”. Coordinates of cluster centers. If the algorithm stops before fully converging (see and ), these will not be consistent with . Sum of squared distances of samples to their closest cluster center, weighted by the sample weights if provided. Number of features seen during fit. Names of features seen during fit. Defined only when has feature names that are all strings.\n\nThe k-means problem is solved using either Lloyd’s or Elkan’s algorithm.\n\nThe average complexity is given by O(k n T), where n is the number of samples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. Refer to “How slow is the k-means method?” D. Arthur and S. Vassilvitskii - SoCG2006. for more details.\n\nIn practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That’s why it can be useful to restart it several times.\n\nIf the algorithm stops before fully converging (because of or ), and will not be consistent, i.e. the will not be the means of the points in each cluster. Also, the estimator will reassign after the last iteration to make consistent with on the training set.\n\nFor examples of common problems with K-Means and how to address them see Demonstration of k-means assumptions.\n\nFor a demonstration of how K-Means can be used to cluster text documents see Clustering text documents using k-means.\n\nFor a comparison between K-Means and MiniBatchKMeans refer to example Comparison of the K-Means and MiniBatchKMeans clustering algorithms.\n\nFor a comparison between K-Means and BisectingKMeans refer to example Bisecting K-Means and Regular K-Means Performance Comparison.\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect."
    },
    {
        "link": "https://datacamp.com/tutorial/k-means-clustering-python",
        "document": "In this course, you will be introduced to unsupervised learning through techniques such as hierarchical and k-means clustering using the SciPy library."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/clustering.html",
        "document": "Clustering of unlabeled data can be performed with the module .\n\nEach clustering algorithm comes in two variants: a class, that implements the method to learn the clusters on train data, and a function, that, given train data, returns an array of integer labels corresponding to the different clusters. For the class, the labels over the training data can be found in the attribute.\n\ncreates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of exemplars, which are identified as those most representative of other samples. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs. This updating happens iteratively until convergence, at which point the final exemplars are chosen, and hence the final clustering is given. Affinity Propagation can be interesting as it chooses the number of clusters based on the data provided. For this purpose, the two important parameters are the preference, which controls how many exemplars are used, and the damping factor which damps the responsibility and availability messages to avoid numerical oscillations when updating these messages. The main drawback of Affinity Propagation is its complexity. The algorithm has a time complexity of the order \\(O(N^2 T)\\), where \\(N\\) is the number of samples and \\(T\\) is the number of iterations until convergence. Further, the memory complexity is of the order \\(O(N^2)\\) if a dense similarity matrix is used, but reducible if a sparse similarity matrix is used. This makes Affinity Propagation most appropriate for small to medium sized datasets. The messages sent between points belong to one of two categories. The first is the responsibility \\(r(i, k)\\), which is the accumulated evidence that sample \\(k\\) should be the exemplar for sample \\(i\\). The second is the availability \\(a(i, k)\\) which is the accumulated evidence that sample \\(i\\) should choose sample \\(k\\) to be its exemplar, and considers the values for all other samples that \\(k\\) should be an exemplar. In this way, exemplars are chosen by samples if they are (1) similar enough to many samples and (2) chosen by many samples to be representative of themselves. More formally, the responsibility of a sample \\(k\\) to be the exemplar of sample \\(i\\) is given by: Where \\(s(i, k)\\) is the similarity between samples \\(i\\) and \\(k\\). The availability of sample \\(k\\) to be the exemplar of sample \\(i\\) is given by: To begin with, all values for \\(r\\) and \\(a\\) are set to zero, and the calculation of each iterates until convergence. As discussed above, in order to avoid numerical oscillations when updating the messages, the damping factor \\(\\lambda\\) is introduced to iteration process: where \\(t\\) indicates the iteration times.\n• None Demo of affinity propagation clustering algorithm: Affinity Propagation on a synthetic 2D datasets with 3 classes\n• None Visualizing the stock market structure Affinity Propagation on financial time series to find groups of companies\n\nThe algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, and , which define formally what we mean when we say dense. Higher or lower indicate higher density necessary to form a cluster. More formally, we define a core sample as being a sample in the dataset such that there exist other samples within a distance of , which are defined as neighbors of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of their neighbors that are core samples, and so on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster. Any core sample is part of a cluster, by definition. Any sample that is not a core sample, and is at least in distance from any core sample, is considered an outlier by the algorithm. While the parameter primarily controls how tolerant the algorithm is towards noise (on noisy and large data sets it may be desirable to increase this parameter), the parameter is crucial to choose appropriately for the data set and distance function and usually cannot be left at the default value. It controls the local neighborhood of the points. When chosen too small, most data will not be clustered at all (and labeled as for “noise”). When chosen too large, it causes close clusters to be merged into one cluster, and eventually the entire data set to be returned as a single cluster. Some heuristics for choosing this parameter have been discussed in the literature, for example based on a knee in the nearest neighbor distances plot (as discussed in the references below). In the figure below, the color indicates cluster membership, with large circles indicating core samples found by the algorithm. Smaller circles are non-core samples that are still part of a cluster. Moreover, the outliers are indicated by black points below. The DBSCAN algorithm is deterministic, always generating the same clusters when given the same data in the same order. However, the results can differ when data is provided in a different order. First, even though the core samples will always be assigned to the same clusters, the labels of those clusters will depend on the order in which those samples are encountered in the data. Second and more importantly, the clusters to which non-core samples are assigned can differ depending on the data order. This would happen when a non-core sample has a distance lower than to two core samples in different clusters. By the triangular inequality, those two core samples must be more distant than from each other, or they would be in the same cluster. The non-core sample is assigned to whichever cluster is generated first in a pass through the data, and so the results will depend on the data ordering. The current implementation uses ball trees and kd-trees to determine the neighborhood of points, which avoids calculating the full distance matrix (as was done in scikit-learn versions before 0.14). The possibility to use custom metrics is retained; for details, see . This implementation is by default not memory efficient because it constructs a full pairwise similarity matrix in the case where kd-trees or ball-trees cannot be used (e.g., with sparse matrices). This matrix will consume \\(n^2\\) floats. A couple of mechanisms for getting around this are:\n• None Use OPTICS clustering in conjunction with the method. OPTICS clustering also calculates the full pairwise matrix, but only keeps one row in memory at a time (memory complexity n).\n• None A sparse radius neighborhood graph (where missing entries are presumed to be out of eps) can be precomputed in a memory-efficient way and dbscan can be run over this with . See .\n• None The dataset can be compressed, either by removing exact duplicates if these occur in your data, or by using BIRCH. Then you only have a relatively small number of representatives for a large number of points. You can then provide a when fitting DBSCAN.\n• None A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise Ester, M., H. P. Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n• None DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). In ACM Transactions on Database Systems (TODS), 42(3), 19.\n\nThe algorithm shares many similarities with the algorithm, and can be considered a generalization of DBSCAN that relaxes the requirement from a single value to a value range. The key difference between DBSCAN and OPTICS is that the OPTICS algorithm builds a reachability graph, which assigns each sample both a distance, and a spot within the cluster attribute; these two attributes are assigned when the model is fitted, and are used to determine cluster membership. If OPTICS is run with the default value of inf set for , then DBSCAN style cluster extraction can be performed repeatedly in linear time for any given value using the method. Setting to a lower value will result in shorter run times, and can be thought of as the maximum neighborhood radius from each point to find other potential reachable points. The reachability distances generated by OPTICS allow for variable density extraction of clusters within a single data set. As shown in the above plot, combining reachability distances and data set produces a reachability plot, where point density is represented on the Y-axis, and points are ordered such that nearby points are adjacent. ‘Cutting’ the reachability plot at a single value produces DBSCAN like results; all points above the ‘cut’ are classified as noise, and each time that there is a break when reading from left to right signifies a new cluster. The default cluster extraction with OPTICS looks at the steep slopes within the graph to find clusters, and the user can define what counts as a steep slope using the parameter . There are also other possibilities for analysis on the graph itself, such as generating hierarchical representations of the data through reachability-plot dendrograms, and the hierarchy of clusters detected by the algorithm can be accessed through the parameter. The plot above has been color-coded so that cluster colors in planar space match the linear segment clusters of the reachability plot. Note that the blue and red clusters are adjacent in the reachability plot, and can be hierarchically represented as children of a larger parent cluster. The results from OPTICS method and DBSCAN are very similar, but not always identical; specifically, labeling of periphery and noise points. This is in part because the first samples of each dense area processed by OPTICS have a large reachability value while being close to other points in their area, and will thus sometimes be marked as noise rather than periphery. This affects adjacent points when they are considered as candidates for being marked as either periphery or noise. Note that for any single value of , DBSCAN will tend to have a shorter run time than OPTICS; however, for repeated runs at varying values, a single run of OPTICS may require less cumulative runtime than DBSCAN. It is also important to note that OPTICS’ output is close to DBSCAN’s only if and are close. Spatial indexing trees are used to avoid calculating the full distance matrix, and allow for efficient memory usage on large sets of samples. Different distance metrics can be supplied via the keyword. For large datasets, similar (but not identical) results can be obtained via . The HDBSCAN implementation is multithreaded, and has better algorithmic runtime complexity than OPTICS, at the cost of worse memory scaling. For extremely large datasets that exhaust system memory using HDBSCAN, OPTICS will maintain \\(n\\) (as opposed to \\(n^2\\)) memory scaling; however, tuning of the parameter will likely need to be used to give a solution in a reasonable amount of wall time.\n• None “OPTICS: ordering points to identify the clustering structure.” Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander. In ACM Sigmod Record, vol. 28, no. 2, pp. 49-60. ACM, 1999."
    },
    {
        "link": "https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html",
        "document": "Go to the end to download the full example code. or to run this example in your browser via JupyterLite or Binder\n\nA demo of K-Means clustering on the handwritten digits data#\n\nIn this example we compare the various initialization strategies for K-means in terms of runtime and quality of the results.\n\nAs the ground truth is known here, we also apply different cluster quality metrics to judge the goodness of fit of the cluster labels to the ground truth.\n\nCluster quality metrics evaluated (see Clustering performance evaluation for definitions and discussions of the metrics):\n\nWe will start by loading the dataset. This dataset contains handwritten digits from 0 to 9. In the context of clustering, one would like to group images such that the handwritten digits on the image are the same.\n\nWe will first our evaluation benchmark. During this benchmark, we intend to compare different initialization methods for KMeans. Our benchmark will:\n• None create a pipeline which will scale the data using a ;\n• None measure the performance of the clustering obtained via different metrics. Name given to the strategy. It will be used to show the results in a The labels used to compute the clustering metrics which requires some # Define the metrics which require only the true labels and estimator"
    },
    {
        "link": "https://medium.com/@chyun55555/quick-guide-to-k-means-clustering-with-python-example-scikit-learn-6efe8e319893",
        "document": "We will use scikit-learn for performing K-means here. Scikit-learn provides the class for performing K-means clustering in Python, and the details about its parameters can be found here. The default parameters of can be seen as below.\n\nSome of the important parameters to know are as following:\n\nfrom scikit-learn also provides several operations such as and . They return the labels of the centroids and their coordinates, respectively. It will be easier to understand what these do exactly with coding.\n\nLet’s now try K-means in Python using the typical iris dataset.\n\nHere’s the dataset we will be using. The dataset contains four features(sepal length and width, petal length and width) of irises as well as the class each iris belongs to. There are three classes, which are Setosa, Versicolor and Virginica. They are expressed as 0, 1, 2 in this case under the column ‘target’.\n\nLet’s now generate and train our K-means model.\n\nUsing returns to which cluster/centroid each data point belongs.\n\nNow let’s compare the clustering done by our k-means model and the actual classes.\n\nAs can be seen above, all of Target 0 are classified as Cluster 1. We can say the irises that belong to Target 0 are clustered together very well. For Target 1, 48 out of 50 irises are clustered as Cluster 0 while 2 are clustered as Cluster 2. However, for Target 2, 14 are grouped together as Cluster 0 and 36 are grouped togethr as Cluster 2.\n\nFrom the result, we can tell the clustering is not perfect, but it was done properly to some extent. However, there must have been some confusion between Target 1 and Target 2 since they share Cluster 0 and 2.\n\nIn order to gain better insights about the clustering result with K-means, let’s now try to visualize the clustering result. Since we have four features, we will reduce the dimension to 2 so that we can express the result in x-y plane. For dimension reduction, let’s use PCA method. (Details about PCA will not be dealt with in this article)\n\nOur four features (sepal length, sepal width, petal length and petal width) are now reduced to only two features, X and Y. We can now visualize the clusters assigned using our k-means model can be visualized in 2D graph.\n\nAs can be seen, Cluster 0 and Cluster 2 are very close to each other while Cluster 1 is positioned relative far from the other two clusters. This explains why Cluster 0 and Cluster 2 appeared together under Target 1 and Target 2."
    }
]