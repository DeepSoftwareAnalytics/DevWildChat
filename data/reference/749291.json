[
    {
        "link": "https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html",
        "document": "\n• Learn how to setup OpenCV-Python on your computer!\n• Here you will learn how to display and save images and videos, control mouse events and create trackbar.\n• In this section you will learn basic operations on image like pixel editing, geometric transformations, code optimization, some mathematical tools etc.\n• In this section you will learn different image processing functions inside OpenCV.\n• In this section you will learn about feature detectors and descriptors\n• In this section you will learn different techniques to work with videos like object tracking etc.\n• In this section we will learn about camera calibration, stereo imaging etc.\n• In this section you will learn different image processing functions inside OpenCV.\n• In this section you will learn different computational photography techniques like image denoising etc.\n• In this section you will learn object detection techniques like face detection etc.\n• In this section, we will see how OpenCV-Python bindings are generated"
    },
    {
        "link": "https://docs.opencv.org/3.4/da/d60/tutorial_face_main.html",
        "document": "OpenCV (Open Source Computer Vision) is a popular computer vision library started by Intel in 1999. The cross-platform library sets its focus on real-time image processing and includes patent-free implementations of the latest computer vision algorithms. In 2008 Willow Garage took over support and OpenCV 2.3.1 now comes with a programming interface to C, C++, Python and Android. OpenCV is released under a BSD license so it is used in academic projects and commercial products alike.\n\nOpenCV 2.4 now comes with the very new FaceRecognizer class for face recognition, so you can start experimenting with face recognition right away. This document is the guide I've wished for, when I was working myself into face recognition. It shows you how to perform face recognition with FaceRecognizer in OpenCV (with full source code listings) and gives you an introduction into the algorithms behind. I'll also show how to create the visualizations you can find in many publications, because a lot of people asked for.\n\nThe currently available algorithms are:\n\nYou don't need to copy and paste the source code examples from this page, because they are available in the src folder coming with this documentation. If you have built OpenCV with the samples turned on, chances are good you have them compiled already! Although it might be interesting for very advanced users, I've decided to leave the implementation details out as I am afraid they confuse new users.\n\nAll code in this document is released under the BSD license, so feel free to use it for your projects.\n\nFace recognition is an easy task for humans. Experiments in [217] have shown, that even one to three day old babies are able to distinguish between known faces. So how hard could it be for a computer? It turns out we know little about human recognition to date. Are inner features (eyes, nose, mouth) or outer features (head shape, hairline) used for a successful face recognition? How do we analyze an image and how does the brain encode it? It was shown by David Hubel and Torsten Wiesel, that our brain has specialized nerve cells responding to specific local features of a scene, such as lines, edges, angles or movement. Since we don't see the world as scattered pieces, our visual cortex must somehow combine the different sources of information into useful patterns. Automatic face recognition is all about extracting those meaningful features from an image, putting them into a useful representation and performing some kind of classification on them.\n\nFace recognition based on the geometric features of a face is probably the most intuitive approach to face recognition. One of the first automated face recognition systems was described in [115] : marker points (position of eyes, ears, nose, ...) were used to build a feature vector (distance between the points, angle between them, ...). The recognition was performed by calculating the euclidean distance between feature vectors of a probe and reference image. Such a method is robust against changes in illumination by its nature, but has a huge drawback: the accurate registration of the marker points is complicated, even with state of the art algorithms. Some of the latest work on geometric face recognition was carried out in [37] . A 22-dimensional feature vector was used and experiments on large datasets have shown, that geometrical features alone may not carry enough information for face recognition.\n\nThe Eigenfaces method described in [218] took a holistic approach to face recognition: A facial image is a point from a high-dimensional image space and a lower-dimensional representation is found, where classification becomes easy. The lower-dimensional subspace is found with Principal Component Analysis, which identifies the axes with maximum variance. While this kind of transformation is optimal from a reconstruction standpoint, it doesn't take any class labels into account. Imagine a situation where the variance is generated from external sources, let it be light. The axes with maximum variance do not necessarily contain any discriminative information at all, hence a classification becomes impossible. So a class-specific projection with a Linear Discriminant Analysis was applied to face recognition in [17] . The basic idea is to minimize the variance within a class, while maximizing the variance between the classes at the same time.\n\nRecently various methods for a local feature extraction emerged. To avoid the high-dimensionality of the input data only local regions of an image are described, the extracted features are (hopefully) more robust against partial occlusion, illumation and small sample size. Algorithms used for a local feature extraction are Gabor Wavelets ([238]), Discrete Cosinus Transform ([154]) and Local Binary Patterns ([3]). It's still an open research question what's the best way to preserve spatial information when applying a local feature extraction, because spatial information is potentially useful information.\n\nLet's get some data to experiment with first. I don't want to do a toy example here. We are doing face recognition, so you'll need some face images! You can either create your own dataset or start with one of the available face databases, http://face-rec.org/databases/ gives you an up-to-date overview. Three interesting databases are (parts of the description are quoted from http://face-rec.org):\n• AT&T Facedatabase The AT&T Facedatabase, sometimes also referred to as ORL Database of Faces, contains ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).\n• None Yale Facedatabase A, also known as Yalefaces. The AT&T Facedatabase is good for initial tests, but it's a fairly easy database. The Eigenfaces method already has a 97% recognition rate on it, so you won't see any great improvements with other algorithms. The Yale Facedatabase A (also known as Yalefaces) is a more appropriate dataset for initial experiments, because the recognition problem is harder. The database consists of 15 people (14 male, 1 female) each with 11 grayscale images sized \\(320 \\times 243\\) pixel. There are changes in the light conditions (center light, left light, right light), facial expressions (happy, normal, sad, sleepy, surprised, wink) and glasses (glasses, no-glasses). The original images are not cropped and aligned. Please look into the Appendix for a Python script, that does the job for you.\n• Extended Yale Facedatabase B The Extended Yale Facedatabase B contains 2414 images of 38 different people in its cropped version. The focus of this database is set on extracting features that are robust to illumination, the images have almost no variation in emotion/occlusion/... . I personally think, that this dataset is too large for the experiments I perform in this document. You better use the AT&T Facedatabase for intial testing. A first version of the Yale Facedatabase B was used in [17] to see how the Eigenfaces and Fisherfaces method perform under heavy illumination changes. [125] used the same setup to take 16128 images of 28 people. The Extended Yale Facedatabase B is the merge of the two databases, which is now known as Extended Yalefacedatabase B.\n\nOnce we have acquired some data, we'll need to read it in our program. In the demo applications I have decided to read the images from a very simple CSV file. Why? Because it's the simplest platform-independent approach I can think of. However, if you know a simpler solution please ping me about it. Basically all the CSV file needs to contain are lines composed of a filename followed by a ; followed by the label (as integer number), making up a line like this:\n\nLet's dissect the line. /path/to/image.ext is the path to an image, probably something like this if you are in Windows: C:/faces/person0/image0.jpg. Then there is the separator ; and finally we assign the label 0 to the image. Think of the label as the subject (the person) this image belongs to, so same subjects (persons) should have the same label.\n\nDownload the AT&T Facedatabase from AT&T Facedatabase and the corresponding CSV file from at.txt, which looks like this (file is without ... of course):\n\nImagine I have extracted the files to D:/data/at and have downloaded the CSV file to D:/data/at.txt. Then you would simply need to Search & Replace ./ with D:/data/. You can do that in an editor of your choice, every sufficiently advanced editor can do this. Once you have a CSV file with valid filenames and labels, you can run any of the demos by passing the path to the CSV file as parameter:\n\nPlease, see Creating the CSV File for details on creating CSV file.\n\nThe problem with the image representation we are given is its high dimensionality. Two-dimensional \\(p \\times q\\) grayscale images span a \\(m = pq\\)-dimensional vector space, so an image with \\(100 \\times 100\\) pixels lies in a \\(10,000\\)-dimensional image space already. The question is: Are all dimensions equally useful for us? We can only make a decision if there's any variance in data, so what we are looking for are the components that account for most of the information. The Principal Component Analysis (PCA) was independently proposed by Karl Pearson (1901) and Harold Hotelling (1933) to turn a set of possibly correlated variables into a smaller set of uncorrelated variables. The idea is, that a high-dimensional dataset is often described by correlated variables and therefore only a few meaningful dimensions account for most of the information. The PCA method finds the directions with the greatest variance in the data, called principal components.\n\nLet \\(X = \\{ x_{1}, x_{2}, \\ldots, x_{n} \\}\\) be a random vector with observations \\(x_i \\in R^{d}\\).\n• None Compute the eigenvalues \\(\\lambda_{i}\\) and eigenvectors \\(v_{i}\\) of \\(S\\)\n• Order the eigenvectors descending by their eigenvalue. The \\(k\\) principal components are the eigenvectors corresponding to the \\(k\\) largest eigenvalues.\n\nThe \\(k\\) principal components of the observed vector \\(x\\) are then given by:\n\nThe reconstruction from the PCA basis is given by:\n\nThe Eigenfaces method then performs face recognition by:\n• Projecting all training samples into the PCA subspace.\n• Projecting the query image into the PCA subspace.\n• Finding the nearest neighbor between the projected training images and the projected query image.\n\nStill there's one problem left to solve. Imagine we are given \\(400\\) images sized \\(100 \\times 100\\) pixel. The Principal Component Analysis solves the covariance matrix \\(S = X X^{T}\\), where \\({size}(X) = 10000 \\times 400\\) in our example. You would end up with a \\(10000 \\times 10000\\) matrix, roughly \\(0.8 GB\\). Solving this problem isn't feasible, so we'll need to apply a trick. From your linear algebra lessons you know that a \\(M \\times N\\) matrix with \\(M > N\\) can only have \\(N - 1\\) non-zero eigenvalues. So it's possible to take the eigenvalue decomposition \\(S = X^{T} X\\) of size \\(N \\times N\\) instead:\n\nand get the original eigenvectors of \\(S = X X^{T}\\) with a left multiplication of the data matrix:\n\nThe resulting eigenvectors are orthogonal, to get orthonormal eigenvectors they need to be normalized to unit length. I don't want to turn this into a publication, so please look into [60] for the derivation and proof of the equations.\n\nFor the first source code example, I'll go through it with you. I am first giving you the whole source code listing, and after this we'll look at the most important lines in detail. Please note: every source code listing is commented in detail, so you should have no problems following it.\n\nThe source code for this demo application is also available in the src folder coming with this documentation:\n\nI've used the jet colormap, so you can see how the grayscale values are distributed within the specific Eigenfaces. You can see, that the Eigenfaces do not only encode facial features, but also the illumination in the images (see the left light in Eigenface #4, right light in Eigenfaces #5):\n\nWe've already seen, that we can reconstruct a face from its lower dimensional approximation. So let's see how many Eigenfaces are needed for a good reconstruction. I'll do a subplot with \\(10,30,\\ldots,310\\) Eigenfaces:\n\n10 Eigenvectors are obviously not sufficient for a good image reconstruction, 50 Eigenvectors may already be sufficient to encode important facial features. You'll get a good reconstruction with approximately 300 Eigenvectors for the AT&T Facedatabase. There are rule of thumbs how many Eigenfaces you should choose for a successful face recognition, but it heavily depends on the input data. [255] is the perfect point to start researching for this:\n\nThe Principal Component Analysis (PCA), which is the core of the Eigenfaces method, finds a linear combination of features that maximizes the total variance in data. While this is clearly a powerful way to represent data, it doesn't consider any classes and so a lot of discriminative information may be lost when throwing components away. Imagine a situation where the variance in your data is generated by an external source, let it be the light. The components identified by a PCA do not necessarily contain any discriminative information at all, so the projected samples are smeared together and a classification becomes impossible (see http://www.bytefish.de/wiki/pca_lda_with_gnu_octave for an example).\n\nThe Linear Discriminant Analysis performs a class-specific dimensionality reduction and was invented by the great statistician Sir R. A. Fisher. He successfully used it for classifying flowers in his 1936 paper The use of multiple measurements in taxonomic problems [75] . In order to find the combination of features that separates best between classes the Linear Discriminant Analysis maximizes the ratio of between-classes to within-classes scatter, instead of maximizing the overall scatter. The idea is simple: same classes should cluster tightly together, while different classes are as far away as possible from each other in the lower-dimensional representation. This was also recognized by Belhumeur, Hespanha and Kriegman and so they applied a Discriminant Analysis to face recognition in [17] .\n\nLet \\(X\\) be a random vector with samples drawn from \\(c\\) classes:\n\nThe scatter matrices \\(S_{B}\\) and S_{W} are calculated as:\n\n, where \\(\\mu\\) is the total mean:\n\nAnd \\(\\mu_i\\) is the mean of class \\(i \\in \\{1,\\ldots,c\\}\\):\n\nFisher's classic algorithm now looks for a projection \\(W\\), that maximizes the class separability criterion:\n\nFollowing [17], a solution for this optimization problem is given by solving the General Eigenvalue Problem:\n\nThere's one problem left to solve: The rank of \\(S_{W}\\) is at most \\((N-c)\\), with \\(N\\) samples and \\(c\\) classes. In pattern recognition problems the number of samples \\(N\\) is almost always samller than the dimension of the input data (the number of pixels), so the scatter matrix \\(S_{W}\\) becomes singular (see [180]). In [17] this was solved by performing a Principal Component Analysis on the data and projecting the samples into the \\((N-c)\\)-dimensional space. A Linear Discriminant Analysis was then performed on the reduced data, because \\(S_{W}\\) isn't singular anymore.\n\nThe optimization problem can then be rewritten as:\n\nThe transformation matrix \\(W\\), that projects a sample into the \\((c-1)\\)-dimensional space is then given by:\n\nThe source code for this demo application is also available in the src folder coming with this documentation:\n\nFor this example I am going to use the Yale Facedatabase A, just because the plots are nicer. Each Fisherface has the same length as an original image, thus it can be displayed as an image. The demo shows (or saves) the first, at most 16 Fisherfaces:\n\nThe Fisherfaces method learns a class-specific transformation matrix, so the they do not capture illumination as obviously as the Eigenfaces method. The Discriminant Analysis instead finds the facial features to discriminate between the persons. It's important to mention, that the performance of the Fisherfaces heavily depends on the input data as well. Practically said: if you learn the Fisherfaces for well-illuminated pictures only and you try to recognize faces in bad-illuminated scenes, then method is likely to find the wrong components (just because those features may not be predominant on bad illuminated images). This is somewhat logical, since the method had no chance to learn the illumination.\n\nThe Fisherfaces allow a reconstruction of the projected image, just like the Eigenfaces did. But since we only identified the features to distinguish between subjects, you can't expect a nice reconstruction of the original image. For the Fisherfaces method we'll project the sample image onto each of the Fisherfaces instead. So you'll have a nice visualization, which feature each of the Fisherfaces describes:\n\nThe differences may be subtle for the human eyes, but you should be able to see some differences:\n\nEigenfaces and Fisherfaces take a somewhat holistic approach to face recognition. You treat your data as a vector somewhere in a high-dimensional image space. We all know high-dimensionality is bad, so a lower-dimensional subspace is identified, where (probably) useful information is preserved. The Eigenfaces approach maximizes the total scatter, which can lead to problems if the variance is generated by an external source, because components with a maximum variance over all classes aren't necessarily useful for classification (see http://www.bytefish.de/wiki/pca_lda_with_gnu_octave). So to preserve some discriminative information we applied a Linear Discriminant Analysis and optimized as described in the Fisherfaces method. The Fisherfaces method worked great... at least for the constrained scenario we've assumed in our model.\n\nNow real life isn't perfect. You simply can't guarantee perfect light settings in your images or 10 different images of a person. So what if there's only one image for each person? Our covariance estimates for the subspace may be horribly wrong, so will the recognition. Remember the Eigenfaces method had a 96% recognition rate on the AT&T Facedatabase? How many images do we actually need to get such useful estimates? Here are the Rank-1 recognition rates of the Eigenfaces and Fisherfaces method on the AT&T Facedatabase, which is a fairly easy image database:\n\nSo in order to get good recognition rates you'll need at least 8(+-1) images for each person and the Fisherfaces method doesn't really help here. The above experiment is a 10-fold cross validated result carried out with the facerec framework at: https://github.com/bytefish/facerec. This is not a publication, so I won't back these figures with a deep mathematical analysis. Please have a look into [149] for a detailed analysis of both methods, when it comes to small training datasets.\n\nSo some research concentrated on extracting local features from images. The idea is to not look at the whole image as a high-dimensional vector, but describe only local features of an object. The features you extract this way will have a low-dimensionality implicitly. A fine idea! But you'll soon observe the image representation we are given doesn't only suffer from illumination variations. Think of things like scale, translation or rotation in images - your local description has to be at least a bit robust against those things. Just like SIFT, the Local Binary Patterns methodology has its roots in 2D texture analysis. The basic idea of Local Binary Patterns is to summarize the local structure in an image by comparing each pixel with its neighborhood. Take a pixel as center and threshold its neighbors against. If the intensity of the center pixel is greater-equal its neighbor, then denote it with 1 and 0 if not. You'll end up with a binary number for each pixel, just like\n• So with 8 surrounding pixels you'll end up with 2\\^8 possible combinations, called Local Binary Patterns or sometimes referred to as LBP codes. The first LBP operator described in literature actually used a fixed 3 x 3 neighborhood just like this:\n\nA more formal description of the LBP operator can be given as:\n\n, with \\((x_c, y_c)\\) as central pixel with intensity \\(i_c\\); and \\(i_n\\) being the intensity of the the neighbor pixel. \\(s\\) is the sign function defined as:\n\nThis description enables you to capture very fine grained details in images. In fact the authors were able to compete with state of the art results for texture classification. Soon after the operator was published it was noted, that a fixed neighborhood fails to encode details differing in scale. So the operator was extended to use a variable neighborhood in [3] . The idea is to align an abritrary number of neighbors on a circle with a variable radius, which enables to capture the following neighborhoods:\n\nFor a given Point \\((x_c,y_c)\\) the position of the neighbor \\((x_p,y_p), p \\in P\\) can be calculated by:\n\nWhere \\(R\\) is the radius of the circle and \\(P\\) is the number of sample points.\n\nThe operator is an extension to the original LBP codes, so it's sometimes called Extended LBP (also referred to as Circular LBP) . If a points coordinate on the circle doesn't correspond to image coordinates, the point get's interpolated. Computer science has a bunch of clever interpolation schemes, the OpenCV implementation does a bilinear interpolation:\n\nBy definition the LBP operator is robust against monotonic gray scale transformations. We can easily verify this by looking at the LBP image of an artificially modified image (so you see what an LBP image looks like!):\n\nSo what's left to do is how to incorporate the spatial information in the face recognition model. The representation proposed by Ahonen et. al [3] is to divide the LBP image into \\(m\\) local regions and extract a histogram from each. The spatially enhanced feature vector is then obtained by concatenating the local histograms (not merging them). These histograms are called Local Binary Patterns Histograms.\n\nThe source code for this demo application is also available in the src folder coming with this documentation:\n\nYou've learned how to use the new FaceRecognizer in real applications. After reading the document you also know how the algorithms work, so now it's time for you to experiment with the available algorithms. Use them, improve them and let the OpenCV community participate!\n\nThis document wouldn't be possible without the kind permission to use the face images of the AT&T Database of Faces and the Yale Facedatabase A/B.\n\nImportant: when using these images, please give credit to \"AT&T Laboratories, Cambridge.\"\n\nThe Database of Faces, formerly The ORL Database of Faces, contains a set of face images taken between April 1992 and April 1994. The database was used in the context of a face recognition project carried out in collaboration with the Speech, Vision and Robotics Group of the Cambridge University Engineering Department.\n\nThere are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).\n\nThe files are in PGM format. The size of each image is 92x112 pixels, with 256 grey levels per pixel. The images are organised in 40 directories (one for each subject), which have names of the form sX, where X indicates the subject number (between 1 and 40). In each of these directories, there are ten different images of that subject, which have names of the form Y.pgm, where Y is the image number for that subject (between 1 and 10).\n\nA copy of the database can be retrieved from: http://www.cl.cam.ac.uk/research/dtg/attarchive/pub/data/att_faces.zip.\n\nWith the permission of the authors I am allowed to show a small number of images (say subject 1 and all the variations) and all images such as Fisherfaces and Eigenfaces from either Yale Facedatabase A or the Yale Facedatabase B.\n\nThe Yale Face Database A (size 6.4MB) contains 165 grayscale images in GIF format of 15 individuals. There are 11 images per subject, one per different facial expression or configuration: center-light, w/glasses, happy, left-light, w/no glasses, normal, right-light, sad, sleepy, surprised, and wink. (Source: http://cvc.yale.edu/projects/yalefaces/yalefaces.html)\n\nWith the permission of the authors I am allowed to show a small number of images (say subject 1 and all the variations) and all images such as Fisherfaces and Eigenfaces from either Yale Facedatabase A or the Yale Facedatabase B.\n\nThe extended Yale Face Database B contains 16128 images of 28 human subjects under 9 poses and 64 illumination conditions. The data format of this database is the same as the Yale Face Database B. Please refer to the homepage of the Yale Face Database B (or one copy of this page) for more detailed information of the data format.\n\nYou are free to use the extended Yale Face Database B for research purposes. All publications which use this database should acknowledge the use of \"the Exteded Yale Face Database B\" and reference Athinodoros Georghiades, Peter Belhumeur, and David Kriegman's paper, \"From Few to Many: Illumination Cone Models for Face Recognition under Variable Lighting and Pose\", PAMI, 2001, [bibtex].\n\nThe extended database as opposed to the original Yale Face Database B with 10 subjects was first reported by Kuang-Chih Lee, Jeffrey Ho, and David Kriegman in \"Acquiring Linear Subspaces for Face Recognition under Variable Lighting, PAMI, May, 2005 [pdf].\" All test image data used in the experiments are manually aligned, cropped, and then re-sized to 168x192 images. If you publish your experimental results with the cropped images, please reference the PAMI2005 paper as well. (Source: http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html)\n\nYou don't really want to create the CSV file by hand. I have prepared you a little Python script (you find it at coming with this tutorial) that automatically creates you a CSV file. If you have your images in hierarchie like this ( ):\n\nThen simply call , here 'at' being the basepath to the folder, just like this and you could save the output:\n\nHere is the script, if you can't find it:\n\nAn accurate alignment of your image data is especially important in tasks like emotion detection, were you need as much detail as possible. Believe me... You don't want to do this by hand. So I've prepared you a tiny Python script. The code is really easy to use. To scale, rotate and crop the face image you just need to call CropFace(image, eye_left, eye_right, offset_pct, dest_sz), where:\n• eye_left is the position of the left eye\n• eye_right is the position of the right eye\n• offset_pct is the percent of the image you want to keep next to the eyes (horizontal, vertical direction)\n• dest_sz is the size of the output image\n\nIf you are using the same offset_pct and dest_sz for your images, they are all aligned at the eyes.\n\nImagine we are given this photo of Arnold Schwarzenegger, which is under a Public Domain license. The (x,y)-position of the eyes is approximately *(252,364)* for the left and *(420,366)* for the right eye. Now you only need to define the horizontal offset, vertical offset and the size your scaled, rotated & cropped face should have.\n\nHere are some examples:"
    },
    {
        "link": "https://datacamp.com/tutorial/face-detection-python-opencv",
        "document": "Discover how to begin responsibly leveraging generative AI. Learn how generative AI models are developed and how they will impact society moving forward."
    },
    {
        "link": "https://docs.opencv.org/4.x/index.html",
        "document": ""
    },
    {
        "link": "https://geeksforgeeks.org/opencv-python-program-face-detection",
        "document": ""
    },
    {
        "link": "https://pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python",
        "document": "Last week we learned how to install and configure dlib on our system with Python bindings.\n\nToday we are going to use dlib and OpenCV to detect facial landmarks in an image.\n\nFacial landmarks are used to localize and represent salient regions of the face, such as:\n\nFacial landmarks have been successfully applied to face alignment, head pose estimation, face swapping, blink detection and much more.\n\nIn today’s blog post we’ll be focusing on the basics of facial landmarks, including:\n• Exactly what facial landmarks are and how they work.\n• How to detect and extract facial landmarks from an image using dlib, OpenCV, and Python.\n\nIn the next blog post in this series we’ll take a deeper dive into facial landmarks and learn how to extract specific facial regions based on these facial landmarks.\n\nTo learn more about facial landmarks, just keep reading.\n\nThe first part of this blog post will discuss facial landmarks and why they are used in computer vision applications.\n\nFrom there, I’ll demonstrate how to detect and extract facial landmarks using dlib, OpenCV, and Python.\n\nFinally, we’ll look at some results of applying facial landmark detection to images.\n\nDetecting facial landmarks is a subset of the shape prediction problem. Given an input image (and normally an ROI that specifies the object of interest), a shape predictor attempts to localize key points of interest along the shape.\n\nIn the context of facial landmarks, our goal is detect important facial structures on the face using shape prediction methods.\n\nDetecting facial landmarks is therefore a two step process:\n• Step #1: Localize the face in the image.\n• Step #2: Detect the key facial structures on the face ROI.\n\nFace detection (Step #1) can be achieved in a number of ways.\n\nWe could use OpenCV’s built-in Haar cascades.\n\nWe might apply a pre-trained HOG + Linear SVM object detector specifically for the task of face detection.\n\nOr we might even use deep learning-based algorithms for face localization.\n\nIn either case, the actual algorithm used to detect the face in the image doesn’t matter. Instead, what’s important is that through some method we obtain the face bounding box (i.e., the (x, y)-coordinates of the face in the image).\n\nGiven the face region we can then apply Step #2: detecting key facial structures in the face region.\n\nThere are a variety of facial landmark detectors, but all methods essentially try to localize and label the following facial regions:\n\nThe facial landmark detector included in the dlib library is an implementation of the One Millisecond Face Alignment with an Ensemble of Regression Trees paper by Kazemi and Sullivan (2014).\n\nThis method starts by using:\n• A training set of labeled facial landmarks on an image. These images are manually labeled, specifying specific (x, y)-coordinates of regions surrounding each facial structure.\n• Priors, of more specifically, the probability on distance between pairs of input pixels.\n\nGiven this training data, an ensemble of regression trees are trained to estimate the facial landmark positions directly from the pixel intensities themselves (i.e., no “feature extraction” is taking place).\n\nThe end result is a facial landmark detector that can be used to detect facial landmarks in real-time with high quality predictions.\n\nFor more information and details on this specific technique, be sure to read the paper by Kazemi and Sullivan linked to above, along with the official dlib announcement.\n\nThe pre-trained facial landmark detector inside the dlib library is used to estimate the location of 68 (x, y)-coordinates that map to facial structures on the face.\n\nThe indexes of the 68 coordinates can be visualized on the image below:\n\nThese annotations are part of the 68 point iBUG 300-W dataset which the dlib facial landmark predictor was trained on.\n\nIt’s important to note that other flavors of facial landmark detectors exist, including the 194 point model that can be trained on the HELEN dataset.\n\nRegardless of which dataset is used, the same dlib framework can be leveraged to train a shape predictor on the input training data — this is useful if you would like to train facial landmark detectors or custom shape predictors of your own.\n\nIn the remaining of this blog post I’ll demonstrate how to detect these facial landmarks in images.\n\nFuture blog posts in this series will use these facial landmarks to extract specific regions of the face, apply face alignment, and even build a blink detection system.\n\nIn order to prepare for this series of blog posts on facial landmarks, I’ve added a few convenience functions to my imutils library, specifically inside face_utils.py.\n\nWe’ll be reviewing two of these functions inside now and the remaining ones next week.\n\nThe first utility function is , short for “rectangle to bounding box”:\n\nThis function accepts a single argument, , which is assumed to be a bounding box rectangle produced by a dlib detector (i.e., the face detector).\n\nThe object includes the (x, y)-coordinates of the detection.\n\nHowever, in OpenCV, we normally think of a bounding box in terms of “(x, y, width, height)” so as a matter of convenience, the function takes this object and transforms it into a 4-tuple of coordinates.\n\nAgain, this is simply a matter of conveinence and taste.\n\nSecondly, we have the function:\n\nThe dlib face landmark detector will return a object containing the 68 (x, y)-coordinates of the facial landmark regions.\n\nUsing the function, we cam convert this object to a NumPy array, allowing it to “play nicer” with our Python code.\n\nGiven these two helper functions, we are now ready to detect facial landmarks in images.\n\nOpen up a new file, name it , and insert the following code:\n\nWe’ll be using the submodule of to access our helper functions detailed above.\n\nWe’ll then import . If you don’t already have dlib installed on your system, please follow the instructions in my previous blog post to get your system properly configured.\n• : This is the path to dlib’s pre-trained facial landmark detector. You can download the detector model here or you can use the “Downloads” section of this post to grab the code + example images + pre-trained detector as well.\n• : The path to the input image that we want to detect facial landmarks on.\n\nNow that our imports and command line arguments are taken care of, let’s initialize dlib’s face detector and facial landmark predictor:\n\nLine 19 initializes dlib’s pre-trained face detector based on a modification to the standard Histogram of Oriented Gradients + Linear SVM method for object detection.\n\nLine 20 then loads the facial landmark predictor using the path to the supplied .\n\nBut before we can actually detect facial landmarks, we first need to detect the face in our input image:\n\nLine 23 loads our input image from disk via OpenCV, then pre-processes the image by resizing to have a width of 500 pixels and converting it to grayscale (Lines 24 and 25).\n\nLine 28 handles detecting the bounding box of faces in our image.\n\nThe first parameter to the is our grayscale image (although this method can work with color images as well).\n\nThe second parameter is the number of image pyramid layers to apply when upscaling the image prior to applying the detector (this it the equivalent of computing cv2.pyrUp N number of times on the image).\n\nThe benefit of increasing the resolution of the input image prior to face detection is that it may allow us to detect more faces in the image — the downside is that the larger the input image, the more computaitonally expensive the detection process is.\n\nGiven the (x, y)-coordinates of the faces in the image, we can now apply facial landmark detection to each of the face regions:\n\nWe start looping over each of the face detections on Line 31.\n\nFor each of the face detections, we apply facial landmark detection on Line 35, giving us the 68 (x, y)-coordinates that map to the specific facial features in the image.\n\nLine 36 then converts the dlib object to a NumPy array with shape (68, 2).\n\nLines 40 and 41 draw the bounding box surrounding the detected face on the while Lines 44 and 45 draw the index of the face.\n\nFinally, Lines 49 and 50 loop over the detected facial landmarks and draw each of them individually.\n\nLines 53 and 54 simply display the output to our screen.\n\nBefore we test our facial landmark detector, make sure you have upgraded to the latest version of which includes the file:\n\nNote: If you are using Python virtual environments, make sure you upgrade the inside the virtual environment.\n\nFrom there, use the “Downloads” section of this guide to download the source code, example images, and pre-trained dlib facial landmark detector.\n\nOnce you’ve downloaded the .zip archive, unzip it, change directory to , and execute the following command:\n\nNotice how the bounding box of my face is drawn in green while each of the individual facial landmarks are drawn in red.\n\nThe same is true for this second example image:\n\nHere we can clearly see that the red circles map to specific facial features, including my jawline, mouth, nose, eyes, and eyebrows.\n\nLet’s take a look at one final example, this time with multiple people in the image:\n\nFor both people in the image (myself and Trisha, my fiancée), our faces are not only detected but also annotated via facial landmarks as well.\n\nDlib’s 68-point facial landmark detector tends to be the most popular facial landmark detector in the computer vision field due to the speed and reliability of the dlib library.\n\nTo start, dlib provides an alternative 5-point facial landmark detector that is faster than the 68-point variant. This model works great if all you need are the locations of the eyes and the nose.\n\nOne of the most popular new facial landmark detectors comes from the MediaPipe library which is capable of computing a 3D face mesh:\n\nI’ll be doing tutorials on the PyImageSearch blog using MediaPipe and face mesh in the near future.\n\nIf you want to avoid using libraries other than OpenCV entirely (i.e., no dlib, MediaPipe, etc.), then it’s worth noting that OpenCV does support a built-in facial landmark detector; however, I have not used it before and I cannot comment on its accuracy, ease of use, etc.\n\nIn today’s blog post we learned what facial landmarks are and how to detect them using dlib, OpenCV, and Python.\n\nDetecting facial landmarks in an image is a two step process:\n• First we must localize a face(s) in an image. This can be accomplished using a number of different techniques, but normally involve either Haar cascades or HOG + Linear SVM detectors (but any approach that produces a bounding box around the face will suffice).\n• Apply the shape predictor, specifically a facial landmark detector, to obtain the (x, y)-coordinates of the face regions in the face ROI.\n\nGiven these facial landmarks we can apply a number of computer vision techniques, including:\n\nIn next week’s blog post I’ll be demonstrating how to access each of the face parts individually and extract the eyes, eyebrows, nose, mouth, and jawline features simply by using a bit of NumPy array slicing magic.\n\nTo be notified when this next blog post goes live, be sure to enter your email address in the form below!"
    },
    {
        "link": "https://pyimagesearch.com/2021/04/19/face-detection-with-dlib-hog-and-cnn",
        "document": "In this tutorial, you will learn how to perform face detection with the dlib library using both HOG + Linear SVM and CNNs.\n\nThe dlib library is arguably one of the most utilized packages for face recognition. A Python package appropriately named wraps dlib’s face recognition functions into a simple, easy to use API.\n\nThe intricacies of face detection necessitate a wide range of face data. Having access to a diverse, well-curated dataset is invaluable in creating models that can handle variations in pose, expression, and lighting conditions.\n\nRoboflow has free tools for each stage of the computer vision pipeline that will streamline your workflows and supercharge your productivity.\n\nSign up or Log in to your Roboflow account to access state of the art dataset libaries and revolutionize your computer vision pipeline.\n\nYou can start by choosing your own datasets or using our PyimageSearch’s assorted library of useful datasets.\n\nBring data in any of 40+ formats to Roboflow, train using any state-of-the-art model architectures, deploy across multiple platforms (API, NVIDIA, browser, iOS, etc), and connect to applications or 3rd party tools.\n\nWith a few images, you can train a working computer vision model in an afternoon. For example, bring data into Roboflow from anywhere via API, label images with the cloud-hosted image annotation tool, kickoff a hosted model training with one-click, and deploy the model via a hosted API endpoint. This process can be executed in a code-centric way, in the cloud-based UI, or any mix of the two.\n\nOver 250,000 developers and machine learning engineers from companies such as Cardinal Health, Walmart, USG, Rivian, Intel, and Medtronic build computer vision pipelines with Roboflow. Get started today, no credit card required.\n\nNote: If you are interested in using dlib and the libraries for face recognition, refer to this tutorial, where I cover the topic in detail.\n\nHowever, I’m often surprised to hear that readers do not know that dlib includes two face detection methods built into the library:\n• A HOG + Linear SVM face detector that is accurate and computationally efficient.\n• A Max-Margin (MMOD) CNN face detector that is both highly accurate and very robust, capable of detecting faces from varying viewing angles, lighting conditions, and occlusion.\n\nBest of all, the MMOD face detector can run on an NVIDIA GPU, making it super fast!\n\nTo learn how to use dlib’s HOG + Linear SVM and MMOD face detectors, just keep reading.\n\nIn the first part of this tutorial, you’ll discover dlib’s two face detection functions, one for a HOG + Linear SVM face detector and another for the MMOD CNN face detector.\n\nFrom there, we’ll configure our development environment and review our project directory structure.\n\nWe’ll then run these face detectors on a set of images and examine the results, noting when to use each face detector in a given situation.\n\nThe dlib library provides two functions that can be used for face detection:\n\nThe function does not accept any parameters. A call to it returns the pre-trained HOG + Linear SVM face detector included in the dlib library.\n\nDlib’s HOG + Linear SVM face detector is fast and efficient. By nature of how the Histogram of Oriented Gradients (HOG) descriptor works, it is not invariant to changes in rotation and viewing angle.\n\nFor more robust face detection, you can use the MMOD CNN face detector, available via the function. This method accepts a single parameter, , which is the path to the pre-trained file residing on disk.\n\nNote: I’ve included the file in the “Downloads” section of this guide, so you don’t have to go hunting for it.\n\nIn the remainder of this tutorial, you will learn how to use both of these dlib face detection methods.\n\nTo follow this guide, you need to have both the OpenCV library and dlib installed on your system.\n\nLuckily, you can install OpenCV and dlib via pip:\n\nIf you need help configuring your development environment for OpenCV and dlib, I highly recommend that you read the following two tutorials:\n\nAll that said, are you:\n• Wanting to skip the hassle of fighting with the command line, package managers, and virtual environments?\n• Ready to run the code right now on your Windows, macOS, or Linux systems?\n\nGain access to Jupyter Notebooks for this tutorial and other PyImageSearch guides that are pre-configured to run on Google Colab’s ecosystem right in your web browser! No installation required.\n\nAnd best of all, these Jupyter Notebooks will run on Windows, macOS, and Linux!\n\nBefore we can perform face detection with dlib, we first need to review our project directory structure.\n\nStart by accessing the “Downloads” section of this tutorial to retrieve the source code and example images.\n\nFrom there, take a look at the directory structure:\n\nWe start with two Python scripts to review:\n• : Performs deep learning-based face detection using dlib by loading the trained model from disk.\n\nOur file contains a Python function, , which will help us:\n• Trim any bounding box coordinates that fall outside the bounds of the input image\n\nThe directory contains three images that we’ll be applying face detection to with dlib. We can compare the HOG + Linear SVM face detection method with the MMOD CNN face detector.\n• In OpenCV, we think of bounding boxes in terms of a 4-tuple of starting x-coordinate, starting y-coordinate, width, and height\n• Dlib represents bounding boxes via object with left, top, right, and bottom properties\n\nFurthermore, bounding boxes returned by dlib may fall outside the bounds of the input image dimensions (negative values or values outside the width and height of the image).\n\nTo make applying face detection with dlib easier, let’s create a helper function to (1) convert the bounding box coordinates to standard OpenCV ordering and (2) trim any bounding box coordinates that fall outside the image’s range.\n\nOpen the file inside the module, and let’s get to work:\n\nOur function requires two parameters: the input we applied face detection to and the object returned by dlib.\n\nLines 4-7 extract the starting and ending (x, y)-coordinates of the bounding box.\n\nWe then ensure the bounding box coordinates fall within the width and height of the input on Lines 11-14.\n\nThe final step is to compute the width and height of the bounding box (Lines 17 and 18) and then return a 4-tuple of the bounding box coordinates in , , , and order.\n\nWith our helper utility implemented, we can move on to perform HOG + Linear SVM face detection using dlib.\n\nOpen the file in your project directory structure and insert the following code:\n\nLines 2-7 import our required Python packages. Notice that the function we just implemented is imported.\n\nWhile we import for our OpenCV bindings, we also import , so we can access its face detection functionality.\n\nNext is our command line arguments:\n\nWe have two command line arguments to parse:\n• : The path to the input image where we apply HOG + Linear SVM face detection.\n• : Number of times to upsample an image before applying face detection.\n\nTo detect small faces in a large input image, we may wish to increase the resolution of the input image, thereby making the smaller faces appear larger. Doing so allows our sliding window to detect the face.\n\nThe downside to upsampling is that it creates more layers of our image pyramid, making the detection process slower.\n\nFor faster face detection, set the value to , meaning that no upsampling is performed (but you risk missing face detections).\n\nWe then proceed to:\n• Resize the image (the smaller the image is, the faster HOG + Linear SVM will run)\n• Convert the image from BGR to RGB channel ordering (dlib expects RGB images)\n\nFrom there, we apply our HOG + Linear SVM face detector on Line 30, timing how long the face detection process takes.\n\nKeep in mind that the returned list needs some work — we need to parse the dlib objects into a 4-tuple of starting x-coordinate, starting y-coordinate, width, and height — and that’s exactly what Line 37 accomplishes.\n\nFor each , we call our function, ensuring that both (1) all bounding box coordinates fall within the spatial dimensions of the and (2) our returned bounding boxes are in the proper 4-tuple format.\n\nLet’s look at the results of applying our dlib HOG + Linear SVM face detector to a set of images.\n\nBe sure to access the “Downloads” section of this tutorial to retrieve the source code, example images, and pre-trained models.\n\nFrom there, open a terminal window and execute the following command:\n\nFigure 3 displays the results of applying dlib’s HOG + Linear SVM face detector to an input image containing multiple faces.\n\nThe face detection process took 0.1 seconds, implying that we could process 10 frames per second in a video stream scenario.\n\nMost importantly, note that each of the four faces was correctly detected.\n\nA couple of years ago, back when Avengers: Endgame came out, my wife and I decided to dress up as “dead Avengers” from the movie (sorry if you haven’t seen the movie but come on, it’s been two years already!)\n\nNotice that my wife’s face (errr, Black Widow?) was detected, but apparently, dlib’s HOG + Linear SVM face detector doesn’t know what Iron Man looks like.\n\nIn all likelihood, my face wasn’t detected because my head is slightly rotated and is not a “straight-on view” for the camera. Again, the HOG + Linear SVM family of object detectors does not perform well under rotation or viewing angle changes.\n\nLet’s look at one final image, this one more densely packed with faces:\n\nBack before COVID, there were these things called “concerts.” Bands used to get together and play live music for people in exchange for money. Hard to believe, I know.\n\nA bunch of my friends got together for a concert a few years ago. And while there are clearly eight faces in this image, only six of them are detected.\n\nAs we’ll see later in this tutorial, we can use dlib’s MMOD CNN face detector to improve face detection accuracy and detect all the faces in this image.\n\nSo far, we have learned how to perform face detection with dlib’s HOG + Linear SVM model. This method worked well, but there is far more accuracy to be obtained by using dlib’s MMOD CNN face detector.\n\nLet’s learn how to use dlib’s deep learning face detector now:\n\nOur imports here are identical to our previous script on HOG + Linear SVM face detection.\n\nThe command line arguments are similar, but with one addition (the ) argument:\n\nWe have three command line arguments here:\n• : The path to the input image residing on disk.\n• : The number of times to upsample an image before applying face detection.\n\nWith our command line arguments taken care of, we can now load dlib’s deep learning face detector from disk:\n\nLine 22 loads the from disk by calling . Here we pass in , the path to where the trained dlib face detector resides.\n\nFrom there, we preprocess our image (Lines 26-28) and then apply the face detector (Line 33).\n\nJust as we parsed the HOG + Linear SVM results, we need to the same here, but one with one caveat:\n\nDlib’s HOG + Linear SVM detector returns a list of objects; however, the MMOD CNN object detector returns a list of result objects, each with its own rectangle (hence we use in the list comprehension). Otherwise, the implementation is the same.\n\nFinally, we loop over the bounding boxes and draw them on our output .\n\nLet’s see how dlib’s MMOD CNN face detector stacks up to the HOG + Linear SVM face detector.\n\nTo follow along, be sure to access the “Downloads” section of this guide to retrieve the source code, example images, and pre-trained dlib face detector.\n\nFrom there, you can open a terminal and execute the following command:\n\nJust like the HOG + Linear SVM implementation, dlib’s MMOD CNN face detector can correctly detect all four faces in the input image.\n\nPreviously, HOG + Linear SVM failed to detect my face on the left. But by using dlib’s deep learning face detector, we can correctly detect both faces.\n\nLet’s look at one final image:\n\nBefore, using HOG + Linear SVM, we could only detect six of the eight faces in this image. But as our output shows, swapping over to dlib’s deep learning face detector results in all eight faces being detected.\n\nWhich dlib face detector should I use?\n\nIf you are using a CPU and speed is not an issue, use dlib’s MMOD CNN face detector. It’s far more accurate and robust than the HOG + Linear SVM face detector.\n\nAdditionally, if you have access to a GPU, then there’s no doubt that you should be using the MMOD CNN face detector — you’ll enjoy all the benefits of accurate face detection along with the speed of being able to run in real-time.\n\nSuppose you are limited to just a CPU. In that case, speed is a concern, and you’re willing to tolerate a bit less accuracy, then go with HOG + Linear SVM — it’s still an accurate face detector and significantly more accurate than OpenCV’s Haar cascade face detector.\n\nIn this tutorial, you learned how to perform face detection using the dlib library.\n\nDlib provides two methods to perform face detection:\n\nThe HOG + Linear SVM face detector will be faster than the MMOD CNN face detector but will also be less accurate as HOG + Linear SVM does not tolerate changes in the viewing angle rotation.\n\nFor more robust face detection, use dlib’s MMOD CNN face detector. This model requires significantly more computation (and is thus slower) but is much more accurate and robust to changes in face rotation and viewing angle.\n\nFurthermore, if you have access to a GPU, you can run dlib’s MMOD CNN face detector on it, resulting in real-time face detection speed. The MMOD CNN face detector combined with a GPU is a match made in heaven — you get both the accuracy of a deep neural network along with the speed of a less computationally expensive model.\n\nTo download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), simply enter your email address in the form below!"
    },
    {
        "link": "https://zoomout.medium.com/how-to-use-facial-landmarks-obtained-from-dlib-b82129e5b352",
        "document": "Use dlib to get facial landmarks on any image containing a face.\n\nThis article is more about how to use the result from the dlib face detection results, so I won't go deep into the initial face detection part.\n\nHere’s the python code for it:\n\nThis is the result we’d get by running this code:\n\nNow, in terms of code, the below line gives us the rectangle for each face:\n\nWhere rects is a list of rectangles having one element for each face in the image\n\nshape contains the list of coordinated corresponding to each facial part\n\nWe convert it into a NumPy array to access each part:"
    },
    {
        "link": "https://dlib.net/face_recognition.py.html",
        "document": "# The contents of this file are in the public domain. See LICENSE_FOR_EXAMPLE_PROGRAMS.txt # This example shows how to use dlib's face recognition tool. This tool maps # an image of a human face to a 128 dimensional vector space where images of # the same person are near to each other and images from different people are # far apart. Therefore, you can perform face recognition by mapping faces to # the 128D space and then checking if their Euclidean distance is small # When using a distance threshold of 0.6, the dlib model obtains an accuracy # of 99.38% on the standard LFW face recognition benchmark, which is # comparable to other state-of-the-art methods for face recognition as of # February 2017. This accuracy means that, when presented with a pair of face # images, the tool will correctly identify if the pair belongs to the same # person or is from different people 99.38% of the time. # Finally, for an in-depth discussion of how dlib's tool works you should # refer to the C++ example program dnn_face_recognition_ex.cpp and the # You can install dlib using the command: # Alternatively, if you want to compile dlib yourself then go into the dlib # Compiling dlib should work on any operating system so long as you have # CMake installed. On Ubuntu, this can be done easily by running the # Also note that this example requires Numpy which can be installed \"Call this program like this: \"You can download a trained facial shape predictor and recognition model from: # Load all the models we need: a detector to find the faces, a shape predictor # to find face landmarks so we can precisely localize the face, and finally the # Now process all the images # Ask the detector to find the bounding boxes of each face. The 1 in the # second argument indicates that we should upsample the image 1 time. This # will make everything bigger and allow us to detect more faces. # Now process each face we found. # Get the landmarks/parts for the face in box d. # Draw the face landmarks on the screen so we can see what face is currently being processed. # Compute the 128D vector that describes the face in img identified by # shape. In general, if two face descriptor vectors have a Euclidean # distance between them less than 0.6 then they are from the same # person, otherwise they are from different people. Here we just print # the vector to the screen. # It should also be noted that you can also call this function like this: # The version of the call without the 100 gets 99.13% accuracy on LFW # while the version with 100 gets 99.38%. However, the 100 makes the # call 100x slower to execute, so choose whatever version you like. To # explain a little, the 3rd argument tells the code how many times to # jitter/resample the image. When you set it to 100 it executes the # the face and returns the average result. You could also pick a more # middle value, such as 10, which is only 10x slower but still gets an # 4th value (0.25) is padding around the face. If padding == 0 then the chip will # be closely cropped around the face. Setting larger padding values will result a looser cropping. # In particular, a padding of 0.5 would double the width of the cropped area, a value of 1. # would triple it, and so forth. # There is another overload of compute_face_descriptor that can take # as an input an aligned image. # Note that it is important to generate the aligned image as # dlib.get_face_chip would do it i.e. the size must be 150x150, # Here is a sample usage of that # Now we simply pass this chip (aligned image) to the api"
    },
    {
        "link": "https://sefiks.com/2020/11/20/facial-landmarks-for-face-recognition-with-dlib",
        "document": "Dlib can incredibly find 68 different facial landmark points including chin and jaw line, eyebrows, nose, eyes and lips. We can extract exact facial area based on those landmark points beyond rough face detection. This will increase the accuracy of face recognition models dramatically because we will discard any noise in this way.\n\nYou can either continue to read this tutorial or watch the following video to follow Normalization in Face Recognition with Dlib Facial Landmarks. They both cover the same procedures.\n\nWe are going to run facial landmarks detector with Dlib in Python. Landmark points are jaw and chin, eyes and eyebrows, inner and outer area of lips and nose.\n\nWe need to load the landmark detector externally even if dlib was installed. The following code block will guide you where to install the required file and how to load it.\n\nYou can read an image with either dlib or opencv. Here, dlib loads images in RGB format whereas opencv loads BGR. So, if you load it via opencv, then you also need to reconvert it to RGB as shown below.\n\nFinding facial landmarks requires to detect faces first. Dlib has its own out-of-the-box frontal face detector module.\n\nWe’ve already known that Dlib HoG is a robust one among common face detectors.\n\nThen, we will pass the detected faces to landmark detector.\n\nHere, there are 68 landmark points. However, I am interested in only boundaries of facial area. So, I will discard eyes, nose and mouth related points. That’s why, I built a for loop in range of 0 and 27.\n\nHere, you can see the facial landmarks of an image of Angelina Jolie.\n\nNotice that I stored the landmark point coordinates in the landmark tuple variable. My route will start the point 17 and end at point 27. In this way, I can visit all points of the boundaries of facial landmarks respectively.\n\nNow, we can discard the background of an image based on facial landmarks. I will feed just routes because it stores the facial landmarks respectively.\n\nIn this way, we can just focus on the facial area. Black pixels represent 0 in RGB and they will be neutral element in my training stage. Thus, the accuracy of my face recognition model will increase because I ignored any noise in the image.\n\nBTW, face swap studies are mainly based on this approach as well.\n\nWe can draw a sketch with dlib facial landmarks detector as well.\n\nIn one of my previous post, I blurred the background of an image except detected face. However, face detection returns a rectangle area and it won’t be sensitive.\n\nHerein, we can blur the image except the facial landmark boundaries. In this way, we can focus on just facial area and we also have more sensitive results.\n\nI will use the blur_img function mentioned in this post.\n\nNotice that facial area is clear whereas the image itself is blurred. This seems more pretty.\n\nDlib can find 68 facial landmarks in 2D space. On the other hand, MediaPipe can find 468 landmarks in 3D space. Recommend you to read this tutorial: Deep Face Detection with MediaPipe. You can see its real-time implementation below.\n\nInstead of focusing on just the facial area, we can remove the background as well. This is very similar to zoom style virtual background setup. Here, we will assign black or white background. You can see its real-time implementation below.\n\nSo, we have mentioned how to find facial landmarks with Dlib. This will ignore the noises of images and this will increase the accuracy of face recognition pipeline.\n\nI pushed the source code of this study as a notebook to GitHub. You can support this study if you star⭐️ the repo.\n\nSupport this blog if you do like!"
    },
    {
        "link": "https://docs.python.org/3/library/sqlite3.html",
        "document": "SQLite is a C library that provides a lightweight disk-based database that doesn’t require a separate server process and allows accessing the database using a nonstandard variant of the SQL query language. Some applications can use SQLite for internal data storage. It’s also possible to prototype an application using SQLite and then port the code to a larger database such as PostgreSQL or Oracle.\n\nThe module was written by Gerhard Häring. It provides an SQL interface compliant with the DB-API 2.0 specification described by PEP 249, and requires SQLite 3.15.2 or newer.\n• None Tutorial teaches how to use the module.\n• None Reference describes the classes and functions this module defines.\n\nHow to use placeholders to bind values in SQL queries¶ SQL operations usually need to use values from Python variables. However, beware of using Python’s string operations to assemble queries, as they are vulnerable to SQL injection attacks. For example, an attacker can simply close the single quote and inject to select all rows: # Never do this -- insecure! SELECT * FROM stocks WHERE symbol = '' OR TRUE; --' Instead, use the DB-API’s parameter substitution. To insert a variable into a query string, use a placeholder in the string, and substitute the actual values into the query by providing them as a of values to the second argument of the cursor’s method. An SQL statement may use one of two kinds of placeholders: question marks (qmark style) or named placeholders (named style). For the qmark style, parameters must be a sequence whose length must match the number of placeholders, or a is raised. For the named style, parameters must be an instance of a (or a subclass), which must contain keys for all named parameters; any extra items are ignored. Here’s an example of both styles: # This is the named style used with executemany(): # This is the qmark style used in a SELECT query: PEP 249 numeric placeholders are not supported. If used, they will be interpreted as named placeholders. How to adapt custom Python types to SQLite values¶ SQLite supports only a limited set of data types natively. To store custom Python types in SQLite databases, adapt them to one of the Python types SQLite natively understands. There are two ways to adapt Python objects to SQLite types: letting your object adapt itself, or using an adapter callable. The latter will take precedence above the former. For a library that exports a custom type, it may make sense to enable that type to adapt itself. As an application developer, it may make more sense to take direct control by registering custom adapter functions. Suppose we have a class that represents a pair of coordinates, and , in a Cartesian coordinate system. The coordinate pair will be stored as a text string in the database, using a semicolon to separate the coordinates. This can be implemented by adding a method which returns the adapted value. The object passed to protocol will be of type . The other possibility is to create a function that converts the Python object to an SQLite-compatible type. This function can then be registered using . How to convert SQLite values to custom Python types¶ Writing an adapter lets you convert from custom Python types to SQLite values. To be able to convert from SQLite values to custom Python types, we use converters. Let’s go back to the class. We stored the x and y coordinates separated via semicolons as strings in SQLite. First, we’ll define a converter function that accepts the string as a parameter and constructs a object from it. Converter functions are always passed a object, no matter the underlying SQLite data type. We now need to tell when it should convert a given SQLite value. This is done when connecting to a database, using the detect_types parameter of . There are three options:\n• None Both: set detect_types to . Column names take precedence over declared types. The following example illustrates the implicit and explicit approaches: This section shows recipes for common adapters and converters. How to use connection shortcut methods¶ Using the , , and methods of the class, your code can be written more concisely because you don’t have to create the (often superfluous) objects explicitly. Instead, the objects are created implicitly and these shortcut methods return the cursor objects. This way, you can execute a statement and iterate over it directly using only a single call on the object. # close() is not a shortcut method and it's not called automatically; # the connection object should be closed manually How to use the connection context manager¶ A object can be used as a context manager that automatically commits or rolls back open transactions when leaving the body of the context manager. If the body of the statement finishes without exceptions, the transaction is committed. If this commit fails, or if the body of the statement raises an uncaught exception, the transaction is rolled back. If is , a new transaction is implicitly opened after committing or rolling back. If there is no open transaction upon leaving the body of the statement, or if is , the context manager does nothing. The context manager neither implicitly opens a new transaction nor closes the connection. If you need a closing context manager, consider using . # con.rollback() is called after the with block finishes with an exception, # the exception is still raised and must be caught # Connection object used as context manager only commits or rollbacks transactions, # so the connection object should be closed manually How to work with SQLite URIs¶\n• None Do not implicitly create a new database file if it does not already exist; will raise if unable to create a new file: More information about this feature, including a list of parameters, can be found in the SQLite URI documentation. How to create and use row factories¶ By default, represents each row as a . If a does not suit your needs, you can use the class or a custom . While exists as an attribute both on the and the , it is recommended to set , so all cursors created from the connection will use the same row factory. provides indexed and case-insensitive named access to columns, with minimal memory overhead and performance impact over a . To use as a row factory, assign it to the attribute: \"SELECT 'Earth' AS name, 6378 AS radius\" The clause can be omitted in the statement, as in the above example. In such cases, SQLite returns a single row with columns defined by expressions, e.g. literals, with the given aliases . You can create a custom that returns each row as a , with column names mapped to values: Using it, queries now return a instead of a : can be used as follows: With some adjustments, the above recipe can be adapted to use a , or any other custom class, instead of a . By default, uses to adapt SQLite values with the data type. This works well for UTF-8 encoded text, but it might fail for other encodings and invalid UTF-8. You can use a custom to handle such cases. Because of SQLite’s flexible typing, it is not uncommon to encounter table columns with the data type containing non-UTF-8 encodings, or even arbitrary data. To demonstrate, let’s assume we have a database with ISO-8859-2 (Latin-2) encoded text, for example a table of Czech-English dictionary entries. Assuming we now have a instance connected to this database, we can decode the Latin-2 encoded text using this : For invalid UTF-8 or arbitrary data in stored in table columns, you can use the following technique, borrowed from the Unicode HOWTO: The module API does not support strings containing surrogates."
    },
    {
        "link": "https://pynative.com/python-sqlite-blob-insert-and-retrieve-digital-data",
        "document": "In this article, you will learn to insert and retrieve a file stored as a BLOB in the SQLite table using Python’s sqlite3 module.\n• Use SQLite BLOB data type to store any binary data into the SQLite table using Python. Binary can be a file, image, video, or a media\n• Read BLOB data from the SQLite table in Python.\n\nBefore executing the following SQLite BLOB operations, please make sure you know the SQLite table name and in which you want to store BLOB data. To Store BLOB data in the SQLite table, we need to create a table that can hold binary data, or you can also modify any existing table and add one extra column with BLOB as its data type.\n\nPlease refer to creating an SQLite table from Python. For this lesson, I am using the ‘new_employee’ table present in my SQLite database.\n\nYou can use the following query to create a table with a BLOB column.\n\nThe table contains two BLOB columns.\n\nBefore proceeding to the examples first understand what is BLOB.\n\nA BLOB (large binary object) is an SQLite data type that stores large objects, typically large files such as images, music, videos, documents, pdf, etc.\n\nWe need to convert our files and images into binary data (byte array in Python) to store it into SQLite database.\n\nInsert Image and File as a BLOB data into SQLite Table\n\nAs of now, a table is empty. Let’s insert employee’s photos and resume files in it.\n\nTo insert BLOB data into SQLite table from Python, you need to follow the below steps: –\n• First, establish the SQLite connection from Python.\n• Second, create a cursor object using the connection object.\n• Then, define the SQLite INSERT Query. You need to know the table and the column name in which you want to insert data.\n• Next, create a function to convert digital data, i.e., images or any file, to binary data\n• Execute the INSERT query in Python using the .\n• After the successful execution of the SQLite insert operation, commit your changes to the database.\n• Most important, Catch SQL exceptions, if any.\n\nLet’s have a look at the new_developer table after inserting the image and file into it.\n\nWe inserted the employee id, name, photo, and resume file into the table. For the image and resume, we passed the location where it is present so our program can read and convert those files into binary data. As you can see, we converted our image and file into a binary format by reading the image and data in mode before inserting it into a BLOB column. Also, we used a parameterized query to insert dynamic data into an SQLite table.\n\nRetrieve Image and File stored as a BLOB from SQLite Table\n\nAssume you want to read the file or images stored in the SQLite table in BLOB format and write that file back to some location on the disk so you can view and read it in a proper format.\n\nIn this example, we are reading the employee photo and resume file that we stored in the SQLite table stored as a BLOB in the previous example.\n\nTo read BLOB data from SQLite Table using Python, you need to follow the below steps: –\n• Second, Define the SELECT query to fetch BLOB columns from the table.\n• Execute the SELECT query in Python using a\n• Use the to retrieve all the rows from the result set and iterate over it.\n• Create a function to convert BLOB data in proper format and save it in a readable format.\n\nAs you can see images and files are stored on disk after reading BLOB data from SQLite.\n\nNote: To copy binary data on the hard drive, we converted binary data(BLOB) to the proper format and wrote it on Hard Disk. In our example, we converted the photo blob column into and resume blob column data into file.\n\nTo practice what you learned in this article, Please solve a Python Database Exercise project to Practice and master the Python Database operations."
    },
    {
        "link": "https://stackoverflow.com/questions/61773037/how-to-display-a-blob-object-image-from-sqlite3-with-python",
        "document": "I saved an image as a BLOB in a sqlite3 database column profile - I summon the function insertBLOB with relevant info:\n\nI tried to access the image file (so I could display it in a Label) like this - by summoning the function readBlobData:\n\nwhen I summon the function readBlobData I get this Error:\n\nDo you have any idea what seems to be the problem? and how can I fix it? How can I access the BLOB object from the SQLite database and present it???"
    },
    {
        "link": "https://sqlite.org/datatype3.html",
        "document": "Most SQL database engines (every SQL database engine other than SQLite, as far as we know) uses static, rigid typing. With static typing, the datatype of a value is determined by its container - the particular column in which the value is stored.\n\nSQLite uses a more general dynamic type system. In SQLite, the datatype of a value is associated with the value itself, not with its container. The dynamic type system of SQLite is backwards compatible with the more common static type systems of other database engines in the sense that SQL statements that work on statically typed databases work the same way in SQLite. However, the dynamic typing in SQLite allows it to do things which are not possible in traditional rigidly typed databases. Flexible typing is a feature of SQLite, not a bug.\n\nUpdate: As of version 3.37.0 (2021-11-27), SQLite provides STRICT tables that do rigid type enforcement, for developers who prefer that kind of thing.\n\nEach value stored in an SQLite database (or manipulated by the database engine) has one of the following storage classes:\n• None NULL. The value is a NULL value.\n• None INTEGER. The value is a signed integer, stored in 0, 1, 2, 3, 4, 6, or 8 bytes depending on the magnitude of the value.\n• None REAL. The value is a floating point value, stored as an 8-byte IEEE floating point number.\n• None TEXT. The value is a text string, stored using the database encoding (UTF-8, UTF-16BE or UTF-16LE).\n• None BLOB. The value is a blob of data, stored exactly as it was input.\n\nA storage class is more general than a datatype. The INTEGER storage class, for example, includes 7 different integer datatypes of different lengths. This makes a difference on disk. But as soon as INTEGER values are read off of disk and into memory for processing, they are converted to the most general datatype (8-byte signed integer). And so for the most part, \"storage class\" is indistinguishable from \"datatype\" and the two terms can be used interchangeably.\n\nAny column in an SQLite version 3 database, except an INTEGER PRIMARY KEY column, may be used to store a value of any storage class.\n\nAll values in SQL statements, whether they are literals embedded in SQL statement text or parameters bound to precompiled SQL statements have an implicit storage class. Under circumstances described below, the database engine may convert values between numeric storage classes (INTEGER and REAL) and TEXT during query execution.\n\nSQLite does not have a separate Boolean storage class. Instead, Boolean values are stored as integers 0 (false) and 1 (true).\n\nSQLite recognizes the keywords \"TRUE\" and \"FALSE\", as of version 3.23.0 (2018-04-02) but those keywords are really just alternative spellings for the integer literals 1 and 0 respectively.\n\nSQLite does not have a storage class set aside for storing dates and/or times. Instead, the built-in Date And Time Functions of SQLite are capable of storing dates and times as TEXT, REAL, or INTEGER values:\n• REAL as Julian day numbers, the number of days since noon in Greenwich on November 24, 4714 B.C. according to the proleptic Gregorian calendar.\n• INTEGER as Unix Time, the number of seconds since 1970-01-01 00:00:00 UTC.\n\nApplications can choose to store dates and times in any of these formats and freely convert between formats using the built-in date and time functions.\n\nSQL database engines that use rigid typing will usually try to automatically convert values to the appropriate datatype. Consider this:\n\nRigidly-typed database will convert the string '123' into an integer 123 and the integer 456 into a string '456' prior to doing the insert.\n\nIn order to maximize compatibility between SQLite and other database engines, and so that the example above will work on SQLite as it does on other SQL database engines, SQLite supports the concept of \"type affinity\" on columns. The type affinity of a column is the recommended type for data stored in that column. The important idea here is that the type is recommended, not required. Any column can still store any type of data. It is just that some columns, given the choice, will prefer to use one storage class over another. The preferred storage class for a column is called its \"affinity\".\n\nEach column in an SQLite 3 database is assigned one of the following type affinities:\n\nA column with TEXT affinity stores all data using storage classes NULL, TEXT or BLOB. If numerical data is inserted into a column with TEXT affinity it is converted into text form before being stored.\n\nA column with NUMERIC affinity may contain values using all five storage classes. When text data is inserted into a NUMERIC column, the storage class of the text is converted to INTEGER or REAL (in order of preference) if the text is a well-formed integer or real literal, respectively. If the TEXT value is a well-formed integer literal that is too large to fit in a 64-bit signed integer, it is converted to REAL. For conversions between TEXT and REAL storage classes, only the first 15 significant decimal digits of the number are preserved. If the TEXT value is not a well-formed integer or real literal, then the value is stored as TEXT. For the purposes of this paragraph, hexadecimal integer literals are not considered well-formed and are stored as TEXT. (This is done for historical compatibility with versions of SQLite prior to version 3.8.6 2014-08-15 where hexadecimal integer literals were first introduced into SQLite.) If a floating point value that can be represented exactly as an integer is inserted into a column with NUMERIC affinity, the value is converted into an integer. No attempt is made to convert NULL or BLOB values.\n\nA string might look like a floating-point literal with a decimal point and/or exponent notation but as long as the value can be expressed as an integer, the NUMERIC affinity will convert it into an integer. Hence, the string '3.0e+5' is stored in a column with NUMERIC affinity as the integer 300000, not as the floating point value 300000.0.\n\nA column that uses INTEGER affinity behaves the same as a column with NUMERIC affinity. The difference between INTEGER and NUMERIC affinity is only evident in a CAST expression: The expression \"CAST(4.0 AS INT)\" returns an integer 4, whereas \"CAST(4.0 AS NUMERIC)\" leaves the value as a floating-point 4.0.\n\nA column with REAL affinity behaves like a column with NUMERIC affinity except that it forces integer values into floating point representation. (As an internal optimization, small floating point values with no fractional component and stored in columns with REAL affinity are written to disk as integers in order to take up less space and are automatically converted back into floating point as the value is read out. This optimization is completely invisible at the SQL level and can only be detected by examining the raw bits of the database file.)\n\nA column with affinity BLOB does not prefer one storage class over another and no attempt is made to coerce data from one storage class into another.\n\nFor tables not declared as STRICT, the affinity of a column is determined by the declared type of the column, according to the following rules in the order shown:\n• None If the declared type contains the string \"INT\" then it is assigned INTEGER affinity.\n• None If the declared type of the column contains any of the strings \"CHAR\", \"CLOB\", or \"TEXT\" then that column has TEXT affinity. Notice that the type VARCHAR contains the string \"CHAR\" and is thus assigned TEXT affinity.\n• None If the declared type for a column contains the string \"BLOB\" or if no type is specified then the column has affinity BLOB.\n• None If the declared type for a column contains any of the strings \"REAL\", \"FLOA\", or \"DOUB\" then the column has REAL affinity.\n• None Otherwise, the affinity is NUMERIC.\n\nNote that the order of the rules for determining column affinity is important. A column whose declared type is \"CHARINT\" will match both rules 1 and 2 but the first rule takes precedence and so the column affinity will be INTEGER.\n\nThe following table shows how many common datatype names from more traditional SQL implementations are converted into affinities by the five rules of the previous section. This table shows only a small subset of the datatype names that SQLite will accept. Note that numeric arguments in parentheses that following the type name (ex: \"VARCHAR(255)\") are ignored by SQLite - SQLite does not impose any length restrictions (other than the large global SQLITE_MAX_LENGTH limit) on the length of strings, BLOBs or numeric values.\n\nNote that a declared type of \"FLOATING POINT\" would give INTEGER affinity, not REAL affinity, due to the \"INT\" at the end of \"POINT\". And the declared type of \"STRING\" has an affinity of NUMERIC, not TEXT.\n\nEvery table column has a type affinity (one of BLOB, TEXT, INTEGER, REAL, or NUMERIC) but expressions do not necessarily have an affinity.\n\nExpression affinity is determined by the following rules:\n• None The right-hand operand of an IN or NOT IN operator has no affinity if the operand is a list, or has the same affinity as the affinity of the result set expression if the operand is a SELECT.\n• None When an expression is a simple reference to a column of a real table (not a VIEW or subquery) then the expression has the same affinity as the table column.\n• None Parentheses around the column name are ignored. Hence if X and Y.Z are column names, then (X) and (Y.Z) are also considered column names and have the affinity of the corresponding columns.\n• None Any operators applied to column names, including the no-op unary \"+\" operator, convert the column name into an expression which always has no affinity. Hence even if X and Y.Z are column names, the expressions +X and +Y.Z are not column names and have no affinity.\n• None An expression of the form \"CAST(expr AS type)\" has an affinity that is the same as a column with a declared type of \"type\".\n• None A COLLATE operator has the same affinity as its left-hand side operand.\n• None Otherwise, an expression has no affinity.\n\nThe \"columns\" of a VIEW or FROM-clause subquery are really the expressions in the result set of the SELECT statement that implements the VIEW or subquery. Thus, the affinity for columns of a VIEW or subquery are determined by the expression affinity rules above. Consider an example:\n\nThe affinity of the v1.x column will be the same as the affinity of t1.b (TEXT), since v1.x maps directly into t1.b. But columns v1.y and v1.z both have no affinity, since those columns map into expression a+c and 42, and expressions always have no affinity.\n\nWhen the SELECT statement that implements a VIEW or FROM-clause subquery is a compound SELECT then the affinity of each column of the VIEW or subquery will be the affinity of the corresponding result column for one of the individual SELECT statements that make up the compound. However, it is indeterminate which of the SELECT statements will be used to determine affinity. Different constituent SELECT statements might be used to determine affinity at different times during query evaluation. The choice might vary across different versions of SQLite. The choice might change between one query and the next in the same version of SQLite. The choice might be different at different times within the same query. Hence, you can never be sure what affinity will be used for columns of a compound SELECT that have different affinities in the constituent subqueries.\n\nBest practice is to avoid mixing affinities in a compound SELECT if you care about the datatype of the result. Mixing affinities in a compound SELECT can lead to surprising and unintuitive results. See, for example, forum post 02d7be94d7.\n\nThe following SQL demonstrates how SQLite uses column affinity to do type conversions when values are inserted into a table.\n\nSQLite version 3 has the usual set of SQL comparison operators including \"=\", \"==\", \"<\", \"<=\", \">\", \">=\", \"!=\", \"\", \"IN\", \"NOT IN\", \"BETWEEN\", \"IS\", and \"IS NOT\", .\n\nThe results of a comparison depend on the storage classes of the operands, according to the following rules:\n• None A value with storage class NULL is considered less than any other value (including another value with storage class NULL).\n• None An INTEGER or REAL value is less than any TEXT or BLOB value. When an INTEGER or REAL is compared to another INTEGER or REAL, a numerical comparison is performed.\n• None A TEXT value is less than a BLOB value. When two TEXT values are compared an appropriate collating sequence is used to determine the result.\n• None When two BLOB values are compared, the result is determined using memcmp().\n\nSQLite may attempt to convert values between the storage classes INTEGER, REAL, and/or TEXT before performing a comparison. Whether or not any conversions are attempted before the comparison takes place depends on the type affinity of the operands.\n\nTo \"apply affinity\" means to convert an operand to a particular storage class if and only if the conversion does not lose essential information. Numeric values can always be converted into TEXT. TEXT values can be converted into numeric values if the text content is a well-formed integer or real literal, but not a hexadecimal integer literal. BLOB values are converted into TEXT values by simply interpreting the binary BLOB content as a text string in the current database encoding.\n\nAffinity is applied to operands of a comparison operator prior to the comparison according to the following rules in the order shown:\n• None If one operand has INTEGER, REAL or NUMERIC affinity and the other operand has TEXT or BLOB or no affinity then NUMERIC affinity is applied to other operand.\n• None If one operand has TEXT affinity and the other has no affinity, then TEXT affinity is applied to the other operand.\n• None Otherwise, no affinity is applied and both operands are compared as is.\n\nThe expression \"a BETWEEN b AND c\" is treated as two separate binary comparisons \"a >= b AND a <= c\", even if that means different affinities are applied to 'a' in each of the comparisons. Datatype conversions in comparisons of the form \"x IN (SELECT y ...)\" are handled as if the comparison were really \"x=y\". The expression \"a IN (x, y, z, ...)\" is equivalent to \"a = +x OR a = +y OR a = +z OR ...\". In other words, the values to the right of the IN operator (the \"x\", \"y\", and \"z\" values in this example) are considered to have no affinity, even if they happen to be column values or CAST expressions.\n\nAll of the results in the example are the same if the comparisons are commuted - if expressions of the form \"a<40\" are rewritten as \"40>a\".\n\nMathematical operators (+, -, *, /, %, <<, >>, &, and |) interpret both operands as if they were numbers. STRING or BLOB operands automatically convert into REAL or INTEGER values. If the STRING or BLOB looks like a real number (if it has a decimal point or an exponent) or if the value is outside the range that can be represented as a 64-bit signed integer, then it converts to REAL. Otherwise the operand converts to INTEGER. The implied type conversion of mathematical operands is slightly different from CAST to NUMERIC in that string and BLOB values that look like real numbers but have no fractional part are kept as REAL instead of being converted into INTEGER as they would be for CAST to NUMERIC. The conversion from STRING or BLOB into REAL or INTEGER is performed even if it is lossy and irreversible. Some mathematical operators (%, <<, >>, &, and |) expect INTEGER operands. For those operators, REAL operands are converted into INTEGER in the same way as a CAST to INTEGER. The <<, >>, &, and | operators always return an INTEGER (or NULL) result, but the % operator returns either INTEGER or REAL (or NULL) depending on the type of its operands. A NULL operand on a mathematical operator yields a NULL result. An operand on a mathematical operator that does not look in any way numeric and is not NULL is converted to 0 or 0.0. Division by zero gives a result of NULL.\n\nWhen query results are sorted by an ORDER BY clause, values with storage class NULL come first, followed by INTEGER and REAL values interspersed in numeric order, followed by TEXT values in collating sequence order, and finally BLOB values in memcmp() order. No storage class conversions occur before the sort.\n\nWhen grouping values with the GROUP BY clause values with different storage classes are considered distinct, except for INTEGER and REAL values which are considered equal if they are numerically equal. No affinities are applied to any values as the result of a GROUP by clause.\n\nThe compound SELECT operators UNION, INTERSECT and EXCEPT perform implicit comparisons between values. No affinity is applied to comparison operands for the implicit comparisons associated with UNION, INTERSECT, or EXCEPT - the values are compared as is.\n\nWhen SQLite compares two strings, it uses a collating sequence or collating function (two terms for the same thing) to determine which string is greater or if the two strings are equal. SQLite has three built-in collating functions: BINARY, NOCASE, and RTRIM.\n• BINARY - Compares string data using memcmp(), regardless of text encoding.\n• NOCASE - Similar to binary, except that it uses sqlite3_strnicmp() for the comparison. Hence the 26 upper case characters of ASCII are folded to their lower case equivalents before the comparison is performed. Note that only ASCII characters are case folded. SQLite does not attempt to do full UTF case folding due to the size of the tables required. Also note that any U+0000 characters in the string are considered string terminators for comparison purposes.\n• RTRIM - The same as binary, except that trailing space characters are ignored.\n\nAn application can register additional collating functions using the sqlite3_create_collation() interface.\n\nCollating functions only matter when comparing string values. Numeric values are always compared numerically, and BLOBs are always compared byte-by-byte using memcmp().\n\nEvery column of every table has an associated collating function. If no collating function is explicitly defined, then the collating function defaults to BINARY. The COLLATE clause of the column definition is used to define alternative collating functions for a column.\n\nThe rules for determining which collating function to use for a binary comparison operator (=, <, >, <=, >=, !=, IS, and IS NOT) are as follows:\n• None If either operand has an explicit collating function assignment using the postfix COLLATE operator, then the explicit collating function is used for comparison, with precedence to the collating function of the left operand.\n• None If either operand is a column, then the collating function of that column is used with precedence to the left operand. For the purposes of the previous sentence, a column name preceded by one or more unary \"+\" operators and/or CAST operators is still considered a column name.\n• None Otherwise, the BINARY collating function is used for comparison.\n\nAn operand of a comparison is considered to have an explicit collating function assignment (rule 1 above) if any subexpression of the operand uses the postfix COLLATE operator. Thus, if a COLLATE operator is used anywhere in a comparison expression, the collating function defined by that operator is used for string comparison regardless of what table columns might be a part of that expression. If two or more COLLATE operator subexpressions appear anywhere in a comparison, the left most explicit collating function is used regardless of how deeply the COLLATE operators are nested in the expression and regardless of how the expression is parenthesized.\n\nThe expression \"x BETWEEN y and z\" is logically equivalent to two comparisons \"x >= y AND x <= z\" and works with respect to collating functions as if it were two separate comparisons. The expression \"x IN (SELECT y ...)\" is handled in the same way as the expression \"x = y\" for the purposes of determining the collating sequence. The collating sequence used for expressions of the form \"x IN (y, z, ...)\" is the collating sequence of x. If an explicit collating sequence is required on an IN operator it should be applied to the left operand, like this: \"x COLLATE nocase IN (y,z, ...)\".\n\nTerms of the ORDER BY clause that is part of a SELECT statement may be assigned a collating sequence using the COLLATE operator, in which case the specified collating function is used for sorting. Otherwise, if the expression sorted by an ORDER BY clause is a column, then the collating sequence of the column is used to determine sort order. If the expression is not a column and has no COLLATE clause, then the BINARY collating sequence is used.\n\nThe examples below identify the collating sequences that would be used to determine the results of text comparisons that may be performed by various SQL statements. Note that a text comparison may not be required, and no collating sequence used, in the case of numeric, blob or NULL values.\n\nThis page last modified on 2022-04-27 09:17:51 UTC"
    },
    {
        "link": "https://stackoverflow.com/questions/51301395/how-to-store-a-jpg-in-an-sqlite-database-with-python",
        "document": "I've been trying for many days to find a solution to this problem. I need to write a small jpg image for each record in an sqlite database. Finally I managed to insert the file but judging from the size it was written in the database as raw instead of a (compressed) jpg. The code I used is:\n\nIf I try to the file it cannot be inserted in the database so, the following code:\n\ngives the following error:\n\nUnfortunately no previous answer in stackoverflow covers me as I've tried them all. Furthermore all the storing retrieving has to be done via a gtk3 interface which I suspect will mean another (series of) problem(s) i.e. how to set an existing image to get its data from the db response etc. Can anyone help?"
    }
]