[
    {
        "link": "https://allaboutcircuits.com/textbook/digital/chpt-11/finite-state-machines",
        "document": "Up to now, every circuit that was presented was a combinatorial circuit. That means that its output is dependent only by its current inputs. Previous inputs for that type of circuits have no effect on the output.\n\nHowever, there are many applications where there is a need for our circuits to have “memory”; to remember previous inputs and calculate their outputs according to them. A circuit whose output depends not only on the present input but also on the history of the input is called a sequential circuit.\n\nIn this section we will learn how to design and build such sequential circuits. In order to see how this procedure works, we will use an example, on which we will study our topic.\n\nSo let’s suppose we have a digital quiz game that works on a clock and reads an input from a manual button. However, we want the switch to transmit only one HIGH pulse to the circuit. If we hook the button directly on the game circuit it will transmit HIGH for as few clock cycles as our finger can achieve. On a common clock frequency our finger can never be fast enough.\n\nThe design procedure has specific steps that must be followed in order to get the work done:\n\nThe first step of the design procedure is to define with simple but clear words what we want our circuit to do:\n\n“Our mission is to design a secondary circuit that will transmit a HIGH pulse with duration of only one cycle when the manual button is pressed, and won’t transmit another pulse until the button is depressed and pressed again.”\n\nThe next step is to design a State Diagram.\n\nThis is a diagram that is made from circles and arrows and describes visually the operation of our circuit. In mathematic terms, this diagram that describes the operation of our sequential circuit is a Finite State Machine. Make a note that this is a Moore Finite State Machine.\n\nIts output is a function of only its current state, not its input. That is in contrast with the Mealy Finite State Machine, where input affects the output. In this tutorial, only the Moore Finite State Machine will be examined.\n\nThe State Diagram of our circuit is the following: (Figure below)\n\nEvery circle represents a “state”, a well-defined condition that our machine can be found at. In the upper half of the circle we describe that condition. The description helps us remember what our circuit is supposed to do at that condition.\n• The first circle is the “stand-by” condition. This is where our circuit starts from and where it waits for another button press.\n• The second circle is the condition where the button has just been just pressed and our circuit needs to transmit a HIGH pulse.\n• The third circle is the condition where our circuit waits for the button to be released before it returns to the “stand-by” condition.\n\nIn the lower part of the circle is the output of our circuit. If we want our circuit to transmit a HIGH on a specific state, we put a 1 on that state. Otherwise we put a 0.\n\nEvery arrow represents a “transition” from one state to another. A transition happens once every clock cycle. Depending on the current Input, we may go to a different state each time. Notice the number in the middle of every arrow. This is the current Input.\n\nFor example, when we are in the “Initial-Stand by” state and we “read” a 1, the diagram tells us that we have to go to the “Activate Pulse” state. If we read a 0 we must stay on the “Initial-Stand by” state.\n\nSo, what does our “Machine” do exactly? It starts from the “Initial - Stand by” state and waits until a 1 is read at the Input. Then it goes to the “Activate Pulse” state and transmits a HIGH pulse on its output. If the button keeps being pressed, the circuit goes to the third state, the “Wait Loop”.\n\nThere it waits until the button is released (Input goes 0) while transmitting a LOW on the output. Then it’s all over again!\n\nThis is possibly the most difficult part of the design procedure, because it cannot be described by simple steps. It takes exprerience and a bit of sharp thinking in order to set up a State Diagram, but the rest is just a set of predetermined steps.\n\nNext, we replace the words that describe the different states of the diagram with binary numbers. We start the enumeration from 0 which is assigned on the initial state. We then continue the enumeration with any state we like, until all states have their number. The result looks something like this: (Figure below)\n\nAfterwards, we fill the State Table. This table has a very specific form. I will give the table of our example and use it to explain how to fill it in. (Figure below)\n\nThe first columns are as many as the bits of the highest number we assigned the State Diagram. If we had 5 states, we would have used up to the number 100, which means we would use 3 columns. For our example, we used up to the number 10, so only 2 columns will be needed. These columns describe the Current State of our circuit.\n\nTo the right of the Current State columns we write the Input Columns. These will be as many as our Input variables. Our example has only one Input.\n\nNext, we write the Next State Columns. These are as many as the Current State columns.\n\nFinally, we write the Outputs Columns. These are as many as our outputs. Our example has only one output. Since we have built a More Finite State Machine, the output is dependent on only the current input states. This is the reason the outputs column has two 1: to result in an output Boolean function that is independant of input I. Keep on reading for further details. The Current State and Input columns are the Inputs of our table. We fill them in with all the binary numbers from 0 to:\n\nIt is simpler than it sounds fortunately. Usually there will be more rows than the actual States we have created in the State Diagram, but that’s ok.\n\nEach row of the Next State columns is filled as follows: We fill it in with the state that we reach when, in the State Diagram, from the Current State of the same row we follow the Input of the same row. If have to fill in a row whose Current State number doesn’t correspond to any actual State in the State Diagram we fill it with Don’t Care terms (X). After all, we don’t care where we can go from a State that doesn’t exist. We wouldn’t be there in the first place! Again it is simpler than it sounds.\n\nThe outputs column is filled by the output of the corresponding Current State in the State Diagram.\n\nThe State Table is complete! It describes the behaviour of our circuit as fully as the State Diagram does.\n\nThe next step is to take that theoretical “Machine” and implement it in a circuit. Most often than not, this implementation involves Flip Flops. This guide is dedicated to this kind of implementation and will describe the procedure for both D - Flip Flops as well as JK - Flip Flops. T - Flip Flops will not be included as they are too similar to the two previous cases. The selection of the Flip Flop to use is arbitrary and usually is determined by cost factors. The best choice is to perform both analysis and decide which type of Flip Flop results in minimum number of logic gates and lesser cost.\n\nFirst we will examine how we implement our “Machine” with D-Flip Flops.\n\nWe will need as many D - Flip Flops as the State columns, 2 in our example. For every Flip Flop we will add one more column in our State table (Figure below) with the name of the Flip Flop’s input, “D” for this case. The column that corresponds to each Flip Flop describes what input we must give the Flip Flop in order to go from the Current State to the Next State. For the D - Flip Flop this is easy: The necessary input is equal to the Next State. In the rows that contain X’s we fill X’s in this column as well.\n\nWe can do the same steps with JK - Flip Flops. There are some differences however. A JK - Flip Flop has two inputs, therefore we need to add two columns for each Flip Flop. The content of each cell is dictated by the JK’s excitation table:\n\nThis table says that if we want to go from State Q to State Q , we need to use the specific input for each terminal. For example, to go from 0 to 1, we need to feed J with 1 and we don’t care which input we feed to terminal K.\n\nWe are in the final stage of our procedure. What remains, is to determine the Boolean functions that produce the inputs of our Flip Flops and the Output. We will extract one Boolean funtion for each Flip Flop input we have. This can be done with a Karnaugh Map. The input variables of this map are the Current State variables as well as the Inputs.\n\nThat said, the input functions for our D - Flip Flops are the following: (Figure below)\n\nIf we chose to use JK - Flip Flops our functions would be the following: (Figure below)\n\nA Karnaugh Map will be used to determine the function of the Output as well: (Figure below)\n\nWe design our circuit. We place the Flip Flops and use logic gates to form the Boolean functions that we calculated. The gates take input from the output of the Flip Flops and the Input of the circuit. Don’t forget to connect the clock to the Flip Flops!\n\nThis is it! We have successfully designed and constructed a Sequential Circuit. At first it might seem a daunting task, but after practice and repetition the procedure will become trivial. Sequential Circuits can come in handy as control parts of bigger circuits and can perform any sequential logic task that we can think of. The sky is the limit! (or the circuit board, at least)\n• A Sequential Logic function has a “memory” feature and takes into account past inputs in order to decide on the output.\n• The Finite State Machine is an abstract mathematical model of a sequential logic function. It has finite inputs, outputs and number of states.\n• FSMs are implemented in real-life circuits through the use of Flip Flops\n• The implementation procedure needs a specific order of steps (algorithm), in order to be carried out."
    },
    {
        "link": "http://dte.us.es/docencia/master/micr/dapa/temas/tema_04/tema04_reg_fsm.pdf",
        "document": ""
    },
    {
        "link": "https://physics.wisc.edu/courses/home/spring2023/623/lecture_notes/digital/digital-03.html",
        "document": "A useful formalism for designing more complex digital circuits is that of the finite state machine (FSM). Here, the circuit's function is broken down into a collection of states and rules which determine when the system moves from one state to another state. This concept can be committed to paper by drawing what is called a state diagram. The state diagram consists of nodes which represent the states and arrows (sometimes called edges) which give the possible transitions between states. The states usually are named something which indicates the function of that state. It will be seen that the state is held in flip flops, therefore there must be some mapping made between the states and their representation in the FFs. The arrows should be labeled with some condition which must be satisfied in order for that state transition to take place. Typically the transitions are taken in response to external stimuli. Finally the state machine must produce some desired configuration of outputs. State transitions and the output configuration are mediated by combinational logic acting on the inputs to the circuit and the circuit's internal state. The basic design of the FSM is shown in the figure at right.\n\nHere is the pattern for designing with the FSM formalism:\n• Determine what states / transitions are needed in order to solve the problem.\n• Draw the state diagram, labelling the states and the edges.\n• Develop a mapping between state and representation in FFs.\n• Write out the state transition table (see below).\n• Find the combinational logic which implements this state.\n• Binary-coded states: in this case, the states are simply numbered 1 through N and the flip-flops represent the number in binary, thus, you need ceil(log N) FFs to hold the state. The order is still a free parameter here and very often, well-chosen ordering can produce a simpler circuit,\n• One-hot states: for the one-hot case, the designer allocates 1 FF per state and when that state is active the corresponding FF is logic HIGH and the rest are logic LOW. This configuration is chosen as it usually produces simpler combinational logic at the expense of exponentially more flip-flops. One-hot mappings are often used in FPGAs where the number of flip flops is very large.\n\nThe logic expressions which implement this table are:\n\nAnd ... realized in discrete gates it looks like this:\n\nThe state transition table is given below:"
    },
    {
        "link": "https://physics.wisc.edu/courses/home/spring2025/623/lecture_notes/digital/digital-03.html",
        "document": "A useful formalism for designing more complex digital circuits is that of the finite state machine (FSM). Here, the circuit's function is broken down into a collection of states and rules which determine when the system moves from one state to another state. This concept can be committed to paper by drawing what is called a state diagram. The state diagram consists of nodes which represent the states and arrows (sometimes called edges) which give the possible transitions between states. The states usually are named something which indicates the function of that state. It will be seen that the state is held in flip flops, therefore there must be some mapping made between the states and their representation in the FFs. The arrows should be labeled with some condition which must be satisfied in order for that state transition to take place. Typically the transitions are taken in response to external stimuli. Finally the state machine must produce some desired configuration of outputs. State transitions and the output configuration are mediated by combinational logic acting on the inputs to the circuit and the circuit's internal state. The basic design of the FSM is shown in the figure at right.\n\nHere is the pattern for designing with the FSM formalism:\n• Determine what states / transitions are needed in order to solve the problem.\n• Draw the state diagram, labelling the states and the edges.\n• Develop a mapping between state and representation in FFs.\n• Write out the state transition table (see below).\n• Find the combinational logic which implements this state.\n• Binary-coded states: in this case, the states are simply numbered 1 through N and the flip-flops represent the number in binary, thus, you need ceil(log N) FFs to hold the state. The order is still a free parameter here and very often, well-chosen ordering can produce a simpler circuit,\n• One-hot states: for the one-hot case, the designer allocates 1 FF per state and when that state is active the corresponding FF is logic HIGH and the rest are logic LOW. This configuration is chosen as it usually produces simpler combinational logic at the expense of exponentially more flip-flops. One-hot mappings are often used in FPGAs where the number of flip flops is very large.\n\nThe logic expressions which implement this table are:\n\nAnd ... realized in discrete gates it looks like this:\n\nThe state transition table is given below:"
    },
    {
        "link": "https://runtimerec.com/crafting-hardware-state-machines-guide",
        "document": "Field Programmable Gate Arrays (FPGAs) offer unparalleled flexibility in implementing digital logic. Their inherent configurability makes them ideal for building complex systems, particularly those requiring real-time processing and hardware customization. A fundamental building block for these systems is the Finite State Machine (FSM).\n\nAn FSM is a mathematical model that describes a system’s behavior through a finite number of states and transitions between them. Transitions are triggered by specific input conditions, and the FSM outputs can depend on either the current state itself (Moore machine) or a combination of state and inputs (Mealy machine). FSMs find application in various digital systems, from traffic light controllers to communication protocols, due to their ability to model sequential logic efficiently.\n\nThe initial step involves meticulously defining the system’s behavior. This translates to identifying the finite set of states the FSM will occupy and the relevant input signals that trigger transitions. The level of granularity for state definition depends on the application’s complexity. For instance, a simple traffic light system might have three states (red, yellow, green), while a communication protocol FSM could involve numerous states representing different message exchange stages. Similarly, input signals could be simple control signals (like a button press) or complex data streams.\n\nUnderstanding the interaction between states and inputs is crucial. This step lays the foundation for accurate state transitions and output behavior. Each state must be clearly defined, considering all possible conditions that could trigger a change. For example, in a traffic light controller FSM, inputs might include a pedestrian button press or a timer expiration. The outputs, in this case, would be the light signals (red, yellow, green).\n\nA state diagram serves as a visual representation of the FSM’s behavior. It depicts states as circles or nodes, and transitions between states are shown as directed arrows. Labels on the arrows indicate the input conditions that trigger the transition. The state diagram also clarifies the output generated upon entering a particular state (Moore machine) or based on the combination of state and input (Mealy machine).\n\nThe state diagram is an essential tool for visualizing and validating the FSM’s logic before coding. It helps identify any missing transitions or states, ensuring completeness. For example, a traffic light controller’s state diagram would show transitions from red to green based on a timer, and from green to yellow, then yellow to red, ensuring all possible sequences are accounted for.\n\nHere, we translate the FSM’s logic into a Hardware Description Language (HDL) like Verilog or VHDL. The code consists of three main components:\n• State Register: This register, typically implemented using flip-flops, stores the current state of the FSM. The state register ensures that the FSM retains its state across clock cycles.\n• Next-State Logic: This combinational logic block determines the next state based on the current state and the incoming inputs. Different state encoding schemes can be employed here, with trade-offs between resource utilization and logic complexity. The next-state logic is crucial for defining how the FSM transitions from one state to another, ensuring correct behavior under all input conditions.\n• Output Logic: This block generates the FSM’s outputs based on the current state or a combination of state and inputs. The output logic is responsible for driving the FSM’s outputs, ensuring they reflect the correct state or state/input combination. Verilog/VHDL code examples will be provided to illustrate these concepts in detail.\n\nBefore deploying the FSM on the FPGA, thorough simulation is crucial. Simulation tools allow us to apply various input scenarios and verify the FSM’s functionality against the defined state diagram. This step helps identify and rectify any errors in the FSM logic before physical implementation.\n\nSimulation involves creating testbenches that mimic real-world scenarios. These testbenches apply inputs to the FSM and observe the outputs, comparing them to expected results. For example, simulating a traffic light controller FSM would involve testing various input sequences, like pedestrian button presses and timer expirations, to ensure the light transitions occur as intended.\n\nSynthesis translates the Verilog/VHDL code into a hardware netlist suitable for the target FPGA architecture. Here, resource utilization (flip-flops, logic gates) is a critical factor. Optimization techniques can be employed to minimize resource usage and improve performance. Timing constraints, which define the allowable delays between signals, also play a role in FSM design and need to be considered during this stage.\n\nThe synthesis process involves converting the high-level HDL description into a low-level hardware representation. This step is crucial for ensuring the FSM can be implemented efficiently on the FPGA. Post-synthesis, implementation tools map the FSM onto the FPGA’s resources, generating a bitstream that configures the FPGA to perform the desired functions.\n\nDesigning a basic FSM on an FPGA involves a well-defined process. This article has provided a detailed breakdown of each step, from initial state definition to FPGA implementation. By leveraging the inherent flexibility of FPGAs, engineers can create efficient hardware solutions for various real-world applications requiring sequential logic. This paves the way for further exploration of advanced FSM techniques and optimizations to create even more sophisticated digital systems.\n\nThe journey from defining states and inputs to implementing and verifying the FSM on an FPGA is both challenging and rewarding. Each step builds upon the previous one, ensuring a robust and reliable design. As digital systems become more complex, mastering FSM design on FPGAs will remain a valuable skill for engineers. By following this guide, you can confidently design and implement FSMs that meet the demands of modern digital applications, unlocking the full potential of FPGA technology.\n\nHire the Best Engineers with RunTime\n\nAt RunTime, we are dedicated to helping you find the best Engineering talent for your recruitment needs. Our team consists of engineers-turned-recruiters with an extensive network and a focus on quality. By partnering with us, you will have access to great engineering talent that drives innovation and excellence in your projects.\n\nDiscover how RunTime has helped 423+ tech companies find highly qualified and talented engineers to enhance their team’s capabilities and achieve strategic goals.\n\nOn the other hand, if you’re a control systems engineer looking for new opportunities, RunTime Recruitment’s job site is the perfect place to find job vacancies."
    },
    {
        "link": "https://sciencedirect.com/science/article/pii/B9780128007303000046",
        "document": ""
    },
    {
        "link": "https://researchgate.net/publication/2428724_High-Performance_Hardware_Description_Language_Specification_Modeling_Issues_and_Recommended_Practices",
        "document": "have emerged as a powerful and important tool for developing and maintaining digital systems. Given their flexibility , HDLs are useful in describing the function of designs ranging from tems. One unfortunate fact of life in electronic design automa- tion is that the need for simulation speed and capacity almost always exceeds what is available. In order to cost-effectively boost the performance of HDL merous groups have been or are developing parallel HDL simu- performance is particularly promising because VHDL and VHDL-AMS explicitly and consistently support the notion of T o help understand the challenges and opportunities in par- sim@vhdl.or g . For further information, readers should send elec- tion language modeling practices to help modelers and users which provides a structure for the recommended practices. Re- search efforts and commercial products fit in this context to better understand them and the appropriate methods for ef fec- How effective is parallel HDL simulation? Depending on whom and when you ask, you are likely to get an amazingly wide range of answers [1]. Answers will range from something like, “Using N processors, my simulation slowed by a factor of two,” through, “W onderful, N processors speed up the simula- tion by M times.” Why such a wide variety of answers? TRANSACTIONS of The Society for Computer Simulation International Parallel simulation of har dwar e description languages (HDL) is no longer solely a subject of largely academic inter est; it is now a significant opportunity for mainstr eam hardwar e and system designers. accessible. However , recognition and practice of suitable HDL modeling practices ar e critical to effi- ciently satisfying performance r equir ements using parallel tool capabilities. This paper develops a set of ing practices ar e part of an effort by the IEEE Design Automation Standar ds Committee (DASC) group on High Performance Modeling for Simulation (HPMSIM) to develop a set of r ecommended practices for discr ete-event and continuous-domain modeling in VHDL, VHDL-AMS and V erilog. Since the optimal modeling style is tightly r elated to the parallel machine ar chitectur e employed and the simulation algo- rithms used, both this paper and the DASC effort key specific modeling r ecommendations to pr ocessor ar chitectur e and simulation algorithm classes. Parallel simulators are a technical and commer cial real- ity , thanks in part to the intrinsically parallel nature of HDLs, especially VHDL and VHDL-AMS. Simu- lation efficiency achieved when using such simulators is heavily dependent on the modeling style used to write the HDL model sour ce. From this paper , r eaders will gain an understanding of the style designers can use to effectively exploit a range of parallel simulators. Our primary focus is on modeling guidelines boosting performance. The utilization efficiency of pr ocessors, memory and network r esour ces is a criti- cal but somewhat secondary concern motivating these guidelines (in the absence of performance gains,\n\nThe somewhat puzzling explanation stems from the wide for parallel simulation. This paper focuses on the performance fectively beyond the model author ’ s control. Issues such as (for example, four test inputs versus booting the operating sys- tem). All have tremendous impact on parallel (as well as se- quential) simulation performance. In such cases, this paper should help model authors to evaluate the potential for improv- Aspects of modeling which are under the model author ’ s control are the most interesting and thus represent the focus of this paper . The aspects we will deal with in this paper include such issues as simulation time resolution, number of processes, data types, and shared variables. The intent of this paper is to proving the performance of models using parallel simulation. ally any simulator with more than one concurrently executing data stream and the ability for communications between the data streams. This paper gives an overview of previous work and the algorithms and architectures typically used for parallel followed by a set of recommended practices for high-perfor- HDL simulation in this paper rather than on the broader field of practices in this paper are not intended to only address a par- The purpose of this paper is to develop recommended practices mended modeling practices already exist for HDL, but do not scribing model requirements with respect to documentation and oped the recommended practices document which provides guidance on how to develop VHDL models for delivery to the and best practices also exist [4, 5, 6]. Generally speaking, these guidelines focus more on the best modeling practices for de- sign productivity and maintenance, not on simulation speed. When speed is discussed, their HDL modeling performance guidelines address serial simulators. Nevertheless, they are good starting points for model developers and are highly recom- to the importance of this problem area, significant interest has focused on this area, with numerous algorithms and implemen- sign automation represents one of the areas of primary research focus due to the vast demand for simulation cycles to ensure Because parallel simulation continues to be such an active area for research, there is a wealth of related material available. For a general overview of parallel logic simulation work, an excellent review is available from Bailey et al. [9]. For a more give good overviews of the issues and techniques [10, 1 1]. Gen- eral parallel simulation technology advances since then have been primarily refinements of the techniques in this paper , and readers should see the proceedings from P ADS for more recent was the subject of an insightful debate among several noted researchers on the issues and future of the field, with the con- clusion that a practical focus is necessary to move the field for - ward, particularly with regard to developing efficient models for simulation [13]. This paper addresses that challenge to make to activity which must occur in the actual system being simu- Each process evaluation and signal propagation contributes to the simulation. Conceptually , conservative PDES is simple to Exploitable parallelism is the greatest challenge to the use able parallelism can be quantitatively expressed as the time-aver - aged duty cycle of all processors assigned to accomplish a paral- fixed number of processors, both the absolute performance and cost-efficiency are proportional to the exploitable parallelism. all processors complete execution of computations at a given simulation time before any may proceed on to executions at the next time step (in practice, pipelining across this time barrier is feasible). In effect, for synchronous, conservative PDES, all pro- cessors synchronize at a barrier before advancing to the next PDES implementations is critically dependent on both the av- erage number of active processes at each instant in time and the uniformity with which these processes can be partitioned among\n\nprocessors. Several studies have used event logs and sampling of a central event queue to characterize the average number of processes active at any instant in time. For event-driven lan- guages (e.g. VHDL) and delay-based modeling styles (such as IEEE Standard 1076.4, commonly known as the VIT AL mod- variance across time (a few active processes at one instant, hun- dreds to millions at others). Research at the Michigan State University showed that even relatively simple, but massive, SIMD machines can be ef fec- tively used for synchronous PDES if the domain of primitive so as to dynamically assign evaluations to processors. Since most VHDL models have at least several active processes at a time step (or none at all), we would expect SpeedW ave MT to yield good speedups for the relatively small number of proces- sors (two to eight) typically configured around a shared memory . These are indeed the kind of encouraging results reported for architecture). V iP uses dedicated hardware to pipeline the VHDL proves cache performance. However , we might expect V iP to be more sensitive to how evenly process activity is distributed across processors at each simulation cycle (due to static parti- ing several tens of processors; however , substantial effort needs to be invested in a suitable coding style and partitioning. Since the total number of processes active at any instant in time is inherently bounded for a given model, stimulus, and time instant, further increases in simulation performance may be gained by allowing each processor participating in the simu- lation to execute evaluations at one or more instants in time, which may be distinct for each processor . Provided these evalu- ations all correspond to the actual system under simulation, we will refer to this approach as asynchronous, conservative PDES. Several projects have studied the natural parallelism inher- ent in VHDL models as a result of the target machine, simula- tion algorithm, and natural parallelism of the model [1, 18, 19, 20, 21]. These studies are essential to improving our under - standing of the constraints governing the absolute performance The U.S. Air Force Institute of T echnology (AFIT) [22] has focuses on optimization of null-messages and partitioning al- gorithms to effectively use Intel hypercubes such as the iPSC/2 [23], The University of Minnesota [24], IBM [25] and FTL Sys- tem s [26], uses an asynchronous, conservative approach to PDES of VHDL and V erilog. During compilation, Auriga clusters elabo- rated HDL processes, process fragments or threads which may execute at the same instant in time (reduces to a cycle-driven approach in special cases) [23]. These clusters retain their own tion progresses. In order to increase the acceptable variance between local clocks, each event message not only implies the time at which an event occurs, but also the minimum time until the next event on that signal or sub-signal. The minimum time until the next event is largely based on a compile-time data- flow analysis. Using this approach, substantial speedups are feasible using systems with tens to hundreds of message-linked processors (such as the Thinking Machines CM-5, Intel Para- Using trace-driven simulation, researchers at the University of Genoa also concluded that asynchrony , correct static parti- tioning, and tight coupling were critical for effective parallel VHDL simulation [27]. Static partitioning needs to take into account both the computational complexity of each process and the predicted event traffic. T ight coupling seems to operation- ally mean a communications latency no more than about an order of magnitude larger than the average time to execute a process from one wait statement to the next. Several parallel VHDL simulator projects, which expect to use conservative PDES, are in the design stages, including a product development at TGI [28] and a research project at the allows simulation time on each processor to advance non-mono- back” to a previous state [30]. A simulator can either immedi- ately roll back and proceed with the simulation, or it can check to see if the non-causal message resulted in any incorrect com- putations. If so, the simulator rolls back; otherwise it proceeds with the simulation. When a rollback does occur , the state is returned to the last saved state immediately prior to the non- causal message received. The local clock is rolled back to the time of the prior state, and work previously performed beyond this point in simulated time is discarded as incorrect. Incorrect messages sent out must also be discarded, which can result in Optimistic simulators can use either aggressive cancellation, in which all incorrect messages are immediately tracked down and discarded via “anti-messages,” or lazy cancellation, in which messages are only discarded after a rollback when the mes- sages are known to be incorrect. Lazy cancellation can improve performance by easing the number of required rollbacks (since some messages may be correct and not require cancellation). If erroneous messages are not cancelled immediately , however , significant work may need to be discarded once the messages are eventually determined to be incorrect. Rajan and W ilsey attempt to exploit the benefits and ameliorate the penalties of each of these techniques by dynamically switching between\n\naggressive and lazy cancellation over the course of a simula- Applying throttling mechanisms to restrict the optimism is another technique to prevent excessive rollbacks and ficed to reduce the penalties associated with rolling back state. A good example of this is Sokol’ s moving time window (MTW) protocol, in which local clocks are only allowed to advance beyond the global simulation time by a fixed amount of time. adaptively change the window size (to “breathe”) based on the characteristics of the simulation. Research at the University of Cincinnati includes application of control theory to adapt to the In order to enable rollbacks, some state saving is required. An optimistic simulator can either save state incrementally with each change, or take periodic checkpoints of its state [34]. One can use analytic techniques to determine which approach is best [35], or control theory can be employed to develop adaptive algorithms for the best state saving frequency [36]. The simula- tor can also either save state to support rollback on an element- by-element basis, or it can save state so that all elements must roll back together . The cost of saving state information and roll- backs for individual elements is higher , but rollbacks can be restricted to only the elements affected by a non-causal event. In essence, the cost of saving state must be balanced against the penalty of rolling back in time. Costa and De Gloria find there is negligible performance degradation when the cost of roll- backs is less than 100 VHDL statements [27]. As the rollback While saving state, the memory requirements of optimistic algorithms can become excessive. Hence, “fossil collection” algorithms are used to determine when it is safe to reclaim on the value of the local clocks, the “global virtual time” (GVT), is periodically computed. No rollback can reset a local clock to a value below the GVT , so the saved state from before the GVT can be discarded, and simulation results up to the GVT can be committed. Several algorithms exist for computing GVT , and trade off the computation cost of time spent computing GVT with the benefit of reducing memory demands [37]. Thus far , few results from optimistic parallel VHDL simu- tion effort to date was developed by Jade Simulations and V antage, but no product was ever sold and no results are known In recent years, researchers at the University of Cincinnati and MTL Systems, Inc., developed the QUEST simulator [38] followed by the V AST simulator [39]. These simulators facili- tated research into many of the issues discussed above, includ- was developed for distributed memory architectures, and a ver- sion of QUEST also exists for execution using MPI (thus sup- but not a mix). V AST evolved from QUEST to support threaded QUEST and V AST can both execute on a network of worksta- The QUEST simulator is freely available for noncommercial use through MTL Systems or the University of Cincinnati. Although parallel simulation protocols typically are either conservative or optimistic, Hamnes and T ripathi propose using adaptive protocols to create simulators that switch between con- servative and optimistic at runtime [41]. This may prove to be\n\nthe best approach for achieving high performance in parallel VHDL simulation, although, once again, research remains to determine which adaptive techniques are most appropriate for particular types of models and parallel architecture. Parallel HDL simulation has been implemented or can be envi- sioned on a number of different types of parallel architectures [42]. For purposes of this discussion, we will generalize paral- lel architectures to include any machine with more than one data stream and the ability for communications between the allel architectures along with illustrative examples of machines at each terminal branch of the taxonomy . It is illustrative that parallel VHDL simulators have already been run on each of these example architectures; parallel VHDL simulation is a re- ality . Flynn [42] suggests the top-most branch in the taxonomy based on the number of independent instruction streams. Ma- chines with multiple instruction streams are the more flexible and thus account for most of the interest in parallel VHDL sim- SIMD machines can be used to simulate HDL models with a very small range of process data flows and component-specific behavior , as is often the case with gate-level models. Such ma- chines often have high raw performance and hundreds to tens of thousands of processors. The processors in SIMD machines tend to be very simple and thus cost-effective. W ithin the MIMD branch, both shared memory and distrib- memory machines generally support two to about 20 processors, whereas distributed memory systems can accommodate from sev- eral to hundreds of processors. The degree of coupling (expressed in terms of communication latency , bandwidth and af finity) ranges tributed memory MIMD machine is to use a network of work- cessing capability . Such computing resources suffer from relatively slow interconnections, however , so load balancing and inter - formance parallel simulation. For example, novel computing resources such as re-configurable or adaptive logic and proces- puting support for garbage collection or rollbacks for optimistic as well [47]. This paper does not include assumptions concern- with more than one concurrently executing data stream and the ability for communications between the data streams. W ith this overview of parallel simulation algorithms and architectures, the reader now has a framework for understand- ing the implementation space available for tool developers. While keeping these possible tool implementations in mind, we now investigate specific modeling issues and recommended V erilog is probably the earliest HDL still in widespread use. V erilog’ s origins are in both gate-level net-list modeling and the C programming language. Large-scale system design and gins are in system-level description languages and the Ada pro- gramming language. The origins of each language shape avail- able modeling capabilities and thus strategies for achieving high V erilog and VHDL differ in their tolerance for references which routines and path names, the dynamically selected value of a net or register may be read and potentially used to impact the behav- ior of any other task in the design. Furthermore, via a pathname, register values may be remotely set from any part of the V erilog model. In VHDL, after static elaboration, all dependencies be- tween processes anywhere in the design hierarchy are manifest. dencies between components of the model forces a parallel simu- lator into apparent synchronous operation. Either all tasks within a V erilog simulation must simultaneously advance from one time step to the next, or an optimistic simulation algorithm must be employed to preserve the appearance of synchronous operation. reference capability is one reason for the absence to date of ing doctrine by careful constraining and early binding of cross- scope references within the HDL. Mechanisms external to the language, such as VIT AL [14] and the Standard Delay Format (SDF), provide for back-annotation during elaboration. After are manifest (with non-zero delay). Thus the simulation kernel can permit distinct parts of the simulation to evaluate asynchro- Since simulation load is generally unevenly distributed at dif- ferent instants in the simulation time domain, opportunities for asynchrony are an important reason why significant speedups are observed when using parallel VHDL simulators. When cr oss-scope r efer ences ar e essential, they should be bound as early as possible in the compilation and simulation\n\nnames should be formed as early as possible, perhaps using Models may be written at many levels of abstraction including cuit (in order from most to least abstract). Performance models deal with abstract units of work (with tokens perhaps represent- ing an Internet packet) and servers (perhaps representing a graph- is doing without detailing how the functionality is implemented does not capture processor pipelining or data caching). Register- transfer models capture more of the internal micro-operations used to implement behavior (perhaps representing an instruction execution as an instruction decode from an instruction issue reg- logic gates (perhaps representing an adder as a sequence of full In general terms, stepping down a level of abstraction in- creases simulation effort by an order of magnitude. When sim- ulation executes on a single processor , an n-fold increase in simulation effort translates almost directly into an n-fold in- crease in simulation time. However with parallel simulation, increasing levels of detail can yield more units of work capable to complete a given simulation run much earlier than could a tail and simulation effort, the potential for parallel speedup con- tinues to improve relative to a uni-processor . However , internal lelism. For example, a thousand-fold increase in simulation ef- tors as high as 80% of linear are achieved with the latest paral- lation, it is important to exploit de-coupling between modeled as well. Representing such delays has an important side benefit: it reduces the potential for modeling artifacts leading to race conditions and non-deterministic simulation. This is particularly tion mechanisms all but disallow non-zero delay . While V erilog pling requires diligent and explicit effort by the model devel- very efficient, abstract models can benefit from parallel simu- lation. T echniques have been developed to increase the time window across which events may be simultaneously evaluated [38] and to permit concurrent evaluation of the same process at different instants in simulation time [48]. Thus even a model consisting of a single large process or task may benefit from Model at the highest level of abstraction which adequately captur es the behavior to be verified. Explicitly r epresent pr o- cessing and communication latencies appr opriate for the communication pathways, especially when using V erilog. ation workload across all processors at each time step. In prac- tice, the work to be done at distinct time steps varies widely , from a single active process or task to many simultaneously ac- tive processes or tasks. Furthermore, the work associated with from a few machine instructions to millions of instruction evalu- ations for each process or task activation. These deviations from uniform granularity tend to reduce the performance available Asynchronous parallel simulators are usually more tolerant of deviations from ideal load balancing; however , they too are tors allow “borrowing” of workload from “future” time steps so as to increase the number of processors effectively employed. While such borrowing helps to even load across time, imbal- ances in the total evaluation load between processors work to niques are hard to implement efficiently because they detract from effort used to execute model functionality . or parasitic information into an HDL simulation (such as SDF or VIT AL) increase the correspondence of the simulation to real- ity at the expense of parallel simulation performance. In addi- tion to the increased computation implied by the detail (seen equally on a uni-processor and parallel processors), such tim- ing models will tend to distribute evaluations at a wider range of instants in the simulation time domain, which decreases po- annotation can be reduced by reducing the precision of delay or On the other extreme, one may reduce temporal detail using a levelized or cycle-driven approach. This approach may be implemented either by compiler transformations [24] or a mod- runs with fewer time populated time steps, many more active processes at each populated time step, and simpler process evalu- ations (due to the simplified time model). W e would expect such approaches to not only yield good uni-processor performance, but also increase the parallelism which can be exploited during\n\nParallel simulation performance will be impr oved by mini- mizing the number of distinct time steps at which pr ocesses ar e active and ensuring that the workload at each such time If the number of VHDL processes is less than the number of processors available for parallel simulation, or the granularity of each process is too small, simulation efficiency can be im- paired. In the absence of complex compiler transformations, such models will seldom if ever approach linear speedup. When targeting a uni-processor simulation, the fixed over - head associated with scheduling dictates that the number of pro- cesses or tasks be minimized. However , for a parallel simulator, independently scheduled process, tasks, or their equivalents of- fer an important opportunity for parallel execution. Ideally , each tering into aggregates of processes, tasks, or their equivalents which are “gang-scheduled” helps to minimize scheduling over- head (often referred to as a physical to logical process transfor- if ther e are significantly mor e processes, tasks, or their equiva- VHDL ’ s strong typing system is a very significant asset when not specify aspects such as actual data type precision and cor- respondence of integers to bit vectors. When executed on a par - allel processing system using a variety of different processor architectures such as a network of workstations, arbitrary map- ping decisions between processes and processors have the po- Scheduling and transmitting an event on a net or signal has overhead almost independent of the net or signal’ s data type. The computational cost of transmitting an event between processors is generally higher than when the communication takes place in the memory system of a single processor . Thus if the value of more than one scalar net or signal value is likely to occur at the same time, parallel simulators benefit proportionally more by aggregation [23], the model developer also implements such aggregation with the potential for less obvious HDL source code. Following the Ada model, VHDL ’ s type system provides a very restricted form of pointer , known as an access type (V erilog does not (yet) have an equivalent mechanism). Operators on access types are very limited, reducing the aliasing problems often encountered when compiling languages such as C (with Group has already recognized the problem and has drafted and balloted a solution based on protected types. One result of this solution is shared variables may not be of access type. likely to be assigned at the same time and the aggr egate does not obscur e meaning of the HDL sour ce. Both to incr ease performance and to minimize opportunities for memory leak- Signals (and V erilog’ s nets) are the primary mechanism for com- municating between sequential processes (and tasks). The cor - respondence between these HDL process/task models and the resolved type (such as V erilog’ s “wired-or” or tristate) are con- venient but computationally expensive, not only because of the resolution computation, but because the pseudo-process often created to implement resolution usually cannot be evaluated until all inputs are known for a given time interval. Thus reso- lution functions locally constrain the ability of even asynchro- time. Again, aggressive compilation techniques are available to minimize the impact of resolution functions as a barrier to par- In or der to impr ove parallel simulation performance, try to minimize the number of drivers associated with implicitly VHDL signals have at least a delta (non-zero) propagation de- lay . Thus there are no zero-delay cycles which would serve to variables were added, resulting in the potential for zero-delay communications involving two or more processes. This com- to deal with deadlock induced by the simulation algorithm, it also leads to potentially non-deterministic results on almost any complicate parallel simulation, both from the standpoint of logi-\n\nwhich are simulation artifacts) and decreased simulation per- variables were a problem when they were added to VHDL ’ s 1993 revision. Therefore, DASC chartered a working group to fix the problem (1076a). This group is working toward a variation of Ada’ s protected types. Whereas this solution provides a com- pletely defined behavior , the solution does not completely solve Unfortunately , register variables are such an intrinsic part of V eri- log that no such solution is contemplated (or likely to be feasible). In particular , processes sharing a variable will either be un- able to evaluate at different times (asynchronous parallel simu- lation), reducing load balancing efficiency , or are likely to be tween the processes which must access it, resulting in throttled issues associated with maintaining accurate data for multiple memory systems, the implementations of tools must be more complicated, while also facing the expensive costs of support- the shared variables. Any of these prospects translates into re- In or der to impr ove parallel simulation performance, try to avoid use of variables r efer enced by mor e than one process. Wher e they must be used, try to maximize the shar ed or r eg- ister ed variable’ s locality to as few pr ocesses as possible, ideally pr ocesses which ar e also closely r elated by signals and timing. For testing purposes, the non-local use of vari- ables to gain access to the values of internal variables should be limited to debugging. When running r egr ession tests, the use of shar ed variables should be avoided to maximize sim- Another popular aspect of V erilog is its Programming Language Interface (PLI), which provides a mechanism for V erilog mod- els to include functionality defined outside the V erilog code. The V erilog PLI is often used to support waveform display tools, modeling the software components in other languages such as C or C++. W ork is underway to add a PLI-like interface to VHDL which is expected to have many of the same strengths and weak- The use of a PLI or the VHDL foreign attribute in order to “trap” out of the HDL model to support outside interactions makes many optimizations difficult or impossible and can severely de- zation protocols must be extremely pessimistic about the possi- bility of zero-time communications in the case of conservative protocols, and optimistic protocols may not be able to support the rollback mechanism in this case. Use of foreign architectures and subprograms complicates the static evaluation of process complexity essential to good load balancing. Whereas use of foreign “library” architectures or sub- programs is often a practical matter defined by the modeling project’ s specifications, it is worth keeping in mind that such li- braries can be an impediment to parallel simulation performance. The performance problems associated with such libraries are also seen for uni-processors, where the overhead and opaque- ness (preventing optimizations such as in-lining) also reduce If your VHDL compiler does global optimization or load balancing, avoid use of for eign ar chitectures or subpr ograms wher e characterizations of the ar chitecture or subpr ogram body ar e not available to the HDL compiler . Given the importance of analog and mixed analog/digital simu- lation, efforts are underway to extend both VHDL and V erilog to embrace analog and mixed-signal design. The VHDL-AMS (analog and mixed-signal) language has already been success- fully balloted by IEEE, and thus will be discussed below in detail. Many of the same observations are expected to be true of V erilog-AMS when it is balloted by IEEE or IEC. Particular At least four parallel VHDL-AMS simulators are under devel- opment. Experience with these implementations is expected to Dividing execution of a single analog solver across more than one processor is generally accepted as inefficient due to the need to communicate substantial information on each cycle of the analog solver . Since several cycles are commonly required for tial for parallel solution. Successful results have been achieved using vector processors (with intrinsically low latency between able increase in convergence risk) [49] and by partitioning of larger circuits into islands of continuous-domain models com- In the context of general-purpose parallel processors and analog systems into multiple islands is one of the most ef fec- tive modeling techniques available to the mixed-signal model designer . When complete analog partitions are not acceptable, isolating subsets of the model which operate at high frequency , surrounded by slowly varying nodes, is often an effective tech- nique available to the user . In or der to maximize parallel simulation performance, when possible, partition lar ge VHDL-AMS into smaller islands of\n\nAnalog solvers conceptually iterate until a set of node and branch state values are determined consistent with a system of differ - set of values need not be reached. Rather the differential equa- tions must be satisfied to within a specified (or implicit) set of tolerances. The broader these tolerances are, the easier it is to divide the parallel solver among more than one processor by facilitating convergence of independent parts of the solution. Unfortunately , broad tolerances allow solutions further from and fidelity to the physical system. In or der to maximize parallel simulation performance, de- note the widest tolerances consistent with the r equir ed ana- When using a relaxation approach to facilitate parallel execu- voltage or current value (state), known as discontinuities, can back (when using an optimistic simulation algorithm). Inertia associated with physical systems means that discontinuities are use of mechanisms, such as VHDL-AMS’ s ramp and slew at- tributes, both reduce the impact of discontinuities on parallel In or der to maximize parallel simulation performance, try to avoid situations in which implicit or explicit value disconti- Parallel HDL simulation has been demonstrated on almost ev- tion for model developers, this paper develops a set of recom- for HDL model development on a wide variety of simulation massively parallel). This treatment allows developers to better understand their options and make more informed choices, helps researchers understand the current state of the art and the rel- evance of certain research efforts, provides a structure that is useful pedagogically , and gives an overview of the current state of the art in parallel VHDL simulation. Through the ef forts of group, these guidelines are being refined and augmented to better The authors wish to acknowledge the IEEE DASC Parallel VHDL Group (more than 100 people) for their contributions to in this paper or other assistance came from Philip W ilsey , V ijay V aidyanathan and others. This ef fort was supported by the Air Force Research Laboratory , as well as DARP A under FTL Systems, Inc. This paper does not necessarily reflect the position of policy of the U.S. Government and no official en- in Design and V erification of Distributed Programs.” [12] Pr oceedings of the W orkshops on Parallel and Distributed Simu-"
    },
    {
        "link": "https://pages.hmc.edu/harris/cmosvlsi/4e/cmosvlsidesign_4e_App.pdf",
        "document": ""
    },
    {
        "link": "https://drops.dagstuhl.de/storage/00lipics/lipics-vol136-snapl2019/LIPIcs.SNAPL.2019.7/LIPIcs.SNAPL.2019.7.pdf",
        "document": ""
    },
    {
        "link": "https://wevolver.com/article/rtl-design-a-comprehensive-guide-to-unlocking-the-power-of-register-transfer-level-design",
        "document": "Register-Transfer Level (RTL) design is a critical aspect of digital circuit design. It is a design abstraction representing how data flows between hardware registers and the operations performed on that data. RTL design approach is used in creating digital systems where the flow of data between registers, and the operations on that data, are important for the system's functionality.\n\nAs digital circuits become more complex, and demand for more efficient systems increases, RTL design has become an essential skill for design engineers alike. It provides a level of abstraction that enables efficient design and verification of complex digital systems. The importance of RTL design is further underscored by its role in the design of microprocessors, digital signal processors, and other complex integrated circuits.\n\nRTL design provides an abstraction that makes it possible to design and verify systems that contain millions of transistors. It provides a manageable way to design complex digital systems and is a critical step in transforming a high-level system description into a fabricated integrated circuit.\n\nRegister-Transfer Level (RTL) design is a methodology used for designing digital circuits. The term \"register-transfer\" refers to the level of abstraction at which the design is considered. [1] At this level, the design is described in terms of the flow of data between registers, and the logic operations that take place on that data.\n\nIn RTL design, the designer specifies the data operations to be performed, and the flow of data between registers. The designer does not need to specify how these operations are to be implemented in the hardware. This level of abstraction allows the designer to focus on the high-level function of the system, without getting stuck in the details of the implementation.\n\nRTL design is typically done using a hardware description language (HDL) such as VHDL or Verilog. These languages provide constructs that allow the designer to describe the system in terms of registers, operations, and data flows. The use of an HDL allows the designer to describe the system in a way that is independent of the specific hardware that will be used to implement the system.\n\nRecommended Reading: Guide to Mastering SystemVerilog: Elevate Your Hardware Design and Verification Skills\n\nIn RTL design, a register is a hardware element that can store a fixed amount of data. The data stored in a register can be used as input to a logic operation, and the result of the operation can be stored back in the register. Registers are the basic building blocks of digital systems, and the flow of data between registers is a key aspect of the system's function. [2]\n\nRegisters in RTL design are typically binary, meaning they store data in the form of binary digits, or bits. The number of bits a register can store is referred to as the width of the register. For example, a register that can store 8 bits of data is called an 8-bit register.\n\nThe data stored in a register is often referred to as the state of the register. The state of a register can change over time as new data is stored in the register. The sequence of states that a register goes through over time is a key aspect of the behavior of the digital system.\n\nIn RTL design, registers are used to hold the inputs, outputs, and intermediate results of the logic operations performed. The designer specifies the sequence of operations to be performed and the flow of data between registers to implement the desired function of the system.\n\nIn RTL design, operations are the logic functions that manipulate and process the data stored in registers. These operations are the fundamental building blocks of digital systems and are responsible for implementing the desired functionality of the system. Operations in RTL design can be broadly categorised into two types: arithmetic operations and logical operations.\n\nArithmetic operations are used to perform mathematical calculations on the data stored in registers. Some common arithmetic operations include addition, subtraction, multiplication, and division. These operations are typically implemented using dedicated hardware elements, such as adders, subtractors, multipliers, and dividers. In RTL design, arithmetic operations are specified using the appropriate constructs in the hardware description language (HDL), such as VHDL or Verilog.\n\nFor example, in VHDL, an addition operation can be specified as follows:\n\nIn this example, a and b are input registers, and sum is the output register that stores the result of the addition operation.\n\nLogical operations are used to perform bitwise manipulation of the data stored in registers. Some common logical operations include AND, OR, NOT, XOR, and shift operations. These operations are implemented using basic logic gates, such as AND gates, OR gates, and inverters. In RTL design, logical operations are specified using the appropriate constructs in the HDL.\n\nFor example, in Verilog, an AND operation can be specified as follows:\n\nIn this example, a and b are input registers, and result is the output register that stores the result of the AND operation.\n\nIn RTL design, operations are used to implement the desired functionality of the system by processing the data stored in registers. The designer specifies the sequence of operations to be performed and the flow of data between registers to achieve the desired system behavior. The designer can efficiently design and verify complex digital systems by focusing on the operations and data flow at the RTL level.\n\nAt the heart of digital circuit design, two fundamental approaches orchestrate data flow: synchronous and asynchronous logic. [3] Within RTL (Register Transfer Level) design, they each play distinct roles, shaping how systems operate and interact. While synchronous logic relies on a global clock signal to guide operations, asynchronous logic empowers elements to move independently, guided by local signals.\n\nAny circuit, from asynchronous to synchronous, may be designed using the RTL\n\nAny digital circuit, whether combinational or sequential, can be designed using RTL. RTL logic can be implemented in a similar fashion to event-based or clock-based logic. The event-based reset logic in that code, which is independent of other events, allows the circuit to be reset, as shown in the figure below. The TL logic for asynchronous logic is shown in the figure:\n\nAdditionally, events in synchronous logic depend on the synchronisation of the clock as depicted below.\n\nIn general, synchronous logic is the preferred choice for most digital systems due to its predictability and ease of design. However, asynchronous logic can offer advantages in specific applications where power consumption or performance is critical.\n\nThe RTL design process is a series of steps that transform a high-level system description into a low-level implementation that can be fabricated onto a chip. This process, also known as front end involves several key stages, including high-level synthesis, RTL coding, and RTL verification.\n\nThe first stage of the RTL design process is high-level synthesis, where a high-level description of the system is transformed into an RTL description. This involves translating the system's behavior, specified in a high-level language such as C or C++, into an equivalent RTL description in a hardware description language (HDL) like VHDL or Verilog.\n\nThe next stage is RTL coding, where the RTL description is further refined and optimised. This involves writing the RTL code that describes the data flow and operations of the system, and optimising this code to meet the design requirements.\n\nThe final stage is RTL verification, where the correctness of the RTL design is checked. This involves simulating the RTL design to ensure it behaves as expected, and formally verifying the design to prove its correctness.\n\nLet's take a closer look at each of these stages.\n\nHigh-level synthesis (HLS) is the process of transforming a high-level system description into an equivalent RTL description. This involves translating the system's behavior, specified in a high-level language such as C or C++, into an equivalent RTL description in a hardware description language (HDL) like VHDL or Verilog.\n\nHLS is a critical step in the RTL design process as it allows designers to describe the system's behavior in a high-level language, which is easier to understand and write than low-level HDL code. This not only speeds up the design process but also reduces the chance of errors.\n\nDuring HLS, the high-level system description is analyzed, and the data flow and operations of the system are extracted. These are then mapped onto an RTL description, which describes the system in terms of registers, operations, and data flows.\n\nFor example, consider a high-level description of a system that performs a simple arithmetic operation, such as adding two numbers. The HLS process would analyse this description, extract the addition operation and the data flow between the input and output, and map these onto an RTL description that specifies the addition operation and the flow of data between registers.\n\nHLS tools, such as Cadence Stratus or Synopsys' Catapult, automate this process, allowing designers to quickly and accurately generate RTL descriptions from high-level system descriptions. [4] These tools also provide optimization features that can improve the performance, power, and area of the resulting RTL design.\n\nRTL coding is the process of writing the RTL description of a digital system using a hardware description language (HDL) such as VHDL or Verilog. This stage of the RTL design process involves specifying the data flow and operations of the system in terms of registers, operations, and data flows. RTL coding is crucial for defining the system's behavior and ensuring that it meets the design requirements.\n\nDuring RTL coding, the designer writes the HDL code that describes the system's registers, operations, and data flows. This code is typically organised into modules, which are reusable blocks of code that represent specific parts of the system. Modules can be instantiated multiple times within a design, allowing for efficient reuse of code and simplification of the overall design.\n\nVHDL and Verilog are the two most widely used HDLs for RTL coding. Both languages provide constructs for describing registers, operations, and data flows, as well as control structures for specifying the sequence of operations.\n\nVHDL (VHSIC Hardware Description Language) is a strongly typed, concurrent language that provides a high level of abstraction for describing digital systems. It supports a wide range of data types and constructs, making it suitable for describing complex systems. VHDL code is typically organised into entities and architectures, where entities define the interface of a module, and architectures define the behavior of the module. [5]\n\nFor example, a simple VHDL module that implements a 4-bit adder might look like this:\n\nVerilog is a weakly typed, concurrent language that provides a lower level of abstraction than VHDL. It is well-suited for describing digital systems at the gate, register-transfer, and behavioral levels. Verilog code is typically organized into modules, which define both the interface and behavior of a part of the system.\n\nFor example, a simple Verilog module that implements a 4-bit adder might look like this:\n\nDuring the RTL coding stage, the designer must ensure that the HDL code accurately describes the desired behavior of the system and meets the design requirements. This may involve optimizing the code for performance, power, and area, as well as ensuring that the code is modular and reusable. Once the RTL coding is complete, the design moves on to the RTL verification stage, where the correctness of the RTL description is checked.\n\nRTL verification is the final stage in the RTL design methodology, where the correctness of the RTL design is checked. This stage is crucial to ensure that the RTL description accurately represents the intended behavior of the digital system and meets the design requirements. There are two main methods used in RTL verification: simulation and formal verification.\n\nVerification is crucial to ensure the design's correctness, robustness, and reliability.\n\nSimulation is a dynamic verification method where the RTL design is tested by applying a set of input vectors and observing the resulting output vectors. The input vectors are typically derived from the system's specifications and are designed to exercise all the functional aspects of the system. The output vectors are then compared with the expected results to check the correctness of the design.\n\nSimulation tools, such as ModelSim or VCS, are used to perform RTL simulation. These tools execute the RTL code and provide a waveform view of the signals in the design, allowing the designer to visually inspect the behavior of the system.\n\nFor example, in a simulation of a 4-bit adder, the input vectors might be pairs of 4-bit numbers, and the expected output vectors would be the sums of these numbers. The simulation tool would apply these input vectors to the adder, observe the resulting output vectors, and compare them with the expected results to verify the correctness of the adder.\n\nFormal verification is a static design verification method that mathematically proves the correctness of the RTL design. This involves expressing the intended behavior of the system as a set of formal properties, and then using a formal verification tool to prove that these properties hold for the RTL design.\n\nFormal verification tools, such as Cadence's JasperGold or Synopsys' VC Formal, use mathematical algorithms to exhaustively explore the state space of the design and prove the correctness of the formal properties. This provides a higher level of assurance of the design's correctness than simulation, as it can prove the correctness of the design for all possible input vectors, not just a subset. UVM is also a standardized framework for creating reusable and scalable testbenches, streamlining verification processes in RTL design.\n\nFor example, in a formal verification of a 4-bit adder, the formal properties might specify that for any pair of 4-bit numbers, the output of the adder is the sum of these numbers. The formal verification tool would then mathematically prove that this property holds for the RTL description of the adder.\n\nRTL verification is a critical stage in the RTL design process, as it ensures that the RTL description accurately represents the intended behavior of the system and meets the design requirements. By using simulation and formal verification methods, designers can confidently verify the correctness of their RTL designs and proceed to the next stages of the design process.\n\nAdvanced concepts in RTL design involve techniques and methodologies that go beyond the basic RTL design process. These include RTL partitioning, pipelining, and clock domain crossing, among others. These advanced concepts are used to manage the complexity of large designs, optimise the performance, power, and area of the design, and handle special design requirements.\n\nRTL partitioning is a technique used to manage the complexity of large RTL designs. It involves dividing the design into smaller, more manageable parts, or partitions, each of which can be designed and verified independently. This not only simplifies the design process but also allows for parallelism, where different partitions can be designed and verified by different designers or teams at the same time.\n\nPartitioning can be done in several ways, depending on the requirements of the design. One common method is functional partitioning, where the design is divided based on its functional blocks. For example, in a microprocessor design, the processor core, memory, and input/output (I/O) interfaces might be separate partitions.\n\nAnother method is hierarchical partitioning, where the design is divided into a hierarchy of partitions. This is often used in complex designs that consist of several layers of hierarchy, with each layer representing a different level of abstraction.\n\nPartitioning can also be done based on the physical characteristics of the design, such as low-power domains or clock domains. This is known as physical partitioning and is used for power estimation and timing analysis of design.\n\nRTL partitioning is a powerful technique for managing the complexity of large designs. By dividing the design into smaller, more manageable parts, designers can focus on the details of each part without being overwhelmed by the complexity of the whole design. This not only simplifies the design process but also improves the quality of the design, as each part can be thoroughly designed and verified.\n\nRTL optimization is a critical step in the RTL design process that aims to improve the performance, power, and area (PPA) of the design. [6] This involves refining the RTL description to make it more efficient and effective in meeting the design requirements.\n\nPerformance optimization involves improving the speed of the design. This can be achieved by techniques such as pipelining, where the design is divided into stages that can be executed in parallel, and parallelism, where multiple operations are performed simultaneously. For example, in a processor design, instruction fetching, decoding, execution, and write-back can be pipelined to improve the processor's instruction throughput.\n\nPower optimization involves reducing the power consumption of the design. This can be achieved by techniques such as clock gating, where the clock signal is disabled for parts of the design that are not in use, and power gating, where the power supply is cut off to parts of the design that are not in use. For example, in a mobile device design, power gating can be used to turn off the GPS module when it is not in use to save power.\n\nArea optimization involves reducing the size of the design. This can be achieved by techniques such as resource sharing, where multiple parts of the design share the same resources, and constant propagation, where constants are propagated through the design to simplify operations. For example, in a digital signal processing (DSP) design, multiple filters can share the same multiplier to save area.\n\nRTL optimization is performed using RTL synthesis tools, such as Synopsys' Design Compiler or Cadence's Genus. These tools analyze the RTL description and apply various optimization techniques to improve the PPA of the design. The designer can guide the optimization process by setting constraints on the performance, power, and area of the design, and the synthesis tool will try to meet these constraints while optimizing the design.\n\nRTL optimization is a complex and challenging task that requires a deep understanding of the design and the optimization techniques. However, it is a crucial step in the RTL design process, as it can significantly improve the PPA of the design and make it more competitive in the market.\n\nRTL design, while a powerful methodology for digital system design, presents several challenges. These include handling design complexity, managing power consumption, and ensuring design correctness. However, various solutions have been developed to address these challenges, including RTL partitioning, power optimization techniques, and formal verification methodology.\n\nOne of the main challenges in RTL design is managing the complexity of large designs. As the size and complexity of digital systems continue to grow, the task of designing and verifying these systems at the RTL level becomes increasingly difficult. This complexity can lead to longer design times, increased risk of design errors, and difficulty in meeting performance, power, and area requirements.\n\nRTL partitioning is a key technique for handling design complexity. By dividing the design into smaller, more manageable parts, designers can focus on the details of each part without being overwhelmed by the complexity of the whole design. This not only simplifies the design process but also improves the quality of the design, as each part can be thoroughly designed and verified.\n\nAnother approach to managing design complexity is the use of high-level synthesis (HLS) tools. These tools allow designers to describe the design at a higher level of abstraction using a high-level programming language, such as C or C++. The HLS tool then automatically generates the RTL description from the high-level description. This not only reduces the complexity of the design process but also allows designers to leverage their software programming skills.\n\nDesign reuse is another effective strategy for managing design complexity. By reusing proven design blocks, or IP cores, designers can reduce the amount of new RTL code that needs to be written and verified. This not only saves design time but also reduces the risk of design errors.\n\nBy using techniques such as RTL partitioning, high-level synthesis, and design reuse, designers can effectively manage the complexity of large designs and produce high-quality, efficient digital systems.\n\nEnsuring design correctness is another significant challenge in RTL design. Given the complexity of modern digital systems, verifying that the RTL description accurately represents the intended behavior of the system is a non-trivial task. Errors in the RTL description can lead to functional errors in the final system, which can be costly to fix if not detected early in the design process.\n\nSimulation is a widely used method for checking design correctness. By applying a set of input vectors to the RTL design and observing the resulting output vectors, designers can check whether the design behaves as expected. However, due to the exponential growth of the state space with the size of the design, it is impossible to simulate all possible input vectors for large designs. Therefore, creating a comprehensive set of test vectors that can effectively exercise all functional aspects of the design is a challenging task.\n\nFormal verification offers a solution to this challenge. By expressing the intended behavior of the system as a set of formal properties, and using a formal verification tool to mathematically prove that these properties hold for the RTL design, designers can ensure the correctness of the design for all possible input vectors. However, writing the formal properties and interpreting the results of the formal verification tool requires a high level of expertise.\n\nAnother approach to ensuring design correctness is the use of assertion-based verification (ABV). In ABV, designers write assertions, which are statements of intended behavior, in the RTL code. [7] These assertions are then checked during simulation or formal verification. If an assertion fails, it indicates a discrepancy between the intended and actual behavior of the design.\n\nLinting is another technique used to ensure design correctness. Linting tools perform static analysis of the RTL code to check for common coding errors, adherence to coding standards, and potential synthesis and timing issues. By catching these issues early in the design process, linting tools can help improve the quality of the RTL code and reduce the risk of design errors.\n\nBy using a combination of simulation, formal verification, assertion-based verification, and linting, designers can ensure that the RTL description accurately represents the intended behavior of the system and meets the design requirements.\n\nA real-world example of implementing RTL design can be seen in the development of a digital signal processing (DSP) system for a wireless communication device. The DSP system is responsible for processing the digital signals received from the wireless interface, and performing operations such as filtering, modulation, and demodulation.\n\nThe first step in the RTL design process was to define the specifications of the DSP system. These specifications included the types of signals to be processed, the required processing operations, and the performance, power, and area requirements of the system.\n\nNext, the RTL description of the DSP system was written using a hardware description language, such as VHDL or Verilog. The RTL description defined the data paths and control logic of the DSP system, including the registers, arithmetic units, and control units.\n\nThe RTL description was then verified using simulation and formal verification methods. For simulation, a set of test vectors was created based on the system's specifications, and these vectors were applied to the RTL design using a simulation tool. The output vectors were observed and compared with the expected results to check the correctness of the design.\n\nFor formal verification, a set of formal properties was written to express the intended behavior of the DSP system. A formal verification tool was used to mathematically prove that these properties hold for the RTL design.\n\nAfter the RTL design was verified, it was optimized using an RTL synthesis tool. The tool analyzed the RTL description and applied various optimization techniques to improve the performance, power, and area of the design. The designer guided the optimization process by setting constraints on the performance, power, and area of the design.\n\nFinally, the optimized RTL design was translated into a gate-level netlist using the synthesis tool. The netlist was then used for physical design, where the layout of the DSP system was created for fabrication. Testbench is used as a virtual environment to simulate and verify the functionality and timing of an RTL design before physical fabrication in VLSI.\n\nThis case study illustrates the application of RTL design in a real-world scenario. Despite the complexity of the DSP system, the RTL design process enabled the designers to effectively manage the design complexity, ensure the correctness of the design, and optimize the performance, power, and area of the system.\n\nRTL design plays a pivotal role in the development of digital systems. It serves as the bridge between the conceptual, high-level system design and the physical, gate-level design. RTL design allows designers to focus on data paths and control logic without worrying about low-level details. RTL design significantly contributes to the competitiveness of digital systems in the market.\n\nWith the evolution of the size and complexity of digital systems, new challenges arise, and new solutions are developed. Techniques such as high-level synthesis, assertion-based verification, and power optimization are continually evolving to meet the changing needs of the industry. RTL design is an evolving field with new challenges and solutions.\n\nQ. What is the difference between RTL design and gate-level design?\n\n\n\nA. RTL design and gate-level design are two levels of abstraction in the design of digital systems. RTL design focuses on the data paths and control logic of the system, while gate-level design focuses on the implementation of the system using logic gates. RTL design is closer to the system's functionality and is easier to understand and modify, while gate-level design is closer to the physical implementation and provides more accurate estimates of the system's performance, power, and area.\n\nQ. What is the role of simulation in RTL design?\n\n\n\nA. Simulation plays a crucial role in verifying the correctness of the RTL design. By applying a set of input vectors to the RTL design and observing the resulting output vectors, designers can check whether the design behaves as expected. However, due to the large state space of modern digital systems, creating a comprehensive set of test vectors is challenging.\n\nQ. What is formal verification, and how is it used in RTL design?\n\n\n\nA. Formal verification is a method for ensuring the correctness of the RTL design. It involves expressing the intended behavior of the system as a set of formal properties and using a formal verification tool to mathematically prove that these properties hold for the RTL design.\n\nQ. What is RTL synthesis and why is it important?\n\n\n\nA. RTL synthesis is the process of translating the RTL design into a gate-level netlist. It involves analyzing the RTL description, applying various optimization techniques to improve the performance, power, and area of the design, and generating the gate-level netlist. RTL synthesis is important because it bridges the gap between the digital design and the physical design, and it plays a crucial role in meeting the design requirements.\n\n[3] Tutorialspoint. Difference between Synchronous and Asynchronous Sequential Circuits [Cited 2024 January 10] Available at: Link\n\n[5] Medium. VHDL — Understanding the Hardware Description Language [Cited 2024 January 10] Available at: Link\n\n[6] synopsys. Optimizing the RTL Design Flow with Real-Time PPA Analysis [Cited 2024 January 10] Available at: Link\n\n[7] ACM. A Survey on Assertion-based Hardware Verification [Cited 2024 January 10] Available at: Link"
    }
]