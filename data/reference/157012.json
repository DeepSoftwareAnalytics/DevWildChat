[
    {
        "link": "https://learn.unity.com/tutorial/creating-physically-based-materials",
        "document": "The Unity Editor uses Physically Based Rendering (PBR) in order to more accurately simulate realistic lighting scenarios. To fully take advantage of PBR, a material should be physically based. This is done through the Shader — the script that mathematically calculates how a material should appear under different lighting conditions. In this tutorial, you will learn about standard Shaders and texture properties."
    },
    {
        "link": "https://docs.unity3d.com/2017.4/Documentation/Manual/MaterialValidator.html",
        "document": "The Physically Based Rendering Material Validator is a draw mode in the Scene View. It allows you to make sure your materials use values which fall within the recommended reference values for physically-based shaders. If pixel values in a particular Material fall outside of the reference ranges, the Material Validator highlights the pixels in different colors to indicate failure.\n\nTo use the Material Validator, select the Scene View’s draw mode drop-down menu, which is is usually set to Shaded by default.\n\nNavigate to the Material Validation section. The Material Validator has two modes: Validate Albedo and Validate Metal Specular.\n\nNote: You can also check the recommended values with Unity’s Material Charts. You still need to use these charts when authoring Materials to decide your albedo and metal specular values. However, the Material Validator provides you with a visual, in-editor way of quickly checking whether your Materials’ values are valid once your Assets are in the Scene.\n\nAlso note: The validator only works in Linear color space. Physically Based Rendering is not intended for use with Gamma color space, so if you are using Physically Based Rendering and the PBR Material Validator, you should also be using Linear color space.\n\nThe PBR Validation Settings that appear in the Scene view when you set Material Validation to Validate Albedo.\n\nUnity’s Material charts define the standard luminance range as 50–243 sRGB for non-metals, and 186–255 sRGB for metals. Validate Albedo mode colors any pixels outside of these ranges with different colors to indicate that the value is too low or too high.\n\nIn the example below, the first texture is below the minimum luminance value, and therefore too dark. The fourth texture is above the maximum luminance value, and therefore too bright.\n\nThe material charts provide albedo values for common Materials. The brightness of albedo values has a dramatic impact on the amount of diffuse bounce light generated, so it is important for Global Illumination baking to make sure that your different Material types are within the correct luminance ranges, in proportion with each other. To help you get these values right, you can select from the presets in the Luminance Validation drop-down, which provides common Material albedo values to verify the luminance ranges of particular Material types.\n\nDepending on the art style of your project, you might want the luminance values of Materials to differ from the preset luminance ranges. In this case, you can override the built-in albedo values used by the Material Validator with your own values. To override the preset luminance ranges, assign an array of AlbedoSwatchInfo values for each desired Material type to the property EditorGraphicsSettings.albedoSwatches.\n\nThe PBR Validation Settings that appear in the Scene view when you set Material Validation to Validate Metal Specular.\n\nIn Unity, all non-metallic Materials have a constant specular color that always falls within the correct range. However, it is common for metallic Materials to have specular values that are too low. To help you identify metallic Materials with this issue, the Material Validator’s Validate Metal Specular mode colors all pixels that have a specular color value that is too low. This includes all non-metallic materials by definition.\n\nIn the example below, the left material is below the minimum specular value, and therefore too dark. This also applies to the Scene’s background. The right material has specular values with in the valid range.\n\nUnity defines physically-based shading materials with a specular color greater than 155 sRGB as metallic. For Unity to define a metallic Material as a pure metal\n\nIf a non-metallic surface has a specular color value that is too high, but has a non-zero albedo value, this is often due to an authoring error. The Material Validator also has an option called Check Pure Metals. When you enable this option, the Material Validator colors in yellow any Material that Unity defines as metallic but which has a non-zero albedo value. An example of this can be seen in the images below. It shows three materials, the left and right materials are pure metals, but the middle material is not, so the Material Validator colors it yellow:\n\nIn the second image above, the background is red because the Materials in the background are below the minimum specular value for the Material Validator’s Validate Metal Specular mode.\n\nFor complex materials that combine metallic and non-metallic properties, the pure metal checker is likely to pick up some invalid pixels, but if a Material is totally invalid, it’s usually a sign of an authoring error.\n\nThe Material Validator works with any Materials that use Unity’s Standard shader or surface shaders. However, custom shaders require a pass named . Most custom shaders that support lightmapping already have this pass defined. See documentation on Meta pass for more details.\n\nCarry out the following steps to make your custom shader compatible with the Material Validator:\n• Add the following pragma to the meta pass:\n• In the structure, assign the specular color of the Material to the field called , as shown in the code example below.\n\nHere is an example of a custom meta pass:"
    },
    {
        "link": "https://3d-ace.com/blog/physically-based-rendering-using-pbr-for-games-animations-and-more",
        "document": "As our technological progress races forward, we are getting more used to higher detail, higher realism, and flashier effects from the digital content that we consume. We have the right tools to satisfy this craving (apps, plugins, executables), but we also need the right technique. This is where physically-based rendering (PBR) comes in: a perfect solution to this modern problem.\n\nHopefully, by the time you finish reading, you will be prepared to make amazing PBR content of your own, or at least hire PBR artists with these qualifications. If you’re just curious and want to know how PBR works, don’t worry. We’ll cover it in this intro to physically-based rendering.\n\nWhat is physically-based rendering (PBR) and why is it important?\n\nPhysically-based rendering can be described as an approach or philosophy to creating 3D content focused on recreating lifelike physics, especially in all aspects pertaining to light. You can think of it as a new and prestigious standard that 3D artists and animators try to follow.\n\nFor example, many game worlds of the past century looked the same in all situations and times of the day, but now, game designers usually work really hard to make sure that most (if not all) light sources impact the world in a dynamic and realistic way, generating proper shadows, reflections, flares, etc.\n\nBeyond the fact that it looks fantastic, PBR grants multiple benefits that can be game-changing:\n• This technique makes it easier to mimic real physical references or add an extra level of realism to new designs.\n• With PBR settings active, you can easily create multiple renders with different conditions (e.g. different times of day) without having to remodel the whole scene.\n• With PBR shaders active, you can easily change the position, intensity, and other qualities of lighting in the scene, and the resulting render will remain consistently accurate.\n• The 3D art community has embraced PBR as a philosophy, and this has led to the creation of amazing tools that make it easy to integrate into any scene.\n\nTo put it simply, PBR is more concerned with objects and materials, while ray tracing focuses solely on rays of light. It’s true that both techniques try to realistically model the effect of light on a scene, but ray tracing does it more accurately (following the bounce from and to polygons in a 3D object).\n\nAt the same time, it takes much more work to implement ray tracing, and it will have little effect on the materials in the scene, since light is the only focus. PBR uses established formulas to quickly calculate which color each pixel in an object’s render should be.\n\nWhere is PBR Used?\n\nSince this form of rendering can only be generated with computer software, it can be applied exclusively to 3D scenes and environments that are also digital. Below are the main places where you can find PBR content.\n• Mobile\n\n There is a lot of debate in the mobile development community about whether PBR should be used for mobile content/apps. After all, users have relatively small screens and rarely get to see the fine details that make 3D models and scenes look lifelike. At the same time, implementing physically-based rendering on mobile is not that difficult or costly, so many developers have started adding it in their releases.\n• PC\n\n PC is perhaps the best platform suited for PBR, because most modern PCs support high-fidelity graphics, which tend to include PBR. It makes sense: given that these graphics are created with computers, they rightfully support it.\n• Web\n\n The internet is a much shakier platform for deploying this type of content, because it is accessed by all manner of devices, from PCs to mobile phones, tablets, smartwatches, and even game consoles. Still renders with PBR pose no problems, but when it comes to software and animation, not all devices will be able to support the content, especially those that were made over a decade ago.\n• VR\n\n Virtual reality has always been more about immersion than attention to detail, and many of the 3D models used in VR experiences are quite simple, without any special environmental effects. With that said, some of the newer and higher-budget VR games (like Half-Life: Alyx) are investing in physically-based rendering VR improvements, while SDKs like PowerVR make the technique accessible to ordinary developers.\n\nWhich Features Should You Include in Physically-Based Rendering?\n\nPBR is not added based on what looks and seems accurate to physics, but rather on a concrete set of principles connected to light. Let’s go over the main ones:\n\nHow to Start Implementing Physically-Based Rendering in Your Content\n\nTo create top-notch PBR content, you can go through the same steps of 3D content design, making a few tweaks and adjustments.\n\nShaders are the main source of PBR creation and certainly the most popular tool used to create realism. Shaders are made up of code, and serve as maps for how pixels in 3D content are supposed to look. They are very helpful because they can instantly adjust the appearance of materials based on light changes – the principles of physical rendering are already hardwired into the code.\n\nMany 3D artists prefer to make physically accurate content in game engines like Unity and Unreal because they have certain PBR shaders built in and support the import of custom versions, too. For example, physically-based rendering in Unity is convenient with the Unity Editor, a tool that supports two different types of default shaders – standard and specular (used for glossy surfaces).\n\nGreat freedom to create custom shaders is also offered in Unreal Engine 4. This is quite easy with Blueprints, a visual scripting system. Thus, physically-based rendering in Unreal Engine 4 can be achieved both with an elaborate shader design system and a streamlined system for beginners.\n\nNaturally, you can also go with other platforms and frameworks. For instance, Three JS physically-based rendering scripts are just as advanced and helpful as those used by the biggest game engines.\n\nIf a shader serves as a guidebook for how objects should look with PBR, then textures are key excerpts in this book containing parameters and details. Texture maps can be created in most 3D design programs or downloaded in a completed state (example – FreePBR).\n\nA texture map is made for a single type of material and defines features such as albedo, smoothness, height, and occlusion. When shaders process these parameters and render content, they add the PBR-specific features we listed earlier (diffusion, reflection, etc.).\n\nAlthough shaders and textures cover most of the work needed to achieve the realism effect, you should also put some thought into the lighting of your 3D scenes. After all, without a lighting source with the right intensity and in the right position, users won’t be able to see your objects, models, and materials in all of their physically accurate glory.\n\nTruth be told, the topic of PBR is very complicated for users new to 3D design, and there is nothing that can be done to remedy it other than to read and try designing. However, if you are in urgent need of high-quality PBR content, you don’t need to learn if you don’t want to. One quick and painless solution is to hire 3D design experts with experience in this field.\n\n3D-Ace is a 2D/3D design studio that has been creating gorgeous art for decades, and whose expertise includes dozens of services. PBR is among the things that we do spectacularly well, and you can see it for yourself in our portfolio.\n\nIf you want to learn more about ordering custom-made PBR content for your business needs, just contact us."
    },
    {
        "link": "https://learn.unity.com/tutorial/creating-physically-based-materials-unity-2019-3",
        "document": "The Unity Editor uses Physically Based Rendering (PBR) in order to more accurately simulate realistic lighting scenarios. To fully take advantage of PBR, a material should be physically based. This is done through the Shader — the script that mathematically calculates how a material should appear under different lighting conditions. In this tutorial, you will learn about standard Shaders and texture properties."
    },
    {
        "link": "https://wiki.secondlife.com/wiki/PBR_Materials",
        "document": "What is it and Why is it used?\n\nThe term “Physically Based Rendering” or “PBR” is a technical term that may need some defining for most people since its use is unique to composing images in computer software. The term itself is an abbreviation for a collection of complex mathematical algorithms that attempt to accurately represent the ways that light reflects off and interacts with objects in the real world. In the real world, it is the behavior of light on a piece of metal that allows us, the observer, to recognize “that object is made of metal” without actually reaching out and touching it. The way a metal reflects light differs from that of a polished plastic or some other material, and these differences have been quantified by science. By mimicking real-world physics principles in the virtual world it allows for the creation of more immersive recognizable realistic spaces, but also it helps us relate to fantastical worlds a little better too. While we may not be familiar with what a newly imagined creation is, a metal's inherent metal-ness and aged wood's inherent wood-ness remain constant, making it easier to intuitively understand what we are interacting with in a virtual environment.\n\nBecause tying the mathematics to simulate materials in virtual spaces to how they behave in the real-world makes things more immediately recognizable, PBR has become the foundation for creating imagined worlds over the last decade. The metallic shine of exoskeletal armor in superhero movies is driven by a PBR workflow, as is the plastic sheen of toys or the glint of frozen ice in animated classics. Now we are moving to bring this standardized quality to your home in Second Life.\n\nBringing PBR to Second Life means updating the basic calculations of how light is represented and interacts with the world of Second Life. The goal is to integrate these changes while minimally changing how everything that presently exists in Second Life (designed prior to the introduction of PBR) looks. While the preservation of creative intent and the aesthetic appeal of items users have enjoyed for over two decades of Second Life is always a priority, Second Life is an ever-evolving platform, and to continue to do so, some changes are inevitable.\n\nLastly, while we are attempting to mimic real-world reflections and material properties, Second Life has to run on a wide variety of devices, so some shortcuts have to be made. Reflections are not mirror-perfect, as has often been a long-standing hope and request from Residents. While the addition of a reflection system does bring dreams of distortion free mirrors for our avatars in Second Life closer to reality, unfortunately, due to the calculation requirements of doing perfect reflections, mirrors are sadly still, for the moment, not practical. Please see the note in the section below for more info.\n\nWhat does this mean for Residents?\n\nThe largest change by far is the addition of an environmental reflection system to Second Life. For most existing items, this change shouldn’t have a drastic immediately observable impact. A few things in your inventory that you already own may appear more reflective with the new graphics configuration and those reflections should feel more realistic and immersive with your current environment. As a general rule: the “shinier” an object was before, the more environment reflections it will pick up and the more visual difference there will be.\n\nAnother notable change is the addition of tonemapping to the viewer. Tonemapping is a way of representing an image with a higher native dynamic range than the display can support. Tonemapping is a de-facto requirement of PBR pipelines. This means that colors in Second Life will generally appear more saturated with less detail being lost in shadows and highlights. Click here for some additional reading on the subject.\n\nThe tonemapping method used is called Academy Color Encoding System (aka \"ACES\"), and can be read about here and here.\n\nBlinn-Phong content making use of the Specular parameters may look different if the items were never viewed under local lights, as the PBR viewer allows for the environment (sun and sky) to contribute to specular reflections. As such, if the object receives blue specular reflections from the sky, these reflections are tinted, and may look odd. This effect is the same as prior viewers, as if the object was lit by a blue local light.\n\nThere have been changes to the Graphics > Advanced Settings... Preferences. The most notable of which is the removal of the “Atmospheric Shaders”, \"Local lights\" and “Advanced Lighting Model” options.\n\nFor those users on lower-end hardware who depended heavily upon those options to navigate Second Life with an acceptable framerate, we recommend the following settings:\n\nThe setting has been superceded by updates made to the World > Improve graphics speed... \"Quality & Speed\" slider options. Users on low-end hardware should ensure that their \"Quality & Speed\" slider is set at an appropriate level for their hardware.\n\nWhat does this mean for Creators?\n\nThe PBR project represents a large step towards integrating standard rendering techniques used in the games industry. As such, the nomenclature of some items has changed, notably, what was once called Materials is now referred to as Blinn-Phong. This does not represent any changes to the underlying rendering techniques (beyond those mentioned above); and as such Blinn-Phong is not the same as PBR Spec/Gloss workflows, seen in some game engines such as Unity.\n\nFor creators who work with external tools such as Blender, Adobe Substance Painter, Cinema4D, 3D-Coat, or have used the Unreal or Godot game engines, the use of PBR texture sets should already be familiar. In fact, some creators have been using tools that use PBR workflows to create content for Second Life and have then been forced to sacrifice visual quality to convert that information into Second Life’s existing Blinn-Phong materials system.\n\nSecond Life is adopting the “Metallic/Roughness” PBR model, and in its ongoing commitment to using Open Source standards whenever it is practical to do so, the glTF 2.0 file format has been chosen as the upload format for PBR Material assets. One of the primary goals of implementing PBR Materials is to have more continuity from content creation applications towards Second Life, and have more consistent content behavior once it’s inworld.\n\nUsing Imported Materials in Second Life\n\nFor people who build exclusively within Second Life, the addition of PBR also means that there will be a new Inventory Type called “Material”. These new Materials can be purchased on the Marketplace and are shareable like any other permitted object in Second Life.\n\nPBR materials come as a bundle of textures. These all travel as a single unit and are applied all at once. If you wish to change tint, transparency, or other similar parameters of the Material you’ll need to modify the Material from the \"Editing Material\" floater.\n\nApplying existing materials works similarly to existing textures. You have two means of doing so:\n\nSimply drag and drop onto the face of a prim or a mesh. For example, if you have a tiled floor material and you place it on a selected prim cube face that face will now look like a tile floor and reflect light like a tiled floor would with all the material qualities contained in the material.\n\nSwitch to PBR and select from Inventory\n\nAlternatively, select a face on the prim or mesh you want to apply your material to and choose the “Blinn-Phong” drop-down and change it to “PBR Metallic Roughness”, then select “Choose an item from your inventory” and apply.\n\nWith PBR materials, texture transforms work in a slightly different way to Blinn-Phong transforms. Blinn-Phong (and OpenGL) has the texture origin in the lower left corner of the texture, whereas glTF materials (and Vulkan) has the texture origin in the upper left corner.\n\nThis means that PBR materials will react differently (namely, they are inverted in the Y (glTF v) axis) to Blinn-Phong materials. For a simple solution to convert a Blinn-Phong texture transform to a PBR texture transform, please see this forum thread.\n\nFor more information, see here and here.\n\nMedia on a prim will continue to work largely as it has done prior to PBR's launch.\n\nWhen using MOAP on a face with a PBR texture, it will function as if the media texture has an override to the Base Color and Emissive maps. Note that the emissive map is overridden, however the emissive tint is not (so, to disable the emissive map you can set the emissive tint to black <0,0,0>).\n\nAs the PBR system is new, it is expected that existing users may be confused at first on how everything works. As such, Linden Lab and some third parties have provided some tutorials and informational videos:\n• Second Life University - How to create PBR Materials\n\nThe above tutorials require knowledge and access to Adobe's Substance Painter (subscription based software on Adobe website, or sold as perpetual license with one year of updates on Steam). Below are some open source tools and links found Googling 'Blender', 'glTF' (requiring Blender 3.3)\n\nIt is worth mentioning that during the adoption period of a new system, a substantial portion of Second Life Residents view the world through third party viewers and mobile viewer solutions that will not have updated to be able to see the new content. Anyone viewing a PBR Material on a viewer that cannot display it will see the “underlying” non-PBR texture. By default, this is a pine box or a completely blank texture. Those who wish their content to be viewable by as many people as possible, might consider creating a Diffuse texture ( with baked lighting, the kind that is easily generated with tools like Substance Painter's “Baked Lighting Filter” ) and applying it as a regular texture to the object they’re placing the PBR material on, prior to applying the PBR material. While this is an extra step in content creation, and complicates things somewhat, if you wish for your content to be appreciated by all, it’s worth considering adding this extra step to your workflow. People designing PBR materials for distribution and sale might also consider offering a “Diffuse Only fallback” texture to accompany the PBR material specifically for people who cannot see the PBR content.\n\nUnderstanding and Assisting the New Reflections System\n\nThose creators that work at house-scale, or produce items that can be walked through, can gain additional control over the lighting in their creations by taking the time to fully understand the new toolkit that influences environmental reflections. There is a new type of volume that can be created and appended to object linksets specifically for scenes with these kinds of spaces. These volumes define a custom area where reflections are calculated, overriding the default solution. So-called “Reflection Probes” should be placed in a manner such that the fewest number of probes fills the largest amount of space possible with minimal overlap. One probe per room is a good reference point for a general living space like a house. If multiple probes exist in a given area they can cause visual artifacts and negatively impact performance (also known as viewer lag). Do not affix reflection probes to small creations such as furniture or decor. Small items such as tables, chairs, musical instruments, candle sticks etc should use the reflection sample volume in the space in which they are placed. They should not have one appended. It is strongly recommended that any object that contains a reflection sample probe be left as “modify”, so the positioning of the probe can be adjusted or even removed by the owner of the item should they wish.\n\nWhen is it Recommended to Create a Reflection Probe?\n\nManually placed probes are good for cleaning up undesirable noise and lighting from automatically placed probes that may be visually disruptive or confusing. If you look in the image of the room in a house, on the left hand side of it, you can see a blue tint on the floor, which is the reflected blue light from the sky. The probe filling half the room blocks this, and when it is extended to completely fill the room and just slightly beyond the thickness of the walls, (lower copy of the image) the problem is solved. The probe does not need to be a part of the linkset for the rest of the room in order to function; however, once you have them placed it’s possible to add them into linksets like any other prim.\n\nStart off in the Build Menu (Ctrl+3) by rezzing a basic prim. Under the tab, at the bottom check the box labelled .\n• : Enabling this creates a Reflection Sampling Volume within the bounding box of the prim.\n• : Choose whether you want this sample to project reflections within a box shaped volume or a spherical one.\n• : Allow skinned objects (e.g. Avatars, animesh, etc.) to be captured in the reflection.\n• : Affects how much objects within the volume are lit as if they have bounced light hitting them from faked indirect illumination. See \"Fine-Tuning Reflection Sample Volumes\" for more info.\n• : Sometimes, there will be a large obstruction in the center of the volume you wish to use as a reflection probe. For example, suppose that there is a large architectural supporting structure in the middle of the room, such as a column. If the probe gets placed internally inside this, it will reflect the internal surface of the column instead of the room in which it is placed. Increasing Near Clip will make the sampling volume exclude objects n meters from the center so they don’t get included in the reflection. See \"Fine-Tuning Reflection Sample Volumes\" for more info.\n\nSelect an appropriate probe shape for your scene. For example, if you want to create a reflection probe that covers a room, select a shape, or for other objects a shape is recommended.\n\nMove the reflection probe and resize it as necessary to fit your desired purpose. For the room example, resize the box so the probe just touches the walls, ceiling and floor of the room.\n\nNaming your probes is also recommended, so other people ( or yourself later on ) will remember why there’s a large transparent prim in the middle of your build.\n\nIf you wish to come back and edit the reflection probe later on, enable both and under .\n\nThat said, it may be helpful to visualize as the hollowing out of your reflection sample volume, but the hollow and near-clip are not in any way linked and doing so is purely a convenience. Reflection probe sample volumes only respond to changes in scale and position; however, different probe volumes behave differently when resized.\n• Probes: These are affected by position and non-uniform scale.\n\nExample: If I create a box reflection probe and scale it to 10m,30m,10m size, the full volume will be affected by the probe.\n• Probes: These are affected by position and uniform scale only, otherwise the smallest dimension is used.\n\nExample: If I create a spherical probe and scale it to 10m,30m, and 10m size , the probe will sample a 10 meter diameter spherical volume at the center of the probe. However, a 30m,30m,30m sphere will create a sampling volume of a 30 meter diameter sphere.\n\ncontrols how ambient light (Found in your environment preset) affects, or does not affect, the contents of the reflection probe, and the intensity of indirect lighting (aka Irradiance). This value is influenced by the value found in the user's environment preset, wherein if the Ambiance value given in the environment preset is higher than the value defined by the probe itself, it inherits the ambiance value of the environment preset, or alternately if the ambiance value defined by the probe is higher, the probe's value is used.\n\nThere are 3 operation modes that are set with the value, in tandem with above:\n• 0 will allow the Environment Ambient color to be applied at full intensity.\n• Greater than 0 and less than 1 will blend the Environment Ambient color and probe irradiance (indirect lighting) in a ratio corresponding to the defined value (e.g. 0.5 is a 50/50 split).\n• 1 or above will block the Environment Ambient color from being applied within the probe volume and irradiance is applied at full intensity (or with a multiplier for values above 1).\n• 4 or above works the same as above, however only indirect lighting received from the sky will be multiplied to a value above 4. Local lights are clamped at 4.0.\n\nAt times, you may wish to place a reflection probe in an area where the reflection probe types (Sphere or Box) do not conform to the shape of the area. For example, a Box probe in a loft room has a triangular shape, which results in \"probe bleed\" wherein the influence volume affects an area larger than what is needed, and thus bleeds out onto the exterior roof, which is undesirable.\n\nIn these cases, you may need to place multiple probes in order to blend together the sample volumes to achieve the desired result. In the loft room example, it's best to start with 3 box probes, which cover the floor, and each side of the roof. Then, use an additional box probe as \"fill\" to cover the gap inbetween the 3 probes. The main fill probe may need to be rotated at an angle to get the most coverage possible. If it is not possible to fill the gap between the 3 probes with this fill probe, you may wish to add more box probes to act as \"secondary fill\".\n\n[TODO: Put photo examples of the above here!]\n\nAlternately, Sphere probes may be used to achieve the same result. Sphere probes have considerably softer blending than box probes, so this may give you the best results in severe bleed conditions.\n\nYou may be tempted to use reflection probes in the following ways, however these use-cases are either intentionally disabled or result in undefined behavior, and may work now but may not in future. You have been warned.\n\nUnder any circumstances, you should NOT:\n• Wear a reflection probe. This intentionally does not work. (Reflection probes are a property of the scene, not an individual object / avatar)\n• Attach a reflection probe to a physics-enabled object, e.g. vehicles. This intentionally does not work. (As above, the reflection probes are part of the scene; not part of an individual object. Reflection probes by design do not update in real-time, thus the object would always have incorrect reflections anyway.)\n\nPBR-defined values are measured by optical sensors and other capture tools and recorded in databases. These parameters are then put into commercial software to be used by content creators. Using software designed to create PBR content is always recommended; however, understanding how PBR materials are assembled can assist with editing and compiling them. Having a direct understanding of what each individual color channel contributes is essential for editing PBR textures without relying upon some of the more advanced toolkits available.\n\nAll PBR Values are listed from 0 to 1.0, though in an actual image, the values range from 0 - 255. PBR Materials are composed of a set of four specifically designed textures; they are as follows:\n\n[RGB]: This is the unlit color of the surface. This differs from the “Diffuse” texture that Second Life uses. Diffuse textures often include faked reflection and specular information as well as added Ambient Occlusion shadows. Base Color textures do not get any of this added information. For metals ( as defined by the metalness value ), Base Color also determines specular reflection color, whereas, in non-PBR systems, this is defined by the Specular texture and tint. In certain PBR texturing applications, Base Color is sometimes also referred to as “Albedo”. \n\n[ A ]: Alpha Channel, dictates the transparency of the entire material overall.\n\nThe Base color texture should be devoid of lighting information.\n\nThis texture is composed of 3 unrelated grayscale images stored in 3 different color channels of an RGB texture. \n\n[R]: (Ambient) Occlusion is lighting data, removing the need to bake down shadows on to the Base Color map. Note that white (<1,1,1>) means no occlusion is applied, and black (<0,0,0>) applies full occlusion. \n\n[G]: Roughness data ranges from 0 to 1.0, but the actual range of physical surfaces ranges from approximately 0.05 to 0.985. No surface is perfectly smooth or completely rough. The rougher a surface is the less mirror-like it behaves. \n\n[B]: Metalness values are mostly black or white. Either the material is a conductive metal like copper, or it’s a non-metal like fabric. 0.0 is Non-Metallic, 1.0 is Metallic. There are almost no materials with gray metalness values. \n\nThe alpha channel is ignored, as per the glTF 2.0 specification.\n\nThis determines the amount and the color of unlit (ignores ambient light conditions) areas of your material. When giving an object a white emissive map, the object will act as if the Blinn-Phong \"Fullbright\" option was checked. This map is useful for items which are expected to emit light, e.g. a table lamp, where the lamp shade would appear to glow when the lamp is turned on. If you wish to toggle the lamp on and off, it's recommended to change the Emissive Tint value to black (functionally disabling the emissive map, thus turning the lamp off), and then changing the tint back to your desired color to turn the lamp back on. Leaving the Emissive slot empty is recommended when it’s unused.\n\nNote that glow (The postprocessing effect), is controlled by a separate parameter in the build floater and is intentionally not part of the PBR material window. Glow is modulated by the emissive map, so black areas of an object's emissive map will not glow, similar to how Blinn-Phong emissive maps and glow work.\n\nAs per the glTF 2.0 specification, the alpha channel is ignored.\n\nThe normal maps generated by your baking application / normal map generation toolkit should be compatible with Second Life in most cases. The most common pitfall is using normal maps generated with “inverted” Green channels, such as those that are used for Direct3D and Unreal Engine. Normal texture data redirects light in a different direction based on the vectors indicated by the color, so if the green channel is backward, it’ll seem to be “pointing the wrong way”. To phrase the problem a different way, all the things that should be bumps look like dents, and vice versa. If this occurs, double check that your settings aren’t for Direct3D and try re-generating it. Also, taking it into image editing software and inverting the green channel only sometimes is a sufficient fix.\n\nNormal map tangent spaces are an extremely technical subject that most users need not be concerned with as most modern applications default to the correct setting. However, if your normal maps look drastically different inside Second Life, compared to your source application, and you’ve already confirmed it’s not an inverted green channel, then checking which tangent space settings are being used is the next step. PBR Normals use Mikkelsen Tangent Space. (Often abbreviated MikkT) If you are unsure what this means, use similar workflows and settings for Second Life PBR as those that are generally recommended for the Godot 4 game engine.\n\nAs per the glTF 2.0 specification, the blue channel of a normal map is only allowed to contain values above 0.5 to a max of 1 (255).\n\nAs per the glTF 2.0 specification, the alpha channel is ignored.\n\nAs PBR is Physically Based, you may wish to know how to recreate a real-life material in PBR form. What color should you use? What metalness value should it have?\n\nFortunately, reference libraries exist which can tell you how to recreate a given material in a PBR workflow.\n• Physically Based\n• Select the Godot Engine, with the Color Space as sRGB Linear and Color Representation as 0-1 (Depending on what you are doing, you may need to convert the value into gamma-corrected sRGB space, which can be done here. Enter the value on the 3rd row.)\n• Grzegorz Baran Library\n• Available on YouTube (video format, older), or PDF (most up-to-date).\n• Dontnod Entertainment Library\n• A very small library, but useful nonetheless.\n• Polycount Wiki\n• Not so much a library in itself, but links out to other libraries. See the Color Charts section.\n\nThere are two different methods of creating PBR material assets:\n\nThe most convenient method to create an entirely new material is to directly upload a glTF file from your computer.\n\nThis is accessible via\n\nSelect a .GLB or a .GLTF file from your computer and Open it: This will give the following window:\n\nAll of the editable material properties that most creators are familiar with are integrated into the Material's vertically sizable window on upload.\n\nClicking “Save” pays the upload fees and creates the Material assets using the local filename. Using \"Save As...\" provides a window to name all the assets to unique inworld names. There is a 63 char limit for names of inventory assets but the naming floater on this Materials upload has no length restrictions.\n\nTextures will be uploaded to the inventory's default Textures folder (NoMod, NoCopy, Transfer) with an appended \"type\" added to the given name. (ie. asset name (Base Color), asset name (Normal), asset name (Metallic Roughness). In naming the asset keep in mind that the bracketed appendix is included in the name's 63 character limit. The PBR object is uploaded (Mod, NoCopy, Transfer) to the inventory's Material folder with the appended text (Material). Presently, naming the glTF asset from Save As... will NOT append (Material) to the inventory's name. Saving from the locally named file will.\n\nIf you have the necessary textures to compose a PBR material but do not have a glTF file you will need to build a Material from inventory. Make sure your textures are in the correct format, with the correct data in the proper channels. This requires some understanding of how PBR materials work. Once you have that, upload your textures as you normally would.\n• Find the \"Materials\" folder in your inventory, right click, and select \"New Material\" from the context menu.\n• Name your new material something appropriate (E.g. \"Red Bricks\" for a red brick wall, etc.)\n• Right click on your new material, and select \"Open\" from the context menu.\n• Upload your PBR textures using the standard texture workflow (Usually, under ).\n• In the material window, select the appropriate maps that you just uploaded in their respective slots.\n• Once done, verify the material parameters are correct (E.g. Base color tint, M/R factor, Emissive tint, etc.)\n• EITHER: Drag and drop the material onto a rezzed prim, OR edit the prim, click the \"Blinn-Phong\" drop-down, select \"PBR Metallic-Roughness\", and click \"Choose from Inventory\", and select your new material.\n\nAfter you’ve uploaded your first Material for your project, if you’re a store owner who would like to release more than one color palette for your creation, as many clothing and furniture designers find it useful to do, it’s important to not upload a new glTF file for every single variant of your Material. Most variant materials, only the base color will change. The Occlusion, Roughness, Metalness, Emissive and Normal map texture slots will remain the same. Since these maps have already been uploaded once, if we upload a second glTF File, they will be duplicated. Having different copies of the same texture, with differing UUID’s means that they will clog up download bandwidth and video card memory (not to mention your inventory as well). This is very bad for Second Life. So, for this reason, it’s recommended that when you create texture variants, you upload the additional copies of the base-color texture separately. Then open your newly uploaded material in edit mode, choose the Base Color texture, and change it out for one of your newly uploaded Base Color textures, and click “Save As”. This will create a second copy of your material that uses all the correct texture maps without needlessly duplicating them and causing additional lag.\n\n“Double Sided” is a new property unique to Materials. When “Double Sided” is checked, the surface upon which this material is placed will be drawn twice; once for the outward-facing portion of the surface, and a second time for the inward-facing side of the surface that is normally invisible without a double-sided material. This option should only be used for very specific meshes that were designed to be used with double-sided materials, since placing materials with this parameter checked on normal objects will simply cause the viewer to draw it twice ( and thereby create additional viewer lag ) for no observable change. It is very strongly recommended that this option be unchecked for any material that is to be distributed for general use. Even more so if the “Modify” permission is revoked. If you wish to distribute a version of a material that has “Double Sided” checked, please include a second copy of the material that has “Double Sided” unchecked as well, with an accompanying explanation to the next user as to why this was done.\n\nWhen designing mesh content for use with the double-sided material parameter, it is also very strongly recommended that you separate the triangles you intend to use the double-sided material upon into a separate mesh “face”, so as to not unintentionally render the portions of the mesh that already have triangles designed to represent the internal portion of the object a second time.\n\nDocumentation for the LSL interface with glTF materials can be found on the following pages:\n\nThe glTF file format is widely adopted and many applications have export functionality for this type of file. Below is a non-comprehensive list of some of the more popular applications that export glTF files.\n\n\n\n Create your materials within Blender using Principled BSDF Shader Nodes and the glTF Settings node. Export them according to the official Blender Documentation.\n\nFor materials creation, 2 example .blend files are provided, one using separate Occlusion, Roughness and Metallic textures (\"Long Form\"), and one that handles a pre-packed ORM map (\"Short Form\")\n\nThe below files are intended for use with Blender versions 3.3 and above.\n• To enable dithering, open the window, under the tab, enable an override for the Base Color texture and swap the output format from to\n\nUnder preferences, verify the following Project settings:\n\nIf you use a custom node to output a pre-compiled ORM map, this should also be set to Raw. \n\n\n\nPlease follow the documentation provided in this blog post. \n\n\n\nglTF export has been implemented since 3DCoat version 2020 and is found in the \n\n\n\nQuixel Mixer has no glTF output, however, it does generate the functional textures, though they do need to be edited and combined in photo editing software. Export them by going to\n\nAlbedo , Roughness, Normal, AO , Metalness and ( if need be ) Emissive.\n\nClick “Export to Disk” and open the folder that the files were placed into.\n\nAlbedo is the Base Color texture in this case.\n\nAO , Roughness , Metalness get combined into the ORM map as per this explanation.\n\nNormal Map : Quixel generates Direct3D Normal Maps. The green channel needs to be inverted as per here.\n\nUpload to Second Life using Method 2. \n\n\n\nAutodesk just added the ability to create glTF files using their new glTF Material and glTF Export functionalities as outlined in Autodesk's official documentation. All prior versions of Autodesk software do not have this functionality. \n\n\n\nWhile Material Maker does not currently export directly to glTF (This may be added in future), materials created with this program are compatible with Second Life.\n\nExport your materials using the Godot 4 ORM export preset.\n\nUpload to Second Life using Method 2. \n\n\n\nWhile Photoshop is not officially a PBR authoring tool, the changes to Alpha Blending (changing into Linear space from sRGB) may require you to change your PS settings to get consistent results. Please see this article for more information. (See the section titled \"A Partial Solution\"). Alternatively, this Reddit post also gives a few options on how to achieve blending in the correct manner. \n\n\n\nWhile GIMP is not officially a PBR authoring tool, the changes to Alpha Blending (changing into Linear space from sRGB) in Second Life will make textures created in GIMP display their alphas correctly (GIMP defaults to linear alpha calculation).\n\nIf you upload a piece of PBR content which does not match your editor, the advice from Linden Lab is to STOP: File a Feedback ticket. Do not attempt to \"fix\" the content.\n\nThat said, there are some troubleshooting steps which you can do yourself:\n• Check that you are using the PBR Linden Midday preset (called \"Midday\") not \"Midday (Legacy)\". Other environments may not match the reference HDRi, causing some visual differences. (This is intended behaviour, and content should be able to be viewed under any light correctly, but for troubleshooting purposes this may be required).\n• At present, there is a bug with the PBR Linden Midday preset which results in an excessive blue sheen, due to the use of an over-saturated (unrealistic) sky color, among some other issues. This has been fixed in the upcoming glTF Maintenance viewer.\n• Are you using a reflection probe? If not, does the problem reproduce if a manual reflection probe is placed over the object?\n• This is because a common source of the \"blue sheen\" in interior scenes is the use of an auto-probe. Auto-probes sample their surroundings, and combined with the approximate (and often incorrect) placement this will mean the sky is sampled on all sides, thus meaning the reflected light from the surroundings (which counters the blue light from the sky) is absent, leading to a larger-than-expected level of sky contribution on the object. Auto-probe placement can be worse in skyboxes or sky platforms\n• Triple-check the settings used for your editor match the ones given here, including the reference HDRi. Any deviation from these settings may cause visual differences between your editor and in-world.\n• Check your content against a glTF reference viewer; such as:\n\nIf the above steps fail, then please file some Feedback.\n• An LM to a location where the problem can be examined in-world.\n• A copy of the glTF content, attached to the ticket.\n• Screenshots of the representation in-world, in a reference viewer, and in-editor.\n• You should include screenshots of the object inside a manually placed reflection probe, and outside."
    },
    {
        "link": "https://discussions.unity.com/t/multiple-shader-best-practices/877718",
        "document": ""
    },
    {
        "link": "https://docs.unity3d.com/560/Documentation/Manual/HOWTO-Water.html",
        "document": "Unity includes several water Prefabs (including the necessary Shaders, scripts and art Assets) within the Standard Assets packages. Separate daylight and nighttime water Prefabs are provided.\n\nPlace one of the existing water Prefabs into your Scene. Make sure you have the Standard Assets installed:\n• Fancier water - Daylight Water and Nighttime Water in Pro Standard Assets > Water (this needs some Assets from Standard Assets > Water as well). Water mode (Simple, Reflective, Refractive) can be set in the Inspector.\n\nThe Prefab uses an oval-shaped Mesh for the water. If you need to use a different Mesh, change it in the Mesh Filter of the water GameObject:\n\nSimple water requires attaching a script to a plane-like mesh and using the water shader:\n• Have mesh for the water. This should be a flat mesh, oriented horizontally. UV coordinates are not required. The water GameObject should use the Water Layer, which you can set in the Inspector.\n• Attach the WaterSimple script (from Standard Assets/Water/Sources) to the GameObject.\n• Use the FX/Water (simple) Shader in the Material, or tweak one of the provided water Materials (Daylight Simple Water or Nighttime Simple Water).\n\nReflective/refractive water requires similar steps to set up from scratch:\n• Create a Mesh for the water. This should be a flat Mesh, oriented horizontally. UV coordinates are not required. The water GameObject should use the water Layer, which you can set in the Inspector.\n• Attach the Water script (from Pro Standard Assets/Water/Sources) to the GameObject (Water rendering mode can be set in the Inspector: Simple, Reflective or Refractive.)\n• Use the FX/Water Shader in the Material, or tweak one of the provided water Materials (Daylight Water or Nighttime Water).\n\nThese properties are used in the Reflective and Refractive water Shaders. Most of them are used in the Simple water Shader as well.\n\nThe rest of the properties are not used by the Reflective and Refractive Shaders, but need to be set up in case the user’s video card does not support it and must fallback to the simpler shader:"
    },
    {
        "link": "https://learn.unity.com/tutorial/creating-physically-based-materials",
        "document": "The Unity Editor uses Physically Based Rendering (PBR) in order to more accurately simulate realistic lighting scenarios. To fully take advantage of PBR, a material should be physically based. This is done through the Shader — the script that mathematically calculates how a material should appear under different lighting conditions. In this tutorial, you will learn about standard Shaders and texture properties."
    },
    {
        "link": "https://yellowbrick.co/blog/animation/pbr-texturing-techniques-tips-and-tricks",
        "document": "PBR texturing, or physically based rendering texturing, is a technique used in the 3D industry to create more realistic and accurate textures for models. This method simulates how light interacts with surfaces in the real world, resulting in more lifelike and visually appealing digital assets.\n\nWhether you are a beginner looking to learn the basics of PBR texturing or a seasoned professional aiming to enhance your skills, this article will provide you with valuable insights and tips to master PBR texturing techniques effectively.\n\nBefore trying advanced PBR texturing techniques, it is crucial to have a solid understanding of the basics. PBR texturing is based on the principles of physics, focusing on how light interacts with different materials and surfaces. By accurately replicating these interactions, artists can achieve a high level of realism in their 3D models.\n\nOne of the key components of PBR texturing is the use of textures such as albedo, roughness, metallic, and normal maps. These textures define various properties of the material, such as its color, surface roughness, metallic sheen, and surface details. By manipulating these textures effectively, artists can create highly detailed and realistic materials in their 3D renders.\n\nChoosing the Right Software and Tools\n\nWhen it comes to PBR texturing, choosing the right software and tools can significantly impact the quality of your work. Several industry-standard software programs like Substance Painter, Quixel Mixer, and Mari are widely used for PBR texturing. These tools offer a range of features and functionalities that streamline the texturing process and allow artists to achieve stunning results.\n\nMoreover, utilizing plugins and scripts can enhance your workflow and productivity when working on PBR textures. Plugins like Substance Source and Megascans provide access to a vast library of high-quality materials and textures, saving artists time and effort in creating custom textures from scratch. By incorporating these tools into your workflow, you can expedite the texturing process and focus on unleashing your creativity.\n\nHow To Be Proficient in PBR Texturing Techniques\n\nTo master PBR texturing techniques, artists must pay attention to detail and strive for realism in their textures. Here are some tips and tricks to help you elevate your PBR texturing skills:\n• Study Real-World Materials: Observing and analyzing real-world materials is essential for creating convincing textures in your 3D models. Pay attention to how light interacts with different surfaces and textures, and try to replicate these effects in your digital creations.\n• Experiment with Different Maps: Utilize a variety of texture maps, including albedo, roughness, metallic, and normal maps, to add depth and detail to your materials. Experimenting with different map combinations can help you achieve a more realistic and visually appealing result.\n• Use Reference Images: Referencing images of real-world materials can provide valuable insights into texture details and color variations. By studying reference images, you can enhance the authenticity of your PBR textures and create more accurate representations of materials.\n• Practice Shader Programming: Understanding shader programming can give you more control over how your materials interact with light in a 3D environment. By experimenting with shader code, you can customize the look and feel of your textures and achieve unique visual effects.\n• Stay Updated on Industry Trends: The 3D industry is constantly evolving, with new techniques and technologies emerging regularly. Stay informed about the latest trends in PBR texturing and incorporate innovative approaches into your workflow to stay ahead of the curve.\n\nBy incorporating these tips and techniques into your PBR texturing workflow, you can enhance the quality and realism of your 3D models and create visually stunning assets that captivate audiences.\n\nAs a professional in the 3D industry proficient in PBR texturing techniques, you can explore a wide range of career opportunities across various sectors. Here are some of the top job roles that require expertise in PBR texturing:\n• Texture Artist: Texture artists are responsible for creating high-quality textures for 3D models, ensuring that they are visually appealing and realistic. They work closely with modelers and lighting artists to achieve the desired look and feel of digital assets.\n• Environment Artist: Environment artists specialize in creating immersive and detailed environments for video games, films, and animations. They use PBR texturing techniques to texture landscapes, buildings, and props, bringing virtual worlds to life.\n• Character Artist: Character artists focus on creating realistic and expressive characters for games, films, and animations. They use PBR texturing to add lifelike details and textures to characters, enhancing their visual appeal and emotional impact.\n• Technical Artist: Technical artists bridge the gap between art and technology in the 3D industry, supporting artists and developers in implementing PBR texturing techniques effectively. They optimize workflows, develop tools, and troubleshoot technical issues to enhance the overall quality of digital assets.\n• 3D Generalist: 3D generalists are versatile artists who possess skills in various aspects of 3D production, including modeling, texturing, lighting, and rendering. Proficiency in PBR texturing techniques allows 3D generalists to create cohesive and visually stunning assets for a wide range of projects.\n\nBy pursuing a career in PBR texturing, you can leverage your skills and expertise to secure rewarding positions in the 3D industry and contribute to the creation of immersive and captivating digital experiences.\n\nIn the fast-paced and competitive world of 3D production, mastering PBR texturing techniques is essential for artists looking to create high-quality and visually appealing digital assets. By understanding the principles of PBR texturing, choosing the right software and tools, and honing your skills through practice and experimentation, you can elevate your work to new heights and unlock exciting career opportunities in the industry.\n• PBR texturing is a crucial technique in the 3D industry for creating realistic textures.\n• Understanding the basics and using the right software/tools are essential for mastering PBR texturing.\n• Tips like studying real-world materials, experimenting with different maps, and staying updated on industry trends can enhance your skills.\n• Career opportunities in PBR texturing include roles like texture artist, environment artist, character artist, technical artist, and 3D generalist.\n\nTo further enhance your skills and advance your career in PBR texturing, consider enrolling in the Yellowbrick online course “NYU Animation Industry Essentials.” This comprehensive program can provide you with valuable insights and practical knowledge to excel in the dynamic world of 3D production."
    },
    {
        "link": "https://allanbishop.com/2022/08/17/interactive-water-simulation-in-unity",
        "document": "Around 2018 I came across a blog post by the game studio Campo Santo for an upcoming game called In The Valley of The gods. The post showcased some amazing water tech that they were developing. It looked awesome. This inspired me to try and create my own water tech and so I began my journey researching and experimenting.\n\nI used the techniques mentioned in their blog post as a starting point and continued researching to find tips and tricks other game companies had come up with.\n\nBy the time I had finished building my water tech I had incorporated the following:\n• Two way interaction of rigidbodies with water\n• Signed Distance Function 3D textures for rendering shadows on the water and for precise water collision\n\nThe starting point was to figure out how to calculate the actual water simulation. Camp Santo’s post mentioned they implemented a GPU-based simulation using a “shallow-water” approximation. Further investigation returned various papers and thesis based on Shallow Water Equations each with varying implementations. The one I chose to implement was based on the pipe method.\n\nWith a shallow water equation only the surface of the water is modelled. This is fast and efficient, especially if implemented on the GPU, but comes with the limitation of not being able to simulate breaking waves.\n\nThe algorithm in layman terms is as follows. The surface of the water is divided into a grid, with each grid cell representing a column of water (height). Each of these columns are connected to its four neighbours, north, west, south and east. As water is added to a column, the hydrostatic water pressure will change and result in either excess water being flooding into the neighbouring pipes or neighbouring pipes flooding into the column.\n\nMore information on the algorithm can be found here https://web.archive.org/web/20201101141245/https://tutcris.tut.fi/portal/files/4312220/kellomaki_1354.pdf\n\nIt so happened that previously I had stumbled upon a blog by a developer who goes by the name Scrawk who had implemented a terrain erosion system on the GPU https://github.com/Scrawk/Interactive-Erosion. From his work I learned how to use Blit() to process data on the GPU and it was a great starting point. I should mention I could have also used compute shaders for this task but I was trying to keep compatibility with older mobile GPU.\n\nHaving developed the bare bones water simulation the next task was to tackle the interaction. Campo Santo mentioned they achieved interaction by using signed distance functions (SDF).\n\nAn SDF stands for a signed distance function, which is a function that returns a signed value specifying a distance of how far a point in space is from the function.\n\nWith this little bit of information we can make assumptions about an environment. Traditionally to display 3D graphics we would use a shader to render a mesh that an artist has made or that we have procedurally coded.\n\nBut in this case we do things a little differently. Instead we raymarch for each fragment in a pixel shader along a viewing ray. This shader is attached to a simple quad or camera. As we march from the starting position we calculate how far to the nearest point on the SDF is by calling the SDF function for the shape. This bit of information allows us to make an assumption on how far we can subsequently march ensuring we do not tunnel through. Then depending on the results after N amount of steps or if a collision is detected we can colour the pixel based on our results.\n\nFor example, imagine a sphere at a given position and radius, we can determine how near to the surface of the sphere we are from any point in space. Therefore, we can step along the viewing ray by the distance returned from the SDF function. This distance is important to ensure we don’t tunnel through the SDF. In a sense raymarching like this is an optimization. When the distance returned is below a threshold, we consider it a collision with the surface. We can then measure how far along the ray we stepped and render the pixel for that ray by using the length to shade the pixel. If no collision was found we step for a max of N steps.\n\nWe can render various primitive shapes like this and even combine them to build even more complex objects.\n\nInigo Quilez who co-developed the site ShaderToy is “the guy” for every and anything related to SDF. On his site he lists all the various functions for calculating the SDF of different shapes. You can even join SDF functions to create composite functions. There are plenty of examples on ShaderToy showcasing what is possible.\n\nThat being said, while the examples are cool they don’t really work when we have a specific shape we want to render. Trying to build a complex character in a game from primitives would be very time consuming. Also, the final function would be very expensive as we would need to calculate the min sum of probably hundreds of SDF that comprise a model.\n\nThe answer to this problem is to prebake the data. To do this I created a 3D grid in the Unity Editor with each grid position corresponding to a position in a 3D texture. Then I would iterate through each grid point and calculate the distance to the closest point to the mesh by iterating over each triangle in the mesh, then call the SDF function for that triangle and finally store this data in the 3D texture.\n\nSome of the more complicated items such as the galleon ship required a 100×100 grid and the ship itself was made up of 12,000 triangles. The problem was this was really slow to bake (hours). So the next step was to move all this code off the CPU and onto the GPU. After creating a compute shader in Unity I was able to bake the SDF to texture in a matter of minutes!\n\nAs this SDF data is stored in a 3D texture it’s then easy to feed into the water simulation shaders.\n\nWithin the shaders I use the 3D textures for two purposes. The first purpose is for collisions and the other for rendering shadows on the water surface.\n\nCalculating the collision required working out how much water should be displaced from its column if an object was occupying the area. To do this I used SDF data to march a ray from the water surface directly down and another from the bottom directly up. Then any difference between the water height and these distances are used to update the displacement of water in the simulation. One of the nice things is the collision box using the SDF is a lot more detailed than using primitives.\n\nYou may remember at the beginning of this article I said that I made a two way water collision system. Therefore not only do the objects move the water, but the water can move the objects.\n\nThis is where I thought outside of the box. You see the problem is that GPUs are designed for having data pushed into them but they are not so good at retrieving data out of. When trying to transfer data from the GPU to the CPU you stall the graphics pipeline which causes a significant performance spike.\n\nMy solution to this problem was to avoid the GPU entirely. At the time when I was researching all this tech, Unity had come out with their burst and job systems that gave a big performance increase to code running on the CPU through threading and optimised memory layout.\n\nWhat I did was to port over the water simulation to Unity jobs but with a lower grid resolution to help minimise the performance gap. The rigidbodies on the CPU then use the velocity data from this CPU simulation to add force so that they are pushed by the water. The GPU and CPU water simulations operate independently but with the same input data. Because both simulations are dampened over time they remain fairly synced and the effect works well.\n\nThe final step for my water tech was to try to render the water data as realistically as possible. The secret to making water look good is the lighting and how it interacts with the water. First I generated the normals by sampling the heightmap. Fresnel was then added in combination with a cube map. This results in the water looking reflective from some angles and transparent from others. Refraction is added by using a GrabTexture together with water surface normals.\n\nI then implemented a fake subsurface scattering using the technique described here https://www.fxguide.com/fxfeatured/assassins-creed-iii-the-tech-behind-or-beneath-the-action/\n\nShadows rendering on top of the water surface was another big feature. The Campo Santo blog post mentioned it was a challenge as the built in Unity pipeline did not support shadows on transparent surfaces. These days if you use the URP render pipeline in Unity there is no longer this limitation. Also, I recently discovered that the ability to sample the inbuilt shadow map is now accessible when using the built in pipeline.\n\nBut back then every forum post suggested making your own shadows.\n\nMy solution was to make use of the SDF textures. I raymarched from the water surface, along the light direction and tested for collisions with any of the SDF items.. If a collision was detected it meant the water surface was being occluded. One of the benefits with using SDF for a shadow system was that it’s easy to incorporate a penumbral effect so that shadows have softer edges the further they fade out!\n\nThe final step for rendering water was to add caustics. The easy way of rendering caustics is to have an animated texture that you project into the scene. But I wanted real time caustics that are calculated from the water surface. By far the best implementation I found was in this WebGL demo by Evan Wallace https://medium.com/@evanwallace/rendering-realtime-caustics-in-webgl-2a99a29a0b2c\n\nThe reason this one is the best is because the algorithm modifies the verts based on the light rays and then uses ddx and ddy in the fragment shader to calculate a rough area of the triangle to either brighten or darken. This creates really interesting formations and patterns that many of the other caustic algorithms lack.\n\nI then set up a camera dedicated to rendering the caustics to a texture where I then applied some blur filtering and chromatic aberration.\n\nThen for all the items and tiles I created a surface shader that sampled this caustic texture using their pixel world position, then checked against the height field to ensure the pixel is under the water surface, and finally compared the dot product of their normal and an up vector to map where the caustic was to be applied.\n\nThe buoyancy physics comes directly from a post by Habrador\n\nLastly I should point out that even with all this tech it requires a great artist to get the most out of it. Ryan Bargiel did an outstanding job creating the environment, assets, tweaking all the shaders and bugging me for feature requests. Even 4 years later I still think this water tech demo stands up well."
    }
]