[
    {
        "link": "https://docs.unity3d.com/Manual/VROverview.html",
        "document": "Get started with virtual reality and mixed realityMixed Reality (MR) combines its own virtual environment with the user’s real-world environment and allows them to interact with each other.\n\nSee in Glossary development in Unity.\n\nVirtual Reality (VR) and Mixed Reality (MR) both refer to extended reality experiences where specialized devices provide a way for the user to interact with a virtual environment.\n\nIn VR experiences, the environment is closed. This means that the user can’t see their surrounding environment, and can only see virtual content displayed on the screen. The user can only interact with virtual content, rather than their physical environment. VR experiences are often achieved with a headset (a Head-Mounted Display [HMD]), where a screen within the headset displays the virtual environment. VR experiences are fully immersive, and so are a good choice for creating immersive, story-driven experiences and gameplay.\n\nMR combines elements of the real and virtual environments, enabling users to see and interact with both simultaneously. MR relies on devices that are able to display a real-time view of the user’s surroundings, and blend the real-world view with virtual content. Some headsets, such as the Meta Quest 3, achieve MR through passthrough cameras, which capture the surrounding environment and display it on the screen. Other devices achieve MR without passthrough cameras, for example the Microsoft HoloLens devices are transparent glasses which project virtual content directly onto the lenses. MR is useful in situations where real-world integration is beneficial, such as training or educational experiences. You can also use MR to create social gaming experiences, or to enhance gameplay with locational information.\n\nNote: On some modern devices, you can develop an app that has both MRMixed Reality\n\nSee in Glossary and VR modes to allow the user to toggle between these modes within your app.\n\nVR and MR development shares common workflows and design considerations with any real-time 3D development in Unity. However, distinguishing factors include:\n• Richer user input: in addition to traditional button and joystick controllers, VR devices provide spatial head, controller, and hand and finger tracking (on supported platforms).\n• More intimate interaction with the environment: in conjunction with the possibilities of richer input, VR raises the expectations of much closer and physical interaction with the environment than typical 3D games and applications. Users expect to be able to pick things up and interact with objects in the environment. With head tracking, the camera \n\n A component which creates an image of a particular viewpoint in your scene. The output is either drawn to the screen or captured as a texture. More info can get much closer to the walls and other boundaries of the environment and can even pass through them.\n• User comfort concerns: many people experience motion sickness in VR when camera movement doesn’t match the movement of their head. You can mitigate the causes of motion sickness by maintaining a high frame rate, offering a range of locomotion options so that users can choose a mode they are comfortable with, and avoiding moving the camera independently of the user’s head tracking.\n\nTo get started with VR development, use the XR Plug-in Management system to install and enable XR provider plug-ins for the devices you want to support. Refer to XR Project set up for more information.\n\nA basic VR or MR sceneA Scene contains the environments and menus of your game. Think of each unique Scene file as a unique level. In each Scene, you place your environments, obstacles, and decorations, essentially designing and building your game in pieces. More info\n\nSee in Glossary should contain an XR Origin, which defines the 3D origin for tracking data. This collection of GameObjectsThe fundamental object in Unity scenes, which can represent characters, props, scenery, cameras, waypoints, and more. A GameObject’s functionality is defined by the Components attached to it. More info\n\nSee in Glossary and components also contains the main scene Camera and the GameObjects representing the user’s controllers. Refer to Set up an XR scene for instructions on setting up a basic VR scene.\n\nYou typically need a way for the user to move around and to interact with the 3D world you have created. The XR Interaction Toolkit provides components for creating interactions like selecting and grabbing objects. It also provides a customizable locomotion system. You can use the Input System in addition to or instead of the XRAn umbrella term encompassing Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) applications. Devices supporting these forms of interactive applications can be referred to as XR devices. More info\n\nSee in Glossary Interaction Toolkit.\n\nMost of the features and APIs used for VR development in Unity are provided through packages. These packages include:\n\nVR and MR platforms are available as provider plug-ins by the XR Plug-inA set of code created outside of Unity that creates functionality in Unity. There are two kinds of plug-ins you can use in Unity: Managed plug-ins (managed .NET assemblies created with tools like Visual Studio) and Native plug-ins (platform-specific native code libraries). More info\n\nSee in Glossary Management system. To understand how to use the XR Plug-in Management system to add and enable provider plug-ins for your target platforms, refer to XR Project set up.\n\nThe following table describes the plug-ins available for VR and MR development, and the devices that they support:\n\nNote: Many headset makers are working toward using the OpenXR runtime as a standard. However, this process isn’t complete and there can be feature discrepancies between OpenXR and a headset maker’s own provider plug-in or SDK.\n\nThe XR Interaction Toolkit can make it easier and faster to develop VR applications. The XR Interaction Toolkit provides:\n• An XR Origin set up with controllers.\n• XR controller setups with Input System presets for basic interactions like select and grab.\n\nThe XR Core Utilities package contains software utilities used by other Unity XR plug-ins and packages. Typically, this package gets installed in your project as a dependency of other XR packages.\n\nThe Unity Input System package not only supports accessing user input from VR controller buttons and joysticks, but also provides access to XR tracking data and haptics. The Input System package is required if you use the XR Interaction Toolkit or the OpenXR provider plug-in.\n\nUnity provides templates for VR and MR development. These templates are accessible from the Unity Hub, and provide a sample scene pre-configured with the relevant packages and components to get started with VR and MR development.\n\nThe available VR and MR templates are:\n\nTo learn more about creating an XR project from a template, refer to Create an XR project.\n\nHand tracking is a feature that allows users to interact with a VR application using their hands. Hand tracking is supported by the XR Hands package.\n• The XR Hand Skeleton Driver component, which maps a set of Transforms to their corresponding hand joints \n\n A physics component allowing a dynamic connection between Rigidbody components, usually allowing some degree of movement such as a hinge. More info and updates those Transforms as tracking data is received.\n• The XR Hand Mesh Controller, which enables and disables a mesh \n\n The main graphics primitive of Unity. Meshes make up a large part of your 3D worlds. Unity supports triangulated or Quadrangulated polygon meshes. Nurbs, Nurms, Subdiv surfaces must be converted to polygons. More info as hand tracking is acquired or lost.\n• A HandVisualizer sample that demonstrates how to use the hand tracking API."
    },
    {
        "link": "https://docs.unity3d.com/2022.3/Documentation/Manual/VROverview.html",
        "document": "Get started with virtual reality and mixed realityMixed Reality (MR) combines its own virtual environment with the user’s real-world environment and allows them to interact with each other.\n\nSee in Glossary development in Unity.\n\nVirtual Reality (VR) and Mixed Reality (MR) both refer to extended reality experiences where specialized devices provide a way for the user to interact with a virtual environment.\n\nIn VR experiences, the environment is closed. This means that the user can’t see their surrounding environment, and can only see virtual content displayed on the screen. The user can only interact with virtual content, rather than their physical environment. VR experiences are often achieved with a headset (a Head-Mounted Display [HMD]), where a screen within the headset displays the virtual environment. VR experiences are fully immersive, and so are a good choice for creating immersive, story-driven experiences and gameplay.\n\nMR combines elements of the real and virtual environments, enabling users to see and interact with both simultaneously. MR relies on devices that are able to display a real-time view of the user’s surroundings, and blend the real-world view with virtual content. Some headsets, such as the Meta Quest 3, achieve MR through passthrough cameras, which capture the surrounding environment and display it on the screen. Other devices achieve MR without passthrough cameras, for example the Microsoft HoloLens devices are transparent glasses which project virtual content directly onto the lenses. MR is useful in situations where real-world integration is beneficial, such as training or educational experiences. You can also use MR to create social gaming experiences, or to enhance gameplay with locational information.\n\nNote: On some modern devices, you can develop an app that has both MRMixed Reality\n\nSee in Glossary and VR modes to allow the user to toggle between these modes within your app.\n\nVR and MR development shares common workflows and design considerations with any real-time 3D development in Unity. However, distinguishing factors include:\n• Richer user input: in addition to traditional button and joystick controllers, VR devices provide spatial head, controller, and hand and finger tracking (on supported platforms).\n• More intimate interaction with the environment: in conjunction with the possibilities of richer input, VR raises the expectations of much closer and physical interaction with the environment than typical 3D games and applications. Users expect to be able to pick things up and interact with objects in the environment. With head tracking, the camera \n\n A component which creates an image of a particular viewpoint in your scene. The output is either drawn to the screen or captured as a texture. More info can get much closer to the walls and other boundaries of the environment and can even pass through them.\n• User comfort concerns: many people experience motion sickness in VR when camera movement doesn’t match the movement of their head. You can mitigate the causes of motion sickness by maintaining a high frame rate, offering a range of locomotion options so that users can choose a mode they are comfortable with, and avoiding moving the camera independently of the user’s head tracking.\n\nTo get started with VR development, use the XR Plug-in Management system to install and enable XR provider plug-ins for the devices you want to support. Refer to XR Project set up for more information.\n\nA basic VR or MR sceneA Scene contains the environments and menus of your game. Think of each unique Scene file as a unique level. In each Scene, you place your environments, obstacles, and decorations, essentially designing and building your game in pieces. More info\n\nSee in Glossary should contain an XR Origin, which defines the 3D origin for tracking data. This collection of GameObjectsThe fundamental object in Unity scenes, which can represent characters, props, scenery, cameras, waypoints, and more. A GameObject’s functionality is defined by the Components attached to it. More info\n\nSee in Glossary and components also contains the main scene Camera and the GameObjects representing the user’s controllers. Refer to Set up an XR scene for instructions on setting up a basic VR scene.\n\nYou typically need a way for the user to move around and to interact with the 3D world you have created. The XR Interaction Toolkit provides components for creating interactions like selecting and grabbing objects. It also provides a customizable locomotion system. You can use the Input System in addition to or instead of the XRAn umbrella term encompassing Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) applications. Devices supporting these forms of interactive applications can be referred to as XR devices. More info\n\nSee in Glossary Interaction Toolkit.\n\nMost of the features and APIs used for VR development in Unity are provided through packages. These packages include:\n\nVR and MR platforms are available as provider plug-ins by the XR Plug-inA set of code created outside of Unity that creates functionality in Unity. There are two kinds of plug-ins you can use in Unity: Managed plug-ins (managed .NET assemblies created with tools like Visual Studio) and Native plug-ins (platform-specific native code libraries). More info\n\nSee in Glossary Management system. To understand how to use the XR Plug-in Management system to add and enable provider plug-ins for your target platforms, refer to XR Project set up.\n\nThe following table describes the plug-ins available for VR and MR development, and the devices that they support:\n\nNote: Many headset makers are working toward using the OpenXR runtime as a standard. However, this process isn’t complete and there can be feature discrepancies between OpenXR and a headset maker’s own provider plug-in or SDK.\n\nThe XR Interaction Toolkit can make it easier and faster to develop VR applications. The XR Interaction Toolkit provides:\n• An XR Origin set up with controllers.\n• XR controller setups with Input System presets for basic interactions like select and grab.\n\nThe XR Core Utilities package contains software utilities used by other Unity XR plug-ins and packages. Typically, this package gets installed in your project as a dependency of other XR packages.\n\nThe Unity Input System package not only supports accessing user input from VR controller buttons and joysticks, but also provides access to XR tracking data and haptics. The Input System package is required if you use the XR Interaction Toolkit or the OpenXR provider plug-in.\n\nUnity provides templates for VR and MR development. These templates are accessible from the Unity Hub, and provide a sample scene pre-configured with the relevant packages and components to get started with VR and MR development.\n\nThe available VR and MR templates are:\n\nTo learn more about creating an XR project from a template, refer to Create an XR project.\n\nHand tracking is a feature that allows users to interact with a VR application using their hands. Hand tracking is supported by the XR Hands package.\n• The XR Hand Skeleton Driver component, which maps a set of Transforms to their corresponding hand joints \n\n A physics component allowing a dynamic connection between Rigidbody components, usually allowing some degree of movement such as a hinge. More info and updates those Transforms as tracking data is received.\n• The XR Hand Mesh Controller, which enables and disables a mesh \n\n The main graphics primitive of Unity. Meshes make up a large part of your 3D worlds. Unity supports triangulated or Quadrangulated polygon meshes. Nurbs, Nurms, Subdiv surfaces must be converted to polygons. More info as hand tracking is acquired or lost.\n• A HandVisualizer sample that demonstrates how to use the hand tracking API."
    },
    {
        "link": "https://learn.unity.com/tutorial/vr-best-practice",
        "document": "Check out the Unity development team's best practices for developing Virtual Reality applications, including optimizing rendering, decreasing latency, and platform-specific recommendations."
    },
    {
        "link": "https://docs.unity.cn/Manual/VROverview.html",
        "document": "VR development shares common workflows and design considerations with any real-time 3D development in Unity. However, distinguishing factors include:\n• Richer user input: in addition to “traditional” button and joystick controllers, VR devices provide spatial head, controller, and (in some cases) hand and finger tracking.\n• More “intimate” interaction with the environment: in conjunction with the possibilities of richer input, VR raises the expectations of much closer and “physical” interaction with the environment than typical 3D games and applications. Users expect to be able to pick things up and interact with objects in the environment. With head tracking, the camera \n\n A component which creates an image of a particular viewpoint in your scene. The output is either drawn to the screen or captured as a texture. More info can get much closer to the walls and other boundaries of the environment – even passing through them.\n• User comfort concerns: many people experience motion sickness in VR when camera movement doesn’t match the movement of their head. You can mitigate the causes of motion sickness by maintaining a high frame rate, offering a range of locomotion options so that users can choose a mode they are comfortable with, and avoiding moving the camera independently of the user’s head tracking.\n\nTo get started with VR development, use the XR Plug-in Management system to install and enable XR provider plug-ins for the devices you want to support. Refer to XR Project set up for more information.\n\nA basic VR sceneA Scene contains the environments and menus of your game. Think of each unique Scene file as a unique level. In each Scene, you place your environments, obstacles, and decorations, essentially designing and building your game in pieces. More info\n\nSee in Glossary should contain an XR Origin, which defines the 3D origin for tracking data. This collection of GameObjectsThe fundamental object in Unity scenes, which can represent characters, props, scenery, cameras, waypoints, and more. A GameObject’s functionality is defined by the Components attached to it. More info\n\nSee in Glossary and components also contains the main scene Camera and the GameObjects representing the user’s controllers. Refer to Set up an XR scene for instructions on setting up a basic VR scene.\n\nBeyond the basics, you typically need a way for the user to move around and to interact with the 3D world you have created. The XR Interaction Toolkit provides components for creating interactions like selecting and grabbing objects. It also provides a customizable locomotion system. You can use the Input System in addition to or instead of the XRAn umbrella term encompassing Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) applications. Devices supporting these forms of interactive applications can be referred to as XR devices. More info\n\nSee in Glossary Interaction Toolkit.\n\nMost of the features and APIs used for VR development in Unity are provided through packages. These packages include:\n\nTo build VR apps in Unity, use the XR Plug-inA set of code created outside of Unity that creates functionality in Unity. There are two kinds of plug-ins you can use in Unity: Managed plug-ins (managed .NET assemblies created with tools like Visual Studio) and Native plug-ins (platform-specific native code libraries). More info\n\nSee in Glossary Management system to add and enable provider plug-ins for the devices you want to support. See XR Project set up for instructions.\n• OpenXR for any device with an OpenXR runtime, including Meta headsets, VIVE headsets, Valve SteamVR, HoloLens, Windows Mixed Reality \n\n Mixed Reality (MR) combines its own virtual environment with the user’s real-world environment and allows them to interact with each other. , and others.\n• PlayStation VR (available to registered PlayStation developers) for Sony PS VR and PS VR2 devices. See PlayStation Partners for more information.\n• Mock HMD for simulating a VR headset in the Unity Editor Play mode view.\n\nNote: Many headset makers are working toward using the OpenXR runtime as a standard. However, this process is not complete and there can be feature discrepancies between OpenXR and a headset maker’s own provider plug-in or SDK\n\nThe XR Interaction Toolkit can make it easier and faster to develop VR applications. The XR Interaction Toolkit provides:\n• An XR Origin set up with controllers.\n• XR controller setups with Input System presets for basic interactions like select and grab.\n\nThe XR Core Utilities package contains software utilities used by other Unity XR plug-ins and packages. Typically, this package gets installed in your project as a dependency of other XR packages.\n\nThe Unity Input System package not only supports accessing user input from VR controller buttons and joysticks, but also provides access to XR tracking data and haptics. The Input System package is required if you use the XR Interaction Toolkit or the OpenXR provider plug-in.\n\nUnity’s VR Project Template provides a starting point for virtual reality development in Unity. The template configures project settingsA broad collection of settings which allow you to configure how Physics, Audio, Networking, Graphics, Input and many other areas of your project behave. More info\n\nSee in Glossary, pre-installs the right packages, and includes a sample scene with various pre-configured example assets to demonstrate how to set up a project that is ready for VR. Access the VR template through the Unity Hub when you create a new project. Refer to Create a new project for information about creating a project with the template.\n\nFor more information about the template assets and how the sample scene is set up, refer to About the VR Project Template.\n\nHand tracking is a feature that allows users to interact with a VR application using their hands. Hand tracking is supported by the XR Hands package.\n• The XR Hand Skeleton Driver component, which maps a set of Transforms to their corresponding hand joints \n\n A physics component allowing a dynamic connection between Rigidbody components, usually allowing some degree of movement such as a hinge. More info and updates those Transforms as tracking data is received.\n• The XR Hand Mesh Controller, which enables and disables a mesh \n\n The main graphics primitive of Unity. Meshes make up a large part of your 3D worlds. Unity supports triangulated or Quadrangulated polygon meshes. Nurbs, Nurms, Subdiv surfaces must be converted to polygons. More info as hand tracking is acquired or lost.\n• A HandVisualizer sample that demonstrates how to use the hand tracking API."
    },
    {
        "link": "https://riseuplabs.com/unity-game-development-guidelines",
        "document": "Unity Game Development is a popular topic among gaming enthusiasts. Read this complete Unity game development guidelines to if you you ever wondered about how your favorite VR game was created? Or how two brothers, Chad & Jared Moldenhauer, built a hit indie title like Cuphead that sold more than one million copies globally within two weeks of release? Well, they were built using Unity Game Engine!\n\nAccording to Unity’s statistics, 50%+ of all mobile, PC, and console games are made using Unity. So, if you’re looking to build a game, Unity development is the right option! According to Grand View Research, Unity 3D Game Development is a large part of a ~$200 Billion Game Industry. Also, Unity’s 2021 Gaming Report shows that Unity is the game engine of choice for over 60% of game developers.\n\nKeeping these stats in mind, Unity Game Development is a hot topic these days. With the rising importance of Virtual Reality (VR), Unity has shot to the forefront of this VR revolution. Unity development powers over 60% of all AR / VR content. So, this article will address all your questions and provide information about Unity Development and how to make a Unity game.\n\nBy reading this Unity game development tutorial, you will have a good idea about Unity’s history, advantages, the cost of developing games, and other background information. Then, the article will elaborate on the guidelines on how to make a Unity game. Let’s begin.\n\nBefore diving into the detailed Unity Game Development Tutorial, you might find a summary of the important information on Unity Development useful. So, I have compiled a list of basic facts in the following summary table. As a result, you can get an overview of the crucial aspects of Unity Development to understand how to make a unity game.\n\nGame Development is the process of building a video game from inception to deployment. The first step is idea generation. It is important to explain your vision for the game to everyone involved. So, the game experts can bring your dream to life.\n\nFirst, a game designer sketches and animates the game elements like characters, objects, and the environment. Next, the game developer programs the game to include the logical progression of the story. In the early days of game development, developers wrote their code from scratch. Soon, however, game engines like Unity were released to facilitate development.\n\nAfter the game has been designed and built, it is playable. However, it is beneficial to perform testing of the written code to iron out bugs and issues. So, a game tester conducts detailed testing processes to find and eliminate problems in the code.\n\nOnce the game has been tested, it is ready for deployment. Your audience can now play your game on supported devices. So, this article aims to address your questions on how to make a Unity game.\n\nGame Engines are special software that helps game developers create video games easily. These engines, also called frameworks, provide graphics, physics, audio, video, and animation functionality.\n\nGame developers can reuse the game engine functionalities to reduce game development time. Popular game engines include:\n\nThe primary use of a game engine is to build games. However, game engines are now powerful enough for other functionality, like 3D visualizations for building digital twins.\n\nDigital Twins are virtual representations of material objects and processes. Companies across various industries can build digital twins for added insights and innovation.\n\nUnity is a real-time, cross-platform game engine for building 2D, 3D, AR, and VR games. It is the ideal platform for artists, designers, developers, and testers to collaborate on a single project.\n\nThe Unity game engine is free for individuals for school and personal projects. However, teams can choose Plus, Pro, and Enterprise plans for better functionalities and support.\n\nUnity has a very involved community. Thus, answers to problems are easy to come by. Also, the Unity Asset Store has a large collection of assets and plug-ins to be bought and sold.\n\nUnity game development involves using Unity’s robust ecosystem to build games for 25+ platforms across desktop, mobile, console, VR, AR, TV, and the web.\n\nUnity’s developers launched the game engine at 2005’s Apple Worldwide Developers Conference (WWDC) event. Their goal was to democratize game development by making Unity easily accessible to developers.\n\nInitially, they released Unity exclusively for Mac OS X. Later, Unity added support for Windows and Browsers. So, let’s look at the major versions of Unity and what they added to Unity game development.\n• Unity 2.0 – launched in 2007 with 50 new features. Support for the Apple App Store added.\n• Unity 3.0 – launched in 2010 with support for consoles, PC, and Android for Unity 3D game development. Becomes the top game engine for mobile platforms.\n• Unity 4.0 – launched in 2012 with DirectX 11 and Adobe Flash support. Mecanim animation support added.\n• Unity 5.0 – launched in 2015 with native support for Nintendo Switch, Google Daydream, Facebook Gameroom, and the Vulkan graphics API.\n• Unity 2017-present – from 2017 onwards, Unity releases that year’s version (i.e., for 2022, the version is called Unity 2022). New features include Virtual Reality Development support.\n\nNow that we know the history of Unity game development let’s explore some of the features available in Unity.\n\nUnity is a preferred tool for game developers because it provides a full set of features for creating, launching, and monetizing games and experiences.\n\nUnity is widely used for both 2D and 3D game development. 2D games use flat graphics with no camera perspectives. However, a 2D game can include 3D graphics for style and depth.\n\n3D games use Unity’s camera to add perspective. As a result, objects near the camera appear larger. Also, the objects use 3D geometry to give them three dimensions. Thus, 3D objects appear solid with textures and materials. Also, using 3D, developers can build Unity VR game development solutions.\n\nUnity has an in-built physics engine to handle the physical interactions of objects with the environment. So, using the physics engine, the game developer can adjust object collisions, accelerations, gravity, and other natural forces.\n\nBy default, Unity enables the object-oriented physics engine. These experiences run on a single core and thread. Recently, Unity has added a data-oriented technology stack for faster, lighter, and optimized multi-threading.\n\nA Unity game developer writes code to control the operation of the game. In Unity, this is called scripting. For example, the developer writes C# code to handle the object hierarchy and order of events in-game.\n\nUnity scripts can create graphical effects. Also, the Unity game programmer can control object behavior and the AI system for the characters to get an interactive experience.\n\nUnity offers advanced support for audio and video integration into gameplay. A Unity expert can produce audio and video outside Unity using tools like Apple Logic Pro.\n\nUnity supports 3D sound with tools for mixing and mastering sound. In addition, developers can add advanced properties like Ambient Occlusion and Spatial Blend. Finally, Unity game developers can integrate video into the gameplay as cut-scenes.\n\nThe Unity animation system helps developers easily animate objects and characters. Humanoid animation is made easy for artists and animators with Unity.\n\nA character is animated by applying code via script. These scripts are reusable so that you can apply the same script to different characters. As a result, the animation process is quicker with Unity.\n\nUnity’s navigation system helps game developers build characters that intelligently move around the game world. Navigation meshes are automatically created from the scene geometry.\n\nThe off-mesh links help with actions like jumping off a ledge or opening a door. Also, dynamic obstacles trigger Unity’s runtime character navigation scripts.\n\nWe have seen what Unity is and the various features it provides. So, you might have the question, what are the benefits of using Unity for game development?\n\nAccording to Game Developer, 47.3% of all games are made using the Unity game engine. Unity is most popular for game development among hobbyists and indie developers.\n\nSo, before you learn how to make a Unity game, you should know about the benefits of the engine. Unity development provides the following advantages and benefits for building games:\n\nThe Unity game engine is free for individuals to get started with Unity 2D and 3D development. Users get all major features in the free version.\n\nA large group of developers can subscribe to the pro versions for high-end features like 3D composition booster, sound channels, and feature playback.\n\nUnity has extensive platform support for game development. Developers can build Unity games on Windows, macOS, and Linux machines. Also, Unity development is supported on the following platforms:\n\nDue to Unity’s support for multiple platforms, an app built for one platform can be easily ported to other platforms by the developer. As a result, the process of building cross-platform games is faster and more efficient.\n\nAn Integrated Development Environment (IDE) allows the developer to have a one-stop shop for writing code. Unity’s built-in IDE supports multiple programming languages like C# and C++ for scripting.\n\nUnity’s IDE is a graphical user interface (GUI) that contains a code editor, compiler, and debugger for complete scripting support. Thus, the developer can work within Unity to build their games from scratch.\n\nHowever, for game developers preferring to use third-party IDEs, Unity has you covered! Unity development is supported for the following IDEs for Windows, macOS, and Linux:\n\nDevelop 2D and 3D games with Unity’s high-end graphics features. Creative artists can leverage Unity and third-party software like Blender and Maya to create optimized and stunning graphics.\n\nDeveloping your game with Unity is beneficial due to a large and supportive community of users. As a result, the game engine is constantly updated for Unity 3D game development.\n\nDue to the Unity development community, users get access to informative posts, forums, and blogs. In addition, the Unity engine gets frequent updates for better rendering, physics, sounds, and controls.\n\nData Analytics is an important part of any business today, including game development. Unity development is supported by an Analytics product for end-to-end data analytics.\n\nYou can use analytics to delve into game performance and player behavior. As a result, Unity’s Analytics solution provides real-time insights into your game and audience. Some of the metrics you can view from Unity Analytics are:\n\nRapidly test your Unity VR game development implementation or 3D experience with interactive editing. Play Mode is one of Unity’s core features, allowing developers to check the gameplay instantly. As a result, the Unity development professionals can test and review the game as they build it. For example, for Augmented Reality Development, developers can use the play mode to check their game in real-time.\n\nUnity’s Asset Store holds thousands of assets that help game developers build games faster. The Asset Store is a library of free and paid resources made by Unity and the community.\n\nUsing the Asset Store means developers do not need to build game elements from scratch. Included in the Asset Store are:\n• Templates – starter packs to get a base game to build upon\n• Tools – for adding useful features like AI and Advanced Physics.\n\nA game developer can get these assets from the store to quickly build many aspects of the game. As a result, the Unity game developer does not need to be an expert in every field like audio or VFX.\n\nThis Unity game development tutorial has given an introduction to what Unity is. However, before diving into how to make a Unity game, you might wonder if Unity is the right game engine for you.\n\nThe only real competition to Unity for game development is the Unreal Engine by Epic Games. So, how does Unity development compare to Unreal game development? Let’s find out.\n\nUnreal Engine is a game engine that supports game development for desktop, console, mobile, and VR platforms. The latest generation, Unreal Engine 5, has raised the bar for high-quality graphics with technologies such as Nanite for advanced geometry. Also, Lumen helps developers with next-gen global illumination.\n\nI have compiled the head-to-head comparison in table form to make this comparison easier. The following table presents several criteria to compare Unity and Unreal Engine against.\n\nAs you can see from the Unity vs. Unreal comparison table, both game engines are good at what they do. However, Unity is easier to learn as it is targeted toward indie developers looking to learn how to make a Unity game. As a result, Unity offers a large asset store and charges no royalties.\n\nOn the other hand, Unreal Engine is harder to learn but is extremely powerful for AAA graphics. As a result, large studios looking to create next-gen games can use Unreal for their development. However, Unreal Engine charges royalties, so you will have to give Epic Games a share of your profits over $1 million.\n\nA Unity Game Developer is an expert who leverages the Unity game engine to build a game. Unity is a very diverse tool for creating various mobile, web, desktop, console, and more projects. As a result, a Unity game developer needs to be skilled in many topics.\n• Scripting with C# to write clear, maintainable, and testable code.\n• Working with Unity Game Assets like meshes, materials, and animations.\n• 3rd Party 3D design, modeling, and animation tools like Maya and Blender.\n• Collaborate with other Unity artists, designers, developers, and testers to create a full-fledged game.\n• Conduct comprehensive testing of Unity’s C# code to debug, optimize, and solve issues.\n• The developer needs to understand Mixed Reality Development and have a working knowledge of VR SDKs from Google, Meta, and other major VR providers for Unity VR development.\n\nA Unity game developer can build 2D, 3D, and XR games for multiple platforms using the skills above. In addition, Unity provides many learning resources, blogs, and documentation, so a Unity VR game development expert can always get help for any issues.\n\nDo you have a Unity game idea in mind? Then, you may want to Hire a Unity Developer to bring your vision to life. So, how exactly do you Hire a Unity Game Developer? There are three main options to choose Unity game developers from:\n• Recruitment agencies that can source Unity developers for you.\n\nBefore approaching any of these channels, you should know what you want. Also, it would help if you narrowed down factors such as your timeline, budget, and expertise needed. Thus, you can find the exact developer(s) to fit your needs.\n\nUnderstanding the genre of the game you want to build will help you in many ways. Chief among those is the overall theme. For example, an action game will have a faster pace while a horror game will be suspenseful.\n\nYour game sounds will also depend on the game genre to evoke the right emotions in your gamers. So, let’s see some of the major game genres for Unity 2D and 3D game development.\n• Action – like Fighters, Beat ’em ups, or Shooters. Action games are typically challenging, focusing on hand-eye coordination and reaction time.\n• Racing – a racing competition where players aim to finish first. The racing can be a simulation, arcade, kart, or futuristic.\n• Sports – simulates the playing of various sports like football (soccer), basketball, and skateboarding.\n• Simulation – simulates real-world activities like farming, construction, vehicles, and sports. They are used for training and entertainment.\n• Casino – games typically in a casino, such as Slots, Blackjack, Poker, and Bingo.\n• Battle-Royale – an online multiplayer game with the goal of being the last player or team standing.\n• Strategy – a strategy video game focuses on thinking and planning to achieve victory. These include high-level strategy, logistics and resource management.\n• Role-Playing – players control one or more characters based on a well-defined world. The characters undergo development by leveling up as the story progresses.\n• Horror – these games are designed to scare the gamer. The horror game can be survival, psychological, action, or jump-scare.\n\nA Unity game developer can build a game based on these genres to reach interested gamers. Furthermore, Unity provides the tools to shake up the industry. For example, you can build a 2D strategy game or a 3D simulation of a popular sport. In addition, the immersiveness of horror games can be amplified with the addition of Virtual Reality (VR). Unity supports them all.\n\nThe creators of the Unity game engine released the first version in 2005 to make games for Mac OS. Over time, Unity has evolved into a cross-platform game engine, supporting desktop, mobile, AR, VR, console, and other platforms.\n\nLarge companies and indie developers alike use Unity’s wide support for building their games. For example, here at Riseup Labs, we used Unity 3D to build Meena Game and Meena Game 2 for UNICEF. As a result, we now have over 3 million downloads combined in the App and Play Stores!\n\nUnity has built up a large library of successful and popular games. So, let’s explore some of the top games built with Unity.\n\nCuphead took the gaming community by storm when it was released in 2017. The developers leveraged the power of Unity to build a 2D, hand-drawn game based on cartoons of the 1930s and 1940s. Subsequently, Cuphead proved to be an instant hit, selling 6 million copies to date.\n\nOri: Will of the Wisps is a 2D game that uses 2D art and 3D animation to create a gripping story that awed gamers globally. The Unity game engine helped the developers target multiple platforms, reaching the top of Mobile, Desktop, and console charts with over 2 million total sales.\n\nEscape From Tarkov is a thrilling story-based 3D game built with Unity. Furthermore, the game offers a multiplayer mode for cooperative and competitive experience. As a result, this game reached up to 200,000 concurrent players at its peak.\n\nSubnautica provides gamers with an in-depth underwater world with oceans to explore. The game was built using Unity 3D game development and was released on five platforms. As a result, Subnautica reached immense popularity among gamers, with 6 million copies selling globally!\n\nUnity is the go-to development engine for Artificial and Virtual Reality creators due to its support for all major VR headsets. As a result, some of the most popular AR / VR games were built in Unity!\n\nPokémon Go united the gaming world during the summer of 2016 with a trending AR mobile game. Players used their smartphones to locate, capture, train, and battle Pokémons alongside other players in their area. Due to its immense popularity, Pokémon Go reached over a billion downloads globally!\n\nCoco VR brings the Land of the Dead from the Coco Film to life using Virtual Reality (VR). Players get to wander the vibrant world from Coco and have mini-experiences. As a result, this game showcases the vast capabilities of VR in gaming, with Unity VR leading the way.\n\nThe Unity game engine is a comprehensive tool covering 2D, 3D, AR, VR, and XR game development. As a result, Unity provides immense scope to developers for experimentation and innovation in game development.\n\nIt is estimated that the Unity Engine has over 2 Billion monthly active users as of 2021! As such, you may want to get yourself a piece of the pie and join the ever-growing number of Unity game developers!\n\nYou need to follow a common process to get started with Unity game development, either yourself or with a professional. Thus, I have listed the major stages of developing games with Unity. So, let’s explore these steps in detail:\n\nWhen you first decide on building a game, you need to analyze the game requirements. For example, will the game be 2D, 3D, VR, or something else? What platforms am I targeting? What is my budget? These are important questions to answer when embarking on Unity game development.\n\nThe analysis stage also identifies potential risks and business requirements. Thus, analysis is essentially a feasibility study of the Unity game project. As a result, you can assess the viability of your game by understanding the advantages and disadvantages.\n\nNow that you have a complete idea of the game requirements, you can begin planning. The planning stage covers many aspects like:\n\nConducting a detailed plan before game development will make the development process faster in Unity.\n\nGame design is an integral part of your Unity game. A pleasing design will compel gamers to try out your game. Also, your game design sets you apart from other game titles in the market.\n\nYou can apply two main design processes to Unity development; High-Level Design (HLD) and Low-Level Design (LLD).\n\nHigh-Level Design (HLD) is the initial stage of designing your game. It is described from a generalized point of view. As a result, it is a high-level overview of the design.\n\nOn the other hand, Low-Level Design (LLD) takes the concepts in the HLD and goes in-depth for the technical details.\n\nThe game designer can create designs outside Unity using 3rd party tools like Blender and Maya. Conversely, the Unity Asset Store has community-built 2D and 3D designs that can be imported directly into Unity to build your game design.\n\nAt this stage, you have the plan and design of the game completed. Thus, you can build a Minimum Viable Product (MVP) for your game in Unity.\n\nThe MVP is a stripped-down version of your game intended to test your core idea. For example, you can build an MVP to determine if your users find your game mechanics engaging.\n\nIf you get negative feedback, you can go back to the drawing board to fix the elements of your game that require a fix. Or, if you get positive feedback, you can begin building the full game on Unity.\n\nGame development in Unity uses C# scripting to animate and give life to the game designs. Also, you have to develop the following solutions in Unity:\n\nThe development of games in Unity is accelerated due to the tight integration of the pipeline. Also, games built in Unity are compatible with major platforms. As a result, you can develop your game entirely within Unity.\n\nGame backends can be immensely complex. Thus, Unity offers many tools and services to simplify the development and management of backends. For example, you can host your game on Unity’s Cloud Code. Also, Unity Economy helps you design and tune your game economy.\n\nTesting is an integral part of the game development process. Unity provides a Test Framework to run automated tests during game development. As a result, you can identify game issues, bugs, and inefficiencies before they can reach the gamers.\n\nThe major testing methodologies that you can follow are:\n\nUnity also supports 3rd party code editors like Visual Studio and JetBrains Rider, so you can use their debuggers to debug the C# code.\n\nA stable version of the game is now complete! As a result, you can launch the game on its release platforms like Steam, Google Play Store, and Apple App Store.\n\nDevelopers can publish their builds easily on Unity. Also, the game engine does the process on its own. So, you can publish your game build to a variety of platforms.\n\nThe first release of your game is often called Early Access or Alpha Access. Gamers can get your game early to debug or provide feedback and suggestions. Also, they are rewarded for that with special in-game items like weapons and skins. As a result, you can fix some of the issues not found during the game’s initial testing.\n\nUsing the suggestions and feedback from the Early Access gamers, you can fix any lingering bugs in Unity before the final release. As a result, your users will get a bug-free version of the game.\n\nThere’s work to do once your game is out in the wild! Gamers will want expansions (called DLCs) for added content in your game. Also, your game will need maintenance for any slowdowns or bugs.\n\nAccording to Unity’s reports, multiplayer and post-launch content engages gaming audiences further. Thus, constantly adding content to your Unity game will help keep gamers happy.\n\nIf your current game is single-player, you can add an online multiplayer component in Unity. In addition, Unity provides useful tools and services to build multiplayer games faster.\n\nThe popularity of gaming has been on the rise for the past few years. Moreover, the onset of the COVID-19 pandemic further accelerated this growth. According to Unity’s 2021 Gaming Report, the number of players gaming during the pandemic rose by 38% in 2020. In addition, the spike in revenues due to the pandemic was 30%.\n\nSo, what are the trends in Unity Game Development? Unity’s 2022 Gaming Report provides us with some insight. Here are the top trends, in no particular order:\n\nGames made with Unity increased by 93% in 2021. From these, hyper-casual games lead the way, with an increase of 137% over the previous year.\n\nOther game genres like casual games have risen by 53, while mid-core and hardcore genres grew by 54% and 55%, respectively.\n\nClearly, Unity game development is growing by the year. Therefore, the time to get into Unity game development has never been better.\n\nUnity has made it easier for developers to build games for multiple platforms. As a result, the number of games released for cross-platforms has doubled compared to five years ago.\n\nGamers can now enjoy a game on their home console, then pick up where they left off on their mobile devices on the go. Thus, Unity has helped creators build multiplatform games that help keep gamers from switching games.\n\nUnity Mobile Gaming Goes from Strength to Strength\n\nMobile gaming continues to grow, with an estimated 8% increase in mobile ad revenue. Also, first-day microtransaction purchases have risen by more than 50% year over year.\n\nMobile game development with Unity is easier than ever, with indie developers building and publishing iOS and Android games every day. For example, the number of new creators in Unity increased by 31%.\n\nThe amount of time spent by gamers playing multiplayer games has increased steadily. As a result, Unity has released several products aimed at helping developers with multiplayer game development.\n\nGamers also want more content for their games. Having added content to multiplayer games increases user retention. As a result, live games are more popular than ever.\n\nMany popular games are built as services with regular content updates such as new maps, challenges, game modes, and storylines. So, Unity game development is straying more towards multiplayer, live games.\n\nThe Unity platform is commonly thought of as just a game engine. Initially, Unity gained popularity due to its amazing features for game development. However, these days, Unity is used for diverse industries such as Engineering, Retail, and Architecture.\n\nLet’s explore the many use cases of Unity in industries.\n\nUnity’s powerful Virtual Reality (VR) capabilities can be leveraged by companies looking to train their employees. Businesses in healthcare, manufacturing, engineering, and education benefit from fully-immersive training environments.\n\nTraining in VR has several benefits:\n• Have a lower risk of injury due to heavy machinery or tools.\n• Safe training of medical professionals on 3D versions of patients.\n• Get hands-on experience with otherwise inaccessible hardware like expensive robots and rare chemicals.\n\nWith immersive VR training programs, businesses have higher worker satisfaction, better workplace safety, and lowered incident costs. Thus, Unity VR game development can help with training simulations to aid businesses in improving their workforce capacity.\n\nA digital twin is the digital or virtual representation of a physical object in 3D. Unity’s 3D visualization tools help businesses in Manufacturing, Architecture, and Automotive industries build real-time, immersive, and interactive digital experiences.\n\nThe main uses of digital twins in the industry are:\n• Monitor and manage operations of physical systems using real-time data from the digital twin.\n• Use real-time data to identify potential issues and anticipate future operations such as IoT predictive maintenance.\n• Design and build new solutions in the digital twin to assess its viability before building the physical product.\n\nThe digital twin empowers businesses to save costs on maintenance, development of new solutions, and training. Thus, the Unity game engine is a powerful tool for businesses to increase operational efficiencies.\n\nYou may be thinking, what is a product configurator? It is a tool that helps customers customize their product’s features and components virtually. As a result, the final product is exactly according to the customer’s expectations.\n\nWant an example of a product configurator?\n\nA retail business can offer its clothing items in a virtual 3D environment. Thus, their customers can try different colors, sizes, and variations. Furthermore, the business can enhance its product configurator by offering it in Augmented Reality (AR). As a result, the customer can ensure they get a perfectly fitting garment.\n\nUnity offers a powerful tool called Unity Forma to create real-time and interactive 3D product configurators. So, businesses can offer custom products to their customers virtually.\n\nEver seen a sports broadcast with a virtual set? Then, you have witnessed the power of real-time rendering. Unity is the king of real-time production.\n\nFilmmakers can leverage the full potential of Unity to build pre-visualizations of their movies. Thus, they can visualize what their final shot will look like to make changes before shooting.\n\nAnimators wanting to create VR experiences can build them with Unity. As a result, their animations will be fluid, thanks to real-time rendering.\n\nUnity is one of the more popular game engines for prototyping and developing games. This popularity is mainly due to Unity’s advanced features and functionalities for 2D, 3D, and XR game development. As a result, many indie developers and big businesses use Unity to create cutting-edge games. Unity VR game development is the future of gaming.\n\nSo, what is the cost of Unity Game Development? The average cost of building a Unity game is $20,000. The cost depends on various factors such as:\n\nUnity is free for individuals and companies to start with. However, there are several pricing tiers for different-sized businesses to choose from. The pricing plans are:\n• Personal ($0, Free) – for businesses with revenue or funding less than $100K in the last 12 months.\n• Plus ($399 / yr per seat) – for large businesses with revenue or funding less than $200K in the last 12 months.\n• Pro ($1800 / yr per seat) – for larger businesses with revenue or funding greater than $200K in the last 12 months.\n• Enterprise ($4000 / yr per 20 seats) – for enterprise-level businesses with revenue or funding greater than $200K in the last 12 months.\n\nThere may be a monthly/yearly cost to use Unity’s services, depending on your business requirements. However, you get better features and support from Unity with the more advanced plans.\n\nThe platform(s) you publish your game to will also factor into the cost. Launching for multiple platforms takes more time. Thus, you need to factor in the developer salaries for this extra period.\n\nMobile games targeted for Android are cheaper to build compared to iOS. As a result, you need to consider whether you want the extra cost (hint: you do). iOS games provide a better gaming experience than Android.\n\nThe cost of game development also depends on what level of development needs to be done. For example, it will cost more to build a game from scratch.\n\nYou will need to pay for experts in:\n\nYou need to think about any other experts you need to build your game in Unity. Thus, the cost increases as you have a game with more complexity. In addition, using advanced technologies like Artificial Intelligence (AI), Virtual Reality (VR), Augmented Reality (AR), and Machine Learning (ML) can cause costs to skyrocket. So Unity VR game development will cost your business more.\n\nExperts in these advanced technologies command a higher fee. The reason is that these experts typically have a higher education background than an average developer. For example, an AI developer earns on average $60 / hour, according to ZipRecruiter.\n\nWhat is the Future of Unity Game Development?\n\nI have established in the article why Unity is an extremely popular game engine today. So, you may be wondering, what is the future of Unity game development? Thus, let’s explore some of the growing technologies for the Unity game engine.\n\nThe Next-Big Thing: AR, VR, and the Metaverse\n\nYou may have heard of the term Metaverse on the news lately. Meta’s Zuckerburg has claimed that the “Metaverse is the next chapter for the internet.” So, what is the Metaverse, and how does it relate to Unity?\n\nThe Metaverse uses Augmented (AR) and Virtual Reality (VR) to create a virtual world used for gaming, socializing, and education. As a result, Unity will play a big role in making the Metaverse a reality.\n\nUnity is a global leader for AR and VR development, with a full suite of tools geared towards easy development. So, you can expect a lot of the future Metaverse Development to be done with Unity VR game development tools.\n\nBlockchain came to prominence due to cryptocurrencies like Bitcoin and Ethereum. However, it has a promising future in gaming too. And Unity is set to play a big role in it.\n\nA Blockchain game has two components, the normal game and the Blockchain integration. So, game developers can start with their Blockchain game using Unity’s easy-to-use engine. Then, the developer can integrate the Blockchain into the game using a Blockchain SDK.\n\nUsing Blockchain in gaming presents many benefits. For one, game data like player score, friends, in-game items, and achievements can all be stored in the secure platform Blockchain. As a result, regardless of its platform, every game can securely access the data.\n\nHave you ever heard of the term gamification? Gamification uses game elements and mechanics in a non-game setting, such as education and training. Unity’s powerful features can be used by businesses to create interesting gamification solutions. As a result, they can engage their employees better.\n\nGamification is set to gain traction in diverse industries due to the popularity of casual gaming. These games make eLearning more exciting for users, leading to better retention of information. For example, having a fun mobile AR experience can encourage employees to take their training seriously.\n\nOther potential uses of gamification include building Unity VR Game Development solutions for an immersive and accurate experience in many industries. For example, you can imagine how an auto mechanic can benefit from fixing cars in VR without any costly mistakes. Furthermore, adding game elements like leaderboards and trophies can make the experience fun!\n\nYou’ve heard about Unity 3D game development, its features, and the stages. Now, you might be interested in becoming a Unity game developer! So, I am providing some information about choosing Unity game development as a career.\n\nUnity is an incredibly popular game engine. As such, game companies are constantly looking for talented Unity experts. The average salary of a Unity 3D game developer in the US is $88,606 / yr, according to Glassdoor. So, if you become a Unity professional, you get a large pay!\n\nTo become a Unity game developer, you need first to learn Unity! Unity’s website has many helpful tutorials to help a beginner get started. And the best way to learn Unity is by doing small projects. For example, if you want to be a Unity VR developer, completing Unity VR game development projects will look good on your resume.\n\nWhat does a Unity developer do? A Unity 3D game development job requires:\n• C# Scripting to transform the game design into functional games.\n• Troubleshooting problems in the C# code for current and previous games.\n• Collaborating with other teams like Game Designers, Sound Experts, Animators, Directors, etc.\n• Be proficient in Extended Reality Development tools that are compatible with Unity.\n\nSounds good? If you are passionate about gaming and this seems like a good career for you, get started today! Learning a skill takes time, and you should build as many projects as possible.\n\nDeveloping games with Unity is more popular than ever. Businesses and individuals can create incredible experiences using Unity VR game development. So, in this article, I have described:\n\nNow that you have an idea of these topics, you or your business can make an informed decision about your next game. For example, are you going to use Unity? Or, are you interested in a Unity game development career?\n\nI hope you learned something about Unity and game development. Now, look at some commonly asked questions about Unity game development.\n\nI have compiled some Frequently Asked Questions (FAQs) on Unity Game Development. Please feel free to comment below if you have any further queries. I will answer any questions on Unity development."
    },
    {
        "link": "https://forum.arduino.cc/t/i-am-a-newbie-and-confused-on-animatronics-with-arduino-guidance-appreciated/195986",
        "document": "The Arduino is a fairly limited environment, due to the amount of memory involved. However, it can probably do most of what you want, though the audio parts may be tricky.\n\nIt also does not provide infrastructure support for doing multiple things, such as threads on processors with higher level OS'es. You can do this yourself, as it has the building blocks. This example, written by Nick Gammon shows the basic method of doing this: http://www.gammon.com.au/blink\n\nNow, a device like a raspberry pi has problems in dealing with real time things that the Arduino can do (motors, servo control, etc.). So you might want to think about splitting the robot into a brain microprocessor (i.e. the pi), and a bunch of smaller microprocessors, that just do one thing. I.e. you might want one that controls the servos for the head turn and tilt. This makes it simple to debug, in that you don't have to worry about the interactions. However, you probably don't want to get separate development boards for each sub-processor. Instead you would get one development board (such as an Uno), and then setup that up a programmer for the remote boards. You can get the same chip in the Uno for $3-5, and add a few extra parts and make your processor for each part. Atmega328 w/ Arduino UNO Optiboot Bootloader ATMEGA328P-PU - dipmicro electronics\n\nIf your needs are simpler than the Uno's, there at ATtiny85 boards that are smaller and cheaper. I just bought some raw ATtiny85 boards at $1.40 each (10 chips for $14) to run some blinky lights. I use my Arduino Uno to program them. Note, for your needs, they don't support the standard servo library due to the internal hardware on the ATtiny85, but there is a tinyServo library that does most things. If you don't want to deal with programming a raw board, you might want to look at Digispark, available directly from digistump, or at Microcenters: Digistump - Home of the Digispark. The Adafruit trinket and gemma also use ATtiny85, though I find it a touch easier to program the digispark.\n\nHowever, I suggest trying to start slow, rather than trying to do everything at once."
    },
    {
        "link": "https://researchgate.net/publication/361217854_Applications_of_Mixed_Reality_with_Unity_and_Arduino/fulltext/636ca36d54eb5f547cbbffdd/Applications-of-Mixed-Reality-with-Unity-and-Arduino.pdf",
        "document": ""
    },
    {
        "link": "https://instructables.com/Arduino-animatronics-make-your-awesome-costumes-m",
        "document": "Tools and materials Soldering iron- A good quality soldering iron is a must. I received an Aoyue 2900 soldering station a couple years ago for Christmas and it's been great. You won't believe the difference once you start using a good soldering iron. \n\n http://sra-solder.com/product.php/6363/22\n\n\n\n I also use a small tip for soldering small surface mount components-\n\nhttp://sra-solder.com/product.php/6397/0 Wire cutters/wire strippers- Small flush cutters are the best. If you don't have wire strippers or cutters then these will work well-\n\nhttp://www.adafruit.com/index.php?main_page=product_info&cPath=8&products_id=152\n\nhttp://www.adafruit.com/index.php?main_page=product_info&cPath=8&products_id=147\n\n\n\n Tweezers- Get some small tweezers to work with surface mount components. Here's an inexpensive set-http://sra-solder.com/product.php/6409/79\n\n\n\n Magnifiers- Being able to see what you're working on makes a world of difference.\n\nhttp://www.adafruit.com/index.php?main_page=product_info&cPath=8&products_id=291 Multimeter- Most any multimeter will work. You don't need to spend big $$$. I personally own a Wavetek Meterman 16XL and it's great. If you don't already own a multimeter and are really getting into hobby electronics then this meter will probably do everything you could ever want- \n\n http://www.adafruit.com/index.php?main_page=product_info&cPath=8&products_id=308\n\n\n\n Servo board PCB-\n\nhttp://batchpcb.com/index.php/Products/47581 MAX1555 IC- 1 ea\n\nhttp://www.sparkfun.com/products/674\n\nhttp://us.element-14.com/maxim-integrated-products/max1555ezk-t/ic-battery-charger-li-ion-340ma/dp/59J2761?Ntt=MAX1555\n\n\n\n Straight break away header pins - 2ea 40 pin row \n\n These come in really handy so it's always good to get extras to have on hand\n\nhttp://www.sparkfun.com/products/116\n\n\n\n Female break away header pins- 2 ea 40 hole row\n\n These also are super handy to have around\n\nhttp://www.sparkfun.com/products/115 Single cell LiPo battery- 1ea (you can use any capacity you like.)\n\nhttp://www.sparkfun.com/products/339 USB mini-B cable- 1 ea\n\n Odds are you've already got one but if you don't here you go-\n\nhttp://www.sparkfun.com/products/598\n\n\n\n\n\nAssembling the servo board\n\n\n\n The first thing to do is build the charging circuit. I usually start with the smallest components first. I've found the easiest way to solder SMD parts is to get a tiny bit of solder on your soldering tip and touch it to one of the component pads on the PCB. Then hold the component in place using tweezers and heat up the pad and component pin- this allows you to get the part attached to the board so you can check its alignment for the rest of the pads. Then simply solder each of the remaining pads. There is a great series of SMD soldering tutorials here- http://www.sparkfun.com/tutorials/36\n\n Begin by soldering on the MAX1555 IC (labeled U1) -this can only go on one way. Next comes the LED- make sure to check the polarity as it is labeled on the PCB (the LED cathode is connected to one end of R1.) Then solder resistor R1 followed by the capacitors C1 and C2. These can be soldered on either direction. Next comes the mini USB connector- this one is a bit tricky as the pins are positioned nearly underneath the connector. Now solder on the JST connector. Make sure to double check your soldering job for these connectors as they receive a fair bit of mechanical stress.\n\n Now test your charging circuit. Plug in a USB cable and check the voltage at the JST battery connector. It should read about 4.2-4.3V. Now connect the LiPo battery. If everything is OK the small LED should turn on, indicating the battery is charging. Disconnect the battery.\n\n Now solder on the pins to connect the Pro Mini board. This is done by soldering on the break away straight header pins. First insert the long pin ends into the PCB, flip the board over and solder them in place. Double check your solder joints. Now flip the board over and place the Pro Mini board in place on top of the exposed pins and solder all the pins in place. Next solder the remaining straight pins into place in the digital out positions and the 3.3v port along the bottom of the board.\n\n To finish the board solder all the female headers in place. The best way I've found to cut the female headers is to remove a pin where you want to make a cut- just yank the pin out the bottom using a pair of pliers. Then take wire cutters and cut through the opening left by the pin. Now take a file (or sandpaper) and smooth out the cut edge.\n\n Make sure your board is getting power by plugging a USB cable into the mini USB port on the controller board. The red LED on the Arduino Pro Mini should light up.\n\n That's it- your controller is ready to go!\n\n\n\n\n\n I really like these boards because you can see the LEDs light up when they are transmitting. You also need to make sure you have the latest FTDI drivers on your computer (you can get the most current drivers on the product web pages.)\n\n\n\n Sparkfun FTDI Basic 3.3V breakout\n\nhttp://www.sparkfun.com/products/10009\n\n\n\n Adafruit FTDI friend\n\nhttp://www.adafruit.com/index.php?main_page=product_info&cPath=18&products_id=284\n\n\n\n You simply plug the FTDI Basic breakout into the programming socket on the controller board and connect it to a computer using a USB mini-B cable. Make sure to line up the GRN and BLK indicators.\n\n To upload code to your Arduino servo board you need a USB to serial adapter. I use the Sparkfun FTDI Basic 3.3V breakout. You can also use the Adafruit FTDI friend (make sure to set it to 3.3V.) Either adapter will work great (you do have to solder a connector to the bottom of the Sparkfun adapter- you can use either straight or 90 degree pins.)I really like these boards because you can see the LEDs light up when they are transmitting. You also need to make sure you have the latest FTDI drivers on your computer (you can get the most current drivers on the product web pages.)Sparkfun FTDI Basic 3.3V breakoutAdafruit FTDI friendYou simply plug the FTDI Basic breakout into the programming socket on the controller board and connect it to a computer using a USB mini-B cable. Make sure to line up the GRN and BLK indicators. If you're using an Arduino with built in USB then you don't need a USB to serial adapter- it's built into the Arduino board. Just connect it to a computer using a USB cable and you're good to go. \n\n\n\nProgramming environment Now you need to download the Arduino software which is located here:http://arduino.cc/en/Main/Software At the time of this writing I am using Arduino 0018. If you want to use the newer Arduino Uno or Mega2560 then you should use the latest release (0021 at this time) as the Uno and Mega2560 use a different type of USB to serial connection that is not supported by previous versions.\n\n\n\n I also highly recommend reading the Arduino environment guide here:http://arduino.cc/en/Guide/Environment\n\n\n\n The code you will use has several parts:\n\n\n\n 1. Program description/comments-\n\n This is where you say what the program does\n\n\n\n 2. Variable declaration section-\n\n This is where you assign input/output pins, etc.\n\n\n\n 3. Setup section-\n\n This is where you set pins as inputs or outputs, etc.\n\n\n\n 4. Loop section-\n\n This is the program that will run based on the conditions of your variables and setup sections.\n\n\n\n When your program runs it will first define your variables, then execute the setup section once and will then execute the loop section over and over .\n\n\n\n So what you do is open the Arduino software, add (or write) your code (called a sketch), verify (compile) your code, connect your Arduino to your computer, select the USB/serial connection, select the type of Arduino you're using then upload your code to the Arduino. 1. Open Arduino window and add/write code-\n\n Just open the Arduino program and paste the code example you want to use into the window (or write your own code.)\n\n\n\n 2. Verify-\n\n Hit the verify button to compile your code. It will inform you if there are any errors with your code. 3. Connect board-\n\n Connect the servo board to your computer using the USB to serial adapter- if you are using an Arduino with built in USB then just plug the Arduino directly into your computer. 4. Select connection-\n\n This tells the USB to serial adapter which serial port you are going to use. The one to select is labeled beginning /dev/tty.usbserial so from the top menu go to Tools>Serial Port>/dev/tty.usbserial-(insert port name here) 5. Select board-\n\n This tells the Arduino program which version board you are using. From the top menu go to Tools>Board>Arduino Pro or Pro Mini (3.3V, 8Mhz) w/ ATmega328 if you are using the Pro Mini servo board or choose the correct model Arduino. 6. Upload code-\n\n Hit the upload button to send the code to your Arduino.\n\n\n\n That's it!\n\n\n\n\n\n\n\nInputs and outputs Now we need to connect a few devices like servos, sensors and LEDs to our controller. The controller has inputs and outputs. Things like sensors and switches are input devices, while servos, LEDs and motors are output devices. The inputs and outputs are both analog and digital- a digital input is like a switch, so it's either on or off. Analog inputs are variable- it's more like a dimmer switch that gives you a range of values.\n\n Digital outputs are similar- if the controller output pin is set HIGH then it's on. If it's set LOW, then it's off. This is great if you want to turn on a motor or LED. If you want to change the brightness of an LED or make a servo motor move then you want to make the controller output pin an analog output. This is done using PWM (pulsewidth modulation.) PWM simply allows the controller to fake an analog voltage output by setting the output pin HIGH and then setting the output pin LOW within a few microseconds or milliseconds of each other. If you pulse the pin HIGH for the same length of time you pulse it LOW you would get an average voltage of half the total voltage so the output pin would give you 1.6V instead of 3.3V. The amount of time the pin stays HIGH is called pulsewidth. The ratio of time for the pin to go from LOW to HIGH to LOW is called duty cycle. If you shorten the amount of time the pin stays HIGH relative to the amount of time it stays LOW you will effectively lower the output pin voltage. It really sounds more complicated than it is but this will come in really handy later on when you want make LEDs dim or make a servo move. Fortunately most of this complex stuff is done for you in the Arduino code libraries but it's still really good to know.\n\n There are all kinds of sensors- bend sensors, force sensitive resistors, accelerometers, potentiometers, joysticks, etc.\n\n\n\n These analog sensors change their output voltage according to how you use them. In the examples we'll use button switches to turn things on and off and we'll use joysticks (potentiometers), bend sensors and accelerometers to make servos move.\n\n\n\n When designing an animatronic system for costuming I try to match the type of sensor used with a specific body motion. Think about how the person wearing the costume is going to use it. Bend sensors are great if you want to make a LED dim or servo move by bending your finger. For even more control I can place a small joystick on a fingertip and use that to make a servo move. For a head tracking system that makes servos follow your head movement I use an accelerometer (from a Wii nunchuck) and I use fingertip switches to trigger sound effects. You'll see how these work in the examples.\n\n\n\n Sparkfun has a good size momentary push button switch that is breadboard friendly-\n\nhttp://www.sparkfun.com/products/9190\n\n Here's the smaller version-\n\nhttp://www.sparkfun.com/products/97\n\n All of the sensors we'll use are connected to the Arduino input pins. A potentiometer is a device commonly used in an application like a stereo volume knob- it's a type of variable resistor. If you supply the potentiometer with 3.3V when you turn the knob the output voltage will range from 0 to 3.3V. A joystick is simply two potentiometers in a common housing- one for the X axis and one for the Y axis.\n\n\n\n Sparkfun has a 10K potentiometer-\n\nhttp://www.sparkfun.com/products/9939\n\n\n\n They also have a couple of small joysticks-\n\nhttp://www.sparkfun.com/products/9032\n\nhttp://www.sparkfun.com/products/9426\n\n\n\n A bend sensor is a resistor that changes its resistance value according to how much you bend it. By adding another resistor and creating a voltage divider, we can change the output voltage of the bend sensor to match the degree of bend. The only real drawback to bend sensors is that they don't have the wide range that a potentiometer has.\n\n\n\n Sparkfun sells a bend sensor here-\n\nhttp://www.sparkfun.com/products/8606\n\n\n\n Accelerometers work by sensing a change in acceleration and then they alter their output relative to the change in acceleration. When you tilt an accelerometer it measures acceleration due to gravity- the more you tilt it the greater the change in output. Accelerometers are commonly used in video game controllers and cell phones.\n\n\n\n A Wii nunchuck has a 3 axis accelerometer, joystick and two pushbuttons for $20. Motors\n\n\n\n Servos\n\n\n\n Hobby servos are small geared motors that have a circuit board and potentiometer to control their rotation. This allows them to be able to move to an exact position relative to your input sensor signal. Most servos can move nearly 180 degrees and some can even do multiple rotations as well as continuous rotation. Servos have three wires- ground, power and signal. The signal wire (usually yellow or white) is connected to the Arduino output pin. The power and ground wires are connected to a separate power source, usually ranging anywhere from 4.8V to 6V. The reason for connecting servos to their own power supply is that motors generate a fair bit of electrical noise, which can cause glitches or a stuttering effect in their movement.\n\n If you have an input sensor that generates an input voltage from 0-3.3V the Arduino takes that analog voltage and assigns it a value from 0-1023 using an analog to digital converter (ADC.) The code on the Arduino then tells the servo how far to move based upon the converted value. So if your sensor outputs 1.65V then you would get a reading of 511 and your servo would move half of its rotation. Many Arduino boards operate on 5V so the same sensor at the same position would read 2.5V and the servo would still rotate half way. A continuous rotation servo would rotate in one direction, stop as the sensor gave a 1.65V reading and then reverse direction as you caused to sensor to raise the input voltage.\n\n Controlling a servo is done by PWM. You send a send a pulse to the servo on the servo signal line every 20 milliseconds. The pulsewidth tells the servo what position to move to. Most servos operate within a 1 to 2 millisecond pulse range so a 1 millisecond pulse tells the servo to move to the 0 degree position and a 2 millisecond pulse tells the servo to move to the 180 degree position. Any pulse between 1 and 2 milliseconds tells the servo to move to a position that is proportionate between 0 and 180 degrees.\n\n\n\n I get all my servos here-\n\nhttp://www.servocity.com\n\n Unlike most servo motors DC motors are best used when you need continuous rotation, especially when you want high RPM. Since DC motors can draw a fair amount of power they are connected to the Arduino output pin using a transistor or a PWM speed controller.\n\n\n\n Pololu sells a large variety of small DC motors-\n\nhttp://www.pololu.com/catalog/category/22\n\n I don't usually use stepper motors in my animatronic projects (at least not yet!) but I felt they are worth mentioning. Stepper motors allow for precise positioning as well as continuous rotation and speed control. The drawback to them is that they require a fair bit of electrical power and they're usually significantly larger and heavier than a servo of equal torque rating. Small stepper motors can be salvaged from old printers and scanners. Unlike DC motors stepper motors have multiple individual coils inside that must be activated in a proper sequence in order to get the motor to move. The Arduino controller is able to drive stepper motors using a specific driver chip or transistor array that is capable of energizing each individual coil in the motor. For more information about steppers have a look in the reference section.\n\n Small LEDs are pretty simple to connect to the Arduino- just remember to use a resistor between the Arduino output pin and the resistor cathode to limit the current flow. You can put a resistor on either the anode or cathode of the LED- either way will work. Most of the small 3.3v LEDs will have a forward current of around 20mA so a resistor value around 100 Ohms works pretty well. For accurate resistor value calculations have a look here-\n\nhttp://led.linear1.org/1led.wiz\n\n\n\n For my Iron Man repulsor I made a small 2\" diameter LED board that has 24 PLCC-2 LEDs. You can get the bare PCB here-\n\nhttp://www.batchpcb.com/index.php/Products/41872\n\n\n\n The board uses 24 1206 package SMD 100 Ohm resistors-\n\nhttp://us.element-14.com/vishay-dale/crcw1206100rjnea/resistor-thick-film-100ohm-250mw/dp/59M6948\n\n\n\n I frequently buy PLCC-2 super bright LEDs on eBay at good prices-\n\nhttp://stores.ebay.com/bestshop2008hk\n\n\n\n High power Luxeon LEDs have a much higher current rating and will work best using some type of constant current source to drive them (there are several instructables on this.) A 1 Watt Luxeon LED will have a forward current of 350mA so you cannot connect it directly to an Arduino output pin. Much like a DC motor you will need to connect it to the output pin using a transistor.\n\n\n\n Sparkfun sells Luxeon LEDs and a constant current driver-\n\nhttp://www.sparkfun.com/search/results?term=Luxeon&what=products\n\nhttp://www.sparkfun.com/products/9642\n\n A transistor is basically just an electronic switch. Each Arduino output pin is limited to 40mA output current so we'll use a particular type of transistor known as an NPN Darlington transistor to turn on high current devices. These transistors have three pins- the collector, emitter and base. The base pin is connected to the Arduino output pin using a 1K Ohm resistor. The collector pin is attached to the high power device and the emitter pin is connected to ground. When the Arduino output pin is set HIGH the transistor turns on and allows electricity to complete a circuit.\n\n\n\n For applications that do not have power requirements over 1 Amp I designed a small transistor board that connects to digital out pins 10-13 using ribbon cable and two eight pin IDC connectors. This uses four SOT-23 package SMD transistors and four 1206 package 1k Ohm SMD resistors. The board is really easy to solder.\n\n\n\n Transistor board PCB-\n\nhttp://batchpcb.com/index.php/Products/41936\n\n\n\n SOT-23 NPN Darlington transistors 4 ea-\n\nhttp://us.element-14.com/fairchild-semiconductor/mmbt6427/bipolar-transistor-npn-40v/dp/58K1891\n\n\n\n 1206 SMD 1K Ohm resistors 4 ea-\n\nhttp://us.element-14.com/yageo/rc1206jr-071kl/resistor-thick-film-1kohm-250mw/dp/68R0298\n\n\n\n 2x4 pin IDC connector 2ea-\n\nhttp://www.surplusgizmos.com/8-Pin-2x4-IDC-Ribbon-Cable-COnnector_p_1879.html\n\n\n\n For loads up to 5A I use a TIP 120 transistor in the TO-220 package. These are great for small DC motors and servos. Use a 1K Ohm resistor to connect the transistor base pin to the Arduino output pin.\n\n\n\n I usually buy TIP 120 transistors from my local Radio Shack. They're very easy to get online as well.\n\n To power the Arduino servo board and servos you need two separate power sources- one single cell LiPo battery for the controller and a small 4.8V- 6V battery pack (4AA batteries work just fine) to power servos. The servo board has an additional socket that provides power from the LiPo cell to power low voltage devices like LEDs.\n\n\n\nExample 5- controlling a servo using analog input\n\n\n\n These two examples show how easy it is to control servos using an analog input. You can use any analog input device you want- I'll use a 10k Ohm potentiometer for the example wiring diagram. As you turn the pot (and change its value) the servo moves proportionally.\n\n\n\n The second code example simply extends the first example to control six servos from six inputs. This kind of control comes in really handy if you want to control several servos using bend sensors attached to a glove. This would work really well for controlling an animatronic mask.\n\n\n\n /*\n\n * Example 5\n\n * Servo Control\n\n * This example uses a servos and analog input to move the servo according to the sensor input value\n\n * Honus 2010\n\n */\n\n\n\n #include \"Servo.h\" // include the servo library\n\n\n\n Servo servo1; // creates an instance of the servo object to control a servo\n\n\n\n int analogPin = 0; // the analog pin that the sensor is on\n\n int analogValue = 0; // the value returned from the analog sensor\n\n\n\n int servoPin = 4; // Control pin for servo motor\n\n void setup() {\n\n servo1.attach(servoPin); // attaches the servo on pin 9 to the servo object\n\n }\n\n\n\n void loop()\n\n {\n\n analogValue = analogRead(analogPin); // read the analog input (value between 0 and 1023)\n\n analogValue = map(analogValue, 0, 1023, 0, 179); // map the analog value (0 - 1023) to the angle of the servo (0 - 179)\n\n servo1.write(analogValue); // write the new mapped analog value to set the position of the servo\n\n delay(15); // waits for the servo to get there\n\n }\n\n \n\n\n\nExample 5a- Controlling 6 servos using multiple inputs\n\n\n\n /*\n\n * Example 5a\n\n * Servo Control6\n\n * This example uses 6 servos and analog inputs to move the servos according to the sensor input values\n\n * Honus 2010\n\n */\n\n\n\n #include // include the servo library\n\n\n\n Servo servoMotor1; // creates an instance of the servo object to control a servo\n\n Servo servoMotor2;\n\n Servo servoMotor3;\n\n Servo servoMotor4;\n\n Servo servoMotor5;\n\n Servo servoMotor6;\n\n\n\n int analogPin1 = 0; // the analog pin that the sensor is on\n\n int analogPin2 = 1;\n\n int analogPin3 = 2;\n\n int analogPin4 = 3;\n\n int analogPin5 = 4;\n\n int analogPin6 = 5;\n\n \n\n int analogValue1 = 0; // the value returned from the analog sensor\n\n int analogValue2 = 0;\n\n int analogValue3 = 0;\n\n int analogValue4 = 0;\n\n int analogValue5 = 0;\n\n int analogValue6 = 0;\n\n\n\n int servoPin1 = 4; // Control pin for servo motor\n\n int servoPin2 = 5;\n\n int servoPin3 = 6;\n\n int servoPin4 = 7;\n\n int servoPin5 = 8;\n\n int servoPin6 = 9;\n\n\n\n void setup() {\n\n servoMotor1.attach(servoPin1); // attaches the servo on pin 4 to the servo object\n\n servoMotor2.attach(servoPin2); // attaches the servo on pin 5 to the servo object\n\n servoMotor3.attach(servoPin3); // attaches the servo on pin 6 to the servo object\n\n servoMotor4.attach(servoPin4); // attaches the servo on pin 7 to the servo object\n\n servoMotor5.attach(servoPin5); // attaches the servo on pin 8 to the servo object\n\n servoMotor6.attach(servoPin6); // attaches the servo on pin 9 to the servo object\n\n }\n\n\n\n void loop()\n\n {\n\n analogValue1 = analogRead(analogPin1); // read the analog input (value between 0 and 1023)\n\n analogValue1 = map(analogValue1, 0, 1023, 0, 179); // map the analog value (0 - 1023) to the angle of the servo (0 - 179)\n\n servoMotor1.write(analogValue1); // write the new mapped analog value to set the position of the servo\n\n \n\n analogValue2 = analogRead(analogPin2); \n\n analogValue2 = map(analogValue2, 0, 1023, 0, 179); \n\n servoMotor2.write(analogValue2); \n\n \n\n analogValue3 = analogRead(analogPin3); \n\n analogValue3 = map(analogValue3, 0, 1023, 0, 179); \n\n servoMotor3.write(analogValue3); \n\n \n\n analogValue4 = analogRead(analogPin4); \n\n analogValue4 = map(analogValue4, 0, 1023, 0, 179); \n\n servoMotor4.write(analogValue4); \n\n \n\n analogValue5 = analogRead(analogPin5); \n\n analogValue5 = map(analogValue5, 0, 1023, 0, 179); \n\n servoMotor5.write(analogValue5); \n\n \n\n analogValue6 = analogRead(analogPin6); \n\n analogValue6 = map(analogValue6, 0, 1023, 0, 179); \n\n servoMotor6.write(analogValue6); \n\n \n\n delay(15); // waits for the servo to get there\n\n }\n\n\n\n\n\n\n\nExample 6- Using a Wii nunchuck as an input device\n\n\n\n I wrote this bit of code back in 2007 to use a Wii nunchuck as an input device for an animatronic Predator cannon (see example 7.) The Wii nunchuck communicates to an Arduino over four wires (power, ground, data and clock) using an I²C interface (Inter-Integrated Circuit aka two-wire interface or TWI.)\n\n\n\n The Wii nunchuck has a three axis accelerometer, joystick and two push buttons- for $20 it's an awesome input device for Arduino projects. The code presented here is a further modification of the code by Tod Kurt that was presented in his Bionic Arduino class- I simply extended it to control everything but the accelerometer Z axis, which I found I rarely used.\n\n\n\n Using this code you can control four servos using the accelerometer and joystick functions and use the two push buttons to turn on LEDs (or transistors or even run a bit of code.)\n\n\n\n\n\n /*\n\n * Example 6\n\n * Nunchuck control for four servos and two button inputs\n\n * Honus 2007\n\n * This allows the use of a Wii nunchuck as an input device and is modified/extended from the original code\n\n * by Tod E. Kurt and Windmeadow Labs\n\n *2007 Tod E. Kurt, http://todbot.com/blog/\n\n *The Wii Nunchuck reading code is taken from Windmeadow Labs, http://www.windmeadow.com/node/42\n\n */\n\n\n\n #include \"Wire.h\" \n\n\n\n int ledPin1 = 13; // Control pin for LED 1\n\n int ledPin2 = 12; // Control pin for LED 2\n\n int servoPin1 = 9; // Control pin for servo motor\n\n int servoPin2 = 8; // Control pin for servo motor\n\n int servoPin3 = 7; // Control pin for servo motor\n\n int servoPin4 = 6; // Control pin for servo motor\n\n\n\n int pulseWidth1 = 0; // Amount to pulse the servo 1\n\n int pulseWidth2 = 0; // Amount to pulse the servo 2\n\n int pulseWidth3 = 0; // Amount to pulse the servo 3\n\n int pulseWidth4 = 0; // Amount to pulse the servo 4\n\n\n\n int refreshTime = 20; // the time in millisecs needed in between pulses\n\n\n\n long lastPulse1;\n\n long lastPulse2;\n\n long lastPulse3;\n\n long lastPulse4;\n\n\n\n int minPulse = 700; // minimum pulse width\n\n int loop_cnt=0;\n\n\n\n void setup()\n\n {\n\n Serial.begin(19200);\n\n pinMode(servoPin1, OUTPUT); // Set servo pin as an output pin\n\n pinMode(servoPin2, OUTPUT); // Set servo pin as an output pin\n\n pinMode(servoPin3, OUTPUT); // Set servo pin as an output pin\n\n pinMode(servoPin4, OUTPUT); // Set servo pin as an output pin\n\n \n\n pulseWidth1 = minPulse; // Set the motor position to the minimum\n\n pulseWidth2 = minPulse; // Set the motor position to the minimum\n\n pulseWidth3 = minPulse; // Set the motor position to the minimum\n\n pulseWidth4 = minPulse; // Set the motor position to the minimum\n\n\n\n nunchuck_init(); // send the initilization handshake\n\n Serial.print(\"NunchuckServo ready\n\n\");\n\n }\n\n\n\n void loop()\n\n {\n\n checkNunchuck1();\n\n updateServo1(); // update servo 1 position\n\n checkNunchuck2();\n\n updateServo2(); // update servo 2 position\n\n checkNunchuck3();\n\n updateServo3(); // update servo 3 position\n\n checkNunchuck4();\n\n updateServo4(); // update servo 4 position\n\n\n\n if( nunchuck_zbutton() ) // light the LED if z button is pressed\n\n digitalWrite(ledPin1, HIGH);\n\n else\n\n digitalWrite(ledPin1,LOW);\n\n \n\n if( nunchuck_cbutton() ) // light the LED if c button is pressed\n\n digitalWrite(ledPin2, HIGH);\n\n else\n\n digitalWrite(ledPin2,LOW);\n\n\n\n delay(1); // this is here to give a known time per loop\n\n }\n\n\n\n\n\n void checkNunchuck1()\n\n {\n\n if( loop_cnt > 100 ) { // loop()s is every 1msec, this is every 100msec\n\n \n\n nunchuck_get_data();\n\n nunchuck_print_data();\n\n\n\n float tilt = nunchuck_accelx(); // x-axis, in this case ranges from ~70 - ~185\n\n tilt = (tilt - 70) * 1.5; // convert to angle in degrees, roughly \n\n pulseWidth1 = (tilt * 9) + minPulse; // convert angle to microseconds\n\n \n\n loop_cnt = 0; // reset for\n\n }\n\n loop_cnt++;\n\n \n\n }\n\n\n\n // called every loop().\n\n // uses global variables servoPin, pulsewidth, lastPulse, & refreshTime\n\n void updateServo1()\n\n {\n\n // pulse the servo again if rhe refresh time (20 ms) have passed:\n\n if (millis() - lastPulse1 >= refreshTime) {\n\n digitalWrite(servoPin1, HIGH); // Turn the motor on\n\n delayMicroseconds(pulseWidth1); // Length of the pulse sets the motor position\n\n digitalWrite(servoPin1, LOW); // Turn the motor off\n\n lastPulse1 = millis(); // save the time of the last pulse\n\n }\n\n }\n\n\n\n void checkNunchuck2()\n\n {\n\n if( loop_cnt > 100 ) { // loop()s is every 1msec, this is every 100msec\n\n \n\n nunchuck_get_data();\n\n nunchuck_print_data();\n\n\n\n float tilt = nunchuck_accely(); // y-axis, in this case ranges from ~70 - ~185\n\n tilt = (tilt - 70) * 1.5; // convert to angle in degrees, roughly \n\n pulseWidth2 = (tilt * 9) + minPulse; // convert angle to microseconds\n\n \n\n loop_cnt = 0; // reset for\n\n }\n\n loop_cnt++;\n\n \n\n }\n\n\n\n // called every loop().\n\n // uses global variables servoPin, pulsewidth, lastPulse, & refreshTime\n\n void updateServo2()\n\n {\n\n // pulse the servo again if rhe refresh time (20 ms) have passed:\n\n if (millis() - lastPulse2 >= refreshTime) {\n\n digitalWrite(servoPin2, HIGH); // Turn the motor on\n\n delayMicroseconds(pulseWidth2); // Length of the pulse sets the motor position\n\n digitalWrite(servoPin2, LOW); // Turn the motor off\n\n lastPulse2 = millis(); // save the time of the last pulse\n\n }\n\n }\n\n\n\n void checkNunchuck3()\n\n {\n\n if( loop_cnt > 100 ) { // loop()s is every 1msec, this is every 100msec\n\n \n\n nunchuck_get_data();\n\n nunchuck_print_data();\n\n\n\n float tilt = nunchuck_joyx(); // x-axis, in this case ranges from ~70 - ~185\n\n tilt = (tilt - 70) * 1.5; // convert to angle in degrees, roughly \n\n pulseWidth3 = (tilt * 9) + minPulse; // convert angle to microseconds\n\n \n\n loop_cnt = 0; // reset for\n\n }\n\n loop_cnt++;\n\n \n\n }\n\n\n\n // called every loop().\n\n // uses global variables servoPin, pulsewidth, lastPulse, & refreshTime\n\n void updateServo3()\n\n {\n\n // pulse the servo again if rhe refresh time (20 ms) have passed:\n\n if (millis() - lastPulse3 >= refreshTime) {\n\n digitalWrite(servoPin3, HIGH); // Turn the motor on\n\n delayMicroseconds(pulseWidth3); // Length of the pulse sets the motor position\n\n digitalWrite(servoPin3, LOW); // Turn the motor off\n\n lastPulse3 = millis(); // save the time of the last pulse\n\n }\n\n }\n\n\n\n void checkNunchuck4()\n\n {\n\n if( loop_cnt > 100 ) { // loop()s is every 1msec, this is every 100msec\n\n \n\n nunchuck_get_data();\n\n nunchuck_print_data();\n\n\n\n float tilt = nunchuck_joyy(); // y-axis, in this case ranges from ~70 - ~185\n\n tilt = (tilt - 70) * 1.5; // convert to angle in degrees, roughly \n\n pulseWidth4 = (tilt * 9) + minPulse; // convert angle to microseconds\n\n \n\n loop_cnt = 0; // reset for\n\n }\n\n loop_cnt++;\n\n \n\n }\n\n\n\n // called every loop().\n\n // uses global variables servoPin, pulsewidth, lastPulse, & refreshTime\n\n void updateServo4()\n\n {\n\n // pulse the servo again if rhe refresh time (20 ms) have passed:\n\n if (millis() - lastPulse4 >= refreshTime) {\n\n digitalWrite(servoPin4, HIGH); // Turn the motor on\n\n delayMicroseconds(pulseWidth4); // Length of the pulse sets the motor position\n\n digitalWrite(servoPin4, LOW); // Turn the motor off\n\n lastPulse4 = millis(); // save the time of the last pulse\n\n }\n\n }\n\n\n\n\n\n //\n\n // Nunchuck functions\n\n //\n\n\n\n static uint8_t nunchuck_buf[6]; // array to store nunchuck data,\n\n\n\n\n\n // initialize the I2C system, join the I2C bus,\n\n // and tell the nunchuck we're talking to it\n\n void nunchuck_init()\n\n {\n\n Wire.begin(); // join i2c bus as master\n\n Wire.beginTransmission(0x52); // transmit to device 0x52\n\n Wire.send(0x40); // sends memory address\n\n Wire.send(0x00); // sends sent a zero. \n\n Wire.endTransmission(); // stop transmitting\n\n }\n\n\n\n // Send a request for data to the nunchuck\n\n // was \"send_zero()\"\n\n void nunchuck_send_request()\n\n {\n\n Wire.beginTransmission(0x52); // transmit to device 0x52\n\n Wire.send(0x00); // sends one byte\n\n Wire.endTransmission(); // stop transmitting\n\n }\n\n\n\n // Receive data back from the nunchuck,\n\n // returns 1 on successful read. returns 0 on failure\n\n int nunchuck_get_data()\n\n {\n\n int cnt=0;\n\n Wire.requestFrom (0x52, 6); // request data from nunchuck\n\n while (Wire.available ()) {\n\n // receive byte as an integer\n\n nunchuck_buf[cnt] = nunchuk_decode_byte(Wire.receive());\n\n cnt++;\n\n }\n\n nunchuck_send_request(); // send request for next data payload\n\n // If we recieved the 6 bytes, then go print them\n\n if (cnt >= 5) {\n\n return 1; // success\n\n }\n\n return 0; //failure\n\n }\n\n\n\n // Print the input data we have recieved\n\n // accel data is 10 bits long\n\n // so we read 8 bits, then we have to add\n\n // on the last 2 bits. That is why I\n\n // multiply them by 2 * 2\n\n void nunchuck_print_data()\n\n {\n\n static int i=0;\n\n int joy_x_axis = nunchuck_buf[0];\n\n int joy_y_axis = nunchuck_buf[1];\n\n int accel_x_axis = nunchuck_buf[2]; // * 2 * 2;\n\n int accel_y_axis = nunchuck_buf[3]; // * 2 * 2;\n\n int accel_z_axis = nunchuck_buf[4]; // * 2 * 2;\n\n\n\n int z_button = 0;\n\n int c_button = 0;\n\n\n\n // byte nunchuck_buf[5] contains bits for z and c buttons\n\n // it also contains the least significant bits for the accelerometer data\n\n // so we have to check each bit of byte outbuf[5]\n\n if ((nunchuck_buf[5] >> 0) & 1)\n\n z_button = 1;\n\n if ((nunchuck_buf[5] >> 1) & 1)\n\n c_button = 1;\n\n\n\n if ((nunchuck_buf[5] >> 2) & 1)\n\n accel_x_axis += 2;\n\n if ((nunchuck_buf[5] >> 3) & 1)\n\n accel_x_axis += 1;\n\n\n\n if ((nunchuck_buf[5] >> 4) & 1)\n\n accel_y_axis += 2;\n\n if ((nunchuck_buf[5] >> 5) & 1)\n\n accel_y_axis += 1;\n\n\n\n if ((nunchuck_buf[5] >> 6) & 1)\n\n accel_z_axis += 2;\n\n if ((nunchuck_buf[5] >> 7) & 1)\n\n accel_z_axis += 1;\n\n\n\n Serial.print(i,DEC);\n\n Serial.print(\"\\t\");\n\n\n\n Serial.print(\"joy:\");\n\n Serial.print(joy_x_axis,DEC);\n\n Serial.print(\",\");\n\n Serial.print(joy_y_axis, DEC);\n\n Serial.print(\" \\t\");\n\n\n\n Serial.print(\"acc:\");\n\n Serial.print(accel_x_axis, DEC);\n\n Serial.print(\",\");\n\n Serial.print(accel_y_axis, DEC);\n\n Serial.print(\",\");\n\n Serial.print(accel_z_axis, DEC);\n\n Serial.print(\"\\t\");\n\n\n\n Serial.print(\"but:\");\n\n Serial.print(z_button, DEC);\n\n Serial.print(\",\");\n\n Serial.print(c_button, DEC);\n\n\n\n Serial.print(\"\\r\n\n\"); // newline\n\n i++;\n\n }\n\n\n\n // Encode data to format that most wiimote drivers except\n\n // only needed if you use one of the regular wiimote drivers\n\n char nunchuk_decode_byte (char x)\n\n {\n\n x = (x ^ 0x17) + 0x17;\n\n return x;\n\n }\n\n\n\n // returns zbutton state: 1=pressed, 0=notpressed\n\n int nunchuck_zbutton()\n\n {\n\n return ((nunchuck_buf[5] >> 0) & 1) ? 0 : 1; // voodoo\n\n }\n\n\n\n // returns zbutton state: 1=pressed, 0=notpressed\n\n int nunchuck_cbutton()\n\n {\n\n return ((nunchuck_buf[5] >> 1) & 1) ? 0 : 1; // voodoo\n\n }\n\n\n\n // returns value of x-axis joystick\n\n int nunchuck_joyx()\n\n {\n\n return nunchuck_buf[0];\n\n }\n\n\n\n // returns value of y-axis joystick\n\n int nunchuck_joyy()\n\n {\n\n return nunchuck_buf[1];\n\n }\n\n\n\n // returns value of x-axis accelerometer\n\n int nunchuck_accelx()\n\n {\n\n return nunchuck_buf[2]; // FIXME: this leaves out 2-bits of the data\n\n }\n\n\n\n // returns value of y-axis accelerometer\n\n int nunchuck_accely()\n\n {\n\n return nunchuck_buf[3]; // FIXME: this leaves out 2-bits of the data\n\n }\n\n\n\n // returns value of z-axis accelerometer\n\n int nunchuck_accelz()\n\n {\n\n return nunchuck_buf[4]; // FIXME: this leaves out 2-bits of the data\n\n }\n\n\n\n\n\n\n\nExample 7- Predator cannon\n\n\n\n Using a modified Wii nunchuck board we can make a \"head tracking\" system to control an animatronic Predator cannon. This system was designed to look like the cannon mechanism in the first Predator movie.\n\n\n\n The nunchuck board is removed from its case, the joystick is removed and the board is placed level in the top of the Predator Bio helmet. The wires are extended for the buttons so they can be used as fingertip buttons to activate the cannon and trigger the firing sequence.\n\n\n\n To remove the circuit board from the Wii nunchuck case you'll need a tri-wing screwdriver-\n\nhttp://www.play-asia.com/paOS-13-71-1e-49-en-70-1fe.html\n\n\n\n The sound effect is handled just like the Iron Man repulsor in example 4 using the Adafruit Wave Shield. Since the Wave Shield code used can support six individual sounds you can add five other Predator sounds and activate them using fingertip switches- neat!\n\n\n\n There is one servo that is geared 4:1 that raises the cannon arm- in the code you can see this as the servo rotating 180 degrees, thereby raising the cannon arm 45 degrees. The other two servos aim the cannon using the accelerometer inputs. There are transistors that turn on the aiming servos and laser sight when one button is pressed. If the aiming servos were always on then the cannon would rotate even when it was in the lowered position, so they need a way of being turned off when the cannon is lowered.\n\n\n\n So push one one button and the cannon raises up, the aiming servos turn on and the laser sight turns on. Push the second button and the cannon fires- two transistors turn on the cannon LED and activate the firing sound. Three red LEDs can be used in place of the laser sight. The cannon LED can be anything from several small LEDs to a high power Luxeon LED. When using a high power Luxeon LED be sure to use a constant current driver to power it.\n\n\n\n Servos can draw a fair bit of power so I use a TIP 120 transistor to turn on the aiming servos.\n\n\n\n The prototype cannon mechanism was built using Delrin plastic scraps and timing belts and gears from old desktop printers and photocopiers I found in the trash. When I build the final version for the Predator costume it will probably be entirely gear driven to make it more compact and cleaner.\n\n\n\n\n\n\n\n For Predator costuming info check out \n\n\n\n Those individuals interested in obtaining a resin cannon casting should contact my friend Carl here- \n\n\n\n Carl's work is absolutely brilliant- check out the photos below of the Predator backpack clay sculpt that he created for this project. That's a lot of clay! When contacting Carl please be patient as he's extremely busy and he has a large backlog of work.\n\n\n\n\n\n Here's the code-\n\n\n\n\n\n /*\n\n * Example 7\n\n * Predator Cannon\n\n * This uses a modified Wii nunchuck as a head tracking input device to control an animatronic Predator cannon\n\n * Adafruit Wave shield is used for sound effects\n\n * Honus 2007, updated 2010\n\n * Wii nunchuck reading code modified/extended from nunchuck code by Tod E. Kurt and Windmeadow Labs\n\n * 2007 Tod E. Kurt, http://todbot.com/blog/\n\n * The Wii Nunchuck reading code is taken from Windmeadow Labs, http://www.windmeadow.com/node/42\n\n */\n\n\n\n \n\n #include \"Wire.h\" // include the Wire library\n\n #include \"Servo.h\" // include the servo library\n\n\n\n Servo servo3; // creates an instance of the servo object to control a servo\n\n\n\n int controlPin1 = 6; // Control pin for sound effects board using z button\n\n int transistorPin1 = 13; // Control pin for LED using z button\n\n int transistorPin2 = 12; // Control pin for laser sight using c button\n\n int transistorPin3 = 11; // Control pin for servo 1 using c button\n\n int transistorPin4 = 10; // Control pin for servo 2 using c button\n\n int servoPin1 = 7; // Control pin for servo 1 using accelerometer x axis\n\n int servoPin2 = 8; // Control pin for servo 2 using accelerometer y axis\n\n int servoPin3 = 9; // control pin for arm servo\n\n\n\n int pulseWidth1 = 0; // Amount to pulse the servo 1\n\n int pulseWidth2 = 0; // Amount to pulse the servo 2\n\n\n\n\n\n int refreshTime = 20; // the time in millisecs needed in between servo pulses\n\n long lastPulse1;\n\n long lastPulse2;\n\n\n\n int minPulse = 700; // minimum servo pulse width\n\n int loop_cnt=0;\n\n\n\n boolean button_down = false;\n\n unsigned long start;\n\n\n\n\n\n void setup()\n\n {\n\n Serial.begin(19200);\n\n\n\n servo3.attach(servoPin3); // attaches the servo on pin 9 to the servo object\n\n \n\n pinMode(controlPin1, OUTPUT); // Set control pin 1 as output\n\n pinMode(transistorPin1, OUTPUT); // Set transistor pin 1 as output\n\n pinMode(transistorPin2, OUTPUT); // Set transistor pin 2 as output\n\n pinMode(transistorPin3, OUTPUT); // Set transistor pin 3 as output\n\n pinMode(transistorPin4, OUTPUT); // Set transistor pin 4 as output\n\n pinMode(servoPin1, OUTPUT); // Set servo pin 1 as output\n\n pinMode(servoPin2, OUTPUT); // Set servo pin 2 as output\n\n\n\n pulseWidth1 = minPulse; // Set the servo position to the minimum\n\n pulseWidth2 = minPulse; // Set the servo position to the minimum\n\n\n\n nunchuck_init(); // send the initilization handshake\n\n Serial.print(\"NunchuckServo ready\n\n\");\n\n }\n\n\n\n void loop()\n\n {\n\n checkNunchuck1();\n\n updateServo1(); // update servo 1 position\n\n checkNunchuck2();\n\n updateServo2(); // update servo 2 position\n\n\n\n\n\n if( nunchuck_cbutton() ) {\n\n digitalWrite(transistorPin2, HIGH); // turn on transistor pin 2 if c button is pressed\n\n digitalWrite(transistorPin3, HIGH); // turn on transistor pin 3 if c button is pressed\n\n digitalWrite(transistorPin4, HIGH); // turn on transistor pin 4 if c button is pressed\n\n\n\n servo3.write(180);\n\n }\n\n else {\n\n digitalWrite(transistorPin2, LOW);\n\n digitalWrite(transistorPin3, LOW);\n\n digitalWrite(transistorPin4, LOW);\n\n servo3.write(0);\n\n }\n\n\n\n if ( nunchuck_zbutton() )\n\n {\n\n if (!button_down) // if button was just pressed do this\n\n {\n\n digitalWrite(controlPin1, HIGH);\n\n button_down = true;\n\n start = millis();\n\n }\n\n else if (millis() - start > 1200) // if timer has elapsed do this\n\n {\n\n digitalWrite(transistorPin1, HIGH);\n\n }\n\n }\n\n else // if button is up do this\n\n {\n\n button_down = false;\n\n digitalWrite(controlPin1, LOW);\n\n digitalWrite(transistorPin1, LOW);\n\n }\n\n\n\n\n\n delay(1); // this is here to give a known time per loop\n\n }\n\n\n\n\n\n void checkNunchuck1()\n\n {\n\n if( loop_cnt > 100 ) { // loop()s is every 1msec, this is every 100msec\n\n\n\n nunchuck_get_data();\n\n nunchuck_print_data();\n\n\n\n float tilt = nunchuck_accelx(); // x-axis, in this case ranges from ~70 - ~185\n\n tilt = (tilt - 70) * 1.5; // convert to angle in degrees, roughly\n\n pulseWidth1 = (tilt * 9) + minPulse; // convert angle to microseconds\n\n\n\n loop_cnt = 0; // reset for\n\n }\n\n loop_cnt++;\n\n\n\n }\n\n\n\n // called every loop().\n\n // uses global variables servoPin, pulsewidth, lastPulse, & refreshTime\n\n void updateServo1()\n\n {\n\n // pulse the servo again if rhe refresh time (20 ms) have passed:\n\n if (millis() - lastPulse1 >= refreshTime) {\n\n digitalWrite(servoPin1, HIGH); // Turn the servo on\n\n delayMicroseconds(pulseWidth1); // Length of the pulse sets the servo position\n\n digitalWrite(servoPin1, LOW); // Turn the servo off\n\n lastPulse1 = millis(); // save the time of the last pulse\n\n }\n\n }\n\n\n\n void checkNunchuck2()\n\n {\n\n if( loop_cnt > 100 ) { // loop()s is every 1msec, this is every 100msec\n\n\n\n nunchuck_get_data();\n\n nunchuck_print_data();\n\n\n\n float tilt = nunchuck_accely(); // y-axis, in this case ranges from ~70 - ~185\n\n tilt = (tilt - 70) * 1.5; // convert to angle in degrees, roughly\n\n pulseWidth2 = (tilt * 9) + minPulse; // convert angle to microseconds\n\n\n\n loop_cnt = 0; // reset for\n\n }\n\n loop_cnt++;\n\n\n\n }\n\n\n\n // called every loop().\n\n // uses global variables servoPin, pulsewidth, lastPulse, & refreshTime\n\n void updateServo2()\n\n {\n\n // pulse the servo again if rhe refresh time (20 ms) have passed:\n\n if (millis() - lastPulse2 >= refreshTime) {\n\n digitalWrite(servoPin2, HIGH); // Turn the servo on\n\n delayMicroseconds(pulseWidth2); // Length of the pulse sets the servo position\n\n digitalWrite(servoPin2, LOW); // Turn the servo off\n\n lastPulse2 = millis(); // save the time of the last pulse\n\n }\n\n }\n\n\n\n //\n\n // Nunchuck functions\n\n //\n\n\n\n static uint8_t nunchuck_buf[6]; // array to store nunchuck data,\n\n\n\n // initialize the I2C system, join the I2C bus,\n\n // and tell the nunchuck we're talking to it\n\n void nunchuck_init()\n\n {\n\n Wire.begin(); // join i2c bus as master\n\n Wire.beginTransmission(0x52); // transmit to device 0x52\n\n Wire.send(0x40); // sends memory address\n\n Wire.send(0x00); // sends sent a zero.\n\n Wire.endTransmission(); // stop transmitting\n\n }\n\n\n\n // Send a request for data to the nunchuck\n\n // was \"send_zero()\"\n\n void nunchuck_send_request()\n\n {\n\n Wire.beginTransmission(0x52); // transmit to device 0x52\n\n Wire.send(0x00); // sends one byte\n\n Wire.endTransmission(); // stop transmitting\n\n }\n\n\n\n // Receive data back from the nunchuck,\n\n // returns 1 on successful read. returns 0 on failure\n\n int nunchuck_get_data()\n\n {\n\n int cnt=0;\n\n Wire.requestFrom (0x52, 6); // request data from nunchuck\n\n while (Wire.available ()) {\n\n // receive byte as an integer\n\n nunchuck_buf[cnt] = nunchuk_decode_byte(Wire.receive());\n\n cnt++;\n\n }\n\n nunchuck_send_request(); // send request for next data payload\n\n // If we recieved the 6 bytes, then go print them\n\n if (cnt >= 5) {\n\n return 1; // success\n\n }\n\n return 0; //failure\n\n }\n\n\n\n // Print the input data we have recieved\n\n // accel data is 10 bits long\n\n // so we read 8 bits, then we have to add\n\n // on the last 2 bits. That is why I\n\n // multiply them by 2 * 2\n\n void nunchuck_print_data()\n\n {\n\n static int i=0;\n\n int joy_x_axis = nunchuck_buf[0];\n\n int joy_y_axis = nunchuck_buf[1];\n\n int accel_x_axis = nunchuck_buf[2]; // * 2 * 2;\n\n int accel_y_axis = nunchuck_buf[3]; // * 2 * 2;\n\n int accel_z_axis = nunchuck_buf[4]; // * 2 * 2;\n\n\n\n int z_button = 0;\n\n int c_button = 0;\n\n\n\n // byte nunchuck_buf[5] contains bits for z and c buttons\n\n // it also contains the least significant bits for the accelerometer data\n\n // so we have to check each bit of byte outbuf[5]\n\n if ((nunchuck_buf[5] >> 0) & 1)\n\n z_button = 1;\n\n if ((nunchuck_buf[5] >> 1) & 1)\n\n c_button = 1;\n\n\n\n if ((nunchuck_buf[5] >> 2) & 1)\n\n accel_x_axis += 2;\n\n if ((nunchuck_buf[5] >> 3) & 1)\n\n accel_x_axis += 1;\n\n\n\n if ((nunchuck_buf[5] >> 4) & 1)\n\n accel_y_axis += 2;\n\n if ((nunchuck_buf[5] >> 5) & 1)\n\n accel_y_axis += 1;\n\n\n\n if ((nunchuck_buf[5] >> 6) & 1)\n\n accel_z_axis += 2;\n\n if ((nunchuck_buf[5] >> 7) & 1)\n\n accel_z_axis += 1;\n\n\n\n Serial.print(i,DEC);\n\n Serial.print(\"\\t\");\n\n\n\n Serial.print(\"joy:\");\n\n Serial.print(joy_x_axis,DEC);\n\n Serial.print(\",\");\n\n Serial.print(joy_y_axis, DEC);\n\n Serial.print(\" \\t\");\n\n\n\n Serial.print(\"acc:\");\n\n Serial.print(accel_x_axis, DEC);\n\n Serial.print(\",\");\n\n Serial.print(accel_y_axis, DEC);\n\n Serial.print(\",\");\n\n Serial.print(accel_z_axis, DEC);\n\n Serial.print(\"\\t\");\n\n\n\n Serial.print(\"but:\");\n\n Serial.print(z_button, DEC);\n\n Serial.print(\",\");\n\n Serial.print(c_button, DEC);\n\n\n\n Serial.print(\"\\r\n\n\"); // newline\n\n i++;\n\n }\n\n\n\n // Encode data to format that most wiimote drivers except\n\n // only needed if you use one of the regular wiimote drivers\n\n char nunchuk_decode_byte (char x)\n\n {\n\n x = (x ^ 0x17) + 0x17;\n\n return x;\n\n }\n\n\n\n // returns zbutton state: 1=pressed, 0=notpressed\n\n int nunchuck_zbutton()\n\n {\n\n return ((nunchuck_buf[5] >> 0) & 1) ? 0 : 1; // voodoo\n\n }\n\n\n\n // returns zbutton state: 1=pressed, 0=notpressed\n\n int nunchuck_cbutton()\n\n {\n\n return ((nunchuck_buf[5] >> 1) & 1) ? 0 : 1; // voodoo\n\n }\n\n\n\n // returns value of x-axis joystick\n\n int nunchuck_joyx()\n\n {\n\n return nunchuck_buf[0];\n\n }\n\n\n\n // returns value of y-axis joystick\n\n int nunchuck_joyy()\n\n {\n\n return nunchuck_buf[1];\n\n }\n\n\n\n // returns value of x-axis accelerometer\n\n int nunchuck_accelx()\n\n {\n\n return nunchuck_buf[2]; // FIXME: this leaves out 2-bits of the data\n\n }\n\n\n\n // returns value of y-axis accelerometer\n\n int nunchuck_accely()\n\n {\n\n return nunchuck_buf[3]; // FIXME: this leaves out 2-bits of the data\n\n }\n\n\n\n // returns value of z-axis accelerometer\n\n int nunchuck_accelz()\n\n {\n\n return nunchuck_buf[4]; // FIXME: this leaves out 2-bits of the data\n\n }\n\n Using a modified Wii nunchuck board we can make a \"head tracking\" system to control an animatronic Predator cannon. This system was designed to look like the cannon mechanism in the first Predator movie.The nunchuck board is removed from its case, the joystick is removed and the board is placed level in the top of the Predator Bio helmet. The wires are extended for the buttons so they can be used as fingertip buttons to activate the cannon and trigger the firing sequence.To remove the circuit board from the Wii nunchuck case you'll need a tri-wing screwdriver-The sound effect is handled just like the Iron Man repulsor in example 4 using the Adafruit Wave Shield. Since the Wave Shield code used can support six individual sounds you can add five other Predator sounds and activate them using fingertip switches- neat!There is one servo that is geared 4:1 that raises the cannon arm- in the code you can see this as the servo rotating 180 degrees, thereby raising the cannon arm 45 degrees. The other two servos aim the cannon using the accelerometer inputs. There are transistors that turn on the aiming servos and laser sight when one button is pressed. If the aiming servos were always on then the cannon would rotate even when it was in the lowered position, so they need a way of being turned off when the cannon is lowered.So push one one button and the cannon raises up, the aiming servos turn on and the laser sight turns on. Push the second button and the cannon fires- two transistors turn on the cannon LED and activate the firing sound. Three red LEDs can be used in place of the laser sight. The cannon LED can be anything from several small LEDs to a high power Luxeon LED. When using a high power Luxeon LED be sure to use a constant current driver to power it.Servos can draw a fair bit of power so I use a TIP 120 transistor to turn on the aiming servos.The prototype cannon mechanism was built using Delrin plastic scraps and timing belts and gears from old desktop printers and photocopiers I found in the trash. When I build the final version for the Predator costume it will probably be entirely gear driven to make it more compact and cleaner.For Predator costuming info check out http://www.thehunterslair.com Those individuals interested in obtaining a resin cannon casting should contact my friend Carl here- http://www.accurizedhunterparts.com/ Carl's work is absolutely brilliant- check out the photos below of the Predator backpack clay sculpt that he created for this project. That's a lot of clay! When contacting Carl please be patient as he's extremely busy and he has a large backlog of work.Here's the code-/** Example 7* Predator Cannon* This uses a modified Wii nunchuck as a head tracking input device to control an animatronic Predator cannon* Adafruit Wave shield is used for sound effects* Honus 2007, updated 2010* Wii nunchuck reading code modified/extended from nunchuck code by Tod E. Kurt and Windmeadow Labs* 2007 Tod E. Kurt, http://todbot.com/blog/* The Wii Nunchuck reading code is taken from Windmeadow Labs, http://www.windmeadow.com/node/42*/#include \"Wire.h\" // include the Wire library#include \"Servo.h\" // include the servo libraryServo servo3; // creates an instance of the servo object to control a servoint controlPin1 = 6; // Control pin for sound effects board using z buttonint transistorPin1 = 13; // Control pin for LED using z buttonint transistorPin2 = 12; // Control pin for laser sight using c buttonint transistorPin3 = 11; // Control pin for servo 1 using c buttonint transistorPin4 = 10; // Control pin for servo 2 using c buttonint servoPin1 = 7; // Control pin for servo 1 using accelerometer x axisint servoPin2 = 8; // Control pin for servo 2 using accelerometer y axisint servoPin3 = 9; // control pin for arm servoint pulseWidth1 = 0; // Amount to pulse the servo 1int pulseWidth2 = 0; // Amount to pulse the servo 2int refreshTime = 20; // the time in millisecs needed in between servo pulseslong lastPulse1;long lastPulse2;int minPulse = 700; // minimum servo pulse widthint loop_cnt=0;boolean button_down = false;unsigned long start;void setup()Serial.begin(19200);servo3.attach(servoPin3); // attaches the servo on pin 9 to the servo objectpinMode(controlPin1, OUTPUT); // Set control pin 1 as outputpinMode(transistorPin1, OUTPUT); // Set transistor pin 1 as outputpinMode(transistorPin2, OUTPUT); // Set transistor pin 2 as outputpinMode(transistorPin3, OUTPUT); // Set transistor pin 3 as outputpinMode(transistorPin4, OUTPUT); // Set transistor pin 4 as outputpinMode(servoPin1, OUTPUT); // Set servo pin 1 as outputpinMode(servoPin2, OUTPUT); // Set servo pin 2 as outputpulseWidth1 = minPulse; // Set the servo position to the minimumpulseWidth2 = minPulse; // Set the servo position to the minimumnunchuck_init(); // send the initilization handshakeSerial.print(\"NunchuckServo ready\n\n\");void loop()checkNunchuck1();updateServo1(); // update servo 1 positioncheckNunchuck2();updateServo2(); // update servo 2 positionif( nunchuck_cbutton() ) {digitalWrite(transistorPin2, HIGH); // turn on transistor pin 2 if c button is presseddigitalWrite(transistorPin3, HIGH); // turn on transistor pin 3 if c button is presseddigitalWrite(transistorPin4, HIGH); // turn on transistor pin 4 if c button is pressedservo3.write(180);else {digitalWrite(transistorPin2, LOW);digitalWrite(transistorPin3, LOW);digitalWrite(transistorPin4, LOW);servo3.write(0);if ( nunchuck_zbutton() )if (!button_down) // if button was just pressed do thisdigitalWrite(controlPin1, HIGH);button_down = true;start = millis();else if (millis() - start > 1200) // if timer has elapsed do thisdigitalWrite(transistorPin1, HIGH);else // if button is up do thisbutton_down = false;digitalWrite(controlPin1, LOW);digitalWrite(transistorPin1, LOW);delay(1); // this is here to give a known time per loopvoid checkNunchuck1()if( loop_cnt > 100 ) { // loop()s is every 1msec, this is every 100msecnunchuck_get_data();nunchuck_print_data();float tilt = nunchuck_accelx(); // x-axis, in this case ranges from ~70 - ~185tilt = (tilt - 70) * 1.5; // convert to angle in degrees, roughlypulseWidth1 = (tilt * 9) + minPulse; // convert angle to microsecondsloop_cnt = 0; // reset forloop_cnt++;// called every loop().// uses global variables servoPin, pulsewidth, lastPulse, & refreshTimevoid updateServo1()// pulse the servo again if rhe refresh time (20 ms) have passed:if (millis() - lastPulse1 >= refreshTime) {digitalWrite(servoPin1, HIGH); // Turn the servo ondelayMicroseconds(pulseWidth1); // Length of the pulse sets the servo positiondigitalWrite(servoPin1, LOW); // Turn the servo offlastPulse1 = millis(); // save the time of the last pulsevoid checkNunchuck2()if( loop_cnt > 100 ) { // loop()s is every 1msec, this is every 100msecnunchuck_get_data();nunchuck_print_data();float tilt = nunchuck_accely(); // y-axis, in this case ranges from ~70 - ~185tilt = (tilt - 70) * 1.5; // convert to angle in degrees, roughlypulseWidth2 = (tilt * 9) + minPulse; // convert angle to microsecondsloop_cnt = 0; // reset forloop_cnt++;// called every loop().// uses global variables servoPin, pulsewidth, lastPulse, & refreshTimevoid updateServo2()// pulse the servo again if rhe refresh time (20 ms) have passed:if (millis() - lastPulse2 >= refreshTime) {digitalWrite(servoPin2, HIGH); // Turn the servo ondelayMicroseconds(pulseWidth2); // Length of the pulse sets the servo positiondigitalWrite(servoPin2, LOW); // Turn the servo offlastPulse2 = millis(); // save the time of the last pulse//// Nunchuck functions//static uint8_t nunchuck_buf[6]; // array to store nunchuck data,// initialize the I2C system, join the I2C bus,// and tell the nunchuck we're talking to itvoid nunchuck_init()Wire.begin(); // join i2c bus as masterWire.beginTransmission(0x52); // transmit to device 0x52Wire.send(0x40); // sends memory addressWire.send(0x00); // sends sent a zero.Wire.endTransmission(); // stop transmitting// Send a request for data to the nunchuck// was \"send_zero()\"void nunchuck_send_request()Wire.beginTransmission(0x52); // transmit to device 0x52Wire.send(0x00); // sends one byteWire.endTransmission(); // stop transmitting// Receive data back from the nunchuck,// returns 1 on successful read. returns 0 on failureint nunchuck_get_data()int cnt=0;Wire.requestFrom (0x52, 6); // request data from nunchuckwhile (Wire.available ()) {// receive byte as an integernunchuck_buf[cnt] = nunchuk_decode_byte(Wire.receive());cnt++;nunchuck_send_request(); // send request for next data payload// If we recieved the 6 bytes, then go print themif (cnt >= 5) {return 1; // successreturn 0; //failure// Print the input data we have recieved// accel data is 10 bits long// so we read 8 bits, then we have to add// on the last 2 bits. That is why I// multiply them by 2 * 2void nunchuck_print_data()static int i=0;int joy_x_axis = nunchuck_buf[0];int joy_y_axis = nunchuck_buf[1];int accel_x_axis = nunchuck_buf[2]; // * 2 * 2;int accel_y_axis = nunchuck_buf[3]; // * 2 * 2;int accel_z_axis = nunchuck_buf[4]; // * 2 * 2;int z_button = 0;int c_button = 0;// byte nunchuck_buf[5] contains bits for z and c buttons// it also contains the least significant bits for the accelerometer data// so we have to check each bit of byte outbuf[5]if ((nunchuck_buf[5] >> 0) & 1)z_button = 1;if ((nunchuck_buf[5] >> 1) & 1)c_button = 1;if ((nunchuck_buf[5] >> 2) & 1)accel_x_axis += 2;if ((nunchuck_buf[5] >> 3) & 1)accel_x_axis += 1;if ((nunchuck_buf[5] >> 4) & 1)accel_y_axis += 2;if ((nunchuck_buf[5] >> 5) & 1)accel_y_axis += 1;if ((nunchuck_buf[5] >> 6) & 1)accel_z_axis += 2;if ((nunchuck_buf[5] >> 7) & 1)accel_z_axis += 1;Serial.print(i,DEC);Serial.print(\"\\t\");Serial.print(\"joy:\");Serial.print(joy_x_axis,DEC);Serial.print(\",\");Serial.print(joy_y_axis, DEC);Serial.print(\" \\t\");Serial.print(\"acc:\");Serial.print(accel_x_axis, DEC);Serial.print(\",\");Serial.print(accel_y_axis, DEC);Serial.print(\",\");Serial.print(accel_z_axis, DEC);Serial.print(\"\\t\");Serial.print(\"but:\");Serial.print(z_button, DEC);Serial.print(\",\");Serial.print(c_button, DEC);Serial.print(\"\\r\n\n\"); // newlinei++;// Encode data to format that most wiimote drivers except// only needed if you use one of the regular wiimote driverschar nunchuk_decode_byte (char x)x = (x ^ 0x17) + 0x17;return x;// returns zbutton state: 1=pressed, 0=notpressedint nunchuck_zbutton()return ((nunchuck_buf[5] >> 0) & 1) ? 0 : 1; // voodoo// returns zbutton state: 1=pressed, 0=notpressedint nunchuck_cbutton()return ((nunchuck_buf[5] >> 1) & 1) ? 0 : 1; // voodoo// returns value of x-axis joystickint nunchuck_joyx()return nunchuck_buf[0];// returns value of y-axis joystickint nunchuck_joyy()return nunchuck_buf[1];// returns value of x-axis accelerometerint nunchuck_accelx()return nunchuck_buf[2]; // FIXME: this leaves out 2-bits of the data// returns value of y-axis accelerometerint nunchuck_accely()return nunchuck_buf[3]; // FIXME: this leaves out 2-bits of the data// returns value of z-axis accelerometerint nunchuck_accelz()return nunchuck_buf[4]; // FIXME: this leaves out 2-bits of the data\n\n\n\nFrequently asked questions-\n\n\n\nDo you offer a servo board kit?\n\n Nope- at least not yet. I might be coaxed into making a fully assembled board though...just message me if you want one.\n\n\n\nWill you do custom work?\n\n On occasion- message me to discuss your project.\n\n\n\nI need help with my own code/project- is there a forum for help?\n\n The Arduino forum is the place to go-\n\nhttp://arduino.cc/cgi-bin/yabb2/YaBB.pl\n\n\n\n\n\nThe Predator setup isn't really a true head tracking system- why?\n\n When I first started looking at this I had just envisioned a simple system that was controlled using bend sensors on your fingers and it wouldn't be able to look up and down. That really was a bit cumbersome to operate and wasn't very natural.\n\n\n\n Next I looked at keeping the bend sensor that would raise the cannon, but I figured out a system that had a cable mounted under the mask that turned a pulley/spring loaded potentiometer (mounted under the cannon mechanism) that would allow the cannon to follow your head rotation. It still wasn't able to look up/down. Now I got a natural (and smooth) rotation but fixing/routing the cable was difficult and cumbersome. Mask removal could also be a real problem.\n\n\n\n Then I started looking at different kinds of sensors. Gyroscopic, compass and accelerometers. Combo boards, IMUs (inertial measurement units), etc. I have a friend that is a data acquisition engineer as well as a friend that is a programmer and I spoke to several electrical engineers for help. Compasses are affected by tilt, so you have to compensate for that and then they don't work when you rotate your whole body vs. just your head, so dual compasses would be necessary and you would have to write code to differentiate between the two.\n\n\n\n The problem with gyros is that you need two of them, preferably with an accelerometer to account for drift as well as front/rear tilt (pitch). One gyro really won't work well because it will respond to whole body rotation as well as just head rotation, so you would need one IMU at the head and another at the cannon base and then you have to write some code to differentiate between the two. Gyros really only sense change in angular velocity so its output only changes with respect to movement and it will reset itself once you stop rotating your body- the cannon will quickly become out of sync with your movements. That's why to really do it right you need a multiple DOF (degree of freedom) IMU and you need two of them for true head tracking. Most of the systems I've seen that people post online as a head tracking system for R/C or video control that use these types of sensors have pretty poor yaw control and I think this is why. And they don't even have to deal with the body vs. head rotation issue as they can sit in a chair- not an option for costuming.\n\n\n\n There are IMUs and digital compasses available now that have on board processing and are programmed to account for drift and tilt so some of the hard work is done for you but you would still have to factor in the difference output of two of them and then generate your necessary servo movement values from that. It can be done but it's pretty darn expensive.\n\n\n\n Most of the solutions I found were pretty complex in terms of programming requiring sophisticated algorithms and/or extremely expensive on the hardware side. There are also pretty severe space restrictions for fitting sensors inside the Predator Bio helmet as some of the IMUs available are pretty large physically. \n\n\n\n Then I found that I could modify the sensor board out of a Wii nunchuck controller and interface it with a microcontroller over its I²C bus. This provided me with an inexpensive and reliable multi axis accelerometer and two finger pushbuttons with an easy to use interface. I ended up writing code that would allow me to access all of the nunchuck's functions- the accelerometer outputs, the joystick and the two pushbuttons.\n\n\n\n When it was all said and done the rotation was still a bit of a problem as the accelerometer only really gives you a stable output with respect to gravity so you have to tilt it to get the rotation. What I found was that if I mounted it as level as possible in Predator helmet it really didn't need much tilt at all to get a stable rotation. The beauty of this system is that there are only only two finger buttons to control everything- it's also pretty easy for me to modify the code. I haven't yet taken apart a Wii Motion Plus to combine it with a nunchuck to create an inexpensive IMU but I'll post the results when I do.\n\n\n\n What I did was to basically fake it by strictly using the accelerometer inputs- when looking sideways you just need to tilt your head to the side like a bird for yaw (rotation) control. The accelerometer reads the tilt and moves everything accordingly- the pitch function is normal. It does take a bit of getting used to but after a while it becomes more natural and it's very convincing.\n\n\n\n For costuming all you need to do is provide the illusion. When I started the project with a friend three years ago (he's sculpting the Pedator backpack) we wanted something that anyone could build from readily available building blocks to add some cool animatronics to their Predator costume without spending a ton of cash.\n\n\n\n Whew...\n\n\n\nI want to power my project and I only want to use one battery to supply different voltages- what do I do?\n\n I would use a switching step down voltage regulator like this one-\n\nhttp://www.pololu.com/catalog/product/2110\n\n It's far more efficient than using a traditional voltage regulator, especially if you need to use something like a 12V battery and drop it down to 5V for servo power. If you need to drop 5V to 3.3V then a traditional voltage regulator is fine. Just don't plug it into the JST connector on the controller board and then plug in a USB cable into the USB mini-B port for the charging circuit- that would be bad.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Nope- at least not yet. I might be coaxed into making a fully assembled board though...just message me if you want one.On occasion- message me to discuss your project.The Arduino forum is the place to go-When I first started looking at this I had just envisioned a simple system that was controlled using bend sensors on your fingers and it wouldn't be able to look up and down. That really was a bit cumbersome to operate and wasn't very natural.Next I looked at keeping the bend sensor that would raise the cannon, but I figured out a system that had a cable mounted under the mask that turned a pulley/spring loaded potentiometer (mounted under the cannon mechanism) that would allow the cannon to follow your head rotation. It still wasn't able to look up/down. Now I got a natural (and smooth) rotation but fixing/routing the cable was difficult and cumbersome. Mask removal could also be a real problem.Then I started looking at different kinds of sensors. Gyroscopic, compass and accelerometers. Combo boards, IMUs (inertial measurement units), etc. I have a friend that is a data acquisition engineer as well as a friend that is a programmer and I spoke to several electrical engineers for help. Compasses are affected by tilt, so you have to compensate for that and then they don't work when you rotate your whole body vs. just your head, so dual compasses would be necessary and you would have to write code to differentiate between the two.The problem with gyros is that you need two of them, preferably with an accelerometer to account for drift as well as front/rear tilt (pitch). One gyro really won't work well because it will respond to whole body rotation as well as just head rotation, so you would need one IMU at the head and another at the cannon base and then you have to write some code to differentiate between the two. Gyros really only sense change in angular velocity so its output only changes with respect to movement and it will reset itself once you stop rotating your body- the cannon will quickly become out of sync with your movements. That's why to really do it right you need a multiple DOF (degree of freedom) IMU and you need two of them for true head tracking. Most of the systems I've seen that people post online as a head tracking system for R/C or video control that use these types of sensors have pretty poor yaw control and I think this is why. And they don't even have to deal with the body vs. head rotation issue as they can sit in a chair- not an option for costuming.There are IMUs and digital compasses available now that have on board processing and are programmed to account for drift and tilt so some of the hard work is done for you but you would still have to factor in the difference output of two of them and then generate your necessary servo movement values from that. It can be done but it's pretty darn expensive.Most of the solutions I found were pretty complex in terms of programming requiring sophisticated algorithms and/or extremely expensive on the hardware side. There are also pretty severe space restrictions for fitting sensors inside the Predator Bio helmet as some of the IMUs available are pretty large physically.Then I found that I could modify the sensor board out of a Wii nunchuck controller and interface it with a microcontroller over its I²C bus. This provided me with an inexpensive and reliable multi axis accelerometer and two finger pushbuttons with an easy to use interface. I ended up writing code that would allow me to access all of the nunchuck's functions- the accelerometer outputs, the joystick and the two pushbuttons.When it was all said and done the rotation was still a bit of a problem as the accelerometer only really gives you a stable output with respect to gravity so you have to tilt it to get the rotation. What I found was that if I mounted it as level as possible in Predator helmet it really didn't need much tilt at all to get a stable rotation. The beauty of this system is that there are only only two finger buttons to control everything- it's also pretty easy for me to modify the code. I haven't yet taken apart a Wii Motion Plus to combine it with a nunchuck to create an inexpensive IMU but I'll post the results when I do.What I did was to basically fake it by strictly using the accelerometer inputs- when looking sideways you just need to tilt your head to the side like a bird for yaw (rotation) control. The accelerometer reads the tilt and moves everything accordingly- the pitch function is normal. It does take a bit of getting used to but after a while it becomes more natural and it's very convincing.For costuming all you need to do is provide the illusion. When I started the project with a friend three years ago (he's sculpting the Pedator backpack) we wanted something that anyone could build from readily available building blocks to add some cool animatronics to their Predator costume without spending a ton of cash.Whew...I would use a switching step down voltage regulator like this one-It's far more efficient than using a traditional voltage regulator, especially if you need to use something like a 12V battery and drop it down to 5V for servo power. If you need to drop 5V to 3.3V then a traditional voltage regulator is fine. Just don't plug it into the JST connector on the controller board and then plug in a USB cable into the USB mini-B port for the charging circuit- that would be bad."
    },
    {
        "link": "https://synthiam.com/Support/Skills/Skills-Overview?srsltid=AfmBOopqKhXYkNTebuFplYGuXIrSAg5HJ4dgeZn8QU6LTha4U8LtlkQu",
        "document": "The ADC (Analog-to-Digital Converter) category in Synthiam ARC provides the capability to read analog signals from sensors, converting real-world signals into digital data. This is essential for interfacing with analog sensors and devices, allowing robots to gather precise data from the physical environment.\n\nThe Artificial Intelligence category in Synthiam ARC empowers robots with advanced cognitive abilities. Through integration with AI services, robots can recognize objects, understand speech, and make intelligent decisions. This skill category opens up a realm of possibilities for creating smart and responsive robotic systems.\n\nThe Audio category in Synthiam ARC enables robots to interact with their surroundings through sound. It includes features such as speech synthesis, audio playback, and voice recognition. This skill set is crucial for creating robots that can communicate effectively using auditory cues.\n\nThe Camera category in Synthiam ARC facilitates visual perception for robots. With camera-related functionalities, robots can capture images, process visual data, and perform tasks such as object recognition and tracking. This category is fundamental for developing vision-based robotic applications.\n\nThe Communication category in Synthiam ARC focuses on enabling robots to interact and exchange information with other devices. This includes communication protocols, data sharing, and network connectivity. It is crucial for building robots that can collaborate and communicate in various environments.\n\nThe Digital category in Synthiam ARC deals with digital signal processing and manipulation. It includes functions for working with digital sensors, logic operations, and data processing. This skill set is essential for handling digital information in robotic applications.\n\nThe Display category in Synthiam ARC allows robots to present information visually. It includes features for controlling displays, showing images, and rendering graphical content. This skill set is valuable for creating robots that can convey information to users in a clear and engaging manner.\n\nThe Games category in Synthiam ARC introduces entertainment and gaming elements to robotic platforms. It includes functionalities for creating interactive games and activities that engage users with the robot. This category adds a playful dimension to the robot's capabilities.\n\nThe General category in Synthiam ARC encompasses a wide range of general-purpose robot skills. It includes versatile functionalities that can be applied across different robotic applications, providing a foundation for building custom behaviors and features.\n\nThe GPS (Global Positioning System) category in Synthiam ARC enables robots to determine their precise geographical location. This is essential for applications that require navigation, mapping, and location-based decision-making. Robots equipped with GPS capabilities can navigate autonomously in various environments.\n\nThe Graphs category in Synthiam ARC focuses on visualizing data through graphical representations. It includes tools for creating charts, graphs, and visual displays of information. This skill set is useful for presenting complex data in a comprehensible manner.\n\nThe I2C (Inter-Integrated Circuit) category in Synthiam ARC facilitates communication between devices using the I2C protocol. This skill set is crucial for connecting and interacting with a variety of sensors and peripherals in a robotics system.\n\nThe Infrared Distance category in Synthiam ARC enables robots to measure distances using infrared sensors. This is essential for obstacle avoidance, navigation, and proximity sensing. Robots equipped with this capability can interact safely in dynamic environments.\n\nThe Misc (Miscellaneous) category in Synthiam ARC includes a collection of miscellaneous robot skills that don't fit into specific categories. It provides a versatile toolkit for handling various tasks and functionalities in robotic applications.\n\nThe Movement Panels category in Synthiam ARC focuses on controlling and managing the movement of robots. It includes features for controlling motors, wheels, and other actuators, allowing robots to navigate and move in a controlled manner. There can only be one movement panel per project. Movement Panel Details\n\nThe Navigation category in Synthiam ARC is dedicated to enabling robots to navigate and move autonomously. It includes functionalities such as path planning, obstacle avoidance, and localization. This skill set is crucial for developing robots capable of exploring and navigating complex environments.\n\nThe Power category in Synthiam ARC focuses on managing and monitoring the power aspects of robots. It includes features for monitoring battery levels, managing power consumption, and implementing power-related behaviors. This skill set is essential for ensuring the efficient and reliable operation of robotic systems.\n\nThe PWM (Pulse Width Modulation) category in Synthiam ARC facilitates precise control of actuators such as motors and servos by adjusting the duty cycle of the signal. This skill set is crucial for achieving fine-grained control over the movement and position of robotic components.\n\nThe Rafiki category in Synthiam ARC introduces social and emotional interaction capabilities to robots. It includes features for recognizing emotions, responding empathetically, and engaging in social interactions. This category adds a human touch to robotic interactions.\n\nThe Remote Control category in Synthiam ARC enables robots to be controlled remotely by users. It includes functionalities for remote operation, telepresence, and remote monitoring. This skill set is valuable for scenarios where direct user control or observation is required.\n\nThe Scripting category in Synthiam ARC empowers users to create custom scripts and behaviors for robots. It includes scripting languages and tools that allow users to define complex and customized robot actions, expanding the flexibility and capabilities of robotic systems. Here are the manuals for JavaScript, Python, EZ-Script, Blockly, and RoboScratch.\n\nThe Servo category in Synthiam ARC focuses on controlling servo motors, allowing precise control over the rotational movement of robotic components. This skill set is essential for tasks requiring accurate and controlled motion in robotic applications.\n\nThe Ultrasonic Distance category in Synthiam ARC enables robots to measure distances using ultrasonic sensors. This is valuable for applications such as obstacle avoidance, object detection, and proximity sensing. Robots equipped with this capability can navigate and interact safely in various environments.\n\nThe Virtual Reality category in Synthiam ARC introduces immersive virtual experiences to robots. It includes features for integrating virtual reality technologies, allowing robots to interact with virtual environments and providing users with unique and engaging experiences."
    },
    {
        "link": "https://projecthub.arduino.cc/metagate/xr2ino-the-first-app-to-visualize-arduino-data-on-meta-quest-in-mixed-reality-27465e",
        "document": "XR2ino: the first app to visualize Arduino data on Meta Quest in Mixed Reality\n\nXR2ino is a Mixed Reality app developed by Metagate that permits the BLE connection between Arduino and MR headsets."
    }
]