[
    {
        "link": "https://jsonapi.org",
        "document": "If you’ve ever argued with your team about the way your JSON responses should be formatted, JSON:API can help you stop the bikeshedding and focus on what matters: your application.\n\nBy following shared conventions, you can increase productivity, take advantage of generalized tooling and best practices. Clients built around JSON:API are able to take advantage of its features around efficiently caching responses, sometimes eliminating network requests entirely.\n\nHere’s an example response from a blog that implements JSON:API:\n\nThe response above contains the first in a collection of “articles”, as well as links to subsequent members in that collection. It also contains resources linked to the article, including its author and comments. Last but not least, links are provided that can be used to fetch or update any of these resources.\n\nJSON:API covers creating and updating resources as well, not just responses.\n\nJSON:API has been properly registered with the IANA. Its media type designation is .\n\nTo get started with JSON:API, check out documentation for the base specification.\n\nThe JSON:API community has created a collection of extensions that APIs can use to provide clients with information or functionality beyond that described in the base JSON:API specification. These extensions are called profiles.\n\nYou can browse existing profiles or create a new one.\n\nA more thorough history is available here.\n\nYou can subscribe to an RSS feed of individual changes here."
    },
    {
        "link": "https://jsonapi.org/format",
        "document": "This page presents the latest published version of JSON:API, which is currently version 1.1. New versions of JSON:API will always be backwards compatible using a never remove, only add strategy. Additions can be proposed in our discussion forum.\n\nIf you catch an error in the specification’s text, or if you write an implementation, please let us know by opening an issue or pull request at our GitHub repository.\n\nJSON:API is a specification for how a client should request that resources be fetched or modified, and how a server should respond to those requests. JSON:API can be easily extended with extensions and profiles.\n\nJSON:API is designed to minimize both the number of requests and the amount of data transmitted between clients and servers. This efficiency is achieved without compromising readability, flexibility, or discoverability.\n\nJSON:API requires use of the JSON:API media type ( ) for exchanging data.\n\nAll document members, query parameters, and processing rules defined by this specification are collectively called “specification semantics”.\n\nCertain document members, query parameters, and processing rules are reserved for implementors to define at their discretion. These are called “implementation semantics”.\n\nAll other semantics are reserved for potential future use by this specification.\n\nThe key words “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD NOT”, “RECOMMENDED”, “NOT RECOMMENDED”, “MAY”, and “OPTIONAL” in this document are to be interpreted as described in BCP 14 [RFC2119] [RFC8174] when, and only when, they appear in all capitals, as shown here.\n\nThe JSON:API specification supports two media type parameters: and , which are used to specify extensions and profiles, respectively.\n\nExtensions provide a means to “extend” the base specification by defining additional specification semantics.\n\nExtensions cannot alter or remove specification semantics, nor can they specify implementation semantics.\n\nProfiles provide a means to share a particular usage of the specification among implementations.\n\nProfiles can specify implementation semantics, but cannot alter, add to, or remove specification semantics.\n\nThe JSON:API media type MUST NOT be specified with any media type parameters other than and . The parameter is used to support extensions and the parameter is used to support profiles.\n\nExtensions and profiles are each uniquely identified by a URI. Visiting an extension’s or a profile’s URI SHOULD return documentation that describes its usage. The values of the and parameters MUST equal a space-separated (U+0020 SPACE, “ “) list of extension or profile URIs, respectively.\n\nAn extension MAY impose additional processing rules or further restrictions and it MAY define new object members as described below.\n\nAn extension MUST NOT lessen or remove any processing rules, restrictions or object member requirements defined in this specification or other extensions.\n\nAn extension MAY define new members within the document structure defined by this specification. The rules for extension member names are covered below.\n\nAn extension MAY define new query parameters. The rules for extension-defined query parameters are covered below.\n\nWhen an extension defines new query parameters or document members, the extension MUST define a namespace to guarantee that extensions will never conflict with current or future versions of this specification. A namespace MUST meet all of the following conditions:\n• A namespace MUST contain at least one character.\n• A namespace MUST contain only these characters:\n\nAn extension MUST NOT define more than one namespace. The namespace used for all query parameters and document members MUST be the same for any given extension.\n\nIn the following example, an extension with the namespace has specified a resource object member to support per-resource versioning. This member might appear as follows:\n\nThe rules for profile usage are dictated by RFC 6906.\n\nA profile MAY define document members and processing rules that are reserved for implementors.\n\nA profile MUST NOT define any query parameters except implementation-specific query parameters.\n\nA profile MUST NOT alter or remove processing rules that have been defined by this specification or by an extension. However, a profile MAY define processing rules for query parameters whose processing rules have been reserved for implementors to define at their discretion.\n\nFor example, a profile could define rules for interpreting the query parameter, but it could not specify that relationship names in the query parameter are space-separated instead of dot-separated.\n\nUnlike extensions, profiles do not need to define a namespace for document members because profiles cannot define specification semantics and thus cannot conflict with current or future versions of this specification. However, it is possible for profiles to conflict with other profiles. Therefore, it is the responsibility of implementors to ensure that they do not support conflicting profiles.\n\nIn the following example, a profile has defined a attribute. According to the profile, the attribute must be an object containing a member and a member and these members’ values must use the RFC 3339 format. With such a profile applied, a response might appear as follows:\n\nClients and servers MUST send all JSON:API payloads using the JSON:API media type in the header.\n\nClients and servers MUST specify the media type parameter in the header when they have applied one or more extensions to a JSON:API document.\n\nClients and servers MUST specify the media type parameters in the header when they have applied one or more profiles to a JSON:API document.\n\nWhen processing a JSON:API response document, clients MUST ignore any parameters other than and parameters in the server’s header.\n\nA client MAY use the media type parameter in an header to require that a server apply all the specified extensions to the response document.\n\nA client MAY use the media type parameter in an header to request that the server apply one or more profiles to the response document.\n\nIf a request specifies the header with the JSON:API media type, servers MUST respond with a status code if that media type contains any media type parameters other than or .\n\nIf a request specifies the header with an instance of the JSON:API media type modified by the media type parameter and that parameter contains an unsupported extension URI, the server MUST respond with a status code.\n\nIf a request’s header contains an instance of the JSON:API media type, servers MUST ignore instances of that media type which are modified by a media type parameter other than or . If all instances of that media type are modified with a media type parameter other than or , servers MUST respond with a status code. If every instance of that media type is modified by the parameter and each contains at least one unsupported extension URI, the server MUST also respond with a .\n\nIf the parameter is received, a server SHOULD attempt to apply any requested profile(s) to its response. A server MUST ignore any profiles that it does not recognize.\n\nServers that support the or media type parameters SHOULD specify the header with as one of its values. This applies to responses with and without any profiles or extensions applied.\n\nThis section describes the structure of a JSON:API document, which is identified by the JSON:API media type. JSON:API documents are defined in JavaScript Object Notation (JSON) [RFC8259].\n\nAlthough the same media type is used for both request and response documents, certain aspects are only applicable to one or the other. These differences are called out below.\n\nExtensions MAY define new members within the document structure. These members MUST comply with the naming requirements specified below.\n\nUnless otherwise noted, objects defined by this specification or any applied extensions MUST NOT contain any additional members. Client and server implementations MUST ignore non-compliant members.\n\nA JSON object MUST be at the root of every JSON:API request and response document containing data. This object defines a document’s “top level”.\n\nA document MUST contain at least one of the following top-level members:\n\nThe members and MUST NOT coexist in the same document.\n\nA document MAY contain any of these top-level members:\n• : a links object related to the document as a whole.\n• : an array of resource objects that are related to the primary data and/or each other (“included resources”).\n\nIf a document does not contain a top-level key, the member MUST NOT be present either.\n\nThe top-level links object MAY contain the following members:\n• : the link that generated the current response document. If a document has extensions or profiles applied to it, this link SHOULD be represented by a link object with the target attribute specifying the JSON:API media type with all applicable parameters.\n• : a link to a description document (e.g. OpenAPI or JSON Schema) for the current document.\n\nThe document’s “primary data” is a representation of the resource or collection of resources targeted by a request.\n\nPrimary data MUST be either:\n• a single resource object, a single resource identifier object, or , for requests that target single resources\n• an array of resource objects, an array of resource identifier objects, or an empty array ( ), for requests that target resource collections\n\nFor example, the following primary data is a single resource object:\n\nThe following primary data is a single resource identifier object that references the same resource:\n\nA logical collection of resources MUST be represented as an array, even if it only contains one item or is empty.\n\n“Resource objects” appear in a JSON:API document to represent resources.\n\nA resource object MUST contain at least the following top-level members:\n\nException: The member is not required when the resource object originates at the client and represents a new resource to be created on the server. In that case, a client MAY include a member to uniquely identify the resource by locally within the document.\n\nIn addition, a resource object MAY contain any of these top-level members:\n• : an attributes object representing some of the resource’s data.\n• : a relationships object describing relationships between the resource and other JSON:API resources.\n• : a links object containing links related to the resource.\n• : a meta object containing non-standard meta-information about a resource that can not be represented as an attribute or relationship.\n\nHere’s how an article (i.e. a resource of type “articles”) might appear in a document:\n\nAs noted above, every resource object MUST contain a member. Every resource object MUST also contain an member, except when the resource object originates at the client and represents a new resource to be created on the server. If is omitted due to this exception, a member MAY be included to uniquely identify the resource by locally within the document. The value of the member MUST be identical for every representation of the resource in the document, including resource identifier objects.\n\nThe values of the , , and members MUST be strings.\n\nWithin a given API, each resource object’s and pair MUST identify a single, unique resource. (The set of URIs controlled by a server, or multiple servers acting as one, constitute an API.)\n\nThe member is used to describe resource objects that share common attributes and relationships.\n\nThe values of members MUST adhere to the same constraints as member names.\n\nA resource object’s attributes and its relationships are collectively called its “fields”.\n\nFields for a resource object MUST share a common namespace with each other and with and . In other words, a resource can not have an attribute and relationship with the same name, nor can it have an attribute or relationship named or .\n\nThe value of the key MUST be an object (an “attributes object”). Members of the attributes object (“attributes”) represent information about the resource object in which it’s defined.\n\nAttributes may contain any valid JSON value, including complex data structures involving JSON objects and arrays.\n\nKeys that reference related resources (e.g. ) SHOULD NOT appear as attributes. Instead, relationships SHOULD be used.\n\nThe value of the key MUST be an object (a “relationships object”). Each member of a relationships object represents a “relationship” from the resource object in which it has been defined to other resource objects.\n\nRelationships may be to-one or to-many.\n\nA relationship’s name is given by its key. The value at that key MUST be an object (“relationship object”).\n\nA “relationship object” MUST contain at least one of the following:\n• : a links object containing at least one of the following:\n• : a link for the relationship itself (a “relationship link”). This link allows the client to directly manipulate the relationship. For example, removing an through an ’s relationship URL would disconnect the person from the without deleting the resource itself. When fetched successfully, this link returns the linkage for the related resources as its primary data. (See Fetching Relationships.)\n• : a meta object that contains non-standard meta-information about the relationship.\n\nA relationship object that represents a to-many relationship MAY also contain pagination links under the member, as described below. Any pagination links in a relationship object MUST paginate the relationship data, not the related resources.\n\nA “related resource link” provides access to resource objects linked in a relationship. When fetched, the related resource object(s) are returned as the response’s primary data.\n\nFor example, an ’s relationship could specify a link that returns a collection of comment resource objects when retrieved through a request.\n\nIf present, a related resource link MUST reference a valid URL, even if the relationship isn’t currently associated with any target resources. Additionally, a related resource link MUST NOT change because its relationship’s content changes.\n\nResource linkage in a compound document allows a client to link together all of the included resource objects without having to any URLs via links.\n\nResource linkage MUST be represented as one of the following:\n• an array of resource identifier objects for non-empty to-many relationships.\n\nFor example, the following article is associated with an :\n\nThe relationship includes a link for the relationship itself (which allows the client to change the related author directly), a related resource link to fetch the resource objects, and linkage information.\n\nThe optional member within each resource object contains links related to the resource.\n\nIf present, this links object MAY contain a link that identifies the resource represented by the resource object.\n\nA server MUST respond to a request to the specified URL with a response that includes the resource as the primary data.\n\nA “resource identifier object” is an object that identifies an individual resource.\n\nA “resource identifier object” MUST contain a member. It MUST also contain an member, except when it represents a new resource to be created on the server. In this case, a member MUST be included that identifies the new resource.\n\nThe values of the , , and members MUST be strings.\n\nA “resource identifier object” MAY also include a member, whose value is a meta object that contains non-standard meta-information.\n\nServers MAY allow responses that include related resources along with the requested primary resources. Such responses are called “compound documents”.\n\nIn a compound document, all included resources MUST be represented as an array of resource objects in a top-level member.\n\nEvery included resource object MUST be identified via a chain of relationships originating in a document’s primary data. This means that compound documents require “full linkage” and that no resource object can be included without a direct or indirect relationship to the document’s primary data.\n\nThe only exception to the full linkage requirement is when relationship fields that would otherwise contain linkage data are excluded due to sparse fieldsets requested by the client.\n\nA compound document MUST NOT include more than one resource object for each and pair.\n\nWhere specified, a member can be used to include non-standard meta-information. The value of each member MUST be an object (a “meta object”).\n\nAny members MAY be specified within objects.\n\nWhere specified, a member can be used to represent links. The value of this member MUST be an object (a “links object”).\n\nWithin this object, a link MUST be represented as either:\n• a string whose value is a URI-reference [RFC3986 Section 4.1] pointing to the link’s target,\n• if the link does not exist.\n\nA link’s relation type SHOULD be inferred from the name of the link unless the link is a link object and the link object has a member.\n\nA link’s context is the top-level object, resource object, or relationship object in which it appears.\n\nIn the example below, the link is a string whereas the link is a link object. The link object provides additional information about the targeted related resource collection as well as a schema that serves as a description document for that collection:\n\nA “link object” is an object that represents a web link.\n\nA link object MUST contain the following member:\n• : a string whose value is a URI-reference [RFC3986 Section 4.1] pointing to the link’s target.\n\nA link object MAY also contain any of the following members:\n• : a string indicating the link’s relation type. The string MUST be a valid link relation type.\n• : a link to a description document (e.g. OpenAPI or JSON Schema) for the link target.\n• : a string which serves as a label for the destination of a link such that it can be used as a human-readable identifier (e.g., a menu entry).\n• : a string indicating the media type of the link’s target.\n• : a string or an array of strings indicating the language(s) of the link’s target. An array of strings indicates that the link’s target is available in multiple languages. Each string MUST be a valid language tag [RFC5646].\n• : a meta object containing non-standard meta-information about the link.\n\nA JSON:API document MAY include information about its implementation under a top level member. If present, the value of the member MUST be an object (a “jsonapi object”).\n\nThe jsonapi object MAY contain any of the following members:\n• - whose value is a string indicating the highest JSON:API version supported.\n• - an array of URIs for all applied extensions.\n• - an array of URIs for all applied profiles.\n\nClients and servers MUST NOT use an or member for content negotiation. Content negotiation MUST only happen based on media type parameters in header.\n\nIf the member is not present, clients should assume the server implements at least version 1.0 of the specification.\n\nImplementation and profile defined member names used in a JSON:API document MUST be treated as case sensitive by clients and servers, and they MUST meet all of the following conditions:\n• Member names MUST contain at least one character.\n• Member names MUST contain only the allowed characters listed below.\n• Member names MUST start and end with a “globally allowed character”, as defined below.\n\nTo enable an easy mapping of member names to URLs, it is RECOMMENDED that member names use only non-reserved, URL safe characters specified in RFC 3986.\n\nThe following “globally allowed characters” MAY be used anywhere in a member name:\n• U+0080 and above (non-ASCII Unicode characters; not recommended, not URL safe)\n\nAdditionally, the following characters are allowed in member names, except as the first or last character:\n\nThe following characters MUST NOT be used in implementation and profile defined member names:\n• U+002B PLUS SIGN, “+” (has overloaded meaning in URL query strings)\n• U+002C COMMA, “,” (used as a separator between relationship paths)\n• U+002E PERIOD, “.” (used as a separator within relationship paths)\n• U+005D RIGHT SQUARE BRACKET, “]” (used in query parameter families)\n• U+0040 COMMERCIAL AT, “@” (except as first character in @-Members)\n\nMember names MAY also begin with an at sign (U+0040 COMMERCIAL AT, “@”). Members named this way are called “@-Members”. @-Members MAY appear anywhere in a document.\n\nThis specification provides no guidance on the meaning or usage of @-Members, which are considered to be implementation semantics. @-Members MUST be ignored when interpreting this specification’s definitions and processing instructions given outside of this subsection. For example, an attribute is defined above as any member of the attributes object. However, because @-Members must be ignored when interpreting that definition, an @-Member that occurs in an attributes object is not an attribute.\n\nThe name of every new member introduced by an extension MUST be prefixed with the extension’s namespace followed by a colon ( ). The remainder of the name MUST adhere to the rules for implementation defined member names.\n\nData, including resources and relationships, can be fetched by sending a request to an endpoint.\n\nResponses can be further refined with the optional features described below.\n\nA server MUST support fetching resource data for every URL provided as:\n• a link as part of the top-level links object\n\nFor example, the following request fetches a collection of articles:\n\nThe following request fetches an article:\n\nAnd the following request fetches an article’s author:\n\nA server MUST respond to a successful request to fetch an individual resource or resource collection with a response.\n\nA server MUST respond to a successful request to fetch a resource collection with an array of resource objects or an empty array ( ) as the response document’s primary data.\n\nFor example, a request to a collection of articles could return:\n\nA similar response representing an empty collection would be:\n\nA server MUST respond to a successful request to fetch an individual resource with a resource object or provided as the response document’s primary data.\n\nis only an appropriate response when the requested URL is one that might correspond to a single resource, but doesn’t currently.\n\nFor example, a request to an individual article could return:\n\nIf the above article’s author is missing, then a request to that related resource would return:\n\nA server MUST respond with when processing a request to fetch a single resource that does not exist, except when the request warrants a response with as the primary data (as described above).\n\nA server MAY respond with other HTTP status codes.\n\nA server MUST prepare responses, and a client MUST interpret responses, in accordance with .\n\nA server MUST support fetching relationship data for every relationship URL provided as a link as part of a relationship’s object.\n\nFor example, the following request fetches data about an article’s comments:\n\nAnd the following request fetches data about an article’s author:\n\nA server MUST respond to a successful request to fetch a relationship with a response.\n\nThe primary data in the response document MUST match the appropriate value for resource linkage, as described above for relationship objects.\n\nThe top-level links object MAY contain and links, as described above for relationship objects.\n\nFor example, a request to a URL from a to-one relationship link could return:\n\nIf the above relationship is empty, then a request to the same URL would return:\n\nA request to a URL from a to-many relationship link could return:\n\nIf the above relationship is empty, then a request to the same URL would return:\n\nA server MUST return when processing a request to fetch a relationship link URL that does not exist.\n\nIf a relationship link URL exists but the relationship is empty, then MUST be returned, as described above.\n\nA server MAY respond with other HTTP status codes.\n\nA server MUST prepare responses, and a client MUST interpret responses, in accordance with .\n\nAn endpoint MAY return resources related to the primary data by default.\n\nAn endpoint MAY also support an query parameter to allow the client to customize which related resources should be returned.\n\nIf an endpoint does not support the parameter, it MUST respond with to any requests that include it.\n\nIf an endpoint supports the parameter and a client supplies it:\n• The server’s response MUST be a compound document with an key — even if that key holds an empty array (because the requested relationships are empty).\n• The server MUST NOT include unrequested resource objects in the section of the compound document.\n\nThe value of the parameter MUST be a comma-separated (U+002C COMMA, “,”) list of relationship paths. A relationship path is a dot-separated (U+002E FULL-STOP, “.”) list of relationship names. An empty value indicates that no related resources should be returned.\n\nIf a server is unable to identify a relationship path or does not support inclusion of resources from a path, it MUST respond with 400 Bad Request.\n\nFor instance, comments could be requested with an article:\n\nIn order to request resources related to other resources, a dot-separated path for each relationship name can be specified:\n\nMultiple related resources can be requested in a comma-separated list:\n\nFurthermore, related resources can be requested from a relationship endpoint:\n\nIn this case, the primary data would be a collection of resource identifier objects that represent linkage to comments for an article, while the full comments and comment authors would be returned as included data.\n\nA client MAY request that an endpoint return only specific fields in the response on a per-type basis by including a query parameter.\n\nThe value of any parameter MUST be a comma-separated (U+002C COMMA, “,”) list that refers to the name(s) of the fields to be returned. An empty value indicates that no fields should be returned.\n\nIf a client requests a restricted set of fields for a given resource type, an endpoint MUST NOT include additional fields in resource objects of that type in its response.\n\nIf a client does not specify the set of fields for a given resource type, the server MAY send all fields, a subset of fields, or no fields for that resource type.\n\nA server MAY choose to support requests to sort resource collections according to one or more criteria (“sort fields”).\n\nAn endpoint MAY support requests to sort the primary data with a query parameter. The value for MUST represent sort fields.\n\nAn endpoint MAY support multiple sort fields by allowing comma-separated (U+002C COMMA, “,”) sort fields. Sort fields SHOULD be applied in the order specified.\n\nThe sort order for each sort field MUST be ascending unless it is prefixed with a minus (U+002D HYPHEN-MINUS, “-“), in which case it MUST be descending.\n\nThe above example should return the newest articles first. Any articles created on the same date will then be sorted by their title in ascending alphabetical order.\n\nIf the server does not support sorting as specified in the query parameter , it MUST return .\n\nIf sorting is supported by the server and requested by the client via query parameter , the server MUST return elements of the top-level array of the response ordered according to the criteria specified. The server MAY apply default sorting rules to top-level if request parameter is not specified.\n\nA server MAY choose to limit the number of resources returned in a response to a subset (“page”) of the whole set available.\n\nPagination links MUST appear in the links object that corresponds to a collection. To paginate the primary data, supply pagination links in the top-level object. To paginate an included collection returned in a compound document, supply pagination links in the corresponding links object.\n\nThe following keys MUST be used for pagination links:\n• : the first page of data\n• : the last page of data\n• : the next page of data\n\nKeys MUST either be omitted or have a value to indicate that a particular link is unavailable.\n\nConcepts of order, as expressed in the naming of pagination links, MUST remain consistent with JSON:API’s sorting rules.\n\nThe query parameter family is reserved for pagination. Servers and clients SHOULD use these parameters for pagination operations.\n\nThe query parameter family is reserved for filtering data. Servers and clients SHOULD use these parameters for filtering operations.\n\nA server MAY allow resources of a given type to be created. It MAY also allow existing resources to be modified or deleted.\n\nA request MUST completely succeed or fail (in a single “transaction”). No partial updates are allowed.\n\nA resource can be created by sending a request to a URL that represents a collection of resources. The request MUST include a single resource object as primary data. The resource object MUST contain at least a member.\n\nFor instance, a new photo might be created with the following request:\n\nIf a relationship is provided in the member of the resource object, its value MUST be a relationship object with a member. The value of this key represents the linkage the new resource is to have.\n\nA server MAY accept a client-generated ID along with a request to create a resource. An ID MUST be specified with an key, the value of which MUST be a universally unique identifier. The client SHOULD use a properly generated and formatted UUID as described in RFC 4122 [RFC4122].\n\nA server MUST return in response to an unsupported request to create a resource with a client-generated ID.\n\nIf the requested resource has been created successfully and the server changes the resource in any way (for example, by assigning an ), the server MUST return a response and a document that contains the resource as primary data.\n\nThe response SHOULD include a header identifying the location of the newly created resource, in order to comply with RFC 7231.\n\nIf the resource object returned by the response contains a key in its member and a header is provided, the value of the member MUST match the value of the header.\n\nA server MAY return a response with a document that contains no primary data if the requested resource has been created successfully and the server does not change the resource in any way (for example, by assigning an or attribute). Other top-level members, such as meta, could be included in the response document.\n\nIf a request to create a resource has been accepted for processing, but the processing has not been completed by the time the server responds, the server MUST return a status code.\n\nIf the requested resource has been created successfully and the server does not change the resource in any way (for example, by assigning an or attribute), the server MUST return either a status code and response document (as described above) or a status code with no response document.\n\nA server MAY return in response to an unsupported request to create a resource.\n\nA server MUST return when processing a request that references a related resource that does not exist.\n\nA server MUST return when processing a request to create a resource with a client-generated ID that already exists.\n\nA server MUST return when processing a request in which the resource object’s is not among the type(s) that constitute the collection represented by the endpoint.\n\nA server SHOULD include error details and provide enough information to recognize the source of the conflict.\n\nA server MAY respond with other HTTP status codes.\n\nA server MUST prepare responses, and a client MUST interpret responses, in accordance with .\n\nA resource can be updated by sending a request to the URL that represents the resource.\n\nThe URL for a resource can be obtained in the link of the resource object. Alternatively, when a request returns a single resource object as primary data, the same request URL can be used for updates.\n\nThe request MUST include a single resource object as primary data. The resource object MUST contain and members.\n\nAny or all of a resource’s attributes MAY be included in the resource object included in a request.\n\nIf a request does not include all of the attributes for a resource, the server MUST interpret the missing attributes as if they were included with their current values. The server MUST NOT interpret missing attributes as values.\n\nFor example, the following request is interpreted as a request to update only the and attributes of an article:\n\nAny or all of a resource’s relationships MAY be included in the resource object included in a request.\n\nIf a request does not include all of the relationships for a resource, the server MUST interpret the missing relationships as if they were included with their current values. It MUST NOT interpret them as or empty values.\n\nIf a relationship is provided in the member of a resource object in a request, its value MUST be a relationship object with a member. The relationship’s value will be replaced with the value specified in this member.\n\nFor instance, the following request will update the relationship of an article:\n\nLikewise, the following request performs a complete replacement of the for an article:\n\nA server MAY reject an attempt to do a full replacement of a to-many relationship. In such a case, the server MUST reject the entire update, and return a response.\n\nIf a server accepts an update but also changes the targeted resource in ways other than those specified by the request (for example, updating the attribute or a computed ), it MUST return a response and a document that contains the updated resource as primary data.\n\nA server MAY return a response with a document that contains no primary data if an update is successful and the server does not change the targeted resource in ways other than those specified by the request. Other top-level members, such as meta, could be included in the response document.\n\nIf an update request has been accepted for processing, but the processing has not been completed by the time the server responds, the server MUST return a status code.\n\nIf an update is successful and the server doesn’t change the targeted resource in ways other than those specified by the request, the server MUST return either a status code and response document (as described above) or a status code with no response document.\n\nA server MUST return in response to an unsupported request to update a resource or relationship.\n\nA server MUST return when processing a request to modify a resource that does not exist.\n\nA server MUST return when processing a request that references a related resource that does not exist.\n\nA server MAY return when processing a request to update a resource if that update would violate other server-enforced constraints (such as a uniqueness constraint on a property other than ).\n\nA server MUST return when processing a request in which the resource object’s or do not match the server’s endpoint.\n\nA server SHOULD include error details and provide enough information to recognize the source of the conflict.\n\nA server MAY respond with other HTTP status codes.\n\nA server MUST prepare responses, and a client MUST interpret responses, in accordance with .\n\nAlthough relationships can be modified along with resources (as described above), JSON:API also supports updating of relationships independently at URLs from relationship links.\n\nA to-one relationship can be updated by sending a request to a URL from a to-one relationship link.\n\nThe request MUST include a top-level member named containing one of:\n• a resource identifier object corresponding to the new related resource.\n\nFor example, the following request updates the author of an article:\n\nAnd the following request clears the author of the same article:\n\nIf the relationship is updated successfully then the server MUST return a successful response.\n\nA to-many relationship can be updated by sending a , , or request to a URL from a to-many relationship link.\n\nFor all request types, the body MUST contain a member whose value is an empty array or an array of resource identifier objects.\n\nIf a client makes a request to a URL from a to-many relationship link, the server MUST either completely replace every member of the relationship, return an appropriate error response if some resources cannot be found or accessed, or return a response if complete replacement is not allowed by the server.\n\nFor example, the following request replaces every tag for an article:\n\nAnd the following request clears every tag for an article:\n\nIf a client makes a request to a URL from a relationship link, the server MUST add the specified members to the relationship unless they are already present. If a given and is already in the relationship, the server MUST NOT add it again.\n\nIf all of the specified resources can be added to, or are already present in, the relationship then the server MUST return a successful response.\n\nIn the following example, the comment with ID is added to the list of comments for the article with ID :\n\nIf the client makes a request to a URL from a relationship link the server MUST delete the specified members from the relationship or return a response. If all of the specified resources are able to be removed from, or are already missing from, the relationship then the server MUST return a successful response.\n\nRelationship members are specified in the same way as in the request.\n\nIn the following example, comments with IDs of and are removed from the list of comments for the article with ID :\n\nIf a server accepts an update but also changes the targeted relationship in other ways than those specified by the request, it MUST return a response and a document that includes the updated relationship data as its primary data.\n\nA server MAY return a response with a document that contains no primary data if an update is successful and the server does not change the targeted relationship in ways other than those specified by the request. Other top-level members, such as meta, could be included in the response document.\n\nIf a relationship update request has been accepted for processing, but the processing has not been completed by the time the server responds, the server MUST return a status code.\n\nIf an update is successful and the server doesn’t change the targeted relationship in ways other than those specified by the request, the server MUST return either a status code and response document (as described above) or a status code with no response document.\n\nA server MUST return in response to an unsupported request to update a relationship.\n\nA server MAY respond with other HTTP status codes.\n\nA server MUST prepare responses, and a client MUST interpret responses, in accordance with .\n\nA resource can be deleted by sending a request to the URL that represents the resource:\n\nA server MAY return a response with a document that contains no primary data if a deletion request is successful. Other top-level members, such as meta, could be included in the response document.\n\nIf a deletion request has been accepted for processing, but the processing has not been completed by the time the server responds, the server MUST return a status code.\n\nIf a deletion request is successful, the server MUST return either a status code and response document (as described above) or a status code with no response document.\n\nA server SHOULD return a status code if a deletion request fails due to the resource not existing.\n\nA server MAY respond with other HTTP status codes.\n\nA server MUST prepare responses, and a client MUST interpret responses, in accordance with .\n\nAlthough “query parameter” is a common term in everyday web development, it is not a well-standardized concept. Therefore, JSON:API provides its own definition of a query parameter.\n\nFor the most part, JSON:API’s definition coincides with colloquial usage, and its details can be safely ignored. However, one important consequence of this definition is that a URL like the following is considered to have two distinct query parameters:\n\nThe two parameters are named and ; there is no single parameter.\n\nIn practice, however, parameters like and are usually defined and processed together, and it’s convenient to refer to them collectively. Therefore, JSON:API introduces the concept of a query parameter family.\n\nA “query parameter family” is the set of all query parameters whose name starts with a “base name”, followed by zero or more instances of empty square brackets (i.e. ), square-bracketed legal member names, or square-bracketed dot-separated lists of legal member names. The family is referred to by its base name.\n\nFor example, the query parameter family includes parameters named: , , , , , , , etc. However, is not a valid parameter name in the family, because is not a valid member name.\n\nThe base name of every query parameter introduced by an extension MUST be prefixed with the extension’s namespace followed by a colon ( ). The remainder of the base name MUST contain only the characters [a-z] (U+0061 to U+007A, “a-z”).\n\nImplementations MAY support custom query parameters. However, the names of these query parameters MUST come from a family whose base name is a legal member name and also contains at least one non a-z character (i.e., outside U+0061 to U+007A).\n\nIt is RECOMMENDED that a capital letter (e.g. camelCasing) be used to satisfy the above requirement.\n\nIf a server encounters a query parameter that does not follow the naming conventions above, or the server does not know how to process it as a query parameter from this specification, it MUST return .\n\nA server MAY choose to stop processing as soon as a problem is encountered, or it MAY continue processing and encounter multiple problems. For instance, a server might process multiple attributes and then return multiple validation problems in a single response.\n\nWhen a server encounters multiple problems for a single request, the most generally applicable HTTP error code SHOULD be used in the response. For instance, might be appropriate for multiple 4xx errors or might be appropriate for multiple 5xx errors.\n\nError objects provide additional information about problems encountered while performing an operation. Error objects MUST be returned as an array keyed by in the top level of a JSON:API document.\n\nAn error object MAY have the following members, and MUST contain at least one of:\n• : a unique identifier for this particular occurrence of the problem.\n• : a links object that MAY contain the following members:\n• : a link that leads to further details about this particular occurrence of the problem. When dereferenced, this URI SHOULD return a human-readable description of the error.\n• : a link that identifies the type of error that this particular error is an instance of. This URI SHOULD be dereferenceable to a human-readable explanation of the general error.\n• : the HTTP status code applicable to this problem, expressed as a string value. This SHOULD be provided.\n• : an application-specific error code, expressed as a string value.\n• : a short, human-readable summary of the problem that SHOULD NOT change from occurrence to occurrence of the problem, except for purposes of localization.\n• : a human-readable explanation specific to this occurrence of the problem. Like , this field’s value can be localized.\n• : an object containing references to the primary source of the error. It SHOULD include one of the following members or be omitted:\n• : a JSON Pointer [RFC6901] to the value in the request document that caused the error [e.g. for a primary data object, or for a specific attribute]. This MUST point to a value in the request document that exists; if it doesn’t, the client SHOULD simply ignore the pointer.\n• : a string indicating the name of a single request header which caused the error.\n• : a meta object containing non-standard meta-information about the error.\n\nA query parameter is a name–value pair extracted from, or serialized into, a URI’s query string.\n\nTo extract the query parameters from a URI, an implementation MUST run the URI’s query string, excluding the leading question mark, through the parsing algorithm, with one exception: JSON:API allows the specification that defines a query parameter’s usage to provide its own rules for parsing the parameter’s value from the bytes identified in steps 3.2 and and 3.3 of the parsing algorithm. The resulting value might not be a string.\n\nSimilarly, to serialize a query parameter into a URI, an implementation MUST use the serializer, with the corresponding exception that a parameter’s value — but not its name — may be serialized differently than that algorithm requires, provided the serialization does not interfere with the ability to parse back the resulting URI.\n\nWith query parameter families, JSON:API allows for query parameters whose names contain square brackets (i.e., U+005B “[” and U+005D “]”).\n\nAccording to the query parameter serialization rules above, a compliant implementation will percent-encode these square brackets. However, some URI producers — namely browsers — do not always encode them. Servers SHOULD accept requests in which these square brackets are left unencoded in a query parameter’s name. If a server does accept these requests, it MUST treat the request as equivalent to one in which the square brackets were percent-encoded."
    },
    {
        "link": "https://swagger.io/specification",
        "document": "The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"NOT RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in BCP 14 RFC2119 RFC8174 when, and only when, they appear in all capitals, as shown here.\n\nThis document is licensed under The Apache License, Version 2.0.\n\nThe OpenAPI Specification (OAS) defines a standard, language-agnostic interface to HTTP APIs which allows both humans and computers to discover and understand the capabilities of the service without access to source code, documentation, or through network traffic inspection. When properly defined, a consumer can understand and interact with the remote service with a minimal amount of implementation logic.\n\nAn OpenAPI Description can then be used by documentation generation tools to display the API, code generation tools to generate servers and clients in various programming languages, testing tools, and many other use cases.\n\nFor examples of OpenAPI usage and additional documentation, please visit [[?OpenAPI-Learn]].\n\nFor extension registries and other specifications published by the OpenAPI Initiative, as well as the authoritative rendering of this specification, please visit spec.openapis.org.\n\nAn OpenAPI Description (OAD) formally describes the surface of an API and its semantics. It is composed of an entry document, which must be an OpenAPI Document, and any/all of its referenced documents. An OAD uses and conforms to the OpenAPI Specification, and MUST contain at least one paths field, components field, or webhooks field.\n\nAn OpenAPI Document is a single JSON or YAML document that conforms to the OpenAPI Specification. An OpenAPI Document compatible with OAS 3.*.* contains a required field which designates the version of the OAS that it uses.\n\nA \"schema\" is a formal description of syntax and structure. This document serves as the schema for the OpenAPI Specification format; a non-authoritative JSON Schema based on this document is also provided on spec.openapis.org for informational purposes. This specification also uses schemas in the form of the Schema Object.\n\nWhen capitalized, the word \"Object\" refers to any of the Objects that are named by section headings in this document.\n\nPath templating refers to the usage of template expressions, delimited by curly braces ( ), to mark a section of a URL path as replaceable using path parameters.\n\nEach template expression in the path MUST correspond to a path parameter that is included in the Path Item itself and/or in each of the Path Item's Operations. An exception is if the path item is empty, for example due to ACL constraints, matching path parameters are not required.\n\nThe value for these path parameters MUST NOT contain any unescaped \"generic syntax\" characters described by RFC3986: forward slashes ( ), question marks ( ), or hashes ( ).\n\nMedia type definitions are spread across several resources. The media type definitions SHOULD be in compliance with RFC6838.\n\nSome examples of possible media type definitions:\n\nThe HTTP Status Codes are used to indicate the status of the executed operation. Status codes SHOULD be selected from the available status codes registered in the IANA Status Code Registry.\n\nAs most field names and values in the OpenAPI Specification are case-sensitive, this document endeavors to call out any case-insensitive names and values. However, the case sensitivity of field names and values that map directly to HTTP concepts follow the case sensitivity rules of HTTP, even if this document does not make a note of every concept.\n\nThis specification deems certain situations to have either undefined or implementation-defined behavior.\n\nBehavior described as undefined is likely, at least in some circumstances, to result in outcomes that contradict the specification. This description is used when detecting the contradiction is impossible or impractical. Implementations MAY support undefined scenarios for historical reasons, including ambiguous text in prior versions of the specification. This support might produce correct outcomes in many cases, but relying on it is NOT RECOMMENDED as there is no guarantee that it will work across all tools or with future specification versions, even if those versions are otherwise strictly compatible with this one.\n\nBehavior described as implementation-defined allows implementations to choose which of several different-but-compliant approaches to a requirement to implement. This documents ambiguous requirements that API description authors are RECOMMENDED to avoid in order to maximize interoperability. Unlike undefined behavior, it is safe to rely on implementation-defined behavior if and only if it can be guaranteed that all relevant tools support the same behavior.\n\nThe OpenAPI Specification is versioned using a . . versioning scheme. The . portion of the version string (for example ) SHALL designate the OAS feature set. versions address errors in, or provide clarifications to, this document, not the feature set. Tooling which supports OAS 3.1 SHOULD be compatible with all OAS 3.1.* versions. The patch version SHOULD NOT be considered by tooling, making no distinction between and for example.\n\nOccasionally, non-backwards compatible changes may be made in versions of the OAS where impact is believed to be low relative to the benefit provided.\n\nAn OpenAPI Document that conforms to the OpenAPI Specification is itself a JSON object, which may be represented either in JSON or YAML format.\n\nFor example, if a field has an array value, the JSON array representation will be used:\n\nAll field names in the specification are case sensitive. This includes all fields that are used as keys in a map, except where explicitly noted that keys are case insensitive.\n\nThe schema exposes two types of fields: fixed fields, which have a declared name, and patterned fields, which have a declared pattern for the field name.\n\nPatterned fields MUST have unique names within the containing object.\n\nIn order to preserve the ability to round-trip between YAML and JSON formats, YAML version 1.2 is RECOMMENDED along with some additional constraints:\n• Tags MUST be limited to those allowed by YAML's JSON schema ruleset, which defines a subset of the YAML syntax and is unrelated to [[JSON-Schema-2020-12|JSON Schema]].\n• Keys used in YAML maps MUST be limited to a scalar string, as defined by the YAML Failsafe schema ruleset.\n\nNote: While APIs may be described by OpenAPI Descriptions in either YAML or JSON format, the API request and response bodies and other content are not required to be JSON or YAML.\n\nAn OpenAPI Description (OAD) MAY be made up of a single JSON or YAML document or be divided into multiple, connected parts at the discretion of the author. In the latter case, Reference Object, Path Item Object and Schema Object fields, as well as the Link Object field, and the URI form of the Discriminator Object field, are used to identify the referenced elements.\n\nIn a multi-document OAD, the document containing the OpenAPI Object where parsing begins is known as that OAD's entry document.\n\nIt is RECOMMENDED that the entry document of an OAD be named: or .\n\nIn order to properly handle Schema Objects, OAS 3.1 inherits the parsing requirements of JSON Schema Specification Draft 2020-12, with appropriate modifications regarding base URIs as specified in Relative References In URIs.\n\nThis includes a requirement to parse complete documents before deeming a Schema Object reference to be unresolvable, in order to detect keywords that might provide the reference target or impact the determination of the appropriate base URI.\n\nImplementations MAY support complete-document parsing in any of the following ways:\n• Detecting JSON Schema documents through detecting keywords or otherwise successfully parsing the document in accordance with the JSON Schema specification\n• Detecting a document containing a referenceable Object at its root based on the expected type of the reference\n• Allowing users to configure the type of documents that might be loaded due to a reference to a non-root Object\n\nImplementations that parse referenced fragments of OpenAPI content without regard for the content of the rest of the containing document will miss keywords that change the meaning and behavior of the reference target. In particular, failing to take into account keywords that change the base URI introduces security risks by causing references to resolve to unintended URIs, with unpredictable results. While some implementations support this sort of parsing due to the requirements of past versions of this specification, in version 3.1, the result of parsing fragments in isolation is undefined and likely to contradict the requirements of this specification.\n\nWhile it is possible to structure certain OpenAPI Descriptions to ensure that they will behave correctly when references are parsed as isolated fragments, depending on this is NOT RECOMMENDED. This specification does not explicitly enumerate the conditions under which such behavior is safe and provides no guarantee for continued safety in any future versions of the OAS.\n\nA special case of parsing fragments of OAS content would be if such fragments are embedded in another format, referred to as an embedding format with respect to the OAS. Note that the OAS itself is an embedding format with respect to JSON Schema, which is embedded as Schema Objects. It is the responsibility of an embedding format to define how to parse embedded content, and OAS implementations that do not document support for an embedding format cannot be expected to parse embedded OAS content correctly.\n\nJSON or YAML objects within an OAD are interpreted as specific Objects (such as Operation Objects, Response Objects, Reference Objects, etc.) based on their context. Depending on how references are arranged, a given JSON or YAML object can be interpreted in multiple different contexts:\n• As the root object of the entry document, which is always interpreted as an OpenAPI Object\n• As the Object type implied by its parent Object within the document\n• As a reference target, with the Object type matching the reference source's context\n\nIf the same JSON/YAML object is parsed multiple times and the respective contexts require it to be parsed as different Object types, the resulting behavior is implementation defined, and MAY be treated as an error if detected. An example would be referencing an empty Schema Object under where a Path Item Object is expected, as an empty object is valid for both types. For maximum interoperability, it is RECOMMENDED that OpenAPI Description authors avoid such scenarios.\n\nSeveral features of this specification require resolution of non-URI-based connections to some other part of the OpenAPI Description (OAD).\n\nThese connections are unambiguously resolved in single-document OADs, but the resolution process in multi-document OADs is implementation-defined, within the constraints described in this section. In some cases, an unambiguous URI-based alternative is available, and OAD authors are RECOMMENDED to always use the alternative:\n\nA fifth implicit connection involves appending the templated URL paths of the Paths Object to the appropriate Server Object's field. This is unambiguous because only the entry document's Paths Object contributes URLs to the described API.\n\nIt is RECOMMENDED to consider all Operation Objects from all parsed documents when resolving any Link Object . This requires parsing all referenced documents prior to determining an to be unresolvable.\n\nThe implicit connections in the Security Requirement Object and Discriminator Object rely on the component name, which is the name of the property holding the component in the appropriately typed sub-object of the Components Object. For example, the component name of the Schema Object at is . The implicit connection of in the Operation Object uses the field of Tag Objects, which (like the Components Object) are found under the root OpenAPI Object. This means resolving component names and tag names both depend on starting from the correct OpenAPI Object.\n\nFor resolving component and tag name connections from a referenced (non-entry) document, it is RECOMMENDED that tools resolve from the entry document, rather than the current document. This allows Security Scheme Objects and Tag Objects to be defined next to the API's deployment information (the top-level array of Server Objects), and treated as an interface for referenced documents to access.\n\nThe interface approach can also work for Discriminator Objects and Schema Objects, but it is also possible to keep the Discriminator Object's behavior within a single document using the relative URI-reference syntax of .\n\nThere are no URI-based alternatives for the Security Requirement Object or for the Operation Object's field. These limitations are expected to be addressed in a future release.\n\nSee Appendix F: Resolving Security Requirements in a Referenced Document for an example of the possible resolutions, including which one is recommended by this section. The behavior for Discrimator Object non-URI mappings and for the Operation Object's field operate on the same principles.\n\nNote that no aspect of implicit connection resolution changes how URIs are resolved, or restricts their possible targets.\n\nData types in the OAS are based on the types defined by the JSON Schema Validation Specification Draft 2020-12: \"null\", \"boolean\", \"object\", \"array\", \"number\", \"string\", or \"integer\". Models are defined using the Schema Object, which is a superset of the JSON Schema Specification Draft 2020-12.\n\nJSON Schema keywords and values operate on JSON \"instances\" which may be one of the six JSON data types, \"null\", \"boolean\", \"object\", \"array\", \"number\", or \"string\", with certain keywords and formats only applying to a specific type. For example, the keyword and the format only apply to strings, and treat any instance of the other five types as automatically valid. This means JSON Schema keywords and formats do NOT implicitly require the expected type. Use the keyword to explicitly constrain the type.\n\nNote that the keyword allows as a value for convenience, but keyword and format applicability does not recognize integers as being of a distinct JSON type from other numbers because [[RFC7159|JSON]] itself does not make that distinction. Since there is no distinct JSON integer type, JSON Schema defines integers mathematically. This means that both and are equivalent, and are both considered to be integers.\n\nAs defined by the JSON Schema Validation specification, data types can have an optional modifier keyword: . As described in that specification, is treated as a non-validating annotation by default; the ability to validate varies across implementations.\n\nThe OpenAPI Initiative also hosts a Format Registry for formats defined by OAS users and other specifications. Support for any registered format is strictly OPTIONAL, and support for one registered format does not imply support for any others.\n\nTypes that are not accompanied by a keyword follow the type definition in the JSON Schema. Tools that do not recognize a specific MAY default back to the alone, as if the is not specified. For the purpose of JSON Schema validation, each format should specify the set of JSON data types for which it applies. In this registry, these types are shown in the \"JSON Data Type\" column.\n\nThe formats defined by the OAS are:\n\nAs noted under Data Type, both and are considered to be numbers in the data model.\n\nThe OAS can describe either raw or encoded binary data.\n• raw binary is used where unencoded binary data is allowed, such as when sending a binary payload as the entire HTTP message body, or as part of a payload that allows binary parts\n• encoded binary is used where binary data is embedded in a text-only format such as or (either as a message body or in the URL query string).\n\nIn the following table showing how to use Schema Object keywords for binary data, we use as an example binary media type. Any binary media type, including , is sufficient to indicate binary content.\n\nNote that the encoding indicated by , which inflates the size of data in order to represent it as 7-bit ASCII text, is unrelated to HTTP's header, which indicates whether and how a message body has been compressed and is applied after all content serialization described in this section has occurred. Since HTTP allows unencoded binary message bodies, there is no standardized HTTP header for indicating base64 or similar encoding of an entire message body.\n\nUsing a of ensures that URL encoding (as required in the query string and in message bodies of type ) does not need to further encode any part of the already-encoded binary data.\n\nThe keyword is redundant if the media type is already set:\n• as the key for a MediaType Object\n• in the field of an Encoding Object\n\nIf the Schema Object will be processed by a non-OAS-aware JSON Schema implementation, it may be useful to include even if it is redundant. However, if contradicts a relevant Media Type Object or Encoding Object, then SHALL be ignored.\n\nThe keyword MAY be used to set an expected upper bound on the length of a streaming payload. The keyword can be applied to either string data, including encoded binary data, or to unencoded binary data. For unencoded binary, the length is the number of octets.\n\nThe following table shows how to migrate from OAS 3.0 binary data descriptions, continuing to use as the example binary media type:\n\nThroughout the specification fields are noted as supporting CommonMark markdown formatting. Where OpenAPI tooling renders rich text it MUST support, at a minimum, markdown syntax as described by CommonMark 0.27. Tooling MAY choose to ignore some CommonMark or extension features to address security concerns.\n\nWhile the framing of CommonMark 0.27 as a minimum requirement means that tooling MAY choose to implement extensions on top of it, note that any such extensions are by definition implementation-defined and will not be interoperable. OpenAPI Description authors SHOULD consider how text using such extensions will be rendered by tools that offer only the minimum support.\n\nURIs used as references within an OpenAPI Description, or to external documentation or other supplementary information such as a license, are resolved as identifiers, and described by this specification as URIs. As noted under Parsing Documents, this specification inherits JSON Schema Specification Draft 2020-12's requirements for loading documents and associating them with their expected URIs, which might not match their current location. This feature is used both for working in development or test environments without having to change the URIs, and for working within restrictive network configurations or security policies.\n\nNote that some URI fields are named for historical reasons, but the descriptive text for those fields uses the correct \"URI\" terminology.\n\nUnless specified otherwise, all fields that are URIs MAY be relative references as defined by RFC3986.\n\nRelative references in Schema Objects, including any that appear as values, use the nearest parent as a Base URI, as described by JSON Schema Specification Draft 2020-12.\n\nRelative URI references in other Objects, and in Schema Objects where no parent schema contains an , MUST be resolved using the referring document's base URI, which is determined in accordance with [[RFC3986]] Section 5.1.2 – 5.1.4. In practice, this is usually the retrieval URI of the document, which MAY be determined based on either its current actual location or a user-supplied expected location.\n\nIf a URI contains a fragment identifier, then the fragment should be resolved per the fragment resolution mechanism of the referenced document. If the representation of the referenced document is JSON or YAML, then the fragment identifier SHOULD be interpreted as a JSON-Pointer as per RFC6901.\n\nRelative references in CommonMark hyperlinks are resolved in their rendered context, which might differ from the context of the API description.\n\nAPI endpoints are by definition accessed as locations, and are described by this specification as URLs.\n\nUnless specified otherwise, all fields that are URLs MAY be relative references as defined by RFC3986. Unless specified otherwise, relative references are resolved using the URLs defined in the Server Object as a Base URL. Note that these themselves MAY be relative to the referring document.\n\nThis section describes the structure of the OpenAPI Description format. This text is the only normative description of the format. A JSON Schema is hosted on spec.openapis.org for informational purposes. If the JSON Schema differs from this section, then this section MUST be considered authoritative.\n\nIn the following description, if a field is not explicitly REQUIRED or described with a MUST or SHALL, it can be considered OPTIONAL.\n\nThis is the root object of the OpenAPI Description.\n\nThis object MAY be extended with Specification Extensions.\n\nThe object provides metadata about the API. The metadata MAY be used by the clients if needed, and MAY be presented in editing or documentation generation tools for convenience.\n\nThis object MAY be extended with Specification Extensions.\n\nThis object MAY be extended with Specification Extensions.\n\nThis object MAY be extended with Specification Extensions.\n\nThis object MAY be extended with Specification Extensions.\n\nA single server would be described as:\n\nThe following shows how multiple servers can be described, for example, at the OpenAPI Object's :\n\nThe following shows how variables can be used for a server configuration:\n\nThis object MAY be extended with Specification Extensions.\n\nHolds a set of reusable objects for different aspects of the OAS. All objects defined within the Components Object will have no effect on the API unless they are explicitly referenced from outside the Components Object.\n\nThis object MAY be extended with Specification Extensions.\n\nAll the fixed fields declared above are objects that MUST use keys that match the regular expression: .\n\nHolds the relative paths to the individual endpoints and their operations. The path is appended to the URL from the Server Object in order to construct the full URL. The Paths Object MAY be empty, due to Access Control List (ACL) constraints.\n\nThis object MAY be extended with Specification Extensions.\n\nAssuming the following paths, the concrete definition, , will be matched first if used:\n\nThe following paths are considered identical and invalid:\n\nThe following may lead to ambiguous resolution:\n\nDescribes the operations available on a single path. A Path Item MAY be empty, due to ACL constraints. The path itself is still exposed to the documentation viewer but they will not know which operations and parameters are available.\n\nThis object MAY be extended with Specification Extensions.\n\nThis object MAY be extended with Specification Extensions.\n\nAllows referencing an external resource for extended documentation.\n\nThis object MAY be extended with Specification Extensions.\n\nA unique parameter is defined by a combination of a name and location.\n\nSee Appendix E for a detailed examination of percent-encoding concerns, including interactions with the query string format.\n\nThere are four possible parameter locations specified by the field:\n• path - Used together with Path Templating, where the parameter value is actually part of the operation's URL. This does not include the host or base path of the API. For example, in , the path parameter is .\n• query - Parameters that are appended to the URL. For example, in , the query parameter is .\n• header - Custom headers that are expected as part of the request. Note that RFC7230 states header names are case insensitive.\n• cookie - Used to pass a specific cookie value to the API.\n\nThe rules for serialization of the parameter are specified in one of two ways. Parameter Objects MUST include either a field or a field, but not both. See Appendix B for a discussion of converting values of various types to string representations.\n\nThese fields MAY be used with either or .\n\nThis object MAY be extended with Specification Extensions.\n\nNote that while as a is not forbidden if is , the effect of defining a cookie parameter that way is undefined; use instead.\n\nFixed Fields for use with\n\nFor simpler scenarios, a and can describe the structure and syntax of the parameter. When or are provided in conjunction with the field, the example SHOULD match the specified schema and follow the prescribed serialization strategy for the parameter. The and fields are mutually exclusive, and if either is present it SHALL override any in the schema.\n\nSerializing with is NOT RECOMMENDED for parameters, parameters that use HTTP header parameters (name=value pairs following a ) in their values, or parameters where values might have non-URL-safe characters; see Appendix D for details.\n\nSee also Appendix C: Using RFC6570-Based Serialization for additional guidance.\n\nFixed Fields for use with\n\nFor more complex scenarios, the field can define the media type and schema of the parameter, as well as give examples of its use. Using with a media type is RECOMMENDED for and parameters where the strategy is not appropriate.\n\nIn order to support common ways of serializing simple parameters, a set of values are defined.\n\nSee Appendix E for a discussion of percent-encoding, including when delimiters need to be percent-encoded and options for handling collisions with percent-encoded data.\n\nAssume a parameter named has one of the following values:\n\nThe following table shows examples, as would be shown with the or keywords, of the different serializations for each value.\n• The value empty denotes the empty string, and is unrelated to the field\n• The behavior of combinations marked n/a is undefined\n• The column replaces the column in previous versions of this specification in order to better align with RFC6570 terminology, which describes certain values including but not limited to as \"undefined\" values with special handling; notably, the empty string is not undefined\n• For and the non-RFC6570 query string styles , , and , each example is shown prefixed with as if it were the only query parameter; see Appendix C for more information on constructing query strings from multiple parameters, and Appendix D for warnings regarding and cookie parameters\n• Note that the prefix is not appropriate for serializing HTTP message bodies, and MUST be stripped or (if constructing the string manually) not added when used in that context; see the Encoding Object for more information\n• The examples are percent-encoded as required by RFC6570 and RFC3986; see Appendix E for a thorough discussion of percent-encoding concerns, including why unencoded ( ), ( ), and ( ) seem to work in some environments despite not being compliant.\n\nA header parameter with an array of 64-bit integer numbers:\n\nAn optional query parameter of a string value, allowing multiple values by repeating the query parameter:\n\nThis object MAY be extended with Specification Extensions.\n\nEach Media Type Object provides schema and examples for the media type identified by its key.\n\nWhen or are provided, the example SHOULD match the specified schema and be in the correct format as specified by the media type and its encoding. The and fields are mutually exclusive, and if either is present it SHALL override any in the schema. See Working With Examples for further guidance regarding the different ways of specifying examples, including non-JSON/YAML values.\n\nThis object MAY be extended with Specification Extensions.\n\nIn contrast to OpenAPI 2.0, input/output content in OAS 3.x is described with the same semantics as any other schema type.\n\nIn contrast to OAS 3.0, the keyword has no effect on the content-encoding of the schema in OAS 3.1. Instead, JSON Schema's and keywords are used. See Working With Binary Data for how to model various scenarios with these keywords, and how to migrate from the previous usage.\n\nThese examples apply to either input payloads of file uploads or response payloads.\n\nA for submitting a file in a operation may look like the following example:\n\nIn addition, specific media types MAY be specified:\n\nTo upload multiple files, a media type MUST be used as shown under Example: Multipart Form with Multiple Files.\n\nSee Encoding the Media Type for guidance and examples, both with and without the field.\n\nSee Encoding Media Types for further guidance and examples, both with and without the field.\n\nA single encoding definition applied to a single schema property. See Appendix B for a discussion of converting values of various types to string representations.\n\nProperties are correlated with parts using the parameter of , and with using the query string parameter names. In both cases, their order is implementation-defined.\n\nSee Appendix E for a detailed examination of percent-encoding concerns for form media types.\n\nThese fields MAY be used either with or without the RFC6570-style serialization fields defined in the next section below.\n\nThis object MAY be extended with Specification Extensions.\n\nThe default values for are as follows, where an n/a in the column means that the presence or value of is irrelevant:\n\nDetermining how to handle a value of depends on how values are being serialized. If values are entirely omitted, then the is irrelevant. See Appendix B for a discussion of data type conversion options.\n\nSee also Appendix C: Using RFC6570 Implementations for additional guidance, including on difficulties caused by the interaction between RFC6570's percent-encoding rules and the media type.\n\nNote that the presence of at least one of , , or with an explicit value is equivalent to using with Parameter Objects. The absence of all three of those fields is the equivalent of using , but with the media type specified in rather than through a Media Type Object.\n\nTo submit content using form url encoding via RFC1866, use the media type in the Media Type Object under the Request Body Object. This configuration means that the request body MUST be encoded per RFC1866 when passed to the server, after any complex objects have been serialized to a string representation.\n\nSee Appendix E for a detailed examination of percent-encoding concerns for form media types.\n\nWhen there is no field, the serialization strategy is based on the Encoding Object's default values:\n\nWith this example, consider an of and a US-style address (with ZIP+4) as follows:\n\nAssuming the most compact representation of the JSON value (with unnecessary whitespace removed), we would expect to see the following request body, where space characters have been replaced with and , , , and have been percent-encoded to , , , and , respectively:\n\nNote that the keyword is treated as per the Encoding Object's default behavior, and is serialized as-is. If it were treated as , then the serialized value would be a JSON string including quotation marks, which would be percent-encoded as .\n\nHere is the parameter (without ) serialized as instead of , and then encoded per RFC1866:\n\nNote that is a text format, which requires base64-encoding any binary data:\n\nGiven a name of and a solid red 2x2-pixel PNG for , this would produce a request body of:\n\nNote that the padding characters at the end need to be percent-encoded, even with the \"URL safe\" . Some base64-decoding implementations may be able to use the string without the padding per RFC4648. However, this is not guaranteed, so it may be more interoperable to keep the padding and rely on percent-decoding.\n\nIt is common to use as a when transferring forms as request bodies. In contrast to OpenAPI 2.0, a is REQUIRED to define the input parameters to the operation when using content. This supports complex structures as well as supporting mechanisms for multiple file uploads.\n\nThe disposition and its parameter are mandatory for (RFC7578). Array properties are handled by applying the same to multiple parts, as is recommended by RFC7578 for supplying multiple values per form field. See RFC7578 for guidance regarding non-ASCII part names.\n\nVarious other types, most notable (RFC2046) neither require nor forbid specific values, which means care must be taken to ensure that any values used are supported by all relevant software. It is not currently possible to correlate schema properties with unnamed, ordered parts in media types such as , but implementations MAY choose to support such types when is used with a parameter.\n\nNote that there are significant restrictions on what headers can be used with media types in general (RFC2046) and in particular (RFC7578).\n\nNote also that is deprecated for (RFC7578) where binary data is supported, as it is in HTTP.\n\n+Using for a multipart field is equivalent to specifying an Encoding Object with a field containing with a schema that requires the value used in . +If is used for a multipart field that has an Encoding Object with a field containing with a schema that disallows the value from , the result is undefined for serialization and parsing.\n\nNote that as stated in Working with Binary Data, if the Encoding Object's , whether set explicitly or implicitly through its default value rules, disagrees with the in a Schema Object, the SHALL be ignored. Because of this, and because the Encoding Object's defaulting rules do not take the Schema Object's into account, the use of with an Encoding Object is NOT RECOMMENDED.\n\nSee Appendix E for a detailed examination of percent-encoding concerns for form media types.\n\nWhen the field is not used, the encoding is determined by the Encoding Object's defaults:\n\nUsing , we can set more specific types for binary data, or non-JSON formats for complex values. We can also describe headers for each part:\n\nIn accordance with RFC7578, multiple files for a single form field are uploaded using the same name ( in this example) for each file's part:\n\nAs seen in the Encoding Object's field documentation, the empty schema for indicates a media type of .\n\nA container for the expected responses of an operation. The container maps a HTTP response code to the expected response.\n\nThe documentation is not necessarily expected to cover all possible HTTP response codes because they may not be known in advance. However, documentation is expected to cover a successful operation response and any known errors.\n\nThe MAY be used as a default Response Object for all HTTP codes that are not covered individually by the Responses Object.\n\nThe Responses Object MUST contain at least one response code, and if only one response code is provided it SHOULD be the response for a successful operation call.\n\nThis object MAY be extended with Specification Extensions.\n\nA 200 response for a successful operation and a default response for others (implying an error):\n\nDescribes a single response from an API operation, including design-time, static to operations based on the response.\n\nThis object MAY be extended with Specification Extensions.\n\nResponse of an array of a complex type:\n\nResponse with no return value:\n\nA map of possible out-of band callbacks related to the parent operation. Each value in the map is a Path Item Object that describes a set of requests that may be initiated by the API provider and the expected responses. The key value used to identify the Path Item Object is an expression, evaluated at runtime, that identifies a URL to use for the callback operation.\n\nTo describe incoming requests from the API provider independent from another API call, use the field.\n\nThis object MAY be extended with Specification Extensions.\n\nThe key that identifies the Path Item Object is a runtime expression that can be evaluated in the context of a runtime HTTP request/response to identify the URL to be used for the callback request. A simple example might be . However, using a runtime expression the complete HTTP message can be accessed. This includes accessing any part of a body that a JSON Pointer RFC6901 can reference.\n\nFor example, given the following HTTP request:\n\nThe following examples show how the various expressions evaluate, assuming the callback operation has a path parameter named and a query parameter named .\n\nThe following example uses the user provided query string parameter to define the callback URL. This is similar to a webhook, but differs in that the callback only occurs because of the initial request that sent the .\n\nThe following example shows a callback where the server is hard-coded, but the query string parameters are populated from the and property in the request body.\n\nAn object grouping an internal or external example value with basic and metadata. This object is typically used in fields named (plural), and is a referenceable alternative to older (singular) fields that do not support referencing or metadata.\n\nExamples allow demonstration of the usage of properties, parameters and objects within OpenAPI.\n\nThis object MAY be extended with Specification Extensions.\n\nIn all cases, the example value SHOULD be compatible with the schema of its associated value. Tooling implementations MAY choose to validate compatibility automatically, and reject the example value(s) if incompatible.\n\nExample Objects can be used in both Parameter Objects and Media Type Objects. In both Objects, this is done through the (plural) field. However, there are several other ways to provide examples: The (singular) field that is mutually exclusive with in both Objects, and two keywords (the deprecated singular and the current plural , which takes an array of examples) in the Schema Object that appears in the field of both Objects. Each of these fields has slightly different considerations.\n\nThe Schema Object's fields are used to show example values without regard to how they might be formatted as parameters or within media type representations. The array is part of JSON Schema and is the preferred way to include examples in the Schema Object, while is retained purely for compatibility with older versions of the OpenAPI Specification.\n\nThe mutually exclusive fields in the Parameter or Media Type Objects are used to show example values which SHOULD both match the schema and be formatted as they would appear as a serialized parameter or within a media type representation. The exact serialization and encoding is determined by various fields in the Parameter Object, or in the Media Type Object's Encoding Object. Because examples using these fields represent the final serialized form of the data, they SHALL override any in the corresponding Schema Object.\n\nThe singular field in the Parameter or Media Type Object is concise and convenient for simple examples, but does not offer any other advantages over using Example Objects under .\n\nSome examples cannot be represented directly in JSON or YAML. For all three ways of providing examples, these can be shown as string values with any escaping necessary to make the string valid in the JSON or YAML format of documents that comprise the OpenAPI Description. With the Example Object, such values can alternatively be handled through the field.\n\nTwo different uses of JSON strings:\n\nFirst, a request or response body that is just a JSON string (not an object containing a string):\n\nIn the above example, we can just show the JSON string (or any JSON value) as-is, rather than stuffing a serialized JSON value into a JSON string, which would have looked like .\n\nIn this example, the JSON string had to be serialized before encoding it into the URL form value, so the example includes the quotation marks that are part of the JSON serialization, which are then URL percent-encoded.\n\nThe Link Object represents a possible design-time link for a response. The presence of a link does not guarantee the caller's ability to successfully invoke it, rather it provides a known relationship and traversal mechanism between responses and other operations.\n\nUnlike dynamic links (i.e. links provided in the response payload), the OAS linking mechanism does not require link information in the runtime response.\n\nFor computing links and providing instructions to execute them, a runtime expression is used for accessing values in an operation and using them as parameters while invoking the linked operation.\n\nThis object MAY be extended with Specification Extensions.\n\nA linked operation MUST be identified using either an or . The identified or reference operation MUST be unique, and in the case of an , it MUST be resolved within the scope of the OpenAPI Description (OAD). Because of the potential for name clashes, the syntax is preferred for multi-document OADs. However, because use of an operation depends on its URL path template in the Paths Object, operations from any Path Item Object that is referenced multiple times within the OAD cannot be resolved unambiguously. In such ambiguous cases, the resulting behavior is implementation-defined and MAY result in an error.\n\nNote that it is not possible to provide a constant value to that matches the syntax of a runtime expression. It is possible to have ambiguous parameter names, e.g. and ; this is NOT RECOMMENDED and the behavior is implementation-defined, however implementations SHOULD prefer the qualified interpretation ( as a path parameter), as the names can always be qualified to disambiguate them (e.g. using for the query parameter).\n\nComputing a link from a request operation where the is used to pass a request parameter to the linked operation.\n\nWhen a runtime expression fails to evaluate, no parameter value is passed to the target operation.\n\nValues from the response body can be used to drive a linked operation.\n\nClients follow all links at their discretion. Neither permissions nor the capability to make a successful call to that link is guaranteed solely by the existence of a relationship.\n\nAs references to MAY NOT be possible (the is an optional field in an Operation Object), references MAY also be made through a relative :\n\nNote that in the use of the escaped forward-slash is necessary when using JSON Pointer, and it is necessary to URL-encode and as and , respectively, when using JSON Pointer as URI fragments.\n\nRuntime expressions allow defining values based on information that will only be available within the HTTP message in an actual API call. This mechanism is used by Link Objects and Callback Objects.\n\nThe runtime expression is defined by the following ABNF syntax\n\nHere, is taken from RFC6901, from RFC7159 and from RFC7230.\n\nThe identifier is case-sensitive, whereas is not.\n\nThe table below provides examples of runtime expressions and examples of their use in a value:\n\nRuntime expressions preserve the type of the referenced value. Expressions can be embedded into string values by surrounding the expression with curly braces.\n\nDescribes a single header for HTTP responses and for individual parts in representations; see the relevant Response Object and Encoding Object documentation for restrictions on which headers can be described.\n\nThe Header Object follows the structure of the Parameter Object, including determining its serialization strategy based on whether or is present, with the following changes:\n• MUST NOT be specified, it is given in the corresponding map.\n• MUST NOT be specified, it is implicitly in .\n• All traits that are affected by the location MUST be applicable to a location of (for example, ). This means that and MUST NOT be used, and , if used, MUST be limited to .\n\nThese fields MAY be used with either or .\n\nThis object MAY be extended with Specification Extensions.\n\nFixed Fields for use with\n\nFor simpler scenarios, a and can describe the structure and syntax of the header. When or are provided in conjunction with the field, the example MUST follow the prescribed serialization strategy for the header.\n\nSerializing with is NOT RECOMMENDED for headers with parameters (name=value pairs following a ) in their values, or where values might have non-URL-safe characters; see Appendix D for details.\n\nWhen or are provided in conjunction with the field, the example SHOULD match the specified schema and follow the prescribed serialization strategy for the header. The and fields are mutually exclusive, and if either is present it SHALL override any in the schema.\n\nSee also Appendix C: Using RFC6570-Based Serialization for additional guidance.\n\nFixed Fields for use with\n\nFor more complex scenarios, the field can define the media type and schema of the header, as well as give examples of its use. Using with a media type is RECOMMENDED for headers where the strategy is not appropriate.\n\nRequiring that a strong header (with a value starting with rather than ) is present. Note the use of , because using and would require the to be percent-encoded as :\n\nAdds metadata to a single tag that is used by the Operation Object. It is not mandatory to have a Tag Object per tag defined in the Operation Object instances.\n\nThis object MAY be extended with Specification Extensions.\n\nA simple object to allow referencing other components in the OpenAPI Description, internally and externally.\n\nThe string value contains a URI RFC3986, which identifies the value being referenced.\n\nSee the rules for resolving Relative References.\n\nThis object cannot be extended with additional properties, and any properties added SHALL be ignored.\n\nNote that this restriction on additional properties is a difference between Reference Objects and Schema Objects that contain a keyword.\n\nThe Schema Object allows the definition of input and output data types. These types can be objects, but also primitives and arrays. This object is a superset of the JSON Schema Specification Draft 2020-12. The empty schema (which allows any instance to validate) MAY be represented by the boolean value and a schema which allows no instance to validate MAY be represented by the boolean value .\n\nFor more information about the keywords, see JSON Schema Core and JSON Schema Validation.\n\nUnless stated otherwise, the keyword definitions follow those of JSON Schema and do not add any additional semantics; this includes keywords such as , , , and being URIs rather than URLs. Where JSON Schema indicates that behavior is defined by the application (e.g. for annotations), OAS also defers the definition of semantics to the application consuming the OpenAPI document.\n\nThe OpenAPI Schema Object dialect is defined as requiring the OAS base vocabulary, in addition to the vocabularies as specified in the JSON Schema Specification Draft 2020-12 general purpose meta-schema.\n\nThe OpenAPI Schema Object dialect for this version of the specification is identified by the URI (the \"OAS dialect schema id\").\n\nThe following keywords are taken from the JSON Schema specification but their definitions have been extended by the OAS:\n• description - CommonMark syntax MAY be used for rich text representation.\n• format - See Data Type Formats for further details. While relying on JSON Schema's defined formats, the OAS offers a few additional predefined formats.\n\nIn addition to the JSON Schema keywords comprising the OAS dialect, the Schema Object supports keywords from any other vocabularies, or entirely arbitrary properties.\n\nJSON Schema implementations MAY choose to treat keywords defined by the OpenAPI Specification's base vocabulary as unknown keywords, due to its inclusion in the OAS dialect with a value of . The OAS base vocabulary is comprised of the following keywords:\n\nThis object MAY be extended with Specification Extensions, though as noted, additional properties MAY omit the prefix within this object.\n\nJSON Schema Draft 2020-12 supports collecting annotations, including treating unrecognized keywords as annotations. OAS implementations MAY use such annotations, including extensions not recognized as part of a declared JSON Schema vocabulary, as the basis for further validation. Note that JSON Schema Draft 2020-12 does not require an prefix for extensions.\n\nThe keyword (when using default format-annotation vocabulary) and the , , and keywords define constraints on the data, but are treated as annotations instead of being validated directly. Extended validation is one way that these constraints MAY be enforced.\n\nThe and keywords are annotations, as JSON Schema is not aware of how the data it is validating is being used. Validation of these keywords MAY be done by checking the annotation, the read or write direction, and (if relevant) the current value of the field. JSON Schema Validation Draft 2020-12 §9.4 defines the expectations of these keywords, including that a resource (described as the \"owning authority\") MAY either ignore a field or treat it as an error.\n\nFields that are both required and read-only are an example of when it is beneficial to ignore a constraint in a PUT, particularly if the value has not been changed. This allows correctly requiring the field on a GET and still using the same representation and schema with PUT. Even when read-only fields are not required, stripping them is burdensome for clients, particularly when the JSON data is complex or deeply nested.\n\nNote that the behavior of in particular differs from that specified by version 3.0 of this specification.\n\nThe OpenAPI Specification allows combining and extending model definitions using the keyword of JSON Schema, in effect offering model composition. takes an array of object definitions that are validated independently but together compose a single object.\n\nWhile composition offers model extensibility, it does not imply a hierarchy between the models. To support polymorphism, the OpenAPI Specification adds the field. When used, the indicates the name of the property that hints which schema definition is expected to validate the structure of the model. As such, the field MUST be a required field. There are two ways to define the value of a discriminator for an inheriting instance.\n• Use the schema name.\n• Override the schema name by overriding the property with a new value. If a new value exists, this takes precedence over the schema name.\n\nImplementations MAY support defining generic or template data structures using JSON Schema's dynamic referencing feature:\n• identifies a set of possible schemas (including a default placeholder schema) to which a can resolve\n• resolves to the first matching encountered on its path from the schema entry point to the reference, as described in the JSON Schema specification\n\nAn example is included in the \"Schema Object Examples\" section below, and further information can be found on the Learn OpenAPI site's \"Dynamic References\" page.\n\nThe Schema Object's keyword does not allow associating descriptions or other information with individual values.\n\nImplementations MAY support recognizing a or where each subschema in the keyword's array consists of a keyword and annotations such as or as an enumerated type with additional information. The exact behavior of this pattern beyond what is required by JSON Schema is implementation-defined.\n\nThe xml field allows extra definitions when translating the JSON definition to XML. The XML Object contains additional information about the available options.\n\nIt is important for tooling to be able to determine which dialect or meta-schema any given resource wishes to be processed with: JSON Schema Core, JSON Schema Validation, OpenAPI Schema dialect, or some custom meta-schema.\n\nThe keyword MAY be present in any Schema Object that is a schema resource root, and if present MUST be used to determine which dialect should be used when processing the schema. This allows use of Schema Objects which comply with other drafts of JSON Schema than the default Draft 2020-12 support. Tooling MUST support the OAS dialect schema id, and MAY support additional values of .\n\nTo allow use of a different default value for all Schema Objects contained within an OAS document, a value may be set within the OpenAPI Object. If this default is not set, then the OAS dialect schema id MUST be used for these Schema Objects. The value of within a resource root Schema Object always overrides any default.\n\nFor standalone JSON Schema documents that do not set , or for Schema Objects in OpenAPI description documents that are not complete documents, the dialect SHOULD be assumed to be the OAS dialect. However, for maximum interoperability, it is RECOMMENDED that OpenAPI description authors explicitly set the dialect through in such documents.\n\nWhen request bodies or response payloads may be one of a number of different schemas, a Discriminator Object gives a hint about the expected schema of the document. This hint can be used to aid in serialization, deserialization, and validation. The Discriminator Object does this by implicitly or explicitly associating the possible values of a named property with alternative schemas.\n\nNote that MUST NOT change the validation outcome of the schema.\n\nThis object MAY be extended with Specification Extensions.\n\nConditions for Using the Discriminator Object\n\nThe Discriminator Object is legal only when using one of the composite keywords , , .\n\nIn both the and use cases, where those keywords are adjacent to , all possible schemas MUST be listed explicitly.\n\nTo avoid redundancy, the discriminator MAY be added to a parent schema definition, and all schemas building on the parent schema via an construct may be used as an alternate schema.\n\nThe form of is only useful for non-validation use cases; validation with the parent schema with this form of does not perform a search for child schemas or use them in validation in any way. This is because cannot change the validation outcome, and no standard JSON Schema keyword connects the parent schema to the child schemas.\n\nThe behavior of any configuration of , , and that is not described above is undefined.\n\nThe value of the property named in is used as the name of the associated schema under the Components Object, unless a is present for that value. The entry maps a specific property value to either a different schema component name, or to a schema identified by a URI. When using implicit or explicit schema component names, inline or subschemas are not considered. The behavior of a value that is both a valid schema name and a valid relative URI reference is implementation-defined, but it is RECOMMENDED that it be treated as a schema name. To ensure that an ambiguous value (e.g. ) is treated as a relative URI reference by all implementations, authors MUST prefix it with the path segment (e.g. ).\n\nMapping keys MUST be string values, but tooling MAY convert response values to strings for comparison. However, the exact nature of such conversions are implementation-defined.\n\nFor these examples, assume all schemas are in the entry document of the OAD; for handling of in referenced documents see Resolving Implicit Connections.\n\nIn OAS 3.x, a response payload MAY be described to be exactly one of any number of types:\n\nwhich means the payload MUST, by validation, match exactly one of the schemas described by , , or . Deserialization of a can be a costly operation, as it requires determining which schema matches the payload and thus should be used in deserialization. This problem also exists for schemas. A MAY be used as a \"hint\" to improve the efficiency of selection of the matching schema. The field cannot change the validation result of the , it can only help make the deserialization more efficient and provide better error messaging. We can specify the exact field that tells us which schema is expected to match the instance:\n\nThe expectation now is that a property with name MUST be present in the response payload, and the value will correspond to the name of a schema defined in the OpenAPI Description. Thus the response payload:\n\nwill indicate that the schema is expected to match this payload.\n\nIn scenarios where the value of the field does not match the schema name or implicit mapping is not possible, an optional definition MAY be used:\n\nHere the discriminating value of will map to the schema , rather than the default (implicit) value of . If the discriminating value does not match an implicit or explicit mapping, no schema can be determined and validation SHOULD fail.\n\nWhen used in conjunction with the construct, the use of the discriminator can avoid ambiguity for serializers/deserializers where multiple schemas may satisfy a single payload.\n\nThis example shows the usage, which avoids needing to reference all child schemas in the parent:\n\nValidated against the schema, a payload like this:\n\nwill indicate that the schema is expected to match. Likewise this payload:\n\nwill map to because the entry in the element maps to which is the schema name for .\n\nA metadata object that allows for more fine-tuned XML model definitions.\n\nWhen using arrays, XML element names are not inferred (for singular/plural forms) and the field SHOULD be used to add that information. See examples for expected behavior.\n\nThis object MAY be extended with Specification Extensions.\n\nThe field is intended to match the syntax of XML namespaces, although there are a few caveats:\n• Versions 3.1.0, 3.0.3, and earlier of this specification erroneously used the term \"absolute URI\" instead of \"non-relative URI\", so authors using namespaces that include a fragment should check tooling support carefully.\n• XML allows but discourages relative URI-references, while this specification outright forbids them.\n• XML 1.1 allows IRIs (RFC3987) as namespaces, and specifies that namespaces are compared without any encoding or decoding, which means that IRIs encoded to meet this specification's URI syntax requirement cannot be compared to IRIs as-is.\n\nEach of the following examples represent the value of the keyword in a Schema Object that is omitted for brevity. The JSON and YAML representations of the value are followed by an example XML representation produced for the single property shown.\n\nIn this example, a full model definition is shown.\n\nThe external field has no effect on the XML:\n\nEven when the array is wrapped, if a name is not explicitly defined, the same name will be used both internally and externally:\n\nTo overcome the naming problem in the example above, the following definition can be used:\n\nIf we change the external element but not the internal ones:\n\nDefines a security scheme that can be used by the operations.\n\nSupported schemes are HTTP authentication, an API key (either as a header, a cookie parameter or as a query parameter), mutual TLS (use of a client certificate), OAuth2's common flows (implicit, password, client credentials and authorization code) as defined in RFC6749, and [[OpenID-Connect-Core]]. Please note that as of 2020, the implicit flow is about to be deprecated by OAuth 2.0 Security Best Current Practice. Recommended for most use cases is Authorization Code Grant flow with PKCE.\n\nThis object MAY be extended with Specification Extensions.\n\nAllows configuration of the supported OAuth Flows.\n\nThis object MAY be extended with Specification Extensions.\n\nThis object MAY be extended with Specification Extensions.\n\nLists the required security schemes to execute this operation. The name used for each property MUST correspond to a security scheme declared in the Security Schemes under the Components Object.\n\nA Security Requirement Object MAY refer to multiple security schemes in which case all schemes MUST be satisfied for a request to be authorized. This enables support for scenarios where multiple query parameters or HTTP headers are required to convey security information.\n\nWhen the field is defined on the OpenAPI Object or Operation Object and contains multiple Security Requirement Objects, only one of the entries in the list needs to be satisfied to authorize the request. This enables support for scenarios where the API allows multiple, independent security schemes.\n\nAn empty Security Requirement Object ( ) indicates anonymous access is supported.\n\nSee also Appendix F: Resolving Security Requirements in a Referenced Document for an example using Security Requirement Objects in multi-document OpenAPI Descriptions.\n\nOptional OAuth2 security as would be defined in an OpenAPI Object or an Operation Object:\n\nWhile the OpenAPI Specification tries to accommodate most use cases, additional data can be added to extend the specification at certain points.\n\nThe extensions properties are implemented as patterned fields that are always prefixed by .\n\nThe OpenAPI Initiative maintains several [[OpenAPI-Registry|extension registries]], including registries for individual extension keywords and extension keyword namespaces.\n\nExtensions are one of the best ways to prove the viability of proposed additions to the specification. It is therefore RECOMMENDED that implementations be designed for extensibility to support community experimentation.\n\nSupport for any one extension is OPTIONAL, and support for one extension does not imply support for others.\n\nSome objects in the OpenAPI Specification MAY be declared and remain empty, or be completely removed, even though they are inherently the core of the API documentation.\n\nThe reasoning is to allow an additional layer of access control over the documentation. While not part of the specification itself, certain libraries MAY choose to allow access to parts of the documentation based on some form of authentication/authorization.\n\nTwo examples of this:\n• The Paths Object MAY be present but empty. It may be counterintuitive, but this may tell the viewer that they got to the right place, but can't access any documentation. They would still have access to at least the Info Object which may contain additional information regarding authentication.\n• The Path Item Object MAY be empty. In this case, the viewer will be aware that the path exists, but will not be able to see any of its operations or parameters. This is different from hiding the path itself from the Paths Object, because the user will be aware of its existence. This allows the documentation provider to finely control what the viewer can see.\n\nOpenAPI Descriptions use a combination of JSON, YAML, and JSON Schema, and therefore share their security considerations:\n\nIn addition, OpenAPI Descriptions are processed by a wide variety of tooling for numerous different purposes, such as client code generation, documentation generation, server side routing, and API testing. OpenAPI Description authors must consider the risks of the scenarios where the OpenAPI Description may be used.\n\nAn OpenAPI Description describes the security schemes used to protect the resources it defines. The security schemes available offer varying degrees of protection. Factors such as the sensitivity of the data and the potential impact of a security breach should guide the selection of security schemes for the API resources. Some security schemes, such as basic auth and OAuth Implicit flow, are supported for compatibility with existing APIs. However, their inclusion in OpenAPI does not constitute an endorsement of their use, particularly for highly sensitive data or operations.\n\nOpenAPI Descriptions may contain references to external resources that may be dereferenced automatically by consuming tools. External resources may be hosted on different domains that may be untrusted.\n\nReferences in an OpenAPI Description may cause a cycle. Tooling must detect and handle cycles to prevent resource exhaustion.\n\nCertain fields allow the use of Markdown which can contain HTML including script. It is the responsibility of tooling to appropriately sanitize the Markdown.\n\nSerializing typed data to plain text, which can occur in message bodies or parts, as well as in the format in either URL query strings or message bodies, involves significant implementation- or application-defined behavior.\n\nSchema Objects validate data based on the JSON Schema data model, which only recognizes four primitive data types: strings (which are only broadly interoperable as UTF-8), numbers, booleans, and . Notably, integers are not a distinct type from other numbers, with being a convenience defined mathematically, rather than based on the presence or absence of a decimal point in any string representation.\n\nThe Parameter Object, Header Object, and Encoding Object offer features to control how to arrange values from array or object types. They can also be used to control how strings are further encoded to avoid reserved or illegal characters. However, there is no general-purpose specification for converting schema-validated non-UTF-8 primitive data types (or entire arrays or objects) to strings.\n• RFC3987 provides guidance for converting non-Unicode strings to UTF-8, particularly in the context of URIs (and by extension, the form media types which use the same encoding rules)\n• RFC6570 specifies which values, including but not limited to , are considered undefined and therefore treated specially in the expansion process when serializing based on that specification\n\nImplementations of RFC6570 often have their own conventions for converting non-string values, but these are implementation-specific and not defined by the RFC itself. This is one reason for the OpenAPI Specification to leave these conversions as implementation-defined: It allows using RFC6570 implementations regardless of how they choose to perform the conversions.\n\nTo control the serialization of numbers, booleans, and (or other values RFC6570 deems to be undefined) more precisely, schemas can be defined as and constrained using , , , and other keywords to communicate how applications must pre-convert their data prior to schema validation. The resulting strings would not require any further type conversion.\n\nThe keyword can assist in serialization. Some formats (such as ) are unambiguous, while others (such as in the Format Registry) are less clear. However, care must be taken with to ensure that the specific formats are supported by all relevant tools as unrecognized formats are ignored.\n\nRequiring input as pre-formatted, schema-validated strings also improves round-trip interoperability as not all programming languages and environments support the same data types.\n\nSerialization is defined in terms of RFC6570 URI Templates in three scenarios:\n\nImplementations of this specification MAY use an implementation of RFC6570 to perform variable expansion, however, some caveats apply.\n\nNote that when using RFC6570 expansion to produce an HTTP message body, it is necessary to remove the prefix that is produced to satisfy the URI query string syntax.\n\nWhen using and similar keywords to produce a body, the query string names are placed in the parameter of the part header, and the values are placed in the corresponding part body; the , , and characters are not used. Note that while RFC7578 allows using [[RFC3986]] percent-encoding in \"file names\", it does not otherwise address the use of percent-encoding within the format. RFC7578 discusses character set and encoding issues for in detail, and it is RECOMMENDED that OpenAPI Description authors read this guidance carefully before deciding to use RFC6570-based serialization with this media type.\n\nNote also that not all RFC6570 implementations support all four levels of operators, all of which are needed to fully support the OpenAPI Specification's usage. Using an implementation with a lower level of support will require additional manual construction of URI Templates to work around the limitations.\n\nCertain field values translate to RFC6570 operators (or lack thereof):\n\nMultiple parameters are equivalent to a single RFC6570 variable list using the prefix operator:\n\nThis example is equivalent to RFC6570's , and NOT . The latter is problematic because if is not defined, the result will be an invalid URI. The prefix operator has no equivalent in the Parameter Object.\n\nNote that RFC6570 does not specify behavior for compound values beyond the single level addressed by . The result of using objects or arrays where no behavior is clearly specified for them is implementation-defined.\n\nDelimiters used by RFC6570 expansion, such as the used to join arrays or object values with , are all automatically percent-encoded as long as is . Note that since RFC6570 does not define a way to parse variables based on a URI Template, users must take care to first split values by delimiter before percent-decoding values that might contain the delimiter character.\n\nWhen is , both percent-encoding (prior to joining values with a delimiter) and percent-decoding (after splitting on the delimiter) must be done manually at the correct time.\n\nSee Appendix E for additional guidance on handling delimiters for values with no RFC6570 equivalent that already need to be percent-encoded when used as delimiters.\n\nConfigurations with no direct RFC6570 equivalent SHOULD also be handled according to RFC6570. Implementations MAY create a properly delimited URI Template with variables for individual names and values using RFC6570 regular or reserved expansion (based on ).\n• the styles , , and , which have no equivalents at all\n• the combination of the style with , which is not allowed because only one prefix operator can be used at a time\n• any parameter name that is not a legal RFC6570 variable name\n\nThe Parameter Object's field has a much more permissive syntax than RFC6570 variable name syntax. A parameter name that includes characters outside of the allowed RFC6570 variable character set MUST be percent-encoded before it can be used in a URI Template.\n\nLet's say we want to use the following data in a form query string, where is exploded, and is not:\n\nThis array of Parameter Objects uses regular expansion, fully supported by RFC6570:\n\nThis translates to the following URI Template:\n\nwhen expanded with the data given earlier, we get:\n\nBut now let's say that (for some reason), we really want that in the formula to show up as-is in the query string, and we want our words to be space-separated like in a written phrase. To do that, we'll add to , and change to for :\n\nWe can't combine the and RFC6570 prefixes, and there's no way with RFC6570 to replace the separator with a space character. So we need to restructure the data to fit a manually constructed URI Template that passes all of the pieces through the right sort of expansion.\n\nHere is one such template, using a made-up convention of for the first entry in the words value, for the second, and for the third:\n\nRFC6570 mentions the use of \"to indicate name hierarchy in substructures,\" but does not define any specific naming convention or behavior for it. Since the usage is not automatic, we'll need to construct an appropriate input structure for this new template.\n\nWe'll also need to pre-process the values for because while and most other reserved characters are allowed in the query string by RFC3986, , , and are not, and , , and all have special behavior in the format, which is what we are using in the query string.\n\nSetting does not make reserved characters that are not allowed in URIs allowed, it just allows them to be passed through expansion unchanged. Therefore, any tooling still needs to percent-encode those characters because reserved expansion will not do it, but it will leave the percent-encoded triples unchanged. See also Appendix E for further guidance on percent-encoding and form media types, including guidance on handling the delimiter characters for , , and in parameter names and values.\n\nSo here is our data structure that arranges the names and values to suit the template above, where values for have pre-percent encoded (although only appears in this example):\n\nExpanding our manually assembled template with our restructured data yields the following query string:\n\nThe and the pre-percent-encoded have been left alone, but the disallowed character (inside a value) and space characters (in the template but outside of the expanded variables) were percent-encoded.\n\nCare must be taken when manually constructing templates to handle the values that RFC6570 considers to be undefined correctly:\n\nUsing this data with our original RFC6570-friendly URI Template, , produces the following:\n\nThis means that the manually constructed URI Template and restructured data need to leave out the object entirely so that the parameter is the first and only parameter in the query string.\n\nIn this example, the heart emoji is not legal in URI Template names (or URIs):\n\nWe can't just pass to an RFC6570 implementation. Instead, we have to pre-percent-encode the name (which is a six-octet UTF-8 sequence) in both the data and the URI Template:\n\nThis will expand to the result:\n\nRFC6570's percent-encoding behavior is not always appropriate for and parameters. In many cases, it is more appropriate to use with a media type such as and require the application to assemble the correct string.\n\nFor both RFC6265 cookies and HTTP headers using the RFC8941 structured fields syntax, non-ASCII content is handled using base64 encoding ( ). Note that the standard base64-encoding alphabet includes non-URL-safe characters that are percent-encoded by RFC6570 expansion; serializing values through both encodings is NOT RECOMMENDED. While also supports the encoding, which is URL-safe, the header and cookie RFCs do not mention this encoding.\n\nMost HTTP headers predate the structured field syntax, and a comprehensive assessment of their syntax and encoding rules is well beyond the scope of this specification. While RFC8187 recommends percent-encoding HTTP (header or trailer) field parameters, these parameters appear after a character. With , that delimiter would itself be percent-encoded, violating the general HTTP field syntax.\n\nUsing with is ambiguous for a single value, and incorrect for multiple values. This is true whether the multiple values are the result of using or not.\n\nThis style is specified to be equivalent to RFC6570 form expansion which includes the character (see Appendix C for more details), which is not part of the cookie syntax. However, examples of this style in past versions of this specification have not included the prefix, suggesting that the comparison is not exact. Because implementations that rely on an RFC6570 implementation and those that perform custom serialization based on the style example will produce different results, it is implementation-defined as to which of the two results is correct.\n\nFor multiple values, is always incorrect as name=value pairs in cookies are delimited by (a semicolon followed by a space character) rather than .\n\nNOTE: In this section, the and media types are abbreviated as and , respectively, for readability.\n\nPercent-encoding is used in URIs and media types that derive their syntax from URIs. This process is concerned with three sets of characters, the names of which vary among specifications but are defined as follows for the purposes of this section:\n• unreserved characters do not need to be percent-encoded; while it is safe to percent-encode them, doing so produces a URI that is not normalized\n• reserved characters either have special behavior in the URI syntax (such as delimiting components) or are reserved for other specifications that need to define special behavior (e.g. defines special behavior for , , and )\n• unsafe characters are known to cause problems when parsing URIs in certain environments\n\nUnless otherwise specified, this section uses RFC3986's definition of reserved and unreserved, and defines the unsafe set as all characters not included in either of those sets.\n\nEach URI component (such as the query string) considers some of the reserved characters to be unsafe, either because they serve as delimiters between the components (e.g. ), or (in the case of and ) were historically considered globally unsafe but were later given reserved status for limited purposes.\n\nReserved characters with no special meaning defined within a component can be left un-percent encoded. However, other specifications can define special meanings, requiring percent-encoding for those characters outside of the additional special meanings.\n\nThe media type defines special meanings for and as delimiters, and as the replacement for the space character (instead of its percent-encoded form of ). This means that while these three characters are reserved-but-allowed in query strings by RFC3986, they must be percent-encoded in query strings except when used for their purposes; see Appendix C for an example of handling in form values.\n\nRFC7578 suggests RFC3986-based percent-encoding as a mechanism to keep text-based per-part header data such as file names within the ASCII character set. This suggestion was not part of older (pre-2015) specifications for , so care must be taken to ensure interoperability.\n\nThe media type allows arbitrary text or binary data in its parts, so percent-encoding is not needed and is likely to cause interoperability problems unless the of the part is defined to require it.\n\nURI percent encoding and the media type have complex specification histories spanning multiple revisions and, in some cases, conflicting claims of ownership by different standards bodies. Unfortunately, these specifications each define slightly different percent-encoding rules, which need to be taken into account if the URIs or message bodies will be subject to strict validation. (Note that many URI parsers do not perform validation by default.)\n\nThis specification normatively cites the following relevant standards:\n\nStyle-based serialization is used in the Parameter Object when is present, and in the Encoding Object when at least one of , , or is present. See Appendix C for more details of RFC6570's two different approaches to percent-encoding, including an example involving .\n\nContent-based serialization is defined by the Media Type Object, and used with the Parameter Object when the field is present, and with the Encoding Object based on the field when the fields , , and are absent. Each part is encoded based on the media type (e.g. or ), and must then be percent-encoded for use in a string.\n\nNote that content-based serialization for does not expect or require percent-encoding in the data, only in per-part header values.\n\nIn most cases, generating query strings in strict compliance with [[RFC3986]] is sufficient to pass validation (including JSON Schema's and ), but some implementations still expect the slightly more restrictive [[RFC1738]] rules to be used.\n\nSince all RFC1738-compliant URIs are compliant with RFC3986, applications needing to ensure historical interoperability SHOULD use RFC1738's rules.\n\nWHATWG is a web browser-oriented standards group that has defined a \"URL Living Standard\" for parsing and serializing URLs in a browser context, including parsing and serializing data. WHATWG's percent-encoding rules for query strings are different depending on whether the query string is being treated as (where it requires more percent-encoding than [[RFC1738]]) or as part of the generic syntax, where it allows characters that [[RFC3986]] forbids.\n\nImplementations needing maximum compatibility with web browsers SHOULD use WHATWG's percent-encoding rules. However, they SHOULD NOT rely on WHATWG's less stringent generic query string rules, as the resulting URLs would fail RFC3986 validation, including JSON Schema's and .\n\nThe percent-decoding algorithm does not care which characters were or were not percent-decoded, which means that URIs percent-encoded according to any specification will be decoded correctly.\n\nSimilarly, all decoding algorithms simply add -for-space handling to the percent-decoding algorithm, and will work regardless of the encoding specification used.\n\nHowever, care must be taken to use decoding if represents a space, and to use regular percent-decoding if represents itself as a literal value.\n\nThe , , , and space characters, which are used as delimiters for the , , and styles, respectively, all MUST be percent-encoded to comply with [[RFC3986]]. This requires users to pre-encode the character(s) in some other way in parameter names and values to distinguish them from the delimiter usage when using one of these styles.\n\nThe space character is always illegal and encoded in some way by all implementations of all versions of the relevant standards. While one could use the convention of to distinguish spaces in parameter names and values from delimiters encoded as , the specifications define the decoding as a single pass, making it impossible to distinguish the different usages in the decoded result.\n\nSome environments use , , and possibly unencoded in query strings without apparent difficulties, and WHATWG's generic query string rules do not require percent-encoding them. Code that relies on leaving these delimiters unencoded, while using regular percent-encoding for them within names and values, is not guaranteed to be interoperable across all implementations.\n\nFor maximum interoperability, it is RECOMMENDED to either define and document an additional escape convention while percent-encoding the delimiters for these styles, or to avoid these styles entirely. The exact method of additional encoding/escaping is left to the API designer, and is expected to be performed before serialization and encoding described in this specification, and reversed after this specification's encoding and serialization steps are reversed. This keeps it outside of the processes governed by this specification.\n\nThis appendix shows how to retrieve an HTTP-accessible multi-document OpenAPI Description (OAD) and resolve a Security Requirement Object in the referenced (non-entry) document. See Resolving Implicit Connections for more information.\n\nFirst, the entry document is where parsing begins. It defines the security scheme to be JWT-based, and it defines a Path Item as a reference to a component in another document:\n\nThis entry document references another document, , without using a file extension. This gives the client the flexibility to choose an acceptable format on a resource-by-resource basis, assuming both representations are available:\n\nIn the document, the referenced path item has a Security Requirement for a Security Scheme, . The same Security Scheme exists in the original entry document. As outlined in Resolving Implicit Connections, is resolved with an implementation-defined behavior. However, documented in that section, it is RECOMMENDED that tools resolve component names from the entry document. As with all implementation-defined behavior, it is important to check tool documentation to determine which behavior is supported."
    },
    {
        "link": "https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Scripting/JSON",
        "document": "JavaScript Object Notation (JSON) is a standard text-based format for representing structured data based on JavaScript object syntax. It is commonly used for transmitting data in web applications (e.g., sending some data from the server to the client, so it can be displayed on a web page, or vice versa). You'll come across it quite often, so in this article, we give you all you need to work with JSON using JavaScript, including parsing JSON so you can access data within it, and creating JSON. An understanding of HTML and the fundamentals of CSS, familiarity with JavaScript basics as covered in previous lessons.\n• What JSON is — a very commonly used data format based on JavaScript object syntax.\n• That JSON can also contain arrays.\n• Retrieve JSON as a JavaScript object using mechanisms available in Web APIs (for example, in the Fetch API).\n• Converting between objects and text using and .\n\nNo, really, what is JSON? JSON is a text-based data format following JavaScript object syntax. It represents structured data as a string, which is useful when you want to transmit data across a network. Even though it closely resembles JavaScript object literal syntax, it can be used independently from JavaScript. Many programming environments feature the ability to read (parse) and generate JSON. In JavaScript, the methods for parsing and generating JSON are provided by the object. Note: Converting a string to a native object is called deserialization, while converting a native object to a string so it can be transmitted across the network is called serialization. A JSON string can be stored in its own file, which is basically just a text file with an extension of , and a MIME type of .\n\nAs described above, JSON is a string whose format very much resembles JavaScript object literal format. The following is a valid JSON string representing an object. Note how it is also a valid JavaScript object literal — just with some more syntax restrictions. { \"squadName\": \"Super hero squad\", \"homeTown\": \"Metro City\", \"formed\": 2016, \"secretBase\": \"Super tower\", \"active\": true, \"members\": [ { \"name\": \"Molecule Man\", \"age\": 29, \"secretIdentity\": \"Dan Jukes\", \"powers\": [\"Radiation resistance\", \"Turning tiny\", \"Radiation blast\"] }, { \"name\": \"Madame Uppercut\", \"age\": 39, \"secretIdentity\": \"Jane Wilson\", \"powers\": [ \"Million tonne punch\", \"Damage resistance\", \"Superhuman reflexes\" ] }, { \"name\": \"Eternal Flame\", \"age\": 1000000, \"secretIdentity\": \"Unknown\", \"powers\": [ \"Immortality\", \"Heat Immunity\", \"Inferno\", \"Teleportation\", \"Interdimensional travel\" ] } ] } If you load this JSON in your JavaScript program as a string, you can parse it into a normal object and then access the data inside it using the same dot/bracket notation we looked at in the JavaScript object basics article. For example:\n• First, we have the variable name — .\n• Inside that, we want to access the property, so we use .\n• contains an array populated by objects. We want to access the second object inside the array, so we use .\n• Inside this object, we want to access the property, so we use .\n• Inside the property is an array containing the selected hero's superpowers. We want the third one, so we use . The key takeaway is that there's really nothing special about working with JSON; after you've parsed it into a JavaScript object, you work with it just like you would with an object declared using the same object literal syntax. Note: We've made the JSON seen above available inside a variable in our JSONTest.html example (see the source code). Try loading this up and then accessing data inside the variable via your browser's JavaScript console.\n\nAbove we mentioned that JSON text basically looks like a JavaScript object inside a string. We can also convert arrays to/from JSON. The below example is perfectly valid JSON: You have to access array items (in its parsed version) by starting with an array index, for example . The JSON can also contain a single primitive. For example, , , or are all valid JSON.\n\nTo begin with, make local copies of our heroes.html and style.css files. The latter contains some simple CSS to style our page, while the former contains some very simple body HTML, plus a element to contain the JavaScript code we will be writing in this exercise: We have made our JSON data available on our GitHub, at https://mdn.github.io/learning-area/javascript/oojs/json/superheroes.json. We are going to load the JSON into our script, and use some nifty DOM manipulation to display it, like this:\n\nThe top-level function looks like this: To obtain the JSON, we use an API called Fetch. This API allows us to make network requests to retrieve resources from a server via JavaScript (e.g. images, text, JSON, even HTML snippets), meaning that we can update small sections of content without having to reload the entire page. In our function, the first four lines use the Fetch API to fetch the JSON from the server:\n• we declare the variable to store the GitHub URL\n• we use the URL to initialize a new object.\n• we make the network request using the function, and this returns a object\n• we retrieve the response as JSON using the function of the object. Note: The API is asynchronous. You can learn about asynchronous functions in detail in our Asynchronous JavaScript module, but for now, we'll just say that we need to add the keyword before the name of the function that uses the fetch API, and add the keyword before the calls to any asynchronous functions. After all that, the variable will contain the JavaScript object based on the JSON. We are then passing that object to two function calls — the first one fills the with the correct data, while the second one creates an information card for each hero on the team, and inserts it into the .\n\nNow that we've retrieved the JSON data and converted it into a JavaScript object, let's make use of it by writing the two functions we referenced above. First of all, add the following function definition below the previous code: Here we first create an h1 element with , set its to equal the property of the object, then append it to the header using . We then do a very similar operation with a paragraph: create it, set its text content and append it to the header. The only difference is that its text is set to a template literal containing both the and properties of the object.\n\nNext, add the following function at the bottom of the code, which creates and displays the superhero cards: function populateHeroes(obj) { const section = document.querySelector(\"section\"); const heroes = obj.members; for (const hero of heroes) { const myArticle = document.createElement(\"article\"); const myH2 = document.createElement(\"h2\"); const myPara1 = document.createElement(\"p\"); const myPara2 = document.createElement(\"p\"); const myPara3 = document.createElement(\"p\"); const myList = document.createElement(\"ul\"); myH2.textContent = hero.name; myPara1.textContent = `Secret identity: ${hero.secretIdentity}`; myPara2.textContent = `Age: ${hero.age}`; myPara3.textContent = \"Superpowers:\"; const superPowers = hero.powers; for (const power of superPowers) { const listItem = document.createElement(\"li\"); listItem.textContent = power; myList.appendChild(listItem); } myArticle.appendChild(myH2); myArticle.appendChild(myPara1); myArticle.appendChild(myPara2); myArticle.appendChild(myPara3); myArticle.appendChild(myList); section.appendChild(myArticle); } } To start with, we store the property of the JavaScript object in a new variable. This array contains multiple objects that contain the information for each hero. Next, we use a for...of loop to loop through each object in the array. For each one, we:\n• Create several new elements: an , an , three s, and a .\n• Set the to contain the current hero's .\n• Fill the three paragraphs with their , , and a line saying \"Superpowers:\" to introduce the information in the list.\n• Store the property in another new constant called — this contains an array that lists the current hero's superpowers.\n• Use another loop to loop through the current hero's superpowers — for each one we create an element, put the superpower inside it, then put the inside the element ( ) using .\n• The very last thing we do is to append the , s, and inside the ( ), then append the inside the . The order in which things are appended is important, as this is the order they will be displayed inside the HTML. Note: If you are having trouble getting the example to work, try referring to our heroes-finished.html source code (see it running live also.) Note: If you are having trouble following the dot/bracket notation we are using to access the JavaScript object, it can help to have the superheroes.json file open in another tab or your text editor, and refer to it as you look at our JavaScript. You should also refer back to our JavaScript object basics article for more information on dot and bracket notation.\n\nThe above example was simple in terms of accessing the JavaScript object, because we converted the network response directly into a JavaScript object using . But sometimes we aren't so lucky — sometimes we receive a raw JSON string, and we need to convert it to an object ourselves. And when we want to send a JavaScript object across the network, we need to convert it to JSON (a string) before sending it. Luckily, these two problems are so common in web development that a built-in JSON object is available in browsers, which contains the following two methods:\n• : Accepts a JSON string as a parameter, and returns the corresponding JavaScript object.\n• : Accepts an object as a parameter, and returns the equivalent JSON string. You can see the first one in action in our heroes-finished-json-parse.html example (see the source code) — this does exactly the same thing as the example we built up earlier, except that:\n• we retrieve the response as text rather than JSON, by calling the method of the response\n• we then use to convert the text to a JavaScript object. The key snippet of code is here: As you might guess, works the opposite way. Try entering the following lines into your browser's JavaScript console one by one to see it in action: let myObj = { name: \"Chris\", age: 38 }; myObj; let myString = JSON.stringify(myObj); myString; Here we're creating a JavaScript object, then checking what it contains, then converting it to a JSON string using — saving the return value in a new variable — then checking it again."
    },
    {
        "link": "https://json.org",
        "document": ""
    },
    {
        "link": "https://geeksforgeeks.org/sql-for-machine-learning",
        "document": "Integrating SQL with machine learning can provide a powerful framework for managing and analyzing data, especially in scenarios where large datasets are involved. By combining the structured querying capabilities of SQL with the analytical and predictive capabilities of machine learning algorithms, you can create robust data pipelines for various tasks, including predictive modeling, classification, clustering, and more.\n\nThe introduction of SQL for machine learning typically involves understanding how SQL can be leveraged at different stages of the machine learning workflow:\n• Data Retrieval and Preparation : SQL is often used to retrieve data from relational databases or data warehouses. This initial step involves crafting SQL queries to extract relevant data for analysis. Additionally, SQL can be employed to preprocess and clean the data, handling tasks such as filtering, joining, aggregating, and handling missing values.\n• Feature Engineering : SQL's capabilities can be harnessed to perform tasks, where new features are derived from existing data to improve the performance of machine learning models. This might involve creating new variables, transforming data, or generating aggregate statistics.\n• Model Training and Evaluation : While SQL itself isn't typically used for model training, it can play a role in and validation. After training machine learning models using traditional programming languages or frameworks, SQL queries can be used to assess model performance by querying relevant metrics from the data.\n• Deployment and Integration : SQL databases are often used as storage repositories for both training data and trained models. Once a model is trained, SQL queries can facilitate model deployment by enabling real-time or batch predictions directly from the database. This integration ensures seamless interaction between the machine learning model and the data it operates on.\n\nOverall, the integration of SQL with machine learning offers a comprehensive approach to data management, analysis, and modeling. It leverages the strengths of both SQL's relational capabilities and machine learning's predictive power, providing a unified platform for data-driven decision-making.\n\nSQL, or Structured Query Language, is a fundamental skill for anyone involved in working with databases. Acting as a universal language for querying databases, SQL empowers users to efficiently manage, structure, and retrieve data within relational databases. This SQL tutorial PDF aims to offer a thorough exploration of SQL's core concepts, making it an invaluable resource for newcomers eager to enhance their understanding and proficiency in SQL.\n\nGetting started with electronically storing data using SQL requires the setup of a database. This section is dedicated to guiding you through essential processes such as creating, selecting, dropping, and renaming databases, accompanied by practical examples.\n\nTables in SQL serve as structured containers for organizing data into rows and columns. They define the structure of the database by specifying the fields or attributes each record will contain. Tables are fundamental components where data is stored, retrieved, and manipulated through SQL queries.\n\nSQL queries are commands used to interact with databases, enabling retrieval, insertion, updating, and deletion of data. They employ statements like SELECT, INSERT, UPDATE, DELETE to perform operations on database tables. SQL queries allow users to extract valuable insights from data by filtering, aggregating, and manipulating information.\n• None SQL UPDATE FROM ONE TABLE TO ANOTHER\n\nyou'll delve into the power of SQL clauses for efficient database querying. Learn to wield SELECT for data retrieval, WHERE for filtering results, JOIN for combining tables, and GROUP BY for aggregation. Mastering these clauses empowers you to extract valuable insights and perform complex operations on your data.\n• None Use of WITH clause to name Sub-Query\n• None Limiting the number of rows returned using LIMIT.\n• None How to LIMIT the number of data points in output.\n\n\"SQL Operators\" encompass the essential symbols and keywords in SQL that allow users to conduct a range of operations, including SQL AND, OR, LIKE, NOT, among other operators on databases. This section thoroughly examines all SQL operators, providing detailed explanations and examples.\n• None BETWEEN & IN Operator in SQL\n\nSQL functions are built-in operations that perform specific tasks on data stored in a relational database. These functions can manipulate data, perform calculations, format output, and more.\n\nSQL joins act like a weaver's loom, enabling you to seamlessly blend data from various tables through common links. Delve into this section to master the usage of the JOIN command.\n\nViews simplify the process of accessing necessary information by eliminating the need for complex queries. They also serve as a protective measure, safeguarding the most sensitive data while still providing access to the required information.\n\nKnowledge of indexing techniques can significantly enhance query performance, especially when dealing with large datasets. Understanding how to create, use, and optimize indexes can improve the efficiency of SQL queries used in machine learning workflows.\n\nWindow functions enable advanced analytical queries by allowing to perform calculations across a set of rows related to the current row. Incorporating window functions can facilitate tasks such as ranking, partitioning, and calculating moving averages, which can be useful for feature engineering and data analysis in machine learning."
    },
    {
        "link": "https://geeksforgeeks.org/how-to-combine-like-and-in-an-sql-statement",
        "document": "How to Combine LIKE and IN in SQL Statement\n\nThe and operators in SQL are essential for building efficient, complex queries that filter data with precision. Combining these two operators in a single query enables users to target specific patterns in data while also filtering based on predefined values\n\nIn this article, we will explain how the LIKE and IN operators work together and help users navigate complex data collection scenarios. Learn how to precisely filter data, creating focused queries that boost our SQL skills.\n\nSQL Combining 'LIKE' and 'IN' Operator\n\nThe 'LIKE' operator searches for a specified pattern in a column within a string, and the 'IN' operator allows users to filter data based on a set of specified values. By combining LIKE and IN, users can filter results based on patterns and specific values simultaneously, allowing users to create more complex and precise search conditions in SQL statements.\n\nExamples of Combining 'LIKE' and 'IN' Operators\n\nTo illustrate how to combine and operators, let's first create a table called products with product details, showcasing how these operators can be effectively used together to filter data based on both patterns and predefined values. This combination allows for more precise and flexible searches in SQL queries.\n\nExample 1: Filtering Products Starting with 'A' or 'B' in Specific Categories\n\nThis query retrieves products whose names start with 'A' or 'B' and belong to the 'Electronics' or 'Clothing' categories. It uses the operator for pattern matching and the operator to filter by specific categories.\n\nExample 2: Filtering Products Including 'Phone' or 'Speaker' in Electronics\n\nThis query retrieves products that have \"Phone\" or \"Speaker\" in their names and belong to either the 'Electronics' or 'Accessories' categories. It combines the operator for pattern matching with the operator for filtering by categories.\n\nBy combining the and operators in SQL, users can create highly tailored queries that filter data based on specific patterns and predefined values. This approach enhances the power and flexibility of SQL, making it easier to navigate complex datasets and retrieve precise information based on multiple criteria. Mastering this combination can lead to more efficient and focused queries, significantly improving our SQL querying capabilities.\n\nCan you use LIKE and OR in SQL?\n\nIs LIKE and IN combination in SQL?\n\nWhat is LIKE and BETWEEN in SQL?"
    },
    {
        "link": "https://medium.com/artificial-intelligence-101/introduction-to-sql-for-data-analysis-65be5523e919",
        "document": "SQL (Structured Query Language) is a powerful language used for managing and analyzing data in relational databases. It allows users to perform a wide range of data operations, from simple queries to complex aggregations and joins. This article will introduce you to SQL and relational databases, basic SQL queries, table joins, aggregations, and provide an overview of BigQuery for large-scale data analysis.\n\nSQL stands for Structured Query Language, and it is the standard language used for interacting with relational databases. It enables users to perform tasks such as querying data, updating records, inserting new data, and deleting data. SQL is essential for managing and analyzing data stored in databases.\n\nRelational databases store data in tables (also called relations) that are organized into rows and columns. Each table represents a specific entity, and relationships between tables are defined using keys. Relational databases use a schema to define the structure of tables and relationships.\n• Tables: Collections of related data organized in rows and columns.\n• Columns: Attributes or fields of the table.\n• Primary Key: A unique identifier for each record in a table.\n• Foreign Key: A field that links to the primary key of another table.\n\nThe statement is used to retrieve data from one or more tables. You can specify the columns you want to retrieve and filter the results based on certain conditions.\n\nExample: Retrieving All Data from a Table\n\nThis query retrieves all columns and all rows from the table.\n\nThis query retrieves only the , , and columns.\n\nThe clause is used to filter records that meet specific conditions.\n\nThis query retrieves all records from the table where the is 'Sales'.\n\nThis query retrieves all records where the is 'Sales' and the is 'Manager'.\n\nThe clause is used to sort the results of a query.\n\nThis query sorts the results by the column in ascending order.\n\nThis query sorts the results by the column in descending order.\n\nJoining tables allows you to combine data from multiple tables based on related columns.\n\nThe clause retrieves records that have matching values in both tables.\n\nThis query retrieves employee names and their corresponding department names by joining the and tables on the column.\n\nThe clause retrieves all records from the left table and the matched records from the right table. Non-matching records from the right table will have values.\n\nThis query retrieves all employees and their department names, including those who do not belong to any department.\n\nAggregation functions are used to perform calculations on a set of values and return a single value.\n\nThe function returns the number of rows that match a specified condition.\n\nThis query returns the total number of employees.\n\nThe function returns the total sum of a numeric column.\n\nThis query returns the total salary paid to all employees.\n\nThe function returns the average value of a numeric column.\n\nThis query returns the average salary of all employees.\n\nThe clause is used to group rows that have the same values in specified columns.\n\nThis query returns the number of employees in each department.\n\nBigQuery is a fully managed, serverless data warehouse provided by Google Cloud Platform. It is designed to handle large-scale data analysis with ease and efficiency. BigQuery allows users to run SQL queries on massive datasets, providing fast and scalable query performance.\n• Serverless Architecture: No need to manage infrastructure or configure servers.\n• Scalability: Automatically scales to handle large volumes of data and complex queries.\n• High Performance: Optimized for speed, allowing for quick query execution on large datasets.\n• Integration: Seamlessly integrates with other Google Cloud services and data visualization tools.\n\nTo use BigQuery for data analysis, follow these steps:\n• Upload Your Data: Import data into BigQuery by uploading files or connecting to external data sources.\n\nClick the advanced button, and set the delimeter to ; (semicolon). Since the column in the csv is separated by that notation\n• Write SQL Queries: Use SQL queries to analyze your data. BigQuery supports standard SQL syntax.\n• Visualize Results: Connect BigQuery with data visualization tools like Google Data Studio to create dashboards and reports.\n\nThis query retrieves the total number of new COVID-19 cases for each country, ordered by the highest number of cases."
    },
    {
        "link": "https://llamaindex.ai/blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b",
        "document": "In this article, we showcase a powerful new query engine ( ) in LlamaIndex that can leverage both a SQL database as well as a vector store to fulfill complex natural language queries over a combination of structured and unstructured data. This query engine can leverage the expressivity of SQL over structured data, and join it with unstructured context from a vector database. We showcase this query engine on a few examples and show that it can handle queries that make use of both structured/unstructured data, or either.\n\nCheck out the full guide here: https://gpt-index.readthedocs.io/en/latest/examples/query_engine/SQLAutoVectorQueryEngine.html.\n\nData lakes in enterprises typically encompass both structured and unstructured data. Structured data is typically stored in a tabular format in SQL databases, organized into tables with predefined schemas and relationships between entities. On the other hand, unstructured data found in data lakes lacks a predefined structure and does not fit neatly into traditional databases. This type of data includes text documents, but also other multimodal formats such as audio recordings, videos, and more.\n\nLarge Language Models (LLMs) have the ability to extract insights from both structured and unstructured data. There have been some initial tooling and stacks that have emerged for tackling both types of data:\n• Text-to-SQL (Structured data): Given a collection of tabular schemas, we convert natural language into a SQL statement which can then be executed against the database.\n• Semantic Search with a Vector Database (Unstructured Data): Store unstructured documents along with their embeddings in a vector database (e.g. Pinecone, Chroma, Milvus, Weaviate, etc.). During query-time, fetch the relevant documents by embedding similarity, and then put into the LLM input prompt to synthesize a response.\n\nEach of these stacks solves particular use cases.\n\nIn the structured setting, SQL is an extremely expressive language for operating over tabular data — in the case of analytics, you can get aggregations, join information across multiple tables, sort by timestamp, and much more. Using the LLM to convert natural language to SQL can be thought as a program synthesis “cheat code” — just let the LLM compile to the right SQL query, and let the SQL engine on the database handle the rest!\n\nUse Case: Text-to-SQL queries are well-suited for analytics use cases where the answer can be found by executing a SQL statement. They are not suited for cases where you’d need more detail than what is found in a structured table, or if you’d need more sophisticated ways of determining relevance to the query beyond simple constructs like conditions.\n• “What is the average population of cities in North America”?\n• “What are the largest cities and populations in each respective continent?”\n\nIn the unstructured setting, the behavior for retrieval-augmented generation systems is to first perform retrieval and then synthesis. During retrieval, we first look up the most relevant documents to the query by embedding similarity. Some vector stores support being able to handle additional metadata filters for retrieval. We can choose to manually specify the set of required filters, or have the LLM “infer” what the query string and metadata filters should be (see our auto-retrieval modules in LlamaIndex or LangChain’s self-query module).\n\nUse Case: Retrieval Augmented Generation is well suited for queries where the answer can be obtained within some sections of unstructured text data. Most existing vector stores (e.g. Pinecone, Chroma) do not offer a SQL-like interface; hence they are less suited for queries that involve aggregations, joins, sums, etc.\n• “Tell me about the historical museums in Berlin”\n• “What does Jordan ask from Nick on behalf of Gatsby?”\n\nFor some queries, we may want to make use of knowledge in both structured tables as well as vector databases/document stores in order to give the best answer to the query. Ideally this can give us the best of both worlds: the analytics capabilities over structured data, and semantic understanding over unstructured data.\n\nHere’s an example use case:\n• You have access to a collection of articles about different cities, stored in a vector database\n• You also have access to a structured table containing statistics for each city.\n\nGiven this data collection, let’s take an example query: “Tell me about the arts and culture of the city with the highest population.”\n\nThe “proper” way to answer this question is roughly as follows:\n• Query the structured table for the city with the highest population.\n• Convert the original question into a more detailed question: “Tell me about the arts and culture of Tokyo.”\n• Ask the new question over your vector database.\n• Use the original question + intermediate queries/responses to SQL db and vector db to synthesize the answer.\n\nLet’s think about some of the high-level implications of such a sequence:\n• Instead of doing embedding search (and optionally metadata filters) to retrieve relevant context, we want to somehow have a SQL query as a first “retrieval” step.\n• We want to make sure that we can somehow “join” the results from the SQL query with the context stored in the vector database. There is no existing language to “join” information between a SQL and vector database. We will have to implement this behavior ourselves.\n• Neither data source can answer this question on its own. The structured table only contains population information. The vector database contains city information but no easy way to query for the city with the maximum population.\n\nWe have created a brand-new query engine ( ) that can query, join, sequence, and combine both structured data from both your SQL database and unstructured data from your vector database in order to synthesize the final answer.\n\nThe is initialized through passing in a SQL query engine ( ) as well as a query engine that uses our vector store auto-retriever module ( ). Both the SQL query engine and vector query engines are wrapped as “Tool” objects containing a and field.\n\nDuring query-time, we run the following steps:\n• A selector prompt (similarly used in our , see guide) first chooses whether we should query the SQL database or the vector database. If it chooses to use the vector query engine, then the rest of the function execution is the same as querying the with .\n• If it chooses to query the SQL database, it will execute a text-to-SQL query operation against the database, and (optionally) synthesize a natural language output.\n• A query transformation is run, to convert the original question into a more detailed question given the results from the SQL query. For instance if the original question is “Tell me about the arts and culture of the city with the highest population.”, and the SQL query returns Tokyo as the city with the highest population, then the new query is “Tell me about the arts and culture of Tokyo.” The one exception is if the SQL query itself is enough to answer the original question; if it is, then function execution returns with the SQL query as the response.\n• The new query is then run through through the vector store query engine, which performs retrieval from the vector store and then LLM response synthesis. We enforce using a module. This allows us to automatically infer the right query parameters (query string, top k, metadata filters), given the result of the SQL query. For instance, with the example above, we may infer the query to be something like and .\n• The original question, SQL query, SQL response, vector store query, and vector store response are combined into a prompt to synthesize the final answer.\n\nTaking a step back, here are some general comments about this approach:\n• Using our auto-retrieval module is our way of simulating a join between the SQL database and vector database. We effectively use the results from our SQL query to determine the parameters to query the vector database with.\n• This also implies that there doesn’t need to be an explicit mapping between the items in the SQL database and the metadata in the vector database, since we can rely on the LLM being able come up with the right query for different items. It would be interesting to model explicit relationships between structured tables and document store metadata though; that way we don’t need to spend an extra LLM call in the auto-retrieval step inferring the right metadata filters.\n\nSo how well does this work? It works surprisingly well across a broad range of queries, from queries that can leverage both structured data and unstructured data to queries that are specific to a structured data collection or unstructured data collection.\n\nOur experiment setup is very simple. We have a SQL table called which contains the city, population, and country of three different cities: Toronto, Tokyo, and Berlin.\n\nWe also use a Pinecone index to store Wikipedia articles corresponding to the three cities. Each article is chunked up and stored as a separate “Node” object; each chunk also contains a metadata attribute containing the city name.\n\nWe then derive the and from the Pinecone vector index.\n\nYou can also get the SQL query engine as follows\n\nBoth the SQL query engine and vector query engine can be wrapped as objects.\n\nFinally, we can define our\n\nWe run some example queries.\n\nThis query runs through the full flow of the . It first queries the SQL database for the city with the highest population (“Tokyo”), and then queries the vector database with the new query. The results are combined into a final response.\n\nBerlin's history dates back to the early 13th century when it was founded as a small settlement. In 1618, the Margraviate of Brandenburg entered into a personal union with the Duchy of Prussia, and in 1701, they formed the Kingdom of Prussia with Berlin as its capital. The city grew and merged with neighboring cities, becoming a center of the Enlightenment under the rule of Frederick the Great in the 18th century. The Industrial Revolution in the 19th century transformed Berlin, expanding its economy, population, and infrastructure. In 1871, it became the capital of the newly founded German Empire. The early 20th century saw Berlin as a hub for the German Expressionist movement and a major world capital known for its contributions to science, technology, arts, and other fields. In 1933, Adolf Hitler and the Nazi Party came to power, leading to a decline in Berlin's Jewish community and the city's involvement in World War II. After the war, Berlin was divided into East and West Berlin, with the former under Soviet control and the latter under the control of the United States, United Kingdom, and France. The Berlin Wall was built in 1961, physically and ideologically dividing the city until its fall in 1989. Following the reunification of Germany in 1990, Berlin once again became the capital of a unified Germany and has since continued to grow and develop as a major global city.\n\nThis query only requires the vector database and not the SQL database. The initial selector correctly identifies that we should just query the vector database and return the result.\n\nThis query can be answered by just querying the SQL database, it does not need additional information from the vector database. The query transform step correctly identifies “None” as the followup question, indicating that the original question has been answered.\n\nSo far, the stacks around LLMs + unstructured data and LLMs + structured data have largely been separate. We’re excited about how combining LLMs on top of both structured and unstructured data can unlock new retrieval/query capabilities in novel and interesting ways!\n\nWe’d love for you to try out the and let us know what you think.\n\nThe full notebook walkthrough can be found in this guide (associated notebook)."
    },
    {
        "link": "https://aingenx.com/sql-with-ai-for-data-analysis",
        "document": "- Overview of SQL Server: Features and capabilities relevant to data analytics. \n\n - Setting up the SQL Server environment.\n\n - Introduction to databases: Understanding schemas, tables, and relationships.\n\n - Basic SQL: Data types, SELECT statements, WHERE clauses.\n\n - Importing data from CSV files into SQL Server.\n\n - Restoring a database from a backup.\n\n - Brief introduction to AI and its relationship with data analytics.\n\n - Practical exercises: Writing simple queries and managing data import.\n\n - Advanced SELECT techniques: using aliases, filtering, and sorting.\n\n - Conditional statements and logic in SQL (CASE statements).\n\n - Functions: String, date, and numerical functions.\n\n - Aggregations and Grouping: SUM, COUNT, AVG, GROUP BY, and HAVING.\n\n - Introduction to ChatGPT for working with SQL Server and R. \n\n - Discuss data types and structures in AI models using SQL data. \n\n - Practical exercises: Creating detailed reports from raw data.\n\n - Understanding JOINs: INNER, LEFT, RIGHT, and FULL JOINS. \n\n - Using Subqueries: Non-correlated and correlated subqueries.\n\n - Set operations: UNION, INTERSECT, and EXCEPT.\n\n - Introduce basic data preparation for AI in R using exported SQL data.\n\n - Practical exercises: Combining data from multiple tables to generate insights.\n\n - Understanding the concept of SQL Views. \n\n - Advantages and use cases of using views in a database.\n\n - Working with Views\n\n - Syntax for creating simple views.\n\n - Retrieving data from a single table using views.\n\n - Incorporating JOIN operations in views.\n\n - Creating views to retrieve data from multiple related tables.\n\n - Dynamic Views and Stored Procedures: Creating dynamic views using parameters and integrating views with stored procedures.\n\n - Introduce R for data visualization; connect R and SQL Server\n\n - SELECT help with ChatGPT: Utilizing AI to optimize SELECT queries. \n\n - Subqueries with ChatGPT: Guidance on crafting efficient subqueries.\n\n - VIEWS with ChatGPT: Assistance in designing and managing SQL views.\n\n - Stored Procedure with ChatGPT: Integrating AI to streamline the creation and maintenance of stored procedures.\n\n - Basic concepts of machine learning: supervised vs. unsupervised learning.\n\n - Practical application of AI regression models in SQL Server environment.\n\n - Implement machine learning algorithms in R using SQL Server data.\n\n - Identify regression tasks and their relevance to AI.\n\n - Perform linear regression and multivariate regression.\n\n - Predictive modeling and interpretation of results.\n\n - Wrapping up AI and machine learning integration with SQL Server.\n\n\n\n- Comprehensive Curriculum: Our training program offers a comprehensive curriculum that covers both SQL fundamentals and advanced AI techniques for data analysis. From mastering SQL queries to harnessing the power of machine learning algorithms, you'll gain a well-rounded understanding of data analysis techniques. \n\n\n\n - Hands-On Learning: We believe in learning by doing. Our training includes hands-on exercises, projects, and real-world case studies that allow you to apply the concepts you learn in a practical setting. This hands-on approach ensures that you not only understand the theory but also develop practical skills that you can immediately apply in your work. \n\n\n\n - Expert Instructors: Our instructors are industry experts with years of experience in data analysis, SQL, and AI. They are dedicated to providing personalized guidance, answering your questions, and helping you succeed in your learning journey. \n\n\n\n - Career Opportunities: Data analysis skills are in high demand across industries, and proficiency in SQL and AI can open up a wide range of career opportunities. Whether you're looking to advance in your current role or transition to a new career, our training can help you acquire the skills you need to succeed. \n\n\n\n - Continuous Support: Learning doesn't end when the training program concludes. We provide ongoing support and resources to help you stay updated with the latest trends, technologies, and best practices in data analysis, SQL, and AI. \n\n\n\n - Practical Applications: The skills you learn in our training program have practical applications in various industries and job roles. Whether you're analyzing sales data, predicting customer behavior, or optimizing business processes, the techniques you learn will empower you to make data-driven decisions and drive business success.\n\n- Basic Understanding of Data Analysis: While no prior experience with SQL or AI is required, participants should have a basic understanding of data analysis concepts. Familiarity with common data analysis tasks, such as data manipulation, visualization, and interpretation, is beneficial.\n\n\n\n - Comfort with Technology: Participants should feel comfortable using computers and software applications. Basic proficiency in using spreadsheet software (e.g., Microsoft Excel) and familiarity with navigating computer interfaces are recommended.\n\n- Advanced Data Analysis Skills: By mastering SQL and AI techniques, participants gain advanced data analysis skills that are highly sought after in today's job market. They learn how to query databases, manipulate data, and apply machine learning algorithms to extract valuable insights from complex datasets. \n\n\n\n - Enhanced Career Opportunities: Proficiency in SQL and AI opens up a wide range of career opportunities across industries. Participants can pursue roles such as data analyst, business intelligence analyst, data scientist, machine learning engineer, and more, with the potential for career advancement and higher earning potential. \n\n\n\n - Data-Driven Decision Making: With SQL and AI skills, participants can make data-driven decisions with confidence. They learn how to leverage data to identify patterns, trends, and correlations, enabling informed decision-making that drives business success. \n\n\n\n - Increased Efficiency and Productivity: SQL enables efficient data manipulation and retrieval, while AI automates repetitive tasks and enhances productivity. Participants learn how to streamline data analysis workflows, saving time and resources while maximizing efficiency. \n\n\n\n - Innovative Solutions: With AI techniques such as machine learning, participants can develop innovative solutions to complex problems. They learn how to build predictive models, perform sentiment analysis, and uncover insights that drive innovation and competitive advantage. \n\n\n\n - Adaptability to Industry Trends: SQL and AI skills are in high demand across industries, from finance and healthcare to retail and technology. By learning these skills, participants position themselves as adaptable professionals capable of thriving in dynamic and evolving industries. \n\n\n\n - Real-World Applications: The skills learned in SQL with AI for data analysis training have practical applications in various real-world scenarios. Participants work on projects and case studies that simulate professional data analysis tasks, providing valuable hands-on experience that translates directly to the workplace. \n\n\n\n - Continuous Learning and Growth: The field of data analysis is constantly evolving, with new technologies and techniques emerging regularly. By learning SQL and AI, participants embark on a journey of continuous learning and growth, staying updated with the latest trends and advancements in the field.\n\n- High Demand for Data Professionals: There is a growing demand for professionals with skills in SQL and AI for data analysis across industries. Organizations of all sizes rely on data to drive decision-making, and individuals proficient in SQL and AI are essential for extracting insights and deriving value from data. \n\n\n\n - Diverse Career Opportunities: Mastery of SQL and AI opens up diverse career opportunities in roles such as data analyst, business intelligence analyst, data scientist, machine learning engineer, database administrator, and more. These roles exist in industries ranging from finance and healthcare to technology and retail, providing flexibility and opportunities for specialization. \n\n\n\n - Industry Applications: SQL with AI for data analysis training has applications across various industries and domains. Participants can apply their skills to analyze financial data, optimize healthcare processes, enhance marketing strategies, improve customer experiences, and much more. \n\n\n\n - Innovation and Problem Solving: With SQL and AI skills, individuals can drive innovation and solve complex problems using data-driven approaches. They can develop predictive models, automate decision-making processes, identify trends and patterns, and uncover insights that lead to actionable outcomes. \n\n\n\n - Continuous Learning and Growth: The field of data analysis is dynamic and constantly evolving, with new technologies, tools, and techniques emerging regularly. Learning SQL with AI for data analysis provides a solid foundation for continuous learning and growth, enabling individuals to stay updated with the latest trends and advancements in the field.\n\nWhat is SQL, and why is it important for data analysis? SQL (Structured Query Language) is a programming language used for managing and manipulating relational databases. It is essential for querying databases, retrieving data, and performing data manipulation tasks, making it a fundamental skill for data analysis. What is AI, and how is it used in data analysis? AI (Artificial Intelligence) refers to the simulation of human intelligence in machines, enabling them to perform tasks that typically require human intelligence, such as learning, reasoning, and problem-solving. In data analysis, AI techniques such as machine learning are used to analyze data, make predictions, and uncover insights. Do I need prior experience with SQL or AI to take this training? No prior experience with SQL or AI is required. Our training is designed to accommodate learners of all levels, from beginners to advanced users. We cover the fundamentals of SQL and AI for data analysis, making it accessible to participants with varying backgrounds and skill levels. What topics are covered in the training course? The training course covers a range of topics, including SQL fundamentals, advanced SQL techniques, introduction to AI and machine learning, application of AI techniques to data analysis, and practical projects and case studies. What is the duration of the course? The duration and format of the training is 4 months. How will this training benefit my career? Mastery of SQL with AI for data analysis opens up diverse career opportunities in roles such as data analyst, business intelligence analyst, data scientist, machine learning engineer, and more. These roles are in high demand across industries and offer competitive salaries and opportunities for career advancement. Is certification available upon completion of the training? Successful participants will get the AIN GenX and Skill Development Council Karachi certificates. What software/tools do I need for the training? Participants will need access to a computer with internet connectivity to participate in the training. Specific software tools required for the training, such as SQL databases and AI platforms, will be provided or recommended as part of the course materials."
    }
]