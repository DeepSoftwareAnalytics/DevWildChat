[
    {
        "link": "https://stackoverflow.com/questions/49126159/hive-table-is-empty",
        "document": "What is the fastest way to check if a table has any records in Hive?\n\nI so far have come across these approaches:\n• Do a , I find this to be slow.\n• Do a , I find that these give if isn't run on table before. Hence would require to be run before\n• Do a . I find this to be the most efficient way.\n\nAre there better ways to do this? (I just want to check if Hive table has at least one record)"
    },
    {
        "link": "https://stackoverflow.com/questions/41525098/how-to-find-empty-tables-in-hive-database",
        "document": "Hive does not keep track of the number of records present in a table. Only during the query execution, the files belonging to the particular table is read and processed. So there is no other way to know the number of records present in each table without querying each table individually.\n\nAlternatively, You can run a disk usage command on the database directory in HDFS\n\nthe table folders with 0B are obviously empty.\n\nThis is possible because Hive stores the table files in the HDFS given at the time of table creation or at the path mentioned for property in . Default is .\n\nIf the tables are a tables, for the database all the tables' records will be stored under ."
    },
    {
        "link": "https://developer.dataiku.com/latest/concepts-and-examples/sql.html",
        "document": "You can use the Python APIs to execute SQL queries on any SQL connection in DSS (including Hive and Impala).\n\nThere are three capabilities related to performing SQL queries in Dataiku’s Python APIs:\n• None , and in the package. It was initially designed for usage within DSS in recipes and Jupyter notebooks. These are used to perform queries and retrieve results, either as an iterator or as a pandas dataframe\n• None “partial recipes”. It is possible to execute a “partial recipe” from a Python recipe, to execute a Hive, Impala or SQL query. This allows you to use Python to dynamically generate a SQL (resp Hive, Impala) query and have DSS execute it, as if your recipe was a SQL query recipe. This is useful when you need complex business logic to generate the final SQL query and can’t do it with only SQL constructs.\n• None in the package. This function was initially designed for usage outside of DSS and only supports returning results as an iterator. It does not support pandas dataframe We recommend the usage of the variants. For more details on the two packages, please see Concepts and examples\n\nYou can retrieve the results of a SELECT query as a Pandas dataframe. # df is a Pandas dataframe with two columns : \"col1\" and \"count\" Alternatively, you can retrieve the results of a query as an iterator. For databases supporting commit, the transaction in which the queries are executed is rolled back at the end, as is the default in DSS. In order to perform queries with side-effects such as INSERT or UPDATE, you need to add to your call. Depending on your database, DDL queries such as will also need a or not.\n\nIt is possible to execute a “partial recipe” from a Python recipe, to execute a Hive, Impala or SQL query. This allows you to use Python to dynamically generate a SQL (resp Hive, Impala) query and have DSS execute it, as if your recipe was a SQL query recipe. This is useful when you need complex business logic to generate the final SQL query and can’t do it with only SQL constructs. Partial recipes are only possible when you are running a Python recipe. It is not available in the notebooks nor outside of DSS. The partial recipe behaves like the corresponding SQL (resp Hive, Impala) recipe w.r.t. the inputs and outputs. Notably, a Python recipe in which a partial Hive recipe is executed can only have HDFS datasets as inputs and outputs. Likewise, a Impala or SQL partial recipe having only one ouput, the output dataset has to be specified for the partial recipe execution. In the following example, we make a first query in order to dynamically build the larger query that runs as the “main” query of the recipe. # get the needed data to prepare the query # for example, load from another table \"SELECT word FROM word_frequency WHERE frequency > 0.01 AND frequency < 0.99\" # no need to instantiate an executor object, the method is static\n\nWe recommend using rather, especially inside DSS.\n• None run it and fetch the data\n• None verify that the streaming of the results wasn’t interrupted The verification will make DSS release the resources taken for the query’s execution, so the call has to be done once the results have been streamed. An example of a SQL query on a connection configured in DSS is: # raises an exception in case something went wrong Queries against Hive and Impala are also possible. In that case, the type must be set to ‘hive’ or ‘impala’ accordingly, and instead of a connection it is possible to pass a database name: In order to run queries before or after the main query, but still in the same session, for example to set variables in the session, the API provides 2 parameters and which take in arrays of queries:\n\nThis section contains more advanced examples on executing SQL queries. Remap Connections between Design and Automation for SQLExecutor2# When you deploy a project from a Design Node to an Automation Node, you may have to remap the Connection name used as a parameter in SQLExecutor2 to the name of the connection used on the Automation node. # Create a mapping between instance types and corresponding connection names. # Instanciate a SQLExecutor2 object with the appropriate connection"
    },
    {
        "link": "https://tariqul-islam-rony.medium.com/apache-spark-with-hdfs-hive-and-hadoop-code-practice-part-1-7ad9b91b5afa",
        "document": "Apache Spark is data analysis engine, we can use it process the bigdata with it, Spark is 100 times faster than hive, better tool selection for data analysis project building.\n\nIn this article, I discuss about how we can configure spark with hive to run Hive Query using Spark.\n• About configuration of Hadoop, hive and Spark\n• Connecting to PySpark with Yarn as Master to process data\n• Running the different Query to Spark and Spark Functionality\n\nConfiguring System into Debien and Hive and Hadoop\n\nAfter restart, login as hadoop user to run operation\n\nFollow below article for Hive and hadoop Installation\n\nFollow the below article to Installation of spark\n\nPASTE those following environment and path variable into end of the file in\n\nUsing the Yarn as Master for Apache Spark (PySpark) by terminal\n\n: Using yarn as master for SparkSession. Yarn will handle the all the process for spark, and Spark Create the Job in Yarn to handle the session.\n\n: Yarn by default pre configured with scheduled queue named , Yarn will take queue and it’s resource to handle the job in SparkSession.\n\nWhen we create the Spark Session, We can also get the into\n\nWe can also use Python3 with Pyspark Installation to create the Spark Session. But Pyspark and Python 3 Pyspark version need to be same, Otherwise we will get exception during PySpark package usage. I use PySpark 3.2.1, so i have to install same package version by pip or other package manager.\n\nThen we can use to access PySpark\n\nWe have to import from package\n\nwe can Initialize the SparkSession with using different way, those are following below\n\nWe can configure the any properties for Spark By following function for Spark\n\nThere is two way to configure hive into Spark\n\nFirst one, Add the config to builder, need to specify to builder also.\n\nSecond one, Configure hive warehouse dir into file\n\nAdd following configuration at end of the file\n\nSo, we do not need to specify config during initlalizing SparkSession\n\nCreate hql file for table in following file /home/hadoop/spark_example/ddl/employees.hql\n\nRun the following code below to create employee table into Hadoop by Spark with Hive Query\n\nHandling the Partition Table for Hive By Spark\n\nCreate hql file for table into following directory /home/hadoop/spark_example/ddl/sales_info.hql\n\nDefine Column structure and applied into spark sql to save the data info table\n\nUsing Spark to create Hive Table with Partitions and Save Data with Partition\n\nWe need to use function to delete the temporary table in spark.\n\nIncrease the sales price for sales spark dataframe by external user defiend function"
    },
    {
        "link": "https://docs.cloudera.com/runtime/7.3.1/using-hue/topics/hue-view-query-information.html",
        "document": "The Query Info tab provides information such as, the Hive query ID, the user who executed the query, the start time, the end time, the total time taken to execute the query, the tables that were read and written, application ID, Directed Acyclic Graph (DAG) IDs, session ID, LLAP app ID, thread ID, and the queue against which the query was run.\n• Log in to the Hue web interface.\n• Go to the tab and click on the query for which you want to view the query details. tab on the Hue web interface: The following image shows thetab on the Hue web interface:"
    },
    {
        "link": "https://hive.apache.org/docs/latest/languagemanual-cli_27362033",
        "document": "$HIVE_HOME/bin/hive is a shell utility which can be used to run Hive queries in either interactive or batch mode.\n\nHiveServer2 (introduced in Hive 0.11) has its own CLI called Beeline, which is a JDBC client based on SQLLine. Due to new development being focused on HiveServer2, Hive CLI will soon be deprecated in favor of Beeline (HIVE-10511).\n\nSee Replacing the Implementation of Hive CLI Using Beeline and Beeline – New Command Line Shell in the HiveServer2 documentation.\n\nTo get help, run “ ” or “ ”.\n\n Usage (as it is in Hive 0.9.0):\n\nAs of Hive 0.10.0 there is one additional command line option:\n\nNote: The variant “ ” is supported as well as “ ”.\n\nSee Variable Substitution for examples of using the option.\n• Example of running a query from the command line\n• Example of dumping data out from a query into a file using silent mode\n• Example of running a script non-interactively from local disk\n• Example of running a script non-interactively from a Hadoop supported filesystem (starting in Hive 0.14)\n• Example of running an initialization script before entering interactive mode\n\nThe CLI when invoked without the option will attempt to load $HIVE_HOME/bin/.hiverc and $HOME/.hiverc as initialization files.\n\nHive uses log4j for logging. These logs are not emitted to the standard output by default but are instead captured to a log file specified by Hive’s log4j properties file. By default Hive will use in the directory of the Hive installation which writes out logs to and uses the level.\n\nIt is often desirable to emit the logs to the standard output and/or change the logging level for debugging purposes. These can be done from the command line as follows:\n\nspecifies the logging level as well as the log destination. Specifying as the target sends the logs to the standard error (instead of the log file).\n\nSee Hive Logging in Getting Started for more information.\n\nSee Scratch Directory Management in Setting Up HiveServer2 for information about scratch directories and a command-line tool for removing dangling scratch directories that can be used in the Hive CLI as well as HiveServer2.\n\nWhen is run with the or option, it executes SQL commands in batch mode.\n• executes one or more SQL queries from a file.\n\nAs of Hive 0.14, can be from one of the Hadoop supported filesystems (HDFS, S3, etc.) as well.\n\nSee HIVE-7136 for more details.\n\nWhen is run without either the or option, it enters interactive shell mode.\n\nUse “;” (semicolon) to terminate commands. Comments in scripts can be specified using the “–” prefix.\n\nHive can manage the addition of resources to a session where those resources need to be made available at query execution time. The resources can be files, jars, or archives. Any locally accessible file can be added to the session.\n\nOnce a resource is added to a session, Hive queries can refer to it by its name (in map/reduce/transform clauses) and the resource is available locally at execution time on the entire Hadoop cluster. Hive uses Hadoop’s Distributed Cache to distribute the added resources to all the machines in the cluster at query execution time.\n• FILE resources are just added to the distributed cache. Typically, this might be something like a transform script to be executed.\n• JAR resources are also added to the Java classpath. This is required in order to reference objects they contain such as UDFs. See Hive Plugins for more information about custom UDFs.\n• ARCHIVE resources are automatically unarchived as part of distributing them.\n\nAs of Hive 1.2.0, resources can be added and deleted using Ivy URLs of the form ivy://group:module:version?query_string.\n• group – Which module group the module comes from. Translates directly to a Maven groupId or an Ivy Organization.\n• module – The name of the module to load. Translates directly to a Maven artifactId or an Ivy artifact.\n• version – The version of the module to use. Any version or * (for latest) or an Ivy Range can be used.\n\nVarious parameters can be passed in the query_string to configure how and which jars are added to the artifactory. The parameters are in the form of key value pairs separated by ‘&’.\n\nAlso, we can mix and in the same ADD and DELETE commands.\n\nThe different parameters that can be passed are:\n• exclude: Takes a comma separated value of the form org:module.\n• transitive: Takes values true or false. Defaults to true. When transitive = true, all the transitive dependencies are downloaded and added to the classpath.\n• ext: The extension of the file to add. ‘jar’ by default.\n• classifier: The maven classifier to resolve by.\n\nThe DELETE command will delete the resource and all its transitive dependencies unless some dependencies are shared by other resources. If two resources share a set of transitive dependencies and one of the resources is deleted using the DELETE syntax, then all the transitive dependencies will be deleted for the resource except the ones which are shared.\n\nIf A is the set containing the transitive dependencies of pig-0.10.0 and B is the set containing the transitive dependencies of pig-0.11.1.15, then after executing the above commands, A-(A intersection B) will be deleted.\n\nSee HIVE-9664 for more details.\n\nIt is not neccessary to add files to the session if the files used in a transform script are already available on all machines in the Hadoop cluster using the same path name. For example:\n• Here is an executable available on all machines.\n• Here may be accessible via an NFS mount point that’s configured identically on all the cluster nodes.\n\nNote that Hive configuration parameters can also specify jars, files, and archives. See Configuration Variables for more information.\n\nHCatalog is installed with Hive, starting with Hive release 0.11.0.\n\nMany (but not all) commands can be issued as commands, and vice versa. See the HCatalog Command Line Interface document in the HCatalog manual for more information."
    },
    {
        "link": "https://hive.apache.org/docs/latest/gettingstarted_27362090",
        "document": "You can install a stable release of Hive by downloading a tarball, or you can download the source code and build Hive from that.\n• Java 1.7\n\n Note: Hive versions 1.2 onward require Java 1.7 or newer. Hive versions 0.14 to 1.1 work with Java 1.6 as well. Users are strongly advised to start moving to Java 1.8 (see HIVE-8607).\n• Hadoop 2.x (preferred), 1.x (not supported by Hive 2.0.0 onward).\n\n Hive versions up to 0.13 also supported Hadoop 0.20.x, 0.23.x.\n• Hive is commonly used in production Linux and Windows environment. Mac is a commonly used development environment. The instructions in this document are applicable to Linux and Mac. Using it on Windows would require slightly different steps.\n\nStart by downloading the most recent stable release of Hive from one of the Apache download mirrors (see Hive Releases).\n\nNext you need to unpack the tarball. This will result in the creation of a subdirectory named (where is the release number):\n\nSet the environment variable to point to the installation directory:\n\nThe Hive GIT repository for the most recent Hive code is located here: (the master branch).\n\nAll release versions are in branches named “branch-0.#” or “branch-1.#” or the upcoming “branch-2.#”, with the exception of release 0.8.1 which is in “branch-0.8-r2”. Any branches with other names are feature branches for works-in-progress. See Understanding Hive Branches for details.\n\nAs of 0.13, Hive is built using Apache Maven.\n\nTo build the current Hive code from the master branch:\n\nHere, {version} refers to the current Hive version.\n\nIf building Hive source using Maven (mvn), we will refer to the directory “/packaging/target/apache-hive-{version}-SNAPSHOT-bin/apache-hive-{version}-SNAPSHOT-bin” as for the rest of the page.\n\nIn branch-1, Hive supports both Hadoop 1.x and 2.x. You will need to specify which version of Hadoop to build against via a Maven profile. To build against Hadoop 1.x use the profile ; for Hadoop 2.x use . For example to build against Hadoop 1.x, the above mvn command becomes:\n\nPrior to Hive 0.13, Hive was built using Apache Ant. To build an older version of Hive on Hadoop 0.20:\n\nIf using Ant, we will refer to the directory “ ” as .\n\nTo build Hive in Ant against Hadoop 0.23, 2.0.0, or other version, build with the appropriate flag; some examples below:\n• you must have Hadoop in your path OR\n\nIn addition, you must use below HDFS commands to create and (aka ) and set them before you can create a table in Hive.\n\nYou may find it useful, though it’s not necessary, to set :\n\nTo use the Hive command line interface (CLI) from the shell:\n\nStarting from Hive 2.1, we need to run the schematool command below as an initialization step. For example, we can use “derby” as db type.\n\nHiveServer2 (introduced in Hive 0.11) has its own CLI called Beeline. HiveCLI is now deprecated in favor of Beeline, as it lacks the multi-user, security, and other capabilities of HiveServer2. To run HiveServer2 and Beeline from shell:\n\nBeeline is started with the JDBC URL of the HiveServer2, which depends on the address and port where HiveServer2 was started. By default, it will be (localhost:10000), so the address will look like jdbc:hive2://localhost:10000.\n\nOr to start Beeline and HiveServer2 in the same process for testing purpose, for a similar user experience to HiveCLI:\n\nTo run the HCatalog server from the shell in Hive release 0.11.0 and later:\n\nTo use the HCatalog command line interface (CLI) in Hive release 0.11.0 and later:\n\nFor more information, see HCatalog Installation from Tarball and HCatalog CLI in the HCatalog manual.\n\nTo run the WebHCat server from the shell in Hive release 0.11.0 and later:\n\nFor more information, see WebHCat Installation in the WebHCat manual.\n• Hive by default gets its configuration from\n• The location of the Hive configuration directory can be changed by setting the environment variable.\n• Configuration variables can be changed by (re-)defining them in\n• Hive configuration is an overlay on top of Hadoop – it inherits the Hadoop configuration variables by default.\n• Hive configuration can be manipulated by:\n• Editing hive-site.xml and defining any desired variables (including Hadoop variables) in it\n• Using the set command (see next section)\n• Invoking Hive (deprecated), Beeline or HiveServer2 using the syntax:\n• None $ bin/hive --hiveconf x1=y1 --hiveconf x2=y2 //this sets the variables x1 and x2 to y1 and y2 respectively\n• $ bin/hiveserver2 –hiveconf x1=y1 –hiveconf x2=y2 //this sets server-side variables x1 and x2 to y1 and y2 respectively\n• $ bin/beeline –hiveconf x1=y1 –hiveconf x2=y2 //this sets client-side variables x1 and x2 to y1 and y2 respectively.\n• Setting the environment variable to “ ” which does the same as above.\n• Hive queries are executed using map-reduce queries and, therefore, the behavior of such queries can be controlled by the Hadoop configuration variables.\n• The HiveCLI (deprecated) and Beeline command ‘SET’ can be used to set any Hadoop (or Hive) configuration variable. For example:\n\nThe latter shows all the current settings. Without the option only the variables that differ from the base Hadoop configuration are displayed.\n\nHive compiler generates map-reduce jobs for most queries. These jobs are then submitted to the Map-Reduce cluster indicated by the variable:\n\nWhile this usually points to a map-reduce cluster with multiple nodes, Hadoop also offers a nifty option to run map-reduce jobs locally on the user’s workstation. This can be very useful to run queries over small data sets – in such cases local mode execution is usually significantly faster than submitting jobs to a large cluster. Data is accessed transparently from HDFS. Conversely, local mode only runs with one reducer and can be very slow processing larger data sets.\n\nStarting with release 0.7, Hive fully supports local mode execution. To enable this, the user can enable the following option:\n\nIn addition, should point to a path that’s valid on the local machine (for example ). (Otherwise, the user will get an exception allocating local disk space.)\n\nStarting with release 0.7, Hive also supports a mode to run map-reduce jobs in local-mode automatically. The relevant options are , , and :\n\nNote that this feature is disabled by default. If enabled, Hive analyzes the size of each map-reduce job in a query and may run it locally if the following thresholds are satisfied:\n• The total input size of the job is lower than: (128MB by default)\n• The total number of map-tasks is less than: (4 by default)\n• The total number of reduce tasks required is 1 or 0.\n\nSo for queries over small data sets, or for queries with multiple map-reduce jobs where the input to subsequent jobs is substantially smaller (because of reduction/filtering in the prior job), jobs may be run locally.\n\nNote that there may be differences in the runtime environment of Hadoop server nodes and the machine running the Hive client (because of different jvm versions or different software libraries). This can cause unexpected behavior/errors while running in local mode. Also note that local mode execution is done in a separate, child jvm (of the Hive client). If the user so wishes, the maximum amount of memory for this child jvm can be controlled via the option . By default, it’s set to zero, in which case Hive lets Hadoop determine the default memory limits of the child jvm.\n\nHive uses log4j for logging. By default logs are not emitted to the console by the CLI. The default logging level is for Hive releases prior to 0.13.0. Starting with Hive 0.13.0, the default logging level is .\n\nThe logs are stored in the directory :\n• Note: In local mode, prior to Hive 0.13.0 the log file name was “ ” instead of “ ”. This bug was fixed in release 0.13.0 (see HIVE-5528 and HIVE-5676).\n\nTo configure a different log location, set in $HIVE_HOME/conf/hive-log4j.properties. Make sure the directory has the sticky bit set ( ).\n\nIf the user wishes, the logs can be emitted to the console by adding the arguments shown below:\n\nAlternatively, the user can change the logging level only by using:\n\nAnother option for logging is TimeBasedRollingPolicy (applicable for Hive 1.1.0 and above, HIVE-9001) by providing DAILY option as shown below:\n\nNote that setting via the ‘set’ command does not change logging properties since they are determined at initialization time.\n\nHive also stores query logs on a per Hive session basis in , but can be configured in hive-site.xml with the property. Starting with Hive 1.1.0, EXPLAIN EXTENDED output for queries can be logged at the INFO level by setting the property to true.\n\nLogging during Hive execution on a Hadoop cluster is controlled by Hadoop configuration. Usually Hadoop will produce one log file per map and reduce task stored on the cluster machine(s) where the task was executed. The log files can be obtained by clicking through to the Task Details page from the Hadoop JobTracker Web UI.\n\nWhen using local mode (using ), Hadoop/Hive execution logs are produced on the client machine itself. Starting with release 0.6 – Hive uses the (falling back to only if it’s missing) to determine where these logs are delivered by default. The default configuration file produces one log file per query executed in local mode and stores it under . The intent of providing a separate configuration file is to enable administrators to centralize execution log capture if desired (on a NFS file server for example). Execution logs are invaluable for debugging run-time errors.\n\nFor information about WebHCat errors and logging, see Error Codes and Responses and Log Files in the WebHCat manual.\n\nError logs are very useful to debug problems. Please send them with any bugs (of which there are many!) to .\n\nFrom Hive 2.1.0 onwards (with HIVE-13027), Hive uses Log4j2’s asynchronous logger by default. Setting hive.async.log.enabled to false will disable asynchronous logging and fallback to synchronous logging. Asynchronous logging can give significant performance improvement as logging will be handled in a separate thread that uses the LMAX disruptor queue for buffering log messages. Refer to https://logging.apache.org/log4j/2.x/manual/async.html for benefits and drawbacks.\n\nHiveServer2 operation logs are available to clients starting in Hive 0.14. See HiveServer2 Logging for configuration.\n\nAudit logs are logged from the Hive metastore server for every metastore API invocation.\n\nAn audit log has the function and some of the relevant function arguments logged in the metastore log file. It is logged at the INFO level of log4j, so you need to make sure that the logging at the INFO level is enabled (see HIVE-3505). The name of the log entry is “HiveMetaStore.audit”.\n\nAudit logs were added in Hive 0.7 for secure client connections (HIVE-1948) and in Hive 0.10 for non-secure connections (HIVE-3277; also see HIVE-2797).\n\nIn order to obtain the performance metrics via the PerfLogger, you need to set DEBUG level logging for the PerfLogger class (HIVE-12675). This can be achieved by setting the following in the log4j properties file.\n\nIf the logger level has already been set to DEBUG at root via hive.root.logger, the above setting is not required to see the performance logs.\n\nThe Hive DDL operations are documented in Hive Data Definition Language.\n\ncreates a table called pokes with two columns, the first being an integer and the other a string.\n\ncreates a table called invites with two columns and a partition column called ds. The partition column is a virtual column. It is not part of the data itself but is derived from the partition that a particular dataset is loaded into.\n\nBy default, tables are assumed to be of text input format and the delimiters are assumed to be ^A(ctrl-a).\n\nlists all the table that end with ’s'. The pattern matching follows Java regular expressions. Check out this link for documentation http://java.sun.com/javase/6/docs/api/java/util/regex/Pattern.html.\n\nTable names can be changed and columns can be added or replaced:\n\nNote that REPLACE COLUMNS replaces all existing columns and only changes the table’s schema, not the data. The table must use a native SerDe. REPLACE COLUMNS can also be used to drop columns from the table’s schema:\n\nMetadata is in an embedded Derby database whose disk storage location is determined by the Hive configuration variable named . By default this location is (see ).\n\nRight now, in the default configuration, this metadata can only be seen by one user at a time.\n\nMetastore can be stored in any database that is supported by JPOX. The location and the type of the RDBMS can be controlled by the two variables and . Refer to JDO (or JPOX) documentation for more details on supported databases. The database schema is defined in JDO metadata annotations file at .\n\nIn the future, the metastore itself can be a standalone server.\n\nIf you want to run the metastore as a network server so it can be accessed from multiple nodes, see Hive Using Derby in Server Mode.\n\nThe Hive DML operations are documented in Hive Data Manipulation Language.\n\nLoads a file that contains two columns separated by ctrl-a into pokes table. ‘LOCAL’ signifies that the input file is on the local file system. If ‘LOCAL’ is omitted then it looks for the file in HDFS.\n\nThe keyword ‘OVERWRITE’ signifies that existing data in the table is deleted. If the ‘OVERWRITE’ keyword is omitted, data files are appended to existing data sets.\n• NO verification of data against the schema is performed by the load command.\n• If the file is in hdfs, it is moved into the Hive-controlled file system namespace.\n\n The root of the Hive directory is specified by the option in . We advise users to create this directory before trying to create tables via Hive.\n\nThe two LOAD statements above load data into two different partitions of the table invites. Table invites must be created as partitioned by the key ds for this to succeed.\n\nThe above command will load data from an HDFS file/directory to the table.\n\n Note that loading data from HDFS will result in moving the file/directory. As a result, the operation is almost instantaneous.\n\nThe Hive query operations are documented in Select.\n\nSome example queries are shown below. They are available in .\n\n More are available in the Hive sources at .\n\nselects column ‘foo’ from all rows of partition of the table. The results are not stored anywhere, but are displayed on the console.\n\nNote that in all the examples that follow, (into a Hive table, local directory or HDFS directory) is optional.\n\nselects all rows from partition of the table into an HDFS directory. The result data is in files (depending on the number of mappers) in that directory.\n\n NOTE: partition columns if any are selected by the use of *. They can also be specified in the projection clauses.\n\nPartitioned tables must always have a partition selected in the clause of the statement.\n\nselects all rows from pokes table into a local directory.\n\nselects the sum of a column. The avg, min, or max can also be used. Note that for versions of Hive which don’t include HIVE-287, you’ll need to use in place of .\n\nNote that for versions of Hive which don’t include HIVE-287, you’ll need to use in place of .\n\nThis streams the data in the map phase through the script (like Hadoop streaming).\n\n Similarly – streaming can be used on the reduce side (please see the Hive Tutorial for examples).\n\nThen, download the data files from MovieLens 100k on the GroupLens datasets page (which also has a README.txt file and index of unzipped files):\n\nNote: If the link to GroupLens datasets does not work, please report it on HIVE-5341 or send a message to the user@hive.apache.org mailing list.\n\nAnd load into the table that was just created:\n\nCount the number of rows in table u_data:\n\nNote that for older versions of Hive which don’t include HIVE-287, you’ll need to use COUNT(1) in place of COUNT(*).\n\nNow we can do some complex data analysis on the table :\n\nNote that if you’re using Hive 0.5.0 or earlier you will need to use in place of .\n\nThe format of Apache weblog is customizable, while most webmasters use the default.\n\n For default Apache weblog, we can create a table with the following command.\n\nMore about RegexSerDe can be found here in HIVE-662 and HIVE-1719."
    },
    {
        "link": "https://devdoc.net/bigdata/hive-0.12.0/language_manual/cli.html",
        "document": "Usage: hive [-hiveconf x=y]* [<-i filename>]* [<-f filename>|<-e query-string>] [-S] -i <filename> Initialization Sql from file (executed automatically and silently before any other commands) -e 'quoted query string' Sql from command line -f <filename> Sql from file -S Silent mode in interactive shell where only data is emitted -hiveconf x=y Use this to set hive/hadoop configuration variables. -e and -f cannot be specified together. In the absence of these options, interactive shell is started. However, -i can be used with any other options. To see this usage help, run hive -h\n• Example of running a Query from the command line\n• Example of dumping data out from a query into a file using silent mode\n• Example of running an initialization script before entering interactive mode\n\nHive uses log4j for logging. These logs are not emitted to the standard output by default but are instead captured to a log file specified by Hive's log4j properties file. By default Hive will use hive-log4j.default in the conf/ directory of the hive installation which writes out logs to /tmp/$USER/hive.log and uses the WARN level. It is often desirable to emit the logs to the standard output and/or change the logging level for debugging purposes. These can be done from the command line as follows: hive.root.logger specifies the logging level as well as the log destination. Specifying console as the target sends the logs to the standard error (instead of the log file).\n\nHive can manage the addition of resources to a session where those resources need to be made available at query execution time. Any locally accessible file can be added to the session. Once a file is added to a session, hive query can refer to this file by its name (in map/reduce/transform clauses) and this file is available locally at execution time on the entire hadoop cluster. Hive uses Hadoop's Distributed Cache to distribute the added files to all the machines in the cluster at query execution time.\n• FILE resources are just added to the distributed cache. Typically, this might be something like a transform script to be executed.\n• JAR resources are also added to the Java classpath. This is required in order to reference objects they contain such as UDF's.\n• ARCHIVE resources are automatically unarchived as part of distributing them. hive> add FILE /tmp/tt.py; hive> list FILES; /tmp/tt.py hive> from networks a MAP a.networkid USING 'python tt.py' as nn where a.ds = '2009-01-04' limit 10; It is not neccessary to add files to the session if the files used in a transform script are already available on all machines in the hadoop cluster using the same path name. For example:\n• ... MAP a.networkid USING 'wc -l' ...: here wc is an executable available on all machines\n• ... MAP a.networkid USING '/home/nfsserv1/hadoopscripts/tt.py' ...: here tt.py may be accessible via a nfs mount point that's configured identically on all the cluster nodes."
    },
    {
        "link": "https://cwiki.apache.org/confluence/display/hive/design",
        "document": ""
    },
    {
        "link": "http://docs.cloudera.com.s3-website-us-east-1.amazonaws.com/documentation/enterprise/5-16-x/topics/cdh_ig_hive_installation.html",
        "document": "Using Hive data in HBase is a common task. See Importing Data Into HBase.\n\nFor information about Hive on Spark, see Running Apache Hive on Spark in CDH.\n\nApache Hive is a powerful data warehousing application for Hadoop. It enables you to access your data using HiveQL, a language similar to SQL.\n\nInstall Hive on your client machine(s) from which you submit jobs; you do not need to install it on the nodes in your Hadoop cluster. As of CDH 5, Hive supports HCatalog which must be installed separately."
    }
]