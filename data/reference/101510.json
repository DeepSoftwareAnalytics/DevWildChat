[
    {
        "link": "https://huggingface.co/docs/transformers/model_doc/auto",
        "document": "and get access to the augmented documentation experience\n\nIn many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the method. AutoClasses are here to do this job for you so that you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.\n\nInstantiating one of AutoConfig, AutoModel, and AutoTokenizer will directly create a class of the relevant architecture. For instance\n\nwill create a model that is an instance of BertModel.\n\nThere is one class of for each task, and for each backend (PyTorch, TensorFlow, or Flax).\n\nEach of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a custom class of model , make sure you have a then you can add those to the auto classes like this:\n\nYou will then be able to use the auto classes like you would usually do!\n\nThe following auto classes are available for instantiating a base model class without a specific head.\n\nThe following auto classes are available for instantiating a model with a pretraining head.\n\nThe following auto classes are available for the following natural language processing tasks.\n\nThe following auto classes are available for the following computer vision tasks.\n\nThe following auto classes are available for the following audio tasks.\n\nThe following auto classes are available for the following multimodal tasks."
    },
    {
        "link": "https://huggingface.co/docs/transformers.js/en/api/models",
        "document": "and get access to the augmented documentation experience\n\nDefinitions of all models available in Transformers.js.\n\nExample: Load and run an .\n\nWe also provide other s (listed below), which you can use in the same way as the Python library. For example:\n\nExample: Load and run an .\n\nA base class for pre-trained models that provides the model configuration and an ONNX session.\n\nCreates a new instance of the class.\n\nGet the model’s generation config, if it exists.\n\nKind: instance property of \n\n Returns: | - The model’s generation config if it exists, otherwise .\n\nDisposes of all the ONNX sessions that were created during inference.\n\nKind: instance method of \n\n Returns: - An array of promises, one for each ONNX session that is being disposed.\n\n Todo\n\nRuns the model with the provided inputs\n\nForward method for a pretrained model. If not overridden by a subclass, the correct forward method will be chosen based on the model type.\n\nKind: instance method of \n\n Returns: - The output data from the model in the format specified in the ONNX model.\n\n Throws:\n• This method must be implemented in subclasses.\n\nThis function returns a [ ] list object that contains all relevant [ ] instances used for multinomial sampling.\n\nThis function merges multiple generation configs together to form a final generation config to be used by the model for text generation. It first creates an empty object, then it applies the model’s own property to it. Finally, if a object was passed in the arguments, it overwrites the corresponding properties in the final config with those of the passed config object.\n\nKind: instance method of \n\n Returns: - The final generation config object to be used by the model for text generation.\n\nConfirms that the model class is compatible with generation. If not, raises an exception that points to the right class to use.\n\nKind: instance method of \n\n Returns: - The updated model inputs for the next generation iteration.\n\nThis function extracts the model-specific for generation.\n\nKind: instance method of \n\n Returns: - The model-specific inputs for generation.\n\nGenerates sequences of token ids for models with a language modeling head.\n\nKind: instance method of \n\n Returns: - The output of the model, which can contain the generated token ids, attentions, and scores.\n\nReturns an object containing past key values from the given decoder results object.\n\nKind: instance method of \n\n Returns: - An object containing past key values.\n\nReturns an object containing attentions from the given model output object.\n\nKind: instance method of \n\n Returns: - An object containing attentions.\n\nAdds past key values to the decoder feeds object. If pastKeyValues is null, creates new tensors for past key values.\n\nInstantiate one of the model classes of the library from a pretrained model.\n\nThe model class to instantiate is selected based on the property of the config object (either passed as an argument or loaded from if possible)\n\nKind: static method of \n\n Returns: - A new instance of the class.\n\nBase class for model’s outputs, with potential hidden states and attentions.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for masked language modeling.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for token classification.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for question answering.\n\nThe bare RoFormer Model transformer outputting raw hidden-states without any specific head on top.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for masked language modeling.\n\nRoFormer Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nRoFormer Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for token classification.\n\nRoFormer Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of the hidden-states output to compute and ).\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for question answering.\n\nThe bare ConvBERT Model transformer outputting raw hidden-states without any specific head on top.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for masked language modeling.\n\nConvBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nConvBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for token classification.\n\nConvBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of the hidden-states output to compute and )\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for question answering.\n\nThe bare Electra Model transformer outputting raw hidden-states without any specific head on top. Identical to the BERT model except that it uses an additional linear layer between the embedding layer and the encoder if the hidden size and embedding size are different.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for masked language modeling.\n\nELECTRA Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for token classification.\n\nLECTRA Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of the hidden-states output to compute and ).\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for question answering.\n\nThe bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for masked language modeling.\n\nCamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nCamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for token classification.\n\nCamemBERT Model with a span classification head on top for extractive question-answering tasks\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for question answering.\n\nThe bare DeBERTa Model transformer outputting raw hidden-states without any specific head on top.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for masked language modeling.\n\nDeBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nDeBERTa Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for token classification.\n\nDeBERTa Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of the hidden-states output to compute and ).\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for question answering.\n\nThe bare DeBERTa-V2 Model transformer outputting raw hidden-states without any specific head on top.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for masked language modeling.\n\nDeBERTa-V2 Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nDeBERTa-V2 Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for token classification.\n\nDeBERTa-V2 Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of the hidden-states output to compute and ).\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for question answering.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for token classification.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for question answering.\n\nCalls the model on new inputs.\n\nThe bare ESM Model transformer outputting raw hidden-states without any specific head on top.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for masked language modeling.\n\nESM Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nESM Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for token classification.\n\nCalls the model on new inputs.\n\nMobileBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n\nCalls the model on new inputs.\n\nMobileBert Model with a span classification head on top for extractive question-answering tasks\n\nCalls the model on new inputs.\n\nThe bare MPNet Model transformer outputting raw hidden-states without any specific head on top.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for masked language modeling.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for token classification.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for question answering.\n\nAn abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n\nThe bare LONGT5 Model transformer outputting raw hidden-states without any specific head on top.\n\nThe bare BART Model outputting raw hidden-states without any specific head on top.\n\nThe BART Model with a language modeling head. Can be used for summarization.\n\nBart model with a sequence classification/head on top (a linear layer on top of the pooled output)\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nThe bare MBART Model outputting raw hidden-states without any specific head on top.\n\nThe MBART Model with a language modeling head. Can be used for summarization, after fine-tuning the pretrained models.\n\nMBart model with a sequence classification/head on top (a linear layer on top of the pooled output).\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nThe bare Blenderbot Model outputting raw hidden-states without any specific head on top.\n\nThe Blenderbot Model with a language modeling head. Can be used for summarization.\n\nThe bare BlenderbotSmall Model outputting raw hidden-states without any specific head on top.\n\nThe BlenderbotSmall Model with a language modeling head. Can be used for summarization.\n\nCalls the model on new inputs.\n\nCalls the model on new inputs.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for token classification.\n\nCalls the model on new inputs.\n\nAn abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n\nThe bare XLM Model transformer outputting raw hidden-states without any specific head on top.\n\nThe XLM Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n\nCalls the model on new inputs.\n\nXLM Model with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n\nCalls the model on new inputs.\n\nXLM Model with a token classification head on top (a linear layer on top of the hidden-states output)\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for token classification.\n\nXLM Model with a span classification head on top for extractive question-answering tasks\n\nCalls the model on new inputs.\n\nCalls the model on new inputs.\n\nCalls the model on new inputs.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for token classification.\n\nCalls the model on new inputs.\n\nThe bare AST Model transformer outputting raw hidden-states without any specific head on top.\n\nAudio Spectrogram Transformer model with an audio classification head on top (a linear layer on top of the pooled output) e.g. for datasets like AudioSet, Speech Commands v2.\n\nTranscribes or translates log-mel input features to a sequence of auto-regressively generated token ids.\n\nKind: instance method of \n\n Returns: - The output of the model, which can contain the generated token ids, attentions, and scores.\n\nCalculates token-level timestamps using the encoder-decoder cross-attentions and dynamic time-warping (DTW) to map each output token to a position in the input audio. If is specified, the encoder-decoder cross-attentions will be cropped before applying DTW.\n\nKind: instance method of \n\n Returns: - tensor containing the timestamps in seconds for each predicted token\n\nVision Encoder-Decoder model based on OpenAI’s GPT architecture for image captioning and other vision tasks\n\nThe LLAVA model which consists of a vision backbone and a language model.\n\nThe Idefics3 model which consists of a vision backbone and a language model.\n\nThe SmolVLM Model with a language modeling head. It is made up a SigLIP vision encoder, with a language modeling head on top.\n\nCLIP Text and Vision Model with a projection layers on top\n\nThe text model from CLIP without any head or projection on top.\n\nCLIP Text Model with a projection layer on top (a linear layer on top of the pooled output)\n\nThe vision model from CLIP without any head or projection on top.\n\nCLIP Vision Model with a projection layer on top (a linear layer on top of the pooled output)\n\nSigLIP Text and Vision Model with a projection layers on top\n\nThe text model from SigLIP without any head or projection on top.\n\nThe vision model from SigLIP without any head or projection on top.\n\nCLIPSeg model with a Transformer-based decoder on top for zero-shot and one-shot image segmentation.\n\nYou can visualize the predictions as follows:\n\nGPT-2 language model head on top of the GPT-2 base model. This model is suitable for text generation tasks.\n\nThe bare JAIS Model transformer outputting raw hidden-states without any specific head on top.\n\nThe JAIS Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n\nCodeGenForCausalLM is a class that represents a code generation model based on the GPT-2 architecture. It extends the class.\n\nThe bare LLama Model outputting raw hidden-states without any specific head on top.\n\nThe bare LLaMA Model outputting raw hidden-states without any specific head on top.\n\nThe bare Cohere Model outputting raw hidden-states without any specific head on top.\n\nThe bare Gemma Model outputting raw hidden-states without any specific head on top.\n\nThe bare Gemma Model outputting raw hidden-states without any specific head on top.\n\nThe bare Gemma2 Model outputting raw hidden-states without any specific head on top.\n\nThe bare Gemma2 Model outputting raw hidden-states without any specific head on top.\n\nThe bare Qwen2 Model outputting raw hidden-states without any specific head on top.\n\nThe bare Qwen2 Model outputting raw hidden-states without any specific head on top.\n\nThe bare Phi Model outputting raw hidden-states without any specific head on top.\n\nThe bare Phi3 Model outputting raw hidden-states without any specific head on top.\n\nThe Bloom Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n\nThe bare Bloom Model transformer outputting raw hidden-states without any specific head on top.\n\nThe Bloom Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n\nThe bare Mpt Model transformer outputting raw hidden-states without any specific head on top.\n\nThe MPT Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n\nThe bare OPT Model outputting raw hidden-states without any specific head on top.\n\nThe OPT Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n\nThe VitPose model with a pose estimation head on top.\n\nYou can visualize the alpha matte as follows:\n\nThe bare Table Transformer Model (consisting of a backbone and encoder-decoder Transformer) outputting raw hidden-states without any specific head on top.\n\nTable Transformer Model (consisting of a backbone and encoder-decoder Transformer) with object detection heads on top, for tasks such as COCO detection.\n\nAn abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n\nThe bare ResNet model outputting raw features without any specific head on top.\n\nResNet Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n\nThe bare Swin2SR Model transformer outputting raw hidden-states without any specific head on top.\n\nSwin2SR Model transformer with an upsampler head on top for image super resolution and restoration.\n\nThe bare DPT Model transformer outputting raw hidden-states without any specific head on top.\n\nDPT Model with a depth estimation head on top (consisting of 3 convolutional layers) e.g. for KITTI, NYUv2.\n\nDepth Anything Model with a depth estimation head on top (consisting of 3 convolutional layers) e.g. for KITTI, NYUv2.\n\nThe bare GLPN encoder (Mix-Transformer) outputting raw hidden-states without any specific head on top.\n\nThe bare ConvNext model outputting raw features without any specific head on top.\n\nConvNext Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n\nThe bare ConvNextV2 model outputting raw features without any specific head on top.\n\nConvNextV2 Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n\nThe bare DINOv2 Model transformer outputting raw hidden-states without any specific head on top.\n\nDinov2 Model transformer with an image classification head on top (a linear layer on top of the final hidden state of the [CLS] token) e.g. for ImageNet.\n\nThe bare Dinov2WithRegisters Model transformer outputting raw hidden-states without any specific head on top.\n\nDinov2WithRegisters Model transformer with an image classification head on top (a linear layer on top of the final hidden state of the [CLS] token) e.g. for ImageNet.\n\nSegment Anything Model (SAM) for generating segmentation masks, given an input image and optional 2D location and bounding boxes.\n\nCompute image embeddings and positional image embeddings, given the pixel values of an image.\n\nKind: instance method of \n\n Returns: - The image embeddings and positional image embeddings.\n\nKind: instance method of \n\n Returns: - The output of the model.\n\nRuns the model with the provided inputs\n\nThe bare Wav2Vec2 Model transformer outputting raw hidden-states without any specific head on top.\n\nExample: Load and run a for feature extraction.\n\nWav2Vec2 Model with a frame classification head on top for tasks like Speaker Diarization.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nThe bare PyAnnote Model transformer outputting raw hidden-states without any specific head on top.\n\nPyAnnote Model with a frame classification head on top for tasks like Speaker Diarization.\n\nExample: Load and run a for speaker diarization.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nThe bare UniSpeech Model transformer outputting raw hidden-states without any specific head on top.\n\nUniSpeech Model with a head on top for Connectionist Temporal Classification (CTC).\n\nUniSpeech Model with a sequence classification head on top (a linear layer over the pooled output).\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nThe bare UniSpeechSat Model transformer outputting raw hidden-states without any specific head on top.\n\nUniSpeechSat Model with a head on top for Connectionist Temporal Classification (CTC).\n\nUniSpeechSat Model with a sequence classification head on top (a linear layer over the pooled output).\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nUniSpeechSat Model with a frame classification head on top for tasks like Speaker Diarization.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nThe bare Wav2Vec2Bert Model transformer outputting raw hidden-states without any specific head on top.\n\nWav2Vec2Bert Model with a head on top for Connectionist Temporal Classification (CTC).\n\nWav2Vec2Bert Model with a sequence classification head on top (a linear layer over the pooled output).\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nThe bare Hubert Model transformer outputting raw hidden-states without any specific head on top.\n\nExample: Load and run a for feature extraction.\n\nHubert Model with a head on top for Connectionist Temporal Classification (CTC).\n\nHubert Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like SUPERB Keyword Spotting.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nAn abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n\nThe bare WavLM Model transformer outputting raw hidden-states without any specific head on top.\n\nExample: Load and run a for feature extraction.\n\nWavLM Model with a head on top for Connectionist Temporal Classification (CTC).\n\nWavLM Model with a sequence classification head on top (a linear layer over the pooled output).\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nWavLM Model with an XVector feature extraction head on top for tasks like Speaker Verification.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits and speaker embeddings.\n\nWavLM Model with a frame classification head on top for tasks like Speaker Diarization.\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - An object containing the model’s output logits for sequence classification.\n\nAn abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n\nThe bare SpeechT5 Encoder-Decoder Model outputting raw hidden-states without any specific pre- or post-nets.\n\nExample: Generate speech from text with .\n\nConverts a sequence of input tokens into a sequence of mel spectrograms, which are subsequently turned into a speech waveform using a vocoder.\n\nKind: instance method of \n\n Returns: - A promise which resolves to an object containing the spectrogram, waveform, and cross-attention tensors.\n\nSee SpeechT5ForSpeechToText for example usage.\n\nThe bare Mistral Model outputting raw hidden-states without any specific head on top.\n\nThe bare Starcoder2 Model outputting raw hidden-states without any specific head on top.\n\nThe bare Falcon Model outputting raw hidden-states without any specific head on top.\n\nCLAP Text Model with a projection layer on top (a linear layer on top of the pooled output).\n\nCLAP Audio Model with a projection layer on top (a linear layer on top of the pooled output).\n\nExample: Generate speech from text with .\n\nCalls the model on new inputs.\n\nKind: instance method of \n\n Returns: - The outputs for the VITS model.\n\nThe bare SegFormer encoder (Mix-Transformer) outputting raw hidden-states without any specific head on top.\n\nSegFormer Model transformer with an image classification head on top (a linear layer on top of the final hidden states) e.g. for ImageNet.\n\nSegFormer Model transformer with an all-MLP decode head on top e.g. for ADE20k, CityScapes.\n\nThe bare StableLm Model transformer outputting raw hidden-states without any specific head on top.\n\nStableLm Model with a head on top for Causal Language Modeling (with past).\n\nThe bare EfficientNet model outputting raw features without any specific head on top.\n\nEfficientNet Model with an image classification head on top (a linear layer on top of the pooled features).\n\nThe bare Musicgen decoder model outputting raw hidden-states without any specific head on top.\n\nThe MusicGen decoder model with a language modelling head on top.\n\nThe composite MusicGen model with a text encoder, audio encoder and Musicgen decoder, for music generation tasks with one or both of text and audio prompts.\n\nExample: Generate music from text with .\n\nApply the pattern mask to the final ids, then revert the pattern delay mask by filtering the pad token id in a single step.\n\nGenerates sequences of token ids for models with a language modeling head.\n\nKind: instance method of \n\n Returns: - The output of the model, which can contain the generated token ids, attentions, and scores.\n\nThe bare MobileNetV1 model outputting raw hidden-states without any specific head on top.\n\nMobileNetV1 model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n\nThe bare MobileNetV2 model outputting raw hidden-states without any specific head on top.\n\nMobileNetV2 model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n\nThe bare MobileNetV3 model outputting raw hidden-states without any specific head on top.\n\nMobileNetV3 model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n\nThe bare MobileNetV4 model outputting raw hidden-states without any specific head on top.\n\nMobileNetV4 model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n\nThe model builds upon the GPT2 architecture to perform autoregressive prediction of actions in an offline RL setting. Refer to the paper for more details: https://arxiv.org/abs/2106.01345\n\nMGP-STR Model transformer with three classification heads on top (three A^3 modules and three linear layer on top of the transformer encoder output) for scene text recognition (STR).\n\nThe bare PatchTST Model outputting raw hidden-states without any specific head.\n\nThe bare PatchTSMixer Model outputting raw hidden-states without any specific head.\n\nKind: instance method of \n\n Returns: - The output tensor of shape .\n\nDecodes the given frames into an output audio waveform.\n\nKind: instance method of \n\n Returns: - The output tensor of shape .\n\nKind: instance method of \n\n Returns: - The output tensor of shape .\n\nDecodes the given frames into an output audio waveform.\n\nKind: instance method of \n\n Returns: - The output tensor of shape .\n\nBase class of all AutoModels. Contains the function which is used to instantiate pretrained models.\n\nWhether to attempt to instantiate the base class ( ) if the model type is not found in the mapping.\n\nHelper class which is used to instantiate pretrained models with the function. The chosen model class is determined by the type specified in the model config.\n\nHelper class which is used to instantiate pretrained sequence classification models with the function. The chosen model class is determined by the type specified in the model config.\n\nHelper class which is used to instantiate pretrained token classification models with the function. The chosen model class is determined by the type specified in the model config.\n\nHelper class which is used to instantiate pretrained sequence-to-sequence models with the function. The chosen model class is determined by the type specified in the model config.\n\nHelper class which is used to instantiate pretrained sequence-to-sequence speech-to-text models with the function. The chosen model class is determined by the type specified in the model config.\n\nHelper class which is used to instantiate pretrained sequence-to-sequence text-to-spectrogram models with the function. The chosen model class is determined by the type specified in the model config.\n\nHelper class which is used to instantiate pretrained text-to-waveform models with the function. The chosen model class is determined by the type specified in the model config.\n\nHelper class which is used to instantiate pretrained causal language models with the function. The chosen model class is determined by the type specified in the model config.\n\nHelper class which is used to instantiate pretrained masked language models with the function. The chosen model class is determined by the type specified in the model config.\n\nHelper class which is used to instantiate pretrained question answering models with the function. The chosen model class is determined by the type specified in the model config.\n\nHelper class which is used to instantiate pretrained vision-to-sequence models with the function. The chosen model class is determined by the type specified in the model config.\n\nHelper class which is used to instantiate pretrained image classification models with the function. The chosen model class is determined by the type specified in the model config.\n\nHelper class which is used to instantiate pretrained image segmentation models with the function. The chosen model class is determined by the type specified in the model config.\n\nHelper class which is used to instantiate pretrained image segmentation models with the function. The chosen model class is determined by the type specified in the model config.\n\nHelper class which is used to instantiate pretrained universal image segmentation models with the function. The chosen model class is determined by the type specified in the model config.\n\nHelper class which is used to instantiate pretrained object detection models with the function. The chosen model class is determined by the type specified in the model config.\n\nHelper class which is used to instantiate pretrained mask generation models with the function. The chosen model class is determined by the type specified in the model config.\n\nDescribes the outputs for the VITS model.\n\nHelper function to perform the following:\n\nIf the model supports providing position_ids, we create position_ids on the fly for batch generation, by computing the cumulative sum of the attention mask along the sequence length dimension."
    },
    {
        "link": "https://huggingface.co/docs/transformers/en/installation",
        "document": "and get access to the augmented documentation experience\n\nInstall 🤗 Transformers for whichever deep learning library you’re working with, setup your cache, and optionally configure 🤗 Transformers to run offline.\n\n🤗 Transformers is tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:\n\nYou should install 🤗 Transformers in a virtual environment. If you’re unfamiliar with Python virtual environments, take a look at this guide. A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies.\n\nCreate a virtual environment with uv (refer to Installation for installation instructions), a fast Rust-based Python package and project manager.\n\nNow you’re ready to install 🤗 Transformers with pip or uv.\n\nFor GPU acceleration, install the appropriate CUDA drivers for PyTorch and TensorFlow(https://www.tensorflow.org/install/pip).\n\nRun the command below to check if your system detects an NVIDIA GPU.\n\nFor CPU-support only, you can conveniently install 🤗 Transformers and a deep learning library in one line. For example, install 🤗 Transformers and PyTorch with:\n\nFinally, check if 🤗 Transformers has been properly installed by running the following command. It will download a pretrained model:\n\nThen print out the label and score:\n\nInstall 🤗 Transformers from source with the following command:\n\nThis command installs the bleeding edge version rather than the latest version. The version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn’t been rolled out yet. However, this means the version may not always be stable. We strive to keep the version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an Issue so we can fix it even sooner!\n\nCheck if 🤗 Transformers has been properly installed by running the following command:\n\nYou will need an editable install if you’d like to:\n• Use the version of the source code.\n• Contribute to 🤗 Transformers and need to test changes in the code.\n\nClone the repository and install 🤗 Transformers with the following commands:\n\nThese commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in , Python will also search the folder you cloned to: .\n\nNow you can easily update your clone to the latest version of 🤗 Transformers with the following command:\n\nYour Python environment will find the version of 🤗 Transformers on the next run.\n\nPretrained models are downloaded and locally cached at: . This is the default directory given by the shell environment variable . On Windows, the default directory is given by . You can change the shell environment variables shown below - in order of priority - to specify a different cache directory:\n\nRun 🤗 Transformers in a firewalled or offline environment with locally cached files by setting the environment variable .\n\nThis script should run without hanging or waiting to timeout because it won’t attempt to download the model from the Hub.\n\nYou can also bypass loading a model from the Hub from each from_pretrained() call with the parameter. When set to , only local files are loaded:\n\nFetch models and tokenizers to use offline\n\nAnother option for using 🤗 Transformers offline is to download the files ahead of time, and then point to their local path when you need to use them offline. There are three ways to do this:\n• None Download a file through the user interface on the Model Hub by clicking on the ↓ icon.\n• None Use the PreTrainedModel.from_pretrained() and PreTrainedModel.save_pretrained() workflow:\n• None Download your files ahead of time with PreTrainedModel.from_pretrained():\n• None Save your files to a specified directory with PreTrainedModel.save_pretrained():\n• None Now when you’re offline, reload your files with PreTrainedModel.from_pretrained() from the specified directory:\n• \n• None Install the library in your virtual environment:\n• None Use the function to download a file to a specific path. For example, the following command downloads the file from the T0 model to your desired path:\n\nOnce your file is downloaded and locally cached, specify it’s local path to load and use it:\n\nSee below for some of the more common installation issues and how to resolve them.\n\nEnsure you are using Python 3.9 or later. Run the command below to check your Python version.\n\nInstall all required dependencies by running the following command. Ensure you’re in the project directory before executing the command.\n\nIf you encounter issues on Windows, you may need to activate Developer Mode. Navigate to Windows Settings > For Developers > Developer Mode.\n\nAlternatively, create and activate a virtual environment as shown below."
    },
    {
        "link": "https://huggingface.co/transformers/v3.0.2/model_doc/auto.html",
        "document": "In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the method.\n\nAutoClasses are here to do this job for you so that you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary:\n\nInstantiating one of , and will directly create a class of the relevant architecture (ex: will create a instance of ).\n\nis a generic model class that will be instantiated as one of the question answering model classes of the library when created with the class method. The method takes care of returning the correct model class instance based on the property of the config object, or when it’s missing, falling back to using pattern matching on the string: This class cannot be instantiated using (throws an error). Instantiates one of the base model classes of the library from a configuration. Loading a model from its configuration file does not load the model weights. It only affects the model’s configuration. Use to load the model weights ( ) instance of a class derived from : The model class to instantiate is selected based on the configuration class: Instantiates one of the question answering model classes of the library from a pre-trained model configuration. The method takes care of returning the correct model class instance based on the property of the config object, or when it’s missing, falling back to using pattern matching on the string: The model is set in evaluation mode by default using (Dropout modules are deactivated) To train the model, you should first set it back in training mode with\n• None a string with the of a pre-trained model to load from cache or download, e.g.: .\n• None a string with the of a pre-trained model that was user-uploaded to our S3, e.g.: .\n• None a path to a containing model weights saved using , e.g.: .\n• None a path or url to a (e.g. ). In the case of a PyTorch checkpoint, should be set to True and a configuration object should be provided as argument. Set to True if the Checkpoint is a PyTorch checkpoint. All remaning positional arguments will be passed to the underlying model’s method Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n• None the model is a model provided by the library (loaded with the string of a pretrained model), or\n• None the model was saved using and is reloaded by suppling the save directory.\n• None the model is loaded by suppling a local directory as and a configuration JSON file named is found in the directory. an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file. This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using and is not a simpler option. Path to a directory in which a downloaded pre-trained model configuration should be cached if the standard cache should not be used. Force to (re-)download the model weights and configuration files and override the cached versions if they exists. Do not delete incompletely recieved file. Attempt to resume the download if such a file exists. A dictionary of proxy servers to use by protocol or endpoint, e.g.: {‘http’: ‘foo.bar:3128’, ‘http://hostname’: ‘foo.bar:4012’}. The proxies are used on each request. Set to to also return a dictionnary containing missing keys, unexpected keys and error messages. Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ). Behave differently depending on whether a is provided or automatically loaded:\n• None If a configuration is provided with , will be directly passed to the underlying model’s method (we assume all relevant updates to the configuration have already been done)\n• None If a configuration is not provided, will be first passed to the configuration class initialization function ( ). Each key of that corresponds to a configuration attribute will be used to override said attribute with the supplied value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model’s function. # Download model and configuration from S3 and cache. # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n\nInstantiates one of the base model classes of the library from a configuration. Loading a model from its configuration file does not load the model weights. It only affects the model’s configuration. Use to load the model weights ( ) instance of a class derived from : The model class to instantiate is selected based on the configuration class: Instantiates one of the question answering model classes of the library from a pre-trained model configuration. The method takes care of returning the correct model class instance based on the property of the config object, or when it’s missing, falling back to using pattern matching on the string: The model is set in evaluation mode by default using (Dropout modules are deactivated) To train the model, you should first set it back in training mode with\n• None a string with the of a pre-trained model to load from cache or download, e.g.: .\n• None a path to a containing model weights saved using , e.g.: .\n• None a path or url to a (e.g. ). In this case, should be set to True and a configuration object should be provided as argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards. All remaning positional arguments will be passed to the underlying model’s method Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n• None the model is a model provided by the library (loaded with the string of a pretrained model), or\n• None the model was saved using and is reloaded by suppling the save directory.\n• None the model is loaded by suppling a local directory as and a configuration JSON file named is found in the directory. an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file. This option can be used if you want to create a model from a pretrained configuration but load your own weights. In this case though, you should check if using and is not a simpler option. Path to a directory in which a downloaded pre-trained model configuration should be cached if the standard cache should not be used. Force to (re-)download the model weights and configuration files and override the cached versions if they exists. A dictionary of proxy servers to use by protocol or endpoint, e.g.: {‘http’: ‘foo.bar:3128’, ‘http://hostname’: ‘foo.bar:4012’}. The proxies are used on each request. Set to to also return a dictionnary containing missing keys, unexpected keys and error messages. Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ). Behave differently depending on whether a is provided or automatically loaded:\n• None If a configuration is provided with , will be directly passed to the underlying model’s method (we assume all relevant updates to the configuration have already been done)\n• None If a configuration is not provided, will be first passed to the configuration class initialization function ( ). Each key of that corresponds to a configuration attribute will be used to override said attribute with the supplied value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model’s function. # Download model and configuration from S3 and cache. # Loading from a TF checkpoint file instead of a PyTorch model (slower)"
    },
    {
        "link": "https://stackoverflow.com/questions/73357305/how-to-use-huggingface-transformers-with-primeqa-model",
        "document": "Here is the model https://huggingface.co/PrimeQA/t5-base-table-question-generator\n\nHugging face says that I should use the following code to use the model in transformers:\n\nIt also provides a link to this documentation in the model https://github.com/primeqa/primeqa/blob/main/notebooks/qg/tableqg_inference.ipynb\n\nIt has the following code:\n\nHow do I combine these two snippets? I did the following but I get errors:\n\nIt gives me the following error:"
    },
    {
        "link": "https://huggingface.co/learn/nlp-course/en/chapter1/7",
        "document": "Encoder-decoder models (also called sequence-to-sequence models) use both parts of the Transformer architecture. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input.\n\nThe pretraining of these models can be done using the objectives of encoder or decoder models, but usually involves something a bit more complex. For instance, T5 is pretrained by replacing random spans of text (that can contain several words) with a single mask special word, and the objective is then to predict the text that this mask word replaces.\n\nSequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering.\n\nRepresentatives of this family of models include:"
    },
    {
        "link": "https://huggingface.co/docs/transformers/en/tasks/token_classification",
        "document": "and get access to the augmented documentation experience\n\nToken classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization.\n\nThis guide will show you how to:\n• Finetune DistilBERT on the WNUT 17 dataset to detect new entities.\n• Use your finetuned model for inference.\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:\n\nStart by loading the WNUT 17 dataset from the 🤗 Datasets library:\n\nThen take a look at an example:\n\nEach number in represents an entity. Convert the numbers to their label names to find out what the entities are:\n\nThe letter that prefixes each indicates the token position of the entity:\n• indicates the beginning of an entity.\n• indicates a token is contained inside the same entity (for example, the token is a part of an entity like ).\n• indicates the token doesn’t correspond to any entity.\n\nThe next step is to load a DistilBERT tokenizer to preprocess the field:\n\nAs you saw in the example field above, it looks like the input has already been tokenized. But the input actually hasn’t been tokenized yet and you’ll need to set to tokenize the words into subwords. For example:\n\nHowever, this adds some special tokens and and the subword tokenization creates a mismatch between the input and labels. A single word corresponding to a single label may now be split into two subwords. You’ll need to realign the tokens and labels by:\n• Mapping all tokens to their corresponding word with the method.\n• Assigning the label to the special tokens and so they’re ignored by the PyTorch loss function (see CrossEntropyLoss).\n• Only labeling the first token of a given word. Assign to other subtokens from the same word.\n\nHere is how you can create a function to realign the tokens and labels, and truncate sequences to be no longer than DistilBERT’s maximum input length:\n\nTo apply the preprocessing function over the entire dataset, use 🤗 Datasets map function. You can speed up the function by setting to process multiple elements of the dataset at once:\n\nNow create a batch of examples using DataCollatorWithPadding. It’s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n\nIncluding a metric during training is often helpful for evaluating your model’s performance. You can quickly load a evaluation method with the 🤗 Evaluate library. For this task, load the seqeval framework (see the 🤗 Evaluate quick tour to learn more about how to load and compute a metric). Seqeval actually produces several scores: precision, recall, F1, and accuracy.\n\nGet the NER labels first, and then create a function that passes your true predictions and true labels to to calculate the scores:\n\nYour function is ready to go now, and you’ll return to it when you setup your training.\n\nBefore you start training your model, create a map of the expected ids to their labels with and :\n\nIf you aren’t familiar with finetuning a model with the Trainer, take a look at the basic tutorial here! You’re ready to start training your model now! Load DistilBERT with AutoModelForTokenClassification along with the number of expected labels, and the label mappings: At this point, only three steps remain:\n• Define your training hyperparameters in TrainingArguments. The only required parameter is which specifies where to save your model. You’ll push this model to the Hub by setting (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the Trainer will evaluate the seqeval scores and save the training checkpoint.\n• Pass the training arguments to Trainer along with the model, dataset, tokenizer, data collator, and function. Once training is completed, share your model to the Hub with the push_to_hub() method so everyone can use your model: If you aren’t familiar with finetuning a model with Keras, take a look at the basic tutorial here! To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters: To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters: Then you can load DistilBERT with TFAutoModelForTokenClassification along with the number of expected labels, and the label mappings: Convert your datasets to the format with prepare_tf_dataset(): Configure the model for training with . Note that Transformers models all have a default task-relevant loss function, so you don’t need to specify one unless you want to: The last two things to setup before you start training is to compute the seqeval scores from the predictions, and provide a way to push your model to the Hub. Both are done by using Keras callbacks. Specify where to push your model and tokenizer in the PushToHubCallback: Then bundle your callbacks together: Finally, you’re ready to start training your model! Call with your training and validation datasets, the number of epochs, and your callbacks to finetune the model: Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n\nGreat, now that you’ve finetuned a model, you can use it for inference!\n\nGrab some text you’d like to run inference on:\n\nThe simplest way to try out your finetuned model for inference is to use it in a pipeline(). Instantiate a for NER with your model, and pass your text to it:\n\nYou can also manually replicate the results of the if you’d like:"
    },
    {
        "link": "https://medium.com/@awaldeep/hugging-face-understanding-tokenizers-1b7e4afdb154",
        "document": "Before you can use your data in a model, the data needs to be processed into an acceptable format for the model. A model does not understand raw text, images or audio. These inputs need to be converted into numbers and assembled into tensors.\n\nThe main tool for processing textual data is a tokenizer. A tokenizer starts by splitting text into tokens according to a set of rules. The tokens are converted into numbers, which are used to build tensors as input to a model. Any additional inputs required by a model are also added by the tokenizer.\n\nIn this blog post, we will try to understand the HuggingFace tokenizers in depth and will go through all the parameters and also the outputs returned by a tokenizer. We’ll dive into the AutoTokenizer class and see how to use a pre-trained tokenizer for our data.\n\nThe definition of tokenization, as given by Stanford NLP group is:\n\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data.\n\nThe goal is to find the most meaningful representation — that is, the one that makes the most sense to the model — and, if possible, the smallest representation.\n\nThere are different solutions available: word-based, character-based but the one used by the state-of-the-art transformer models are sub-word tokenizers: Byte-level BPE(GPT-2), WordPiece(BERT) etc.\n\nHugging Face is a New York based company that has swiftly developed language processing expertise. The company’s aim is to advance NLP and democratize it for use by practitioners and researchers around the world.\n\nIn an effort to offer access to fast, state-of-the-art, and easy-to-use tokenization that plays well with modern NLP pipelines, Hugging Face contributors have developed and open-sourced Tokenizers. Tokenizers is, as the name implies, an implementation of today’s most widely used tokenizers with emphasis on performance and versatility.\n\nAn implementation of a tokenizer consists of the following pipeline of processes, each applying different transformations to the textual information:\n\nLet’s go through these steps:\n\nThe normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. If you’re familiar with Unicode normalization (such as NFC or NFKC), this is also something the tokenizer may apply.\n\nGiven the input above, the normalization step would transform it into:\n\nA tokenizer cannot be trained on raw text alone. Instead, we first need to split the texts into small entities, like words. That’s where the pre-tokenization step comes in. A word-based tokenizer can simply split a raw text into words on whitespace and punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training.\n\nGiven this string, the pre-tokenizer’s output will be something like:\n\nAs we can see, the tokenizer also keeps track of the offsets. Also, the rules for pre-tokenization can vary with the tokenizer being used. For instance, BERT will have different set of rules for this step than GPT-2.\n\nAfter normalization and pre-processing steps, we apply a training algorithm to the text data. This output of this step is dependent on the type of training strategy we are going to use. The state-of-the-art models use subword tokenization algorithms, for example BERT uses WordPiece tokenization, GPT, GPT-2 use BPE, AIBERT uses unigram etc.\n\nUsing a BERT tokenizer, will tokenize the sentence like this:\n\nSimilar to the modeling part, a number of post-processors are available depending on the training strategy used. They’re responsible for adding the special tokens to the input sequence as needed by the model.\n\nUsing a BERT post-processor to our sequence will result in:\n\nHere, [CLS] denotes the classification token, which tells the model that this is a classification task and [SEP] denotes the end of sentence and is also used between two sentences.\n\nFor a detailed and mathematical explanation of numerous training algorithms, you can check the official HuggingFace documentation."
    },
    {
        "link": "https://chrisyandata.medium.com/handling-long-input-prompts-with-hugging-face-models-9cb631e10597",
        "document": "Hugging Face models, particularly those based on the Transformer architecture, have revolutionized natural language processing (NLP). However, a common challenge when working with these models is managing input prompts that exceed the maximum token length, leading to errors such as token indices sequence length is longer than the specified maximum sequence length for this model (733>512). Running this sequence through the model will result in indexing errors . This article will explore strategies to handle such scenarios, with a deep dive into the last two advanced methods: using a sliding window approach and leveraging long-context models.\n\nEvery Hugging Face model has a maximum token limit, which varies depending on the model’s architecture. For instance, BERT and GPT-2 typically have a maximum token length of 512, while some versions of GPT-3 can handle up to 2048 tokens. This limit is crucial for ensuring efficient processing and preventing memory overflow.\n• Tokenization and Token Count: Begin by tokenizing the input text to understand its token length."
    },
    {
        "link": "https://stackoverflow.com/questions/76422222/how-to-do-tokenizer-batch-processing-huggingface",
        "document": "How to tokenize a list of sentences?\n\nIf it's just tokenizing a list of sentences, do this:\n\nIt does the batching automatically:\n\nHow to use it with the ?\n\nAnd to use it with , it's this:\n\nHow to use the model for sentiment classification?\n\nWhat happens when I've OOM issues with GPU?\n\nIf it's the , you should just use the CPU. For that you won't face much OOM issues.\n\nIf you need to use a GPU, consider using the inference and it comes with the batch_size option, e.g.\n\nWhen you face OOM issues, it is usually not the tokenizer creating the problem unless you loaded the full large dataset into the device.\n\nIf it is just the model not being able to predict when you feed in the large dataset, consider using instead of using the\n\nTake a look at https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching\n\nIf the question is regarding the arguments, then from the doc\n\nAnd from the code\n\nAnd if we try that to see if your inputs :\n\nBut when you wrap the tokens around a list,\n\nTherefore, the usage of the tokenizer and to get the batch processing working properly would look something like this:\n\nNote: The use of the argument is not to process batches of sentence but it's used to specify when your input to the tokenizers are already pre-tokenized."
    }
]