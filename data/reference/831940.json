[
    {
        "link": "https://pytorch.org/docs/stable/tensors.html",
        "document": ""
    },
    {
        "link": "https://pytorch.org/cppdocs/notes/tensor_indexing.html",
        "document": "Indexing a tensor in the PyTorch C++ API works very similar to the Python API. All index types such as / / integer / boolean / slice / tensor are available in the C++ API, making translation from Python indexing code to C++ very simple. The main difference is that, instead of using the -operator similar to the Python API syntax, in the C++ API the indexing methods are:\n\nIt’s also important to note that index types such as / / live in the namespace, and it’s recommended to put before any indexing code for convenient use of those index types.\n\nHere are some examples of translating Python indexing code to C++:"
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html",
        "document": "Follow along with the video below or on youtube.\n\nTensors are the central data abstraction in PyTorch. This interactive notebook provides an in-depth introduction to the class.\n\nFirst things first, let’s import the PyTorch module. We’ll also add Python’s math module to facilitate some of the examples.\n\nAs with any object in Python, assigning a tensor to a variable makes the variable a label of the tensor, and does not copy it. For example: # ...and b is also altered But what if you want a separate copy of the data to work on? The method is there for you: # ...but still with the same contents! # ...but b is still all ones There is an important thing to be aware of when using ``clone()``. If your source tensor has autograd, enabled then so will the clone. This will be covered more deeply in the video on autograd, but if you want the light version of the details, continue on. In many cases, this will be what you want. For example, if your model has multiple computation paths in its method, and both the original tensor and its clone contribute to the model’s output, then to enable model learning you want autograd turned on for both tensors. If your source tensor has autograd enabled (which it generally will if it’s a set of learning weights or derived from a computation involving the weights), then you’ll get the result you want. On the other hand, if you’re doing a computation where neither the original tensor nor its clone need to track gradients, then as long as the source tensor has autograd turned off, you’re good to go. There is a third case, though: Imagine you’re performing a computation in your model’s function, where gradients are turned on for everything by default, but you want to pull out some values mid-stream to generate some metrics. In this case, you don’t want the cloned copy of your source tensor to track gradients - performance is improved with autograd’s history tracking turned off. For this, you can use the method on the source tensor:\n• None We create with turned on. We haven’t covered this optional argument yet, but will during the unit on autograd.\n• None When we print , it informs us that the property - this means that autograd and computation history tracking are turned on.\n• None We clone and label it . When we print , we can see that it’s tracking its computation history - it has inherited ’s autograd settings, and added to the computation history.\n• None We clone into , but we call first.\n• None Printing , we see no computation history, and no . The method detaches the tensor from its computation history. It says, “do whatever comes next as if autograd was off.” It does this without changing - you can see that when we print again at the end, it retains its property.\n\nIn the section above on broadcasting, it was mentioned that PyTorch’s broadcast semantics are compatible with NumPy’s - but the kinship between PyTorch and NumPy goes even deeper than that. If you have existing ML or scientific code with data stored in NumPy ndarrays, you may wish to express that same data as PyTorch tensors, whether to take advantage of PyTorch’s GPU acceleration, or its efficient abstractions for building ML models. It’s easy to switch between ndarrays and PyTorch tensors: PyTorch creates a tensor of the same shape and containing the same data as the NumPy array, going so far as to keep NumPy’s default 64-bit float data type. The conversion can just as easily go the other way: It is important to know that these converted objects are using the same underlying memory as their source objects, meaning that changes to one are reflected in the other:"
    },
    {
        "link": "https://dataquest.io/blog/pytorch-for-deep-learning",
        "document": "Getting Started with PyTorch for Deep Learning\n\nDeep learning is transforming many aspects of technology, from image recognition breakthroughs to conversational AI systems. For years, TensorFlow was widely regarded as the dominant deep learning framework, praised for its robust ecosystem and community support. However, a growing number of developers and researchers are turning to PyTorch, citing its intuitive Pythonic interface and flexible execution model. Whether you’re transitioning from TensorFlow or just breaking into deep learning, PyTorch offers a streamlined environment that makes it more approachable than ever to build and train powerful neural networks.\n\nIn this post, we’ll walk through what deep learning is, why PyTorch has become a favorite among AI developers, and how to use PyTorch to build a simple model that predicts salaries based on just age and years of experience. By the end, you’ll understand the essential building blocks of deep learning and have enough knowledge to start experimenting on your own.\n\nA Quick Look at the Landscape\n\nPyTorch is an open-source deep learning framework developed by Meta (formerly Facebook). While TensorFlow was once the dominant name in this space, according to the O’Reilly Technology Trends for 2025, “Usage of TensorFlow content declined 28%; its continued decline indicates that PyTorch has won the hearts and minds of AI developers.” But why does PyTorch stand out?\n• Pythonic and Easy to Learn: If you’ve used libraries like NumPy or pandas, PyTorch will feel very intuitive—it’s essentially Python code with powerful added features for deep learning.\n• Immediate Execution (Eager Mode): PyTorch lets you run operations as you write them. Think of it like writing normal Python: no separate compilation step or “static graph” you have to define ahead of time. This makes debugging your models much more straightforward.\n• Strong Ecosystem and Community: From official tutorials to third-party add-ons, there’s a wealth of resources that help you learn and troubleshoot.\n• Widespread Real-World Adoption: While self-driving cars and robotics are the flashy side of deep learning, PyTorch also powers recommendation engines (think Netflix or YouTube), voice assistants, medical image analysis, and countless other daily-life applications.\n\nFor newcomers, this combination of familiarity and community support is a huge plus. You can stick to your Python comfort zone while exploring a field that might otherwise feel unapproachable.\n\nDeep learning typically centers on using artificial neural networks to learn patterns from data, which is stored in tensors—multi-dimensional arrays that PyTorch can process on both CPU and GPU. Here’s a quick overview of how these networks work:\n• Layers: A network is composed of layers of interconnected neurons. Each neuron is like a small mathematical function that takes some input (in the form of a tensor), multiplies it by a weight, adds a bias, and outputs a value.\n• Forward Pass: Your tensor data (e.g., numerical features) flows forward from the input layer, through any hidden layers, to the final output layer, producing a prediction.\n\nBelow is a simplified diagram that illustrates a feed-forward neural network architecture:\n\nThink of the forward pass like making an educated guess. For example, if we’re trying to predict a person’s salary, the forward pass might take their age and years of experience and spit out a salary estimate. Of course, that guess could be too high or too low—which is where the network learns to adjust itself.\n\nHow to Use PyTorch to Predict a Person's Salary\n\nTo make this concrete, we’ll walk through an example of how to predict a person’s salary given their age and years of experience. It’s a simplified scenario, but it neatly demonstrates the basics of neural networks—forward pass, loss calculation, and backpropagation.\n\nHow the Model Works (Step by Step)\n\nLet’s say we have a single hidden layer with two neurons ($n_1$ and $n_2$) as shown in the image below.\n\nFor each neuron, the calculation goes like this:\n\nThen these neuron outputs get passed forward to the final output layer to produce a single salary estimate ($y_{est}$):\n\nAt this point, the model updates its weights by a process known as backpropagation, which we’ll go over in a moment. Notice how the new weights affect the predicted salary—and how the error (difference from the actual salary) has gone down.\n\nEvery time the network makes a prediction, we use a loss function to measure how far off we are. For regression tasks (like predicting a continuous salary), a commonly used loss is Mean Squared Error (MSE):\n\nHere, $N$ is the total number of data points (or people) in the dataset, $i$ indexes each individual from 1 to $N$, $y_{\\text{est}_i}$ is the predicted salary for the $i$-th person, and $y_{\\text{act}_i}$ is their actual salary.\n\nBy squaring the differences between the estimate and the actual value, large errors get penalized more heavily, encouraging the model to aim for more accurate predictions.\n\nAfter computing the loss, the network adjusts weights and biases through backpropagation—a process that calculates how much each parameter contributed to the error and determines how to adjust it to reduce the loss.\n\nAn important factor in this adjustment is the learning rate, a hyperparameter that controls the size of the steps taken during each update. Its value typically ranges from 0.0001 to 0.1. A learning rate that’s too high may cause the model to overshoot optimal values, while one that’s too low can make learning very slow. It’s common to start with a learning rate of 0.001 and tweak as necessary.\n\nIn PyTorch, once you define your model and loss function, backpropagation (triggered by calling ) automatically computes the gradients—numerical values that indicate how much each parameter should change to reduce the error—and the optimizer uses these gradients—scaled by the learning rate—to update the model’s parameters.\n\nBefore we dive into coding our salary predictor, let’s make sure you have PyTorch properly installed.\n\nIf you use conda or Python’s built-in venv, you can create a clean environment to avoid dependency conflicts with other projects. For example:\n\nThe Official PyTorch “Get Started” Page provides specific commands based on your system (Windows, Mac, Linux) and whether you have a GPU. A simple CPU-only install might look like:\n\nOnce installed, open a Python shell (or Jupyter Notebook) and try:\n\nYou should see a version number (e.g., ) and a / indicating whether PyTorch detects a GPU.\n\nIt’s often helpful to run a small tensor operation to confirm PyTorch is working as expected:\n\nIf it prints , PyTorch is working fine.\n\nNow let’s construct a bare-bones neural network that models the “age + experience → salary” idea. We’ll keep it as simple as possible. All code below is runnable if you’re following along in a Python file or notebook.\n\nHere, we have nine data points. Obviously, this is a tiny dataset—just for demonstration purposes. Typically, you’ll have thousands of data points to help the model learn a pattern.\n\nWe’ll use a single Linear layer (no hidden layer) to keep things simple:\n\nThis layer automatically creates two weights (for and ) and a bias term.\n• MSELoss: Perfect for regression, penalizing the square of differences.\n• SGD (Stochastic Gradient Descent): Optimizer that adjusts weights based on the gradients. A moderate learning rate ( ) ensures we don’t “overshoot” model updates while also speeding up the training process.\n• Epoch: One complete pass through the entire training dataset. During each epoch, every data point is processed by the model—going through the forward pass, loss calculation, gradient reset, backward pass, and parameter update. Training over multiple epochs allows the model to gradually refine its predictions.\n• Forward Pass: The model multiplies your inputs by its weights, adds the bias, and outputs salary predictions.\n• Loss Calculation: Compares those predictions to the actual salaries.\n• Gradient Reset: We zero out gradients so they don’t accumulate from previous steps.\n• Backward Pass: PyTorch computes how to adjust each weight/bias by analyzing how they contributed to the loss.\n• Step: The optimizer moves the parameters in the direction that (hopefully) reduces the loss next time.\n\nAfter running this code, you should see the loss decrease, indicating the model is learning to approximate the relationship between age, experience, and salary.\n\nYou might see something like:\n\nEven if they’re not exact, the predictions should be much closer than the model’s initial guesses—showing that PyTorch has successfully learned from this tiny dataset.\n• Check Your Dimensions: PyTorch expects inputs (X) and targets (y) to have compatible shapes. A common mistake is forgetting to add that extra dimension to target values (i.e., and not ).\n• Start with a Simple Model: Don’t jump into multi-layer architectures right away. A single can teach you the fundamentals of forward, backward, and training loops without making things more complex than they need to be.\n• Go Easy on the Learning Rate: If your loss is bouncing around wildly, you might need to lower the learning rate. If training is crawling, a higher rate could help—but tweak carefully.\n• Scale Your Inputs and Targets: Normalizing your inputs and targets (e.g., scaling numerical features to a similar range) can greatly improve training stability. Unscaled features might lead to excessively large gradients and/or unstable learning. Techniques like standard normalization or min-max scaling are recommended.\n• GPU or CPU: If you’re working with large datasets or deeper networks, a GPU can drastically reduce training time. See PyTorch’s GPU documentation for how to move your model/data to CUDA.\n• Monitor and Debug: Print the loss periodically to see if it’s trending down. If it’s not, double-check your data shapes, your learning rate, and whether you’re zeroing out gradients after each iteration through the training data.\n\nNow that you’ve built a simple model and watched PyTorch do its magic, you might be wondering where to go next. Here are a few suggestions:\n• PyTorch 60-Minute Blitz: A fast-paced introduction that walks you through everything from creating tensors to training a neural network on a classic dataset (like MNIST digits).\n• Learn the Basics: If you prefer a more step-by-step approach, this series explores data loading, building models, and more advanced concepts at a methodical pace.\n• Explore Real Datasets: Try applying your new skills to a bigger dataset. For instance, check out a dataset on house prices (think of a house, , , etc.). Even this small Kaggle dataset can teach you a ton about real-world data cleaning, overfitting, and hyperparameter tuning.\n• Dive into CNNs or NLP: Convolutional Neural Networks (CNNs) excel at image-related tasks, while Natural Language Processing (NLP) or text-based applications often rely on Recurrent Neural Networks (RNNs) or Transformers. Both can be done in PyTorch with relevant tutorials in the official docs.\n\nHere’s a quick summary of the key concepts we defined in the article:\n• Tensor: A multi-dimensional array (similar to a NumPy array) that PyTorch uses to store and process data on both CPUs and GPUs.\n• Neural Network: A model made up of layers of neurons, each multiplying inputs by weights and adding biases.\n• Forward Pass: The process of sending inputs through the network to get a prediction.\n• Loss (or Error): A measurement of how far off predictions are from actual values; lower is better.\n• Mean Squared Error (MSE): Squares the difference between predicted and actual values, penalizing large errors more strongly.\n• Backpropagation: An algorithm that calculates how to adjust weights and biases based on the loss.\n• Gradients: Numerical values computed during backpropagation that indicate how much each parameter (weight or bias) contributes to the error. These values guide how the model should adjust its parameters to minimize the loss.\n• Optimizer: An algorithm that uses gradients to update the model's parameters. It determines the step size for each update (often influenced by the learning rate) to help the model converge toward a solution that minimizes the loss.\n• Learning Rate: Controls how big a step you take when updating weights. Too large can cause instability; too small can slow training.\n• Epoch: One complete pass over your training data—often repeated many times.\n\nYou’ve just walked through the basics of deep learning with PyTorch, focusing on a concrete example: predicting salaries from just age and experience. We covered:\n• Why PyTorch is a top choice for modern AI\n• Building a minimal linear model with actual code you can run\n• Practical tips to smooth out your learning journey\n\nRemember, every expert started at square one. PyTorch’s strength lies in letting you experiment iteratively—try new architectures, tweak parameters, and see immediate results. As you get comfortable, you’ll find countless avenues to explore: from image recognition to speech analysis, recommendation systems, and beyond.\n\nHappy coding, and welcome to the exciting world of PyTorch for deep learning!"
    },
    {
        "link": "https://shawnzhong.github.io/PyTorchDoc",
        "document": "PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\n\nThe torch package contains data structures for multi-dimensional tensors and mathematical operations over these are defined. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0. Returns True if the data type of is a floating point data type i.e., one of , and . Sets the default floating point dtype to . This type will be used as default floating point type for type inference in . d ( ) – the floating point dtype to make the default # default is now changed to torch.float64 # changed to torch.float32, the dtype for torch.FloatTensor Sets the default type to floating point tensor type . This type will also be used as default floating point type for type inference in . t (type or string) – the floating point tensor type or its name Returns the total number of elements in the tensor. Set options for printing. Items shamelessly taken from NumPy\n• None precision – Number of digits of precision for floating point output (default = 4).\n• None threshold – Total number of array elements which trigger summarization rather than full (default = 1000).\n• None edgeitems – Number of array items in summary at beginning and end of each dimension (default = 3).\n• None linewidth – The number of characters per line for the purpose of inserting line breaks (default = 80). Thresholded matrices will ignore this parameter.\n• None profile – Sane defaults for pretty printing. Can override with any of the above options. (any one of , , )\n• None sci_mode – Enable (True) or disable (False) scientific notation. If None (default) is specified, the value is defined by Returns if your system supports flushing denormal numbers and it successfully configures flush denormal mode. is only supported on x86 architectures supporting SSE3. mode (bool) – Controls whether to enable flush denormal mode or not Random sampling creation ops are listed under Random sampling and include: You may also use with the In-place random sampling methods to create s with values sampled from a broader range of distributions. always copies . If you have a Tensor and want to avoid a copy, use or . If you have a NumPy and want to avoid a copy, use . When data is a tensor , reads out ‘the data’ from whatever it is passed, and constructs a leaf variable. Therefore is equivalent to and is equivalent to . The equivalents using and are recommended.\n• None data (array_like) – Initial data for the tensor. Can be a list, tuple, NumPy , scalar, and other types.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , infers data type from .\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given with the given . A sparse tensor can be , in that case, there are duplicate coordinates in the indices, and the value at that index is the sum of all duplicate value entries: torch.sparse.\n• None indices (array_like) – Initial data for the tensor. Can be a list, tuple, NumPy , scalar, and other types. Will be cast to a internally. The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.\n• None values (array_like) – Initial values for the tensor. Can be a list, tuple, NumPy , scalar, and other types.\n• None size (list, tuple, or , optional) – Size of the sparse tensor. If not provided the size will be inferred as the minimum size big enough to hold all non-zero elements.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if None, infers data type from .\n• None device ( , optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . # Create an empty sparse tensor with the following invariants: # For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and # sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0)) # and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and Convert the data into a . If the data is already a with the same and , no copy will be performed, otherwise a new will be returned with computational graph retained if data has . Similarly, if the data is an of the corresponding and the is the cpu, no copy will be performed.\n• None data (array_like) – Initial data for the tensor. Can be a list, tuple, NumPy , scalar, and other types.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , infers data type from .\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. The returned tensor and share the same memory. Modifications to the tensor will be reflected in the and vice versa. The returned tensor is not resizable. Returns a tensor filled with the scalar value , with the shape defined by the variable argument .\n• None sizes (int...) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ).\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Returns a tensor filled with the scalar value , with the same size as . is equivalent to . As of 0.4, this function does not support an keyword. As an alternative, the old is equivalent to .\n• None input (Tensor) – the size of will determine size of the output tensor\n• None dtype ( , optional) – the desired data type of returned Tensor. Default: if , defaults to the dtype of .\n• None layout ( , optional) – the desired layout of returned tensor. Default: if , defaults to the layout of .\n• None device ( , optional) – the desired device of returned tensor. Default: if , defaults to the device of .\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Returns a tensor filled with the scalar value , with the shape defined by the variable argument .\n• None sizes (int...) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ).\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Returns a tensor filled with the scalar value , with the same size as . is equivalent to . As of 0.4, this function does not support an keyword. As an alternative, the old is equivalent to .\n• None input (Tensor) – the size of will determine size of the output tensor\n• None dtype ( , optional) – the desired data type of returned Tensor. Default: if , defaults to the dtype of .\n• None layout ( , optional) – the desired layout of returned tensor. Default: if , defaults to the layout of .\n• None device ( , optional) – the desired device of returned tensor. Default: if , defaults to the device of .\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Returns a 1-D tensor of size \\(\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor\\) with values from the interval taken with common difference beginning from . Note that non-integer is subject to floating point rounding errors when comparing against ; to avoid inconsistency, we advise adding a small epsilon to in such cases.\n• None start (Number) – the starting value for the set of points. Default: .\n• None end (Number) – the ending value for the set of points\n• None step (Number) – the gap between each pair of adjacent points. Default: .\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ). If is not given, infer the data type from the other input arguments. If any of , , or are floating-point, the is inferred to be the default dtype, see . Otherwise, the is inferred to be .\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Returns a 1-D tensor of size \\(\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\\) with values from to with step . Step is the gap between two values in the tensor. This function is deprecated in favor of .\n• None start (float) – the starting value for the set of points. Default: .\n• None end (float) – the ending value for the set of points\n• None step (float) – the gap between each pair of adjacent points. Default: .\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ).\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Returns a one-dimensional tensor of equally spaced points between and . The output tensor is 1-D of size .\n• None start (float) – the starting value for the set of points\n• None end (float) – the ending value for the set of points\n• None steps (int) – number of points to sample between and . Default: .\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ).\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Returns a one-dimensional tensor of points logarithmically spaced between \\(10^{\\text{start}}\\) and \\(10^{\\text{end}}\\). The output tensor is 1-D of size .\n• None start (float) – the starting value for the set of points\n• None end (float) – the ending value for the set of points\n• None steps (int) – number of points to sample between and . Default: .\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ).\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n• None m (int, optional) – the number of columns with default being\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ).\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . A 2-D tensor with ones on the diagonal and zeros elsewhere Returns a tensor filled with uninitialized data. The shape of the tensor is defined by the variable argument .\n• None sizes (int...) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ).\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Returns an uninitialized tensor with the same size as . is equivalent to .\n• None input (Tensor) – the size of will determine size of the output tensor\n• None dtype ( , optional) – the desired data type of returned Tensor. Default: if , defaults to the dtype of .\n• None layout ( , optional) – the desired layout of returned tensor. Default: if , defaults to the layout of .\n• None device ( , optional) – the desired device of returned tensor. Default: if , defaults to the device of .\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: .\n• None size (int...) – a list, tuple, or of integers defining the shape of the output tensor.\n• None fill_value – the number to fill the output tensor with.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ).\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Returns a tensor with the same size as filled with . is equivalent to .\n• None input (Tensor) – the size of will determine size of the output tensor\n• None fill_value – the number to fill the output tensor with.\n• None dtype ( , optional) – the desired data type of returned Tensor. Default: if , defaults to the dtype of .\n• None layout ( , optional) – the desired layout of returned tensor. Default: if , defaults to the layout of .\n• None device ( , optional) – the desired device of returned tensor. Default: if , defaults to the device of .\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Concatenates the given sequence of tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty. can be seen as an inverse operation for and . can be best understood via examples.\n• None tensors (sequence of Tensors) – any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension.\n• None dim (int, optional) – the dimension over which the tensors are concatenated Last chunk will be smaller if the tensor size along the given dimension is not divisible by .\n• None dim (int) – dimension along which to split the tensor Gathers values along an axis specified by . For a 3-D tensor the output is specified by: If is an n-dimensional tensor with size \\((x_0, x_1..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})\\) and , then must be an \\(n\\)-dimensional tensor with size \\((x_0, x_1, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})\\) where \\(y \\geq 1\\) and will have the same size as .\n• None dim (int) – the axis along which to index\n• None index (LongTensor) – the indices of elements to gather\n• None sparse_grad (bool,optional) – If , gradient w.r.t. will be a sparse tensor. Returns a new tensor which indexes the tensor along dimension using the entries in which is a . The returned tensor has the same number of dimensions as the original tensor ( ). The th dimension has the same size as the length of ; other dimensions have the same size as in the original tensor. The returned tensor does not use the same storage as the original tensor. If has a different shape than expected, we silently change it to the correct shape, reallocating the underlying storage if necessary.\n• None dim (int) – the dimension in which we index\n• None index (LongTensor) – the 1-D tensor containing the indices to index Returns a new 1-D tensor which indexes the tensor according to the binary mask which is a . The shapes of the tensor and the tensor don’t need to match, but they must be broadcastable. The returned tensor does not use the same storage as the original tensor\n• None mask (ByteTensor) – the tensor containing the binary mask to index with Returns a new tensor that is a narrowed version of tensor. The dimension is input from to . The returned tensor and tensor share the same underlying storage.\n• None dimension (int) – the dimension along which to narrow\n• None length (int) – the distance to the ending dimension Returns a tensor containing the indices of all non-zero elements of . Each row in the result contains the indices of a non-zero element in . If has dimensions, then the resulting indices tensor is of size \\((z \\times n)\\), where \\(z\\) is the total number of non-zero elements in the tensor.\n• None out (LongTensor, optional) – the output tensor containing indices Returns a tensor with the same data and number of elements as , but with the specified shape. When possible, the returned tensor will be a view of . Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior. See on when it is possible to return a view. A single dimension may be -1, in which case it’s inferred from the remaining dimensions and the number of elements in .\n• None input (Tensor) – the tensor to be reshaped\n• None shape (tuple of python:ints) – the new shape If is an integer type, then will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension is not divisible by . If is a list, then will be split into chunks with sizes in according to .\n• None split_size_or_sections (int) or (list(int)) – size of a single chunk or list of sizes for each chunk\n• None dim (int) – dimension along which to split the tensor. Returns a tensor with all the dimensions of of size removed. For example, if is of shape: \\((A \\times 1 \\times B \\times C \\times 1 \\times D)\\) then the tensor will be of shape: \\((A \\times B \\times C \\times D)\\). When is given, a squeeze operation is done only in the given dimension. If is of shape: \\((A \\times 1 \\times B)\\), leaves the tensor unchanged, but will squeeze the tensor to the shape \\((A \\times B)\\). The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.\n• None dim (int, optional) – if given, the input will be squeezed only in this dimension Concatenates sequence of tensors along a new dimension. All tensors need to be of the same size.\n• None seq (sequence of Tensors) – sequence of tensors to concatenate\n• None dim (int) – dimension to insert. Has to be between 0 and the number of dimensions of concatenated tensors (inclusive) Expects to be <= 2-D tensor and transposes dimensions 0 and 1. 0-D and 1-D tensors are returned as it is and 2-D tensor can be seen as a short-hand function for . Returns a new tensor with the elements of at the given indices. The input tensor is treated as if it were viewed as a 1-D tensor. The result takes the same shape as the indices. Returns a tensor that is a transposed version of . The given dimensions and are swapped. The resulting tensor shares it’s underlying storage with the tensor, so changing the content of one would change the content of the other.\n• None dim0 (int) – the first dimension to be transposed\n• None dim1 (int) – the second dimension to be transposed Returns a tuple of all slices along a given dimension, already without it. Returns a new tensor with a dimension of size one inserted at the specified position. The returned tensor shares the same underlying data with this tensor. A value within the range can be used. Negative will correspond to applied at = .\n• None dim (int) – the index at which to insert the singleton dimension Return a tensor of elements selected from either or , depending on . The operation is defined as: The tensors , , must be broadcastable.\n• None x (Tensor) – values selected at indices where is\n• None y (Tensor) – values selected at indices where is A tensor of shape equal to the broadcasted shape of , , Returns the initial seed for generating random numbers as a Python . The tensor should be a tensor containing probabilities to be used for drawing the binary random number. Hence, all values in have to be in the range: \\(0 \\leq \\text{input}_i \\leq 1\\). The \\(\\text{i}^{th}\\) element of the output tensor will draw a value \\(1\\) according to the \\(\\text{i}^{th}\\) probability value given in . The returned tensor only has values 0 or 1 and is of the same shape as . can have integral , but must have floating point .\n• None input (Tensor) – the input tensor of probability values for the Bernoulli distribution Returns a tensor where each row contains indices sampled from the multinomial probability distribution located in the corresponding row of tensor . The rows of do not need to sum to one (in which case we use the values as weights), but must be non-negative, finite and have a non-zero sum. Indices are ordered from left to right according to when each was sampled (first samples are placed in first column). If is a vector, is a vector of size . If is a matrix with rows, is an matrix of shape \\((m \\times \\text{num\\_samples})\\). If replacement is , samples are drawn with replacement. If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row. When drawn without replacement, must be lower than number of non-zero elements in (or the min number of non-zero elements in each row of if it is a matrix).\n• None replacement (bool, optional) – whether to draw with replacement or not not enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320 Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given. The is a tensor with the mean of each output element’s normal distribution The is a tensor with the standard deviation of each output element’s normal distribution The shapes of and don’t need to match, but the total number of elements in each tensor need to be the same. When the shapes do not match, the shape of is used as the shape for the returned output tensor\n• None mean (Tensor) – the tensor of per-element means Similar to the function above, but the means are shared among all drawn elements.\n• None mean (float, optional) – the mean for all distributions Similar to the function above, but the standard-deviations are shared among all drawn elements.\n• None mean (Tensor) – the tensor of per-element means\n• None std (float, optional) – the standard deviation for all distributions Returns a tensor filled with random numbers from a uniform distribution on the interval \\([0, 1)\\) The shape of the tensor is defined by the variable argument .\n• None sizes (int...) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ).\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Returns a tensor with the same size as that is filled with random numbers from a uniform distribution on the interval \\([0, 1)\\). is equivalent to .\n• None input (Tensor) – the size of will determine size of the output tensor\n• None dtype ( , optional) – the desired data type of returned Tensor. Default: if , defaults to the dtype of .\n• None layout ( , optional) – the desired layout of returned tensor. Default: if , defaults to the layout of .\n• None device ( , optional) – the desired device of returned tensor. Default: if , defaults to the device of .\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Returns a tensor filled with random integers generated uniformly between (inclusive) and (exclusive). The shape of the tensor is defined by the variable argument .\n• None low (int, optional) – Lowest integer to be drawn from the distribution. Default: 0.\n• None high (int) – One above the highest integer to be drawn from the distribution.\n• None size (tuple) – a tuple defining the shape of the output tensor.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ).\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Returns a tensor with the same shape as Tensor filled with random integers generated uniformly between (inclusive) and (exclusive).\n• None input (Tensor) – the size of will determine size of the output tensor\n• None low (int, optional) – Lowest integer to be drawn from the distribution. Default: 0.\n• None high (int) – One above the highest integer to be drawn from the distribution.\n• None dtype ( , optional) – the desired data type of returned Tensor. Default: if , defaults to the dtype of .\n• None layout ( , optional) – the desired layout of returned tensor. Default: if , defaults to the layout of .\n• None device ( , optional) – the desired device of returned tensor. Default: if , defaults to the device of .\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Returns a tensor filled with random numbers from a normal distribution with mean and variance (also called the standard normal distribution). The shape of the tensor is defined by the variable argument .\n• None sizes (int...) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ).\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Returns a tensor with the same size as that is filled with random numbers from a normal distribution with mean 0 and variance 1. is equivalent to .\n• None input (Tensor) – the size of will determine size of the output tensor\n• None dtype ( , optional) – the desired data type of returned Tensor. Default: if , defaults to the dtype of .\n• None layout ( , optional) – the desired layout of returned tensor. Default: if , defaults to the layout of .\n• None device ( , optional) – the desired device of returned tensor. Default: if , defaults to the device of .\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . Returns a random permutation of integers from to .\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:\n• None f – a file-like object (has to implement write and flush) or a string containing a file name\n• None pickle_module – module used for pickling metadata and objects\n• None pickle_protocol – can be specified to override the default protocol If you are using Python 2, torch.save does NOT support StringIO.StringIO as a valid file-like object. This is because the write method should return the number of bytes written; StringIO.write() does not do this. Please use something like io.BytesIO instead. Loads an object saved with from a file. uses Python’s unpickling facilities but treats storages, which underlie tensors, specially. They are first deserialized on the CPU and are then moved to the device they were saved from. If this fails (e.g. because the run time system doesn’t have certain devices), an exception is raised. However, storages can be dynamically remapped to an alternative set of devices using the argument. If is a callable, it will be called once for each serialized storage with two arguments: storage and location. The storage argument will be the initial deserialization of the storage, residing on the CPU. Each serialized storage has a location tag associated with it which identifies the device it was saved from, and this tag is the second argument passed to map_location. The builtin location tags are for CPU tensors and (e.g. ) for CUDA tensors. should return either None or a storage. If returns a storage, it will be used as the final deserialized object, already moved to the right device. Otherwise, \\(torch.load\\) will fall back to the default behavior, as if wasn’t specified. If is a string, it should be a device tag, where all tensors should be loaded. Otherwise, if is a dict, it will be used to remap location tags appearing in the file (keys), to ones that specify where to put the storages (values). User extensions can register their own location tags and tagging and deserialization methods using .\n• None f – a file-like object (has to implement read, readline, tell, and seek), or a string containing a file name\n• None map_location – a function, torch.device, string or a dict specifying how to remap storage locations\n• None pickle_module – module used for unpickling metadata and objects (has to match the pickle_module used to serialize file)\n• None pickle_load_args – optional keyword arguments passed over to and , e.g., . When you call on a file which contains GPU tensors, those tensors will be loaded to GPU by default. You can call and then to avoid GPU RAM surge when loading a model checkpoint. In Python 3, when loading files saved by Python 2, you may encounter . This is caused by the difference of handling in byte strings in Python2 and Python 3. You may use extra keyword argument to specify how these objects should be loaded, e.g., decodes them to strings using encoding, and keeps them as byte arrays which can be decoded later with . # Load all tensors onto the CPU # Load all tensors onto the CPU, using a function Gets the number of OpenMP threads used for parallelizing CPU operations Sets the number of OpenMP threads used for parallelizing CPU operations The context managers , , and are helpful for locally disabling and enabling gradient computation. See Locally disabling gradient computation for more details on their usage. # this can also be used as a function Computes the element-wise absolute value of the given tensor. Returns a new tensor with the arccosine of the elements of . Adds the scalar to each element of the input and returns a new resulting tensor. If is of type FloatTensor or DoubleTensor, must be a real number, otherwise it should be an integer.\n• None value (Number) – the number to be added to each element of Each element of the tensor is multiplied by the scalar and added to each element of the tensor . The resulting tensor is returned. The shapes of and must be broadcastable. If is of type FloatTensor or DoubleTensor, must be a real number, otherwise it should be an integer.\n• None value (Number) – the scalar multiplier for\n• None other (Tensor) – the second input tensor Performs the element-wise division of by , multiply the result by the scalar and add it to . The shapes of , , and must be broadcastable. For inputs of type or , must be a real number, otherwise an integer.\n• None tensor (Tensor) – the tensor to be added Performs the element-wise multiplication of by , multiply the result by the scalar and add it to . The shapes of , , and must be broadcastable. For inputs of type or , must be a real number, otherwise an integer.\n• None tensor (Tensor) – the tensor to be added\n• None tensor1 (Tensor) – the tensor to be multiplied\n• None tensor2 (Tensor) – the tensor to be multiplied Returns a new tensor with the arcsine of the elements of . Returns a new tensor with the arctangent of the elements of . Returns a new tensor with the arctangent of the elements of and . The shapes of and must be broadcastable. Returns a new tensor with the ceil of the elements of , the smallest integer greater than or equal to each element. Clamp all elements in into the range , and return a resulting tensor: If is of type or , args and must be real numbers, otherwise they should be integers.\n• None min (Number) – lower-bound of the range to be clamped to\n• None max (Number) – upper-bound of the range to be clamped to Clamps all elements in to be larger or equal . If is of type or , should be a real number, otherwise it should be an integer.\n• None value (Number) – minimal value of each element in the output Clamps all elements in to be smaller or equal . If is of type or , should be a real number, otherwise it should be an integer.\n• None value (Number) – maximal value of each element in the output Returns a new tensor with the cosine of the elements of . Returns a new tensor with the hyperbolic cosine of the elements of . Divides each element of the input with the scalar and returns a new resulting tensor. If is of type or , should be a real number, otherwise it should be an integer\n• None value (Number) – the number to be divided to each element of Each element of the tensor is divided by each element of the tensor . The resulting tensor is returned. The shapes of and must be broadcastable. Computes the logarithmic derivative of the gamma function on . input (Tensor) – the tensor to compute the digamma function on Computes the error function of each element. The error function is defined as follows: Computes the complementary error function of each element of . The complementary error function is defined as follows: Computes the inverse error function of each element of . The inverse error function is defined in the range \\((-1, 1)\\) as: Returns a new tensor with the exponential of the elements of the input tensor . Returns a new tensor with the exponential of the elements minus 1 of . Returns a new tensor with the floor of the elements of , the largest integer less than or equal to each element. The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the dividend . When is a tensor, the shapes of and must be broadcastable.\n• None divisor (Tensor or float) – the divisor, which may be either a number or a tensor of the same shape as the dividend Computes the fractional portion of each element in . Does a linear interpolation of two tensors and based on a scalar or tensor and returns the resulting tensor. The shapes of and must be broadcastable. If is a tensor, then the shapes of , must be broadcastable.\n• None start (Tensor) – the tensor with the starting points\n• None end (Tensor) – the tensor with the ending points\n• None weight (float or tensor) – the weight for the interpolation formula Returns a new tensor with the natural logarithm of the elements of . Returns a new tensor with the logarithm to the base 10 of the elements of . Returns a new tensor with the natural logarithm of (1 + ). This function is more accurate than for small values of Returns a new tensor with the logarithm to the base 2 of the elements of . Multiplies each element of the input with the scalar and returns a new resulting tensor. If is of type or , should be a real number, otherwise it should be an integer\n• None value (Number) – the number to be multiplied to each element of Each element of the tensor is multiplied by the corresponding element of the Tensor . The resulting tensor is returned. The shapes of and must be broadcastable.\n• None other (Tensor) – the second multiplicand tensor Computes the multivariate log-gamma function ([reference]) with dimension \\(p\\) element-wise, given by where \\(C = \\log(\\pi) \\times \\frac{p (p - 1)}{4}\\) and \\(\\Gamma(\\cdot)\\) is the Gamma function. If any of the elements are less than or equal to \\(\\frac{p - 1}{2}\\), then an error is thrown.\n• None input (Tensor) – the tensor to compute the multivariate log-gamma function Returns a new tensor with the negative of the elements of . Takes the power of each element in with and returns a tensor with the result. can be either a single number or a with the same number of elements as . When is a scalar value, the operation applied is: When is a tensor, the operation applied is: When is a tensor, the shapes of and must be broadcastable.\n• None exponent (float or tensor) – the exponent value is a scalar value, and is a tensor. The returned tensor is of the same shape as\n• None base (float) – the scalar base value for the power operation Returns a new tensor with the reciprocal of the elements of The divisor and dividend may contain both for integer and floating point numbers. The remainder has the same sign as the divisor. When is a tensor, the shapes of and must be broadcastable.\n• None divisor (Tensor or float) – the divisor that may be either a number or a Tensor of the same shape as the dividend , which computes the element-wise remainder of division equivalently to the C library function . Returns a new tensor with each of the elements of rounded to the closest integer. Returns a new tensor with the reciprocal of the square-root of each of the elements of . Returns a new tensor with the sigmoid of the elements of . Returns a new tensor with the sign of the elements of . Returns a new tensor with the sine of the elements of . Returns a new tensor with the hyperbolic sine of the elements of . Returns a new tensor with the square-root of the elements of . Returns a new tensor with the tangent of the elements of . Returns a new tensor with the hyperbolic tangent of the elements of . Returns a new tensor with the truncated integer values of the elements of . Returns the indices of the maximum values of a tensor across a dimension. This is the second value returned by . See its documentation for the exact semantics of this method.\n• None dim (int) – the dimension to reduce. If , the argmax of the flattened input is returned.\n• None keepdim (bool) – whether the output tensors have retained or not. Ignored if . Returns the indices of the minimum values of a tensor across a dimension. This is the second value returned by . See its documentation for the exact semantics of this method.\n• None dim (int) – the dimension to reduce. If , the argmin of the flattened input is returned.\n• None keepdim (bool) – whether the output tensors have retained or not. Ignored if . Returns the cumulative product of elements of in the dimension . For example, if is a vector of size N, the result will also be a vector of size N, with elements.\n• None dim (int) – the dimension to do the operation over\n• None dtype ( , optional) – the desired data type of returned tensor. If specified, the input tensor is casted to before the operation is performed. This is useful for preventing data type overflows. Default: None. Returns the cumulative sum of elements of in the dimension . For example, if is a vector of size N, the result will also be a vector of size N, with elements.\n• None dim (int) – the dimension to do the operation over\n• None dtype ( , optional) – the desired data type of returned tensor. If specified, the input tensor is casted to before the operation is performed. This is useful for preventing data type overflows. Default: None. The shapes of and must be broadcastable.\n• None p (float, optional) – the norm to be computed Returns the log of summed exponentials of each row of the tensor in the given dimension . The computation is numerically stabilized. For summation index \\(j\\) given by and other indices \\(i\\), the result is If is , the output tensor is of the same size as except in the dimension(s) where it is of size 1. Otherwise, is squeezed (see ), resulting in the output tensor having 1 (or ) fewer dimension(s).\n• None dim (int or tuple of python:ints) – the dimension or dimensions to reduce\n• None keepdim (bool) – whether the output tensor has retained or not Returns the mean value of all elements in the tensor. Returns the mean value of each row of the tensor in the given dimension . If is a list of dimensions, reduce over all of them. If is , the output tensor is of the same size as except in the dimension(s) where it is of size 1. Otherwise, is squeezed (see ), resulting in the output tensor having 1 (or ) fewer dimension(s).\n• None dim (int or tuple of python:ints) – the dimension or dimensions to reduce\n• None keepdim (bool) – whether the output tensor has retained or not Returns the median value of all elements in the tensor. Returns a namedtuple where is the median value of each row of the tensor in the given dimension . And is the index location of each median value found. By default, is the last dimension of the tensor. If is , the output tensors are of the same size as except in the dimension where they are of size 1. Otherwise, is squeezed (see ), resulting in the outputs tensor having 1 fewer dimension than .\n• None keepdim (bool) – whether the output tensors have retained or not Returns a namedtuple where is the mode value of each row of the tensor in the given dimension , i.e. a value which appears most often in that row, and is the index location of each mode value found. By default, is the last dimension of the tensor. If is , the output tensors are of the same size as except in the dimension where they are of size 1. Otherwise, is squeezed (see ), resulting in the output tensors having 1 fewer dimension than . This function is not defined for yet.\n• None keepdim (bool) – whether the output tensors have retained or not Returns the matrix norm or vector norm of a given tensor.\n• None the order of norm. Default: The following norms can be calculated: as vec norm when dim is None\n• None dim (int, 2-tuple of python:ints, 2-list of python:ints, optional) – If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated. If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension. If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.\n• None keepdim (bool, optional) – whether the output tensors have retained or not. Ignored if = and = . Default:\n• None out (Tensor, optional) – the output tensor. Ignored if = and = .\n• None dtype ( , optional) – the desired data type of returned tensor. If specified, the input tensor is casted to :attr:’dtype’ while performing the operation. Default: None. Returns the product of all elements in the tensor.\n• None dtype ( , optional) – the desired data type of returned tensor. If specified, the input tensor is casted to before the operation is performed. This is useful for preventing data type overflows. Default: None. Returns the product of each row of the tensor in the given dimension . If is , the output tensor is of the same size as except in the dimension where it is of size 1. Otherwise, is squeezed (see ), resulting in the output tensor having 1 fewer dimension than .\n• None keepdim (bool) – whether the output tensor has retained or not\n• None dtype ( , optional) – the desired data type of returned tensor. If specified, the input tensor is casted to before the operation is performed. This is useful for preventing data type overflows. Default: None. Returns the standard-deviation of all elements in the tensor. If is , then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel’s correction will be used.\n• None unbiased (bool) – whether to use the unbiased estimation or not Returns the standard-deviation of each row of the tensor in the dimension . If is a list of dimensions, reduce over all of them. If is , the output tensor is of the same size as except in the dimension(s) where it is of size 1. Otherwise, is squeezed (see ), resulting in the output tensor having 1 (or ) fewer dimension(s). If is , then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel’s correction will be used.\n• None dim (int or tuple of python:ints) – the dimension or dimensions to reduce\n• None keepdim (bool) – whether the output tensor has retained or not\n• None unbiased (bool) – whether to use the unbiased estimation or not Returns the sum of all elements in the tensor.\n• None dtype ( , optional) – the desired data type of returned tensor. If specified, the input tensor is casted to before the operation is performed. This is useful for preventing data type overflows. Default: None. Returns the sum of each row of the tensor in the given dimension . If is a list of dimensions, reduce over all of them. If is , the output tensor is of the same size as except in the dimension(s) where it is of size 1. Otherwise, is squeezed (see ), resulting in the output tensor having 1 (or ) fewer dimension(s).\n• None dim (int or tuple of python:ints) – the dimension or dimensions to reduce\n• None keepdim (bool) – whether the output tensor has retained or not\n• None dtype ( , optional) – the desired data type of returned tensor. If specified, the input tensor is casted to before the operation is performed. This is useful for preventing data type overflows. Default: None. Returns the unique scalar elements of the input tensor as a 1-D tensor.\n• None sorted (bool) – Whether to sort the unique elements in ascending order before returning as output.\n• None return_inverse (bool) – Whether to also return the indices for where elements in the original input ended up in the returned unique list.\n• None dim (int) – the dimension to apply unique. If , the unique of the flattened input is returned. default: A tensor or a tuple of tensors containing\n• None inverse_indices (Tensor): (optional) if is True, there will be a 2nd returned tensor (same shape as input) representing the indices for where elements in the original input map to in the output; otherwise, this function will only return a single tensor. Returns the variance of all elements in the tensor. If is , then the variance will be calculated via the biased estimator. Otherwise, Bessel’s correction will be used.\n• None unbiased (bool) – whether to use the unbiased estimation or not Returns the variance of each row of the tensor in the given dimension . If is , the output tensor is of the same size as except in the dimension(s) where it is of size 1. Otherwise, is squeezed (see ), resulting in the output tensor having 1 (or ) fewer dimension(s). If is , then the variance will be calculated via the biased estimator. Otherwise, Bessel’s correction will be used.\n• None dim (int or tuple of python:ints) – the dimension or dimensions to reduce\n• None keepdim (bool) – whether the output tensor has retained or not\n• None unbiased (bool) – whether to use the unbiased estimation or not This function checks if all and satisfy the condition: elementwise, for all elements of and . The behaviour of this function is analogous to numpy.allclose\n• None self (Tensor) – first tensor to compare\n• None other (Tensor) – second tensor to compare\n• None equal_nan (float, optional) – if , then two s will be compared as equal. Default: Returns the indices that sort a tensor along a given dimension in ascending order by value. This is the second value returned by . See its documentation for the exact semantics of this method.\n• None dim (int, optional) – the dimension to sort along The second argument can be a number or a tensor whose shape is broadcastable with the first argument.\n• None other (Tensor or float) – the tensor or value to compare\n• None out (Tensor, optional) – the output tensor. Must be a A containing a 1 at each location where comparison is true if two tensors have the same size and elements, otherwise. The second argument can be a number or a tensor whose shape is broadcastable with the first argument.\n• None other (Tensor or float) – the tensor or value to compare\n• None out (Tensor, optional) – the output tensor that must be a A containing a 1 at each location where comparison is true The second argument can be a number or a tensor whose shape is broadcastable with the first argument.\n• None other (Tensor or float) – the tensor or value to compare\n• None out (Tensor, optional) – the output tensor that must be a A containing a 1 at each location where comparison is true Returns a new tensor with boolean elements representing if each element is or not. A containing a 1 at each location of finite elements and 0 otherwise Returns a new tensor with boolean elements representing if each element is or not. A containing a 1 at each location of elements and 0 otherwise Returns a new tensor with boolean elements representing if each element is or not. A containing a 1 at each location of elements. Returns a namedtuple where is the th smallest element of each row of the tensor in the given dimension . And is the index location of each element found. If is not given, the last dimension of the is chosen. If is , both the and tensors are the same size as , except in the dimension where they are of size 1. Otherwise, is squeezed (see ), resulting in both the and tensors having 1 fewer dimension than the tensor.\n• None dim (int, optional) – the dimension to find the kth value along\n• None keepdim (bool) – whether the output tensors have retained or not\n• None out (tuple, optional) – the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers The second argument can be a number or a tensor whose shape is broadcastable with the first argument.\n• None other (Tensor or float) – the tensor or value to compare\n• None out (Tensor, optional) – the output tensor that must be a A containing a 1 at each location where comparison is true The second argument can be a number or a tensor whose shape is broadcastable with the first argument.\n• None other (Tensor or float) – the tensor or value to compare\n• None out (Tensor, optional) – the output tensor that must be a A containing a 1 at each location where comparison is true Returns the maximum value of all elements in the tensor. Returns a namedtuple where is the maximum value of each row of the tensor in the given dimension . And is the index location of each maximum value found (argmax). If is , the output tensors are of the same size as except in the dimension where they are of size 1. Otherwise, is squeezed (see ), resulting in the output tensors having 1 fewer dimension than .\n• None keepdim (bool, optional) – whether the output tensors have retained or not. Default: .\n• None out (tuple, optional) – the result tuple of two output tensors (max, max_indices) Each element of the tensor is compared with the corresponding element of the tensor and an element-wise maximum is taken. The shapes of and don’t need to match, but they must be broadcastable. When the shapes do not match, the shape of the returned output tensor follows the broadcasting rules.\n• None other (Tensor) – the second input tensor Returns the minimum value of all elements in the tensor. Returns a namedtuple where is the minimum value of each row of the tensor in the given dimension . And is the index location of each minimum value found (argmin). If is , the output tensors are of the same size as except in the dimension where they are of size 1. Otherwise, is squeezed (see ), resulting in the output tensors having 1 fewer dimension than .\n• None keepdim (bool) – whether the output tensors have retained or not\n• None out (tuple, optional) – the tuple of two output tensors (min, min_indices) Each element of the tensor is compared with the corresponding element of the tensor and an element-wise minimum is taken. The resulting tensor is returned. The shapes of and don’t need to match, but they must be broadcastable. When the shapes do not match, the shape of the returned output tensor follows the broadcasting rules.\n• None other (Tensor) – the second input tensor The second argument can be a number or a tensor whose shape is broadcastable with the first argument.\n• None other (Tensor or float) – the tensor or value to compare\n• None out (Tensor, optional) – the output tensor that must be a A containing a 1 at each location where comparison is true. Sorts the elements of the tensor along a given dimension in ascending order by value. If is not given, the last dimension of the is chosen. If is then the elements are sorted in descending order by value. A tuple of (sorted_tensor, sorted_indices) is returned, where the sorted_indices are the indices of the elements in the original tensor.\n• None dim (int, optional) – the dimension to sort along\n• None out (tuple, optional) – the output tuple of ( , ) that can be optionally given to be used as output buffers Returns the largest elements of the given tensor along a given dimension. If is not given, the last dimension of the is chosen. If is then the smallest elements are returned. A tuple of is returned, where the are the indices of the elements in the original tensor. The boolean option if , will make sure that the returned elements are themselves sorted\n• None dim (int, optional) – the dimension to sort along\n• None largest (bool, optional) – controls whether to return largest or smallest elements\n• None sorted (bool, optional) – controls whether to return the elements in sorted order\n• None out (tuple, optional) – the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers This method computes the complex-to-complex discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression: where \\(d\\) = is number of dimensions for the signal, and \\(N_i\\) is the size of signal dimension \\(i\\). This method supports 1D, 2D and 3D complex-to-complex transforms, indicated by . must be a tensor with last dimension of size 2, representing the real and imaginary components of complex numbers, and should have at least dimensions with optionally arbitrary number of leading batch dimensions. If is set to , this normalizes the result by dividing it with \\(\\sqrt{\\prod_{i=1}^K N_i}\\) so that the operator is unitary. Returns the real and the imaginary parts together as one tensor of the same shape of . The inverse of this function is . For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same same configuration. Changing (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions) controls the capacity of this cache. Some cuFFT plans may allocate GPU memory. You can use to query the number of plans currently in cache, and to clear the cache. For CPU tensors, this method is currently only available with MKL. Use to check if MKL is installed.\n• None input (Tensor) – the input tensor of at least dimensions\n• None signal_ndim (int) – the number of dimensions in each signal. can only be 1, 2 or 3 This method computes the complex-to-complex inverse discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression: where \\(d\\) = is number of dimensions for the signal, and \\(N_i\\) is the size of signal dimension \\(i\\). The argument specifications are almost identical with . However, if is set to , this instead returns the results multiplied by \\(\\sqrt{\\prod_{i=1}^d N_i}\\), to become a unitary operator. Therefore, to invert a , the argument should be set identically for . Returns the real and the imaginary parts together as one tensor of the same shape of . The inverse of this function is . For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same same configuration. Changing (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions) controls the capacity of this cache. Some cuFFT plans may allocate GPU memory. You can use to query the number of plans currently in cache, and to clear the cache. For CPU tensors, this method is currently only available with MKL. Use to check if MKL is installed.\n• None input (Tensor) – the input tensor of at least dimensions\n• None signal_ndim (int) – the number of dimensions in each signal. can only be 1, 2 or 3 This method computes the real-to-complex discrete Fourier transform. It is mathematically equivalent with with differences only in formats of the input and output. This method supports 1D, 2D and 3D real-to-complex transforms, indicated by . must be a tensor with at least dimensions with optionally arbitrary number of leading batch dimensions. If is set to , this normalizes the result by dividing it with \\(\\sqrt{\\prod_{i=1}^K N_i}\\) so that the operator is unitary, where \\(N_i\\) is the size of signal dimension \\(i\\). where the index arithmetic is computed modulus the size of the corresponding dimension, \\(\\ ^*\\) is the conjugate operator, and \\(d\\) = . flag controls whether to avoid redundancy in the output results. If set to (default), the output will not be full complex result of shape \\((*, 2)\\), where \\(*\\) is the shape of , but instead the last dimension will be halfed as of size \\(\\lfloor \\frac{N_d}{2} \\rfloor + 1\\). The inverse of this function is . For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same same configuration. Changing (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions) controls the capacity of this cache. Some cuFFT plans may allocate GPU memory. You can use to query the number of plans currently in cache, and to clear the cache. For CPU tensors, this method is currently only available with MKL. Use to check if MKL is installed.\n• None input (Tensor) – the input tensor of at least dimensions\n• None signal_ndim (int) – the number of dimensions in each signal. can only be 1, 2 or 3\n• None onesided (bool, optional) – controls whether to return half of results to avoid redundancy. Default: This method computes the complex-to-real inverse discrete Fourier transform. It is mathematically equivalent with with differences only in formats of the input and output. The argument specifications are almost identical with . Similar to , if is set to , this normalizes the result by multiplying it with \\(\\sqrt{\\prod_{i=1}^K N_i}\\) so that the operator is unitary, where \\(N_i\\) is the size of signal dimension \\(i\\). Due to the conjugate symmetry, do not need to contain the full complex frequency values. Roughly half of the values will be sufficient, as is the case when is given by with . In such case, set the argument of this method to . Moreover, the original signal shape information can sometimes be lost, optionally set to be the size of the original signal (without the batch dimensions if in batched mode) to recover it with correct shape. Therefore, to invert an , the and arguments should be set identically for , and preferrably a is given to avoid size mismatch. See the example below for a case of size mismatch. See for details on conjugate symmetry. The inverse of this function is . Generally speaking, the input of this function should contain values following conjugate symmetry. Note that even if is , often symmetry on some part is still needed. When this requirement is not satisfied, the behavior of is undefined. Since estimates numerical Jacobian with point perturbations, will almost certainly fail the check. For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same same configuration. Changing (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions) controls the capacity of this cache. Some cuFFT plans may allocate GPU memory. You can use to query the number of plans currently in cache, and to clear the cache. For CPU tensors, this method is currently only available with MKL. Use to check if MKL is installed.\n• None input (Tensor) – the input tensor of at least dimensions\n• None signal_ndim (int) – the number of dimensions in each signal. can only be 1, 2 or 3\n• None onesided (bool, optional) – controls whether was halfed to avoid redundancy, e.g., by . Default:\n• None signal_sizes (list or , optional) – the size of the original signal (without batch dimension). Default: # notice that with onesided=True, output size does not determine the original signal size # now we use the original shape to recover x Ignoring the optional batch dimension, this method computes the following expression: where \\(m\\) is the index of the sliding window, and \\(\\omega\\) is the frequency that \\(0 \\leq \\omega < \\text{n\\_fft}\\). When is the default value ,\n• None must be either a 1-D time sequence or a 2-D batch of time sequences.\n• None If is (default), it is treated as equal to .\n• None If is (default), it is treated as equal to .\n• None can be a 1-D tensor of size , e.g., from . If is (default), it is treated as if having \\(1\\) everywhere in the window. If \\(\\text{win\\_length} < \\text{n\\_fft}\\), will be padded on both sides to length before being applied.\n• None If is (default), will be padded on both sides so that the \\(t\\)-th frame is centered at time \\(t \\times \\text{hop\\_length}\\). Otherwise, the \\(t\\)-th frame begins at time \\(t \\times \\text{hop\\_length}\\).\n• None determines the padding method used on when is . See for all available options. Default is .\n• None If is (default), only values for \\(\\omega\\) in \\(\\left[0, 1, 2, \\dots, \\left\\lfloor \\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right]\\) are returned because the real-to-complex Fourier transform satisfies the conjugate symmetry, i.e., \\(X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*\\).\n• None If is (default is ), the function returns the normalized STFT results, i.e., multiplied by \\((\\text{frame\\_length})^{-0.5}\\). Returns the real and the imaginary parts together as one tensor of size \\((* \\times N \\times T \\times 2)\\), where \\(*\\) is the optional batch size of , \\(N\\) is the number of frequencies where STFT is applied, \\(T\\) is the total number of frames used, and each pair in the last dimension represents a complex number as the real part and the imaginary part. This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result.\n• None hop_length (int, optional) – the distance between neighboring sliding window frames. Default: (treated as equal to )\n• None win_length (int, optional) – the size of window frame and STFT filter. Default: (treated as equal to )\n• None window (Tensor, optional) – the optional window function. Default: (treated as window of all \\(1\\) s)\n• None center (bool, optional) – whether to pad on both sides so that the \\(t\\)-th frame is centered at time \\(t \\times \\text{hop\\_length}\\). Default:\n• None pad_mode (string, optional) – controls the padding method used when is . Default:\n• None normalized (bool, optional) – controls whether to return the normalized STFT results Default:\n• None onesided (bool, optional) – controls whether to return half of results to avoid redundancy Default: A tensor containing the STFT result with shape described above where \\(N\\) is the full window size. The input is a positive integer controlling the returned window size. flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like . Therefore, if is true, the \\(N\\) in above formula is in fact \\(\\text{window\\_length} + 1\\). Also, we always have equal to . If \\(=1\\), the returned window contains a single value 1.\n• None periodic (bool, optional) – If True, returns a window to be used as periodic function. If False, return a symmetric window.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ). Only floating point types are supported.\n• None layout ( , optional) – the desired layout of returned window tensor. Only (dense layout) is supported.\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . A 1-D tensor of size \\((\\text{window\\_length},)\\) containing the window where \\(N\\) is the full window size. The input is a positive integer controlling the returned window size. flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like . Therefore, if is true, the \\(N\\) in above formula is in fact \\(\\text{window\\_length} + 1\\). Also, we always have equal to . If \\(=1\\), the returned window contains a single value 1.\n• None periodic (bool, optional) – If True, returns a window to be used as periodic function. If False, return a symmetric window.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ). Only floating point types are supported.\n• None layout ( , optional) – the desired layout of returned window tensor. Only (dense layout) is supported.\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . A 1-D tensor of size \\((\\text{window\\_length},)\\) containing the window where \\(N\\) is the full window size. The input is a positive integer controlling the returned window size. flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like . Therefore, if is true, the \\(N\\) in above formula is in fact \\(\\text{window\\_length} + 1\\). Also, we always have equal to . If \\(=1\\), the returned window contains a single value 1. This is a generalized version of .\n• None periodic (bool, optional) – If True, returns a window to be used as periodic function. If False, return a symmetric window.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ). Only floating point types are supported.\n• None layout ( , optional) – the desired layout of returned window tensor. Only (dense layout) is supported.\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . A 1-D tensor of size \\((\\text{window\\_length},)\\) containing the window where \\(N\\) is the full window size. The input is a positive integer controlling the returned window size. flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like . Therefore, if is true, the \\(N\\) in above formula is in fact \\(\\text{window\\_length} + 1\\). Also, we always have equal to . If \\(=1\\), the returned window contains a single value 1.\n• None periodic (bool, optional) – If True, returns a window to be used as periodic function. If False, return a symmetric window.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , uses a global default (see ). Only floating point types are supported.\n• None layout ( , optional) – the desired layout of returned window tensor. Only (dense layout) is supported.\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n• None requires_grad (bool, optional) – If autograd should record operations on the returned tensor. Default: . A 1-D tensor of size \\((\\text{window\\_length},)\\) containing the window Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in unless is empty, in which case the result is a tensor of size 0. If is specified, the number of bins is at least and if is empty, then the result is tensor of size filled with zeros. If is the value at position , if is specified else . When using the CUDA backend, this operation may induce nondeterministic behaviour that is not easily switched off. Please see the notes on /notes/randomness for background.\n• None weights (Tensor) – optional, weight for each value in the input tensor. Should be of same size as input tensor.\n• None minlength (int) – optional, minimum number of bins. Should be non-negative. a tensor of shape if is non-empty, else Broadcasts the given tensors according to broadcasting-semantics. *tensors – any number of tensors of the same type More than one element of a broadcasted tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first. Do cartesian product of the given sequence of tensors. The behavior is similar to python’s . A tensor equivalent to converting all the input tensors into lists, do on these lists, and finally convert the resulting list into tensor. Compute combinations of length \\(r\\) of the given tensor. The behavior is similar to python’s when is set to , and when is set to .\n• None with_replacement (boolean, optional) – whether to allow duplication in combination A tensor equivalent to converting all the input tensors into lists, do or on these lists, and finally convert the resulting list into tensor. Returns the cross product of vectors in dimension of and . and must have the same size, and the size of their dimension should be 3. If is not given, it defaults to the first dimension found with the size 3.\n• None other (Tensor) – the second input tensor\n• None dim (int, optional) – the dimension to take the cross-product in.\n• None If is a vector (1-D tensor), then returns a 2-D square tensor with the elements of as the diagonal.\n• None If is a matrix (2-D tensor), then returns a 1-D tensor with the diagonal elements of . The argument controls which diagonal to consider:\n• None If = 0, it is the main diagonal.\n• None If > 0, it is above the main diagonal.\n• None If < 0, it is below the main diagonal.\n• None diagonal (int, optional) – the diagonal to consider always returns the diagonal of its input. always constructs a tensor with diagonal elements specified by the input. Get the square matrix where the input vector is the diagonal: Get the k-th diagonal of a given matrix: Creates a tensor whose diagonals of certain 2D planes (specified by and ) are filled by . To facilitate creating batched diagonal matrices, the 2D planes formed by the last two dimensions of the returned tensor are chosen by default. The argument controls which diagonal to consider:\n• None If = 0, it is the main diagonal.\n• None If > 0, it is above the main diagonal.\n• None If < 0, it is below the main diagonal. The size of the new matrix will be calculated to make the specified diagonal of the size of the last input dimension. Note that for other than \\(0\\), the order of and matters. Exchanging them is equivalent to changing the sign of . Applying to the output of this function with the same arguments yields a matrix identical to input. However, has different default dimensions, so those need to be explicitly specified.\n• None input (Tensor) – the input tensor. Must be at least 1-dimensional.\n• None offset (int, optional) – which diagonal to consider. Default: 0 (main diagonal).\n• None dim1 (int, optional) – first dimension with respect to which to take diagonal. Default: -2.\n• None dim2 (int, optional) – second dimension with respect to which to take diagonal. Default: -1.\n• None If is a vector (1-D tensor), then returns a 2-D square tensor with the elements of as the diagonal.\n• None If is a tensor with more than one dimension, then returns a 2-D tensor with diagonal elements equal to a flattened . The argument controls which diagonal to consider:\n• None If = 0, it is the main diagonal.\n• None If > 0, it is above the main diagonal.\n• None If < 0, it is below the main diagonal.\n• None offset (int, optional) – the diagonal to consider. Default: 0 (main diagonal). Returns a partial view of with the its diagonal elements with respect to and appended as a dimension at the end of the shape. The argument controls which diagonal to consider:\n• None If = 0, it is the main diagonal.\n• None If > 0, it is above the main diagonal.\n• None If < 0, it is below the main diagonal. Applying to the output of this function with the same arguments yields a diagonal matrix with the diagonal entries of the input. However, has different default dimensions, so those need to be explicitly specified.\n• None input (Tensor) – the input tensor. Must be at least 2-dimensional.\n• None offset (int, optional) – which diagonal to consider. Default: 0 (main diagonal).\n• None dim1 (int, optional) – first dimension with respect to which to take diagonal. Default: 0.\n• None dim2 (int, optional) – second dimension with respect to which to take diagonal. Default: 1. To take a batch diagonal, pass in dim1=-2, dim2=-1. This function provides a way of computing multilinear expressions (i.e. sums of products) using the Einstein summation convention.\n• None equation (string) – The equation is given in terms of lower case letters (indices) to be associated with each dimension of the operands and result. The left hand side lists the operands dimensions, separated by commas. There should be one index letter per tensor dimension. The right hand side follows after and gives the indices for the output. If the and right hand side are omitted, it implicitly defined as the alphabetically sorted list of all indices appearing exactly once in the left hand side. The indices not apprearing in the output are summed over after multiplying the operands entries. If an index appears several times for the same operand, a diagonal is taken. Ellipses represent a fixed number of dimensions. If the right hand side is inferred, the ellipsis dimensions are at the beginning of the output.\n• None operands (list of Tensors) – The operands to compute the Einstein sum of.\n• None start_dim (int) – the first dim to flatten\n• None end_dim (int) – the last dim to flatten Reverse the order of a n-D tensor along given axis in dims.\n• None dims (a list or tuple) – axis to flip on Rotate a n-D tensor by 90 degrees in the plane specified by dims axis. Rotation direction is from the first towards the second axis if k > 0, and from the second towards the first for k < 0. The elements are sorted into equal width bins between and . If and are both zero, the minimum and maximum values of the data are used. Take \\(N\\) tensors, each of which can be either scalar or 1-dimensional vector, and create \\(N\\) N-dimensional grids, where the \\(i\\) th grid is defined by expanding the \\(i\\) th input over dimensions defined by other inputs. tensors (list of Tensor): list of scalars or 1 dimensional tensors. Scalars will be treated as tensors of size \\((1,)\\) automatically seq (sequence of Tensors): If the input has \\(k\\) tensors of size \\((N_1,), (N_2,), \\ldots , (N_k,)\\), then the output would also has \\(k\\) tensors, where all tensors are of size \\((N_1, N_2, \\ldots , N_k)\\). Returns a tensor where each sub-tensor of along dimension is normalized such that the -norm of the sub-tensor is lower than the value If the norm of a row is lower than , the row is unchanged\n• None p (float) – the power for the norm computation\n• None dim (int) – the dimension to slice over to get the sub-tensors\n• None maxnorm (float) – the maximum norm to keep each sub-tensor under Roll the tensor along the given dimension(s). Elements that are shifted beyond the last position are re-introduced at the first position. If a dimension is not specified, the tensor will be flattened before rolling and then restored to the original shape.\n• None shifts (int or tuple of python:ints) – The number of places by which the elements of the tensor are shifted. If shifts is a tuple, dims must be a tuple of the same size, and each dimension will be rolled by the corresponding value\n• None dims (int or tuple of python:ints) – Axis along which to roll Returns a contraction of a and b over multiple dimensions.\n• None dims (int or tuple of two lists of python:integers) – number of dimensions to contract or explicit lists of dimensions for and respectively When called with an integer argument = \\(d\\), and the number of dimensions of and is \\(m\\) and \\(n\\), respectively, it computes When called with of the list form, the given dimensions will be contracted in place of the last \\(d\\) of and the first \\(d\\) of \\(b\\). The sizes in these dimensions must match, but will deal with broadcasted dimensions. Returns the sum of the elements of the diagonal of the input 2-D matrix. Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices , the other elements of the result tensor are set to 0. The lower triangular part of the matrix is defined as the elements on and below the diagonal. The argument controls which diagonal to consider. If = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices \\(\\lbrace (i, i) \\rbrace\\) for \\(i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]\\) where \\(d_{1}, d_{2}\\) are the dimensions of the matrix.\n• None diagonal (int, optional) – the diagonal to consider Returns the indices of the lower triangular part of a -by- matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates. Indices are ordered based on rows and then columns. The lower triangular part of the matrix is defined as the elements on and below the diagonal. The argument controls which diagonal to consider. If = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices \\(\\lbrace (i, i) \\rbrace\\) for \\(i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]\\) where \\(d_{1}, d_{2}\\) are the dimensions of the matrix. NOTE: when running on ‘cuda’, row * col must be less than \\(2^{59}\\) to prevent overflow during calculation.\n• None row ( ) – number of rows in the 2-D matrix.\n• None column ( ) – number of columns in the 2-D matrix.\n• None offset ( ) – diagonal offset from the main diagonal. Default: if not provided, 0.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , .\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices , the other elements of the result tensor are set to 0. The upper triangular part of the matrix is defined as the elements on and above the diagonal. The argument controls which diagonal to consider. If = 0, all elements on and below the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices \\(\\lbrace (i, i) \\rbrace\\) for \\(i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]\\) where \\(d_{1}, d_{2}\\) are the dimensions of the matrix.\n• None diagonal (int, optional) – the diagonal to consider Returns the indices of the upper triangular part of a by matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates. Indices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and above the diagonal. The argument controls which diagonal to consider. If = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices \\(\\lbrace (i, i) \\rbrace\\) for \\(i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]\\) where \\(d_{1}, d_{2}\\) are the dimensions of the matrix. NOTE: when running on ‘cuda’, row * col must be less than \\(2^{59}\\) to prevent overflow during calculation.\n• None row ( ) – number of rows in the 2-D matrix.\n• None column ( ) – number of columns in the 2-D matrix.\n• None offset ( ) – diagonal offset from the main diagonal. Default: if not provided, 0.\n• None dtype ( , optional) – the desired data type of returned tensor. Default: if , .\n• None device ( , optional) – the desired device of returned tensor. Default: if , uses the current device for the default tensor type (see ). will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. Performs a batch matrix-matrix product of matrices stored in and , with a reduced add step (all matrix multiplications get accumulated along the first dimension). is added to the final result. and must be 3-D tensors each containing the same number of matrices. If is a \\((b \\times n \\times m)\\) tensor, is a \\((b \\times m \\times p)\\) tensor, must be broadcastable with a \\((n \\times p)\\) tensor and will be a \\((n \\times p)\\) tensor. For inputs of type or , arguments and must be real numbers, otherwise they should be integers.\n• None batch1 (Tensor) – the first batch of matrices to be multiplied\n• None batch2 (Tensor) – the second batch of matrices to be multiplied Performs a matrix multiplication of the matrices and . The matrix is added to the final result. If is a \\((n \\times m)\\) tensor, is a \\((m \\times p)\\) tensor, then must be broadcastable with a \\((n \\times p)\\) tensor and will be a \\((n \\times p)\\) tensor. and are scaling factors on matrix-vector product between and and the added matrix respectively. For inputs of type or , arguments and must be real numbers, otherwise they should be integers.\n• None mat1 (Tensor) – the first matrix to be multiplied\n• None mat2 (Tensor) – the second matrix to be multiplied Performs a matrix-vector product of the matrix and the vector . The vector is added to the final result. If is a \\((n \\times m)\\) tensor, is a 1-D tensor of size , then must be broadcastable with a 1-D tensor of size and will be 1-D tensor of size . and are scaling factors on matrix-vector product between and and the added tensor respectively. For inputs of type or , arguments and must be real numbers, otherwise they should be integers Performs the outer-product of vectors and and adds it to the matrix . Optional values and are scaling factors on the outer product between and and the added matrix respectively. If is a vector of size and is a vector of size , then must be broadcastable with a matrix of size \\((n \\times m)\\) and will be a matrix of size \\((n \\times m)\\). For inputs of type or , arguments and must be real numbers, otherwise they should be integers\n• None vec1 (Tensor) – the first vector of the outer product\n• None vec2 (Tensor) – the second vector of the outer product Performs a batch matrix-matrix product of matrices in and . is added to the final result. and must be 3-D tensors each containing the same number of matrices. If is a \\((b \\times n \\times m)\\) tensor, is a \\((b \\times m \\times p)\\) tensor, then must be broadcastable with a \\((b \\times n \\times p)\\) tensor and will be a \\((b \\times n \\times p)\\) tensor. Both and mean the same as the scaling factors used in . For inputs of type or , arguments and must be real numbers, otherwise they should be integers.\n• None mat (Tensor) – the tensor to be added\n• None batch1 (Tensor) – the first batch of matrices to be multiplied\n• None batch2 (Tensor) – the second batch of matrices to be multiplied Performs a batch matrix-matrix product of matrices stored in and . and must be 3-D tensors each containing the same number of matrices. If is a \\((b \\times n \\times m)\\) tensor, is a \\((b \\times m \\times p)\\) tensor, will be a \\((b \\times n \\times p)\\) tensor. This function does not broadcast. For broadcasting matrix products, see .\n• None batch1 (Tensor) – the first batch of matrices to be multiplied\n• None batch2 (Tensor) – the second batch of matrices to be multiplied Returns a tuple containing the LU factorization and pivots. Pivoting is done if is set. LU factorization with = is not available for CPU, and attempting to do so will throw an error. However, LU factorization with = is available for CUDA.\n• None pivot (bool, optional) – controls whether pivoting is done This is a version of that always creates an info , and returns it as the third return value.\n• None pivot (bool, optional) – controls whether pivoting is done A tuple containing factorization, pivots, and an where non-zero values indicate whether factorization for each minibatch sample succeeds. Returns the LU solve of the linear system \\(Ax = b\\).\n• None LU_data (Tensor) – the pivoted LU factorization of A from .\n• None LU_pivots (IntTensor) – the pivots of the LU factorization Unpacks the data and pivots from a batched LU factorization (btrifact) of a tensor.\n• None unpack_data (bool) – flag indicating if the data should be unpacked\n• None unpack_pivots (bool) – flag indicating if the pivots should be unpacked Returns the matrix product of the \\(N\\) 2-D tensors. This product is efficiently computed using the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms of arithmetic operations ([CLRS]). Note that since this is a function to compute the product, \\(N\\) needs to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned. If \\(N\\) is 1, then this is a no-op - the original matrix is returned as is. matrices (Tensors...) – a sequence of 2 or more 2-D tensors whose product is to be determined. if the \\(i^{th}\\) tensor was of dimensions \\(p_{i} \\times p_{i + 1}\\), then the product would be of dimensions \\(p_{1} \\times p_{N + 1}\\). Computes the Cholesky decomposition of a symmetric positive-definite matrix \\(A\\) or for batches of symmetric positive-definite matrices. If is , the returned matrix is upper-triangular, and the decomposition has the form: If is , the returned matrix is lower-triangular, and the decomposition has the form: If is , and is a batch of symmetric positive-definite matrices, then the returned tensor will be composed of upper-triangular Cholesky factors of each of the individual matrices. Similarly, when is , the returned tensor will be composed of lower-triangular Cholesky factors of each of the individual matrices.\n• None a (Tensor) – the input tensor of size (*, n, n) where is zero or more batch dimensions consisting of symmetric positive-definite matrices.\n• None upper (bool, optional) – flag that indicates whether to return a upper or lower triangular matrix. Default: Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix . If is , is and lower triangular and is returned such that: If is or not provided, is upper triangular and is returned such that: can take in 2D inputs or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs The keyword only supports 2D matrix inputs, that is, must be 2D matrices.\n• None b (Tensor) – input matrix of size \\((*, m, k)\\), where \\(*\\) is zero or more batch dimensions\n• None u (Tensor) – input matrix of size \\((*, m, m)\\), where \\(*\\) is zero of more batch dimensions composed of upper or lower triangular Cholesky factor\n• None upper (bool, optional) – whether to consider the Cholesky factor as a lower or upper triangular matrix. Default: .\n• None out (Tensor, optional) – the output tensor for Computes the dot product (inner product) of two tensors. This function does not broadcast. Computes the eigenvalues and eigenvectors of a real square matrix. Since eigenvalues and eigenvectors might be complex, backward pass is supported only\n• None a (Tensor) – the square matrix of shape \\((n \\times n)\\) for which the eigenvalues and eigenvectors will be computed\n• None eigenvectors (bool) – to compute both eigenvalues and eigenvectors; otherwise, only eigenvalues will be computed\n• None eigenvalues (Tensor): Shape \\((n \\times 2)\\). Each row is an eigenvalue of , where the first element is the real part and the second element is the imaginary part. The eigenvalues are not necessarily ordered.\n• None eigenvectors (Tensor): If , it’s an empty tensor. Otherwise, this tensor of shape \\((n \\times n)\\) can be used to compute normalized (unit length) eigenvectors of corresponding eigenvalues as follows. If the corresponding is a real number, column is the eigenvector corresponding to . If the corresponding and form a complex conjugate pair, then the true eigenvectors can be computed as \\(\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]\\), \\(\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]\\). Computes the solution to the least squares and least norm problems for a full rank matrix \\(A\\) of size \\((m \\times n)\\) and a matrix \\(B\\) of size \\((m \\times k)\\). Returned tensor \\(X\\) has shape \\((\\max(m, n) \\times k)\\). The first \\(n\\) rows of \\(X\\) contains the solution. If \\(m \\geq n\\), the residual sum of squares for the solution in each column is given by the sum of squares of elements in the remaining \\(m - n\\) rows of that column.\n• None qr (Tensor): the details of the QR factorization The returned matrices will always be transposed, irrespective of the strides of the input matrices. That is, they will have stride instead of . This is a low-level function for calling LAPACK directly. This function returns a namedtuple (a, tau) as defined in LAPACK documentation for geqrf . You’ll generally want to use instead. Computes a QR decomposition of , but without constructing \\(Q\\) and \\(R\\) as explicit separate matrices. Rather, this directly calls the underlying LAPACK function which produces a sequence of ‘elementary reflectors’. See LAPACK documentation for geqrf for further details.\n• None out (tuple, optional) – the output tuple of (Tensor, Tensor) Outer product of and . If is a vector of size \\(n\\) and is a vector of size \\(m\\), then must be a matrix of size \\((n \\times m)\\). This function does not broadcast. This function returns the solution to the system of linear equations represented by \\(AX = B\\) and the LU factorization of A, in order as a tuple . For more information regarding , please check . is deprecated in favour of and will be removed in the next release. Please use instead. Takes the inverse of the square matrix . can be batches of 2D square tensors, in which case this function would return a tensor composed of individual inverses. Irrespective of the original strides, the returned tensors will be transposed, i.e. with strides like\n• None input (Tensor) – the input tensor of size (*, n, n) where is zero or more batch dimensions Backward through internally uses SVD results when is not invertible. In this case, double backward through will be unstable in when doesn’t have distinct singular values. See for details. Result is if has zero log determinant, and is if has negative determinant. Backward through internally uses SVD results when is not invertible. In this case, double backward through will be unstable in when doesn’t have distinct singular values. See for details. Calculates the sign and log value of a 2D square tensor’s determinant. If has zero determinant, this returns . Backward through internally uses SVD results when is not invertible. In this case, double backward through will be unstable in when doesn’t have distinct singular values. See for details. A tuple containing the sign of the determinant, and the log value of the absolute determinant. The behavior depends on the dimensionality of the tensors as follows:\n• None If both tensors are 1-dimensional, the dot product (scalar) is returned.\n• None If both arguments are 2-dimensional, the matrix-matrix product is returned.\n• None If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed.\n• None If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned.\n• None If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), then a batched matrix multiply is returned. If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are broadcasted (and thus must be broadcastable). For example, if is a \\((j \\times 1 \\times n \\times m)\\) tensor and is a \\((k \\times m \\times p)\\) tensor, will be an \\((j \\times k \\times n \\times p)\\) tensor. The 1-dimensional dot product version of this function does not support an parameter.\n• None tensor1 (Tensor) – the first tensor to be multiplied\n• None tensor2 (Tensor) – the second tensor to be multiplied Returns the matrix raised to the power for square matrices. For batch of matrices, each individual matrix is raised to the power . If is negative, then the inverse of the matrix (if invertible) is raised to the power . For a batch of matrices, the batched inverse (if invertible) is raised to the power . If is 0, then an identity matrix is returned.\n• None n (int) – the power to raise the matrix to Returns the numerical rank of a 2-D tensor. The method to compute the matrix rank is done using SVD by default. If is , then is assumed to be symmetric, and the computation of the rank is done by obtaining the eigenvalues. is the threshold below which the singular values (or the eigenvalues when is ) are considered to be 0. If is not specified, is set to where is the singular values (or the eigenvalues when is ), and is the epsilon value for the datatype of .\n• None symmetric (bool, optional) – indicates whether is symmetric. Default: Performs a matrix multiplication of the matrices and . If is a \\((n \\times m)\\) tensor, is a \\((m \\times p)\\) tensor, will be a \\((n \\times p)\\) tensor. This function does not broadcast. For broadcasting matrix products, see .\n• None mat1 (Tensor) – the first matrix to be multiplied\n• None mat2 (Tensor) – the second matrix to be multiplied Performs a matrix-vector product of the matrix and the vector . If is a \\((n \\times m)\\) tensor, is a 1-D tensor of size \\(m\\), will be 1-D of size \\(n\\). This function does not broadcast. Computes the orthogonal matrix of a QR factorization, from the tuple returned by . This directly calls the underlying LAPACK function . See LAPACK documentation for orgqr for further details. Multiplies by the orthogonal matrix of the QR factorization formed by that is represented by . This directly calls the underlying LAPACK function . See LAPACK documentation for ormqr for further details.\n• None mat (Tensor) – the matrix to be multiplied. Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor. Please look at Moore-Penrose inverse for more details This method is implemented using the Singular Value Decomposition. The pseudo-inverse is not necessarily a continuous function in the elements of the matrix [1]. Therefore, derivatives are not always existent, and exist for a constant rank only [2]. However, this method is backprop-able due to the implementation by using SVD results, and could be unstable. Double-backward will also be unstable due to the usage of SVD internally. See for more details.\n• None rcond (float) – A floating point value to determine the cutoff for small singular values. Default: 1e-15 The pseudo-inverse of of dimensions \\(n \\times m\\) For more information regarding , please check . is deprecated in favour of and will be removed in the next release. Please use instead and note that the argument in defaults to . Computes the inverse of a positive semidefinite matrix given its Cholesky factor : returns matrix If is or not provided, is upper triangular such that the returned tensor is If is , is lower triangular such that the returned tensor is\n• None upper (bool, optional) – whether to return a upper (default) or lower triangular matrix\n• None out (Tensor, optional) – the output tensor for Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix . For more information regarding , please check . is deprecated in favour of and will be removed in the next release. Please use instead and note that the argument in defaults to . Computes the pivoted Cholesky decomposition of a symmetric positive-definite matrix . returns a namedtuple (u, pivot) of matrice. If is or not provided, is upper triangular such that \\(a = p^T u^T u p\\), with the permutation given by . If is , is lower triangular such that \\(a = p^T u u^T p\\). is deprecated in favour of and will be removed in the next release.\n• None upper (bool, optional) – whether to return a upper (default) or lower triangular matrix\n• None out (tuple, optional) – namedtuple of and tensors Computes the QR decomposition of a matrix , and returns a namedtuple (Q, R) of matrices such that \\(\\text{input} = Q R\\), with \\(Q\\) being an orthogonal matrix and \\(R\\) being an upper triangular matrix. precision may be lost if the magnitudes of the elements of are large While it should always give you a valid decomposition, it may not give you the same one across platforms - it will depend on your LAPACK implementation. Irrespective of the original strides, the returned matrix \\(Q\\) will be transposed, i.e. with strides instead of .\n• None out (tuple, optional) – tuple of and tensors This function returns the solution to the system of linear equations represented by \\(AX = B\\) and the LU factorization of A, in order as a tuple . contains and factors for LU factorization of . can take in 2D inputs or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs . Irrespective of the original strides, the returned matrices and will be transposed, i.e. with strides like and respectively.\n• None B (Tensor) – input matrix of size \\((*, m, k)\\) , where \\(*\\) is zero or more batch dimensions.\n• None A (Tensor) – input square matrix of size \\((*, m, m)\\), where \\(*\\) is zero or more batch dimensions. returns a namedtuple which the singular value decomposition of a input real matrix of size such that \\(A = USV^T\\). is a diagonal matrix of shape \\((n \\times m)\\), represented as a vector of size \\(\\min(n, m)\\) containing the non-negative diagonal entries. If is (default), the returned and matrices will contain only \\(min(n, m)\\) orthonormal columns. If is , the returned and matrices will be zero matrices of shape \\((n \\times n)\\) and \\((m \\times m)\\) respectively. will be ignored here. The implementation of SVD on CPU uses the LAPACK routine (a divide-and-conquer algorithm) instead of for speed. Analogously, the SVD on GPU uses the MAGMA routine as well. Irrespective of the original strides, the returned matrix will be transposed, i.e. with strides instead of . Extra care needs to be taken when backward through and outputs. Such operation is really only stable when is full rank with all distinct singular values. Otherwise, can appear as the gradients are not properly defined. Also, notice that double backward will usually do an additional backward through and even if the original backward is only on . When = , the gradients on and will be ignored in backward as those vectors can be arbitrary bases of the subspaces. When = , backward cannot be performed since and from the forward pass is required for the backward operation.\n• None some (bool, optional) – controls the shape of returned and\n• None out (tuple, optional) – the output tuple of tensors This function returns eigenvalues and eigenvectors of a real symmetric matrix , represented by a namedtuple (eigenvalues, eigenvectors). and \\(V\\) are \\((m \\times m)\\) matrices and \\(e\\) is a \\(m\\) dimensional vector. This function calculates all eigenvalues (and vectors) of such that \\(\\text{input} = V \\text{diag}(e) V^T\\). The boolean argument defines computation of eigenvectors or eigenvalues only. If it is , only eigenvalues are computed. If it is , both eigenvalues and eigenvectors are computed. Since the input matrix is supposed to be symmetric, only the upper triangular portion is used by default. If is , then lower triangular portion is used. Irrespective of the original strides, the returned matrix will be transposed, i.e. with strides instead of . Extra care needs to be taken when backward through outputs. Such operation is really only stable when all eigenvalues are distinct. Otherwise, can appear as the gradients are not properly defined.\n• None eigenvectors (boolean, optional) – controls whether eigenvectors have to be computed\n• None upper (boolean, optional) – controls whether to consider upper-triangular or lower-triangular region\n• None out (tuple, optional) – the output tuple of (Tensor, Tensor)\n• None eigenvalues (Tensor): Shape \\((m)\\). Each element is an eigenvalue of , The eigenvalues are in ascending order.\n• None eigenvectors (Tensor): Shape \\((m \\times m)\\). If , it’s a tensor filled with zeros. Otherwise, this tensor contains the orthonormal eigenvectors of the . Solves a system of equations with a triangular coefficient matrix \\(A\\) and multiple right-hand sides . In particular, solves \\(AX = b\\) and assumes \\(A\\) is upper-triangular with the default keyword arguments.\n• None b (Tensor) – multiple right-hand sides. Each column of \\(b\\) is a right-hand side for the system of equations.\n• None upper (bool, optional) – whether to solve the upper-triangular system of equations (default) or the lower-triangular system of equations. Default: True.\n• None transpose (bool, optional) – whether \\(A\\) should be transposed before being sent into the solver. Default: False.\n• None unitriangular (bool, optional) – whether \\(A\\) is unit triangular. If True, the diagonal elements of \\(A\\) are assumed to be 1 and not referenced from \\(A\\). Default: False. A tuple \\((X, M)\\) where \\(M\\) is a clone of \\(A\\) and \\(X\\) is the solution to \\(AX = b\\) (or whatever variant of the system of equations, depending on the keyword arguments.) Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1\n\nsupports three backends, each with different capabilities. The table below shows which functions are available for use with CPU / CUDA tensors. MPI supports CUDA only if the implementation used to build PyTorch supports it. Backends that come with PyTorch¶ PyTorch distributed currently only supports Linux. By default, the Gloo and NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA). MPI is an optional backend that can only be included if you build PyTorch from source. (e.g. building PyTorch on a host that has MPI installed.) In the past, we were often asked: “which backend should I use?”.\n• \n• None Use the NCCL backend for distributed GPU training\n• None Use the Gloo backend for distributed CPU training.\n• \n• None Use NCCL, since it’s the only backend that currently supports InfiniBand and GPUDirect.\n• \n• None Use NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)\n• \n• None If your InfiniBand has enabled IP over IB, use Gloo, otherwise, use MPI instead. We are planning on adding InfiniBand support for Gloo in the upcoming releases.\n• \n• None Use Gloo, unless you have specific reasons to use MPI. By default, both NCCL and Gloo backends will try to find the network interface to use for communication. However, this is not always guaranteed to be successful from our experiences. Therefore, if you encounter any problem on either backend not being able to find the correct network interface. You can try to set the following environment variables (each one applicable to its respective backend): NCCL has also provided a number of environment variables for fine-tuning purposes. Commonly used ones include the following for debugging purposes: For the full list of NCCL environment variables, please refer to NVIDIA NCCL’s official documentation The package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines. The class builds on this functionality to provide synchronous distributed training as a wrapper around any PyTorch model. This differs from the kinds of parallelism provided by Multiprocessing package - torch.multiprocessing and in that it supports multiple network-connected machines and in that the user must explicitly launch a separate copy of the main training script for each process. In the single-machine synchronous case, or the wrapper may still have advantages over other approaches to data-parallelism, including :\n• None Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes.\n• None Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and “GIL-thrashing” that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components. The package needs to be initialized using the function before calling any other methods. This blocks until all processes have joined. Currently three initialization methods are supported: There are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired . The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks. Note that multicast address is not supported anymore in the latest distributed package. is deprecated as well. # Use address of one of the machines Another initialization method makes use of a file system that is shared and visible from all machines in a group, along with a desired . The URL should start with and contain a path to a non-existent file (in an existing directory) on a shared file system. File-system initialization will automatically create that file if it doesn’t exist, but will not delete the file. Therefore, it is your responsibility to make sure that the file is cleaned up before the next call on the same file path/name. Note that automatic rank assignment is not supported anymore in the latest distributed package and is deprecated as well. This method assumes that the file system supports locking using - most local systems and NFS support it. This method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty everytime is called. # rank should always be specified This method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:\n• None - required; has to be a free port on machine with rank 0\n• None - required (except for rank 0); address of rank 0 node\n• None - required; can be set either here, or in a call to init function\n• None - required; can be set either here, or in a call to init function The machine with rank 0 will be used to set up all connections. This is the default method, meaning that does not have to be specified (or can be ). By default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns). Currently does not support creating groups with different backends. In other words, each group being created will use the same backend as you specified in . and return distributed request objects when used. In general, the type of this object is unspecified as they should never be created manually, but they are guaranteed to support two methods:\n• None - returns True if the operation has finished\n• None - will block the process until the operation is finished. is guaranteed to return True once it returns. Every collective operation function supports the following two kinds of operations: synchronous operation - the default mode, when is set to False. when the function returns, it is guaranteed that the collective operation is performed (not necessarily completed if it’s a CUDA op since all CUDA ops are asynchronous), and any further function calls depending on the data of the collective operation can be called. In the synchronous mode, the collective function does not return anything asynchronous operation - when is set to True. The collective operation function returns a distributed request object. In general, you don’t need to create it manually and it is guaranteed to support two methods:\n• None - returns True if the operation has finished\n• None - will block the process until the operation is finished. is recommended to use instead. If you have more than one GPU on each node, when using the NCCL and Gloo backend, and support distributed collective operations among multiple GPUs within each node. These functions can potentially improve the overall distributed training performance and be easily used by passing a list of tensors. Each Tensor in the passed tensor list needs to be on a separate GPU device of the host where the function is called. Note that the length of the tensor list needs to be identical among all the distributed processes. Also note that currently the multi-GPU collective functions are only supported by the NCCL backend. For example, if the system we use for distributed training has 2 nodes, each of which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would like to all-reduce. The following code can serve as a reference: After the call, all 16 tensors on the two nodes will have the all-reduced value of 16 The package also provides a launch utility in . This helper utility can be used to launch multiple processes per node for distributed training. This utility also supports both python2 and python3. is a module that spawns up multiple distributed training processes on each of the training nodes. The utility can be used for single-node distributed training, in which one or more processes per node will be spawned. The utility can be used for either CPU training or GPU training. If the utility is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. It can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be benefitial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth. In both cases of single-node distributed training or multi-node distributed training, this utility will launch the given number of processes per node ( ). If used for GPU training, this number needs to be less or euqal to the number of GPUs on the current system ( ), and each process will be operating on a single GPU from GPU 0 to GPU (nproc_per_node - 1). YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other and all other arguments of your training script) and all other arguments of your training script)\n• None To look up what optional arguments this module offers: 1. This utilty and multi-process distributed (single-node or multi-node) GPU training currently only achieves the best performance using the NCCL distributed backend. Thus NCCL backend is the recommended backend to use for GPU training. 2. In your training program, you must parse the command-line argument: , which will be provided by this module. If your training program uses GPUs, you should ensure that your code only runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by: Set your device to local rank using either 3. In your training program, you are supposed to call the following function at the beginning to start the distributed backend. You need to make sure that the init_method uses , which is the only supported by this module. 4. In your training program, you can either use regular distributed functions or use module. If your training program uses GPUs for training and you would like to use module, here is how to configure it. Please ensure that argument is set to be the only GPU device id that your code will be operating on. This is generally the local rank of the process. In other words, the needs to be , and needs to be in order to use this utility 5. Another way to pass to the subprocesses via environment variable . This behavior is enabled when you launch the script with . You must adjust the subprocess example above to replace with ; the launcher will not pass when you specify this flag. is NOT globally unique: it is only unique per process on a machine. Thus, don’t use it to decide if you should, e.g., write to a networked filesystem. See https://github.com/pytorch/pytorch/issues/12042 for an example of how things can go wrong if you don’t do this correctly. The torch.multiprocessing package also provides a function in . This helper function can be used to spawn multiple processes. It works by passing in the function that you want to run and spawns N processes to run it. This can be used for multiprocess distributed training as well. For references on how to use it, please refer to PyToch example - ImageNet implementation Note that this function requires Python 3.4 or higher.\n\nTorchScript is a way to create serializable and optimizable models from PyTorch code. Any code written in TorchScript can be saved from your Python process and loaded in a process where there is no Python dependency. We provide tools to incrementally transition a model from being a pure Python program to a TorchScript program that can be run independently from Python, for instance, in a standalone C++ program. This makes it possible to train models in PyTorch using familiar tools and then export the model to a production environment where it is not a good idea to run models as Python programs for performance and multi-threading reasons. The core data structure in TorchScript is the . It is an analogue of torch’s nn.Module and represents an entire model as a tree of submodules. Like normal modules, each individual module in a ScriptModule can have submodules, parameters, and methods. In nn.Modules methods are implemented as Python functions, but in ScriptModules methods typically implemented as TorchScript functions, a statically-typed subset of Python that contains all of PyTorch’s built-in Tensor operations. This difference allows your ScriptModules code to run without the need for a Python interpreter. ScriptModules and the TorchScript functions inside of them can be created in two ways: Using , you can take an existing module or python function, provide example inputs, and we run the function, recording the operations performed on all the tensors. We turn the resulting recording into a TorchScript method that is installed as the method of a ScriptModule. This module also contains any parameters that the original module had as well. Tracing a function will produce a with a single method that implements that function, and that contains no parameters. Tracing only records operations done when the given function is run on the given tensors. Therefore, the returned will always run the same traced graph on any input. This has some important implications when your module is expected to run different sets of operations, depending on the input and/or the module state. For example,\n• None Tracing will not record any control-flow like if statements or loops. When this control-flow is constant across your module, this is fine and it often just inlines configuration decisions. But sometimes the control-flow is actually part of the model itself. For instance, a recurrent network is a loop over the (possibly dynamic) length of an input sequence.\n• None In the returned , operations that have different behaviors in and modes will always behave as if it is in the mode it was in during tracing, no matter which mode the is in. In cases like these, tracing would not be appropriate and scripting is a better choice. You can write TorchScript code directly using Python syntax. You do this using the annotation (for functions) or annotation (for methods) on subclasses of ScriptModule. With this annotation the body of the annotated function is directly translated into TorchScript. TorchScript itself is a subset of the Python language, so not all features in python work, but we provide enough functionality to compute on tensors and do control-dependent operations. A script function annotation will construct a ScriptModule with a single method that implements that function, and that contains no parameters. Save an offline version of this module for use in a separate process. The saved module serializes all of the methods and parameters of this module. It can be loaded into the C++ API using or into the Python API with . To be able to save a module, it must not make any calls to native python functions. This means that all submodules must be subclasses of ScriptModules as well. All modules, no matter their device, are always loaded onto the CPU during loading. This is different from ’s semantics and may change in the future. All previously saved modules, no matter their device, are first loaded onto CPU, and then are moved to the devices they were saved from. If this fails (e.g. because the run time system doesn’t have certain devices), an exception is raised. However, storages can be dynamically remapped to an alternative set of devices using the argument. Comparing to , in this function is simplified, which only accepts a string (e.g., ‘cpu’, ‘cuda:0’), or torch.device (e.g., torch.device(‘cpu’))\n• None f – a file-like object (has to implement read, readline, tell, and seek), or a string containing a file name\n• None _extra_files – map from filename to content. The extra filenames given in the map would be loaded and their content would be stored in the provided map. # Load all tensors to the original device # Load all tensors onto CPU, using a device # Load all tensors onto CPU, using a string Trace a function and return an executable trace that will be optimized using just-in-time compilation. Tracing only correctly records functions and modules which are not data dependent (e.g., have conditionals on data in tensors) and do not have any untracked external dependencies (e.g., perform input/output or access global variables). If you trace such models, you may silently get incorrect results on subsequent invocations of the model. The tracer will try to emit warnings when doing something that may cause an incorrect trace to be produced.\n• None func (callable or torch.nn.Module) – a python function or torch.nn.Module that will be run with example_inputs. arguments and returns to func must be Tensors or (possibly nested) tuples that contain tensors.\n• None example_inputs (tuple) – a tuple of example inputs that will be passed to the function while tracing. The resulting trace can be run with inputs of different types and shapes assuming the traced operations support those types and shapes. example_inputs may also be a single Tensor in which case it is automatically wrapped in a tuple\n• None optimize (bool, optional) – whether or not to apply optimizations. Default: .\n• None check_trace (bool, optional) – check if the same inputs run through traced code produce the same outputs. Default: . You might want to disable this if, for example, your network contains non- deterministic ops or if you are sure that the network is correct despite a checker failure.\n• None check_inputs (list of tuples, optional) – A list of tuples of input arguments that should be used to check the trace against what is expected. Each tuple is equivalent to a seet of input arguments that would be specified in . For best results, pass in a set of checking inputs representative of the space of shapes and types of inputs you expect the network to see. If not specified, the original is used for checking\n• None check_tolerance (float, optional) – Floating-point comparison tolerance to use in the checker procedure. This can be used to relax the checker strictness in the event that results diverge numerically for a known reason, such as operator fusion. A object with a single method containing the traced code. When func is a , the returned will have the same set of sub-modules and parameters as func. In many cases either tracing or script is an easier approach for converting a model. We allow you to compose tracing and scripting to suit the particular requirements of a part of a model. Scripted functions can call traced ones. This is particularly useful when you need to use control-flow around a simple feed-forward model. For instance the beam search of a sequence to sequence model will typically be written in script but can call an encoder module generated using tracing. Traced functions can call script functions. This is useful when a small part of a model requires some control-flow even though most of the model is just a feed-forward network. Control-flow inside of a script function called by a traced function is preserved correctly: This composition also works for modules as well, where it can be used to generate a submodule using tracing that can be called from the methods of a script module: TorchScript is a subset of Python that can either be written directly (using the @script annotations) or generated automatically from Python code via tracing. When using tracing, code is automatically converted into this subset of Python by recording only the actual operators on tensors and simply executing and discarding the other surrounding Python code. When writing TorchScript directly using @script annotations, the programmer must only use the subset of Python supported in TorchScript. This section documents what is supported in TorchScript as if it were a language reference for a stand alone language. Any features of Python not mentioned in this reference are not part of TorchScript. As a subset of Python any valid TorchScript function is also a valid Python function. This makes it possible to remove the @script annotations and debug the function using standard Python tools like pdb. The reverse is not true: there are many valid python programs that are not valid TorchScript programs. Instead, TorchScript focuses specifically on the features of Python that are needed to represent neural network models in Torch. Setting the environment variable will disable all script and tracing annotations. If there is hard-to-debug error in one of your ScriptModules, you can use this flag to force everything to run using native Python. This allows the use of tools like to debug code. The largest difference between TorchScript and the full Python language is that TorchScript only support a small set of types that are needed to express neural net models. In particular TorchScript supports: A PyTorch tensor of any dtype, dimension, or backend. A list of which all members are type A value which is either None or type A dict with key type and value type . Only , , and are allowed as key types. Unlike Python, each variable in TorchScript function must have a single static type. This makes it easier to optimize TorchScript functions. # Type mismatch: r is set to type Tensor in the true branch # and type int in the false branch There are 2 scenarios in which you can annotate: By default, all parameters to a TorchScript function are assumed to be Tensor because this is the most common type used in modules. To specify that an argument to a TorchScript function is another type, it is possible to use MyPy-style type annotations using the types listed above: It is also possible to annotate types with Python 3 type annotations. In our examples, we use comment-based annotations to ensure Python 2 compatibility as well. A list by default is assumed to be and empty dicts . To instantiate an empty list or dict of other types, use . # This annotates the list to be a `List[Tuple[Tensor, Tensor]]` # This annotates the list to be a `Dict[int, Tensor]` TorchScript will refine the type of a variable of type Optional[T] when a comparison to None is made inside the conditional of an if statement. The compiler can reason about multiple None checks that are combined with AND, OR, or NOT. Refinement will also occur for else blocks of if statements that are not explicitly written. The expression must be emitted within the conditional; assigning a None check to a variable and using it in the conditional will not refine types. The following Python Expressions are supported See Variable Resolution for how variables are resolved. an empty list is assumed have type . The types of other list literals are derived from the type of the members. an empty dict is assumed have type . The types of other dict literals are derived from the type of the members. TorchScript currently does not support mutating tensors in place, so any tensor indexing can only appear on the right-hand size of an expression. Calls to methods of builtin types like tensor: When defining a Script method inside of a ScriptModule, the annotation is used. Inside of these methods it is possible to call other methods of this class or access methods on the submodules. Calling a submodule directly (e.g. ) is equivalent to calling its method (e.g. ) TorchScript supports the following types of statements: # short-hand for a = a + b, does not operate in-place on a Script currently does not support iterating over generic iterable objects like lists or tensors. Script currently does not support start or increment parameters to range. These will be added in a future version. for loops over tuples will unroll the loop, generating a body for each member of the tuple. The body must type-check correctly for each member. To use a module list inside a it must be marked constant by adding the name of the attribute to the list for the type. For loops over a ModuleList will unroll the body of the loop at compile time, with each member of the constant module list. TorchScript allows returns in the following circumstances:\n• None In an if-statement where <true> and <false> both return\n• None In an if-statement where <true> returns and <false> is empty (an early return) TorchScript supports a subset of Python’s variable resolution (i.e. scoping) rules. Local variables behave the same as in Python, except for the restriction that a variable must have the same type along all paths through a function. If a variable has a different type on different sides of an if statement, it is an error to use it after the end of the if statement. Similarly, a variable is not allowed to be used if it is only defined along some paths through the function. Non-local variables are resolved to Python values at compile time when the function is defined. These values are then converted into TorchScript values using the rules described in Use of Python Values. To make writing TorchScript more convenient, we allow script code to refer to Python values in the surrounding scope. For instance, any time there is a reference to , the TorchScript compiler is actually resolving it to the Python module when the function is declared. These Python values are not a first class part of TorchScript. Instead they are desugared at compile-time into the primitive types that TorchScript supports. This section describes the rules that are used when accessing Python values in TorchScript. They depend on the dynamic type of the python valued referenced. TorchScript can call python functions. This functionality is very useful when incrementally converting a model into script. The model can be moved function-by-function to script, leaving calls to Python functions in place. This way you can incrementally check the correctness of the model as you go. Attempting to call on a ScriptModule that contains calls to Python functions will fail. The intention is that this pathway is used for debugging and the calls removed or turned into script functions before saving. TorchScript can lookup attributes on modules. Builtin functions like are accessed this way. This allows TorchScript to call functions defined in other modules. TorchScript also provides a way to use constants that are defined in Python. These can be used to hard-code hyper-parameters into the function, or to define universal constants. There are two ways of specifying that a Python value should be treated as a constant.\n• None Values looked up as attributes of a module are assumed to be constant. Example:\n• None Attributes of a ScriptModule can be marked constant by listing them as a member of the property of the class:\n• None which can be used in a TorchScript for loop If you want to disable all JIT modes (tracing and scripting) so you can debug your program in raw Python, you can use the environment variable. can be used to globally disable the JIT by setting its value to . Given an example script: Debugging this script with PDB works except for when we invoke the @script function. We can globally disable JIT, so that we can call the @script function as a normal python function and not compile it. If the above script is called , we can invoke it like so: and we will be able to step into the @script function as a normal Python function. TorchScript uses a static single assignment (SSA) intermediate representation (IR) to represent computation. The instructions in this format consist of ATen (the C++ backend of PyTorch) operators and other primitive operators, including control flow operators for loops and conditionals. As an example: A with a single method will have an attribute , which you can use to inspect the IR representing the computation. If the ScriptModule has more than one method, you will need to access on the method itself and not the module. We can inspect the graph of a method named on a ScriptModule by accessing . The example script above produces the graph: Take the instruction for example. means we assign the output to a (unique) value named , and that value is of type, i.e. we do not know its concrete shape. is the operator (equivalent to ) and the input list specifies which values in scope should be passed as inputs. The schema for built-in functions like can be found at Builtin Functions. Notice that operators can also have associated , namely the and operators. In the graph print-out, these operators are formatted to reflect their equivalent source code forms to facilitate easy debugging. Graphs can be inspected as shown to confirm that the computation described by a is correct, in both automated and manual fashion, as described below. There are some edge cases that exist where the trace of a given Python function/module will not be representative of the underlying code. These cases can include:\n• None Tracing of control flow that is dependent on inputs (e.g. tensor shapes)\n• None Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment) Note that these cases may in fact be traceable in the future. One way to automatically catch many errors in traces is by using on the API. takes a list of tuples of inputs that will be used to re-trace the computation and verify the results. For example: Gives us the following diagnostic information:: This message indicates to us that the computation differed between when we first traced it and when we traced it with the . Indeed, the loop within the body of depends on the shape of the input , and thus when we try another with a different shape, the trace differs. In this case, data-dependent control flow like this can be captured using script instead: The tracer produces warnings for several problematic patterns in traced computation. As an example, take a trace of a function that contains an in-place assignment on a slice (a view) of a Tensor: Produces several warnings and a graph which simply returns the input: fill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe. x[0] = torch.rand(*x.shape[1:2]) fill_row_zero.py:6: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error: Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%) traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),)) graph(%0 : Float(3, 4)) { return (%0); } We can fix this by modifying the code to not use the in-place update, but rather build up the result tensor out-of-place with : TorchScript supports a subset of the builtin tensor and neural network functions that PyTorch provides. Most methods on Tensor as well as functions in the namespace, all functions in and all modules from are supported in TorchScript, excluding those in the table below. For unsupported modules, we suggest using . Q: I would like to train a model on GPU and do inference on CPU. What are the best practices? First convert your model from GPU to CPU and then save it, like so: # ... later, when using the model: This is recommended because the tracer may witness tensor creation on a specific device, so casting an already-loaded model may have unexpected effects. Casting the model before saving it ensures that the tracer has the correct device information.\n\ntorch.distributed.deprecated is the older version of torch.distributed and currently deprecated. It will be removed soon. Please use and refer the doc for torch.distributed, which is the latest distributed communication package for PyTorch torch.distributed.deprecated provides an MPI-like interface for exchanging tensor data across multi-machine networks. It supports a few different backends and initialization methods. Currently torch.distributed.deprecated supports four backends, each with different capabilities. The table below shows which functions are available for use with CPU / CUDA tensors. MPI supports cuda only if the implementation used to build PyTorch supports it. The package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines. The class builds on this functionality to provide synchronous distributed training as a wrapper around any PyTorch model. This differs from the kinds of parallelism provided by Multiprocessing package - torch.multiprocessing and in that it supports multiple network-connected machines and in that the user must explicitly launch a separate copy of the main training script for each process. In the single-machine synchronous case, or the wrapper may still have advantages over other approaches to data-parallelism, including :\n• None Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes.\n• None Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and “GIL-thrashing” that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components. The package needs to be initialized using the function before calling any other methods. This blocks until all processes have joined.\n• None backend (str) – Name of the backend to use. Depending on build-time configuration valid values include: , , and .\n• None init_method (str, optional) – URL specifying how to initialize the package.\n• None world_size (int, optional) – Number of processes participating in the job.\n• None group_name (str, optional) – Group name. See description of init methods. To enable , PyTorch needs to built from source on a system that supports MPI. If you want to use Open MPI with CUDA-aware support, please use Open MPI major version 2 and above. This method initializes CUDA context. Therefore, if multiple processes run on a single machine but use different GPUs, make sure to use before this method to avoid unnecessarily creating context on the first visible device. Rank is a unique identifier assigned to each process within a distributed group. They are always consecutive integers ranging from to (inclusive). Returns the number of processes in the distributed group. Currently three initialization methods are supported: There are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired . The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks. Alternatively, the address has to be a valid IP multicast address, in which case ranks can be assigned automatically. Multicast initialization also supports a argument, which allows you to use the same address for multiple jobs, as long as they use different group names. # Use address of one of the machines # or a multicast address - rank will be assigned automatically if unspecified Another initialization method makes use of a file system that is shared and visible from all machines in a group, along with a desired . The URL should start with and contain a path to a non-existent file (in an existing directory) on a shared file system. This initialization method also supports a argument, which allows you to use the same shared file path for multiple jobs, as long as they use different group names. This method assumes that the file system supports locking using - most local systems and NFS support it. # Rank will be assigned automatically if unspecified This method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:\n• None - required; has to be a free port on machine with rank 0\n• None - required (except for rank 0); address of rank 0 node\n• None - required; can be set either here, or in a call to init function\n• None - required; can be set either here, or in a call to init function The machine with rank 0 will be used to set up all connections. This is the default method, meaning that does not have to be specified (or can be ). By default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns). This function requires that all processes in the main group (i.e., all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes. A handle of distributed group that can be given to collective calls.\n• None src (int, optional) – Source rank. Will receive from any process if unspecified. and return distributed request objects when used. In general, the type of this object is unspecified as they should never be created manually, but they are guaranteed to support two methods:\n• None - returns True if the operation has finished\n• None - will block the process until the operation is finished. is guaranteed to return True once it returns. When using the MPI backend, and support non-overtaking, which has some guarantees on supporting message order. For more detail, see http://mpi-forum.org/docs/mpi-2.2/mpi22-report/node54.htm#Node54 Broadcasts the tensor to the whole group. must have the same number of elements in all processes participating in the collective.\n• None tensor (Tensor) – Data to be sent if is the rank of current process, and tensor to be used to save received data otherwise. Reduces the tensor data across all machines in such a way that all get the final result. After the call will be bitwise identical in all processes.\n• None tensor (Tensor) – Input and output of the collective. The function operates in-place.\n• None op (optional) – One of the values from enum. Specifies an operation used for element-wise reductions. Reduces the tensor data across all machines. Only the process with rank is going to receive the final result.\n• None tensor (Tensor) – Input and output of the collective. The function operates in-place.\n• None op (optional) – One of the values from enum. Specifies an operation used for element-wise reductions. Gathers tensors from the whole group in a list.\n• None tensor_list (list[Tensor]) – Output list. It should contain correctly-sized tensors to be used for output of the collective.\n• None tensor (Tensor) – Tensor to be broadcast from current process.\n• None dst (int) – Destination rank. Required in all processes except the one that is receiveing the data.\n• None gather_list (list[Tensor]) – List of appropriately-sized tensors to use for received data. Required only in the receiving process. Scatters a list of tensors to all processes in a group. Each process will receive exactly one tensor and store its data in the argument.\n• None src (int) – Source rank. Required in all processes except the one that is sending the data.\n• None scatter_list (list[Tensor]) – List of tensors to scatter. Required only in the process that is sending the data. This collective blocks processes until the whole group enters this function. If you have more than one GPU on each node, when using the NCCL backend, and support distributed collective operations among multiple GPUs within each node. These functions can potentially improve the overall distributed training performance and be easily used by passing a list of tensors. Each Tensor in the passed tensor list needs to be on a separate GPU device of the host where the function is called. Note that the length of the tensor list needs to be identical among all the distributed processes. Also note that currently the multi-GPU collective functions are only supported by the NCCL backend. For example, if the system we use for distributed training has 2 nodes, each of which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would like to all-reduce. The following code can serve as a reference: After the call, all 16 tensors on the two nodes will have the all-reduced value of 16 Broadcasts the tensor to the whole group with multiple GPU tensors per node. must have the same number of elements in all the GPUs from all processes participating in the collective. each tensor in the list must be on a different GPU. Only NCCL backend is currently supported. should only contain GPU tensors.\n• None tensor_list (List[Tensor]) – Tensors that participate in the collective operation. if is the rank, then the first element of ( ) will be broadcasted to all other tensors (on different GPUs) in the src process and all tensors in of other non-src processes. You also need to make sure that is the same for all the distributed processes calling this function. Reduces the tensor data across all machines in such a way that all get the final result. This function reduces a number of tensors on every node, while each tensor resides on a different GPU. Therefore, the input tensor in the tensor list needs to be GPU tensors. Also, each tensor in the tensor list needs to reside on a different GPU. After the call, all tensors in will be bitwise identical in all processes. Only NCCL backend is currently supported. should only contain GPU tensors.\n• None tensor_list (List[Tensor]) – List of input and output tensors of the collective. The function operates in-place and requires that each tensor to be a GPU tensor on different GPUs. You also need to make sure that is the same for all the distributed processes calling this function.\n• None op (optional) – One of the values from enum. Specifies an operation used for element-wise reductions. Reduces the tensor data on multiple GPUs across all machines. Each tensor in should reside on a separate GPU. Only the GPU of on the process with rank is going to receive the final result. Only NCCL backend is currently supported. should only contain GPU tensors.\n• None tensor_list (List[Tensor]) – Input and output GPU tensors of the collective. The function operates in-place. You also need to make sure that is the same for all the distributed processes calling this function.\n• None op (optional) – One of the values from enum. Specifies an operation used for element-wise reductions. Gathers tensors from the whole group in a list. Each tensor in should reside on a separate GPU. Only NCCL backend is currently supported. and should only contain GPU tensors.\n• None output_tensor_lists (List[List[Tensor]]) – Output lists. It should contain correctly-sized tensors on each GPU to be used for output of the collective. e.g. contains the all_gather result that resides on the GPU of . Note that each element of has the size of , since the function all gathers the result from every single GPU in the group. To interpret each element of , note that of rank k will be appear in Also note that , and the size of each element in (each element is a list, therefore ) need to be the same for all the distributed processes calling this function.\n• None input_tensor_list (List[Tensor]) – List of tensors (on different GPUs) to be broadcast from current process. Note that needs to be the same for all the distributed processes calling this function. The package also provides a launch utility in . is a module that spawns up multiple distributed training processes on each of the training nodes. The utility can be used for single-node distributed training, in which one or more processes per node will be spawned. The utility can be used for either CPU training or GPU training. If the utility is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. It can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be benefitial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth. In both cases of single-node distributed training or multi-node distributed training, this utility will launch the given number of processes per node ( ). If used for GPU training, this number needs to be less or euqal to the number of GPUs on the current system ( ), and each process will be operating on a single GPU from GPU 0 to GPU (nproc_per_node - 1). YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other and all other arguments of your training script) and all other arguments of your training script)\n• None To look up what optional arguments this module offers: 1. This utilty and multi-process distributed (single-node or multi-node) GPU training currently only achieves the best performance using the NCCL distributed backend. Thus NCCL backend is the recommended backend to use for GPU training. 2. In your training program, you must parse the command-line argument: , which will be provided by this module. If your training program uses GPUs, you should ensure that your code only runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by: Set your device to local rank using either 3. In your training program, you are supposed to call the following function at the beginning to start the distributed backend. You need to make sure that the init_method uses , which is the only supported by this module. 4. In your training program, you can either use regular distributed functions or use module. If your training program uses GPUs for training and you would like to use module, here is how to configure it. Please ensure that argument is set to be the only GPU device id that your code will be operating on. This is generally the local rank of the process. In other words, the needs to be , and needs to be in order to use this utility 5. Another way to pass to the subprocesses via environment variable . This behavior is enabled when you launch the script with . You must adjust the subprocess example above to replace with ; the launcher will not pass when you specify this flag. is NOT globally unique: it is only unique per process on a machine. Thus, don’t use it to decide if you should, e.g., write to a networked filesystem. See https://github.com/pytorch/pytorch/issues/12042 for an example of how things can go wrong if you don’t do this correctly."
    },
    {
        "link": "https://myscale.com/blog/mastering-pytorch-mseloss-calculating-mean-squared-error",
        "document": "In the realm of machine learning (opens new window), PyTorch MSELoss stands out as a fundamental component for evaluating model performance. This loss function (opens new window) calculates the mean squared error between predicted and actual values, providing insights into the accuracy of regression models (opens new window).\n\nThe significance of PyTorch MSELoss lies in its ability to quantify prediction errors effectively. By squaring the differences between predicted and true values, it emphasizes larger errors, crucial for refining model training and enhancing predictive capabilities.\n\n# The Role of MSELoss in Machine Learning\n\nResearch studies, such as those conducted by Stanford University and Google AI Department, have highlighted the pivotal role of MSE loss in machine learning tasks. These findings underscore the effectiveness of PyTorch MSELoss in regression scenarios, where precise error measurement is paramount.\n\nIn practical settings, PyTorch MSELoss finds applications across various domains like finance, healthcare, and image processing. Its versatility and accuracy make it a go-to choice for assessing model performance and driving data-driven decision-making processes.\n\n# Understanding the Basics of Mean Squared Error\n\nWhen delving into the realm of machine learning, understanding the intricacies of PyTorch MSELoss becomes paramount. The mean squared error (MSE) (opens new window) serves as a pivotal metric for assessing model performance by quantifying the average squared differences between predicted and actual values. This calculation ensures that outlier predictions with significant errors (opens new window) are duly noted, thanks to the emphasis placed on larger errors through squaring.\n\nIn comparison to other loss functions, MSE stands out for its unique approach in measuring prediction accuracy. By aiming to minimize the MSE loss (opens new window), machine learning models strive to bring their predictions as close as possible to the ground truth values. This goal underscores the importance of precise error measurement in refining model training and enhancing overall predictive capabilities.\n\nDiving deeper into how PyTorch handles MSE calculations sheds light on its inner workings. The framework computes MSE by averaging the squared differences between predicted and target values across all elements in the input tensor. This meticulous process ensures a comprehensive evaluation of prediction accuracy, essential for fine-tuning models and optimizing performance.\n• None Robustness: Despite its sensitivity to outliers (opens new window), MSE offers robustness in evaluating model predictions.\n• None Interpretability: Research from the University of California, Berkeley highlights MSE's interpretability compared to other (opens new window) loss functions.\n• None Effectiveness: Studies from Stanford University emphasize MSE's effectiveness in classification tasks, showcasing its versatility across different machine learning scenarios.\n\nIn essence, mastering PyTorch MSELoss involves grasping not only its mathematical underpinnings but also its practical implications in enhancing model accuracy and performance.\n\nNow that we have explored the theoretical foundations of PyTorch MSELoss, it's time to delve into its practical implementation within your projects. Understanding how to effectively utilize this loss function is crucial for optimizing model performance and enhancing predictive accuracy.\n\nBefore integrating PyTorch MSELoss into your projects, ensure that you have a stable development environment set up. This includes installing the necessary libraries, such as PyTorch, and configuring your IDE for seamless integration. By creating a conducive workspace, you pave the way for efficient implementation of the MSE loss function.\n\nTo begin using PyTorch MSELoss, follow these fundamental steps:\n• None Data Preparation: Organize your dataset and preprocess it according to the requirements of your machine learning model.\n• None Model Definition: Construct your neural network architecture (opens new window) using PyTorch's flexible framework.\n• None Loss Function Integration: Incorporate MSELoss into your model training pipeline to evaluate prediction errors effectively.\n• None Evaluation and Fine-Tuning: Assess model performance based on MSE metrics and fine-tune hyperparameters for optimal results.\n\nOne advanced technique for optimizing MSE calculations involves leveraging mini-batch processing. By dividing your dataset into smaller batches during training, you can enhance computational efficiency and improve convergence speed. Additionally, consider implementing learning rate schedules to fine-tune the training process further.\n\nIn the realm of machine learning, encountering challenges is inevitable. When working with PyTorch MSELoss, common issues such as overfitting (opens new window) or vanishing gradients may arise. To address these issues effectively, focus on regularization techniques (opens new window) like dropout or batch normalization to prevent overfitting and ensure stable gradient flow throughout the network.\n\nBy mastering the implementation of PyTorch MSELoss in your projects and incorporating advanced optimization techniques, you can elevate the performance of your machine learning models significantly.\n\nWhen aiming for optimal PyTorch MSELoss performance, incorporating best practices can significantly enhance the accuracy and efficiency of your machine learning models.\n\nPrior to applying PyTorch MSELoss in your projects, meticulous data preprocessing plays a vital role in ensuring accurate results. Consider the following tips:\n• None Normalization: Scale your input features to a standard range to prevent bias in model training.\n• None Outlier Handling: Identify and address outliers in your dataset to avoid skewed predictions.\n• None Feature Engineering (opens new window): Enhance model performance by selecting relevant features and transforming data appropriately.\n\nOptimizing hyperparameters is crucial for fine-tuning the performance of PyTorch MSELoss. Follow these guidelines:\n• None Cross-Validation: Validate model performance across different parameter settings to prevent overfitting.\n• None Regularization Techniques: Implement L1 or L2 regularization to control model complexity and improve generalization.\n\nIn the realm of machine learning, certain pitfalls can hinder PyTorch MSELoss performance. Be mindful of these common mistakes:\n• None Overfitting: Regularize your model and validate on unseen data to mitigate overfitting risks.\n\n# How to Improve Your MSELoss Performance\n\nTo enhance the performance of PyTorch MSELoss, focus on continuous learning and experimentation. Embrace iterative improvements, seek feedback from peers, and stay updated with the latest advancements in machine learning techniques.\n\nIn wrapping up our exploration of PyTorch MSELoss, it becomes evident that mastering this essential component is key to enhancing model performance and accuracy in machine learning tasks.\n• None Understanding the basics of Mean Squared Error is fundamental for leveraging MSELoss effectively.\n• None Implementing advanced techniques like mini-batch processing can optimize MSE calculations and improve model convergence.\n• None Embrace experimentation and continuous learning in your machine learning projects.\n• None Explore different optimization strategies and fine-tuning techniques to elevate the performance of your models.\n• None Don't shy away from challenges; each obstacle presents an opportunity for growth and improvement.\n\nAs you venture further into the realm of machine learning with PyTorch MSELoss as your ally, remember that innovation thrives on experimentation. Keep pushing boundaries, testing new approaches, and honing your skills to unlock the full potential of this powerful tool."
    },
    {
        "link": "https://discuss.pytorch.org/t/calculating-mse-between-2-tensors/162517",
        "document": "The most likely explanation is that the input and output images\n\n of your deblurring network have different normalizations. (This is\n\n certainly possible; your network outputs whatever you train it to\n\n output, so if you train it to output an image whose normalization\n\n differs from that of the input, it will.)\n\nFor example, your network might take in an unnormalized grayscale\n\n image whose pixel values range from 0 to 255, normalize it internally,\n\n and then output a normalized, deblurred image whose pixel values\n\n range from -1.0 to 1.0.\n\nPrint out the , , and of your three images (original,\n\n blurred, and deblurred) to see if inconsistent normalization explains\n\n your issue.\n\nIf this is the cause, you might consider retraining your network to produce\n\n a deblurred image with the desired normalization (or, alternatively, accept\n\n an input image with the desired normalization).\n\nBut, as a work-around, you could try just normalizing your original and\n\n deblurred images consistently before computing the mean-squared-error\n\n between them."
    },
    {
        "link": "https://neptune.ai/blog/pytorch-loss-functions",
        "document": "Your neural networks can do a lot of different tasks. Whether it’s classifying data, like grouping pictures of animals into cats and dogs, regression tasks, like predicting monthly revenues, or anything else. Every task has a different output and needs a different type of loss function.\n\nThe way you configure your loss functions can make or break the performance of your algorithm. By correctly configuring the loss function, you can make sure your model will work how you want it to.\n\nLuckily for us, there are loss functions we can use to make the most of machine learning tasks.\n\nIn this article, we’ll talk about popular loss functions in PyTorch, and about building custom loss functions. Once you’re done reading, you should know which one to choose for your project.\n\nWhat are the loss functions?\n\nBefore we jump into PyTorch specifics, let’s refresh our memory of what loss functions are.\n\nLoss functions are used to gauge the error between the prediction output and the provided target value. A loss function tells us how far the algorithm model is from realizing the expected outcome. The word ‘loss’ means the penalty that the model gets for failing to yield the desired results.\n\nFor example, a loss function (let’s call it J) can take the following two parameters:\n\nThis function will determine your model’s performance by comparing its predicted output with the expected output. If the deviation between y_pred and y is very large, the loss value will be very high.\n\nIf the deviation is small or the values are nearly identical, it’ll output a very low loss value. Therefore, you need to use a loss function that can penalize a model properly when it is training on the provided dataset.\n\nLoss functions change based on the problem statement that your algorithm is trying to solve.\n\nPyTorch’s torch.nn module has multiple standard loss functions that you can use in your project.\n\nTo add them, you need to first import the libraries:\n\nNext, define the type of loss you want to use. Here’s how to define the mean absolute error loss function:\n\nAfter adding a function, you can use it to accomplish your specific task.\n\nWhich loss functions are available in PyTorch?\n\nBroadly speaking, loss functions in PyTorch are divided into two main categories: regression losses and classification losses.\n\nRegression loss functions are used when the model is predicting a continuous value, like the age of a person.\n\nClassification loss functions are used when the model is predicting a discrete value, such as whether an email is spam or not.\n\nRanking loss functions are used when the model is predicting the relative distances between inputs, such as ranking products according to their relevance on an e-commerce search page.\n\nNow we’ll explore the different types of loss functions in PyTorch, and how to use them:\n\nThe Mean Absolute Error (MAE), also called L1 Loss, computes the average of the sum of absolute differences between actual values and predicted values.\n\nIt checks the size of errors in a set of predicted values, without caring about their positive or negative direction. If the absolute values of the errors are not used, then negative values could cancel out the positive values.\n\nThe Pytorch L1 Loss is expressed as:\n\nx represents the actual value and y the predicted value.\n\nWhen could it be used?\n• Regression problems, especially when the distribution of the target variable has outliers, such as small or big values that are a great distance from the mean value. It is considered to be more robust to outliers.\n\nThe Mean Squared Error (MSE), also called L2 Loss, computes the average of the squared differences between actual values and predicted values.\n\nPytorch MSE Loss always outputs a positive result, regardless of the sign of actual and predicted values. To enhance the accuracy of the model, you should try to reduce the L2 Loss—a perfect value is 0.0.\n\nThe squaring implies that larger mistakes produce even larger errors than smaller ones. If the classifier is off by 100, the error is 10,000. If it’s off by 0.1, the error is 0.01. This punishes the model for making big mistakes and encourages small mistakes.\n\nThe Pytorch L2 Loss is expressed as:\n\nx represents the actual value and y the predicted value.\n\nWhen could it be used?\n• MSE is the default loss function for most Pytorch regression problems.\n\nThe Negative Log-Likelihood Loss function (NLL) is applied only on models with the softmax function as an output activation layer. Softmax refers to an activation function that calculates the normalized exponential function of every unit in the layer.\n\nThe Softmax function is expressed as:\n\nThe function takes an input vector of size N, and then modifies the values such that every one of them falls between 0 and 1. Furthermore, it normalizes the output such that the sum of the N values of the vector equals to 1.\n\nNLL uses a negative connotation since the probabilities (or likelihoods) vary between zero and one, and the logarithms of values in this range are negative. In the end, the loss value becomes positive.\n\nIn NLL, minimizing the loss function assists us get a better output. The negative log likelihood is retrieved from approximating the maximum likelihood estimation (MLE). This means that we try to maximize the model’s log likelihood, and as a result, minimize the NLL.\n\nIn NLL, the model is punished for making the correct prediction with smaller probabilities and encouraged for making the prediction with higher probabilities. The logarithm does the punishment.\n\nNLL does not only care about the prediction being correct but also about the model being certain about the prediction with a high score.\n\nThe Pytorch NLL Loss is expressed as:\n\nwhere x is the input, y is the target, w is the weight, and N is the batch size.\n\nWhen could it be used?\n\nThis loss function computes the difference between two probability distributions for a provided set of occurrences or random variables.\n\nIt is used to work out a score that summarizes the average difference between the predicted values and the actual values. To enhance the accuracy of the model, you should try to minimize the score—the cross-entropy score is between 0 and 1, and a perfect value is 0.\n\nOther loss functions, like the squared loss, punish incorrect predictions. Cross-Entropy penalizes greatly for being very confident and wrong.\n\nUnlike the Negative Log-Likelihood Loss, which doesn’t punish based on prediction confidence, Cross-Entropy punishes incorrect but confident predictions, as well as correct but less confident predictions.\n\nThe Cross-Entropy function has a wide range of variants, of which the most common type is the Binary Cross-Entropy (BCE). The BCE Loss is mainly used for binary classification models; that is, models having only 2 classes.\n\nThe Pytorch Cross-Entropy Loss is expressed as:\n\nWhere x is the input, y is the target, w is the weight, C is the number of classes, and N spans the mini-batch dimension.\n\nWhen could it be used?\n• Binary classification tasks, for which it’s the default loss function in Pytorch.\n• Creating confident models—the prediction will be accurate and with a higher probability.\n\nThe Hinge Embedding Loss is used for computing the loss when there is an input tensor, x, and a labels tensor, y. Target values are between {1, -1}, which makes it good for binary classification tasks.\n\nWith the Hinge Loss function, you can give more error whenever a difference exists in the sign between the actual class values and the predicted class values. This motivates examples to have the right sign.\n\nThe Hinge Embedding Loss is expressed as:\n\nWhen could it be used?\n• Classification problems, especially when determining if two inputs are dissimilar or similar.\n\nThe Margin Ranking Loss computes a criterion to predict the relative distances between inputs. This is different from other loss functions, like MSE or Cross-Entropy, which learn to predict directly from a given set of inputs.\n\nWith the Margin Ranking Loss, you can calculate the loss provided there are inputs x1, x2, as well as a label tensor, y (containing 1 or -1).\n\nWhen y == 1, the first input will be assumed as a larger value. It’ll be ranked higher than the second input. If y == -1, the second input will be ranked higher.\n\nThe Pytorch Margin Ranking Loss is expressed as:\n\nWhen could it be used?\n\nThe Triplet Margin Loss computes a criterion for measuring the triplet loss in models. With this loss function, you can calculate the loss provided there are input tensors, x1, x2, x3, as well as margin with a value greater than zero.\n\nThe Pytorch Triplet Margin Loss is expressed as:\n\nWhen could it be used?\n• It is used in content-based retrieval problems\n\nThe Kullback-Leibler Divergence, shortened to KL Divergence, computes the difference between two probability distributions.\n\nWith this loss function, you can compute the amount of lost information (expressed in bits) in case the predicted probability distribution is utilized to estimate the expected target probability distribution.\n\nIts output tells you the proximity of two probability distributions. If the predicted probability distribution is very far from the true probability distribution, it’ll lead to a big loss. If the value of KL Divergence is zero, it implies that the probability distributions are the same.\n\nKL Divergence behaves just like Cross-Entropy Loss, with a key difference in how they handle predicted and actual probability. Cross-Entropy punishes the model according to the confidence of predictions, and KL Divergence doesn’t. KL Divergence only assesses how the probability distribution prediction is different from the distribution of ground truth.\n\nThe KL Divergence Loss is expressed as:\n\nx represents the true label’s probability and y represents the predicted label’s probability.\n\nWhen could it be used?\n• If you want to make sure that the distribution of predictions is similar to that of training data\n\nHow to create a custom loss function in PyTorch?\n\nPyTorch lets you create your own custom loss functions to implement in your projects.\n\nHere’s how you can create your own simple Cross-Entropy Loss function.\n\nYou can also create other advanced PyTorch custom loss functions.\n\nLet’s modify the Dice coefficient, which computes the similarity between two samples, to act as a loss function for binary classification problems:\n\nIt is quite obvious that while training a model, one needs to keep an eye on the loss function values to track the model’s performance. As the loss value keeps decreasing, the model keeps getting better. There are a number of ways that we can do this. Let’s take a look at them.\n\nFor this, we will be training a simple Neural Network created in PyTorch which will perform classification on the famous Iris dataset.\n\nMaking the required imports for getting the dataset.\n\nScaling the dataset to have mean=0 and variance=1, gives quick model convergence.\n\nSplitting the dataset into train and test in an 80-20 ratio.\n\nMaking the necessary imports for our Neural Network and its training.\n\nDefining functions for getting accuracy and training the network.\n\nAbove, we have used print statements in the train_network function to monitor the loss as well as accuracy. Let’s see this in action:\n\nWe get an output like this:\n\nSince we’ve stored the intermediate values in lists, we can also plot the metrics using Matplotlib:\n\nThis will give us a graph that helps us analyze the correlation between loss and accuracy:\n\nThis method gets the job done. But if we train several model versions with different parameters, or have to analyze the model’s performance over time, we need a more capable experiment tracking solution.\n\nA simpler way to monitor your metrics would be to log them in a service like neptune.ai and focus on more important tasks, such as building and training the model.\n\nTo get started with Neptune, we need to install a couple of libraries:\n\nneptune is a Python SDK to authorize communication between your scripts and Neptune. We will need python-dotenv for managing environment variables, such as Neptune project name and your API token.\n\nFirst, you need to retrieve those variables from your Neptune account:\n\nHere’s how it looks in the UI.\n\n3. Copy your API key and the project name which is in the format of workspace-name/project-name.\n\n4. Return to your code editor and create a file named .env in your working directory:\n\n5. Paste the following contents into the file:\n\nNext, we need to initialize a new Neptune run using the init_run() method. The method requires your API token and the project name, which we retrieve using the os and python-dotenv libraries:\n\nrun is an instance of a Run object which we can use to log any metadata related to our ML experiments including:\n\nThe syntax to log metadata is very intuitive as the Run object can be thought of as a special dictionary (don’t run the below snippet just yet):\n\nIf you need them later on, you can retrieve the logged details using a similar syntax:\n\nIn the next sections, we will use this pattern of code to capture our model’s training process to Neptune.\n\nTo log the loss of our model, we need to add a couple of lines to the train_network function (notice the three lines where we use the run object):\n\nLet’s rerun our model training and inspect the data that ends up in our Neptune project:\n\nUsing the PyTorch integration for advanced logging\n\nFor more sophisticated logging features such as automated capture of model parameters, logging frequency configuration, and model checkpointing, you can use neptune_pytorch library:\n\nThe NeptuneLogger class requires both the run and model objects to enable logging. Then, it can automatically capture model parameters and gradients at a frequency specified by log_freq.\n\nUsing the neptune_callback object requires us to change the lines of code where the run object is used. The PyTorch integration collects all the data under a specific key of the run object defined by neptune_callback.base_namespace, so we replace run[‘key’] by run[neptune_callback.base_namespace][‘key’]. For example:\n\nWith those lines changed, we can retrain the model:\n\nYou can view the result in the Neptune app.\n\nTo stop the connection to Neptune and sync all data, call the stop() method:\n\nUsing the `neptune_pytorch` integration is the recommended method for logging PyTorch models. It gives you finer control over metadata generated during training and allows you to log more challenging artifacts such as model checkpoints and predictions in a formatted syntax.\n\nThis article covered the most common loss functions in machine learning and how to use them in PyTorch. Choosing a loss function depends on the problem type like regression, classification or ranking. If none of the functions in today’s list don’t meet your requirements, PyTorch allows creating custom loss functions as well.\n\nLoss functions are critical in informing you about the performance of your model. Therefore, you will spend a lot of time monitoring the loss and changing your model training strategy accordingly. And in our view, the best way to monitor loss is by using an experiment tracking tool such as Neptune."
    },
    {
        "link": "https://codingnomads.com/deep-learning-regression-mean-squared-error",
        "document": "You are probably familiar with regression tasks from the DSML course.\n\nIf you are solving a regression problem, you are trying to predict a continuous output between $`-\\inf`$ to $`\\inf`$.\n\nBack to the Mean Squared Error (MSE)\n\nIn deep learning, mean squared error (MSE) is the loss function generally used for regression tasks. has a function that you've probably used to evaluate a model in the past. In that case, you were probably using it to evaluate your model; in deep learning, you will use it to train the model.\n\nIn machine learning, a loss function measures how far off your model's predictions are from the actual values. If your model's predictions are way off, the loss function will output a high number. If the predictions are quite close to the actual values, it'll output a lower number. The goal of training a machine learning model is to find the model parameters that minimize this loss function.\n\nWhen you're trying to predict a continuous value, like temperature, you're dealing with a regression task. A common loss function used for regression tasks is the Mean Squared Error (MSE).\n\nThe MSE takes in two inputs: the actual target values ($`y`$) and your predictions ($`\\widehat{y}`$, pronounced y-hat). It calculates the average of the squared differences between the predicted and actual values. The formula for MSE is:\n\nIn Python, you can implement this as follows:\n\nIt's time to create a mock dataset to understand how MSE works. You'll generate some random outputs and targets and calculate the MSE between them. The function below creates two tensors:\n• The value mocks what a neural network would return. That's your\n• The are what you're trying to predict. That's your .\n\nWith these two values, you will calculate the mean squared error of these outputs.\n\nFirst, calculate the error. This is generally done by taking the actual values ( ) and subtracting the predicted values ( ).\n\nNext, you square each value in the tensor to get the squared error.\n\nTake the Mean\n\nFinally, you can take the mean of the tensor. This is the mean squared error.\n\nIt's time to codify that in a function and check that you get the same output.\n\nhas multiple implementations of MSE. The first is in the module, demonstrated below.\n\nIt's time to take a look at how mean squared error behaves over different errors. You can see that as the absolute value of the error grows larger, the mean squared error gets larger pretty quickly! This is good because it heavily penalizes large errors.\n\nOne important thing to note about MSE is that it heavily penalizes large errors. As the absolute value of the error grows larger, the mean squared error gets larger pretty quickly. This is good because it encourages your model to avoid large errors.\n\nHowever, MSE does not work well when your regression targets are small (i.e., between -1 and 1). Why? If your error (y - y_hat) is between -1 and 1, squaring the error actually makes it smaller. If this is the case, you may want to consider log-transforming your targets before training.\n\nIn this lesson, you've learned about the Mean Squared Error loss function, a common loss function for regression tasks. You've seen how to implement it yourself and how to use the built-in functions in PyTorch. You've also explored the relationship between the error and mean squared error and discussed when to use and when not to use MSE."
    },
    {
        "link": "https://stackoverflow.com/questions/36964457/how-is-mse-defined-for-image-comparison",
        "document": "There is no determinant involved when calculating MSE. MSE stands for Mean Squared Error, and it is simply a sum over squares of the differences per each single pixel in your matrix. In other words - cost is model agnostic, MSE is defined in exactly the same way whether you use conv-autoencoder, simple autoencoder or simple MLP."
    },
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html",
        "document": "Tensor interpolated to either the given or the given\n\nThe algorithm used for interpolation is determined by .\n\nCurrently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.\n\nThe input dimensions are interpreted in the form: .\n\nThe modes available for resizing are: , (3D-only), , (4D-only), (5D-only), ,\n• None size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) – output spatial size.\n• None scale_factor (float or Tuple[float]) – multiplier for spatial size. If is a tuple, its length has to match the number of spatial dimensions; .\n• None align_corners (bool, optional) – Geometrically, we consider the pixels of the input and output as squares rather than points. If set to , the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to , the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when is kept the same. This only has an effect when is , , or . Default:\n• None recompute_scale_factor (bool, optional) – recompute the scale_factor for use in the interpolation calculation. If is , then must be passed in and is used to compute the output . The computed output will be used to infer new scales for the interpolation. Note that when is floating-point, it may differ from the recomputed due to rounding and precision issues. If is , then or will be used directly for interpolation. Default: .\n• None antialias (bool, optional) – flag to apply anti-aliasing. Default: . Using anti-alias option together with , interpolation result would match Pillow result for downsampling operation. Supported modes: , .\n\nWith , it’s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call if you want to reduce the overshoot when displaying the image.\n\nThe gradients for the dtype on CUDA may be inaccurate in the upsample operation when using modes . For more details, please refer to the discussion in issue#104157."
    },
    {
        "link": "https://stackoverflow.com/questions/72769563/shifting-an-image-with-bilinear-interpolation-in-pytorch",
        "document": "I believe you can achieve this by applying grid sampling on your original input and using a grid to guide the sampling process. If you take a coordinate grid of your image and sample using that the resulting image will be equal to the original image. However you can apply a shift on this grid and therefore sample with the given shift. Grid sampling works with floating-point grids of course, which means you can apply an arbitrary non-round shift to your image and choose a sampling mode (bilinear is the default).\n\nThis can be implemented out of the box with . Given an image tensor , we first construct a pixel grid of that image using . Keep in mind the grid used by the sampler must be normalized to . Therefore pixel should be mapped to , pixel mapped to , and the center pixel will end up at around .\n\nUse two with a -normalization followed by a remapping to :\n\nSo the resulting grid has a shape of which will be the dimensions of the output image produced by the sampling process.\n\nSince we are not working with batched elements, we need to unsqueeze singleton dimensions on both and . Then we can apply :\n\nFollowing this you can apply your arbitrary , shift and even easily use this to batches of images and shifts. The way you would define your sampling is by defining a shifted grid:\n\nWhere mu_x and mu_y are the values in pixels (floating point) with wish which the image is shifted on the horizontal and vertical axes respectively. To acquire the sampled image, apply F.grid_sampling on a grid made up of and :"
    },
    {
        "link": "https://stackoverflow.com/tags/bilinear-interpolation",
        "document": ""
    },
    {
        "link": "https://pytorch.org/vision/main/transforms.html",
        "document": "Torchvision supports common computer vision transformations in the and modules. Transforms can be used to transform or augment data for training or inference of different tasks (image classification, detection, segmentation, video classification).\n\nTransforms are typically passed as the or argument to the Datasets.\n\nMost transformations accept both PIL images and tensor inputs. Both CPU and CUDA tensors are supported. The result of both backends (PIL or Tensors) should be very close. In general, we recommend relying on the tensor backend for performance. The conversion transforms may be used to convert to and from PIL images, or for converting dtypes and ranges. Tensor image are expected to be of shape , where is the number of channels, and and refer to height and width. Most transforms support batched tensor input. A batch of Tensor images is a tensor of shape , where is a number of images in the batch. The v2 transforms generally accept an arbitrary number of leading dimensions and can handle batched images or batched videos. The expected range of the values of a tensor image is implicitly defined by the tensor dtype. Tensor images with a float dtype are expected to have values in . Tensor images with an integer dtype are expected to have values in where is the largest value that can be represented in that dtype. Typically, images of dtype are expected to have values in . Use to convert both the dtype and range of the inputs.\n\nV1 or V2? Which one should I use?¶ TL;DR We recommending using the transforms instead of those in . They’re faster and they can do more things. Just change the import and you should be good to go. Moving forward, new features and improvements will only be considered for the v2 transforms. In Torchvision 0.15 (March 2023), we released a new set of transforms available in the namespace. These transforms have a lot of advantages compared to the v1 ones (in ):\n• None They can transform images but also bounding boxes, masks, or videos. This provides support for tasks beyond image classification: detection, segmentation, video classification, etc. See Getting started with transforms v2 and Transforms v2: End-to-end object detection/segmentation example.\n• None They support more transforms like and . See How to use CutMix and MixUp.\n• None Future improvements and features will be added to the v2 transforms only. These transforms are fully backward compatible with the v1 ones, so if you’re already using tranforms from , all you need to do to is to update the import to . In terms of output, there might be negligible differences due to implementation differences.\n\nWe recommend the following guidelines to get the best performance out of the transforms:\n• None Rely on the v2 transforms from\n• None Use tensors instead of PIL images\n• None Use dtype, especially for resizing This is what a typical transform pipeline could look like: # Convert to tensor, only needed if you had a PIL image # optional, most input are already uint8 at this point The above should give you the best performance in a typical training environment that relies on the with . Transforms tend to be sensitive to the input strides / memory format. Some transforms will be faster with channels-first images while others prefer channels-last. Like operators, most transforms will preserve the memory format of the input, but this may not always be respected due to implementation details. You may want to experiment a bit if you’re chasing the very best performance. Using on individual transforms may also help factoring out the memory format variable (e.g. on ). Note that we’re talking about memory format, not tensor shape. Note that resize transforms like and typically prefer channels-last input and tend not to benefit from at this time.\n\nTransforms are available as classes like , but also as functionals like in the namespace. This is very much like the package which defines both classes and functional equivalents in . The functionals support PIL images, pure tensors, or TVTensors, e.g. both and are valid. Random transforms like will randomly sample some parameter each time they’re called. Their functional counterpart ( ) does not do any kind of random sampling and thus have a slighlty different parametrization. The class method of the transforms class can be used to perform parameter sampling when using the functional APIs. The namespace also contains what we call the “kernels”. These are the low-level functions that implement the core functionalities for specific types, e.g. or . They are public, although not documented. Check the code to see which ones are available (note that those starting with a leading underscore are not public!). Kernels are only really useful if you want torchscript support for types like bounding boxes or masks.\n\nMost transform classes and functionals support torchscript. For composing transforms, use instead of : v2 transforms support torchscript, but if you call on a v2 class transform, you’ll actually end up with its (scripted) v1 equivalent. This may lead to slightly different results between the scripted and eager executions due to implementation differences between v1 and v2. If you really need torchscript support for the v2 transforms, we recommend scripting the functionals from the namespace to avoid surprises. Also note that the functionals only support torchscript for pure tensors, which are always treated as images. If you need torchscript support for other types like bounding boxes or masks, you can rely on the low-level kernels. For any custom transformations to be used with , they should be derived from ."
    },
    {
        "link": "https://gist.github.com/peteflorence/a1da2c759ca1ac2b74af9a83f69ce20e",
        "document": "Here's a simple implementation of bilinear interpolation on tensors using PyTorch.\n\nI wrote this up since I ended up learning a lot about options for interpolation in both the numpy and PyTorch ecosystems. More generally than just interpolation, too, it's also a nice case study in how PyTorch magically can put very numpy-like code on the GPU (and by the way, do autodiff for you too).\n\nFor interpolation in PyTorch, this open issue calls for more interpolation features. There is now a feature but at least at first this didn't look like what I needed (but we'll come back to this later).\n\nIn particular I wanted to take an image, , and sample it many times at different random locations. Note also that this is different than upsampling which exhaustively samples and also doesn't give us flexibility with the precision of sampling.\n\nFirst let's look at a comparable implementation in numpy which is slightly modified from here.\n\nAnd now here I've converted this implementation to PyTorch:\n\nBilinear interpolation is very simple but there are a few things that can be easily messed up.\n\nI did a quick comparison for correctness with SciPy's .\n• Side note: there are actually a ton of interpolation options in SciPy but none I tested met my critera of (a) doing bilinear interpolation for high-dimensional spaces and (b) efficiently use gridded data. The ones I tested that were built for many dimensions were requiring me to specify sample points for all of those dimensions (and doing trilinear, or other) interpolation. I could get to do bilinear interpolation for high dimensional vectors but this does not meet criteria (b). There's probably a better option but, at any rate, I gave up and went back to my numpy and PyTorch options.\n\nThe above gives:\n\nFor the correctness test comparing with scipy, we couldn't do interpolation for anything but . Now though, we can do bilinear interpolation in either numpy or torch for arbitrary :\n\nYou'll find that the above numpy and torch versions give the same result.\n\nNow we do some simple benchmarking:\n\nOn my machine (CPU: 10-core i7-6950X, GPU: GTX 1080) I get the following times (in seconds):\n\nInterestingly we have torch on the GPU beating numpy (CPU-only) by about 10x. I'm not sure why torch on the CPU is that slow for this test case. Note that the ratios between these change quite drastically for different .\n\nI ended up figuring out how to use although it was a little odd of a fit for my needs. (Data needs to be in tensor input, and samples need to be as normalized between [-1,1], and AFAIK the ordering of the samples do not have any meaning other -- they are completely separate samples.)\n\nIt was good practice in using .\n\nMy wrapping of grid_sample produces the same bilinear interpolation results and at speeds comparable to our function:\n\nAnother note about the interface is that it forces the sampled interpolations into a wrapper even though this seems best left to the programmer to decide.\n• It's surprisingly easy to convert powerful vectorized numpy code into more-powerful vectorized PyTorch code\n• PyTorch is very fast on the GPU\n• Some of the higher-level feature (like ) are nice but so too is writing your own tensor manipulations (and can be comparably fast)"
    }
]