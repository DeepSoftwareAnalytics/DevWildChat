[
    {
        "link": "https://nxp.com/docs/en/user-manual/UM11732.pdf",
        "document": ""
    },
    {
        "link": "https://blog.mbedded.ninja/electronics/communication-protocols/i2s-communication-protocol",
        "document": "Inter-Integrated Circuit (Inter-IC) Sound (I2S) is a synchronous, serial communication protocol which uses Pulse Code Modulation (PCM) to transmit audio data between integrated circuits, typically those within the same PCB. It is commonly used to send audio between electronics such as MCUs, DSPs and audio amplifiers.\n\nIt can be written/pronounced as:\n\nICs connected using I2S for audio data transfer will typically also be connected via I2C for control and configuration.\n\nI2S was originally developed by Phillips in the 1980s. The first I2S specification was published in 1986, and it was revised in 19962 3.\n\nThere are three signals in the I2S protocol:\n• Serial Clock (SCK): This is the clock signal that is used to synchronise the data transfer between the master and slave devices. The clock signal is generated by the master device and is used by the slave device to sample the data.\n• Word Select (WS): This signal is used to indicate whether the data being transferred is left-channel, right-channel or a control word. The WS signal is generated by the master device. It is also known as left-right clock (LRCLK)4 or frame sync (FS)5.\n• Serial Data (SD): This is the actual audio data that is being transferred between the master and slave devices.\n\nThis is a placeholder for the reference: fig-i2s-signals-between-master-and-slave shows the I2S signals required between master and slave devices. In a), the master device (transmitter) is sending data to the slave device (receiver). In b), the slave device (transmitter) is sending data to the master device (receiver).\n\nThis is a placeholder for the reference: fig-i2s-waveform-diagram shows the digital waveforms of the three I2S signals.\n\nThe three signals are explained in more detail below.\n\nThe Serial Clock (SCK) signal is used to synchronise the data transfer between the master and slave devices. The clock signal is generated by the master device and is used by the slave device to sample the data.\n\nThe SCK frequency is dependent on:\n• The number of bits per sample (typically 8, 12, 16 or 24 bits).\n• The number of channels (e.g. 2 for left and right audio).\n• The audio sample rate (samples per second, commonly ).\n\nFor example, for a 16-bit stereo audio signal with a sample rate of 44.1 kHz, the SCK frequency would be:\n\nSo even though you typically think of audio as being in the 0-20 kHz range, the digital bitstream that represents the audio can quickly climb into the MHz range. 1-10 MHz is generally not a problem for short traces on a PCB without high-speed design considerations.\n\nThe Word Select (WS) signal lets the receiver know whether the data being transmitted is for channel 1 (WS = 0) or channel 2 (WS = 1). For stereo signals WS = 0 is used for the left channel and WS = 1 is used for the right channel.\n\nIt is important to note that WS changes one clock cycle before the MSB of a sample is transmitted, rather than on the same clock cycle. This is so that a slave transmitter knows to start transmission of the next sample on the next clock cycle. It also allows the receiver to store the previous sample and clear the input ready for the next sample6.\n\nIt is also known as left-right clock (LRCLK, LRCK4) or frame sync (FS).\n\nWS is normally synchronized with the falling edge of SCK, and is latched by the receiver on the rising edge.\n\nWS has a 50% duty cycle and has the same frequency as the audio sample rate.\n\nThe Serial Data signal (SD) is used to transmit the audio samples. Data is sent as a signed number in two’s complement format. The most significant byte (MSB) is sent first.\n\nData may be clocked out by the transmitter on either the rising or falling edge of SCK (usually on the falling edge). However, at the receiver data is always sampled on the rising edge of SCK.62\n\nMaster vs. Slave vs. Transmitter vs. Receiver\n\nThe Master device is the device which is generating the SCK and WS signals. The Slave is the other device. Master and slave devices can both be either the transmitter or receiver in an I2S connection. The transmitter is the device which is generating the SD signal.\n\nA slave device, not having control of the SCK and WS signals, is normally designed to handle receiving more or less bits per sample that it expects. This allows for two devices with different sample rates to communicate. If the slave device receives more bits than it needs, it can ignore the extra bits (waiting for WS to change) knowing that the bits it has received are the most significant bits. If it receives fewer bits than it needs, it can pad the least significant bits of it’s sample with zeros.\n\nI2S was designed to be run short distances on PCBs. However, some companies have developed ways to run I2S over cables.\n\nPS Audio started the trend of using I2S over HDMI cables. They use differential I2S signals rather than the single-ended signals used in the formal I2S standard.\n\nThe Nordic nRF52 and nRF53 MCU families have I2S peripherals. These peripherals allow for both the transmission and reception of audio data. They usually support DMA for transferring audio data in and out of memory without CPU intervention.4 The Nordic I2S peripheral supports both the standard “I2S format” and an “aligned format” --- the aligned format is when WS is no longer changes 1 bit before the MSB of the next sample, but rather changes at the same time as the MSB.4\n\nNote that the nRF52 I2S peripheral does not support padding when acting as a master. This means that it does not support audio amplifiers such as the Texas Instruments TAS5720x family which require a and that is at least 64 times the audio sample rate.9 These values are shown in This is a placeholder for the reference: fig-ti-i2s-audio-amp-tas5720x-min-clock-freq-highlighted . The problem here is that with at 64x the sample rate, this means at least 32-bits per left and right channel are required. But the nRF52 I2S peripheral only supports up to 24-bits per sample, and there is no ability to add padding to increase it up to 32-bits.\n\nThe nRF53 MCU can support these high clock frequency audio amplifiers because it supports “separate sample and word widths” (as per the nRF53 specifications).8 This is essentially “padding”, and the sample will be either left or right aligned at the start of each half frame.\n\nPSoC MCUs support I2S via components in PSoC Creator. The following diagram shows the PSoC “I2S TX Master and RX Master” component in PSoC Creator:\n\nThe MAX98365 is a “plug-and-play” digital class-D amplifier that supports I2S. It is “plug-and-play” in the sense that it auto-detects different PCM and TDM clocking schemes and has no I2C configuration required. This means you can typically power it up, provide , and and it will work. Given it has no I2C configuration, it does provide a pin called which can be used for gain selection in I2S mode.10\n\nThe MAX98365 also supports TDM mode, where up to 8 of the MAX98365s can be connected together with only 4 wires, as shown in This is a placeholder for the reference: fig-max98365-tdm-operation-schematic-8-channels .\n\nZephyr RTOS has an I2S API that can be used to configure and use I2S peripherals on supported MCUs. It also supports non-standard extensions such as PCM short/long frame sync and left/right justified data formats11."
    },
    {
        "link": "https://ti.com/lit/pdf/spra595",
        "document": ""
    },
    {
        "link": "https://analog.com/media/en/technical-documentation/technical-articles/ms-2275.pdf",
        "document": ""
    },
    {
        "link": "https://keysight.com/blogs/en/tech/bench/2022/04/29/the-i2s-protocol-and-why-digital-audio-is-everywhere",
        "document": "Regardless of subjective preference, no one can deny that going digital has allowed audio to find a place in many new applications over the past few decades.\n\nWhen digital audio was first introduced in the 1970s, it was regarded with suspicion and dismissed as sterile and lifeless. Even today, audiophiles swear by purely analog systems, despite the technical advantages of digital audio quality. Audio discussions amongst engineers are bound to be filled with both dense technical theory and passionate subjective opinion.\n\nToday, digital audio is everywhere and in everything. Consumers use their mobile phones and smart speakers to stream music and videos, call friends, and talk to their cloud-connected digital assistants. Products which always included audio, such as mobile phones, laptops, and televisions, have increased their sound quality by taking advantage of digital processing. Other products which did not have audio in the past, such as security cameras, video doorbells and even refrigerators now have two-way audio communication. It’s difficult to imagine these applications the same way in an analog world.\n\nMany technological advancements made the proliferation of digital audio possible. Advanced compression algorithms, Wi-fi, and Bluetooth make it possible for us to stream hours of audio from cloud servers to our devices. But the technology that often gets overlooked is the inter-chip communication protocols that allow audio chips to work with one another: Inter-IC Sound (I2S) and Inter-Integrated Circuit (I2C).\n\nHow Audio Chips Communicate\n\nEstablishing a standard for communication between digital audio circuits was essential for the development and spread of digital audio systems in consumer markets that we see today. Before the introduction of I2S in 1986, there was no standard way for devices to communicate digital audio, and a mixture of generic protocols, such as Time-division Multiplexing (TDM).\n\nI2S is a serial bus interface specially designed for communicating digital audio data between integrated circuits (ICs). The I2S protocol sends pulse-code modulation (PCM) audio data from a controller to a target. It has at least three lines: the bit clock, the word select, and a data line. Word select is used to specify which of the stereo channel, left or right, the data should be sent to.\n\nAnother important standard, I2C, is used to control all the devices in the audio signal chain. I2C was also developed in the 1980s, but it is not specific to audio electronics. I2C connects audio devices such as data converters and amplifier ICs to the host processor. This allows the host processor to configure devices upon startup and control which devices are awake, among other functions like controlling sampling speed, volume, etc.\n\nAudio ICs often support other protocols to give developers more flexibility. Some audio devices have options for TDM and PCM interfaces, which can be used for audio data communication. Serial Peripheral Interfaces (SPI) can be used for control functions instead of I2C.\n\nAudio systems come in many configurations depending on the intended use of the product. Some systems keep the audio as analog for as long as possible, using analog microphones and speaker amplifiers with minimal digital processing in between. Others take in digital audio data, process it, and output to another system without any analog audio signal presence whatsoever.\n\n\n\nFigure 1: An audio system block diagram showing I2C and I2C connections between ICs.\n\nCost and quality are key considerations in making such decisions. Audiophile systems tend to favor high-quality discrete components, especially data converters, and therefore have more analog elements. Consumer electronics, on the other hand, value cost-effective integration: digital micro-electro-mechanical systems (MEMS) microphones, digital signal processors (DSP) with integrated audio codecs, and digital input speaker amplifiers. A simplified example of such a system can be seen in Figure 1.\n\nDigital audio has many advantages for systems that need to do a lot with the audio signal. It can be easily processed and communicated to a wireless module. In a smart speaker, audio is transmitted and received over Wi-Fi for cloud-based processing and streaming. Digital microphone arrays are better for picking up far-field audio, and digital-input amplifiers (like smart amplifiers) can process the I2S stream for internal equalization and volume control to make small speakers sound louder. For these reasons, in addition to the cost benefits, the market for digital audio ICs has grown in many new and unexpected sectors.\n\nA side-effect of this growth is that engineers with little experience in audio or acoustics are now tasked with designing, testing, and debugging audio systems. Audio developers have traditionally tested and debugged I2S and I2C manually, by visually counting bits. This method is tedious and prone to errors. Having an I2S and I2C signal analysis toolkit is essential for any developer working in consumer electronics.\n\nThankfully, triggering and decoding these protocols are simple tasks for an oscilloscope with the right analysis software. No matter which Keysight oscilloscope you have, there is a software package available to simplify the process. For Infiniium oscilloscopes, the D9010LSSP application software enables your scope to trigger and decode I2S, I2C, SPI and more low speed serial protocols. Similarly, embedded analysis software packages are available for each InfiniiVision X-Series oscilloscope and include support for these common audio serial protocols and more. The InfiniiVision 3000G X-Series comes standard with the D3000GENB Embedded Analysis Software, which supports trigger and decode for I2S, I2C, SPI, and more serial protocols.\n\nLearn more about How to Characterize Embedded Serial Buses by visiting our latest use cases."
    },
    {
        "link": "https://github.com/miketeachman/micropython-i2s-examples",
        "document": "This repository provides MicroPython example code, showing how to use the I2S protocol with development boards supporting MicroPython. The I2S protocol can be used to play WAV audio files through a speaker or headphone, or to record microphone audio to a WAV file on a SD card.\n\nThe examples are supported on 4 ports: stm32, esp32, rp2, and mimxrt.\n\nTo use I2S with MicroPython on the Pyboards, ESP32, Raspberry Pi Pico, and mimxrt boards you will need to install a version of MicroPython firmware that supports I2S. For these ports, I2S is supported in the v1.20 release and all nightly builds.\n\nThe easiest way to get started with I2S is playing a pure tone to ear phones using a DAC board such as the I2S UDA1334A breakout board or the I2S PCM5102 Stereo DAC Decoder board. Here are the steps:\n• Download and program the appropriate firmware that supports the I2S protocol into the MicroPython development board\n• Load the example code into a text editor, found in the examples folder\n• Make the following wiring connections using a quality breadboard and jumper wires. Use the GPIO pins that are listed in the example code file. Refer to the section on below.\n• Copy the code from the editor e.g. ctrl-A, ctrl-C\n• Ctrl-D in the REPL to run the code\n• Result: the tone should play in the ear phones\n\nMicroPython example code is contained in the examples folder. WAV files used in the examples are contained in the wav folder.\n\nEach example file has configuration parameters, marked with\n\nEach example supports all MicroPython ports that offer I2S. This has the benefit of having fewer example files, but comes at the cost of large example files (e.g. many blocks of code that start with lines like ). Examples can be simplified by removing the blocks of port-specific code that are not needed for a particular development board.\n\nAll Pyboard V1.1 and Pyboard D examples use the following I2S peripheral ID and GPIO pins\n\nTo use different GPIO mappings refer to the sections below\n\nAll ESP32 examples use the following I2S peripheral ID and GPIO pins\n\nTo use different GPIO mappings refer to the sections below\n\nAll Pico examples use the following I2S peripheral ID and GPIO pins\n\nTo use different GPIO mappings refer to the sections below\n\nAll MIMXRT examples are designed for the Teensy 4.0/4.1 boards and use the following I2S peripheral ID and GPIO pins. Note: the Teensy 4.1 is a better choice compared to the Teensy 4.0. The Teensy 4.1 has a built-in SD card slot that can be used with the class, which offers fast SD card access. The Teensy 4.0 requires an external SD card module, which only works with the slower driver.\n\nTo use different GPIO mappings refer to the sections below\n\nThe file contains an easy-to-use micropython example for playing WAV files. This example requires an SD card (to store the WAV files). Pyboards have a built in SD card. Some ESP32 development boards have a built-in SD Card, such as the Lolin D32 Pro. Other devices, such as the TinyPico and Raspberry Pi Pico require an external SD card module to be wired in. Additionally, for the Raspberry Pi Pico sdcard.py needs to be copied to the Pico's filesystem to enable SD card support.\n• Wire up the hardware. e.g. connect the I2S playback module to the development board, and connect an external SD Card Module (if needed). See tips on hardware wiring below. The example uses the default GPIO pins outlined above. These can be customized, if needed.\n• copy file to the internal flash file system using a command line tool such as ampy or rshell. The Thonny IDE also offers an easy way to copy this file (View->Files, option).\n• copy the WAV file(s) you want to play to an SD card. Plug the SD card into the SD card Module.\n• configure the file to specify the WAV file(s) to play\n• copy the file to the internal flash file system using a command line tool such as ampy or rshell. The Thonny IDE also offers an easy way to copy this file (View->Files, option).\n• run by importing the file into the REPL. e.g. import easy_wav_player\n• try various ways of playing a WAV file, using the , , and methods\n\nMP3 files can be converted to WAV files using online applications such as online-convert\n\nWAV file tag data can be inspected using a downloadable application such as MediaInfo. This application is useful to check the sample rate, stereo versus mono, and sample bit size (16, 24, or 32 bits)\n\nOn Pyboard devices I2S compatible GPIO pins are mapped to a specific I2S hardware bus. The tables below show this mapping. For example, the GPIO pin \"Y6\" can only be used with I2S ID=2.\n\nAll ESP32 GPIO pins can be used for I2S, with attention to special cases:\n• GPIO strapping pins: see note below on using strapping pins\n\nStrapping Pin consideration: The following ESP32 GPIO strapping pins should be used with caution. There is a risk that the state of the attached hardware can affect the boot sequence. When possible, use other GPIO pins.\n• GPIO0 - used to detect boot-mode. Bootloader runs when pin is low during powerup. Internal pull-up resistor.\n• GPIO4 - technical reference indicates this is a strapping pin, but usage is not described. Internal pull-down resistor.\n• GPIO15 - used to configure silencing of boot messages. Internal pull-up resistor.\n\nAll Pico GPIO pins can be used for I2S, with one limitation. The WS pin number must be one greater than the SCK pin number.\n\nOn boards supporting NXP i.MX RT processors I2S compatible GPIO pins are mapped to a specific I2S hardware bus. In addition, GPIO pins are further specified as either Transmit or Receive. The tables below show this mapping for 3 boards. For example, the GPIO pin 4 can be used with I2S ID=2 and transmitting to a DAC. Other I2S pin mapping combinations exist, but are not needed for simple-to-use I2S hardware, such as the INMP441 microphone, or the I2S PCM5102 Stereo DAC Decoder.\n\nSome audio codecs require a Master Clock signal at a typical frequency of 256 * sampling frequency.\n\n Only the mimxrt port supports Master Clock generation in the I2S class. For other ports, the machine.PWM class offers a convenient way to generate the Master Clock signal. Here is some example code showing how to generate a Master Clock for the teensy audio shield.\n\nI have found the best audio quality is acheived when:\n• modules are connected with header pins and 10cm long female-female jumpers, OR\n\nThe following images show example connections between microcontroller boards and breakout boards. The following colour conventions are used for the signals:\n\nThe I2S protocol is different than other protocols such as I2C and SPI. Those protocols are transactional. A controller requests data from a peripheral and waits for a reply. I2S is a streaming protocol. Data flows continuously, ideally without gaps.\n\nIt's interesting to use a water and bucket analogy for the MicroPython I2S implementation. Consider writing a DAC using I2S. The internal buffer(ibuf) can be considered as a large bucket of water, with a hole in the bottom that drains the bucket. The water streaming out of the bottom is analogous to the flow of audio samples going into the I2S hardware. That flow must be constant and at a fixed rate. The user facing buffer is like a small bucket that is used to fill the large bucket. In the case of I2S writes, the small bucket is used to transport audio samples from a Wav file \"lake\" and fill the large bucket (ibuf). Imagine a person using the small bucket to move audio samples from the Wav file to the large bucket; if the large bucket becomes full, the person might go do another task, and come back later to see if there is more room in the large bucket. When they return, if there is space in the large bucket, they will pour some more water (samples) into the large bucket. Initially, the large buffer is empty. Almost immediately after water is poured into the large bucket audio samples stream out of the bottom and sound is heard almost immediately. After the last small bucket is poured into the large bucket it will take some time to drain the large bucket -- sound will be heard for some amount of time after the last small bucket is poured in.\n\nIf the person is too slow to refill the large bucket it will run dry and the water flow stops, a condition called \"underflow\" -- there will be a gap in sound produced.\n\nDoes a water analogy help to explain I2S? comments welcome !\n\nQ: Are there sizing guidelines for the internal buffer (ibuf)?\n\n A: For playback of wave files, a good starting point is to size the ibuf = 2x user buffer size. For example, if the user buffer is 10kB, ibuf could be sized at 20kB. If gaps are detected in the audio playback increasing the size of ibuf may mitigate the gaps. For microphone recordings, ibuf will often need to be much larger. Unlike reads, data writes to most SD cards are not consistent - some writes can take much longer. For example, the average write speed might be 1000kB/s, but some writes may slow to 10kB/s. Having a large ibuf accommodates these periods of slow data writes. Consider starting with ibuf = 4x user buffer size. Increase the ibuf size if gaps are detected in the recording.\n\nQ: How many seconds of audio data is held in the internal buffer (ibuf)?\n\n A: T[seconds] = ibuf-size-in-bytes / sample-rate-in-samples-per-second / num-channels / sample-size-in-bytes\n\n stereo = 2 channels, mono = 1 channel.\n\nQ: Are there sizing guidelines for the user buffer?\n\n A: Smaller sizes will favour efficient use of heap space, but suffer from the inherent inefficiency of more switching between filling and emptying. A larger user buffer size suffers from a longer time of processing the samples or time to fill from a SD card - this longer time may block critical time functions from running. A good starting point is a user buffer of 5kB.\n\nQ: What conditions causes gaps in the sample stream?\n\n A: For writes to a DAC, a gap will happen when the internal buffer is filled at a slower rate than samples being sent to the I2S DAC. This is called underflow. For reads from a microphone, a gap will happen when the internal buffer is emptied at a slower rate than sample data is being read from the microphone. This is called overflow.\n\nQ: Does the MicroPython I2S class support devices that need a MCK signal?\n\n A: Yes, one port currently supports MCK. The mimxrt port of I2S allows a MCK output to be defined. Most devices that are popular with users do not need a MCK signal.\n\nThis is a well designed breakout board based on the SPH0645LM4H microphone device. Users need to be aware that the SPH0645LM4H device implements non-standard Philips I2S timing. When used with the ESP32, all audio samples coming from the I2S microphone are shifted to the left by one bit. This increases the sound level by 6dB. More details on this problem are outlined a StreetSense project log.\n\n Workaround: Use the static I2S class method to right shift all samples that are read from the microphone.\n\n\"Le blues de la vache\" by Le Collectif Unifié de la Crécelle is licensed under CC BY-NC-SA 2.1"
    },
    {
        "link": "https://github.com/miketeachman/micropython-i2s-examples/blob/master/README.md",
        "document": "This repository provides MicroPython example code, showing how to use the I2S protocol with development boards supporting MicroPython. The I2S protocol can be used to play WAV audio files through a speaker or headphone, or to record microphone audio to a WAV file on a SD card.\n\nThe examples are supported on 4 ports: stm32, esp32, rp2, and mimxrt.\n\nTo use I2S with MicroPython on the Pyboards, ESP32, Raspberry Pi Pico, and mimxrt boards you will need to install a version of MicroPython firmware that supports I2S. For these ports, I2S is supported in the v1.20 release and all nightly builds.\n\nThe easiest way to get started with I2S is playing a pure tone to ear phones using a DAC board such as the I2S UDA1334A breakout board or the I2S PCM5102 Stereo DAC Decoder board. Here are the steps:\n• Download and program the appropriate firmware that supports the I2S protocol into the MicroPython development board\n• Load the example code into a text editor, found in the examples folder\n• Make the following wiring connections using a quality breadboard and jumper wires. Use the GPIO pins that are listed in the example code file. Refer to the section on below.\n• Copy the code from the editor e.g. ctrl-A, ctrl-C\n• Ctrl-D in the REPL to run the code\n• Result: the tone should play in the ear phones\n\nMicroPython example code is contained in the examples folder. WAV files used in the examples are contained in the wav folder.\n\nEach example file has configuration parameters, marked with\n\nEach example supports all MicroPython ports that offer I2S. This has the benefit of having fewer example files, but comes at the cost of large example files (e.g. many blocks of code that start with lines like ). Examples can be simplified by removing the blocks of port-specific code that are not needed for a particular development board.\n\nAll Pyboard V1.1 and Pyboard D examples use the following I2S peripheral ID and GPIO pins\n\nTo use different GPIO mappings refer to the sections below\n\nAll ESP32 examples use the following I2S peripheral ID and GPIO pins\n\nTo use different GPIO mappings refer to the sections below\n\nAll Pico examples use the following I2S peripheral ID and GPIO pins\n\nTo use different GPIO mappings refer to the sections below\n\nAll MIMXRT examples are designed for the Teensy 4.0/4.1 boards and use the following I2S peripheral ID and GPIO pins. Note: the Teensy 4.1 is a better choice compared to the Teensy 4.0. The Teensy 4.1 has a built-in SD card slot that can be used with the class, which offers fast SD card access. The Teensy 4.0 requires an external SD card module, which only works with the slower driver.\n\nTo use different GPIO mappings refer to the sections below\n\nThe file contains an easy-to-use micropython example for playing WAV files. This example requires an SD card (to store the WAV files). Pyboards have a built in SD card. Some ESP32 development boards have a built-in SD Card, such as the Lolin D32 Pro. Other devices, such as the TinyPico and Raspberry Pi Pico require an external SD card module to be wired in. Additionally, for the Raspberry Pi Pico sdcard.py needs to be copied to the Pico's filesystem to enable SD card support.\n• Wire up the hardware. e.g. connect the I2S playback module to the development board, and connect an external SD Card Module (if needed). See tips on hardware wiring below. The example uses the default GPIO pins outlined above. These can be customized, if needed.\n• copy file to the internal flash file system using a command line tool such as ampy or rshell. The Thonny IDE also offers an easy way to copy this file (View->Files, option).\n• copy the WAV file(s) you want to play to an SD card. Plug the SD card into the SD card Module.\n• configure the file to specify the WAV file(s) to play\n• copy the file to the internal flash file system using a command line tool such as ampy or rshell. The Thonny IDE also offers an easy way to copy this file (View->Files, option).\n• run by importing the file into the REPL. e.g. import easy_wav_player\n• try various ways of playing a WAV file, using the , , and methods\n\nMP3 files can be converted to WAV files using online applications such as online-convert\n\nWAV file tag data can be inspected using a downloadable application such as MediaInfo. This application is useful to check the sample rate, stereo versus mono, and sample bit size (16, 24, or 32 bits)\n\nOn Pyboard devices I2S compatible GPIO pins are mapped to a specific I2S hardware bus. The tables below show this mapping. For example, the GPIO pin \"Y6\" can only be used with I2S ID=2.\n\nAll ESP32 GPIO pins can be used for I2S, with attention to special cases:\n• GPIO strapping pins: see note below on using strapping pins\n\nStrapping Pin consideration: The following ESP32 GPIO strapping pins should be used with caution. There is a risk that the state of the attached hardware can affect the boot sequence. When possible, use other GPIO pins.\n• GPIO0 - used to detect boot-mode. Bootloader runs when pin is low during powerup. Internal pull-up resistor.\n• GPIO4 - technical reference indicates this is a strapping pin, but usage is not described. Internal pull-down resistor.\n• GPIO15 - used to configure silencing of boot messages. Internal pull-up resistor.\n\nAll Pico GPIO pins can be used for I2S, with one limitation. The WS pin number must be one greater than the SCK pin number.\n\nOn boards supporting NXP i.MX RT processors I2S compatible GPIO pins are mapped to a specific I2S hardware bus. In addition, GPIO pins are further specified as either Transmit or Receive. The tables below show this mapping for 3 boards. For example, the GPIO pin 4 can be used with I2S ID=2 and transmitting to a DAC. Other I2S pin mapping combinations exist, but are not needed for simple-to-use I2S hardware, such as the INMP441 microphone, or the I2S PCM5102 Stereo DAC Decoder.\n\nSome audio codecs require a Master Clock signal at a typical frequency of 256 * sampling frequency.\n\n Only the mimxrt port supports Master Clock generation in the I2S class. For other ports, the machine.PWM class offers a convenient way to generate the Master Clock signal. Here is some example code showing how to generate a Master Clock for the teensy audio shield.\n\nI have found the best audio quality is acheived when:\n• modules are connected with header pins and 10cm long female-female jumpers, OR\n\nThe following images show example connections between microcontroller boards and breakout boards. The following colour conventions are used for the signals:\n\nThe I2S protocol is different than other protocols such as I2C and SPI. Those protocols are transactional. A controller requests data from a peripheral and waits for a reply. I2S is a streaming protocol. Data flows continuously, ideally without gaps.\n\nIt's interesting to use a water and bucket analogy for the MicroPython I2S implementation. Consider writing a DAC using I2S. The internal buffer(ibuf) can be considered as a large bucket of water, with a hole in the bottom that drains the bucket. The water streaming out of the bottom is analogous to the flow of audio samples going into the I2S hardware. That flow must be constant and at a fixed rate. The user facing buffer is like a small bucket that is used to fill the large bucket. In the case of I2S writes, the small bucket is used to transport audio samples from a Wav file \"lake\" and fill the large bucket (ibuf). Imagine a person using the small bucket to move audio samples from the Wav file to the large bucket; if the large bucket becomes full, the person might go do another task, and come back later to see if there is more room in the large bucket. When they return, if there is space in the large bucket, they will pour some more water (samples) into the large bucket. Initially, the large buffer is empty. Almost immediately after water is poured into the large bucket audio samples stream out of the bottom and sound is heard almost immediately. After the last small bucket is poured into the large bucket it will take some time to drain the large bucket -- sound will be heard for some amount of time after the last small bucket is poured in.\n\nIf the person is too slow to refill the large bucket it will run dry and the water flow stops, a condition called \"underflow\" -- there will be a gap in sound produced.\n\nDoes a water analogy help to explain I2S? comments welcome !\n\nQ: Are there sizing guidelines for the internal buffer (ibuf)?\n\n A: For playback of wave files, a good starting point is to size the ibuf = 2x user buffer size. For example, if the user buffer is 10kB, ibuf could be sized at 20kB. If gaps are detected in the audio playback increasing the size of ibuf may mitigate the gaps. For microphone recordings, ibuf will often need to be much larger. Unlike reads, data writes to most SD cards are not consistent - some writes can take much longer. For example, the average write speed might be 1000kB/s, but some writes may slow to 10kB/s. Having a large ibuf accommodates these periods of slow data writes. Consider starting with ibuf = 4x user buffer size. Increase the ibuf size if gaps are detected in the recording.\n\nQ: How many seconds of audio data is held in the internal buffer (ibuf)?\n\n A: T[seconds] = ibuf-size-in-bytes / sample-rate-in-samples-per-second / num-channels / sample-size-in-bytes\n\n stereo = 2 channels, mono = 1 channel.\n\nQ: Are there sizing guidelines for the user buffer?\n\n A: Smaller sizes will favour efficient use of heap space, but suffer from the inherent inefficiency of more switching between filling and emptying. A larger user buffer size suffers from a longer time of processing the samples or time to fill from a SD card - this longer time may block critical time functions from running. A good starting point is a user buffer of 5kB.\n\nQ: What conditions causes gaps in the sample stream?\n\n A: For writes to a DAC, a gap will happen when the internal buffer is filled at a slower rate than samples being sent to the I2S DAC. This is called underflow. For reads from a microphone, a gap will happen when the internal buffer is emptied at a slower rate than sample data is being read from the microphone. This is called overflow.\n\nQ: Does the MicroPython I2S class support devices that need a MCK signal?\n\n A: Yes, one port currently supports MCK. The mimxrt port of I2S allows a MCK output to be defined. Most devices that are popular with users do not need a MCK signal.\n\nThis is a well designed breakout board based on the SPH0645LM4H microphone device. Users need to be aware that the SPH0645LM4H device implements non-standard Philips I2S timing. When used with the ESP32, all audio samples coming from the I2S microphone are shifted to the left by one bit. This increases the sound level by 6dB. More details on this problem are outlined a StreetSense project log.\n\n Workaround: Use the static I2S class method to right shift all samples that are read from the microphone.\n\n\"Le blues de la vache\" by Le Collectif Unifié de la Crécelle is licensed under CC BY-NC-SA 2.1"
    },
    {
        "link": "https://medium.com/@jatin.dhall7385/pythonic-wav-file-handling-a-guide-to-reading-wav-files-without-external-libraries-f5869b27b2e7",
        "document": "Audio is a crucial component of all digital material. Sound, or audio, is analogue in nature since it is a wave. This indicates the audio data must be transformed to digital signals in order to be directly stored. WAV is one of these file formats.\n\nWAV (Waveform Audio File Format) files have long been a standard for storing audio on computers. In this blog post, we’ll embark on a journey to understand the intricacies of WAV files, from their basic structure to their role in preserving high-quality audio. Whether you’re a curious enthusiast or a developer looking to work with audio data, this guide will provide a comprehensive overview.\n\nAudio compression algorithms fall under two categories: Lossless and Lossy compression. In Lossless algorithms, compression is applied to the audio without removing or losing any audio data. However, this results in large file sizes. Conversely, Lossy algorithms remove portions of the audio that are considered inaudible or indistinguishable to the human ear, leading to smaller file sizes.\n\nWAV, short for WAVE, is a file format designed to store acoustic waves, serving as a container for uncompressed audio data and associated metadata. Jointly developed by Microsoft and IBM, WAV files are renowned for preserving high-quality, lossless audio. As one of the earliest audio encoding formats still in use, WAV has become the preferred choice for archiving digital audio and Audio Signal Processing due to its lossless nature and exceptional quality. However, its sizable file size typically discourages internet downloads.\n\nLike many media formats, WAV is an example of the container-codec paradigm. The container establishes the file structure. It holds metadata about the recording, as well as info on how the audio is encoded. The software that handles the bits of audio data is the codec. In simpler terms, Codec’s are encoders and decoders(co-coders, dec-decoders or co- compressor, dec- decompressor). The reason why WAV files are so big, is that they normally use the LPCM(Linear Pulse Code modulation) codec that doesn’t compress the audio data. In definition “LPCM is a lossless audio coding that delivers CD quality sound, so it occupies huge disc space. It is used to transmit uncompressed digital audio information via HDMI”. What Archivists like most, is that these files support any kind of metadata.\n\nWAV files utilize a specific type of RIFF (Resource Interchange File Format), a format designed for bundling data and metadata. RIFF files are composed of “Chunks,” each dedicated to storing specific types of information. In the case of WAV files, the structure involves a header followed by audio data. Within the header lies vital information about the audio, encompassing details such as the sample rate, bit depth, and the number of channels. The following comprehensive grasp of the structure is crucial for effective programmatic handling of WAV files.\n• Chunk ID (4 bytes): It usually contains the ASCII characters “RIFF” (0x52494646 in hexadecimal).\n• Chunk Size (4 bytes): Total size of the file excluding the first 8 bytes (header size). It is little-endian, meaning the least significant byte is stored first.\n• Format (4 bytes): It usually contains the ASCII characters “WAVE” (0x57415645 in hexadecimal).\n• Chunk ID (4 bytes): It usually contains the ASCII characters “fmt “ (0x666D7420 in hexadecimal).\n• Chunk Size (4 bytes): Size of the format chunk, which is typically 16 bytes for PCM data.\n• Audio Format (2 bytes): Audio format. PCM (Pulse Code Modulation) is the most common and has a value of 1.\n• Number of Channels (2 bytes): Number of audio channels (1 for mono, 2 for stereo, etc.).\n• Sample Rate (4 bytes): Number of samples per second (in hertz).\n• Byte Rate (4 bytes): Data rate in bytes per second. Calculated as Sample Rate * Number of Channels * Bits per Sample / 8.\n• Block Align (2 bytes): Number of bytes for one sample, including all channels.\n• Bits per Sample (2 bytes): Number of bits in each sample.\n• Chunk ID (4 bytes): It usually contains the ASCII characters “data” (0x64617461 in hexadecimal).\n• Chunk Size (4 bytes): Size of the data section (number of bytes of audio data).\n\nThe WAV file format can support various audio formats and compression schemes, but PCM(Pulse Code Modulation) is the most common. PCM is a straightforward method of digitally representing analog signals, where the amplitude of the signal is sampled at regular intervals, and each sample is represented by a binary number. PCM audio is uncompressed and provides a high-quality representation of the original audio signal.\n\nIt’s important to note that the WAV format can also support compressed audio data using codecs like ADPCM or MP3, but these are less common and not always supported universally across all WAV file readers.\n\nHere we will discover how to decode WAV files using pure Python trying to avoid external libraries, as much as possible. Generally it is much easier, to use external libraries, in your projects, but for whatever reason, if you have constraints, where the use of external libraries is not recommended then here is a method to read wav files without libraries. For verification purposes, we will also plot the Amplitude waveform, and Log Mel Spectogram using the proposed method as well as using Librosa library\n\nTo open the WAV file, we will use the inbuilt open() method of Python. We will be opening the file in ‘rb’ mode which signifies “Opening a binary file in read mode”. Since audio_file is in the format of BufferedReader, we have to print audio_file.read().\n\nThe output of the print statement should be something like the following :\n\nOn initial glance, if you correlate with the format shown in the “Structure of a WAV file” section, you can see the “RIFF” as the Chunk ID of the header chunk, “WAVE” as the Format of the header chunk, “fmt” as the Chunk ID of the format chunk, and finally “data” as the Chunk ID of the Data Chunk. Now let’s look at the hexadecimal content in between, and breakdown this WAV file manually first :\n• Chunk Size : “\\x10\\x00\\x00\\x00” represent the size of the format chunk.(16 bytes in decimal)\n• Byte Rate : “\\x10\\xb1\\x02\\x00” ( 176400 bytes per second, Little Endian)\n\nLittle Endian basically means storing data in which the least significant byte(smallest unit of data in a binary number) is stored at the lowest memory address or transmitted first.\n\nIn the above code, we use the load_audio function to load the WAV file, and extract information from it according to the format.\n\nCalling the function with the File name :\n\nOutput for the particular file being :\n\nThe data array elements are kept in float32 as WAV files loaded using Librosa are also loaded as float32. This would help, when switching from using external libraries as a proof of concept, to pure python implementation, for your Audio Processing tasks.\n\nFirst let’s plot the Amplitude Waveplot using Librosa library, using our implementation of reading audio data from the WAV file.\n\nNow, let’s plot the Amplitude Waveplot using Librosa library, while loading the Audio data using librosa.load().\n\nHence we can see, that the Amplitude values are normalized with the librosa.load() function, but the waveplot is the same. So for tasks, like Audio Classification, this would do, but if the function needs to be as close to librosa as possible, then we need to normalize the waveplot to between [-1,1].\n\nNow we try plotting the Log Mel Spectograms for both the ways, to confirm our proof of concept. Log Mel Spectogram is a audio feature, that is heavily used in Audio Signal Processing.\n\nFirst, let’s plot the Log Mel Spectogram using the Librosa Library, using our implementation of reading audio data from the WAV file.\n\nNow let’s implement the Log Mel Spectogram using the Librosa library, while loading the audio data using librosa.load() function.\n\nAs we can see, no issues here either. Both the spectograms are alike. This proves that, the reading of the WAV file is accurate, and can be utilized in your projects.\n\nWAV, MP3, and FLAC represent distinct audio formats, each with its own set of characteristics catering to different needs. MP3, employing lossy compression, achieves significantly smaller file sizes at the cost of some audio quality. This trade-off has positioned MP3 as a dominant choice for portable media and online streaming, where efficient use of storage and bandwidth is crucial.\n\nFLAC strikes a balance by offering lossless compression, providing high-quality audio with smaller file sizes compared to WAV. This makes FLAC suitable for archival storage and applications demanding uncompromised audio fidelity, albeit with more efficient file sizes than WAV.\n\nThe preference for WAV in specific applications, despite its large file sizes, underscores its irreplaceable role in preserving the pristine quality of audio recordings. As users navigate the trade-offs between file size and audio quality, understanding the strengths of each format allows for informed decisions aligning with the specific requirements of the task at hand.\n\nThe foremost challenge lies in their substantial file sizes, a consequence of eschewing compression. This drawback can prove impractical for applications constrained by storage capacity or bandwidth limitations, making alternative, compressed formats like MP3 or AAC more suitable in such scenarios. Furthermore, the absence of compression in WAV files can be inefficient for long recordings, consuming storage resources disproportionately. The consequential increase in bandwidth usage during streaming or transfers may also hinder practical applications, prompting a reevaluation of formats that balance efficiency with audio quality. Additionally, the universal support for WAV files across devices and media players is not guaranteed, potentially limiting their seamless integration into diverse ecosystems. Users navigating these limitations should weigh the benefits of lossless audio against practical considerations, choosing the appropriate audio format for their specific use case.\n\nIn the up and coming areas of AI models being applied to Speech recognition, Text to Speech, Environmental Sound Classification etc, the most optimum format to use is WAV due to it’s lossless compression. This aspect of it being lossless, helps the engineers be assured that the model is not missing out on crucial data that might have been cut during the compression.\n\nIn the realm of film and video production, WAV files are extensively employed for sound design, Foley work, and post-production audio tasks. The lossless quality of WAV ensures that crucial elements such as sound effects, dialogue, and musical scores maintain their original integrity, contributing significantly to the overall immersive quality of cinematic experiences.\n\nArchival storage also benefits greatly from WAV’s lossless compression. Museums, libraries, and archives leverage WAV files to preserve cultural artifacts, historical recordings, and important audio documents, ensuring that these resources remain intact and of the highest quality for future generations.\n\nWhether you’re an engineer, musician, or audiophile, understanding the fundamentals of WAV files empowers you to navigate the vast landscape of digital audio.\n\nIn the realm of audio storage, WAV files stand as a testament to the marriage of simplicity and quality. Hope this blog was helpful to you, in someway. Do leave a like and comment, letting me know your thoughts, and if you think there are any improvements to be made. Thanks!\n\nYou can find the entire notebook : https://github.com/Jatin7385/Reading_WAV_Files_Without_Libraries/blob/master/Reading_WAV_Files_Without_Libraries.ipynb"
    },
    {
        "link": "https://stackoverflow.com/questions/4207326/parse-wav-file-header",
        "document": "I am writing a program to parse a WAV file header and print the information to the screen. Before writing the program i am doing some research\n\nThe part between > and < in both files are the sample rate.\n\nHow does \"40 1f 00 00\" translate to 8000Hz and \"44 ac 00 00\" to 44100Hz? Information like number of channels and audio format can be read directly from the dump. I found a Python script called WavHeader that parses the sample rate correctly in both files. This is the core of the script:\n\nI do not understand how this can extract the corret sample rates, when i cannot using hexdump?\n\nI am using information about the WAV file format from this page:"
    },
    {
        "link": "https://forum.arduino.cc/t/record-audio-with-digital-i2s-mems-microphone-and-store-audio-as-wav-file-on-pc/1116333",
        "document": "I would like to use a digital I2S microphone to record audio and store it as wav on a PC, preferably over serial connection. I have tried this with the boards ESP32 dev kit v1, MKR Wifi 1010 and Adafruit Feather M0 in combination with the microphones SPH0645 or INMP441, but never succeeded.\n\nHere are the exemplary codes for the use of the ESP32 dev kit v1 with the SPH0645 from Adafruit (Overview | Adafruit I2S MEMS Microphone Breakout | Adafruit Learning System).\n\nThe above setup with the ESP32 dev kit v1 and the SPH0645 results in a wav file with the correct file structure (44 byte header etc.) but a signal that doesn't show the expected amplitudes and spectrogram features. It rather looks interrupted and distorted (see image).\n\n\n\nHas anyone tried this and could help me figure out the correct hardware and code setup to achieve a successful recording?"
    }
]