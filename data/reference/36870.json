[
    {
        "link": "https://nhtsa.gov/vehicle-safety/automated-vehicles-safety",
        "document": "What are the safety benefits of automated vehicles? Types of automated technologies, such as advanced driver assistance system technologies already in use on the roads and future automated driving systems at their mature state, have the potential to reduce crashes, prevent injuries, and save lives. In some circumstances, automated technologies may be able to detect the threat of a crash and act faster than drivers. These technologies could greatly support drivers and reduce human errors and the resulting crashes, injuries, and economic tolls.\n\nI’ve heard stories about \"self-driving\" vehicles that have crashed. Why are they on the road? There is no vehicle currently available for sale that is fully automated or \"self-driving.\" Every vehicle currently for sale in the United States requires the full attention of the driver at all times for safe operation. While an increasing number of vehicles now offer some automated features designed to assist the driver under specific conditions, these vehicles are not fully automated. Currently, states permit a limited number of “self-driving” vehicles to conduct testing, research, and pilot programs on public streets and NHTSA monitors their safety through its Standing General Order. NHTSA and USDOT are committed to overseeing the safe testing, development and deployment of these systems – currently in limited, restricted and designated locations and conditions.\n\nWhat automated features are currently available in vehicles? Many vehicles today include features that assist drivers in specific circumstances, such as keeping us from drifting out of our lane or helping us stop in time to avoid a crash or reduce its severity. Read more about this on NHTSA's safety technologies topic. If you’re currently shopping for a new vehicle, review NHTSA's 5-Star Safety Ratings to make informed decisions about the safety features included in the vehicle.\n\nHow will I know automated driving systems are safe? Vehicles are tested by the companies that build them. Companies must comply with Federal Motor Vehicle Safety Standards and certify that their vehicle is free of safety risks. Many companies today are testing vehicles with higher levels of automation to ensure that they operate as intended, but many experts indicate that more work remains to be done by developers to ensure their safe operation before they are available for consumers to purchase.\n\nWill automated vehicles be more vulnerable to hacking? Cybersecurity is a critical issue that USDOT and automotive companies are working to address for the future safe deployment of these technologies. Advanced vehicle safety technologies depend on an array of electronics, sensors, and computing power. In advancing these features and exploring the potential of full automation, USDOT and NHTSA are focused on cybersecurity to ensure that companies appropriately safeguard these systems to be resilient and work as intended. You can read more about our approach by visiting NHTSA's vehicle cybersecurity topic.\n\nIf a vehicle is driving itself, who is liable if the vehicle crashes? How is the vehicle insured? It is vital to emphasize that drivers will continue to share driving responsibilities for the foreseeable future and must remain engaged and attentive to the driving task and the road ahead with the consumer available technologies today. However, questions about liability and insurance are among many important questions, in addition to technical considerations that, policymakers are working to address before automated driving systems reach their maturity and are available to the public.\n\nI’ve seen concept automated vehicles that don’t even have a steering wheel, accelerator or brake pedal. Will I be allowed to drive my own vehicle in the future if it is automated? A vehicle that is fully automated will be capable of controlling all aspects of driving without human intervention, regardless of whether its design includes controls for an actual driver. Companies may take different design approaches to vehicles that do or do not include controls allowing for a traditional driver. As is the case now, consumers will decide what types of vehicle designs best suit their needs.\n\nWill automated vehicles help the elderly and people with disabilities who cannot drive today? Some older Americans and people with disabilities are able to drive today by adapting or modifying their vehicles to meet their specific needs. Vehicles with partial and full automation could offer new mobility options to many more people, helping them to live independently or to better connect them to jobs, education and training, and other opportunities.\n\nExplain the different terms: automated driving system, automated vehicle and \"self-driving\" vehicle. When discussing types of vehicles where a traditional driver would no longer be needed, NHTSA refers to them as automated driving systems. These types of vehicles have also been referred to as automated vehicles. NHTSA follows industry standards in not using the term \"self-driving\" to describe higher levels of automation, as this describes a vehicle's state of operation but not necessarily its capabilities and too often is falsely associated with how today's drivers need to interact with a vehicle."
    },
    {
        "link": "https://news.mit.edu/2023/exploring-methods-increasing-safety-reliability-autonomous-vehicles-0523",
        "document": ""
    },
    {
        "link": "https://nature.com/articles/s41598-024-77586-1",
        "document": "MADM necessitates the meticulous evaluation and consideration of a multitude of critical factors in order to arrive at a decision in a complex setting. Determining the optimal selection can present decision-makers with a substantial challenge. At times, they are required to manage multiple, potentially conflicting objectives in order to make a decision from the available options. The purpose of MADM is to ascertain the most desirable alternative decision from a finite set of alternatives by applying suitable methods to a collection of attributes. In addition to economics, management, and procedural and military affairs, numerous other fields have made extensive use of MADM’s techniques and principles. Aggregation operators (AOs) in MADM are demonstrated to be effective by their capacity to address practical issues that encompass a diverse array of disciplines, such as engineering, natural sciences, social and environmental studies, and related fields. Individuals typically contend that decision-making regarding a choice relies on the existence of specific numerical information elucidating the characteristics of that alternative and the significance of these attributes. Through the consolidation of multiple values into a single value within a designated collection, the AOs facilitate the final aggregate output’s amalgamation of all individual values. The intricacy of decision-making issues has increased alongside the evolution of social dynamics, requiring the utilization of sophisticated mathematical models and procedures. These advances are intended to make it easier for decision-makers to articulate preferences that are best suited to confusing situations. Numerous researchers and scientists have formulated diverse mathematical theories in order to address this ambiguity in real-world applications, including engineering, sciences, technology, medical diagnosis, etc. Zadeh1 proposed the idea of fuzzy sets (FS) in 1965 as a technique for managing ambiguous information. Kahne2, Jain3, and Dubois and Prade4 have all significantly contributed to the domain of decision-making and operations in terms of fuzzy sets. Yager5 introduced a number of AOs that can be applied to FSs. In many cases, relying just on membership degrees may fail to represent the nuanced information present in practical issue domains. With the intention of tackling this matter, Atanassov6 defined the notion of IFSs in 1986. This concept deals with a more advanced theoretical framework by specifying quantified degrees for membership and non-membership attributes such that the sum of the degrees is within the range of \\(\\:\\left[\\text{0,1}\\right]\\). Numerical representation is customarily employed when conveying data in a quantitative context. However, these precise numerical values are impractical when measuring data in a qualitative context, that is distinguished by the presence of subjective or vague information. A pragmatic approach in such circumstances entails replacing numerical quantification with linguistic evaluation. This involves the assessment of problem-domain variables via the utilization of linguistic terminology. A variety of methodologies have been devised to address challenges that involve the fusion of linguistic data, specifically in the context of MADM.\n\nThe notion of IFS has profoundly influenced various domains, including medical diagnostics, engineering, information technology, decision science, and pattern recognition. The significant influence and widespread incorporation of this theoretical framework are evident, confirming its substantial contribution to the development of methodologies and practices in the fields of science and technology in question.\n\nIn the year 19947, Chen introduced the scoring functions for IFSs. By employing the IF framework8, unveiled a methodology for identifying techniques in group decision-making. Regarding MADM in an IF scenario, Li introduced several linear programming frameworks and methods in9. In10, Xu and Yager proposed geometric AOs for the IF framework in 2006. Xu11 devised arithmetic AOs for IFS in a subsequent development in 2007. Zhao et al. presented generalized AOs on IFS to solve the MADM problems12. Implementing scenarios involving group decision-making, Xu and Wang13 formally presented the induced generalized AOs for IFS. In 2014, Huang14 investigated Hamacher AOs on IFS with the intention of constructing a methodology for decision-making. Verma15 explored a generalized method for fuzzy number IFSs and its applications in MADM. Moreover, a variety of efficacious approaches have been developed to address the difficulties linked to various MADM issues in the framework of FS and IFS knowledge, as elaborated in the cited sources16,17,18,19.\n\nIn 1982, Dombi20 methodically elaborated on a wide variety of fuzzy operators. Later on, it was proven that these operators are the most effective methods for addressing MADM challenges. The Pythagorean fuzzy Dombi AOs and the spherical fuzzy Dombi AOs were introduced in references21] and [22, respectively. In23,24,25,26, various Dombi AOs have been developed to address MADM problems under different fuzzy environments. Seikh and Mandal27 implemented Dombi operators on IFS in 2021. Karaaslan and Husseinawi28 introduced the hesitant T-sperical fuzzy Dombi AOs in 2022, in addition to discussing their implementation in MADM. Moreover, numerous advancements within the Dombi aggregation environment are evident in29,30,31,32,33,34. Zhang et al.35,36 proposed fuzzy control and discrete switched models for nonlinear and dynamic supply chain networks. The authors37 proposed a fuzzy emergency model and robust emergency strategy for the supply chain system. Sarwar and Li presented fuzzy fixed point results in their work in38. In addition, many important developments were made in the framework of fuzzy sampled-data stabilization of chaotic nonlinear systems in39. For more details on the development of fuzzy control systems, we refer to40,41,42,43,44.\n\nIn45, an innovative mechanism to address decision-making issues in the context of linguistic knowledge is introduced. Xu46 defined the notion of uncertain linguistic variables, which represent fuzzy information utilizing a linguistic interval as opposed to a singular linguistic value. In 2021, Liu et al.47 utilized LIF cubic AOs to solve MADM issues. Wei et al.48 introduced uncertain linguistic Bonferroni mean operators for MADM. Ou et al.49 employ the TOPSIS approach for linguistic MADM utilizing LIF information. In50, the authors initiated the study of linguistic picture fuzzy Dombi AOs and their application in MADM. Mehmood et al.51 put forward research work on the identification and classification of AOs using bipolar complex fuzzy settings in 2022. Meng et al.52 proposed the concept of LIFS as a methodological innovation to improve the applicability of IFSs in qualitative and complex environments. By combining linguistic models and IFSs, this integration permits the consideration of linguistic membership and non-membership degrees simultaneously. LIFSs are endowed with the ability to efficiently navigate and manage ambiguous and dubious information due to this synergistic incorporation. Many AOs in the framework of LIF knowledge, specifically LIFOWA and LIFWA, along with their respective geometric counterparts, were presented in53.\n\nResearch gaps, motivations and advantages of the current study\n\nIn contrast to conventional IFSs, LIFSs evaluate both linguistic membership and non-membership degrees simultaneously. This dual consideration facilitates a more sophisticated strategy for circumstances in which membership and non-membership data are relevant by incorporating ambiguity. Language components are incorporated into IF frameworks by LIFSs. This facilitates the comprehension and interpretation of data transfer. Quantifying language concepts corresponds with the manner in which individuals communicate uncertainty and ambiguity. The integration of linguistic models with IFSs enhances the relevance of LIFSs in complex and qualitative contexts. This feature is advantageous for decision-making systems that rely heavily on qualitative judgments and language. The above discussion motivates us to study the concept of LIF knowledge to solve MADM scenarios more appropriately in this article.\n\nDombi AOs are of tremendous importance in decision-making processes because of their exceptional flexibility and ability to simulate human-like reasoning. These operators can enable a dynamic equilibrium between optimism and pessimism in decision scenarios by seamlessly adjusting between conjunctive and disjunctive forms of aggregation through the use of a control parameter. They are particularly well-suited for MADM problems, which necessitate the meticulous management of uncertainty, imprecision, and the interdependencies between criteria. Furthermore, the precision of decision-making models is improved by their capacity to represent non-linear relationships, which results in more accurate and adaptable solutions to complex, real-world problems that involve LIFS. The significance of these operators encourages us to analyse the behaviour of Dombi AOs within the context of the LIF environment in this study.\n\nIn the automotive industry, autonomous vehicle technology has been a significant focus of research and development for the past decade. Automobile industry technologies have direct applications in the construction, mining, agricultural, maritime, and unmanned aerial vehicle sectors. Recent studies have emphasized the potential of autonomous vehicles to improve routing efficiency and message delivery within urban environments54. Prominent research and development endeavors in this field have been ongoing for the past three decades. Autonomous vehicles have been a subject of research and development ever since the inception of the automobile. Many scholarly and commercial entities are diligently striving to advance autonomous vehicles, which possess the capacity to annually preserve tens of thousands of lives and furnish substantial societal advantages. The enhancement of vehicle navigation, especially in urban areas, is further aided by the integration of multi-sensor systems, ensuring greater measurement quality control55. Distributors and supply chains frequently experience bottlenecks due to problems encountered in the last mile of delivery. Sometimes, problems still arise when a product is getting close to its final customer. Vehicle delivery services that do not require a human operator are presently being trialed by several companies. By cutting costs and minimizing disruptions, autonomous vehicles have the potential to greatly influence logistics, production facilities, and distribution. Research has shown that improved path-tracking performance of autonomous cars can be achieved through advanced control techniques, which ensures smoother and more efficient operations56. The ability of autonomous vehicles to increase safety is a significant benefit. In addition, there are significant cost benefits to using autonomous vehicles. Business operations can be made more efficient, which means labor expenses can be reduced. Efforts to improve commute experiences and route optimization, particularly for private car users, have also been bolstered by emerging technologies like blockchain-enabled multitask learning57. We can optimize operational procedures and reduce errors by reducing our reliance on human resources. Furthermore, autonomous vehicles can enhance the efficacy of transportation and logistics operations by refining routes and expediting deliveries. The utilization of independent task offloading strategies in MEC-enabled 5G Internet of Vehicles further enhances the profit potential and operational efficiency of autonomous vehicles58. The novelty of this article is to introduce an innovative approach to identifying the most effective approach for improving the efficacy of autonomous vehicle control systems on a commercial scale by incorporating Dombi ordered weighted AOs into the framework of LIF setup. By employing LIFSs to capture nuanced linguistic expressions that function as attributes and criteria for decision-making, we effectively represent and control the uncertainties and ambiguity that are inherent to autonomous driving environments.\n\nBelow is a summary of this research’s primary contributions:\n\n1) We develop a ranking criteria for LIF numbers (LIFNs) to overcome decision-making obstacles.\n\n2) We define the fundamental Dombi operations for LIFNs.\n\n3) We propose two novel Dombi AOs, LIFDOWA and LIFDOWG operators, and investigate their significant architectural characteristics.\n\n4) We use LIF Dombi ordered weighted AOs to design an algorithm for MADM problems. This entails carrying out an operational characterization of a procedural framework, which outlines methodical and precise procedures for utilizing these innovative operators in the evaluation and analysis of situations that demand complex decision-making.\n\n5) We demonstrate the practical implementation of the proposed algorithm by solving the MADM problem, where we identify the most optimal approach to enhance the efficiency of autonomous vehicle control systems on a commercial scale.\n\nThe subsequent sections of this article are systematically structured as follows: Sect. 2 delineates foundational definitions crucial for comprehending the novelty embedded in this article’s contributions. Section 3 describes two novel score and accuracy functions that are developed to tackle MADM challenges within the LIF context. Section 4 explores the structural properties of Dombi AOs, namely LIF Dombi Ordered weighted averaging and LIF Dombi Ordered weighted geometric operators. Section 5 explains the step-by-step mathematical mechanism for MADM problems in the LIF environment using the proposed strategies and illustrates the solution to the decision-making problem of selecting an optimal approach to improve the efficiency of autonomous vehicle control systems on a commercial scale. It also includes a comparative examination to demonstrate the validity and applicability of the newly defined techniques with existing methodologies. Finally, Sect. 6 encapsulates the paper with definitive conclusions drawn from the presented research."
    },
    {
        "link": "https://lids.mit.edu/news-and-events/news/exploring-new-methods-increasing-safety-and-reliability-autonomous-vehicles",
        "document": "When we think of getting on the road in our cars, our first thoughts may not be that fellow drivers are particularly safe or careful — but human drivers are more reliable than one may expect. For each fatal car crash in the United States, motor vehicles log a whopping hundred million miles on the road.\n\nHuman reliability also plays a role in how autonomous vehicles are integrated in the traffic system, especially around safety considerations. Human drivers continue to surpass autonomous vehicles in their ability to make quick decisions and perceive complex environments: Autonomous vehicles are known to struggle with seemingly common tasks, such as taking on- or off-ramps, or turning left in the face of oncoming traffic. Despite these enormous challenges, embracing autonomous vehicles in the future could yield great benefits, like clearing congested highways; enhancing freedom and mobility for non-drivers; and boosting driving efficiency, an important piece in fighting climate change.\n\nMIT engineer Cathy Wu envisions ways that autonomous vehicles could be deployed with their current shortcomings, without experiencing a dip in safety. “I started thinking more about the bottlenecks. It’s very clear that the main barrier to deployment of autonomous vehicles is safety and reliability,” Wu says.\n\nOne path forward may be to introduce a hybrid system, in which autonomous vehicles handle easier scenarios on their own, like cruising on the highway, while transferring more complicated maneuvers to remote human operators. Wu, who is a member of the Laboratory for Information and Decision Systems (LIDS), a Gilbert W. Winslow Assistant Professor of Civil and Environmental Engineering (CEE) and a member of the MIT Institute for Data, Systems, and Society (IDSS), likens this approach to air traffic controllers on the ground directing commercial aircraft.\n\nIn a paper published April 12 in IEEE Transactions on Robotics, Wu and co-authors Cameron Hickert and Sirui Li (both graduate students at LIDS) introduced a framework for how remote human supervision could be scaled to make a hybrid system efficient without compromising passenger safety. They noted that if autonomous vehicles were able to coordinate with each other on the road, they could reduce the number of moments in which humans needed to intervene.\n\nHumans and cars: finding a balance that’s just right\n\nFor the project, Wu, Hickert, and Li sought to tackle a maneuver that autonomous vehicles often struggle to complete. They decided to focus on merging, specifically when vehicles use an on-ramp to enter a highway. In real life, merging cars must accelerate or slow down in order to avoid crashing into cars already on the road. In this scenario, if an autonomous vehicle was about to merge into traffic, remote human supervisors could momentarily take control of the vehicle to ensure a safe merge. In order to evaluate the efficiency of such a system, particularly while guaranteeing safety, the team specified the maximum amount of time each human supervisor would be expected to spend on a single merge. They were interested in understanding whether a small number of remote human supervisors could successfully manage a larger group of autonomous vehicles, and the extent to which this human-to-car ratio could be improved while still safely covering every merge.\n\nWith more autonomous vehicles in use, one might assume a need for more remote supervisors. But in scenarios where autonomous vehicles coordinated with each other, the team found that cars could significantly reduce the number of times humans needed to step in. For example, a coordinating autonomous vehicle already on a highway could adjust its speed to make room for a merging car, eliminating a risky merging situation altogether.\n\nThe team substantiated the potential to safely scale remote supervision in two theorems. First, using a mathematical framework known as queuing theory, the researchers formulated an expression to capture the probability of a given number of supervisors failing to handle all merges pooled together from multiple cars. This way, the researchers were able to assess how many remote supervisors would be needed in order to cover every potential merge conflict, depending on the number of autonomous vehicles in use. The researchers derived a second theorem to quantify the influence of cooperative autonomous vehicles on surrounding traffic for boosting reliability, to assist cars attempting to merge.\n\nWhen the team modeled a scenario in which 30 percent of cars on the road were cooperative autonomous vehicles, they estimated that a ratio of one human supervisor to every 47 autonomous vehicles could cover 99.9999 percent of merging cases. But this level of coverage drops below 99 percent, an unacceptable range, in scenarios where autonomous vehicles did not cooperate with each other.\n\n“If vehicles were to coordinate and basically prevent the need for supervision, that’s actually the best way to improve reliability,” Wu says.\n\nThe team decided to focus on merging not only because it’s a challenge for autonomous vehicles, but also because it’s a well-defined task associated with a less-daunting scenario: driving on the highway. About half of the total miles traveled in the United States occur on interstates and other freeways. Since highways allow higher speeds than city roads, Wu says, “If you can fully automate highway driving … you give people back about a third of their driving time.”\n\nIf it became feasible for autonomous vehicles to cruise unsupervised for most highway driving, the challenge of safely navigating complex or unexpected moments would remain. For instance, “you [would] need to be able to handle the start and end of the highway driving,” Wu says. You would also need to be able to manage times when passengers zone out or fall asleep, making them unable to quickly take over controls should it be needed. But if remote human supervisors could guide autonomous vehicles at key moments, passengers may never have to touch the wheel. Besides merging, other challenging situations on the highway include changing lanes and overtaking slower cars on the road.\n\nAlthough remote supervision and coordinated autonomous vehicles are hypotheticals for high-speed operations, and not currently in use, Wu hopes that thinking about these topics can encourage growth in the field.\n\n“This gives us some more confidence that the autonomous driving experience can happen,” Wu says. “I think we need to be more creative about what we mean by ‘autonomous vehicles.’ We want to give people back their time — safely. We want the benefits, we don’t strictly want something that drives autonomously.”"
    },
    {
        "link": "https://parasoft.com/blog/compliance-for-autonomous-driving-software",
        "document": ""
    },
    {
        "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7506726",
        "document": "Global Status Report on Road Safety is released by World Health Organization (WHO, Geneva 27, Switzerland) 2018, in which WHO claims that about 1.35 million people die each year in road traffic accidents [1,2]. Similarly, American Automobile Association (AAA) Foundation released press report in 2016 that 50,658 vehicle roads accidents occurred only in America from the year 2011 to 2014 due to roadway obstacles. Roadway obstacles were the main factor of vehicle crashes and caused 9850 injuries and 125 deaths annually in the United States [3]. Reports indicate that over 90% of crashes are caused by errors of driver [4]. To improve this situation, governments, municipal departments, and car manufacture companies have considered significant investments to support the development of various technologies such as autonomous vehicles and cognitive robots. About 1 billion euros already have been invested by EU agencies on such type of projects [5]. In 2009, autonomous vehicles were developed and tested in four different states in the United States under the supervision of companies such as Waymo (Google, Mountain View, CA, U.S) and Uber with the support of traditional car manufacturers such as Ford and BMW [6]. Since then, this technology has been evolved and currently it is introduced in 33 different states of the United States with its specific regulations. In addition, Victoria Transport Policy Institute quoted that this technology will be widely used after 2040–2050 [7]. Currently, the most advanced features found in autonomous vehicles are Lane Changing (LC) Control [8,9], Adaptive Cruise Control (ACC) [10], Automatic Emergency Braking System (AEBS) [11], Light Detection and Ranging (LIDAR) [12], Street Sign Recognition [13], Vehicle to Vehicle (V2V) Communication [14], Object or Collision Avoidance System (CAS) [15], etc. With the continuous development of the highways, roads, city streets, and expressways, challenging problems are increasing to distinguish because the road environment is complex and constantly developing. It will be influenced by small obstacles or debris, shadows, light, water spots, and other factors. Those objects have been fallen from vehicles, construction sites or are littering. Different types of sensors, active sensors (RADAR or LIDAR) to passive sensors (camera), were used to solve this problem. Active sensors such as RADAR or LIDAR offer high precision in measuring distance and speed from point to point but they often suffer from low resolution and high costs. However, in comparison of passive sensors such as camera, its accuracy is crucial for the timely detection of small obstacles and an appropriate response by safety critical moving platforms. Detecting the small obstacle that is displayed in a very small area of the image with all possible shapes and forms is also very challenging problem. Gradient induced by the edges of obstacles can also be caused by paper or soil dust on the road or due to moisture gradient after a rain, mud, or road marking, which can be potential sources of false positives. Figure 2 describes the phenomena very well. In recent research and development, Convolutional Neural Network (CNN) models are used in autonomous vehicles to navigate safely. For example, during training, the CNN-based end to end driving model maps a relationship between the driving behavior of humans using roadway images collected from stereo camera and the steering wheel angle [16,17,18], and during testing, the CNN-models predict the steering wheel angle to navigate the autonomous vehicle safely. So that the autonomous vehicle is depended on the training dataset. If the CNN model is not trained on the roadway obstacle than navigation system of autonomous vehicle may generate incorrect information about the steering wheel angle and cause a collision in result. In addition, the number of research studies shows that the autonomous vehicle navigation system may fail to navigate safely due to several reasons, such as Radar sensor failure, camera sensor failure, and software failure [19]. For example, Tesla Model 3 failing to stop for an overturned truck and slamming right into it on highway in Taiwan even ignores the pedestrian with autopilot on. This study addresses how to improve the robustness of obstacle detection method in a complex environment, by integrating a Markov random field (MRF) for obstacles detection, road segmentation, and CNN model to navigate safely [20]. We segment out the obstacle from the image in the framework of MRF by fuses intensity gradient, curvature cues, and variance in disparity. After analyzing the obstacle from the captured image, CNN-model helps to navigate the autonomous vehicle safely. The main contributions of this study are as follows:\n• None Pixel label optimization of images as a small obstacle or hindrance on the road detected by using an MRF model.\n• None Navigating an autonomous vehicle on a roadway from unexpected obstacle. The remaining part of the research is organized as follows:\n• None Section 2—Reviews the relevant works carried out and developed in the past few years.\n• None Section 3—Introduces the method for detecting the physical obstacles or hindrances on the road and predicts the steering wheel angle for AV.\n• None Section 5—Discusses the results and its comparison.\n\nThe probability of occupation map is one of the main directions of work for obstacle detection [21]. It is developed through orthogonal projection of the 3D world onto a plane road surface (assuming that the environment structured of the road surface is almost planar). The plane is discretized in cells to form a grid; therefore, the algorithm predicts the probability of occupation of each cell. We conclude that this method can accurately detect the large obstacle (such as cyclists or cars) by using the stereo vision sensor [22] and can also help to identify the road boundaries by using LIDAR data [23]. However, since the probability function is closely related to the number of measurements in one grid, this method may not be suitable for small obstacle detection by using the stereo vision if the observation is scarce and noisy. Digital Elevation Map (DEM) [24] is one of the algorithms that tries to detect obstacles relying on the fact that they protrude up from a dominant ground surface. The obstacle detection algorithm proposed in [24] marks DEM cells as road or obstacles, using the density of 3D points as a criterion. It also involves fitting a surface model to the road surface. Oniga and Nedevschi [25] presented a random sample consensus (RANSAC)-based algorithm for road surface estimation and density base classifier for obstacle detection by using the DEM constructed on stereo data, whereas in [26], authors used a similar RANSAC-based methodology for curb detection by using the polynomial fitting in stereo DEM. Although, RANSAC-based algorithm is not suitable for detection of small obstacle because the small obstacle and variation of disparity in the image is often similar to the noise levels and a least square fitting may smooth out the small obstacle along with road surface, RANSAC shows an accurate estimate of the vehicles position relative to the road [27]. Fusion of 3D-LIDAR and high-resolution camera seems to be a reasonable approach for robust curb detection. Traditional range visual fusion methods [28,29] use the detection results from the range data to guide the curb searching in the image, which have the advantage of enlarging the detection spot. However, fusion of 3D-LIDAR and visual data search small obstacle in the visual images is more reliable in enhance the robustness of obstacle and edge detection as compared to traditional range visual fusion methods [30]. It recovers a dense depth image of the scene by propagating depth information from sparse range points to the whole high-resolution visual image [31]. J. Tan and J. Li et al. [32] used the following geometric properties to robustly curb detection such as depth image recovery, curb point detection, curb point linking and curb refinement, and parametrization. Scene flow-based approach is also used for obstacle detection [33,34], where each point in constructed 3D cloud is tracked temporally and the flow is analyzed to classify the object in the image such as road surface, obstacle, pedestrian, etc. This approach has limited applicability because it just detected the moving obstacles such as moving vehicle, bicycle, or pedestrians. In addition, the decision-making process is based on the flow of nearby points in 3D cloud, which can be too sparse for small obstacles. For obstacle detection in [35], authors used advanced stereo-based algorithms, combining full 3D data with motion-based approach, and yet it focuses only on large obstacle detection such as vehicles. Hadsell et al. [36] presented the method of vision-based neural network classifier. His idea was to use an online classifier, which is optimized for long-distance prediction and deep stereo analysis to predict the obstacles and to navigate vehicles safely. Ess A. et al. [37] defined the method of combining image-based category detectors with its geometric information received from stereo cameras (such as vehicle or pedestrian detection). However, due to the large differences in its shape, size, and visual look, it will become difficult for vision sensor to train the dataset on small obstacles along the way. Bernini and Bertozzi et al. [38] proposed an overview of several stereo-based generic obstacle detection such as Stixels algorithm [39] and geometric point clusters [40,41]. The Stixels algorithm distinguishes between a global ground surface model and a set of obstacle segments in rectangular vertical, thus, providing a compact and robust representation of the 3D scene. However, geometric relation between 3D point is used to detect and cluster obstacle point. This method is suitable for detecting medium-sized obstacle over close or medium distance. If the distance increases and the size of the obstacle decreases, then position accuracy and obstacle detection become challenging. Zhou J. and J. Li [42] proposed a solution to detect obstacles using a ground plane estimation algorithm based on homography. This work extends [43] to a smaller obstacle by combining several indicators, such as homography score, super-pixel segmentation, and line segment detector in an MRF framework. However, such indexation based on appearance such as line segment detection and pixel segmentation fail when they occurred. Therefore, we directly use the lowest level of details available, i.e., image gradients in both stereo depth images and appearance. Many researchers [44,45,46] work on the hypothesis of a flat surface assumption, free space, or the ground as a single planar and characterize the obstacles according to their height from the ground. The geometric deviation from the reference plan can be estimated directly from the image data, precalculated point cloud [47], or by extracting from v-disparity histogram model [48]. Instead of relying on the flatness of the road [49], the vertical road profile is modeled as a clothoid [50] and splines [51], which is estimated from the lateral projection of the 3D points. Similarly, free ground profile model was examined using adaptive threshold values in v-disparity domain and multiple filter steps [52]. In addition, we also explore existing datasets that are used in autonomous vehicle community. The dataset is provided by the Udacity (Mountain View, CA, USA) [53], which supports the end-to-end data and image segmentation, but it does not provide the ground truth for the obstacle in the roadway lane. Similarly, KTTI [54] and Cityscape [55] datasets also do not support the obstacle detection as a ground truth data. The dataset which matches our requirements is the Lost and Found dataset [56]. Pinggera P. and Kanzow C. have worked on planar hypothesis testing (PHT) [57,58], fast direct planar hypothesis testing (FPHT), point compatibility (PC) [40,41], Stixels [39,59], and mid-level representation: Cluster-Stixels (CStix) [60,61,62] for detecting the small obstacle in dataset. In addition, Ramos and Gehrig et al. [63] further extends this work by merging the deep learning with Lost and Found dataset hypothesis results. The people who have worked on Lost and Found dataset [56] did not discuss that how to navigate an autonomous vehicle safely or predict the steering wheel angle for an autonomous vehicle from unexpected obstacle on a roadway. Since the existing dataset is not able to meet our requirements, we have carefully created our dataset using the steering angle sensor (SAS) and ZED stereo device mounted on an electric vehicle as discuss in the experimental setup section. After that, pixel labelling of small obstacles or hindrance on the road are detected by using MRF model and road segmentation model, which help the CNN-model to navigate the autonomous vehicle on roadways from unexpected obstacle by prediction the steering wheel angle.\n\nIn this section, we will describe how to develop a safe autonomous driving system in unexpected and dangerous environments. In this section, we discussed three models.\n• In the first model, we used various stochastic techniques (such as curvature prepotential, gradient potential, and depth variance potential) to segment obstacles in the image from Markov random field (MRF) frames. These three techniques measure pixel-level images to extract useful information and store it for orientation. In this method, each pixel in the node of interest was distributed in the MRF. Finally, instead of using OR gates, we used AND gates to combine the results of previous techniques.\n• In the second model, semantic segmentation technology was used to segment paths and filter outliers and other important obstacles.\n• Third model was used to predict the steering wheel angle of the autonomous vehicle. We analyzed the unexpected obstacle on the roadway and determined the threat factor ( ). This threat factor helped us to ignore that obstacle or consider as accident risk. The overall pipeline is graphically illustrated in Figure 1. The system input consisted of optical encoders used in applications for angle detection as a SAS in vehicles and two stereo images, which were used to calculate depth using the SGBM algorithm (semi-global block matching) proposed by Hirschmuller [64]. The depth variance and color images were used to calculate three different cues, i.e., image gradient, disparity variance, and depth curvature. These three cues were combined to a unary potential in an MRF with an additional pairwise potential. Then, the standard graph cuts were used to obtain the obstacle. In addition, deep learning-based semantic segmentation [65] was used to segment the roads and filter out the abnormal values. Pipeline of detection of hazard object and prediction of steering wheel angles. During the past decade, various MRF models, inference, and learning methods have been developed to solve many low-, medium-, and high-level vision problems [66]. While inference concern underlying the image and scene structure to solve problem such as image restoration, reconstruction, segmentation, edge detection, 3D vision, and the object labeling in the image [67]. In our research work, we elaborate our query regarding small obstacle segmentation from the road by defining an energy function (such as cost) over an MRF. We associate the image with random process X with the elements , where s ϵ S represents a position of a pixel in the image. Each pixel position in the image is considered as a node in the MRF and affiliates each node with the unary or pairwise cost [68]. Minimum energy function E is defined in equation as: where represents the unary term and represents the pairwise term. is set of random variables associated with the set of nodes S that takes label ϵ [0, 1], which depends on the nature of appearance, either it is road, texture, or the small obstacle on the road. Around each pixel, we calculate the unary term independently. It is the combination of three cues such as gradient potential , depth curvature potential , and depth variance potential . The unary term is defined as follows: The potential gradient at the site (i, j) is defined as follows: Partial derivative ( and ) is calculated in the horizontal and vertical direction in the original color image [69]. In our work, we use the image gradients rather than edge detectors, because edge detectors mostly perform the thresholding on the feeble gradient response, whereas our work avoid such types of thresholding, and it builds several weak cues as a strong cue. Curvature can be defined as the reciprocal of the radius of a circle that is tangent to the given curve at a point. Curvature is “1” for a curve and “0” for a straight line. Smaller circles bend more sharply and hence have higher curvature. This feature is very useful for autonomous vehicle in multiple purposes such as free space detection, curb detection, etc. [70,71]. When performing an Eigen analysis near the 3D point under consideration, the algorithm greatly quantifies the changes along the normal surface. Equation of the curvature potential is defined: where is a curvature prior at the corresponding position obtained after reprojection the 3D point in the image space. The of the eigenvalue ( can better distinguish nonplanar surfaces. This technique is little different than the M. Pauly and M. Gross et al. [70] algorithm. The curvature prior feature is stable and robust and stable than tangent plane normal vectors because it does not make a plane surface assumption very clearly. If we slide horizontally on the image, we can calculate this potential to detect sudden changes in depth [72]. This is computed by summation of these sudden depth horizontal windows ( ) in a square window ( ) and multiplying the figured-out value with corresponding disparity in the pixel, even the obstacles are not available. Equation of depth variance is defined as: Here, the depth map is represented by D, which we obtained with the help of stereo equipment. The final variance potential is defined as follows: Here, parallax value of the specific located pixel (i, j) is represented by . The results of above three potentials are shown in Figure 2. Thus, we can see that the curvature prior potential and depth variance potential are reliable. However, adding some noisy depth values to the stereo will result in false positive values, as shown in Figure 2b,c. To get the more accurate unary potential result, as seen in Figure 2e, weighting the gradient potential in Figure 2d can pacify the problem by using the depth potential. Result extracted from capture image by applying following potentials. By using the Potts model, we define the pairwise potential : In the above equation, Kronecker delta is represented by ; whenever , the value is 1 otherwise 0. The equation of finding the global minima of energy function of the small obstacle that is detected on the road is: With the help of graph cut feature, we can find the global minima efficiently, whereas minimum graph cut is found using Boykov Y. and Kolmogorov V. et al. [73] procedure of min-cut/max-flow. Appendix A provides the pseudocode for obstacles detection by using the MRF-model. 3.6. Determination of the Obstacle Threat Value in the Image In this model, we determine the threat value ( ) by detecting obstacles in the capture image. Coordinate (x, y) of the obstacle in the capture image is identified once we combine the result of semantic segmentation model and MRK mode. We can compute the threat value ( ) by measuring the distance of the obstacles in the image size (height (h) and width (w)) from the bottom center pixel ( ). Finally, the threat value is computed from the following formula: To obtain the latitudinal and longitudinal distance of the obstacle from the center line, we subtract the height (h) with half of the width ( ) values from x and y values, respectively. Code related detection of obstacle in the captured image whose pixels size >= 50 along with it coordinate specified in Appendix A. In our research, we have implemented an autonomous driving model similar to DAVE-2, i.e., an end-to-end autonomous driving model [18]. Here, we train the weights of the network to minimize the average square error between the steering command issue. As shown in Figure 3, the network receives an obstacle detection and labeling image of 400 × 600 × 3 pixels and generates a steering wheel angle as output. This network architecture consists of 11 layers, including 1 lambda layer, 1 normalization layer, 5 convolution layers, and 4 fully connected layers. We use 5 × 5 kernel and 2 × 2 stride in the first three convolution layers and 3 × 3 kernel and 1 × 1 stride used in last two convolution layers. The whole network contains 7,970,619 trainable parameters. We train autonomous vehicle models based on the result of obstacle detection by MRF and SAS and then test their performance through prediction of steering wheel angle as describe in Algorithm 1. After training, our well-trained autonomous vehicle model detects high threat value obstacle on the roadway and navigate the autonomous vehicle safely. Appendix A shows the model generation code used for steering wheel angle prediction. Image normalization to avoid saturation and make gradients work better.\n\nSince little attention has been paid to finding small obstacles in the past, no reasonable dataset is available for public use. Therefore, we created a new small dataset for obstacle detection. The dataset contains 5626 stereo images. These images are obtained from ZED stereo device, which were installed on electric vehicles in various environments. ZED camera can create the real-time point cloud at a framerate of 30 fps with the resolution of 2 × (1280 × 720), and we extracted them from different video sequences of electric vehicle view, while running on road. Specification of the ZED camera is presented in Table 1. Optical encoders or separate arrays of LEDs and photodiodes are used as steering angle sensor (SAS) in vehicle angle sensing applications [74,75]. Accurate and reliable steering angle detection is the main challenge of modern vehicles. This data is used not only for electronic power steering (EPS) and electronic stability control (ECS) but also for controlling adaptive headlights, lane keeping assist system, or other advanced driver assistance systems (ADAS). Optical steering angle sensor consists of an LED and a photodiode array or a CMOS line sensor. The sensor output is a digital square wave signal whose frequency depends on the speed of the turning wheel. Thus, the sensor can help determine the actual angle of rotation (ground truth value), which allows us to compare it with our proposed model. There are some mathematical tools that can adjust the steering angle values between different vehicles according to the formula of Equation (10): where “R” is the radius of curvature, “a” is the steering wheel angle, “n” is the steering ratio, and “s” is the steering wheel frame. The onboard computer is NVIDA Jetson TX2. In the training phase, the captured images from ZED stereo cameras associated with steering angle are sent to this main computer and saved frame by frame as a custom image object. Later, the training can be accomplished offline using a more powerful computer than the onboard computer. According to the steering angle sensor, we have many zero steering values and only a few left and right steering values. In fact, we mostly go straight, if we put all of them equal, then it will go left and right frequently. We cutoff 4402 images with zero values, which help us to balance the dataset, and the remaining 1224 stereo images are used to train the model as shown in Figure 4. Balancing of the dataset shown in histogram graph. Many obstacles can be found in this dataset, such as bricks, dirt, newspapers, animals, and pedestrians, and even, we can see the nature of the road to provide driving assistance and more information. Naturally, there are many obstacles on the road, while we have deliberately placed a few obstacles to further complicate our dataset. There are total of 52 obstacles found in this dataset with variable height (8–45 cm). In addition, we also create the ground truth by manually driving the electric vehicle on the roadways. In order to obtain satisfactory driving model performance, it is necessary to train the model on multiple training dataset. Increasing the size of existing data through affine transformation [76], specifically by random change in contrast or brightness, horizontal flipping of the image, and random rotation. After development of autonomous vehicle driving model, we use the augmented dataset for training. The pseudocode for augmented dataset is included in Appendix A. This dataset is divided into two parts, 80% for training and 20% for validation, as shown in Table 2.\n\nWe can use the following quantitative indicators to analyze the performance of the model: root mean square error (RMSE) and mean absolute error (MAE), as well as qualitative indicators through visualization: appearance challenges, distance and size challenges, clustering and shape challenges, and prediction of steering wheel angle challenges. Through our method and the measurement of RMSE and MAE, quantitative comparisons can be made to successfully identifying the obstacles. By comparing the RMSE and MAE values, we can predict the angle of the steering wheel. If the number of pixels of the obstacle is greater than the 50 pixels marked by the MRF model, the obstacle can be identified. If the pixel size of the segmented obstacle is higher than 50 pixels, it will be marked as a false alarm or high threat value and label it on the autonomous vehicle lane. After successfully detecting the obstacles on the roadway by using the MRF model, we extracted the following two errors, root mean square error (RMSE) and mean absolute error (MAE), by which we can compare ground truth value with predicting steering wheel angle. For extracting these errors, we used following equations: Total number of images in the dataset is represented by “N”, ground truth represented by “G ”, and the steering wheel angle represented by “P ” for the image ith of the dataset. We find out the RMSE and MAE values in different weather conditions and different times of the day and compare it with the ground truth value of the electric vehicle. In Figure 5, RMSE and MAE values are low in the following situation: daytime, shadow, and street light, while the error values are high in the following situations: night and rainy weather. The RMSE and MAE values are less than the value indicating that the predicted steering wheel angle closely follows the ground truth values. Measurement of root mean square error (RMSE) and mean absolute error (MAE) in different situation. The Frenet Coordinate System [77] is used to analyze the situation avoiding the obstacles on the roadway of autonomous vehicle. We did not follow the Frenet Coordinate System for performance evaluation, but compared the ground truth value and RMSE value with time steps from 72,000 to 82,000 ms (represented on x-axis) and latitude (represented on y-axis). It can help us to understand that how autonomous vehicle closely follow the ground truth trajectory data and avoid the obstacle on the road as shown in Figure 6. As we can see in Figure 6, in the following situation: daytime, shadow, and under street light, the autonomous driving model closely follows the ground truth values while difference between the night and rainy weather is high. When camera performance declines at night or in rainy weather, it is a challenge to identify obstacles in such situations. Appendix A mentions code related to extracting the ground truth and predicting RMSE values of steering wheel for comparison. Comparison of the trajectory of the ground truth value with the RMSE value in different situation. Here, we will examine methods implemented for proposed dataset. We categorize them based on challenges related to appearance, distance and size, cluttering and shape, and steering wheel angle prediction. The appearance is challenging to see in two ways. First, the deceptive texture by dirt or mud on the road due to rain or flat object (such as newspaper and cardboard box) is very different from other objects (such as rock and brick) that may mislead autonomous vehicle. Second, the appearance of obstacles seems to be its background. It is very challenging to detect obstacle due to week intensity gradient. However, the method we propose can solve the above problems and robustly extract obstacles. As you can see in Figure 7d, we can accurately identify obstacle even if the road is wet. Likewise, it can avoid the objects such as mud and shadow in Figure 7c. It also helps to identify obstacles that look like road (obstacle 4 in Figure 7d). Our method can also take into account local changes in the road structure and even applies to cement floors (Figure 7a,c). Obstacles in the captured image successfully identified by Markov random field (MRF) model. As we all know, stereo depth is unreliable after the range of 10–12 m, which makes obstacle detection challenging at long distance. Detecting the small obstacles from distance, large changes in noise, or light reflection can cause serious problem. Our model solves these two problems well. As shown in Figure 7e, label 1 shows that our method has detected a 9 cm height obstacle at a distance of 8 m. Similarly, in Figure 7a, an obstacle of 7 cm height is detected at a distance of 7 m. Obstacle appear on the road in random shapes. Therefore, it makes sense to only allow certain shapes to use for modeled. However, our proposed algorithm has advantages in this regard because it does not require any assumptions about the shape or appearance of obstacles and always performs reliable detection. We can be seen rocks and tires in Figure 7b,c, respectively. The rule-based classification method is used to divide all obstacles into two categories: (a) the ones to be ignored (the pixel size of obstacle is less than 50) and (b) which causes of vehicle to collide (the pixel size of the obstacle is greater than 50). We use MRF model and segmented image to calculate the threat value using equation 9 as discuss in methodology section. After pixel segmentation of obstacles, the coordinates of the risky obstacles in the image are obtained. After collecting the data, we prepare the image dataset for end-to-end driving model training by normalizing and resizing of the images [78,79]. As shown in Figure 8, the steering wheel angle is normalized between the −1.0 to +1.0, where positive values represent right rotation and a negative value represents the left rotation of the steering wheel. We normalize the values by using the liner transformation Equation (13): Comparison of ground truth steering wheel angle (real) with predicting steering wheel angle by model (predict). Among them, is the normalized steering wheel angle between −1.0 and +1.0, is steering wheel angle in radians, and are the minimum and maximum steering wheel angle, respectively. In the Figure 8, we have plotted the “steering angle” and “image sequence” on the y-axis and x-axis, respectively. Figure 8 shows that how closely proposed model follows the ground truth values. Using our model, we conducted a statistical significance test between the ground truth values and prediction values; we obtained a 91.5% confidence interval. The accuracy of the prediction can be qualitatively represented by observing the direction of the driving angle of an autonomous vehicle as shown in Figure 9. As shown in Figure 9, when there are obstacles on the road, our proposed model can predict that the angle of the steering wheel will be closer to the ground truth angle. In Figure 9, the green line represents the ground truth angle and red line represents the expected angle of the steering wheel of our proposed model. Comparison of ground truth angle with proposed model angle with random dataset images in different environment.\n\nCrop the image (removing the sky at the top and the car front at the bottom) Resize the image to the input shape used by the network model Convert the image from RGB to YUV (This is what the NVIDIA model does.) Randomly flip the image left <-> right, and adjust the steering angle. Combine all required results 1, 2 and 3 by AND Gate #image location of an object seen by the left and right image of a stereo pair #Calculate the signed curvature of a 2D curve at each point using interpolating splines. #In the second case the curve is represented as a np.array of complex numbers. #The admissible error when interpolating the splines #Detect the obstacle along with coordinate on the roadway whose pixel >= 50 # Plot all the time sections of steering data"
    },
    {
        "link": "https://mdpi.com/1424-8220/20/17/4719",
        "document": "Global Status Report on Road Safety is released by World Health Organization (WHO, Geneva 27, Switzerland) 2018, in which WHO claims that about 1.35 million people die each year in road traffic accidents [ 1 2 ]. Similarly, American Automobile Association (AAA) Foundation released press report in 2016 that 50,658 vehicle roads accidents occurred only in America from the year 2011 to 2014 due to roadway obstacles. Roadway obstacles were the main factor of vehicle crashes and caused 9850 injuries and 125 deaths annually in the United States [ 3 ]. Reports indicate that over 90% of crashes are caused by errors of driver [ 4 ]. To improve this situation, governments, municipal departments, and car manufacture companies have considered significant investments to support the development of various technologies such as autonomous vehicles and cognitive robots. About 1 billion euros already have been invested by EU agencies on such type of projects [ 5 ]. In 2009, autonomous vehicles were developed and tested in four different states in the United States under the supervision of companies such as Waymo (Google, Mountain View, CA, U.S) and Uber with the support of traditional car manufacturers such as Ford and BMW [ 6 ]. Since then, this technology has been evolved and currently it is introduced in 33 different states of the United States with its specific regulations. In addition, Victoria Transport Policy Institute quoted that this technology will be widely used after 2040–2050 [ 7 ]. Currently, the most advanced features found in autonomous vehicles are Lane Changing (LC) Control [ 8 9 ], Adaptive Cruise Control (ACC) [ 10 ], Automatic Emergency Braking System (AEBS) [ 11 ], Light Detection and Ranging (LIDAR) [ 12 ], Street Sign Recognition [ 13 ], Vehicle to Vehicle (V2V) Communication [ 14 ], Object or Collision Avoidance System (CAS) [ 15 ], etc. With the continuous development of the highways, roads, city streets, and expressways, challenging problems are increasing to distinguish because the road environment is complex and constantly developing. It will be influenced by small obstacles or debris, shadows, light, water spots, and other factors. Those objects have been fallen from vehicles, construction sites or are littering. Different types of sensors, active sensors (RADAR or LIDAR) to passive sensors (camera), were used to solve this problem. Active sensors such as RADAR or LIDAR offer high precision in measuring distance and speed from point to point but they often suffer from low resolution and high costs. However, in comparison of passive sensors such as camera, its accuracy is crucial for the timely detection of small obstacles and an appropriate response by safety critical moving platforms. Detecting the small obstacle that is displayed in a very small area of the image with all possible shapes and forms is also very challenging problem. Gradient induced by the edges of obstacles can also be caused by paper or soil dust on the road or due to moisture gradient after a rain, mud, or road marking, which can be potential sources of false positives. Figure 2 describes the phenomena very well. 17, In recent research and development, Convolutional Neural Network (CNN) models are used in autonomous vehicles to navigate safely. For example, during training, the CNN-based end to end driving model maps a relationship between the driving behavior of humans using roadway images collected from stereo camera and the steering wheel angle [ 16 18 ], and during testing, the CNN-models predict the steering wheel angle to navigate the autonomous vehicle safely. So that the autonomous vehicle is depended on the training dataset. If the CNN model is not trained on the roadway obstacle than navigation system of autonomous vehicle may generate incorrect information about the steering wheel angle and cause a collision in result. In addition, the number of research studies shows that the autonomous vehicle navigation system may fail to navigate safely due to several reasons, such as Radar sensor failure, camera sensor failure, and software failure [ 19 ]. For example, Tesla Model 3 failing to stop for an overturned truck and slamming right into it on highway in Taiwan even ignores the pedestrian with autopilot on. This study addresses how to improve the robustness of obstacle detection method in a complex environment, by integrating a Markov random field (MRF) for obstacles detection, road segmentation, and CNN model to navigate safely [ 20 ]. We segment out the obstacle from the image in the framework of MRF by fuses intensity gradient, curvature cues, and variance in disparity. After analyzing the obstacle from the captured image, CNN-model helps to navigate the autonomous vehicle safely. The main contributions of this study are as follows:\n• None Pixel label optimization of images as a small obstacle or hindrance on the road detected by using an MRF model.\n• None Navigating an autonomous vehicle on a roadway from unexpected obstacle. The remaining part of the research is organized as follows:\n• None —Reviews the relevant works carried out and developed in the past few years. —Reviews the relevant works carried out and developed in the past few years. Section 2 —Reviews the relevant works carried out and developed in the past few years.\n• None —Introduces the method for detecting the physical obstacles or hindrances on the road and predicts the steering wheel angle for AV. —Introduces the method for detecting the physical obstacles or hindrances on the road and predicts the steering wheel angle for AV. Section 3 —Introduces the method for detecting the physical obstacles or hindrances on the road and predicts the steering wheel angle for AV.\n• None —Discusses the results and its comparison. —Discusses the results and its comparison. Section 5 —Discusses the results and its comparison.\n\nThe probability of occupation map is one of the main directions of work for obstacle detection [ 21 ]. It is developed through orthogonal projection of the 3D world onto a plane road surface (assuming that the environment structured of the road surface is almost planar). The plane is discretized in cells to form a grid; therefore, the algorithm predicts the probability of occupation of each cell. We conclude that this method can accurately detect the large obstacle (such as cyclists or cars) by using the stereo vision sensor [ 22 ] and can also help to identify the road boundaries by using LIDAR data [ 23 ]. However, since the probability function is closely related to the number of measurements in one grid, this method may not be suitable for small obstacle detection by using the stereo vision if the observation is scarce and noisy. Digital Elevation Map (DEM) [ 24 ] is one of the algorithms that tries to detect obstacles relying on the fact that they protrude up from a dominant ground surface. The obstacle detection algorithm proposed in [ 24 ] marks DEM cells as road or obstacles, using the density of 3D points as a criterion. It also involves fitting a surface model to the road surface. Oniga and Nedevschi [ 25 ] presented a random sample consensus (RANSAC)-based algorithm for road surface estimation and density base classifier for obstacle detection by using the DEM constructed on stereo data, whereas in [ 26 ], authors used a similar RANSAC-based methodology for curb detection by using the polynomial fitting in stereo DEM. Although, RANSAC-based algorithm is not suitable for detection of small obstacle because the small obstacle and variation of disparity in the image is often similar to the noise levels and a least square fitting may smooth out the small obstacle along with road surface, RANSAC shows an accurate estimate of the vehicles position relative to the road [ 27 ]. Fusion of 3D-LIDAR and high-resolution camera seems to be a reasonable approach for robust curb detection. Traditional range visual fusion methods [ 28 29 ] use the detection results from the range data to guide the curb searching in the image, which have the advantage of enlarging the detection spot. However, fusion of 3D-LIDAR and visual data search small obstacle in the visual images is more reliable in enhance the robustness of obstacle and edge detection as compared to traditional range visual fusion methods [ 30 ]. It recovers a dense depth image of the scene by propagating depth information from sparse range points to the whole high-resolution visual image [ 31 ]. J. Tan and J. Li et al. [ 32 ] used the following geometric properties to robustly curb detection such as depth image recovery, curb point detection, curb point linking and curb refinement, and parametrization. Scene flow-based approach is also used for obstacle detection [ 33 34 ], where each point in constructed 3D cloud is tracked temporally and the flow is analyzed to classify the object in the image such as road surface, obstacle, pedestrian, etc. This approach has limited applicability because it just detected the moving obstacles such as moving vehicle, bicycle, or pedestrians. In addition, the decision-making process is based on the flow of nearby points in 3D cloud, which can be too sparse for small obstacles. For obstacle detection in [ 35 ], authors used advanced stereo-based algorithms, combining full 3D data with motion-based approach, and yet it focuses only on large obstacle detection such as vehicles. Hadsell et al. [ 36 ] presented the method of vision-based neural network classifier. His idea was to use an online classifier, which is optimized for long-distance prediction and deep stereo analysis to predict the obstacles and to navigate vehicles safely. Ess A. et al. [ 37 ] defined the method of combining image-based category detectors with its geometric information received from stereo cameras (such as vehicle or pedestrian detection). However, due to the large differences in its shape, size, and visual look, it will become difficult for vision sensor to train the dataset on small obstacles along the way. Bernini and Bertozzi et al. [ 38 ] proposed an overview of several stereo-based generic obstacle detection such as Stixels algorithm [ 39 ] and geometric point clusters [ 40 41 ]. The Stixels algorithm distinguishes between a global ground surface model and a set of obstacle segments in rectangular vertical, thus, providing a compact and robust representation of the 3D scene. However, geometric relation between 3D point is used to detect and cluster obstacle point. This method is suitable for detecting medium-sized obstacle over close or medium distance. If the distance increases and the size of the obstacle decreases, then position accuracy and obstacle detection become challenging. Zhou J. and J. Li [ 42 ] proposed a solution to detect obstacles using a ground plane estimation algorithm based on homography. This work extends [ 43 ] to a smaller obstacle by combining several indicators, such as homography score, super-pixel segmentation, and line segment detector in an MRF framework. However, such indexation based on appearance such as line segment detection and pixel segmentation fail when they occurred. Therefore, we directly use the lowest level of details available, i.e., image gradients in both stereo depth images and appearance. 45, Many researchers [ 44 46 ] work on the hypothesis of a flat surface assumption, free space, or the ground as a single planar and characterize the obstacles according to their height from the ground. The geometric deviation from the reference plan can be estimated directly from the image data, precalculated point cloud [ 47 ], or by extracting from v-disparity histogram model [ 48 ]. Instead of relying on the flatness of the road [ 49 ], the vertical road profile is modeled as a clothoid [ 50 ] and splines [ 51 ], which is estimated from the lateral projection of the 3D points. Similarly, free ground profile model was examined using adaptive threshold values in v-disparity domain and multiple filter steps [ 52 ]. In addition, we also explore existing datasets that are used in autonomous vehicle community. The dataset is provided by the Udacity (Mountain View, CA, USA) [ 53 ], which supports the end-to-end data and image segmentation, but it does not provide the ground truth for the obstacle in the roadway lane. Similarly, KTTI [ 54 ] and Cityscape [ 55 ] datasets also do not support the obstacle detection as a ground truth data. The dataset which matches our requirements is the Lost and Found dataset [ 56 ]. 61, Pinggera P. and Kanzow C. have worked on planar hypothesis testing (PHT) [ 57 58 ], fast direct planar hypothesis testing (FPHT), point compatibility (PC) [ 40 41 ], Stixels [ 39 59 ], and mid-level representation: Cluster-Stixels (CStix) [ 60 62 ] for detecting the small obstacle in dataset. In addition, Ramos and Gehrig et al. [ 63 ] further extends this work by merging the deep learning with Lost and Found dataset hypothesis results. The people who have worked on Lost and Found dataset [ 56 ] did not discuss that how to navigate an autonomous vehicle safely or predict the steering wheel angle for an autonomous vehicle from unexpected obstacle on a roadway. Since the existing dataset is not able to meet our requirements, we have carefully created our dataset using the steering angle sensor (SAS) and ZED stereo device mounted on an electric vehicle as discuss in the experimental setup section. After that, pixel labelling of small obstacles or hindrance on the road are detected by using MRF model and road segmentation model, which help the CNN-model to navigate the autonomous vehicle on roadways from unexpected obstacle by prediction the steering wheel angle.\n\nIn this section, we will describe how to develop a safe autonomous driving system in unexpected and dangerous environments. In this section, we discussed three models. In the first model, we used various stochastic techniques (such as curvature prepotential, gradient potential, and depth variance potential) to segment obstacles in the image from Markov random field (MRF) frames. These three techniques measure pixel-level images to extract useful information and store it for orientation. In this method, each pixel in the node of interest was distributed in the MRF. Finally, instead of using OR gates, we used AND gates to combine the results of previous techniques. In the second model, semantic segmentation technology was used to segment paths and filter outliers and other important obstacles. Third model was used to predict the steering wheel angle of the autonomous vehicle. We analyzed the unexpected obstacle on the roadway and determined the threat factor ( ). This threat factor helped us to ignore that obstacle or consider as accident risk. The overall pipeline is graphically illustrated in Figure 1 . The system input consisted of optical encoders used in applications for angle detection as a SAS in vehicles and two stereo images, which were used to calculate depth using the SGBM algorithm (semi-global block matching) proposed by Hirschmuller [ 64 ]. The depth variance and color images were used to calculate three different cues, i.e., image gradient, disparity variance, and depth curvature. These three cues were combined to a unary potential in an MRF with an additional pairwise potential. Then, the standard graph cuts were used to obtain the obstacle. In addition, deep learning-based semantic segmentation [ 65 ] was used to segment the roads and filter out the abnormal values. with the elements , where s ϵ S represents a position of a pixel in the image. Each pixel position in the image is considered as a node in the MRF and affiliates each node with the unary or pairwise cost [ where represents the unary term and represents the pairwise term. is set of random variables associated with the set of nodes S that takes label ϵ [0, 1], which depends on the nature of appearance, either it is road, texture, or the small obstacle on the road. Around each pixel, we calculate the unary term independently. It is the combination of three cues such as gradient potential , depth curvature potential , and depth variance potential . The unary term is defined as follows: During the past decade, various MRF models, inference, and learning methods have been developed to solve many low-, medium-, and high-level vision problems [ 66 ]. While inference concern underlying the image and scene structure to solve problem such as image restoration, reconstruction, segmentation, edge detection, 3D vision, and the object labeling in the image [ 67 ]. In our research work, we elaborate our query regarding small obstacle segmentation from the road by defining an energy function (such as cost) over an MRF. We associate the image with random processwith the elements, where s ϵ S represents a position of a pixel in the image. Each pixel position in the image is considered as a node in the MRF and affiliates each node with the unary or pairwise cost [ 68 ]. Minimum energy function E is defined in equation as:whererepresents the unary term andrepresents the pairwise term.is set of random variables associated with the set of nodes S that takes labelϵ [0, 1], which depends on the nature of appearance, either it is road, texture, or the small obstacle on the road. Around each pixel, we calculate the unary termindependently. It is the combination of three cues such as gradient potential, depth curvature potential, and depth variance potential. The unary termis defined as follows: , ) is defined as follows: The potential gradient at the site () is defined as follows: and ) is calculated in the horizontal and vertical direction in the original color image [ Partial derivative (and) is calculated in the horizontal and vertical direction in the original color image [ 69 ]. In our work, we use the image gradients rather than edge detectors, because edge detectors mostly perform the thresholding on the feeble gradient response, whereas our work avoid such types of thresholding, and it builds several weak cues as a strong cue. where is a curvature prior at the corresponding position obtained after reprojection the 3D point in the image space. The of the eigenvalue ( can better distinguish nonplanar surfaces. This technique is little different than the M. Pauly and M. Gross et al. [ Curvature can be defined as the reciprocal of the radius of a circle that is tangent to the given curve at a point. Curvature is “1” for a curve and “0” for a straight line. Smaller circles bend more sharply and hence have higher curvature. This feature is very useful for autonomous vehicle in multiple purposes such as free space detection, curb detection, etc. [ 70 71 ]. When performing an Eigen analysis near the 3D point under consideration, the algorithm greatly quantifies the changes along the normal surface. Equation of the curvature potential is defined:whereis a curvature prior at the corresponding position obtained after reprojection the 3D point in the image space. Theof the eigenvalue (can better distinguish nonplanar surfaces. This technique is little different than the M. Pauly and M. Gross et al. [ 70 ] algorithm. The curvature prior feature is stable and robust and stable than tangent plane normal vectors because it does not make a plane surface assumption very clearly. ) in a square window ( ) and multiplying the figured-out value with corresponding disparity in the pixel, even the obstacles are not available. Equation of depth variance is defined as: If we slide horizontally on the image, we can calculate this potential to detect sudden changes in depth [ 72 ]. This is computed by summation of these sudden depth horizontal windows () in a square window () and multiplying the figured-out value with corresponding disparity in the pixel, even the obstacles are not available. Equation of depth variance is defined as: , which we obtained with the help of stereo equipment. The final variance potential is defined as follows: Here, the depth map is represented by, which we obtained with the help of stereo equipment. The final variance potential is defined as follows: Here, parallax value of the specific located pixel ( , ) is represented by . The results of above three potentials are shown in Figure 2 . Thus, we can see that the curvature prior potential and depth variance potential are reliable. However, adding some noisy depth values to the stereo will result in false positive values, as shown in Figure 2 b,c. To get the more accurate unary potential result, as seen in Figure 2 e, weighting the gradient potential in Figure 2 d can pacify the problem by using the depth potential. : By using the Potts model, we define the pairwise potential ; whenever , the value is 1 otherwise 0. The equation of finding the global minima of energy function of the small obstacle that is detected on the road is: In the above equation, Kronecker delta is represented by; whenever, the value is 1 otherwise 0. The equation of finding the global minima of energy function of the small obstacle that is detected on the road is: With the help of graph cut feature, we can find the global minima efficiently, whereas minimum graph cut is found using Boykov Y. and Kolmogorov V. et al. [ 73 ] procedure of min-cut/max-flow. Appendix A provides the pseudocode for obstacles detection by using the MRF-model. 3.6. Determination of the Obstacle Threat Value in the Image ) by detecting obstacles in the capture image. Coordinate ( ) of the obstacle in the capture image is identified once we combine the result of semantic segmentation model and MRK mode. We can compute the threat value ( ) by measuring the distance of the obstacles in the image size ( ( ) and ( )) from the bottom center pixel ( ). Finally, the threat value is computed from the following formula: In this model, we determine the threat value () by detecting obstacles in the capture image. Coordinate () of the obstacle in the capture image is identified once we combine the result of semantic segmentation model and MRK mode. We can compute the threat value () by measuring the distance of the obstacles in the image size () and)) from the bottom center pixel (). Finally, the threat value is computed from the following formula: ) with half of the width ( ) values from x and y values, respectively. Code related detection of obstacle in the captured image whose pixels size >= 50 along with it coordinate specified in To obtain the latitudinal and longitudinal distance of the obstacle from the center line, we subtract the height () with half of the width () values from x and y values, respectively. Code related detection of obstacle in the captured image whose pixels size >= 50 along with it coordinate specified in Appendix A In our research, we have implemented an autonomous driving model similar to DAVE-2, i.e., an end-to-end autonomous driving model [ 18 ]. Here, we train the weights of the network to minimize the average square error between the steering command issue. As shown in Figure 3 , the network receives an obstacle detection and labeling image of 400 × 600 × 3 pixels and generates a steering wheel angle as output. This network architecture consists of 11 layers, including 1 lambda layer, 1 normalization layer, 5 convolution layers, and 4 fully connected layers. We use 5 × 5 kernel and 2 × 2 stride in the first three convolution layers and 3 × 3 kernel and 1 × 1 stride used in last two convolution layers. The whole network contains 7,970,619 trainable parameters. Image normalization to avoid saturation and make gradients work better. We train autonomous vehicle models based on the result of obstacle detection by MRF and SAS and then test their performance through prediction of steering wheel angle as describe in Algorithm 1. After training, our well-trained autonomous vehicle model detects high threat value obstacle on the roadway and navigate the autonomous vehicle safely. Appendix A shows the model generation code used for steering wheel angle prediction.\n\n, which were installed on electric vehicles in various environments. ZED camera can create the real-time point cloud at a framerate of 30 fps with the resolution of 2 × (1280 × 720), and we extracted them from different video sequences of electric vehicle view, while running on road. Specification of the ZED camera is presented in Since little attention has been paid to finding small obstacles in the past, no reasonable dataset is available for public use. Therefore, we created a new small dataset for obstacle detection. The dataset contains 5626 stereo images. These images are obtained from, which were installed on electric vehicles in various environments. ZED camera can create the real-time point cloud at a framerate of 30 fps with the resolution of 2 × (1280 × 720), and we extracted them from different video sequences of electric vehicle view, while running on road. Specification of the ZED camera is presented in Table 1 or separate arrays of LEDs and photodiodes are used as steering angle sensor (SAS) in vehicle angle sensing applications [ where “R” is the radius of curvature, “ ” is the steering wheel angle, “ ” is the steering ratio, and “ ” is the steering wheel frame. The onboard computer is NVIDA Jetson TX2. In the training phase, the captured images from ZED stereo cameras associated with steering angle are sent to this main computer and saved frame by frame as a custom image object. Later, the training can be accomplished offline using a more powerful computer than the onboard computer. or separate arrays of LEDs and photodiodes are used as steering angle sensor (SAS) in vehicle angle sensing applications [ 74 75 ]. Accurate and reliable steering angle detection is the main challenge of modern vehicles. This data is used not only for electronic power steering (EPS) and electronic stability control (ECS) but also for controlling adaptive headlights, lane keeping assist system, or other advanced driver assistance systems (ADAS). Optical steering angle sensor consists of an LED and a photodiode array or a CMOS line sensor. The sensor output is a digital square wave signal whose frequency depends on the speed of the turning wheel. Thus, the sensor can help determine the actual angle of rotation (ground truth value), which allows us to compare it with our proposed model. There are some mathematical tools that can adjust the steering angle values between different vehicles according to the formula of Equation (10):where “R” is the radius of curvature, “” is the steering wheel angle, “” is the steering ratio, and “” is the steering wheel frame. The onboard computer is NVIDA Jetson TX2. In the training phase, the captured images from ZED stereo cameras associated with steering angle are sent to this main computer and saved frame by frame as a custom image object. Later, the training can be accomplished offline using a more powerful computer than the onboard computer. According to the steering angle sensor, we have many zero steering values and only a few left and right steering values. In fact, we mostly go straight, if we put all of them equal, then it will go left and right frequently. We cutoff 4402 images with zero values, which help us to balance the dataset, and the remaining 1224 stereo images are used to train the model as shown in Figure 4 Many obstacles can be found in this dataset, such as bricks, dirt, newspapers, animals, and pedestrians, and even, we can see the nature of the road to provide driving assistance and more information. Naturally, there are many obstacles on the road, while we have deliberately placed a few obstacles to further complicate our dataset. There are total of 52 obstacles found in this dataset with variable height (8–45 cm). In addition, we also create the ground truth by manually driving the electric vehicle on the roadways. In order to obtain satisfactory driving model performance, it is necessary to train the model on multiple training dataset. Increasing the size of existing data through affine transformation [ 76 ], specifically by random change in contrast or brightness, horizontal flipping of the image, and random rotation. After development of autonomous vehicle driving model, we use the augmented dataset for training. The pseudocode for augmented dataset is included in Appendix A . This dataset is divided into two parts, 80% for training and 20% for validation, as shown in Table 2\n\nWe can use the following quantitative indicators to analyze the performance of the model: root mean square error (RMSE) and mean absolute error (MAE), as well as qualitative indicators through visualization: appearance challenges, distance and size challenges, clustering and shape challenges, and prediction of steering wheel angle challenges. Through our method and the measurement of RMSE and MAE, quantitative comparisons can be made to successfully identifying the obstacles. By comparing the RMSE and MAE values, we can predict the angle of the steering wheel. If the number of pixels of the obstacle is greater than the 50 pixels marked by the MRF model, the obstacle can be identified. If the pixel size of the segmented obstacle is higher than 50 pixels, it will be marked as a false alarm or high threat value and label it on the autonomous vehicle lane. After successfully detecting the obstacles on the roadway by using the MRF model, we extracted the following two errors, root mean square error (RMSE) and mean absolute error (MAE), by which we can compare ground truth value with predicting steering wheel angle. For extracting these errors, we used following equations: Total number of images in the dataset is represented by “ ”, ground truth represented by “ ”, and the steering wheel angle represented by “ ” for the image of the dataset. We find out the RMSE and MAE values in different weather conditions and different times of the day and compare it with the ground truth value of the electric vehicle. In Figure 5 , RMSE and MAE values are low in the following situation: daytime, shadow, and street light, while the error values are high in the following situations: night and rainy weather. The RMSE and MAE values are less than the value indicating that the predicted steering wheel angle closely follows the ground truth values. -axis) and latitude (represented on -axis). It can help us to understand that how autonomous vehicle closely follow the ground truth trajectory data and avoid the obstacle on the road as shown in The Frenet Coordinate System [ 77 ] is used to analyze the situation avoiding the obstacles on the roadway of autonomous vehicle. We did not follow the Frenet Coordinate System for performance evaluation, but compared the ground truth value and RMSE value with time steps from 72,000 to 82,000 ms (represented on-axis) and latitude (represented on-axis). It can help us to understand that how autonomous vehicle closely follow the ground truth trajectory data and avoid the obstacle on the road as shown in Figure 6 . As we can see in Figure 6 , in the following situation: daytime, shadow, and under street light, the autonomous driving model closely follows the ground truth values while difference between the night and rainy weather is high. When camera performance declines at night or in rainy weather, it is a challenge to identify obstacles in such situations. Appendix A mentions code related to extracting the ground truth and predicting RMSE values of steering wheel for comparison. Here, we will examine methods implemented for proposed dataset. We categorize them based on challenges related to appearance, distance and size, cluttering and shape, and steering wheel angle prediction. The appearance is challenging to see in two ways. First, the deceptive texture by dirt or mud on the road due to rain or flat object (such as newspaper and cardboard box) is very different from other objects (such as rock and brick) that may mislead autonomous vehicle. Second, the appearance of obstacles seems to be its background. It is very challenging to detect obstacle due to week intensity gradient. However, the method we propose can solve the above problems and robustly extract obstacles. As you can see in Figure 7 d, we can accurately identify obstacle even if the road is wet. Likewise, it can avoid the objects such as mud and shadow in Figure 7 c. It also helps to identify obstacles that look like road (obstacle 4 in Figure 7 d). Our method can also take into account local changes in the road structure and even applies to cement floors ( Figure 7 a,c). As we all know, stereo depth is unreliable after the range of 10–12 m, which makes obstacle detection challenging at long distance. Detecting the small obstacles from distance, large changes in noise, or light reflection can cause serious problem. Our model solves these two problems well. As shown in Figure 7 e, label 1 shows that our method has detected a 9 cm height obstacle at a distance of 8 m. Similarly, in Figure 7 a, an obstacle of 7 cm height is detected at a distance of 7 m. Obstacle appear on the road in random shapes. Therefore, it makes sense to only allow certain shapes to use for modeled. However, our proposed algorithm has advantages in this regard because it does not require any assumptions about the shape or appearance of obstacles and always performs reliable detection. We can be seen rocks and tires in Figure 7 b,c, respectively. The rule-based classification method is used to divide all obstacles into two categories: (a) the ones to be ignored (the pixel size of obstacle is less than 50) and (b) which causes of vehicle to collide (the pixel size of the obstacle is greater than 50). We use MRF model and segmented image to calculate the threat value using equation 9 as discuss in methodology section. After pixel segmentation of obstacles, the coordinates of the risky obstacles in the image are obtained. After collecting the data, we prepare the image dataset for end-to-end driving model training by normalizing and resizing of the images [ 78 79 ]. As shown in Figure 8 , the steering wheel angle is normalized between the −1.0 to +1.0, where positive values represent right rotation and a negative value represents the left rotation of the steering wheel. We normalize the values by using the liner transformation Equation (13): Among them, is the normalized steering wheel angle between −1.0 and +1.0, is steering wheel angle in radians, and are the minimum and maximum steering wheel angle, respectively. In the Figure 8 , we have plotted the “steering angle” and “image sequence” on the y-axis and x-axis, respectively. Figure 8 shows that how closely proposed model follows the ground truth values. Using our model, we conducted a statistical significance test between the ground truth values and prediction values; we obtained a 91.5% confidence interval. The accuracy of the prediction can be qualitatively represented by observing the direction of the driving angle of an autonomous vehicle as shown in Figure 9 . As shown in Figure 9 , when there are obstacles on the road, our proposed model can predict that the angle of the steering wheel will be closer to the ground truth angle. In Figure 9 , the green line represents the ground truth angle and red line represents the expected angle of the steering wheel of our proposed model."
    },
    {
        "link": "https://researchgate.net/publication/226765213_Obstacle_Detection_and_Terrain_Classification_for_Autonomous_Off-Road_Navigation",
        "document": " 2005 Springer Science + Business Media, Inc. Manufactured in The Netherlands. Abstract. Autonomous navigation in cross-country environments presents many ne w challenges with respect to more traditional, urban environments. The lack of highly structured components in the scene complicates the design of even basic functionalities such as obstacle detection. In addition to the geometric description of the scene, terrain typing is also an important component of the perceptual system. Recognizing the different classes of terrain and obstacles enables the path planner to choose the most efﬁcient route toward the desired goal. This paper presents new sensor processing algorithms that are suitable for cross-country autonomous navigation. We consider two sensor systems that complement each other in an ideal sensor suite: a color stereo camera, and a single axis ladar. W e propose an obstacle detection technique, based on stereo range measurements, that does not rely on typical structural assumption on the scene (such as the presence of a visible ground plane); a color-based classiﬁcation system to label the detected obstacles according to a set of terrain classes; and an algorithm for the analysis of ladar data that allows one to discriminate between grass and obstacles (such as tree trunks or rocks), ev en when such obstacles are partially hidden in the grass. These algorithms have been developed and implemented by the Jet Propulsion Laboratory (JPL) as part of its involvement in a number of projects sponsored by the US Department of Defense, and have enabled safe autonomous navigation in high-ve getated, off-road terrain. Robots that can drive autonomously in off–road en- in recent years. The US Department of Defense through its various agencies has been the ma- jor sponsor of research in this ﬁeld. Notable ex- (Shoemaker and Bornstein, 1998) and the DARP A the U.S. Congress set a goal that by 2015 one third of the operational ground vehicle be un- Autonomous off-road robots will be employed not only in military operations, but also in civilian applications covering, search-and-rescue activities, as well as plan- for these systems is the ability to sense the environ- ment and to use such perceptual information for con- trol. Indeed, even if a robot is equipped with a Global sary for autonomous operation beyond the line of sight from the operator. Lacking perception capabilities, the robot would have to rely solely on self-localization (for\n\nexample, using GPS) and on prior environment maps. Unfortunately, the resolution of GPS is too lo w for tasks such as obstacle avoidance. Furthermore, maps normally have too low a resolution for small obsta- tion, and become obsolete. Thus, environment sensing is essential for the task of efﬁcient navigation over long It should be clear that driving in outdoor, non-urban environments is more challenging than driving indoors or in urban scenarios (as high- ing vehicles failed within the ﬁrst 7 miles of the 142 miles route). In the latter case, the environment is highly ing and action strategies. For example, in urban situ- ations one may expect the ground surface in front of the robot be planar, which helps detecting obstacles (anything sticking out of the ground plane should be av oided). In contrast, on a bumpy dirt road the robot should constantly determine which bumps and holes are small enough to be negotiated (possibly by slow- ing down) and which ones should be avoided. V ege- tated terrain introduces one more degree of freedom to the problem: what is considered an “obstacle” from a purely geometric point of view , may not represent a danger for the vehicle if it is composed of compressible ve getation (for example, a tuft of tall grass or a small ence of negative obstacles (such as ditches), elements such as water, mud or snow , and adverse atmospheric This paper addresses a set of perception tasks that are at the core of any control system for efﬁcient au- More precisely, we introduce ne w algorithms for (1) The problem of obstacle detection and avoidance is well studied in robotics, however , existing algorithms apply mostly to urban or indoor environments and don’t work well in off-road conditions. This is because typi- cal assumptions about the scene, such as the existence of ﬂat ground surface, do not hold in this case. Our new algorithm analyzes the slant of surface patches in front of the vehicle, and identiﬁes patches that are steep enough to represent a hurdle for the vehicle. The anal- ysis is carried out on the range data produced by a laser rangeﬁnder (ladar) or, as in the examples in this paper , points, our algorithm clusters such points into distinct resentation of the scene in terms of an obstacle map. tection are essential tasks, they are not sufﬁcient to In these environments, there are sev eral possible ter- rain types (such as soil, grass, mud) that should be negotiated by the vehicle in different ways. An ef- fective description of the outside world should con- sist of the combination of geometric terrain type (Bellutta et al., 2000; Hebert et al., 2002). The DEMO this concept, resulting in possibly the most advanced present two approaches to terrain cover perception, one based on stereo and color analysis (from color cam- eras), and the other based on range data processing (from a ladar). These two systems are somewhat com- plementary in the cues they use to discriminate between different terrain cover classes: surface reﬂecti vity in the ﬁrst case, local range statistics in the second one. Color classiﬁcation allows one to recognize a certain num- ber of distinctive classes such as grass and foliage, dry ve getation and bark, soil and rocks. Since stereo data is co-registered with color data, it is possible to label the obstacle detected on the basis of range information with the terrain type estimated by color analysis. The main challenges of color-based classiﬁcation are the inherent ambiguity of the reﬂectivity spectrum for some classes of interest (such as dry grass and soil), the effect of the illumination spectrum on the perceived color (the so- due to atmospheric effects. The analysis of local range statistics from ladar data, which is at the core of our second technique, can be used to successfully discrim- inate grass or foliage from other smooth surfaces such as rocks or tree trunks. This “range texture” approach is very promising, allowing one to detect surfaces e ven when they are partially hidden by grass. The two systems for terrain typing (color-stereo and ladar) discussed in this paper have complementary functionalities in the context of off-road autonomous navigation, and ideally should both be part of the sen- sor suite of a robot for such application, as shown in 1. A color stereo camera is most effective for detect- ing and characterizing isolated obstacles, as well as for ladar placed in the lower portion of the front of the\n\nbe dense and therefore in such cases lected to be equal to up the run-time of the obstacle detection algorithm. On data sets with medium-poor quality stereo, on the other hand, a large value for is necessary to handle miss- ing segments in the point cloud. Howev er, this increases the computational load of the algorithm, since termines the size of the search window . Furthermore, patible (it is easy to see that By limiting to a value smaller than or equal to the vehicle’s width, one ensures that the algorithm does not mistakenly merge two obstacles when there is enough horizontal clearance to drive between them. In our tests, It should also be noted that our “canonical” hypoth- esis that the axis of the chosen reference frame al- ways points vertically is not very realistic. Normally , the camera reference frame is attached to the vehicle, hence when the vehicle itself is tilted our original hy- pothesis fails. If the actual vertical direction can be recovered (based on IMU information), it is in princi- ple possible to adaptively re-adjust the camera refer- ence frame to account for the new attitude. This simply amounts to pre-multiplying the values of coordinates of Note that the search windows of OD Algorithms 1 and will also change when the vehicle tilts. If the vehicle tilts by angle , the height of each projected triangle will change by a factor of approximately while the base of the triangle projection will be approx- triangles will rotate by the same amount on the im- age plane. For combined rotation and tilt, the projected triangles will rotate and their lengths and heights will change. W e compensate for small vehicle tilt and roll by using a larger projected triangle search window than the need to estimate vehicle state information from IMU Besides detecting obstacle points, it is useful to identify all distinct obstacles in the image. This facilitates fur- ther reasoning about obstacle size and type, and inhibits undesired “gap ﬁlling” if the measured 3-D points are In our earlier work (Matthies et al., 1996; Bellutta ﬁnding the connected components of the obstacle pixel map on the image plane. This approach is computation- ally simple, but may incorrectly link two distinct obsta- cles when their projections on the image plane overlap, ev en though they are well separated in space (under- segmentation). Additionally , this prior solution may as- sign more than one label to a single obstacle when it This can be avoided by using the equiv alence relation- ship stated in Deﬁnitions 1 and 2, which makes full use It is useful to associate each measured point with one node of an undirected graph ( ), where two nodes are linked by an edge if the corresponding points are compatible. Thus, according to Deﬁnition 2, two points and belong to the same obstacle if and only if there exists a path in the graph from We can extend this notion to deﬁne a single obstacle as ponent) of the point graph. The connected components of the point graph may be computed online at the same time of graph creation (i.e., within OD Algorithms 1 or Note that our obstacle segmentation algorithm does not require pixels colored with the same label to be spatially connected (in 2D image space) to each other. Indeed, the algorithm is able to correctly assign only one label to a single obstacle even when there are gaps in the range measurement. This is an important advan- as distinct sets of image pixels. It is often the case that, due to measurement noise, a number of such clusters need to be rejected as spurious. Previous work (Bellutta et al., 2000) used 2-D area information (such as region areas) of pixel clusters in the image plane as a crite- rion for rejection, the rationale being that very small connected components are likely to be “outliers” and therefore should be discarded. However , it should be clear that, due to perspective, 2-D region area is not a very signiﬁcant measure. Given that the spatial coordi- nates of the obstacle points are known, it makes more sense to deﬁne meaningful 3-D attributes of shape and ysis. Some of the features we have experimented with\n\nmarginal variance, and their distribution may be con- fused with that of a patch of grass if the sampling order The spatial coherence hypothesis, however , is weak- ened when the surface is partially hidden by vegeta- tion, because laser beams have a certain probability of being intercepted by grass blades before hitting the surface. T o minimize this effect, we can pre-process the measured data with a non-linear ﬁlter that out- puts the maximum value of range within a window of contiguous samples. This ﬁlter exploits the fact ment of points beyond such surface can be taken (see Fig. 12). Thus, if a laser beam within a small win- dow hits the surface of an obstacle, then the maxi- mum range measured within this window of samples is likely to correspond to an obstacle surface measure- ment. In a sense, the maximum-value ﬁlter acts as a “grass remover”. This concept can be better formalized by looking at how the ﬁlter transforms the distribution of the input data. Let where F ( )i s the marginal cdf of the range measure- ments. By differentiation, we ﬁnd the pdf of where ( )i s the marginal pdf of the range measure- ﬁltered range measurements for the case of pure grass line). It is seen that in the second case, the effect of the maximum-value ﬁlter is to concentrate mass around the values corresponding to the surface (thereby reducing variance), while, in the ﬁrst case (pure grass), variance Our surface detection algorithm works as follows. First, the data is subdivided into windows of non- ov erlapping samples, and the maximum value the samples in each window is extracted. If is large (in our case, greater than 3), the set of non-overlapping windows can be replaced by a -sample sliding win- check for segments with high spatial coherence that ence can be expressed in terms of the differential char- acteristics of the data. In particular, we compute the magnitude of the second derivativ e of This has the advantage of producing a very small or null output value when the input is a linear ramp (cor- responding to a locally ﬂat surface, in any orientation). Thus, small values of should indicate possible surface The identiﬁcation of surface hits is not done on a scan-by-scan basis but on a sequence of scans. Let’s assume that the analysis of a scan at time indicates that a return from a laser beam ﬁred in direction , that yielded a range estimate ), is likely to have hit an obstacle surface. This hypothesis can be veriﬁed by ob- serving the range estimate in directions adjacent to at time . Thus, hits are classiﬁed as surfaces only after they are compared against hits in previous scans. In essence, this methodology applies consistency of the scene through time, i.e., if is sufﬁciently small, the relative positions of ladar and surface can change very little regardless of the relative motions and speeds of both ladar and surfaces. Thus, a hit in a direction with range estimate ) obtained at time is likely to be an obstacle surface if there is a hit in direction with a range estimate at time for suf- ﬁciently small and . This is the primary cri- heuristic criteria can be added to remove false positiv es from such candidates. For example, candidate hits are removed when there are no other candidate hits nearby . We also remove candidate hits that lie outside the oper- ative range [0.3 m 2.0 m]. As a reference, for our exper - iments 0 2si sg iv en by the 5 Hz rotation rate of the laser. Like wise, the value of is determined by the maximum distance that the robot can move in , i.e., given a maximum speed of 1 ms, the robot can move in the operational range. Thus, the robot is able to observe an approaching object (either because of the motion of the object or the motion of the robot) up to 10 times before reaching it, which gives the algorithm time to ev aluate the scene. Finally, in our setup we ha ve 30 . There are many situations in which the relative change of orientation between the robot and obstacle\n\nrotation of the robot, a fast straight motion of the robot passing by a stationary obstacle and any combination of these, among others (there are corresponding con- ditions when the robot is not moving but the obstacle is in motion, and when both are in motion). In these cases, the absence of a sufﬁciently small does not mean that the obstacles cannot be detected but only that they cannot be tracked; when the value of becomes sufﬁciently small, the obstacle will be detected, as a tested on scenes taken by the Acuity laser while mounted on the Urbie robot, as shown in Fig. 12. The experiments were run on eight scenes for periods of length between 15 and 90 seconds. The scenes included environments with sparse and dense grass, with and without obstacles (where the rocks were both trees and rocks), with obstacles in the clear and partially hidden and with the robot both stationary and on the move at various speeds. From the 1313 scans produced by these scenes we obtained 4 false alarms (i.e., four cases in which grass was misclassiﬁed as an obstacle surface); all the obstacles in the sequences were identiﬁed cor- rectly, at some point of the run, i.e., not e very obstacle is identiﬁed in every single frame in which it appears Figure 16 . (a) The “Urbie” robot facing two rocks at 45 and 315 (repeated from Fig. 12 for reference). (b) Obstacle surface detection on the bu te v ery obstacle is identiﬁed as such, in at least one frame of the sequence. This classiﬁcation is consistent with the hazard detection and avoidance objectiv e of the task, in contrast to a target tracking task that would penalize the loss of the target during any frame of the sequence The right half of Fig. 16 shows the obsta- cle classiﬁcation that corresponds to the scene in left half of Fig. 12, repeated in Fig. 16 as a reference. The lines shown in light gray indicate hits that are asso- ciated with obstacles. Note that both rocks (including the partially hidden one) have been detected correctly . Running times for the algorithm in a Pentium 4, 2.2 Ghz machine, are in the order of 450 s and 215 s per rev- olution (512 samples) for scenes with and without ob- stacles, respectively . Thus, the algorithm can be easily the sequence of images in Fig. 17 shows some results of the obstacle detection and avoidance in tall grass using Urbie. In this case, the algorithm was ported to C and ran under VxW orks. At the time of the port- ing, the customized Acuity ladar of Urbie had been replaced by a Sick ladar with a sampling interval of expected range error of 70 mm at 4 m. The sequence in Fig. 17 shows Urbie following a path that has an ob-\n\nthe obstacle and steers to the left to avoid it. Images and movies of the results of these tests are available at Note that other techniques for discriminating vege- tation from soil and obstacles using ladar data from a moving robot have been proposed (Hebert et al., 2002; Lacaze et al., 2002). These approaches, which are ap- propriate for use with a 2-axes ladar, are vox el-based representations of the 3-D data. This representation al- lows for the easy integration of measurements as the vehicle moves ahead (if accurate information about the vehicle’s position and attitude is a vailable). With re- spect to voxel-based techniques, our algorithm has a much lower computational complexity and does not re- quire integration of data over multiple scans (although it does use multiple scans to rule out false positives), We have presented novel algorithms for obstacle de- ronments. Our techniques, which use data from a stereo color cameras and from a single-axis ladar, are at the core of JPL ’s perceptual system for autonomous off- road navigation and have been tested in the context of the several projects funded by the US Department of Defense. Our experimental result in the course of these projects have shown the viability and rob ustness of our Our more recent work is looking at further extend- ing the terrain perception capabilities of a vehicle by using new sensors and new visual analysis algorithms. Fo re xample, color analysis can be made more robust by taking atmospheric effects into account (Nayar and 2001) could be used to either complement color classi- ﬁcation or for terrain typing at night, when color cannot be used. Possible additional sensors include multispec- tral cameras in the thermal IR (Abedin et al., 2003), which also could be used for terrain classiﬁcation at night, and polarization cameras, which may help de- In this Appendix we compute the distribution of the range in the case of an obstacle partially hidden by grass. The geometry of the obstacle surface is repre- sented in Fig. 13; we are interested in the distribution within the angular sector [0, A]. T o simplify our treat- ment, we will henceforth assume that as in Fig. 13 (the case wardly). It is assumed that the area between the ladar and the obstacle is uniformly covered by grass with where we used the total probability theorem, condition- ing with respect to the measuring angle , and assuming that the pdf ( )o f is uniform within [0, A]. Using\n\nRoberto Manduchi is an Assistant Professor of Computer Engineer- ing at the University of California, Santa Cruz. Before joining UCSC, he was a Senior Staff Member at the Jet Propulsion Laboratory , an Adjunct Assistant Professor at the University of Southern Califor- nia, and a Senior Engineer at Apple Computer, Inc. He received a “Dottorato di Ricerca” from the University of Padov a, Italy, in 1993. He works in the area of computer vision and sensor processing, with for the blind. His research has bee funded by NSF, D ARPA, and Andres Castano received the B.S. and M.E.E. degrees from the University of Los Andes, Bogota, Colombia, and the M.S. and Ph.D. degrees from the University of Illinois at Urbana-Champaign, all in Electrical Engineering. At Urbana-Champaign he worked at the Beckman Institute in visual servo-control of manipulators and panoramic cameras with very large depth-of-ﬁeld. Later he joined the Information Sciences Institute at the University of Southern California where he was the lead roboticist of the Conro project on reconﬁgurable robots. Currently he is with the NASA/Jet Propulsion Laboratory at the California Institute of Technology . His research Mellon University (CMU). He is currently a senior researcher at Jet Propulsion Laboratory. He has extensi ve experience in signal and nition, controls and optimization. His current research interests are in nal processing, and product inspection. He has led several DARP A, NIH, and NASA sponsored projects. He has a provisional patent for the design of software architecture and algorithms for biomedical data analysis and visualization, which was nominated for the NASA software of the year. He has ov er 40 publications, has chaired several SPIE conference sessions & given 3 invited conference talks, and is on the technical program committee of the Optical Pattern Recogni- tion Conference in the SPIE Defense and Security Symposium. He Larry Matthies received his Ph.D. in Computer Science from Carnegie Mellon University (CMU) in 1989 and has been at the Jet Propulsion Laboratory since then, where he is now supervisor of the Machine Vision Group. His research interests are in perception for autonomous navigation of robotic ground and air vehicles. He try algorithm. Both of these algorithms were used in the 2003/2004 feature tracking in down-looking imagery to estimate the horizon- tal velocity of the MER landers during terminal descent. He also motion analysis for mobile robots for Earth-based applications. He is an Adjunct Professor in the Computer Science Department at the University of Southern California and is a member of the editorial"
    },
    {
        "link": "https://sciencedirect.com/science/article/pii/S0921889024000137",
        "document": ""
    },
    {
        "link": "https://mdpi.com/1424-8220/24/11/3573",
        "document": "Autonomous robots are machines or devices capable of operating independently and making decisions in their environment. These robots are equipped with sensors and embedded systems to gather information about their surroundings, such as mapping, navigation, and obstacle detection. Obstacle avoidance plays a crucial role in the operation of autonomous robots, enabling them to navigate their environment efficiently and safely. Obstacle avoidance algorithms assist robots in avoiding obstacles and minimizing collisions, allowing them to reach their destination safely and accomplish their tasks. Thus, obstacle avoidance is an indispensable element of effective and reliable operation for autonomous robots. This paper explores a literature review of alternative route planning and mobile robot navigation methods. The main algorithms it considers are discussed in the following sections. Global path planning involves navigating a robot based on preexisting environmental data, which is loaded into the robot’s planning system to compute a trajectory from the starting point to the destination. This method generates a complete path before the robot begins its journey, essentially optimizing the route gradually [ 1 ]. Global path planning is consciously determining the best way to move a robot from a starting point to a destination. In global route planning, the robot has already been moved from the starting location to the destination, and the robot is then released into the specified environment [ 2 ]. In contrast, local path planning involves navigating a robot in dynamic or unknown environments where the algorithm adapts to real-time obstacles and changes. This method primarily focuses on real-time obstacle avoidance using sensor-based data for safe navigation [ 3 ]. The robot typically follows the shortest, straight-line path from the start to the destination until encountering an obstacle. Upon detection, it deviates from this path while updating essential details like the new distance to the target and the point of obstacle bypass [ 2 ]. Continuous knowledge of the target’s position relative to the robot is critical for accurate navigation, as depicted in Figure 1 The diagram of the algorithms of each type included in this paper is shown in Figure 2 . Another classification divides the methods into classical and heuristic algorithms ( Figure 3 ). The classification according to classic and heuristic obstacle avoidance algorithms makes the selection and application of algorithms more transparent and manageable. Users can more easily identify which algorithm best meets the requirements of a given problem. Classical algorithms such as Dijkstra perform well for minor deterministic problems, while heuristic algorithms such as A* can be more efficient for larger and more complex issues. Moreover, the separation between global and local search algorithms is less clear. There are heuristic algorithms (such as A* or the DL-based algorithms) that have both versions of the search algorithm. This paper follows the latter classification. One group of algorithms is called optimization methods. These mathematical procedures and algorithms aim to find the best possible solution to a given problem within the constraints available. An optimal solution is usually a combination of the values of one or more variables that maximizes or minimizes the value of the objective function while taking into account various constraints or conditions. For example, Particle Swarm Optimization, Cuckoo Search Algorithm, Artificial Bee Colony, Ant Colony Optimization, and Grey Wolf Optimization are based on such optimization techniques. These methods are called swarm (population)-based because they are inspired by animal behavior. Usually, some population of individuals (solutions) is used, and these individuals are iteratively developed and modified to find the best solution. They can effectively find optimal solutions to complex and diverse problems that traditional algorithms cannot manage with difficulty or at all. discusses bright spaces and fundamental obstacle avoidance methods. Then, in the discusses bright spaces and fundamental obstacle avoidance methods. Then, in the Section 2 discusses bright spaces and fundamental obstacle avoidance methods. Then, in the Section 3 , classical avoidance algorithms such as Dijkstra, Floyd–Warshall (FW), Bellman-Ford (BF), Artificial Potential Field (APF), Bug Algorithms, Vector Field Histogram (VFH), Probabilistic Roadmap Method (PRM), Rapidly exploring Random Tree (RRT), Cell Decomposition (CD), and the Following Gap Method (FGM) are discussed. Heuristic algorithms are presented in Section 3 . This includes the A* Algorithm, Fuzzy Logic (FL), Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Cuckoo Search Algorithm (CSA), Artificial Bee Colony (ABC), and Ant Colony Optimization (ACO). It also mentions using deep learning (DL) strategies and predictive methods. In this section, we discuss Artificial Neural Networks (ANNs), Model Predictive Control (MPC), and Deep Reinforcement Learning (DRL). Other algorithms are mentioned here, such as Dynamic Window Approach (DWA), Golden Jackal Optimization (GJO), or Grey Wolf Optimization (GWO). At the end of the chapter, a further so-called hybrid algorithm consists of two or more of the algorithms discussed earlier. Sliding Mode (SM) is presented in more detail among the hybrid methods. This article tries to collect and analyze most of the algorithms commonly used in practice. However, covering all currently existing methods in a single article is impossible, so this is not the aim here. This paper attempts to provide an overview of historically important and currently significant algorithms in practice as comprehensively as possible. Of course, it is impossible to discuss all possible methods (especially in the case of hybrid algorithms), but we tried to present the development directions of each algorithm. Such an extensive literature review cannot be found in other works. One of this article’s most important values, after the description of the theoretical background, is the summary table of the individual algorithms, which provides a sufficient comparison based on the algorithms’ main properties (e.g., convergence, calculation time).\n\nDutch scientist Edsger Wybe Dijkstra introduced the Dijkstra Algorithm (DA) in 1956, which he published in 1959 [ 5 ]. The question of the shortest path between two nodes in a directed graph is solved by this method, which is one of the most commonly used techniques for mapping isolated workspace paths [ 6 ]. This method is a well-known strategy, but it is less effective when the origin and destination are farther apart. In this case, the algorithm calculates the shortest path for all nodes, even if the node is irrelevant for the optimal route. Consequently, most of the calculations may be redundant, resulting in a time-consuming process. Another factor that may contribute to the time-consuming process is the presence of long edges in the graph. In this case, the Dijkstra algorithm has to spend a considerable amount of time processing the edges [ 7 ]. and must be introduced. The S heap records the vertices for which the shortest path is not found and the distance between the vertex and the starting point [ To plan the shortest path in Dijkstra’s algorithm, the starting position must be specified, and two heapsandmust be introduced. The S heap records the vertices for which the shortest path is not found and the distance between the vertex and the starting point [ 8 ]. The flowchart of the Dijkstra algorithm is shown in Figure 4 The work in [ 9 ] used DA to define vehicle routes on toll roads. Path planning is in a localization-insecure environment based on the Dijkstra method in [ 10 ]. Dijkstra was used to determine the shortest distance between cities on the island of Java [ 11 ]. This method was later modified to handle the situation where most of the network parameters are unknown and expressed as neutrosophic values (can be true, false, and neutral simultaneously, depending on the point of view) [ 12 ]. A Dijkstra-based route planning strategy for autonomous vehicles is included in [ 13 ]. In [ 14 ], a Dijkstra algorithm is applied to unmanned aerial vehicles (UAVs). Ref. [ 15 ] presents the optimal route planning of an unmanned surface vehicle in a real-time maritime environment using the Dijkstra algorithm. The Floyd-Warshall algorithm can be considered dynamic programming, and it was published in 1962 by Robert Floyd. The algorithm efficiently and simultaneously finds the shortest paths between all pairs of vertices of a weighted and potentially directed graph [ 16 17 ]. ( number of vertexes). Suppose there is also a shortest path function which gives the shortest path from to using only the node from 1 to as an intermediate point. The ultimate goal of using this function is to find the shortest path from each vertex to vertex using the intermediate node from 1 to . This algorithm first computes the function for each pair , then uses the results to compute for each pair , and so on. This process continues until , and the shortest path is found for all pairs with the interpolation of vertices [ The algorithm compares all possible paths for each line of all points on the graph. The graph’s vertices should be numbered from 1 tonumber of vertexes). Suppose there is also a shortest path functionwhich gives the shortest path fromtousing only the node from 1 toas an intermediate point. The ultimate goal of using this function is to find the shortest path from each vertexto vertexusing the intermediate node from 1 to. This algorithm first computes the functionfor each pair, then uses the results to computefor each pair, and so on. This process continues until, and the shortest path is found for allpairs with the interpolation of vertices [ 18 ]. of the shortest path between each two points. The elements of row and column of the matrix are the length of the shortest path from vertex to vertex . The state transition equation mathematically calculates the shortest distance between each point ( [ where the notation represents the shortest path from to that also passes through vertex . For example, is the edge length between vertices and . The algorithm consists of two parts: the construction of the path matrix and the state transition equation. The construction of the path matrix is based on the weight matrix of the graph to obtain the matrixof the shortest path between each two points. The elements of rowand columnof the matrixare the length of the shortest path from vertexto vertex. The state transition equation mathematically calculates the shortest distance between each point ( 1 ). The computation time is 19 ].where the notationrepresents the shortest path fromtothat also passes through vertex. For example,is the edge length between verticesand time for a graph with vertices and edges. The BF algorithm can handle edges with negative weights, unlike Dijkstra’s algorithm, which only works with edges with positive weights. For this reason, the BF algorithm is mainly used for graphs with negative edge weights. Although its efficiency is lower than that of Dijkstra’s algorithm, some problems would be impossible without negative weights. The BF algorithm is similar to Dijkstra’s algorithm, but it approximates all edges instead of selecting vertices with minimum distance. This operation is performed times, where is the number of vertices in the graph, and these iterations provide an exact prior in the graph [21, The Bellman-Ford algorithm is a classical method that computes the shortest paths in a weighted graph from a single source. This algorithm considers the negative-weighted edges of the graph, so it can handle graphs that contain negative-weighted cycles. These cycles generate several paths from the origin to the destination, where each cycle minimizes the shortest path length. The algorithm efficiently usestime for a graph withvertices andedges. The BF algorithm can handle edges with negative weights, unlike Dijkstra’s algorithm, which only works with edges with positive weights. For this reason, the BF algorithm is mainly used for graphs with negative edge weights. Although its efficiency is lower than that of Dijkstra’s algorithm, some problems would be impossible without negative weights. The BF algorithm is similar to Dijkstra’s algorithm, but it approximates all edges instead of selecting vertices with minimum distance. This operation is performedtimes, whereis the number of vertices in the graph, and these iterations provide an exact prior in the graph [ 20 22 ]. The idea is that the mobile robot moves within a potential field where the robot and obstacles behave as positive charges while the target behaves as a negative charge. The mismatch between attractive and repulsive forces helps the robot to move in the environment. The attractive force attracts the robot to the target location, while the repulsive force keeps it away from each obstacle [ 23 ], as shown in Figure 5 where is the Euclidean distance between the current position and the target, and is the scaling factor. The repulsive force can be calculated by adding the repulsive effect of the obstacles on the robot. This can be obtained by calculating the obstacles’ distance and direction (angle) from the robot. An obstacle close to the robot has a high repulsive force. The formula described by [ The final force acting on the robot is the vector sum of all repulsive and attractive forces. However, the distance determines the magnitude of the force, i.e., obstacles close to the robot will have a more significant effect. Similarly, if the robot is far from the target, its speed will be high and slow down as it approaches the target. As mentioned in the literature [ 24 ], the attractive force is the negative gradient of the attractive potential ( 2 ).whereis the Euclidean distance between the current position and the target, andis the scaling factor. The repulsive force can be calculated by adding the repulsive effect of the obstacles on the robot. This can be obtained by calculating the obstacles’ distance and direction (angle) from the robot. An obstacle close to the robot has a high repulsive force. The formula described by [ 25 ] is ( 3 To avoid local minima, various methods have been devised. One such method is the left-turning potential field approach, which compels the robot to change direction when encountering a local minimum. Conversely, the virtual target point method involves strategically placing a virtual target point when the robot reaches a local minimum. During this process, the robot disregards the influence of both the target point and obstacles, enabling it to pivot and break free from the local minimum. Another critical issue with APF is its susceptibility to local minima, which can hinder the robot’s progress. Symmetric and U-shaped obstacles exemplify these dead-end scenarios, leading to the robot becoming trapped. Figure 6 illustrates symmetric obstacles, where the forces exerted by the target and obstacles cancel each other out, resulting in a stalemate for the robot—a classic instance of local minima. To address this problem, significant attractor forces are temporarily applied at random locations to prevent the robot from being trapped in local minima. These measures aim to disrupt the equilibrium between attractive and repulsive forces, enabling the robot to navigate effectively [ 26 ]. In summary, while APF offers a direct path from source to destination, its susceptibility to local minima poses a significant challenge. Various strategies, such as the left-turning potential field and virtual target point methods, have been developed to mitigate this issue and ensure smoother navigation in complex environments [ 27 ]. The APF has been used in a dynamically changing, obstacle-filled environment between unmanned aerial vehicles (UAVs) [ 28 ]. Ref. [ 29 ] proposes an improved artificial potential field method for autonomous underwater vehicle (AUV) route planning. Based on an improved artificial potential field, ref. [ 30 ] introduces dynamic route planning for autonomous vehicles on icy and snowy roads. Ref. [ 31 ] discusses local path planning for multi-robot systems using an improved APF. Furthermore, ref. [ 32 ] outlines an active obstacle avoidance method for autonomous vehicles that is also based on an improved APF. Despite the presence of more efficient algorithms, Bug algorithms are still significant in robotics. These were the earliest navigation and obstacle avoidance algorithms that achieved relatively reliable results with speedy computation times. The algorithms are designed to work assuming that the robot is a single point in 2D space and that its movement is between each point. Bug algorithms are a popular type of robot navigation algorithms that provide a trajectory following an obstacle boundary in navigation scenarios with unknown obstacles, similar to the behavior of a bug [ 33 ]. The algorithm can be divided into three main variants based on their obstacle avoidance behavior, as discussed below [ 34 35 ]:\n• None The Bug-1 algorithm activates when the robot detects an obstacle. It starts circumnavigating the obstacle until it reaches the starting point from which it began while calculating the shortest distance from the destination to the departure point and creating a new path from the calculated departure point to the destination as it circumnavigates the obstacle. After completing full circles, it resumes circumnavigating the obstacles until it reaches the departure point, then proceeds on the newly generated path toward the destination.\n• None The bug-2 algorithm sets a direction from the starting position to the destination, and the robot follows it until it encounters an obstacle. Upon interruption, it follows the obstacle’s edge and calculates a new direction from each new position until the new direction matches the original direction. Once reaching this position, the robot resumes following the previously generated path towards the destination.\n• None In contrast, the Dist-Bug algorithm relies on distances to targets and obstacles. When encountering an obstacle on the path, the robot begins following the obstacle’s edge and calculates the distance between that point and the destination at each point. The point with the smallest distance to the target is called the distance point. Subsequently, the robot creates a new path along which it moves to the destination when it finds the distance point during its movement around the obstacle. The three versions are shown in Figure 7 A maritime search route planning method for unmanned surface vehicles (USVs) based on the improved Bug algorithm presented here is also presented in [ 36 ]. The FGM avoids obstacles by finding the gap between them. It calculates the gap angle. The minimum gap between obstacles is the threshold gap from which the robot can move. If the measured gap is larger than the threshold, the robot follows the calculated gap angle. Obstacle avoidance using the FGM is achieved in three main steps [ 37 ]. The algorithm uses sensory information to identify gaps with the largest angle and works in three steps, as follows [ 38 ]:\n• None The initial step involves computing the arrays of gaps. During this phase, the algorithm utilizes the current sensory data, such as information from the LIDAR sensor, to produce a gap array. This array provides details regarding the sizes of the available gaps surrounding the robot in angular form. The FGM algorithm identifies the largest gap by the conclusion of this stage.\n• None The FGM calculates the angle to the gap’s center point using specific geometric relations.\n• None , using ( In the third stage, this method calculates the final heading angle,, using ( 5 , the angle to the goal point , the distance to the nearest obstacle , and a safety parameter denoted as . Higher values of the alpha parameter prompt the robot to maintain a safe distance from obstacles and align with the center of the safe gap. Conversely, lower values of alpha lead the robot to prioritize the goal point, potentially approaching obstacles too closely in certain scenarios. The weighted function described in Equation ( 5 ) comprises the angle to the center point of the widest gap, the angle to the goal point, the distance to the nearest obstacle, and a safety parameter denoted as. Higher values of the alpha parameter prompt the robot to maintain a safe distance from obstacles and align with the center of the safe gap. Conversely, lower values of alpha lead the robot to prioritize the goal point, potentially approaching obstacles too closely in certain scenarios. The representation of the gaps accessible to the robot, the angle towards the midpoint of the widest gap, the angle towards the destination, and the final heading angle determined by FGM are depicted in a robot-obstacle configuration, as illustrated in Figure 8 So, in this procedure, the robot selects the largest gap around it and moves towards the target, taking into account the largest gap and the minimum distance from the obstacle. One of the drawbacks of this method is the lengthening of the path, which can sometimes be unnecessary. Another challenge is the subtle differences in gap sizes. This can sometimes result in the robot changing the number of selected gaps instantly, which can lead to zigzag paths [ 39 ]. In [ 40 ], the collision avoidance task is accomplished with the Follow the Gap Vector Method. A central part of the approach proposed in [ 2 ] is to identify gaps in the environment by analyzing sensor data. , and in its close vicinity. The algorithm initiates by generating a 2D histogram around the robot to depict obstacles. Subsequently, the 2D histogram undergoes updates with new sensor detections. It converts this 2D histogram into a 1D histogram and further into a polar histogram. Finally, the algorithm identifies the most suitable sector characterized by low polar obstacle density and computes the steering angle and velocity towards this direction. Figure 9 is from the work of [ 41 ] which illustrates the 2D histogram grid. The conversion from 2D to 1D histogram is shown in Figure 10 a, and Figure 10 b is a representation of the 1D polar histogram with obstacle density for a situation where the robot has three obstacles,andin its close vicinity. The first step is to sort the costs of the traversable area and then calculate the cost using the cost function based on the indicated direction of the polar histogram. The designated directions are selected from the traversable areas taking into account the robot’s kinematic and dynamic characteristics. Inaccessible sectors, as determined by the robot’s capabilities, are classified as impassable areas. Areas above the threshold are labeled as impassable, whereas those below the threshold are considered passable. To continue, the histogram generated in the previous step must be converted into a binary format by choosing the appropriate threshold based on the current situation. The commonly used cost function is shown as follows ( 6 ) [ 43 ]: The candidate direction represents the cost value . , , and are three parameters to be determined according to the actual situation. The is the target direction, is the previous direction, and the orientation of the robot is . The absolute difference between and is denoted by . The difference between the marked direction and the orientation of the robot is denoted by . The difference between and is denoted by . To determine the robot’s desired control command, this algorithm employs a two-stage data reduction process. Although this ensures accurate computation of the robot’s path to the target, it necessitates additional resources, such as memory and processing power [ 44 ]. Initial tests have shown that the mobile robot can use the VHF to traverse very crowded obstacle courses at high average speeds, and can pass through narrow openings (e.g., doorways) and move through narrow corridors without oscillating [ 41 ]. In [ 45 ], an improved 3D-VFH algorithm is proposed for autonomous flight and local obstacle avoidance of multirotor UAVs in confined environments. The cell-by-cell technique divides the area into non-overlapping grids, called cells, and uses grids that can be connected from the initial cells to the target to move from one cell to another. This method is classified as exact, approximate, and probabilistic CD depending on the assignment of boundaries between cells. For exact CD, the resolution is lossless and the shape and size of the cells are not fixed and each element is assigned a number. In contrast, for approximate CD, the decomposition result approximates the actual map and the grid has a fixed shape and size. And the probabilistic CD is like the approximate CD, except for the cell boundaries, which do not represent a physical meaning [ 46 ]. Figure 11 shows that the CD systems can be divided into three classes. Classical methods face several drawbacks, such as high time requirements at large scales and getting bogged down in local minima, which makes them ineffective in practical scenarios. To address these limitations and increase efficiency, probabilistic algorithms have been proposed. These algorithms aim at providing practical paths for robots through static workspaces [ 48 ]. One of the most important examples is the Probabilistic Roadmap Method (PRM) [ 49 ]. It uses lines to delimit the connectivity of the robot’s free areas. This includes the visibility graph and the Voronoi graph [ 47 ]. Figure 12 illustrates these two graphs. In the visibility graph, the obstacles are represented as polygons [ 50 ], and the vertical nodes of the polygonal obstacles are connected in such a way that the path length is minimized while the lines remain close to the obstacles. In contrast, the Voronoi graph uses the two closest points of the edges of the obstacles for planning and divides the domain into subdomains. In the latter case, the robot moves farther away from the obstacles, which increases safety but results in longer paths compared with the visibility graph [ 51 ]. To link the initial state with the goal region, PRMs explore this roadmap graph and pinpoint a sequence of states and local connections that the robot can traverse. While these algorithms can theoretically create arbitrarily accurate representations as the number of samples approaches infinity, in practice, only a handful of critical states are needed to define solution trajectories. These critical states often have significant structure, such as entries to narrow passages, but they can only be identified through exhaustive sampling [ 52 ]. These algorithms offer highly accurate representations with a theoretically infinite number of samples. In practice, however, this is only necessary in a few cases. For example, entering a bottleneck. The Voronoi graph continues to play a crucial role in the further development of different algorithms for different purposes [ 53 ]. Notably, ref. [ 54 ] presents a useful visibility Voronoi graph search algorithm for generating routes for unmanned surface vehicles. In addition, ref. [ 55 ] uses the Voronoi graph to partition agricultural areas into multiple fields, making it easier for multiple robots to perform agricultural tasks. The Rapidly exploring Random Tree (RRT) method facilitates swift exploration of the configuration space [ 56 ]. Initially proposed by LaValle [ 57 ], the RRT algorithm generates a graph, termed a “tree”, where nodes signify potential reachable states and edges denote transitions between states. The RRT’s root denotes the initial state, with all other states reachable along the path from the root to the corresponding node. Leveraging a sampling approach, this algorithm operates effectively in complex environments, evading local minima [ 58 ]. It has proven effective in tackling nonholonomic and kinodynamic motion planning challenges. In robotics, algorithms employed to generate RRTs are versatile, allowing trajectories to incorporate turns at any angle, albeit subject to kinematic and dynamic constraints [ 56 ]. When sampling, it allows all nodes in the robot configuration space to be reached with equal probability. Based on the constraints of the algorithm, it selects a node in the random tree. On impact, it resamples and discards the previous node. If no collision occurs, the selected node is added to the random tree. If a node on the route is redundant, it is deleted; otherwise, it remains as a node in the random tree [ 59 ]. The flow chart of an RRT is shown in Figure 13 This method does not require the modeling of space and can be used in large-scale environments. It also takes into account the objective constraints of unmanned vehicles, making it suitable for handling route planning problems in dynamic and multiobstacle environments. However, the route is randomly generated, leading to distortion. Second, the random tree has no orientation during the search process, resulting in slow convergence speed and low search efficiency [ 60 ]. Several improvements have been made to address the limitations of the algorithm. Among others, the RRT-Connect algorithm, the asymptotically optimal Rapidly Exploring Random Tree (RRT), the asymptotically optimal bidirectional Rapidly Exploring Random Tree (B-RRT), and the intelligent bidirectional RRT (IB-RRT) were born out of this necessity. Ref. [ 60 ]. The RRT* algorithm can construct an RRT whose branches converge asymptotically to the optimal solution given a given cost function. It solves feasibility problems efficiently and qualitatively concerning the cost function [ 56 ]. RRT has been used to plan the routes of ships [ 58 ], industrial robots [ 59 ] and micro aerial vehicles (MAVs) [ 61 ], among others.\n\nA heuristic approach is used to solve problems faster [ 62 ]. The method has proven its effectiveness and is widely used in autonomous navigation [ 5 ]. The A* algorithm is a graph search algorithm similar to Dijkstra’s algorithm, developed by Hart (1968) [ 63 ] to speed up the search process of Dijkstra’s algorithm. To do this, they introduced a heuristic cost function, which is the distance between the current point and the target point. Like the Dijkstra algorithm, the A* algorithm needs an environment model, e.g., a grid map. In the A* algorithm, the search area is usually divided into small squares, where each square represents a node. The algorithm can solve various routing problems with superior performance and accuracy compared with Dijkstra’s algorithm. Algorithm A* solves problems by finding the path with the lowest cost (e.g., the shortest time) among all possible paths to the solution. Of these paths, it first considers those that appear to lead the fastest to the solution. The A* algorithm uses an evaluation function ( 7 ): represents the cumulative cost from the starting point to the current point, extending to the target point. Meanwhile, denotes the shortest cost from the initial point to the current position , and predicts the optimal path cost from the current point n to the destination, often calculated as the Manhattan distance [ The functionrepresents the cumulative cost from the starting point to the current point, extending to the target point. Meanwhile,denotes the shortest cost from the initial point to the current position, andpredicts the optimal path cost from the current point n to the destination, often calculated as the Manhattan distance [ 64 ]. Initially applied in port areas, Casalino used the A* algorithm [ 65 ] for local pathfinding. Guan proposed an improved version of the A* algorithm [ 66 ], which helps Unmanned Surface Vessels (USVs) avoid static obstacles at sea and reach their destination smoothly while avoiding local minima. In addition, a collision-free trajectory planning method for space robots based on the A* algorithm has been developed in [ 67 ]. The geometric A* presented in [ 68 ] is designed for route planning of automated guided vehicles (AGVs) operating in port environments. Despite its advantages, traditional A* does not always provide an optimal solution, as it does not take into account all feasible routes. In each iteration, A* evaluates the nodes based on their values, which is a computationally expensive process, especially in large map search areas. Consequently, this approach can significantly slow down the speed of route planning. Fuzzy logic (FL) is a technique for persuading the human intellect. FL is a uniform approximate (linguistic) method for inferring uncertain facts using uncertain rules [ 69 ]. In 1965, Lotfi A. Zadeh was the first to introduce the idea of an FL system [ 70 ]. The fuzzy sets he created are an extension of the traditional notion of a set, going beyond the Aristotelian (true–not–true; yes–no) division. The fuzzy set A is defined as follows [ 70 ]: where X is the so-called reference surface, and is the so-called membership function, which takes values in the complete closed interval between 0 and 1. In the special case where takes only values 0 and 1, reduces to a classical set. The three basic operations on fuzzy sets (intersection, union, complement) are defined as extensions of the corresponding operations on classical sets. The standard properties of sets (De Morgan, absorption, associativity, distributivity, idempotence) hold here as well. Fuzzy inference (or fuzzy reasoning) is an extension of classical inference [ where X is the so-called reference surface, andis the so-called membership function, which takes values in the complete closed interval between 0 and 1. In the special case wheretakes only values 0 and 1,reduces to a classical set. The three basic operations on fuzzy sets (intersection, union, complement) are defined as extensions of the corresponding operations on classical sets. The standard properties of sets (De Morgan, absorption, associativity, distributivity, idempotence) hold here as well. Fuzzy inference (or fuzzy reasoning) is an extension of classical inference [ 69 ]. However, Zadeh’s vision was later expanded in several areas. The FL serves as a formal blueprint for representing and implementing the heuristic intelligence and observation-based methods of experts [ 71 72 ]. is an example of the primary FL driver used in [ is an example of the primary FL driver used in [ Figure 14 is an example of the primary FL driver used in [ 73 ]. The general architecture of a fuzzy logic controller consists of four units: IF–THEN rules, whose associated linguistic variable values can be not only true or false but can vary between the two; a fuzzy inference mechanism, which is a process to identify the output values associated with the input variables based on the fuzzy rules; an input fuzzification unit; and an output defuzzification unit. Hex Moor [ 74 ] was the first to apply the FL concept to robot path planning and obstacle avoidance. Since then, for example, the FL route planning approach has been applied in unknown environments [ 73 ]. Mobile robot routing algorithm based on FL and neural networks designed [ 75 ]. Chelsea and Kelly demonstrate FL controller for UAVs in a two-dimensional environment [ 6 ]. Then, 3D space navigation was demonstrated using FL for aerial [ 76 ] and underwater [ 77 ] robots and a Mamdani-type FL-based controller for a nonholonomic wheeled mobile robot that tracks moving obstacles [ 78 ]. A genetic algorithm is an optimization technique referring to genetics and natural selection, first introduced by Bremermann in 1958 [ 79 ]. It is based on Darwinian evolutionary theory and mimics the concept of survival of individuals best adapted to their environment. The most viable members of the population survive, while the weakest die off. The surviving members, depending on their fitness, allow the genes to be passed on to the next generation through cross-breeding, mutation, and selection. In this way, the individual fitness of the population continuously approaches the optimum. This random structure information was used to create a search algorithm that provided solutions to the problem of finding feasible pathways [ 79 ]. GAs stand for a sequence of algorithms. They randomly initialize populations with a character string and an objective function. Then, based on Darwinian evolutionary theory, they generate a new population using the three genetic operators (mutation, crossover, and selection). The new populations are created until the stopping conditions are met [ 50 ]. Such stopping conditions are a time limit, the required fitness value, and the maximum number of generations. During mutation, elements of an arbitrary string mutate with a given mutation probability. In a crossover, the elements of two strings are crossed according to a certain rule, thus creating two new strings. In selection, two strings selected by probability based on their objective function are compared based on their fitness, and the higher ranked higher-ranked one is selected to create the new population. The GA process is illustrated in Figure 15 . The initial input comes from the population variables. This is followed by the encoding and decoding of chromosomes, the initialization of the population, and, the evaluation of the fitness values of the individuals within it. If the conditions are met, the optimal solution is obtained directly. Otherwise, the algorithm iterates, evolves, and selects new individuals from the population, whose fitness is re-evaluated until the condition is met. After that, the process stops. GAs are used in many areas for mobile robot path planning problems, for example, for humanoid robot navigation [ 81 ], for the underwater robot navigation challenge in 3D route planning [ 82 ], and for aerial robots [ 83 84 ], as well as genetic-algorithm-based trajectory optimization for digital twin robots [ 85 ]. Work using improved genetic algorithms can be found in [ 86 87 ]. Simulated annealing and the Tabu search are approximate (heuristic) algorithms and therefore do not guarantee the optimal solution. They do not know when the optimal solution has been reached. Therefore, they need to be told when to stop. Easily designed to implement any combinatorial optimization problem, under some conditions, they converge asymptotically to the optimal solution. The same can be said for GAs [ 88 ]. than to ( SA is an iterative search method based on the analogy of annealing metals. Annealing is a process in which a low-energy state of the metal is created by melting the metal and then slowly cooling it. Temperature is the control variable in the annealing process and determines how random the energy state is [ 88 ]. Consider an energy diagram with two potential barriers. A ball is randomly placed on the potential curve and can only move down the curve. The ball then has an equal chance of going tothan to Figure 16 ). to pit with a higher probability, you have to increase its temperature. The probability of accepting upward movement decreases as T decreases. At high temperatures, the search becomes almost random, while at low temperatures it becomes almost greedy. At zero temperature, the search becomes completely greedy, i.e., it accepts only downward movements. The algorithm is based on the Metropolis procedure, which simulates the heat treatment process at a given temperature T [ Upward movements can be accepted at times with a probability controlled by the parameter temperature (T). For example, if you want the ball to move from pitto pitwith a higher probability, you have to increase its temperature. The probability of accepting upward movement decreases as T decreases. At high temperatures, the search becomes almost random, while at low temperatures it becomes almost greedy. At zero temperature, the search becomes completely greedy, i.e., it accepts only downward movements. The algorithm is based on the Metropolis procedure, which simulates the heat treatment process at a given temperature T [ 89 ]. At the beginning of the procedure, the current temperature and solution are given, as well as the time for which the heat treatment at the given temperature should be maintained. The SA algorithm should start from a high temperature. However, if the initial temperature is too high, it will only result in a loss of time. The initial temperature should be such that virtually any proposed movement is acceptable, whether upward or downward. Thereafter, the temperature will gradually decrease. The annealing time increases as the temperature decreases. The annealing process stops when the time exceeds the permissible time [ 90 ]. probability based on the current temperature and the cost change ( The main part of the algorithm consists of two circles. In the inner circle, a possible move is generated and the acceptance of the move is decided by an acceptance function. The acceptance function assigns aprobability based on the current temperature and the cost change 8 ). At high temperatures, most uphill movements are likely to be accepted by the algorithm, regardless of the increase in costs. However, as temperatures fall, only downward movements are accepted. If the step is accepted, it is applied to the current path to generate the next state. The outer loop checks if the stopping condition is satisfied. Each time the inner loop completes, the temperature is updated using a function, and the stopping condition is checked again. This continues until the stop condition is met [ 90 ]. TS is a combinatorial optimization technique that optimizes an initial given permutation or converts it to the closest possible optimal solution, by alternating successive steps. Using this method, it is possible to reduce the cost of a path by a series of edge swaps in a randomly generated round trip. The process continues until the path with the minimum cost is found. The selection of the best step to improve or not improve the current solution is based on the fact that good steps are more likely to reach the optimal or close to the optimal solution. The set of acceptable solutions in a given iteration forms a candidate list. The Tabu search selects the best solution from this candidate list, whose size reflects the trade-off between quality and performance. To Tabu the relocation attributes, a Tabu constraint is introduced to prevent the reversal of moves. This constraint is enforced by a Tabu list that stores the relocation attributes. The aspiration-level component allows the Tabu state to be temporarily overridden if the reversal results in a better solution than the best one achieved so far [ 88 ]. In [ 91 ], the design of minimal-cost delivery routes for goods-carrying mobile robots is developed using hybrid simulated annealing/Tabu search and approximation methods based on Tabu search algorithms, which start and end from a central warehouse while the robots serve customers. Each customer is supplied exactly once per vehicle path. Particle Swarm Optimization (PSO) is a nature-inspired approach that mimics the collective behavior of bird flocks, fish schools, or animal herds as they seek food, adapt to their surroundings, and interact with predators [ 92 ]. PSO draws inspiration from the foraging strategy observed in bird flocks, where individuals move towards the most favorable food sources guided by their knowledge, collective wisdom, and momentum. This behavior is emulated by the PSO algorithm through the representation of each potential solution as a particle, with personal and global best positions and inertia. Each particle maintains specific attributes such as position, velocity, and objective, striving to converge toward the global optimum over multiple iterations. The PSO process begins with the initialization of a randomly generated particle swarm, with each particle assigned a unique velocity to navigate the search space. Notably, unlike genetic algorithms, PSO assigns random weights to all potential solutions, enabling particles to explore the solution space dynamically. The algorithm’s functioning revolves around the interplay between particle positions and velocities, with each particle’s position updated based on its velocity conditions. Refer to Figure 17 for an illustration of this process [ 93 94 ]. -dimensional, and the th particle of the population can be represented by a -dimensional vector . The velocity of this particle can be represented by another -dimensional vector . The previously best-visited position of the th particle is denoted by , and the best particle in the swarm is denoted by . The update of the particle’s position is accomplished by the following two equations: Equation ( where is the iteration number, ; ; and is the swarm size. is inertia weight, which controls the momentum of the particle by weighing the contribution of the previous velocity. and are positive constants, called acceleration coefficients. Alternatively, is also called the cognitive (local or personal) weight, and is the social (or global) weight. and are random values ranging from . is the velocity associated with the particle at time , and is the position of the particle at time . Suppose that the search space is-dimensional, and theth particle of the population can be represented by a-dimensional vector. The velocity of this particle can be represented by another-dimensional vector. The previously best-visited position of theth particle is denoted by, and the best particle in the swarm is denoted by. The update of the particle’s position is accomplished by the following two equations: Equation ( 9 ) calculates a new velocity for each particle based on its previous velocity, and ( 10 ) updates each particle’s position in the search space [ 92 95 ].whereis the iteration number,; andis the swarm size.is inertia weight, which controls the momentum of the particle by weighing the contribution of the previous velocity.andare positive constants, called acceleration coefficients. Alternatively,is also called the cognitive (local or personal) weight, andis the social (or global) weight.andare random values ranging fromis the velocity associated with the particle at time, andis the position of the particle at time The PSO process, depicted in Figure 18 , is characterized by rapid convergence but shows slower responses during particle search within a region. This limitation, due to its fixed convergence rate, can lead to localization issues [ 96 ]. PSO is widely applied in mobile robot path planning across various types, including humanoid [ 97 ], industrial, [ 98 ], wheeled [ 99 ], aerial [ 100 ], and underwater robots [ 101 ], particularly in complex three-dimensional environments. The concept of a cuckoo search is inspired by the behavior of cuckoo birds, which lay their eggs in the nests of other host birds (of different species). The cuckoo bird attempts to deposit its eggs in the nest of a host bird by removing one of the host’s eggs and replacing it with one of its own, which closely resembles the host bird’s eggs. Afterward, the cuckoo bird swiftly departs. The primary goal of this behavior is to safeguard its eggs from predators, as well as to ensure that its offspring have access to food and protection in the host nest. However, there is a risk that the host bird may detect the cuckoo egg and either remove it from the nest or abandon the nest to construct a new one. Consequently, the cuckoo continuously evolves its egg appearance to mimic that of the host bird’s eggs, reducing the likelihood of detection. Importantly, the host bird also learns to detect foreign eggs over time, perpetuating the cycle of egg-laying and detection. Once the cuckoo successfully places its egg in the host nest, a new phase ensues. Cuckoo chicks hatch earlier than the host bird’s offspring and may attempt to eject the host eggs or chicks from the nest. Additionally, cuckoo chicks compel the host mother bird to provide them with more food, potentially depriving the host chicks of sustenance altogether [ 102 ]. The interaction between the cuckoo and the host bird results in a direct conflict, as the host bird has a probability, denoted as P and ranging from 0 to 1, of detecting the cuckoo’s egg. If a host bird detects cuckoo eggs in its nest, it may either discard the egg or desert the nest altogether. These fundamental occurrences form the basis of the cuckoo search algorithm. The primary features of the CSA are outlined in [ 103 ]: In the cuckoo search algorithm, a single egg is deposited by a cuckoo in a nest selected at random, symbolizing a potential solution to an optimization problem. The nest containing the most promising eggs—representing the optimal solutions—is carried forward to subsequent iterations. The total number of available nests remains constant, and each egg laid by a cuckoo is subject to a probability within the interval of being detected and consequently abandoned. Consequently, during each iteration , a proportion of the entire population undergoes alteration. where represents the new solution, indicates the current generation (iteration) of the solution, is the step-wise parameter that controls the moving step size of the cuckoo, ⊕ is entry-wise multiplication, and denotes the step size factor, which is usually set to 0.01. and is Levy exponent, which stands for a random search path, which can be expressed as where and are two random numbers subjected to the normal distribution, is set to 1.5. is defined as: The efficiency of the cuckoo search algorithm is enhanced through the utilization of Levy flight instead of random walk. Numerous animals and insects exhibit the characteristic Levy flight behavior. Levy flight entails a random walk with step lengths determined by a heavy-tailed probability distribution, as depicted in ( 11 ) [ 104 ]. Levy flight outperforms random walk in this regard. Hence, we opted for the cuckoo search algorithm in this research due to its ability to achieve faster convergence rates.whererepresents the new solution,indicates the current generation (iteration) of the solution,is the step-wise parameter that controls the moving step size of the cuckoo, ⊕ is entry-wise multiplication, anddenotes the step size factor, which is usually set to 0.01. andis Levy exponent, which stands for a random search path, which can be expressed aswhereandare two random numbers subjected to the normal distribution,is set to 1.5.is defined as: Algorithms with high computational complexity typically demand significant resources, which may not always be feasible. The cuckoo search algorithm (CSA), however, requires only a few initial parameters, enabling efficient resolution of multimodal problems. The CSA, depicted in Figure 19 , involves three key operations: (i) Levy flight for generating new solutions, (ii) replacement of nests with superior solutions based on fitness evaluations, and (iii) greedy selection to maintain the best solutions until the goal is achieved. CSA has been effectively hybridized with an adaptive neuro-fuzzy inference system for enhancing the navigation of multiple mobile robots in unknown environments [ 105 ] and applied in vehicle track design [ 106 ] and scheduling [ 107 ]. Additionally, it has been used in a novel artificial neural network approach to predict ground vibrations from mine blasting [ 108 ]. Karaboga developed the Artificial Bee Colony (ABC) technique, a swarm-based algorithm inspired by the foraging behaviors of bees [ 109 ]. The three rules of the ABC model are as follows: (a) Forager bees: Forager bees are sent to the food sites (the nearest colony) and inspect the quality of the food. (b) Inactive forager bees: Based on information from active forager bees, inactive bees inspect the food sources detected and assess/assess them. (c) Food sources: Forager bees that find rich food sources distribute them, while forager bees with few food sources give them up, creating a problematic situation. ) that the bee is responsible for, and according to ( where the objective function shows the fitness value of source . In addition, the failure counter, which is a limit value for each food source, is defined and initialized to zero. The population is initialized from the set of employed and onlooker bees. Each worker is sent to the food source () that the bee is responsible for, and according to ( 15 ), the fitness value of each source of food is determined [ 110 ].where the objective functionshows the fitness value of source. In addition, the failure counter, which is a limit value for each food source, is defined and initialized to zero. ). where . The problem dimension is defined by . . N represents the total number of employed or onlooker bees. The value of is not equal to , and is a random number generated from a uniform distribution in . Then, using ( 16 ), they try to find a better food source ().where. The problem dimension is defined by. N represents the total number of employed or onlooker bees. The value ofis not equal to, andis a random number generated from a uniform distribution in and selects the optimal food source . The probability evaluation of the food source is determined using Equation ( where is the fitness value of the solution . This is clear from the ABC algorithm’s general flow chart (depicted in Should the fitness value of the new position surpass that of the current one, the bee retains the newly identified food source location and disregards the previous source. The worker bee then communicates the fitness value of this new food source to the onlooker bee. The onlooker bee evaluates each food source based on the probabilityand selects the optimal food source. The probability evaluation of the food source is determined using Equation ( 17 ) [ 110 ].whereis the fitness value of the solution. This is clear from the ABC algorithm’s general flow chart (depicted in Figure 20 ) [ 111 ]. The ABC algorithm has been used to solve many real-world problems. Ref. [ 112 ]’s applications of the ABC algorithm can be seen in many situations where MR (moving robot) systems operate in static environments [ 113 114 ]. For example, they tested a wheeled MR underwater [ 115 ], applying it to the routing problem of autonomous vehicles [ 116 ], as well as aerial robots [ 117 ]. The modified ABC algorithm was used for the Unmanned Combat Aerial Vehicle (UCAV) navigation problem [ 118 ] to plan optimal routes in a three-dimensional environment, including unmanned helicopters [ 119 ]. where is the transition probability, represents the pheromone concentration, is the heuristic function, is a collection of access points, and is a heuristic function, usually expressed as the reciprocal of the distance between and . where represents the pheromone volatility coefficient, is the total number of ants in the ant colony, and represents the pheromone amount released by the th ant. The ACO process is illustrated in This algorithm is inspired by the foraging behavior and communication of ants, and it was presented by Dorigo and Maniezzo in 1991 [ 120 ]. Ants leave behind a kind of pheromone on the paths they traverse. The more ants travel along a path, the more pheromone accumulates on it, and other ants will follow stronger pheromone trails left by other ants in the area. When an ant initiates a search process in a problem, for example, searching for a route on a map, it randomly selects a route and follows it. As it progresses, the ant senses the amount of pheromones in the environment and makes decisions to modify its route based on this information. Ants prefer routes with higher pheromone concentrations. The ACO algorithm runs repeated colonies of ants and compares the results of each colony to optimize the pathways based on the amount of pheromones. In this process, the algorithm gradually converges to an optimal solution to the problem. The formulae of the ACO algorithm ( 19 ) are described in [ 80 ]:whereis the transition probability,represents the pheromone concentration,is the heuristic function,is a collection of access points, andis a heuristic function, usually expressed as the reciprocal of the distancebetweenandwhererepresents the pheromone volatility coefficient,is the total number of ants in the ant colony, andrepresents the pheromone amount released by theth ant. The ACO process is illustrated in Figure 21 Initially applied to solving the Traveling Salesman Problem (TSP) [ 120 ], the principles and mathematical models of the ACO algorithm have since been systematically studied and have undergone significant development, such as in [ 121 ] with airport AGV route optimization model based on the ant colony algorithm for optimizing Dijkstra’s algorithm in urban systems. In [ 122 ], a search and rescue is presented in a maze-like environment with ant and Dijkstra algorithms. The work in [ 123 ] describes the application of odometry and Dijkstra’s algorithm to warehouse mobile robot navigation and shortest path determination. Machine learning (ML) is the process of using computer systems to learn and improve without their experience, explicitly programming them. Machine learning algorithms rely on recognizing patterns and rules from data and making decisions or predictions based on them. Basic machine learning techniques include supervised learning (where algorithms are trained on labeled data), unsupervised learning (where algorithms try to find structure from unlabeled data), and semisupervised learning, which uses a combination of the two methods. Deep learning is a specialized field of machine learning that uses deep neural networks to learn complex patterns and representations. Deep learning enables computer systems to learn representations of data using multilayered, hierarchical structures. These layers gradually learn higher-level features, which makes deep learning algorithms particularly effective in image recognition, speech recognition, natural language processing, and many other complex tasks. Deep learning models such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) have made significant breakthroughs in various application areas of artificial intelligence. The main advantage of solutions based on machine learning is that they can learn from the data, so their models already incorporate the nonlinear behavior of the control plant. This enables better performance in many control applications than classical approaches. Deep learning techniques are suitable for handling both global and local path-planning problems [ 124 ]. A neural network, which draws inspiration from the natural human senses, serves as an intelligent system and was originally devised for mobile robot route planning [ 125 ]. It consists of simulated networks composed of neuron-like units. These networks undergo optimization through comprehensive training on designated tasks, with the connection strengths between units being gradually adjusted over time [ 126 ]. In a neural network, the processing elements (neurons) are usually ordered topologically and interconnected in a well-defined way. The structure of the neural network plays an important role in the execution of the task. Due to the internal parallel structure of neural nets, computations can be performed in parallel, thus ensuring high processing speed. Thus, neural networks are particularly suitable for solving real-time tasks. Where is input to the neuron, is the input vector ( represents the number of inputs on the neuron). is a constant input (bias-offset value), and is the neuron’s output. represents the weight factor associated with the th input, is the weight vector, and represents the activation function. scalar inputs are summed by weighting and the weighted sum is then summed to a nonlinear element. The weighted sum of the input signals, which is the input to the activation function, is called the excitation, while the output signal is called the response (activation). The function is called the activation function. The output of a neuron can be calculated as follows: Thescalar inputs are summed by weightingand the weighted sum is then summed to a nonlinear element. The weighted sum of the input signals, which is the input to the activation function, is called the excitation, while the output signal is called the response (activation). Thefunction is called the activation function. The output of a neuron can be calculated as follows: The weight factors determine the degree of influence on connections with neighboring neurons within a neuron’s vicinity. A neural network’s functionality relies on these weight factors, which encapsulate information or the processing of information during the learning phase. Utilizing a nonlinear activation function enables the neural network to model any nonlinear function when applied to a suitable neuron. Conversely, a linear activation function leads to a linear neural network. To imbue a neural network with nonlinearity, it is imperative to incorporate at least one nonlinear activation function. Additionally, differentiation plays a crucial role, as gradient-based learning stands as the predominant method for adjusting neural network weights. serves as the input for layer . The known data enter the input layer, accompanied by a bias term. Subsequently, these data are subjected to multiplication by initial weights, followed by summation. The resulting values are then passed through functions to the subsequent layer, iterating until reaching the output layer, where the final value is derived. Transitioning from one layer to the next in an ANN involves the utilization of transfer functions [ ANNs are structured into distinct layers: the input layer, where known data are fed into the model; the intermediate layers, referred to as hidden layers; and the output layer, which yields the final sought-after value. Each layer comprises various units (neurons or nodes), with each unit connected to the subsequent layer through a transfer function. Within an ANN, the output of layerserves as the input for layer. The known data enter the input layer, accompanied by a bias term. Subsequently, these data are subjected to multiplication by initial weights, followed by summation. The resulting values are then passed through functions to the subsequent layer, iterating until reaching the output layer, where the final value is derived. Transitioning from one layer to the next in an ANN involves the utilization of transfer functions [ 127 ]. A possible layout of an artificial neural network is illustrated in Figure 23 The operation of neural networks can typically be divided into two phases:\n• None Learning phase—the network stores the desired information processing procedure in some way.\n• None Recall phase—the stored procedure is used to execute information processing. The main forms of learning in neural networks [ 128 ]:\n• None Learning with a teacher (called supervised or guided learning (also known as controlled learning)). Learning neural networks is nothing more than a multivariate optimization procedure based on a predefined criterion function (cost function). Various optimization techniques have been widely used for learning neural networks: gradient-based strategies [ 129 130 ], evolutionary methods, genetic algorithms [ 131 132 ], and particle swarm optimization (PSO; see later) algorithms [ 133 ]. ANN has been applied in a wide range of fields, including search optimization [ 134 ], pattern recognition [ 135 136 ], image processing [ 137 138 ], mobile robot routing [ 139 ], signal processing [ 140 ], and many more. A hybrid approach to mobile robot navigation combining an ANN and FL [ 141 142 ] was designed for a mobile robot navigation controller using a neuro-fuzzy logic system. In [ 143 ], a single-layer approach to robot tracking control was proposed. Through experiments on a KUKA LBR4+ robotic manipulator, the feasibility of the novel ANN approximation for robot control was examined. Ref. [ 144 ] presents positioning the error compensation of an industrial robot using neural networks. Ref. [ 145 ] presents a recurrent neural network for prediction of motion paths in human robot collaborative assembly. The ANN was extended to create the Guided Adaptive Pulse Coupled Neural Network (GAPCNN) for mobile robots [ 146 ]. The GAPCNN aims to achieve fast parameter convergence to help the robot move in both static and dynamic environments. In particular, the ANN method has been used in MATLAB for mobile robot trajectory planning problems for aerial robots [ 147 ], humanoid robots [ 148 ], underwater robots [ 149 ], and industrial robots [ 150 ]. The MPC method ( Figure 24 ) is used to predict the behavior of the system for a given time interval and, based on the prediction, optimize the intervention signal at each time instant. As a result, it minimizes the cost function and determines the optimal control sequence. The method has the advantage of a user-friendly design process and easy implementation. It has many applications in the automotive industry, for example, it is used to solve tracking problems [ 151 ]. The path planning of autonomous vehicles can also be conducted using a predictive approach [ 152 153 ]. Due to the high computational complexity of numerical optimization, it is of paramount importance to ensure real-time computability, which requires the right formulation of the problem and the choice of the appropriate procedure for its solution. The most commonly used MPC approach is based on linear models, but this also has limitations that can reduce performance. Advances in recent decades have allowed engineers to use control approaches that have a higher computational cost, such as Nonlinear Model Predictive Control (NMPC). The main drawback of an NMPC is its complexity, which can lead to high computational time. As a consequence, in most cases, only suboptimal solutions can be obtained, which may degrade the performance of the closed-loop system [ 154 ]. Ref. [ 155 ] also proposes a cooperative regulatory strategy for docking unmanned aerial vehicles (UAVs) based on MPC. The proposed strategy implements a nonlinear and a linear MPC for the coarse approach (long range) and the fine docking maneuver (short range) based on the same objective function with tailored optimization strategies. Docking is a complex, critical maneuver that requires knowledge of the flight safety of the docking route and the constraints associated with the position of the platform to be docked. In addition, nonlinear effects such as vorticity due to the close approach of the lead agent must be taken into account. Using the MPC method, it is easier to prove the stability and performance of the system, as it does not require knowledge of the system model. Instead, a local model is used and updated every time step. In addition, other methods are available to redefine the learning characteristics compared with neural networks. For example, in [ 156 ], an MPC-based control solution is proposed where the terminal cost and the set are determined through an iterative process. The presented algorithms do not guarantee stability, which makes their application in safety-critical systems, such as autonomous vehicles, risky. However, some solutions address this problem. For example, ref. [ 157 ] presents a control strategy based on safety settings that can modify the input signal of the system when the output of a machine learning agent may destabilize the system. Another solution is given in [ 158 ], in which a Hamilton–Jacobi reachability algorithm is exploited that can work with any machine learning-based solution. A combined approach is presented in [ 159 ], in which a classical controller is used to control the linearized system, while the machine learning-based algorithm handles the nonlinearities of the system. B. Németh [ 160 ] incorporated machine learning into the usual model-based robust control theory framework, but emphasized it as a new tool and an additional data-driven branch. For all its learning nature, however, robust control remains, i.e., the traditional model-based solution has been extended to fit today’s new approach to new types of tasks. The method is independent of the internal structure of the learning-based control element. Hence, a control element with any structure can be incorporated in its place, providing considerable freedom in control design. For example, solutions based on neural networks, which are already well established in practice, can be implemented in the developed control solutions for reference signal training or feedback loops. However, data-based MPC-type control schemes typically have a more closed, less flexible formalism for the optimization formulated in them. Another consequence of the hierarchical structure is that the learning-based management element can be physically separated from the supervisor and robust management elements. The vehicle motion dynamics are considered in the robust control element and the learning functions in the learning-based control element. For example, in the context of automated vehicles, the supervisor-robust control dual, which has a low computational demand, can be placed on board the vehicle, while the learning-based control element can be placed on an independent platform, such as a cloud. Control solutions that rely predominantly on solving an optimization task online typically do not have this advantage. The supervisor element, which requires online computation, has significantly lower computational requirements than traditional MPC or more advanced data-based (learning) MPC solutions. This is because the supervisor performs significantly fewer tasks than the main optimization task of the MPC. In the supervisor, it is not necessary to perform an optimization over a long horizon, since the impact on future motion states is taken into account in the learning process by running on episodes or prespecified patterns. Reinforcement learning (RL), inspired by animal psychological learning, learns optimal decision-making strategies from experience [ 161 ]. RF is a special type of ML algorithm that does not require large amounts of data for training. The RF algorithm is modeled based on reward, and several papers address the problem of autonomous vehicle control using RF methods [ 162 163 ]. Although RF-based solutions can be efficient, the stability of the closed-loop system is still an open question. A proposed solution is an RF-based algorithm combined with a robust controller [ 164 ], which achieves the stability of the algorithm by applying uncertainty models. The Deep Reinforcement Learning (DRL) model is particularly promising for solving Vehicle Routing Problems (VRPs). DRL can estimate patterns that are difficult to find with manual heuristics, especially for large-scale problems. Moreover, DRL can generate and infer routes quickly, making it extremely useful for solving time-sensitive VRPs. The use of DRL in mobile robot navigation is a growing trend. The purpose of using the DRL algorithm in an autonomous navigation task is to find the optimal policy for guiding the robot to the target position through interaction with the environment. The advantage of DRL-based navigation is that it is map-free, has strong learning ability, and has little dependence on sensor accuracy [ 124 ]. Without wishing to be exhaustive, we briefly mention some algorithms that have recently become common. The DWA can generally be classified as a heuristic method, as it does not rely on rigorous mathematical models or algorithms to solve the problem but rather on an empirical approach. This method is designed for local routing and obstacle avoidance [ 165 ]. It takes into account the robot’s current speed, acceleration limits, and immediate surroundings to calculate a safe and feasible path to the destination. Creates a dynamic window based on possible velocities and angular velocities. An objective function calculates the optimal value of these pairs of velocities based on the minimum distance from the obstacles, the final bearing angle, and the velocity values of the robots. While in less complex environments the DWA can deftly avoid obstacles, its performance in extremely crowded environments may be suboptimal [ 166 ]. The DWA has the local minima and the global convergence problems [ 167 ]. Once the task creator has set one or more targets and the global route has been planned, the execution phase involves the robot scanning the surrounding environment, planning local trajectories, and moving forward. This sequence is repeated until the goal is reached. At the beginning of this flow, it samples all the speed pairs corresponding to the kinematic constraints of the robot. DWA computes the coordinates of the waypoints for each input velocity pair using iterations ( 23 ) to ( 25 ) [ 168 ]. along the heading of and then rotates an angle of , where and represent the coordinates, and represents the heading of the robot. By iterating the input velocities, this method computes the coordinates and heading of the robot from time to . The computation time depends on the number of iterations. In the next step, DWA calculates the distance between each obstacle and waypoint using matrix operations. The calculation time is influenced by the number of paths and obstacle points. Subsequently, DWA swiftly determines the direction to the path’s endpoint and the distance to the target. After assessing all possible speed pairs, the optimal speed command is generated. This model is extensively utilized in research involving wheeled robots [ The model assumes that the robot moves a distance ofalong the heading ofand then rotates an angle of, whereandrepresent the coordinates, andrepresents the heading of the robot. By iterating the input velocities, this method computes the coordinates and heading of the robot from timeto. The computation time depends on the number of iterations. In the next step, DWA calculates the distance between each obstacle and waypoint using matrix operations. The calculation time is influenced by the number of paths and obstacle points. Subsequently, DWA swiftly determines the direction to the path’s endpoint and the distance to the target. After assessing all possible speed pairs, the optimal speed command is generated. This model is extensively utilized in research involving wheeled robots [ 168 ]. GJO is a metaheuristic, swarm-intelligence-based algorithm proposed by Nitish Chopra and Muhammad Mohsin Ansari, which models the cooperative hunting behavior and tactics of golden jackals in nature. Because these opportunistic animals are famous for their ability to adapt to different environments [ 169 ]. Golden jackals usually hunt with males and females. After finding the prey, they begin to move towards it cautiously. The prey is then surrounded and stalked until it stops. Finally, it is attacked and captured. Updating the position of the prey often depends on the male golden jackal. For this reason, the diversity of golden jackals is not adequate in some cases, and the search algorithm tends to fall into the local optimum [ 170 ]. where and are the maximum and minimum values of the variable , and is a uniform random vector in the range of 0 to 1. GJO initiates with a randomized distribution of the first solution across the search space, as shown in Equation ( 26 ) [ 169 ]:whereandare the maximum and minimum values of the variable, andis a uniform random vector in the range of 0 to 1. In [ 171 ], a hybrid-strategy-based GJO algorithm for robot path planning is presented. , who leads the hunt; , who assists the leader; , who scouts and guards; and the rest . The wolves’ hunting process is generally broken down into three phases: encirclement, pursuit, and attack. During the encirclement phase, the algorithm updates positions using Equation ( GWO is another type of swarm intelligence algorithm [ 172 ] that mimics the hunting strategy of wolves. It categorizes the wolves into different roles: the chief wolf,, who leads the hunt;, who assists the leader;, who scouts and guards; and the rest. The wolves’ hunting process is generally broken down into three phases: encirclement, pursuit, and attack. During the encirclement phase, the algorithm updates positions using Equation ( 27 ): Although GWO is efficient, it needs a unique initial population. Another drawback is its slow convergence and easily falling into a local optimum [ 173 ]. Shitu Singh [ 174 ] proposed a more advanced version using Levy’s flight model to modify the population and the greedy selection method to update the path. The Grey Wolf Optimization algorithm has been successfully applied to route planning [ 175 ]. The Golden Sine Grey Wolf Optimizer (GSGWO) has been improved from the Grey Wolf Optimizer (GWO), which provides slow convergence speed and easily falls into local optimum, especially without an obstacle-crossing function [ 176 ]. GSA is also a robust metaheuristic population-based search algorithm based on gravity rules [ 177 ]. Objects are attracted to each other by the force of gravity, and this force is responsible for the global movement of all objects towards more massive objects. The masses thus interact through gravitational force. Heavy masses, which are good solutions, move more slowly than lighter masses (bad solutions). The position of the mass corresponds to the solution of the problem. The gravitational and inertial mass of bodies is determined by a fitness function. GSA can be seen as an isolated system for masses. acting on mass by mass is the equation giving the gravitational force and the gravitational acceleration caused by it ( where and represent the active gravitational mass of particle and passive gravitational mass of particle , respectively, is the distance between masses, and represents the inertia mass of particle . “G(t) is the gravitational constant that decreases iteratively” [ Like other metaheuristic systems, GSA has parameters that greatly affect its performance. The massacting on massby massis the equationgiving the gravitational force and the gravitational accelerationcaused by it ( 28 ) [ 177 ]:whereandrepresent the active gravitational mass of particleand passive gravitational mass of particle, respectively,is the distance between masses, andrepresents the inertia mass of particle. “G(t) is the gravitational constant that decreases iteratively” [ 178 ]: is the most sensitive entity in the GSA model and effectively controls the balance between the exploration and exploitation capabilities of the algorithm. and are constant parameters that affect the performance of the algorithm. As for the tuning of the mentioned parameters, many GSA variants have been developed [ The gravitational constantis the most sensitive entity in the GSA model and effectively controls the balance between the exploration and exploitation capabilities of the algorithm.andare constant parameters that affect the performance of the algorithm. As for the tuning of the mentioned parameters, many GSA variants have been developed [ 178 ].\n\nAs you can see, there are many other ways to avoid obstacles. Among these are many that use several classical or heuristic algorithms at the same time, which are also described in this article. These are commonly referred to as “hybrids”. In this article, three such algorithms are mentioned as an addition. The so-called “new hybrid navigation” algorithm consists of two independent layers, the deliberative and reactive layers. The deliberative layer plans the reference route using the A* search algorithm based on the stored preliminary information. The reactive layer takes over the reference trajectory and guides the robot autonomously along the planned route [ 25 ]. The reference path is temporary, and it can be changed by the reactive layer during movement. This layer uses the D-H error algorithm (Distance Histogram bug). It is an improved version of the bug-2 algorithm [ 42 ], which allows the robot to freely rotate at angles less than 90° to avoid obstacles. If a rotation of 90° or more is required to avoid an obstacle, the bug-2 algorithm behaves as a bug [ 44 ]. The algorithm needs prior information about the environment, which it stores as a binary grid map. The state of each grid on the map is free or occupied: free if there is no obstacle in it, and occupied if it has an obstacle. Figure 25 shows the results of [ 25 ], which shows the planned and shortest paths generated by the algorithm. Figure 26 a shows the path of the robot with the Dist-Bug algorithm, while Figure 26 b shows the behavior of the robot with the D-H error algorithm [ 25 ]. This algorithm is designed to effectively handle environments where the robot encounters obstacles during movement. During navigation, the robot can deviate from its path to avoid obstacles using reactive navigation strategies, but is always limited within the area. By ensuring the robot moves within a convex area encompassing the target node’s location, it is assured to reach the target in the presence of static obstacles by following a straight path. In certain scenarios, the mobile robot must navigate around obstacles or come to a halt when faced with an obstacle. [ 44 ]. The main difference between the hybrid navigation algorithm and NHNA is that it uses APF instead of D-H BUG in the reactive layer. NHNA did not describe any constraints on the deviation from the reference path, but HNA used the concept of roaming trails for the same purpose. Figure 27 shows the roaming traces with the preliminary map (top) and the safe trajectory of the robot on the roaming traces (bottom) [ 179 ]. For more than ten years, the approach has been extensively tested on robots, in particular on the autonomous robot Staffetta [ 179 ]. Staffetta is specifically designed for autonomous transport in hospitals, with a payload of 120 kg and a maximum speed of 1 m/s. The robot is equipped with sensors to detect nearby objects and touch sensors to avoid collisions. Furthermore, it is equipped with a laser-based localization system that allows regular position corrections. Based on the experimental experience gained, the second generation of the robot (Merry Porter™) has been further developed and is now independently transporting waste within the Modena Polyclinic. can be derived as follows: where denotes the velocity vector at the center of the moving platform, which is constrained along the longitudinal axis fixed to the robot due to nonholonomic kinematics. In the robot fixed coordinate system , a local harmonic potential field is generated [ The intelligent space learns motion control by tracking the robot’s movements [ 180 ], thus being able to learn an obstacle avoidance strategy. This learning is based on a neuro-fuzzy approximation of vector-field-based obstacle avoidance. The efficiency of navigation is crucial, as the main application tasks of a mobile robot may include, for example, the guidance of visually impaired people, which requires an immediate reaction to any disturbance. Using the artificial potential field, a collision-free trajectory is guaranteed along gradient lines. The equations of motion of the robot concerning the fixed world systemcan be derived as follows:wheredenotes the velocity vector at the center of the moving platform, which is constrained along the longitudinal axis fixed to the robot due to nonholonomic kinematics. In the robot fixed coordinate system, a local harmonic potential fieldis generated [ 181 ]. According to Laplace’s equation, this harmonic field corresponds to at (0,0) in a 2D Cartesian : and the associated gradient : The solution to ( 32 ) gives the potential of a singular point of powerat (0,0) in a 2D Cartesianand the associated gradient in the middle of the obstacle, , where is the distance between the target and the center of the obstacle, and is the radius of the circular safety zone. As circular obstacle protection zones cannot be applied directly [ The configuration of the fundamental potential field consists of a negative unit singular point in the target and a positive singular point of magnitudein the middle of the obstacle,, whereis the distance between the target and the center of the obstacle, andis the radius of the circular safety zone. As circular obstacle protection zones cannot be applied directly [ 181 ], elliptical safety zones have been designed. Each obstacle has one safety ellipse, but if there are multiple obstacles, two ellipses are needed on either side of the selected route. In this case, the two potential fields must somehow be “merged” to form a single potential field. A good alternative method is to always consider only the nearest safety ellipse. However, this requires switching potential fields at the intersection of equidistant lines between ellipses. This switching, as shown in Figure 28 , results in a noncontinuous gradient field. . When switching between gradient lines, the scattering appears as oscillations. This effect can be reduced by smoothing the gradient lines near the equidistance line: in the boundary layer along the equidistance line between the two safety zones by spatial domain smoothing. The gradient of the resulting smooth gradient field is the weighted sum of the two gradients. The control inputs are usually the outputs of some actuator. The gradient is implemented as a velocity field. Kinematics constrains robot motion from three-dimensional to two-dimensional along the velocity vector. We assume that the state variables and the kinematic parameters L and W are known. The orientation of the robot’s angle must be controlled to be colinear concerning the gradient . So the desired orientation at the point ( , ) is [ In this case, the sliding surface can be described by the line. When switching between gradient lines, the scattering appears as oscillations. This effect can be reduced by smoothing the gradient lines near the equidistance line: in the boundary layer along the equidistance line between the two safety zones by spatial domain smoothing. The gradient of the resulting smooth gradient field is the weighted sum of the two gradients. The control inputs are usually the outputs of some actuator. The gradientis implemented as a velocity field. Kinematics constrains robot motion from three-dimensional to two-dimensional along the velocity vector. We assume that the state variablesand the kinematic parameters L and W are known. The orientation of the robot’s anglemust be controlled to be colinear concerning the gradient. So the desired orientation at the point () is [ 181 ]: , , where is defined by the orientation error , as follows: Because speed control is simple, the desired direction of movement, whereis defined by the orientation error, as follows: The sliding surface of the orientation error is defined as follows: surface is created, but at the same time, the direction of motion is changed, and changing the sign of should be avoided. This can be avoided by monotonically decreasing by controlling the value of [ . Differentiating this function along the trajectories of the system: where describes the rate of change in the curvature of the gradient along the track lines, , and the is A sliding mode along thesurface is created, but at the same time, the direction of motion is changed, and changing the sign ofshould be avoided. This can be avoided by monotonically decreasingby controlling the value of 181 ]. The Lyapunov function in this case is. Differentiating this function along the trajectories of the system:wheredescribes the rate of change in the curvature of the gradient along the track lines,, and theis Just a few more examples of hybrid algorithms are provided below:\n• None Ref. [ 182 ] presents a hybrid path-planning algorithm based on improved A* and an artificial potential field for unmanned surface vehicle formations.\n• None Researchers have also exploited GA hybridization with other approaches to MR navigation for better results in route planning problems, such as GA-PSO [ 183 ], GA-FL [ 184 ], and GA-ANN [ 185 ].\n• None In [ 186 ], a hybrid genetic algorithm (HGA)-based approach applied to the image denoising problem is presented. HGA provides the dynamic mutation rate and a switchable global-local search method for the mutation operator of the ordinary genetic algorithm [ 187 ].\n• None In [ 188 ], the dynamic modeling of the impact of polymer insulators in polluted conditions based on the HGA-PSO algorithm is presented\n• None Ref. [ 189 ] used a Voronoi diagram and the particle swarm optimization algorithm to achieve multirobot navigation and obstacle avoidance.\n• None Ref. [ 190 ] presents a UGV routing algorithm based on an improved A* with an improved artificial potential field.\n• None Ref. [ 43 ] used VFH*, combining the VFH+ local obstacle avoidance algorithm and the A* path planning algorithm.\n\nNavigation and route planning are the central difficulties of mobile robots and have been the subject of decades of research. As a result, several methodologies have been presented and applied to the problem of route planning for mobile robots. Strategies for mobile robot optimization can be classified into deterministic or classical approaches and nondeterministic or heuristic approaches. Traditional algorithms execute a given task step by step according to predefined instructions, and their results are exact and deterministic. (One of the simplest algorithms is the Pythagorean theorem, which determines a third parameter in an identical way given two input parameters, and the term heuristic is derived from the Greek word heuresis, which means to find.) Theoretically, constructing an exact solution procedure would make it possible to calculate how to reach the goal based on the robot’s current position and by analyzing all possible paths. The problem is that the situation becomes too complex above a given complexity of the environment. A heuristic algorithm does not consider all possible steps but only decides according to some logic based on a particular part of the problem space. A considerable advantage of heuristic algorithms is that they can deliver results relatively quickly for high-complexity problems with little computation. However, they have the disadvantage that the optimal solution cannot be guaranteed completely. They are helpful when the solution to a problem cannot be found within a foreseeable time by a conventional method that provides an exact solution. They can also provide an optimal or approximate solution for large problem sizes. Among the earliest developed error avoidance algorithms, they are straightforward to calibrate but time-intensive. These methods are not goal-oriented; they trace edges without considering the ultimate objective [ 37 ]. The Dijkstra algorithm is a graph search algorithm designed to find paths and determine the shortest paths [ 5 ]. The Floyd-Warshall (FW) algorithm uses a weighted and directed graph and can compute opposing weighted edges. The solutions are derived from the previous results, and multiple solutions can be generated [ 191 ]. This algorithm finds the shortest path between each pair of nodes and is particularly useful when the distance between all pair nodes of the graph needs to be determined. However, it is inefficient for large graphs due to its high memory and time requirements. The Bellman-Ford (BF) algorithm can find the shortest path from one peak to another, which is simple and does not require complex data structures to apply. The algorithm iteratively extends the search to all nodes, not just along the current shortest path. For this reason, it can be slower than Dijkstra’s algorithm for positive-weight graphs but can handle graphs with negative weights, whereas Dijkstra’s algorithm cannot. If there is a negative cycle in the graph, the Dijkstra algorithm would run the cycle infinitely, as this would theoretically result in an infinitely negative cost. In contrast, the BF algorithm would detect this and terminate [ 192 ]. This algorithm can handle opposing weight edges and detect negative cycles, an advantage for specific problems. Likewise, artificial potential field (APF) is a simple technique for avoiding obstacles, but robots following this principle can get stuck in so-called local minima [ 37 42 ]. This is a time-consuming algorithm, as the robot can stop before the obstacle until it moves. The Bug algorithm is also an early version of the obstacle avoidance algorithm used in robot navigation [ 34 ]. The gap tracking method (FGM) is another early obstacle avoidance algorithm used in environments where the robot must navigate narrow spaces. Still, it can not avoid U-shaped obstacles [ 37 193 ]. Fuzzy logic (FL) has been developed among the heuristic algorithms for various applications, including obstacle avoidance robotics [ 71 72 ]. Since their initial research, genetic algorithms (GAs) have been widely applied to solve various optimization problems, including obstacle avoidance algorithms [ 79 ]. Simulated annealing (SA) and Tabu search (TS) have proven to be very effective and robust in solving a wide range of problems across various applications. They are also helpful in dealing with issues where specific parameters are not known in advance. These properties are missing in all conventional optimization techniques [ 88 ]. They apply an appropriate cost function to give feedback to the algorithm on the progress of the search. The difference in principle is how and where domain-specific knowledge is used. For example, SA obtains such information mainly from the cost function. The disturbed items are selected randomly, and the acceptance or rejection of disturbances is based on the Metropolis criterion, which is a function of cost. The cooling schedule also has a significant impact on the algorithm’s performance. It must be carefully tailored to the problem domain and the specific problem instance. TS differs from GA and SA because it has an explicit memory component. At each iteration, the neighborhood of the current solution is partially explored, and the move is made toward the best nontaboo solution in that neighborhood. The neighborhood function and the size and content of the Tabu list are problem-specific. Memory structures also influence the direction of the search. Particle swarm optimization (PSO) is a population-based heuristic optimization method derived from standing wave theory [ 48 ]. The cuckoo search algorithm (CSA) was introduced as an efficient and straightforward global search technique among evolutionary algorithms [ 194 ]. The Artificial Bee Colony (ABC) algorithm was developed to model the behavior of living organisms and is one of the evolutionary algorithms [ 109 ]. While machine learning requires human intervention, deep learning can learn from mistakes. Deep learning requires a more significant amount of data, which demands higher computational power. In deep learning, algorithms learn autonomously by analyzing large amounts of data. In contrast, reinforcement learning requires feedback from the agent to know what actions lead to the desired outcome. Significant developments in neural networks occurred in the 1980s and beyond and have been applied to obstacle avoidance robotics [ 125 ]. In control systems, the star of reinforcement learning solutions is now gone, replaced by data-driven MPC solutions that can provide theoretical guarantees of performance [ 195 ]. It is questionable whether a suitable fitness function can solve all our problems, not to mention the theoretical guarantees of stability or convergence. Nevertheless, it is worth using machine learning algorithms in engineering because, presumably, they will be able to solve more and more routine tasks for us. Furthermore, there is also the question of how data-driven MPC algorithms solve all control theory problems. Specifically, where is the space left for model-based robust control? The Hybrid Navigation Algorithm (HNA) with wandering trails combines different methods and techniques for optimal route planning, which has been applied to a partially known environment [ 179 ]. Recent developments have resulted in a New Hybrid Navigation Algorithm (NHNA) similar to the HNA. It is a complete algorithm that uses several approaches to achieve efficient and stable robot navigation. However, it cannot be used in unknown environments as it requires prior environmental information [ 25 193 ]. Sliding mode (SM) algorithms employ several methods and have seen significant development and application, especially in robotics and control systems [ 181 ]. Essential characteristics of algorithms are convergence time, computation time, and memory requirements. The convergence time is required for the algorithm to reach convergence, i.e., to achieve a stable or desired state. This time may vary depending on the algorithm type, the task’s nature, and the initial conditions. The goal is to make the algorithm converge as fast as possible to solve the task or problem efficiently. Computation time and memory requirements are closely related. The more complex the environment in which we want to navigate the robot, the more data and more complex computations are needed to find the optimum. Among the previously developed methods, heuristic approaches are relatively new and have significant applications in mobile robot navigation. Contemporary research increasingly focuses on optimizing algorithms through hybridization to achieve superior performance. Historically, classical methodologies were prevalent but faced limitations such as susceptibility to local minima and high computational demands. In response, researchers have shifted towards heuristic methods, particularly effective in uncertain or unknown environments. These heuristic approaches, often enhanced by hybridization with classical methods, have proven successful in complex three-dimensional workspaces, such as those encountered by underwater, unmanned aerial vehicles, and humanoid robots. This shift underscores the improved adaptability and efficiency of heuristic strategies over classical approaches in dynamic settings. As technology advances and robotics becomes increasingly integrated into various aspects of our lives, obstacle avoidance algorithms are poised to undergo significant developments to meet the demands of emerging applications. With the proliferation of machine learning techniques, we can expect obstacle avoidance algorithms to incorporate more advanced learning-based approaches. These algorithms will be capable of adapting and improving their performance over time through experience and feedback, leading to more efficient and robust obstacle avoidance in dynamic environments. Future obstacle avoidance systems will rely on sophisticated sensor fusion techniques to integrate data from multiple sensors, such as LIDAR, cameras, radar, and ultrasonic sensors. By combining information from diverse sources, these algorithms will achieve a more comprehensive understanding of the environment, enhancing their ability to detect and avoid obstacles accurately. Future obstacle avoidance algorithms prioritize real-time adaptive planning to navigate complex and dynamic environments effectively. These algorithms will continuously analyze sensor data and adjust robot trajectories to avoid obstacles and navigate changing scenarios in real-time, unreal-time, and efficient robot operation. Collaborative obstacle avoidance algorithms will become increasingly important in environments where multiple robots or autonomous vehicles operate concurrently. These algorithms will enable robots to communicate and coordinate their movements to avoid collisions and optimize path planning, leading to smoother and more efficient operations in shared spaces. Drawing inspiration from nature, future obstacle avoidance algorithms may incorporate bio-inspired principles, such as swarm intelligence or mimicry of animal behavior. These approaches could lead to innovative solutions for navigating challenging environments, leveraging the collective intelligence of swarms, or mimicking the agility and adaptability of animals in natural habitats. As robots become more prevalent, ethical considerations regarding obstacle avoidance will gain prominence. Future algorithms must balance efficiency with moral considerations, prioritizing human safety and well-being in crowded environments. Additionally, human-robot interaction will play a crucial role, with obstacle avoidance algorithms designed to anticipate and respond effectively to human intentions and behaviors. With these exciting developments, researchers and engineers can pave the way for safer, more efficient, and more adaptive robotic systems in various applications."
    },
    {
        "link": "https://academic.oup.com/jcmc/article/28/5/zmad014/7248794",
        "document": ""
    },
    {
        "link": "https://sciencedirect.com/science/article/pii/S2212827124008667/pdf?md5=304bfc4aa9713c1cc36dba32d7a8af1a&pid=1-s2.0-S2212827124008667-main.pdf",
        "document": ""
    },
    {
        "link": "https://fidus.com/blog/driving-the-future-the-critical-role-of-embedded-software-in-autonomous-vehicles",
        "document": "Autonomous vehicles (AVs) are at the forefront of a technological revolution, poised to transform transportation with promises of safer, smarter, and more efficient mobility solutions. At the core of these capabilities is embedded software that drives the perception, decision-making, and control systems in AVs. Embedded software enables AVs to interpret real-time data, communicate seamlessly across complex subsystems, and make split-second decisions crucial for safety and reliability.\n\nWith over 20 years of experience, Fidus Systems is uniquely positioned to support the development of embedded software for autonomous vehicles. Our expertise spans a comprehensive range of services, including embedded software development, FPGA design, hardware, PCB layout, signal and power integrity, and system integration.\n\nBy combining our deep understanding of automotive standards and advanced engineering practices, Fidus ensures that AV solutions are robust, compliant, and ready to meet the demands of the future. This blog explores the essential architecture and components of embedded software in AVs, the unique challenges it faces, and the technological innovations that are shaping its future.\n\nAutonomous vehicles represent the future of transportation, promising safer, more efficient mobility solutions. These vehicles are designed to operate with minimal human intervention by using advanced embedded software to interpret sensor data, make decisions, and control mechanical functions. AVs rely on complex networks of cameras, LiDAR, radar, etc. and other sensors to understand their surroundings, with embedded software playing a critical role in analyzing this data and ensuring the vehicle’s responses are both timely and appropriate.\n\nToday’s vehicles contain hundreds of microcontrollers and sensors, all of which rely on in-vehicle networking to function properly. The typical communication interfaces include LIN, CAN, FlexRay, and Ethernet. It might be helpful to note that the AV embedded software not only manages inter-vehicle communications but also plays a critical role in managing these in-vehicle networks.\n\nEmbedded software is specialized code that operates directly on hardware components, such as microcontrollers and sensors, within a constrained environment. Unlike general-purpose software, embedded software in AVs is specifically designed to handle real-time constraints, strict safety requirements, and hardware limitations. It interprets data from various sensors, communicates between subsystems, and drives control commands that dictate vehicle movement, all while ensuring reliability and efficiency.\n\nTo learn more about Fidus’ expertise in embedded software for ARM®, PowerPC™, MIPS®, and x86 processors, check out our Embedded Software Development Services.\n\nDeveloping embedded software for AVs is inherently challenging due to the high level of precision, reliability, and speed required to operate in unpredictable environments. Below are the primary challenges that automotive embedded software developers face and the solutions Fidus offers.\n\nAutonomous vehicles must process enormous volumes of data from sensors (e.g., cameras, LiDAR, radar) in real time to make instant decisions, such as braking for a pedestrian or avoiding an obstacle. Embedded software must prioritize, filter, and analyze this data at incredible speeds, as any latency could jeopardize safety. Real-time decision-making requires software optimized for efficiency, with algorithms tailored for high-speed data handling and low latency.\n\nFidus designs optimized processing algorithms and utilizes dedicated microcontrollers to handle real-time data demands. Our embedded systems prioritize critical data streams and execute commands with minimal latency, enhancing AV safety.\n\nSafety is the most critical factor for AV software, as any failure in system operation could endanger lives. Embedded software in AVs must comply with stringent safety standards, such as ISO 26262, which dictates functional safety requirements for automotive applications. Beyond compliance, the software must also incorporate fail-safe mechanisms to maintain operation even if part of the system fails.\n\nAt Fidus, we develop ISO 26262-compliant software solutions with rigorous testing and validation processes to ensure reliability. Our approach includes redundancy and fault-tolerant designs that provide backup pathways in case of software or hardware failures, ensuring continuous operation.\n\nAutonomous vehicles comprise various subsystems—each with its own embedded software—such as the perception system, decision-making unit, control system, and communication modules. These systems must operate in sync to ensure smooth and reliable vehicle operation. The embedded software must effectively integrate and manage these subsystems, ensuring seamless communication and synchronized actions.\n\nFidus excels in System Integration, using standardized protocols and robust communication architectures to unify multiple subsystems. Our integration strategies enable AVs to operate as cohesive units, minimizing miscommunication and enhancing performance.\n\nWith AVs increasingly connected to external networks (e.g., Vehicle-to-Everything or V2X), cybersecurity becomes a crucial concern. Embedded software must incorporate protective measures to defend against cyber threats, including unauthorized access, data breaches, and control hijacking. Cybersecurity in embedded software involves secure boot mechanisms, data encryption, and authentication protocols to protect both the vehicle and the user’s data.\n\nAt Fidus, we prioritize cybersecurity in our embedded software solutions, integrating industry-leading protocols to safeguard autonomous systems from cyber risks. Our embedded security practices ensure that AV systems remain resilient, safe, and compliant in the face of evolving threats.\n\nFor more insights into embedded system security, explore our on-demand webinar on Secure Boot in Embedded Systems: The Foundation of Device Security.\n\nAV software requires regular updates for feature enhancements and security patches. Over-the-Air (OTA) updates allow these updates to be installed remotely, ensuring AVs are always operating with the latest improvements. However, OTA updates must be secure, reliable, and minimize downtime to avoid disrupting vehicle operations.\n\nFidus provides robust OTA solutions that securely manage software updates without requiring AVs to be offline. Our OTA systems are designed for minimal disruption, allowing AVs to continue operating while receiving updates in the background.\n\nEmbedded software in AVs is organized across multiple systems, each with specialized functions to handle distinct operational tasks. These systems collectively enable AVs to perceive their environment, make decisions, and control movement.\n\nEmbedded systems in AVs include sensors, Electronic Control Units (ECUs), actuators, and communication interfaces. Each of these components has a distinct role, from data collection to executing control commands.\n\nTo dive deeper into Fidus’ expertise in system architecture, explore our Hardware Design and PCB Layout Services, which ensure seamless integration across AV components.\n\nEmbedded software in AVs manages a range of advanced technologies, each playing a crucial role in the vehicle’s ability to interpret and interact with its surroundings.\n\nFidus has developed robust solutions to address the unique challenges posed by AV embedded software:\n• Redundancy and Fault Tolerance: We design fault-tolerant architectures that incorporate redundancy, ensuring AV systems maintain functionality in the event of component or subsystem failure.\n• OTA Updates: Fidus enables AVs to stay up-to-date with secure OTA solutions, allowing seamless updates for new features and security patches without requiring downtime.\n• Cybersecurity Protocols: Our cybersecurity solutions incorporate secure boot, encryption, and advanced authentication methods, protecting AVs from potential vulnerabilities.\n\nFor more insights, watch our on-demand webinar : Implementing Secure Software Upgrades in Embedded Systems: Best Practices and TPM Integration.\n\nFidus has a proven track record in developing embedded software solutions that meet the rigorous demands of autonomous vehicle applications. Here’s a recent project where we applied our expertise to solve critical challenges in AV development.\n\nOur client, a leading player in the automotive industry, required a multi-sensor fusion system that could process data from LiDAR, radar, and cameras in real time for obstacle detection, collision avoidance, and enhanced situational awareness. The system needed to be highly reliable, support fault tolerance, and comply with stringent automotive safety standards.\n\nFidus developed a comprehensive embedded software solution that integrated data from multiple sensors into a unified perception model. Our team designed optimized algorithms for real-time processing, leveraging a high-performance Real-Time Operating System (RTOS) to ensure minimal latency. The software architecture included fault-tolerant features, allowing the system to maintain functionality even in the event of component failure.\n\nThe solution significantly improved the AV’s ability to detect obstacles and respond to changing road conditions. By unifying data from LiDAR, radar, and cameras, Fidus enhanced the AV’s situational awareness and reliability. The client was able to accelerate their development timeline while achieving compliance with ISO 26262, thanks to the rigorous testing and validation we applied.\n\nFidus collaborated with a major automotive client to develop embedded software for an advanced electric inverter, which controls a 3-phase AC motor in heavy-duty vehicles such as trucks and industrial machinery. This system is designed to enable precise control of motor power, supporting a range of vehicles with both manual and automated control capabilities, which are integral to the emerging autonomous and semi-autonomous applications in the heavy automotive sector. The system needed to be flexible, supporting motors from 40KW to 400KW and handling power inputs from 125V to 800V DC.\n\nThe embedded software was developed using specialized tools, including Simulink, PXROS (an operating system tailored to automotive applications), and the HiTEK compiler. To ensure software quality and safety, the project employed rigorous testing with tools like Vectorcast and Polyspace, maintaining compliance with automotive standards critical to AV systems.\n\nTo simulate real-world operational conditions, Fidus utilized Hardware-in-the-Loop (HIL) testing, which allowed the team to validate control algorithms and ensure reliability under actual driving scenarios. These validation techniques are essential in autonomous vehicle applications, where real-time responses to environmental changes are critical.\n\nThe project faced several challenges, from managing global team coordination to addressing stringent ISO 26262 safety standards. Initially, the development was focused on creating a prototype, but as safety requirements evolved, Fidus adapted the design to meet full safety compliance. This transition required substantial modifications to both the software and hardware architectures, including the incorporation of redundancy and fault-tolerance mechanisms, which are fundamental in autonomous applications.\n\nA unique aspect of this project was the use of a resolver to track motor position accurately, a technology that plays a crucial role in autonomous systems for navigation and control. Additionally, the project required implementing advanced safety mechanisms, ensuring that the embedded software could handle mission-critical tasks reliably—a capability essential for the operation of autonomous heavy vehicles.\n\nFidus’ embedded software expertise enabled the client to achieve their development goals while meeting the necessary automotive safety and compliance standards. This project highlights Fidus’ capability to design, develop, and validate embedded systems that are adaptable for autonomous and semi-autonomous applications in heavy vehicles, such as automated logistics or public transport fleets.\n\nAs AV technology evolves, several trends will shape the future of embedded software, from advances in AI to the role of edge computing and IoT.\n• Legislative and Regulatory Considerations: As AV technology advances, regulatory bodies will continue to implement new standards governing safety, data privacy, and cybersecurity. Compliance with these regulations is crucial for AVs to operate legally and safely in various regions.\n\nEmbedded software is the bedrock of autonomous vehicle technology, enabling real-time data processing, robust decision-making, and seamless system integration. As AVs become more advanced, the demands on embedded software will continue to grow, encompassing new technologies like AI, machine learning, and edge computing, alongside increasing safety and regulatory requirements.\n\nWith deep expertise in embedded software, system architecture, and cybersecurity, Fidus Systems is a trusted partner for organizations navigating the complex challenges of AV development. Our holistic approach integrates hardware and software solutions, providing clients with end-to-end support, from initial concept to final implementation. As autonomous technology progresses, Fidus is committed to driving innovation, ensuring AVs remain safe, reliable, and compliant in an ever-evolving industry landscape.\n\nContact Fidus Today to learn how our embedded software expertise can support your autonomous vehicle project and ensure your project achieves the highest standards of safety, performance, and innovation."
    },
    {
        "link": "https://mdpi.com/1424-8220/21/6/2140",
        "document": "With the significant advancement of sensor and communication technology and the reliable application of obstacle detection techniques and algorithms, automated driving is becoming a pivotal technology that can revolutionize the future of transportation and mobility. Sensors are fundamental to the perception of vehicle surroundings in an automated driving system, and the use and performance of multiple integrated sensors can directly determine the safety and feasibility of automated driving vehicles. Sensor calibration is the foundation block of any autonomous system and its constituent sensors and must be performed correctly before sensor fusion and obstacle detection processes may be implemented. This paper evaluates the capabilities and the technical performance of sensors which are commonly employed in autonomous vehicles, primarily focusing on a large selection of vision cameras, LiDAR sensors, and radar sensors and the various conditions in which such sensors may operate in practice. We present an overview of the three primary categories of sensor calibration and review existing open-source calibration packages for multi-sensor calibration and their compatibility with numerous commercial sensors. We also summarize the three main approaches to sensor fusion and review current state-of-the-art multi-sensor fusion techniques and algorithms for object detection in autonomous driving applications. The current paper, therefore, provides an end-to-end review of the hardware and software methods required for sensor fusion object detection. We conclude by highlighting some of the challenges in the sensor fusion field and propose possible future research directions for automated driving systems."
    },
    {
        "link": "https://researchgate.net/publication/346735994_Implementation_of_a_Sensor_Big_Data_Processing_System_for_Autonomous_Vehicles_in_the_C-ITS_Environment",
        "document": "car big data is required. This study ﬁrst aims to design and develop such a platform to improve the function of providing vehicle and road condition information of the previously deﬁned central Local Dynamic Map (LDM). Our platform extends the range of connected car big data collection from OBU (On Board Unit) and CAN to camera, LiDAR, and GPS sensors. By using data of vehicles being driven, the range of roads available for analysis can be expanded, and the road condition determination method can be diversiﬁed. Herein, the system was designed and implemented based on the Hadoop ecosystem, i.e., Hadoop, Spark, and Kafka, to collect and store connected car big data. We propose a dir ection of the cooperative intelligent transport system (C-ITS) development by showing a plan to utilize the platform in the C-ITS environment. Internet of Things (IoT) is a technology in which all its elements, such as humans, objects, and services, are connected through wireless communication, and an object capable of wireless communication with a sensor is referred to as an IoT edge device. The core characteristic of IoT is connectivity . New services are provided to users by collecting and sharing new information generated by IoT edge devices between devices. Therefore, IoT is one of the signiﬁcant foundation technologies of the 4th industrial revolution, where various types of convergence and exchange through innovation IoT technology has gradually evolved, and its application range has expanded from devices used in everyday life, such as smartwatches, to connected cars. A connected car uses wireless communication technology and exchanges information with connected cars and infrastructure. Despite the progress and development of research on autonomous driving technology that autonomously recognizes and determines the surrounding environment through sensors, such as radar , LiDAR, GPS, and camera, autonomous vehicles still do not guarantee safety [ to an autonomous vehicle, it is possible to provide a service that guarantees driver comfort and safety . Therefore, in this study , an autonomous vehicle with connected technology is set as an IoT edge device. In order to provide IoT services to IoT edge devices, a platform that can collect and process all data In the automotive ﬁeld, the environment in which autonomous vehicles are connected to other vehicles and infrastructure through wireless communication is deﬁned as a cooperative intelligent\n\ntransport system (C-ITS). C-ITS changed the existing ITS from road management to user safety through smooth two-way communication and collaboration between vehicles, roads, and improved safety when driving a connected car by supplementing the limitation that immediate response is complicated [ V ehicle to Everything (V2X) is a technology that communicates with vehicles, infrastructure, and people through wireless networks in C-ITS. Connected cars deliver various road and surrounding information using V2X and use the information for autonomous driving along with vehicle sensor data. Institute (ETSI) to store data among various elements constituting C-ITS, stores data such as high-deﬁnition map (HD map), weather, and tra c information into four layers [ roughly divided into central LDM and vehicle LDM, and data generated by vehicles and infrastructure are collected from the central LDM. In addition, the central LDM collects and provides additional information necessary for safe driving, such as weather information and accident information, to vehicle LDM. Therefore, in the C-ITS environment, the central LDM acts as a platform to collect data Camera, LiDAR, and GPS sensors mounted on the connected car are critical sensors for providing connected car services and can be used for processing, such as HD map creation and detection of ]. However, in the existing C-ITS environment, the central LDM collects only simple driving information such as speed, position, and direction from the connected car , and the connected car acts as a device to receive services. It is also conﬁgured to focus on collecting ]. However, because the RSU is mainly installed on highways, road conditions cannot be determined in areas where the RSU is not installed, such as national highways, and it is time-consuming and expensive to install and use additional equipment for implementing C-ITS. On the other hand, by utilizing the connected car’s sensor data while driving, it is possible to reduce the number of additional measuring devices to be installed in the RSU and to build an e cient C-ITS infrastructure. In addition, data collected directly from multiple vehicles on the road improves road situation awareness and the accuracy of tra c information. The central LDM, which collects vehicle data, has no analytics and processing capabilities implemented. Therefore, we intend to build a platform that provides connected car data by collecting and analyzing all big data of vehicles, such as sensor data and simple driving information collected by the existing central LDM. The platform created by separating the vehicle data collection function of the central LDM can be asynchronously interlocked with the central LDM to provide the processed data according to the central LDM standard to be delivered to the vehicle. In this paper, we pr opose a platform, as shown in Figure , that includes a big data processing system that collects and processes vehicle big data generated from connected cars and a messaging system that delivers large-capacity vehicle sensor data and tra c information in real time. The proposed system was built as a testbed, and its usefulness and scalability were veriﬁed through scenarios. The information analyzed by the proposed vehicle big data analysis system is delivered to the central LDM and vehicle LDM in near real-time to increase the safety of autonomous driving. The proposed big data processing system was built using the Hadoop ecosystem. Vehicle data are stor ed in the Hadoop distributed ﬁle system (HDFS) and MariaDB, and Apache Spark is used for big data analysis. MariaDB’s structured data are periodically transferred to and loaded into the HDFS, and the big data and analysis results stored in MariaDB and HDFS are used for the development of various services and business models. Data transfer between the vehicle and the server is realized through the Apache Kafka messaging system. Data transmitted in real-time from the vehicle to the backend server through Kafka is analyzed in real-time using the Spark Streaming function. The remainder of this paper is structured as follows. Section introduces research trends, e.g., LDM and big data platforms. Section proposes the structure of the entire platform, and Section shows an example of implementing a platform and using autonomous vehicle data. Section concludes\n\nSung et al. (2017) intended to develop a driving environment prediction platform based on road tra c information collected for driver safety . The method is changed to collecting data in real-time from mobile-type sensors rather than from existing ﬁxed-type sensors, thus allowing for various-big-data collection. The data are analyzed on a driving environment analysis platform based on the Hadoop ecosystem, and information is provided through a web environment. However, in the research, vehicle sensor data are limited to road surface temperature, humidity , and vehicle speed. Among the research on the structure of a platform that collects connected car data, Han (2018) collects vehicle interior data and passenger’s body reaction information through OBD and smart watch. The collection and processing phases were designed using GS1, a global standard for business communication, to establish standards for sharing information collected and stored in a connected environment with the outside world. However, the platform proposed in the paper focuses on collecting vehicle information and has the disadvantage that messages shared among connected units are limited to simple vehicle status information such as driving and stopping times. Islam et al. (2020) aimed to implement a platform for developing connected car services, where the platform users could access data collected from connected cars [ platform directly deliver messages such as BSM to a connected car . Connected car data collected by the platform are message-based information such as vehicle speed, location, and time. However, in the existing r esearch, autonomous driving environment data are deﬁned as CAN data, speed, and location information collected from the in-vehicle network. A majority of the data collected from a vehicle only serves to deliver the vehicle driving information rather than to derive new information based on the data. Therefore, there is a need for an additional method to utilize vehicle data, and for this, it is necessary to collect various vehicle data such as images and LiDAR data. Biral et al. (2019) built a C-ITS application for safety using LDM [ proposed in the paper seeks to collect vehicle driving information without equipment by utilizing infrastructure such as ORU installed on the road. When developing LDM, Roh (2020) points out the limitations of the evaluation method focusing on rapid information delivery rather than information reliability [ In the case of existing studies on LDM, the focus is on developing and evaluating the LDM performance. However , research on the process and application of generating information to be provided by LDM is insu cient. Therefore, it is necessary to propose a platform that can apply an algorithm based on various connected car big data and their usage. Y ao et al. (2019) used black box images to detect tra c accidents [ ]. It has been shown that tra c accidents or abnormal situations can be grasped from a dynamic camera, rather than a ﬁxed camera image such as CCTV . Therefore, road accidents can be identiﬁed with vision data of autonomous vehicles, and vision data should be collected and used on a platform. The goal of the proposed C-ITS environment is to incorporate the utilization of vehicle big data. By transmitting vehicle big data from the platform to the central LDM, the accuracy and quality of Figure a is a C-ITS environment based on the existing central LDM that processes the collected data and transmits them to the vehicle LDM. The central LDM detects unexpected situations on the road through the RSU and receives information on the road conditions fr om the tra c information central. However, the ﬁxed RSU can only gather information on a limited area of about 1 km, making it di cult to assess the road conditions accurately . Another disadvantage is that additional equipment is required to utilize the RSU for C-ITS. However , vehicles are also equipped with sensors similar to the\n\nobject information is converted into JSON and sent to the Kafka server. The raw images collected in Vision are deliver ed to Kafka servers in JSON format while driving. After driving is ﬁnished, they are transmitted to the HDFS, a large-capacity data storage in the form of video ﬁles. LiDAR data are also transferred directly to the HDFS, similar to V ision’s raw images. The platform conﬁgured by using the Hadoop ecosystem is described in Figure . The platform can be primarily divided into a data collection module, data storage module, and data processing and identified object information is converted into JS ON and sent to the Kafka server. The raw images collected in Vision are delivered to Kafka servers in JSON format while driving. After driving is finished, they are transmitted to the HDFS, a large-ca pacity data storage in the form of video files. LiDAR data are also transferred directly to The platform configured by using the Hadoop ecosystem is described in Figure 7. The platform can be primarily divided into a data collection module, data storage module, and data processin g Figure 7. Architecture of the platform in the proposed C-ITS environment. Kafka is distributed across multiple servers, and th e risk of message loss is low because messages are replicated and stored on multiple servers even during normal times. Therefore, the Kafk a messaging system is the core system of the data collection module. In t he data collection module, an autonomous vehicle driving on the road functions as a Kafka publishe r and delivers the data to the Kafka server. In this process, the data collecte d from each sensor are collected from the topic responsible for each sensor. If the topic for each sensor is separated, the transmitted data can be parsed appropriately and used for analysis. Even if a problem occurs in one topic, other data can be collected continuously. In addition, Kafka is highly compatible and can not only connect within the platform, e.g., MariaDB, HDFS, and Spark, b ut also deliver messages to the service system outside the platform. Therefore, the service system serves as Kafka’s consumer. In addition, as multiple of the Hadoop ecosystem. It is a ppropriate to store the structured data collected from vehicles in a database that can be systematic MariaDB is used in the proposed system because it is management system (RDBMS) that guarantees h igh stability. Conversely, the HDFS is more suitable Figure 7. Architecture of the platform in the proposed C-ITS environment. Kafka is distributed across multiple servers, and the risk of message loss is low because messages are replicated and stored on multiple servers even during normal times. Therefore, the Kafka messaging system is the core system of the data collection module. In the data collection module, an autonomous vehicle driving on the road functions as a Kafka publisher and delivers the data to the Kafka server . In this process, the data collected from each sensor are collected from the topic responsible for each sensor. If the topic for each sensor is separated, the transmitted data can be parsed appropriately and used for analysis. Even if a problem occurs in one topic, other data can be collected continuously . In addition, Kafka is highly compatible and can not only connect within the platform, e.g., MariaDB, HDFS, and Spark, but also deliver messages to the service system outside the platform. Therefore, the service system serves as Kafka’s consumer . In addition, as multiple consumers can access a topic, the central LDM, vehicles, and external a liated organizations can simultaneously access the analysis topic. The data storage module consists of Maria DB and HDFS, the representative loading framework of the Hadoop ecosystem. It is appropriate to store the structured data collected from vehicles in a database that can be systematically managed after designating a schema. Among several databases, MariaDB is used in the proposed system because it is a lightweight, fast, and large relational database management system (RDBMS) that guarantees high stability . Conversely , the HDFS is more suitable for storing large amounts of data, raw image data, or Vision’s LiDAR data collected fr om vehicles. In\n\naddition, owing to the rapidly increasing characteristics of vehicle data, data collected from MariaDB within a certain period are periodically transferred to the HDFS. Information such as GPS and IMU data, which are sensor data, vehicle driving information, object type identiﬁed by Vision, and weather , are deﬁned in the schema, as shown in Figure . addition, owing to the rapidly increasing characteristics of vehicle data, data collected f rom MariaDB within a certain period are periodically transferred to the HDFS. Information such as GPS and IMU data, which are s ensor data, vehicle driving information, object type identified by Vision, and weather, are de fined in the schema, as shown in Figure 8. As the data of each table of MariaDB are continuou sly generated while driving, it is designed in a 1:M structure. The Driver table contains the driver’s identification information, and the Drive t able includes information on the driver’s driving situat ion. Therefore, the Drive table contains all data from the moment the engine is turned on to the moment the engine is turned off. The GPS and IMU tables store each sensor data value generated du ring driving, and the Vision table records the timestamp when an object is detected while driv recorded in the Object table based on the timestamp of the Vision table. The position of the object in the frame is expressed as the bounding box’s pixel po sition, and based on this, the size of the object and its coordinates in the real coordinate plane can be known. The HDFS stores Vision raw image data or LiDAR delivered through Kafka. I n addition, raw image data of Vision can be converted into an image file after reprocessing and st ored, and preserved data transferred from Maria DB are also loaded. processes data requiring real-time processing through streaming, and is used when analyzing large- scale data for specific purposes. Therefore, Spark and Zep pelin are used for processing and analy sis modules. Spark is an open-source cluster computin g framewo rk that allows users to analyze data results in multiple formats. At this time, data storag e formats such as CSV and parquet are also used, but because data stored in the HDFS can be analyzed In addition, Spark provides a streaming function. This function i s used when analyzing data collected in real time through Kafka. For example, GPS data collected from vehicles can be streamed, and the real-time location and speed of each vehicle can be identified to de termine whether a specific road is congested. When real-time road conditions are ident ified through Spark, this information is transmitted to the central LDM, which can be comb accuracy by analyzing the road conditions from var ious angles based on combined information. In addition, it is possible to derive analysis results by re ading vehicle data for a specific period from the This process can be performed directly from Sp ark or by accessing Spark through Zeppelin. In addition, Zeppelin can be used when it is necessary to output a visualized analysis result. As the data of each table of MariaDB are continuously generated while driving, it is designed in a 1:M structure. The Driver table contains the driver’s identiﬁcation information, and the Drive table includes information on the driver’s driving situation. Therefore, the Drive table contains all data from the moment the engine is turned on to the moment the engine is turned o . The GPS and IMU tables store each sensor data value generated during driving, and the Vision table r ecords the timestamp when an object is detected while driving. Object information detected by camera is recorded in the Object table based on the timestamp of the V ision table. The position of the object in the frame is expressed as the bounding box’s pixel position, and based on this, the size of the object and its coordinates in the real coordinate plane can be known. The HDFS stores Vision raw image data or LiDAR deliver ed through Kafka. In addition, raw image data of Vision can be converted into an image ﬁle after r eprocessing and stored, and preserved data transferred from Maria DB are also loaded. The processing and analysis module visually grasps the collection status of vehicle data, processes data requiring real-time processing through str eaming, and is used when analyzing large-scale data for speciﬁc purposes. Therefore, Spark and Zeppelin are used for processing and analysis modules. Spark is an open-source cluster computing framework that allows users to analyze data results in multiple formats. At this time, data storage formats such as CSV and parquet are also used, but because data stored in the HDFS can be analyzed directly as a source, it shows fast throughput. In addition, Spark provides a streaming function. This function is used when analyzing data collected in real time through Kafka. For example, GPS data collected from vehicles can be streamed, and the real-time location and speed of each vehicle can be identiﬁed to determine whether a speciﬁc road is congested. When real-time road conditions are identiﬁed through Spark, this information is transmitted to the central LDM, which can be combined with existing information and obtain high accuracy by analyzing the road conditions from various angles based on combined information. In addition, it is possible to derive analysis results by reading vehicle data for a speciﬁc period from the HDFS and MariaDB. This process can be performed directly from Spark or by accessing Spark through Zeppelin. In addition, Zeppelin can be used when it is necessary to output a visualized analysis result.\n\nT o implement 1 in Figure , an autonomous vehicle owned by the department drove on campus, and ROS collected the sensor data. The vehicle is equipped with sensors for autonomous driving, and the sensors are collected as ROS to control the car . In order to realize the actual autonomous driving environment, all driving records are stored in ROSBAG, the r ecord ﬁle of ROS, and implemented similarly to the connected car environment using multiple ROSBAGs. The replayed ROSBAG outputs the stored sensor data and delivers them to the Kafka server through the ROS-Kafka connector connected to the ROS topic. There are three main types of data collected from vehicles in the system: image, IMU, and GPS data. Therefore, Kafka topics are created according to the data from the ROS topics. Autonomous vehicles identify objects through image recognition while driving and output the results or pass them over to the control unit. 2 in Figure proceeds with object detection using T ensorﬂow and YOLO based on the information received from the ROS-Kafka topic in 1. The detection results and sensor data collected from ROS are converted to JSON format with a schema for DB storage and passed through Kafka topics linked to each table in MariaDB. In addition, the video data are converted into a video ﬁle after collecting and detecting real-time raw image ﬁles and stored in Figure 11 describes the elaborated ROS topic and the Kafka topic structure for collecting and transmitting driving data, and an example of the exchanged message is shown in Figure . To implement 1 in Figure 10, an autonomous vehicle owned by the depart ment drove on campus, and ROS collected the sensor data. The vehicle is equipped with sensors for autonomous driving, and the sensors are collected as ROS to control the car. In order t o realize the actual autonomous driving environment, all driving records are stored in RO SBAG, the record file of ROS, and implemented similarly to the connected The replayed ROSBAG outputs the stored sensor data and delivers them to the Kafka server through the ROS-Kafka connector connected to the ROS topic. There are three main types of data collected from vehicles in the sy according to the data from the ROS topics. Autonomous vehicles identify objects through image recognition while drivin g and output the results or pass them over to the control unit. 2 in Figure 10 proceeds with object detection using Tensorflow and YOLO based on the information received from the ROS-Kafka topic in 1. The detection results and sensor data collected from ROS are converted to JSON format with a schema for DB storage and passed through Kafka topics linked to each table in MariaDB. In addition, the video data are converted into a video fil e after collecting and detecting real-time raw image files and transmitting driving data, and an exam ple of the exchanged message is shown in Figure 12. Figure 12. Message from ROS to the database. To implement 1 in Figure 10, an autonomous vehicle owned by the depart ment drove on campus, and ROS collected the sensor data. The vehicle is equipped with sensors for autonomous driving, and the sensors are collected as ROS to control the car. In order t o realize the actual autonomous driving environment, all driving records are stored in RO SBAG, the record file of ROS, and implemented similarly to the connected The replayed ROSBAG outputs the stored sensor data and delivers them to the Kafka server through the ROS-Kafka connector connected to the ROS topic. There are three main types of data collected from vehicles in the sy according to the data from the ROS topics. Autonomous vehicles identify objects through image recognition while drivin g and output the results or pass them over to the control unit. 2 in Figure 10 proceeds with object detection using Tensorflow and YOLO based on the information received from the ROS-Kafka topic in 1. The detection results and sensor data collected from ROS are converted to JSON format with a schema for DB storage and passed through Kafka topics linked to each table in MariaDB. In addition, the video data are converted into a video fil e after collecting and detecting real-time raw image files and transmitting driving data, and an exam ple of the exchanged message is shown in Figure 12. Figure 12. Message from ROS to the database. Figure 12. Message from ROS to the database.\n\nSpark analyzes the data based on the stored data and visualizes them using Zeppelin, as shown in Spark creates Dataframe based on the data accumulated in Maria DB and HDFS. Subsequently , writing the code in Zeppelin’s notebook, which is designed to facilitate this process, creates a temporary view ‘tmpT able’ based on the data frame for an SQL query . Finally, users can visually understand the types of items detected while driving based on the temporary view of the object table of MariaDB, as Spark analyzes the data based on the stored da ta and visualizes them using Zeppelin, as shown Spark creates Dataframe based on the da ta accumulated in Maria DB and HDFS. Subse quently, writing the code in Zeppelin’s notebook, which is des igned to facilitate this process, creates a temporary view ‘tmpTable’ based on the data fram e for an SQL query. Finally, users can visually understand the types of items detected while drivin g based on the temporary view of the object table of MariaDB, as shown in Figure 13. Thereafter, 3 and 4 processes proceed, and the analys is result is delivered to the Kafka cl uster. Kafka clusters organize topics that can deliver mess ages to vehicles and service environments outside the server for analysis-result-based services. Therefor e, the process of transmitting the information to the vehicle LDM by linking the message received in 5, t he information collected by the central LDM from the server, and exte As shown in Table 4, the latency evaluation of was performed. The overall latency evaluation process is the same as the autonomous vehicle data collection process in Figure 11. The process of transf erring the sensor data collected from the vehicle to the Python application through Kafka is the same as steps 1–3 in Figure 11, which takes 14 ms on average. Additionally, after converting t he message format in a Python application, the data i s transferred to the topic connected to the Database. At this time, Central LDM can also receive data simultaneously based on Kafka’s abilit y to subscribe to the same topic. This process also had an Thereafter , 3 and 4 processes proceed, and the analysis result is delivered to the Kafka cluster . Kafka clusters organize topics that can deliver messages to vehicles and service environments outside the server for analysis-result-based services. Therefore, the process of transmitting the information to the vehicle LDM by linking the message received in 5, the information collected by the central LDM from the server , and external information is 6. As shown in T able , the latency evaluation of the proposed platform and C-ITS environment was performed. The overall latency evaluation process is the same as the autonomous vehicle data collection process in Figure . The process of transferring the sensor data collected from the vehicle to the Python application through Kafka is the same as steps 1–3 in Figure , which takes 14 ms on average. Additionally , after converting the message format in a Python application, the data is transferred to the topic connected to the Database. At this time, Central LDM can also receive data simultaneously based on Kafka’s ability to subscribe to the same topic. This process also had an Type of T ime ROS to Python Application Python Application to Database Overall Process\n\nThe latency maximum situation occurred when data was ﬁrst transferred from the application to the database. This is the ﬁrst time that Kafka and the database connector are connected, causing a delay . In addition, when using the Kafka server for both ROS to Python application and Python application to Database processes, the proposed C-ITS environment guarantees stable latency because the maximum latency for occupant safety does not exceed 200 ms, and an average speed of 14 ms is guaranteed [ On cracked roads, the vehicle vibrates while driving, reducing the passenger’s ride comfort and increasing the risk of accidents. Therefore, continuous monitoring and management of cracked roads are required. As the acceleration sensor IMU provides acceleration in the x, y , and z axes, the degree of the vibration of the vehicle can be determined through the sensor value analysis. Therefore, the tra c center can check the road surface condition using the vehicle IMU sensor data. There is an example that recognizes the road surface condition using the built system. O It delivers the IMU data collected from the vehicle to the IMU topic of Kafka. The data in MariaDB from the IMU topics are stored. At the same time, Spark uses the streaming function to ﬁnd data with signiﬁcant ﬂuctuations in the y- and z-axis values in real time. When abnormal data are detected, Spark delivers information on the location of the occurrence The information collected for road management and monitoring is displayed on a web-based The result of recognizing the road surface condition using the built system is shown in Figur e . Currently , the driving information and the IMU sensor data are used. However , if an advanced object detection algorithm is applied to the vehicle control, the data in the object table of the RDBMS and the sensor data are fused to obtain the accurate road condition information. visualization of the messages delivered to the central LDM. depends on the infrastructure, a structure that connected cars was proposed and implemented based on the Hadoop ecosystem. The implemented in charge of controlling autonomous vehicles. It delivers them to Kafka, a distributed messag ing system that guarantees fast latency. Messages de livered from the vehicle system to the platform through Kafka are stored in MariaDB for structured data and HDFS for unstructured big data. The platform analyzes and processes the loaded data using Sp ark and visualizes the results with Zeppelin. The data processed in th e platform transmit a message to the central LDM through Kafka for vehicle services. As the proposed platform is constructed using the Hadoop ecosystem, which ca n be dis tribu ted, i t is po ssibl e to ex pand i f the ve hi cle big data are increased and to ensure stability and Additional studies that can be conducted based on this study are as follows. (1) For a systematic management of the vehicle big data, metadata of MariaDB and HDFS are created and constructed. As the big data accumulate, it is difficult to obtain the location and type of the existing data, so it is crucial to optimize the data management. Therefore, in future work, metadata storage using NoSQL should be investigated. (2) In order to utilize LDM, a real-time system can be configur ed, and a process of transmitting and receiving messages conforming to the standard ca n be performed. It can also be linked with research on security enhancement of the message transmission and reception processes. visualization of the messages delivered to the central LDM.\n\nThis study proposes an autonomous driving sensor big data-based C-ITS environment to improve the safety of autonomous vehicles. Unlike the current C-ITS environment, which highly depends on the infrastructure, a structure that utilizes big data for autonomous vehicles and connected cars was proposed and implemented based on the Hadoop ecosystem. The implemented Hadoop ecosystem-based LDM linkage platform collects images, sensors, and LiDAR data from ROS in charge of controlling autonomous vehicles. It delivers them to Kafka, a distributed messaging system that guarantees fast latency . Messages delivered from the vehicle system to the platform through Kafka are stored in MariaDB for structured data and HDFS for unstructured big data. The platform analyzes and processes the loaded data using Spark and visualizes the results with Zeppelin. The data processed in the platform transmit a message to the central LDM through Kafka for vehicle services. As the proposed platform is constructed using the Hadoop ecosystem, which can be distributed, it is possible to expand if the vehicle big data are increased and to ensure stability and fault tolerance. Additional studies that can be conducted based on this study are as follows. (1) For a systematic management of the vehicle big data, metadata of MariaDB and HDFS are created and constructed. As the big data accumulate, it is di cult to obtain the location and type of the existing data, so it is crucial to optimize the data management. Therefore, in future work, metadata storage using NoSQL should be investigated. In order to utilize LDM, a real-time system can be conﬁgured, and a process of transmitting and receiving messages conforming to the standard can be performed. It can also be linked with research on security enhancement of the message transmission and reception processes. Conceptualization, A.Y . and C.M.; methodology , A.Y . and C.M.; software, A.Y ., S.S. and J.L.; validation, A.Y ., S.S. and J.L.; investigation, A.Y . and S.S.; data curation, A.Y . and S.S.; writing—original draft project administration, C.M.; All authors have read and agreed to the published version of the manuscript. This paper was supported by the Korea Institute for Advancement of T echnology (KIAT) grant funded by the Korea Government (MOTIE) (N0002428, The Competency Development Program for Industry Specialist). Conﬂicts of Interest: The authors declare no conﬂict of interest. Y e, L.; Yamamoto, T . Evaluating the impact of connected and autonomous vehicles on tra c safety. Jang, J.; Ko, J.; Park, J. Identiﬁcation of safety beneﬁts by inter-vehicle crash risk analysis using connected Map (LDM); Rationale for and Guidance on Standardization Han, J.; Kim, H.; Heo, S. GS1 Connected Car: An Integrated V ehicle Information Platform and Its Ecosystem for Connected Car Services based on GS1 Standards. In Proceedings of the 2018 IEEE Intelligent Vehicles Sung, H.K.; Chong, K.S. Development of road tra c analysis platform using big data. In Proceedings of the 4th International Conference on Advances in Big Data Analytics, Las V egas, NV , USA, 17–20 July 2017;"
    }
]