[
    {
        "link": "https://hello-algo.com/en/chapter_data_structure/character_encoding",
        "document": "In the computer system, all data is stored in binary form, and is no exception. To represent characters, we need to develop a \"character set\" that defines a one-to-one mapping between each character and binary numbers. With the character set, computers can convert binary numbers to characters by looking up the table.\n\nThe is one of the earliest character sets, officially known as the American Standard Code for Information Interchange. It uses 7 binary digits (the lower 7 bits of a byte) to represent a character, allowing for a maximum of 128 different characters. As shown in Figure 3-6, ASCII includes uppercase and lowercase English letters, numbers 0 ~ 9, various punctuation marks, and certain control characters (such as newline and tab).\n\nHowever, ASCII can only represent English characters. With the globalization of computers, a character set called was developed to represent more languages. It expands from the 7-bit structure of ASCII to 8 bits, enabling the representation of 256 characters.\n\nGlobally, various region-specific EASCII character sets have been introduced. The first 128 characters of these sets are consistent with the ASCII, while the remaining 128 characters are defined differently to accommodate the requirements of different languages.\n\nLater, it was found that EASCII still could not meet the character requirements of many languages. For instance, there are nearly a hundred thousand Chinese characters, with several thousand used regularly. In 1980, the Standardization Administration of China released the character set, which included 6763 Chinese characters, essentially fulfilling the computer processing needs for the Chinese language.\n\nHowever, GB2312 could not handle some rare and traditional characters. The character set expands GB2312 and includes 21886 Chinese characters. In the GBK encoding scheme, ASCII characters are represented with one byte, while Chinese characters use two bytes.\n\nWith the rapid evolution of computer technology and a plethora of character sets and encoding standards, numerous problems arose. On the one hand, these character sets generally only defined characters for specific languages and could not function properly in multilingual environments. On the other hand, the existence of multiple character set standards for the same language caused garbled text when information was exchanged between computers using different encoding standards.\n\nResearchers of that era thought: What if a comprehensive character set encompassing all global languages and symbols was developed? Wouldn't this resolve the issues associated with cross-linguistic environments and garbled text? Inspired by this idea, the extensive character set, Unicode, was born.\n\nis referred to as \"统一码\" (Unified Code) in Chinese, theoretically capable of accommodating over a million characters. It aims to incorporate characters from all over the world into a single set, providing a universal character set for processing and displaying various languages and reducing the issues of garbled text due to different encoding standards.\n\nSince its release in 1991, Unicode has continually expanded to include new languages and characters. As of September 2022, Unicode contains 149,186 characters, including characters, symbols, and even emojis from various languages. In the vast Unicode character set, commonly used characters occupy 2 bytes, while some rare characters may occupy 3 or even 4 bytes.\n\nUnicode is a universal character set that assigns a number (called a \"code point\") to each character, but it does not specify how these character code points should be stored in a computer system. One might ask: How does a system interpret Unicode code points of varying lengths within a text? For example, given a 2-byte code, how does the system determine if it represents a single 2-byte character or two 1-byte characters?\n\nA straightforward solution to this problem is to store all characters as equal-length encodings. As shown in Figure 3-7, each character in \"Hello\" occupies 1 byte, while each character in \"算法\" (algorithm) occupies 2 bytes. We could encode all characters in \"Hello 算法\" as 2 bytes by padding the higher bits with zeros. This method would enable the system to interpret a character every 2 bytes, recovering the content of the phrase.\n\nHowever, as ASCII has shown us, encoding English only requires 1 byte. Using the above approach would double the space occupied by English text compared to ASCII encoding, which is a waste of memory space. Therefore, a more efficient Unicode encoding method is needed.\n\nCurrently, UTF-8 has become the most widely used Unicode encoding method internationally. It is a variable-length encoding, using 1 to 4 bytes to represent a character, depending on the complexity of the character. ASCII characters need only 1 byte, Latin and Greek letters require 2 bytes, commonly used Chinese characters need 3 bytes, and some other rare characters need 4 bytes.\n\nThe encoding rules for UTF-8 are not complex and can be divided into two cases:\n• For 1-byte characters, set the highest bit to , and the remaining 7 bits to the Unicode code point. Notably, ASCII characters occupy the first 128 code points in the Unicode set. This means that UTF-8 encoding is backward compatible with ASCII. This implies that UTF-8 can be used to parse ancient ASCII text.\n• For characters of length bytes (where ), set the highest bits of the first byte to , and the bit to ; starting from the second byte, set the highest 2 bits of each byte to ; the rest of the bits are used to fill the Unicode code point.\n\nFigure 3-8 shows the UTF-8 encoding for \"Hello算法\". It can be observed that since the highest \\(n\\) bits are set to \\(1\\), the system can determine the length of the character as \\(n\\) by counting the number of highest bits set to \\(1\\).\n\nBut why set the highest 2 bits of the remaining bytes to \\(10\\)? Actually, this \\(10\\) serves as a kind of checksum. If the system starts parsing text from an incorrect byte, the \\(10\\) at the beginning of the byte can help the system quickly detect anomalies.\n\nThe reason for using \\(10\\) as a checksum is that, under UTF-8 encoding rules, it's impossible for the highest two bits of a character to be \\(10\\). This can be proven by contradiction: If the highest two bits of a character are \\(10\\), it indicates that the character's length is \\(1\\), corresponding to ASCII. However, the highest bit of an ASCII character should be \\(0\\), which contradicts the assumption.\n\nApart from UTF-8, other common encoding methods include:\n• UTF-16 encoding: Uses 2 or 4 bytes to represent a character. All ASCII characters and commonly used non-English characters are represented with 2 bytes; a few characters require 4 bytes. For 2-byte characters, the UTF-16 encoding equals the Unicode code point.\n• UTF-32 encoding: Every character uses 4 bytes. This means UTF-32 occupies more space than UTF-8 and UTF-16, especially for texts with a high proportion of ASCII characters.\n\nFrom the perspective of storage space, using UTF-8 to represent English characters is very efficient because it only requires 1 byte; using UTF-16 to encode some non-English characters (such as Chinese) can be more efficient because it only requires 2 bytes, while UTF-8 might need 3 bytes.\n\nFrom a compatibility perspective, UTF-8 is the most versatile, with many tools and libraries supporting UTF-8 as a priority.\n\nHistorically, many programming languages utilized fixed-length encodings such as UTF-16 or UTF-32 for processing strings during program execution. This allows strings to be handled as arrays, offering several advantages:\n• Random access: Strings encoded in UTF-16 can be accessed randomly with ease. For UTF-8, which is a variable-length encoding, locating the character requires traversing the string from the start to the position, taking time.\n• Character counting: Similar to random access, counting the number of characters in a UTF-16 encoded string is an operation. However, counting characters in a UTF-8 encoded string requires traversing the entire string.\n• String operations: Many string operations like splitting, concatenating, inserting, and deleting are easier on UTF-16 encoded strings. These operations generally require additional computation on UTF-8 encoded strings to ensure the validity of the UTF-8 encoding.\n\nThe design of character encoding schemes in programming languages is an interesting topic involving various factors:\n• Java’s type uses UTF-16 encoding, with each character occupying 2 bytes. This was based on the initial belief that 16 bits were sufficient to represent all possible characters and proven incorrect later. As the Unicode standard expanded beyond 16 bits, characters in Java may now be represented by a pair of 16-bit values, known as “surrogate pairs.”\n• JavaScript and TypeScript use UTF-16 encoding for similar reasons as Java. When JavaScript was first introduced by Netscape in 1995, Unicode was still in its early stages, and 16-bit encoding was sufficient to represent all Unicode characters.\n• C# uses UTF-16 encoding, largely because the .NET platform, designed by Microsoft, and many Microsoft technologies, including the Windows operating system, extensively use UTF-16 encoding.\n\nDue to the underestimation of character counts, these languages had to use \"surrogate pairs\" to represent Unicode characters exceeding 16 bits. This approach has its drawbacks: strings containing surrogate pairs may have characters occupying 2 or 4 bytes, losing the advantage of fixed-length encoding. Additionally, handling surrogate pairs adds complexity and debugging difficulty to programming.\n\nAddressing these challenges, some languages have adopted alternative encoding strategies:\n• Python’s type uses Unicode encoding with a flexible representation where the storage length of characters depends on the largest Unicode code point in the string. If all characters are ASCII, each character occupies 1 byte, 2 bytes for characters within the Basic Multilingual Plane (BMP), and 4 bytes for characters beyond the BMP.\n• Go’s type internally uses UTF-8 encoding. Go also provides the type for representing individual Unicode code points.\n• Rust’s and types use UTF-8 encoding internally. Rust also offers the type for individual Unicode code points.\n\nIt’s important to note that the above discussion pertains to how strings are stored in programming languages, which is different from how strings are stored in files or transmitted over networks. For file storage or network transmission, strings are usually encoded in UTF-8 format for optimal compatibility and space efficiency."
    },
    {
        "link": "https://tarunjain07.medium.com/a-deep-dive-into-character-encoding-for-beginners-533201c35913",
        "document": "Character encoding is a fundamental concept in computing that allows computers to understand and display text.\n\nAt its core, it’s like a secret code that transforms letters, numbers, and symbols into a language that computers can comprehend — numbers.\n\nImagine you have a secret code to talk with your friends, where each letter is replaced by a special symbol or number. For example:\n• This is essentially what character encoding does for computers.\n• Since computers can only understand numbers, character encoding acts as a special dictionary that tells the computer how to turn letters and symbols into numbers it can process, and vice versa.\n• When you type a letter on your keyboard, the computer uses this “dictionary” to know which number represents that letter. And when it needs to show you text on the screen, it uses the same dictionary to turn the numbers back into letters you can read.\n\n— ASCII (American Standard Code for Information Interchange): One of the oldest and simplest encodings. In ASCII:\n• The letter ‘A’ is represented by the number 65\n\n— UTF-8 (Unicode Transformation Format — 8-bit): A modern and flexible encoding that can represent almost every character from all writing systems in the world. In UTF-8:\n• Basic Latin letters are represented the same as in ASCII\n• The Chinese character ‘中’ is represented by three numbers: 228, 184, 173\n• The emoji ‘😊’ is represented by four numbers: 240, 159, 152, 138\n\n— UTF-16 (Unicode Transformation Format — 16-bit): Another way to encode characters, using bigger “chunks”. In UTF-16:\n• The letter ‘A’ is represented by 65 (same as ASCII)\n• The Chinese character ‘中’ is represented by a single number: 20013\n• The emoji ‘😊’ is represented by two numbers: 55357 and 56842\n\nLet’s take the word “Hello” and see how it’s encoded using UTF-8:\n• Convert each character to its Unicode code point:\n\n4. In UTF-8, “Hello” is represented as this sequence of bytes:\n\nFor a more complex example, let’s look at “Café”:\n\nNotice how the “é” character requires two bytes in UTF-8, while the others only need one byte each.\n\nDifferent encoding formats handle spaces and new lines in various ways:\n\n— Space: U+0020\n\n — New line: Same as ASCII/UTF-8 (LF: U+000A, CR: U+000D)\n• Unix/Linux/macOS typically use LF for new lines\n• HTML collapses multiple spaces unless in a <pre> tag or using\n• Databases may have their own conventions\n• Network protocols often use CR+LF to denote end of line\n\nUnderstanding these differences is crucial when working across different platforms and systems to avoid formatting issues or data corruption.\n\nIn conclusion, character encoding is the unsung hero that allows our digital devices to communicate in the rich tapestry of human languages. From the simplest space to the most complex emoji, it’s all just numbers to a computer — but with the magic of encoding, it becomes the text we read every day."
    },
    {
        "link": "https://geeksforgeeks.org/what-is-character-encoding-system",
        "document": "As we all know, computers do not understand the English alphabet, numbers except 0 and 1, or text symbols. We use encoding to convert these. So, encoding is the method or process of converting a series of characters, i.e, letters, numbers, punctuation, and symbols into a special or unique format for transmission or storage in computers. Data is represented in computers using ASCII, UTF8, UTF32, ISCII, and Unicode encoding schemes. All types of data, including numbers, text, photos, audio, and video files, can be handled by computers. For example, 65 is represented as A because all the characters, symbols, numbers are assigned some unique code by the standard encoding schemes. Some of the commonly used encoding schemes are described below:\n\n1. ASCII: ASCII is known as American Standard Code for Information Interchange. The X3 group, part of the ASA, produced and published ASCII for the first time in 1963. (American Standards Association). The ASCII standard was first published in 1963 as ASA X3.4-1963, and it was revised ten times between 1967 and 1986. ASCII is an 8-bit code standard that divides the 256 slots into letters, numbers, and other characters. The ASCII decimal (Dec) number is constructed using binary, which is the universal computer language. The decimal value of the lowercase “h” character (char) is 104, which is “01101000” in binary.\n\nThe ASCII table is broken down into three sections.\n\n2. ISCII: ISCII (Indian Script Code for Information Interchange) is the abbreviation for the Indian Script Code for Information Interchange. ISCII is a method of encoding that can be used to encode a wide range of Indian languages, both written and spoken. To ease transliteration across multiple writing systems, ISCII adopts a single encoding mechanism.\n\nISCII was established in 1991 by the Bureau of Indian Standards (BIS). It has a character count of roughly 256 and employs an 8-bit encoding technique. From 0-127, the first 128 characters are the same as in ASCII. The following characters, which range from 128 to 255, represent characters from Indian scripts.\n• The vast majority of Indian languages are represented in this.\n• The character set is simple and straightforward.\n• It is possible to easily transliterate between languages.\n• Because Unicode was created later, and Unicode included ISCII characters, ISCII became obsolete.ISCII (Indian Script Code for Information Interchange) is the Indian Script Code for Information Interchange.\n• ISCII is a method of encoding that can encode a wide range of Indian languages, both written and spoken. To ease transliteration across multiple writing systems, ISCII adopts a single encoding mechanism.\n\n3. Unicode: Unicode Characters are translated and stored in computer systems as numbers (bit sequences) that the processor can handle. In Unicode, a code page is an encoding system that converts a set of bits into a character representation. Hundreds of different encoding techniques allocated a number to each letter or character in the globe before Unicode. Many of these methods used code pages with only 256 characters and each of which required 8 bits of storage.\n• Unicode enables the creation of a single software product or website for multiple platforms, languages, and countries (without re-engineering), resulting in significant cost savings over older character sets.\n• Unicode data can be used without generating data corruption in a variety of systems.\n• Unicode is a universal encoding technique that can be used to encode any language or letter irrespective of devices, operating systems, or software.\n• Unicode is a character encoding standard that allows you to convert between multiple character encoding systems. Because Unicode is a superset of all other major character encoding systems, you can convert from one encoding scheme to Unicode and then from Unicode to a different encoding scheme.\n• The most extensively used encoding is Unicode.\n• The applicable versions of ISO/IEC 10646, which defines the Universal Character Set character encoding, are fully compatible and synchronized with Unicode Standard versions. Or we can say that it includes 96,447 character codes that are far enough to decode any character symbol present in the world.\n\n4. UTF-8: It is a character encoding with variable widths that are used in electronic communication. With one to four one-byte (8-bit) code units, it can encode all 1,112,064[nb 1] valid Unicode character code points. Code points with lower numerical values are encoded with fewer bytes since they occur more frequently. When it was created the creators make sure that this encoding scheme is ASCII compatible and the first 128 Unicode characters that are one-to-one to ASCII are encoded using a single byte with the same binary value as ASCII and ensure that ASCII text is also valid UTF-8-encoded Unicode.\n\n5. UTF-32: UTF-32 is known as 32-bit Unicode Transformation Format. It is a fixed-length encoding that encodes Unicode code points using 32 bits per code. It uses 4-bytes per character and we can count the number of characters in UTF-32 string simply by just counting bytes. The main advantage of using UTF-32 is that Unicode code points can be directly indexed (although letters in general, such as “grapheme clusters” or some emojis, cannot be directly indexed, thus determining the displayed width of a string is more complex). A constant-time operation is finding the Nth code point in a sequence of code points. On the other hand, a variable-length code necessitates sequential access to locate the Nth code point in a row. As a result, UTF-32 is a straightforward substitute for ASCII code that examines each issue in a string using numbers incremented by one."
    },
    {
        "link": "https://borkorajkovic.com/posts/character-encoding-demystified",
        "document": "Have you ever wondered how exactly a computer stores letters like A, B, C… or Chinese characters like 吃, or even some emojis like 😆 and 🚀?\n\nMaybe you are familiar with ASCII and you are aware of Unicode, but you don’t know exactly how it works?\n\nIn this article, we’ll cover the basics of character encoding and several encoding schemes and introduce some fundamental concepts on the way.\n\nBack in the day, when people wanted to communicate over great distances, they had limited options. They could write messages in a form of mail letters and send them over via post service or even use pigeons to fly and carry them over.\n\nBut still, there was a need for immediate communication in cases of emergency like the warning of bad weather, or military communication.\n\nOver time different civilizations developed quite a few techniques of communication such as smoke signals, church bells, whistling, reflecting sunlight with mirrors, etc. All of these were limited in their capabilities and were generally not suitable for transferring arbitrary messages. They were mostly used to signal if there is some kind of danger ahead or call for help.\n\nSome of the most used and famous telegraph systems are the Morse code and Flag semaphore (used in maritime and aviation even in the present time).\n\nAs good as they are, these systems were not suitable for computer processing.\n\nAs we all know, computer stores data in a binary format, i.e. ones and zeros. So, to store textual data in computer memory, or transfer it over a digital network, we need a way to represent textual data in a binary format that the computer understands.\n\nA single unit of textual data is called a (or in most programming languages). For now, it’s enough to know that can be any sign used for creating textual content, such as a letter of the English alphabet, digit, or some other signs like space, comma, exclamation mark, question mark, etc.\n\nEssentially, encoding is a process of assigning unique numeric values to specific characters and converting those numbers into binary language. These binary numbers later can be converted back (or ) to original characters based on their values.\n\nSimply put, is an agreement that defines the correlation between binary numbers and characters from a .\n\nWhat would you do if you were to make up your character set for the English language?\n\nProbably you would take all letters from the English alphabet both upper and lower case:\n\nThen, you would add digits as well:\n\nAlso, you would need space, comma, semicolon, and other signs that complement letters and digits:\n\nIf you count these up, you will get 95 distinct characters:\n\nTo represent 95 characters, we need at least 7 bits, which allows us to define characters.\n\nNow, we can make up a table that will contain the mapping between each binary number and character from the set we just defined.\n\nWe could make A = 0000 0001, B = 0000 0010 and so on… Or in any other way we like.\n\nMost surely you heard of and used many different fonts on a computer. It must be obvious by now, that the character set is only defining what is sent, not how it looks.\n\nWhen we are referring to a character as “A” or “B”, we have a common understanding of what it means (it’s the Platonic “idea” of a particular character). But we can all read and write the same characters in different variations (shapes).\n\nA font is a collection of glyph definitions, in other words, the shapes (or images) that are associated with the character they represent.\n\nWe’ve shown that at least 7 bits are needed to represent characters used for creating textual content in the English alphabet.\n\nAs with most things in engineering, character sets (encodings) should be standardized.\n\nOne such (most famous) 7-bit encoding is ASCII (based on Telegraph code).\n\nASCII table has arranged characters in a very elegant fashion.\n\nFor example, all the uppercase and lowercase letters are in their alphabetical order. The same goes for digits.\n\nYou can easily learn ASCII code for the character , character and character because they are arranged in such an elegant way:\n• Character starts with 1, followed by all zeros and 1 (as the first letter in the alphabet) at the end: 100 0001\n• Lower case letters have the same ASCII codes as upper case letters, with the second digit being 1 instead of 0. It’s easy now to find - just take 1000001 and flip the second digit to 1, you get 110 0001\n\nA summary of these characters is given here in a table:\n\nNow you can derive any other letter/digit by simply counting up to the one you need.\n\nExtra 35 spaces were used to represent some special, so-called “control” characters.\n\nSome would be (backspace), (delete), (horizontal tab).\n\nLet’s take a look at two particularly interesting examples. Namely, telegraph machines would require 2 operations to go to the next line when printing text:\n\nIt’s based on manual typewriter machines like this one:\n\nWhen you type text and come to the right end of the page, you would like to go to the next line.\n\nSo, you would first push the handle on the left side (marked as on the picture) to the right to move the , hence (Carriage return).\n\nThe next step is to “feed” the typewriter with more next line of paper with a knob either on the right end of the carriage (market as on the picture). Hence, (Line feed).\n\nThis brought confusion to users of different OS, as there is no standardized end-of-line notation:\n\nIn most programming languages non-printable characters are represented using so-called character notation, usually with a backslash character .\n\nSome examples are given here:\n\nLet’s now try to encode text in ASCII.\n\nIt would get you something like this:\n\nWhat about the 8th bit?\n\nA byte is the smallest unit of storage that a computer would use. You may think that since the beginnings of the computing era byte was always 8 bits long. But that is not the case, as you can see with ASCII, which uses 7 bits. ASCII was standardized in 1963, but the first commercial 8-bit CPU came out in 1972. And as late as 1993 it was formally standardized that a byte is 8 bits long.\n\nFor modern CPUs byte is 8 bits long, so when storing ASCII characters, you are left with one extra bit.\n\nAll original text encoded with 7-bit ASCII can be encoded in 8-bit simply by appending 0 to the left side of the binary code, so its decimal and hexadecimal value stay the same.\n\nNow, taking our last example we can finally write it in the 8-bit format:\n\nOne question arises here: can we somehow leverage this extra bit 🤔?\n\nThis 8th bit allows us to have another 128 spaces for new characters, right?\n\nEncodings that are using same mappings for ASCII printable characters are called Extended ASCII and are commonly referred to as . They guarantee that all ASCII encoded files can be processed with these extended encodings.\n\nInstead of just one, we have a huge number of standardized . A full list of code pages can be found on Wikipedia: https://en.wikipedia.org/wiki/Code_page\n\nSome of the most known are:\n• None CP437 (also known as OEM 437, or DOS Latin the US) - an original code page for IBM PC.\n• None ISO-8859-1, aka Latin-1 (also useful for any Western European language)\n\nNow, if you wanted to write textual content using Cyrillic you would use Windows-1251 encoding. When you transmit this data, the other party would need to use the same encoding scheme (code page) to successfully read the data they received. If the encoding scheme is not the same, the text would appear as if written in a different language.\n\nIf not explicitly set, ISO 8859–1 was the default encoding of the document delivered via HTTP with the MIME Type beginning with . Since HTML5 this was changed to UTF-8.\n\nCode pages solved only part of the problem - storing additional characters for other languages since ASCII was designed to be sufficient for the English alphabet.\n\nFor most European languages this was acceptable, but not a great solution. You could not for example write in in the same textual file, because you can use only one code page while processing text.\n\nA bigger issue than that was the lack of support for languages that have much more characters than available 128 spaces within ASCII extended 8-bit code pages. Some examples are Arabic, Hindu, and Chinese (which have more than 10 thousand symbols called , which are actual words rather than letters as we are used to in European languages for example).\n\nIn a response to all the problems “code pages” had been introduced, and a new standard was initiated named Unicode. It was an attempt to make a huge single character set for all spoken languages and even some made-up ones and other signs such as emojis 🎉. The first version came out in 1991 and had many new versions since then, the latest one being in 2021 at the time of this writing. The Unicode Consortium also maintains the standard for UTF (Unicode Transformation Format) encodings. More on this later. But first…\n\nUp until now, we described all characters as self-contained symbols that can be represented with a single binary number in the encoding schema.\n\nIn simple words, the letter is just encoded with some binary number (0100 0001 in ASCII).\n\nBut, things got a lot more complicated when considering other languages.\n\nSome languages have modifiers of the letters such as accent modifiers. One such example that is also used in the English language for some foreign words is É (e-acute) which is an ordinary E letter with the acute symbol (that forward slash above the letter).\n\nGrammar of some languages requires the use of diacritics for letters when certain conditions are met. Therefore, term became ambiguous, so a new term is adopted for describing written symbols that could be with or without diacritics - a .\n\nAs there can be a huge number of combinations of letters and their possible modifiers, instead of making an encoding schema for all of them, it’s much more efficient to encode them separately. Therefore we separate grapheme in code points.\n\nSo, one or more code points can make up a grapheme. Therefore, the Unicode character set is defined as a set of code points rather than a set of graphemes.\n\nSome people believe that Unicode is just a simple 16-bit code where each character is mapped to a 16-bit number and so there are possible code points. That is not the case and there are 144 697 defined characters at the time of writing this article.\n\nWhat is true is that all characters that can fit into 2 bytes, in other words, code points are considered to make up BMP - basic multilingual plane ( to ). This was the first attempt to unite all code points needed for storing textual data, but soon it became obvious that it needed even more space, so 2 bytes were not sufficient anymore.\n\nLet’s go back to for a moment. This particular symbol can be encoded in two ways:\n• None using one code point that represents character É (U+00C9)\n• None using two code points, one for the letter E (U+0065) and its accent modifier (U+02CA)\n\nAs we’ve seen, the usual notation for Unicode characters is following:\n• are bytes expressed in hexadecimal numbers (can be two or more)\n\nIf we go back to É described as 2 code points, we can easily track the first code point as hexadecimal , which is indeed the letter E in the ASCII table.\n\nSome graphemes have more than 2 bytes. For example thumbs up emoji 👍 has the notation: .\n\nASCII has a very simple encoding scheme. As it uses only one byte, it’s very easy to map all characters to binary format and vice-versa.\n\nFor Unicode, things are not that simple. There are varying lengths of code points, going from 1 up to 4 bytes in size.\n\nNext, we’ll take a look at the most interesting encoding schemes for Unicode. There are two groups of schemes:\n\nThere are similarities and differences between them. We’ll cover the most relevant in this article.\n\nAs the name suggests, UTF-32 consists of 32 bits, i.e. 4 bytes. This is the simplest encoding strategy. Every code point is converted to a 32-bit value. We’ll see in short why this strategy is not very efficient in terms of space.\n\nUCS-4 is the same in every aspect as UTF-32.\n\nLet’s go back to our example from the beginning of this article with a simple change: let’s add emoji 🚀 between and :\n\nDo you notice something? For the majority of the content, we use much more space than needed. All ASCII characters require just one byte of space, but here we are spending additional 3 bytes per code point, which are all zeros. In other words, UTF-32 wastes a lot of memory for ASCII characters. If the text consists of only ASCII characters, UTF-32 would be 4X larger than the ASCII representation of the same text!\n\nOne advantage that UTF-32 has over other Unicode encoding schemes is that because of its simplicity, it’s easy to index code points in a file. As you only need to go 4 bytes per code point, so you can go to the desired index very fast, in both the forward and backward directions.\n\nWhat happens case when the ASCII text processor tries to read out UCS-4 encoded string? In ASCII text processors, the end of a string is usually presented with 0x00, which means that string is going to be terminated once it comes across a byte that has all zeros. So, it would not read the text till the end if it consists of code points that are less than 4 bytes in size.\n\nUTF-32/UCS-4 is not in use anymore by modern text processors, instead, you will find UTF-16 or UTF-8.\n\nRemember BMP (Basic Multilingual Plane)? We said that all characters that fit into 2 bytes are considered to be part of BMP. UCS-2 was exactly that - 2 bytes per code-point and nothing more!\n\nAs this was an improvement over 8-bit code pages, it’s still not enough to represent more and more demanding and ever-expanding Unicode character set, so it quickly became obsolete in favor of a more flexible, yet very similar UTF-16 encoding scheme.\n\nHere is a quick overview of support for code points using different encoding schemes:\n\nThis encoding scheme uses either 2 or 4 bytes to represent a single code point, so it’s not limited to UCS-2 to only 65 536 code points.\n\nThe majority of the code points that takes up to 16 bits can be directly converted in the same way as UCS-2 is doing - just a simple binary representation of the code point hexadecimal value.\n\nThe mechanism it uses is called surrogate pairs.\n\nIt’s easier to look at an example of how a code point is encoded with the UTF-16 encoding scheme.\n\nLet’s take emoji like 🚀 that has Unicode value .\n\nNow, we see that it goes over 16 bits in size. To represent a character of more than 16 bits in size, we need to use a “surrogate pair”, with which we get a single supplementary character. The first (high) surrogate is a 16-bit code value in the range to , and the second (low) surrogate is a 16-bit code value in the range to .\n\nNow, we need to subtract from the binary representation of emoji.\n\nThen, we are going to take the lower 20 binary digits:\n\nAnd replace signs in high and low surrogate with bits we just calculated:\n\nAnd there you have it! Emoji 🚀 is represented in UTF-16 encoding schemes with 4 bytes:\n\nIt’s important to note that since we are using surrogate pairs for marking 4-byte UTF-16 code points, we cannot use ranges for a high and low surrogate for 16-bit code points:\n\nYou can notice that these two ranges makes one continuous range:\n\nIn this range, there are no code points, in other words, this hole makes unavailable code points.\n\nBecause of these restrictions, designers of the Unicode standard decided to exclude the same range of code points from UCS-2, so UTF-16 is fully compatible with UCS-2 for all 2-byte-sized code points.\n\nThis restricted range is the same for all Unicode encodings.\n\nThe site https://unicode-table.com/ contains a lot of useful information regarding Unicode characters. We can even check the info page for 🚀 emoji here: https://unicode-table.com/en/1F680/ to verify we get the correct value for our example for UTF-16 encoding.\n\nHere we can see basic info, like:\n• CSS-code (hexadecimal value in a different format than Unicode notation)\n\nNear the end of the page, you can check out Encoding values for this Unicode code point:\n\nAs you can see, we successfully calculated UTF-16 encoding, yeah 🥳!\n\nBut wait, you might ask what are now these 2 variations of UTF-16 called and 🤔? That brings us to the next topic…..\n\nThe order of the bytes for multi-byte in which they are stored in memory is called of the computer system. Depending on the place of the (Most Significant Byte) and (Least Significant Byte) there are:\n• - Big-endian ( is stored at the smallest memory address)\n• - Little-endian ( is stored at the smallest memory address)\n\nWhy does this matter in the first place?\n\nCPU usually does not take one byte when processing data, but it takes multiple bytes. This measure is called in CPU terminology. It becomes natural that the size is multiple of 8 since the smallest unit for storage is (8 bits). Modern CPUs are 32-bit or 64-bit in size.\n\nMost modern computer systems (Intel processors for example) use a little-endian format to store the data in the memory. The reason is beyond the scope of this article, but it’s related to the internal structure of the CPU since particular endianness allows for certain features on different CPU designs.\n\nIf we have 32-bit integer number like for example, we would write it in binary format as:\n\nNow, you can see that if we didn’t know what was the endianness, we could interpret this integer number in the wrong way.\n\nFor our example, instead of reading , we could by mistake think it’s !\n\nNow that you understand the implications of using the wrong endian in processing data, let’s go back to Unicode.\n\nText processors need to know how to parse the text. That’s where Endianness comes into the picture.\n\nAll Unicode encodings use at least 2 bytes of data per code point, meaning that the CPU is storing multiple bytes at once in either or .\n\nA little trick we can use to make sure proper endianness is applied when reading files written in Unicode encodings is the so-called Byte Order Mark or BOM for short.\n\nLet’s take a look at the UTF-16 example.\n\nBOM is nothing more than a code point that has a special property.\n\nIts Unicode value is and it represents “zero-width, non-breaking space”.\n\nWhat that means is this code point is not visible on screen, yet it is a valid code point for UTF-16 encoding.\n\nBut, the catch is that if we reverse the order of the bytes to we get to a value that is considered invalid for UTF-16 encoding. Hence, the text processor understands that it needs to read the bytes in a different order, using Little Endian.\n\nTo use BOM, these 2 bytes will be saved at the beginning of a file, so the text processor can immediately figure out what Endianness is used for the file.\n\nNow, to demonstrate this, I will show how it looks when saving characters in a file using UTF-16 LE and UTF-16 BE:\n\nThis brilliant encoding scheme is one of the most used today alongside , because of its very useful features.\n\nCode points that take 1 byte of size and are encoded with the same scheme as ASCII encoding. For all other code points, uses from 2 up to 4 bytes (even 6 in some cases), depending on the code point itself.\n\nWhat this encoding allows as a bonus is backward compatibility with ASCII encoded files, as all ASCII characters would be read properly. This is not the case with , , and encoding schemes, as they expect the exact number of bytes per code point (4 for , 2 for and 2-4 for ).\n\nAlso, programs that use ASCII encoding can read files written in the schema, as long as they used only ASCII characters.\n\nWhat is the downside of the encoding scheme? It should be obvious by now, that it’s the fact that code points are variable in size, so it’s hard to index code points in a file (in other words searching for the n-th character in the file), in contrast to and encoding schemes.\n\nAnother downside is that it uses 50% more space than for East Asian text.\n\nalso has the nice property that ignorant old string-processing code that wants to use a single 0 byte as the null-terminator will not truncate strings, as is the case with other encoding schemes.\n\nThe encoding algorithm that is used for UTF-8 is very simple, yet brilliant!\n\nIt can be summarized in the following rules:\n• If the code point is in the range of ASCII characters, it is encoded in the same way as ASCII. Simple as that!\n• For other characters, we need to use more than one byte, in the following way:\n• The first byte must start with the same number of zeros as the number of bytes that will be used for this code point, followed by a zero.\n• Every other byte must start with .\n• Once we create such masked bytes, we fill in the binary form of the code point in data spaces.\n\nAs this was a mouthful, let’s go over some examples to better understand this process.\n\nCharacter falls in a range of ASCII characters. Therefore, it’s encoded using ASCII encoding like this:\n\nAs we can see, it has more than 7 bits in size, hence we need to use step 2. in the algorithm for encoding this code point to UTF-8.\n\nLet’s check if 2 bytes will be enough to encode this code point. In such a case, the first byte will have the following mask:\n\nNext byte would have a mask:\n\nIf we count up X signs, we see that we have 11 spaces for binary representation of Unicode code point.\n\nAs our code point from example, Φ can fit in 10 binary digits, 2 bytes is enough for this code point.\n\nWhat is left to do is to replace masks with binary digits that represent our Unicode code point:\n\nOr, in hexadecimal, we get .\n\nIf you check https://unicode-table.com/en/03A6/, you can verify we got the correct value. Yeah 🥳!\n\nFor the final example, let’s take one more look at the 🚀 emoji.\n\nAs this one needs 17 bits, it will not fit in 2 bytes for the UTF-8 encoding.\n\nIt will not fit into 3 bytes as well, because:\n\nThis gives us 16 bits of space, but we need 17.\n\nSo, we need 4 bytes:\n\nThis gives us bits of space for Unicode code points.\n\nNow, let’s populate masked bits and we get:\n\nWhich is in hexadecimal. You can verify it here: https://unicode-table.com/en/1F680/\n\nHere is a summary of code point ranges and their respective UTF-8 byte sizes:\n\nWhat about BOM for UTF-8?\n\nFor UTF-8 there is also a possibility to define BOM in the same way as for UTF-16.\n\nWe know that BOM code point is . Binary form is:\n\nAs we’ve seen in an exercise just before, since this code point is more than 8 bits in size, we need to use multiple bytes. In this case, we need to use 3 bytes, as it gives us 16 bits of space, which is exactly how much we need to represent this code point.\n\nWe already know that the first byte must start with several ones that represent how many bytes this code point requires by UTF-8 encoding (in this case 3), followed by zero. For other bytes, they must start with :\n\nSo, complete mask for our code point is:\n\nNow what is left is to populate signs with bits themselves. We finally get:\n\nIf file is saved in encoding scheme, it’s first three bytes will be:\n\nHere is another demo of a file saved in encoding:\n\nIn most tutorials for beginners in any programming language, you learn how to print out the console famous “Hello world!” sentence.\n\nAs it would be overkill for a beginner student to learn all about character encodings, this information is usually left out.\n\nBut it’s very important to know that not all programming languages are Unicode aware (meaning they operate on a sequence of Unicode characters rather than ASCII characters).\n\nHere is a list of some mainstream programming languages and their default character encodings:\n\nThis overview shows only the default character encoding used by these programming languages, it does not mean they cannot process Unicode text. They all have support for Unicode, either through additional data types or 3rd party libraries.\n\nThe key point to take away from here is that you should be aware of the limitations your programming language has when processing data, otherwise you can get in all kinds of funny situations. For example, counting the number of characters in Unicode text that has at least one character outside the ASCII range using C++ datatype would give you the wrong result.\n\nFor most DB engines there are, either SQL or NoSQL, there are settings per-database level where you can choose character encoding by yourself.\n\nAll modern DB engines have support for Unicode, but you need to be cautious when choosing encoding schema.\n\nFor example, there is a case of MySQL server that has an encoding scheme called “utf8”, which is just an alias for “utfmb3”. What it represents is UTF-8 with Maximum Bytes 3.\n\nAs shown in section “UTF-8 encoding summary”, we can see that with 3 bytes you can store only a range from up to , so-called BMP (Basic Multilingual Plane).\n\nTherefore, the recommended character encoding is “utf8mb4”, which allows up to 4 bytes in size.\n\nWhen sending data over a network, the recipient does not know the character encoding used to create that data. So, if you don’t specify it somehow, the recipient can only guess what encoding to use to read the content.\n\nWhen sending e-mail messages you should (e-mail client actually) write a Content-Type HTTP header. It is used to indicate MIME type, where text can be one of the values. An example of UTF-8 encoding would be:\n\nAs for the HTML pages, the encoding scheme is set up inside the same HTML file, with a special tag.\n\nNow you may wonder how a text processor (in this case web browser) can even read the first part of the HTML without knowing the character encoding used?\n\nIf you take a look at the code we just wrote, you can notice that all the characters used in this part of the file are in a range of ASCII printable characters. That allows for any ASCII-compatible encoding scheme (the vast majority of encoding schemes in use today) to read this part of the file.\n\nAfter the charset is read by a web browser, it can switch to a different encoding scheme if needed.\n\nYou may ask what happens if there is no charset defined in HTML? Did the browser try to guess the encoding, or it will just use some default encoding scheme? Well, browsers do try to guess the encoding based on statistics on how many times particular characters appear in a text. As you may assume, it was not very successful in achieving a good result."
    },
    {
        "link": "https://dev.to/untilyou58/character-encoding-lgb",
        "document": "When crafting characters in a narrative, an author builds their own personalized \"character set\" by establishing traits, behaviors, motivations, and other attributes that represent each individual. As computer character encodings allow standardized interpretation, defining characters helps readers understand their roles.\n\n\"ASCII code\" is the earliest character set, and its full name is the American Standard Code for Information Interchange. It uses a 7-bit binary number (the lower 7 bits of a byte) to represent a character and can represent up to 128 different characters. ASCII code includes uppercase and lowercase English letters, numbers 0 ~ 9, some punctuation marks, and some control characters (such as line feeds and tabs).\n\nAround the world, several EASCII character sets suitable for different regions have emerged. The first 128 characters of these character sets are unified into ASCII codes, and the last 128 characters are defined differently to adapt to the needs of different languages.\n\nWith the rapid development of computer technology, character sets and encoding standards have flourished, which has brought about many problems. On the one hand, these character sets generally only define characters for a specific language and cannot work properly in multi-language environments. On the other hand, there are multiple character set standards for the same language. If two computers use different encoding standards, garbled characters will appear when transmitting information.\n\nResearchers at that time were thinking: If a sufficiently complete character set was launched to include all languages ​​and symbols in the world, wouldn't it be possible to solve the problem of cross-language environments and garbled characters? Driven by this idea, Unicode, a large and comprehensive character set, came into being.\n\nReleased in 1991, Unicode has undergone continuous expansion to include new languages and characters. As of September 2022, Unicode already contains 149,186 characters, encompassing a wide range of characters, symbols, and even emoticons from various languages. Within the vast Unicode character set, commonly used characters occupy 2 bytes, while rarer characters may require 3 or 4 bytes.\n\nUnicode is essentially a universal character set that assigns a unique number, called a \"code point,\" to each character. However, it does not specify how these code points are stored in a computer. This raises the question: How does the system interpret characters of different lengths within a text? For example, when encountering a 2-byte code, how does the system determine whether it represents a single 2-byte character or two 1-byte characters?\n\nA straightforward solution to this problem is to store all characters using equal-length encodings. This approach ensures consistency in character length.\n\nHowever, ASCII code has proven to us that encoding English only requires 1 byte. If the above solution is adopted, the space occupied by the English text will be twice that of ASCII encoding, which is a huge waste of memory space. Therefore, we need a more efficient Unicode encoding method.\n\nUTF-8, which stands for \"Unicode Transformation Format 8-bit,\" has indeed become the most widely used Unicode encoding method worldwide. It is a variable-length encoding scheme that can represent characters using 1 to 4 bytes, depending on the complexity of the character.\n\nOne of the key advantages of UTF-8 is its backward compatibility with ASCII (American Standard Code for Information Interchange). In UTF-8, ASCII characters are represented using a single byte, ensuring that text encoded in ASCII remains unaltered when represented in UTF-8.\n\nLatin letters, Greek letters, and other commonly used characters from various scripts, such as Cyrillic or Hebrew, are represented using 2 bytes in UTF-8. This allows for the inclusion of a wide range of alphabets compactly and efficiently.\n\nFor characters outside the ASCII range and the commonly used scripts, such as Chinese characters, UTF-8 uses 3 bytes to represent them. This ensures that a vast array of characters from different languages and scripts can be encoded and displayed correctly.\n\nIn addition, UTF-8 can handle even more complex characters, including rare and less commonly used characters, by using 4 bytes for their representation.\n\nThe versatility of UTF-8 has made it the de facto standard for encoding Unicode characters, as it strikes a balance between storage efficiency and compatibility with existing systems and applications. Its widespread adoption has enabled seamless communication and interoperability across different languages and scripts on the Internet and other digital platforms.\n\nIn addition to UTF-8, common encoding methods include the following two.\n• UTF-16 encoding: uses 2 or 4 bytes to represent a character. All ASCII characters and commonly used non-English characters are represented by 2 bytes; a few characters require 4 bytes. For 2-byte characters, the UTF-16 encoding is equivalent to the Unicode code point.\n• UTF-32 encoding: uses 4 bytes per character. This means that UTF-32 takes up more space than UTF-8 and UTF-16, especially for text with a high proportion of ASCII characters.\n\nFrom the perspective of storage space usage, using UTF-8 to represent English characters is very efficient because it only requires 1 byte; using UTF-16 to encode certain non-English characters (such as Chinese) will be more efficient because it only requires 2 characters byte, while UTF-8 may require 3 bytes.\n\nFrom a compatibility perspective, UTF-8 is the most versatile, and many tools and libraries give priority to supporting UTF-8.\n\nFor most programming languages ​​in the past, the strings used in program execution used equal-length encodings such as UTF-16 or UTF-32. Under equal-length encoding, we can treat strings as arrays. This approach has the following advantages.\n• Random Access: With equal-length encoding, like UTF-16, accessing characters at any position in a string is straightforward. Since UTF-8 is a variable-length encoding, finding a specific character requires traversing the string from the beginning, resulting in additional time complexity. O(n) time.\n• Character Count: Calculating the length of a UTF-16 encoded string is a constant-time operation. However, determining the length of a UTF-8 encoded string involves traversing the entire string to count the number of characters. O(1) operation.\n• String Operations: Performing various string operations such as splitting, concatenating, inserting, and deleting are generally easier with equal-length encoded strings, like UTF-16. Handling these operations with UTF-8 encoded strings often requires additional calculations to ensure proper encoding.\n• Java: Java's String type uses UTF-16 encoding. Initially, when Java was designed, it was believed that 16 bits (2 bytes) would be sufficient to represent all possible characters. However, with the expansion of the Unicode specification, characters in Java can now be represented by a pair of 16-bit values, known as a surrogate pair.\n• JavaScript and TypeScript: Strings in JavaScript and TypeScript also use UTF-16 encoding. When JavaScript was first introduced by Netscape in 1995, Unicode was still in early development, and a 16-bit encoding was considered sufficient for representing all Unicode characters.\n• C#: C# primarily uses UTF-16 encoding. This choice is influenced by Microsoft, as many Microsoft technologies, including the Windows operating system, widely use UTF-16 encoding.\n• Python: Python's str type uses Unicode encoding and employs a flexible string representation. The size of characters stored in memory depends on the largest Unicode code point in the string. If all characters are within the ASCII range, each character occupies 1 byte. If characters extend beyond ASCII but are within the Basic Multilingual Plane (BMP), each character occupies 2 bytes. If characters extend beyond BMP, each character occupies 4 bytes.\n• Go: Go's string type internally uses UTF-8 encoding. Additionally, the Go language provides the rune type, which represents a single Unicode code point.\n• Rust: Rust's String type also uses UTF-8 encoding internally. Rust also provides the char type for representing individual Unicode code points.\n\nIt is important to note that the discussion above focuses on how strings are stored within programming languages. This is distinct from how strings are stored in files or transmitted over the network. In file storage or network transmission, UTF-8 encoding is commonly used to achieve optimal compatibility and space efficiency."
    },
    {
        "link": "https://algocademy.com/blog/implementing-algorithms-for-data-compression-a-comprehensive-guide",
        "document": "In the ever-expanding digital landscape, data compression plays a crucial role in optimizing storage and transmission of information. As aspiring programmers and software engineers, understanding and implementing data compression algorithms is an essential skill that can significantly enhance your coding prowess and prepare you for technical interviews at top tech companies. In this comprehensive guide, we’ll dive deep into the world of data compression algorithms, exploring their principles, implementations, and practical applications.\n\nData compression is the process of encoding information using fewer bits than the original representation. This technique is essential for reducing storage requirements and improving transmission speeds across networks. Compression algorithms can be broadly categorized into two types:\n• Lossless compression: Preserves all original data and allows for perfect reconstruction.\n• Lossy compression: Achieves higher compression ratios by discarding some less critical information.\n\nThe choice between lossless and lossy compression depends on the specific application and the nature of the data being compressed. For instance, text documents and program files typically require lossless compression to maintain integrity, while images and audio files can often benefit from lossy compression techniques.\n\nLossless compression algorithms ensure that the decompressed data is identical to the original. Let’s explore three popular lossless compression techniques:\n\nRun-Length Encoding is one of the simplest forms of data compression. It works by replacing sequences of identical data elements (runs) with a single data value and count.\n\nRLE is particularly effective for data with long runs of repeated values, such as simple graphics or binary data.\n\nHuffman coding is a variable-length prefix coding algorithm that assigns shorter codes to more frequent symbols and longer codes to less frequent ones. It builds a binary tree (Huffman tree) based on the frequency of each symbol in the input data.\n\nThe algorithm follows these steps:\n• Calculate the frequency of each symbol in the input data.\n• Create a leaf node for each symbol and add it to a priority queue.\n• While there is more than one node in the queue:\n• Remove the two nodes with the lowest frequency.\n• Create a new internal node with these two nodes as children.\n• Add the new node back to the queue.\n• The remaining node is the root of the Huffman tree.\n• Traverse the tree to assign binary codes to each symbol.\n\nHuffman coding is widely used in various file compression formats and is particularly effective for text compression.\n\nLZW is a dictionary-based compression algorithm that builds a dictionary of substrings as it processes the input data. It replaces repeated occurrences of substrings with references to their earlier occurrences in the dictionary.\n\nThe LZW algorithm works as follows:\n• Find the longest matching substring in the dictionary.\n• Output the code for the matching substring.\n• Add the matched substring plus the next character to the dictionary.\n• Repeat steps 2-5 until the end of input.\n\nLZW is particularly effective for compressing data with repeated patterns and is used in various file formats, including GIF and TIFF.\n\nLossy compression algorithms achieve higher compression ratios by discarding some information, making them suitable for applications where perfect reconstruction is not necessary, such as image and audio compression.\n\nTransform coding is a lossy compression technique that involves transforming the input data into a different domain, typically using mathematical transforms like the Discrete Cosine Transform (DCT) or Wavelet Transform. The transformed data is then quantized and encoded.\n\nThe general steps in transform coding are:\n• Apply the mathematical transform to each block.\n\nTransform coding is widely used in image and video compression standards like JPEG and MPEG.\n\nVector Quantization (VQ) is a lossy compression technique that works by dividing the input data into vectors and mapping each vector to the nearest representative vector from a predefined codebook.\n• Mapping each input vector to the nearest codebook vector.\n• Encoding the index of the matched codebook vector.\n\nVQ is used in various applications, including image and speech compression.\n\nNow that we’ve covered the theoretical aspects of various compression algorithms, let’s dive into their implementations. We’ll focus on implementing the lossless compression algorithms discussed earlier: Run-Length Encoding, Huffman Coding, and LZW.\n\nThis implementation demonstrates both encoding and decoding functions for RLE. The function compresses the input string, while reconstructs the original data from the compressed format.\n\nImplementing Huffman coding is more complex. Here’s a basic implementation in Python:\n\nThis implementation includes functions for building the Huffman tree, generating Huffman codes, encoding, and decoding. The function compresses the input string, while reconstructs the original data from the compressed format and the Huffman codes.\n\nHere’s a basic implementation of the LZW algorithm in Python:\n\nThis implementation includes both encoding and decoding functions for the LZW algorithm. The function compresses the input string, while reconstructs the original data from the compressed format.\n\nWhen implementing compression algorithms, it’s crucial to analyze their performance in terms of compression ratio, time complexity, and space complexity. Here are some key considerations:\n• Compression ratio: Measure the ratio of compressed size to original size for different types of input data.\n• Time complexity: Analyze the running time of encoding and decoding operations for various input sizes.\n• Space complexity: Consider the memory usage of the algorithm, especially for large inputs.\n• Trade-offs: Understand the trade-offs between compression ratio, speed, and memory usage.\n\nTo optimize the performance of compression algorithms, consider the following techniques:\n• Use efficient data structures: Implement priority queues, hash tables, or trees to improve algorithm efficiency.\n• Preprocessing: Apply techniques like sorting or filtering to improve compression effectiveness.\n• File compression: ZIP, RAR, and other archive formats use combinations of compression algorithms.\n• Audio compression: MP3, AAC, and other formats compress audio data for efficient storage and streaming.\n• Database systems: Compression techniques are used to reduce storage requirements and improve query performance.\n• Backup and archiving: Efficient compression is crucial for long-term data storage and retrieval.\n\nUnderstanding these applications can help you appreciate the importance of compression algorithms in modern computing and guide your focus when preparing for technical interviews.\n\nAs data volumes continue to grow exponentially, the field of data compression is evolving to meet new challenges. Some emerging trends include:\n• Machine learning-based compression: Utilizing neural networks and other ML techniques to achieve better compression ratios.\n• Context-aware compression: Adapting compression strategies based on the semantic content of the data.\n\nStaying informed about these trends can give you an edge in technical interviews and help you contribute to cutting-edge projects in your future career.\n\nMastering data compression algorithms is an essential skill for aspiring software engineers and computer scientists. By understanding the principles behind various compression techniques and implementing them from scratch, you’ll develop a deeper appreciation for algorithmic thinking and problem-solving.\n\nAs you prepare for technical interviews at top tech companies, remember that compression algorithms often appear in coding challenges and system design questions. The knowledge and skills you’ve gained from this guide will serve you well in tackling such problems and demonstrating your expertise to potential employers.\n\nContinue to practice implementing and optimizing these algorithms, and explore their applications in real-world scenarios. With dedication and hands-on experience, you’ll be well-equipped to excel in your coding interviews and contribute to the ever-evolving field of data compression in your future career."
    },
    {
        "link": "https://medium.com/@rajat01221/an-in-depth-exploration-of-data-compression-algorithms-c409ec43f1e5",
        "document": "Data compression is a fundamental concept in computer science, essential for reducing the size of data to save storage space, speed up transmission, and optimize performance across various applications. Whether it’s compressing a file before sending it over the internet, encoding video streams for efficient playback, or reducing the size of massive datasets, data compression plays a crucial role in modern computing. Compression algorithms are classified into two broad categories: lossless and lossy. Lossless compression ensures that the original data can be perfectly reconstructed from the compressed data, making it ideal for text files, software, and other data where precision is critical. On the other hand, lossy compression allows some loss of data fidelity, often imperceptible to the human senses, making it ideal for multimedia applications like images, audio, and video. This article dives into the mathematical foundations and mechanisms of various data compression algorithms, both lossless and lossy, and explores their real-world applications.\n\nEntropy represents the minimum average number of bits required to encode the source symbols. A higher entropy indicates that the data is more random and less compressible, while lower entropy suggests that the data contains redundancy, which can be exploited by compression algorithms. Redundancy refers to the presence of excess data that is not essential for conveying the intended information. In data compression, reducing redundancy is key to achieving more compact representations. Redundant data can often be predicted or represented in a shorter form without losing information. For instance, in a text file, the repeated occurrence of a word like “the” can be encoded more efficiently by assigning a shorter code to it. Shannon’s Source Coding Theorem states that the entropy H(X) of a source represents the theoretical limit of lossless compression. According to this theorem, no lossless compression algorithm can reduce the average number of bits per symbol below the entropy of the source. This sets a fundamental limit on the efficiency of compression algorithms.\n\nHuffman coding is a widely used lossless compression algorithm that assigns variable-length codes to input characters based on their frequencies. Characters that occur more frequently are assigned shorter codes, while less frequent characters receive longer codes, resulting in a more efficient overall encoding. The Huffman coding process can be summarized as follows:\n• Frequency Analysis: Count the frequency of each character in the input data.\n• Create a leaf node for each character and arrange them in a priority queue based on their frequencies.\n• Repeatedly extract the two nodes with the smallest frequencies, create a new node with a frequency equal to the sum of the two nodes, and add this new node back to the queue.\n• Continue until only one node remains, representing the root of the Huffman tree. 3. Generating Codes: Traverse the Huffman tree to assign binary codes to each character, with left branches representing ‘0’ and right branches ‘1’. The optimality of Huffman coding comes from the fact that it minimizes the weighted average code length, which is closely related to the entropy of the data. Consider a message consisting of the characters {A, B, C, D} with frequencies {0.4, 0.3, 0.2, 0.1}. The Huffman tree construction would yield a binary code such as {A: 0, B: 10, C: 110, D: 111}, resulting in a more compact representation than using fixed-length codes. Arithmetic coding is another lossless compression technique that differs from Huffman coding by representing the entire message as a single number, a fraction between 0 and 1. Instead of assigning codes to individual symbols, arithmetic coding encodes the message as a continuous interval on the real number line.\n• Interval Division: The probability range [0,1) is divided into subintervals based on the probabilities of the symbols.\n• Encoding: The message is encoded by successively narrowing the interval to reflect the symbols in the message.\n• Final Output: The final interval is represented by a single binary fraction. For a binary message with probabilities {0.8, 0.2}, arithmetic coding would represent the message “011” as a fraction like 0.425, encoded as a binary fraction. Run-Length Encoding (RLE) is a simple yet effective compression technique particularly well-suited for data containing long sequences of repeated characters. Instead of storing each character individually, RLE compresses data by recording the character and the number of times it repeats consecutively. Mathematically, if a sequence contains a character x repeated n times, RLE represents this as (x,n). For a sequence “AAAAABBBBCCCC”, RLE compresses it to “5A4B4C”, significantly reducing the size when there are many repetitions. LZW is a dictionary-based compression algorithm that builds a dictionary of substrings encountered in the input data and encodes these substrings using references to the dictionary. The LZW algorithm works as follows:\n• Initialization: Start with a dictionary containing all possible single-character strings.\n• Processing Input: Read a sequence of characters, and if the sequence exists in the dictionary, continue reading until a sequence that is not in the dictionary is encountered.\n• Dictionary Update: Add the new sequence to the dictionary and encode the longest matching sequence found with its corresponding dictionary entry. Mathematically, if www represents the current sequence and KKK the next character, the new sequence wKwKwK is added to the dictionary. For a string “ABABABABA”, LZW would recognize repeating patterns and encode the string using references to previously seen patterns, significantly compressing the data.\n\nTransform coding is a lossy compression technique commonly used in image and video compression, where the goal is to reduce the amount of data required to represent a signal by transforming it into a domain where the important information can be more easily compressed. The Discrete Cosine Transform (DCT) is one of the most widely used transforms in image compression (e.g., JPEG). The DCT transforms spatial data into frequency components, allowing for the separation of high-frequency (detail) components from low-frequency (smooth) components. Mathematically, the 1D DCT of a signal x(n) of length N is given by: The DCT coefficients X(k)X(k)X(k) represent the signal in the frequency domain. During compression, many high-frequency coefficients are quantized (reduced in precision) or discarded, resulting in data loss that is often imperceptible to human vision. In JPEG compression, an 8x8 block of image pixels is transformed using the 2D DCT, quantized, and then encoded. The quantization process discards less important information, leading to compression. Vector quantization (VQ) is a lossy compression technique that approximates data vectors by a set of representative code vectors. VQ is particularly useful in image and speech compression. The VQ process can be described as follows:\n• Codebook Generation: Create a codebook consisting of a finite set of vectors (codewords) that best represent the data.\n• Encoding: Replace each input vector with the nearest codeword in the codebook. Mathematically, for an input vector xxx, the encoded value is the index of the codeword y that minimizes the Euclidean distance ∣∣x−y∣∣. In image compression, a small block of pixels might be represented by the closest matching codeword from the codebook, effectively compressing the image by reducing the number of unique pixel patterns. Fractal compression is a technique that exploits the self-similar nature of data, particularly in images, to achieve compression. This method encodes an image by finding mathematical transformations that can reproduce the image using repeated patterns. The key idea behind fractal compression is that an image can be represented as a collection of smaller parts, each of which is similar to the entire image or to other parts of the image. These similarities are encoded as affine transformations, which include scaling, rotation, translation, and reflection. Mathematically, if a portion of the image can be represented by a transformation T applied to another portion, the transformation parameters are stored instead of the pixel values. In fractal image compression, an image of a fern might be compressed by recognizing that the same basic shape is repeated at different scales throughout the image.\n\nWavelet-based compression, such as JPEG2000, is an advanced method that offers superior performance over traditional DCT-based techniques. The wavelet transform decomposes data into a set of coefficients that represent the data at different scales. Mathematically, the wavelet transform involves convolving the signal with a wavelet function, which is localized in both time and frequency. The result is a set of coefficients that can be efficiently quantized and encoded. In JPEG2000, an image is decomposed using the discrete wavelet transform (DWT), and the resulting coefficients are quantized and encoded. The wavelet-based approach allows for better compression ratios and image quality, especially at lower bit rates. The Burrows-Wheeler Transform is a reversible data transformation that rearranges the input data in a way that clusters similar characters together, making the data more amenable to compression algorithms like RLE. The BWT works as follows:\n• Transform: Generate a matrix of all possible rotations of the input string, sort the rows lexicographically, and take the last column as the output.\n• Reverse Transform: The original string can be reconstructed from the BWT output by following a specific process. For a string “banana”, the BWT would produce “annb$aa”, which has clusters of repeated characters, making it easier to compress using RLE. PPM is a statistical modeling technique used in lossless data compression. It predicts the next symbol in a sequence based on the context provided by the previous symbols. Mathematically, PPM uses a context model to estimate the probability of the next symbol. The model is updated as more data is processed, and the predictions become more accurate. In text compression, PPM might predict the letter “e” to follow “th” with high probability, and encode this prediction efficiently.\n\nThe compression ratio is a key metric in evaluating the efficiency of a compression algorithm. It is defined as: If a file originally 10 MB in size is compressed to 2 MB, the compression ratio is 5:1. The bit rate measures the number of bits used per unit of time in lossy compression, particularly in audio and video. A lower bit rate often results in lower quality, but also smaller file sizes. For a video stream compressed to 1 Mbps, the bit rate reflects the trade-off between quality and file size. MSE and PSNR are metrics used to evaluate the quality of lossy compression, especially for images and videos. If an image is compressed and results in an MSE of 100, the PSNR might be around 28 dB, indicating moderate quality loss.\n\nCompression algorithms often involve trade-offs between compression ratio, speed, and quality. Achieving the best balance depends on the specific application. A video streaming service might choose a compression algorithm that offers the best trade-off between video quality and bandwidth usage. As technology advances, new compression methods are being developed. AI-driven techniques, such as neural network-based compression, are gaining traction for their ability to optimize compression dynamically. Deep learning models can be trained to compress images or videos more efficiently than traditional methods, adapting to the content in real-time. Compression raises ethical and legal issues, especially in multimedia distribution. Compression can alter content, raising concerns about data integrity and copyright infringement. In the digital distribution of movies, the compression used must ensure that the artistic intent of the content creator is preserved, while also respecting copyright laws.\n\nData compression is a vital technology that underpins many aspects of modern computing, from file storage and multimedia processing to data transmission and big data analytics. The mathematical principles behind compression, including entropy, redundancy, and information theory, provide the foundation for various compression algorithms. Lossless algorithms like Huffman coding, arithmetic coding, and LZW ensure that data can be perfectly reconstructed, while lossy algorithms like DCT-based transform coding and vector quantization offer greater compression at the cost of some data loss. The future of data compression lies in the development of more sophisticated algorithms that can adapt to the ever-increasing demands for storage, speed, and quality. With the advent of AI and machine learning, we can expect new breakthroughs in compression techniques that will further optimize data handling in our digital world."
    },
    {
        "link": "https://algocademy.com/blog/string-compression-and-encoding-algorithms-a-comprehensive-guide",
        "document": "In the world of computer science and programming, efficient data storage and transmission are crucial. String compression and encoding algorithms play a vital role in achieving these goals by reducing the size of data without losing essential information. This comprehensive guide will dive deep into various string compression and encoding techniques, their implementations, and their applications in real-world scenarios.\n\nString compression is the process of reducing the size of a string by encoding it in a more compact form. The primary goal is to represent the same information using fewer bits or characters. This technique is particularly useful when dealing with large amounts of text data or when bandwidth or storage is limited.\n\nThere are two main types of string compression:\n• Lossless compression: This type of compression allows the original data to be perfectly reconstructed from the compressed data.\n• Lossy compression: This type of compression reduces data size by eliminating some less critical information, resulting in a close approximation of the original data.\n\nIn this article, we’ll focus primarily on lossless compression techniques, as they are more commonly used in string compression scenarios.\n\nRun-Length Encoding is one of the simplest forms of data compression. It works by replacing consecutive occurrences of a character with a single instance of that character followed by the count of occurrences.\n\nFor example, the string “AABBBCCCC” would be encoded as “2A3B4C”. This technique is particularly effective for strings with many repeated characters in sequence.\n\nThis implementation will produce the following output:\n• Effective for data with many repeated characters\n• Can potentially increase the size of data with few repeated characters\n• Not effective for complex or diverse data patterns\n\nHuffman coding is a more sophisticated lossless data compression technique that assigns variable-length codes to characters based on their frequency of occurrence. More frequent characters are assigned shorter codes, while less frequent characters get longer codes.\n• Calculate the frequency of each character in the input string.\n• Build a priority queue (min-heap) of nodes, where each node represents a character and its frequency.\n• Repeatedly combine the two nodes with the lowest frequencies to create a new internal node until only one node remains (the root of the Huffman tree).\n• Traverse the tree to assign binary codes to each character (0 for left branches, 1 for right branches).\n• Use these codes to encode the original string.\n\nThis implementation will produce output similar to:\n• Provides optimal lossless compression for known character frequencies\n• Adaptable to different types of data\n• More complex to implement than simpler methods like RLE\n• Requires storing or transmitting the Huffman tree or codes along with the compressed data\n• Less effective for small amounts of data or data with uniform character distributions\n\nLempel-Ziv-Welch (LZW) is a universal lossless compression algorithm that is particularly effective for text compression. It works by building a dictionary of substrings encountered in the input and replacing them with shorter codes.\n• If a substring is not in the dictionary, add it and output the code for the previous substring.\n• If a substring is in the dictionary, continue building a longer substring.\n• Repeat until the entire input is processed.\n\nThis implementation will produce output similar to:\n• Adaptive compression that doesn’t require prior knowledge of the input\n• Can be less effective for small inputs or highly random data\n• Patent issues (now expired) previously limited its use in some applications\n\nThe Burrows-Wheeler Transform is not a compression algorithm itself, but a reversible transformation that can make data more compressible. It’s often used as a preprocessing step for other compression algorithms.\n• Generate all rotations of the string and sort them lexicographically.\n• Extract the last column of the sorted rotations.\n\nHere’s a basic implementation of the Burrows-Wheeler Transform in Python:\n\nThis implementation will produce the following output:\n• Can significantly improve compression ratios when used with other algorithms\n• Particularly effective for text with repeated substrings\n• Not a compression algorithm on its own\n• Can be computationally expensive for large inputs\n\nString compression and encoding algorithms have numerous practical applications across various domains:\n• Efficient comparison and analysis of DNA and protein sequences\n• Reducing size of text-based assets in games and multimedia applications\n\nSelecting the appropriate string compression or encoding algorithm depends on various factors:\n• Data characteristics: Consider the type of data you’re working with (e.g., text, binary, repetitive patterns).\n• Compression ratio: Evaluate the trade-off between compression effectiveness and computational complexity.\n• Speed requirements: Consider the importance of fast compression and decompression times for your application.\n• Memory constraints: Take into account the available memory for both compression and decompression processes.\n• Lossless vs. lossy: Determine whether perfect reconstruction of the original data is necessary.\n• Implementation complexity: Consider the development time and maintenance requirements for different algorithms.\n• Compatibility: Ensure the chosen algorithm is compatible with your target systems and platforms.\n\nString compression and encoding algorithms play a crucial role in modern computing, enabling efficient storage, transmission, and processing of data. From simple techniques like Run-Length Encoding to more sophisticated methods like Huffman coding and LZW compression, each algorithm offers unique advantages and trade-offs.\n\nAs a programmer or computer scientist, understanding these algorithms and their applications is essential for developing efficient and scalable systems. By mastering string compression techniques, you’ll be better equipped to tackle challenges related to data management, optimization, and algorithm design.\n\nRemember that the field of data compression is constantly evolving, with new algorithms and techniques being developed to address emerging challenges. Stay curious and keep exploring new approaches to improve your skills in this fascinating area of computer science.\n\nBy exploring these resources and implementing the algorithms discussed in this guide, you’ll develop a deeper understanding of string compression and encoding techniques, enabling you to create more efficient and powerful software solutions."
    },
    {
        "link": "https://reddit.com/r/askscience/comments/5cwqyl/is_there_an_optimal_way_to_compress_information",
        "document": "I mean this in the most general sense, like, can a large chunk of information be reduced to a very small piece and Is there a limit to this or can we (with the right system) reduce any amount of information?"
    },
    {
        "link": "https://encode.su/threads/4368-General-guidelines-Rules-Of-Thumb",
        "document": "This newsgroup is dedicated to image compression:"
    }
]