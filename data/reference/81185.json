[
    {
        "link": "https://tensorflow.org/tutorials/images/cnn",
        "document": "This tutorial demonstrates training a simple Convolutional Neural Network (CNN) to classify CIFAR images. Because this tutorial uses the Keras Sequential API, creating and training your model will take just a few lines of code.\n\nThe CIFAR10 dataset contains 60,000 color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them.\n\nTo verify that the dataset looks correct, let's plot the first 25 images from the training set and display the class name below each image:\n\nThe 6 lines of code below define the convolutional base using a common pattern: a stack of Conv2D and MaxPooling2D layers.\n\nAs input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size. If you are new to these dimensions, color_channels refers to (R,G,B). In this example, you will configure your CNN to process inputs of shape (32, 32, 3), which is the format of CIFAR images. You can do this by passing the argument to your first layer.\n\nLet's display the architecture of your model so far:\n\nAbove, you can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64). Typically, as the width and height shrink, you can afford (computationally) to add more output channels in each Conv2D layer.\n\nTo complete the model, you will feed the last output tensor from the convolutional base (of shape (4, 4, 64)) into one or more Dense layers to perform classification. Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor. First, you will flatten (or unroll) the 3D output to 1D, then add one or more Dense layers on top. CIFAR has 10 output classes, so you use a final Dense layer with 10 outputs.\n\nHere's the complete architecture of your model:\n\nThe network summary shows that (4, 4, 64) outputs were flattened into vectors of shape (1024) before going through two Dense layers.\n\nYour simple CNN has achieved a test accuracy of over 70%. Not bad for a few lines of code! For another CNN style, check out the TensorFlow 2 quickstart for experts example that uses the Keras subclassing API and ."
    },
    {
        "link": "https://datacamp.com/tutorial/cnn-tensorflow-python",
        "document": "Master the basics of data analysis with Python in just four hours. This online course will introduce the Python interface and explore popular packages."
    },
    {
        "link": "https://cnvrg.io/cnn-tensorflow",
        "document": "You don’t always have to design your convolutional neural networks from scratch. Other times one can try architectures developed by experts. These have proven to perform well on many image tasks. Some of these architectures are:\n• .. and many more. \n\n\n\n They can be accessed via Keras applications. These applications have also been pre-trained on the ImageNet dataset. The dataset contains over a million images. This makes these applications robust enough for use in the real world. When instantiating the model, you have the choice whether to include the pre-trained weights or not. When the weights are used, you can start using the model for classification right away. Other ways of using the pre-trained models are:\n• None extracting features and passing them to a new model Let’s take a look at how you can load the Xception architecture without weights. Since weights are not included, you can use your dataset to train the model.\n\nThe next step is to define the convolutional neural network. Here is where the convolution, pooling, and flattening layers will be applied. The first layer is the `Conv2D`layer. It’s defined with the following parameters:\n• None `same` padding to result in even padding for the input\n• None input shape of `(32, 32, 3)` because the images are of size 32 by 32. 3 notifies the network that images are colored\n• None the `relu` activation function so as to achieve non-linearity The next layer is a max-pooling layer defined with the following parameters:\n• None a `pool_size` of (2, 2) that defines the size of the pooling window\n• None 2 strides that define the number of steps taken by the pooling window Remember that you can design your network as you like. You just have to monitor the metrics and tweak the design and settle on the one that results in the best performance. In this case, another convolution and pooling layer is created. That is followed by the flatten layer whose results are passed to the dense layer. The final layer has 10 units because the dataset has 10 classes. Since it’s a multiclass problem, the Softmax activation function is applied."
    },
    {
        "link": "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/convolutional_network.ipynb",
        "document": "To see all available qualifiers, see our documentation .\n\nSaved searches Use saved searches to filter your results more quickly\n\nWe read every piece of feedback, and take your input very seriously.\n\nYou signed in with another tab or window. Reload to refresh your session.\n\nYou signed out in another tab or window. Reload to refresh your session.\n\nYou switched accounts on another tab or window. Reload to refresh your session."
    },
    {
        "link": "https://tensorflow.org/tutorials/quickstart/beginner",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nThis short introduction uses Keras to:\n• Evaluate the accuracy of the model.\n\nThis tutorial is a Google Colaboratory notebook. Python programs are run directly in the browser—a great way to learn and use TensorFlow. To follow this tutorial, run the notebook in Google Colab by clicking the button at the top of this page.\n• In Colab, connect to a Python runtime: At the top-right of the menu bar, select CONNECT.\n• To run all the code in the notebook, select Runtime > Run all. To run the code cells one at a time, hover over each cell and select the Run cell icon.\n\nImport TensorFlow into your program to get started:\n\nIf you are following along in your own development environment, rather than Colab, see the install guide for setting up TensorFlow for development.\n\nLoad and prepare the MNIST dataset. The pixel values of the images range from 0 through 255. Scale these values to a range of 0 to 1 by dividing the values by . This also converts the sample data from integers to floating-point numbers:\n\nis useful for stacking layers where each layer has one input tensor and one output tensor. Layers are functions with a known mathematical structure that can be reused and have trainable variables. Most TensorFlow models are composed of layers. This model uses the , , and layers.\n\nFor each example, the model returns a vector of logits or log-odds scores, one for each class.\n\nThe function converts these logits to probabilities for each class:\n\nThe loss function takes a vector of ground truth values and a vector of logits and returns a scalar loss for each example. This loss is equal to the negative log probability of the true class: The loss is zero if the model is sure of the correct class.\n\nThis untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to .\n\nBefore you start training, configure and compile the model using Keras . Set the class to , set the to the function you defined earlier, and specify a metric to be evaluated for the model by setting the parameter to .\n\nUse the method to adjust your model parameters and minimize the loss:\n\nThe method checks the model's performance, usually on a validation set or test set.\n\nThe image classifier is now trained to ~98% accuracy on this dataset. To learn more, read the TensorFlow tutorials.\n\nIf you want your model to return a probability, you can wrap the trained model, and attach the softmax to it:\n\nCongratulations! You have trained a machine learning model using a prebuilt dataset using the Keras API.\n\nFor more examples of using Keras, check out the tutorials. To learn more about building models with Keras, read the guides. If you want learn more about loading and preparing data, see the tutorials on image data loading or CSV data loading."
    },
    {
        "link": "https://geeksforgeeks.org/detect-an-object-with-opencv-python",
        "document": "OpenCV is the huge open-source library for computer vision, machine learning, and image processing and now it plays a major role in real-time operation which is very important in today’s systems. By using it, one can process images and videos to identify objects, faces, or even the handwriting of a human. This article focuses on detecting objects.\n\nNote: For more information, refer to Introduction to OpenCV.\n\nObject Detection is a computer technology related to computer vision, image processing, and deep learning that deals with detecting instances of objects in images and videos. We will do object detection in this article using something known as haar cascades.\n\nHaar Cascade classifiers are an effective way for object detection. This method was proposed by Paul Viola and Michael Jones in their paper Rapid Object Detection using a Boosted Cascade of Simple Features. Haar Cascade is a machine learning-based approach where a lot of positive and negative images are used to train the classifier.\n• Positive images – These images contain the images that we want our classifier to identify.\n• Negative Images – Images of everything else, which do not contain the object we want to detect.\n\nSteps to download the requirements below\n• None Run The following command in the terminal to install opencv.\n• None Run the following command to in the terminal install the Matplotlib.\n• None To download the haar cascade file and image used in the below code as a zip file click\n\nNote: Put the XML file and the PNG image in the same folder as your Python script.\n\nWe will use the function of OpenCV to recognize big signs as well as small ones:\n\nHere is the full script for lazy devs:\n\nHow to Detect Objects Using OpenCV Python\n\nHow to Use OpenCV to Detect Objects in Image and Track Them Over Time\n\nFor tracking objects over time in video streams or video files, you can use the combination of object detection (using methods like the Haar Cascade) followed by object tracking algorithms provided by OpenCV, such as KCF, TLD, or MIL.\n• Detect the Object in the First Frame : Use any object detection method to locate the object in the first frame.\n• Initialize the Tracker : Once the object is detected, initialize a tracker with the object’s position.\n• Update Tracker for Each New Frame : For each new frame in the video, update the tracker to find the object’s new position. \n\n \n\n init_box = None # Assuming init_box is the bounding box of detected object \n\n \n\n\n\n\n\n\n\n\n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n \n\n \n\n\n\n \n\n\n\n \n\n \n\n\n\n\n\n\n\n\n\nHow Do You Count the Number of Objects in an Image Using OpenCV Python?"
    },
    {
        "link": "https://configr.medium.com/mastering-computer-vision-with-pencv-in-python-185045ae045e",
        "document": "This incredibly powerful library empowers Python developers to dive into computer vision, enabling them to build applications that can see, understand, and interact with the visual world.\n\nThis article is a guide for Python enthusiasts who want to harness the potential of computer vision.\n\nWe’ll explore the fundamentals of OpenCV, walk through practical applications, and uncover best practices to help you achieve your computer vision goals."
    },
    {
        "link": "https://opencv.org/blog/deep-learning-with-computer-vision",
        "document": "The advancement of computer vision, a field blending machine learning with computer science, has been significantly uplifted by the emergence of deep learning. This article on deep learning for computer vision explores the transformative journey from traditional computer vision methods to the innovative heights of deep learning.\n\nWe begin with an overview of foundational techniques like thresholding and edge detection and the critical role of OpenCV in traditional approaches.\n\nBrief History and Evolution of Traditional Computer Vision\n\nComputer vision, a field at the intersection of machine learning and computer science, has its roots in the 1960s when researchers first attempted to enable computers to interpret visual data. The journey began with simple tasks like distinguishing shapes and progressed to more complex functions. Key milestones include the development of the first algorithm for digital image processing in the early 1970s and the subsequent evolution of feature detection methods. These early advancements laid the groundwork for modern computer vision, enabling computers to perform tasks ranging from object detection to complex scene understanding.\n\nThresholding: This technique is fundamental in image processing and segmentation. It involves converting a grayscale image into a binary image, where pixels are marked as either foreground or background based on a threshold value. For instance, in a basic application, thresholding can be used to distinguish an object from its background in a black-and-white image.\n\nEdge Detection: Critical in feature detection and image analysis, edge detection algorithms like the Canny edge detector identify the boundaries of objects within an image. By detecting discontinuities in brightness, these algorithms help understand the shapes and positions of various objects in the image, laying the foundation for more advanced analysis.\n\nOpenCV (Open Source Computer Vision Library) is a key player in computer vision, offering over 2500 optimized algorithms since the late 1990s. Its ease of use and versatility in tasks like facial recognition and traffic monitoring have made it a favorite in academia and industry, especially in real-time applications.\n\nThe field of computer vision has evolved significantly with the advent of deep learning, shifting from traditional, rule-based methods to more advanced and adaptable systems. Earlier techniques, such as thresholding and edge detection, had limitations in complex scenarios. Deep learning, particularly Convolutional Neural Networks (CNNs), overcomes these by learning directly from data, allowing for more accurate and versatile image recognition and classification.\n\nThis advancement, propelled by increased computational power and large datasets, has led to significant breakthroughs in areas like autonomous vehicles and medical imaging, making deep learning a fundamental aspect of modern computer vision.\n\nResNet-50 is a variant of the ResNet (Residual Network) model, which has been a breakthrough in the field of deep learning for computer vision, particularly in image classification tasks. The “50” in ResNet-50 refers to the number of layers in the network – it contains 50 layers deep, a significant increase compared to previous models.\n\n1. Residual Blocks: The core idea behind ResNet-50 is its use of residual blocks. These blocks allow the model to skip one or more layers through what are known as “skip connections” or “shortcut connections.” This design addresses the vanishing gradient problem, a common issue in deep networks where gradients get smaller and smaller as they backpropagate through layers, making it hard to train very deep networks.\n\n2. Improved Training: Thanks to these residual blocks, ResNet-50 can be trained much deeper without suffering from the vanishing gradient problem. This depth enables the network to learn more complex features at various levels, which is a key factor in its improved performance in image classification tasks.\n\n3. Versatility and Efficiency: Despite its depth, ResNet-50 is relatively efficient in terms of computational resources compared to other deep models. It achieves excellent accuracy on various image classification benchmarks like ImageNet, making it a popular choice in the research community and industry.\n\n4. Applications: ResNet-50 has been widely used in various real-world applications. Its ability to classify images into thousands of categories makes it suitable for tasks like object recognition in autonomous vehicles, content categorization in social media platforms, and aiding diagnostic procedures in healthcare by analyzing medical images.\n\nResNet-50 has significantly advanced the field of image classification. Its architecture serves as a foundation for many subsequent innovations in deep learning and computer vision. By enabling the training of deeper neural networks, ResNet-50 opened up new possibilities in the accuracy and complexity of tasks that computer vision systems can handle.\n\nYOLO (You Only Look Once) Model\n\nThe YOLO (You Only Look Once) model is a revolutionary approach in the field of computer vision, particularly for object detection tasks. YOLO stands out for its speed and efficiency, making real-time object detection a reality.\n\nSingle Neural Network for Detection: Unlike traditional object detection methods which typically involve separate steps for generating region proposals and classifying these regions, YOLO uses a single convolutional neural network (CNN) to do both simultaneously. This unified approach allows it to process images in real-time.\n\nSpeed and Real-Time Processing: YOLO’s architecture allows it to process images extremely fast, making it suitable for applications that require real-time detection, such as video surveillance and autonomous vehicles.\n\nGlobal Contextual Understanding: YOLO looks at the entire image during training and testing, allowing it to learn and predict with context. This global perspective helps in reducing false positives in object detection.\n\nVersion Evolution: Recent iterations such as YOLOv5, YOLOv6, YOLOv7, and the latest YOLOv8, have introduced significant improvements. These newer models focus on refining the architecture with more layers and advanced features, enhancing their performance in various real-world applications.\n\nYOLO’s contribution to the field of deep learning for computer vision has been significant. Its ability to perform object detection in real-time, accurately, and efficiently has opened up numerous possibilities for practical applications that were previously limited by slower detection speeds. Its evolution over time also reflects the rapid advancement and innovation within the field of deep learning in computer vision.\n\nTraffic Management and Surveillance Systems: A pertinent real-world application of the YOLO model is in the domain of traffic management and surveillance systems. This application showcases the model’s ability to process visual data in real time, a critical requirement for managing and monitoring urban traffic flow.\n\nImplementation in Traffic Surveillance: Vehicle and Pedestrian Detection – YOLO is employed to detect and track vehicles and pedestrians in real-time through traffic cameras. Its ability to process video feeds quickly allows for the immediate identification of different types of vehicles, pedestrians, and even anomalies like jaywalking.\n\nTraffic Flow Analysis: By continuously monitoring traffic, YOLO helps in analyzing traffic patterns and densities. This data can be used to optimize traffic light control, reducing congestion and improving traffic flow.\n\nAccident Detection and Response: The model can detect potential accidents or unusual events on roads. In case of an accident, it can alert the concerned authorities promptly, enabling faster emergency response.\n\nEnforcement of Traffic Rules: YOLO can also assist in enforcing traffic rules by detecting violations like speeding, illegal lane changes, or running red lights. Automated ticketing systems can be integrated with YOLO to streamline enforcement procedures.\n\nThis model applies the principles of transformers, originally designed for natural language processing, to image classification and detection tasks. It involves splitting an image into fixed-size patches, embedding these patches, adding positional information, and then feeding them into a transformer encoder.\n\nThe model uses a combination of Multi-head Attention Networks and Multi-Layer Perceptrons within its architecture to process these image patches and perform classification.\n\nPatch-based Image Processing: ViT divides an image into patches and linearly embeds them, treating the image as a sequence of patches.\n\nPositional Embeddings: To maintain the spatial relationship of image parts, positional embeddings are added to the patch embeddings.\n\nMulti-head Attention Mechanism: It utilizes a multi-head attention network to focus on critical regions within the image and understand the relationships between different patches.\n\nLayer Normalization: This feature ensures stable training by normalizing the inputs across the layers.\n\nMultilayer Perceptron (MLP) Head: The final stage of the ViT model, where the outputs of the transformer encoder are processed for classification.\n\nEnhanced Accuracy and Efficiency: ViT models have demonstrated significant improvements in accuracy and computational efficiency over traditional CNNs in image classification.\n\nAdaptability to Different Tasks: Beyond image classification, ViTs are effectively applied in object detection, image segmentation, and other complex vision tasks.\n\nScalability: The patch-based approach and attention mechanism make ViT scalable for processing large and complex images.\n\nInnovative Approach: By applying the transformer architecture to images, ViT represents a paradigm shift in how machine learning models perceive and process visual information.\n\nThe Vision Transformer marks a significant advancement in the field of computer vision, offering a powerful alternative to conventional CNNs and paving the way for more sophisticated image analysis techniques.\n\nVision Transformers (ViTs) are increasingly being used in a variety of real-world applications across different fields due to their efficiency and accuracy in handling complex image data.\n\nImage Classification and Object Detection: ViTs are highly effective in image classification, categorizing images into predefined classes by learning intricate patterns and relationships within the image. In object detection, they not only classify objects within an image but also localize their positions precisely. This makes them suitable for applications in autonomous driving and surveillance, where accurate detection and positioning of objects are crucial​​​​.\n\nImage Segmentation: In image segmentation, ViTs divide an image into meaningful segments or regions. They excel in discerning fine-grained details within an image and accurately delineating object boundaries. This capability is particularly valuable in medical imaging, where precise segmentation can aid in diagnosing diseases and conditions​​.\n\nAction Recognition: ViTs are being utilized in action recognition to understand and classify human actions in videos. Their robust image processing capabilities, makes them useful in areas such as video surveillance and human-computer interaction​​.\n\nGenerative Modeling and Multi-Modal Tasks: ViTs have applications in generative modeling and multi-modal tasks, including visual grounding (linking textual descriptions to corresponding image regions), visual-question answering, and visual reasoning. This reflects their versatility in integrating visual and textual information for comprehensive analysis and interpretation​​​​.\n\nTransfer Learning: An important feature of ViTs is their capacity for transfer learning. By leveraging pre-trained models on large datasets, ViTs can be fine-tuned for specific tasks with relatively small datasets. This significantly reduces the need for extensive labeled data, making ViTs practical for a wide range of applications​​.\n\nIndustrial Monitoring and Inspection: In a practical application, the DINO pre-trained ViT was integrated into Boston Dynamics’ Spot robot for monitoring and inspection of industrial sites. This application showcased the ability of ViTs to automate tasks like reading measurements from industrial processes and taking data-driven actions, demonstrating their utility in complex, real-world environments​​.\n\nAdvanced Text-to-Image Models: Stable Diffusion V2 incorporates robust text-to-image models, utilizing a new text encoder (OpenCLIP) that enhances the quality of generated images. These models can produce images with resolutions like 512×512 pixels and 768×768 pixels, offering significant improvements over previous versions​​.\n\nSuper-resolution Upscaler: A notable addition in V2 is the Upscaler Diffusion model that can increase the resolution of images by a factor of 4. This feature allows for converting low-resolution images into much higher-resolution versions, up to 2048×2048 pixels or more when combined with text-to-image models​​.\n\nDepth-to-Image Diffusion Model: This new model, known as depth2img, extends the image-to-image feature from the earlier version. It can infer the depth of an input image and then generate new images using both text and depth information. This feature opens up possibilities for creative applications in structure-preserving image-to-image and shape-conditional image synthesis​​.\n\nEnhanced Inpainting Model: Stable Diffusion V2 includes an updated text-guided inpainting model, allowing for intelligent and quick modification of parts of an image. This makes it easier to edit and enhance images with high precision​​.\n\nOptimized for Accessibility: The model is optimized to run on a single GPU, making it more accessible to a wider range of users. This optimization reflects a commitment to democratizing access to advanced AI technologies​​.\n\nRevolutionizing Image Generation: Stable Diffusion V2’s enhanced capabilities in generating high-quality, high-resolution images from textual descriptions represent a leap forward in computer-generated imagery. This opens new avenues in various fields like digital art, graphic design, and content creation.\n\nFacilitating Creative Applications: With features like depth-to-image and upscaling, Stable Diffusion V2 enables more complex and creative applications. Artists and designers can experiment with depth information and high-resolution outputs, pushing the boundaries of digital creativity.\n\nImproving Image Editing and Manipulation: The advanced inpainting capabilities of Stable Diffusion V2 allow for more sophisticated image editing and manipulation. This can have practical applications in fields like advertising, where quick and intelligent image modifications are often required.\n\nEnhancing Accessibility and Collaboration: By optimizing the model for single GPU use, Stable Diffusion V2 becomes accessible to a broader audience. This democratization could lead to more collaborative and innovative uses of AI in visual tasks, fostering a community-driven approach to AI development.\n\nSetting a New Benchmark in AI: Stable Diffusion V2’s combination of advanced features and accessibility may set new standards in the AI and computer vision community, encouraging further innovations and applications in these fields.\n\nMedical and Health Education: MultiMed, a health technology company, uses Stable Diffusion technology to provide accessible and accurate medical guidance and public health education in multiple languages​​.\n\nAudio Transcription and Image Generation: AudioSonic project transforms audio narratives into images, enhancing the listening experience with corresponding visuals​​.\n\nInterior Design: A web application utilizes Stable Diffusion to empower individuals with AI in home design, allowing customers to create and visualize interior designs quickly and efficiently​​.\n\nComic Book Production: AI-Comic-Factory combines Falcon AI and SDXL technology with Stable Diffusion to revolutionize comic book production, enhancing both narratives and visuals​​.\n\nEducational Summarization Tool: Summerize, a web application, offers structured information retrieval and summarization from online articles, along with relevant image prompts, aiding research and presentations​​.\n\nInteractive Storytelling in Gaming: SonicVision integrates generative music and dynamic art with storytelling, creating an immersive gaming experience​​.\n\nCooking and Recipe Generation: DishForge uses Stable Diffusion to visualize ingredients and generate personalized recipes based on user preferences and dietary needs​​.\n\nMarketing and Advertising: EvoMate, an autonomous marketing agent, creates targeted campaigns and content, leveraging Stable Diffusion for content creation​​.\n\nPodcast Fact-Checking and Media Enhancement: TrueCast uses AI algorithms for real-time fact-checking and media presentation during live podcasts​​.\n\nPersonal AI Assistants: Projects like Shadow AI and BlaBlaLand use Stable Diffusion for generating relevant images and creating immersive, personalized AI interactions​​.\n\n3D Meditation and Learning Platforms: Applications like 3D Meditation and PhenoVis utilize Stable Diffusion for creating immersive meditation experiences and educational 3D simulations​​.\n\nAI in Medical Education: Patient Simulator aids medical professionals in practicing patient interactions, using Stable Diffusion for enhanced communication and training​​.\n\nAdvertising Production Efficiency: ADS AI aims to improve advertising production time by using AI technologies, including Stable Diffusion, for creative product image and content generation​​.\n\nCreative Content and World Building: Platforms like Text2Room and The Universe use Stable Diffusion for generating 3D content and immersive game worlds​​.\n\nEnhanced Online Meetings: Baatcheet.AI revolutionizes online meetings with voice cloning and AI-generated backgrounds, improving focus and communication efficiency​​.\n\nThese applications demonstrate the versatility and potential of Stable Diffusion V2 in enhancing various industries by providing innovative solutions to complex problems.\n\nDeveloped by Facebook’s AI Research lab, PyTorch is an open-source machine learning library. It’s known for its flexibility, ease of use, and native support for dynamic computation graphs, which makes it particularly suitable for research and prototyping. PyTorch also provides strong support for GPU acceleration, which is essential for training large neural networks efficiently.\n\nKeras, now integrated with TensorFlow (Google’s AI framework), is a high-level neural networks API designed for simplicity and ease of use. Initially developed as an independent project, Keras focuses on enabling fast experimentation and prototyping through its user-friendly interface. It supports all the essential features needed for building deep learning models but abstracts away many of the complex details, making it very accessible for beginners.\n\nBoth frameworks are extensively used in both academic and industrial settings for a variety of machine learning and AI applications, from simple regression models to complex deep neural networks.\n\nPyTorch is often preferred for research and development due to its flexibility, while Keras is favored for its simplicity and ease of use, especially for beginners.\n\nAs we look towards the future of AI and machine learning, it’s crucial to acknowledge that one model does not fit all. Even a decade from now, we might still see the use of classic models like ResNet alongside contemporary ones like Vision Transformers or Stable Diffusion V2.\n\nThe field of AI is characterized by continuous evolution and innovation. It reminds us that the tools and models we use must adapt and diversify to meet the ever-changing demands of technology and society."
    },
    {
        "link": "https://domino.ai/blog/feature-extraction-and-image-classification-using-deep-neural-networks",
        "document": "In a previous blog post we talked about the foundations of Computer vision, the history and capabilities of the OpenCV framework, and how to make your first steps in accessing and visualising images with Python and OpenCV. Here we dive deeper into using OpenCV and DNNs for feature extraction and image classification.\n\nImage classification is one of the most promising applications of machine learning aiming to deliver algorithms with the capability to recognise and classify the content of an image with a near human accuracy.\n\nThe image classification approaches are typically divided into traditional methods, based on extraction of images features and their utility in classification, or more advanced approaches that exploit the strength of deep neural networks.\n\nThere are various features that can potentially be extracted using different machine learning algorithms. Lowe et al. (2004) developed Scale Invariant Feature Transform (SIFT) aiming to solve intensity, viewpoint changes and image rotation in feature matching [1]. SIFT allows estimation of scale-space extrema followed by keypoint localisation, orientation and subsequently computation of local image descriptor for each key point. Moreover, SIFT offers efficient recognition of objects in a given image, however, its implementation is computationally expensive and time-consuming.\n\nSIFT has inspired the development of other variants in particular to overcome the complexity and the associated computational demand. One such variant is Speed up Robust Feature (SURF), which reportedly outperforms SIFT without a negative impact on the robustness of detected points and overall model accuracy [2]. Another alternative to SIFT is Robust Independent Elementary Features (BRIEF) offering similar performance with significantly less complexity [3]. Furthermore, Rublee et al., (2011) reported that Oriented FAST and Rotated BRIEF (ORB) provides more efficient performance [4].\n\nAdditionally, Histogram of Oriented Gradients (HOG) HOG is utilised in various tasks including object detection allowing the occurrence of edge orientations to be measured. In principle, this descriptor is representative of a local statistic of the orientations for the image gradients for a key point. Simply, each descriptor can be considered as a collection of histograms that are composed of pixel orientations assigned by their gradients [5].\n\nScale-Invariant Feature transform was initially proposed by David Lowe in 2004 as a method for extracting distinctive invariant features from images with the view to use it for feature matching between images. This is a particularly useful approach as it can detect image features irrespective of orientation and size [1]. Below we provide the steps required to extract SIFT from the \"face\" image displayed in the previous step (see the \"Opening and displaying the image file\" section in the previous blog post).\n\nFigure 1. Keypoints detected with/without size in the left and right images respectively\n\nFeatures from accelerated segment test (FAST) is a corner detection method to extract feature points originally proposed by Edward Rosten and Tom Drummond in 2006. This method is very efficient and thus suitable for resource-intensive applications including real-time video processing [6]. Below we will provide the steps required to extract SIFT from the \"face\" image displayed in the previous step.\n\nOriented FAST and Rotated BRIEF (ORB) was originally proposed by Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary R. Bradski in 2011 (OpenCV Labs). This method is rotation invariant and resistant to noise. The main rationale was to provide a more efficient alternative to SIFT and SURF that is free to use without any restriction [4]. SIFT and SURF are, however, still patented algorithms. We will use the same image for the extraction of ORB descriptors.\n\nFigure 3. Keypoints detected with/without size in the left and right images respectively.\n\nFigure 4. Matching of keypoints detected in the original (main) and query (top) images.\n\nOpenCV can be utilised to solve image classification problems. The OpenCV library offers a Deep Neural Network (DNN) module, which facilitates the use of deep learning related functions within OpenCV. This module was introduced in OpenCV version 3 and this tutorial is using OpenCV v4.5.2. The main function of the DNN module is that it allows transfer learning and the use of pre-trained models. Note that the DNN modules cannot be used to train models but work with models that are trained using other popular neural network frameworks including PyTorch, TensorFlow, Caffe, Darknet and ONNX.\n\nThere are several advantages of using OpenCV DNN for inference only. Offloading model training to other libraries and using pre-trained models leads to simple and straightforward code. This makes it easier to adopt CV concepts and onboard users who don't necessarily have deep understanding of DNNs. Furthermore, this approach offers support to OpenCL-based Intel GPUs in addition to CUDA-based NVIDIA CPUs allowing for higher performance.\n\nIn this tutorial, we will use the pre-trained DenseNet121 model, trained using the Caffe framework on the ImageNet database, which consists of 1000 image classes. A DenseNet is a type of convolutional neural network (CNN) that uses dense connections between layers (via Dense Blocks). All layers with matching feature-map sizes are connected directly with each other. To use the pre-trained DenseNet model we will use the OpenCV for loading the model architecture and pre-trained weights. In this process we will perform the following steps:\n\nThe images were obtained from Unsplash, which offers free stock images. The images offered can be used for commercial and non-commercial purposes and no permission for use is required. Prior to feeding the image into the model, some pre-processing is required. These include resizing the images to 224x224, as required by the model, setting scale, and cropping the images where necessary. The pre-processing is handled by the OpenCV's cv2.dnn.blobFromImage() function.\n\nNext, we load the ImageNet image classes, create a labels list, and initialise the DNN module. OpenCV is capable to initialise Caffe models using cv2.dnn.readNetFromCaffe, TensorFlow models using cv2.dnn.readNetFromTensorFlow, and PyTorch models using cv2.dnn.readNetFromTorch, respectively. This choice is determined by the original framework that the model was trained under. DenseNet121 was trained using Caffe and thus we will initialise the DNN module using cv2.dnn.readNetFromCaffe. The pre-processed image is fed to the network and undergoes a series of computations in the forward pass phase.\n\nFigure 5. Image classification using OpenCV DNN module with pre-trained DenseNet model. For the images used in this figure, both classified names were correct with 98.7 and 91% for Persian cat and Pug classification, respectively.\n\nFor object detection, we will use two approaches: Haar cascades and OpenCV DNN module together with MobileNet SSD pre-trained model.\n\nHaar cascades, introduced by Viola and Jones in 2001 [7] is an approach that can detect objects in images irrespective of the scale and location of those objects in the image. The authors were primarily concerned with face detection, although their method can be adapted to allow detection of other objects. This is a fast method compared to more advanced approaches, but consequently the accuracy of the Haar cascade is lower. In many cases it produces a large number of false positives.\n\nOpenCV DNN module with MobileNet SSD pre-trained model. This method receives pre-processed images and feeds them to a pre-trained model. We will show it in action and then visualise the output for two different images. We will use MobileNet Single Shot Detector (SSD) trained originally on the COCO dataset using the TensorFlow deep learning framework. SSD models typically offer higher performance in terms of computation time compared to other alternatives and thus are more suitable for this tutorial.\n\nTo illustrate the use of Haar cascades we'll show how to detect eyes and faces in humans and faces in cats. The implementation is as follows.\n\nFigure 6. Object detection using Haar cascade classifiers. The image on the left is the result of eye and face detection for humans and the image on the right is the output for cat face detection.\n\nWe aim to detect objects in two images depicted in Figure 7 using the following code.\n\nFigure 7. Object detection using OpenCV and pre-trained MobileNet. In both images were a mixture of animals on the left image and animals with humans on the right image are present, this object detection method resulted in accurate labelling of the objects with appropriate corresponding bounding boxes around the objects.\n\nOpenCV is one of the main libraries utilised in computer vision. Herein, we covered a brief background on this powerful library and described the steps required for feature extraction, image classification and object detection.\n\nThe profound advancement in neural networks and deep learning over the past 10 years have resulted in significant progress in computer vision. OpenCV is one of the libraries that has made this journey possible, and we hope to have provided sufficient information to encourage our readers to seek further training on the many functions offered by OpenCV in solving their computer vision problems.\n\nYou can check the following additional resources:\n• None Read our blog on Getting Started with OpenCV\n• None There is an accompanying project with all code from this article, which you can access by signing up for the free Domino MLOps trial environment:\n\n[1] D. G. Lowe, \"Distinctive image features from scale-invariant keypoints,\" International journal of computer vision, vol. 60, no. 2, pp. 91-110, 2004.\n\n[2] H. Bay, A. Ess, T. Tuytelaars, and L. Van Gool, \"Speeded-up robust features (SURF),\" Computer vision and image understanding, vol. 110, no. 3, pp. 346-359, 2008.\n\n[3] M. Calonder, V. Lepetit, C. Strecha, and P. Fua, \"Brief: Binary robust independent elementary features,\" in European conference on computer vision, 2010: Springer, pp. 778-792.\n\n[4] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, \"ORB: An efficient alternative to SIFT or SURF,\" in 2011 International conference on computer vision, 2011: Ieee, pp. 2564-2571.\n\n[5] D. Monzo, A. Albiol, J. Sastre, and A. Albiol, \"Precise eye localization using HOG descriptors,\" Machine Vision and Applications, vol. 22, no. 3, pp. 471-480, 2011.\n\n[6] E. Rosten and T. Drummond, \"Machine learning for high-speed corner detection,\" in European conference on computer vision, 2006: Springer, pp. 430-443.\n\n[7] P. Viola and M. Jones, \"Rapid object detection using a boosted cascade of simple features,\" in Proceedings of the 2001 IEEE computer society conference on computer vision and pattern recognition. CVPR 2001, 2001, vol. 1: Ieee, pp. I-I."
    },
    {
        "link": "https://productteacher.com/quick-product-tips/opencv-basics-for-computer-vision-tasks",
        "document": "OpenCV (Open Source Computer Vision Library) is a popular open-source library packed with tools and functions that enable developers to implement a wide variety of computer vision applications. From image processing to object detection, OpenCV offers the foundational building blocks to kickstart computer vision tasks in a flexible and accessible way. In this article, we’ll explore the core functions of OpenCV and how they support common computer vision tasks.\n\nOpenCV is a computer vision library designed to process and analyze visual data from cameras, images, or videos. Written primarily in C++, it also provides interfaces in Python, Java, and other languages, making it accessible for developers across various platforms. OpenCV’s wide range of tools allows users to process images, detect patterns, and even create machine learning models tailored for visual tasks.\n\nOne of the first steps in any computer vision project is loading and preparing images for analysis. OpenCV provides straightforward functions to load images, resize them, adjust colors, and apply transformations.\n• None Loading Images: The function reads an image from a file, while allows you to display it.\n• None Resizing: With , you can adjust image dimensions, which is particularly useful for standardizing inputs for machine learning models.\n• None Color Manipulation: Functions like make it easy to convert images between color spaces, such as from RGB to grayscale, which is often necessary for simplifying analysis tasks.\n\nFiltering techniques help improve image quality by removing noise, enhancing edges, or highlighting specific details. OpenCV offers several built-in filters that are essential for extracting features from images.\n• None Blurring: The function applies a Gaussian filter to reduce noise. Blurring can make it easier to detect objects or edges in noisy images.\n• None Edge Detection: OpenCV’s function is a widely-used edge detection tool that highlights the boundaries of objects within an image. Edge detection is especially useful in object recognition, as it simplifies complex images into outlines.\n\nOpenCV provides a range of methods for detecting and recognizing objects within an image. Some of the most common techniques include template matching, contour detection, and feature-based matching.\n• None Template Matching: Template matching finds smaller image patterns within a larger image. It’s useful for recognizing fixed shapes, like detecting a company logo in various images.\n• None Contours: The function detects outlines of shapes within an image, which can be helpful for tasks like counting objects, recognizing shapes, or tracking motion.\n• None Feature Matching: OpenCV includes tools for identifying unique features within an image, such as edges and corners. By matching these features between images, OpenCV can help track movements or align images for further analysis.\n\nOpenCV also supports video processing, making it possible to analyze live or recorded video feeds frame by frame. This capability is essential for applications like surveillance, gesture recognition, and real-time tracking.\n• None Capturing Video: The function allows OpenCV to access video streams from cameras or video files, enabling frame-by-frame analysis.\n• None Frame Processing: Each frame can be processed with the same image functions, allowing for consistent analysis over time. For example, edge detection, blurring, and contour finding can be applied to each frame to detect motion or track objects.\n\nOpenCV’s capabilities make it a powerful tool for real-time object tracking, which is essential for applications such as surveillance, robotics, and automated quality control in manufacturing. Using contour and feature matching functions, OpenCV can detect, track, and analyze objects in motion.\n\nOpenCV’s filtering functions help product teams enhance image quality, making visual insights clearer and more accurate. This can be useful in fields like healthcare, where enhanced medical images improve diagnostic accuracy, or in e-commerce, where better images improve product presentation.\n\nProduct teams exploring machine learning applications can leverage OpenCV for quick data preprocessing and prototyping. From resizing and cropping images to detecting and isolating features, OpenCV simplifies the steps required to prepare image data for model training.\n\nOpenCV’s extensive libraries make it accessible for teams of various skill levels. With support for multiple programming languages and platforms, it’s easy to integrate into diverse tech stacks, enabling both rapid prototyping and production-ready implementations.\n\nAs an open-source library, OpenCV is free to use, making it a cost-effective choice for product teams that need robust image processing and computer vision tools without investing in costly software.\n\nOpenCV is designed for efficiency and can handle large volumes of images or video frames at high speed. This allows product teams to analyze data in real time, which is crucial for applications where timely insights drive decision-making, such as automated inspection in manufacturing.\n\nOpenCV is an invaluable tool for product teams looking to add computer vision capabilities to their applications. From basic image preprocessing to advanced object detection and real-time tracking, OpenCV offers a comprehensive suite of tools that make it easy to build and deploy visual applications. By understanding the core functions of OpenCV, product teams can unlock new capabilities in fields such as real-time analytics, augmented reality, and automated quality control."
    }
]