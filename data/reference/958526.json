[
    {
        "link": "https://stackoverflow.com/questions/61289076/expression-evaluation-using-visitor-pattern",
        "document": "As I understand it, you want to parse a textual expression to produce something that you can evaluate multiple times.\n\nBy the time you get that \"something you can evaluate\", the details of the expression language you parsed, including the tree structure, the kinds of operators, etc., should all be taken care of. Don't push those concerns into the code the just needs to evaluate.\n\nSo, your parsing process should produce an object that implements an interface something like this:\n\nHere, the object holds the values of variables, function definitions, or whatever -- anything that expressions can use as input.\n\nAs you can see the interface allows any expression to be evaluated as a boolean (if used in an if, for example), as a number, or as a string. Depending on your expression language, some of these may throw . For something like JavaScript, for example, they would almost always all work.\n\nYou can also have methods that tell you things about the expression like . Depending on the language, it may be important to provide the type of object that the expression naturally produces.\n\nTo keep operation implementation concerns out of the parser, you may want to provide it with a factory interface like this:\n\nThe parser would call a method in here for each sub-expression, to create it from its children.\n\nThe different kinds of subexpression can be implemented any way you like. In Java, I usually use classes for each category with lambda arguments like:\n\nThat saves you from having to having to write a class for each operator. When I'm super concerned about evaluation speed, I'll use inline classes with abstract bases.\n\nIMPORTANT: Note that because a compiled doesn't expose the expression tree that was parsed to create it, you don't have to preserve that structure, and can do optimizations like constant folding:\n\nBy processing as much as possible during compilation, you can make evaluation faster, by getting rid of all type/conversion checks, and performing various optimizations."
    },
    {
        "link": "https://koerbitz.me/posts/Sum-Types-Visitors-and-the-Expression-Problem.html",
        "document": "I've heard that the visitor pattern is just a poor way of getting the benefit of sum types^[These are also known as disjoint union or variant types.] in functional programming circles several times. I must admit that I never had completely thought this through, but I was nevertheless a bit surprised when I saw that walking the AST in Rust was implemented by what looks like a use of the visitor pattern. Languages with sum types usually use pattern matching to achieve the same effect and I had always considered this a superior approach. In this blog post I try to understand the differences and similarities of the two approaches a little better.\n\nTo set the stage, both pattern matching and the visitor pattern solve one side of the expression problem, which is the problem of adding both variants of a data type and functions that act on those variants without changing or recompiling old code and without loosing type safety.\n\nTo make this a bit more concrete, consider a very simple expression languages consisting of numbers and addition as an example (no post on this topic can do without one!). We have two variants of expressions, (1) numbers and (2) addition. Let's assume that we want to compute the values represented by an expression as a first operation.\n\nIn Haskell a straightforward way of solving this problem is as follows\n\nIf you're not familiar with Haskell, the first line defines a data type with two variants, it can either be a , which holds an , or it is an which holds two expressions. and are called constructors of . The function pattern-matches and handles each case.\n\nNow imagine that we do not only want to evaluate expressions but also pretty-print them. Adding operations is easy in Haskell, we just write a new function:\n\nIn Java we might achieve something similar by introducing an class:\n\nBut now, if we want to add the operation, we have to touch every class. This is the side of the expression problem that functional languages tend to solve better than object oriented languages. However, the object oriented programming community has devised the visitor pattern as a way to solve this problem:\n\nOk, this is not exactly pretty, but let's not forget that this is the side of the problem where OO languages are not good at. At least we can pull something of. And now we are in a situation where we can add new operations pretty easily:\n\nThis works, but the Haskell solution is clearly more elegant. Does the visitor pattern have any additional advantages? Well, neither approach solves the expression problem: if we want to add a new variant, say a , then we have to change existing code in both cases.\n\nI can't really think of an advantage for the visitor pattern. I've thought of two possibilities, default implementations and almost-but-not-quite-solving-the-expression-problem. But then I realized that the first problem is also similarly solvable in the pattern matching approach and that the second problem doesn't work without loosing type safety or duplicating code:^[Both maintaining type safety and not duplicating code are requirements in the expression problem.]\n• None Default implementations are easy to implement with both approaches: in the visitor pattern defaults can be achieved by inherenting from a visitor with default implementations and overriding only certain methods. In the pattern-matching approach we would match all the constructors where we want to override the defaults and insert a wildcard match for the rest and call the default implementaiton on the bound variable.\n• None Almost-but-not-quite-solving-the-expression-problem: I first thought that we could use some inheritance based trickery to solve the expression problem at least for new code. But none of these seems to work: If we add a new variant, say , it can't derive from because then it would have to implement 's accept method, which it can't sensibly do (because there is no right method in ). Thus we must introduce a new interface . cannot derive from , lest we have the same problem as before. But the old variants don't derive from , so this is of limited use. Whichever way we twist or turn it, there is no easy way to solve the expression problem with this pattern.\n\nSo, as it stands, I can't really come up with an advantage for the visitor pattern over pattern matching. If you work in a language without sum types then it is certainly a great workaround, but in a language that does pattern matching seems much both more concise and more efficient.^[Due to the virtual method calls, which prevent inlining, I would expect the visitor pattern to be much slower than a direct function call.]"
    },
    {
        "link": "https://reddit.com/r/haskell/comments/6ugzhe/walking_the_ast_without_the_visitor_pattern",
        "document": "I am currently writing a compiler in Rust, and even though I am not a big fan of patterns, I semi successfully used the visitor pattern to iterate several times over the AST.\n\nI am wondering how you would approach it in Haskell? I have tried to find few toy language implementations written in Haskell, but most were abandoned before they got interesting.\n\nI'll try to keep the Rust code at a minimum.\n\nI already need to walk the AST several times, generate scopes, gather type information, type checking, generate ASM etc. Because I use traits (type classes) with default implementations I can very quickly write code that walks the AST as I only need to change the methods that I need to change. And when I do need to change something I can reuse the walk functions.\n\nWithout it I would have to manually write the walk logic over and over again. The ugly part is that return types don't really fit nicely in this pattern and I write into hashmaps which makes multithreading almost impossible.\n\nI am wondering how you would do it in Haskell? I hope it is not too offtopic."
    },
    {
        "link": "https://stackoverflow.com/questions/63242850/how-to-simulate-haskells-pattern-matching-using-visitor-pattern-in-java",
        "document": "I am new to Java and I am trying to use visitor pattern to simulate pattern matching. For this Haskell function\n\nwhich returns a value of type (given a value of type and a default value), the following Java codes work.\n\nWhen I try to simulate this function that obtains the \"length\" of a value of type\n\nBut it doesn't work, since and are incompatible. How to fix this?"
    },
    {
        "link": "https://ghc.gitlab.haskell.org/ghc/doc/users_guide/exts/view_patterns.html",
        "document": "View patterns are enabled by the language extension . More information and examples of view patterns can be found on the Wiki page.\n\nView patterns are somewhat like pattern guards that can be nested inside of other patterns. They are a convenient way of pattern-matching against values of abstract types. For example, in a programming language implementation, we might represent the syntax of the types of the language as follows:\n\nThe representation of Typ is held abstract, permitting implementations to use a fancy representation (e.g., hash-consing to manage sharing). Without view patterns, using this signature is a little inconvenient:\n\nIt is necessary to iterate the case, rather than using an equational function definition. And the situation is even worse when the matching against is buried deep inside another pattern.\n\nView patterns permit calling the view function inside the pattern and matching against the result:\n\nThat is, we add a new form of pattern, written ⟨expression⟩ ⟨pattern⟩ that means “apply the expression to whatever we’re trying to match against, and then match the result of that application against the pattern”. The expression can be any Haskell expression of function type, and view patterns can be used wherever patterns are used.\n\nThe semantics of a pattern ⟨exp⟩ ⟨pat⟩ are as follows:\n• None Scoping: The variables bound by the view pattern are the variables bound by ⟨pat⟩. Any variables in ⟨exp⟩ are bound occurrences, but variables bound “to the left” in a pattern are in scope. This feature permits, for example, one argument to a function to be used in the view of another argument. For example, the function from Pattern guards can be written using view patterns as follows: More precisely, the scoping rules are:\n• None In a single pattern, variables bound by patterns to the left of a view pattern expression are in scope. For example: Additionally, in function definitions, variables bound by matching earlier curried arguments may be used in view pattern expressions in later arguments: That is, the scoping is the same as it would be if the curried arguments were collected into a tuple.\n• None In mutually recursive bindings, such as , , or the top level, view patterns in one declaration may not mention variables bound by other declarations. That is, each declaration must be self-contained. For example, the following program is not allowed: (For some amplification on this design choice see #4061.\n• None Typing: If ⟨exp⟩ has type ⟨T1⟩ ⟨T2⟩ and ⟨pat⟩ matches a ⟨T2⟩, then the whole view pattern matches a ⟨T1⟩.\n• None Matching: To the equations in Section 3.17.3 of the Haskell 98 Report, add the following: That is, to match a variable ⟨v⟩ against a pattern ⟨exp⟩ ⟨pat⟩ , evaluate ⟨exp⟩ ⟨v⟩ and match the result against ⟨pat⟩.\n• None Efficiency: When the same view function is applied in multiple branches of a function definition or a case expression (e.g., in above), GHC makes an attempt to collect these applications into a single nested case expression, so that the view function is only applied once. Pattern compilation in GHC follows the matrix algorithm described in Chapter 4 of The Implementation of Functional Programming Languages. When the top rows of the first column of a matrix are all view patterns with the “same” expression, these patterns are transformed into a single nested case. This includes, for example, adjacent view patterns that line up in a tuple, as in The current notion of when two view pattern expressions are “the same” is very restricted: it is not even full syntactic equality. However, it does include variables, literals, applications, and tuples; e.g., two instances of will be collected. However, the current implementation does not compare up to alpha-equivalence, so two instances of will not be coalesced."
    },
    {
        "link": "https://hackage.haskell.org/package/base/docs/Control-Applicative.html",
        "document": "pure :: a -> IO a Source # (<*>) :: IO (a -> b) -> IO a -> IO b Source # liftA2 :: (a -> b -> c) -> IO a -> IO b -> IO c Source # (*>) :: IO a -> IO b -> IO b Source # (<*) :: IO a -> IO b -> IO a Source #\n\npure :: a -> K1 i c a Source # (<*>) :: K1 i c (a -> b) -> K1 i c a -> K1 i c b Source # liftA2 :: (a -> b -> c0) -> K1 i c a -> K1 i c b -> K1 i c c0 Source # (*>) :: K1 i c a -> K1 i c b -> K1 i c b Source # (<*) :: K1 i c a -> K1 i c b -> K1 i c a Source #\n\npure :: a -> M1 i c f a Source # (<*>) :: M1 i c f (a -> b) -> M1 i c f a -> M1 i c f b Source # liftA2 :: (a -> b -> c0) -> M1 i c f a -> M1 i c f b -> M1 i c f c0 Source # (*>) :: M1 i c f a -> M1 i c f b -> M1 i c f b Source # (<*) :: M1 i c f a -> M1 i c f b -> M1 i c f a Source #"
    },
    {
        "link": "https://en.wikibooks.org/wiki/Haskell/Applicative_functors",
        "document": "When covering the vital and type classes, we glossed over a third type class: , the class for applicative functors. Like monads, applicative functors are functors with extra laws and operations; in fact, is an intermediate class between and . is a widely used class with a wealth of applications. It enables the eponymous applicative style, a convenient way of structuring functorial computations, and also provides means to express a number of important patterns.\n\nWe will begin with a quick review of the Functor class chapter. is characterised by the function:\n\nIf a type is an instance of , you can use to apply a function to values in it. Another way of describing is saying that it promotes functions to act on functorial values. To ensure works sanely, any instance of must comply with the following two laws:\n\n, for example, has a instance, and so we can easily modify the value inside it...\n\n...as long as it exists, of course.\n\nhas an infix synonym, . It often helps readability, and also suggests how can be seen as a different kind of function application.\n\nAs useful as it is, isn't much help if we want to apply a function of two arguments to functorial values. For instance, how could we sum and ? The brute force approach would be extracting the values from the wrapper. That, however, would mean having to do tedious checks for . Even worse: in a different extracting the value might not even be an option (just think about ).\n\nWe could use to partially apply to the first argument:\n\nBut now we are stuck: we have a function and a value both wrapped in , and no way of applying one to the other. The closest we can get to the desired is probably this:\n\nWhat we would like to have is an operator with a type akin to to apply functions in the context of a functor. If that operator was called , we would be able to write:\n\nThe type of is:\n\nis one of the methods of , the type class of applicative functors - functors that support function application within their contexts. Expressions such as are said to be written in applicative style, which is as close as we can get to regular function application while working with a functor. If you pretend for a moment the , and aren't there, our example looks just like .\n\nThe definition of is:\n\nBeyond , the class has a second method, , which brings arbitrary values into the functor. As an example, let's have a look at the instance:\n\nIt doesn't do anything surprising: wraps the value with ; applies the function to the value if both exist, and results in otherwise.\n\n\n\n Just like , has a set of laws which reasonable instances should follow. They are:\n\nThose laws are a bit of a mouthful. They become easier to understand if you think of as a way to inject values into the functor in a default, featureless way, so that the result is as close as possible to the plain value. Thus:\n• The identity law says that applying the morphism does nothing, exactly like with the plain function.\n• The homomorphism law says that applying a \"pure\" function to a \"pure\" value is the same as applying the function to the value in the normal way and then using on the result. In a sense, that means preserves function application.\n• The interchange law says that applying a morphism to a \"pure\" value is the same as applying to the morphism. No surprises there - as we have seen in the higher order functions chapter, is the function that supplies as an argument to another function.\n• The composition law says that composes morphisms similarly to how composes functions: applying the composed morphism to gives the same result as applying to the result of applying to . 1\n\nThere is also a bonus law about the relation between and :\n\nApplying a \"pure\" function with is equivalent to using . This law is a consequence of the other ones, so you need not bother with proving it when writing instances of .\n\nDoes remind you of anything?\n\nThe only difference between that and...\n\n... is the class constraint. and serve the same purpose; that is, bringing values into functors. The uncanny resemblances do not stop here. Back in the chapter about we mentioned a function called ...\n\n... which could be used to make functions with many arguments less painful to handle in monadic code:\n\nThose, of course, are not coincidences. inherits from ...\n\n... because and are enough to implement and [2].\n\nSeveral other monadic functions have more general applicative versions. Here are a few of them:\n\nLists are applicative functors as well. Specialised to lists, the type of becomes...\n\n... and so applies a list of functions to another list. But exactly how is that done?\n\nThe standard instance of for lists, which follows from the instance, applies every function to every element, like an explosive version of .\n\nInterestingly, there is another reasonable way of applying a list of functions. Instead of using every combination of functions and values, we can match each function with the value in the corresponding position in the other list. A function which can be used for that is :\n\nWhen there are two useful possible instances for a single type, the dilemma is averted by creating a which implements one of them. In this case, we have , which lives in Control.Applicative:\n\nWe have already seen what should be for zip-lists; all that is needed is to add the wrappers:\n\nAs for , it is tempting to use , following the standard list instance. We can't do that, however, as it violates the applicative laws. According to the identity law:\n\nSubstituting and the suggested , we get:\n\nNow, suppose is the infinite list :\n\nThe problem is that produces lists whose length is that of the shortest list passed as argument, and so will cut off all elements of the other zip-list after the first. The only way to ensure never removes elements is making infinite. The correct follows from that:\n\nThe applicative instance offers an alternative to all the zipN and zipWithN functions in Data.List which can be extended to any number of arguments:\n\nAs we have just seen, the standard instance for lists applies every function in one list to every element of the other. That, however, does not specify unambiguously. To see why, try to guess what is the result of without looking at the example above or the answer just below.\n\nUnless you were paying very close attention or had already analysed the implementation of , the odds of getting it right were about even. The other possibility would be . The difference is that for the first (and correct) answer the result is obtained by taking the skeleton of the first list and replacing each element by all possible combinations with elements of the second list, while for the other possibility the starting point is the second list.\n\nIn more general terms, the difference between is one of sequencing of effects. Here, by effects we mean the functorial context, as opposed to the values within the functor (some examples: the skeleton of a list, actions performed in the real world in , the existence of a value in ). The existence of two legal implementations of for lists which only differ in the sequencing of effects indicates that is a non-commutative applicative functor. A commutative applicative functor, by contrast, leaves no margin for ambiguity in that respect. More formally, a commutative applicative functor is one for which the following holds:\n\nBy the way, if you hear about commutative monads in Haskell, the concept involved is the same, only specialised to .\n\nCommutativity (or the lack thereof) affects other functions which are derived from as well. is a clear example:\n\ncombines effects while preserving only the values of its second argument. For monads, it is equivalent to . Here is a demonstration of it using , which is commutative:\n\nSwapping the arguments does not affect the effects (that is, the being and nothingness of wrapped values). For , however, swapping the arguments does reorder the effects:\n\nThe convention in Haskell is to always implement and other applicative operators using left-to-right sequencing. Even though this convention helps reducing confusion, it also means appearances sometimes are misleading. For instance, the function is not , as it sequences effects from left to right just like :\n\nFor the same reason, from is not . That means it provides a way of inverting the sequencing:\n\nAn alternative is the Control.Applicative.Backwards module from , which offers a for flipping the order of effects:\n\n, , . Three closely related functor type classes; three of the most important classes in Haskell. Though we have seen many examples of and in use, and a few of , we have not compared them head to head yet. If we ignore / for a moment, the characteristic methods of the three classes are:\n\nWhile those types look disparate, we can change the picture with a few cosmetic adjustments. Let's replace by its infix synonym, ; by its flipped version, ; and tidy up the signatures a bit:\n\nSuddenly, the similarities are striking. , and are all mapping functions over s [3]. The differences between them are in what is being mapped over in each case:\n\nThe day-to-day differences in uses of , and follow from what the types of those three mapping functions allow you to do. As you move from to and then to , you gain in power, versatility and control, at the cost of guarantees about the results. We will now slide along this scale. While doing so, we will use the contrasting terms values and context to refer to plain values within a functor and to whatever surrounds them, respectively.\n\nThe type of and the two associated laws ensure that it is impossible to use it to change the context, no matter which function it is given. In , the function has nothing to do with the context of the functorial value, and so applying it cannot affect the context. For that reason, if you do on some list the number of elements of the list will never change.\n\nThat can be taken as a safety guarantee or as an unfortunate restriction, depending on what you intend. In any case, is clearly able to change the context:\n\nThe morphism carries a context of its own, which is combined with that of the functorial value. , however, is subject to a more subtle restriction. While morphisms carry context, within them there are plain , which are still unable to modify the context. That means the changes to the context performs are fully determined by the context of its arguments, and the values have no influence over the resulting context.\n\nThus with list you know that the length of the resulting list will be the product of the lengths of the original lists, with you know that all real-world effects will happen as long as the evaluation terminates, and so forth.\n\nWith , however, we are in a very different game. takes a function, and so it is able to create context from values. That means a lot of flexibility:\n\nTaking advantage of the extra flexibility, however, might mean having fewer guarantees about, for instance, whether your functions are able to unexpectedly erase parts of a data structure for pathological inputs, or whether the control flow in your application remains intelligible. In some situations there might be performance implications as well, as the complex data dependencies monadic code makes possible might prevent useful refactorings and optimisations. All in all, it is a good idea to only use as much power as needed for the task at hand. If you do need the extra capabilities of , go right ahead; however, it is often worth it to check whether or are sufficient.\n\nBack in Understanding monads, we saw how the class can be specified using either or instead of . In a similar way, also has an alternative presentation, which might be implemented through the following type class:\n\nThere are deep theoretical reasons behind the name \"monoidal\" [4]. In any case, we can informally say that it does look a lot like a monoid: provides a default functorial value whose context wraps nothing of interest, and combines functorial values by pairing values and combining effects. The formulation provides a clearer view of how manipulates functorial contexts. Naturally, and can be used to define and , and vice-versa.\n\nThe laws are equivalent to the following set of laws, stated in terms of :\n\nThe functions to the left of the are just boilerplate to convert between equivalent types, such as and . If you ignore them, the laws are a lot less opaque than in the usual formulation. By the way, just like for there is a bonus law, which is guaranteed to hold in Haskell:\n• The corresponding property for and plain functions is .\n• And if the instance follows the monad laws, the resulting and will automatically follow the applicative laws.\n• It is not just a question of type signatures resembling each other: the similarity has theoretical ballast. One aspect of the connection is that it is no coincidence that all three type classes have identity and composition laws.\n• For extra details, follow the leads from the corresponding section of the Typeclasseopedia and the blog post by Edward Z. Yang which inspired it"
    },
    {
        "link": "https://stackoverflow.com/questions/61226027/why-is-lifta2-added-to-applicative-as-a-method",
        "document": "The email is written in 2017. At that time the typeclass looked like:\n\nSo without the as part of the typeclass. It was defined as [src]:\n\nso one could not make a special implementation in the typeclass. This means that sometimes can be implemented more efficiently, but one can not define that.\n\nFor example the functor and are implemented as:\n\nThis thus means that the for a is implemented similar to:\n\nBut this is not optimal. It means that will inspect if the parameter is a , or , and then return a or a . But regardless, will again inspect that, whereas we can already know that in advance. We can make a more efficient implementation with:\n\nhere we avoid extra unpacking of data constructors. This is not that problematic however. For certain data structures like a the overhead will be more severe because the number of objects is larger.\n\nOn June 23, 2017, a new library was published where the function was added as a method to the type class."
    },
    {
        "link": "https://learnyouahaskell.com/functors-applicative-functors-and-monoids",
        "document": "Haskell's combination of purity, higher order functions, parameterized algebraic data types, and typeclasses allows us to implement polymorphism on a much higher level than possible in other languages. We don't have to think about types belonging to a big hierarchy of types. Instead, we think about what the types can act like and then connect them with the appropriate typeclasses. An Int can act like a lot of things. It can act like an equatable thing, like an ordered thing, like an enumerable thing, etc.\n\nTypeclasses are open, which means that we can define our own data type, think about what it can act like and connect it with the typeclasses that define its behaviors. Because of that and because of Haskell's great type system that allows us to know a lot about a function just by knowing its type declaration, we can define typeclasses that define behavior that's very general and abstract. We've met typeclasses that define operations for seeing if two things are equal or comparing two things by some ordering. Those are very abstract and elegant behaviors, but we just don't think of them as anything very special because we've been dealing with them for most of our lives. We recently met functors, which are basically things that can be mapped over. That's an example of a useful and yet still pretty abstract property that typeclasses can describe. In this chapter, we'll take a closer look at functors, along with slightly stronger and more useful versions of functors called applicative functors. We'll also take a look at monoids, which are sort of like socks.\n\nWe've already talked about functors in their own little section. If you haven't read it yet, you should probably give it a glance right now, or maybe later when you have more time. Or you can just pretend you read it.\n\nStill, here's a quick refresher: Functors are things that can be mapped over, like lists, Maybes, trees, and such. In Haskell, they're described by the typeclass Functor, which has only one typeclass method, namely fmap, which has a type of fmap :: (a -> b) -> f a -> f b. It says: give me a function that takes an a and returns a b and a box with an a (or several of them) inside it and I'll give you a box with a b (or several of them) inside it. It kind of applies the function to the element inside the box.\n\nA word of advice. Many times the box analogy is used to help you get some intuition for how functors work, and later, we'll probably use the same analogy for applicative functors and monads. It's an okay analogy that helps people understand functors at first, just don't take it too literally, because for some functors the box analogy has to be stretched really thin to still hold some truth. A more correct term for what a functor is would be computational context. The context might be that the computation can have a value or it might have failed ( and ) or that there might be more values (lists), stuff like that.\n\nIf we want to make a type constructor an instance of Functor, it has to have a kind of * -> *, which means that it has to take exactly one concrete type as a type parameter. For example, Maybe can be made an instance because it takes one type parameter to produce a concrete type, like Maybe Int or Maybe String. If a type constructor takes two parameters, like Either, we have to partially apply the type constructor until it only takes one type parameter. So we can't write instance Functor Either where, but we can write instance Functor (Either a) where and then if we imagine that fmap is only for Either a, it would have a type declaration of fmap :: (b -> c) -> Either a b -> Either a c. As you can see, the Either a part is fixed, because Either a takes only one type parameter, whereas just Either takes two so fmap :: (b -> c) -> Either b -> Either c wouldn't really make sense.\n\nWe've learned by now how a lot of types (well, type constructors really) are instances of Functor, like [], Maybe, Either a and a Tree type that we made on our own. We saw how we can map functions over them for great good. In this section, we'll take a look at two more instances of functor, namely IO and (->) r.\n\nIf some value has a type of, say, IO String, that means that it's an I/O action that, when performed, will go out into the real world and get some string for us, which it will yield as a result. We can use <- in do syntax to bind that result to a name. We mentioned that I/O actions are like boxes with little feet that go out and fetch some value from the outside world for us. We can inspect what they fetched, but after inspecting, we have to wrap the value back in IO. By thinking about this box with little feet analogy, we can see how IO acts like a functor.\n\nLet's see how IO is an instance of Functor. When we fmap a function over an I/O action, we want to get back an I/O action that does the same thing, but has our function applied over its result value.\n\nThe result of mapping something over an I/O action will be an I/O action, so right off the bat we use do syntax to glue two actions and make a new one. In the implementation for fmap, we make a new I/O action that first performs the original I/O action and calls its result result. Then, we do return (f result). return is, as you know, a function that makes an I/O action that doesn't do anything but only presents something as its result. The action that a do block produces will always have the result value of its last action. That's why we use return to make an I/O action that doesn't really do anything, it just presents f result as the result of the new I/O action.\n\nWe can play around with it to gain some intuition. It's pretty simple really. Check out this piece of code:\n\nThe user is prompted for a line and we give it back to the user, only reversed. Here's how to rewrite this by using fmap:\n\nJust like when we fmap reverse over Just \"blah\" to get Just \"halb\", we can fmap reverse over getLine. getLine is an I/O action that has a type of IO String and mapping reverse over it gives us an I/O action that will go out into the real world and get a line and then apply reverse to its result. Like we can apply a function to something that's inside a Maybe box, we can apply a function to what's inside an IO box, only it has to go out into the real world to get something. Then when we bind it to a name by using <-, the name will reflect the result that already has reverse applied to it.\n\nThe I/O action fmap (++\"!\") getLine behaves just like getLine, only that its result always has \"!\" appended to it!\n\nIf we look at what fmap's type would be if it were limited to IO, it would be fmap :: (a -> b) -> IO a -> IO b. fmap takes a function and an I/O action and returns a new I/O action that's like the old one, except that the function is applied to its contained result.\n\nIf you ever find yourself binding the result of an I/O action to a name, only to apply a function to that and call that something else, consider using fmap, because it looks prettier. If you want to apply multiple transformations to some data inside a functor, you can declare your own function at the top level, make a lambda function or ideally, use function composition:\n\nAs you probably know, intersperse '-' . reverse . map toUpper is a function that takes a string, maps toUpper over it, the applies reverse to that result and then applies intersperse '-' to that result. It's like writing (\\xs -> intersperse '-' (reverse (map toUpper xs))), only prettier.\n\nAnother instance of Functor that we've been dealing with all along but didn't know was a Functor is (->) r. You're probably slightly confused now, since what the heck does (->) r mean? The function type r -> a can be rewritten as (->) r a, much like we can write 2 + 3 as (+) 2 3. When we look at it as (->) r a, we can see (->) in a slighty different light, because we see that it's just a type constructor that takes two type parameters, just like Either. But remember, we said that a type constructor has to take exactly one type parameter so that it can be made an instance of Functor. That's why we can't make (->) an instance of Functor, but if we partially apply it to (->) r, it doesn't pose any problems. If the syntax allowed for type constructors to be partially applied with sections (like we can partially apply + by doing (2+), which is the same as (+) 2), you could write (->) r as (r ->). How are functions functors? Well, let's take a look at the implementation, which lies in Control.Monad.Instances\n\nWe usually mark functions that take anything and return anything as . is the same thing, we just used different letters for the type variables.\n\nIf the syntax allowed for it, it could have been written as\n\nBut it doesn't, so we have to write it in the former fashion.\n\nFirst of all, let's think about fmap's type. It's fmap :: (a -> b) -> f a -> f b. Now what we'll do is mentally replace all the f's, which are the role that our functor instance plays, with (->) r's. We'll do that to see how fmap should behave for this particular instance. We get fmap :: (a -> b) -> ((->) r a) -> ((->) r b). Now what we can do is write the (->) r a and (-> r b) types as infix r -> a and r -> b, like we normally do with functions. What we get now is fmap :: (a -> b) -> (r -> a) -> (r -> b).\n\nHmmm OK. Mapping one function over a function has to produce a function, just like mapping a function over a Maybe has to produce a Maybe and mapping a function over a list has to produce a list. What does the type fmap :: (a -> b) -> (r -> a) -> (r -> b) for this instance tell us? Well, we see that it takes a function from a to b and a function from r to a and returns a function from r to b. Does this remind you of anything? Yes! Function composition! We pipe the output of r -> a into the input of a -> b to get a function r -> b, which is exactly what function composition is about. If you look at how the instance is defined above, you'll see that it's just function composition. Another way to write this instance would be:\n\nThis makes the revelation that using fmap over functions is just composition sort of obvious. Do :m + Control.Monad.Instances, since that's where the instance is defined and then try playing with mapping over functions.\n\nWe can call fmap as an infix function so that the resemblance to . is clear. In the second input line, we're mapping (*3) over (+100), which results in a function that will take an input, call (+100) on that and then call (*3) on that result. We call that function with 1.\n\nHow does the box analogy hold here? Well, if you stretch it, it holds. When we use fmap (+3) over Just 3, it's easy to imagine the Maybe as a box that has some contents on which we apply the function (+3). But what about when we're doing fmap (*3) (+100)? Well, you can think of the function (+100) as a box that contains its eventual result. Sort of like how an I/O action can be thought of as a box that will go out into the real world and fetch some result. Using fmap (*3) on (+100) will create another function that acts like (+100), only before producing a result, (*3) will be applied to that result. Now we can see how fmap acts just like . for functions.\n\nThe fact that fmap is function composition when used on functions isn't so terribly useful right now, but at least it's very interesting. It also bends our minds a bit and let us see how things that act more like computations than boxes (IO and (->) r) can be functors. The function being mapped over a computation results in the same computation but the result of that computation is modified with the function.\n\nBefore we go on to the rules that fmap should follow, let's think about the type of fmap once more. Its type is fmap :: (a -> b) -> f a -> f b. We're missing the class constraint (Functor f) =>, but we left it out here for brevity, because we're talking about functors anyway so we know what the f stands for. When we first learned about curried functions, we said that all Haskell functions actually take one parameter. A function a -> b -> c actually takes just one parameter of type a and then returns a function b -> c, which takes one parameter and returns a c. That's how if we call a function with too few parameters (i.e. partially apply it), we get back a function that takes the number of parameters that we left out (if we're thinking about functions as taking several parameters again). So a -> b -> c can be written as a -> (b -> c), to make the currying more apparent.\n\nIn the same vein, if we write fmap :: (a -> b) -> (f a -> f b), we can think of fmap not as a function that takes one function and a functor and returns a functor, but as a function that takes a function and returns a new function that's just like the old one, only it takes a functor as a parameter and returns a functor as the result. It takes an a -> b function and returns a function f a -> f b. This is called lifting a function. Let's play around with that idea by using GHCI's :t command:\n\nThe expression fmap (*2) is a function that takes a functor f over numbers and returns a functor over numbers. That functor can be a list, a Maybe , an Either String, whatever. The expression fmap (replicate 3) will take a functor over any type and return a functor over a list of elements of that type.\n\nWhen we say a functor over numbers, you can think of that as a functor that has numbers in it. The former is a bit fancier and more technically correct, but the latter is usually easier to get.\n\nThis is even more apparent if we partially apply, say, fmap (++\"!\") and then bind it to a name in GHCI.\n\nYou can think of fmap as either a function that takes a function and a functor and then maps that function over the functor, or you can think of it as a function that takes a function and lifts that function so that it operates on functors. Both views are correct and in Haskell, equivalent.\n\nThe type fmap (replicate 3) :: (Functor f) => f a -> f [a] means that the function will work on any functor. What exactly it will do depends on which functor we use it on. If we use fmap (replicate 3) on a list, the list's implementation for fmap will be chosen, which is just map. If we use it on a Maybe a, it'll apply replicate 3 to the value inside the Just, or if it's Nothing, then it stays Nothing.\n\nNext up, we're going to look at the functor laws. In order for something to be a functor, it should satisfy some laws. All functors are expected to exhibit certain kinds of functor-like properties and behaviors. They should reliably behave as things that can be mapped over. Calling fmap on a functor should just map a function over the functor, nothing more. This behavior is described in the functor laws. There are two of them that all instances of Functor should abide by. They aren't enforced by Haskell automatically, so you have to test them out yourself.\n\nThe first functor law states that if we map the id function over a functor, the functor that we get back should be the same as the original functor. If we write that a bit more formally, it means that fmap id = id. So essentially, this says that if we do fmap id over a functor, it should be the same as just calling id on the functor. Remember, id is the identity function, which just returns its parameter unmodified. It can also be written as \\x -> x. If we view the functor as something that can be mapped over, the fmap id = id law seems kind of trivial or obvious.\n\nLet's see if this law holds for a few values of functors.\n\nIf we look at the implementation of fmap for, say, Maybe, we can figure out why the first functor law holds.\n\nWe imagine that id plays the role of the f parameter in the implementation. We see that if wee fmap id over Just x, the result will be Just (id x), and because id just returns its parameter, we can deduce that Just (id x) equals Just x. So now we know that if we map id over a Maybe value with a Just value constructor, we get that same value back.\n\nSeeing that mapping id over a Nothing value returns the same value is trivial. So from these two equations in the implementation for fmap, we see that the law fmap id = id holds.\n\nThe second law says that composing two functions and then mapping the resulting function over a functor should be the same as first mapping one function over the functor and then mapping the other one. Formally written, that means that fmap (f . g) = fmap f . fmap g. Or to write it in another way, for any functor F, the following should hold: fmap (f . g) F = fmap f (fmap g F).\n\nIf we can show that some type obeys both functor laws, we can rely on it having the same fundamental behaviors as other functors when it comes to mapping. We can know that when we use fmap on it, there won't be anything other than mapping going on behind the scenes and that it will act like a thing that can be mapped over, i.e. a functor. You figure out how the second law holds for some type by looking at the implementation of fmap for that type and then using the method that we used to check if Maybe obeys the first law.\n\nIf you want, we can check out how the second functor law holds for Maybe. If we do fmap (f . g) over Nothing, we get Nothing, because doing a fmap with any function over Nothing returns Nothing. If we do fmap f (fmap g Nothing), we get Nothing, for the same reason. OK, seeing how the second law holds for Maybe if it's a Nothing value is pretty easy, almost trivial.\n\nHow about if it's a Just something value? Well, if we do fmap (f . g) (Just x), we see from the implementation that it's implemented as Just ((f . g) x), which is, of course, Just (f (g x)). If we do fmap f (fmap g (Just x)), we see from the implementation that fmap g (Just x) is Just (g x). Ergo, fmap f (fmap g (Just x)) equals fmap f (Just (g x)) and from the implementation we see that this equals Just (f (g x)).\n\nIf you're a bit confused by this proof, don't worry. Be sure that you understand how function composition works. Many times, you can intuitively see how these laws hold because the types act like containers or functions. You can also just try them on a bunch of different values of a type and be able to say with some certainty that a type does indeed obey the laws.\n\nLet's take a look at a pathological example of a type constructor being an instance of the Functor typeclass but not really being a functor, because it doesn't satisfy the laws. Let's say that we have a type:\n\nThe C here stands for counter. It's a data type that looks much like Maybe a, only the Just part holds two fields instead of one. The first field in the CJust value constructor will always have a type of Int, and it will be some sort of counter and the second field is of type a, which comes from the type parameter and its type will, of course, depend on the concrete type that we choose for CMaybe a. Let's play with our new type to get some intuition for it.\n\nIf we use the CNothing constructor, there are no fields, and if we use the CJust constructor, the first field is an integer and the second field can be any type. Let's make this an instance of Functor so that everytime we use fmap, the function gets applied to the second field, whereas the first field gets increased by 1.\n\nThis is kind of like the instance implementation for Maybe, except that when we do fmap over a value that doesn't represent an empty box (a CJust value), we don't just apply the function to the contents, we also increase the counter by 1. Everything seems cool so far, we can even play with this a bit:\n\nDoes this obey the functor laws? In order to see that something doesn't obey a law, it's enough to find just one counter-example.\n\nAh! We know that the first functor law states that if we map id over a functor, it should be the same as just calling id with the same functor, but as we've seen from this example, this is not true for our CMaybe functor. Even though it's part of the Functor typeclass, it doesn't obey the functor laws and is therefore not a functor. If someone used our CMaybe type as a functor, they would expect it to obey the functor laws like a good functor. But CMaybe fails at being a functor even though it pretends to be one, so using it as a functor might lead to some faulty code. When we use a functor, it shouldn't matter if we first compose a few functions and then map them over the functor or if we just map each function over a functor in succession. But with CMaybe, it matters, because it keeps track of how many times it's been mapped over. Not cool! If we wanted CMaybe to obey the functor laws, we'd have to make it so that the Int field stays the same when we use fmap.\n\nAt first, the functor laws might seem a bit confusing and unnecessary, but then we see that if we know that a type obeys both laws, we can make certain assumptions about how it will act. If a type obeys the functor laws, we know that calling fmap on a value of that type will only map the function over it, nothing more. This leads to code that is more abstract and extensible, because we can use laws to reason about behaviors that any functor should have and make functions that operate reliably on any functor.\n\nAll the Functor instances in the standard library obey these laws, but you can check for yourself if you don't believe me. And the next time you make a type an instance of Functor, take a minute to make sure that it obeys the functor laws. Once you've dealt with enough functors, you kind of intuitively see the properties and behaviors that they have in common and it's not hard to intuitively see if a type obeys the functor laws. But even without the intuition, you can always just go over the implementation line by line and see if the laws hold or try to find a counter-example.\n\nWe can also look at functors as things that output values in a context. For instance, Just 3 outputs the value 3 in the context that it might or not output any values at all. [1,2,3] outputs three values—1, 2, and 3, the context is that there may be multiple values or no values. The function (+3) will output a value, depending on which parameter it is given.\n\nIf you think of functors as things that output values, you can think of mapping over functors as attaching a transformation to the output of the functor that changes the value. When we do fmap (+3) [1,2,3], we attach the transformation (+3) to the output of [1,2,3], so whenever we look at a number that the list outputs, (+3) will be applied to it. Another example is mapping over functions. When we do fmap (+3) (*3), we attach the transformation (+3) to the eventual output of (*3). Looking at it this way gives us some intuition as to why using fmap on functions is just composition (fmap (+3) (*3) equals (+3) . (*3), which equals \\x -> ((x*3)+3)), because we take a function like (*3) then we attach the transformation (+3) to its output. The result is still a function, only when we give it a number, it will be multiplied by three and then it will go through the attached transformation where it will be added to three. This is what happens with composition.\n\nIn this section, we'll take a look at applicative functors, which are beefed up functors, represented in Haskell by the Applicative typeclass, found in the Control.Applicative module.\n\nAs you know, functions in Haskell are curried by default, which means that a function that seems to take several parameters actually takes just one parameter and returns a function that takes the next parameter and so on. If a function is of type a -> b -> c, we usually say that it takes two parameters and returns a c, but actually it takes an a and returns a function b -> c. That's why we can call a function as f x y or as (f x) y. This mechanism is what enables us to partially apply functions by just calling them with too few parameters, which results in functions that we can then pass on to other functions.\n\nSo far, when we were mapping functions over functors, we usually mapped functions that take only one parameter. But what happens when we map a function like *, which takes two parameters, over a functor? Let's take a look at a couple of concrete examples of this. If we have Just 3 and we do fmap (*) (Just 3), what do we get? From the instance implementation of Maybe for Functor, we know that if it's a Just something value, it will apply the function to the something inside the Just. Therefore, doing fmap (*) (Just 3) results in Just ((*) 3), which can also be written as Just (* 3) if we use sections. Interesting! We get a function wrapped in a Just!\n\nIf we map compare, which has a type of (Ord a) => a -> a -> Ordering over a list of characters, we get a list of functions of type Char -> Ordering, because the function compare gets partially applied with the characters in the list. It's not a list of (Ord a) => a -> Ordering function, because the first a that got applied was a Char and so the second a has to decide to be of type Char.\n\nWe see how by mapping \"multi-parameter\" functions over functors, we get functors that contain functions inside them. So now what can we do with them? Well for one, we can map functions that take these functions as parameters over them, because whatever is inside a functor will be given to the function that we're mapping over it as a parameter.\n\nBut what if we have a functor value of Just (3 *) and a functor value of Just 5 and we want to take out the function from Just (3 *) and map it over Just 5? With normal functors, we're out of luck, because all they support is just mapping normal functions over existing functors. Even when we mapped \\f -> f 9 over a functor that contained functions inside it, we were just mapping a normal function over it. But we can't map a function that's inside a functor over another functor with what fmap offers us. We could pattern-match against the Just constructor to get the function out of it and then map it over Just 5, but we're looking for a more general and abstract way of doing that, which works across functors.\n\nMeet the Applicative typeclass. It lies in the Control.Applicative module and it defines two methods, pure and <*>. It doesn't provide a default implementation for any of them, so we have to define them both if we want something to be an applicative functor. The class is defined like so:\n\nThis simple three line class definition tells us a lot! Let's start at the first line. It starts the definition of the Applicative class and it also introduces a class constraint. It says that if we want to make a type constructor part of the Applicative typeclass, it has to be in Functor first. That's why if we know that if a type constructor is part of the Applicative typeclass, it's also in Functor, so we can use fmap on it.\n\nThe first method it defines is called pure. Its type declaration is pure :: a -> f a. f plays the role of our applicative functor instance here. Because Haskell has a very good type system and because everything a function can do is take some parameters and return some value, we can tell a lot from a type declaration and this is no exception. pure should take a value of any type and return an applicative functor with that value inside it. When we say inside it, we're using the box analogy again, even though we've seen that it doesn't always stand up to scrutiny. But the a -> f a type declaration is still pretty descriptive. We take a value and we wrap it in an applicative functor that has that value as the result inside it.\n\nA better way of thinking about pure would be to say that it takes a value and puts it in some sort of default (or pure) context—a minimal context that still yields that value.\n\nThe <*> function is really interesting. It has a type declaration of f (a -> b) -> f a -> f b. Does this remind you of anything? Of course, fmap :: (a -> b) -> f a -> f b. It's a sort of a beefed up fmap. Whereas fmap takes a function and a functor and applies the function inside the functor, <*> takes a functor that has a function in it and another functor and sort of extracts that function from the first functor and then maps it over the second one. When I say extract, I actually sort of mean run and then extract, maybe even sequence. We'll see why soon.\n\nLet's take a look at the Applicative instance implementation for Maybe.\n\nAgain, from the class definition we see that the f that plays the role of the applicative functor should take one concrete type as a parameter, so we write instance Applicative Maybe where instead of writing instance Applicative (Maybe a) where.\n\nFirst off, pure. We said earlier that it's supposed to take something and wrap it in an applicative functor. We wrote pure = Just, because value constructors like Just are normal functions. We could have also written pure x = Just x.\n\nNext up, we have the definition for <*>. We can't extract a function out of a Nothing, because it has no function inside it. So we say that if we try to extract a function from a Nothing, the result is a Nothing. If you look at the class definition for Applicative, you'll see that there's a Functor class constraint, which means that we can assume that both of <*>'s parameters are functors. If the first parameter is not a Nothing, but a Just with some function inside it, we say that we then want to map that function over the second parameter. This also takes care of the case where the second parameter is Nothing, because doing fmap with any function over a Nothing will return a Nothing.\n\nSo for Maybe, <*> extracts the function from the left value if it's a Just and maps it over the right value. If any of the parameters is Nothing, Nothing is the result.\n\nWe see how doing pure (+3) and Just (+3) is the same in this case. Use pure if you're dealing with Maybe values in an applicative context (i.e. using them with <*>), otherwise stick to Just. The first four input lines demonstrate how the function is extracted and then mapped, but in this case, they could have been achieved by just mapping unwrapped functions over functors. The last line is interesting, because we try to extract a function from a Nothing and then map it over something, which of course results in a Nothing.\n\nWith normal functors, you can just map a function over a functor and then you can't get the result out in any general way, even if the result is a partially applied function. Applicative functors, on the other hand, allow you to operate on several functors with a single function. Check out this piece of code:\n\nWhat's going on here? Let's take a look, step by step. <*> is left-associative, which means that pure (+) <*> Just 3 <*> Just 5 is the same as (pure (+) <*> Just 3) <*> Just 5. First, the + function is put in a functor, which is in this case a Maybe value that contains the function. So at first, we have pure (+), which is Just (+). Next, Just (+) <*> Just 3 happens. The result of this is Just (3+). This is because of partial application. Only applying 3 to the + function results in a function that takes one parameter and adds 3 to it. Finally, Just (3+) <*> Just 5 is carried out, which results in a Just 8.\n\nIsn't this awesome?! Applicative functors and the applicative style of doing pure f <*> x <*> y <*> ... allow us to take a function that expects parameters that aren't necessarily wrapped in functors and use that function to operate on several values that are in functor contexts. The function can take as many parameters as we want, because it's always partially applied step by step between occurences of <*>.\n\nThis becomes even more handy and apparent if we consider the fact that pure f <*> x equals fmap f x. This is one of the applicative laws. We'll take a closer look at them later, but for now, we can sort of intuitively see that this is so. Think about it, it makes sense. Like we said before, pure puts a value in a default context. If we just put a function in a default context and then extract and apply it to a value inside another applicative functor, we did the same as just mapping that function over that applicative functor. Instead of writing pure f <*> x <*> y <*> ..., we can write fmap f x <*> y <*> .... This is why Control.Applicative exports a function called <$>, which is just fmap as an infix operator. Here's how it's defined:\n\nYo! Quick reminder: type variables are independent of parameter names or other value names. The in the function declaration here is a type variable with a class constraint saying that any type constructor that replaces should be in the typeclass. The in the function body denotes a function that we map over . The fact that we used to represent both of those doesn't mean that they somehow represent the same thing.\n\nBy using <$>, the applicative style really shines, because now if we want to apply a function f between three applicative functors, we can write f <$> x <*> y <*> z. If the parameters weren't applicative functors but normal values, we'd write f x y z.\n\nLet's take a closer look at how this works. We have a value of Just \"johntra\" and a value of Just \"volta\" and we want to join them into one String inside a Maybe functor. We do this:\n\nBefore we see how this happens, compare the above line with this:\n\nAwesome! To use a normal function on applicative functors, just sprinkle some <$> and <*> about and the function will operate on applicatives and return an applicative. How cool is that?\n\nAnyway, when we do (++) <$> Just \"johntra\" <*> Just \"volta\", first (++), which has a type of (++) :: [a] -> [a] -> [a] gets mapped over Just \"johntra\", resulting in a value that's the same as Just (\"johntra\"++) and has a type of Maybe ([Char] -> [Char]). Notice how the first parameter of (++) got eaten up and how the as turned into Chars. And now Just (\"johntra\"++) <*> Just \"volta\" happens, which takes the function out of the Just and maps it over Just \"volta\", resulting in Just \"johntravolta\". Had any of the two values been Nothing, the result would have also been Nothing.\n\nSo far, we've only used Maybe in our examples and you might be thinking that applicative functors are all about Maybe. There are loads of other instances of Applicative, so let's go and meet them!\n\nLists (actually the list type constructor, []) are applicative functors. What a suprise! Here's how [] is an instance of Applicative:\n\nEarlier, we said that pure takes a value and puts it in a default context. Or in other words, a minimal context that still yields that value. The minimal context for lists would be the empty list, [], but the empty list represents the lack of a value, so it can't hold in itself the value that we used pure on. That's why pure takes a value and puts it in a singleton list. Similarly, the minimal context for the Maybe applicative functor would be a Nothing, but it represents the lack of a value instead of a value, so pure is implemented as Just in the instance implementation for Maybe.\n\nWhat about <*>? If we look at what <*>'s type would be if it were limited only to lists, we get (<*>) :: [a -> b] -> [a] -> [b]. It's implemented with a list comprehension. <*> has to somehow extract the function out of its left parameter and then map it over the right parameter. But the thing here is that the left list can have zero functions, one function, or several functions inside it. The right list can also hold several values. That's why we use a list comprehension to draw from both lists. We apply every possible function from the left list to every possible value from the right list. The resulting list has every possible combination of applying a function from the left list to a value in the right one.\n\nThe left list has three functions and the right list has three values, so the resulting list will have nine elements. Every function in the left list is applied to every function in the right one. If we have a list of functions that take two parameters, we can apply those functions between two lists.\n\nBecause <*> is left-associative, [(+),(*)] <*> [1,2] happens first, resulting in a list that's the same as [(1+),(2+),(1*),(2*)], because every function on the left gets applied to every value on the right. Then, [(1+),(2+),(1*),(2*)] <*> [3,4] happens, which produces the final result.\n\nUsing the applicative style with lists is fun! Watch:\n\nAgain, see how we used a normal function that takes two strings between two applicative functors of strings just by inserting the appropriate applicative operators.\n\nYou can view lists as non-deterministic computations. A value like 100 or \"what\" can be viewed as a deterministic computation that has only one result, whereas a list like [1,2,3] can be viewed as a computation that can't decide on which result it wants to have, so it presents us with all of the possible results. So when you do something like (+) <$> [1,2,3] <*> [4,5,6], you can think of it as adding together two non-deterministic computations with +, only to produce another non-deterministic computation that's even less sure about its result.\n\nUsing the applicative style on lists is often a good replacement for list comprehensions. In the second chapter, we wanted to see all the possible products of [2,5,10] and [8,10,11], so we did this:\n\nWe're just drawing from two lists and applying a function between every combination of elements. This can be done in the applicative style as well:\n\nThis seems clearer to me, because it's easier to see that we're just calling * between two non-deterministic computations. If we wanted all possible products of those two lists that are more than 50, we'd just do:\n\nIt's easy to see how pure f <*> xs equals fmap f xs with lists. pure f is just [f] and [f] <*> xs will apply every function in the left list to every value in the right one, but there's just one function in the left list, so it's like mapping.\n\nAnother instance of Applicative that we've already encountered is IO. This is how the instance is implemented:\n\nSince pure is all about putting a value in a minimal context that still holds it as its result, it makes sense that pure is just return, because return does exactly that; it makes an I/O action that doesn't do anything, it just yields some value as its result, but it doesn't really do any I/O operations like printing to the terminal or reading from a file.\n\nIf <*> were specialized for IO it would have a type of (<*>) :: IO (a -> b) -> IO a -> IO b. It would take an I/O action that yields a function as its result and another I/O action and create a new I/O action from those two that, when performed, first performs the first one to get the function and then performs the second one to get the value and then it would yield that function applied to the value as its result. We used do syntax to implement it here. Remember, do syntax is about taking several I/O actions and gluing them into one, which is exactly what we do here.\n\nWith Maybe and [], we could think of <*> as simply extracting a function from its left parameter and then sort of applying it over the right one. With IO, extracting is still in the game, but now we also have a notion of sequencing, because we're taking two I/O actions and we're sequencing, or gluing, them into one. We have to extract the function from the first I/O action, but to extract a result from an I/O action, it has to be performed.\n\nThis is an I/O action that will prompt the user for two lines and yield as its result those two lines concatenated. We achieved it by gluing together two getLine I/O actions and a return, because we wanted our new glued I/O action to hold the result of a ++ b. Another way of writing this would be to use the applicative style.\n\nWhat we were doing before was making an I/O action that applied a function between the results of two other I/O actions, and this is the same thing. Remember, getLine is an I/O action with the type getLine :: IO String. When we use <*> between two applicative functors, the result is an applicative functor, so this all makes sense.\n\nIf we regress to the box analogy, we can imagine getLine as a box that will go out into the real world and fetch us a string. Doing (++) <$> getLine <*> getLine makes a new, bigger box that sends those two boxes out to fetch lines from the terminal and then presents the concatenation of those two lines as its result.\n\nThe type of the expression (++) <$> getLine <*> getLine is IO String, which means that this expression is a completely normal I/O action like any other, which also holds a result value inside it, just like other I/O actions. That's why we can do stuff like:\n\nIf you ever find yourself binding some I/O actions to names and then calling some function on them and presenting that as the result by using return, consider using the applicative style because it's arguably a bit more concise and terse.\n\nAnother instance of Applicative is (->) r, so functions. They are rarely used with the applicative style outside of code golf, but they're still interesting as applicatives, so let's take a look at how the function instance is implemented.\n\nIf you're confused about what means, check out the previous section where we explain how is a functor.\n\nWhen we wrap a value into an applicative functor with pure, the result it yields always has to be that value. A minimal default context that still yields that value as a result. That's why in the function instance implementation, pure takes a value and creates a function that ignores its parameter and always returns that value. If we look at the type for pure, but specialized for the (->) r instance, it's pure :: a -> (r -> a).\n\nBecause of currying, function application is left-associative, so we can omit the parentheses.\n\nThe instance implementation for <*> is a bit cryptic, so it's best if we just take a look at how to use functions as applicative functors in the applicative style.\n\nCalling <*> with two applicative functors results in an applicative functor, so if we use it on two functions, we get back a function. So what goes on here? When we do (+) <$> (+3) <*> (*100), we're making a function that will use + on the results of (+3) and (*100) and return that. To demonstrate on a real example, when we did (+) <$> (+3) <*> (*100) $ 5, the 5 first got applied to (+3) and (*100), resulting in 8 and 500. Then, + gets called with 8 and 500, resulting in 508.\n\nSame here. We create a function that will call the function \\x y z -> [x,y,z] with the eventual results from (+3), (*2) and (/2). The 5 gets fed to each of the three functions and then \\x y z -> [x, y, z] gets called with those results.\n\nYou can think of functions as boxes that contain their eventual results, so doing k <$> f <*> g creates a function that will call k with the eventual results from f and g. When we do something like (+) <$> Just 3 <*> Just 5, we're using + on values that might or might not be there, which also results in a value that might or might not be there. When we do (+) <$> (+10) <*> (+5), we're using + on the future return values of (+10) and (+5) and the result is also something that will produce a value only when called with a parameter.\n\nWe don't often use functions as applicatives, but this is still really interesting. It's not very important that you get how the (->) r instance for Applicative works, so don't despair if you're not getting this right now. Try playing with the applicative style and functions to build up an intuition for functions as applicatives.\n\nAn instance of Applicative that we haven't encountered yet is ZipList, and it lives in Control.Applicative.\n\nIt turns out there are actually more ways for lists to be applicative functors. One way is the one we already covered, which says that calling <*> with a list of functions and a list of values results in a list which has all the possible combinations of applying functions from the left list to the values in the right list. If we do [(+3),(*2)] <*> [1,2], (+3) will be applied to both 1 and 2 and (*2) will also be applied to both 1 and 2, resulting in a list that has four elements, namely [4,5,2,4].\n\nHowever, [(+3),(*2)] <*> [1,2] could also work in such a way that the first function in the left list gets applied to the first value in the right one, the second function gets applied to the second value, and so on. That would result in a list with two values, namely [4,4]. You could look at it as [1 + 3, 2 * 2].\n\nBecause one type can't have two instances for the same typeclass, the ZipList a type was introduced, which has one constructor ZipList that has just one field, and that field is a list. Here's the instance:\n\n<*> does just what we said. It applies the first function to the first value, the second function to the second value, etc. This is done with zipWith (\\f x -> f x) fs xs. Because of how zipWith works, the resulting list will be as long as the shorter of the two lists.\n\npure is also interesting here. It takes a value and puts it in a list that just has that value repeating indefinitely. pure \"haha\" results in ZipList ([\"haha\",\"haha\",\"haha\".... This might be a bit confusing since we said that pure should put a value in a minimal context that still yields that value. And you might be thinking that an infinite list of something is hardly minimal. But it makes sense with zip lists, because it has to produce the value on every position. This also satisfies the law that pure f <*> xs should equal fmap f xs. If pure 3 just returned ZipList [3], pure (*2) <*> ZipList [1,5,10] would result in ZipList [2], because the resulting list of two zipped lists has the length of the shorter of the two. If we zip a finite list with an infinite list, the length of the resulting list will always be equal to the length of the finite list.\n\nSo how do zip lists work in an applicative style? Let's see. Oh, the ZipList a type doesn't have a Show instance, so we have to use the getZipList function to extract a raw list out of a zip list.\n\nThe function is the same as . Also, the function is the same as .\n\nAside from zipWith, the standard library has functions such as zipWith3, zipWith4, all the way up to 7. zipWith takes a function that takes two parameters and zips two lists with it. zipWith3 takes a function that takes three parameters and zips three lists with it, and so on. By using zip lists with an applicative style, we don't have to have a separate zip function for each number of lists that we want to zip together. We just use the applicative style to zip together an arbitrary amount of lists with a function, and that's pretty cool.\n\nControl.Applicative defines a function that's called liftA2, which has a type of liftA2 :: (Applicative f) => (a -> b -> c) -> f a -> f b -> f c . It's defined like this:\n\nNothing special, it just applies a function between two applicatives, hiding the applicative style that we've become familiar with. The reason we're looking at it is because it clearly showcases why applicative functors are more powerful than just ordinary functors. With ordinary functors, we can just map functions over one functor. But with applicative functors, we can apply a function between several functors. It's also interesting to look at this function's type as (a -> b -> c) -> (f a -> f b -> f c). When we look at it like this, we can say that liftA2 takes a normal binary function and promotes it to a function that operates on two functors.\n\nHere's an interesting concept: we can take two applicative functors and combine them into one applicative functor that has inside it the results of those two applicative functors in a list. For instance, we have Just 3 and Just 4. Let's assume that the second one has a singleton list inside it, because that's really easy to achieve:\n\nOK, so let's say we have Just 3 and Just [4]. How do we get Just [3,4]? Easy.\n\nRemember, : is a function that takes an element and a list and returns a new list with that element at the beginning. Now that we have Just [3,4], could we combine that with Just 2 to produce Just [2,3,4]? Of course we could. It seems that we can combine any amount of applicatives into one applicative that has a list of the results of those applicatives inside it. Let's try implementing a function that takes a list of applicatives and returns an applicative that has a list as its result value. We'll call it sequenceA.\n\nAh, recursion! First, we look at the type. It will transform a list of applicatives into an applicative with a list. From that, we can lay some groundwork for an edge condition. If we want to turn an empty list into an applicative with a list of results, well, we just put an empty list in a default context. Now comes the recursion. If we have a list with a head and a tail (remember, x is an applicative and xs is a list of them), we call sequenceA on the tail, which results in an applicative with a list. Then, we just prepend the value inside the applicative x into that applicative with a list, and that's it!\n\nSo if we do sequenceA [Just 1, Just 2], that's (:) <$> Just 1 <*> sequenceA [Just 2] . That equals (:) <$> Just 1 <*> ((:) <$> Just 2 <*> sequenceA []). Ah! We know that sequenceA [] ends up as being Just [], so this expression is now (:) <$> Just 1 <*> ((:) <$> Just 2 <*> Just []), which is (:) <$> Just 1 <*> Just [2], which is Just [1,2]!\n\nAnother way to implement sequenceA is with a fold. Remember, pretty much any function where we go over a list element by element and accumulate a result along the way can be implemented with a fold.\n\nWe approach the list from the right and start off with an accumulator value of pure []. We do liftA2 (:) between the accumulator and the last element of the list, which results in an applicative that has a singleton in it. Then we do liftA2 (:) with the now last element and the current accumulator and so on, until we're left with just the accumulator, which holds a list of the results of all the applicatives.\n\nLet's give our function a whirl on some applicatives.\n\nAh! Pretty cool. When used on Maybe values, sequenceA creates a Maybe value with all the results inside it as a list. If one of the values was Nothing, then the result is also a Nothing. This is cool when you have a list of Maybe values and you're interested in the values only if none of them is a Nothing.\n\nWhen used with functions, sequenceA takes a list of functions and returns a function that returns a list. In our example, we made a function that took a number as a parameter and applied it to each function in the list and then returned a list of results. sequenceA [(+3),(+2),(+1)] 3 will call (+3) with 3, (+2) with 3 and (+1) with 3 and present all those results as a list.\n\nDoing (+) <$> (+3) <*> (*2) will create a function that takes a parameter, feeds it to both (+3) and (*2) and then calls + with those two results. In the same vein, it makes sense that sequenceA [(+3),(*2)] makes a function that takes a parameter and feeds it to all of the functions in the list. Instead of calling + with the results of the functions, a combination of : and pure [] is used to gather those results in a list, which is the result of that function.\n\nUsing sequenceA is cool when we have a list of functions and we want to feed the same input to all of them and then view the list of results. For instance, we have a number and we're wondering whether it satisfies all of the predicates in a list. One way to do that would be like so:\n\nRemember, and takes a list of booleans and returns True if they're all True. Another way to achieve the same thing would be with sequenceA:\n\nsequenceA [(>4),(<10),odd] creates a function that will take a number and feed it to all of the predicates in [(>4),(<10),odd] and return a list of booleans. It turns a list with the type (Num a) => [a -> Bool] into a function with the type (Num a) => a -> [Bool]. Pretty neat, huh?\n\nBecause lists are homogenous, all the functions in the list have to be functions of the same type, of course. You can't have a list like [ord, (+3)], because ord takes a character and returns a number, whereas (+3) takes a number and returns a number.\n\nWhen used with [], sequenceA takes a list of lists and returns a list of lists. Hmm, interesting. It actually creates lists that have all possible combinations of their elements. For illustration, here's the above done with sequenceA and then done with a list comprehension:\n\nThis might be a bit hard to grasp, but if you play with it for a while, you'll see how it works. Let's say that we're doing sequenceA [[1,2],[3,4]]. To see how this happens, let's use the sequenceA (x:xs) = (:) <$> x <*> sequenceA xs definition of sequenceA and the edge condition sequenceA [] = pure []. You don't have to follow this evaluation, but it might help you if have trouble imagining how sequenceA works on lists of lists, because it can be a bit mind-bending.\n• We start off with\n• Evaluating the inner further, we get\n• We've reached the edge condition, so this is now\n• Now, we evaluate the part, which will use with every possible value in the left list (possible values are and ) with every possible value on the right list (only possible value is ), which results in , which is . So now we have\n• Now, is used with every possible value from the left list ( and ) with every possible value in the right list ( and ), which results in , which is\n\nDoing (+) <$> [1,2] <*> [4,5,6]results in a non-deterministic computation x + y where x takes on every value from [1,2] and y takes on every value from [4,5,6]. We represent that as a list which holds all of the possible results. Similarly, when we do sequence [[1,2],[3,4],[5,6],[7,8]], the result is a non-deterministic computation [x,y,z,w], where x takes on every value from [1,2], y takes on every value from [3,4] and so on. To represent the result of that non-deterministic computation, we use a list, where each element in the list is one possible list. That's why the result is a list of lists.\n\nWhen used with I/O actions, sequenceA is the same thing as sequence! It takes a list of I/O actions and returns an I/O action that will perform each of those actions and have as its result a list of the results of those I/O actions. That's because to turn an [IO a] value into an IO [a] value, to make an I/O action that yields a list of results when performed, all those I/O actions have to be sequenced so that they're then performed one after the other when evaluation is forced. You can't get the result of an I/O action without performing it.\n\nLike normal functors, applicative functors come with a few laws. The most important one is the one that we already mentioned, namely that pure f <*> x = fmap f x holds. As an exercise, you can prove this law for some of the applicative functors that we've met in this chapter.The other functor laws are:\n\nWe won't go over them in detail right now because that would take up a lot of pages and it would probably be kind of boring, but if you're up to the task, you can take a closer look at them and see if they hold for some of the instances.\n\nIn conclusion, applicative functors aren't just interesting, they're also useful, because they allow us to combine different computations, such as I/O computations, non-deterministic computations, computations that might have failed, etc. by using the applicative style. Just by using <$> and <*> we can use normal functions to uniformly operate on any number of applicative functors and take advantage of the semantics of each one.\n\nSo far, we've learned how to make our own algebraic data types by using the data keyword. We've also learned how to give existing types synonyms with the type keyword. In this section, we'll be taking a look at how to make new types out of existing data types by using the newtype keyword and why we'd want to do that in the first place.\n\nIn the previous section, we saw that there are actually more ways for the list type to be an applicative functor. One way is to have <*> take every function out of the list that is its left parameter and apply it to every value in the list that is on the right, resulting in every possible combination of applying a function from the left list to a value in the right list.\n\nThe second way is to take the first function on the left side of <*> and apply it to the first value on the right, then take the second function from the list on the left side and apply it to the second value on the right, and so on. Ultimately, it's kind of like zipping the two lists together. But lists are already an instance of Applicative, so how did we also make lists an instance of Applicative in this second way? If you remember, we said that the ZipList a type was introduced for this reason, which has one value constructor, ZipList, that has just one field. We put the list that we're wrapping in that field. Then, ZipList was made an instance of Applicative, so that when we want to use lists as applicatives in the zipping manner, we just wrap them with the ZipList constructor and then once we're done, unwrap them with getZipList:\n\nSo, what does this have to do with this newtype keyword? Well, think about how we might write the data declaration for our ZipList a type. One way would be to do it like so:\n\nA type that has just one value constructor and that value constructor has just one field that is a list of things. We might also want to use record syntax so that we automatically get a function that extracts a list from a ZipList:\n\nThis looks fine and would actually work pretty well. We had two ways of making an existing type an instance of a type class, so we used the data keyword to just wrap that type into another type and made the other type an instance in the second way.\n\nThe newtype keyword in Haskell is made exactly for these cases when we want to just take one type and wrap it in something to present it as another type. In the actual libraries, ZipList a is defined like this:\n\nInstead of the data keyword, the newtype keyword is used. Now why is that? Well for one, newtype is faster. If you use the data keyword to wrap a type, there's some overhead to all that wrapping and unwrapping when your program is running. But if you use newtype, Haskell knows that you're just using it to wrap an existing type into a new type (hence the name), because you want it to be the same internally but have a different type. With that in mind, Haskell can get rid of the wrapping and unwrapping once it resolves which value is of what type.\n\nSo why not just use newtype all the time instead of data then? Well, when you make a new type from an existing type by using the newtype keyword, you can only have one value constructor and that value constructor can only have one field. But with data, you can make data types that have several value constructors and each constructor can have zero or more fields:\n\nWhen using newtype, you're restricted to just one constructor with one field.\n\nWe can also use the deriving keyword with newtype just like we would with data. We can derive instances for Eq, Ord, Enum, Bounded, Show and Read. If we derive the instance for a type class, the type that we're wrapping has to be in that type class to begin with. It makes sense, because newtype just wraps an existing type. So now if we do the following, we can print and equate values of our new type:\n\nIn this particular newtype, the value constructor has the following type:\n\nIt takes a [Char] value, such as \"my sharona\" and returns a CharList value. From the above examples where we used the CharList value constructor, we see that really is the case. Conversely, the getCharList function, which was generated for us because we used record syntax in our newtype, has this type:\n\nIt takes a CharList value and converts it to a [Char] value. You can think of this as wrapping and unwrapping, but you can also think of it as converting values from one type to the other.\n\nMany times, we want to make our types instances of certain type classes, but the type parameters just don't match up for what we want to do. It's easy to make Maybe an instance of Functor, because the Functor type class is defined like this:\n\nSo we just start out with:\n\nAnd then implement fmap. All the type parameters add up because the Maybe takes the place of f in the definition of the Functor type class and so if we look at fmap like it only worked on Maybe, it ends up behaving like:\n\nIsn't that just peachy? Now what if we wanted to make the tuple an instance of Functor in such a way that when we fmap a function over a tuple, it gets applied to the first component of the tuple? That way, doing fmap (+3) (1,1) would result in (4,1). It turns out that writing the instance for that is kind of hard. With Maybe, we just say instance Functor Maybe where because only type constructors that take exactly one parameter can be made an instance of Functor. But it seems like there's no way to do something like that with (a,b) so that the type parameter a ends up being the one that changes when we use fmap. To get around this, we can newtype our tuple in such a way that the second type parameter represents the type of the first component in the tuple:\n\nAnd now, we can make it an instance of Functor so that the function is mapped over the first component:\n\nAs you can see, we can pattern match on types defined with newtype. We pattern match to get the underlying tuple, then we apply the function f to the first component in the tuple and then we use the Pair value constructor to convert the tuple back to our Pair b a. If we imagine what the type fmap would be if it only worked on our new pairs, it would be:\n\nAgain, we said instance Functor (Pair c) where and so Pair c took the place of the f in the type class definition for Functor:\n\nSo now, if we convert a tuple into a Pair b a, we can use fmap over it and the function will be mapped over the first component:\n\nWe mentioned that newtype is usually faster than data. The only thing that can be done with newtype is turning an existing type into a new type, so internally, Haskell can represent the values of types defined with newtype just like the original ones, only it has to keep in mind that the their types are now distinct. This fact means that not only is newtype faster, it's also lazier. Let's take a look at what this means.\n\nLike we've said before, Haskell is lazy by default, which means that only when we try to actually print the results of our functions will any computation take place. Furthemore, only those computations that are necessary for our function to tell us the result will get carried out. The undefined value in Haskell represents an erronous computation. If we try to evaluate it (that is, force Haskell to actually compute it) by printing it to the terminal, Haskell will throw a hissy fit (technically referred to as an exception):\n\nHowever, if we make a list that has some undefined values in it but request only the head of the list, which is not undefined, everything will go smoothly because Haskell doesn't really need to evaluate any other elements in a list if we only want to see what the first element is:\n\nNow consider the following type:\n\nIt's your run-of-the-mill algebraic data type that was defined with the data keyword. It has one value constructor, which has one field whose type is Bool. Let's make a function that pattern matches on a CoolBool and returns the value \"hello\" regardless of whether the Bool inside the CoolBool was True or False:\n\nInstead of applying this function to a normal CoolBool, let's throw it a curveball and apply it to undefined!\n\nYikes! An exception! Now why did this exception happen? Types defined with the data keyword can have multiple value constructors (even though CoolBool only has one). So in order to see if the value given to our function conforms to the (CoolBool _) pattern, Haskell has to evaluate the value just enough to see which value constructor was used when we made the value. And when we try to evaluate an undefined value, even a little, an exception is thrown.\n\nInstead of using the data keyword for CoolBool, let's try using newtype:\n\nWe don't have to change our helloMe function, because the pattern matching syntax is the same if you use newtype or data to define your type. Let's do the same thing here and apply helloMe to an undefined value:\n\nIt worked! Hmmm, why is that? Well, like we've said, when we use newtype, Haskell can internally represent the values of the new type in the same way as the original values. It doesn't have to add another box around them, it just has to be aware of the values being of different types. And because Haskell knows that types made with the newtype keyword can only have one constructor, it doesn't have to evaluate the value passed to the function to make sure that it conforms to the (CoolBool _) pattern because newtype types can only have one possible value constructor and one field!\n\nThis difference in behavior may seem trivial, but it's actually pretty important because it helps us realize that even though types defined with data and newtype behave similarly from the programmer's point of view because they both have value constructors and fields, they are actually two different mechanisms. Whereas data can be used to make your own types from scratch, newtype is for making a completely new type out of an existing type. Pattern matching on newtype values isn't like taking something out of a box (like it is with data), it's more about making a direct conversion from one type to another.\n\nAt this point, you may be a bit confused about what exactly the difference between type, data and newtype is, so let's refresh our memory a bit.\n\nThe type keyword is for making type synonyms. What that means is that we just give another name to an already existing type so that the type is easier to refer to. Say we did the following:\n\nAll this does is to allow us to refer to the [Int] type as IntList. They can be used interchangeably. We don't get an IntList value constructor or anything like that. Because [Int] and IntList are only two ways to refer to the same type, it doesn't matter which name we use in our type annotations:\n\nWe use type synonyms when we want to make our type signatures more descriptive by giving types names that tell us something about their purpose in the context of the functions where they're being used. For instance, when we used an association list of type [(String,String)] to represent a phone book, we gave it the type synonym of PhoneBook so that the type signatures of our functions were easier to read.\n\nThe newtype keyword is for taking existing types and wrapping them in new types, mostly so that it's easier to make them instances of certain type classes. When we use newtype to wrap an existing type, the type that we get is separate from the original type. If we make the following newtype:\n\nWe can't use ++ to put together a CharList and a list of type [Char]. We can't even use ++ to put together two CharLists, because ++ works only on lists and the CharList type isn't a list, even though it could be said that it contains one. We can, however, convert two CharLists to lists, ++ them and then convert that back to a CharList.\n\nWhen we use record syntax in our newtype declarations, we get functions for converting between the new type and the original type: namely the value constructor of our newtype and the function for extracting the value in its field. The new type also isn't automatically made an instance of the type classes that the original type belongs to, so we have to derive or manually write them.\n\nIn practice, you can think of newtype declarations as data declarations that can only have one constructor and one field. If you catch yourself writing such a data declaration, consider using newtype.\n\nThe data keyword is for making your own data types and with them, you can go hog wild. They can have as many constructors and fields as you wish and can be used to implement any algebraic data type by yourself. Everything from lists and Maybe-like types to trees.\n\nIf you just want your type signatures to look cleaner and be more descriptive, you probably want type synonyms. If you want to take an existing type and wrap it in a new type in order to make it an instance of a type class, chances are you're looking for a newtype. And if you want to make something completely new, odds are good that you're looking for the data keyword.\n\nType classes in Haskell are used to present an interface for types that have some behavior in common. We started out with simple type classes like Eq, which is for types whose values can be equated, and Ord, which is for things that can be put in an order and then moved on to more interesting ones, like Functor and Applicative.\n\nWhen we make a type, we think about which behaviors it supports, i.e. what it can act like and then based on that we decide which type classes to make it an instance of. If it makes sense for values of our type to be equated, we make it an instance of the Eq type class. If we see that our type is some kind of functor, we make it an instance of Functor, and so on.\n\nNow consider the following: * is a function that takes two numbers and multiplies them. If we multiply some number with a 1, the result is always equal to that number. It doesn't matter if we do 1 * x or x * 1, the result is always x. Similarly, ++ is also a function which takes two things and returns a third. Only instead of multiplying numbers, it takes two lists and concatenates them. And much like *, it also has a certain value which doesn't change the other one when used with ++. That value is the empty list: [].\n\nIt seems that both * together with 1 and ++ along with [] share some common properties:\n• The parameters and the returned value have the same type.\n• There exists such a value that doesn't change other values when used with the binary function.\n\nThere's another thing that these two operations have in common that may not be as obvious as our previous observations: when we have three or more values and we want to use the binary function to reduce them to a single result, the order in which we apply the binary function to the values doesn't matter. It doesn't matter if we do (3 * 4) * 5 or 3 * (4 * 5). Either way, the result is 60. The same goes for ++:\n\nWe call this property associativity. * is associative, and so is ++, but -, for example, is not. The expressions (5 - 3) - 4 and 5 - (3 - 4) result in different numbers.\n\nBy noticing and writing down these properties, we have chanced upon monoids! A monoid is when you have an associative binary function and a value which acts as an identity with respect to that function. When something acts as an identity with respect to a function, it means that when called with that function and some other value, the result is always equal to that other value. 1 is the identity with respect to * and [] is the identity with respect to ++. There are a lot of other monoids to be found in the world of Haskell, which is why the Monoid type class exists. It's for types which can act like monoids. Let's see how the type class is defined:\n\nThe Monoid type class is defined in import Data.Monoid. Let's take some time and get properly acquainted with it.\n\nFirst of all, we see that only concrete types can be made instances of Monoid, because the m in the type class definition doesn't take any type parameters. This is different from Functor and Applicative, which require their instances to be type constructors which take one parameter.\n\nThe first function is mempty. It's not really a function, since it doesn't take parameters, so it's a polymorphic constant, kind of like minBound from Bounded. mempty represents the identity value for a particular monoid.\n\nNext up, we have mappend, which, as you've probably guessed, is the binary function. It takes two values of the same type and returns a value of that type as well. It's worth noting that the decision to name mappend as it's named was kind of unfortunate, because it implies that we're appending two things in some way. While ++ does take two lists and append one to the other, * doesn't really do any appending, it just multiplies two numbers together. When we meet other instances of Monoid, we'll see that most of them don't append values either, so avoid thinking in terms of appending and just think in terms of mappend being a binary function that takes two monoid values and returns a third.\n\nThe last function in this type class definition is mconcat. It takes a list of monoid values and reduces them to a single value by doing mappend between the list's elements. It has a default implementation, which just takes mempty as a starting value and folds the list from the right with mappend. Because the default implementation is fine for most instances, we won't concern ourselves with mconcat too much from now on. When making a type an instance of Monoid, it suffices to just implement mempty and mappend. The reason mconcat is there at all is because for some instances, there might be a more efficient way to implement mconcat, but for most instances the default implementation is just fine.\n\nBefore moving on to specific instances of Monoid, let's take a brief look at the monoid laws. We mentioned that there has to be a value that acts as the identity with respect to the binary function and that the binary function has to be associative. It's possible to make instances of Monoid that don't follow these rules, but such instances are of no use to anyone because when using the Monoid type class, we rely on its instances acting like monoids. Otherwise, what's the point? That's why when making instances, we have to make sure they follow these laws:\n\nThe first two state that mempty has to act as the identity with respect to mappend and the third says that mappend has to be associative i.e. that it the order in which we use mappend to reduce several monoid values into one doesn't matter. Haskell doesn't enforce these laws, so we as the programmer have to be careful that our instances do indeed obey them.\n\nYes, lists are monoids! Like we've seen, the ++ function and the empty list [] form a monoid. The instance is very simple:\n\nLists are an instance of the Monoid type class regardless of the type of the elements they hold. Notice that we wrote instance Monoid [a] and not instance Monoid [], because Monoid requires a concrete type for an instance.\n\nGiving this a test run, we encounter no surprises:\n\nNotice that in the last line, we had to write an explicit type annotation, because if we just did mempty, GHCi wouldn't know which instance to use, so we had to say we want the list instance. We were able to use the general type of [a] (as opposed to specifying [Int] or [String]) because the empty list can act as if it contains any type.\n\nBecause mconcat has a default implementation, we get it for free when we make something an instance of Monoid. In the case of the list, mconcat turns out to be just concat. It takes a list of lists and flattens it, because that's the equivalent of doing ++ between all the adjecent lists in a list.\n\nThe monoid laws do indeed hold for the list instance. When we have several lists and we mappend (or ++) them together, it doesn't matter which ones we do first, because they're just joined at the ends anyway. Also, the empty list acts as the identity so all is well. Notice that monoids don't require that a `mappend` b be equal to b `mappend` a. In the case of the list, they clearly aren't:\n\nAnd that's okay. The fact that for multiplication 3 * 5 and 5 * 3 are the same is just a property of multiplication, but it doesn't hold for all (and indeed, most) monoids.\n\nWe already examined one way for numbers to be considered monoids. Just have the binary function be * and the identity value 1. It turns out that that's not the only way for numbers to be monoids. Another way is to have the binary function be + and the identity value 0:\n\nThe monoid laws hold, because if you add 0 to any number, the result is that number. And addition is also associative, so we get no problems there. So now that there are two equally valid ways for numbers to be monoids, which way do choose? Well, we don't have to. Remember, when there are several ways for some type to be an instance of the same type class, we can wrap that type in a newtype and then make the new type an instance of the type class in a different way. We can have our cake and eat it too.\n\nThe Data.Monoid module exports two types for this, namely Product and Sum. Product is defined like this:\n\nSimple, just a newtype wrapper with one type parameter along with some derived instances. Its instance for Monoid goes a little something like this:\n\nmempty is just 1 wrapped in a Product constructor. mappend pattern matches on the Product constructor, multiplies the two numbers and then wraps the resulting number back. As you can see, there's a Num a class constraint. So this means that Product a is an instance of Monoid for all a's that are already an instance of Num. To use Producta a as a monoid, we have to do some newtype wrapping and unwrapping:\n\nThis is nice as a showcase of the Monoid type class, but no one in their right mind would use this way of multiplying numbers instead of just writing 3 * 9 and 3 * 1. But a bit later, we'll see how these Monoid instances that may seem trivial at this time can come in handy.\n\nSum is defined like Product and the instance is similar as well. We use it in the same way:\n\nAnother type which can act like a monoid in two distinct but equally valid ways is Bool. The first way is to have the or function || act as the binary function along with False as the identity value. The way or works in logic is that if any of its two parameters is True, it returns True, otherwise it returns False. So if we use False as the identity value, it will return False when or-ed with False and True when or-ed with True. The Any newtype constructor is an instance of Monoid in this fashion. It's defined like this:\n\nIts instance looks goes like so:\n\nThe reason it's called Any is because x `mappend` y will be True if any one of those two is True. Even if three or more Any wrapped Bools are mappended together, the result will hold True if any of them are True:\n\nThe other way for Bool to be an instance of Monoid is to kind of do the opposite: have && be the binary function and then make True the identity value. Logical and will return True only if both of its parameters are True. This is the newtype declaration, nothing fancy:\n\nAnd this is the instance:\n\nWhen we mappend values of the All type, the result will be True only if all the values used in the mappend operations are True:\n\nJust like with multiplication and addition, we usually explicitly state the binary functions instead of wrapping them in newtypes and then using mappend and mempty. mconcat seems useful for Any and All, but usually it's easier to use the or and and functions, which take lists of Bools and return True if any of them are True or if all of them are True, respectively.\n\nHey, remember the Ordering type? It's used as the result when comparing things and it can have three values: LT, EQ and GT, which stand for less than, equal and greater than respectively:\n\nWith lists, numbers and boolean values, finding monoids was just a matter of looking at already existing commonly used functions and seeing if they exhibit some sort of monoid behavior. With Ordering, we have to look a bit harder to recognize a monoid, but it turns out that its Monoid instance is just as intuitive as the ones we've met so far and also quite useful:\n\nThe instance is set up like this: when we mappend two Ordering values, the one on the left is kept, unless the value on the left is EQ, in which case the right one is the result. The identity is EQ. At first, this may seem kind of arbitrary, but it actually resembles the way we alphabetically compare words. We compare the first two letters and if they differ, we can already decide which word would go first in a dictionary. However, if the first two letters are equal, then we move on to comparing the next pair of letters and repeat the process.\n\nFor instance, if we were to alphabetically compare the words \"ox\" and \"on\", we'd first compare the first two letters of each word, see that they are equal and then move on to comparing the second letter of each word. We see that 'x' is alphabetically greater than 'n', and so we know how the words compare. To gain some intuition for EQ being the identity, we can notice that if we were to cram the same letter in the same position in both words, it wouldn't change their alphabetical ordering. \"oix\" is still alphabetically greater than and \"oin\".\n\nIt's important to note that in the Monoid instance for Ordering, x `mappend` y doesn't equal y `mappend` x. Because the first parameter is kept unless it's EQ, LT `mappend` GT will result in LT, whereas GT `mappend` LT will result in GT:\n\nOK, so how is this monoid useful? Let's say you were writing a function that takes two strings, compares their lengths, and returns an Ordering. But if the strings are of the same length, then instead of returning EQ right away, we want to compare them alphabetically. One way to write this would be like so:\n\nWe name the result of comparing the lengths a and the result of the alphabetical comparison b and then if it turns out that the lengths were equal, we return their alphabetical ordering.\n\nBut by employing our understanding of how Ordering is a monoid, we can rewrite this function in a much simpler manner:\n\nWe can try this out:\n\nRemember, when we use mappend, its left parameter is always kept unless it's EQ, in which case the right one is kept. That's why we put the comparison that we consider to be the first, more important criterion as the first parameter. If we wanted to expand this function to also compare for the number of vowels and set this to be the second most important criterion for comparison, we'd just modify it like this:\n\nWe made a helper function, which takes a string and tells us how many vowels it has by first filtering it only for letters that are in the string \"aeiou\" and then applying length to that.\n\nVery cool. Here, we see how in the first example the lengths are found to be different and so LT is returned, because the length of \"zen\" is less than the length of \"anna\". In the second example, the lengths are the same, but the second string has more vowels, so LT is returned again. In the third example, they both have the same length and the same number of vowels, so they're compared alphabetically and \"zen\" wins.\n\nThe Ordering monoid is very cool because it allows us to easily compare things by many different criteria and put those criteria in an order themselves, ranging from the most important to the least.\n\nLet's take a look at the various ways that Maybe a can be made an instance of Monoid and what those instances are useful for.\n\nOne way is to treat Maybe a as a monoid only if its type parameter a is a monoid as well and then implement mappend in such a way that it uses the mappend operation of the values that are wrapped with Just. We use Nothing as the identity, and so if one of the two values that we're mappending is Nothing, we keep the other value. Here's the instance declaration:\n\nNotice the class constraint. It says that Maybe a is an instance of Monoid only if a is an instance of Monoid. If we mappend something with a Nothing, the result is that something. If we mappend two Just values, the contents of the Justs get mappended and then wrapped back in a Just. We can do this because the class constraint ensures that the type of what's inside the Just is an instance of Monoid.\n\nThis comes in use when you're dealing with monoids as results of computations that may have failed. Because of this instance, we don't have to check if the computations have failed by seeing if they're a Nothing or Just value; we can just continue to treat them as normal monoids.\n\nBut what if the type of the contents of the Maybe aren't an instance of Monoid? Notice that in the previous instance declaration, the only case where we have to rely on the contents being monoids is when both parameters of mappend are Just values. But if we don't know if the contents are monoids, we can't use mappend between them, so what are we to do? Well, one thing we can do is to just discard the second value and keep the first one. For this, the First a type exists and this is its definition:\n\nWe take a Maybe a and we wrap it with a newtype. The Monoid instance is as follows:\n\nJust like we said. mempty is just a Nothing wrapped with the First newtype constructor. If mappend's first parameter is a Just value, we ignore the second one. If the first one is a Nothing, then we present the second parameter as a result, regardless of whether it's a Just or a Nothing:\n\nFirst is useful when we have a bunch of Maybe values and we just want to know if any of them is a Just. The mconcat function comes in handy:\n\nIf we want a monoid on Maybe a such that the second parameter is kept if both parameters of mappend are Just values, Data.Monoid provides a the Last a type, which works like First a, only the last non-Nothing value is kept when mappending and using mconcat:\n\nOne of the more interesting ways to put monoids to work is to make them help us define folds over various data structures. So far, we've only done folds over lists, but lists aren't the only data structure that can be folded over. We can define folds over almost any data structure. Trees especially lend themselves well to folding.\n\nBecause there are so many data structures that work nicely with folds, the Foldable type class was introduced. Much like Functor is for things that can be mapped over, Foldable is for things that can be folded up! It can be found in Data.Foldable and because it export functions whose names clash with the ones from the Prelude, it's best imported qualified (and served with basil):\n\nTo save ourselves precious keystrokes, we've chosen to import it qualified as F. Alright, so what are some of the functions that this type class defines? Well, among them are foldr, foldl, foldr1 and foldl1. Huh? But we already know these functions, what's so new about this? Let's compare the types of Foldable's foldr and the foldr from the Prelude to see how they differ:\n\nAh! So whereas foldr takes a list and folds it up, the foldr from Data.Foldable accepts any type that can be folded up, not just lists! As expected, both foldr functions do the same for lists:\n\nOkay then, what are some other data structures that support folds? Well, there's the Maybe we all know and love!\n\nBut folding over a Maybe value isn't terribly interesting, because when it comes to folding, it just acts like a list with one element if it's a Just value and as an empty list if it's Nothing. So let's examine a data structure that's a little more complex then.\n\nRemember the tree data structure from the Making Our Own Types and Typeclasses chapter? We defined it like this:\n\nWe said that a tree is either an empty tree that doesn't hold any values or it's a node that holds one value and also two other trees. After defining it, we made it an instance of Functor and with that we gained the ability to fmap functions over it. Now, we're going to make it an instance of Foldable so that we get the abilty to fold it up. One way to make a type constructor an instance of Foldable is to just directly implement foldr for it. But another, often much easier way, is to implement the foldMap function, which is also a part of the Foldable type class. The foldMap function has the following type:\n\nIts first parameter is a function that takes a value of the type that our foldable structure contains (denoted here with a) and returns a monoid value. Its second parameter is a foldable structure that contains values of type a. It maps that function over the foldable structure, thus producing a foldable structure that contains monoid values. Then, by doing mappend between those monoid values, it joins them all into a single monoid value. This function may sound kind of odd at the moment, but we'll see that it's very easy to implement. What's also cool is that implementing this function is all it takes for our type to be made an instance of Foldable. So if we just implement foldMap for some type, we get foldr and foldl on that type for free!\n\nThis is how we make Tree an instance of Foldable:\n\nWe think like this: if we are provided with a function that takes an element of our tree and returns a monoid value, how do we reduce our whole tree down to one single monoid value? When we were doing fmap over our tree, we applied the function that we were mapping to a node and then we recursively mapped the function over the left sub-tree as well as the right one. Here, we're tasked with not only mapping a function, but with also joining up the results into a single monoid value by using mappend. First we consider the case of the empty tree — a sad and lonely tree that has no values or sub-trees. It doesn't hold any value that we can give to our monoid-making function, so we just say that if our tree is empty, the monoid value it becomes is mempty.\n\nThe case of a non-empty node is a bit more interesting. It contains two sub-trees as well as a value. In this case, we recursively foldMap the same function f over the left and the right sub-trees. Remember, our foldMap results in a single monoid value. We also apply our function f to the value in the node. Now we have three monoid values (two from our sub-trees and one from applying f to the value in the node) and we just have to bang them together into a single value. For this purpose we use mappend, and naturally the left sub-tree comes first, then the node value and then the right sub-tree.\n\nNotice that we didn't have to provide the function that takes a value and returns a monoid value. We receive that function as a parameter to foldMap and all we have to decide is where to apply that function and how to join up the resulting monoids from it.\n\nNow that we have a Foldable instance for our tree type, we get foldr and foldl for free! Consider this tree:\n\nIt has 5 at its root and then its left node is has 3 with 1 on the left and 6 on the right. The root's right node has a 9 and then an 8 to its left and a 10 on the far right side. With a Foldable instance, we can do all of the folds that we can do on lists:\n\nAnd also, foldMap isn't only useful for making new instances of Foldable; it comes in handy for reducing our structure to a single monoid value. For instance, if we want to know if any number in our tree is equal to 3, we can do this:\n\nHere, \\x -> Any $ x == 3 is a function that takes a number and returns a monoid value, namely a Bool wrapped in Any. foldMap applies this function to every element in our tree and then reduces the resulting monoids into a single monoid with mappend. If we do this:\n\nAll of the nodes in our tree would hold the value Any False after having the function in the lambda applied to them. But to end up True, mappend for Any has to have at least one True value as a parameter. That's why the final result is False, which makes sense because no value in our tree is greater than 15.\n\nWe can also easily turn our tree into a list by doing a foldMap with the \\x -> [x] function. By first projecting that function onto our tree, each element becomes a singleton list. The mappend action that takes place between all those singleton list results in a single list that holds all of the elements that are in our tree:\n\nWhat's cool is that all of these trick aren't limited to trees, they work on any instance of Foldable."
    },
    {
        "link": "http://cmsc-16100.cs.uchicago.edu/2016/Lectures/14-applicative.php",
        "document": "Functors are very nice. They give us a uniform mechanism for \"lifting\" functions defined at a simple type to act on complicated types that are parameterized by that type, For example, if is a Functor, and , then .\n\nBut what happens if isn't unary? Consider a hypothetical . Intuitively, we might hope , but it doesn't work that way. We've already defined so that , i.e., the result of the first application is to produce a function in an box, rather than a function that takes boxed values, and returns a boxed value. We could work with this if had a mechanism for applying boxed functions to boxed arguments, returning boxed results.\n\nThe typeclass, defined in , addresses this issue.\n\nA type is if it is a that, additionally, comes with two members: an infix operator that takes a boxed function of type , applies it to a boxed argument of type , and produces a boxed result of type ; and a function of type that \"lifts\" simple values into boxed ones. Once we have this, we can \"factor\" :\n\nOr, for those who like to play with eta-reduction:\n\nIn addition to the laws involving , every functor must satisfy the following laws:\n\nWe're not going to dwell on these laws now, but in time, they'll seem obvious.\n\nLet's get a little bit of practice with before considering more complicated examples. We can easily write an instance definition:\n\nNote here that the instance definition in the is quite different, and we'll come to that in due course.\n\nIntuitively, we can understand arithmetic expressions in as expressions where operands or operations might be missing. Thus, e.g.,\n\nPretty clearly, any expression in which any operand or operation is missing is going to evaluate to . Note here that I'm just using to clue the type checker in as to what I have in mind. Alternatively:\n\nAt this point, we can consider a few bits of simplifying syntax. The most common use case of applicatives involves applying an ordinary \"unboxed\" function to several boxed arguments. We can inject the function into the using , and then use for \"application under the box.\" This situation is quite common, so the function is defined to apply an unboxed function to a boxed argument. Thus,\n\nA moment's reflection shows that is just an infix-version of . Expressions like the one above are said to be written in \"applicative style\" because they look like normal, curried function applications except for the presence of and sprinkled in between.\n\nIn addition to the special syntax for applying unboxed functions to boxed arguments, there are also functions , , and , that generalize to one or more arguments, i.e.,\n\nwhich is pretty much where the discussion of began. Thus,\n\nIt is sometimes useful to think of a function as a non-deterministic function of type , i.e., a function that can have zero or more return values. In this case, it may help to think of the list as a \"computational context\" rather than a \"box\" of values. From such a perspective the effect of \"applying\" a list of functions to a list of arguments ought to be the list of all ways we can pair functions with arguments, and a pure object is just a singleton list, i.e.,\n\nNote that the resulting list for the last expression above is , in precisely this order.\n\nThis particular instance of becomes very natural with use, in part because this generalizes to the usual instance, of which we will see more later.\n\nOur definition of can be written more concisely using Haskell's list comprehension syntax:\n\nList comprehensions allow writing \"generators\" that resemble set comprehensions found in mathematical notation. As we'll soon see, list comprehensions are directly tied with yet another nifty type class. In the meantime, if you'd like to learn the basics of list comprehensions, check out these notes.\n\nIt turns out that there's a second, natural way to implement a list as an . The idea is to represent parallel computation, i.e., that the i-th element of the result list comes from applying the i-th operation to the i-th operand. As we are well aware by now, however, a type can be an instance of a typeclass in only one way, and therefore we need a bit of a type-theoretic work-around. We've already seen how to do this several times:\n\nThus, a is just a inside a (virtual) box.\n\nOf course, we do this to provide a distinctive instance, but we have to provide a instance as well. We'll use what is essentially the standard definition of for lists, acting within the box:\n\nNote here that the on the right hand side of the definition is 's , i.e., our old friend . We now define the following:\n\nThis requires a bit of explanation, because it's probably not what you'd expect. Certainly, I'd expect something like a binary function that performs application (e.g., ). But...\n\nOr, to put this same pun differently,\n\nSo we didn't need to \"roll up\" a special purpose binary application function. We already had one, in the identity function. Weird.\n\nThe alert reader/listener will have noticed that I haven't yet provided a definition of for . This takes a bit of thought... Let's think about what we want:\n\nHmm. So has to be a function that contains in every coordinate of a list of indeterminate length. It's a good thing that Haskell allows this:\n\nremembering that builds an infinite list. Of course, this has implications for as well:\n\nThe typeclass is useful but describes only ground types. What about type operators that also exhibit monoidal structure? For example, recall that we defined a wrapper type for called that constituted a with an operator that returns the first (leftmost) non- value.\n\nWe might like to describe this monoidal structure directly for even though it is not a ground type. The class describes functors that also exhibit monoidal structure.\n\nNotice the correspondence between and in to and in , respectively. Unsurprisingly, an type ought to satisfy all of the same laws as s... in addition to the ones for ... in addition to the ones for !\n\nThe instance for is quite like the instance of :\n\nAnd now we can write the previous examples directly in terms of the monoid ( ):\n\nThere are many other applicative functors with monoidal structure. For example, types are quite handy when writing parsers, as we shall see later in the course."
    },
    {
        "link": "https://jakewheat.github.io/intro_to_parsing",
        "document": "This is an introduction to parsing with Haskell and Parsec. Prerequisites: you should know some basic Haskell and have GHC and cabal-install installed (installing the Haskell Platform will give you this). This tutorial was originally written using GHC 7.6.3 and Parsec 3.1.3, which are the versions which come with the Haskell Platform 2013.2.0.0. It should also work fine with GHC 7.8.4 and GHC 7.10.2 and through to at least the latest release of Parsec 3.1.x. This tutorial was written using Literate Haskell files available here: https://github.com/JakeWheat/intro_to_parsing. I recommend you download them all, and follow along in your favourite editor, and use GHCi to experiment. To download the intro_to_parsing files, use: This parser is in the module . There is a wrapper in this tutorial’s project, , which gives this function a simplified type. Whenever we write a parser which parses to a value of type , we give it the return type of . In this case, we parse a character so the return type is . The type itself is in the module . We will cover this in more detail later. Let’s use this parser. I will assume you have GHC and cabal-install installed (which provides the 'cabal' executable) and both are in your PATH. The Haskell Platform is one way that provides this. Change to the directory where you downloaded the intro_to_parsing source files (which will contain the GettingStarted.lhs file). Then you can set up a cabal sandbox and be ready to work with the code by running the following commands in that directory: Now you will get the ghci prompt. Type in ':l GettingStarted.lhs'. You can run the parser using a wrapper, enter the following at the ghci prompt: . Here is a transcript of running ghci via 'cabal repl': $ cabal repl Warning: The repl command is a part of the legacy v1 style of cabal usage. Please switch to using either the new project style and the new-repl command or the legacy v1-repl alias as new-style projects will become the default in the next version of cabal-install. Please file a bug if you cannot replicate a working v1- use case with the new-style commands. For more information, see: https://wiki.haskell.org/Cabal/NewBuild GHCi, version 8.6.5: http://www.haskell.org/ghc/ :? for help Prelude> :l GettingStarted.lhs [1 of 5] Compiling Text.Parsec.String.Char ( Text/Parsec/String/Char.hs, interpreted ) [2 of 5] Compiling Text.Parsec.String.Combinator ( Text/Parsec/String/Combinator.hs, interpreted ) [3 of 5] Compiling Text.Parsec.String.Parsec ( Text/Parsec/String/Parsec.hs, interpreted ) [4 of 5] Compiling FunctionsAndTypesForParsing ( FunctionsAndTypesForParsing.lhs, interpreted ) [5 of 5] Compiling Main ( GettingStarted.lhs, interpreted ) Ok, five modules loaded. *Main> regularParse anyChar \"a\" Right 'a' *Main> You can exit ghci by entering ':quit' or using Ctrl-d. From now on, to start ghci again, you can just change to the directory with GettingStarted.lhs and run 'cabal repl'. ghci should have readline support so you can browse through your command history using up and down arrow, etc. This is the type of . It is wrapper which takes a parser function such as anyChar, and wraps it so you can parse a string to either a parse error, or the return value from your parser function: Here are some examples of running this parser on various input: *Main> regularParse anyChar \"a\" Right 'a' *Main> regularParse anyChar \"b\" Right 'b' *Main> regularParse anyChar \"0\" Right '0' *Main> regularParse anyChar \" \" Right ' ' *Main> regularParse anyChar \"\n\n\" Right '\n\n' *Main> regularParse anyChar \"aa\" Right 'a' *Main> regularParse anyChar \"\" Left (line 1, column 1): unexpected end of input *Main> regularParse anyChar \" a\" Right ' ' You can see that if there are no characters, we get an error. Otherwise, it takes the first character and returns it, and throws away any trailing characters. The details of the helper function will come later. Here are two alternatives to you can also use for experimenting for the time being: These can be useful when you are not sure if your parser is consuming all your input string or not. The eof parser will error if you haven’t consumed all the input, and the leftover parser can instead tell you what was not consumed from the input. *Main> regularParse anyChar \"a\" Right 'a' *Main> parseWithEof anyChar \"a\" Right 'a' *Main> parseWithLeftOver anyChar \"a\" Right ('a',\"\") *Main> *Main> regularParse anyChar \"\" Left (line 1, column 1): unexpected end of input *Main> parseWithEof anyChar \"\" Left (line 1, column 1): unexpected end of input *Main> parseWithLeftOver anyChar \"\" Left (line 1, column 1): unexpected end of input *Main> regularParse anyChar \"aa\" Right 'a' *Main> parseWithEof anyChar \"aa\" Left (line 1, column 2): unexpected 'a' expecting end of input *Main> parseWithLeftOver anyChar \"aa\" Right ('a',\"a\") *Main> parseWithLeftOver anyChar \"abc\" Right ('a',\"bc\") You can use these functions and ghci to experiment. Try running all the parsers in ghci on various input strings as you work through the document to get a good feel for all the different features. Tip: you can also write the parsers inline in the function call, for example: This can be used to quickly try out new ad hoc parsing code. The real Parsec functions have quite complex type signatures. This makes a lot of things very tricky before you understand them, and can make some of the error messages you’ll see really difficult to understand. I’ve created some wrapper modules, which set the types of all the functions from Parsec we use to be much more restricted. This will make the types easy to understand, and reduce the amount of tricky to understand compiler errors you get. You can use this approach when writing your own parser code with Parsec. These wrapper modules are created with the following name pattern: → . Later on, we will look at the general types in more detail. Let’s go through some of the functions in module from the Parsec package. The haddock is available here: http://hackage.haskell.org/package/parsec-3.1.3/docs/Text-Parsec-Char.html. Here is the function, with its full type signature. This is one of the main primitive functions in Parsec. This looks at the next character from the current input, and if the function ( ) returns true for this character, it 'pops' it from the input and returns it. In this way, the current position in the input string is tracked behind the scenes. In the simplified type wrappers, the function’s type is this: This makes it a bit clearer what it is doing. All the functions in are reproduced in the local module with simplified types (<https://github.com/JakeWheat/intro_to_parsing/blob/master/Text/Parsec/String/Char.hs>). Here are some examples of satisfy in action. *Main> parseWithEof (satisfy (=='a')) \"a\" Right 'a' *Main> parseWithEof (satisfy (=='b')) \"a\" Left (line 1, column 1): unexpected \"a\" *Main> parseWithEof (satisfy (`elem` \"abc\")) \"a\" Right 'a' *Main> parseWithEof (satisfy (`elem` \"abc\")) \"d\" Left (line 1, column 1): unexpected \"d\" *Main> parseWithEof (satisfy isDigit) \"d\" Left (line 1, column 1): unexpected \"d\" *Main> parseWithEof (satisfy isDigit) \"1\" Right '1' You can see that it is easy to use , or or one of the functions from the Data.Char module. If you look at the docs on hackage http://hackage.haskell.org/package/parsec-3.1.3/docs/Text-Parsec-Char.html, you can view the source. The implementations of most of the functions in are straightforward. I recommend you look at the source for all of these functions. You can see in the source that the function is a little more primitive than the other functions. Here is the parser we used above in the parser: If you look at the source via the haddock link above, you can see it uses . Here are some other simple wrappers of from which use different validation functions. The parser parses a specific character which you supply: They all return a . You might be able to guess what each of them returns, you can double check your intuition using ghci. These parser all parse one character from a hardcoded set of characters: In these cases, the return value is less redundant. and parse any of the characters in the given list These are just simple wrappers of satisfy using . You should try all these parsers out in ghci, e.g.: Here are the final functions in : matches a complete string, one character at a time. I think the implementation of this function is like it is for efficiency when parsing from, e.g., , instead of , but I’m not sure. We will skip the detailed explanation of the implementation for now. *Main> regularParse (string \"one\") \"one\" Right \"one\" *Main> regularParse (string \"one\") \"two\" Left (line 1, column 1): unexpected \"t\" expecting \"one\" Here is the parser, which, if you look at the source, you can see uses a combinator ( ). We will cover this combinator shortly. *Main> regularParse spaces \"\" Right () *Main> regularParse spaces \" \" Right () *Main> regularParse spaces \" \" Right () *Main> regularParse spaces \" a \" Right () *Main> regularParse spaces \"a a \" Right () Here are two exes which you can use to parse either a string or a file to help you experiment. This will save you having to figure out how to write this boilerplate until later. Now you can easily experiment using ghci, or with a string on the command line, or by putting the text to parse into a file and parsing that.\n\nIn this tutorial we will develop a parser for a very simple expression language, and start learning about the set of combinators which comes with Parsec. The first element we will have in this expression language is positive integral numbers: TODO: make examples with parsing failures for all of the example scripts below? To parse a number, we need to parse one or more digits, and then read the resulting string. We can use the combinator to help with this. We will also use do notation. How does it work? First, we parse one or more ( ) digits ( ), and give the result the name 'n'. Then we convert the string to an integer using . The function’s type looks like this: It applies the parser given one or more times, returning the result. Let’s see what happens when we use the combinator which parses zero or more items instead of one or more. *Main> regularParse num1 \"1\" Right 1 *Main> regularParse num1 \"123456\" Right 123456 *Main> regularParse num1 \"aa\" Right *** Exception: Prelude.read: no parse For var, we have to decide on a syntax for the identifiers. Let’s go for a common choice: identifiers must start with a letter or underscore, and then they can be followed by zero or more letters, underscores or digits in any combination. This time, we create two helper parsers: , which parses a letter or underscore, and which parses a digit, letter or underscore. This time, we use the function instead of . Try it out in ghci. I like to try things which you expect to work, and also to try things which you expect to not work and make sure you get an error. The parens parser will eventually parse any expression inside parentheses. First it will just parse integers inside parentheses. There is a new function: . This might be familiar to you already. This is used to ignore the result of the parser, since we are not interested in this value. You can also write the function without , but ghc will give you a warning if you have warnings turned on. One way of turning warnings on in ghci is to enter at the ghci prompt. *Main> :set -Wall *Main> :l \"VerySimpleExpressions.lhs\" ... FirstRealParsing.lhs:140:7: Warning: A do-notation statement discarded a result of type Char. Suppress this warning by saying \"_ <- char '('\", or by using the flag -fno-warn-unused-do-bind FirstRealParsing.lhs:142:7: Warning: A do-notation statement discarded a result of type Char. Suppress this warning by saying \"_ <- char ')'\", or by using the flag -fno-warn-unused-do-bind ... As you can see, another way to suppress the warning is to use . One issue with this parser is that it doesn’t handle whitespace: We will look at this issue below. Now we will write a little parser to parse strings like 'a+b' where a and b are numbers. It has the same whitespace issues as the parens parser. Here is a parser which will skip zero or more whitespace characters. We can use this to make our parsers handle whitespace better. *Main> regularParse whitespace \" \" Right () *Main> regularParse whitespace \" \" Right () *Main> regularParse whitespace \"\\t\" Right () *Main> regularParse whitespace \" \n\n \" Right () *Main> regularParse whitespace \"\" Right () Notice that it always succeeds. Here is the parens parser rewritten with a common approach to whitespace handling: *Main> regularParse parensW \"(1)\" Right (Parentheses 1) *Main> regularParse parensW \" (1)\" Right (Parentheses 1) *Main> regularParse parensW \" (1 )\" Right (Parentheses 1) *Main> regularParse parensW \" ( 1 ) \" Right (Parentheses 1) In the original parsec documentation, one of the concepts mentioned is the idea of 'lexeme' parsing. This is a style in which every token parser should also consume and ignore any trailing whitespace. This is a simple convention which with a bit of care allows skipping whitespace exactly once wherever it needs to be skipped. To complete the lexeme style, we should also always skip leading whitespace at the top level only. This feels more elegant than spamming all the parsing code with many calls to . Here is the parens parser rewritten to use lexeme: *Main> parseWithWhitespace parensL \"(1)\" Right (Parentheses 1) *Main> parseWithWhitespace parensL \" (1)\" Right (Parentheses 1) *Main> parseWithWhitespace parensL \" ( 1)\" Right (Parentheses 1) *Main> parseWithWhitespace parensL \" ( 1 ) \" Right (Parentheses 1) The function can also use to make it a bit shorter, . Here is the shorter version of this function using : Try rewriting the SingleAdd parser to use , and test it out to convince yourself that it skips whitespace correctly. Now we are ready to write a parser which parses simple expressions made from these components. Here is the data type for these expressions. It’s so simple that it is almost useless at the moment. Here are all our component parsers with , and with the constructors: There doesn’t seem to be a unique obviously correct place to put the lexeme call in the var parser: Here is an alternative, with the call to lexeme in a different place, but gives effectively the same function. In the parens parser, we can reuse the parser like this: Here is the add parser using also. To combine these, we can use an operator called : It tries the first parser, and it if fails (without consuming any input), it tries the second parser. More about the 'consuming input' concept later. Here is another way to write the numOrVar parser: is just wrapper around . You can choose which one to use based on which is more readable in each particular case. Here is the first version of the simpleExpr parser: *Main> parseWithWhitespace simpleExpr \"12\" Right (Num 12) *Main> parseWithWhitespace simpleExpr \"aa\" Right (Var \"aa\") *Main> parseWithWhitespace simpleExpr \"1+2\" Left (line 1, column 2): unexpected '+' expecting digit or end of input *Main> parseWithWhitespace simpleExpr \"(1)\" Right (Parens (Num 1)) *Main> parseWithWhitespace simpleExpr \"(aa)\" Left (line 1, column 2): unexpected \"a\" expecting digit It works well for some of the parsers. One problem is that the and parsers don’t parse general expressions as the components, but just . Another problem is that the doesn’t work at all: the parser parses the first number, and the parser is never tried. This is an example of not trying the second parser if the first parser succeeds, even if a later alternative would consume more input or successfully parse the whole input. Let’s try and rearrange the order: *Main> parseWithWhitespace simpleExpr1 \"12\" Left (line 1, column 3): unexpected end of input expecting digit or \"+\" *Main> parseWithWhitespace simpleExpr1 \"aa\" Right (Var \"aa\") *Main> parseWithWhitespace simpleExpr1 \"1+2\" Right (Add (Num 1) (Num 2)) *Main> parseWithWhitespace simpleExpr1 \"(1)\" Right (Parens (Num 1)) We swapped one problem for another. Let’s fix this using the function. *Main> parseWithWhitespace simpleExpr2 \"12\" Right (Num 12) *Main> parseWithWhitespace simpleExpr2 \"aa\" Right (Var \"aa\") *Main> parseWithWhitespace simpleExpr2 \"1+2\" Right (Add (Num 1) (Num 2)) *Main> parseWithWhitespace simpleExpr2 \"(1)\" Right (Parens (Num 1)) Now everything seems to work fine. The function is very powerful and easy to use, and can be used where in a more traditional parsing approach you would have to use left factoring or something else. The function implements backtracking. When this is used with , it means that if the first parser fails, it will undo the consumed input and carry on with the next option, instead of failing completely. This works even if the is nested deeply within the first parser given to . has its downsides (some of which we will see later), and I usually try to minimise its use or eliminate it completely. I found I often got into a complete mess when I used too much when writing parsers for something a little tricky like SQL, and that although doing some left-factoring appeared at first to be tedious and appeared to make the code less readable, I eventually decided that for me it made the code more readable since what was happening was more transparent. Now we are going to fix this parser to parse arbitrarily nested expressions. In a way, the method used will roughly mean we are left factoring the and common prefix. Here is the naive implementation: If you run this parser, it will enter an infinite loop, since and will keep calling each other recursively without making any progress. *Main> parseWithWhitespace simpleExpr4 \"a\" Right (Var \"a\") *Main> parseWithWhitespace simpleExpr4 \"1\" Right (Num 1) *Main> parseWithWhitespace simpleExpr4 \"(1)\" Right (Parens (Num 1)) *Main> parseWithWhitespace simpleExpr4 \"((a))\" Right (Parens (Parens (Var \"a\"))) At least this part seems to work OK. Let’s try to stop the add parser from calling itself indirectly: Here is a parameterized parens parser where we supply the nested expression parser as an argument. This is used here to try to make the code easier to follow and avoid rewriting this parser out again and again in full. Here is a new parser, which parses expressions except add. *Main> parseWithWhitespace simpleExpr5 \"1\" Right (Num 1) *Main> parseWithWhitespace simpleExpr5 \"a\" Right (Var \"a\") *Main> parseWithWhitespace simpleExpr5 \"(a)\" Right (Parens (Var \"a\")) *Main> parseWithWhitespace simpleExpr5 \"1+2\" Right (Add (Num 1) (Num 2)) *Main> parseWithWhitespace simpleExpr5 \"1+a\" Right (Add (Num 1) (Var \"a\")) *Main> parseWithWhitespace simpleExpr5 \"(1+a)\" Right (Parens (Add (Num 1) (Var \"a\"))) *Main> parseWithWhitespace simpleExpr5 \"1+a+b\" Left (line 1, column 4): unexpected '+' expecting end of input Almost. Let’s see what happens when the second in is changed to the general expression parser. *Main> parseWithWhitespace simpleExpr6 \"a + b + c\" Right (Add (Var \"a\") (Add (Var \"b\") (Var \"c\"))) Maybe it looks like we’ve made it. But there is a problem. We’ve parsed the + operator as if it has right associativity: But it should be left associative: Let’s left factor the parsing and fix this: -- then see if it is followed by an '+ expr' suffix -- this function takes an expression, and parses a -- it recursively calls itself via the maybeAddSuffix function -- this is the wrapper for addSuffix, which adapts it so that if -- addSuffix fails, it returns just the original expression *Main> parseWithWhitespace simpleExpr7 \"a + b + c\" Right (Add (Add (Var \"a\") (Var \"b\")) (Var \"c\")) Now the parser seems to work for everything it should. There is a combinator function in Parsec we can use which abstracts this sort of pattern, . How does this work? Here is the type of : The type of the Add constructor in pseudocode is: The parser here now just parses the operator itself, i.e. '+' (and not the second expression like our simpleExpr7 parser). The return from the function is a function which accepts two elements and combines them using the appropriate operator representation. In this case, the represenation is the constructor. You can look at the source http://hackage.haskell.org/package/parsec-3.1.3/docs/src/Text-Parsec-Combinator.html#chainl1 and see if you can understand how it works. If you can’t work it out, you could come back to it later when you have more experience writing parsing code. TODO: write a little manual tester that accepts a parser and a list of examples, and checks they all parse correctly. Let’s see if we can check with quickcheck. It’s a bit tricky testing parsers in this way, but one way to do something useful is to generate random asts, convert them to concrete syntax, parse them, and check the result. We can write a simple 'pretty printer' to convert an ast to concrete syntax. TODO: a really simple pretty printer just pasting strings together, no layout. TODO: write a quickcheck property and arbitary instance and show running it at the ghci prompt\n\nNow we can go back over the expression parsing code written in the last tutorial, and make it much more concise, and also make it more readable. We are going to do this mainly by using functions from the typeclass Applicative. Remember you can (and should) use the functions and its variations (TODO list them here) to try out the all these parsers in ghci, and you can write your own variations to experiment with if you are unsure about anything. Here is the SimpleExpr type again: Here is the basic pattern behind most of the rewrites we are going to cover. Here is a function which takes a constructor and two parsers for the two arguments for the constructor. It parses the two arguments, then applies the constructor to the results: TODO: concrete example, plus examples at the bottom of this section (for ctor <$> a, ctor <$> a <*> b, ctor <$> a <*> b <*> c). This can be rewritten without the do syntactic sugar like this: And can also be rewritten like this: (This uses functions from Applicative instead of Monad.) We replace the use of with and . This isn’t always possible, but it often is. Here is the version using the operators for and ( changed to , and changed to ). This style takes less typing, and is often much simpler to write and read. This pattern 'scales', you can use: for a single argument constructor. This might also be familiar to you as All of which mean the same thing, just slightly different spellings. This can also be written using Monad operators: These versions effectively mean the same thing as the previous versions with and . for three args, and so on. So you use between the pure constructor and the first arg, then between each subsequent arg. Let’s go over the simple expression parsers and try to rewrite them using this style. We will see a few other new functions. I will break things down into a lot of steps. Here is the old lexeme parser, 'D' suffix for 'do notation'. First, we can move the from its own separate line. The expression means run , then run , and return the result of . It is sort of equivalent to this code: Now we can use the usual monad syntax rewrites, first eliminate the name . Now remove the redundant do: Let’s move 'read' to the first line. This uses which we saw above. You may have done code rewrites like this using with IO in other Haskell code. Now let’s move the ctor as well: You can also write it in this way: Why does this work? It it equivalent to the previous version partly because of the applicative laws. In terms of style, which do you think looks better: or . The next step for num, we can eliminate the temporary name and the : In more 'industrial' parser code, I would usually write some tokenization parsers separately like this: Then the num expression parser looks like this: and we also get a integer parser which we can reuse if we need to parse an integer in another context. Here is the previous var parser: The first thing we can do is to make the and a little easier to read, using , , and : Here is another way of making the function a little better: We can lift the using the Applicative operators. We used the prefix version of to use it with and . The lexemeA call was moved to the helper function. Now tidy it up using with the constructor: We could also split the into a separate top level function, with the same idea as with splitting the parser. Here is the starting point: Here is the rewrite in one step: Here you can see that there is a which works in the opposite direction to . The precendence of these operators means that we have to use some extra parentheses (!) here. TODO: lost the chained <*. Put something below about this so there is a concrete example. We can simplify the function using the techniques we’ve already seen: The pattern can use a different operator like this: . Here it is in the expression parser: You could also write the parser inline: Maybe this last step makes it less readable? Here is the finished job for all the simple expression code without separate token parsers: Here they are with separate token parsers and a helper function: Here is a lexeme wrapper for parsing single character symbols. Here is another little helper function. It barely pays its way in this short example, but even though it is only used once, I think it is worth it to make the code clearer. Splitting the lexer parser layer out means that we have one place where we have to remember to add wrappers, and also I think makes the code easier to follow.\n\nIn this tutorial we will go through all the functions in Text.Parsec.Combinator, and some useful ones in Control.Applicative and Control.Monad as well. You should look at the source for these functions and try to understand how they are implemented. The style of the source code in the Parsec library sources is a little different to what we used at the end of the last tutorial. You can try reimplementing each of the Text.Parsec.Combinator module functions using the Applicative style. See if you can find a way to reassure yourself that the rewritten versions you make are correct, perhaps via writing automated tests, or perhaps some other method. You should be able to easily understand the implementation of all the functions in Text.Parsec.Combinator except possibly and . tries to apply the parsers in the list in order, until one of them succeeds. It returns the value of the succeeding parser. *Main> regularParse aOrB \"a\" Right 'a' *Main> regularParse aOrB \"b\" Right 'b' *Main> regularParse aOrB \"c\" Left (line 1, column 1): unexpected \"c\" expecting \"a\" or \"b\" using with try If a parser fails with or , then it will only try the next parser if the last parser consumed no input. TODO: make the parsers return the keyword and update the examples Since both of these have the same prefix - b - if we combine them using then it doesn’t work correctly: *Main> regularParse byKeyword \"by\" Right () *Main> regularParse byKeyword \"between\" Left (line 1, column 1): unexpected \"e\" expecting \"by\" *Main> regularParse betweenKeyword \"between\" Right () *Main> regularParse betweenKeyword \"by\" Left (line 1, column 1): unexpected \"y\" expecting \"between\" *Main> regularParse (choice [betweenKeyword,byKeyword]) \"between\" Right () *Main> regularParse (choice [betweenKeyword,byKeyword]) \"by\" Left (line 1, column 1): unexpected \"y\" expecting \"between\" *Main> regularParse (choice [byKeyword,betweenKeyword]) \"between\" Left (line 1, column 1): unexpected \"e\" expecting \"by\" *Main> regularParse (choice [byKeyword,betweenKeyword]) \"by\" Right () If we use on the first option, then it all works fine. *Main> regularParse (choice [try byKeyword,betweenKeyword]) \"by\" Right () *Main> regularParse (choice [try byKeyword,betweenKeyword]) \"between\" Right () parses occurrences of . If is smaller or equal to zero, the parser is equivalent to . It returns a list of the n values returned by . *Main> regularParse (count 5 a) \"aaaaa\" Right \"aaaaa\" *Main> regularParse (count 5 a) \"aaaa\" Left (line 1, column 5): unexpected end of input expecting \"a\" *Main> regularParse (count 5 a) \"aaaab\" Left (line 1, column 5): unexpected \"b\" expecting \"a\" *Main> regularParse (count 5 aOrB) \"aabaa\" Right \"aabaa\" parses , followed by and . It returns the value returned by . We can replace the betweenParens from the previous tutorial using this: It hardly seems worth it to make this change, but it might be slightly quicker to read and understand if you aren’t already familiar with some code or haven’t viewed it for a while. This is good for 'code maintenance', where we need to fix bugs or add new features quickly to code we haven’t looked at for two years or something. Here are the support functions for this parser. tries to apply parser . If fails without consuming input, it returns the value , otherwise the value returned by . *Main> regularParse (option \"\" (count 5 aOrB)) \"aaaaa\" Right \"aaaaa\" *Main> regularParse (option \"\" (count 5 aOrB)) \"caaaa\" Right \"\" *Main> regularParse (option \"\" (count 5 aOrB)) \"aaaa\" Left (line 1, column 5): unexpected end of input expecting \"a\" or \"b\" *Main> regularParse (option \"\" (count 5 aOrB)) \"aaaac\" Left (line 1, column 5): unexpected \"c\" expecting \"a\" or \"b\" *Main> regularParse (option \"\" (try (count 5 aOrB))) \"aaaa\" Right \"\" tries to apply parser . If fails without consuming input, it returns , otherwise it returns the value returned by . *Main> regularParse (optionMaybe (count 5 aOrB)) \"aaaaa\" Right (Just \"aaaaa\") *Main> regularParse (optionMaybe (count 5 aOrB)) \"caaaa\" Right Nothing *Main> regularParse (optionMaybe (count 5 aOrB)) \"caaa\" Right Nothing *Main> regularParse (optionMaybe (count 5 aOrB)) \"aaaa\" Left (line 1, column 5): unexpected end of input expecting \"a\" or \"b\" *Main> regularParse (optionMaybe (count 5 aOrB)) \"aaaac\" Left (line 1, column 5): unexpected \"c\" expecting \"a\" or \"b\" *Main> regularParse (optionMaybe (try $ count 5 aOrB)) \"aaaac\" Right Nothing tries to apply parser . It will parse or nothing. It only fails if fails after consuming input. It discards the result of . *Main> parseWithLeftOver (optional (count 5 aOrB)) \"aaaaa\" Right ((),\"\") *Main> parseWithLeftOver (optional (count 5 aOrB)) \"caaaa\" Right ((),\"caaaa\") *Main> parseWithLeftOver (optional (count 5 aOrB)) \"caaa\" Right ((),\"caaa\") *Main> parseWithLeftOver (optional (count 5 aOrB)) \"aaaa\" Left (line 1, column 5): unexpected end of input expecting \"a\" or \"b\" *Main> parseWithLeftOver (optional (count 5 aOrB)) \"aaaac\" Left (line 1, column 5): unexpected \"c\" expecting \"a\" or \"b\" *Main> parseWithLeftOver (optional (try $ count 5 aOrB)) \"aaaac\" Right ((),\"aaaac\") applies the parser one or more times, skipping its result. many1 p applies the parser p one or more times. Returns a list of the returned values of p. sepBy p sep parses zero or more occurrences of p, separated by sep. Returns a list of values returned by p. sepBy1 p sep parses one or more occurrences of p, separated by sep. Returns a list of values returned by p. endBy p sep parses zero or more occurrences of p, seperated and ended by sep. Returns a list of values returned by p. endBy1 p sep parses one or more occurrences of p, seperated and ended by sep. Returns a list of values returned by p. sepEndBy p sep parses zero or more occurrences of p, separated and optionally ended by sep, ie. haskell style statements. Returns a list of values returned by p. sepEndBy1 p sep parses one or more occurrences of p, separated and optionally ended by sep. Returns a list of values returned by p. chainl p op x parser zero or more occurrences of p, separated by op. Returns a value obtained by a left associative application of all functions returned by op to the values returned by p. If there are zero occurrences of p, the value x is returned. chainl1 p op x parser one or more occurrences of p, separated by op Returns a value obtained by a left associative application of all functions returned by op to the values returned by p. . This parser can for example be used to eliminate left recursion which typically occurs in expression grammars. chainr p op x parser zero or more occurrences of p, separated by op Returns a value obtained by a right associative application of all functions returned by op to the values returned by p. If there are no occurrences of p, the value x is returned. chainr1 p op x parser one or more occurrences of , separated by op Returns a value obtained by a right associative application of all functions returned by op to the values returned by p. This parser only succeeds at the end of the input. This is not a primitive parser but it is defined using notFollowedBy. The (<?>) operator is used for error messages. We will come back to error messages after writing the basic SQL parser. notFollowedBy p only succeeds when parser p fails. This parser does not consume any input. This parser can be used to implement the 'longest match' rule. For example, when recognizing keywords (for example let), we want to make sure that a keyword is not followed by a legal identifier character, in which case the keyword is actually an identifier (for example lets). We can program this behaviour as follows: manyTill p end applies parser p zero or more times until parser end succeeds. Returns the list of values returned by p. This parser can be used to scan comments: Note the overlapping parsers anyChar and string \"-->\", and therefore the use of the try combinator. If p fails and consumes some input, so does lookAhead. Combine with try if this is undesirable. The parser anyToken accepts any kind of token. It is for example used to implement eof. Returns the accepted token. Here are the functions from Applicative that are used: TODO: examples for all of these We’ve already seen all of these, except . This is often used to parse a keyword and return a no argument constructor: There is also which is with the arguments flipped. TODO: double check using these from Parsec instead of Control.Applicative: possible performance implictions? One use of return is to always succeed, and return a value: This function is used in the implementation of : TODO: go through a bunch of functions + do notation examples >>= =<< >> void mapM, mapM_ sequence,sequence_ guard return mzero mplus when, unless liftMN ap quick note about fail, will return to this in the error messages stage\n\nThis is a tutorial about an issue with the token parsing we have so far. Here is a simplified expression type and parser: What happened? The parser tried to parse > as >=, failed, and since the failure consumed some input (the first >), it failed completely. We are going to change the symbol parser to fix this. Here is a parameterized version of the simpleExpr parser so we can try a few variations out. We are going to look at two possible solutions. *Main> regularParse (simpleExprP (try . symbol)) \"1>=2\" Right (BinaryOp (Num 1) \">=\" (Num 2)) *Main> regularParse (simpleExprP (try . symbol)) \"1>2\" Right (BinaryOp (Num 1) \">\" (Num 2)) This seems to have done the job. There is still a problem though. Consider a case when the precedence is the other way round - the and are higher precedence than and , *Main> regularParse (simpleExprP1 (try . symbol)) \"1>2\" Right (BinaryOp (Num 1) \">\" (Num 2)) *Main> regularParse (simpleExprP1 (try . symbol)) \"1>=2\" Left (line 1, column 3): unexpected \"=\" expecting digit Although the precendence order is a little contrived in this case, this issue could easily crop up for real when we start adding more operators. Let’s fix it now. This could be solved by adding a at a earlier place in the parsing. Because of how the function works, it’s not obvious where the could go. Let’s try tackling the problem in a different way. One way of looking at this is to consider that the symbol parser stops parsing too soon: What it should do is keep parsing symbol characters until it gets a result string which can’t be a symbol, and stop one character before this.. Here is a slightly naive way of doing it, which will be good enough for quite a while: *Main> parseWithLeftOver (symbol1 \">\") \">=\" Left (line 1, column 3): unexpected end of input *Main> parseWithLeftOver (symbol1 \">\") \">\" Right (\">\",\"\") *Main> parseWithLeftOver (symbol1 \">\") \">= 3\" Left (line 1, column 3): unexpected \" \" *Main> parseWithLeftOver (symbol1 \">=\") \">= 3\" Right (\">=\",\" 3\") The error messages don’t seem very good, but it parses and fails to parse correctly. This one appears to give better error messages in this limited scenario, apart from that they both work the same. Let’s try them out in the full expression parser: *Main> regularParse (simpleExprP symbol1) \"1>=2\" Right (BinaryOp (Num 1) \">=\" (Num 2)) *Main> regularParse (simpleExprP symbol1) \"1>2\" Right (BinaryOp (Num 1) \">\" (Num 2)) *Main> regularParse (simpleExprP symbol2) \"1>=2\" Right (BinaryOp (Num 1) \">=\" (Num 2)) *Main> regularParse (simpleExprP symbol2) \"1>2\" Right (BinaryOp (Num 1) \">\" (Num 2)) They both work fine here. Let’s see some error messages in this context. Both error messages are a bit crap. So much for the second variation producing better error messages. Let’s look at the equivalent issue with respect to keyword parsing. We can get a similar problem here. *Main> parseWithEof (keyword \"not\") \"not\" Right \"not\" *Main> parseWithEof (keyword \"not\") \"nothing\" Left (line 1, column 4): unexpected 'h' expecting end of input *Main> parseWithEof (keyword \"not\" <|> keyword \"nothing\") \"nothing\" Left (line 1, column 4): unexpected 'h' expecting end of input *Main> parseWithEof (keyword \"nothing\" <|> keyword \"not\") \"nothing\" Right \"nothing\" We can fix this overlapping prefix issue by reordering the choices. But let’s fix the parser in a similar way to the symbol parser. TODO: I don’t know if symbol is the right name, I don’t think Parsec usually uses symbol in this way. Maybe it should be called operator. TODO: later note in error messages about choosing identifier here instead of e.g. many1 letter. *Main> parseWithEof (keyword1 \"not\") \"not\" Right \"not\" *Main> parseWithEof (keyword1 \"not\") \"nothing\" Left (line 1, column 8): unexpected end of input expecting digit, letter or \"_\" *Main> parseWithEof (keyword1 \"not\" <|> keyword1 \"nothing\") \"nothing\" Right \"nothing\" *Main> parseWithEof (keyword1 \"nothing\" <|> keyword1 \"not\") \"nothing\" Right \"nothing\" *Main> parseWithEof (keyword1 \"not\" <|> keyword1 \"nothing\") \"not\" Right \"not\" *Main> parseWithEof (keyword1 \"nothing\" <|> keyword1 \"not\") \"not\" Right \"not\" Try implementing the parser which uses instead of , using something analogous to the change from to above. After this, you can try reimplementing the expression parser from the Text.Parsec.Expr tutorial using the new symbol and keyword parsers.\n\nIn this tutorial, we will build a parser for a subset of SQL value expressions. These are roughly the same as the expressions used in languages like Haskell or C. This will follow on from the work on expressions in previous tutorials. Our value expressions will support literals, identifiers, asterisk, some simple operators, case expression and parentheses. The phrase 'value expression' is from the ANSI SQL standards. What we will develop here isn’t exactly ANSI SQL value expressions, and we won’t use them exactly how the standards do, but the differences really aren’t important right now. I will come back to this in a later tutorial. TODO: not so sure anymore what is a 'value expression' in the standard, and what is a 'scalar expression'. Find out and document it. We will start supporting comments. It will support the two standard comment syntaxes from the standard: The comments do not nest. It will just support positive integral and string literals at this time. Proper SQL supports more literal types including some quite weird syntax which we will skip for now. We will use simple identifiers: an identifier may start with a letter or underscore, and contain letters, underscores and numbers. Full SQL identifiers are more complicated to support so we will skip this for now also. We will do some limited support for identifiers with two parts separated by a dot. I don’t want to get into the exact meaning or the various names used to describe these since it is a bit confusing, especially in SQL. We can just stick to the syntax. Both parts must parse according to the identifier rules above. We will support the star as special expression which can be used at the top level of select lists (and a few other places in SQL). We will also support a 'dotted star'. This represents any syntax which looks like the normal function application used in languages like C. The function name must parse as a valid identifier according to the rules above. We will only support a small range of binary operators for now plus a single prefix unary operator ( ). We will attempt to support correct precedence and associativity for these via the Text.Parsec.Expr module. Here is a complete list of all the supported operators. There are two standard variations of case expressions in SQL. One is more like a switch statement in C (but is an expression, not a statement): The other has a boolean expression in each branch: The else branch is optional (if it is missing, it implicitly means 'else null'). It will parse and represent parentheses explicitly in the abstract syntax, like we did with the previous expression parsers. Here is a type to represent value expressions: Here is the plan for tackling this: Let’s write some simple automated tests to check our progress and check for regressions. We can start by using the code already written to produce a partial expression parser, then add parsing for each new constructor one at a time. Let’s start with some examples we can turn into automated tests. Here are the constructors above which I think we’ve already more or less implemented the parsers for in previous tutorials: , , , and . *ValueExpressions> import Data.List *ValueExpressions Data.List> putStrLn $ intercalate \"\n\n\" $ map show operatorTests (\"not a\",PrefOp \"not\" (Iden \"a\")) (\"+ a\",PrefOp \"+\" (Iden \"a\")) (\"- a\",PrefOp \"-\" (Iden \"a\")) (\"a = b\",BinOp (Iden \"a\") \"=\" (Iden \"b\")) (\"a > b\",BinOp (Iden \"a\") \">\" (Iden \"b\")) (\"a < b\",BinOp (Iden \"a\") \"<\" (Iden \"b\")) (\"a >= b\",BinOp (Iden \"a\") \">=\" (Iden \"b\")) (\"a <= b\",BinOp (Iden \"a\") \"<=\" (Iden \"b\")) (\"a != b\",BinOp (Iden \"a\") \"!=\" (Iden \"b\")) (\"a <> b\",BinOp (Iden \"a\") \"<>\" (Iden \"b\")) (\"a and b\",BinOp (Iden \"a\") \"and\" (Iden \"b\")) (\"a or b\",BinOp (Iden \"a\") \"or\" (Iden \"b\")) (\"a + b\",BinOp (Iden \"a\") \"+\" (Iden \"b\")) (\"a - b\",BinOp (Iden \"a\") \"-\" (Iden \"b\")) (\"a * b\",BinOp (Iden \"a\") \"*\" (Iden \"b\")) (\"a / b\",BinOp (Iden \"a\") \"/\" (Iden \"b\")) (\"a || b\",BinOp (Iden \"a\") \"||\" (Iden \"b\")) (\"a like b\",BinOp (Iden \"a\") \"like\" (Iden \"b\")) Here is a test runner which uses HUnit: 11.4. the parsing we already have The whitespace parser is below, since it includes some new code to deal with comments. TODO: find a place to discuss putting try in the keyword and symbol parsers. I think maybe this should come in the error message tutorial? I’m going to parameterize the parens parser again to avoid rewriting lots of very similar code in this tutorial. 11.4.4. operator table and the first value expression parser I’ve added all the new operators in this table. Now we can run the tests: Let’s start extending things one constructor at a time, but first we will do comments. Here is our old whitespace parser: We are going to change this to do comments as well. TODO: build these two parsers up in stages, show why each bit is there -- no nesting of block comments in SQL -- TODO: why is try used here todo: discuss how to compose to create the whitespace parser todo: create the whitespace parser without many1 and return () and show the problem. Here is the final parser for whitespace: Our string literal is any characters except single quote enclosed in single quotes. We aren’t going to support other string literal syntaxes or escaping single quotes within a string right now. We need a new token parser for string literals: Here is the new value expression parser: Here is a new token parser to use: *ValueExpressions> H.runTestTT $ H.TestList $ map (makeTest valueExpr2) (basicTests ++ stringLiteralTests ++ dIdenTests) ### Failure in: 25:t.a (line 1, column 2): unexpected '.' expecting digit, letter, \"_\", \"--\", \"/*\", operator or end of input Cases: 26 Tried: 26 Errors: 0 Failures: 1 Counts {cases = 26, tried = 26, errors = 0, failures = 1} Do you know why this happened? Can you think of a way to solve this? Here is the usual blunt hammer technique: reorder the choices to put the longest choice first and use when there is a common prefix. We’ll have the same issue we’ve seen before, so let’s go straight to the solution. The App constructor is used for syntax which looks like regular function application: f(), f(a), f(a,b), etc. Here is the parser, parameterized like the parser for the same reason, so we can reuse the code for different versions of the value expression parser: And here is the helper: It is another parser with the prefix, so another . Here is something a little more interesting. \"case a when 1 then 2 when 3 then 4 end\" \"case a when 1 then 2 when 3 then 4 else 5 end\" \"case when a=1 then 2 when a=3 then 4 else 5 end\" How can we approach this? We know that there will always be a case keyword at the start, and an end keyword at the end. Each when branch seems to be self contained. Here is the case parser based simply on this pseudo-code. Let’s try it on its own first and see if we have any problems: *ValueExpressions> H.runTestTT $ H.TestList $ map (makeTest (caseValue0 valueExpr6)) caseTests in: 3:case when a=1 then 2 when a=3 then 4 else 5 end (line 1, column 12): unexpected \"=\" expecting operator, digit, letter, \"_\", \"--\" or \"/*\" Cases: 4 Tried: 4 Errors: 0 Failures: 1 Counts {cases = 4, tried = 4, errors = 0, failures = 1} Three work, and one fails. What happened here? Try to run the parsing code in your head to see if you can work out what the problem is. Here is a method we can use when you can’t manage to locate a problem in this way. Let’s single out the failure first: *ValueExpressions> parseWithEof (caseValue0 valueExpr6) \"case when a=1 then 2 when a=3 then 4 else 5 end\" Left (line 1, column 12): unexpected \"=\" expecting operator, digit, letter, \"_\", \"--\" or \"/*\" Here is the parser rewritten with interleaved: Now when we run the test, we can see a trace of what happens: *ValueExpressions> parseWithEof (caseValue1 valueExpr6) \"case when a=1 then 2 when a=3 then 4 else 5 end\" start case read case keyword read testExpr: Just (Iden \"when\") start when clause Left (line 1, column 12): unexpected \"=\" expecting operator, digit, letter, \"_\", \"--\" or \"/*\" Look at the trace and see if you spot the problem. We’ve parsed the keyword as an identifier for the optional first expression. The way we deal with this is to use a new identifier parser which has a blacklist of keywords which can’t be identifiers. -- here is the fix, we replace calls to `val` with calls to -- this function which prevents any of the case keywords from Maybe this blacklist used in this way isn’t permissive enough, or is too permissive, but let’s not get lost in these details right now, and come back to it later. *ValueExpressions> parseWithEof (caseValue2 valueExpr6) \"case when a=1 then 2 when a=3 then 4 else 5 end\" start case read case keyword read testExpr: Nothing start when clause read when keyword read when exp: BinOp (Iden \"a\") \"=\" (NumLit 1) read then keyword read then exp: NumLit 2 read when exp: BinOp (Iden \"a\") \"=\" (NumLit 3) read then keyword read then exp: NumLit 4 read whens: 2 start else clause read else keyword read else exp: NumLit 5 read else: Just (NumLit 5) read end keyword Right (Case Nothing [(BinOp (Iden \"a\") \"=\" (NumLit 1),NumLit 2),(BinOp (Iden \"a\") \"=\" (NumLit 3),NumLit 4)] (Just (NumLit 5))) Looks good. Have a close look through the trace to see if you can follow it all and it makes sense. TODO: There appear to be some messages missing - I think it is some sort of memoization effect. Let’s run all the tests with the tracing still in so if there are any failures we can zero in on them. *ValueExpressions> H.runTestTT $ H.TestList $ map (makeTest (caseValue2 valueExpr6)) caseTests Cases: 4 Tried: 0 Errors: 0 Failures: 0start case read case keyword read testExpr: Just (Iden \"a\") start when clause read when keyword read when exp: NumLit 1 read then keyword read then exp: NumLit 2 read whens: 1 start else clause read else: Nothing read end keyword Cases: 4 Tried: 1 Errors: 0 Failures: 0read testExpr: Just (Iden \"a\") read when exp: NumLit 1 read then keyword read then exp: NumLit 2 read when exp: NumLit 3 read then keyword read then exp: NumLit 4 read whens: 2 read else: Nothing read end keyword Cases: 4 Tried: 2 Errors: 0 Failures: 0read testExpr: Just (Iden \"a\") read when exp: NumLit 1 read then keyword read then exp: NumLit 2 read when exp: NumLit 3 read then keyword read then exp: NumLit 4 read whens: 2 read else keyword read else exp: NumLit 5 read else: Just (NumLit 5) read end keyword Cases: 4 Tried: 3 Errors: 0 Failures: 0read testExpr: Nothing read when exp: BinOp (Iden \"a\") \"=\" (NumLit 1) read then keyword read then exp: NumLit 2 read when exp: BinOp (Iden \"a\") \"=\" (NumLit 3) read then keyword read then exp: NumLit 4 read whens: 2 read else exp: NumLit 5 read else: Just (NumLit 5) read end keyword Cases: 4 Tried: 4 Errors: 0 Failures: 0 Counts {cases = 4, tried = 4, errors = 0, failures = 0} *ValueExpressions> H.runTestTT $ H.TestList $ map (makeTest valueExpr7) (basicTests ++ stringLiteralTests ++ dIdenTests ++ starTests ++ dStarTests ++ appTests ++ caseTests) Cases: 35 Tried: 0 Errors: 0 Failures: 0start case Cases: 35 Tried: 31 Errors: 0 Failures: 0read case keyword read testExpr: Just (Iden \"a\") start when clause read when keyword read when exp: NumLit 1 read then keyword read then exp: NumLit 2 read whens: 1 start else clause read else: Nothing read end keyword Cases: 35 Tried: 32 Errors: 0 Failures: 0read testExpr: Just (Iden \"a\") read when exp: NumLit 1 read then keyword read then exp: NumLit 2 read when exp: NumLit 3 read then keyword read then exp: NumLit 4 read whens: 2 read else: Nothing read end keyword Cases: 35 Tried: 33 Errors: 0 Failures: 0read testExpr: Just (Iden \"a\") read when exp: NumLit 1 read then keyword read then exp: NumLit 2 read when exp: NumLit 3 read then keyword read then exp: NumLit 4 read whens: 2 read else keyword read else exp: NumLit 5 read else: Just (NumLit 5) read end keyword Cases: 35 Tried: 34 Errors: 0 Failures: 0read testExpr: Nothing read when exp: BinOp (Iden \"a\") \"=\" (NumLit 1) read then keyword read then exp: NumLit 2 read when exp: BinOp (Iden \"a\") \"=\" (NumLit 3) read then keyword read then exp: NumLit 4 read whens: 2 read else exp: NumLit 5 read else: Just (NumLit 5) read end keyword Cases: 35 Tried: 35 Errors: 0 Failures: 0 Counts {cases = 35, tried = 35, errors = 0, failures = 0} Everything looks good. Let’s refactor the case parser and tidy everything up. Here is the parser with the trace`s removed, and using the new `blackListValueExpr parser. And here it is after some shortening: I think we passed the point where is more readable than : TODO: review code above for new syntax patterns to talk about. Or maybe don’t talk about them? Let’s predict that we will need more blacklisting when we work on the query expression parsing, and create a parser which can be used in this way: TODO: need to change app, dstar and diden for blacklisting?\n\nIn this tutorial, we extend the from clause support to the following: we will support implicit and explicit joins, including keywords natural, inner, outer, left, right, full, cross, on and using, plus parens and simple aliases (e.g. select a from t u, but not select a from t(a,b)). We don’t support oracle outer join syntax (+) or the other 'pre-ANSI' variations on this theme. No lateral keyword or apply or pivot. TODO: should explicitly import from these two modules (and same in QueryExpressions.lhs) Here are is the updated and the new abstract syntax types. This syntax for table references can represent invalid syntax, for instance two nested aliases. The justification for this is that sometimes trying to accurately represent only exactly what is valid creates something much more complex. Maybe this is a good tradeoff in this situation, and maybe not. With the join condition, we’ve done the opposite to TableRef - we’ve combined and / , since only one of these can be present, even though this departs a little from the concrete syntax. First we will develop the standalone from clause parser, then we will update the query expression syntax and parsing to incorporate our new from clause parser. Let’s start with something simple: a from clause can be multiple comma separated tablerefs, aka an implicit join. Let’s do the query expression, parens and alias first, before tackling joins. Here is the query expression parser we can use: We can’t do a sensible example for these right now - we need explicit joins and then the parens can be used to override the associativity of a three way join, or to specify over what part of the expression to apply an alias. We can write some more tests for parens after we’ve done the explicit joins. The alias can be treated like a postfix operator. TODO: ?? not sure about this How to make it keep nesting? Here is a casual sketch of the target grammar: tref (cross | [natural] ([inner] | left [outer] | right [outer] | full [outer] ) join tref [on expr | using (...)] Let’s start with parsers for the 'join operator' in the middle and for the join condition: *Main> parseWithEof joinType \"cross join\" Right JoinCross *Main> parseWithEof joinType \"inner join\" Right JoinInner *Main> parseWithEof joinType \"left outer join\" Right JoinLeft *Main> parseWithEof joinType \"left join\" Right JoinLeft *Main> parseWithEof joinType \"right outer join\" Right JoinRight *Main> parseWithEof joinType \"right join\" Right JoinRight *Main> parseWithEof joinType \"full outer join\" Right JFull *Main> parseWithEof joinType \"full join\" Right JoinFull *Main> parseWithEof joinType \"join\" Right JoinInner I thought about factoring out the common bits with the joinType parser: But I think the longer version is much easier to follow, even if it is a little more boring. The idea with the join condition is that we pass a bool to say whether we’ve already seen the 'natural' keyword. If so, then we don’t try to parse 'on' or 'using'. *Main> parseWithEof (joinCondition False) \"on a\" Right (JoinOn (Iden \"a\")) *Main> parseWithEof (joinCondition False) \"on a + b\" Right (JoinOn (BinOp (Iden \"a\") \"+\" (Iden \"b\"))) *Main> parseWithEof (joinCondition False) \"using (a,b)\" Right (JoinUsing [\"a\",\"b\"]) *Main> parseWithEof (joinCondition True) \"using (a,b)\" Left (line 1, column 1): unexpected 'u' expecting end of input *Main> parseWithEof (joinCondition True) \"\" Right JoinNatural \"from a join b on a.x = b.y\" We want to parse the first table, then optionally parse the 'natural' keyword, then the join type, then the second table, then optionally parse the join condition. Let’s start extending this into the full target parser. In this next version, I’ve tried to combine all the versions we’ve seen so far. *Main> H.runTestTT $ H.TestList $ map (makeTest from5) (multipleTRSimpleTests ++ trQueryExprTests ++ trParensTests ++ trAliasTests ++ simpleBinaryJoinTests) ### Failure in: 5:from a join b (line 1, column 14): unexpected end of input expecting digit, letter, \"_\", \"--\" or \"/*\" ### Failure in: 6:from a natural join b from a natural join b expected: [TRJoin (TRSimple \"a\") JoinInner (TRSimple \"b\") (Just JoinNatural)] but got: [TRJoin (TRAlias (TRSimple \"a\") \"natural\") JoinInner (TRSimple \"b\") Nothing] ### Failure in: 7:from a join b on a.x = b.y (line 1, column 15): unexpected \"o\" expecting \"--\" or \"/*\" ### Failure in: 8:from a join b using(x,y) (line 1, column 15): unexpected \"u\" expecting \"--\" or \"/*\" ### Failure in: 9:from a cross join b from a cross join b expected: [TRJoin (TRSimple \"a\") JoinCross (TRSimple \"b\") Nothing] but got: [TRJoin (TRAlias (TRSimple \"a\") \"cross\") JoinInner (TRSimple \"b\") Nothing] Cases: 10 Tried: 10 Errors: 0 Failures: 5 Counts {cases = 10, tried = 10, errors = 0, failures = 5} What’s going wrong? If you look at some of the issues, it looks like we are getting keywords parsed as aliases. Let’s fix that first: That didn’t solve the problem. I think we also have a problem since the can now fail after consuming input, we need to use . The final step is to make it parse n-way explicit joins. We get left associative with this code. I don’t know if this is correct. We should do some more testing to make sure this code is good. TODO \"select a from t inner join u on expr\" \"select a from t right join u on expr\" Here are all the other query expression tests updated with the new QueryExpr type. \"select a,sum(b) from t group by a having sum(b) > 5\" *Main> H.runTestTT $ H.TestList $ map (makeTest (queryExpr1 from8)) (selectListTests ++ fromTests ++ whereTests ++ groupByTests ++ havingTests ++ orderByTests ++ queryExprJoinTests) ### Failure in: 7:select a from t where a = 5 (line 1, column 25): unexpected \"=\" expecting \"--\" or \"/*\" ### Failure in: 8:select a,sum(b) from t group by a (line 1, column 33): unexpected \"a\" expecting \"--\" or \"/*\" ### Failure in: 9:select a,b,sum(c) from t group by a,b (line 1, column 35): unexpected \"a\" expecting \"--\" or \"/*\" ### Failure in: 10:select a,sum(b) from t group by a having sum(b) > 5 (line 1, column 33): unexpected \"a\" expecting \"--\" or \"/*\" ### Failure in: 11:select a from t order by a (line 1, column 26): unexpected \"a\" expecting \"--\" or \"/*\" ### Failure in: 12:select a from t order by a, b (line 1, column 26): unexpected \"a\" expecting \"--\" or \"/*\" Cases: 26 Tried: 26 Errors: 0 Failures: 6 Counts {cases = 26, tried = 26, errors = 0, failures = 6} The problem is the table alias parser is trying to parse keywords again. Here is the parser with the alias name blacklist expanded. Here is the final query expression parser:"
    },
    {
        "link": "https://book.realworldhaskell.org/read/using-parsec.html",
        "document": "The task of parsing a file, or data of various types, is a common one for programmers. We already learned about Haskell's support for regular expressions back in the section called “Regular expressions in Haskell”. Regular expressions are nice for many tasks, but they rapidly become unwieldy, or cannot be used at all, when dealing with a complex data format. For instance, we cannot use regular expressions to parse source code from most programming languages.\n\nParsec is a useful parser combinator library, with which we combine small parsing functions to build more sophisticated parsers. Parsec provides some simple parsing functions, as well as functions to tie them all together. It should come as no surprise that this parser library for Haskell is built around the notion of functions.\n\nIt's helpful to know where Parsec fits compared to the tools used for parsing in other languages. Parsing is sometimes divided into two stages: lexical analysis (the domain of tools like flex) and parsing itself (performed by programs such as bison). Parsec can perform both lexical analysis and parsing.\n\nLet's jump right in by writing some code for parsing a CSV file. CSV files are often used as a plain text representation of spreadsheets or databases. Each line is a record, and each field in the record is separated from the next by a comma. There are ways of dealing with fields that contain commas, but to start with, we won't worry about it. This first example is much longer than it really needs to be. We will introduce more Parsec features in a little bit that will shrink the parser down to only four lines! -- file: ch16/csv1.hs import Text.ParserCombinators.Parsec {- A CSV file contains 0 or more lines, each of which is terminated by the end-of-line character (eol). -} csvFile :: GenParser Char st [[String]] csvFile = do result <- many line eof return result -- Each line contains 1 or more cells, separated by a comma line :: GenParser Char st [String] line = do result <- cells eol -- end of line return result -- Build up a list of cells. Try to parse the first cell, then figure out -- what ends the cell. cells :: GenParser Char st [String] cells = do first <- cellContent next <- remainingCells return (first : next) -- The cell either ends with a comma, indicating that 1 or more cells follow, -- or it doesn't, indicating that we're at the end of the cells for this line remainingCells :: GenParser Char st [String] remainingCells = (char ',' >> cells) -- Found comma? More cells coming <|> (return []) -- No comma? Return [], no more cells -- Each cell contains 0 or more characters, which must not be a comma or -- EOL cellContent :: GenParser Char st String cellContent = many (noneOf \",\n\n\") -- The end of line character is \n\n eol :: GenParser Char st Char eol = char '\n\n' parseCSV :: String -> Either ParseError [[String]] parseCSV input = parse csvFile \"(unknown)\" input Let's take a look at the code for this example. We didn't use many shortcuts here, so remember that this will get shorter and simpler! We've built it from the top down, so our first function is . The type of this function is . This means that the type of the input is a sequence of characters, which is exactly what a Haskell string is, since is the same as . It also means that we will return a value of type : a list of a list of strings. The can be ignored for now. Parsec programmers often omit type declarations, since we write so many small functions. Haskell's type inference can figure it out. We've listed the types for the first example here so you can get a better idea of what's going on. You can always use in ghci to inspect types as well. The uses a block. As this implies, Parsec is a monadic library: it defines its own special parsing monad[ ], GenParser. We start by running . is a function that takes a function as an argument. It tries to repeatedly parse the input using the function passed to it. It gathers up the results from all that repeated parsing and returns a list of them. So, here, we are storing the results of parsing all lines in . Then we look for the end-of-file indicator, called . Finally, we return the . So, a CSV file is made up of many lines, then the end of file. We can often read out Parsec functions in plain English just like this. Now we must answer the question: what is a line? We define the function to do just that. Reading the function, we can see that a line consists of cells followed by the end of line character. So what are cells? We defined them in the function. The cells of a line start with the content of the first cell, then continue with the content of the remaining cells, if any. The result is simply the first cell and the remaining cells assembled into a list. Let's skip over for a minute and look at . A cell contains any number of characters, but each character must not be a comma or end of line character. The function matches one item, so long as it isn't in the list of items that we pass. So, saying defines a cell the way we want it. Back in , we have the first example of a choice in Parsec. The choice operator is . This operator behaves like this: it will first try the parser on the left. If it consumed no input[ ], it will try the parser on the right. So, in , our task is to come up with all the cells after the first. Recall that uses . So it will not consume the comma or end-of-line character from the input. If we see a comma after parsing a cell, it means that at least one more cell follows. Otherwise, we're done. So, our first choice in is . This parser simply matches the passed character in the input. If we found a comma, we want this function to return the remaining cells on the line. At this point, the \"remaining cells\" looks exactly like the start of the line, so we call recursively to parse them. If we didn't find a comma, we return the empty list, signifying no remaining cells on the line. Finally, we must define what the end-of-line indicator is. We set it to , which will suit our purposes fine for now. At the very end of the program, we define a function that takes a and parses it as a CSV file. This function is just a shortcut that calls Parsec's function, filling in a few parameters. returns for the CSV file. If there was an error, the return value will be with the error; otherwise, it will be with the result. Now that we understand this code, let's play with it a bit and see what it does. [1 of 1] Compiling Main ( csv1.hs, interpreted ) Ok, modules loaded: Main. Loading package parsec-2.1.0.0 ... linking ... done. Right [] That makes sense: parsing the empty string returns an empty list. Let's try parsing a single cell. Look at that. Recall how we defined that each line must end with the end-of-line character, and we didn't give it. Parsec's error message helpfully indicated the line number and column number of the problem, and even told us what it was expecting! Let's give it an end-of-line character and continue experimenting. Right [[\"hi\"]] Right [[\"line1\"],[\"line2\"],[\"line3\"]] Right [[\"cell1\",\"cell2\",\"cell3\"]] Right [[\"l1c1\",\"l1c2\"],[\"l2c1\",\"l2c2\"]] Right [[\"Hi\",\"\"],[\"\"],[\"\",\"Hello\"]] You can see that is doing exactly what we wanted it to do. It's even handling empty cells and empty lines properly.\n\nWe promised you earlier that we could simplify our CSV parser significantly by using a few Parsec helper functions. There are two that will dramatically simplify this code. The first tool is the function. This function takes two functions as arguments: the first function parses some sort of content, while the second function parses a separator. starts by trying to parse content, then separators, and alternates back and forth until it can't parse a separator. It returns a list of all the content that it was able to parse. The second tool is . It's similar to , but expects the very last item to be followed by the separator. That is, it continues parsing until it can't parse any more content. So, we can use to parse lines, since every line must end with the end-of-line character. We can use to parse cells, since the last cell will not end with a comma. Take a look at how much simpler our parser is now: This program behaves exactly the same as the first one. We can verify this by using ghci to re-run our examples from the earlier example. We'll get the same result from every one. Yet the program is much shorter and more readable. It won't be long before you can translate Parsec code like this into a file format definition in plain English. As you read over this code, you can see that:\n• None A CSV file contains 0 or more lines, each of which is terminated by the end-of-line character.\n• None A line contains 1 or more cells, separated by a comma.\n• None A cell contains 0 or more characters, which must be neither the comma nor the end-of-line character.\n• None The end-of-line character is the newline, .\n\nDifferent operating systems use different characters to mark the end-of-line. Unix/Linux systems, plus Windows in text mode, use simply . DOS and Windows systems use , and Macs traditionally used . We could add in support for too, just in case anybody uses that. We could easily adapt our example to be able to handle all these types of line endings in a single file. We would need to make two modifications: adjust to recognize the different endings, and adjust the pattern in to ignore . This must be done carefully. Recall that our earlier definition of was simply . There is a parser called that we can use to match the multi-character patterns. Let's start by thinking of how we would add support for . Our first attempt might look like this: -- file: ch16/csv3.hs -- This function is not correct! eol = string \"\n\n\" <|> string \"\n\n\\r\" This isn't quite right. Recall that the operator always tries the left alternative first. Looking for the single character will match both types of line endings, so it will look to the system that the following line begins with . Not what we want. Try it in ghci: Loading package parsec-2.1.0.0 ... linking ... done. Right \"\n\n\" Right \"\n\n\" It may seem like the parser worked for both endings, but actually looking at it this way, we can't tell. If it left something un-parsed, we don't know, because we're not trying to consume anything else from the input. So let's look for the end-of-file after our end of line: As expected, we got an error from the ending. So the next temptation may be to try it this way: -- file: ch16/csv4.hs -- This function is not correct! eol = string \"\n\n\\r\" <|> string \"\n\n\" This also isn't right. Recall that only attempts the option on the right if the option on the left consumed no input. But by the time we are able to see if there is a after the , we've already consumed the . This time, we fail on the other case in ghci: Loading package parsec-2.1.0.0 ... linking ... done. Right () Left (line 1, column 1): unexpected end of input expecting \"\n\n\\r\" We've stumbled upon the lookahead problem. It turns out that, when writing parsers, it's often very convenient to be able to \"look ahead\" at the data that's coming in. Parsec supports this, but before showing you how to use it, let's see how you would have to write this to get along without it. You'd have to manually expand all the options after the like this: This function first looks for . If it is found, then it will look for , consuming it if possible. Since the return type of is a , the alternative action is to simply return a without attempting to parse anything. Parsec has a function that can also express this idiom as . Let's test this with ghci. [1 of 1] Compiling Main ( csv5.hs, interpreted ) Ok, modules loaded: Main. Loading package parsec-2.1.0.0 ... linking ... done. Right () Right () This time, we got the right result! But we could have done it easier with Parsec's lookahead support. Parsec has a function called that is used to express lookaheads. takes one function, a parser. It applies that parser. If the parser doesn't succeed, behaves as if it hadn't consumed any input at all. So, when you use on the left side of , Parsec will try the option on the right even if the left side failed after consuming some input. only has an effect if it is on the left of a . Keep in mind, though, that many functions use internally. Here's a way to add expanded end-of-line support to our CSV parser using : -- file: ch16/csv6.hs import Text.ParserCombinators.Parsec csvFile = endBy line eol line = sepBy cell (char ',') cell = many (noneOf \",\n\n\\r\") eol = try (string \"\n\n\\r\") <|> try (string \"\\r\n\n\") <|> string \"\n\n\" <|> string \"\\r\" parseCSV :: String -> Either ParseError [[String]] parseCSV input = parse csvFile \"(unknown)\" input Here we put both of the two-character endings first, and run both tests under . Both of them occur to the left of a , so they will do the right thing. We could have put within a , but it wouldn't have altered any behavior since they look at only one character anyway. We can load this up and test the function in ghci. [1 of 1] Compiling Main ( csv6.hs, interpreted ) Ok, modules loaded: Main. Loading package parsec-2.1.0.0 ... linking ... done. Right () Right () Right () Right () All four endings were handled properly. You can also test the full CSV parser with some different endings like this: As you can see, this program even supports different line endings within a single file. At the beginning of this chapter, you saw how Parsec could generate error messages that list the location where the error occurred as well as what was expected. As parsers get more complex, the list of what was expected can become cumbersome. Parsec provides a way for you to specify custom error messages in the event of parse failures. Let's look at what happens when our current CSV parser encounters an error: That's a pretty long, and technical, error message. We could make an attempt to resolve this by using the monad function like so: Under ghci, we can see the result: [1 of 1] Compiling Main ( csv7.hs, interpreted ) Ok, modules loaded: Main. Loading package parsec-2.1.0.0 ... linking ... done. Left \"(unknown)\" (line 1, column 6): unexpected end of input expecting \",\", \"\n\n\\r\", \"\\r\n\n\", \"\n\n\" or \"\\r\" Couldn't find EOL We added to the error result, but didn't really help clean up the output. Parsec has an operator that is designed for just these situations. It is similar to in that it first tries the parser on its left. Instead of trying another parser in the event of a failure, it presents an error message. Here's how we'd use it: -- file: ch16/csv8.hs eol = try (string \"\n\n\\r\") <|> try (string \"\\r\n\n\") <|> string \"\n\n\" <|> string \"\\r\" <?> \"end of line\" Now, when you generate an error, you'll get more helpful output: [1 of 1] Compiling Main ( csv8.hs, interpreted ) Ok, modules loaded: Main. Loading package parsec-2.1.0.0 ... linking ... done. Left \"(unknown)\" (line 1, column 6): unexpected end of input expecting \",\" or end of line That's pretty helpful! The general rule of thumb is that you put a human description of what you're looking for to the right of .\n\nOur earlier CSV examples have had an important flaw: they weren't able to handle cells that contain a comma. CSV generating programs typically put quotation marks around such data. But then you have another problem: what to do if a cell contains a quotation mark and a comma. In these cases, the embedded quotation marks are doubled up. Here is a full CSV parser. You can use this from ghci, or if you compile it to a standalone program, it will parse a CSV file on standard input and convert it to a different format on output. -- file: ch16/csv9.hs import Text.ParserCombinators.Parsec csvFile = endBy line eol line = sepBy cell (char ',') cell = quotedCell <|> many (noneOf \",\n\n\\r\") quotedCell = do char '\"' content <- many quotedChar char '\"' <?> \"quote at end of cell\" return content quotedChar = noneOf \"\\\"\" <|> try (string \"\\\"\\\"\" >> return '\"') eol = try (string \"\n\n\\r\") <|> try (string \"\\r\n\n\") <|> string \"\n\n\" <|> string \"\\r\" <?> \"end of line\" parseCSV :: String -> Either ParseError [[String]] parseCSV input = parse csvFile \"(unknown)\" input main = do c <- getContents case parse csvFile \"(stdin)\" c of Left e -> do putStrLn \"Error parsing input:\" print e Right r -> mapM_ print r That's a full-featured CSV parser in just 21 lines of code, plus an additional 10 lines for the and utility functions. Let's look at the changes in this program from the previous versions. First, a cell may now be either a bare cell or a \"quoted\" cell. We give the option first, because we want to follow that path if the first character in a cell is the quote mark. The begins and ends with a quote mark, and contains zero or more characters. These characters can't be copied directly, though, because they may contain embedded, doubled-up, quote marks themselves. So we define a custom to process them. When we're processing characters inside a quoted cell, we first say . This will match and return any single character as long as it's not the quote mark. Otherwise, if it is the quote mark, we see if we have two of them in a row. If so, we return a single quote mark to go on our result string. Notice that in on the right side of . Recall that I said that only has an effect if it is on the left side of . This does occur on the left side of a , but on the left of one that must be within the implementation of . This is important. Let's say we are parsing a quoted cell, and are getting towards the end of it. There will be another cell following. So we will expect to see a quote to end the current cell, followed by a comma. When we hit , we will fail the test and proceed to the test that looks for two quotes in a row. We'll also fail that one because we'll have a quote, then a comma. If we hadn't used , we'd crash with an error at this point, saying that it was expecting the second quote, because the first quote was already consumed. Since we use , this is properly recognized as not a character that's part of the cell, so it terminates the expression as expected. Lookahead has once again proven very useful, and the fact that it is so easy to add makes it a remarkable tool in Parsec. We can test this program with ghci over some quoted cells. [1 of 1] Compiling Main ( csv9.hs, interpreted ) Ok, modules loaded: Main. parseCSV \"\\\"This, is, one, big, cell\\\"\n\n\" Loading package parsec-2.1.0.0 ... linking ... done. Right [[\"This, is, one, big, cell\"]] Left \"(unknown)\" (line 2, column 1): unexpected end of input expecting \"\\\"\\\"\" or quote at end of cell Let's run it over a real CSV file. Here's one generated by a spreadsheet program: Now, we can run this under our test program and watch:\n\nWe'll begin by rewriting our existing form parser from the bottom up, beginning with , which parses a hexadecimal escape sequence. Here's the code in normal -notation style. Although the individual parsers are mostly untouched, the combinators that we're gluing them together with have changed. The only familiar one is , which we already know is a synonym for . From our definition of Applicative, we know that is . The remaining unfamiliar combinator is , which applies its first argument, throws away its result, then applies the second and returns its result. In other words, it's similar to . Before we continue, here's a useful aid for remembering what all the angle brackets are for in the combinators from : if there's an angle bracket pointing to some side, the result from that side should be used. For example, returns the result on its right; returns results from both sides; and , which we have not yet seen, returns the result on its left. Although the concepts here should mostly be familiar from our earlier coverage of functors and monads, we'll walk through this function to explain what's happening. First, to get a grip on our types, we'll hoist to the top level and give it a signature. Therefore, has the same type, since returns the result on its right. (The CharParser type is nothing more than a synonym for GenParser Char.) The expression is a parser that matches a “%” character followed by hex digit, and whose result is a function. Finally, applies the parser on its left, then the parser on its right, and applies the function that's the result of the left parse to the value that's the result of the right. If you've been able to follow this, then you understand the and combinators: is plain old lifted to applicative functors, and the same thing lifted to monads. Next, we'll consider the parser. This remains almost the same in an applicative style, save for one piece of convenient notation. Here, the combinator uses the value on the left if the parser on the right succeeds. Finally, the equivalent of is almost identical. All we've changed is the combinator we use for lifting: the functions act in the same ways as their cousins.\n\nTo give ourselves a better feel for parsing with applicative functors, and to explore a few more corners of Parsec, we'll write a JSON parser that follows the definition in RFC 4627. At the top level, a JSON value must be either an object or an array. These are structurally similar, with an opening character, followed by one or more items separated by commas, followed by a closing character. We capture this similarity by writing a small helper function. -- file: ch16/JSONParsec.hs p_series :: Char -> CharParser () a -> Char -> CharParser () [a] p_series left parser right = between (char left <* spaces) (char right) $ (parser <* spaces) `sepBy` (char ',' <* spaces) Here, we finally have a use for the combinator that we introduced earlier. We use it to skip over any white space that might follow certain tokens. With this function, parsing an array is simple. Dealing with a JSON object is hardly more complicated, requiring just a little additional effort to produce a name/value pair for each of the object's fields. Parsing an individual value is a matter of calling an existing parser, then wrapping its result with the appropriate JValue constructor. -- file: ch16/JSONParsec.hs p_value :: CharParser () JValue p_value = value <* spaces where value = JString <$> p_string <|> JNumber <$> p_number <|> JObject <$> p_object <|> JArray <$> p_array <|> JBool <$> p_bool <|> JNull <$ string \"null\" <?> \"JSON value\" p_bool :: CharParser () Bool p_bool = True <$ string \"true\" <|> False <$ string \"false\" The combinator allows us to represent this kind of ladder-of-alternatives as a list. It returns the result of the first parser to succeed. -- file: ch16/JSONParsec.hs p_value_choice = value <* spaces where value = choice [ JString <$> p_string , JNumber <$> p_number , JObject <$> p_object , JArray <$> p_array , JBool <$> p_bool , JNull <$ string \"null\" ] <?> \"JSON value\" This leads us to the two most interesting parsers, for numbers and strings. We'll deal with numbers first, since they're simpler. Our trick here is to take advantage of Haskell's standard number parsing library functions, which are defined in the module. The function reads an unsigned floating point number, and takes a parser for an unsigned number and turns it into a parser for possibly signed numbers. Since these functions know nothing about Parsec, we have to work with them specially. Parsec's function gives us direct access to Parsec's unconsumed input stream. If succeeds, it returns both the parsed number and the rest of the unparsed input. We then use to give this back to Parsec as its new unconsumed input stream. -- file: ch16/JSONParsec.hs p_string :: CharParser () String p_string = between (char '\\\"') (char '\\\"') (many jchar) where jchar = char '\\\\' *> (p_escape <|> p_unicode) <|> satisfy (`notElem` \"\\\"\\\\\") We can parse and decode an escape sequence with the help of the combinator that we just met. -- file: ch16/JSONParsec.hs p_escape = choice (zipWith decode \"bnfrt\\\\\\\"/\" \"\\b\n\n\\f\\r\\t\\\\\\\"/\") where decode c r = r <$ char c Finally, JSON lets us encode a Unicode character in a string as “ ” followed by four hexadecimal digits. The only piece of functionality that applicative functors are missing, compared to monads, is the ability to bind a value to a variable, which we need here in order to be able to validate the value we're trying to decode. This is the one place in our parser that we've needed to use a monadic function. This pattern extends to more complicated parsers, too: only infrequently do we need the extra bit of power that monads offer. As we write this book, applicative functors are still quite new to Haskell, and people are only beginning to explore the possible uses for them beyond the realm of parsing.\n\nAs another example of applicative parsing, we will develop a basic parser for HTTP requests. An HTTP request consists of a method, an identifier, a series of headers, and an optional body. For simplicity, we'll focus on just two of the six method types specified by the HTTP 1.1 standard. A method has a body; a has none. Because we're writing in an applicative style, our parser can be both brief and readable. Readable, that is, if you're becoming used to the applicative parsing notation. -- file: ch16/HttpRequestParser.hs p_request :: CharParser () HttpRequest p_request = q \"GET\" Get (pure Nothing) <|> q \"POST\" Post (Just <$> many anyChar) where q name ctor body = liftM4 HttpRequest req url p_headers body where req = ctor <$ string name <* char ' ' url = optional (char '/') *> manyTill notEOL (try $ string \" HTTP/1.\" <* oneOf \"01\") <* crlf Briefly, the helper function accepts a method name, the type constructor to apply to it, and a parser for a request's optional body. The helper does not attempt to validate a URL, because the HTTP specification does not specify what characters an URL contain. The function just consumes input until either the line ends or it reaches a HTTP version identifier. The combinator has to hold onto input in case it needs to restore it, so that an alternative parser can be used. This practice is referred to as backtracking. Because must save input, it is expensive to use. Sprinkling a parser with unnecessary uses of is a very effective way to slow it down, sometimes to the point of unacceptable performance. The standard way to avoid the need for backtracking is to tidy up a parser so that we can decide whether it will succeed or fail using only a single token of input. In this case, the two parsers consume the same initial tokens, so we turn them into a single parser. Even better, Parsec gives us an improved error message if we feed it non-matching input. Following the first line of a HTTP request is a series of zero or more headers. A header begins with a field name, followed by a colon, followed by the content. If the lines that follow begin with spaces, they are treated as continuations of the current content. Our HTTP request parser is too simple to be useful in real deployments. It is missing vital functionality, and is not resistant to even the most basic denial of service attacks. Make the parser honour the field properly, if it is present. A popular denial of service attack against naive web servers is simply to send unreasonably long headers. A single header might contain tens or hundreds of megabytes of garbage text, causing a server to run out of memory. Restructure the header parser so that it will fail if any line is longer than 4096 characters. It must fail immediately when this occurs; it cannot wait until the end of a line eventually shows up. Add the ability to honour the header if it is present. See section 3.6.1 of RFC 2616 for details. Another popular attack is to open a connection and either leave it idle or send data extremely slowly. Write a wrapper in the IO monad that will invoke the parser. Use the module to close the connection if the parser has not completed within 30 seconds."
    },
    {
        "link": "https://reddit.com/r/haskell/comments/v8p73h/is_there_good_introduction_to_the_parsec_library",
        "document": "I try to follow the articles on this page, and most of them are obsolete ... It seems their API has changed, and there is little documentations.\n\nFor example, I don't understand what is.\n\nMy understanding of parsec all comes from this video, but there is a gap from here to the parsec library APIs. Is there any document/articles on this? Or maybe this library is not really used often?"
    },
    {
        "link": "https://jsdw.me/posts/haskell-parsec-basics",
        "document": "First off, why would you use Parsec as opposed to things like regular expressions for parsing content? Coming from other languages, splitting content into arrays and processing ever smaller chunks using regular expressions and the like can be quite common practise. In Haskell we can go down that route too, but now I've seen the light with Parsec I want to introduce you to a better approach.\n\nMost tutorials get stuck in with a complete example straight off the bat, but I'm going to present the different functions one by one so that they can be used as a reference of reminder (to me as much as anyone!) of how things work. I'll try to keep examples relatively self contained, so it shouldn't be hard to skip to bits, but keep in mind the basic setup I do first. I have also put all of the example code into a file right here that is ready to be ed straight into and played with.\n\nParsec works its way along some stream of text from beginning to end, attempting to match the stream of inputs to some rule or a set of rules. Parsec is also monadic, so we can piece together our different rules in sequence using the convenient do notation. As a general overview, rules work by consuming a character at a time from the input and determining whether they match or not, so when you piece a number of rules in sequence, each rule will consume part of the input until you have no input left, run out of rules, or one of your rules fails to match (resulting in an error).\n\nLet's start with a very basic setup. I import qualified so it's plainly obvious when I'm using functions, and also import so we can play with parsing things in the applicative style later on, and make a short alias for for use in the examples:\n\nThis will be our base setup, with a simple utility function parse defined which just ignored the second argument of Parsec.parse (which in the real world would be the name of the file whose contents you are parsing, and is just used in error messages by Parsec to give you that piece of extra information).\n\nParsec comes with a host of building blocks, each of which can be a rule in its own right, or can be combined with others to build more complex rules up. Let's look at some of these basic building blocks and see how they work with the above setup.\n\nThis function returns a rule that matches the current character in the text that we are parsing to whatever character you provide it. Lets have a go in the interactive ghci console:\n\nHere, running returns a rule that will match a single character so long as it's 'H'. If we thus use it in parsing our string, which begins with an H, it is perfectly happy. If we try looking for any letter that isn't 'H', we fail. The result is always of type ; we get back a if the rule was successful, and if the rule fails. Thus, we can pattern match to see which the case is simply enough:\n\nThis function returns a rule that attempts to match the provided string of characters:\n\nThe parser will consume characters from the input one by one until all characters match or one of them is not as expected. Since both of the above attempts begin with 'h', the error complains about an unexpected 'o'. This consuming of characters will become significant when multiple rules are chained together.\n\nSometimes we want to match a range of characters; that's where Parsec.oneOf comes in handy. Similar to , except you provide this function a list of characters you're OK with matching:\n\nHere, we can see that the parser will consume any single character from 'a' to 'e'. We can use ranges here to simplify things, so to match any lowercase letter we'd just use for instance.\n\nParsec comes with pre-prepared rules which do just that, for example , which will consume one of anything:\n\nThe rule will happily consume any lower or uppercase letter, will consume any lowercase letter, will consume any number, and any letter or number. All of these can be constructed manually using as above, though they come with nicer error messages (which you can add to your own rules; we'll look at that later).\n\nThe opposite of the above, you provide this function a list of characters that aren't allowed and it'll match anything else. Again, ranges are your friend:\n\nLet's face it, there are times when you want to parse more than just one letter. tries to run whatever rule it is given as an argument over and over until it fails. Even if the rule matches no times, many will return without error, but just give back an empty result. Let's see how it's used:\n\nAs we can see, can't go wrong, as it'll happily match the rule it's given zero times and just return nothing. It'll go as far as it can and give you back everything that matched. is similar except that the rule it's provided must match at least once for it to return successfully:\n\nUseful when you need to match some set of letters or numbers for instance, but must see at least one.\n\nIf you need to match a specific number of something, comes in handy. It takes a number and a parser, and expects to match that parser that number of times (or will fail), returning the result. Here's an example:\n\nThis parser takes in two arguments; the rule it will try matching and the rule it expects to be able to match right after it. As with many, it expects zero or more of the first rule, but it will error on anything that matches neither rule. The below tries matching letters and then expects numbers immediately following them:\n\nIt's important to keep in mind the fact that this will consume (and output) all of the first rule, and then consume whatever the second rule matches (but ignore it in the output). When we start stringing together rules in sequence, it becomes increasingly important what we consume and what we leave ready for the next rule to have a go at.\n\nOne thing I think is great about Parsec is that it provides useful error information straight off the bat; in this case the string we passed in back at the beginning (\"(source)\") along with the line and column number of the error and some useful message as to what went wrong. We're only dealing with single lines at the moment, but to have this from the word go is really cool.\n\nNow you have some of the basic rules under your belt, lets talk about how to combine them! Parsec, being monadic, allows you to write parsers using Haskell's do notation sugar. Here's an example that puts some simple parsers above in sequence, getting the results of a couple and returning them:\n\nNote that I have given this parser an explicit type of . The arguments to this type in order are simply input type, some state you'd like threaded through your parsers (we'll use the unit type for now, and so have no meaningful state, and have a quick look at state later), then output type. In this case, we're taking in a String and returning a tuple of two strings. If you use to inspect the type of rules in ghci, you'll see that they are made from the type, not . is just a monad transformer which has the same type parameters as , along with a new parameter representing the monad that this wraps. Needless to say, these two types are equivalent:\n\nWhen you inspect the types of functions in the package, bare this in mind to help understand what you are dealing with! Every rule is of a similar type, though the return value varies from rule to rule. , for instance, returns an array of matches. Have a look yourself in ghci!\n\nAnyway, now we have defined , We can use it with our function as so:\n\nBecause we have used , we require there to be at least one letter followed by zero or more spaces and finally at least one number. Our rule then wraps these into a tuple for us (but could instead return them packaged in a custom type or any other arrangement).\n\nSay we have a number of these letter/digit pairs, separated by some separator, for example a comma. In this case, we might want to parse them all into a list of tuples of the type seen above. Let's define another rule to parse our separator:\n\nI have once again added an explicit type signature. I do this because, when written independent of any usage in my test file, Haskell cannot determine which types things will be. Note that the only thing returned is the last line, which has a type matching our signature; values from any other parsing done prior to this return are ignored. We could also add an explicit at the end, but returns this anyway.\n\nThis rule matches zero or more spaces, followed by a comma, followed by zero or more spaces. Given that we don't care about grabbing and returning any of the values parsed by these rules, we can desugar the above into a one liner:\n\nNow we have , and , each made from smaller parsing rules. In the same way that we have created them, we can now combine our new rules to create ever larger rules. I'll start with a more verbose rule, built from things we have learned above:\n\nThis basically uses to parse 0 or more instances of followed by , saving the result of myParser and returning it. Notice that I use do sugar to build up a rule which is then passed as an argument to . This desugars to the following, which makes it more clear that everything inside the block is simply one argument to :\n\nGiven that returns a list of whatever is passed in (see the last bit of its type signature), the result is thus a list of pairs. Let's give it a go:\n\nAs we can see, as a result of using , the parser will happily find no instances, but if the parser successfully begins matching input, failure (for example a missing separator at the end) leads to an error being thrown. As it happens, there are built in functions for parsing the common pattern of items split by some separator.\n\nTakes two arguments, a rule to parse items, and a rule to parse separators. essentially does exactly as above, and expects a string consisting of rule then separator, returning an array of whatever the rule returns:\n\nTakes two arguments as does, but this time does not expect the separator to come after the final instance of the rule we're parsing.\n\nAs such, it does not require a final separator (in fact the presence of one would be an error):\n\nand for matching one of multiple rules\n\nUsing or the shorthand infix operator (also in ) we can present more than one rule to parse, and the first rule that successfully consumes input is used (even if it later fails; a caveat we'll get to). Let's see how this works in practice by removing the need for a separator at the end of our pairs:\n\nNow, our rule will consume many letter-digit pairs, each followed by either the end of file (a rule parsec provides) or our defined separator. This can also be written using the infix operator as:\n\nWhere I make note to import the operator so I don't have to prefix it and make it look ugly in use. Both the infix operator and support as many alternatives as you like, for example or . In either case, the first rule in the sequence to consume some input is the rule that is then used. By accepting either end of file or our custom separator, we no longer require the separator at the end:\n\nThe important thing to remember here is that the first rule that consumes input is the one that is used. This can lead to unexpected failure. Take the following example:\n\nOffhand one might expect the parser to attempt to match \"hello\", and on failure to match that, succeed instead at matching \"howdy\". In actuality the parsing fails entirely:\n\nThis is because on attempting to match the string \"hello\", the rule created from consumes the 'h' successfully, and is thus rule selected for use, before subsequently failing on the next character. Taking another example, this becomes clearer:\n\nHere, the first rule fails before it can successfully consume any input, and so the second rule is selected without issue. By default, Parsec will not \"look ahead\" to see whether a rule matches or not, for performance reasons. The first solution to this (and probably most performant) is to parse any input that is common to either match separately, and then try the rest, avoiding the need to perform any lookahead, for example:\n\nNoting that since we are ignoring the result of the first parser (which consumes the 'h'), we no longer return the full string. This is simple enough to rectify if necessary; we'll turn our inline notation into a more explicit rule to do so:\n\nBy manually deciding what to return from our rule, we can choose to return the correct string by appending our initial char to the rest of the string. Errors are now based on the parts of the string that each rule tries to consume rather than the whole thing, increasing their precision but perhaps at the cost of clarity:\n\nThe first error comes from , the second from the failing . We'll show you how to provide your own custom errors instead if you'd prefer, but first lets look at lookahead as a neater way to parse these strings.\n\nAvoiding lookahead can quickly become unwieldy when rules become more complex. In these cases, we can instruct Parsec to try a rule, and rewind us to the previous state if the rule fails at any point. does just this; it catches any failure and rewinds us. Due to the performance implications, it's best to keep the lookahead bounded to as small a region as possible; the less potential parsing is wrapped in a try function, the better. also suppresses any error messages you would otherwise have been given in the wrapped rule, and so can lead to odd and unhelpful error messages if used inappropriately. That said, when used properly it can be taken advantage of to give better errors. Let's give it a go:\n\nThis results in the correct parsing, and generally clear errors, but since any errors from failure to parse hello are suppressed, errors will only describe a failure to match the choice operator or a failure to match \"howdy\", ignoring the failure to match \"hello\":\n\nCreating your own error messages with\n\nSometimes, often when building up your own rules, you want to use your own custom error messages in case of parsing failure. allows you to attach a custom error to any rule quite simply. Let's see it in action:\n\nHere, we simply attach a new error message to the rule we create from . has the lowest precedence possible, which means that anything else will evaluate first. Attaching a new error message to the end of a chain of rules created with for instance will result in that error being used if all of the rules fail without consuming any input, as then the rule generated from the chain has failed. As soon as a rule consumes input, it becomes up to the error reporting of that rule to describe future failure (unless of course a try block surrounds the rule, which suppresses error messages from it). This basic example illustrates the fact:\n\nIf you'd like to provide a single custom error message for a rule you've created, you can incase the rule in a to catch any errors that would otherwise originate from it, and instead provide your own in the event that the rule fails. Here's a simple example doing just that:\n\nThis is not recommended for more significant rules as it would replace precise error messages from sub rules with some general and less helpful error. However, when building small rules it can be more descriptive to provide your own error over those provides.\n\nThe module imports several functions, mostly infix operators, that can help make your rules more concise and readable in the right situations. It turns out that we've already been using one such operator, , already! Applicative functions often make code shorter, since they are all about being point-free, that is, not making explicit references to the variables being passed around.\n\nLet's convert our original pair parser to applicative style and then run over what each operator does:\n\nLet's run through the main applicative operators one by one and see what they each actually do:\n\nThis operator is essentially . it takes a function on the left and a rule on the right, and applies the function to the result of the rule (provided the rule succeeds, otherwise we get a parse error instead) before returning. If we want to apply the function to more than one argument, we separate the arguments wih <*>. lets run through a couple of examples in ghci:\n\nThe neat thing about this is that you can chain as many arguments as you need to the function by adding 's to the end.\n\nA prefix version of the above, takes x number of subsequent arguments and applies them to the first. Not as flexible as the infix versions but there may be times when it is more readable. Here are the same examples as above with lift:\n\nSometimes you want to run some rules, discarding the results of all but one. These operators each take two rules and return the result of the one that the angle bracket points to. Examples:\n\nThey can also be chained in such a way that you can ignore several rules:\n\nAnd often some in handy when you want to do things like trim whitespace or other bits and pieces from around some piece of information.\n\nRuns the rule on the right, and then returns whatever is on the left if the rule succeeds. Let's see it in action along with some equivalent ways to do the same thing:\n\nAs you can see, there are no shortage of different ways to do things! I can see myself using the more obvious second example most often despite it being a few extra characters from the first, but whatever floats you boat!\n\nMore recently I learned that you can thread state through your parsers as well (thanks Chobbsy!). This is useful for keeping track of things like amount of indentation. Here is a very basic example using state to count a letter:\n\nAs well as getting and setting, we can use to modify state in place. Thus, a simpler version of would be:\n\nIt's worth noting however that being a monad transformer, we also have the option of combining a parser with something like the monad for instance in order to thread state through it in an approach that is more consistent with the monad transformer way of doing things. Using the Monad instead, we would have something like:\n\nWe have wandered through some of the built in functions and rules provided. We've then looked at combining rules to build larger ones, branching by way of allowing a choice of rules, lookahead by way of , and finally adding custom error messages to your rules and had a quick play with maintaining state. Armed with the above, you should be off to a good start!\n\nI encourage you to peruse functions in by importing the module under some alias (or qualified as I have done) and using tab complete to get a list of everything that provides. using on these can provide further insight, and forms the basis for how I explored my way through many of the available functions. The chapter here at Real World Haskell is also excellent and runs through far more substantial examples, though note that some small pieces are a little out of date.\n\nI hope that this has been helpful! If I've left anything out, please leave a comment and let me know!"
    },
    {
        "link": "https://futurelearn.com/info/courses/functional-programming-haskell/0/steps/27222",
        "document": "This screencast by Wim Vanderbauwhede shows step by step how to build a text parser in Haskell using the Parsec library and how to create XML output.\n\nThe aim of this tutorial is to explain step by step how to build a simple parser using the Parsec library.\n\nThe source code can be found on GitHub\n\nThe polymorphic function returns a string representation of any data type that is an instance of the type class . The easiest way to make a data type an instance of type class is using the clause.\n\nIn this tutorial we will show how to create a parser that will parse the output of a derived and return it in XML format.\n\nThe types and are defined as follows:\n\nWe derive using the clause. The compiler will automatically create the show function for this data type. Our parser will parse the output of this automatically derived show function.\n\nThen we create some instances of :\n\nWe can test this very easily:\n\nThis program produces the following output:\n\nThe derived Show format can be summarized as follows:\n\nThe module provides a number of basic parsers. Each of these takes as argument a lexer, generated by using a language definition. Here we use from the module.\n\nIt is convenient to create a shorter name for the predefined parsers you want to use, e.g.\n\nThe function takes the output from (a ) and produces the corresponding XML (also a ). It is composed of the actual parser and the function which applies the parser to a string.\n\nWe define an XML format for a generic Haskell data structure. We use some helper functions to create XML tags with and without attributes.\n\nWe also use some functions to join strings together. From the Prelude we take:\n\nWe also define a function to join strings with newline characters:\n\nThis is identical to from the Prelude, just to illustrate the use of and the module.\n\nCombine all parsers using the choice combinator .\n\nParsec will try all choices in order of occurrence.\n\n Remember that is used to avoid consuming the input.\n• Parsec makes it easy to build powerful text parsers from building blocks using predefined parsers and parser combinators.\n• The basic structure of a Parsec parser is quite generic and reusable\n• The example shows how to parse structured text (output from Show) and generate an XML document containing the same information."
    }
]