[
    {
        "link": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html",
        "document": "As an instance of the class, object inherits from it a collection of generic methods (see below for the full list), and completes them with details specific for this particular distribution.\n\nThe probability density function for is:\n\nfor \\(x \\ge 0\\), \\(a > 0\\). Here \\(\\Gamma(a)\\) refers to the gamma function.\n\nWhen \\(a\\) is an integer, reduces to the Erlang distribution, and when \\(a=1\\) to the exponential distribution.\n\nGamma distributions are sometimes parameterized with two variables, with a probability density function of:\n\nNote that this parameterization is equivalent to the above, with .\n\nThe probability density above is defined in the “standardized” form. To shift and/or scale the distribution use the and parameters. Specifically, is identically equivalent to with . Note that shifting the location of a distribution does not make it a “noncentral” distribution; noncentral generalizations of some distributions are available in separate classes.\n\nCalculate the first four moments: Alternatively, the distribution object can be called (as a function) to fix the shape, location and scale parameters. This returns a “frozen” RV object holding the given parameters fixed. Freeze the distribution and display the frozen :"
    },
    {
        "link": "https://docs.scipy.org/doc/scipy-1.15.2/reference/generated/scipy.stats.gamma.html",
        "document": "As an instance of the class, object inherits from it a collection of generic methods (see below for the full list), and completes them with details specific for this particular distribution.\n\nThe probability density function for is:\n\nfor \\(x \\ge 0\\), \\(a > 0\\). Here \\(\\Gamma(a)\\) refers to the gamma function.\n\nWhen \\(a\\) is an integer, reduces to the Erlang distribution, and when \\(a=1\\) to the exponential distribution.\n\nGamma distributions are sometimes parameterized with two variables, with a probability density function of:\n\nNote that this parameterization is equivalent to the above, with .\n\nThe probability density above is defined in the “standardized” form. To shift and/or scale the distribution use the and parameters. Specifically, is identically equivalent to with . Note that shifting the location of a distribution does not make it a “noncentral” distribution; noncentral generalizations of some distributions are available in separate classes.\n\nCalculate the first four moments: Alternatively, the distribution object can be called (as a function) to fix the shape, location and scale parameters. This returns a “frozen” RV object holding the given parameters fixed. Freeze the distribution and display the frozen :"
    },
    {
        "link": "https://numpy.org/doc/2.2/reference/random/generated/numpy.random.gamma.html",
        "document": "Samples are drawn from a Gamma distribution with specified parameters, (sometimes designated “k”) and scale (sometimes designated “theta”), where both parameters are > 0.\n\nThe probability density for the Gamma distribution is\n\nwhere \\(k\\) is the shape and \\(\\theta\\) the scale, and \\(\\Gamma\\) is the Gamma function.\n\nThe Gamma distribution is often used to model the times to failure of electronic components, and arises naturally in processes for which the waiting times between Poisson distributed events are relevant.\n\nDisplay the histogram of the samples, along with the probability density function:"
    },
    {
        "link": "https://geeksforgeeks.org/scipy-stats-gamma-python",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/69980984/random-samples-from-gamma-distribution-with-two-parameters-python",
        "document": "If I would like to generate 10 random samples from a gamma distribution with (with the following form):\n\nwith alpha = 2 and beta = 3, how would I do it?\n\nThe documentation: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html is a bit unclear to me.\n\nMy guess is that it would be like:\n\nCan anyone verify wether this is correct or provide the correct solution?"
    },
    {
        "link": "https://matplotlib.org/stable/tutorials/pyplot.html",
        "document": "Go to the end to download the full example code.\n\nAn introduction to the pyplot interface. Please also see Quick start guide for an overview of how Matplotlib works and Matplotlib Application Interfaces (APIs) for an explanation of the trade-offs between the supported user APIs.\n\nLines have many attributes that you can set: linewidth, dash style, antialiased, etc; see . There are several ways to set line properties\n• None Use the setter methods of a instance. returns a list of objects; e.g., . In the code below we will suppose that we have only one line so that the list returned is of length 1. We use tuple unpacking with to get the first element of that list:\n• None Use . The example below uses a MATLAB-style function to set multiple properties on a list of lines. works transparently with a list of objects or a single object. You can either use python keyword arguments or MATLAB-style string/value pairs: Here are the available properties. To get a list of settable line properties, call the function with a line or lines as argument"
    },
    {
        "link": "https://matplotlib.org/stable/users/explain/quick_start.html",
        "document": "Go to the end to download the full example code.\n\nThis tutorial covers some basic usage patterns and best practices to help you get started with Matplotlib.\n\nMatplotlib graphs your data on s (e.g., windows, Jupyter widgets, etc.), each of which can contain one or more , an area where points can be specified in terms of x-y coordinates (or theta-r in a polar plot, x-y-z in a 3D plot, etc.). The simplest way of creating a Figure with an Axes is using . We can then use to draw some data on the Axes, and to display the figure: # Plot some data on the Axes. Depending on the environment you are working in, can be left out. This is for example the case with Jupyter notebooks, which automatically show all figures created in a code cell.\n\nYou can open multiple Figures with multiple calls to or . By keeping the object references you can add Artists to either Figure. Multiple Axes can be added a number of ways, but the most basic is as used above. One can achieve more complex layouts, with Axes objects spanning columns or rows, using . Matplotlib has quite sophisticated tools for arranging Axes: See Arranging multiple Axes in a Figure and Complex and semantic figure composition (subplot_mosaic)."
    },
    {
        "link": "https://stackoverflow.com/questions/15415455/plotting-probability-density-function-by-sample-with-matplotlib",
        "document": "If you want to plot a distribution, and you know it, define it as a function, and plot it as so:\n\nIf you don't have the exact distribution as an analytical function, perhaps you can generate a large sample, take a histogram and somehow smooth the data:\n\nYou can increase or decrease (smoothing factor) within the function call to increase or decrease smoothing. For example, using the two you get:"
    },
    {
        "link": "https://matplotlib.org",
        "document": "You can help by answering questions on discourse , reporting a bug or requesting a feature on GitHub , or improving the documentation and code !\n\nMatplotlib is a community project maintained for and by its users\n\nMatplotlib is the result of development efforts by John Hunter (1968–2012) and the project's many contributors.\n\nIf Matplotlib contributes to a project that leads to a scientific publication, please acknowledge this work by citing the project!"
    },
    {
        "link": "https://stackoverflow.com/questions/66237525/plotting-probability-density-function-in-python",
        "document": "I want to plot two probability density functions (pdf) based on values of a certain column in a dataframe. The first one for all the values that correspond to rows with and second one where .\n\nMy attempt is below, but as you can see the curves do not look like a pdf (the max value is 0 and they are not confined to X axis in range 0-1 and 5-6. I assume I can get something close by playing around with bw factor, but I am looking for a one-liner that just figures out right params and plots a pdf(including figuiring out the right X-axis start/end to use). Is there any such built in function that does this. If not, would appreciate some pointers on how to build something like this."
    },
    {
        "link": "https://tariqueakhtar-39220.medium.com/central-limit-theorem-and-its-application-using-python-67b374d557dc",
        "document": "The Central Limit Theorem (CLT) states that, given a sufficiently large sample size from a population with a finite level of variance, the mean of all samples from the same population will be approximately equal to the mean of the population. Furthermore, the distribution of all sample means will be approximately normally distributed, regardless of the distribution of the population from which the samples were taken.\n\nThe Central Limit Theorem has many important applications in statistics, such as hypothesis testing and confidence intervals. For example, if we want to estimate the mean weight of a certain type of fruit, we can take a sample of fruits, calculate the mean weight of the sample, and use the CLT to construct a confidence interval for the true mean weight of all fruits of that type.\n\nIn Python, we can use the library to generate random samples and calculate their means, as well as the library to visualize the distribution of sample means.\n\nFirst, let’s generate a random sample of 100 fruits from a population with a mean weight of 10 grams and a standard deviation of 2 grams:\n\nLet’s see the distribution of this population_weights.\n\nNext, we can use a for loop to generate 1000 samples of size 100 from the population and calculate the mean weight of each sample:\n\nFinally, we can use to visualize the distribution of sample means:\n\nThe resulting histogram should be approximately normally distributed, with a mean close to 10 grams (the population mean) and a standard deviation (known as the standard error) that is equal to the standard deviation of the population divided by the square root of the sample size.\n\nThe above is a simple application of CLT, it can be applied in many other more complex situations in statistics, finance, and other fields. The theorem is a fundamental concept that is widely used in statistical analysis, and it is important to have a good understanding of it in order to correctly analyze data and draw meaningful conclusions."
    },
    {
        "link": "https://geeksforgeeks.org/python-central-limit-theorem",
        "document": "Central Limit Theorem (CLT) is a foundational principle in statistics, and implementing it using Python can significantly enhance data analysis capabilities. Statistics is an important part of data science projects. We use statistical tools whenever we want to make any inference about the population of the dataset from a sample of the dataset, gather information from the dataset, or make any assumption about the parameter of the dataset.\n\nSuppose we are sampling from a population with a finite mean and a finite standard deviation (sigma). Then Mean and standard deviation of the sampling distribution of the sample mean can be given as: \n\n\\qquad \\qquad \\mu_{\\bar{X}}=\\mu \\qquad \\sigma_{\\bar{X}}=\\frac{\\sigma}{\\sqrt{n}}\n\nWhere [Tex]\\bar{X} [/Tex] represents the sampling distribution of the sample mean of size n each, [Tex]\\mu [/Tex] and [Tex]\\sigma [/Tex] are the mean and standard deviation of the population respectively. \n\nThe distribution of the sample tends towards the normal distribution as the sample size increases.\n\nWe can use central limit theorem for various purposes in data science project some the key uses are listed below\n• Population Parameter Estimation – We can use CLT to estimate the parameters of the population like population mean or population proportion based on a sampled data.\n• Hypothesis testing – CLT can be used for various hypothesis assumptions tests as It helps in constructing test statistics, such as the z-test or t-test, by assuming that the sampling distribution of the test statistic is approximately normal.\n• Confidence interval – Confidence interval plays a very important role in defang the range in which the population parameter lies. CLT plays a very crucial role in determining the confidence interval of these population parameter.\n• Sampling Techniques – sampling technique help in collecting representative samples and generalize the findings to the larger population. The CLT supports various sampling techniques used in survey sampling and experimental design.\n• Simulation and Monte Carlo Methods – This methods involve generating random samples from known distributions to approximate the behavior of complex systems or estimate statistical quantities. CLT plays a very key role in the simulation and monte carlo methods.\n\nWe will generate random numbers from -40 to 40 and and collect their mean in a list. we will iteratively perform his operation for different count of numbers and we will plot their sampling distribution.\n• Import Required Libraries : First, import the necessary libraries. We’ll use\n• Generate Population Data : Generate data from a non-normal distribution to demonstrate CLT. Here, we’ll generate samples from an exponential distribution, which is not normally distributed.\n• Define Sample Size and Number of Samples ) and the number of samples ( ) you want to generate from the population. Larger sample sizes typically show better convergence to a normal distribution.\n• Calculate Sample Means : Generate multiple samples from the population data, calculate the mean for each sample, and store these sample means.\n• Plot Sample Means Distribution : Visualize the distribution of sample means and compare it with a normal distribution using a\n• Interpretation : The histogram demonstrate how the distribution of sample means approaches a normal distribution, regardless of the underlying population distribution. As the sample size ( ) increases or as more samples ( ) are taken, the sample means’ distribution more closely resembles a normal distribution, validating the Central Limit Theorem.\n\nIt is evident from the graphs that as we keep on increasing the sample size from 1 to 100 the histogram tends to take the shape of a normal distribution.\n\nGenerally, the Central Limit Theorem is used when the sample size is fairly big, usually larger than or equal to 30. In some cases even if the sample size is less than 30 central limit theorem still holds but for this the population distribution should be close to normal or symmetric.\n\nCentral Limit Theorem is a cornerstone of statistical theory, facilitating accurate analysis and interpretation of data across various disciplines. By understanding its principles and applications, researchers and analysts can make informed decisions and draw reliable conclusions from sample data, ensuring robust and effective decision-making in practice.\n\nImplementing the Central Limit Theorem in Python enhances data analysis capabilities by leveraging powerful libraries like NumPy, SciPy, and Matplotlib. By applying CLT principles, data scientists can derive meaningful insights, perform rigorous hypothesis testing, and make informed decisions based on robust statistical inference.\n\nWhat are the three rules of central limit theorem?\n\nWhat is the main principle of central limit theorem?\n\nProblem 1: Generate a population with a skewed distribution (e.g., log-normal distribution). Calculate the sample means for different sample sizes (e.g., 10, 30, 50) and plot the histograms. Observe how the distribution of sample means changes with sample size.\n\nProblem 2: Use real-world data (e.g., housing prices) to demonstrate the Central Limit Theorem. Take multiple samples from the dataset, calculate their means, and plot the distribution of these sample means.\n\nProblem 3: Conduct a simulation to show how the Central Limit Theorem can be used to approximate the probability of certain outcomes in a given dataset. For example, simulate rolling a die multiple times and calculate the distribution of the average outcome."
    },
    {
        "link": "https://medium.com/@charlesdirenzo/central-limit-theorem-confidence-intervals-and-hypothesis-testing-fb104723e572",
        "document": "This is one part of many where I’ll be going through some older programming exercises for my statistics classes. After this I’ll go through my Machine Learning course, and then the code that I wrote for my operations research course. Most of this code is a bit on the older side, but it should still work and I’ve been informed that these assignments have since been rewritten, so this won’t interfere with any current assignments.\n\nIn this article we’ll go over central limit theorem, confidence intervals, and hypothesis testing. We’ll discuss why these are important concepts to know and go over a few exercises that demonstrate them in R.\n\nThe Law of Large Numbers (LLN) states that as the size of a sample drawn from a population increases, the sample mean approaches the population mean. In simple terms, the larger the sample, the closer the sample mean gets to the actual average of the entire population. The LLN provides assurance that you’re getting closer to the “true” population parameter as you collect more data.\n\nThe Central Limit Theorem (CLT) states that the distribution of the sum (or average) of a large number of independent, identically distributed random variables approaches a normal distribution, regardless of the shape of the original distribution. This is crucial because the normal distribution is well-understood, allowing for easier statistical analysis. The CLT enables us to make statistical inferences about population parameters using sample statistics, and it forms the theoretical basis for many statistical procedures, like hypothesis testing and confidence interval estimation. Essentially, the theorem allows us to apply statistical techniques that are based on normal distribution assumptions to samples from populations that are not normally distributed, provided the sample size is large enough.\n\nHow are these related? The LLN tells us that sample averages converge to the population average as the sample size grows, while the CLT tells us about the shape of the distribution that these sample averages will form. Both theorems work in tandem when you’re trying to make inferences about a population based on a sample.\n\nNow let’s put some of this into practice with R.\n\nThe following block of code will produce a simulated sampling distribution of the sample mean for 1,000 samples of size 23 drawn from an exponential distribution and then make a histogram of the results. Some R programmers insist that for loops should be avoided because they are slow, but we’re aiming for transparency at the moment, not efficiency.\n\nThis and the following blocks of code will demonstrate the central limit theorem and show that the distribution of the sample means becomes increasingly normal as the sample size increases.\n\nNow we’ll draw a random sample of 1000 observations from the exponential distribution and make a histogram to illustrate just how skewed the exponential distribution is.\n\nNow we’ll use the block of code that we used at the top and use it to simulate the sampling distribution of sample means for 1000 samples of size 10. After the histogram, we’ll use qqnorm() to make a normal probability plot of sampleStat. We’ll add a fit line to the plot with qqline().\n\nBoth the histogram and the normal probability plot show that, for samples of size 10 from the exponential distribution, the sample mean distribution is skewed to the right, but far less skewed than the exponential distribution itself."
    },
    {
        "link": "https://geeksforgeeks.org/central-limit-theorem-in-machine-learning",
        "document": "The Central Limit Theorem states that \"if we repeatedly take random samples from any population the distribution of their means will tend to be normal (bell-shaped) as the sample size increases and it is independent of the population's original distribution.\" for making statistical inferences about data.\n\nBy calculate sample means these averages will tend to form a normal distribution. This normality holds true as long as the sample size is sufficiently large typically n≥30 providing the foundation for making inferences about populations even when we don’t have access to all the data.\n\nImagine you have a population where the data follows some random variable and this population has:\n• Mean the average of the population\n\nlet’s say we take a sample of size from this population and calculate its mean then the Z-Score is given below:\n\nAs the sample size increases the distribution of sample means becomes more concentrated around and resembles a normal distribution.\n\nBefore we dive into how the theorem works it is important to know a few things that need to be true for it to work properly. These assumptions ensure that we can make valid conclusions from our sample data.\n\n1. Random Sampling: Samples must be randomly selected from the population. Imagine you're trying to estimate the average income in a city. If you only survey a few wealthy neighborhoods your results will be biased. Randomly selecting people from all neighborhoods ensures your sample represents the whole city.\n\n2. Independence: Samples must be independent of each other meaning that the outcome of one sample does not influence the outcome of another. For example: If you're measuring the test scores of students in a class one student’s score shouldn’t affect another.\n\n3. Sufficiently large Sample Size: Generally a sample size of 30 or more is considered large enough for it to work because the theorem says that with a sample size of 30 or more the sample mean tends to follow a normal distribution. Suppose if you're testing the average time people spend on a website a sample of 30 people may be enough but a larger sample e.g., 100 or more would provide even more reliable insights.\n\n4. Finite Mean and Variance: Population data must have a finite mean and variance. This means that the data shouldn't have extreme values or be infinite. Let's assume if the average income of people in a city is a finite number. But you were measuring something with no upper limit like the amount of money people could theoretically earn it would be difficult to apply the CLT in that case because the data could go on forever.\n\nBy ensuring these assumptions are met. The theorem can be used to draw conclusions about the population.\n\nSkewness measures the asymmetry of a dataset’s distribution. In an ideal normal distribution, data is symmetrically spread around the mean. However, real-world data often leans towards one side, creating skewness.\n\n1. Positive Skew (Right-Skewed) : Tail extends towards higher values, meaning a few significantly large values pull the distribution to the right. In such cases, the mean is greater than the median, which is greater than the mode. A classic example is income distribution, where a small number of people earn exceptionally high salaries while most earn moderate amounts.\n\n2. Negative Skew (Left-Skewed: Tail that extends towards lower values. Here, the mean is less than the median, which is less than the mode. A common example is exam scores, where most students perform well, but a few score much lower, pulling the distribution towards the left.\n\n3. Zero Skew (Symmetric Distribution: Data is evenly distributed around the mean, creating a balanced shape. In this case, the mean, median, and mode are approximately equal. Height distribution in a large population often follows this pattern, where most individuals fall near the average height with fewer extreme values on either side.\n\nThe Central Limit Theorem (CLT) is a powerful concept in statistics but it come with some limitations that are important to understand. Let's understand them one by one:\n• Assumption: It works best when the sample size is large.\n• Limitation: If the sample size is too small and the data is highly skewed, the sample mean may not accurately represent the population. In such cases the distribution of the sample means might not appear normal and lead to unreliable conclusions.\n• Assumption: The Central Limit Theorem assumes that the sample can be drawn from any population whether it's normally distributed or not.\n• Limitation: If the population is highly skewed or has very large variations, a much larger sample size is required for the CLT to produce reliable results. In such cases, using smaller samples may lead to inaccurate conclusions, as the averages may not approximate a normal distribution effectively.\n• Assumption: It says that the data points in the sample must be independent of each other.\n• Limitation : This doesn’t hold true in cases where data points are dependent such as in time series data where each point is related to previous ones or when sampling without replacement like drawing cards from a deck. In these situations the samples are connected which can lead to unexpected behavior in standard errors and provide incorrect conclusions about the data.\n• Assumption: The sample is drawn randomly from the population.\n• Limitation : A key limitation arises when the sampling method introduces bias such as through non-random sampling. In such cases the sample may not accurately represent the population and lead to incorrect or skewed results. This violates the conditions of the CLT and can compromise the reliability of any conclusions.\n• Assumption: The population from which the sample is drawn must have finite variance.\n• Limitation : if the population has infinite variance. In such cases the CLT does not apply means that the sample mean will not converge to a normal distribution regardless of how large the sample size is.\n• Assumption: CLT suggests that with a large enough sample the sampling distribution will approach normality.\n• Limitation : In this the rate of convergence to normality can be slow, especially for highly skewed or heavy-tailed distributions. In these situations, a very large sample size may be necessary to achieve a reasonable approximation to a normal distribution.\n\nLet’s imagine you’re a data scientist working for an agricultural company. Your company owns a large orchard and you’re tasked with estimating the average weight of apples produced in the orchard. However it's impractical to weigh every apple in the orchard because there are millions of them.\n\nSo you decide to take a sample of apples measure their weight and use this sample to estimate the average weight for the entire orchard. But how can you be sure that your sample accurately represents the whole orchard especially when you can’t measure every apple?\n\nYou want to estimate the average weight of apples but instead of weighing all the apples you’re going to randomly select a small sample. It helps us make reliable predictions about the population (all the apples) by using sample data (the apples we measured).\n• None You first take a random sample of 100 apples from the orchard and calculate the average weight.\n• None You repeat this process multiple times—say 1000 times—each time taking 100 apples and then calculate the average weight for each of these samples.\n• None The theorem tells us that the distribution of these sample means will form a normal distribution even if the original data is not normally distributed.\n\nThis is important because with a large enough sample size it allow us to make inferences about the population’s average weight even if we don’t have access to all the data.\n\nStep 3: How to Implement the CLT\n\nNow that we understand the scenario let’s walk through the steps of how to implement the Central Limit Theorem using Python.\n\nWe'll model the weights of apples and take multiple samples calculate their means and observing how the sample means follow a normal distribution.\n\nwe conclude that the Central Limit Theorem allows us to estimate the average weight of apples in the orchard with high accuracy using sample data. Even with non-normally distributed data the sample means approximate a normal distribution\n\nThe Central Limit Theorem (CLT) is widely used:\n• Model Evaluation and Confidence Intervals : The Central Limit Theorem (CLT) helps us understand how much we can trust a model's predictions. After training a model on a sample we can use this to create confidence intervals. These intervals show how much uncertainty is in the model’s predictions. When we collect more data these intervals get smaller means we can be more confident that the model's predictions are accurate.\n• A/B Testing in Product Development : A/B testing is a common practice in product development especially for evaluating different versions of a feature or webpage. It helps to ensure that when data is skewed or non-normally distributed the sample means from repeated experiments will approach a normal distribution as the sample size increases.\n• Error Estimation and Uncertainty : In machine learning predictions made by a model on unseen data often come with some uncertainty. By applying CLT we can estimate the error distribution of a model’s predictions and calculate standard errors which helps in identify the uncertainty in model forecasts.\n• Bootstrapping for Model Evaluation : Bootstrapping means taking many random samples from your data to understand how your model might perform. The Central Limit Theorem helps us treat these samples like they come from the same population. This helps in building more reliable models and in estimating model performance metrics like the mean squared error (MSE) or confidence intervals for model coefficients.\n• Feature Importance and Stability : When selecting features CLT helps us see if the importance of features across different samples of data. For example with methods like random forests it check if the feature rankings stay the same as we use more data and make sure we choose the most reliable features."
    },
    {
        "link": "https://openstax.org/books/principles-data-science/pages/4-1-statistical-inference-and-confidence-intervals",
        "document": "Data scientists interested in inferring the value of a population truth or parameter such as a population mean or a population proportion turn to inferential statistics. A data scientist is often interested in making generalizations about a population based on the characteristics derived from a sample; inferential statistics allows a data scientist to draw conclusions about a population based on sample data. In addition, inferential statistics is used by data scientists to assess model performance and compare different algorithms in machine learning application. Inferential statistics provides methods for generating predictive forecasting models, and this allows data scientists to generate predictions and trends to assist in effective and accurate decision-making. In this section, we explore the use of confidence intervals, which is used extensively in inferential statistical analysis.\n\nWe begin by introducing confidence intervals, which are used to estimate the range within which a population parameter is likely to fall. We discuss estimation of parameters for the mean both when the standard deviation is known and when it is not known. We discuss sample size determination, building on the sampling techniques presented in Collecting and Preparing Data. And we discuss bootstrapping, a method used to construct a confidence interval based on repeated sampling. Hypothesis Testing will move on to hypothesis testing, which is used to make inferences about unknown parameters.\n\nA point estimate is a single value that is used to estimate a population parameter. For example, a sample mean is a point estimate that is representative of the true population mean in that the sample mean is used as an estimate for the unknown population mean. When researchers collect data from a sample to make inferences about a population, they calculate a point estimate based on the observed sample data. (See Survey Design and Implementation for coverage of sampling techniques.) The point estimate serves as the best guess or approximation for the parameter's actual value. A confidence interval estimates the range within which a population parameter, such as a mean or a proportion, is likely to fall. The confidence interval provides a level of uncertainty associated with the estimate and is expressed as a range of values. A confidence interval will provide both a lower and an upper bound for the population parameter, where the point estimate is centered within the interval. Table 4.1 describes the point estimates and corresponding population parameters for the mean and proportion. Population mean is denoted as . Point estimate is the sample mean . Population Parameters and Point Estimates for Mean and Proportion Let’s say a researcher is interested in estimating the mean income for all residents of California. Since it is not feasible to collect data from every resident of California, the researcher selects a random sample of 1,000 residents and calculates the sample mean income for these 1,000 people. This average income estimate from the sample then provides an estimate for the population mean income of all California residents. The sample mean is chosen as the point estimate for the population mean in that the sample mean provides the most unbiased estimate for the population mean. In the same way, the sample proportion provides the most unbiased estimate for the population proportion. An unbiased estimator is a statistic that provides an accurate estimate for the corresponding population parameter without overestimating or underestimating the parameter. In order to calculate a confidence interval, two quantities are needed: As noted, the point estimate is a single number that is used to estimate the population parameter. The margin of error (usually denoted by E) provides an indication of the maximum error of the estimate. The margin of error can be viewed as the maximum distance around the point estimate where the population parameter exists based on a specified confidence. Once the point estimate and margin of error are determined, the confidence interval is calculated as follows: The margin of error reflects the desired confidence level of the researcher. The confidence level is the probability that the interval estimate will contain the population parameter, given that the estimation process on the parameter is repeated over and over. Confidence levels typically range from 80% to 95% confidence. In addition to the confidence level, both the variability of the sample and the sample size will affect the margin of error. Once the confidence interval has been calculated, a concluding statement is made that reflects both the confidence level and the unknown population parameter. The concluding statement indicates that there is a certain level of confidence that the population parameter is contained within the bounds of the confidence interval. As an example, consider a data scientist who is interested in forecasting the median income for all residents of California. Income data is collected from a sample of 1,000 residents, and the median income level is $68,500. Also assume the margin of error for a 95% confidence interval is $4,500 (more details on the calculation of margin of error are provided next). Now the data scientist can construct a 95% confidence interval to forecast income levels as follows: The data scientist can then state the following conclusion: There is 95% confidence that the forecast for median income for all residents of California is between $64,000 and $73,000. A medical researcher is interested in estimating the population mean age for patients suffering from arthritis. A sample of 100 patients is taken, and the sample mean is determined to be 64 years old. Assume that the corresponding margin of error for a 95% confidence interval is calculated to be 4 years. Calculate the confidence interval and provide a conclusion regarding the confidence interval. The point estimate is the sample mean, which is 64, and the margin of error is given as 4 years. The 95% confidence interval can be calculated as follows: Concluding statement:\n\n The researcher is 95% confident that the mean population age for patients suffering from arthritis is contained in the interval from 60 to 68 years of age. Distribution for the Mean A researcher takes repeated samples of size 1,000 from the residents of New York to collect data on mean income of residents of New York. For each sample of size 1,000, we can calculate a sample mean, . If the researcher were to take 50 such samples (each of sample size 1,000), a series of sample means can be calculated: A probability distribution based on all possible random samples of a certain size from a population—or sampling (or sample) distribution—can then be analyzed. For example, we can calculate the mean of these sample means, and we can calculate the standard deviation of these sample means. There are two important properties of the sample distribution of these sample means:\n• The mean of the sample means (notated as ) is equal to the population mean .\n• The standard deviation of the sample means (notated as ) is equal to the population standard deviation divided by the square root of the sample size . The central limit theorem describes the relationship between the sample distribution of sample means and the underlying population. This theorem is an important tool that allows data scientists and researchers to use sample data to generate inferences for population parameters.\n• If random samples are taken from any population with mean and standard deviation, where the sample size is at least 30, then the distribution of the sample means approximates a normal (bell-shaped) distribution.\n• If random samples are taken from a population that is normally distributed with mean and standard deviation , then the distribution of the sample means approximates a normal (bell-shaped) distribution for any sample size An economist takes random samples of size 50 to estimate the mean salary of chemical engineers. Assume that the population mean salary is $85,000 and the population standard deviation is $9,000. It is unknown if the distribution of salaries follows a normal distribution. Calculate the mean and standard deviation of the sampling distribution of the sample means and comment on the shape of the distribution for the sample means. The mean of the sample means is equal to the population mean . The standard deviation of the sample means is equal to the population standard deviation divided by the square root of the sample size . Since the sample size of 50 is greater than 30, the distribution of the sample means approximates a normal (bell-shaped) distribution. Confidence Interval for the Mean When the Population Standard Deviation Is Known Although in many situations the population standard deviation is unknown, in some cases a reasonable estimate for the population standard deviation can be obtained from past studies or historical data. Here is what is needed to calculate the confidence interval for the mean when the population standard deviation is known:\n• A random sample is selected from the population.\n• The sample size is at least 30, or the underlying population is known to follow a normal distribution.\n• The population standard deviation ( ) is known. Once these conditions are met, the margin of error is calculated according to the following formula: is called the critical value of the normal distribution and is calculated as the z-score, which includes the area corresponding to the confidence level between and . For example, for a 95% confidence interval, the corresponding critical value is since an area of 0.95 under the normal curve is contained between the z-scores of and . Note: is called the standard error of the mean. Typical values of for various confidence levels are shown in Table 4.2. Typical Values of for Various Confidence Levels Graphically, the critical values can be marked on the normal distribution curve, as shown in Figure 4.2. (See Discrete and Continuous Probability Distributions for a review of the normal distribution curve.) Figure 4.2 is an example for a 95% confidence interval where the area of 0.95 is centered as the area under the standard normal curve showing the values of and . Since the standard normal curve is symmetric, and since the total area under the curve is known to be 1, the area in each of the two tails can be calculated to be 0.025. Thus, the critical value is that z-score which cuts off an area of 0.025 in the upper tail of the standard normal curve. Critical Values of z Marked on Standard Normal Curve for 95% Confidence Interval Once the margin of error has been calculated, the confidence interval can be calculated as follows: Note that this is the general format for constructing any confidence interval, namely the margin of error is added and subtracted to the sample statistic to generate the upper and lower bounds of the confidence interval, respectively. A sample statistic describes some aspect of the sample, such as a sample mean or sample proportion. A college professor collects data on the amount of time spent on homework assignments per week for a sample of 50 students in a statistics course. From the sample of 50 students, the mean amount of time spent on homework assignments per week is 12.5 hours. The population standard deviation is known to be 6.3 hours. The professor would like to forecast the amount of time spent on homework in future semesters. Create a forecasted confidence interval using both a 90% and 95% confidence interval and provide a conclusion regarding the confidence interval. Also compare the widths of the two confidence intervals. Which confidence interval is wider? For a 90% confidence interval, the corresponding critical value is . The margin of error is calculated as follows: The 90% confidence interval is calculated as follows: The college professor can forecast with 90% confident that the mean amount of time spent on homework assignments in a future semester is the interval from 11.03 to 13.97 hours per week. For a 95% confidence interval, the corresponding critical value is . The margin of error is calculated as follows: The 95% confidence interval is calculated as follows: The college professor can forecast with 95% confident that the mean amount of time spent on homework assignments in a future semester is the interval from 10.75 to 14.25 hours per week. The 90% confidence interval extends from 11.03 to 13.97.\n\n The 95% confidence interval extends from 10.75 to 14.25. Notice that the 95% confidence interval is wider. If the confidence level is increased, with all other parameters held constant, we should expect that the confidence interval will become wider. Another way to consider this: the wider the confidence interval, the more likely the interval is to contain the true population mean. This makes intuitive sense in that if you want to be more certain that the true value of the parameter is within an interval, then the interval needs to be wider to account for a larger range of potential values. As an analogy, consider a person trying to catch a fish. The wider the net used, the higher the probability of catching a fish. You might notice that the sample size is located in the denominator of the formula for margin of error. This indicates that as the sample size increases, the margin of error will decrease, which will then result in a narrower confidence interval. On the other hand, as the sample size decreases, the margin of error increases and the confidence interval becomes wider. Confidence Interval for the Mean When the Population Standard Deviation Is Unknown A confidence interval can still be determined when the population standard deviation is unknown by calculating a sample standard deviation based on the sample data. This is actually the more common application of confidence intervals. Here are the conditions required to use this procedure:\n• A random sample is selected from the population.\n• The sample size is at least 30, or the underlying population is known to follow a normal distribution.\n• The population standard deviation ( ) is unknown; the sample standard deviation can be calculated. Once these requirements are met, the margin of error (E) is calculated according to the following formula: is called the critical value of the t-distribution. The t-distribution is a bell-shaped, symmetric distribution similar to the normal distribution, though the t-distribution has “thicker tails” as compared to the normal distribution. The comparison of the normal and t-distribution curves is shown in Figure 4.3. The t-distribution is actually a family of curves, determined by a parameter called degrees of freedom (df), where df is equal to . The critical value is similar to a z-score and specifies the area under the t-distribution curve corresponding to the confidence level between and . Values of can be obtained using either a look-up table or technology. For example, for a 95% confidence interval and sample size of 30, the corresponding critical value is since an area of 0.95 under the t-distribution curve is contained between the t-scores of and . df is degrees of freedom, where . Typical values of for various confidence levels and degrees of freedom are shown in Table 4.3. Typical Values of for Various Confidence Levels and Degrees of Freedom Note that Python can be used to calculate these critical values. Python provides a function called that generates the value of the t-distribution corresponding to a specified area under the t-distribution curve and specified degrees of freedom. This function is part of the scipy.stats library. The syntax for the function is:\n\n For example, for a 95% confidence interval, there is an area of 0.95 centered under the t-distribution curve, which leaves a remaining area of 0.05 for the two tails of the distribution, which implies an area of 0.025 in each tail. To find a critical value corresponding to the upper 0.025 area, note that the area to the left of this critical value will then be 0.975. Here is an example of Python code to generate the critical value for a 95% confidence interval and 15 degrees of freedom. The resulting output will look like this: Once the margin of error is calculated, the confidence interval is formed in the same way as the previous section, namely: A company’s human resource administrator wants to estimate the average commuting distance for all 5,000 employees at the company. Since it is impractical to collect commuting distances from all 5,000 employees, the administrator decides to sample 16 employees and collects data on commuting distance from each employee in the sample. The sample data indicates a sample mean of 15.8 miles with a standard deviation of 3.2 miles. Calculate a 99% confidence interval and provide a conclusion regarding the confidence interval. Since the sample size is 16 employees, the degrees of freedom is one less than the sample size, which is . For a 99% confidence interval, the corresponding critical value is . The margin of error is calculated as follows: The 99% confidence interval is calculated as follows: The administrator can be 99% confident that the true population mean commuting distance for all employees is between 13.44 to 18.16 miles. We can also calculate a confidence interval for a population proportion based on the use of sample data. The basis for the confidence interval will be the application of a normal approximation to the binomial distribution. Recall from Discrete and Continuous Probability Distributions that a binomial distribution is a probability distribution for a discrete random variable where there are only two possible outcomes of an experiment. A proportion measures the number of successes in a sample. For example, if 10 survey respondents out of 50 indicate they are planning to travel internationally within the next 12 months, then the proportion is 10 out of 50, which is 0.2, or 20%. Note that the term success does not necessarily imply a positive outcome. For example, a researcher might be interested in the proportion of smokers among U.S. adults, and the number of smokers would be considered the number of successes. Some terminology will be helpful:\n\n represents the population proportion, which is typically unknown.\n\n represents the sample proportion.\n\n represents the number of successes in the sample.\n\n represents the sample size. Here are the requirements to use this procedure:\n• A random sample is selected from the population.\n• Verify that the normal approximation to the binomial distribution is appropriate by ensuring that both and are both at least 5, where represents the sample proportion. The sample proportion is calculated as the number of successes divided by the sample size: When these requirements are met, the margin of error (E) for a confidence interval for proportions is calculated according to the following formula: is called the critical value of the normal distribution and is calculated as the z-score, which includes the area corresponding to the confidence level between and . Once the margin of error is calculated, the confidence interval is formed in the same way as the previous section. In this case, the point estimate is the sample proportion : A medical researcher wants to know if there has been a statistically significant change in the proportion of smokers from five years ago, when the proportion of adult smokers in the United States was approximately 28%. The researcher selects a random sample of 1,500 adults, and of those, 360 respond that they are smokers. Calculate a 95% confidence interval for the true population proportion of adults who smoke. Also, determine if there has been a statistically significant change in the proportion of smokers as compared to five years ago. For a 95% confidence interval, the corresponding critical value is . Start off by calculating the sample proportion : Verify that the normal approximation to the binomial distribution is appropriate by ensuring that both and are both at least 5, where represents the sample proportion. For this example, , and . Both of these results are at least 5, which verifies that the normal approximation to the binomial distribution is appropriate. The margin of error is then calculated as follows: The 95% confidence interval is calculated as follows: The researcher can be 95% confident that the true population of adult smokers in the United States is between 0.218 and 0.262, which can also be written as 21.8% to 26.2%. Since this 95% confidence interval excludes the previous value of 28% from five years ago, the researcher can state that there has been a statistically significant decrease in the proportion of smokers as compared to five years ago.\n\nWhen collecting sample data in order to construct a confidence interval for a mean or proportion, how does the researcher determine the optimal sample size? Too small of a sample size may lead to a wide confidence interval that is not very useful. Too large of a sample size can result in wasted resources if a smaller sample size would be sufficient. Sampling is covered in some depth in Handling Large Datasets. Here, we review methods to determine minimum sample size requirements when constructing confidence intervals for means or proportions. Note that the desired margin of error plays a key role in this sample size determination. Sample Size for Confidence Interval for the Mean When determining a confidence interval for a mean, a researcher can use the margin of error formula for the mean; solving this formula algebraically for the sample size ( ), the following minimum sample size formula can be used to determine the minimum sample size needed to achieve a certain margin of error: Sample size formula for confidence interval for mean: Where:\n\n is the critical value of the normal distribution.\n\n is the population standard deviation.\n\n is the desired margin of error. Note that for sample size calculations, sample size results are rounded up to the next higher whole number. For example, if the formula previously shown results in a sample size of 59.2, then the sample size will be rounded to 60. A benefits analyst is interested in a 90% confidence interval for the mean salary for chemical engineers. What sample size should the analyst use if a margin of error of $1,000 is desired? Assume a population standard deviation of $8,000. For a 90% confidence interval, the corresponding critical value is . The population standard deviation is given as $8,000, and the margin of error is given as $1,000. Round to the next higher whole number, so the desired sample size is 174. The analyst should target a sample size of 174 chemical engineers for a salary-related survey in order to them calculate a 90% confidence interval for the mean salary of chemical engineers. In Example 4.5, how would the researcher know the optimal sample size to use to collect sample data on the proportion of adults who are smokers? The margin of error formula can be used to derive the sample size for a proportion as follows: Where:\n\n is the critical value of the normal distribution.\n\n is the sample proportion.\n\n is the desired margin of error. Notice in this formula, it’s assumed that some prior estimate for the sample proportion is available, perhaps from historical data or previous studies. If a prior estimate for the sample proportion is not available, then use the value of 0.5 for . When determining sample size needed for a confidence interval for a proportion: If a prior estimate for the sample proportion is available, then that prior estimate should be utilized.\n• If a prior estimate for the sample proportion is not available, then use . Political candidate Smith is planning a survey to determine a 95% confidence interval for the proportion of voters who plan to vote for Smith. How many people should be surveyed? Assume a margin of error is 3%.\n• Assume there is no prior estimate for the proportion of voters who plan to vote for candidate Smith.\n• Assume that from prior election results, approximately 42% of people previously voted for candidate Smith.\n• For a 95% confidence interval, the corresponding critical value is . The margin of error is specified as 0.03. Since a prior estimate for the sample proportion is unknown, use a value of 0.5 for . Using the sample size formula: Round to the next higher whole number, so the desired sample size is 1,068 people to be surveyed.\n• Since a prior estimate for the sample proportion is available, use the value of 0.42 for . Using the sample size formula: Round to the next higher whole number, so the desired sample size is 1,040 people to be surveyed. Note that having a prior estimate of the sample proportion will result in a smaller sample size requirement. This is a general conclusion, so if a researcher has a prior estimate for the population proportion, this will result in a smaller sample size requirement as compared to the situation where no prior estimate is available.\n\nIn the earlier discussion of confidence intervals, certain requirements needed to be met in order to use the estimation methods. For example, when calculating the confidence interval for a mean, this requirement was indicated:\n• The sample size is at least 30, or the underlying population is known to follow a normal distribution. When calculating the confidence interval for a proportion, this requirement was noted:\n• Verify that the normal approximation to the binomial distribution is appropriate by ensuring that both and are at least 5. What should a researcher do when these requirements are not met? Fortunately, there is another option called “bootstrapping” that can be used to find confidence intervals when the underlying distribution is unknown or if one of the conditions is not met. This bootstrapping method involves repeatedly taking samples with replacement. Sampling with replacement means that when a sample is selected, the sample is replaced back in the dataset before selecting the next sample. For example, a casino dealer plans to take a sample of two cards from a standard 52-card deck. If the sampling is to be done with replacement, then after selecting the sample of the first card, this card is then returned to the deck before the second card is selected. This implies that the same data value can appear multiple times in a given sample since once a data value is selected for the sample, it is replaced back in the dataset and is eligible to be selected again for the sample. For example, a researcher who has collected 100 samples to use for inference may want to estimate a confidence interval for the population mean. The researcher can then resample, with replacement, from this limited set of 100 observations. They would repeatedly sample 100 and calculate the sample mean. These means would then represent the distribution of means (remember the discussion of sampling distribution). Typically, for bootstrapping, repeated samples are taken hundreds or thousands of times and then the sample mean is calculated for each sample. The term “bootstrapping” comes from the old saying “pull yourself up by your bootstraps” to imply a task that was accomplished without any outside help. In the statistical sense, bootstrapping refers to the ability to estimate parameters based solely on one sample of data from the population without any other assumptions. The bootstrapping method is considered a nonparametric method since the technique makes no assumptions about the probability distribution from which the data are sampled. (Parametric methods, by contrast, assume a specific form for the underlying distribution and require estimating parameters.) Since bootstrapping requires a large number of repeated samples, software (such as Excel, Python, or R) is often used to automate the repetitive sampling procedures and construct the confidence intervals. The bootstrapping procedure for confidence interval estimation for a mean or proportion is as follows:\n• Start out with a random sample of size . Collect many random bootstrap samples of size —for example, hundreds or thousands of such samples. Keep in mind that the sampling is done with replacement.\n• For each sample, calculate the sample statistic, which is the sample mean or the sample proportion .\n• Rank order the sample statistics from smallest to largest.\n• For a 95% confidence interval, find the percentiles P and P in the ranked data. These values establish the 95% confidence interval.\n• For a 90% confidence interval, find the percentiles P and P . These values establish the 90% confidence interval. A college administrator is developing new marketing materials to increase enrollment at the college, and the administrator is interested in a 90% confidence interval for the mean age of students attending the college. The administrator believes the underlying distribution of ages is skewed (does not follow a normal distribution), so a bootstrapping method will be used to construct the confidence interval. The administrator selects a random sample of 20 students and records their ages as shown in Table 4.4. Use the bootstrapping method to construct the 90% confidence interval by taking repeated samples of size 10. Ages of 20 Randomly Selected Students from One College For the bootstrapping process, we will form samples of size 10 for convenience. Note that the sampling is with replacement, so once an age is selected, the age is returned back to the dataset and then that particular age might be selected again as part of the sample. Imagine forming hundreds or thousands of such samples, each of sample size 10. For each one of these samples, calculate the sample mean (see column labeled “Sample Means”). In the example shown in Figure 4.4, only 20 samples are taken for space considerations since showing the results of thousands of samples is not practical for this text. However, typically a bootstrapping process involves many such samples—on the order of thousands of samples—made possible by using software (such as Excel, Python, or R). Next, sort the sample means from smallest to largest (see column labeled “Sorted Sample Means”). The last step is to calculate the 90% confidence interval based on these sorted sample means. For a 90% confidence interval, find the percentiles P and P . These values establish the 90% confidence interval. To find the 5th percentile (P ), multiply the percentile (5%) times the number of data values, which is 20. The result is 1, so to find the 5th percentile, add the first and second data values together and divide by 2. For this example, add 21.9 to 22.2 and divide by 2. The result is 22.05. To find the 95th percentile (P ), multiply the percentile (95%) times the number of data values, which is 20. The result is 19, so to find the 95th percentile, add the 19th and 20th data values together and divide by 2. For this example, add 28.5 to 28.6 and divide by 2. The result is 28.6. Based on the bootstrapping method, the 90% confidence interval is (22.05, 28.6). Python provides a function called as part of the library to automate this bootstrapping process. Within the function, the user can specify the number of resamples to be performed as part of the bootstrapping process. In this example, we will use 50,000 samples by way of the _resample parameter. The resulting output will look like this: Using 50,000 bootstrapped samples, a 95% confidence interval is generated as (22.45, 26.7). A number of websites and resources, such as the online textbook Online Statistics Education: A Multimedia Course of Study (http://onlinestatbook.com/) (project leader: David M. Lane, Rice University), allow the user to simulate confidence intervals for means or proportions using simulated sample data. It is useful to observe how many intervals contain an assumed value for the population mean or population proportion. The Stapplet website provides a similar tool."
    }
]