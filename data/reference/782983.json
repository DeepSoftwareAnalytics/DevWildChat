[
    {
        "link": "https://geeksforgeeks.org/new-and-delete-operators-in-cpp-for-dynamic-memory",
        "document": "Dynamic memory allocation in C/C++ refers to performing memory allocation manually by a programmer. Dynamically allocated memory is allocated on Heap, and non-static and local variables get memory allocated on Stack (Refer to Memory Layout C Programs for details).\n• None One use of dynamically allocated memory is to allocate memory of variable size, which is not possible with compiler allocated memory except for\n• None The most important use is the flexibility provided to programmers. We are free to allocate and deallocate memory whenever we need it and whenever we don’t need it anymore. There are many cases where this flexibility helps. Examples of such cases are\n\nHow is it different from memory allocated to normal variables?\n\nFor normal variables like “int a”, “char str[10]”, etc, memory is automatically allocated and deallocated. For dynamically allocated memory like “int *p = new int[10]”, it is the programmer’s responsibility to deallocate memory when no longer needed. If the programmer doesn’t deallocate memory, it causes a memory leak (memory is not deallocated until the program terminates).\n\nHow is memory allocated/deallocated in C++? \n\nC uses the malloc() and calloc() function to allocate memory dynamically at run time and uses a free() function to free dynamically allocated memory. C++ supports these functions and also has two operators new and delete, that perform the task of allocating and freeing the memory in a better and easier way.\n\nThe new operator denotes a request for memory allocation on the Free Store. If sufficient memory is available, a new operator initializes the memory and returns the address of the newly allocated and initialized memory to the pointer variable.\n\nSyntax to use new operator\n\nHere, the pointer variable is the pointer of type data-type. Data type could be any built-in data type including array or any user-defined data type including structure and class. \n\nExample:\n\nInitialize memory: We can also initialize the memory for built-in data types using a new operator. For custom data types, a constructor is required (with the data type as input) for initializing the value. Here’s an example of the initialization of both data types :\n\nAllocate a block of memory: a new operator is also used to allocate a block(an array) of memory of type data type.\n\nwhere size(a variable) specifies the number of elements in an array.\n\nDynamically allocates memory for 10 integers continuously of type int and returns a pointer to the first element of the sequence, which is assigned top(a pointer). p[0] refers to the first element, p[1] refers to the second element, and so on.\n\nNormal Array Declaration vs Using new \n\nThere is a difference between declaring a normal array and allocating a block of memory using new. The most important difference is, that normal arrays are deallocated by the compiler (If the array is local, then deallocated when the function returns or completes). However, dynamically allocated arrays always remain there until either they are deallocated by the programmer or the program terminates.\n\nWhat if enough memory is not available during runtime? \n\nIf enough memory is not available in the heap to allocate, the new request indicates failure by throwing an exception of type std::bad_alloc, unless “nothrow” is used with the new operator, in which case it returns a NULL pointer (scroll to section “Exception handling of new operator” in this article). Therefore, it may be a good idea to check for the pointer variable produced by the new before using its program.\n\nSince it is the programmer’s responsibility to deallocate dynamically allocated memory, programmers are provided delete operator in C++ language.\n\nHere, the pointer variable is the pointer that points to the data object created by new.\n\nTo free the dynamically allocated array pointed by pointer variable, use the following form of delete:\n\nTime Complexity: O(n), where n is the given memory size.\n• None Quiz on new and delete"
    },
    {
        "link": "https://cplusplus.com/doc/tutorial/dynamic",
        "document": "std; main () { i,n; * p; cout << \"How many numbers would you like to type? \" ; cin >> i; p= (nothrow) [i]; (p == ) cout << \"Error: memory could not be allocated\" ; { (n=0; n<i; n++) { cout << ; cin >> p[n]; } cout << ; (n=0; n<i; n++) cout << p[n] << ; [] p; } 0; }\n\nHow many numbers would you like to type? 5 Enter number : 75 Enter number : 436 Enter number : 1067 Enter number : 8 Enter number : 32 You have entered: 75, 436, 1067, 8, 32,"
    },
    {
        "link": "https://learn.microsoft.com/en-us/cpp/cpp/new-and-delete-operators?view=msvc-170",
        "document": "C++ supports dynamic allocation and deallocation of objects using the and operators. These operators allocate memory for objects from a pool called the free store (also known as the heap). The operator calls the special function , and the operator calls the special function .\n\nFor a list of the library files in the C Runtime Library and the C++ Standard Library, see CRT Library Features.\n\nThe compiler translates a statement such as this one into a call to the function :\n\nIf the request is for zero bytes of storage, returns a pointer to a distinct object. That is, repeated calls to return different pointers.\n\nIf there's insufficient memory for the allocation request, throws a exception. Or, it returns if you've used the placement form , or if you've linked in non-throwing support. For more information, see Allocation failure behavior.\n\nThe two scopes for functions are described in the following table.\n\nThe first argument of must be of type , and the return type is always .\n\nThe global function is called when the operator is used to allocate objects of built-in types, objects of class type that don't contain user-defined functions, and arrays of any type. When the operator is used to allocate objects of a class type where an is defined, that class's is called.\n\nAn function defined for a class is a static member function (which can't be virtual) that hides the global function for objects of that class type. Consider the case where is used to allocate and set memory to a given value:\n\nThe argument supplied in parentheses to is passed to as the argument. However, the global function is hidden, causing code such as the following to generate an error:\n\nThe compiler supports member array and operators in a class declaration. For example:\n\nThe function in the C++ Standard Library supports the behavior specified in the C++ standard since C++98. When there's insufficient memory for an allocation request, throws a exception.\n\nOlder C++ code returned a null pointer for a failed allocation. If you have code that expects the non-throwing version of , link your program with . The file replaces global with a version that returns if an allocation fails. no longer throws . For more information about and other linker option files, see Link options.\n\nYou can't mix code that checks for exceptions from global with code that checks for null pointers in the same application. However, you can still create class-local that behaves differently. This possibility means the compiler must act defensively by default and include checks for null pointer returns in calls. For more information on a way to optimize these compiler checks, see .\n\nThe way you test for a failed allocation from a expression depends on whether you use the standard exception mechanism, or you use a return. Standard C++ expects an allocator to throw either or a class derived from . You can handle such an exception as shown in this sample:\n\nWhen you use the form of , you can test for an allocation failure as shown in this sample:\n\nYou can test for a failed memory allocation when you've used file to replace global as shown here:\n\nYou can provide a handler for failed memory allocation requests. It's possible to write a custom recovery routine to handle such a failure. It could, for example, release some reserved memory, then allow the allocation to run again. For more information, see .\n\nMemory that is dynamically allocated using the operator can be freed using the operator. The delete operator calls the function, which frees memory back to the available pool. Using the operator also causes the class destructor (if one exists) to be called.\n\nThere are global and class-scoped functions. Only one function can be defined for a given class; if defined, it hides the global function. The global function is always called for arrays of any type.\n\nThe global function. Two forms exist for the global and class-member functions:\n\nOnly one of the preceding two forms can be present for a given class. The first form takes a single argument of type , which contains a pointer to the object to deallocate. The second form, sized deallocation, takes two arguments: the first is a pointer to the memory block to deallocate, and the second is the number of bytes to deallocate. The return type of both forms is ( can't return a value).\n\nThe intent of the second form is to speed up searching for the correct size category of the object to delete. This information often isn't stored near the allocation itself, and is likely uncached. The second form is useful when an function from a base class is used to delete an object of a derived class.\n\nThe function is static, so it can't be virtual. The function obeys access control, as described in Member-Access Control.\n\nThe following example shows user-defined and functions designed to log allocations and deallocations of memory:\n\nThe preceding code can be used to detect \"memory leakage\", that is, memory that's allocated on the free store but never freed. To detect leaks, the global and operators are redefined to count allocation and deallocation of memory.\n\nThe compiler supports member array and operators in a class declaration. For example:"
    },
    {
        "link": "https://programiz.com/cpp-programming/memory-management",
        "document": "C++ allows us to allocate the memory dynamically in run time. This is known as dynamic memory allocation.\n\nIn other programming languages such as Java and Python, the compiler automatically manages the memories allocated to variables. But this is not the case in C++.\n\nIn C++, we need to deallocate the dynamically allocated memory manually after we have no use for the variable.\n\nWe can allocate and then deallocate memory dynamically using the and operators respectively.\n\nWe can use the expression to allocate memory in run time. For example,\n\nHere, we have dynamically allocated memory for an variable using the expression.\n\nNotice that we have used the pointer to allocate the memory dynamically. This is because the expression returns the address of the memory location.\n\nWe can also allocate memory and initialize the value in the same step as:\n\nUsing this syntax avoids uninitialized pointers. Uninitialized pointers may cause undefined behavior when dereferenced. So this is the preferred syntax.\n\nThe syntax for using the expression is:\n\nOnce we no longer need to use a variable that we have declared dynamically, we can deallocate the memory occupied by the variable.\n\nFor this, we can use the expression. It returns the memory back to the operating system. This is known as memory deallocation.\n\nThe syntax for expression is:\n\nLet's look at an example.\n\nHere, we have dynamically allocated memory for an variable using the pointer .\n\nAfter printing the contents of , we deallocated the memory using .\n\nIt is a good practice to set pointer to after deallocating the memory to avoid undefined behavior if the pointer is dereferenced.\n\nNote: Not deallocating memory properly can cause memory leaks which in turn causes the program to consume a large amount of memory. Proper use of the expression is essential to prevent memory leaks and ensure efficient memory management.\n\nIn this program, we dynamically allocated memory to two variables of and types.\n\nAfter assigning values to them and printing them, we finally deallocate the memories using the expression.\n\nExample 2: C++ new and delete Expression for Arrays\n\nIn this program, we have asked the user to enter the number of students and store it in the variable.\n\nThen, we have allocated the memory dynamically for the array using .\n\nWe enter data into the array (and later print them) using pointer notation.\n\nAfter we no longer needed the array, we deallocated the array memory using the code:\n\nNotice the use of after . We use the square brackets in order to denote that the memory deallocation is that of an array.\n\nExample 3: C++ new and delete Expression for Objects\n\nIn this program, we have created a class that has a private variable .\n\nWe have initialized to in the default constructor and printed its value with the function .\n\nIn , we have created a object using the expression and use the pointer to point to its address.\n\nThe moment the object is created, the constructor initializes to .\n\nWe then call the function using the code:\n\nNotice the arrow operator . This operator is used to access class members using pointers.\n\nDynamic memory allocation has several advantages, such as:\n• Flexibility: Dynamic memory allocation allows us to allocate memory as needed during runtime. This flexibility is useful when the size of data structures is not known at compile time or when the size changes during program execution.\n• Data Structures: Data structures such as linked lists, trees, graphs, and resizable arrays (vectors in C++) often need to allocate memory dynamically to accommodate varying amounts of data.\n• Resource Management: We can allocate memory when needed and deallocate it when it's no longer required. This leads to better resource utilization.\n• Dynamic Arrays: In languages like C++, static arrays have fixed sizes determined at compile time. Dynamic memory allocation allows us to create arrays whose size can be determined during runtime."
    },
    {
        "link": "https://stackoverflow.com/questions/73646474/problem-in-deleting-the-elements-of-an-array-allocated-with-new",
        "document": "All of or and even C's do exactly the same in respect to memory: requesting a fix block of memory from the operating system.\n\nYou cannot split up this block of memory and return it partially to the operating system, that's simply not supported, thus you cannot a single element from the array either. Only all or none…\n\nIf you need to remove an element from an array all you can do is copy the subsequent elements one position towards front, overwriting the element to delete and additionally remember how many elements actually are valid – the elements at the end of the array stay alive!\n\nIf these need to be destructed immediately you might call the destructor explicitly – and then assure that it isn't called again on an already destructed element when ing the array (otherwise undefined behaviour!) – ending in not calling and at all but instead , placement for each element, any pointer to any element created that way and finally explicitly calling the constructor when needed.\n\nSounds like much of a hassle, doesn't it? Well, there's doing all this stuff for you! You should this one it instead…\n\nSide note: You could get similar behaviour if you use an array of pointers; you then can – and need to – maintain (i.e. control its lifetime) each object individually. Further disadvantages are an additional level of pointer indirection whenever you access the array members and the array members indeed being scattered around the memory (though this can turn into an advantage if you need to move objects around your array and copying/moving objects is expensive – still you would to prefer a , of pointers this time, though; insertions, deletions and managing the pointer array itself, among others, get much safer and much less complicated)."
    },
    {
        "link": "https://stackoverflow.com/questions/5426946/persistence-strategies-for-main-memory-b-trees",
        "document": "I am trying to develop a main-memory index for key-value pairs using C++. I need to make sure the index is recoverable after a crash. I am using a CSB+-Tree implementation (BSD Licence) that I found here The main challenge I am facing is maintaining the parent-child relation data after re-instantiating the nodes. I have searched for various strategies to save and recover a \"tree-structure\" to/from a disk. Some of them are:\n• Saving the nodes objects in Pre-order and writing NULLS for empty child pointers.\n• Giving IDS to nodes and saving the ID of a node instead of the pointer while writing to disk and then resolving the pointers during re-instantiation using the IDs.\n• Using file-offset values (addresses in physical memory) rather than main memory addresses of the child nodes while saving. This might mean I have to save from leaf-up.\n\nI have also looked at a couple of serialization libraries. Google ProtocolBuffers and Boost Serialization.\n\nNow the \"Nodes\" in the implementations have a number of pointer variables.Some of these are pointers to other nodes, while others are pointers to \"key values\". The code below is simplified version to retain the essence.\n\nI was thinking of writing the entry values directly into the data for the nodehead rather than saving a link.And giving each NodeHead instance an ID and use that to maintain the \"children\" relationships. I would like some advice if this can be done in a better way."
    },
    {
        "link": "https://reddit.com/r/cpp/comments/3ban6p/c_persistent_data_structures_implementation",
        "document": "Hi everyone! Does anyboody know a good C++ (fully) persistent data structures implementation (sets, trees, queues)? I want to use it in my little pet project. Hope they will be fast, tested and easy to use. :)\n\nUPD: By saying \"persistent\", I mean https://en.wikipedia.org/wiki/Persistent_data_structure"
    },
    {
        "link": "https://en.wikipedia.org/wiki/Persistent_data_structure",
        "document": "Data structure that always preserves the previous version of itself when it is modified\n\nIn computing, a persistent data structure or not ephemeral data structure is a data structure that always preserves the previous version of itself when it is modified. Such data structures are effectively immutable, as their operations do not (visibly) update the structure in-place, but instead always yield a new updated structure. The term was introduced in Driscoll, Sarnak, Sleator, and Tarjan's 1986 article.[1]\n\nA data structure is partially persistent if all versions can be accessed but only the newest version can be modified. The data structure is fully persistent if every version can be both accessed and modified. If there is also a meld or merge operation that can create a new version from two previous versions, the data structure is called confluently persistent. Structures that are not persistent are called ephemeral.[2]\n\nThese types of data structures are particularly common in logical and functional programming,[2] as languages in those paradigms discourage (or fully forbid) the use of mutable data.\n\nIn the partial persistence model, a programmer may query any previous version of a data structure, but may only update the latest version. This implies a linear ordering among each version of the data structure.[3] In the fully persistent model, both updates and queries are allowed on any version of the data structure. In some cases the performance characteristics of querying or updating older versions of a data structure may be allowed to degrade, as is true with the rope data structure.[4] In addition, a data structure can be referred to as confluently persistent if, in addition to being fully persistent, two versions of the same data structure can be combined to form a new version which is still fully persistent.[5]\n\nA type of data structure where user may query any version of the structure but may only update the latest version.\n\nAn ephemeral data structure can be converted to partially persistent data structure using a few techniques.\n\nOne of the technique is by using randomized version of Van Emde Boas Tree which is created using dynamic perfect hashing. This data structure is created as follows:\n• A stratified tree with m elements is implemented using dynamic perfect hashing.\n• The tree is pruned by dividing the m elements into buckets of size log(log n) such that the elements of bucket 1 is smaller than the elements of bucket 2 and so on.\n• The maximal element in each bucket is stored in the stratified tree and each bucket is stored in the structure as an unordered linked list.\n\nThe size of this data structure is bounded by the number of elements stored in the structure that is O(m). The insertion of a new maximal element is done in constant O(1) expected and amortized time. Finally query to find an element can be done in this structure in O(log(log n)) worst-case time.[6]\n\nOne method for creating a persistent data structure is to use a platform provided ephemeral data structure such as an array to store the data in the data structure and copy the entirety of that data structure using copy-on-write semantics for any updates to the data structure. This is an inefficient technique because the entire backing data structure must be copied for each write, leading to worst case O(n·m) performance characteristics for m modifications of an array of size n.[citation needed]\n\nThe fat node method is to record all changes made to node fields in the nodes themselves, without erasing old values of the fields. This requires that nodes be allowed to become arbitrarily “fat”. In other words, each fat node contains the same information and pointer fields as an ephemeral node, along with space for an arbitrary number of extra field values. Each extra field value has an associated field name and a version stamp which indicates the version in which the named field was changed to have the specified value. Besides, each fat node has its own version stamp, indicating the version in which the node was created. The only purpose of nodes having version stamps is to make sure that each node only contains one value per field name per version. In order to navigate through the structure, each original field value in a node has a version stamp of zero.\n\nWith using fat node method, it requires O(1) space for every modification: just store the new data. Each modification takes O(1) additional time to store the modification at the end of the modification history. This is an amortized time bound, assuming modification history is stored in a growable array. At access time, the right version at each node must be found as the structure is traversed. If \"m\" modifications were to be made, then each access operation would have O(log m) slowdown resulting from the cost of finding the nearest modification in the array.\n\nWith the path copying method a copy of all nodes is made on the path to any node which is about to be modified. These changes must then be cascaded back through the data structure: all nodes that pointed to the old node must be modified to point to the new node instead. These modifications cause more cascading changes, and so on, until the root node is reached.\n\nWith m modifications, this costs O(log m) additive lookup time. Modification time and space are bounded by the size of the longest path in the data structure and the cost of the update in the ephemeral data structure. In a Balanced Binary Search Tree without parent pointers the worst case modification time complexity is O(log n + update cost). However, in a linked list the worst case modification time complexity is O(n + update cost).\n\nDriscoll, Sarnak, Sleator, Tarjan came up[1] with a way to combine the techniques of fat nodes and path copying, achieving O(1) access slowdown and O(1) modification space and time complexity.\n\nIn each node, one modification box is stored. This box can hold one modification to the node—either a modification to one of the pointers, or to the node's key, or to some other piece of node-specific data—and a timestamp for when that modification was applied. Initially, every node's modification box is empty.\n\nWhenever a node is accessed, the modification box is checked, and its timestamp is compared against the access time. (The access time specifies the version of the data structure being considered.) If the modification box is empty, or the access time is before the modification time, then the modification box is ignored and only the normal part of the node is considered. On the other hand, if the access time is after the modification time, then the value in the modification box is used, overriding that value in the node.\n\nModifying a node works like this. (It is assumed that each modification touches one pointer or similar field.) If the node's modification box is empty, then it is filled with the modification. Otherwise, the modification box is full. A copy of the node is made, but using only the latest values. The modification is performed directly on the new node, without using the modification box. (One of the new node's fields is overwritten and its modification box stays empty.) Finally, this change is cascaded to the node's parent, just like path copying. (This may involve filling the parent's modification box, or making a copy of the parent recursively. If the node has no parent—it's the root—it is added the new root to a sorted array of roots.)\n\nWith this algorithm, given any time t, at most one modification box exists in the data structure with time t. Thus, a modification at time t splits the tree into three parts: one part contains the data from before time t, one part contains the data from after time t, and one part was unaffected by the modification.\n\nTime and space for modifications require amortized analysis. A modification takes O(1) amortized space, and O(1) amortized time. To see why, use a potential function ϕ, where ϕ(T) is the number of full live nodes in T . The live nodes of T are just the nodes that are reachable from the current root at the current time (that is, after the last modification). The full live nodes are the live nodes whose modification boxes are full.\n\nEach modification involves some number of copies, say k, followed by 1 change to a modification box. Consider each of the k copies. Each costs O(1) space and time, but decreases the potential function by one. (First, the node to be copied must be full and live, so it contributes to the potential function. The potential function will only drop, however, if the old node isn't reachable in the new tree. But it is known that it isn't reachable in the new tree—the next step in the algorithm will be to modify the node's parent to point at the copy. Finally, it is known that the copy's modification box is empty. Thus, replaced a full live node has been replaced with an empty live node, and ϕ goes down by one.) The final step fills a modification box, which costs O(1) time and increases ϕ by one.\n\nPutting it all together, the change in ϕ is Δϕ =1 − k. Thus, the algorithm takes O(k +Δϕ)= O(1) space and O(k +Δϕ +1) = O(1) time\n\nPath copying is one of the simple methods to achieve persistency in a certain data structure such as binary search trees. It is nice to have a general strategy for implementing persistence that works with any given data structure. In order to achieve that, we consider a directed graph G. We assume that each vertex v in G has a constant number c of outgoing edges that are represented by pointers. Each vertex has a label representing the data. We consider that a vertex has a bounded number d of edges leading into it which we define as inedges(v). We allow the following different operations on G.\n• CREATE-NODE(): Creates a new vertex with no incoming or outgoing edges.\n• CHANGE-EDGE( , , ): Changes the th edge of to point to\n• CHANGE-LABEL( , ): Changes the value of the data stored at to\n\nAny of the above operations is performed at a specific time and the purpose of the persistent graph representation is to be able to access any version of G at any given time. For this purpose we define a table for each vertex v in G. The table contains c columns and rows. Each row contains in addition to the pointers for the outgoing edges, a label which represents the data at the vertex and a time t at which the operation was performed. In addition to that there is an array inedges(v) that keeps track of all the incoming edges to v. When a table is full, a new table with rows can be created. The old table becomes inactive and the new table becomes the active table.\n\nA call to CREATE-NODE creates a new table and set all the references to null\n\nIf we assume that CHANGE-EDGE(v, i, u) is called, then there are two cases to consider.\n• There is an empty row in the table of the vertex : In this case we copy the last row in the table and we change the th edge of vertex to point to the new vertex\n• Table of the vertex is full: In this case we need to create a new table. We copy the last row of the old table into the new table. We need to loop in the array inedges( ) in order to let each vertex in the array point to the new table created. In addition to that, we need to change the entry in the inedges(w) for every vertex such that edge exists in the graph .\n\nIt works exactly the same as CHANGE-EDGE except that instead of changing the ith edge of the vertex, we change the ith label.\n\nIn order to find the efficiency of the scheme proposed above, we use an argument defined as a credit scheme. The credit represents a currency. For example, the credit can be used to pay for a table. The argument states the following:\n• The creation of one table requires one credit\n• Each call to CREATE-NODE comes with two credits\n• Each call to CHANGE-EDGE comes with one credit\n\nThe credit scheme should always satisfy the following invariant: Each row of each active table stores one credit and the table has the same number of credits as the number of rows. Let us confirm that the invariant applies to all the three operations CREATE-NODE, CHANGE-EDGE and CHANGE-LABEL.\n• CREATE-NODE: It acquires two credits, one is used to create the table and the other is given to the one row that is added to the table. Thus the invariant is maintained.\n• CHANGE-EDGE: There are two cases to consider. The first case occurs when there is still at least one empty row in the table. In this case one credit is used to the newly inserted row. The second case occurs when the table is full. In this case the old table becomes inactive and the credits are transformed to the new table in addition to the one credit acquired from calling the CHANGE-EDGE. So in total we have credits. One credit will be used for the creation of the new table. Another credit will be used for the new row added to the table and the credits left are used for updating the tables of the other vertices that need to point to the new table. We conclude that the invariant is maintained.\n• CHANGE-LABEL: It works exactly the same as CHANGE-EDGE.\n\nAs a summary, we conclude that having calls to CREATE_NODE and calls to CHANGE_EDGE will result in the creation of tables. Since each table has size without taking into account the recursive calls, then filling in a table requires where the additional d factor comes from updating the inedges at other nodes. Therefore, the amount of work required to complete a sequence of operations is bounded by the number of tables created multiplied by . Each access operation can be done in and there are m edge and label operations, thus it requires . We conclude that There exists a data structure that can complete any n sequence of CREATE-NODE, CHANGE-EDGE and CHANGE-LABEL in .\n\nOne of the useful applications that can be solved efficiently using persistence is the Next Element Search. Assume that there are n non intersecting line segments that don't cross each other that are parallel to the x-axis. We want to build a data structure that can query a point p and return the segment above p (if any). We will start by solving the Next Element Search using the naïve method then we will show how to solve it using the persistent data structure method.\n\nWe start with a vertical line segment that starts off at infinity and we sweep the line segments from the left to the right. We take a pause every time we encounter an end point of these segments. The vertical lines split the plane into vertical strips. If there are n line segments then we can get vertical strips since each segment has 2 end points. No segment begins and ends in the strip. Every segment either it doesn't touch the strip or it completely crosses it. We can think of the segments as some objects that are in some sorted order from top to bottom. What we care about is where the point that we are looking at fits in this order. We sort the endpoints of the segments by their x coordinate. For each strip , we store the subset segments that cross in a dictionary. When the vertical line sweeps the line segments, whenever it passes over the left endpoint of a segment then we add it to the dictionary. When it passes through the right endpoint of the segment, we remove it from the dictionary. At every endpoint, we save a copy of the dictionary and we store all the copies sorted by the x coordinates. Thus we have a data structure that can answer any query. In order to find the segment above a point p, we can look at the x coordinate of p to know which copy or strip it belongs to. Then we can look at the y coordinate to find the segment above it. Thus we need two binary searches, one for the x coordinate to find the strip or the copy, and another for the y coordinate to find the segment above it. Thus the query time takes . In this data structure, the space is the issue since if we assume that we have the segments structured in a way such that every segment starts before the end of any other segment, then the space required for the structure to be built using the naïve method would be . Let us see how we can build another persistent data structure with the same query time but with a better space.\n\nWe can notice that what really takes time in the data structure used in the naïve method is that whenever we move from a strip to the next, we need to take a snap shot of whatever data structure we are using to keep things in sorted order. We can notice that once we get the segments that intersect , when we move to either one thing leaves or one thing enters. If the difference between what is in and what is in is only one insertion or deletion then it is not a good idea to copy everything from to . The trick is that since each copy differs from the previous one by only one insertion or deletion, then we need to copy only the parts that change. Let us assume that we have a tree rooted at T. When we insert a key k into the tree, we create a new leaf containing k. Performing rotations to rebalance the tree will only modify the nodes of the path from k to T. Before inserting the key k into the tree, we copy all the nodes on the path from k to T. Now we have 2 versions of the tree, the original one which doesn't contain k and the new tree that contains k and whose root is a copy of the root of T. Since copying the path from k to T doesn't increase the insertion time by more than a constant factor then the insertion in the persistent data structure takes time. For the deletion, we need to find which nodes will be affected by the deletion. For each node v affected by the deletion, we copy the path from the root to v. This will provide a new tree whose root is a copy of the root of the original tree. Then we perform the deletion on the new tree. We will end up with 2 versions of the tree. The original one which contains k and the new one which doesn't contain k. Since any deletion only modifies the path from the root to v and any appropriate deletion algorithm runs in , thus the deletion in the persistent data structure takes . Every sequence of insertion and deletion will cause the creation of a sequence of dictionaries or versions or trees where each is the result of operations . If each contains m elements, then the search in each takes . Using this persistent data structure we can solve the next element search problem in query time and space instead of . Please find below the source code for an example related to the next search problem.\n\nPerhaps the simplest persistent data structure is the singly linked list or cons-based list, a simple list of objects formed by each carrying a reference to the next in the list. This is persistent because the tail of the list can be taken, meaning the last k items for some k, and new nodes can be added in front of it. The tail will not be duplicated, instead becoming shared between both the old list and the new list. So long as the contents of the tail are immutable, this sharing will be invisible to the program.\n\nMany common reference-based data structures, such as red–black trees,[7] stacks,[8] and treaps,[9] can easily be adapted to create a persistent version. Some others need slightly more effort, for example: queues, dequeues, and extensions including min-deques (which have an additional O(1) operation min returning the minimal element) and random access deques (which have an additional operation of random access with sub-linear, most often logarithmic, complexity).\n\nThere also exist persistent data structures which use destructive[clarification needed] operations, making them impossible to implement efficiently in purely functional languages (like Haskell outside specialized monads like state or IO), but possible in languages like C or Java. These types of data structures can often be avoided with a different design. One primary advantage to using purely persistent data structures is that they often behave better in multi-threaded environments.\n\nSingly linked lists are the bread-and-butter data structure in functional languages.[10] Some ML-derived languages, like Haskell, are purely functional because once a node in the list has been allocated, it cannot be modified, only copied, referenced or destroyed by the garbage collector when nothing refers to it. (Note that ML itself is not purely functional, but supports non-destructive list operations subset, that is also true in the Lisp (LISt Processing) functional language dialects like Scheme and Racket.)\n\nConsider the two lists:\n\nThese would be represented in memory by:\n\nwhere a circle indicates a node in the list (the arrow out representing the second element of the node which is a pointer to another node).\n\nNow concatenating the two lists:\n\nresults in the following memory structure:\n\nNotice that the nodes in list have been copied, but the nodes in are shared. As a result, the original lists ( and ) persist and have not been modified.\n\nThe reason for the copy is that the last node in (the node containing the original value ) cannot be modified to point to the start of , because that would change the value of .\n\nConsider a binary search tree,[10] where every node in the tree has the recursive invariant that all subnodes contained in the left subtree have a value that is less than or equal to the value stored in the node, and subnodes contained in the right subtree have a value that is greater than the value stored in the node.\n\nFor instance, the set of data\n\nmight be represented by the following binary search tree:\n\nA function which inserts data into the binary tree and maintains the invariant is:\n\nThe following configuration is produced:\n\nNotice two points: first, the original tree ( ) persists. Second, many common nodes are shared between the old tree and the new tree. Such persistence and sharing is difficult to manage without some form of garbage collection (GC) to automatically free up nodes which have no live references, and this is why GC is a feature commonly found in functional programming languages.\n\nGitHub repo containing implementations of persistent BSTs using Fat Nodes, Copy-on-Write, and Path Copying Techniques.\n\nTo use the persistent BST implementations, simply clone the repository and follow the instructions provided in the README file.[11]\n\nA persistent hash array mapped trie is a specialized variant of a hash array mapped trie that will preserve previous versions of itself on any updates. It is often used to implement a general purpose persistent map data structure.[12]\n\nHash array mapped tries were originally described in a 2001 paper by Phil Bagwell entitled \"Ideal Hash Trees\". This paper presented a mutable Hash table where \"Insert, search and delete times are small and constant, independent of key set size, operations are O(1). Small worst-case times for insert, search and removal operations can be guaranteed and misses cost less than successful searches\".[13] This data structure was then modified by Rich Hickey to be fully persistent for use in the Clojure programming language.[14]\n\nConceptually, hash array mapped tries work similar to any generic tree in that they store nodes hierarchically and retrieve them by following a path down to a particular element. The key difference is that Hash Array Mapped Tries first use a hash function to transform their lookup key into a (usually 32 or 64 bit) integer. The path down the tree is then determined by using slices of the binary representation of that integer to index into a sparse array at each level of the tree. The leaf nodes of the tree behave similar to the buckets used to construct hash tables and may or may not contain multiple candidates depending on hash collisions.[12]\n\nMost implementations of persistent hash array mapped tries use a branching factor of 32 in their implementation. This means that in practice while insertions, deletions, and lookups into a persistent hash array mapped trie have a computational complexity of O(log n), for most applications they are effectively constant time, as it would require an extremely large number of entries to make any operation take more than a dozen steps.[15]\n\nHaskell is a pure functional language and therefore does not allow for mutation. Therefore, all data structures in the language are persistent, as it is impossible to not preserve the previous state of a data structure with functional semantics.[16] This is because any change to a data structure that would render previous versions of a data structure invalid would violate referential transparency.\n\nIn its standard library Haskell has efficient persistent implementations for linked lists,[17] Maps (implemented as size balanced trees),[18] and Sets[19] among others.[20]\n\nLike many programming languages in the Lisp family, Clojure contains an implementation of a linked list, but unlike other dialects its implementation of a linked list has enforced persistence instead of being persistent by convention.[21] Clojure also has efficient implementations of persistent vectors, maps, and sets based on persistent hash array mapped tries. These data structures implement the mandatory read-only parts of the Java collections framework.[22]\n\nThe designers of the Clojure language advocate the use of persistent data structures over mutable data structures because they have value semantics which gives the benefit of making them freely shareable between threads with cheap aliases, easy to fabricate, and language independent.[23]\n\nThese data structures form the basis of Clojure's support for parallel computing since they allow for easy retries of operations to sidestep data races and atomic compare and swap semantics.[24]\n\nThe Elm programming language is purely functional like Haskell, which makes all of its data structures persistent by necessity. It contains persistent implementations of linked lists as well as persistent arrays, dictionaries, and sets.[25]\n\nElm uses a custom virtual DOM implementation that takes advantage of the persistent nature of Elm data. As of 2016 it was reported by the developers of Elm that this virtual DOM allows the Elm language to render HTML faster than the popular JavaScript frameworks React, Ember, and Angular.[26]\n\nThe Java programming language is not particularly functional. Despite this, the core JDK package java.util.concurrent includes CopyOnWriteArrayList and CopyOnWriteArraySet which are persistent structures, implemented using copy-on-write techniques. The usual concurrent map implementation in Java, ConcurrentHashMap, is not persistent, however. Fully persistent collections are available in third-party libraries,[27] or other JVM languages.\n\nThe popular JavaScript frontend framework React is frequently used along with a state management system that implements the Flux architecture,[28][29] a popular implementation of which is the JavaScript library Redux. The Redux library is inspired by the state management pattern used in the Elm programming language, meaning that it mandates that users treat all data as persistent.[30] As a result, the Redux project recommends that in certain cases users make use of libraries for enforced and efficient persistent data structures. This reportedly allows for greater performance than when comparing or making copies of regular JavaScript objects.[31]\n\nOne such library of persistent data structures Immutable.js is based on the data structures made available and popularized by Clojure and Scala.[32] It is mentioned by the documentation of Redux as being one of the possible libraries that can provide enforced immutability.[31] Mori.js brings data structures similar to those in Clojure to JavaScript.[33] Immer.js brings an interesting approach where one \"creates the next immutable state by mutating the current one\". [34] Immer.js uses native JavaScript objects and not efficient persistent data structures and it might cause performance issues when data size is big.\n\nProlog terms are naturally immutable and therefore data structures are typically persistent data structures. Their performance depends on sharing and garbage collection offered by the Prolog system.[35] Extensions to non-ground Prolog terms are not always feasible because of search space explosion. Delayed goals might mitigate the problem.\n\nSome Prolog systems nevertheless do provide destructive operations like setarg/3, which might come in different flavors, with/without copying and with/without backtracking of the state change. There are cases where setarg/3 is used to the good of providing a new declarative layer, like a constraint solver.[36]\n\nThe Scala programming language promotes the use of persistent data structures for implementing programs using \"Object-Functional Style\".[37] Scala contains implementations of many persistent data structures including linked lists, red–black trees, as well as persistent hash array mapped tries as introduced in Clojure.[38]\n\nBecause persistent data structures are often implemented in such a way that successive versions of a data structure share underlying memory[39] ergonomic use of such data structures generally requires some form of automatic garbage collection system such as reference counting or mark and sweep.[40] In some platforms where persistent data structures are used it is an option to not use garbage collection which, while doing so can lead to memory leaks, can in some cases have a positive impact on the overall performance of an application.[41]"
    },
    {
        "link": "https://stackoverflow.com/questions/2374066/how-to-make-data-structures-persistent-in-c",
        "document": "In general, you will need to serialise the structure so that you can write it to a file or a database. If you have a custom structure, then you will need to write the method to serialise and deserialise (i.e. write out and read in the structure). Otherwise, if you have used a structure from a library, there may already be (de)serialisation methods.\n\neg. A linked list might serialise as a string like so:"
    },
    {
        "link": "https://bartoszmilewski.com/2013/12/10/functional-data-structures-and-concurrency-in-c",
        "document": "In my previous blog posts I described C++ implementations of two basic functional data structures: a persistent list and a persistent red-black tree. I made an argument that persistent data structures are good for concurrency because of their immutability. In this post I will explain in much more detail the role of immutability in concurrent programming and argue that functional data structures make immutability scalable and composable.\n\nTo understand the role of functional data structures in concurrent programming we first have to understand concurrent programming. Okay, so maybe one blog post is not enough, but I’ll try my best at mercilessly slashing through the complexities and intricacies of concurrency while brutally ignoring all the details and subtleties.\n\nThe archetype for all concurrency is message passing. Without some form of message passing you have no communication between processes, threads, tasks, or whatever your units of execution are. The two parts of “message passing” loosely correspond to data (message) and action (passing). So there is the fiddling with data by one thread, some kind of handover between threads, and then the fiddling with data by another thread. The handover process requires synchronization.\n\nThere are two fundamental problems with this picture: Fiddling without proper synchronization leads to data races, and too much synchronization leads to deadlocks.\n\nLet’s start with a simpler world and assume that our concurrent participants share no memory — in that case they are called processes. And indeed it might be physically impossible to share memory between isolated units because of distances or hardware protection. In that case messages are just pieces of data that are magically transported between processes. You just put them (serialize, marshall) in a special buffer and tell the system to transmit them to someone else, who then picks them up from the system.\n\nSo the problem reduces to the proper synchronization protocols. The theory behind such systems is the good old CSP (Communicating Sequential Processes) from the 1970s. It has subsequently been extended to the Actor Model and has been very successful in Erlang. There are no data races in Erlang because of the isolation of processes, and no traditional deadlocks because there are no locks (although you can have distributed deadlocks when processes are blocked on receiving messages from each other).\n\nThe fact that Erlang’s concurrency is process-based doesn’t mean that it’s heavy-weight. The Erlang runtime is quite able to spawn thousands of light-weight user-level processes that, at the implementation level, may share the same address space. Isolation is enforced by the language rather than by the operating system. Banning direct sharing of memory is the key to Erlang’s success as the language for concurrent programming.\n\nSo why don’t we stop right there? Because shared memory is so much faster. It’s not a big deal if your messages are integers, but imagine passing a video buffer from one process to another. If you share the same address space (that is, you are passing data between threads rather than processes) why not just pass a pointer to it?\n\nShared memory is like a canvas where threads collaborate in painting images, except that they stand on the opposite sides of the canvas and use guns rather than brushes. The only way they can avoid killing each other is if they shout “duck!” before opening fire. This is why I like to think of shared-memory concurrency as the extension of message passing. Even though the “message” is not physically moved, the right to access it must be passed between threads. The message itself can be of arbitrary complexity: it could be a single word of memory or a hash table with thousands of entries.\n\nIt’s very important to realize that this transfer of access rights is necessary at every level, starting with a simple write into a single memory location. The writing thread has to send a message “I have written” and the reading thread has to acknowledge it: “I have read.” In standard portable C++ this message exchange might look something like this:\n\nYou rarely have to deal with such low level code because it’s abstracted into higher order libraries. You would, for instance, use locks for transferring access. A thread that acquires a lock gains unique access to a data structure that’s protected by it. It can freely modify it knowing that nobody else can see it. It’s the release of the lock variable that makes all those modifications visible to other threads. This release (e.g., ) is then matched with the subsequent acquire (e.g., ) by another thread. In reality, the locking protocol is more complicated, but it is at its core based on the same principle as message passing, with corresponding to a message send (or, more general, a broadcast), and to a message receive.\n\nThe point is, there is no sharing of memory without communication.\n\nThe first rule of synchronization is:\n\nThe only time you don’t need synchronization is when the shared data is immutable.\n\nWe would like to use as much immutability in implementing concurrency as possible. It’s not only because code that doesn’t require synchronization is faster, but it’s also easier to write, maintain, and reason about. The only problem is that:\n\nImmutable objects never change, but all data, immutable or not, must be initialized before being read. And initialization means mutation. Static global data is initialized before entering , so we don’t have to worry about it, but everything else goes through a construction phase.\n\nFirst, we have to answer the question: At what point after initialization is data considered immutable?\n\nHere’s what needs to happen: A thread has to somehow construct the data that it destined to be immutable. Depending on the structure of that data, this could be a very simple or a very complex process. Then the state of that data has to be frozen — no more changes are allowed. But still, before the data can be read by another thread, a synchronization event has to take place. Otherwise the other thread might see partially constructed data. This problem has been extensively discussed in articles about the singleton pattern, so I won’t go into more detail here.\n\nOne such synchronization event is the creation of the receiving thread. All data that had been frozen before the new thread was created is seen as immutable by that thread. That’s why it’s okay to pass immutable data as an argument to a thread function.\n\nAnother such event is message passing. It is always safe to pass a pointer to immutable data to another thread. The handover always involves the release/acquire protocol (as illustrated in the example above).\n\nAll memory writes that happened in the first thread before it released the message become visible to the acquiring thread after it received it.\n\nThe act of message passing establishes the “happens-before” relationship for all memory writes prior to it, and all memory reads after it. Again, these low-level details are rarely visible to the programmer, since they are hidden in libraries (channels, mailboxes, message queues, etc.). I’m pointing them out only because there is no protection in the language against the user inadvertently taking affairs into their own hands and messing things up. So creating an immutable object and passing a pointer to it to another thread through whatever message passing mechanism is safe. I also like to think of thread creation as a message passing event — the payload being the arguments to the thread function.\n\nThe beauty of this protocol is that, once the handover is done, the second (and the third, and the fourth, and so on…) thread can read the whole immutable data structure over and over again without any need for synchronization. The same is not true for shared mutable data structures! For such structures every read has to be synchronized at a non-trivial performance cost.\n\nHowever, it can’t be stressed enough that this is just a protocol and any deviation from it may be fatal. There is no language mechanism in C++ that may enforce this protocol.\n\nAs I argued before, access rights to shared memory have to be tightly controlled. The problem is that shared memory is not partitioned nicely into separate areas, each with its own army, police, and border controls. Even though we understand that an object is frozen after construction and ready to be examined by other threads without synchronization, we have to ask ourselves the question: Where exactly does this object begin and end in memory? And how do we know that nobody else claims writing privileges to any of its parts? After all, in C++ it’s pointers all the way. This is one of the biggest problems faced by imperative programmers trying to harness concurrency — who’s pointing where?\n\nFor instance, what does it mean to get access to an immutable linked list? Obviously, it’s not enough that the head of the list never changes, every single element of the list must be immutable as well. In fact, any memory that can be transitively accessed from the head of the list must be immutable. Only then can you safely forgo synchronization when accessing the list, as you would in a single-threaded program. This transitive closure of memory accessible starting from a given pointer is often called a cluster. So when you’re constructing an immutable object, you have to be able to freeze the whole cluster before you can pass it to other threads.\n\nBut that’s not all! You must also guarantee that there are no mutable pointers outside of the cluster pointing to any part of it. Such pointers could be inadvertently used to modify the data other threads believe to be immutable.\n\nThat means the construction of an immutable object is a very delicate operation. You not only have to make sure you don’t leak any pointers, but you have to inspect every component you use in building your object for potential leaks — you either have to trust all your subcontractors or inspect their code under the microscope. This clearly is no way to build software! We need something that it scalable and composable. Enter…\n\nFunctional data structures let you construct new immutable objects by composing existing immutable objects.\n\nRemember, an immutable object is a complete cluster with no pointers sticking out of it, and no mutable pointers poking into it. A sum of such objects is still an immutable cluster. As long as the constructor of a functional data structure doesn’t violate the immutability of its arguments and does not leak mutable pointers to the memory it is allocating itself, the result will be another immutable object.\n\nOf course, it would be nice if immutability were enforced by the type system, as it is in the D language. In C++ we have to replace the type system with discipline, but still, it helps to know exactly what the terms of the immutability contract are. For instance, make sure you pass only (const) references to other immutable objects to the constructor of an immutable object.\n\nLet’s now review the example of the persistent binary tree from my previous post to see how it follows the principles I described above. In particular, let me show you that every forms an immutable cluster, as long as user data is stored in it by value (or is likewise immutable).\n\nThe proof proceeds through structural induction, but it’s easy to understand. An empty tree forms an immutable cluster trivially. A non-empty tree is created by combining two other trees. We can assume by the inductive step that both of them form immutable clusters:\n\nIn particular, there are no external mutating pointers to , , or to any of their nodes.\n\nInside the constructor we allocate a fresh node and pass it the three arguments:\n\nHere is a private member of the :\n\nNotice that the only reference to the newly allocated is stored in through a pointer and is never leaked. Moreover, there are no methods of the tree that either modify or expose any part of the tree to modification. Therefore the newly constructed forms an immutable cluster. (With the usual caveat that you don’t try to bypass the C++ type system or use other dirty tricks).\n\nAs I discussed before, there is some bookkeeping related to reference counting in C++, which is however totally transparent to the user of functional data structures.\n\nImmutable data structures play an important role in concurrency but there’s more to them that meets the eye. In this post I tried to demonstrate how to use them safely and productively. In particular, functional data structures provide a scalable and composable framework for working with immutable objects.\n\nOf course not all problems of concurrency can be solved with immutability and not all immutable object can be easily created from other immutable objects. The classic example is a doubly-linked list: you can’t add a new element to it without modifying pointers in it. But there is a surprising variety of composable immutable data structures that can be used in C++ without breaking the rules. I will continue describing them in my future blog posts."
    }
]