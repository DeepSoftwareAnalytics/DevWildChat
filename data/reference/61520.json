[
    {
        "link": "https://docs.aiogram.dev",
        "document": "# Bot token can be obtained via https://t.me/BotFather # All handlers should be attached to the Router (or Dispatcher) # Most event objects have aliases for API methods that can be called in events' context # For example if you want to answer to incoming message you can use `message.answer(...)` alias # and the target chat will be passed to :ref:`aiogram.methods.send_message.SendMessage` Handler will forward receive a message back to the sender By default, message handler will handle all message types (like a text, photo, sticker etc.) # But not all the types is supported to be copied so need to handle it # Initialize Bot instance with default bot properties which will be passed to all API calls"
    },
    {
        "link": "https://docs.aiogram.dev/en/latest/dispatcher/index.html",
        "document": "aiogram includes Dispatcher mechanism. Dispatcher is needed for handling incoming updates from Telegram.\n\nWith dispatcher you can do:\n• None Filter incoming events before it will be processed by specific handler;\n• None Separate bot functionality between different handlers, modules and packages\n\nDispatcher is also separated into two entities - Router and Dispatcher. Dispatcher is subclass of router and should be always is root router.\n• None Webhook - you should configure your web server to receive updates from Telegram;\n• None Long polling - you should request updates from Telegram.\n\nSo, you can use both of them with aiogram."
    },
    {
        "link": "https://stackoverflow.com/questions/74512308/how-to-make-handler-for-messages-from-stripe-using-aiogrampython",
        "document": "Is this possible to make handler for receiving messages by Telegram Bot from Stripe, when I create new product in Stripe store(account). I use Aiogram. I want to do something similar to this:\n\nCan aiogram Dispatcher handle message with type something like StripeEvent type. Or I can do this only using web-framework, for example, Flask app - as the Stripe documentation says: https://stripe.com/docs/webhooks"
    },
    {
        "link": "https://docs.aiogram.dev/en/v2.25.1",
        "document": "aiogram is a pretty simple and fully asynchronous framework for Telegram Bot API written in Python 3.7 with asyncio and aiohttp. It helps you to make your bots faster and simpler.\n• None Can reply into webhook. (In other words make requests in response to updates)"
    },
    {
        "link": "https://restack.io/p/best-telegram-bot-frameworks-for-ai-answer-aiogram-bot-structure-cat-ai",
        "document": "Integrating middleware into your Aiogram bot can significantly enhance its performance, security, and user experience. Middleware functions as a bridge between the request and response cycle, allowing you to execute code, modify requests and responses, and end requests before they reach your handlers. Here are some key use cases for middleware in Aiogram:\n\nMiddleware can be used to ensure user identity and check session cookies before granting access to specific commands or API routes. This is crucial for maintaining the security of your bot and ensuring that only authorized users can access certain functionalities.\n\nYou can implement server-side redirects based on specific conditions, such as user roles or preferences. This allows for a more personalized user experience, directing users to the most relevant parts of your bot.\n\nMiddleware can support A/B testing or feature rollouts by dynamically rewriting paths to commands based on request properties. This is particularly useful for testing new features with a subset of users before a full rollout.\n\nProtect your resources by implementing middleware that detects and blocks bot traffic. This can help prevent abuse and ensure that your bot serves genuine users effectively.\n\nCapture and analyze request data before processing it further. Middleware can log user interactions, which can be invaluable for understanding user behavior and improving your bot's performance.\n\nEnable or disable features dynamically using middleware. This allows for seamless feature rollouts or testing without disrupting the user experience.\n\nWhen Not to Use Middleware\n\nWhile middleware can be powerful, there are scenarios where it may not be the best approach. For instance, if the processing logic is simple and does not require additional checks or modifications, using middleware might add unnecessary complexity. Always evaluate the specific needs of your bot before implementing middleware solutions.\n\nHere’s a simple example of how to implement middleware in an Aiogram bot:\n\nThis code sets up a logging middleware that will log all incoming messages, helping you monitor interactions with your bot.\n\nBy understanding and implementing middleware effectively, you can create a more robust and user-friendly Aiogram bot that meets the needs of your users while maintaining high performance and security standards."
    },
    {
        "link": "https://ffmpeg.org/ffmpeg.html",
        "document": "is a universal media converter. It can read a wide variety of inputs - including live grabbing/recording devices - filter, and transcode them into a plethora of output formats.\n\nreads from an arbitrary number of inputs (which can be regular files, pipes, network streams, grabbing devices, etc.), specified by the option, and writes to an arbitrary number of outputs, which are specified by a plain output url. Anything found on the command line which cannot be interpreted as an option is considered to be an output url.\n\nEach input or output can, in principle, contain any number of elementary streams of different types (video/audio/subtitle/attachment/data), though the allowed stream counts and/or types may be limited by the container format. Selecting which streams from which inputs will go into which output is either done automatically or with the option (see the Stream selection chapter).\n\nTo refer to inputs/outputs in options, you must use their indices (0-based). E.g. the first input is , the second is , etc. Similarly, streams within an input/output are referred to by their indices. E.g. refers to the fourth stream in the third input or output. Also see the Stream specifiers chapter.\n\nAs a general rule, options are applied to the next specified file. Therefore, order is important, and you can have the same option on the command line multiple times. Each occurrence is then applied to the next input or output file. Exceptions from this rule are the global options (e.g. verbosity level), which should be specified first.\n\nDo not mix input and output files – first specify all input files, then all output files. Also do not mix options which belong to different files. All options apply ONLY to the next input or output file and are reset between files.\n• Convert an input media file to a different format, by re-encoding media streams:\n• Set the video bitrate of the output file to 64 kbit/s:\n• Force the frame rate of the output file to 24 fps:\n• Force the frame rate of the input file (valid for raw formats only) to 1 fps and the frame rate of the output file to 24 fps:\n\nThe format option may be needed for raw input files.\n\nbuilds a transcoding pipeline out of the components listed below. The program’s operation then consists of input data chunks flowing from the sources down the pipes towards the sinks, while being transformed by the components they encounter along the way.\n\nThe following kinds of components are available:\n• Demuxers (short for \"demultiplexers\") read an input source in order to extract\n• global properties such as metadata or chapters;\n• list of input elementary streams and their properties One demuxer instance is created for each option, and sends encoded packets to decoders or muxers. In other literature, demuxers are sometimes called splitters, because their main function is splitting a file into elementary streams (though some files only contain one elementary stream). A schematic representation of a demuxer looks like this: ┌──────────┬───────────────────────┐ │ demuxer │ │ packets for stream 0 ╞══════════╡ elementary stream 0 ├──────────────────────► │ │ │ │ global ├───────────────────────┤ │properties│ │ packets for stream 1 │ and │ elementary stream 1 ├──────────────────────► │ metadata │ │ │ ├───────────────────────┤ │ │ │ │ │ ........... │ │ │ │ │ ├───────────────────────┤ │ │ │ packets for stream N │ │ elementary stream N ├──────────────────────► │ │ │ └──────────┴───────────────────────┘ ▲ │ │ read from file, network stream, │ grabbing device, etc. │\n• Decoders receive encoded (compressed) packets for an audio, video, or subtitle elementary stream, and decode them into raw frames (arrays of pixels for video, PCM for audio). A decoder is typically associated with (and receives its input from) an elementary stream in a demuxer, but sometimes may also exist on its own (see Loopback decoders). A schematic representation of a decoder looks like this:\n• Filtergraphs process and transform raw audio or video frames. A filtergraph consists of one or more individual filters linked into a graph. Filtergraphs come in two flavors - simple and complex, configured with the and options, respectively. A simple filtergraph is associated with an output elementary stream; it receives the input to be filtered from a decoder and sends filtered output to that output stream’s encoder. A simple video filtergraph that performs deinterlacing (using the deinterlacer) followed by resizing (using the filter) can look like this: ┌────────────────────────┐ │ simple filtergraph │ frames from ╞════════════════════════╡ frames for a decoder │ ┌───────┐ ┌───────┐ │ an encoder ────────────►├─►│ yadif ├─►│ scale ├─►│────────────► │ └───────┘ └───────┘ │ └────────────────────────┘ A complex filtergraph is standalone and not associated with any specific stream. It may have multiple (or zero) inputs, potentially of different types (audio or video), each of which receiving data either from a decoder or another complex filtergraph’s output. It also has one or more outputs that feed either an encoder or another complex filtergraph’s input. The following example diagram represents a complex filtergraph with 3 inputs and 2 outputs (all video): Frames from second input are overlaid over those from the first. Frames from the third input are rescaled, then the duplicated into two identical streams. One of them is overlaid over the combined first two inputs, with the result exposed as the filtergraph’s first output. The other duplicate ends up being the filtergraph’s second output.\n• Encoders receive raw audio, video, or subtitle frames and encode them into encoded packets. The encoding (compression) process is typically lossy - it degrades stream quality to make the output smaller; some encoders are lossless, but at the cost of much higher output size. A video or audio encoder receives its input from some filtergraph’s output, subtitle encoders receive input from a decoder (since subtitle filtering is not supported yet). Every encoder is associated with some muxer’s output elementary stream and sends its output to that muxer. A schematic representation of an encoder looks like this:\n• Muxers (short for \"multiplexers\") receive encoded packets for their elementary streams from encoders (the transcoding path) or directly from demuxers (the streamcopy path), interleave them (when there is more than one elementary stream), and write the resulting bytes into the output file (or pipe, network stream, etc.). A schematic representation of a muxer looks like this: ┌──────────────────────┬───────────┐ packets for stream 0 │ │ muxer │ ──────────────────────►│ elementary stream 0 ╞═══════════╡ │ │ │ ├──────────────────────┤ global │ packets for stream 1 │ │properties │ ──────────────────────►│ elementary stream 1 │ and │ │ │ metadata │ ├──────────────────────┤ │ │ │ │ │ ........... │ │ │ │ │ ├──────────────────────┤ │ packets for stream N │ │ │ ──────────────────────►│ elementary stream N │ │ │ │ │ └──────────────────────┴─────┬─────┘ │ write to file, network stream, │ grabbing device, etc. │ │ ▼\n\nThe simplest pipeline in is single-stream streamcopy, that is copying one input elementary stream’s packets without decoding, filtering, or encoding them. As an example, consider an input file called with 3 elementary streams, from which we take the second and write it to file . A schematic representation of such a pipeline looks like this:\n\nThe above pipeline can be constructed with the following commandline:\n• there are no input options for this input;\n• there are two output options for this output:\n• selects the input stream to be used - from input with index 0 (i.e. the first one) the stream with index 1 (i.e. the second one);\n• selects the encoder, i.e. streamcopy with no decoding or encoding.\n\nStreamcopy is useful for changing the elementary stream count, container format, or modifying container-level metadata. Since there is no decoding or encoding, it is very fast and there is no quality loss. However, it might not work in some cases because of a variety of factors (e.g. certain information required by the target container is not available in the source). Applying filters is obviously also impossible, since filters work on decoded frames.\n\nMore complex streamcopy scenarios can be constructed - e.g. combining streams from two input files into a single output:\n\nthat can be built by the commandline\n\nThe output option is used twice here, creating two streams in the output file - one fed by the first input and one by the second. The single instance of the option selects streamcopy for both of those streams. You could also use multiple instances of this option together with Stream specifiers to apply different values to each stream, as will be demonstrated in following sections.\n\nA converse scenario is splitting multiple streams from a single input into multiple outputs:\n\nNote how a separate instance of the option is needed for every output file even though their values are the same. This is because non-global options (which is most of them) only apply in the context of the file before which they are placed.\n\nThese examples can of course be further generalized into arbitrary remappings of any number of inputs into any number of outputs.\n\nTranscoding is the process of decoding a stream and then encoding it again. Since encoding tends to be computationally expensive and in most cases degrades the stream quality (i.e. it is lossy), you should only transcode when you need to and perform streamcopy otherwise. Typical reasons to transcode are:\n• you want to feed the stream to something that cannot decode the original codec.\n\nNote that will transcode all audio, video, and subtitle streams unless you specify for them.\n\nConsider an example pipeline that reads an input file with one audio and one video stream, transcodes the video and copies the audio into a single output file. This can be schematically represented as follows\n\nand implemented with the following commandline:\n\nNote how it uses stream specifiers and to select input streams and apply different values of the option to them; see the Stream specifiers section for more details.\n\nWhen transcoding, audio and video streams can be filtered before encoding, with either a simple or complex filtergraph.\n\nSimple filtergraphs are those that have exactly one input and output, both of the same type (audio or video). They are configured with the per-stream option (with and aliases for (video) and (audio) respectively). Note that simple filtergraphs are tied to their output stream, so e.g. if you have multiple audio streams, will create a separate filtergraph for each one.\n\nTaking the trancoding example from above, adding filtering (and omitting audio, for clarity) makes it look like this:\n\nComplex filtergraphs are those which cannot be described as simply a linear processing chain applied to one stream. This is the case, for example, when the graph has more than one input and/or output, or when output stream type is different from input. Complex filtergraphs are configured with the option. Note that this option is global, since a complex filtergraph, by its nature, cannot be unambiguously associated with a single stream or file. Each instance of creates a new complex filtergraph, and there can be any number of them.\n\nA trivial example of a complex filtergraph is the filter, which has two video inputs and one video output, containing one video overlaid on top of the other. Its audio counterpart is the filter.\n\nWhile decoders are normally associated with demuxer streams, it is also possible to create \"loopback\" decoders that decode the output from some encoder and allow it to be fed back to complex filtergraphs. This is done with the directive, which takes as a parameter the index of the output stream that should be decoded. Every such directive creates a new loopback decoder, indexed with successive integers starting at zero. These indices should then be used to refer to loopback decoders in complex filtergraph link labels, as described in the documentation for .\n\nDecoding AVOptions can be passed to loopback decoders by placing them before , analogously to input/output options.\n\nE.g. the following example:\n• (line 2) encodes it with at low quality;\n• (line 4) places decoded video side by side with the original input video;\n• (line 5) combined video is then losslessly encoded and written into .\n\nSuch a transcoding pipeline can be represented with the following diagram:\n\nprovides the option for manual control of stream selection in each output file. Users can skip and let ffmpeg perform automatic stream selection as described below. The options can be used to skip inclusion of video, audio, subtitle and data streams respectively, whether manually mapped or automatically selected, except for those streams which are outputs of complex filtergraphs.\n\nThe sub-sections that follow describe the various rules that are involved in stream selection. The examples that follow next show how these rules are applied in practice.\n\nWhile every effort is made to accurately reflect the behavior of the program, FFmpeg is under continuous development and the code may have changed since the time of this writing.\n\nIn the absence of any map options for a particular output file, ffmpeg inspects the output format to check which type of streams can be included in it, viz. video, audio and/or subtitles. For each acceptable stream type, ffmpeg will pick one stream, when available, from among all the inputs.\n\nIt will select that stream based upon the following criteria:\n• for video, it is the stream with the highest resolution,\n• for audio, it is the stream with the most channels,\n• for subtitles, it is the first subtitle stream found but there’s a caveat. The output format’s default subtitle encoder can be either text-based or image-based, and only a subtitle stream of the same type will be chosen.\n\nIn the case where several streams of the same type rate equally, the stream with the lowest index is chosen.\n\nData or attachment streams are not automatically selected and can only be included using .\n\nWhen is used, only user-mapped streams are included in that output file, with one possible exception for filtergraph outputs described below.\n\nIf there are any complex filtergraph output streams with unlabeled pads, they will be added to the first output file. This will lead to a fatal error if the stream type is not supported by the output format. In the absence of the map option, the inclusion of these streams leads to the automatic stream selection of their types being skipped. If map options are present, these filtergraph streams are included in addition to the mapped streams.\n\nComplex filtergraph output streams with labeled pads must be mapped once and exactly once.\n\nStream handling is independent of stream selection, with an exception for subtitles described below. Stream handling is set via the option addressed to streams within a specific output file. In particular, codec options are applied by ffmpeg after the stream selection process and thus do not influence the latter. If no option is specified for a stream type, ffmpeg will select the default encoder registered by the output file muxer.\n\nAn exception exists for subtitles. If a subtitle encoder is specified for an output file, the first subtitle stream found of any type, text or image, will be included. ffmpeg does not validate if the specified encoder can convert the selected stream or if the converted stream is acceptable within the output format. This applies generally as well: when the user sets an encoder manually, the stream selection process cannot check if the encoded stream can be muxed into the output file. If it cannot, ffmpeg will abort and all output files will fail to be processed.\n\nThe following examples illustrate the behavior, quirks and limitations of ffmpeg’s stream selection methods.\n\nThey assume the following three input files.\n\nThere are three output files specified, and for the first two, no options are set, so ffmpeg will select streams for these two files automatically.\n\nis a Matroska container file and accepts video, audio and subtitle streams, so ffmpeg will try to select one of each type.\n\n For video, it will select from , which has the highest resolution among all the input video streams.\n\n For audio, it will select from , since it has the greatest number of channels.\n\n For subtitles, it will select from , which is the first subtitle stream from among and .\n\naccepts only audio streams, so only from is selected.\n\nFor , since a option is set, no automatic stream selection will occur. The option will select all audio streams from the second input . No other streams will be included in this output file.\n\nFor the first two outputs, all included streams will be transcoded. The encoders chosen will be the default ones registered by each output format, which may not match the codec of the selected input streams.\n\nFor the third output, codec option for audio streams has been set to , so no decoding-filtering-encoding operations will occur, or can occur. Packets of selected streams shall be conveyed from the input file and muxed within the output file.\n\nAlthough is a Matroska container file which accepts subtitle streams, only a video and audio stream shall be selected. The subtitle stream of is image-based and the default subtitle encoder of the Matroska muxer is text-based, so a transcode operation for the subtitles is expected to fail and hence the stream isn’t selected. However, in , a subtitle encoder is specified in the command and so, the subtitle stream is selected, in addition to the video stream. The presence of disables audio stream selection for .\n\nA filtergraph is setup here using the option and consists of a single video filter. The filter requires exactly two video inputs, but none are specified, so the first two available video streams are used, those of and . The output pad of the filter has no label and so is sent to the first output file . Due to this, automatic selection of the video stream is skipped, which would have selected the stream in . The audio stream with most channels viz. in , is chosen automatically. No subtitle stream is chosen however, since the MP4 format has no default subtitle encoder registered, and the user hasn’t specified a subtitle encoder.\n\nThe 2nd output file, , only accepts text-based subtitle streams. So, even though the first subtitle stream available belongs to , it is image-based and hence skipped. The selected stream, in , is the first text-based subtitle stream.\n\nThe above command will fail, as the output pad labelled has been mapped twice. None of the output files shall be processed.\n\nThis command above will also fail as the hue filter output has a label, , and hasn’t been mapped anywhere.\n\nThe command should be modified as follows,\n\nThe video stream from is sent to the hue filter, whose output is cloned once using the split filter, and both outputs labelled. Then a copy each is mapped to the first and third output files.\n\nThe overlay filter, requiring two video inputs, uses the first two unused video streams. Those are the streams from and . The overlay output isn’t labelled, so it is sent to the first output file , regardless of the presence of the option.\n\nThe aresample filter is sent the first unused audio stream, that of . Since this filter output is also unlabelled, it too is mapped to the first output file. The presence of only suppresses automatic or manual stream selection of audio streams, not outputs sent from filtergraphs. Both these mapped streams shall be ordered before the mapped stream in .\n\nThe video, audio and subtitle streams mapped to are entirely determined by automatic stream selection.\n\nconsists of the cloned video output from the hue filter and the first audio stream from . \n\n\n\nAll the numerical options, if not specified otherwise, accept a string representing a number as input, which may be followed by one of the SI unit prefixes, for example: ’K’, ’M’, or ’G’.\n\nIf ’i’ is appended to the SI unit prefix, the complete prefix will be interpreted as a unit prefix for binary multiples, which are based on powers of 1024 instead of powers of 1000. Appending ’B’ to the SI unit prefix multiplies the value by 8. This allows using, for example: ’KB’, ’MiB’, ’G’ and ’B’ as number suffixes.\n\nOptions which do not take arguments are boolean options, and set the corresponding value to true. They can be set to false by prefixing the option name with \"no\". For example using \"-nofoo\" will set the boolean option with name \"foo\" to false.\n\nOptions that take arguments support a special syntax where the argument given on the command line is interpreted as a path to the file from which the actual argument value is loaded. To use this feature, add a forward slash ’/’ immediately before the option name (after the leading dash). E.g.\n\nwill load a filtergraph description from the file named .\n\nSome options are applied per-stream, e.g. bitrate or codec. Stream specifiers are used to precisely specify which stream(s) a given option belongs to.\n\nA stream specifier is a string generally appended to the option name and separated from it by a colon. E.g. contains the stream specifier, which matches the second audio stream. Therefore, it would select the ac3 codec for the second audio stream.\n\nA stream specifier can match several streams, so that the option is applied to all of them. E.g. the stream specifier in matches all audio streams.\n\nAn empty stream specifier matches all streams. For example, or would copy all the streams without reencoding.\n\nPossible forms of stream specifiers are:\n\nThese options are shared amongst the ff* tools.\n\nThese options are provided directly by the libavformat, libavdevice and libavcodec libraries. To see the list of available AVOptions, use the option. They are separated into two categories:\n\nFor example to write an ID3v2.3 header instead of a default ID3v2.4 to an MP3 file, use the private option of the MP3 muxer:\n\nAll codec AVOptions are per-stream, and thus a stream specifier should be attached to them:\n\nIn the above example, a multichannel audio stream is mapped twice for output. The first instance is encoded with codec ac3 and bitrate 640k. The second instance is downmixed to 2 channels and encoded with codec aac. A bitrate of 128k is specified for it using absolute index of the output stream.\n\nNote: the syntax cannot be used for boolean AVOptions, use / .\n\nNote: the old undocumented way of specifying per-stream AVOptions by prepending v/a/s to the options name is now obsolete and will be removed soon.\n\nForce input or output file format. The format is normally auto detected for input files and guessed from the file extension for output files, so this option is not needed in most cases. Do not overwrite output files, and exit immediately if a specified output file already exists. Set number of times input stream shall be looped. Loop 0 means no loop, loop -1 means infinite loop. Allow forcing a decoder of a different media type than the one detected or designated by the demuxer. Useful for decoding media data muxed as data streams. Select an encoder (when used before an output file) or a decoder (when used before an input file) for one or more streams. is the name of a decoder/encoder or a special value (output only) to indicate that the stream is not to be re-encoded. encodes all video streams with libx264 and copies all audio streams. For each stream, the last matching option is applied, so will copy all the streams except the second video, which will be encoded with libx264, and the 138th audio, which will be encoded with libvorbis. When used as an input option (before ), limit the of data read from the input file. When used as an output option (before an output url), stop writing the output after its duration reaches . must be a time duration specification, see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual. -to and -t are mutually exclusive and -t has priority. Stop writing the output or reading the input at . must be a time duration specification, see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual. -to and -t are mutually exclusive and -t has priority. Set the file size limit, expressed in bytes. No further chunk of bytes is written after the limit is exceeded. The size of the output file is slightly more than the requested file size. When used as an input option (before ), seeks in this input file to . Note that in most formats it is not possible to seek exactly, so will seek to the closest seek point before . When transcoding and is enabled (the default), this extra segment between the seek point and will be decoded and discarded. When doing stream copy or when is used, it will be preserved. When used as an output option (before an output url), decodes but discards input until the timestamps reach . must be a time duration specification, see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual. Like the option but relative to the \"end of file\". That is negative values are earlier in the file, 0 is at EOF. This will take the difference between the start times of the target and reference inputs and offset the timestamps of the target file by that difference. The source timestamps of the two inputs should derive from the same clock source for expected results. If is set then must also be set. If either of the inputs has no starting timestamp then no sync adjustment is made. Acceptable values are those that refer to a valid ffmpeg input index. If the sync reference is the target index itself or , then no adjustment is made to target timestamps. A sync reference may not itself be synced to any other input. must be a time duration specification, see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual. The offset is added to the timestamps of the input files. Specifying a positive offset means that the corresponding streams are delayed by the time duration specified in . Set the recording timestamp in the container. must be a date specification, see (ffmpeg-utils)the Date section in the ffmpeg-utils(1) manual. An optional may be given to set metadata on streams, chapters or programs. See documentation for details. This option overrides metadata set with . It is also possible to delete metadata by using an empty value. For example, for setting the title in the output file: To set the language of the first audio stream: Default value: by default, all disposition flags are copied from the input stream, unless the output stream this option applies to is fed by a complex filtergraph - in that case no disposition flags are set by default. is a sequence of disposition flags separated by ’+’ or ’-’. A ’+’ prefix adds the given disposition, ’-’ removes it. If the first flag is also prefixed with ’+’ or ’-’, the resulting disposition is the default value updated by . If the first flag is not prefixed, the resulting disposition is . It is also possible to clear the disposition by setting it to 0. If no options were specified for an output file, ffmpeg will automatically set the ’default’ disposition flag on the first stream of each type, when there are multiple streams of this type in the output file and no stream of that type is already marked as default. The option lists the known disposition flags. For example, to make the second audio stream the default stream: To make the second subtitle stream the default stream and remove the default disposition from the first subtitle stream: To add the ’original’ and remove the ’comment’ disposition flag from the first audio stream without removing its other disposition flags: To remove the ’original’ and add the ’comment’ disposition flag to the first audio stream without removing its other disposition flags: To set only the ’original’ and ’comment’ disposition flags on the first audio stream (and remove its other disposition flags): To remove all disposition flags from the first audio stream: Not all muxers support embedded thumbnails, and those who do, only support a few formats, like JPEG or PNG. Creates a program with the specified , and adds the specified (s) to it. Creates a stream group of the specified and , or by ping an input group, adding the specified (s) and/or previously defined (s) to it. can be one of the following: Groups s that belong to the same IAMF Audio Element For this group , the following options are available The Audio Element type. The following values are supported: Demixing information used to reconstruct a scalable channel audio representation. This option must be separated from the rest with a ’,’, and takes the following key=value options An identifier parameters blocks in frames may refer to Recon gain information used to reconstruct a scalable channel audio representation. This option must be separated from the rest with a ’,’, and takes the following key=value options An identifier parameters blocks in frames may refer to A layer defining a Channel Layout in the Audio Element. This option must be separated from the rest with a ’,’. Several ’,’ separated entries can be defined, and at least one must be set. It takes the following \":\"-separated key=value options The following flags are available: Wether to signal if recon_gain is present as metadata in parameter blocks within frames Which channels output_gain applies to. The following flags are available: The ambisonics mode. This has no effect if audio_element_type is set to channel. The following values are supported: Each ambisonics channel is coded as an individual mono stream in the group Groups s that belong to all IAMF Audio Element the same IAMF Mix Presentation references For this group , the following options are available A sub-mix within the Mix Presentation. This option must be separated from the rest with a ’,’. Several ’,’ separated entries can be defined, and at least one must be set. It takes the following \":\"-separated key=value options An identifier parameters blocks in frames may refer to, for post-processing the mixed audio signal to generate the audio signal for playback The sample rate duration fields in parameters blocks in frames that refer to this are expressed as Default mix gain value to apply when there are no parameter blocks sharing the same for a given frame References an Audio Element used in this Mix Presentation to generate the final output audio signal for playback. This option must be separated from the rest with a ’|’. Several ’|’ separated entries can be defined, and at least one must be set. It takes the following \":\"-separated key=value options: The for an Audio Element which this sub-mix refers to An identifier parameters blocks in frames may refer to, for applying any processing to the referenced and rendered Audio Element before being summed with other processed Audio Elements The sample rate duration fields in parameters blocks in frames that refer to this are expressed as Default mix gain value to apply when there are no parameter blocks sharing the same for a given frame A key=value string describing the sub-mix element where \"key\" is a string conforming to BCP-47 that specifies the language for the \"value\" string. \"key\" must be the same as the one in the mix’s Indicates whether the input channel-based Audio Element is rendered to stereo loudspeakers or spatialized with a binaural renderer when played back on headphones. This has no effect if the referenced Audio Element’s is set to channel. The following values are supported: Specifies the layouts for this sub-mix on which the loudness information was measured. This option must be separated from the rest with a ’|’. Several ’|’ separated entries can be defined, and at least one must be set. It takes the following \":\"-separated key=value options: The layout follows the loudspeaker sound system convention of ITU-2051-3. Channel layout matching one of Sound Systems A to J of ITU-2051-3, plus 7.1.2 and 3.1.2 This has no effect if is set to binaural. The program integrated loudness information, as defined in ITU-1770-4. The digital (sampled) peak value of the audio signal, as defined in ITU-1770-4. The true peak of the audio signal, as defined in ITU-1770-4. The Dialogue loudness information, as defined in ITU-1770-4. The Album loudness information, as defined in ITU-1770-4. A key=value string string describing the mix where \"key\" is a string conforming to BCP-47 that specifies the language for the \"value\" string. \"key\" must be the same as the ones in all sub-mix element’s s E.g. to create an scalable 5.1 IAMF file from several WAV input files To copy the two stream groups (Audio Element and Mix Presentation) from an input IAMF file with four streams into an mp4 output Specify target file type ( , , , , ). may be prefixed with , or to use the corresponding standard. All the format options (bitrate, codecs, buffer sizes) are then set automatically. You can just type: Nevertheless you can specify additional options as long as you know they do not conflict with the standard, as in: The parameters set for each target are as follows. The target is identical to the target except that the pixel format set is for all three standards. Any user-set value for a parameter above will override the target preset value. In that case, the output may not comply with the target standard. As an input option, blocks all data streams of a file from being filtered or being automatically selected or mapped for any output. See option to disable streams individually. As an output option, disables data recording i.e. automatic selection or mapping of any data stream. For full manual control see the option. Set the number of data frames to output. This is an obsolete alias for , which you should use instead. Stop writing to the stream after frames. Use fixed quality scale (VBR). The meaning of / is codec-dependent. If is used without a then it applies only to the video stream, this is to maintain compatibility with previous behavior and as specifying the same codec specific value to 2 different codecs that is audio and video generally is not what is intended when no stream_specifier is used. Create the filtergraph specified by and use it to filter the stream. is a description of the filtergraph to apply to the stream, and must have a single input and a single output of the same type of the stream. In the filtergraph, the input is associated to the label , and the output to the label . See the ffmpeg-filters manual for more information about the filtergraph syntax. See the -filter_complex option if you want to create filtergraphs with multiple inputs and/or outputs. This boolean option determines if the filtergraph(s) to which this stream is fed gets reinitialized when input frame parameters change mid-stream. This option is enabled by default as most video and all audio filters cannot handle deviation in input frame properties. Upon reinitialization, existing filter state is lost, like e.g. the frame count reference available in some filters. Any frames buffered at time of reinitialization are lost. The properties where a change triggers reinitialization are, for video, frame resolution or pixel format; for audio, sample format, sample rate, channel count or channel layout. Defines how many threads are used to process a filter pipeline. Each pipeline will produce a thread pool with this many threads available for parallel processing. The default is the number of available CPUs. Specify the preset for matching stream(s). Log encoding progress/statistics as \"info\"-level log (see ). It is on by default, to explicitly disable it you need to specify . Set period at which encoding progress/statistics are updated. Default is 0.5 seconds. Progress information is written periodically and at the end of the encoding process. It is made of \" = \" lines. consists of only alphanumeric characters. The last key of a sequence of progress information is always \"progress\" with the value \"continue\" or \"end\". The update period is set using . For example, log progress information to stdout: Enable interaction on standard input. On by default unless standard input is used as an input. To explicitly disable interaction you need to specify . Disabling interaction on standard input is useful, for example, if ffmpeg is in the background process group. Roughly the same result can be achieved with but it requires a shell. Print timestamp/latency information. It is off by default. This option is mostly useful for testing and debugging purposes, and the output format may change from one version to another, so it should not be employed by portable scripts. See also the option . Add an attachment to the output file. This is supported by a few formats like Matroska for e.g. fonts used in rendering subtitles. Attachments are implemented as a specific type of stream, so this option will add a new stream to the file. It is then possible to use per-stream options on this stream in the usual way. Attachment streams created with this option will be created after all the other streams (i.e. those created with or automatic mappings). Note that for Matroska you also have to set the mimetype metadata tag: (assuming that the attachment stream will be third in the output file). Extract the matching attachment stream into a file named . If is empty, then the value of the metadata tag will be used. E.g. to extract the first attachment to a file named ’out.ttf’: To extract all attachments to files determined by the tag: Technical note – attachments are implemented as codec extradata, so this option can actually be used to extract extradata from any stream, not just attachments.\n\nSet pixel format. Use to show all the supported pixel formats. If the selected pixel format can not be selected, ffmpeg will print a warning and select the best pixel format supported by the encoder. If is prefixed by a , ffmpeg will exit with an error if the requested pixel format can not be selected, and automatic conversions inside filtergraphs are disabled. If is a single , ffmpeg selects the same pixel format as the input (or graph output) and automatic conversions are disabled. Set default flags for the libswscale library. These flags are used by automatically inserted filters and those within simple filtergraphs, if not overridden within the filtergraph definition. See the (ffmpeg-scaler)ffmpeg-scaler manual for a list of scaler options. Rate control override for specific intervals, formatted as \"int,int,int\" list separated with slashes. Two first values are the beginning and end frame numbers, last one is quantizer to use if positive, or quality factor if negative. Dump video coding statistics to . See the vstats file format section for the format description. Dump video coding statistics to . See the vstats file format section for the format description. Specify which version of the vstats format to use. Default is . See the vstats file format section for the format description. Force video tag/fourcc. This is an alias for . can take arguments of the following form: If the argument consists of timestamps, ffmpeg will round the specified times to the nearest output timestamp as per the encoder time base and force a keyframe at the first frame having timestamp equal or greater than the computed timestamp. Note that if the encoder time base is too coarse, then the keyframes may be forced on frames with timestamps lower than the specified time. The default encoder time base is the inverse of the output framerate but may be set otherwise via . If one of the times is \" [ ]\", it is expanded into the time of the beginning of all chapters in the file, shifted by , expressed as a time in seconds. This option can be useful to ensure that a seek point is present at a chapter mark or any other designated place in the output file. For example, to insert a key frame at 5 minutes, plus key frames 0.1 second before the beginning of every chapter: If the argument is prefixed with , the string is interpreted like an expression and is evaluated for each frame. A key frame is forced in case the evaluation is non-zero. The expression in can contain the following constants: the number of current processed frame, starting from 0 the number of the previous forced frame, it is when no keyframe was forced yet the time of the previous forced frame, it is when no keyframe was forced yet the time of the current processed frame For example to force a key frame every 5 seconds, you can specify: To force a key frame 5 seconds after the time of the last forced one, starting from second 13: If the argument is , ffmpeg will force a key frame if the current frame being encoded is marked as a key frame in its source. In cases where this particular source frame has to be dropped, enforce the next available frame to become a key frame instead. Note that forcing too many keyframes is very harmful for the lookahead algorithms of certain encoders: using fixed-GOP options or similar would be more efficient. Automatically crop the video after decoding according to file metadata. Default is all. Apply both codec and container level croppping. This is the default mode. When doing stream copy, copy also non-key frames found at the beginning. Initialise a new hardware device of type called , using the given device parameters. If no name is specified it will receive a default name of the form \" %d\". The meaning of and the following arguments depends on the device type: is the number of the CUDA device. The following options are recognized: If set to 1, uses the primary device context instead of creating a new one. Choose the second device on the system. Choose the first device and use the primary device context. is the number of the Direct3D 9 display adapter. is the number of the Direct3D 11 display adapter. If not specified, it will attempt to use the default Direct3D 11 display adapter or the first Direct3D 11 display adapter whose hardware VendorId is specified by ‘ ’. Create a d3d11va device on the Direct3D 11 display adapter specified by index 1. Create a d3d11va device on the first Direct3D 11 display adapter whose hardware VendorId is 0x8086. is either an X11 display name, a DRM render node or a DirectX adapter index. If not specified, it will attempt to open the default X11 display ($DISPLAY) and then the first DRM render node (/dev/dri/renderD128), or the default DirectX adapter on Windows. The following options are recognized: When is not specified, use this option to specify the name of the kernel driver associated with the desired device. This option is available only when the hardware acceleration method drm and vaapi are enabled. When and are not specified, use this option to specify the vendor id associated with the desired device. This option is available only when the hardware acceleration method drm and vaapi are enabled and kernel_driver is not specified. Create a vaapi device on a device associated with kernel driver ‘ ’. Create a vaapi device on a device associated with vendor id ‘ ’. is an X11 display name. If not specified, it will attempt to open the default X11 display ($DISPLAY). selects a value in ‘ ’. Allowed values are: If not specified, ‘ ’ is used. (Note that it may be easier to achieve the desired result for QSV by creating the platform-appropriate subdevice (‘ ’ or ‘ ’ or ‘ ’) and then deriving a QSV device from that.) The following options are recognized: Specify a DRM render node on Linux or DirectX adapter on Windows. Choose platform-appropriate subdevice type. On Windows ‘ ’ is used as default subdevice type when is specified at configuration time, ‘ ’ is used as default subdevice type when is specified at configuration time. On Linux user can use ‘ ’ only as subdevice type. Choose the GPU subdevice with type ‘ ’ and create QSV device with ‘ ’. Choose the GPU subdevice with type ‘ ’ and create QSV device with ‘ ’. Create a QSV device with ‘ ’ on DirectX adapter 1 with subdevice type ‘ ’. Create a VAAPI device called ‘ ’ on , then derive a QSV device called ‘ ’ from device ‘ ’. selects the platform and device as platform_index.device_index. The set of devices can also be filtered using the key-value pairs to find only devices matching particular platform or device strings. The strings usable as filters are: The indices and filters must together uniquely select a device. Choose the second device on the first platform. Choose the device with a name containing the string Foo9000. Choose the GPU device on the second platform supporting the cl_khr_fp16 extension. If is an integer, it selects the device by its index in a system-dependent list of devices. If is any other string, it selects the first device with a name containing that string as a substring. The following options are recognized: If set to 1, enables the validation layer, if installed. If set to 1, images allocated by the hwcontext will be linear and locally mappable. A plus separated list of additional instance extensions to enable. A plus separated list of additional device extensions to enable. Choose the second device on the system. Choose the first device with a name containing the string RADV. Choose the first device and enable the Wayland and XCB instance extensions. Initialise a new hardware device of type called , deriving it from the existing device with the name . List all hardware device types supported in this build of ffmpeg. Pass the hardware device called to all filters in any filter graph. This can be used to set the device to upload to with the filter, or the device to map to with the filter. Other filters may also make use of this parameter when they require a hardware device. Note that this is typically only required when the input is not already in hardware frames - when it is, filters will derive the device they require from the context of the frames they receive as input. This is a global setting, so all filters will receive the same device. Use hardware acceleration to decode the matching stream(s). The allowed values of are: Do not use any hardware acceleration (the default). Use VDPAU (Video Decode and Presentation API for Unix) hardware acceleration. Use the Intel QuickSync Video acceleration for video transcoding. Unlike most other values, this option does not enable accelerated decoding (that is used automatically whenever a qsv decoder is selected), but accelerated transcoding, without copying the frames into the system memory. For it to work, both the decoder and the encoder must support QSV acceleration and no filters must be used. This option has no effect if the selected hwaccel is not available or not supported by the chosen decoder. Note that most acceleration methods are intended for playback and will not be faster than software decoding on modern CPUs. Additionally, will usually need to copy the decoded frames from the GPU memory into the system memory, resulting in further performance loss. This option is thus mainly useful for testing. Select a device to use for hardware acceleration. This option only makes sense when the option is also specified. It can either refer to an existing device created with by name, or it can create a new device as if ‘ ’ : were called immediately before. List all hardware acceleration components enabled in this build of ffmpeg. Actual runtime availability depends on the hardware and its suitable driver being installed. Set a specific output video stream as the heartbeat stream according to which to split and push through currently in-progress subtitle upon receipt of a random access packet. This lowers the latency of subtitles for which the end packet or the following subtitle has not yet been received. As a drawback, this will most likely lead to duplication of subtitle events in order to cover the full duration, so when dealing with use cases where latency of when the subtitle event is passed on to output is not relevant this option should not be utilized. Requires to be set for the relevant input subtitle stream for this to have any effect, as well as for the input subtitle stream having to be directly mapped to the same output in which the heartbeat stream resides.\n\nCreate one or more streams in the output file. This option has two forms for specifying the data source(s): the first selects one or more streams from some input file (specified with ), the second takes an output from some complex filtergraph (specified with ). In the first form, an output stream is created for every stream from the input file with the index . If is given, only those streams that match the specifier are used (see the Stream specifiers section for the syntax). A character before the stream identifier creates a \"negative\" mapping. It disables matching streams from already created mappings. An optional may be given after the stream specifier, which for multiview video specifies the view to be used. The view specifier may have one of the following formats: select a view by its ID; may be set to ’all’ to use all the views interleaved into one stream; select a view by its index; i.e. 0 is the base view, 1 is the first non-base view, etc. select a view by its display position; may be or The default for transcoding is to only use the base view, i.e. the equivalent of . For streamcopy, view specifiers are not supported and all views are always copied. A trailing after the stream index will allow the map to be optional: if the map matches no streams the map will be ignored instead of failing. Note the map will still fail if an invalid input file index is used; such as if the map refers to a non-existent input. An alternative form will map outputs from complex filter graphs (see the option) to the output file. must correspond to a defined output link label in the graph. This option may be specified multiple times, each adding more streams to the output file. Any given input stream may also be mapped any number of times as a source for different output streams, e.g. in order to use different encoding options and/or filters. The streams are created in the output in the same order in which the options are given on the commandline. Using this option disables the default mappings for this output file. To map ALL streams from the first input file to output If you have two audio streams in the first input file, these streams are identified by and . You can use to select which streams to place in an output file. For example: will map the second input stream in to the (single) output stream in . To select the stream with index 2 from input file (specified by the identifier ), and stream with index 6 from input (specified by the identifier ), and copy them to the output file : To select all video and the third audio stream from an input file: To map all the streams except the second audio, use negative mappings To map the video and audio streams from the first input, and using the trailing , ignore the audio mapping if no audio streams exist in the first input: Ignore input streams with unknown type instead of failing if copying such streams is attempted. Allow input streams with unknown type to be copied instead of failing if copying such streams is attempted. Set metadata information of the next output file from . Note that those are file indices (zero-based), not filenames. Optional parameters specify, which metadata to copy. A metadata specifier can have the following forms: global metadata, i.e. metadata that applies to the whole file per-stream metadata. is a stream specifier as described in the Stream specifiers chapter. In an input metadata specifier, the first matching stream is copied from. In an output metadata specifier, all matching streams are copied to. If metadata specifier is omitted, it defaults to global. By default, global metadata is copied from the first input file, per-stream and per-chapter metadata is copied along with streams/chapters. These default mappings are disabled by creating any mapping of the relevant type. A negative file index can be used to create a dummy mapping that just disables automatic copying. For example to copy metadata from the first stream of the input file to global metadata of the output file: To do the reverse, i.e. copy global metadata to all audio streams: Note that simple would work as well in this example, since global metadata is assumed by default. Copy chapters from input file with index to the next output file. If no chapter mapping is specified, then chapters are copied from the first input file with at least one chapter. Use a negative file index to disable any chapter copying. Show benchmarking information at the end of an encode. Shows real, system and user time used and maximum memory consumption. Maximum memory consumption is not supported on all systems, it will usually display as 0 if not supported. Show benchmarking information during the encode. Shows real, system and user time used in various steps (audio/video encode/decode). Exit after ffmpeg has been running for seconds in CPU user time. When dumping packets, also dump the payload. Its value is a floating-point positive number which represents the maximum duration of media, in seconds, that should be ingested in one second of wallclock time. Default value is zero and represents no imposed limitation on speed of ingestion. Value represents real-time speed and is equivalent to . Mainly used to simulate a capture device or live input stream (e.g. when reading from a file). Should not be used with a low value when input is an actual capture device or live stream as it may cause packet loss. It is useful for when flow speed of output packets is important, such as live streaming. Read input at native frame rate. This is equivalent to setting . Set an initial read burst time, in seconds, after which will be enforced. If either the input or output is blocked leading to actual read speed falling behind the specified readrate, then this rate takes effect till the input catches up with the specified readrate. Must not be lower than the primary readrate. Set video sync method / framerate mode. vsync is applied to all output video streams but can be overridden for a stream by setting fps_mode. vsync is deprecated and will be removed in the future. For compatibility reasons some of the values for vsync can be specified as numbers (shown in parentheses in the following table). Each frame is passed with its timestamp from the demuxer to the muxer. Frames will be duplicated and dropped to achieve exactly the requested constant frame rate. Frames are passed through with their timestamp or dropped so as to prevent 2 frames from having the same timestamp. Chooses between cfr and vfr depending on muxer capabilities. This is the default method. Note that the timestamps may be further modified by the muxer, after this. For example, in the case that the format option is enabled. With -map you can select from which stream the timestamps should be taken. You can leave either video or audio unchanged and sync the remaining stream(s) to the unchanged one. Frame drop threshold, which specifies how much behind video frames can be before they are dropped. In frame rate units, so 1.0 is one frame. The default is -1.1. One possible usecase is to avoid framedrops in case of noisy timestamps or to increase frame drop precision in case of exact timestamps. Pad the output audio stream(s). This is the same as applying . Argument is a string of filter parameters composed the same as with the filter. must be set for this output for the option to take effect. Do not process input timestamps, but keep their values without trying to sanitize them. In particular, do not remove the initial start time offset value. Note that, depending on the option or on specific muxer processing (e.g. in case the format option is enabled) the output timestamps may mismatch with the input timestamps even when this option is selected. When used with , shift input timestamps so they start at zero. This means that using e.g. will make output timestamps start at 50 seconds, regardless of what timestamp the input file started at. Specify how to set the encoder timebase when stream copying. is an integer numeric value, and can assume one of the following values: The time base is copied to the output encoder from the corresponding input demuxer. This is sometimes required to avoid non monotonically increasing timestamps when copying video streams with variable frame rate. The time base is copied to the output encoder from the corresponding input decoder. Try to make the choice automatically, in order to generate a sane output. Set the encoder timebase. can assume one of the following values: Assign a default value according to the media type. For video - use 1/framerate, for audio - use 1/samplerate. Use the timebase from the demuxer. Use the timebase from the filtergraph. Use the provided number as the timebase. This field can be provided as a ratio of two integers (e.g. 1:24, 1:48000) or as a decimal number (e.g. 0.04166, 2.0833e-5) Note that this option may require buffering frames, which introduces extra latency. The maximum amount of this latency may be controlled with the option. The option may require buffering potentially large amounts of data when at least one of the streams is \"sparse\" (i.e. has large gaps between frames – this is typically the case for subtitles). This option controls the maximum duration of buffered frames in seconds. Larger values may allow the option to produce more accurate results, but increase memory use and latency. The default value is 10 seconds. The timestamp discontinuity correction enabled by this option is only applied to input formats accepting timestamp discontinuity (for which the flag is enabled), e.g. MPEG-TS and HLS, and is automatically disabled when employing the option (unless wrapping is detected). If a timestamp discontinuity is detected whose absolute value is greater than , ffmpeg will remove the discontinuity by decreasing/increasing the current DTS and PTS by the corresponding delta value. The timestamp correction enabled by this option is only applied to input formats not accepting timestamp discontinuity (for which the flag is not enabled). If a timestamp discontinuity is detected whose absolute value is greater than , ffmpeg will drop the PTS/DTS timestamp value. The default value is (30 hours), which is arbitrarily picked and quite conservative. Assign a new stream-id value to an output stream. This option should be specified prior to the output filename to which it applies. For the situation where multiple output files exist, a streamid may be reassigned to a different value. For example, to set the stream 0 PID to 33 and the stream 1 PID to 36 for an output mpegts file: Apply bitstream filters to matching streams. The filters are applied to each packet as it is received from the demuxer (when used as an input option) or before it is sent to the muxer (when used as an output option). is a comma-separated list of bitstream filter specifications, each of the form Any of the ’,=:’ characters that are to be a part of an option value need to be escaped with a backslash. Use the option to get the list of bitstream filters. applies the bitstream filter (which converts MP4-encapsulated H.264 stream to Annex B) to the input video stream. applies the bitstream filter (which extracts text from MOV subtitles) to the output subtitle stream. Note, however, that since both examples use , it matters little whether the filters are applied on input or output - that would change if transcoding was happening. Specify Timecode for writing. is ’:’ for non drop timecode and ’;’ (or ’.’) for drop. Define a complex filtergraph, i.e. one with arbitrary number of inputs and/or outputs. For simple graphs – those with one input and one output of the same type – see the options. is a description of the filtergraph, as described in the “Filtergraph syntax” section of the ffmpeg-filters manual. This option may be specified multiple times - each use creates a new complex filtergraph. Inputs to a complex filtergraph may come from different source types, distinguished by the format of the corresponding link label:\n• To connect an input stream, use (i.e. the same syntax as ). If matches multiple streams, the first one will be used. For multiview video, the stream specifier may be followed by the view specifier, see documentation for the option for its syntax.\n• To connect a loopback decoder use [dec: ], where is the index of the loopback decoder to be connected to given input. For multiview video, the decoder index may be followed by the view specifier, see documentation for the option for its syntax.\n• To connect an output from another complex filtergraph, use its link label. E.g the following example:\n• (line 2) uses a complex filtergraph with one input and two outputs to scale the video to 1920x1080 and duplicate the result to both outputs;\n• (line 3) encodes one scaled output with and writes the result to ;\n• (line 5) places the output of the loopback decoder (i.e. the -encoded video) side by side with the scaled original input;\n• (line 6) combined video is then losslessly encoded and written into . Note that the two filtergraphs cannot be combined into one, because then there would be a cycle in the transcoding pipeline (filtergraph output goes to encoding, from there to decoding, then back to the same graph), and such cycles are not allowed. An unlabeled input will be connected to the first unused input stream of the matching type. Output link labels are referred to with . Unlabeled outputs are added to the first output file. Note that with this option it is possible to use only lavfi sources without normal input files. For example, to overlay an image over video Here refers to the first video stream in the first input file, which is linked to the first (main) input of the overlay filter. Similarly the first video stream in the second input is linked to the second (overlay) input of overlay. Assuming there is only one video stream in each input file, we can omit input labels, so the above is equivalent to Furthermore we can omit the output label and the single output from the filter graph will be added to the output file automatically, so we can simply write As a special exception, you can use a bitmap subtitle stream as input: it will be converted into a video with the same size as the largest video in the file, or 720x576 if no video is present. Note that this is an experimental and temporary solution. It will be removed once libavfilter has proper support for subtitles. For example, to hardcode subtitles on top of a DVB-T recording stored in MPEG-TS format, delaying the subtitles by 1 second: To generate 5 seconds of pure red video using lavfi source: Defines how many threads are used to process a filter_complex graph. Similar to filter_threads but used for graphs only. The default is the number of available CPUs. Define a complex filtergraph, i.e. one with arbitrary number of inputs and/or outputs. Equivalent to . This option enables or disables accurate seeking in input files with the option. It is enabled by default, so seeking is accurate when transcoding. Use to disable it, which may be useful e.g. when copying some streams and transcoding the others. This option enables or disables seeking by timestamp in input files with the option. It is disabled by default. If enabled, the argument to the option is considered an actual timestamp, and is not offset by the start time of the file. This matters only for files which do not start from timestamp 0, such as transport streams. For input, this option sets the maximum number of queued packets when reading from the file or device. With low latency / high rate live streams, packets may be discarded if they are not read in a timely manner; setting this value can force ffmpeg to use a separate input thread and read packets as soon as they arrive. By default ffmpeg only does this if multiple inputs are specified. For output, this option specified the maximum number of packets that may be queued to each muxing thread. Print sdp information for an output stream to . This allows dumping sdp information when at least one output isn’t an rtp stream. (Requires at least one of the output formats to be rtp). Allows discarding specific streams or frames from streams. Any input stream can be fully discarded, using value whereas selective discarding of frames from a stream occurs at the demuxer and is not supported by all demuxers. Stop and abort on various conditions. The following flags are available: No packets were passed to the muxer, the output is empty. No packets were passed to the muxer in some of the output streams. Set fraction of decoding frame failures across all inputs which when crossed ffmpeg will return exit code 69. Crossing this threshold does not terminate processing. Range is a floating-point number between 0 to 1. Default is 2/3. When transcoding audio and/or video streams, ffmpeg will not begin writing into the output until it has one packet for each such stream. While waiting for that to happen, packets for other streams are buffered. This option sets the size of this buffer, in packets, for the matching output stream. The default value of this option should be high enough for most uses, so only touch this option if you are sure that you need it. This is a minimum threshold until which the muxing queue size is not taken into account. Defaults to 50 megabytes per stream, and is based on the overall size of packets passed to the muxer. Enable automatically inserting format conversion filters in all filter graphs, including those defined by , , and . If filter format negotiation requires a conversion, the initialization of the filters will fail. Conversions can still be performed by inserting the relevant conversion filter (scale, aresample) in the graph. On by default, to explicitly disable it you need to specify . Declare the number of bits per raw sample in the given output stream to be . Note that this option sets the information provided to the encoder/muxer, it does not change the stream to conform to this value. Setting values that do not match the stream properties may result in encoding failures or invalid output files. Write per-frame encoding information about the matching streams into the file given by . writes information about raw video or audio frames right before they are sent for encoding, while writes information about encoded packets as they are received from the encoder. writes information about packets just as they are about to be sent to the muxer. Every frame or packet produces one line in the specified file. The format of this line is controlled by / / . When stats for multiple streams are written into a single file, the lines corresponding to different streams will be interleaved. The precise order of this interleaving is not specified and not guaranteed to remain stable between different invocations of the program, even with the same options. Specify the format for the lines written with / / . is a string that may contain directives of the form . is backslash-escaped — use \\{, \\}, and \\\\ to write a literal {, }, or \\, respectively, into the output. The directives given with may be one of the following: Index of the output stream in the file. Frame number. Pre-encoding: number of frames sent to the encoder so far. Post-encoding: number of packets received from the encoder so far. Muxing: number of packets submitted to the muxer for this stream so far. Input frame number. Index of the input frame (i.e. output by a decoder) that corresponds to this output frame or packet. -1 if unavailable. Timebase in which this frame/packet’s timestamps are expressed, as a rational number . Note that encoder and muxer may use different timebases. Timebase for , as a rational number . Available when is available, otherwise. Presentation timestamp of the frame or packet, as an integer. Should be multiplied by the timebase to compute presentation time. Presentation timestamp of the input frame (see ), as an integer. Should be multiplied by to compute presentation time. Printed as (2^63 - 1 = 9223372036854775807) when not available. Presentation time of the frame or packet, as a decimal number. Equal to multiplied by . Presentation time of the input frame (see ), as a decimal number. Equal to multiplied by . Printed as inf when not available. Decoding timestamp of the packet, as an integer. Should be multiplied by the timebase to compute presentation time. Decoding time of the frame or packet, as a decimal number. Equal to multiplied by . Number of audio samples sent to the encoder so far. Number of audio samples in the frame. Size of the encoded packet in bytes. Current bitrate in bits per second. Average bitrate for the whole stream so far, in bits per second, -1 if it cannot be determined at this point. Character ’K’ if the packet contains a keyframe, character ’N’ otherwise. Directives tagged with packet may only be used with and . Directives tagged with frame may only be used with . Directives tagged with audio may only be used with audio streams. In the future, new items may be added to the end of the default formatting strings. Users who depend on the format staying exactly the same, should prescribe it manually. Note that stats for different streams written into the same file may have different formats.\n\nA preset file contains a sequence of = pairs, one for each line, specifying a sequence of options which would be awkward to specify on the command line. Lines starting with the hash (’#’) character are ignored and are used to provide comments. Check the directory in the FFmpeg source tree for examples.\n\nThere are two types of preset files: ffpreset and avpreset files.\n\nffpreset files are specified with the , , , and options. The option takes the filename of the preset instead of a preset name as input and can be used for any kind of codec. For the , , and options, the options specified in a preset file are applied to the currently selected codec of the same type as the preset option.\n\nThe argument passed to the , , and preset options identifies the preset file to use according to the following rules:\n\nFirst ffmpeg searches for a file named .ffpreset in the directories (if set), and , and in the datadir defined at configuration time (usually ) or in a folder along the executable on win32, in that order. For example, if the argument is , it will search for the file .\n\nIf no such file is found, then ffmpeg will search for a file named - .ffpreset in the above-mentioned directories, where is the name of the codec to which the preset file options will be applied. For example, if you select the video codec with and use , then it will search for the file .\n\navpreset files are specified with the option. They work similar to ffpreset files, but they only allow encoder- specific options. Therefore, an = pair specifying an encoder cannot be used.\n\nWhen the option is specified, ffmpeg will look for files with the suffix .avpreset in the directories (if set), and , and in the datadir defined at configuration time (usually ), in that order.\n\nFirst ffmpeg searches for a file named - .avpreset in the above-mentioned directories, where is the name of the codec to which the preset file options will be applied. For example, if you select the video codec with and use , then it will search for the file .\n\nIf no such file is found, then ffmpeg will search for a file named .avpreset in the same directories.\n\nThe and options enable generation of a file containing statistics about the generated video outputs.\n\nThe option controls the format version of the generated file.\n\nWith version the format is:\n\nWith version the format is:\n\nThe value corresponding to each key is described below:\n\nSee also the -stats_enc options for an alternative way to show encoding statistics.\n\nIf you specify the input format and device then ffmpeg can grab video and audio directly.\n\nOr with an ALSA audio source (mono input, card id 1) instead of OSS:\n\nNote that you must activate the right video source and channel before launching ffmpeg with any TV viewer such as xawtv by Gerd Knorr. You also have to set the audio recording levels correctly with a standard mixer.\n\nGrab the X11 display with ffmpeg via\n\n0.0 is display.screen number of your X11 server, same as the DISPLAY environment variable.\n\n0.0 is display.screen number of your X11 server, same as the DISPLAY environment variable. 10 is the x-offset and 20 the y-offset for the grabbing.\n\nAny supported file format and protocol can serve as input to ffmpeg:\n• You can use YUV files as input: It will use the files: The Y files use twice the resolution of the U and V files. They are raw files, without header. They can be generated by all decent video decoders. You must specify the size of the image with the option if ffmpeg cannot guess it.\n• You can input from a raw YUV420P file: test.yuv is a file containing raw YUV planar data. Each frame is composed of the Y plane followed by the U and V planes at half vertical and horizontal resolution.\n• You can output to a raw YUV420P file:\n• You can set several input files and output files: Converts the audio file a.wav and the raw YUV video file a.yuv to MPEG file a.mpg.\n• You can also do audio and video conversions at the same time:\n• You can encode to several formats at the same time and define a mapping from input stream to output streams: Converts a.wav to a.mp2 at 64 kbits and to b.mp2 at 128 kbits. ’-map file:index’ specifies which input stream is used for each output stream, in the order of the definition of output streams.\n• You can transcode decrypted VOBs: This is a typical DVD ripping example; the input is a VOB file, the output an AVI file with MPEG-4 video and MP3 audio. Note that in this command we use B-frames so the MPEG-4 stream is DivX5 compatible, and GOP size is 300 which means one intra frame every 10 seconds for 29.97fps input video. Furthermore, the audio stream is MP3-encoded so you need to enable LAME support by passing to configure. The mapping is particularly useful for DVD transcoding to get the desired audio language. NOTE: To see the supported input formats, use .\n• You can extract images from a video, or create a video from many images: This will extract one video frame per second from the video and will output them in files named , , etc. Images will be rescaled to fit the new WxH values. If you want to extract just a limited number of frames, you can use the above command in combination with the or option, or in combination with -ss to start extracting from a certain point in time. For creating a video from many images: The syntax specifies to use a decimal number composed of three digits padded with zeroes to express the sequence number. It is the same syntax supported by the C printf function, but only formats accepting a normal integer are suitable. When importing an image sequence, -i also supports expanding shell-like wildcard patterns (globbing) internally, by selecting the image2-specific option. For example, for creating a video from filenames matching the glob pattern :\n• You can put many streams of the same type in the output: The resulting output file will contain the first four streams from the input files in reverse order.\n• The four options lmin, lmax, mblmin and mblmax use ’lambda’ units, but you may use the QP2LAMBDA constant to easily convert from ’q’ units:\n\nFor details about the authorship, see the Git history of the project (https://git.ffmpeg.org/ffmpeg), e.g. by typing the command in the FFmpeg source directory, or browsing the online repository at https://git.ffmpeg.org/ffmpeg.\n\nMaintainers for the specific components are listed in the file in the source code tree.\n\nThis document was generated on March 23, 2025 using makeinfo."
    },
    {
        "link": "https://ffmpeg.org/ffprobe-all.html",
        "document": "ffprobe gathers information from multimedia streams and prints it in human- and machine-readable fashion.\n\nFor example it can be used to check the format of the container used by a multimedia stream and the format and type of each media stream contained in it.\n\nIf a url is specified in input, ffprobe will try to open and probe the url content. If the url cannot be opened or recognized as a multimedia file, a positive exit code is returned.\n\nIf no output is specified as output with ffprobe will write to stdout.\n\nffprobe may be employed both as a standalone application or in combination with a textual filter, which may perform more sophisticated processing, e.g. statistical processing or plotting.\n\nOptions are used to list some of the formats supported by ffprobe or for specifying which information to display, and for setting how ffprobe will show it.\n\nffprobe output is designed to be easily parsable by a textual filter, and consists of one or more sections of a form defined by the selected writer, which is specified by the option.\n\nSections may contain other nested sections, and are identified by a name (which may be shared by other sections), and an unique name. See the output of .\n\nMetadata tags stored in the container or in the streams are recognized and printed in the corresponding \"FORMAT\", \"STREAM\", \"STREAM_GROUP_STREAM\" or \"PROGRAM_STREAM\" section.\n\nAll the numerical options, if not specified otherwise, accept a string representing a number as input, which may be followed by one of the SI unit prefixes, for example: ’K’, ’M’, or ’G’.\n\nIf ’i’ is appended to the SI unit prefix, the complete prefix will be interpreted as a unit prefix for binary multiples, which are based on powers of 1024 instead of powers of 1000. Appending ’B’ to the SI unit prefix multiplies the value by 8. This allows using, for example: ’KB’, ’MiB’, ’G’ and ’B’ as number suffixes.\n\nOptions which do not take arguments are boolean options, and set the corresponding value to true. They can be set to false by prefixing the option name with \"no\". For example using \"-nofoo\" will set the boolean option with name \"foo\" to false.\n\nOptions that take arguments support a special syntax where the argument given on the command line is interpreted as a path to the file from which the actual argument value is loaded. To use this feature, add a forward slash ’/’ immediately before the option name (after the leading dash). E.g.\n\nwill load a filtergraph description from the file named .\n\nSome options are applied per-stream, e.g. bitrate or codec. Stream specifiers are used to precisely specify which stream(s) a given option belongs to.\n\nA stream specifier is a string generally appended to the option name and separated from it by a colon. E.g. contains the stream specifier, which matches the second audio stream. Therefore, it would select the ac3 codec for the second audio stream.\n\nA stream specifier can match several streams, so that the option is applied to all of them. E.g. the stream specifier in matches all audio streams.\n\nAn empty stream specifier matches all streams. For example, or would copy all the streams without reencoding.\n\nPossible forms of stream specifiers are:\n\nThese options are shared amongst the ff* tools.\n\nThese options are provided directly by the libavformat, libavdevice and libavcodec libraries. To see the list of available AVOptions, use the option. They are separated into two categories:\n\nFor example to write an ID3v2.3 header instead of a default ID3v2.4 to an MP3 file, use the private option of the MP3 muxer:\n\nAll codec AVOptions are per-stream, and thus a stream specifier should be attached to them:\n\nIn the above example, a multichannel audio stream is mapped twice for output. The first instance is encoded with codec ac3 and bitrate 640k. The second instance is downmixed to 2 channels and encoded with codec aac. A bitrate of 128k is specified for it using absolute index of the output stream.\n\nNote: the syntax cannot be used for boolean AVOptions, use / .\n\nNote: the old undocumented way of specifying per-stream AVOptions by prepending v/a/s to the options name is now obsolete and will be removed soon.\n\nA writer defines the output format adopted by , and will be used for printing all the parts of the output.\n\nA writer may accept one or more arguments, which specify the options to adopt. The options are specified as a list of = pairs, separated by \":\".\n\nAll writers support the following options:\n\nA description of the currently available writers follows.\n\nPrint each section in the form:\n\nMetadata tags are printed as a line in the corresponding FORMAT, STREAM, STREAM_GROUP_STREAM or PROGRAM_STREAM section, and are prefixed by the string \"TAG:\".\n\nA description of the accepted options follows.\n\nThe writer is equivalent to , but supports different defaults.\n\nEach section is printed on a single line. If no option is specified, the output has the form:\n\nMetadata tags are printed in the corresponding \"format\" or \"stream\" section. A metadata tag key, if printed, is prefixed by the string \"tag:\".\n\nThe description of the accepted options follows.\n\nA free-form output where each line contains an explicit key=value, such as \"streams.stream.3.tags.foo=bar\". The output is shell escaped, so it can be directly embedded in sh scripts as long as the separator character is an alphanumeric character or an underscore (see option).\n\nThe description of the accepted options follows.\n\nThe following conventions are adopted:\n• all key and values are UTF-8\n• newline, ‘ ’, ‘ ’, ‘ ’ and the following characters are escaped\n• ‘ ’ is not used but usually parsed as key/value separator\n\nThis writer accepts options as a list of = pairs, separated by ‘ ’.\n\nThe description of the accepted options follows.\n\nEach section is printed using JSON notation.\n\nThe description of the accepted options follows.\n\nFor more information about JSON, see http://www.json.org/.\n\nThe XML output is described in the XML schema description file installed in the FFmpeg datadir.\n\nAn updated version of the schema can be retrieved at the url http://www.ffmpeg.org/schema/ffprobe.xsd, which redirects to the latest schema committed into the FFmpeg development source code tree.\n\nNote that the output issued will be compliant to the schema only when no special global output options ( , , , etc.) are specified.\n\nThe description of the accepted options follows.\n\nFor more information about the XML format, see https://www.w3.org/XML/.\n• MPEG1/2 timecode is extracted from the GOP, and is available in the video stream details ( , see ).\n• MOV timecode is extracted from tmcd track, so is available in the tmcd stream metadata ( , see ).\n• DV, GXF and AVI timecodes are available in format metadata ( , see ).\n\nThis section documents the syntax and formats employed by the FFmpeg libraries and tools.\n\nFFmpeg adopts the following quoting and escaping mechanism, unless explicitly specified. The following rules are applied:\n• ‘ ’ and ‘ ’ are special characters (respectively used for quoting and escaping). In addition to them, there might be other special characters depending on the specific syntax where the escaping and quoting are employed.\n• A special character is escaped by prefixing it with a ‘ ’.\n• All characters enclosed between ‘ ’ are included literally in the parsed string. The quote character ‘ ’ itself cannot be quoted, so you may need to close the quote and escape it.\n• Leading and trailing whitespaces, unless escaped or quoted, are removed from the parsed string.\n\nNote that you may need to add a second level of escaping when using the command line or a script, which depends on the syntax of the adopted shell language.\n\nThe function defined in can be used to parse a token quoted or escaped according to the rules defined above.\n\nThe tool in the FFmpeg source tree can be used to automatically quote or escape a string in a script.\n• Escape the string containing the special character:\n• The string above contains a quote, so the needs to be escaped when quoting it:\n• Include leading or trailing whitespaces using quoting: ' this string starts and ends with whitespaces '\n• Escaping and quoting can be mixed together:\n• To include a literal ‘ ’ you can use either escaping or quoting: 'c:\\foo' can be written as c:\\\\foo\n\nIf the value is \"now\" it takes the current time.\n\nTime is local time unless Z is appended, in which case it is interpreted as UTC. If the year-month-day part is not specified it takes the current year-month-day.\n\nThere are two accepted syntaxes for expressing time duration.\n\nexpresses the number of hours, the number of minutes for a maximum of 2 digits, and the number of seconds for a maximum of 2 digits. The at the end expresses decimal value for .\n\nexpresses the number of seconds, with the optional decimal part . The optional literal suffixes ‘ ’, ‘ ’ or ‘ ’ indicate to interpret the value as seconds, milliseconds or microseconds, respectively.\n\nIn both expressions, the optional ‘ ’ indicates negative duration.\n\nThe following examples are all valid time duration:\n\nSpecify the size of the sourced video, it may be a string of the form x , or the name of a size abbreviation.\n\nThe following abbreviations are recognized:\n\nSpecify the frame rate of a video, expressed as the number of frames generated per second. It has to be a string in the format / , an integer number, a float number or a valid video frame rate abbreviation.\n\nThe following abbreviations are recognized:\n\nA ratio can be expressed as an expression, or in the form : .\n\nNote that a ratio with infinite (1/0) or negative value is considered valid, so you should check on the returned value if you want to exclude those values.\n\nThe undefined value can be expressed using the \"0:0\" string.\n\nIt can be the name of a color as defined below (case insensitive match) or a sequence, possibly followed by @ and a string representing the alpha component.\n\nThe alpha component may be a string composed by \"0x\" followed by an hexadecimal number or a decimal number between 0.0 and 1.0, which represents the opacity value (‘ ’ or ‘ ’ means completely transparent, ‘ ’ or ‘ ’ completely opaque). If the alpha component is not specified then ‘ ’ is assumed.\n\nThe string ‘ ’ will result in a random color.\n\nThe following names of colors are recognized:\n\nA channel layout specifies the spatial disposition of the channels in a multi-channel audio stream. To specify a channel layout, FFmpeg makes use of a special syntax.\n\nIndividual channels are identified by an id, as given by the table below:\n\nStandard channel layout compositions can be specified by using the following identifiers:\n\nA custom channel layout can be specified as a sequence of terms, separated by ’+’. Each term can be:\n• the name of a single channel (e.g. ‘ ’, ‘ ’, ‘ ’, ‘ ’, etc.), each optionally containing a custom name after a ’@’, (e.g. ‘ ’, ‘ ’, ‘ ’, ‘ ’, etc.)\n\nA standard channel layout can be specified by the following:\n• the name of a single channel (e.g. ‘ ’, ‘ ’, ‘ ’, ‘ ’, etc.)\n• the name of a standard channel layout (e.g. ‘ ’, ‘ ’, ‘ ’, ‘ ’, ‘ ’, etc.)\n• a number of channels, in decimal, followed by ’c’, yielding the default channel layout for that number of channels (see the function ). Note that not all channel counts have a default layout.\n• a number of channels, in decimal, followed by ’C’, yielding an unknown channel layout with the specified number of channels. Note that not all channel layout specification strings support unknown channel layouts.\n• a channel layout mask, in hexadecimal starting with \"0x\" (see the macros in .\n\nBefore libavutil version 53 the trailing character \"c\" to specify a number of channels was optional, but now it is required, while a channel layout mask can also be specified as a decimal number (if and only if not followed by \"c\" or \"C\").\n\nSee also the function defined in .\n\nWhen evaluating an arithmetic expression, FFmpeg uses an internal formula evaluator, implemented through the interface.\n\nAn expression may contain unary, binary operators, constants, and functions.\n\nTwo expressions and can be combined to form another expression \" ; \". and are evaluated in turn, and the new expression evaluates to the value of .\n\nThe following binary operators are available: , , , , .\n\nThe following unary operators are available: , .\n\nSome internal variables can be used to store and load intermediary results. They can be accessed using the and functions with an index argument varying from 0 to 9 to specify which internal variable to access.\n\nThe following functions are available:\n\nThe following constants are available:\n\nAssuming that an expression is considered \"true\" if it has a non-zero value, note that:\n\nFor example the construct:\n\nIn your C code, you can extend the list of unary and binary functions, and define recognized constants, so that they are available for your expressions.\n\nThe evaluator also recognizes the International System unit prefixes. If ’i’ is appended after the prefix, binary prefixes are used, which are based on powers of 1024 instead of powers of 1000. The ’B’ postfix multiplies the value by 8, and can be appended after a unit prefix or used alone. This allows using for example ’KB’, ’MiB’, ’G’ and ’B’ as number postfix.\n\nThe list of available International System prefixes follows, with indication of the corresponding powers of 10 and of 2.\n\nlibavcodec provides some generic global options, which can be set on all the encoders and decoders. In addition, each codec may support so-called private options, which are specific for a given codec.\n\nSometimes, a global option may only affect a specific kind of codec, and may be nonsensical or ignored by another, so you need to be aware of the meaning of the specified options. Also some options are meant only for decoding or encoding.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the options or using the API for programmatic use.\n\nDecoders are configured elements in FFmpeg which allow the decoding of multimedia streams.\n\nWhen you configure your FFmpeg build, all the supported native decoders are enabled by default. Decoders requiring an external library must be enabled manually via the corresponding option. You can list all available decoders using the configure option .\n\nYou can disable all the decoders with the configure option and selectively enable / disable single decoders with the options / .\n\nThe option of the ff* tools will display the list of enabled decoders.\n\nA description of some of the currently available video decoders follows.\n\nThe decoder supports MV-HEVC multiview streams with at most two views. Views to be output are selected by supplying a list of view IDs to the decoder (the option). This option may be set either statically before decoder init, or from the callback - useful for the case when the view count or IDs change dynamically during decoding.\n\nOnly the base layer is decoded by default.\n\nNote that if you are using the CLI tool, you should be using view specifiers as documented in its manual, rather than the options documented here.\n\nlibdav1d allows libavcodec to decode the AOMedia Video 1 (AV1) codec. Requires the presence of the libdav1d headers and library during configuration. You need to explicitly configure the build with .\n\nThe following options are supported by the libdav1d wrapper.\n\nThis decoder allows libavcodec to decode AVS2 streams with davs2 library.\n\nlibuavs3d allows libavcodec to decode AVS3 streams. Requires the presence of the libuavs3d headers and library during configuration. You need to explicitly configure the build with .\n\nThe following option is supported by the libuavs3d wrapper.\n\nThis decoder requires the presence of the libxevd headers and library during configuration. You need to explicitly configure the build with .\n\nThe xevd project website is at https://github.com/mpeg5/xevd.\n\nThe following options are supported by the libxevd wrapper. The xevd-equivalent options or values are listed in parentheses for easy migration.\n\nTo get a more accurate and extensive documentation of the libxevd options, invoke the command or consult the libxevd documentation.\n\nThe following options are supported by all qsv decoders.\n\nA description of some of the currently available audio decoders follows.\n\nThis decoder implements part of ATSC A/52:2010 and ETSI TS 102 366, as well as the undocumented RealAudio 3 (a.k.a. dnet).\n\nThis decoder aims to implement the complete FLAC specification from Xiph.\n\nThis decoder generates wave patterns according to predefined sequences. Its use is purely internal and the format of the data it accepts is not publicly documented.\n\nlibcelt allows libavcodec to decode the Xiph CELT ultra-low delay audio codec. Requires the presence of the libcelt headers and library during configuration. You need to explicitly configure the build with .\n\nlibgsm allows libavcodec to decode the GSM full rate audio codec. Requires the presence of the libgsm headers and library during configuration. You need to explicitly configure the build with .\n\nThis decoder supports both the ordinary GSM and the Microsoft variant.\n\nlibilbc allows libavcodec to decode the Internet Low Bitrate Codec (iLBC) audio codec. Requires the presence of the libilbc headers and library during configuration. You need to explicitly configure the build with .\n\nThe following option is supported by the libilbc wrapper.\n\nlibopencore-amrnb allows libavcodec to decode the Adaptive Multi-Rate Narrowband audio codec. Using it requires the presence of the libopencore-amrnb headers and library during configuration. You need to explicitly configure the build with .\n\nAn FFmpeg native decoder for AMR-NB exists, so users can decode AMR-NB without this library.\n\nlibopencore-amrwb allows libavcodec to decode the Adaptive Multi-Rate Wideband audio codec. Using it requires the presence of the libopencore-amrwb headers and library during configuration. You need to explicitly configure the build with .\n\nAn FFmpeg native decoder for AMR-WB exists, so users can decode AMR-WB without this library.\n\nlibopus allows libavcodec to decode the Opus Interactive Audio Codec. Requires the presence of the libopus headers and library during configuration. You need to explicitly configure the build with .\n\nAn FFmpeg native decoder for Opus exists, so users can decode Opus without this library.\n\nImplements profiles A and C of the ARIB STD-B24 standard.\n\nYet another ARIB STD-B24 caption decoder using external libaribcaption library.\n\nImplements profiles A and C of the Japanse ARIB STD-B24 standard, Brazilian ABNT NBR 15606-1, and Philippines version of ISDB-T.\n\nRequires the presence of the libaribcaption headers and library (https://github.com/xqq/libaribcaption) during configuration. You need to explicitly configure the build with . If both libaribb24 and libaribcaption are enabled, libaribcaption decoder precedes.\n\nThis codec decodes the bitmap subtitles used in DVDs; the same subtitles can also be found in VobSub file pairs and in some Matroska files.\n\nLibzvbi allows libavcodec to decode DVB teletext pages and DVB teletext subtitles. Requires the presence of the libzvbi headers and library during configuration. You need to explicitly configure the build with .\n\nWhen you configure your FFmpeg build, all the supported bitstream filters are enabled by default. You can list all available ones using the configure option .\n\nYou can disable all the bitstream filters using the configure option , and selectively enable any bitstream filter using the option , or you can disable a particular bitstream filter using the option .\n\nThe option of the ff* tools will display the list of all the supported bitstream filters included in your build.\n\nThe ff* tools have a -bsf option applied per stream, taking a comma-separated list of filters, whose parameters follow the filter name after a ’=’.\n\nBelow is a description of the currently available bitstream filters, with their parameters, if any.\n\nThis filter creates an MPEG-4 AudioSpecificConfig from an MPEG-2/4 ADTS header and removes the ADTS header.\n\nThis filter is required for example when copying an AAC stream from a raw ADTS AAC or an MPEG-TS container to MP4A-LATM, to an FLV file, or to MOV/MP4 files and related formats such as 3GP or M4A. Please note that it is auto-inserted for MP4A-LATM and MOV/MP4 and related formats.\n\nRemove zero padding at the end of a packet.\n\nExtract the core from a DCA/DTS stream, dropping extensions such as DTS-HD.\n\nAdd extradata to the beginning of the filtered packets except when said packets already exactly begin with the extradata that is intended to be added.\n\nIf not specified it is assumed ‘ ’.\n\nFor example the following command forces a global header (thus disabling individual packet headers) in the H.264 packets generated by the encoder, but corrects them by adding the header stored in extradata to the key packets:\n\nBlocks in DV which are marked as damaged are replaced by blocks of the specified color.\n\nCertain codecs allow the long-term headers (e.g. MPEG-2 sequence headers, or H.264/HEVC (VPS/)SPS/PPS) to be transmitted either \"in-band\" (i.e. as a part of the bitstream containing the coded frames) or \"out of band\" (e.g. on the container level). This latter form is called \"extradata\" in FFmpeg terminology.\n\nThis bitstream filter detects the in-band headers and makes them available as extradata.\n\nRemove units with types in or not in a given set from the stream.\n\nThe types used by pass_types and remove_types correspond to NAL unit types (nal_unit_type) in H.264, HEVC and H.266 (see Table 7-1 in the H.264 and HEVC specifications or Table 5 in the H.266 specification), to marker values for JPEG (without 0xFF prefix) and to start codes without start code prefix (i.e. the byte following the 0x000001) for MPEG-2. For VP8 and VP9, every unit has type zero.\n\nExtradata is unchanged by this transformation, but note that if the stream contains inline parameter sets then the output may be unusable if they are removed.\n\nFor example, to remove all non-VCL NAL units from an H.264 stream:\n\nTo remove all AUDs, SEI and filler from an H.265 stream:\n\nTo remove all user data from a MPEG-2 stream, including Closed Captions:\n\nTo remove all SEI from a H264 stream, including Closed Captions:\n\nTo remove all prefix and suffix SEI from a HEVC stream, including Closed Captions and dynamic HDR:\n\nExtract Rgb or Alpha part of an HAPQA file, without recompression, in order to create an HAPQ or an HAPAlphaOnly file.\n\nConvert an H.264 bitstream from length prefixed mode to start code prefixed mode (as defined in the Annex B of the ITU-T H.264 specification).\n\nThis is required by some streaming formats, typically the MPEG-2 transport stream format (muxer ).\n\nFor example to remux an MP4 file containing an H.264 stream to mpegts format with , you can use the command:\n\nPlease note that this filter is auto-inserted for MPEG-TS (muxer ) and raw H.264 (muxer ) output formats.\n\nThis applies a specific fixup to some Blu-ray BDMV H264 streams which contain redundant PPSs. The PPSs modify irrelevant parameters of the stream, confusing other transformations which require the correct extradata.\n\nThe encoder used on these impacted streams adds extra PPSs throughout the stream, varying the initial QP and whether weighted prediction was enabled. This causes issues after copying the stream into a global header container, as the starting PPS is not suitable for the rest of the stream. One side effect, for example, is seeking will return garbled output until a new PPS appears.\n\nThis BSF removes the extra PPSs and rewrites the slice headers such that the stream uses a single leading PPS in the global header, which resolves the issue.\n\nConvert an HEVC/H.265 bitstream from length prefixed mode to start code prefixed mode (as defined in the Annex B of the ITU-T H.265 specification).\n\nThis is required by some streaming formats, typically the MPEG-2 transport stream format (muxer ).\n\nFor example to remux an MP4 file containing an HEVC stream to mpegts format with , you can use the command:\n\nPlease note that this filter is auto-inserted for MPEG-TS (muxer ) and raw HEVC/H.265 (muxer or ) output formats.\n\nModifies the bitstream to fit in MOV and to be usable by the Final Cut Pro decoder. This filter only applies to the mpeg2video codec, and is likely not needed for Final Cut Pro 7 and newer with the appropriate .\n\nFor example, to remux 30 MB/sec NTSC IMX to MOV:\n\nMJPEG is a video codec wherein each video frame is essentially a JPEG image. The individual frames can be extracted without loss, e.g. by\n\nUnfortunately, these chunks are incomplete JPEG images, because they lack the DHT segment required for decoding. Quoting from http://www.digitalpreservation.gov/formats/fdd/fdd000063.shtml:\n\nAvery Lee, writing in the rec.video.desktop newsgroup in 2001, commented that \"MJPEG, or at least the MJPEG in AVIs having the MJPG fourcc, is restricted JPEG with a fixed – and *omitted* – Huffman table. The JPEG must be YCbCr colorspace, it must be 4:2:2, and it must use basic Huffman encoding, not arithmetic or progressive. . . . You can indeed extract the MJPEG frames and decode them with a regular JPEG decoder, but you have to prepend the DHT segment to them, or else the decoder won’t have any idea how to decompress the data. The exact table necessary is given in the OpenDML spec.\"\n\nThis bitstream filter patches the header of frames extracted from an MJPEG stream (carrying the AVI1 header ID and lacking a DHT segment) to produce fully qualified JPEG images.\n\nAdd an MJPEG A header to the bitstream, to enable decoding by Quicktime.\n\nExtract a representable text file from MOV subtitles, stripping the metadata header from each subtitle packet.\n\nSee also the text2movsub filter.\n\nDivX-style packed B-frames are not valid MPEG-4 and were only a workaround for the broken Video for Windows subsystem. They use more space, can cause minor AV sync issues, require more CPU power to decode (unless the player has some decoded picture queue to compensate the 2,0,2,0 frame per packet style) and cause trouble if copied into a standard container like mp4 or mpeg-ps/ts, because MPEG-4 decoders may not be able to decode them, since they are not valid MPEG-4.\n\nFor example to fix an AVI file containing an MPEG-4 stream with DivX-style packed B-frames using , you can use the command:\n\nDamages the contents of packets or simply drops them without damaging the container. Can be used for fuzzing or testing error resilience/concealment.\n\nBoth and accept expressions containing the following variables:\n\nApply modification to every byte but don’t drop any packets.\n\nDrop every video packet not marked as a keyframe after timestamp 30s but do not modify any of the remaining packets.\n\nDrop one second of audio every 10 seconds and add some random noise to the rest.\n\nThis bitstream filter passes the packets through unchanged.\n\nRepacketize PCM audio to a fixed number of samples per packet or a fixed packet rate per second. This is similar to the (ffmpeg-filters)asetnsamples audio filter but works on audio packets instead of audio frames.\n\nYou can generate the well known 1602-1601-1602-1601-1602 pattern of 48kHz audio for NTSC frame rate using the option.\n\nMerge a sequence of PGS Subtitle segments ending with an \"end of display set\" segment into a single packet.\n\nThis is required by some containers that support PGS subtitles (muxer ).\n\nSet Rec709 colorspace for each frame of the file\n\nSet Hybrid Log-Gamma parameters for each frame of the file\n\nIt accepts the following parameter:\n\nIt accepts the following parameters:\n\nThe expressions are evaluated through the eval API and can contain the following constants:\n\nFor example, to set PTS equal to DTS (not recommended if B-frames are involved):\n\nLog basic packet information. Mainly useful for testing, debugging, and development.\n\nConvert text subtitles to MOV subtitles (as used by the codec) with metadata headers.\n\nSee also the mov2textsub filter.\n\nLog trace output containing all syntax elements in the coded stream headers (everything above the level of individual coded blocks). This can be useful for debugging low-level stream issues.\n\nSupports AV1, H.264, H.265, (M)JPEG, MPEG-2 and VP9, but depending on the build only a subset of these may be available.\n\nMerge VP9 invisible (alt-ref) frames back into VP9 superframes. This fixes merging of split/segmented VP9 streams where the alt-ref frame was split from its visible counterpart.\n\nGiven a VP9 stream with correct timestamps but possibly out of order, insert additional show-existing-frame packets to correct the ordering.\n\nThe libavformat library provides some generic global options, which can be set on all the muxers and demuxers. In addition each muxer or demuxer may support so-called private options, which are specific for that component.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the options or using the API for programmatic use.\n\nThe list of supported options follows:\n\nFormat stream specifiers allow selection of one or more streams that match specific properties.\n\nThe exact semantics of stream specifiers is defined by the function declared in the header and documented in the (ffmpeg)Stream specifiers section in the ffmpeg(1) manual.\n\nDemuxers are configured elements in FFmpeg that can read the multimedia streams from a particular type of file.\n\nWhen you configure your FFmpeg build, all the supported demuxers are enabled by default. You can list all available ones using the configure option .\n\nYou can disable all the demuxers using the configure option , and selectively enable a single demuxer with the option , or disable it with the option .\n\nThe option of the ff* tools will display the list of enabled demuxers. Use to view a combined list of enabled demuxers and muxers.\n\nThe description of some of the currently available demuxers follows.\n\nThis demuxer is used to demux Audible Format 2, 3, and 4 (.aa) files.\n\nThis demuxer is used to demux an ADTS input containing a single AAC stream alongwith any ID3v1/2 or APE tags in it.\n\nThis demuxer is used to demux APNG files. All headers, but the PNG signature, up to (but not including) the first fcTL chunk are transmitted as extradata. Frames are then split as being all the chunks between two fcTL ones, or between the last fcTL and IEND chunks.\n\nThis demuxer is used to demux ASF files and MMS network streams.\n\nThis demuxer reads a list of files and other directives from a text file and demuxes them one after the other, as if all their packets had been muxed together.\n\nThe timestamps in the files are adjusted so that the first file starts at 0 and each next file starts where the previous one finishes. Note that it is done globally and may cause gaps if all streams do not have exactly the same length.\n\nAll files must have the same streams (same codecs, same time base, etc.).\n\nThe duration of each file is used to adjust the timestamps of the next file: if the duration is incorrect (because it was computed using the bit-rate or because the file is truncated, for example), it can cause artifacts. The directive can be used to override the duration stored in each file.\n\nThe script is a text file in extended-ASCII, with one directive per line. Empty lines, leading spaces and lines starting with ’#’ are ignored. The following directive is recognized:\n\nThis demuxer accepts the following option:\n• Use absolute filenames and include some comments: # my first filename file /mnt/share/file-1.wav # my second filename including whitespace file '/mnt/share/file 2.wav' # my third filename including whitespace plus single quote file '/mnt/share/file 3'\\''.wav'\n• Allow for input format auto-probing, use safe filenames and set the duration of the first file:\n\nThis demuxer presents all AVStreams found in the manifest. By setting the discard flags on AVStreams the caller can decide which streams to actually receive. Each stream mirrors the and properties from the as metadata keys named \"id\" and \"variant_bitrate\" respectively.\n\nThis demuxer accepts the following option:\n\nCan directly ingest DVD titles, specifically sequential PGCs, into a conversion pipeline. Menu assets, such as background video or audio, can also be demuxed given the menu’s coordinates (at best effort).\n\nBlock devices (DVD drives), ISO files, and directory structures are accepted. Activate with in front of one of these inputs.\n\nThis demuxer does NOT have decryption code of any kind. You are on your own working with encrypted DVDs, and should not expect support on the matter.\n\nUnderlying playback is handled by libdvdnav, and structure parsing by libdvdread. FFmpeg must be built with GPL library support available as well as the configure switches and .\n\nYou will need to provide either the desired \"title number\" or exact PGC/PG coordinates. Many open-source DVD players and tools can aid in providing this information. If not specified, the demuxer will default to title 1 which works for many discs. However, due to the flexibility of the format, it is recommended to check manually. There are many discs that are authored strangely or with invalid headers.\n\nIf the input is a real DVD drive, please note that there are some drives which may silently fail on reading bad sectors from the disc, returning random bits instead which is effectively corrupt data. This is especially prominent on aging or rotting discs. A second pass and integrity checks would be needed to detect the corruption. This is not an FFmpeg issue.\n\nDVD-Video is not a directly accessible, linear container format in the traditional sense. Instead, it allows for complex and programmatic playback of carefully muxed MPEG-PS streams that are stored in headerless VOB files. To the end-user, these streams are known simply as \"titles\", but the actual logical playback sequence is defined by one or more \"PGCs\", or Program Group Chains, within the title. The PGC is in turn comprised of multiple \"PGs\", or Programs\", which are the actual video segments (and for a typical video feature, sequentially ordered). The PGC structure, along with stream layout and metadata, are stored in IFO files that need to be parsed. PGCs can be thought of as playlists in easier terms.\n\nAn actual DVD player relies on user GUI interaction via menus and an internal VM to drive the direction of demuxing. Generally, the user would either navigate (via menus) or automatically be redirected to the PGC of their choice. During this process and the subsequent playback, the DVD player’s internal VM also maintains a state and executes instructions that can create jumps to different sectors during playback. This is why libdvdnav is involved, as a linear read of the MPEG-PS blobs on the disc (VOBs) is not enough to produce the right sequence in many cases.\n\nThere are many other DVD structures (a long subject) that will not be discussed here. NAV packets, in particular, are handled by this demuxer to build accurate timing but not emitted as a stream. For a good high-level understanding, refer to: https://code.videolan.org/videolan/libdvdnav/-/blob/master/doc/dvd_structures\n\nThis demuxer accepts the following options:\n• Open chapters 3-6 from title 1 from a given DVD structure:\n• Open only chapter 5 from title 1 from a given DVD structure:\n• Demux menu with language 1 from VTS 1, PGC 1, starting at PG 1:\n\nThis format is used by various Electronic Arts games.\n\nThis demuxer presents audio and video streams found in an IMF Composition, as specified in SMPTE ST 2067-2.\n\nIf is not specified, the demuxer looks for a file called in the same directory as the CPL.\n\nThis demuxer is used to demux FLV files and RTMP network streams. In case of live network streams, if you force format, you may use live_flv option instead of flv to survive timestamp discontinuities. KUX is a flv variant used on the Youku platform.\n\nIt accepts the following options:\n\nFor example, with the overlay filter, place an infinitely looping GIF over another video:\n\nNote that in the above example the shortest option for overlay filter is used to end the output video at the length of the shortest input file, which in this case is as the GIF in this example loops infinitely.\n\nThis demuxer presents all AVStreams from all variant streams. The id field is set to the bitrate variant index number. By setting the discard flags on AVStreams (by pressing ’a’ or ’v’ in ffplay), the caller can decide which variant streams to actually receive. The total bitrate of the variant that the stream belongs to is available in a metadata key named \"variant_bitrate\".\n\nIt accepts the following options:\n\nThis demuxer reads from a list of image files specified by a pattern. The syntax and meaning of the pattern is specified by the option .\n\nThe pattern may contain a suffix which is used to automatically determine the format of the images contained in the files.\n\nThe size, the pixel format, and the format of each image must be the same for all the files in the sequence.\n\nThis demuxer accepts the following options:\n• Use for creating a video from the images in the file sequence , , ..., assuming an input frame rate of 10 frames per second:\n• As above, but start by reading from a file with index 100 in the sequence:\n• Read images matching the \"*.png\" glob pattern , that is all the files terminating with the \".png\" suffix:\n\nThe Game Music Emu library is a collection of video game music file emulators.\n\nSee https://bitbucket.org/mpyne/game-music-emu/overview for more information.\n\nIt accepts the following options:\n\nIt will export one 2-channel 16-bit 44.1 kHz audio stream. Optionally, a 16-color video stream can be exported with or without printed metadata.\n\nIt accepts the following options:\n\nSee https://lib.openmpt.org/libopenmpt/ for more information.\n\nSome files have multiple subsongs (tracks) this can be set with the option.\n\nIt accepts the following options:\n\nDemuxer for Quicktime File Format & ISO/IEC Base Media File Format (ISO/IEC 14496-12 or MPEG-4 Part 12, ISO/IEC 15444-12 or JPEG 2000 Part 12).\n\nThis demuxer accepts the following options:\n\nAudible AAX files are encrypted M4B files, and they can be decrypted by specifying a 4 byte activation secret.\n\nThis demuxer accepts the following options:\n\nThis demuxer allows reading of MJPEG, where each frame is represented as a part of multipart/x-mixed-replace stream.\n\nThis demuxer allows one to read raw video data. Since there is no header specifying the assumed video parameters, the user must specify them in order to be able to decode the data correctly.\n\nThis demuxer accepts the following options:\n\nFor example to read a rawvideo file with , assuming a pixel format of , a video size of , and a frame rate of 10 images per second, use the command:\n\nRCWT (Raw Captions With Time) is a format native to ccextractor, a commonly used open source tool for processing 608/708 Closed Captions (CC) sources. For more information on the format, see (ffmpeg-formats)rcwtenc.\n\nThis demuxer implements the specification as of March 2024, which has been stable and unchanged since April 2014.\n• Render CC to ASS using the built-in decoder: Note that if your output appears to be empty, you may have to manually set the decoder’s option to pick the desired CC substream.\n• Convert an RCWT backup to Scenarist (SCC) format: Note that the SCC format does not support all of the possible CC extensions that can be stored in RCWT (such as EIA-708).\n\nThis demuxer reads the script language used by SBaGen http://uazu.net/sbagen/ to generate binaural beats sessions. A SBG script looks like that:\n\nA SBG script can mix absolute and relative timestamps. If the script uses either only absolute timestamps (including the script start time) or only relative ones, then its layout is fixed, and the conversion is straightforward. On the other hand, if the script mixes both kind of timestamps, then the reference for relative timestamps will be taken from the current time of day at the time the script is read, and the script layout will be frozen according to that reference. That means that if the script is directly played, the actual times will match the absolute timestamps up to the sound controller’s clock accuracy, but if the user somehow pauses the playback or seeks, all times will be shifted accordingly.\n\nTED does not provide links to the captions, but they can be guessed from the page. The file from the FFmpeg source tree contains a bookmarklet to expose them.\n\nThis demuxer accepts the following option:\n\nExample: convert the captions to a format most players understand:\n\nDue to security concerns, Vapoursynth scripts will not be autodetected so the input format has to be forced. For ff* CLI tools, add before the input .\n\nThis demuxer accepts the following option:\n\nThis demuxer accepts the following options:\n\nThis demuxer accepts the following options:\n\nFFmpeg is able to dump metadata from media files into a simple UTF-8-encoded INI-like text file and then load it back using the metadata muxer/demuxer.\n\nThe file format is as follows:\n• A file consists of a header and a number of metadata tags divided into sections, each on its own line.\n• The header is a ‘ ’ string, followed by a version number (now 1).\n• Metadata tags are of the form ‘ ’\n• After global metadata there may be sections with per-stream/per-chapter metadata.\n• A section starts with the section name in uppercase (i.e. STREAM or CHAPTER) in brackets (‘ ’, ‘ ’) and ends with next section or end of file.\n• At the beginning of a chapter section there may be an optional timebase to be used for start/end values. It must be in form ‘ ’, where and are integers. If the timebase is missing then start/end times are assumed to be in nanoseconds. Next a chapter section must contain chapter start and end times in form ‘ ’, ‘ ’, where is a positive integer.\n• Empty lines and lines starting with ‘ ’ or ‘ ’ are ignored.\n• Metadata keys or values containing special characters (‘ ’, ‘ ’, ‘ ’, ‘ ’ and a newline) must be escaped with a backslash ‘ ’.\n• Note that whitespace in metadata (e.g. ‘ ’) is considered to be a part of the tag (in the example above key is ‘ ’, value is ‘ ’).\n\nA ffmetadata file might look like this:\n\nBy using the ffmetadata muxer and demuxer it is possible to extract metadata from an input file to an ffmetadata file, and then transcode the file into an output file with the edited ffmetadata file.\n\nExtracting an ffmetadata file with goes as follows:\n\nReinserting edited metadata information from the FFMETADATAFILE file can be done as:\n\nThe libavformat library provides some generic global options, which can be set on all the protocols. In addition each protocol may support so-called private options, which are specific for that component.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the options or using the API for programmatic use.\n\nThe list of supported options follows:\n\nProtocols are configured elements in FFmpeg that enable access to resources that require specific protocols.\n\nWhen you configure your FFmpeg build, all the supported protocols are enabled by default. You can list all available ones using the configure option \"–list-protocols\".\n\nYou can disable all the protocols using the configure option \"–disable-protocols\", and selectively enable a protocol using the option \"–enable-protocol= \", or you can disable a particular protocol using the option \"–disable-protocol= \".\n\nThe option \"-protocols\" of the ff* tools will display the list of supported protocols.\n\nAll protocols accept the following options:\n\nA description of the currently available protocols follows.\n\nFFmpeg must be compiled with –enable-librabbitmq to support AMQP. A separate AMQP broker must also be run. An example open-source AMQP broker is RabbitMQ.\n\nAfter starting the broker, an FFmpeg client may stream data to the broker using the command:\n\nWhere hostname and port (default is 5672) is the address of the broker. The client may also set a user/password for authentication. The default for both fields is \"guest\". Name of virtual host on broker can be set with vhost. The default value is \"/\".\n\nMuliple subscribers may stream from the broker using the command:\n\nIn RabbitMQ all data published to the broker flows through a specific exchange, and each subscribing client has an assigned queue/buffer. When a packet arrives at an exchange, it may be copied to a client’s queue depending on the exchange and routing_key fields.\n\nThe following options are supported:\n\nFill data in a background thread, to decouple I/O operation from demux thread.\n\nRead angle 2 of playlist 4 from BluRay mounted to /mnt/bluray, start from chapter 2:\n\nCache the input stream to temporary file. It brings seeking capability to live streams.\n\nRead and seek from many resources in sequence as if they were a unique resource.\n\nA URL accepted by this protocol has the syntax:\n\nwhere , , ..., are the urls of the resource to be concatenated, each one possibly specifying a distinct protocol.\n\nFor example to read a sequence of files , , with use the command:\n\nNote that you may need to escape the character \"|\" which is special for many shells.\n\nRead and seek from many resources in sequence as if they were a unique resource.\n\nA URL accepted by this protocol has the syntax:\n\nwhere is the url containing a line break delimited list of resources to be concatenated, each one possibly specifying a distinct protocol. Special characters must be escaped with backslash or single quotes. See (ffmpeg-utils)the \"Quoting and escaping\" section in the ffmpeg-utils(1) manual.\n\nFor example to read a sequence of files , , listed in separate lines within a file with use the command:\n\nWhere contains the lines:\n\nData in-line in the URI. See http://en.wikipedia.org/wiki/Data_URI_scheme.\n\nFor example, to convert a GIF file given inline with :\n\nIf is not specified, by default the stdout file descriptor will be used for writing, stdin for reading. Unlike the pipe protocol, fd protocol has seek support if it corresponding to a regular file. fd protocol doesn’t support pass file descriptor via URL for security.\n\nThis protocol accepts the following options:\n\nRead from or write to a file.\n\nA file URL can have the form:\n\nwhere is the path of the file to read.\n\nAn URL that does not have a protocol prefix will be assumed to be a file URL. Depending on the build, an URL that looks like a Windows path with the drive letter at the beginning will also be assumed to be a file URL (usually not the case in builds for unix-like systems).\n\nFor example to read from a file with use the command:\n\nThis protocol accepts the following options:\n\nRead from or write to remote resources using FTP protocol.\n\nThis protocol accepts the following options.\n\nNOTE: Protocol can be used as output, but it is recommended to not do it, unless special care is taken (tests, customized server configuration etc.). Different FTP servers behave in different way during seek operation. ff* tools may produce incomplete content due to server limitations.\n\nRead Apple HTTP Live Streaming compliant segmented stream as a uniform one. The M3U8 playlists describing the segments can be remote HTTP resources or local files, accessed using the standard file protocol. The nested protocol is declared by specifying \"+ \" after the hls URI scheme name, where is either \"file\" or \"http\".\n\nUsing this protocol is discouraged - the hls demuxer should work just as well (if not, please report the issues) and is more complete. To use the hls demuxer instead, simply use the direct URLs to the m3u8 files.\n\nThis protocol accepts the following options:\n\nSome HTTP requests will be denied unless cookie values are passed in with the request. The option allows these cookies to be specified. At the very least, each cookie must specify a value along with a path and domain. HTTP requests that match both the domain and path will automatically include the cookie value in the HTTP Cookie header field. Multiple cookies can be delimited by a newline.\n\nThe required syntax to play a stream specifying a cookie is:\n\nThis protocol accepts the following options:\n\nInterPlanetary File System (IPFS) protocol support. One can access files stored on the IPFS network through so-called gateways. These are http(s) endpoints. This protocol wraps the IPFS native protocols (ipfs:// and ipns://) to be sent to such a gateway. Users can (and should) host their own node which means this protocol will use one’s local gateway to access files on the IPFS network.\n\nThis protocol accepts the following options:\n\nOne can use this protocol in 2 ways. Using IPFS:\n\nOr the IPNS protocol (IPNS is mutable IPFS):\n\nComputes the MD5 hash of the data to be written, and on close writes this to the designated output or stdout if none is specified. It can be used to test muxers without writing an actual file.\n\nNote that some formats (typically MOV) require the output protocol to be seekable, so they will fail with the MD5 output protocol.\n\nIf isn’t specified, is the number corresponding to the file descriptor of the pipe (e.g. 0 for stdin, 1 for stdout, 2 for stderr). If is not specified, by default the stdout file descriptor will be used for writing, stdin for reading.\n\nFor example to read from stdin with :\n\nFor writing to stdout with :\n\nThis protocol accepts the following options:\n\nNote that some formats (typically MOV), require the output protocol to be seekable, so they will fail with the pipe output protocol.\n\nThe Pro-MPEG CoP#3 FEC is a 2D parity-check forward error correction mechanism for MPEG-2 Transport Streams sent over RTP.\n\nThis protocol must be used in conjunction with the muxer and the protocol.\n\nThe destination UDP ports are for the column FEC stream and for the row FEC stream.\n\nThis protocol accepts the following options:\n\nThe Real-Time Messaging Protocol (RTMP) is used for streaming multimedia content across a TCP/IP network.\n\nAdditionally, the following parameters can be set via command line options (or in code via s):\n\nFor example to read with a multimedia resource named \"sample\" from the application \"vod\" from an RTMP server \"myserver\":\n\nTo publish to a password protected server, passing the playpath and app names separately:\n\nThe Encrypted Real-Time Messaging Protocol (RTMPE) is used for streaming multimedia content within standard cryptographic primitives, consisting of Diffie-Hellman key exchange and HMACSHA256, generating a pair of RC4 keys.\n\nThe Real-Time Messaging Protocol (RTMPS) is used for streaming multimedia content across an encrypted connection.\n\nThe Real-Time Messaging Protocol tunneled through HTTP (RTMPT) is used for streaming multimedia content within HTTP requests to traverse firewalls.\n\nThe Encrypted Real-Time Messaging Protocol tunneled through HTTP (RTMPTE) is used for streaming multimedia content within HTTP requests to traverse firewalls.\n\nThe Real-Time Messaging Protocol tunneled through HTTPS (RTMPTS) is used for streaming multimedia content within HTTPS requests to traverse firewalls.\n\nThis protocol accepts the following options.\n\nFor more information see: http://www.samba.org/.\n\nRead from or write to remote resources using SFTP protocol.\n\nThis protocol accepts the following options.\n\nReal-Time Messaging Protocol and its variants supported through librtmp.\n\nRequires the presence of the librtmp headers and library during configuration. You need to explicitly configure the build with \"–enable-librtmp\". If enabled this will replace the native RTMP protocol.\n\nThis protocol provides most client functions and a few server functions needed to support RTMP, RTMP tunneled in HTTP (RTMPT), encrypted RTMP (RTMPE), RTMP over SSL/TLS (RTMPS) and tunneled variants of these encrypted types (RTMPTE, RTMPTS).\n\nwhere is one of the strings \"rtmp\", \"rtmpt\", \"rtmpe\", \"rtmps\", \"rtmpte\", \"rtmpts\" corresponding to each RTMP variant, and , , and have the same meaning as specified for the RTMP native protocol. contains a list of space-separated options of the form = .\n\nSee the librtmp manual page (man 3 librtmp) for more information.\n\nFor example, to stream a file in real-time to an RTMP server using :\n\nTo play the same stream using :\n\nThe required syntax for an RTP URL is:\n\nspecifies the RTP port to use.\n\ncontains a list of &-separated options of the form = .\n\nThe following URL options are supported:\n• If is not set the RTCP port will be set to the RTP port value plus 1.\n• If (the local RTP port) is not set any available port will be used for the local RTP and RTCP ports.\n• If (the local RTCP port) is not set it will be set to the local RTP port value plus 1.\n\nRTSP is not technically a protocol handler in libavformat, it is a demuxer and muxer. The demuxer supports both normal RTSP (with data transferred over RTP; this is used by e.g. Apple and Microsoft) and Real-RTSP (with data transferred over RDT).\n\nThe muxer can be used to send a stream using RTSP ANNOUNCE to a server supporting it (currently Darwin Streaming Server and Mischa Spiegelmock’s RTSP server).\n\nThe required syntax for a RTSP url is:\n\nOptions can be set on the / command line, or set in code via s or in .\n\nThe following options are supported.\n\nThe following options are supported.\n\nWhen receiving data over UDP, the demuxer tries to reorder received packets (since they may arrive out of order, or packets may get lost totally). This can be disabled by setting the maximum demuxing delay to zero (via the field of AVFormatContext).\n\nWhen watching multi-bitrate Real-RTSP streams with , the streams to display can be chosen with and for video and audio respectively, and can be switched on the fly by pressing and .\n\nThe following examples all make use of the and tools.\n• Watch a stream over UDP, with a max reordering delay of 0.5 seconds:\n• Send a stream in realtime to a RTSP server, for others to watch:\n\nSession Announcement Protocol (RFC 2974). This is not technically a protocol handler in libavformat, it is a muxer and demuxer. It is used for signalling of RTP streams, by announcing the SDP for the streams regularly on a separate port.\n\nThe syntax for a SAP url given to the muxer is:\n\nThe RTP packets are sent to on port , or to port 5004 if no port is specified. is a -separated list. The following options are supported:\n\nTo broadcast a stream on the local subnet, for watching in VLC:\n\nAnd for watching in , over IPv6:\n\nThe syntax for a SAP url given to the demuxer is:\n\nis the multicast address to listen for announcements on, if omitted, the default 224.2.127.254 (sap.mcast.net) is used. is the port that is listened on, 9875 if omitted.\n\nThe demuxers listens for announcements on the given address and port. Once an announcement is received, it tries to receive that particular stream.\n\nTo play back the first stream announced on the normal SAP multicast address:\n\nTo play back the first stream announced on one the default IPv6 SAP multicast address:\n\nThe protocol accepts the following options:\n\nThe supported syntax for a SRT URL is:\n\ncontains a list of &-separated options of the form = .\n\nThis protocol accepts the following options.\n\nFor more information see: https://github.com/Haivision/srt.\n\nVirtually extract a segment of a file or another stream. The underlying stream must be seekable.\n\nExtract a chapter from a DVD VOB file (start and end sectors obtained externally and multiplied by 2048):\n\nWrites the output to multiple protocols. The individual outputs are separated by |\n\nThe required syntax for a TCP url is:\n\ncontains a list of &-separated options of the form = .\n\nThe list of supported options follows.\n\nThe following example shows how to setup a listening TCP connection with , which is then accessed with :\n\nThe required syntax for a TLS/SSL url is:\n\nThe following parameters can be set via command line options (or in code via s):\n\nTo create a TLS/SSL server that serves an input stream.\n\nTo play back a stream from the TLS/SSL server using :\n\nThe required syntax for an UDP URL is:\n\ncontains a list of &-separated options of the form = .\n\nIn case threading is enabled on the system, a circular buffer is used to store the incoming data, which allows one to reduce loss of data due to UDP socket buffer overruns. The and options are related to this buffer.\n\nThe list of supported options follows.\n• Use to stream over UDP to a remote endpoint:\n• Use to stream in mpegts format over UDP using 188 sized UDP packets, using a large input buffer:\n• Use to receive over UDP from a remote endpoint:\n\nThe required syntax for a Unix socket URL is:\n\nThe following parameters can be set via command line options (or in code via s):\n\nThis library supports unicast streaming to multiple clients without relying on an external server.\n\nThe required syntax for streaming or connecting to a stream is:\n\nMultiple clients may connect to the stream using:\n\nStreaming to multiple clients is implemented using a ZeroMQ Pub-Sub pattern. The server side binds to a port and publishes data. Clients connect to the server (via IP address/port) and subscribe to the stream. The order in which the server and client start generally does not matter.\n\nffmpeg must be compiled with the –enable-libzmq option to support this protocol.\n\nOptions can be set on the / command line. The following options are supported:\n\nThe libavdevice library provides the same interface as libavformat. Namely, an input device is considered like a demuxer, and an output device like a muxer, and the interface and generic device options are the same provided by libavformat (see the ffmpeg-formats manual).\n\nIn addition each input or output device may support so-called private options, which are specific for that component.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the device options or using the API for programmatic use.\n\nInput devices are configured elements in FFmpeg which enable accessing the data coming from a multimedia device attached to your system.\n\nWhen you configure your FFmpeg build, all the supported input devices are enabled by default. You can list all available ones using the configure option \"–list-indevs\".\n\nYou can disable all the input devices using the configure option \"–disable-indevs\", and selectively enable an input device using the option \"–enable-indev= \", or you can disable a particular input device using the option \"–disable-indev= \".\n\nThe option \"-devices\" of the ff* tools will display the list of supported input devices.\n\nA description of the currently available input devices follows.\n\nTo enable this input device during configuration you need libasound installed on your system.\n\nThis device allows capturing from an ALSA device. The name of the device to capture has to be an ALSA card identifier.\n\nAn ALSA identifier has the syntax:\n\nwhere the and components are optional.\n\nThe three arguments (in order: , , ) specify card number or identifier, device number and subdevice number (-1 means any).\n\nTo see the list of cards currently recognized by your system check the files and .\n\nFor example to capture with from an ALSA device with card id 0, you may run the command:\n\nFor more information see: http://www.alsa-project.org/alsa-doc/alsa-lib/pcm.html\n\nThis input devices uses the Android Camera2 NDK API which is available on devices with API level 24+. The availability of android_camera is autodetected during configuration.\n\nThis device allows capturing from all cameras on an Android device, which are integrated into the Camera2 NDK API.\n\nThe available cameras are enumerated internally and can be selected with the parameter. The input file string is discarded.\n\nGenerally the back facing camera has index 0 while the front facing camera has index 1.\n\nAVFoundation is the currently recommended framework by Apple for streamgrabbing on OSX >= 10.7 as well as on iOS.\n\nThe input filename has to be given in the following syntax:\n\nThe first entry selects the video input while the latter selects the audio input. The stream has to be specified by the device name or the device index as shown by the device list. Alternatively, the video and/or audio input device can be chosen by index using the and/or , overriding any device name or index given in the input filename.\n\nAll available devices can be enumerated by using , listing all device names and corresponding indices.\n\nThere are two device name aliases:\n• Print the list of AVFoundation supported devices and exit:\n• Record video from video device 0 and audio from audio device 0 into out.avi:\n• Record video from video device 2 and audio from audio device 1 into out.avi:\n• Record video from the system default video device using the pixel format bgr0 and do not record any audio into out.avi:\n• Record raw DV data from a suitable input device and write the output into out.dv:\n\nBSD video input device. Deprecated and will be removed - please contact the developers if you are interested in maintaining it.\n\nThe decklink input device provides capture capabilities for Blackmagic DeckLink devices.\n\nTo enable this input device, you need the Blackmagic DeckLink SDK and you need to configure with the appropriate and . On Windows, you need to run the IDL files through .\n\nDeckLink is very picky about the formats it supports. Pixel format of the input can be set with . Framerate and video size must be determined for your device with . Audio sample rate is always 48 kHz and the number of channels can be 2, 8 or 16. Note that all audio channels are bundled in one single audio track.\n\nDirectShow support is enabled when FFmpeg is built with the mingw-w64 project. Currently only audio and video devices are supported.\n\nMultiple devices may be opened as separate inputs, but they may also be opened on the same input, which should improve synchronism between them.\n\nThe input name should be in the format:\n\nwhere can be either or , and is the device’s name or alternative name..\n\nIf no options are specified, the device’s defaults are used. If the device does not support the requested options, it will fail to open.\n• Print the list of DirectShow supported devices and exit:\n• Open second video device with name :\n• Print the list of supported options in selected device and exit:\n• Specify pin names to capture by name or alternative name, specify alternative device name:\n• Configure a crossbar device, specifying crossbar pins, allow user to adjust video capture properties at startup:\n\nThe Linux framebuffer is a graphic hardware-independent abstraction layer to show graphics on a computer monitor, typically on the console. It is accessed through a file device node, usually .\n\nFor more detailed information read the file Documentation/fb/framebuffer.txt included in the Linux source tree.\n\nSee also http://linux-fbdev.sourceforge.net/, and fbset(1).\n\nTo record from the framebuffer device with :\n\nYou can take a single screenshot image with the command:\n\nThis device allows you to capture a region of the display on Windows.\n\nAmongst options for the imput filenames are such elements as:\n\nThe first option will capture the entire desktop, or a fixed region of the desktop. The second and third options will instead capture the contents of a single window, regardless of its position on the screen.\n\nFor example, to grab the entire desktop using :\n\nGrab the contents of the window named \"Calculator\"\n\nTo enable this input device, you need libiec61883, libraw1394 and libavc1394 installed on your system. Use the configure option to compile with the device enabled.\n\nThe iec61883 capture device supports capturing from a video device connected via IEEE1394 (FireWire), using libiec61883 and the new Linux FireWire stack (juju). This is the default DV/HDV input method in Linux Kernel 2.6.37 and later, since the old FireWire stack was removed.\n\nSpecify the FireWire port to be used as input file, or \"auto\" to choose the first port connected.\n• Grab and show the input of a FireWire DV/HDV device.\n• Grab and record the input of a FireWire DV/HDV device, using a packet buffer of 100000 packets if the source is HDV.\n\nTo enable this input device during configuration you need libjack installed on your system.\n\nA JACK input device creates one or more JACK writable clients, one for each audio channel, with name :input_ , where is the name provided by the application, and is a number which identifies the channel. Each writable client will send the acquired data to the FFmpeg input device.\n\nOnce you have created one or more JACK readable clients, you need to connect them to one or more JACK writable clients.\n\nTo connect or disconnect JACK clients you can use the and programs, or do it through a graphical interface, for example with .\n\nTo list the JACK clients and their properties you can invoke the command .\n\nFollows an example which shows how to capture a JACK readable client with .\n\nCaptures the KMS scanout framebuffer associated with a specified CRTC or plane as a DRM object that can be passed to other hardware functions.\n\nRequires either DRM master or CAP_SYS_ADMIN to run.\n\nIf you don’t understand what all of that means, you probably don’t want this. Look at instead.\n• Capture from the first active plane, download the result to normal frames and encode. This will only work if the framebuffer is both linear and mappable - if not, the result may be scrambled or fail to download.\n• Capture from CRTC ID 42 at 60fps, map the result to VAAPI, convert to NV12 and encode as H.264.\n• To capture only part of a plane the output can be cropped - this can be used to capture a single window, as long as it has a known absolute position and size. For example, to capture and encode the middle quarter of a 1920x1080 plane:\n\nThis input device reads data from the open output pads of a libavfilter filtergraph.\n\nFor each filtergraph open output, the input device will create a corresponding stream which is mapped to the generated output. The filtergraph is specified through the option .\n• Create a color video stream and play it back with :\n• As the previous example, but use filename for specifying the graph description, and omit the \"out0\" label:\n• Create three different video test filtered sources and play them:\n• Read an audio stream from a file using the amovie source and play it back with :\n• Read an audio stream and a video stream and play it back with :\n• Dump decoded frames to images and Closed Captions to an RCWT backup:\n\nTo enable this input device during configuration you need libcdio installed on your system. It requires the configure option .\n\nThis device allows playing and grabbing from an Audio-CD.\n\nFor example to copy with the entire Audio-CD in , you may run the command:\n\nThe OpenAL input device provides audio capture on all systems with a working OpenAL 1.1 implementation.\n\nTo enable this input device during configuration, you need OpenAL headers and libraries installed on your system, and need to configure FFmpeg with .\n\nOpenAL headers and libraries should be provided as part of your OpenAL implementation, or as an additional download (an SDK). Depending on your installation you may need to specify additional flags via the and for allowing the build system to locate the OpenAL headers and libraries.\n\nAn incomplete list of OpenAL implementations follows:\n\nThis device allows one to capture from an audio input device handled through OpenAL.\n\nYou need to specify the name of the device to capture in the provided filename. If the empty string is provided, the device will automatically select the default device. You can get the list of the supported devices by using the option .\n\nPrint the list of OpenAL supported devices and exit:\n\nCapture from the default device (note the empty string ” as filename):\n\nCapture from two devices simultaneously, writing to two different files, within the same command:\n\nNote: not all OpenAL implementations support multiple simultaneous capture - try the latest OpenAL Soft if the above does not work.\n\nThe filename to provide to the input device is the device node representing the OSS input device, and is usually set to .\n\nFor example to grab from using use the command:\n\nFor more information about OSS see: http://manuals.opensound.com/usersguide/dsp.html\n\nTo enable this output device you need to configure FFmpeg with .\n\nThe filename to provide to the input device is a source device or the string \"default\"\n\nTo list the PulseAudio source devices and their properties you can invoke the command .\n\nMore information about PulseAudio can be found on http://www.pulseaudio.org.\n\nTo enable this input device during configuration you need libsndio installed on your system.\n\nThe filename to provide to the input device is the device node representing the sndio input device, and is usually set to .\n\nFor example to grab from using use the command:\n\n\"v4l2\" can be used as alias for \"video4linux2\".\n\nIf FFmpeg is built with v4l-utils support (by using the configure option), it is possible to use it with the input device option.\n\nThe name of the device to grab is a file device node, usually Linux systems tend to automatically create such nodes when the device (e.g. an USB webcam) is plugged into the system, and has a name of the kind , where is a number associated to the device.\n\nVideo4Linux2 devices usually support a limited set of x sizes and frame rates. You can check which are supported using for Video4Linux2 devices. Some devices, like TV cards, support one or more standards. It is possible to list all the supported standards using .\n\nThe time base for the timestamps is 1 microsecond. Depending on the kernel version and configuration, the timestamps may be derived from the real time clock (origin at the Unix Epoch) or the monotonic clock (origin usually at boot time, unaffected by NTP or manual changes to the clock). The or option can be used to force conversion into the real time clock.\n\nSome usage examples of the video4linux2 device with and :\n• Grab and show the input of a video4linux2 device:\n• Grab and record the input of a video4linux2 device, leave the frame rate and size as previously set:\n\nFor more information about Video4Linux, check http://linuxtv.org/.\n\nThe filename passed as input is the capture driver number, ranging from 0 to 9. You may use \"list\" as filename to print a list of drivers. Any other filename will be interpreted as device number 0.\n\nTo enable this input device during configuration you need libxcb installed on your system. It will be automatically detected during configuration.\n\nThis device allows one to capture a region of an X11 display.\n\nThe filename passed as input has the syntax:\n\n: . specifies the X11 display name of the screen to grab from. can be omitted, and defaults to \"localhost\". The environment variable contains the default display name.\n\nand specify the offsets of the grabbed area with respect to the top-left border of the X11 screen. They default to 0.\n\nCheck the X11 documentation (e.g. ) for more detailed information.\n\nUse the program for getting basic information about the properties of your X11 display (e.g. grep for \"name\" or \"dimensions\").\n\nFor example to grab from using :\n\nThe audio resampler supports the following named options.\n\nOptions may be set by specifying - in the FFmpeg tools, = for the aresample filter, by setting the value explicitly in the options or using the API for programmatic use.\n\nThe video scaler supports the following named options.\n\nOptions may be set by specifying - in the FFmpeg tools, with a few API-only exceptions noted below. For programmatic use, they can be set explicitly in the options or through the API.\n\nFiltering in FFmpeg is enabled through the libavfilter library.\n\nIn libavfilter, a filter can have multiple inputs and multiple outputs. To illustrate the sorts of things that are possible, we consider the following filtergraph.\n\nThis filtergraph splits the input stream in two streams, then sends one stream through the crop filter and the vflip filter, before merging it back with the other stream by overlaying it on top. You can use the following command to achieve this:\n\nThe result will be that the top half of the video is mirrored onto the bottom half of the output video.\n\nFilters in the same linear chain are separated by commas, and distinct linear chains of filters are separated by semicolons. In our example, are in one linear chain, and are separately in another. The points where the linear chains join are labelled by names enclosed in square brackets. In the example, the split filter generates two outputs that are associated to the labels and .\n\nThe stream sent to the second output of , labelled as , is processed through the filter, which crops away the lower half part of the video, and then vertically flipped. The filter takes in input the first unchanged output of the split filter (which was labelled as ), and overlay on its lower half the output generated by the filterchain.\n\nSome filters take in input a list of parameters: they are specified after the filter name and an equal sign, and are separated from each other by a colon.\n\nThere exist so-called that do not have an audio/video input, and that will not have audio/video output.\n\nThe program included in the FFmpeg directory can be used to parse a filtergraph description and issue a corresponding textual representation in the dot language.\n\nto see how to use .\n\nYou can then pass the dot description to the program (from the graphviz suite of programs) and obtain a graphical representation of the filtergraph.\n\nFor example the sequence of commands:\n\ncan be used to create and display an image representing the graph described by the string. Note that this string must be a complete self-contained graph, with its inputs and outputs explicitly defined. For example if your command line is of the form:\n\nyour string will need to be of the form:\n\nyou may also need to set the parameters and add a filter in order to simulate a specific input file.\n\nA filtergraph is a directed graph of connected filters. It can contain cycles, and there can be multiple links between a pair of filters. Each link has one input pad on one side connecting it to one filter from which it takes its input, and one output pad on the other side connecting it to one filter accepting its output.\n\nEach filter in a filtergraph is an instance of a filter class registered in the application, which defines the features and the number of input and output pads of the filter.\n\nA filter with no input pads is called a \"source\", and a filter with no output pads is called a \"sink\".\n\nA filtergraph has a textual representation, which is recognized by the / / and options in and / in , and by the function defined in .\n\nA filterchain consists of a sequence of connected filters, each one connected to the previous one in the sequence. A filterchain is represented by a list of \",\"-separated filter descriptions.\n\nA filtergraph consists of a sequence of filterchains. A sequence of filterchains is represented by a list of \";\"-separated filterchain descriptions.\n\nA filter is represented by a string of the form: [ ]...[ ] @ = [ ]...[ ]\n\nis the name of the filter class of which the described filter is an instance of, and has to be the name of one of the filter classes registered in the program optionally followed by \"@ \". The name of the filter class is optionally followed by a string \"= \".\n\nis a string which contains the parameters used to initialize the filter instance. It may have one of two forms:\n• A ’:’-separated list of . In this case, the keys are assumed to be the option names in the order they are declared. E.g. the filter declares three options in this order – , and . Then the parameter list means that the value is assigned to the option , to and to .\n• A ’:’-separated list of mixed direct and long pairs. The direct must precede the pairs, and follow the same constraints order of the previous point. The following pairs can be set in any preferred order.\n\nIf the option value itself is a list of items (e.g. the filter takes a list of pixel formats), the items in the list are usually separated by ‘ ’.\n\nThe list of arguments can be quoted using the character ‘ ’ as initial and ending mark, and the character ‘ ’ for escaping the characters within the quoted text; otherwise the argument string is considered terminated when the next special character (belonging to the set ‘ ’) is encountered.\n\nA special syntax implemented in the CLI tool allows loading option values from files. This is done be prepending a slash ’/’ to the option name, then the supplied value is interpreted as a path from which the actual value is loaded. E.g.\n\nwill load the text to be drawn from . API users wishing to implement a similar feature should use the functions together with custom IO code.\n\nThe name and arguments of the filter are optionally preceded and followed by a list of link labels. A link label allows one to name a link and associate it to a filter output or input pad. The preceding labels ... , are associated to the filter input pads, the following labels ... , are associated to the output pads.\n\nWhen two link labels with the same name are found in the filtergraph, a link between the corresponding input and output pad is created.\n\nIf an output pad is not labelled, it is linked by default to the first unlabelled input pad of the next filter in the filterchain. For example in the filterchain\n\nthe split filter instance has two output pads, and the overlay filter instance two input pads. The first output pad of split is labelled \"L1\", the first input pad of overlay is labelled \"L2\", and the second output pad of split is linked to the second input pad of overlay, which are both unlabelled.\n\nIn a filter description, if the input label of the first filter is not specified, \"in\" is assumed; if the output label of the last filter is not specified, \"out\" is assumed.\n\nIn a complete filterchain all the unlabelled filter input and output pads must be connected. A filtergraph is considered valid if all the filter input and output pads of all the filterchains are connected.\n\nLeading and trailing whitespaces (space, tabs, or line feeds) separating tokens in the filtergraph specification are ignored. This means that the filtergraph can be expressed using empty lines and spaces to improve redability.\n\nFor example, the filtergraph:\n\ncan be represented as:\n\nLibavfilter will automatically insert scale filters where format conversion is required. It is possible to specify swscale flags for those automatically inserted scalers by prepending to the filtergraph description.\n\nHere is a BNF description of the filtergraph syntax:\n\nFiltergraph description composition entails several levels of escaping. See (ffmpeg-utils)the \"Quoting and escaping\" section in the ffmpeg-utils(1) manual for more information about the employed escaping procedure.\n\nA first level escaping affects the content of each filter option value, which may contain the special character used to separate values, or one of the escaping characters .\n\nA second level escaping affects the whole filter description, which may contain the escaping characters or the special characters used by the filtergraph description.\n\nFinally, when you specify a filtergraph on a shell commandline, you need to perform a third level escaping for the shell special characters contained within it.\n\nFor example, consider the following string to be embedded in the drawtext filter description value:\n\nThis string contains the special escaping character, and the special character, so it needs to be escaped in this way:\n\nA second level of escaping is required when embedding the filter description in a filtergraph description, in order to escape all the filtergraph special characters. Thus the example above becomes:\n\n(note that in addition to the escaping special characters, also needs to be escaped).\n\nFinally an additional level of escaping is needed when writing the filtergraph description in a shell command, which depends on the escaping rules of the adopted shell. For example, assuming that is special and needs to be escaped with another , the previous string will finally result in:\n\nIn order to avoid cumbersome escaping when using a commandline tool accepting a filter specification as input, it is advisable to avoid direct inclusion of the filter or options specification in the shell.\n\nFor example, in case of the drawtext filter, you might prefer to use the option in place of to specify the text to render.\n\nSome filters support a generic option. For the filters supporting timeline editing, this option can be set to an expression which is evaluated before sending a frame to the filter. If the evaluation is non-zero, the filter will be enabled, otherwise the frame will be sent unchanged to the next filter in the filtergraph.\n\nThe expression accepts the following values:\n\nAdditionally, these filters support an command that can be used to re-define the expression.\n\nLike any other filtering option, the option follows the same rules.\n\nFor example, to enable a blur filter (smartblur) from 10 seconds to 3 minutes, and a curves filter starting at 3 seconds:\n\nSee to view which filters have timeline support.\n\nSome options can be changed during the operation of the filter using a command. These options are marked ’T’ on the output of . The name of the command is the name of the option and the argument is the new value.\n\n28 Options for filters with several inputs (framesync)\n\nSome filters with several inputs support a common set of options. These options can only be set by name, not with the short notation.\n\nWhen you configure your FFmpeg build, you can disable any of the existing filters using . The configure output will show the audio filters included in your build.\n\nBelow is a description of the currently available audio filters.\n\nApply Affine Projection algorithm to the first audio stream using the second audio stream.\n\nThis adaptive filter is used to estimate unknown audio based on multiple input audio samples. Affine projection algorithm can make trade-offs between computation complexity with convergence speed.\n\nA description of the accepted options follows.\n\nA compressor is mainly used to reduce the dynamic range of a signal. Especially modern music is mostly compressed at a high ratio to improve the overall loudness. It’s done to get the highest attention of a listener, \"fatten\" the sound and bring more \"power\" to the track. If a signal is compressed too much it may sound dull or \"dead\" afterwards or it may start to \"pump\" (which could be a powerful effect but can also destroy a track completely). The right compression is the key to reach a professional sound and is the high art of mixing and mastering. Because of its complex settings it may take a long time to get the right feeling for this kind of effect.\n\nCompression is done by detecting the volume above a chosen level and dividing it by the factor set with . So if you set the threshold to -12dB and your signal reaches -6dB a ratio of 2:1 will result in a signal at -9dB. Because an exact manipulation of the signal would cause distortion of the waveform the reduction can be levelled over the time. This is done by setting \"Attack\" and \"Release\". determines how long the signal has to rise above the threshold before any reduction will occur and sets the time the signal has to fall below the threshold to reduce the reduction again. Shorter signals than the chosen attack time will be left untouched. The overall reduction of the signal can be made up afterwards with the setting. So compressing the peaks of a signal about 6dB and raising the makeup to this level results in a signal twice as loud than the source. To gain a softer entry in the compression the flattens the hard edge at the threshold in the range of the chosen decibels.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nCopy the input audio source unchanged to the output. This is mainly useful for testing purposes.\n\nApply cross fade from one input audio stream to another input audio stream. The cross fade is applied for specified duration near the end of first stream.\n\nThe filter accepts the following options:\n• Cross fade from one input to another:\n• Cross fade from one input to another but without overlapping:\n\nThis filter splits audio stream into two or more frequency ranges. Summing all streams back will give flat output.\n\nThe filter accepts the following options:\n• Split input audio stream into two bands (low and high) with split frequency of 1500 Hz, each band will be in separate stream:\n• Same as above, but with higher filter order:\n• Same as above, but also with additional middle band (frequencies between 1500 and 8000):\n\nThis filter is bit crusher with enhanced functionality. A bit crusher is used to audibly reduce number of bits an audio signal is sampled with. This doesn’t change the bit depth at all, it just produces the effect. Material reduced in bit depth sounds more harsh and \"digital\". This filter is able to even round to continuous values instead of discrete bit depths. Additionally it has a D/C offset which results in different crushing of the lower and the upper half of the signal. An Anti-Aliasing setting is able to produce \"softer\" crushing sounds.\n\nAnother feature of this filter is the logarithmic mode. This setting switches from linear distances between bits to logarithmic ones. The result is a much more \"natural\" sounding crusher which doesn’t gate low signals for example. The human ear has a logarithmic perception, so this kind of crushing is much more pleasant. Logarithmic crushing is also able to get anti-aliased.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDelay audio filtering until a given wallclock timestamp. See the cue filter.\n\nSamples detected as impulsive noise are replaced by interpolated samples using autoregressive modelling.\n\nSamples detected as clipped are replaced by interpolated samples using autoregressive modelling.\n\nThe filter accepts the following options:\n\nDelay one or more audio channels.\n\nSamples in delayed channel are filled with silence.\n\nThe filter accepts the following option:\n• Delay first channel by 1.5 seconds, the third channel by 0.5 seconds and leave the second channel (and any other channels that may be present) unchanged.\n• Delay second channel by 500 samples, the third channel by 700 samples and leave the first channel (and any other channels that may be present) unchanged.\n• Delay all channels by same number of samples:\n\nThis filter shall be placed before any filter that can produce denormals.\n\nA description of the accepted parameters follows.\n\nThis filter supports the all above options as commands.\n\nApplying both filters one after another produces original audio.\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n• Apply spectral compression to all frequencies with threshold of -50 dB and 1:6 ratio:\n• Similar to above but with 1:2 ratio and filtering only front center channel:\n• Apply spectral noise gate to all frequencies with threshold of -85 dB and with short attack time and short release time:\n• Apply spectral expansion to all frequencies with threshold of -10 dB and 1:2 ratio:\n• Apply limiter to max -60 dB to all frequencies, with attack of 2 ms and release of 10 ms:\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nEchoes are reflected sound and can occur naturally amongst mountains (and sometimes large buildings) when talking or shouting; digital echo effects emulate this behaviour and are often used to help fill out the sound of a single instrument or vocal. The time difference between the original signal and the reflection is the , and the loudness of the reflected signal is the . Multiple echoes can have different delays and decays.\n\nA description of the accepted parameters follows.\n• Make it sound as if there are twice as many instruments as are actually playing:\n• If delay is very short, then it sounds like a (metallic) robot playing music:\n• A longer delay will sound like an open air concert in the mountains:\n• Same as above but with one more mountain:\n\nAudio emphasis filter creates or restores material directly taken from LPs or emphased CDs with different filter curves. E.g. to store music on vinyl the signal has to be altered by a filter first to even out the disadvantages of this recording medium. Once the material is played back the inverse filter has to be applied to restore the distortion of the frequency response.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nModify an audio signal according to the specified expressions.\n\nThis filter accepts one or more expressions (one for each channel), which are evaluated and used to modify a corresponding audio signal.\n\nIt accepts the following parameters:\n\nEach expression in can contain the following constants and functions:\n\nNote: this filter is slow. For faster processing you should use a dedicated filter.\n• Invert phase of the second channel:\n\nAn exciter is used to produce high sound that is not present in the original signal. This is done by creating harmonic distortions of the signal which are restricted in range and added to the original signal. An Exciter raises the upper end of an audio signal without simply raising the higher frequencies like an equalizer would do to create a more \"crisp\" or \"brilliant\" sound.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nA description of the accepted parameters follows.\n\nThis filter supports the all above options as commands.\n• Fade in first 15 seconds of audio:\n• Fade out last 25 seconds of a 900 seconds audio:\n\nA description of the accepted parameters follows.\n\nThis filter supports the some above mentioned options as commands.\n• Reduce white noise by 10dB, and use previously measured noise floor of -40dB:\n• Reduce white noise by 10dB, also set initial noise floor to -80dB and enable automatic tracking of noise floor so noise floor will gradually change during processing:\n• Reduce noise by 20dB, using noise floor of -40dB and using commands to take noise profile of first 0.4 seconds of input audio:\n• Leave almost only low frequencies in audio:\n\nThis filter is designed for applying long FIR filters, up to 60 seconds long.\n\nIt can be used as component for digital crossover filters, room equalization, cross talk cancellation, wavefield synthesis, auralization, ambiophonics, ambisonics and spatialization.\n\nThis filter uses the streams higher than first one as FIR coefficients. If the non-first stream holds a single channel, it will be used for all input channels in the first stream, otherwise the number of channels in the non-first stream must be same as the number of channels in the first stream.\n\nIt accepts the following parameters:\n• Apply reverb to stream using mono IR file as second input, complete command using ffmpeg:\n• Apply true stereo processing given input stereo stream, and two stereo impulse responses for left and right channel, the impulse response files are files with names l_ir.wav and r_ir.wav, and setting irnorm option value:\n• Similar to above example, but with explicitly set to estimated value and with disabled:\n\nSet output format constraints for the input audio. The framework will negotiate the most appropriate format to minimize conversions.\n\nIt accepts the following parameters:\n\nIf a parameter is omitted, all values are allowed.\n\nForce the output to either unsigned 8-bit or signed 16-bit stereo\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nA gate is mainly used to reduce lower parts of a signal. This kind of signal processing reduces disturbing noise between useful signals.\n\nGating is done by detecting the volume below a chosen level and dividing it by the factor set with . The bottom of the noise floor is set via . Because an exact manipulation of the signal would cause distortion of the waveform the reduction can be levelled over time. This is done by setting and .\n\ndetermines how long the signal has to fall below the threshold before any reduction will occur and sets the time the signal has to rise above the threshold to reduce the reduction again. Shorter signals than the chosen attack time will be left untouched.\n\nThis filter supports the all above options as commands.\n\nIt accepts the following parameters:\n\nCoefficients in and format are separated by spaces and are in ascending order.\n\nCoefficients in format are separated by spaces and order of coefficients doesn’t matter. Coefficients in format are complex numbers with imaginary unit.\n\nDifferent coefficients and gains can be provided for every channel, in such case use ’|’ to separate coefficients or gains. Last provided coefficients will be used for all remaining channels.\n• Apply 2 pole elliptic notch at around 5000Hz for 48000 Hz sample rate:\n• Same as above but in format:\n\nThe limiter prevents an input signal from rising over a desired threshold. This limiter uses lookahead technology to prevent your signal from distorting. It means that there is a small delay after the signal is processed. Keep in mind that the delay it produces is the attack time you set.\n\nThe filter accepts the following options:\n\nDepending on picked setting it is recommended to upsample input 2x or 4x times with aresample before applying this filter.\n\nApply a two-pole all-pass filter with central frequency (in Hz) , and filter-width . An all-pass filter changes the audio’s frequency to phase relationship without changing its frequency to amplitude relationship.\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThe filter accepts the following options:\n\nMerge two or more audio streams into a single multi-channel stream.\n\nThe filter accepts the following options:\n\nIf the channel layouts of the inputs are disjoint, and therefore compatible, the channel layout of the output will be set accordingly and the channels will be reordered as necessary. If the channel layouts of the inputs are not disjoint, the output will have all the channels of the first input then all the channels of the second input, in that order, and the channel layout of the output will be the default value corresponding to the total number of channels.\n\nFor example, if the first input is in 2.1 (FL+FR+LF) and the second input is FC+BL+BR, then the output will be in 5.1, with the channels in the following order: a1, a2, b1, a3, b2, b3 (a1 is the first channel of the first input, b1 is the first channel of the second input).\n\nOn the other hand, if both input are in stereo, the output channels will be in the default order: a1, a2, b1, b2, and the channel layout will be arbitrarily set to 4.0, which may or may not be the expected value.\n\nAll inputs must have the same sample rate, and format.\n\nIf inputs do not have the same duration, the output will stop with the shortest.\n\nNote that this filter only supports float samples (the and audio filters support many formats). If the input has integer samples then aresample will be automatically inserted to perform the conversion to float samples.\n\nIt accepts the following parameters:\n• This will mix 3 input audio streams to a single output with the same duration as the first input and a dropout transition time of 3 seconds:\n• This will mix one vocal and one music input audio stream to a single output with the same duration as the longest input. The music will have quarter the weight as the vocals, and the inputs are not normalized:\n\nThis filter supports the following commands:\n\nMultiply first audio stream with second audio stream and store result in output audio stream. Multiplication is done by multiplying each sample from first stream with sample at same position from second stream.\n\nWith this element-wise multiplication one can create amplitude fades and amplitude modulations.\n\nIt accepts the following parameters:\n• Lower gain by 10 of central frequency 200Hz and width 100 Hz for first 2 channels using Chebyshev type 1 filter:\n\nThis filter supports the following commands:\n\nEach sample is adjusted by looking for other samples with similar contexts. This context similarity is defined by comparing their surrounding patches of size . Patches are searched in an area of around the sample.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nApply Normalized Least-Mean-(Squares|Fourth) algorithm to the first audio stream using the second audio stream.\n\nThis adaptive filter is used to mimic a desired filter by finding the filter coefficients that relate to producing the least mean square of the error signal (difference between the desired, 2nd input audio stream and the actual signal, the 1st input audio stream).\n\nA description of the accepted options follows.\n• One of many usages of this filter is noise reduction, input audio is filtered with same samples that are delayed by fixed amount, one such example for stereo audio is:\n\nThis filter supports the same commands as options, excluding option .\n\nPass the audio source unchanged to the output.\n\nPad the end of an audio stream with silence.\n\nThis can be used together with to extend audio streams to the same length as the video stream.\n\nA description of the accepted options follows.\n\nIf neither the nor the nor nor option is set, the filter will add silence to the end of the input stream indefinitely.\n\nNote that for ffmpeg 4.4 and earlier a zero or also caused the filter to add silence indefinitely.\n• Add 1024 samples of silence to the end of the input:\n• Make sure the audio output will contain at least 10000 samples, pad the input with silence if required:\n• Use to pad the audio input with silence, so that the video stream will always result the shortest and will be converted until the end in the output file when using the option:\n\nA phaser filter creates series of peaks and troughs in the frequency spectrum. The position of the peaks and troughs are modulated so that they vary over time, creating a sweeping effect.\n\nA description of the accepted parameters follows.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter takes two audio streams for input, and outputs first audio stream. Results are in dB per channel at end of either input.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nAudio pulsator is something between an autopanner and a tremolo. But it can produce funny stereo effects as well. Pulsator changes the volume of the left and right channel based on a LFO (low frequency oscillator) with different waveforms and shifted phases. This filter have the ability to define an offset between left and right channel. An offset of 0 means that both LFO shapes match each other. The left and right channel are altered equally - a conventional tremolo. An offset of 50% means that the shape of the right channel is exactly shifted in phase (or moved backwards about half of the frequency) - pulsator acts as an autopanner. At 1 both curves match again. Every setting in between moves the phase shift gapless between all stages and produces some \"bypassing\" sounds with sine and triangle waveforms. The more you set the offset near 1 (starting from the 0.5) the faster the signal passes from the left to the right speaker.\n\nThe filter accepts the following options:\n\nResample the input audio to the specified parameters, using the libswresample library. If none are specified then the filter will automatically convert between its input and output.\n\nThis filter is also able to stretch/squeeze the audio data to make it match the timestamps or to inject silence / cut out audio to make it match the timestamps, do a combination of both or do neither.\n\nThe filter accepts the syntax [ :] , where expresses a sample rate and is a list of = pairs, separated by \":\". See the (ffmpeg-resampler)\"Resampler Options\" section in the ffmpeg-resampler(1) manual for the complete list of supported options.\n• Stretch/squeeze samples to the given timestamps, with a maximum of 1000 samples per second compensation:\n\nWarning: This filter requires memory to buffer the entire clip, so trimming is suggested.\n• Take the first 5 seconds of a clip, and reverse it.\n\nApply Recursive Least Squares algorithm to the first audio stream using the second audio stream.\n\nThis adaptive filter is used to mimic a desired filter by recursively finding the filter coefficients that relate to producing the minimal weighted linear least squares cost function of the error signal (difference between the desired, 2nd input audio stream and the actual signal, the 1st input audio stream).\n\nA description of the accepted options follows.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter takes two audio streams for input, and outputs first audio stream. Results are in dB per channel at end of either input.\n\nSet the number of samples per each output audio frame.\n\nThe last output packet may contain a different number of samples, as the filter will flush all the remaining samples when the input audio signals its end.\n\nThe filter accepts the following options:\n\nFor example, to set the number of per-frame samples to 1234 and disable padding for the last frame, use:\n\nSet the sample rate without altering the PCM data. This will result in a change of speed and pitch.\n\nThe filter accepts the following options:\n\nShow a line containing various information for each input audio frame. The input audio is not modified.\n\nThe shown line contains a sequence of key/value pairs of the form : .\n\nThe following values are shown in the output:\n\nThis filter takes two audio streams for input, and outputs first audio stream. Results are in dB per channel at end of either input.\n\nSoft clipping is a type of distortion effect where the amplitude of a signal is saturated along a smooth curve, rather than the abrupt shape of hard-clipping.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDisplay frequency domain statistical information about the audio channels. Statistics are calculated and stored as metadata for each audio channel and for each audio frame.\n\nIt accepts the following option:\n\nA list of each metadata key follows:\n\nThis filter uses PocketSphinx for speech recognition. To enable compilation of this filter, you need to configure FFmpeg with .\n\nIt accepts the following options:\n\nThe filter exports recognized speech as the frame metadata .\n\nDisplay time domain statistical information about the audio channels. Statistics are calculated and displayed for each audio channel and, where applicable, an overall figure is also given.\n\nIt accepts the following option:\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter allows to set custom, steeper roll off than highpass filter, and thus is able to more attenuate frequency content in stop-band.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts exactly one parameter, the audio tempo. If not specified then the filter will assume nominal 1.0 tempo. Tempo must be in the [0.5, 100.0] range.\n\nNote that tempo greater than 2 will skip some samples rather than blend them in. If for any reason this is a concern it is always possible to daisy-chain several instances of atempo to achieve the desired product tempo.\n• To speed up audio to 300% tempo:\n• To speed up audio to 300% tempo by daisy-chaining two atempo instances:\n\nThis filter supports the following commands:\n\nThis filter apply any spectral roll-off slope over any specified frequency band.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nTrim the input so that the output contains one continuous subpart of the input.\n\nIt accepts the following parameters:\n\n, , and are expressed as time duration specifications; see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual.\n\nNote that the first two sets of the start/end options and the option look at the frame timestamp, while the _sample options simply count the samples that pass through the filter. So start/end_pts and start/end_sample will give different results when the timestamps are wrong, inexact or do not start at zero. Also note that this filter does not modify the timestamps. If you wish to have the output timestamps start at zero, insert the asetpts filter after the atrim filter.\n\nIf multiple start or end options are set, this filter tries to be greedy and keep all samples that match at least one of the specified constraints. To keep only the part that matches all the constraints at once, chain multiple atrim filters.\n\nThe defaults are such that all the input is kept. So it is possible to set e.g. just the end values to keep everything before the specified time.\n• Drop everything except the second minute of input:\n• Keep only the first 1000 samples:\n\nResulted samples are always between -1 and 1 inclusive. If result is 1 it means two input samples are highly correlated in that selected segment. Result 0 means they are not correlated at all. If result is -1 it means two input samples are out of phase, which means they cancel each other.\n\nThe filter accepts the following options:\n\nApply a two-pole Butterworth band-pass filter with central frequency , and (3dB-point) band-width width. The option selects a constant skirt gain (peak gain = Q) instead of the default: constant 0dB peak gain. The filter roll off at 6dB per octave (20dB per decade).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nApply a two-pole Butterworth band-reject filter with central frequency , and (3dB-point) band-width . The filter roll off at 6dB per octave (20dB per decade).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nBoost or cut the bass (lower) frequencies of the audio using a two-pole shelving filter with a response similar to that of a standard hi-fi’s tone-controls. This is also known as shelving equalisation (EQ).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nApply a biquad IIR filter with the given coefficients. Where , , and , , are the numerator and denominator coefficients respectively. and , specify which channels to filter, by default all available are filtered.\n\nThis filter supports the following commands:\n\nBauer stereo to binaural transformation, which improves headphone listening of stereo audio records.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nIt accepts the following parameters:\n\nIt accepts the following parameters:\n\nIf no mapping is present, the filter will implicitly map input channels to output channels, preserving indices.\n• For example, assuming a 5.1+downmix input MOV file, will create an output WAV file tagged as stereo from the downmix channels of the input.\n\nSplit each channel from an input audio stream into a separate output stream.\n\nIt accepts the following parameters:\n• For example, assuming a stereo input MP3 file, will create an output Matroska file with two audio streams, one containing only the left channel and the other the right channel.\n\nCan make a single vocal sound like a chorus, but can also be applied to instrumentation.\n\nChorus resembles an echo effect with a short delay, but whereas with echo the delay is constant, with chorus, it is varied using using sinusoidal or triangular modulation. The modulation depth defines the range the modulated delay is played before or after the delay. Hence the delayed sound will sound slower or faster, that is the delayed sound tuned around the original one, like in a chorus where some vocals are slightly off key.\n\nIt accepts the following parameters:\n\nIt accepts the following parameters:\n• Make music with both quiet and loud passages suitable for listening to in a noisy environment: Another example for audio with whisper and explosion parts:\n• A noise gate for when the noise is at a lower level than the signal:\n• Here is another noise gate, this time for when the noise is at a higher level than the signal (making it, in some ways, similar to squelch):\n\nCompensation Delay Line is a metric based delay to compensate differing positions of microphones or speakers.\n\nFor example, you have recorded guitar with two microphones placed in different locations. Because the front of sound wave has fixed speed in normal conditions, the phasing of microphones can vary and depends on their location and interposition. The best sound mix can be achieved when these microphones are in phase (synchronized). Note that a distance of ~30 cm between microphones makes one microphone capture the signal in antiphase to the other microphone. That makes the final mix sound moody. This filter helps to solve phasing problems by adding different delays to each microphone track and make them synchronized.\n\nThe best result can be reached when you take one track as base and synchronize other tracks one by one with it. Remember that synchronization/delay tolerance depends on sample rate, too. Higher sample rates will give more tolerance.\n\nThe filter accepts the following parameters:\n\nThis filter supports the all above options as commands.\n\nCrossfeed is the process of blending the left and right channels of stereo audio recording. It is mainly used to reduce extreme stereo separation of low frequencies.\n\nThe intent is to produce more speaker like sound to the listener.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter linearly increases differences between each audio sample.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis can be useful to remove a DC offset (caused perhaps by a hardware problem in the recording chain) from the audio. The effect of a DC offset is reduced headroom and hence volume. The astats filter can be used to determine if a signal has a DC offset.\n\nThis filter accepts stereo input and produce surround (3.0) channels output. The newly produced front center channel have enhanced speech dialogue originally available in both stereo channels. This filter outputs front left and front right channels same as available in stereo input.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDR values of 14 and higher is found in very dynamic material. DR of 8 to 13 is found in transition material. And anything less that 8 have very poor dynamics and is very compressed.\n\nThe filter accepts the following options:\n\nThis filter applies a certain amount of gain to the input audio in order to bring its peak magnitude to a target level (e.g. 0 dBFS). However, in contrast to more \"simple\" normalization algorithms, the Dynamic Audio Normalizer *dynamically* re-adjusts the gain factor to the input audio. This allows for applying extra gain to the \"quiet\" sections of the audio while avoiding distortions or clipping the \"loud\" sections. In other words: The Dynamic Audio Normalizer will \"even out\" the volume of quiet and loud sections, in the sense that the volume of each section is brought to the same target level. Note, however, that the Dynamic Audio Normalizer achieves this goal *without* applying \"dynamic range compressing\". It will retain 100% of the dynamic range *within* each section of the audio file.\n\nThis filter supports the all above options as commands.\n\nMake audio easier to listen to on headphones.\n\nThis filter adds ‘cues’ to 44.1kHz stereo (i.e. audio CD format) audio so that when listened to on headphones the stereo image is moved from inside your head (standard for headphones) to outside and in front of the listener (standard for speakers).\n\nApply a two-pole peaking equalisation (EQ) filter. With this filter, the signal-level at and around a selected frequency can be increased or decreased, whilst (unlike bandpass and bandreject filters) that at all other frequencies is unchanged.\n\nIn order to produce complex equalisation curves, this filter can be given several times, each with a different central frequency.\n\nThe filter accepts the following options:\n• Attenuate 10 dB at 1000 Hz, with a bandwidth of 200 Hz:\n• Apply 2 dB gain at 1000 Hz with Q 1 and attenuate 5 dB at 100 Hz with Q 2:\n\nThis filter supports the following commands:\n\nLinearly increases the difference between left and right channels which adds some sort of \"live\" effect to playback.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following option:\n• higher delay with zero phase to compensate delay:\n• lowpass on left channel, highpass on right channel:\n\nThe filter accepts the following options:\n\nNote that this makes most sense to apply on mono signals. With this filter applied to mono signals it give some directionality and stretches its stereo image.\n\nThe filter accepts the following options:\n\nDecodes High Definition Compatible Digital (HDCD) data. A 16-bit PCM stream with embedded HDCD codes is expanded into a 20-bit PCM stream.\n\nThe filter supports the Peak Extend and Low-level Gain Adjustment features of HDCD, and detects the Transient Filter flag.\n\nWhen using the filter with wav, note the default encoding for wav is 16-bit, so the resulting 20-bit stream will be truncated back to 16-bit. Use something like after the filter to get 24-bit PCM output.\n\nThe filter accepts the following options:\n\nApply head-related transfer functions (HRTFs) to create virtual loudspeakers around the user for binaural listening via headphones. The HRIRs are provided via additional streams, for each channel one stereo input stream is needed.\n\nThe filter accepts the following options:\n• Full example using wav files as coefficients with amovie filters for 7.1 downmix, each amovie filter use stereo file with IR coefficients as input. The files give coefficients for each position of virtual loudspeaker:\n• Full example using wav files as coefficients with amovie filters for 7.1 downmix, but now in format.\n\nApply a high-pass filter with 3dB point frequency. The filter can be either single-pole, or double-pole (the default). The filter roll off at 6dB per pole per octave (20dB per pole per decade).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nIt accepts the following parameters:\n\nThe filter will attempt to guess the mappings when they are not specified explicitly. It does so by first trying to find an unused matching input channel and if that fails it picks the first unused input channel.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n• List all available plugins within amp (LADSPA example plugin) library:\n• List all available controls and their valid ranges for plugin from library:\n• Add reverberation to the audio using TAP-plugins (Tom’s Audio Processing plugins):\n• Generate 20 bpm clicks using plugin from the (CAPS) library:\n• Increase volume by 20dB using fast lookahead limiter from Steve Harris collection:\n• Reduce stereo image using from the (CAPS) library:\n• Another white noise, now using (CAPS) library:\n\nThis filter supports the following commands:\n\nEBU R128 loudness normalization. Includes both dynamic and linear normalization modes. Support for both single pass (livestreams, files) and double pass (files) modes. This algorithm can target IL, LRA, and maximum true peak. In dynamic mode, to accurately detect true peaks, the audio stream will be upsampled to 192 kHz. Use the option or filter to explicitly set an output sample rate.\n\nThe filter accepts the following options:\n\nApply a low-pass filter with 3dB point frequency. The filter can be either single-pole or double-pole (the default). The filter roll off at 6dB per pole per octave (20dB per pole per decade).\n\nThe filter accepts the following options:\n• Lowpass only LFE channel, it LFE is not present it does nothing:\n\nThis filter supports the following commands:\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThis filter supports all options that are exported by plugin as commands.\n\nThe input audio is divided into bands using 4th order Linkwitz-Riley IIRs. This is akin to the crossover of a loudspeaker, and results in flat frequency response when absent compander action.\n\nIt accepts the following parameters:\n\nMix channels with specific gain levels. The filter accepts the output channel layout followed by a set of channels definitions.\n\nThis filter is also designed to efficiently remap the channels of an audio stream.\n\nThe filter accepts parameters of the form: \" | | |...\"\n\nIf the ‘=’ in a channel specification is replaced by ‘<’, then the gains for that specification will be renormalized so that the total is 1, thus avoiding clipping noise.\n\nFor example, if you want to down-mix from stereo to mono, but with a bigger factor for the left channel:\n\nA customized down-mix to stereo that works automatically for 3-, 4-, 5- and 7-channels surround:\n\nNote that integrates a default down-mix (and up-mix) system that should be preferred (see \"-ac\" option) unless you have very specific needs.\n\nThe channel remapping will be effective if, and only if:\n• gain coefficients are zeroes or ones,\n• only one input per channel output,\n\nIf all these conditions are satisfied, the filter will notify the user (\"Pure channel mapping detected\"), and use an optimized and lossless method to do the remapping.\n\nFor example, if you have a 5.1 source and want a stereo audio stream by dropping the extra channels:\n\nGiven the same source, you can also switch front left and front right channels and keep the input channel layout:\n\nIf the input is a stereo audio stream, you can mute the front left channel (and still keep the stereo channel layout) with:\n\nStill with a stereo audio stream input, you can copy the right channel in both front left and right:\n\nReplayGain scanner filter. This filter takes an audio stream as an input and outputs it unchanged. At end of filtering it displays and .\n\nThe filter accepts the following exported read-only options:\n\nConvert the audio sample format, sample rate and channel layout. It is not meant to be used directly.\n\nTo enable compilation of this filter, you need to configure FFmpeg with .\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThis filter acts like normal compressor but has the ability to compress detected signal using second input signal. It needs two input streams and returns one output stream. First input stream will be processed depending on second stream signal. The filtered signal then can be filtered with other filters in later stages of processing. See pan and amerge filter.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n• Full ffmpeg example taking 2 audio inputs, 1st input to be compressed depending on the signal of 2nd input and later compressed signal to be merged with 2nd input:\n\nA sidechain gate acts like a normal (wideband) gate but has the ability to filter the detected signal before sending it to the gain reduction stage. Normally a gate uses the full range signal to detect a level above the threshold. For example: If you cut all lower frequencies from your sidechain signal the gate will decrease the volume of your track only if not enough highs appear. With this technique you are able to reduce the resonation of a natural drum or remove \"rumbling\" of muted strokes from a heavily distorted guitar. It needs two input streams and returns one output stream. First input stream will be processed depending on second stream signal.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter logs a message when it detects that the input audio volume is less or equal to a noise tolerance value for a duration greater or equal to the minimum detected noise duration.\n\nThe printed times and duration are expressed in seconds. The or metadata key is set on the first frame whose timestamp equals or exceeds the detection duration and it contains the timestamp of the first frame of the silence.\n\nThe or and or metadata keys are set on the first frame after the silence. If is enabled, and each channel is evaluated separately, the suffixed keys are used, and corresponds to the channel number.\n\nThe filter accepts the following options:\n• Complete example with to detect silence with 0.0001 noise tolerance in :\n\nRemove silence from the beginning, middle or end of the audio.\n\nThe filter accepts the following options:\n• The following example shows how this filter can be used to start a recording that does not contain the delay at the start which usually occurs between pressing the record button and the start of the performance:\n• Trim all silence encountered from beginning to end where there is more than 1 second of silence in audio:\n• Trim all digital silence samples, using peak detection, from beginning to end where there is more than 0 samples of digital silence in audio and digital silence is detected in all channels at same positions in stream:\n• Trim every 2nd encountered silence period from beginning to end where there is more than 1 second of silence per silence period in audio:\n• Similar as above, but keep maximum of 0.5 seconds of silence from each trimmed period:\n• Similar as above, but keep maximum of 1.5 seconds of silence from start of audio:\n\nThis filter supports some above options as commands.\n\nSOFAlizer uses head-related transfer functions (HRTFs) to create virtual loudspeakers around the user for binaural listening via headphones (audio formats up to 9 channels supported). The HRTFs are stored in SOFA files (see http://www.sofacoustics.org/ for a database). SOFAlizer is developed at the Acoustics Research Institute (ARI) of the Austrian Academy of Sciences.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThe filter accepts the following options:\n• Using ClubFritz12 sofa file and bigger radius with small rotation:\n• Similar as above but with custom speaker positions for front left, front right, back left and back right and also with custom gain:\n\nThis filter expands or compresses each half-cycle of audio samples (local set of samples all above or all below zero and between two nearest zero crossings) depending on threshold value, so audio reaches target peak value under conditions controlled by below options.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter has some handy utilities to manage stereo signals, for converting M/S stereo recordings to L/R signal while having control over the parameters or spreading the stereo image of master track.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter enhance the stereo effect by suppressing signal common to both channels and by delaying the signal of left into right and vice versa, thereby widening the stereo effect.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options except as commands.\n\nThe filter accepts the following options:\n\nThis filter allows to produce multichannel output from audio stream.\n\nThe filter accepts the following options:\n\nBoost or cut the lower frequencies and cut or boost higher frequencies of the audio using a two-pole shelving filter with a response similar to that of a standard hi-fi’s tone-controls. This is also known as shelving equalisation (EQ).\n\nThe filter accepts the following options:\n\nThis filter supports some options as commands.\n\nBoost or cut treble (upper) frequencies of the audio using a two-pole shelving filter with a response similar to that of a standard hi-fi’s tone-controls. This is also known as shelving equalisation (EQ).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter accepts stereo input and produce stereo with LFE (2.1) channels output. The newly produced LFE channel have enhanced virtual bass originally obtained from both stereo channels. This filter outputs front left and front right channels unchanged as available in stereo input.\n\nThe filter accepts the following options:\n\nIt accepts the following parameters:\n\nThe volume expression can contain the following parameters.\n\nNote that when is set to ‘ ’ only the and variables are available, all other variables will evaluate to NAN.\n\nThis filter supports the following commands:\n• Halve the input audio volume: In all the above example the named key for can be omitted, for example like in:\n• Fade volume after time 10 with an annihilation period of 5 seconds:\n\nDetect the volume of the input video.\n\nThe filter has no parameters. It supports only 16-bit signed integer samples, so the input will be converted when needed. Statistics about the volume will be printed in the log when the input stream end is reached.\n\nIn particular it will show the mean volume (root mean square), maximum volume (on a per-sample basis), and the beginning of a histogram of the registered volume values (from the maximum value to a cumulated 1/1000 of the samples).\n\nAll volumes are in decibels relative to the maximum PCM value.\n\nHere is an excerpt of the output:\n• The mean square energy is approximately -27 dB, or 10^-2.7.\n• The largest sample is at -4 dB, or more precisely between -4 dB and -5 dB.\n• There are 6 samples at -4 dB, 62 at -5 dB, 286 at -6 dB, etc.\n\nIn other words, raising the volume by +4 dB does not cause any clipping, raising it by +5 dB causes clipping for 6 samples, etc.\n\nBelow is a description of the currently available audio sources.\n\nBuffer audio frames, and make them available to the filter chain.\n\nThis source is mainly intended for a programmatic use, in particular through the interface defined in .\n\nIt accepts the following parameters:\n\nwill instruct the source to accept planar 16bit signed stereo at 44100Hz. Since the sample format with name \"s16p\" corresponds to the number 6 and the \"stereo\" channel layout corresponds to the value 0x3, this is equivalent to:\n\nGenerate an audio signal specified by an expression.\n\nThis source accepts in input one or more expressions (one for each channel), which are evaluated and used to generate a corresponding audio signal.\n\nThis source accepts the following options:\n\nEach expression in can contain the following constants:\n• Generate a sin signal with frequency of 440 Hz, set sample rate to 8000 Hz:\n• Generate a two channels signal, specify the channel layout (Front Center + Back Center) explicitly:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nThe null audio source, return unprocessed audio frames. It is mainly useful as a template and to be employed in analysis / debugging tools, or as the source for filters which ignore the input data (for example the sox synth filter).\n\nThis source accepts the following options:\n• Set the sample rate to 48000 Hz and the channel layout to AV_CH_LAYOUT_MONO.\n• Do the same operation with a more obvious syntax:\n\nAll the parameters need to be explicitly defined.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nNote that versions of the flite library prior to 2.0 are not thread-safe.\n\nThe filter accepts the following options:\n• Read from file , and synthesize the text using the standard flite voice:\n• Read the specified text selecting the voice: flite=text='So fare thee well, poor devil of a Sub-Sub, whose commentator I am':voice=slt\n• Input text to ffmpeg: ffmpeg -f lavfi -i flite=text='So fare thee well, poor devil of a Sub-Sub, whose commentator I am':voice=slt\n• Make speak the specified text, using and the device: ffplay -f lavfi flite=text='No more be grieved for which that thou hast done.'\n\nFor more information about libflite, check: http://www.festvox.org/flite/\n\nThe filter accepts the following options:\n• Generate 60 seconds of pink noise, with a 44.1 kHz sampling rate and an amplitude of 0.5:\n\nThe resulting stream can be used with afir filter for phase-shifting the signal by 90 degrees.\n\nThis is used in many matrix coding schemes and for analytic signal generation. The process is often written as a multiplication by i (or j), the imaginary unit.\n\nThe filter accepts the following options:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nGenerate an audio signal made of a sine wave with amplitude 1/8.\n\nThe filter accepts the following options:\n• Generate a 220 Hz sine wave with a 880 Hz beep each second, for 5 seconds:\n\nBelow is a description of the currently available audio sinks.\n\nBuffer audio frames, and make them available to the end of filter chain.\n\nThis sink is mainly intended for programmatic use, in particular through the interface defined in or the options system.\n\nIt accepts a pointer to an AVABufferSinkContext structure, which defines the incoming buffers’ formats, to be passed as the opaque parameter to for initialization.\n\nNull audio sink; do absolutely nothing with the input audio. It is mainly useful as a template and for use in analysis / debugging tools.\n\nWhen you configure your FFmpeg build, you can disable any of the existing filters using . The configure output will show the video filters included in your build.\n\nBelow is a description of the currently available video filters.\n\nThe frame data is passed through unchanged, but metadata is attached to the frame indicating regions of interest which can affect the behaviour of later encoding. Multiple regions can be marked by applying the filter multiple times.\n• Mark the centre quarter of the frame as interesting.\n• Mark the 100-pixel-wide region on the left edge of the frame as very uninteresting (to be encoded at much lower quality than the rest of the frame).\n\nExtract the alpha component from the input as a grayscale video. This is especially useful with the filter.\n\nAdd or replace the alpha component of the primary input with the grayscale value of a second input. This is intended for use with to allow the transmission or storage of frame sequences that have alpha in a format that doesn’t support an alpha channel.\n\nFor example, to reconstruct full frames from a normal YUV-encoded video and a separate video created with , you might use:\n\nAmplify differences between current pixel and pixels of adjacent frames in same pixel location.\n\nThis filter accepts the following options:\n\nThis filter supports the following commands that corresponds to option of same name:\n\nSame as the subtitles filter, except that it doesn’t require libavcodec and libavformat to work. On the other hand, it is limited to ASS (Advanced Substation Alpha) subtitles files.\n\nThis filter accepts the following option in addition to the common options from the subtitles filter:\n\nApply an Adaptive Temporal Averaging Denoiser to the video input.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options except option . The command accepts the same syntax of the corresponding option.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nCompute the bounding box for the non-black pixels in the input frame luma plane.\n\nThis filter computes the bounding box containing all the pixels with a luma value greater than the minimum allowed value. The parameters describing the bounding box are printed on the filter log.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nDetect video intervals that are (almost) completely black. Can be useful to detect chapter transitions, commercials, or invalid recordings.\n\nThe filter outputs its detection analysis to both the log as well as frame metadata. If a black segment of at least the specified minimum duration is found, a line with the start and end timestamps as well as duration is printed to the log with level . In addition, a log line with level is printed per frame showing the black amount detected for that frame.\n\nThe filter also attaches metadata to the first frame of a black segment with key and to the first frame after the black segment ends with key . The value is the frame’s timestamp. This metadata is added regardless of the minimum duration specified.\n\nThe filter accepts the following options:\n\nThe following example sets the maximum pixel threshold to the minimum value, and detects only black intervals of 2 or more seconds:\n\nDetect frames that are (almost) completely black. Can be useful to detect chapter transitions or commercials. Output lines consist of the frame number of the detected frame, the percentage of blackness, the position in the file if known or -1 and the timestamp in seconds.\n\nIn order to display the output lines, you need to set the loglevel at least to the AV_LOG_INFO value.\n\nThis filter exports frame metadata . The value represents the percentage of pixels in the picture that are below the threshold value.\n\nIt accepts the following parameters:\n\nBlend two video frames into each other.\n\nThe filter takes two input streams and outputs one stream, the first input is the \"top\" layer and second input is \"bottom\" layer. By default, the output terminates when the longest input terminates.\n\nThe (time blend) filter takes two consecutive frames from one single stream, and outputs the result obtained by blending the new frame on top of the old frame.\n\nA description of the accepted options follows.\n\nThe filter also supports the framesync options.\n• Apply transition from bottom layer to top layer in first 10 seconds:\n• Split diagonally video and shows top and bottom layer on each side:\n• Display differences between the current and the previous frame:\n\nThis filter supports same commands as options.\n\nDetermines blockiness of frames without altering the input frames.\n\nBased on Remco Muijs and Ihor Kirenko: \"A no-reference blocking artifact measure for adaptive video processing.\" 2005 13th European signal processing conference.\n\nThe filter accepts the following options:\n• Determine blockiness for the first plane and search for periods within [8,32]:\n\nDetermines blurriness of frames without altering the input frames.\n\nBased on Marziliano, Pina, et al. \"A no-reference perceptual blur metric.\" Allows for a block-based abbreviation.\n\nThe filter accepts the following options:\n• Determine blur for 80% of most significant 32x32 blocks:\n\nThe filter accepts the following options.\n• Same as above, but filtering only luma:\n• Same as above, but with both estimation modes:\n• Same as above, but prefilter with nlmeans filter instead:\n\nIt accepts the following parameters:\n\nA description of the accepted options follows.\n• Apply a boxblur filter with the luma, chroma, and alpha radii set to 2:\n• Set the luma radius to 2, and alpha and chroma radius to 0:\n• Set the luma and chroma radii to a fraction of the video dimension:\n\nMotion adaptive deinterlacing based on yadif with the use of w3fdif and cubic interpolation algorithms. It accepts the following parameters:\n\nThis filter fixes various issues seen with commerical encoders related to upstream malformed CEA-708 payloads, specifically incorrect number of tuples (wrong cc_count for the target FPS), and incorrect ordering of tuples (i.e. the CEA-608 tuples are not at the first entries in the payload).\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nRemove all color information for all colors except for certain one.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n• Make every green pixel in the input image transparent:\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDisplay CIE color diagram with pixels overlaid onto it.\n\nThe filter accepts the following options:\n\nSome codecs can export information through frames using side-data or other means. For example, some MPEG based codecs export motion vectors through the flag in the codec option.\n\nThe filter accepts the following option:\n• Visualize forward predicted MVs of all frames using :\n• Visualize multi-directionals MVs of P and B-Frames using :\n\nModify intensity of primary colors (red, green and blue) of input frames.\n\nThe filter allows an input frame to be adjusted in the shadows, midtones or highlights regions for the red-cyan, green-magenta or blue-yellow balance.\n\nA positive adjustment value shifts the balance towards the primary color, a negative value towards the complementary color.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nAdjust color white balance selectively for blacks and whites. This filter operates in YUV colorspace.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter modifies a color channel by adding the values associated to the other channels of the same pixels. For example if the value to modify is red, the output value will be:\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nRGB colorspace color keying. This filter operates on 8-bit RGB format frames by setting the alpha component of each pixel which falls within the similarity radius of the key color to 0. The alpha value for pixels outside the similarity radius depends on the value of the blend option.\n\nThe filter accepts the following options:\n• Make every green pixel in the input image transparent:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nRemove all color information for all RGB colors except for certain one.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter needs three input video streams. First stream is video stream that is going to be filtered out. Second and third video stream specify color patches for source color to target color mapping.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nFor example to convert from BT.601 to SMPTE-240M, use the command:\n\nConvert colorspace, transfer characteristics or color primaries. Input video needs to have an even size.\n\nThe filter accepts the following options:\n\nThe filter converts the transfer characteristics, color space and color primaries to the specified user values. The output value, if not specified, is set to a default value based on the \"all\" property. If that property is also not specified, the filter will log an error. The output color range and format default to the same value as the input color range and format. The input transfer characteristics, color space, color primaries and color range should be set on the input data. If any of these are missing, the filter will log an error and no conversion will take place.\n\nFor example to convert the input to SMPTE-240M, use the command:\n\nAdjust color temperature in video to simulate variations in ambient color temperature.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nApply convolution of 3x3, 5x5, 7x7 or horizontal/vertical up to 49 elements.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nApply 2D convolution of video stream in frequency domain using second stream as impulse.\n\nThe filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nCopy the input video source unchanged to the output. This is mainly useful for testing purposes.\n\nVideo filtering on GPU using Apple’s CoreImage API on OSX.\n\nHardware acceleration is based on an OpenGL context. Usually, this means it is processed by video hardware. However, software-based OpenGL implementations exist which means there is no guarantee for hardware processing. It depends on the respective OSX.\n\nThere are many filters and image generators provided by Apple that come with a large variety of options. The filter has to be referenced by its name along with its options.\n\nThe coreimage filter accepts the following options:\n\nSeveral filters can be chained for successive processing without GPU-HOST transfers allowing for fast processing of complex filter chains. Currently, only filters with zero (generators) or exactly one (filters) input image and one output image are supported. Also, transition filters are not yet usable as intended.\n\nSome filters generate output images with additional padding depending on the respective filter kernel. The padding is automatically removed to ensure the filter output has the same size as the input image.\n\nFor image generators, the size of the output image is determined by the previous output image of the filter chain or the input image of the whole filterchain, respectively. The generators do not use the pixel information of this image to generate their output. However, the generated output is blended onto this image, resulting in partial or complete coverage of the output image.\n\nThe coreimagesrc video source can be used for generating input images which are directly fed into the filter chain. By using it, providing input images by another video source or an input video is not required.\n• Use the CIBoxBlur filter with default options to blur an image:\n• Use a filter chain with CISepiaTone at default values and CIVignetteEffect with its center at 100x100 and a radius of 50 pixels:\n• Use nullsrc and CIQRCodeGenerator to create a QR code for the FFmpeg homepage, given as complete and escaped command-line for Apple’s standard bash shell:\n\nObtain the correlation between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained per component, average, min and max correlation is printed through the logging system.\n\nThe filter stores the calculated correlation of each frame in frame metadata.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nIt accepts the following options:\n• Cover a rectangular object by the supplied image of a given video using :\n\nCrop the input video to given dimensions.\n\nIt accepts the following parameters:\n\nThe , , , parameters are expressions containing the following constants:\n\nThe expression for may depend on the value of , and the expression for may depend on , but they cannot depend on and , as and are evaluated after and .\n\nThe and parameters specify the expressions for the position of the top-left corner of the output (non-cropped) area. They are evaluated for each frame. If the evaluated value is not valid, it is approximated to the nearest valid value.\n\nThe expression for may depend on , and the expression for may depend on .\n• Crop area with size 100x100 at position (12,34). Using named options, the example above becomes:\n• Crop the central input area with size 2/3 of the input video:\n• Delimit the rectangle with the top-left corner placed at position 100:100 and the right-bottom corner corresponding to the right-bottom corner of the input image.\n• Crop 10 pixels from the left and right borders, and 20 pixels from the top and bottom borders\n• Keep only the bottom right quarter of the input image:\n• Set x depending on the value of y:\n\nThis filter supports the following commands:\n\nIt calculates the necessary cropping parameters and prints the recommended parameters via the logging system. The detected dimensions correspond to the non-black or video area of the input video according to .\n\nIt accepts the following parameters:\n• Find an embedded video area, use motion vectors from decoder:\n\nThis filter supports the following commands:\n\nDelay video filtering until a given wallclock timestamp. The filter first passes on amount of frames, then it buffers at most amount of frames and waits for the cue. After reaching the cue it forwards the buffered frames and also any subsequent frames coming in its input.\n\nThe filter can be used synchronize the output of multiple ffmpeg processes for realtime output devices like decklink. By putting the delay in the filtering chain and pre-buffering frames the process can pass on data to output almost immediately after the target wallclock timestamp is reached.\n\nPerfect frame accuracy cannot be guaranteed, but the result is good enough for some use cases.\n\nThis filter is similar to the Adobe Photoshop and GIMP curves tools. Each component (red, green and blue) has its values defined by key points tied from each other using a smooth curve. The x-axis represents the pixel values from the input frame, and the y-axis the new pixel values to be set for the output frame.\n\nBy default, a component curve is defined by the two points and . This creates a straight line where each original pixel value is \"adjusted\" to its own value, which means no change to the image.\n\nThe filter allows you to redefine these two points and add some more. A new curve will be defined to pass smoothly through all these new coordinates. The new defined points need to be strictly increasing over the x-axis, and their and values must be in the interval. The curve is formed by using a natural or monotonic cubic spline interpolation, depending on the option (default: ). The spline produces a smoother curve in general while the monotonic ( ) spline guarantees the transitions between the specified points to be monotonic. If the computed curves happened to go outside the vector spaces, the values will be clipped accordingly.\n\nThe filter accepts the following options:\n\nTo avoid some filtergraph syntax conflicts, each key points list need to be defined using the following syntax: .\n\nThis filter supports same commands as options.\n• Vintage effect: Here we obtain the following coordinates for each components:\n• The previous example can also be achieved with the associated built-in preset:\n• Use a Photoshop preset and redefine the points of the green component:\n• Check out the curves of the profile using and :\n\nThis filter shows hexadecimal pixel values of part of video.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options excluding option.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThis filter is not designed for real time.\n\nThe filter accepts the following options:\n\nThe same operation can be achieved using the expression system:\n\nRemove banding artifacts from input video. It works by replacing banded pixels with average value of referenced pixels.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n• Deblock using weak filter and block size of 4 pixels.\n• Deblock using strong filter, block size of 4 pixels and custom thresholds for deblocking more edges.\n• Similar as above, but filter only first plane.\n• Similar as above, but filter only second and third plane.\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nApply 2D deconvolution of video stream in frequency domain using second stream as impulse.\n\nThe filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nIt accepts the following options:\n\nThis filter replaces the pixel by the local(3x3) average by taking into account only values lower than the pixel.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nIt accepts the following options:\n\nJudder can be introduced, for instance, by pullup filter. If the original source was partially telecined content then the output of will have a variable frame rate. May change the recorded frame rate of the container. Aside from that change, this filter will not affect constant frame rate video.\n\nThe option available in this filter is:\n\nSuppress a TV station logo by a simple interpolation of the surrounding pixels. Just set a rectangle covering the logo and watch it disappear (and sometimes something even uglier appear - your mileage may vary).\n\nIt accepts the following parameters:\n• Set a rectangle covering the area with top left corner coordinates 0,0 and size 100x77:\n\nRemove the rain in the input image/video by applying the derain methods based on convolutional neural networks. Supported models:\n\nTraining as well as model generation scripts are provided in the repository at https://github.com/XueweiMeng/derain_filter.git.\n\nThe filter accepts the following options:\n\nTo get full functionality (such as async execution), please use the dnn_processing filter.\n\nAttempt to fix small changes in horizontal and/or vertical shift. This filter helps remove camera shake from hand-holding a camera, bumping a tripod, moving on a vehicle, etc.\n\nThe filter accepts the following options:\n\nRemove unwanted contamination of foreground colors, caused by reflected color of greenscreen or bluescreen.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nApply an exact inverse of the telecine operation. It requires a predefined pattern specified using the pattern option which must be the same as that passed to the telecine filter.\n\nThis filter accepts the following options:\n\nThis filter replaces the pixel by the local(3x3) maximum.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDisplace pixels as indicated by second and third input stream.\n\nIt takes three input streams and outputs one stream, the first input is the source, and second and third input are displacement maps.\n\nThe second input specifies how much to displace pixels along the x-axis, while the third input specifies how much to displace pixels along the y-axis. If one of displacement map streams terminates, last frame from that displacement map will be used.\n\nNote that once generated, displacements maps can be reused over and over again.\n\nA description of the accepted options follows.\n\nDo classification with deep neural networks based on bounding boxes.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nDo image processing with deep neural networks. It works together with another filter which converts the pixel format of the Frame to what the dnn network requires.\n\nThe filter accepts the following options:\n• Remove rain in rgb24 frame with can.pb (see derain filter):\n• Handle the Y channel with srcnn.pb (see sr filter) for frame with yuv420p (planar YUV formats supported):\n• Handle the Y channel with espcn.pb (see sr filter), which changes frame size, for format yuv420p (planar YUV formats supported), please use tools/python/tf_sess_config.py to get the configs of TensorFlow backend for your system.\n\nIt accepts the following parameters:\n\nThe parameters for , , and and are expressions containing the following constants:\n• Draw a black box around the edge of the input image:\n• Draw a box with color red and an opacity of 50%: The previous example can be specified as:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nIt accepts the following parameters:\n\nExample using metadata from signalstats filter:\n\nExample using metadata from ebur128 filter:\n\nIt accepts the following parameters:\n\nThe parameters for , , and and are expressions containing the following constants:\n• Draw a grid with cell 100x100 pixels, thickness 2 pixels, with color red and an opacity of 50%:\n• Draw a white 3x3 grid with an opacity of 50%:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nDraw a text string or text from a specified file on top of a video, using the libfreetype library.\n\nTo enable compilation of this filter, you need to configure FFmpeg with and . To enable default font fallback and the option you need to configure FFmpeg with . To enable the option, you need to configure FFmpeg with .\n\nIt accepts the following parameters:\n\nThe parameters for and are expressions containing the following constants and functions:\n\nIf is set to , the filter recognizes sequences accepted by the C function in the provided text and expands them accordingly. Check the documentation of . This feature is deprecated in favor of expansion with the or expansion functions.\n\nIf is set to , the text is printed verbatim.\n\nIf is set to (which is the default), the following expansion mechanism is used.\n\nThe backslash character ‘ ’, followed by any character, always expands to the second character.\n\nSequences of the form are expanded. The text between the braces is a function name, possibly followed by arguments separated by ’:’. If the arguments contain special characters or delimiters (’:’ or ’}’), they should be escaped.\n\nNote that they probably must also be escaped as the value for the option in the filter argument string and as the filter argument in the filtergraph description, and possibly also for the shell, that makes up to four levels of escaping; using a text file with the option avoids these problems.\n\nThe following functions are available:\n\nThe following options are also supported as commands:\n• Draw \"Test Text\" with font FreeSerif, using the default values for the optional parameters.\n• Draw ’Test Text’ with font FreeSerif of size 24 at position x=100 and y=50 (counting from the top-left corner of the screen), text is yellow with a red box around it. Both the text and the box have an opacity of 20%. Note that the double quotes are not necessary if spaces are not used within the parameter list.\n• Show the text at the center of the video frame:\n• Show the text at a random position, switching to a new position every 30 seconds:\n• Show a text line sliding from right to left in the last row of the video frame. The file is assumed to contain a single line with no newlines.\n• Show the content of file off the bottom of the frame and scroll up.\n• Draw a single green letter \"g\", at the center of the input video. The glyph baseline is placed at half screen height.\n• Show text for 1 second every 3 seconds:\n• Use fontconfig to set the font. Note that the colons need to be escaped.\n• Draw \"Test Text\" with font size dependent on height of the video.\n• Print the date of a real-time encoding (see documentation for the C function):\n• Show text fading in and out (appearing/disappearing):\n• Horizontally align multiple separate texts. Note that and the value are included in the offset.\n• Plot special metadata onto each frame if such metadata exists. Otherwise, plot the string \"NA\". Note that image2 demuxer must have option for the special metadata fields to be available for filters.\n\nFor more information about libfreetype, check: http://www.freetype.org/.\n\nFor more information about fontconfig, check: http://freedesktop.org/software/fontconfig/fontconfig-user.html.\n\nFor more information about libfribidi, check: http://fribidi.org/.\n\nFor more information about libharfbuzz, check: https://github.com/harfbuzz/harfbuzz.\n\nDetect and draw edges. The filter uses the Canny Edge Detection algorithm.\n\nThe filter accepts the following options:\n• Standard edge detection with custom values for the hysteresis thresholding:\n\nFor each input image, the filter will compute the optimal mapping from the input to the output given the codebook length, that is the number of distinct output colors.\n\nThis filter accepts the following options.\n\nMeasure graylevel entropy in histogram of color channels of video frames.\n\nIt accepts the following parameters:\n\nApply the EPX magnification filter which is designed for pixel art.\n\nIt accepts the following option:\n\nThe filter accepts the following options:\n\nThe expressions accept the following parameters:\n\nThe filter supports the following commands:\n\nThis filter replaces the pixel by the local(3x3) minimum.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nSpatial only filter that uses edge slope tracing algorithm to interpolate missing lines. It accepts the following parameters:\n\nThis filter supports same commands as options.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nThe filter accepts the following option:\n• Extract luma, u and v color channel component from input video frame into 3 grayscale outputs:\n\nIt accepts the following parameters:\n• Fade in the first 30 frames of video: The command above is equivalent to:\n• Fade out the last 45 frames of a 200-frame video:\n• Fade in the first 25 frames and fade out the last 25 frames of a 1000-frame video:\n• Make the first 5 frames yellow, then fade in from frame 5-24:\n• Fade in alpha over first 25 frames of video:\n• Make the first 5.5 seconds black, then fade in for 0.5 seconds:\n\nThis filter pass cropped input frames to 2nd output. From there it can be filtered with other video filters. After filter receives frame from 2nd input, that frame is combined on top of original frame from 1st input and passed to 1st output.\n\nThe typical usage is filter only part of frame.\n\nThe filter accepts the following options:\n• Blur only top left rectangular part of video frame size 100x100 with gblur filter.\n• Draw black box on top left part of video frame of size 100x100 with drawbox filter.\n• Pixelize rectangular part of video frame of size 100x100 with pixelize filter.\n\nThe filter accepts the following options:\n\nExtract a single field from an interlaced image using stride arithmetic to avoid wasting CPU time. The output frames are marked as non-interlaced.\n\nThe filter accepts the following options:\n\nCreate new frames by copying the top and bottom fields from surrounding frames supplied as numbers by the hint file.\n\nExample of first several lines of file for mode:\n\nField matching filter for inverse telecine. It is meant to reconstruct the progressive frames from a telecined stream. The filter does not drop duplicated frames, so to achieve a complete inverse telecine needs to be followed by a decimation filter such as decimate in the filtergraph.\n\nThe separation of the field matching and the decimation is notably motivated by the possibility of inserting a de-interlacing filter fallback between the two. If the source has mixed telecined and real interlaced content, will not be able to match fields for the interlaced parts. But these remaining combed frames will be marked as interlaced, and thus can be de-interlaced by a later filter such as yadif before decimation.\n\nIn addition to the various configuration options, can take an optional second stream, activated through the option. If enabled, the frames reconstruction will be based on the fields and frames from this second stream. This allows the first input to be pre-processed in order to help the various algorithms of the filter, while keeping the output lossless (assuming the fields are matched properly). Typically, a field-aware denoiser, or brightness/contrast adjustments can help.\n\nNote that this filter uses the same algorithms as TIVTC/TFM (AviSynth project) and VIVTC/VFM (VapourSynth project). The later is a light clone of TFM from which is based on. While the semantic and usage are very close, some behaviour and options names can differ.\n\nThe decimate filter currently only works for constant frame rate input. If your input has mixed telecined (30fps) and progressive content with a lower framerate like 24fps use the following filterchain to produce the necessary cfr stream: .\n\nThe filter accepts the following options:\n\nWe assume the following telecined stream:\n\nThe numbers correspond to the progressive frame the fields relate to. Here, the first two frames are progressive, the 3rd and 4th are combed, and so on.\n\nWhen is configured to run a matching from bottom ( = ) this is how this input stream get transformed:\n\nAs a result of the field matching, we can see that some frames get duplicated. To perform a complete inverse telecine, you need to rely on a decimation filter after this operation. See for instance the decimate filter.\n\nThe same operation now matching from top fields ( = ) looks like this:\n\nIn these examples, we can see what , and mean; basically, they refer to the frame and field of the opposite parity:\n• matches the field of the opposite parity in the previous frame\n• matches the field of the opposite parity in the current frame\n• matches the field of the opposite parity in the next frame\n\nThe and matching are a bit special in the sense that they match from the opposite parity flag. In the following examples, we assume that we are currently matching the 2nd frame (Top:2, bottom:2). According to the match, a ’x’ is placed above and below each matched fields.\n\nAdvanced IVTC, with fallback on yadif for still combed frames:\n\nTransform the field order of the input video.\n\nIt accepts the following parameters:\n\nThe default value is ‘ ’.\n\nThe transformation is done by shifting the picture content up or down by one line, and filling the remaining line with appropriate picture content. This method is consistent with most broadcast field order converters.\n\nIf the input video is not flagged as being interlaced, or it is already flagged as being of the required output field order, then this filter does not alter the incoming video.\n\nIt is very useful when converting to or from PAL DV material, which is bottom field first.\n\nFill borders of the input video, without changing video stream dimensions. Sometimes video can have garbage at the four edges and you may not want to crop video input to keep size multiple of some number.\n\nThis filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe object to search for must be specified as a gray8 image specified with the option.\n\nFor each possible match, a score is computed. If the score reaches the specified threshold, the object is considered found.\n\nIf the input video contains multiple instances of the object, the filter will find only one of them.\n\nWhen an object is found, the following metadata entries are set in the matching frame:\n\nIt accepts the following options:\n• Cover a rectangular object by the supplied image of a given video using :\n• Find the position of an object in each frame using and write it to a log file:\n\nFlood area with values of same pixel components with another values.\n\nIt accepts the following options:\n\nConvert the input video to one of the specified pixel formats. Libavfilter will try to pick one that is suitable as input to the next filter.\n\nIt accepts the following parameters:\n• Convert the input video to the format Convert the input video to any of the formats in the list\n\nConvert the video to specified constant frame rate by duplicating or dropping frames as necessary.\n\nIt accepts the following parameters:\n\nAlternatively, the options can be specified as a flat string: [: [: ]].\n\nSee also the setpts filter.\n• A typical usage in order to set the fps to 25:\n• Sets the fps to 24, using abbreviation and rounding method to round to nearest:\n\nPack two different video streams into a stereoscopic video, setting proper metadata on supported codecs. The two views should have the same size and framerate and processing will stop when the shorter video ends. Please note that you may conveniently adjust view properties with the scale and fps filters.\n\nIt accepts the following parameters:\n\nChange the frame rate by interpolating new video output frames from the source frames.\n\nThis filter is not designed to function correctly with interlaced media. If you wish to change the frame rate of interlaced media then you are required to deinterlace before this filter and re-interlace after this filter.\n\nA description of the accepted options follows.\n\nThis filter accepts the following option:\n\nThis filter logs a message and sets frame metadata when it detects that the input video has no significant change in content during a specified duration. Video freeze detection calculates the mean average absolute difference of all the components of video frames and compares it to a noise floor.\n\nThe printed times and duration are expressed in seconds. The metadata key is set on the first frame whose timestamp equals or exceeds the detection duration and it contains the timestamp of the first frame of the freeze. The and metadata keys are set on the first frame after the freeze.\n\nThe filter accepts the following options:\n\nThis filter freezes video frames using frame from 2nd input.\n\nThe filter accepts the following options:\n\nTo enable the compilation of this filter, you need to install the frei0r header and configure FFmpeg with .\n\nIt accepts the following parameters:\n\nA frei0r effect parameter can be a boolean (its value is either \"y\" or \"n\"), a double, a color (specified as / / , where , , and are floating point numbers between 0.0 and 1.0, inclusive) or a color description as specified in the (ffmpeg-utils)\"Color\" section in the ffmpeg-utils manual, a position (specified as / , where and are floating point numbers) and/or a string.\n\nThe number and types of parameters depend on the loaded effect. If an effect parameter is not specified, the default value is set.\n• Apply the distort0r effect, setting the first two double parameters:\n• Apply the colordistance effect, taking a color as the first parameter:\n• Apply the perspective effect, specifying the top left and top right image positions:\n\nFor more information, see http://frei0r.dyne.org\n\nThis filter supports the option as commands.\n\nApply fast and simple postprocessing. It is a faster version of spp.\n\nIt splits (I)DCT into horizontal/vertical passes. Unlike the simple post- processing filter, one of them is performed once per block, not per pixel. This allows for much higher speed.\n\nThe filter accepts the following options:\n\nSynchronize video frames with an external mapping from a file.\n\nFor each input PTS given in the map file it either drops or creates as many frames as necessary to recreate the sequence of output frames given in the map file.\n\nThis filter is useful to recreate the output frames of a framerate conversion by the fps filter, recorded into a map file using the ffmpeg option , and do further processing to the corresponding frames e.g. quality comparison.\n\nEach line of the map file must contain three items per input frame, the input PTS (decimal), the output PTS (decimal) and the output TIMEBASE (decimal/decimal), seperated by a space. This file format corresponds to the output of .\n\nThe filter assumes the map file is sorted by increasing input PTS.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following options:\n\nThe colorspace is selected according to the specified options. If one of the , , or options is specified, the filter will automatically select a YCbCr colorspace. If one of the , , or options is specified, it will select an RGB colorspace.\n\nIf one of the chrominance expression is not defined, it falls back on the other one. If no alpha expression is specified it will evaluate to opaque value. If none of chrominance expressions are specified, they will evaluate to the luma expression.\n\nThe expressions can use the following variables and functions:\n\nFor functions, if and are outside the area, the value will be automatically clipped to the closer edge.\n\nPlease note that this filter can use multiple threads in which case each slice will have its own expression state. If you want to use only a single expression state because your expressions depend on previous state then you should limit the number of filter threads to 1.\n• Generate a bidimensional sine wave, with angle and a wavelength of 100 pixels:\n• Create a radial gradient that is the same size as the input (also see the vignette filter):\n\nFix the banding artifacts that are sometimes introduced into nearly flat regions by truncation to 8-bit color depth. Interpolate the gradients that should go where the bands are, and dither them.\n\nIt is designed for playback only. Do not use it prior to lossy compression, because compression tends to lose the dither and bring back the bands.\n\nIt accepts the following parameters:\n\nAlternatively, the options can be specified as a flat string: [: ]\n• Apply the filter with a strength and radius of :\n• Specify radius, omitting the strength (which will fall-back to the default value):\n\nWith this filter one can debug complete filtergraph. Especially issues with links filling with queued frames.\n\nThe filter accepts the following options:\n\nA color constancy filter that applies color correction based on the grayworld assumption\n\nThe algorithm uses linear light, so input data should be linearized beforehand (and possibly correctly tagged).\n\nA color constancy variation filter which estimates scene illumination via grey edge algorithm and corrects the scene colors accordingly.\n\nThe filter accepts the following options:\n\nApply guided filter for edge-preserving smoothing, dehazing and so on.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n• Dehazing, structure-transferring filtering, detail enhancement with guided filter. For the generation of guidance image, refer to paper \"Guided Image Filtering\". See: http://kaiminghe.com/publications/pami12guidedfilter.pdf.\n\nFirst input is the video stream to process, and second one is the Hald CLUT. The Hald CLUT input can be a simple picture or a complete video stream.\n\nThe filter accepts the following options:\n\nalso has the same interpolation options as lut3d (both filters share the same internals).\n\nThis filter also supports the framesync options.\n\nMore information about the Hald CLUT can be found on Eskil Steenberg’s website (Hald CLUT author) at http://www.quelsolaar.com/technology/clut.html.\n\nThis filter supports the option as commands.\n\nGenerate an identity Hald CLUT stream altered with various effects:\n\nNote: make sure you use a lossless codec.\n\nThen use it with to apply it on some random stream:\n\nThe Hald CLUT will be applied to the 10 first seconds (duration of ), then the latest picture of that CLUT stream will be applied to the remaining frames of the stream.\n\nA Hald CLUT is supposed to be a squared image of by pixels. For a given Hald CLUT, FFmpeg will select the biggest possible square starting at the top left of the picture. The remaining padding pixels (bottom or right) will be ignored. This area can be used to add a preview of the Hald CLUT.\n\nTypically, the following generated Hald CLUT will be supported by the filter:\n\nIt contains the original and a preview of the effect of the CLUT: SMPTE color bars are displayed on the right-top, and below the same color bars processed by the color changes.\n\nThen, the effect of this Hald CLUT can be visualized with:\n\nFor example, to horizontally flip the input video with :\n\nIt can be used to correct video that has a compressed range of pixel intensities. The filter redistributes the pixel intensities to equalize their distribution across the intensity range. It may be viewed as an \"automatically adjusting contrast filter\". This filter is useful only for correcting degraded or poorly captured source video.\n\nThe filter accepts the following options:\n\nCompute and draw a color distribution histogram for the input video.\n\nThe computed histogram is a representation of the color component distribution in an image.\n\nStandard histogram displays the color components distribution in an image. Displays color graph for each color component. Shows distribution of the Y, U, V, A or R, G, B components, depending on input format, in the current frame. Below each graph a color component scale meter is shown.\n\nThe filter accepts the following options:\n\nThis is a high precision/quality 3d denoise filter. It aims to reduce image noise, producing smooth images and making still images really still. It should enhance compressibility.\n\nIt accepts the following optional parameters:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe input must be in hardware frames, and the output a non-hardware format. Not all formats will be supported on the output - it may be necessary to insert an additional filter immediately following in the graph to get the output in a supported format.\n\nMap hardware frames to system memory or to another device.\n\nThis filter has several different modes of operation; which one is used depends on the input and output formats:\n• Hardware frame input, normal frame output Map the input frames to system memory and pass them to the output. If the original hardware frame is later required (for example, after overlaying something else on part of it), the filter can be used again in the next mode to retrieve it.\n• Normal frame input, hardware frame output If the input is actually a software-mapped hardware frame, then unmap it - that is, return the original hardware frame. Otherwise, a device must be provided. Create new hardware surfaces on that device for the output, then map them back to the software format at the input and give those frames to the preceding filter. This will then act like the filter, but may be able to avoid an additional copy when the input is already in a compatible format.\n• Hardware frame input and output A device must be supplied for the output, either directly or with the option. The input and output devices must be of different types and compatible - the exact meaning of this is system-dependent, but typically it means that they must refer to the same underlying hardware context (for example, refer to the same graphics card). If the input frames were originally created on the output device, then unmap to retrieve the original frames. Otherwise, map the frames to the output device - create new hardware frames on the output corresponding to the frames on the input.\n\nThe following additional parameters are accepted:\n\nThe device to upload to must be supplied when the filter is initialised. If using ffmpeg, select the appropriate device with the option or with the option. The input and output devices must be of different types and compatible - the exact meaning of this is system-dependent, but typically it means that they must refer to the same underlying hardware context (for example, refer to the same graphics card).\n\nThe following additional parameters are accepted:\n\nIt accepts the following optional parameters:\n\nApply a high-quality magnification filter designed for pixel art. This filter was originally created by Maxim Stepin.\n\nIt accepts the following option:\n\nAll streams must be of same pixel format and of same height.\n\nNote that this filter is faster than using overlay and pad filter to create same output.\n\nThe filter accepts the following option:\n\nThis filter measures color difference between set HSV color in options and ones measured in video stream. Depending on options, output colors can be changed to be gray or not.\n\nThe filter accepts the following options:\n\nThis filter measures color difference between set HSV color in options and ones measured in video stream. Depending on options, output colors can be changed to transparent by adding alpha channel.\n\nThe filter accepts the following options:\n\nModify the hue and/or the saturation of the input.\n\nIt accepts the following parameters:\n\nand are mutually exclusive, and can’t be specified at the same time.\n\nThe , , and option values are expressions containing the following constants:\n• Set the hue to 90 degrees and the saturation to 1.0:\n• Same command but expressing the hue in radians:\n• Rotate hue and make the saturation swing between 0 and 2 over a period of 1 second:\n• Apply a 3 seconds saturation fade-in effect starting at 0: The general fade-in expression can be written as:\n• Apply a 3 seconds saturation fade-out effect starting at 5 seconds: The general fade-out expression can be written as:\n\nThis filter supports the following commands:\n\nThis filter accepts the following options:\n\nGrow first stream into second stream by connecting components. This makes it possible to build more robust edge masks.\n\nThis filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nDetect the colorspace from an embedded ICC profile (if present), and update the frame’s tags accordingly.\n\nThis filter accepts the following options:\n\nGenerate ICC profiles and attach them to frames.\n\nThis filter accepts the following options:\n\nObtain the identity score between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained per component, average, min and max identity score is printed through the logging system.\n\nThe filter stores the calculated identity scores of each frame in frame metadata.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nThis filter tries to detect if the input frames are interlaced, progressive, top or bottom field first. It will also try to detect fields that are repeated between adjacent frames (a sign of telecine).\n\nSingle frame detection considers only immediately adjacent frames when classifying each frame. Multiple frame detection incorporates the classification history of previous frames.\n\nThe filter will log these metadata values:\n\nThe filter accepts the following options:\n\nInspect the field order of the first 360 frames in a video, in verbose detail:\n\nThe idet filter will add analysis metadata to each frame, which will then be discarded. At the end, the filter will also print a final report with statistics.\n\nThis filter allows one to process interlaced images fields without deinterlacing them. Deinterleaving splits the input frame into 2 fields (so called half pictures). Odd lines are moved to the top half of the output image, even lines to the bottom half. You can process (filter) them independently and then re-interleave them.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter replaces the pixel by the local(3x3) average by taking into account only values higher than the pixel.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nSimple interlacing filter from progressive contents. This interleaves upper (or lower) lines from odd frames with lower (or upper) lines from even frames, halving the frame rate and preserving image height.\n\nIt accepts the following optional parameters:\n\nDeinterlace input video by applying Donald Graft’s adaptive kernel deinterling. Work on interlaced parts of a video to produce progressive frames.\n\nThe description of the accepted parameters follows.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThis filter makes short flashes of light appear longer. This filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter can be used to correct for radial distortion as can result from the use of wide angle lenses, and thereby re-rectify the image. To find the right parameters one can use tools available for example as part of opencv or simply trial-and-error. To use opencv use the calibration sample (under samples/cpp) from the opencv sources and extract the k1 and k2 coefficients from the resulting matrix.\n\nNote that effectively the same filter is available in the open-source tools Krita and Digikam from the KDE project.\n\nIn contrast to the vignette filter, which can also be used to compensate lens errors, this filter corrects the distortion of the image, whereas vignette corrects the brightness distribution, so you may want to use both filters together in certain cases, though you will have to take care of ordering, i.e. whether vignetting should be applied before or after lens correction.\n\nThe filter accepts the following options:\n\nThe formula that generates the correction is:\n\nwhere is halve of the image diagonal and and are the distances from the focal point in the source and target images, respectively.\n\nThis filter supports the all above options as commands.\n\nThe filter requires the camera make, camera model, and lens model to apply the lens correction. The filter will load the lensfun database and query it to find the corresponding camera and lens entries in the database. As long as these entries can be found with the given options, the filter can perform corrections on frames. Note that incomplete strings will result in the filter choosing the best match with the given options, and the filter will output the chosen camera and lens models (logged with level \"info\"). You must provide the make, camera model, and lens model as they are required.\n\nTo obtain a list of available makes and models, leave out one or both of and options. The filter will send the full list to the log with level . The first column is the make and the second column is the model. To obtain a list of available lenses, set any values for make and model and leave out the option. The filter will send the full list of lenses in the log with level . The ffmpeg tool will exit after the list is printed.\n\nThe filter accepts the following options:\n• Apply lens correction with make \"Canon\", camera model \"Canon EOS 100D\", and lens model \"Canon EF-S 18-55mm f/3.5-5.6 IS STM\" with focal length of \"18\" and aperture of \"8.0\".\n• Apply the same as before, but only for the first 5 seconds of video.\n\nThe options for this filter are divided into the following sections:\n\nThese options control the overall output mode. By default, libplacebo will try to preserve the source colorimetry and size as best as it can, but it will apply any embedded film grain, dolby vision metadata or anamorphic SAR present in source frames.\n\nIn addition to the expression constants documented for the scale filter, the , , , , , , and options can also contain the following constants:\n\nThe options in this section control how libplacebo performs upscaling and (if necessary) downscaling. Note that libplacebo will always internally operate on 4:4:4 content, so any sub-sampled chroma formats such as will necessarily be upsampled and downsampled as part of the rendering process. That means scaling might be in effect even if the source and destination resolution are the same.\n\nDeinterlacing is automatically supported when frames are tagged as interlaced, however frames are not deinterlaced unless a deinterlacing algorithm is chosen.\n\nLibplacebo comes with a built-in debanding filter that is good at counteracting many common sources of banding and blocking. Turning this on is highly recommended whenever quality is desired.\n\nA collection of subjective color controls. Not very rigorous, so the exact effect will vary somewhat depending on the input primaries and colorspace.\n\nTo help deal with sources that only have static HDR10 metadata (or no tagging whatsoever), libplacebo uses its own internal frame analysis compute shader to analyze source frames and adapt the tone mapping function in realtime. If this is too slow, or if exactly reproducible frame-perfect results are needed, it’s recommended to turn this feature off.\n\nThe options in this section control how libplacebo performs tone-mapping and gamut-mapping when dealing with mismatches between wide-gamut or HDR content. In general, libplacebo relies on accurate source tagging and mastering display gamut information to produce the best results.\n\nBy default, libplacebo will dither whenever necessary, which includes rendering to any integer format below 16-bit precision. It’s recommended to always leave this on, since not doing so may result in visible banding in the output, even if the filter is enabled. If maximum performance is needed, use instead of disabling dithering.\n\nlibplacebo supports a number of custom shaders based on the mpv .hook GLSL syntax. A collection of such shaders can be found here: https://github.com/mpv-player/mpv/wiki/User-Scripts#user-shaders\n\nA full description of the mpv shader format is beyond the scope of this section, but a summary can be found here: https://mpv.io/manual/master/#options-glsl-shader\n\nAll of the options in this section default off. They may be of assistance when attempting to squeeze the maximum performance at the cost of quality.\n\nThis filter supports almost all of the above options as commands.\n• Rescale input to fit into standard 1080p, with high quality scaling:\n• Run this filter on the CPU, on systems with Mesa installed (and with the most expensive options disabled):\n• Suppress CPU-based AV1/H.274 film grain application in the decoder, in favor of doing it with this filter. Note that this is only a gain if the frames are either already on the GPU, or if you’re using libplacebo for other purposes, since otherwise the VRAM roundtrip will more than offset any expected speedup.\n• Interop with VAAPI hwdec to avoid round-tripping through RAM:\n\nCalculate the VMAF (Video Multi-Method Assessment Fusion) score for a reference/distorted pair of input videos.\n\nThe first input is the distorted video, and the second input is the reference video.\n\nThe obtained VMAF score is printed through the logging system.\n\nIt requires Netflix’s vmaf library (libvmaf) as a pre-requisite. After installing the library it can be enabled using: .\n\nThe filter has following options:\n\nThis filter also supports the framesync options.\n• In the examples below, a distorted video is compared with a reference file .\n• Example with options and different containers:\n\nThis is the CUDA variant of the libvmaf filter. It only accepts CUDA frames.\n\nIt requires Netflix’s vmaf library (libvmaf) as a pre-requisite. After installing the library it can be enabled using: .\n\nApply limited difference filter using second and optionally third video stream.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands except option ‘ ’.\n\nLimits the pixel components values to the specified range [min, max].\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the option as commands.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nCompute a look-up table for binding each pixel component input value to an output value, and apply it to the input video.\n\napplies a lookup table to a YUV input video, to an RGB input video.\n\nThese filters accept the following parameters:\n\nEach of them specifies the expression to use for computing the lookup table for the corresponding pixel component values.\n\nThe exact component associated to each of the options depends on the format in input.\n\nThe filter requires either YUV or RGB pixel formats in input, requires RGB pixel formats in input, and requires YUV.\n\nThe expressions can contain the following constants and functions:\n\nThis filter supports same commands as options.\n• Negate input video: The above is the same as:\n\nThe filter takes two input streams and outputs one stream.\n\nThe (time lut2) filter takes two consecutive frames from one single stream.\n\nThis filter accepts the following parameters:\n\nThe filter also supports the framesync options.\n\nEach of them specifies the expression to use for computing the lookup table for the corresponding pixel component values.\n\nThe exact component associated to each of the options depends on the format in inputs.\n\nThe expressions can contain the following constants:\n\nThis filter supports the all above options as commands except option .\n\nClamp the first input stream with the second input and third input stream.\n\nReturns the value of first stream to be between second input stream - and third input stream + .\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nMerge the second and third input stream into output stream using absolute differences between second input stream and first input stream and absolute difference between third input stream and first input stream. The picked value will be from second input stream if second absolute difference is greater than first one or from third input stream otherwise.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nMerge the first input stream with the second input stream using per pixel weights in the third input stream.\n\nA value of 0 in the third stream pixel component means that pixel component from first stream is returned unchanged, while maximum value (eg. 255 for 8-bit videos) means that pixel component from second stream is returned unchanged. Intermediate values define the amount of merging between both input stream’s pixel components.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nMerge the second and third input stream into output stream using absolute differences between second input stream and first input stream and absolute difference between third input stream and first input stream. The picked value will be from second input stream if second absolute difference is less than first one or from third input stream otherwise.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nPick pixels comparing absolute difference of two video streams with fixed threshold.\n\nIf absolute difference between pixel component of first and second video stream is equal or lower than user supplied threshold than pixel component from first video stream is picked, otherwise pixel component from second video stream is picked.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nFor example it is useful to create motion masks after filter.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nIt needs one field per frame as input and must thus be used together with yadif=1/3 or equivalent.\n\nThis filter accepts the following options:\n\nPick median pixel from certain rectangle defined by radius.\n\nThis filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts up to 4 input streams, and merge selected input planes to the output video.\n\nThis filter accepts the following options:\n• Merge three gray video streams of same width and height into single video stream:\n\nEstimate and export motion vectors using block matching algorithms. Motion vectors are stored in frame side data to be used by other filters.\n\nThis filter accepts the following options:\n\nMidway Image Equalization adjusts a pair of images to have the same histogram, while maintaining their dynamics as much as possible. It’s useful for e.g. matching exposures from a pair of stereo cameras.\n\nThis filter has two inputs and one output, which must be of same pixel format, but may be of different sizes. The output of filter is first input adjusted with midway histogram of both inputs.\n\nThis filter accepts the following option:\n\nConvert the video to specified frame rate using motion interpolation.\n\nThis filter accepts the following options:\n\nMix several video input streams into one video stream.\n\nA description of the accepted options follows.\n\nThis filter supports the following commands:\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nThis filter allows to apply main morphological grayscale transforms, erode and dilate with arbitrary structures set in second input stream.\n\nUnlike naive implementation and much slower performance in erosion and dilation filters, when speed is critical filter should be used instead.\n\nThe filter also supports the framesync options.\n\nThis filter supports same commands as options.\n\nDrop frames that do not differ greatly from the previous frame in order to reduce frame rate.\n\nThe main use of this filter is for very-low-bitrate encoding (e.g. streaming over dialup modem), but it could in theory be used for fixing movies that were inverse-telecined incorrectly.\n\nA description of the accepted options follows.\n\nObtain the MSAD (Mean Sum of Absolute Differences) between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained per component, average, min and max MSAD is printed through the logging system.\n\nThe filter stores the calculated MSAD of each frame in frame metadata.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nMultiply first video stream pixels values with second video stream pixels values.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nIt accepts the following option:\n\nThis filter supports same commands as options.\n\nEach pixel is adjusted by looking for other pixels with similar contexts. This context similarity is defined by comparing their surrounding patches of size x . Patches are searched in an area of x around the pixel.\n\nNote that the research area defines centers for patches, which means some patches will be made of pixels outside that research area.\n\nThe filter accepts the following options.\n\nThis filter accepts the following options:\n\nThis filter supports same commands as options, excluding option.\n\nForce libavfilter not to use any of the specified pixel formats for the input to the next filter.\n\nIt accepts the following parameters:\n• Force libavfilter to use a format different from for the input to the vflip filter:\n• Convert the input video to any of the formats not contained in the list:\n\nThe filter accepts the following options:\n\nFor each channel of each frame, the filter computes the input range and maps it linearly to the user-specified output range. The output range defaults to the full dynamic range from pure black to pure white.\n\nTemporal smoothing can be used on the input range to reduce flickering (rapid changes in brightness) caused when small dark or bright objects enter or leave the scene. This is similar to the auto-exposure (automatic gain control) on a video camera, and, like a video camera, it may cause a period of over- or under-exposure of the video.\n\nThe R,G,B channels can be normalized independently, which may cause some color shifting, or linked together as a single channel, which prevents color shifting. Linked normalization preserves hue. Independent normalization does not, so it can be used to remove some color casts. Independent and linked normalization can be combined in any ratio.\n\nThe normalize filter accepts the following options:\n\nThis filter supports same commands as options, excluding option. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nStretch video contrast to use the full dynamic range, with no temporal smoothing; may flicker depending on the source content:\n\nAs above, but with 50 frames of temporal smoothing; flicker should be reduced, depending on the source content:\n\nAs above, but with hue-preserving linked channel normalization:\n\nAs above, but with half strength:\n\nMap the darkest input color to red, the brightest input color to cyan:\n\nPass the video source unchanged to the output.\n\nThis filter uses Tesseract for optical character recognition. To enable compilation of this filter, you need to configure FFmpeg with .\n\nIt accepts the following options:\n\nThe filter exports recognized text as the frame metadata . The filter exports confidence of recognized words as the frame metadata .\n\nTo enable this filter, install the libopencv library and headers and configure FFmpeg with .\n\nIt accepts the following parameters:\n\nRefer to the official libopencv documentation for more precise information: http://docs.opencv.org/master/modules/imgproc/doc/filtering.html\n\nSeveral libopencv filters are supported; see the following subsections.\n\nDilate an image by using a specific structuring element. It corresponds to the libopencv function .\n\nrepresents a structuring element, and has the syntax: x + x /\n\nand represent the number of columns and rows of the structuring element, and the anchor point, and the shape for the structuring element. must be \"rect\", \"cross\", \"ellipse\", or \"custom\".\n\nIf the value for is \"custom\", it must be followed by a string of the form \"= \". The file with name is assumed to represent a binary image, with each printable character corresponding to a bright pixel. When a custom is used, and are ignored, the number or columns and rows of the read file are assumed instead.\n\nThe default value for is \"3x3+0x0/rect\".\n\nspecifies the number of times the transform is applied to the image, and defaults to 1.\n\nErode an image by using a specific structuring element. It corresponds to the libopencv function .\n\nIt accepts the parameters: : , with the same syntax and semantics as the dilate filter.\n\nThe filter takes the following parameters: | | | | .\n\nis the type of smooth filter to apply, and must be one of the following values: \"blur\", \"blur_no_scale\", \"median\", \"gaussian\", or \"bilateral\". The default value is \"gaussian\".\n\nThe meaning of , , , and depends on the smooth type. and accept integer positive values or 0. and accept floating point values.\n\nThe default value for is 3. The default value for the other parameters is 0.\n\nThese parameters correspond to the parameters assigned to the libopencv function .\n\nUseful to measure spatial impulse, step responses, chroma delays, etc.\n\nIt accepts the following parameters:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nOverlay one video on top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid.\n\nIt accepts the following parameters:\n\nA description of the accepted options follows.\n\nThe , and expressions can contain the following parameters.\n\nThis filter also supports the framesync options.\n\nNote that the , variables are available only when evaluation is done per frame, and will evaluate to NAN when is set to ‘ ’.\n\nBe aware that frames are taken from each input video in timestamp order, hence, if their initial timestamps differ, it is a good idea to pass the two inputs through a filter to have them begin in the same zero timestamp, as the example for the filter does.\n\nYou can chain together more overlays but you should test the efficiency of such approach.\n\nThis filter supports the following commands:\n• Draw the overlay at 10 pixels from the bottom right corner of the main video: Using named options the example above becomes:\n• Insert a transparent PNG logo in the bottom left corner of the input, using the tool with the option:\n• Insert 2 different transparent PNG logos (second logo on bottom right corner) using the tool:\n• Add a transparent color layer on top of the main video; must specify the size of the main input to the overlay filter:\n• Play an original video and a filtered version (here with the deshake filter) side by side using the tool: The above command is the same as:\n• Make a sliding overlay appearing from the left to the right top part of the screen starting since time 2:\n• Compose output by putting two input videos side to side:\n• Mask 10-20 seconds of a video by applying the delogo filter to a section\n\nThe filter accepts the following options:\n\nAdd paddings to the input image, and place the original input at the provided , coordinates.\n\nIt accepts the following parameters:\n\nThe value for the , , , and options are expressions containing the following constants:\n• Add paddings with the color \"violet\" to the input video. The output video size is 640x480, and the top-left corner of the input video is placed at column 0, row 40 The example above is equivalent to the following command:\n• Pad the input to get an output with dimensions increased by 3/2, and put the input video at the center of the padded area:\n• Pad the input to get a squared output with size equal to the maximum value between the input width and height, and put the input video at the center of the padded area:\n• Pad the input to get a final w/h ratio of 16:9:\n• In case of anamorphic video, in order to set the output display aspect correctly, it is necessary to use in the expression, according to the relation: Thus the previous example needs to be modified to:\n• Double the output size and put the input video in the bottom-right corner of the output padded area:\n\nGenerate one palette for a whole video stream.\n\nIt accepts the following options:\n\nThe filter also exports the frame metadata ( ) which you can use to evaluate the degree of color quantization of the palette. This information is also visible at logging level.\n• Generate a representative palette of a given video using :\n\nUse a palette to downsample an input video stream.\n\nThe filter takes two inputs: one video stream and a palette. The palette must be a 256 pixels image.\n\nIt accepts the following options:\n• Use a palette (generated for example with palettegen) to encode a GIF using :\n\nCorrect perspective of video not recorded perpendicular to the screen.\n\nA description of the accepted parameters follows.\n\nDelay interlaced video by one field time so that the field order changes.\n\nThe intended use is to fix PAL movies that have been captured with the opposite field order to the film-to-video transfer.\n\nA description of the accepted parameters follows.\n\nThis filter supports the all above options as commands.\n\nReduce various flashes in video, so to help users with epilepsy.\n\nIt accepts the following options:\n\nPixel format descriptor test filter, mainly useful for internal testing. The output video should be equal to the input video.\n\ncan be used to test the monowhite pixel format descriptor definition.\n\nThe filter accepts the following options:\n\nThis filter supports all options as commands.\n\nDisplay sample values of color channels. Mainly useful for checking color and levels. Minimum supported resolution is 640x480.\n\nThe filters accept the following options:\n\nThis filter supports same commands as options.\n\nEnable the specified chain of postprocessing subfilters using libpostproc. This library should be automatically selected with a GPL build ( ). Subfilters must be separated by ’/’ and can be disabled by prepending a ’-’. Each subfilter and some options have a short and a long name that can be used interchangeably, i.e. dr/dering are the same.\n\nThe filters accept the following options:\n\nAll subfilters share common options to determine their scope:\n\nThese options can be appended after the subfilter name, separated by a ’|’.\n\nThe horizontal and vertical deblocking filters share the difference and flatness values so you cannot set different horizontal and vertical thresholds.\n• Apply deblocking on luma only, and switch vertical deblocking on or off automatically depending on available CPU time:\n\nApply Postprocessing filter 7. It is variant of the spp filter, similar to spp = 6 with 7 point DCT, where only the center sample is used after IDCT.\n\nThe filter accepts the following options:\n\nApply alpha premultiply effect to input video stream using first plane of second stream as alpha.\n\nBoth streams must have same dimensions and same pixel format.\n\nThe filter accepts the following option:\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThis filter accepts the following options:\n\nEach of the expression options specifies the expression to use for computing the lookup table for the corresponding pixel component values.\n\nThe expressions can contain the following constants and functions:\n\nThis filter supports the all above options as commands.\n\nObtain the average, maximum and minimum PSNR (Peak Signal to Noise Ratio) between two input videos.\n\nThis filter takes in input two input videos, the first input is considered the \"main\" source and is passed unchanged to the output. The second input is used as a \"reference\" video for computing the PSNR.\n\nBoth video inputs must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained average PSNR is printed through the logging system.\n\nThe filter stores the accumulated MSE (mean squared error) of each frame, and at the end of the processing it is averaged across all frames equally, and the following formula is applied to obtain the PSNR:\n\nWhere MAX is the average of the maximum values of each component of the image.\n\nThe description of the accepted parameters follows.\n\nThis filter also supports the framesync options.\n\nThe file printed if is selected, contains a sequence of key/value pairs of the form : for each compared couple of frames.\n\nIf a greater than 1 is specified, a header line precedes the list of per-frame-pair stats, with key value pairs following the frame format with the following parameters:\n\nA description of each shown per-frame-pair parameter follows:\n• For example: On this example the input file being processed is compared with the reference file . The PSNR of each individual frame is stored in .\n• Another example with different containers:\n\nThe pullup filter is designed to take advantage of future context in making its decisions. This filter is stateless in the sense that it does not lock onto a pattern to follow, but it instead looks forward to the following fields in order to identify matches and rebuild progressive frames.\n\nTo produce content with an even framerate, insert the fps filter after pullup, use if the input frame rate is 29.97fps, for 30fps and the (rare) telecined 25fps input.\n\nThe filter accepts the following options:\n\nFor best results (without duplicated frames in the output file) it is necessary to change the output frame rate. For example, to inverse telecine NTSC input:\n\nThe filter accepts the following option:\n\nThe expression is evaluated through the eval API and can contain, among others, the following constants:\n\nGenerate a QR code using the libqrencode library (see https://fukuchi.org/works/qrencode/), and overlay it on top of the current frame.\n\nTo enable the compilation of this filter, you need to configure FFmpeg with .\n\nThe QR code is generated from the provided text or text pattern. The corresponding QR code is scaled and overlayed into the video output according to the specified options.\n\nIn case no text is specified, no QR code is overlaied.\n\nThis filter accepts the following options:\n\nThe expressions set by the options contain the following constants and functions.\n\nIf is set to , the text is printed verbatim.\n\nIf is set to (which is the default), the following expansion mechanism is used.\n\nThe backslash character ‘ ’, followed by any character, always expands to the second character.\n\nSequences of the form are expanded. The text between the braces is a function name, possibly followed by arguments separated by ’:’. If the arguments contain special characters or delimiters (’:’ or ’}’), they should be escaped.\n\nNote that they probably must also be escaped as the value for the option in the filter argument string and as the filter argument in the filtergraph description, and possibly also for the shell, that makes up to four levels of escaping; using a text file with the option avoids these problems.\n\nThe following functions are available:\n• Generate a QR code encoding the specified text with the default size, overalaid in the top left corner of the input video, with the default size:\n• Same as below, but select blue on pink colors:\n• Place the QR code in the bottom right corner of the input video:\n• Generate a QR code with width of 200 pixels and padding, making the padded width 4/3 of the QR code width:\n• Generate a QR code with padded width of 200 pixels and padding, making the QR code width 3/4 of the padded width:\n• Make the QR code a fraction of the input video width:\n\nIdentify and decode a QR code using the libquirc library (see https://github.com/dlbeer/quirc/), and print the identified QR codes positions and payload as metadata.\n\nTo enable the compilation of this filter, you need to configure FFmpeg with .\n\nFor each found QR code in the input video, some metadata entries are added with the prefix , where is the index, starting from 0, associated to the QR code.\n\nA description of each metadata value follows:\n\nFlush video frames from internal cache of frames into a random order. No frame is discarded. Inspired by frei0r nervous filter.\n\nRead closed captioning (EIA-608) information from the top lines of a video frame.\n\nThis filter adds frame metadata for and , where is the number of the identified line with EIA-608 data (starting from 0). A description of each metadata value follows:\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n• Output a csv with presentation time and the first two lines of identified EIA-608 captioning data.\n\nRead vertical interval timecode (VITC) information from the top lines of a video frame.\n\nThe filter adds frame metadata key with the timecode value, if a valid timecode has been detected. Further metadata key is set to 0/1 depending on whether timecode data has been found or not.\n\nThis filter accepts the following options:\n• Detect and draw VITC data onto the video frame; if no valid VITC is detected, draw as a placeholder:\n\nDestination pixel at position (X, Y) will be picked from source (x, y) position where x = Xmap(X, Y) and y = Ymap(X, Y). If mapping values are out of range, zero value for pixel will be used for destination pixel.\n\nXmap and Ymap input video streams must be of same dimensions. Output video stream will have Xmap/Ymap video stream dimensions. Xmap and Ymap input video streams are 16bit depth, single channel.\n\nThe removegrain filter is a spatial denoiser for progressive video.\n\nRange of mode is from 0 to 24. Description of each mode follows:\n\nSuppress a TV station logo, using an image file to determine which pixels comprise the logo. It works by filling in the pixels that comprise the logo with neighboring pixels.\n\nThe filter accepts the following options:\n\nPixels in the provided bitmap image with a value of zero are not considered part of the logo, non-zero pixels are considered part of the logo. If you use white (255) for the logo and black (0) for the rest, you will be safe. For making the filter bitmap, it is recommended to take a screen capture of a black frame with the logo visible, and then using a threshold filter followed by the erode filter once or twice.\n\nIf needed, little splotches can be fixed manually. Remember that if logo pixels are not covered, the filter quality will be much reduced. Marking too many pixels as part of the logo does not hurt as much, but it will increase the amount of blurring needed to cover over the image and will destroy more information than necessary, and extra pixels will slow things down on a large logo.\n\nThis filter uses the repeat_field flag from the Video ES headers and hard repeats fields based on its value.\n\nWarning: This filter requires memory to buffer the entire clip, so trimming is suggested.\n• Take the first 5 seconds of a clip, and reverse it.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nRotate video by an arbitrary angle expressed in radians.\n\nThe filter accepts the following options:\n\nA description of the optional parameters follows.\n\nThe expressions for the angle and the output size can contain the following constants and functions:\n• Apply a constant rotation with period T, starting from an angle of PI/3:\n• Make the input video rotation oscillating with a period of T seconds and an amplitude of A radians:\n• Rotate the video, output size is chosen so that the whole rotating input video is always completely contained in the output:\n• Rotate the video, reduce the output size so that no background is ever shown:\n\nThe filter supports the following commands:\n\nThe filter accepts the following options:\n\nEach chroma option value, if not explicitly specified, is set to the corresponding luma option value.\n\nScale (resize) the input video, using the libswscale library.\n\nThe scale filter forces the output display aspect ratio to be the same of the input, by changing the output sample aspect ratio.\n\nIf the input image format is different from the format requested by the next filter, the scale filter will convert the input to the requested format.\n\nThe filter accepts the following options, any of the options supported by the libswscale scaler, as well as any of the framesync options.\n\nSee (ffmpeg-scaler)the ffmpeg-scaler manual for the complete list of scaler options.\n\nThe values of the and options are expressions containing the following constants:\n• Scale the input video to a size of 200x100 This is equivalent to:\n• Specify a size abbreviation for the output size: which can also be written as:\n• The above is the same as:\n• Scale the input to 2x with forced interlaced scaling:\n• Increase the width, and set the height to the same size:\n• Increase the height, and set the width to 3/2 of the height:\n• Increase the size, making the size a multiple of the chroma subsample values:\n• Increase the width to a maximum of 500 pixels, keeping the same aspect ratio as the input:\n• Make pixels square using reset_sar, making sure the resulting resolution is even (required by some codecs):\n• Scale to target exactly, however reset SAR to 1:\n• Scale to even dimensions that fit within 400x300, preserving input SAR:\n• Scale to produce square pixels with even dimensions that fit within 400x300:\n• Scale a subtitle stream (sub) to match the main video (main) in size before overlaying. (\"scale2ref\")\n• Scale a logo to 1/10th the height of a video, while preserving its display aspect ratio.\n\nThis filter supports the following commands:\n\nScale and convert the color parameters using VTPixelTransferSession.\n\nThe filter accepts the following options:\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThis filter sets frame metadata with mafd between frame, the scene score, and forward the frame to the next filter, so they can use these metadata to detect scene change or others.\n\nIn addition, this filter logs a message and sets frame metadata when it detects a scene change by .\n\nmetadata keys are set with mafd for every frame.\n\nmetadata keys are set with scene change score for every frame to detect scene change.\n\nmetadata keys are set with current filtered frame time which detect scene change with .\n\nThe filter accepts the following options:\n\nAdjust cyan, magenta, yellow and black (CMYK) to certain ranges of colors (such as \"reds\", \"yellows\", \"greens\", \"cyans\", ...). The adjustment range is defined by the \"purity\" of the color (that is, how saturated it already is).\n\nThis filter is similar to the Adobe Photoshop Selective Color tool.\n\nThe filter accepts the following options:\n\nAll the adjustment settings ( , , ...) accept up to 4 space separated floating point adjustment values in the [-1,1] range, respectively to adjust the amount of cyan, magenta, yellow and black for the pixels of its range.\n• Increase cyan by 50% and reduce yellow by 33% in every green areas, and increase magenta by 27% in blue areas:\n\nThe takes a frame-based video input and splits each frame into its components fields, producing a new half height clip with twice the frame rate and twice the frame count.\n\nThis filter use field-dominance information in frame to decide which of each pair of fields to place first in the output. If it gets it wrong use setfield filter before filter.\n\nThe filter sets the Display Aspect Ratio for the filter output video.\n\nThis is done by changing the specified Sample (aka Pixel) Aspect Ratio, according to the following equation:\n\nKeep in mind that the filter does not modify the pixel dimensions of the video frame. Also, the display aspect ratio set by this filter may be changed by later filters in the filterchain, e.g. in case of scaling or if another \"setdar\" or a \"setsar\" filter is applied.\n\nThe filter sets the Sample (aka Pixel) Aspect Ratio for the filter output video.\n\nNote that as a consequence of the application of this filter, the output display aspect ratio will change according to the equation above.\n\nKeep in mind that the sample aspect ratio set by the filter may be changed by later filters in the filterchain, e.g. if another \"setsar\" or a \"setdar\" filter is applied.\n\nIt accepts the following parameters:\n\nThe parameter is an expression containing the following constants:\n• To change the display aspect ratio to 16:9, specify one of the following:\n• To change the sample aspect ratio to 10:11, specify:\n• To set a display aspect ratio of 16:9, and specify a maximum integer value of 1000 in the aspect ratio reduction, use the command:\n\nThe filter marks the interlace type field for the output frames. It does not change the input frame, but only sets the corresponding property, which affects how the frame is treated by following filters (e.g. or ).\n\nThe filter accepts the following options:\n\nThe filter marks interlace and color range for the output frames. It does not change the input frame, but only sets the corresponding property, which affects how the frame is treated by filters/encoders.\n\nThis filter supports the following options:\n\nThis filter supports the all above options as commands.\n\nShow a line containing various information for each input video frame. The input video is not modified.\n\nThis filter supports the following options:\n\nThe shown line contains a sequence of key/value pairs of the form : .\n\nThe following values are shown in the output:\n\nDisplays the 256 colors palette of each frame. This filter is only relevant for pixel format frames.\n\nIt accepts the following option:\n\nIt accepts the following parameters:\n\nThe first frame has the index 0. The default is to keep the input unchanged.\n• Swap second and third frame of every three frames of the input:\n• Swap 10th and 1st frame of every ten frames of the input:\n\nThis filter accepts the following options:\n\nIt accepts the following parameters:\n\nThe first plane has the index 0. The default is to keep the input unchanged.\n• Swap the second and third planes of the input:\n\nEvaluate various visual metrics that assist in determining issues associated with the digitization of analog video media.\n\nBy default the filter will log these metadata values:\n\nThe filter accepts the following options:\n• Output specific data about the minimum and maximum values of the Y plane per frame:\n• Playback video while highlighting pixels that are outside of broadcast range in red.\n• Playback video with signalstats metadata drawn over the frame. The contents of signalstat_drawtext.txt used in the command are:\n\nCalculates the MPEG-7 Video Signature. The filter can handle more than one input. In this case the matching between the inputs can be calculated additionally. The filter always passes through the first input. The signature of each stream can be written into a file.\n\nIt accepts the following options:\n• To calculate the signature of an input video and store it in signature.bin:\n• To detect whether two videos match and store the signatures in XML format in signature0.xml and signature1.xml:\n\nCalculate Spatial Information (SI) and Temporal Information (TI) scores for a video, as defined in ITU-T Rec. P.910 (11/21): Subjective video quality assessment methods for multimedia applications. Available PDF at https://www.itu.int/rec/T-REC-P.910-202111-S/en. Note that this is a legacy implementation that corresponds to a superseded recommendation. Refer to ITU-T Rec. P.910 (07/22) for the latest version: https://www.itu.int/rec/T-REC-P.910-202207-I/en\n\nIt accepts the following option:\n\nBlur the input video without impacting the outlines.\n\nIt accepts the following options:\n\nIf a chroma or alpha option is not explicitly set, the corresponding luma value is set.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nApply a simple postprocessing filter that compresses and decompresses the image at several (or - in the case of level - all) shifts and average the results.\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nScale the input by applying one of the super-resolution methods based on convolutional neural networks. Supported models:\n\nTraining scripts as well as scripts for model file (.pb) saving can be found at https://github.com/XueweiMeng/sr/tree/sr_dnn_native. Original repository is at https://github.com/HighVoltageRocknRoll/sr.git.\n\nThe filter accepts the following options:\n\nTo get full functionality (such as async execution), please use the dnn_processing filter.\n\nUpscale (size increasing) for the input video using AMD Advanced Media Framework library for hardware acceleration. Use advanced algorithms for upscaling with higher output quality. Setting the output width and height works in the same way as for the scale filter.\n\nThe filter accepts the following options:\n• Scale input to 720p, keeping aspect ratio and ensuring the output is yuv420p.\n\nObtain the SSIM (Structural SImilarity Metric) between two input videos.\n\nThis filter takes in input two input videos, the first input is considered the \"main\" source and is passed unchanged to the output. The second input is used as a \"reference\" video for computing the SSIM.\n\nBoth video inputs must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe filter stores the calculated SSIM of each frame.\n\nThe description of the accepted parameters follows.\n\nThe file printed if is selected, contains a sequence of key/value pairs of the form : for each compared couple of frames.\n\nA description of each shown parameter follows:\n\nThis filter also supports the framesync options.\n• For example: On this example the input file being processed is compared with the reference file . The SSIM of each individual frame is stored in .\n• Another example with both psnr and ssim at same time:\n• Another example with different containers:\n\nThe filters accept the following options:\n• Convert input video from side by side parallel to anaglyph yellow/blue dubois:\n• Convert input video from above below (left eye above, right eye below) to side by side crosseye.\n\nThe filter accepts the following options:\n\nThe and filter supports the following commands:\n• Select first 5 seconds 1st stream and rest of time 2nd stream:\n• Same as above, but for audio:\n\nDraw subtitles on top of input video using the libass library.\n\nTo enable compilation of this filter you need to configure FFmpeg with . This filter also requires a build with libavcodec and libavformat to convert the passed subtitles file to ASS (Advanced Substation Alpha) subtitles format.\n\nThe filter accepts the following options:\n\nIf the first key is not specified, it is assumed that the first value specifies the .\n\nFor example, to render the file on top of the input video, use the command:\n\nwhich is equivalent to:\n\nTo render the default subtitles stream from file , use:\n\nTo render the second subtitles stream from that file, use:\n\nTo make the subtitles stream from appear in 80% transparent blue , use:\n\nScale the input by 2x and smooth using the Super2xSaI (Scale and Interpolate) pixel art scaling algorithm.\n\nUseful for enlarging pixel art images without reducing sharpness.\n\nThis filter accepts the following options:\n\nThe all options are expressions containing the following constants:\n\nThis filter supports the all above options as commands.\n\nThis filter accepts the following options:\n\nCompute and draw a color distribution histogram for the input video across time.\n\nUnlike histogram video filter which only shows histogram of single input frame at certain time, this filter shows also past histograms of number of frames defined by option.\n\nThe computed histogram is a representation of the color component distribution in an image.\n\nThe filter accepts the following options:\n\nThis filter needs four video streams to perform thresholding. First stream is stream we are filtering. Second stream is holding threshold values, third stream is holding min values, and last, fourth stream is holding max values.\n\nThe filter accepts the following option:\n\nFor example if first stream pixel’s component value is less then threshold value of pixel component from 2nd threshold stream, third stream value will picked, otherwise fourth stream pixel component value will be picked.\n\nUsing color source filter one can perform various types of thresholding:\n\nThis filter supports the all options as commands.\n• Threshold to zero, using gray color as threshold:\n• Inverted threshold to zero, using gray color as threshold:\n\nSelect the most representative frame in a given sequence of consecutive frames.\n\nThe filter accepts the following options:\n\nSince the filter keeps track of the whole frames sequence, a bigger value will result in a higher memory usage, so a high value is not recommended.\n• Complete example of a thumbnail creation with :\n\nThe untile filter can do the reverse.\n\nThe filter accepts the following options:\n• Produce 8x8 PNG tiles of all keyframes ( ) in a movie: The is necessary to prevent from duplicating each output frame to accommodate the originally detected frame rate.\n• Display pictures in an area of frames, with pixels between them, and pixels of initial margin, using mixed flat and named options:\n\nWhat happens when you invert time and space?\n\nNormally a video is composed of several frames that represent a different instant of time and shows a scene that evolves in the space captured by the frame. This filter is the antipode of that concept, taking inspiration from tilt and shift photography.\n\nA filtered frame contains the whole timeline of events composing the sequence, and this is obtained by placing a slice of pixels from each frame into a single one. However, since there are no infinite-width frames, this is done up the width of the input frame, and a video is recomposed by shifting away one column for each subsequent frame. In order to map space to time, the filter tilts each input frame as well, so that motion is preserved. This is accomplished by progressively selecting a different column from each input frame.\n\nThe end result is a sort of inverted parallax, so that far away objects move much faster that the ones in the front. The ideal conditions for this video effect are when there is either very little motion and the backgroud is static, or when there is a lot of motion and a very wide depth of field (e.g. wide panorama, while moving on a train).\n\nThe filter accepts the following parameters:\n\nNormally the filter shifts and tilts from the very first frame, and stops when the last one is received. However, before filtering starts, normal video may be preseved, so that the effect is slowly shifted in its place. Similarly, the last video frame may be reconstructed at the end. Alternatively it is possible to just start and end with black.\n\nFrames are counted starting from 1, so the first input frame is considered odd.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter supports all above options as commands, excluding option .\n\nMidway Video Equalization adjusts a sequence of video frames to have the same histograms, while maintaining their dynamics as much as possible. It’s useful for e.g. matching exposures from a video frames sequence.\n\nThis filter accepts the following option:\n\nA description of the accepted options follows.\n• Similar as above but only showing temporal differences:\n\nThis filter supports the following commands:\n\nThis filter expects data in single precision floating point, as it needs to operate on (and can output) out-of-range values. Another filter, such as zscale, is needed to convert the resulting frame to a usable format.\n\nThe tonemapping algorithms implemented only work on linear light, so input data should be linearized beforehand (and possibly correctly tagged).\n\nThe filter accepts the following options.\n\nThe filter accepts the following options:\n\nTranspose rows with columns in the input video and optionally flip it.\n\nIt accepts the following parameters:\n\nFor example to rotate by 90 degrees clockwise and preserve portrait layout:\n\nThe command above can also be specified as:\n\nTrim the input so that the output contains one continuous subpart of the input.\n\nIt accepts the following parameters:\n\n, , and are expressed as time duration specifications; see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual for the accepted syntax.\n\nNote that the first two sets of the start/end options and the option look at the frame timestamp, while the _frame variants simply count the frames that pass through the filter. Also note that this filter does not modify the timestamps. If you wish for the output timestamps to start at zero, insert a setpts filter after the trim filter.\n\nIf multiple start or end options are set, this filter tries to be greedy and keep all the frames that match at least one of the specified constraints. To keep only the part that matches all the constraints at once, chain multiple trim filters.\n\nThe defaults are such that all the input is kept. So it is possible to set e.g. just the end values to keep everything before the specified time.\n• Drop everything except the second minute of input:\n• Keep only the first second:\n\nApply alpha unpremultiply effect to input video stream using first plane of second stream as alpha.\n\nBoth streams must have same dimensions and same pixel format.\n\nThe filter accepts the following option:\n\nIt accepts the following parameters:\n\nAll parameters are optional and default to the equivalent of the string ’5:5:1.0:5:5:0.0’.\n• Apply a strong blur of both luma and chroma parameters:\n\nDecompose a video made of tiled images into the individual images.\n\nThe frame rate of the output video is the frame rate of the input video multiplied by the number of tiles.\n\nThis filter does the reverse of tile.\n\nThe filter accepts the following options:\n• Produce a 1-second video from a still image file made of 25 frames stacked vertically, like an analogic film reel:\n\nApply ultra slow/simple postprocessing filter that compresses and decompresses the image at several (or - in the case of level - all) shifts and average the results.\n\nThe way this differs from the behavior of spp is that uspp actually encodes & decodes each case with libavcodec Snow, whereas spp uses a simplified intra only 8x8 DCT similar to MJPEG.\n\nThis filter is not available in ffmpeg versions between 5.0 and 6.0.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n• Convert equirectangular video to cubemap with 3x2 layout and 1% padding using bicubic interpolation:\n• Convert transposed and horizontally flipped Equi-Angular Cubemap in side-by-side stereo format to equirectangular top-bottom stereo format:\n\nThis filter supports subset of above options as commands.\n\nIt transforms each frame from the video input into the wavelet domain, using Cohen-Daubechies-Feauveau 9/7. Then it applies some filtering to the obtained coefficients. It does an inverse wavelet transform after. Due to wavelet properties, it should give a nice smoothed result, and reduced noise, without blurring picture features.\n\nThis filter accepts the following options:\n\nApply variable blur filter by using 2nd video stream to set blur radius. The 2nd stream must have the same dimensions.\n\nThis filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nThis filter supports all the above options as commands.\n\nDisplay 2 color component values in the two dimensional graph (which is called a vectorscope).\n\nThis filter accepts the following options:\n\nAnalyze video stabilization/deshaking. Perform pass 1 of 2, see vidstabtransform for pass 2.\n\nThis filter generates a file with relative translation and rotation transform information about subsequent frames, which is then used by the vidstabtransform filter.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThis filter accepts the following options:\n• Analyze strongly shaky movie and put the results in file :\n• Visualize the result of internal transformations in the resulting video:\n\nVideo stabilization/deshaking: pass 2 of 2, see vidstabdetect for pass 1.\n\nRead a file with transform information for each frame and apply/compensate them. Together with the vidstabdetect filter this can be used to deshake videos. See also http://public.hronopik.de/vid.stab. It is important to also use the unsharp filter, see below.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n• Use for a typical stabilization with default values: Note the use of the unsharp filter which is always recommended.\n• Zoom in a bit more and load transform data from a given file:\n• Smoothen the video even more:\n\nFor example, to vertically flip a video with :\n\nThis filter tries to detect if the input is variable or constant frame rate.\n\nAt end it will output number of frames detected as having variable delta pts, and ones with constant delta pts. If there was frames with variable delta, than it will also show min, max and average delta encountered.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nObtain the average VIF (Visual Information Fidelity) between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained average VIF score is printed through the logging system.\n\nThe filter stores the calculated VIF score of each frame.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nThe filter accepts the following options:\n\nThe , and expressions can contain the following parameters.\n\nObtain the average VMAF motion score of a video. It is one of the component metrics of VMAF.\n\nThe obtained average motion score is printed through the logging system.\n\nThe filter accepts the following options:\n\nScale (resize) and convert colorspace, transfer characteristics or color primaries for the input video, using AMD Advanced Media Framework library for hardware acceleration. Setting the output width and height works in the same way as for the scale filter.\n\nThe filter accepts the following options:\n• Scale input to 720p, keeping aspect ratio and ensuring the output is yuv420p.\n• Upscale to 4K and change color profile to bt2020.\n\nAll streams must be of same pixel format and of same width.\n\nNote that this filter is faster than using overlay and pad filter to create same output.\n\nThe filter accepts the following options:\n\nBased on the process described by Martin Weston for BBC R&D, and implemented based on the de-interlace algorithm written by Jim Easterbrook for BBC R&D, the Weston 3 field deinterlacing filter uses filter coefficients calculated by BBC R&D.\n\nThis filter uses field-dominance information in frame to decide which of each pair of fields to place first in the output. If it gets it wrong use setfield filter before filter.\n\nThere are two sets of filter coefficients, so called \"simple\" and \"complex\". Which set of filter coefficients is used can be set by passing an optional parameter:\n\nThis filter supports same commands as options.\n\nThe waveform monitor plots color component intensity. By default luma only. Each column of the waveform corresponds to a column of pixels in the source video.\n\nIt accepts the following options:\n\nThe takes a field-based video input and join each two sequential fields into single frame, producing a new double height clip with half the frame rate and half the frame count.\n\nThe works same as but without halving frame rate and frame count.\n\nIt accepts the following option:\n\nApply the xBR high-quality magnification filter which is designed for pixel art. It follows a set of edge-detection rules, see https://forums.libretro.com/t/xbr-algorithm-tutorial/123.\n\nIt accepts the following option:\n\nApply normalized cross-correlation between first and second input video stream.\n\nSecond input video stream dimensions must be lower than first input video stream.\n\nThe filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nApply cross fade from one input video stream to another input video stream. The cross fade is applied for specified duration.\n\nBoth inputs must be constant frame-rate and have the same resolution, pixel format, frame rate and timebase.\n\nThe filter accepts the following options:\n• Cross fade from one input video to another input video, with fade transition and duration of transition of 2 seconds starting at offset of 5 seconds:\n\nThe filter accepts the following options:\n\nThis filter supports all above options as commands, excluding option .\n\nObtain the average (across all input frames) and minimum (across all color plane averages) eXtended Perceptually weighted peak Signal-to-Noise Ratio (XPSNR) between two input videos.\n\nThe XPSNR is a low-complexity psychovisually motivated distortion measurement algorithm for assessing the difference between two video streams or images. This is especially useful for objectively quantifying the distortions caused by video and image codecs, as an alternative to a formal subjective test. The logarithmic XPSNR output values are in a similar range as those of traditional psnr assessments but better reflect human impressions of visual coding quality. More details on the XPSNR measure, which essentially represents a blockwise weighted variant of the PSNR measure, can be found in the following freely available papers:\n• C. R. Helmrich, M. Siekmann, S. Becker, S. Bosse, D. Marpe, and T. Wiegand, \"XPSNR: A Low-Complexity Extension of the Perceptually Weighted Peak Signal-to-Noise Ratio for High-Resolution Video Quality Assessment,\" in Proc. IEEE Int. Conf. Acoustics, Speech, Sig. Process. (ICASSP), virt./online, May 2020. www.ecodis.de/xpsnr.htm\n• C. R. Helmrich, S. Bosse, H. Schwarz, D. Marpe, and T. Wiegand, \"A Study of the Extended Perceptually Weighted Peak Signal-to-Noise Ratio (XPSNR) for Video Compression with Different Resolutions and Bit Depths,\" ITU Journal: ICT Discoveries, vol. 3, no. 1, pp. 65 - 72, May 2020. http://handle.itu.int/11.1002/pub/8153d78b-en\n\nWhen publishing the results of XPSNR assessments obtained using, e.g., this FFmpeg filter, a reference to the above papers as a means of documentation is strongly encouraged. The filter requires two input videos. The first input is considered a (usually not distorted) reference source and is passed unchanged to the output, whereas the second input is a (distorted) test signal. Except for the bit depth, these two video inputs must have the same pixel format. In addition, for best performance, both compared input videos should be in YCbCr color format.\n\nThe obtained overall XPSNR values mentioned above are printed through the logging system. In case of input with multiple color planes, we suggest reporting of the minimum XPSNR average.\n\nThe following parameter, which behaves like the one for the psnr filter, is accepted:\n\nThis filter also supports the framesync options.\n• XPSNR analysis of two 1080p HD videos, ref_source.yuv and test_video.yuv, both at 24 frames per second, with color format 4:2:0, bit depth 8, and output of a logfile named \"xpsnr.log\":\n• XPSNR analysis of two 2160p UHD videos, ref_source.yuv with bit depth 8 and test_video.yuv with bit depth 10, both at 60 frames per second with color format 4:2:0, no logfile output:\n\nAll streams must be of same pixel format.\n\nThe filter accepts the following options:\n• Display 4 inputs into 2x2 grid. Note that if inputs are of different sizes, gaps or overlaps may occur.\n• Display 4 inputs into 1x4 grid. Note that if inputs are of different widths, unused space will appear.\n• Display 9 inputs into 3x3 grid. Note that if inputs are of different sizes, gaps or overlaps may occur.\n• Display 16 inputs into 4x4 grid. Note that if inputs are of different sizes, gaps or overlaps may occur.\n\nDeinterlace the input video (\"yadif\" means \"yet another deinterlacing filter\").\n\nIt accepts the following parameters:\n\nApply blur filter while preserving edges (\"yaepblur\" means \"yet another edge preserving blur filter\"). The algorithm is described in \"J. S. Lee, Digital image enhancement and noise filtering by use of local statistics, IEEE Trans. Pattern Anal. Mach. Intell. PAMI-2, 1980.\"\n\nIt accepts the following parameters:\n\nThis filter supports same commands as options.\n\nThis filter accepts the following options:\n\nEach expression can contain the following constants:\n• Zoom in up to 1.5x and pan at same time to some spot near center of picture:\n• Zoom in up to 1.5x and pan always at center of picture:\n• Same as above but without pausing:\n• Zoom in 2x into center of picture only for the first second of the input video:\n\nScale (resize) the input video, using the z.lib library: https://github.com/sekrit-twc/zimg. To enable compilation of this filter, you need to configure FFmpeg with .\n\nThe zscale filter forces the output display aspect ratio to be the same as the input, by changing the output sample aspect ratio.\n\nIf the input image format is different from the format requested by the next filter, the zscale filter will convert the input to the requested format.\n\nThe filter accepts the following options.\n\nThe values of the and options are expressions containing the following constants:\n\nThis filter supports the following commands:\n\nTo enable CUDA and/or NPP filters please refer to configuration guidelines for CUDA and for CUDA NPP filters.\n\nRunning CUDA filters requires you to initialize a hardware device and to pass that device to all filters in any filter graph.\n\nFor more detailed information see https://www.ffmpeg.org/ffmpeg.html#Advanced-Video-options\n• Example of initializing second CUDA device on the system and running scale_cuda and bilateral_cuda filters.\n\nSince CUDA filters operate exclusively on GPU memory, frame data must sometimes be uploaded (hwupload) to hardware surfaces associated with the appropriate CUDA device before processing, and downloaded (hwdownload) back to normal memory afterward, if required. Whether hwupload or hwdownload is necessary depends on the specific workflow:\n• If the input frames are already in GPU memory (e.g., when using or ), explicit use of hwupload is not needed, as the data is already in the appropriate memory space.\n• If the input frames are in CPU memory (e.g., software-decoded frames or frames processed by CPU-based filters), it is necessary to use hwupload to transfer the data to GPU memory for CUDA processing.\n• If the output of the CUDA filters needs to be further processed by software-based filters or saved in a format not supported by GPU-based encoders, hwdownload is required to transfer the data back to CPU memory.\n\nNote that hwupload uploads data to a surface with the same layout as the software frame, so it may be necessary to add a format filter immediately before hwupload to ensure the input is in the correct format. Similarly, hwdownload may not support all output formats, so an additional format filter may need to be inserted immediately after hwdownload in the filter graph to ensure compatibility.\n\nBelow is a description of the currently available Nvidia CUDA video filters.\n\nNote: If FFmpeg detects the Nvidia CUDA Toolkit during configuration, it will enable CUDA filters automatically without requiring any additional flags. If you want to explicitly enable them, use the following options:\n• Configure FFmpeg with . Additional requirement: lib must be installed.\n\nCUDA accelerated bilateral filter, an edge preserving filter. This filter is mathematically accurate thanks to the use of GPU acceleration. For best output quality, use one to one chroma subsampling, i.e. yuv444p format.\n\nThe filter accepts the following options:\n\nDeinterlace the input video using the bwdif algorithm, but implemented in CUDA so that it can work as part of a GPU accelerated pipeline with nvdec and/or nvenc.\n\nIt accepts the following parameters:\n\nThis filter works like normal chromakey filter but operates on CUDA frames. for more details and parameters see chromakey.\n• Make all the green pixels in the input video transparent and use it as an overlay for another video:\n\nIt is by no means feature complete compared to the software colorspace filter, and at the current time only supports color range conversion between jpeg/full and mpeg/limited range.\n\nThe filter accepts the following options:\n\nOverlay one video on top of another.\n\nThis is the CUDA variant of the overlay filter. It only accepts CUDA frames. The underlying input pixel formats have to match.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid.\n\nIt accepts the following parameters:\n\nThis filter also supports the framesync options.\n\nScale (resize) and convert (pixel format) the input video, using accelerated CUDA kernels. Setting the output width and height works in the same way as for the scale filter.\n\nThe filter accepts the following options:\n• Scale input to 720p, keeping aspect ratio and ensuring the output is yuv420p.\n• Don’t do any conversion or scaling, but copy all input frames into newly allocated ones. This can be useful to deal with a filter and encode chain that otherwise exhausts the decoders frame pool.\n\nDeinterlace the input video using the yadif algorithm, but implemented in CUDA so that it can work as part of a GPU accelerated pipeline with nvdec and/or nvenc.\n\nIt accepts the following parameters:\n\nBelow is a description of the currently available NVIDIA Performance Primitives (libnpp) video filters.\n\nUse the NVIDIA Performance Primitives (libnpp) to perform scaling and/or pixel format conversion on CUDA video frames. Setting the output width and height works in the same way as for the filter.\n\nThe following additional options are accepted:\n\nThe values of the and options are expressions containing the following constants:\n\nUse the NVIDIA Performance Primitives (libnpp) to scale (resize) the input video, based on a reference video.\n\nSee the scale_npp filter for available options, scale2ref_npp supports the same but uses the reference video instead of the main input as basis. scale2ref_npp also supports the following additional constants for the and options:\n• Scale a subtitle stream (b) to match the main video (a) in size before overlaying\n• Scale a logo to 1/10th the height of a video, while preserving its display aspect ratio.\n\nUse the NVIDIA Performance Primitives (libnpp) to perform image sharpening with border control.\n\nThe following additional options are accepted:\n\nTranspose rows with columns in the input video and optionally flip it. For more in depth examples see the transpose video filter, which shares mostly the same options.\n\nIt accepts the following parameters:\n\nBelow is a description of the currently available OpenCL video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with .\n\nRunning OpenCL filters requires you to initialize a hardware device and to pass that device to all filters in any filter graph.\n\nFor more detailed information see https://www.ffmpeg.org/ffmpeg.html#Advanced-Video-options\n• Example of choosing the first device on the second platform and running avgblur_opencl filter with default parameters on it.\n\nSince OpenCL filters are not able to access frame data in normal memory, all frame data needs to be uploaded(hwupload) to hardware surfaces connected to the appropriate device before being used and then downloaded(hwdownload) back to normal memory. Note that hwupload will upload to a surface with the same layout as the software frame, so it may be necessary to add a format filter immediately before to get the input into the right format and hwdownload does not support all formats on the output - it may be necessary to insert an additional format filter immediately following in the graph to get the output in a supported format.\n\nThe filter accepts the following options:\n• Apply average blur filter with horizontal and vertical size of 3, setting each pixel of the output to the average value of the 7x7 region centered on it in the input. For pixels on the edges of the image, the region does not extend beyond the image boundaries, and so out-of-range coordinates are not used in the calculations.\n\nIt accepts the following parameters:\n\nA description of the accepted options follows.\n\nApply boxblur filter, setting each pixel of the output to the average value of box-radiuses , , for each plane respectively. The filter will apply , , times onto the corresponding plane. For pixels on the edges of the image, the radius does not extend beyond the image boundaries, and so out-of-range coordinates are not used in the calculations.\n• Apply a boxblur filter with the luma, chroma, and alpha radius set to 2 and luma, chroma, and alpha power set to 3. The filter will run 3 times with box-radius set to 2 for every plane of the image.\n• Apply a boxblur filter with luma radius set to 2, luma_power to 1, chroma_radius to 4, chroma_power to 5, alpha_radius to 3 and alpha_power to 7. For the luma plane, a 2x2 box radius will be run once. For the chroma plane, a 4x4 box radius will be run 5 times. For the alpha plane, a 3x3 box radius will be run 7 times.\n\nThe filter accepts the following options:\n• Make every semi-green pixel in the input transparent with some slight blending:\n\nThe filter accepts the following options:\n\nThis filter replaces the pixel by the local(3x3) minimum.\n\nIt accepts the following options:\n• Apply erosion filter with threshold0 set to 30, threshold1 set 40, threshold2 set to 50 and coordinates set to 231, setting each pixel of the output to the local minimum between pixels: 1, 2, 3, 6, 7, 8 of the 3x3 region centered on it in the input. If the difference between input pixel and local minimum is more then threshold of the corresponding plane, output pixel will be set to input pixel - threshold of corresponding plane.\n\nThe filter accepts the following options:\n• Stabilize a video with debugging (both in console and in rendered video):\n\nThis filter replaces the pixel by the local(3x3) maximum.\n\nIt accepts the following options:\n• Apply dilation filter with threshold0 set to 30, threshold1 set 40, threshold2 set to 50 and coordinates set to 231, setting each pixel of the output to the local maximum between pixels: 1, 2, 3, 6, 7, 8 of the 3x3 region centered on it in the input. If the difference between input pixel and local maximum is more then threshold of the corresponding plane, output pixel will be set to input pixel + threshold of corresponding plane.\n\nNon-local Means denoise filter through OpenCL, this filter accepts same options as nlmeans.\n\nOverlay one video on top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid. This filter requires same memory layout for all the inputs. So, format conversion may be needed.\n\nThe filter accepts the following options:\n• Overlay an image LOGO at the top-left corner of the INPUT video. Both inputs are yuv420p format.\n• The inputs have same memory layout for color channels , the overlay has additional alpha plane, like INPUT is yuv420p, and the LOGO is yuva420p.\n\nAdd paddings to the input image, and place the original input at the provided , coordinates.\n\nIt accepts the following options:\n\nThe value for the , , , and options are expressions containing the following constants:\n\nThe filter accepts the following option:\n• Apply the Prewitt operator with scale set to 2 and delta set to 10.\n\nThe filter also supports the framesync options.\n\nThe program source file must contain a kernel function with the given name, which will be run once for each plane of the output. Each run on a plane gets enqueued as a separate 2D global NDRange with one work-item for each pixel to be generated. The global ID offset for each work-item is therefore the coordinates of a pixel in the destination image.\n\nThe kernel function needs to take the following arguments:\n• Destination image, . This image will become the output; the kernel should write all of it.\n• Frame index, . This is a counter starting from zero and increasing by one for each frame.\n• Source images, . These are the most recent images on each input. The kernel may read from them to generate the output, but they can’t be written to.\n• Copy the input to the output (output must be the same size as the input).\n• Apply a simple transformation, rotating the input by an amount increasing with the index counter. Pixel values are linearly interpolated by the sampler, and the output need not have the same dimensions as the input.\n• Blend two inputs together, with the amount of each input used varying with the index counter.\n\nDestination pixel at position (X, Y) will be picked from source (x, y) position where x = Xmap(X, Y) and y = Ymap(X, Y). If mapping values are out of range, zero value for pixel will be used for destination pixel.\n\nXmap and Ymap input video streams must be of same dimensions. Output video stream will have Xmap/Ymap video stream dimensions. Xmap and Ymap input video streams are 32bit float pixel format, single channel.\n\nThe filter accepts the following option:\n• Apply the Roberts cross operator with scale set to 2 and delta set to 10\n\nThe filter accepts the following option:\n• Apply sobel operator with scale set to 2 and delta set to 10\n\nIt accepts the following parameters:\n\nIt accepts the following parameters:\n\nAll parameters are optional and default to the equivalent of the string ’5:5:1.0:5:5:0.0’.\n• Apply a strong blur of both luma and chroma parameters:\n\nCross fade two videos with custom transition effect by using OpenCL.\n\nIt accepts the following options:\n\nThe program source file must contain a kernel function with the given name, which will be run once for each plane of the output. Each run on a plane gets enqueued as a separate 2D global NDRange with one work-item for each pixel to be generated. The global ID offset for each work-item is therefore the coordinates of a pixel in the destination image.\n\nThe kernel function needs to take the following arguments:\n• Destination image, . This image will become the output; the kernel should write all of it.\n• First Source image, . Second Source image, . These are the most recent images on each input. The kernel may read from them to generate the output, but they can’t be written to.\n• Transition progress, . This value is always between 0 and 1 inclusive.\n\nVAAPI Video filters are usually used with VAAPI decoder and VAAPI encoder. Below is a description of VAAPI video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with .\n\nTo use vaapi filters, you need to setup the vaapi device correctly. For more information, please read https://trac.ffmpeg.org/wiki/Hardware/VAAPI\n\nOverlay one video on the top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid.\n\nThe filter accepts the following options:\n\nThis filter also supports the framesync options.\n• Overlay an image LOGO at the top-left corner of the INPUT video. Both inputs for this filter are yuv420p format.\n• Overlay an image LOGO at the offset (200, 100) from the top-left corner of the INPUT video. The inputs have same memory layout for color channels, the overlay has additional alpha plane, like INPUT is yuv420p, and the LOGO is yuva420p.\n\nPerform HDR-to-SDR or HDR-to-HDR tone-mapping. It currently only accepts HDR10 as input.\n\nIt accepts the following parameters:\n\nThis is the VA-API variant of the hstack filter, each input stream may have different height, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the VA-API variant of the vstack filter, each input stream may have different width, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the VA-API variant of the xstack filter, each input stream may have different size, this filter will scale down/up each input stream to the given output size, or the size of the first input stream.\n\nIt accepts the following options:\n\nAdd paddings to the input image, and place the original input at the provided , coordinates.\n\nIt accepts the following options:\n\nThe value for the , , , and options are expressions containing the following constants:\n\nIt accepts the following parameters:\n\nThe parameters for , , and and are expressions containing the following constants:\n• Draw a black box around the edge of the input image:\n• Draw a box with color red and an opacity of 50%: The previous example can be specified as:\n\nBelow is a description of the currently available Vulkan video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with and either or .\n\nRunning Vulkan filters requires you to initialize a hardware device and to pass that device to all filters in any filter graph.\n\nFor more detailed information see https://www.ffmpeg.org/ffmpeg.html#Advanced-Video-options\n• Example of choosing the first device and running nlmeans_vulkan filter with default parameters on it.\n\nAs Vulkan filters are not able to access frame data in normal memory, all frame data needs to be uploaded (hwupload) to hardware surfaces connected to the appropriate device before being used and then downloaded (hwdownload) back to normal memory. Note that hwupload will upload to a frame with the same layout as the software frame, so it may be necessary to add a format filter immediately before to get the input into the right format and hwdownload does not support all formats on the output - it is usually necessary to insert an additional format filter immediately following in the graph to get the output in a supported format.\n\nApply an average blur filter, implemented on the GPU using Vulkan.\n\nThe filter accepts the following options:\n\nBlend two Vulkan frames into each other.\n\nThe filter takes two input streams and outputs one stream, the first input is the \"top\" layer and second input is \"bottom\" layer. By default, the output terminates when the longest input terminates.\n\nA description of the accepted options follows.\n\nDeinterlacer using bwdif, the \"Bob Weaver Deinterlacing Filter\" algorithm, implemented on the GPU using Vulkan.\n\nIt accepts the following parameters:\n\nApply an effect that emulates chromatic aberration. Works best with RGB inputs, but provides a similar effect with YCbCr inputs too.\n\nVideo source that creates a Vulkan frame of a solid color. Useful for benchmarking, or overlaying.\n\nIt accepts the following parameters:\n\nFlips an image along both the vertical and horizontal axis.\n\nThe filter accepts the following options:\n\nDenoise frames using Non-Local Means algorithm, implemented on the GPU using Vulkan. Supports more pixel formats than nlmeans or nlmeans_opencl, including alpha channel support.\n\nThe filter accepts the following options.\n\nOverlay one video on top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid. This filter requires all inputs to use the same pixel format. So, format conversion may be needed.\n\nThe filter accepts the following options:\n\nTranspose rows with columns in the input video and optionally flip it. For more in depth examples see the transpose video filter, which shares mostly the same options.\n\nIt accepts the following parameters:\n\nTranspose rows with columns in the input video and optionally flip it. For more in depth examples see the transpose video filter, which shares mostly the same options.\n\nIt accepts the following parameters:\n\nBelow is a description of the currently available QSV video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with or .\n\nTo use QSV filters, you need to setup the QSV device correctly. For more information, please read https://trac.ffmpeg.org/wiki/Hardware/QuickSync\n\nThis is the QSV variant of the hstack filter, each input stream may have different height, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the QSV variant of the vstack filter, each input stream may have different width, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the QSV variant of the xstack filter.\n\nIt accepts the following options:\n\nBelow is a description of the currently available video sources.\n\nBuffer video frames, and make them available to the filter chain.\n\nThis source is mainly intended for a programmatic use, in particular through the interface defined in .\n\nIt accepts the following parameters:\n\nwill instruct the source to accept video frames with size 320x240 and with format \"yuv410p\", assuming 1/24 as the timestamps timebase and square pixels (1:1 sample aspect ratio). Since the pixel format with name \"yuv410p\" corresponds to the number 6 (check the enum AVPixelFormat definition in ), this example corresponds to:\n\nAlternatively, the options can be specified as a flat string, but this syntax is deprecated:\n\nThe initial state of the cellular automaton can be defined through the and options. If such options are not specified an initial state is created randomly.\n\nAt each new frame a new row in the video is filled with the result of the cellular automaton next generation. The behavior when the whole frame is filled is defined by the option.\n\nThis source accepts the following options:\n• Read the initial state from , and specify an output of size 200x400.\n• Generate a random initial row with a width of 200 cells, with a fill ratio of 2/3:\n• Create a pattern generated by rule 18 starting by a single alive cell centered on an initial row with width 100:\n\nVideo source generated on GPU using Apple’s CoreImage API on OSX.\n\nThis video source is a specialized version of the coreimage video filter. Use a core image generator at the beginning of the applied filterchain to generate the content.\n\nThe coreimagesrc video source accepts the following options:\n\nAdditionally, all options of the coreimage video filter are accepted. A complete filterchain can be used for further processing of the generated input without CPU-HOST transfer. See coreimage documentation and examples for details.\n• Use CIQRCodeGenerator to create a QR code for the FFmpeg homepage, given as complete and escaped command-line for Apple’s standard bash shell: This example is equivalent to the QRCode example of coreimage without the need for a nullsrc video source.\n\nThe filter exclusively returns D3D11 Hardware Frames, for on-gpu encoding or processing. So an explicit hwdownload is needed for any kind of software processing.\n\nIt accepts the following options:\n\nYou can also skip the lavfi device and directly use the filter. Also demonstrates downloading the frame and encoding with libx264. Explicit output format specification is required in this case:\n\nIf you want to capture only a subsection of the desktop, this can be achieved by specifying a smaller size and its offsets into the screen:\n\nThis source supports the some above options as commands.\n\nGenerate a Mandelbrot set fractal, and progressively zoom towards the point specified with and .\n\nThis source accepts the following options:\n\nGenerate various test patterns, as generated by the MPlayer test filter.\n\nThe size of the generated video is fixed, and is 256x256. This source is useful in particular for testing encoding features.\n\nThis source accepts the following options:\n\nTo enable compilation of this filter you need to install the frei0r header and configure FFmpeg with .\n\nThis source accepts the following parameters:\n\nFor example, to generate a frei0r partik0l source with size 200x200 and frame rate 10 which is overlaid on the overlay filter main input:\n\nThis source is based on a generalization of John Conway’s life game.\n\nThe sourced input represents a life grid, each pixel represents a cell which can be in one of two possible states, alive or dead. Every cell interacts with its eight neighbours, which are the cells that are horizontally, vertically, or diagonally adjacent.\n\nAt each interaction the grid evolves according to the adopted rule, which specifies the number of neighbor alive cells which will make a cell stay alive or born. The option allows one to specify the rule to adopt.\n\nThis source accepts the following options:\n• Read a grid from , and center it on a grid of size 300x300 pixels:\n• Generate a random grid of size 200x200, with a fill ratio of 2/3:\n• Full example with slow death effect (mold) using :\n\nPerlin noise is a kind of noise with local continuity in space. This can be used to generate patterns with continuity in space and time, e.g. to simulate smoke, fluids, or terrain.\n\nIn case more than one octave is specified through the option, Perlin noise is generated as a sum of components, each one with doubled frequency. In this case the option specify the ratio of the amplitude with respect to the previous component. More octave components enable to specify more high frequency details in the generated noise (e.g. small size variations due to boulders in a generated terrain).\n• Use Perlin noise with 7 components, each one with a halved contribution to total amplitude:\n• Chain Perlin noise with the lutyuv to generate a black&white effect:\n• Stretch noise along the y axis, and convert gray level to red-only signal:\n\nGenerate a QR code using the libqrencode library (see https://fukuchi.org/works/qrencode/).\n\nTo enable the compilation of this source, you need to configure FFmpeg with .\n\nThe QR code is generated from the provided text or text pattern. The corresponding QR code is scaled and put in the video output according to the specified output size options.\n\nIn case no text is specified, the QR code is not generated, but an empty colored output is returned instead.\n\nThis source accepts the following options:\n• Generate a QR code encoding the specified text with the default size:\n• Same as below, but select blue on pink colors:\n• Generate a QR code with width of 200 pixels and padding, making the padded width 4/3 of the QR code width:\n• Generate a QR code with padded width of 200 pixels and padding, making the QR code width 3/4 of the padded width:\n\nThe source returns frames of size 4096x4096 of all rgb colors.\n\nThe source returns frames of size 4096x4096 of all yuv colors.\n\nThe source provides an uniformly colored input.\n\nThe source provides an identity Hald CLUT. See also haldclut filter.\n\nThe source returns unprocessed video frames. It is mainly useful to be employed in analysis / debugging tools, or as the source for filters which ignore the input data.\n\nThe source generates a color bars pattern, based on EBU PAL recommendations with 75% color levels.\n\nThe source generates a color bars pattern, based on EBU PAL recommendations with 100% color levels.\n\nThe source generates an RGB test pattern useful for detecting RGB vs BGR issues. You should see a red, green and blue stripe from top to bottom.\n\nThe source generates a color bars pattern, based on the SMPTE Engineering Guideline EG 1-1990.\n\nThe source generates a color bars pattern, based on the SMPTE RP 219-2002.\n\nThe source generates a test video pattern, showing a color pattern, a scrolling gradient and a timestamp. This is mainly intended for testing purposes.\n\nThe source is similar to testsrc, but supports more pixel formats instead of just . This allows using it as an input for other tests without requiring a format conversion.\n\nThe source generates an YUV test pattern. You should see a y, cb and cr stripe from top to bottom.\n\nThe sources accept the following parameters:\n• Generate a video with a duration of 5.3 seconds, with size 176x144 and a frame rate of 10 frames per second:\n• The following graph description will generate a red source with an opacity of 0.2, with size \"qcif\" and a frame rate of 10 frames per second:\n• If the input content is to be ignored, can be used. The following command generates noise in the luma plane by employing the filter:\n\nThe source supports the following commands:\n\nFor details of how the program loading works, see the program_opencl filter.\n• Generate a colour ramp by setting pixel values from the position of the pixel in the output image. (Note that this will work with all pixel formats, but the generated output will not be the same.)\n• Generate a Sierpinski carpet pattern, panning by a single pixel each frame. __kernel void sierpinski_carpet(__write_only image2d_t dst, unsigned int index) { int2 loc = (int2)(get_global_id(0), get_global_id(1)); float4 value = 0.0f; int x = loc.x + index; int y = loc.y + index; while (x > 0 || y > 0) { if (x % 3 == 1 && y % 3 == 1) { value = 1.0f; break; } x /= 3; y /= 3; } write_imagef(dst, loc, value); }\n\nThis source accepts the following options:\n\nThis source accepts the following options:\n\nThis source supports the some above options as commands.\n\nBelow is a description of the currently available video sinks.\n\nBuffer video frames, and make them available to the end of the filter graph.\n\nThis sink is mainly intended for programmatic use, in particular through the interface defined in or the options system.\n\nIt accepts a pointer to an AVBufferSinkContext structure, which defines the incoming buffers’ formats, to be passed as the opaque parameter to for initialization.\n\nNull video sink: do absolutely nothing with the input video. It is mainly useful as a template and for use in analysis / debugging tools.\n\nBelow is a description of the currently available multimedia filters.\n\nThe filter accepts the following options:\n\nFilter supports the some above options as commands.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nMeasures phase of input audio, which is exported as metadata , representing mean phase of current audio frame. A video output can also be produced and is enabled by default. The audio is passed through as first output.\n\nAudio will be rematrixed to stereo if it has a different channel layout. Phase value is in range where means left and right channels are completely out of phase and means channels are in phase.\n\nThe filter accepts the following options, all related to its video output:\n\nThe filter also detects out of phase and mono sequences in stereo streams. It logs the sequence start, end and duration when it lasts longer or as long as the minimum set.\n\nThe filter accepts the following options for this detection:\n• Complete example with to detect 1 second of mono with 0.001 phase tolerance:\n\nThe filter is used to measure the difference between channels of stereo audio stream. A monaural signal, consisting of identical left and right signal, results in straight vertical line. Any stereo separation is visible as a deviation from this line, creating a Lissajous figure. If the straight (or deviation from it) but horizontal line appears this indicates that the left and right channels are out of phase.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands except options and .\n\nThe filter accepts the following options:\n\nConcatenate audio and video streams, joining them together one after the other.\n\nThe filter works on segments of synchronized video and audio streams. All segments must have the same number of streams of each type, and that will also be the number of streams at output.\n\nThe filter accepts the following options:\n\nThe filter has + outputs: first video outputs, then audio outputs.\n\nThere are x( + ) inputs: first the inputs for the first segment, in the same order as the outputs, then the inputs for the second segment, etc.\n\nRelated streams do not always have exactly the same duration, for various reasons including codec frame size or sloppy authoring. For that reason, related synchronized streams (e.g. a video and its audio track) should be concatenated at once. The concat filter will use the duration of the longest stream in each segment (except the last one), and if necessary pad shorter audio streams with silence.\n\nFor this filter to work correctly, all segments must start at timestamp 0.\n\nAll corresponding streams must have the same parameters in all segments; the filtering system will automatically select a common pixel format for video streams, and a common sample format, sample rate and channel layout for audio streams, but other settings, such as resolution, must be converted explicitly by the user.\n\nDifferent frame rates are acceptable but will result in variable frame rate at output; be sure to configure the output file to handle it.\n• Concatenate an opening, an episode and an ending, all in bilingual version (video in stream 0, audio in streams 1 and 2):\n• Concatenate two parts, handling audio and video separately, using the (a)movie sources, and adjusting the resolution: Note that a desync will happen at the stitch if the audio and video streams do not have exactly the same duration in the first file.\n\nThis filter supports the following commands:\n\nEBU R128 scanner filter. This filter takes an audio stream and analyzes its loudness level. By default, it logs a message at a frequency of 10Hz with the Momentary loudness (identified by ), Short-term loudness ( ), Integrated loudness ( ) and Loudness Range ( ).\n\nThe filter can only analyze streams which have sample format is double-precision floating point. The input stream will be converted to this specification, if needed. Users may need to insert aformat and/or aresample filters after this filter to obtain the original parameters.\n\nThe filter also has a video output (see the option) with a real time graph to observe the loudness evolution. The graphic contains the logged message mentioned above, so it is not printed anymore when this option is set, unless the verbose logging is set. The main graphing area contains the short-term loudness (3 seconds of analysis), and the gauge on the right is for the momentary loudness (400 milliseconds), but can optionally be configured to instead display short-term loudness (see ).\n\nThe green area marks a +/- 1LU target range around the target loudness (-23LUFS by default, unless modified through ).\n\nMore information about the Loudness Recommendation EBU R128 on http://tech.ebu.ch/loudness.\n\nThe filter accepts the following options:\n\nThese filters read frames from several inputs and send the oldest queued frame to the output.\n\nInput streams must have well defined, monotonically increasing frame timestamp values.\n\nIn order to submit one frame to output, these filters need to enqueue at least one frame for each input, so they cannot work in case one input is not yet terminated and will not receive incoming frames.\n\nFor example consider the case when one input is a filter which always drops input frames. The filter will keep reading from that input, but it will never be able to send new frames to output until the input sends an end-of-stream signal.\n\nAlso, depending on inputs synchronization, the filters will drop frames in case one input receives more frames than the other ones, and the queue is already filled.\n\nThese filters accept the following options:\n• Interleave frames belonging to different streams using :\n\nReport previous filter filtering latency, delay in number of audio samples for audio filters or number of video frames for video filters.\n\nOn end of input stream, filter will report min and max measured latency for previous running filter in filtergraph.\n\nThis filter accepts the following options:\n• Print all metadata values for frames with key with values between 0 and 1.\n• Direct all metadata to a pipe with file descriptor 4.\n\nThese filters are mainly aimed at developers to test direct path in the following filter in the filtergraph.\n\nThe filters accept the following options:\n\nNote: in case of auto-inserted filter between the permission filter and the following one, the permission might not be received as expected in that following filter. Inserting a format or aformat filter before the perms/aperms filter can avoid this problem.\n\nThese filters will pause the filtering for a variable amount of time to match the output rate with the input timestamps. They are similar to the option to .\n\nThey accept the following options:\n\nBoth filters supports the all above options as commands.\n\nThis filter does opposite of concat filters.\n\nThis filter accepts the following options:\n\nIn all cases, prefixing an each segment with ’+’ will make it relative to the previous segment.\n• Split input audio stream into three output audio streams, starting at start of input audio stream and storing that in 1st output audio stream, then following at 60th second and storing than in 2nd output audio stream, and last after 150th second of input audio stream store in 3rd output audio stream:\n\nThis filter accepts the following options:\n\nThe expression can contain the following constants:\n\nThe default value of the select expression is \"1\".\n• Select all frames in input: The example above is the same as:\n• Select only frames contained in the 10-20 time interval:\n• Select only I-frames contained in the 10-20 time interval:\n• Use aselect to select only audio frames with samples number > 100:\n• Create a mosaic of the first scenes: Comparing against a value between 0.3 and 0.5 is generally a sane choice.\n• Send even and odd frames to separate outputs, and compose them:\n• Select useful frames from an ffconcat file which is using inpoints and outpoints but where the source files are not intra frame only.\n\nSend commands to filters in the filtergraph.\n\nThese filters read commands to be sent to other filters in the filtergraph.\n\nmust be inserted between two video filters, must be inserted between two audio filters, but apart from that they act the same way.\n\nThe specification of commands can be provided in the filter arguments with the option, or in a file specified by the option.\n\nThese filters accept the following options:\n\nA commands description consists of a sequence of interval specifications, comprising a list of commands to be executed when a particular event related to that interval occurs. The occurring event is typically the current frame time entering or leaving a given time interval.\n\nAn interval is specified by the following syntax:\n\nThe time interval is specified by the and times. is optional and defaults to the maximum time.\n\nThe current frame time is considered within the specified interval if it is included in the interval [ , ), that is when the time is greater or equal to and is lesser than .\n\nconsists of a sequence of one or more command specifications, separated by \",\", relating to that interval. The syntax of a command specification is given by:\n\nis optional and specifies the type of events relating to the time interval which enable sending the specified command, and must be a non-null sequence of identifier flags separated by \"+\" or \"|\" and enclosed between \"[\" and \"]\".\n\nThe following flags are recognized:\n\nIf is not specified, a default value of is assumed.\n\nspecifies the target of the command, usually the name of the filter class or a specific filter instance name.\n\nspecifies the name of the command for the target filter.\n\nis optional and specifies the optional list of argument for the given .\n\nBetween one interval specification and another, whitespaces, or sequences of characters starting with until the end of line, are ignored and can be used to annotate comments.\n\nA simplified BNF description of the commands specification syntax follows:\n• Specify audio tempo change at second 4:\n• Specify a list of drawtext and hue commands in a file. # show text in the interval 5-10 5.0-10.0 [enter] drawtext reinit 'fontfile=FreeSerif.ttf:text=hello world', [leave] drawtext reinit 'fontfile=FreeSerif.ttf:text='; # desaturate the image in the interval 15-20 15.0-20.0 [enter] hue s 0, [enter] drawtext reinit 'fontfile=FreeSerif.ttf:text=nocolor', [leave] hue s 1, [leave] drawtext reinit 'fontfile=FreeSerif.ttf:text=color'; # apply an exponential saturation fade-out effect, starting from time 25 25 [enter] hue s exp(25-t) A filtergraph allowing to read and process the above command list stored in a file , can be specified with:\n\nChange the PTS (presentation timestamp) of the input frames.\n\nThis filter accepts the following options:\n\nThe expression is evaluated through the eval API and can contain the following constants:\n• Set fixed rate of 25 frames per second:\n• Apply an offset of 10 seconds to the input PTS:\n• Generate timestamps from a \"live source\" and rebase onto the current timebase:\n\nBoth filters support all above options as commands.\n\nThe filter marks the color range property for the output frames. It does not change the input frame, but only sets the corresponding property, which affects how the frame is treated by following filters.\n\nThe filter accepts the following options:\n\nSet the timebase to use for the output frames timestamps. It is mainly useful for testing timebase configuration.\n\nIt accepts the following parameters:\n\nThe value for is an arithmetic expression representing a rational. The expression can contain the constants \"AVTB\" (the default timebase), \"intb\" (the input timebase) and \"sr\" (the sample rate, audio only). Default value is \"intb\".\n\nConvert input audio to a video output representing frequency spectrum logarithmically using Brown-Puckette constant Q transform algorithm with direct frequency domain coefficient calculation (but the transform itself is not really constant Q, instead the Q factor is actually variable/clamped), with musical tone scale, from E0 to D#10.\n\nThe filter accepts the following options:\n• Same as above, but with frame rate 30 fps:\n• Same as above, but with more accuracy in frequency domain:\n• Custom gamma, now spectrum is linear to the amplitude.\n• Custom fontcolor and fontfile, C-note is colored green, others are colored blue:\n\nConvert input audio to video output representing frequency spectrum using Continuous Wavelet Transform and Morlet wavelet.\n\nThe filter accepts the following options:\n\nConvert input audio to video output representing the audio power spectrum. Audio amplitude is on Y-axis while frequency is on X-axis.\n\nThe filter accepts the following options:\n\nConvert stereo input audio to a video output, representing the spatial relationship between two channels.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThe usage is very similar to the showwaves filter; see the examples in that section.\n• Complete example for a colored and sliding spectrum per channel using :\n\nThe filter accepts the following options:\n• Extract an audio spectrogram of a whole audio track in a 1024x1024 picture using :\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n• Output the input file audio and the corresponding video representation at the same time:\n• Create a synthetic signal and show it with showwaves, forcing a frame rate of 30 frames per second:\n\nThe filter accepts the following options:\n• Extract a channel split representation of the wave form of a whole audio track in a 1024x800 picture using :\n\nDelete frame side data, or select frames based on it.\n\nThis filter accepts the following options:\n\nSynthesize audio from 2 input video spectrums, first input stream represents magnitude across time and second represents phase across time. The filter will transform from frequency domain as displayed in videos back to time domain as presented in audio output.\n\nThis filter is primarily created for reversing processed showspectrum filter outputs, but can synthesize sound from other spectrograms too. But in such case results are going to be poor if the phase data is not available, because in such cases phase data need to be recreated, usually it’s just recreated from random noise. For best results use gray only output ( color mode in showspectrum filter) and scale for magnitude video and scale for phase video. To produce phase, for 2nd video, use option. Inputs videos should generally use slide mode as that saves resources needed for decoding video.\n\nThe filter accepts the following options:\n• First create magnitude and phase videos from audio, assuming audio is stereo with 44100 sample rate, then resynthesize videos back to audio with spectrumsynth:\n\nThe filter accepts a single parameter which specifies the number of outputs. If unspecified, it defaults to 2.\n• Create two separate outputs from the same input:\n• To create 3 or more outputs, you need to specify the number of outputs, like in:\n• Create two separate outputs from the same input, one cropped and one padded:\n• Create 5 copies of the input audio with :\n\nReceive commands sent through a libzmq client, and forward them to filters in the filtergraph.\n\nand work as a pass-through filters. must be inserted between two video filters, between two audio filters. Both are capable to send messages to any filter type.\n\nTo enable these filters you need to install the libzmq library and headers and configure FFmpeg with .\n\nFor more information about libzmq see: http://www.zeromq.org/\n\nThe and filters work as a libzmq server, which receives messages sent through a network interface defined by the (or the abbreviation \" \") option. Default value of this option is . You may want to alter this value to your needs, but do not forget to escape any ’:’ signs (see filtergraph escaping).\n\nThe received message must be in the form:\n\nspecifies the target of the command, usually the name of the filter class or a specific filter instance name. The default filter instance name uses the pattern ‘ ’, but you can override this by using the ‘ ’ syntax (see Filtergraph syntax).\n\nspecifies the name of the command for the target filter.\n\nis optional and specifies the optional argument list for the given .\n\nUpon reception, the message is processed and the corresponding command is injected into the filtergraph. Depending on the result, the filter will send a reply to the client, adopting the format:\n\nLook at for an example of a zmq client which can be used to send commands processed by these filters.\n\nConsider the following filtergraph generated by . In this example the last overlay filter has an instance name. All other filters will have default instance names.\n\nTo change the color of the left side of the video, the following command can be used:\n\nTo change the right side:\n\nTo change the position of the right side:\n\nBelow is a description of the currently available multimedia sources.\n\nThis is the same as movie source, except it selects an audio stream by default.\n\nGenerated stream periodically shows flash video frame and emits beep in audio. Useful to inspect A/V sync issues.\n\nIt accepts the following options:\n\nThis source supports the some above options as commands.\n\nIt accepts the following parameters:\n\nIt allows overlaying a second video on top of the main input of a filtergraph, as shown in this graph:\n• Skip 3.2 seconds from the start of the AVI file in.avi, and overlay it on top of the input labelled \"in\": movie=in.avi:seek_point=3.2, scale=180:-1, setpts=PTS-STARTPTS [over]; [in] setpts=PTS-STARTPTS [main]; [main][over] overlay=16:16 [out]\n• Read from a video4linux2 device, and overlay it on top of the input labelled \"in\": movie=/dev/video0:f=video4linux2, scale=180:-1, setpts=PTS-STARTPTS [over]; [in] setpts=PTS-STARTPTS [main]; [main][over] overlay=16:16 [out]\n• Read the first video stream and the audio stream with id 0x81 from dvd.vob; the video is connected to the pad named \"video\" and the audio is connected to the pad named \"audio\":\n\nBoth movie and amovie support the following commands:\n\nFFmpeg can be hooked up with a number of external libraries to add support for more formats. None of them are used by default, their use has to be explicitly requested by passing the appropriate flags to .\n\nFFmpeg can make use of the AOM library for AV1 decoding and encoding.\n\nGo to http://aomedia.org/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can use the AMD Advanced Media Framework library for accelerated H.264 and HEVC(only windows) encoding on hardware with Video Coding Engine (VCE).\n\nTo enable support you must obtain the AMF framework header files(version 1.4.9+) from https://github.com/GPUOpen-LibrariesAndSDKs/AMF.git.\n\nCreate an directory in the system include path. Copy the contents of into that directory. Then configure FFmpeg with .\n\nInitialization of amf encoder occurs in this order: 1) trying to initialize through dx11(only windows) 2) trying to initialize through dx9(only windows) 3) trying to initialize through vulkan\n\nTo use h.264(AMD VCE) encoder on linux amdgru-pro version 19.20+ and amf-amdgpu-pro package(amdgru-pro contains, but does not install automatically) are required.\n\nThis driver can be installed using amdgpu-pro-install script in official amd driver archive.\n\nFFmpeg can read AviSynth scripts as input. To enable support, pass to configure after installing the headers provided by AviSynth+. AviSynth+ can be configured to install only the headers by either passing to the normal CMake-based build system, or by using the supplied .\n\nFor Windows, supported AviSynth variants are AviSynth 2.6 RC1 or higher for 32-bit builds and AviSynth+ r1718 or higher for 32-bit and 64-bit builds.\n\nFor Linux, macOS, and BSD, the only supported AviSynth variant is AviSynth+, starting with version 3.5.\n\nFFmpeg can make use of the Chromaprint library for generating audio fingerprints. Pass to configure to enable it. See https://acoustid.org/chromaprint.\n\nFFmpeg can make use of the codec2 library for codec2 decoding and encoding. There is currently no native decoder, so libcodec2 must be used for decoding.\n\nGo to http://freedv.org/, download \"Codec 2 source archive\". Build and install using CMake. Debian users can install the libcodec2-dev package instead. Once libcodec2 is installed you can pass to configure to enable it.\n\nThe easiest way to use codec2 is with .c2 files, since they contain the mode information required for decoding. To encode such a file, use a .c2 file extension and give the libcodec2 encoder the -mode option: . Playback is as simple as . For a list of supported modes, run . Raw codec2 files are also supported. To make sense of them the mode in use needs to be specified as a format option: .\n\nFFmpeg can make use of the dav1d library for AV1 video decoding.\n\nGo to https://code.videolan.org/videolan/dav1d and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the davs2 library for AVS2-P2/IEEE1857.4 video decoding.\n\nGo to https://github.com/pkuvcl/davs2 and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the uavs3d library for AVS3-P2/IEEE1857.10 video decoding.\n\nGo to https://github.com/uavs3/uavs3d and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the Game Music Emu library to read audio from supported video game music file formats. Pass to configure to enable it. See https://bitbucket.org/mpyne/game-music-emu/overview.\n\nFFmpeg can use Intel QuickSync Video (QSV) for accelerated decoding and encoding of multiple codecs. To use QSV, FFmpeg must be linked against the dispatcher, which loads the actual decoding libraries.\n\nThe dispatcher is open source and can be downloaded from https://github.com/lu-zero/mfx_dispatch.git. FFmpeg needs to be configured with the option and needs to be able to locate the dispatcher’s files.\n\nFFmpeg can make use of the Kvazaar library for HEVC encoding.\n\nGo to https://github.com/ultravideo/kvazaar and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the LAME library for MP3 encoding.\n\nGo to http://lame.sourceforge.net/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the liblcevc_dec library for LCEVC enhacement layer decoding on supported bitstreams.\n\nGo to https://github.com/v-novaltd/LCEVCdec and follow the instructions for installing the library. Then pass to configure to enable it.\n\niLBC is a narrowband speech codec that has been made freely available by Google as part of the WebRTC project. libilbc is a packaging friendly copy of the iLBC codec. FFmpeg can make use of the libilbc library for iLBC decoding and encoding.\n\nGo to https://github.com/TimothyGu/libilbc and follow the instructions for installing the library. Then pass to configure to enable it.\n\nJPEG XL is an image format intended to fully replace legacy JPEG for an extended period of life. See https://jpegxl.info/ for more information, and see https://github.com/libjxl/libjxl for the library source. You can pass to configure in order enable the libjxl wrapper.\n\nFFmpeg can make use of the libvpx library for VP8/VP9 decoding and encoding.\n\nGo to http://www.webmproject.org/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of this library, originating in Modplug-XMMS, to read from MOD-like music files. See https://github.com/Konstanty/libmodplug. Pass to configure to enable it.\n\nSpun off Google Android sources, OpenCore, VisualOn and Fraunhofer libraries provide encoders for a number of audio codecs.\n\nFFmpeg can make use of the OpenCORE libraries for AMR-NB decoding/encoding and AMR-WB decoding.\n\nGo to http://sourceforge.net/projects/opencore-amr/ and follow the instructions for installing the libraries. Then pass and/or to configure to enable them.\n\nFFmpeg can make use of the VisualOn AMR-WBenc library for AMR-WB encoding.\n\nGo to http://sourceforge.net/projects/opencore-amr/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the Fraunhofer AAC library for AAC decoding & encoding.\n\nGo to http://sourceforge.net/projects/opencore-amr/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the Google LC3 library for LC3 decoding & encoding.\n\nGo to https://github.com/google/liblc3/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the OpenH264 library for H.264 decoding and encoding.\n\nGo to http://www.openh264.org/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFor decoding, this library is much more limited than the built-in decoder in libavcodec; currently, this library lacks support for decoding B-frames and some other main/high profile features. (It currently only supports constrained baseline profile and CABAC.) Using it is mostly useful for testing and for taking advantage of Cisco’s patent portfolio license (http://www.openh264.org/BINARY_LICENSE.txt).\n\nFFmpeg can use the OpenJPEG libraries for decoding/encoding J2K videos. Go to http://www.openjpeg.org/ to get the libraries and follow the installation instructions. To enable using OpenJPEG in FFmpeg, pass to .\n\nFFmpeg can make use of rav1e (Rust AV1 Encoder) via its C bindings to encode videos. Go to https://github.com/xiph/rav1e/ and follow the instructions to build the C library. To enable using rav1e in FFmpeg, pass to .\n\nFFmpeg can make use of the Scalable Video Technology for AV1 library for AV1 encoding.\n\nGo to https://gitlab.com/AOMediaCodec/SVT-AV1/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the TwoLAME library for MP2 encoding.\n\nGo to http://www.twolame.org/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can read VapourSynth scripts as input. To enable support, pass to configure. Vapoursynth is detected via . Versions 42 or greater supported. See http://www.vapoursynth.com/.\n\nDue to security concerns, Vapoursynth scripts will not be autodetected so the input format has to be forced. For ff* CLI tools, add before the input .\n\nFFmpeg can make use of the x264 library for H.264 encoding.\n\nGo to http://www.videolan.org/developers/x264.html and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the x265 library for HEVC encoding.\n\nGo to http://x265.org/developers.html and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the xavs library for AVS encoding.\n\nGo to http://xavs.sf.net/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the xavs2 library for AVS2-P2/IEEE1857.4 video encoding.\n\nGo to https://github.com/pkuvcl/xavs2 and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the XEVE library for EVC video encoding.\n\nGo to https://github.com/mpeg5/xeve and follow the instructions for installing the XEVE library. Then pass to configure to enable it.\n\nFFmpeg can make use of the XEVD library for EVC video decoding.\n\nGo to https://github.com/mpeg5/xevd and follow the instructions for installing the XEVD library. Then pass to configure to enable it.\n\nZVBI is a VBI decoding library which can be used by FFmpeg to decode DVB teletext pages and DVB teletext subtitles.\n\nGo to http://sourceforge.net/projects/zapping/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nYou can use the and options to have an exhaustive list.\n\nFFmpeg supports the following file formats through the library:\n\nmeans that the feature in that column (encoding / decoding) is supported.\n\nFFmpeg can read and write images for each frame of a video sequence. The following image formats are supported:\n\nmeans that the feature in that column (encoding / decoding) is supported.\n\nmeans that support is provided through an external library.\n\nmeans that the feature in that column (encoding / decoding) is supported.\n\nmeans that support is provided through an external library.\n\nmeans that the feature in that column (encoding / decoding) is supported.\n\nmeans that support is provided through an external library.\n\nmeans that an integer-only version is available, too (ensures high performance on systems without hardware floating point support).\n\nmeans that the feature is supported.\n\nmeans that support is provided through an external library.\n\nmeans that the protocol is supported.\n\nmeans that support is provided through an external library.\n\nFor details about the authorship, see the Git history of the project (https://git.ffmpeg.org/ffmpeg), e.g. by typing the command in the FFmpeg source directory, or browsing the online repository at https://git.ffmpeg.org/ffmpeg.\n\nMaintainers for the specific components are listed in the file in the source code tree.\n\nThis document was generated on March 23, 2025 using makeinfo."
    },
    {
        "link": "https://stackoverflow.com/questions/6239350/how-to-extract-duration-time-from-ffmpeg-output",
        "document": "To get a lot of information about a media file one can do\n\nwhere it will output a lot of lines, one in particular\n\nI would like to output only , so I try\n\nBut it prints everything, and not just the length.\n\nHow do I get just the duration length?"
    },
    {
        "link": "https://ffmpeg.org/ffprobe.html",
        "document": "ffprobe gathers information from multimedia streams and prints it in human- and machine-readable fashion.\n\nFor example it can be used to check the format of the container used by a multimedia stream and the format and type of each media stream contained in it.\n\nIf a url is specified in input, ffprobe will try to open and probe the url content. If the url cannot be opened or recognized as a multimedia file, a positive exit code is returned.\n\nIf no output is specified as output with ffprobe will write to stdout.\n\nffprobe may be employed both as a standalone application or in combination with a textual filter, which may perform more sophisticated processing, e.g. statistical processing or plotting.\n\nOptions are used to list some of the formats supported by ffprobe or for specifying which information to display, and for setting how ffprobe will show it.\n\nffprobe output is designed to be easily parsable by a textual filter, and consists of one or more sections of a form defined by the selected writer, which is specified by the option.\n\nSections may contain other nested sections, and are identified by a name (which may be shared by other sections), and an unique name. See the output of .\n\nMetadata tags stored in the container or in the streams are recognized and printed in the corresponding \"FORMAT\", \"STREAM\", \"STREAM_GROUP_STREAM\" or \"PROGRAM_STREAM\" section.\n\nAll the numerical options, if not specified otherwise, accept a string representing a number as input, which may be followed by one of the SI unit prefixes, for example: ’K’, ’M’, or ’G’.\n\nIf ’i’ is appended to the SI unit prefix, the complete prefix will be interpreted as a unit prefix for binary multiples, which are based on powers of 1024 instead of powers of 1000. Appending ’B’ to the SI unit prefix multiplies the value by 8. This allows using, for example: ’KB’, ’MiB’, ’G’ and ’B’ as number suffixes.\n\nOptions which do not take arguments are boolean options, and set the corresponding value to true. They can be set to false by prefixing the option name with \"no\". For example using \"-nofoo\" will set the boolean option with name \"foo\" to false.\n\nOptions that take arguments support a special syntax where the argument given on the command line is interpreted as a path to the file from which the actual argument value is loaded. To use this feature, add a forward slash ’/’ immediately before the option name (after the leading dash). E.g.\n\nwill load a filtergraph description from the file named .\n\nSome options are applied per-stream, e.g. bitrate or codec. Stream specifiers are used to precisely specify which stream(s) a given option belongs to.\n\nA stream specifier is a string generally appended to the option name and separated from it by a colon. E.g. contains the stream specifier, which matches the second audio stream. Therefore, it would select the ac3 codec for the second audio stream.\n\nA stream specifier can match several streams, so that the option is applied to all of them. E.g. the stream specifier in matches all audio streams.\n\nAn empty stream specifier matches all streams. For example, or would copy all the streams without reencoding.\n\nPossible forms of stream specifiers are:\n\nThese options are shared amongst the ff* tools.\n\nShow help. An optional parameter may be specified to print help about a specific item. If no argument is specified, only basic (non advanced) tool options are shown. Possible values of are: Print advanced tool options in addition to the basic tool options. Print complete list of options, including shared and private options for encoders, decoders, demuxers, muxers, filters, etc. Print detailed information about the decoder named . Use the option to get a list of all decoders. Print detailed information about the encoder named . Use the option to get a list of all encoders. Print detailed information about the demuxer named . Use the option to get a list of all demuxers and muxers. Print detailed information about the muxer named . Use the option to get a list of all muxers and demuxers. Print detailed information about the filter named . Use the option to get a list of all filters. Print detailed information about the bitstream filter named . Use the option to get a list of all bitstream filters. Print detailed information about the protocol named . Use the option to get a list of all protocols. Show the build configuration, one option per line. Show all codecs known to libavcodec. Note that the term ’codec’ is used throughout this documentation as a shortcut for what is more correctly called a media bitstream format. Show autodetected sources of the input device. Some devices may provide system-dependent source names that cannot be autodetected. The returned list cannot be assumed to be always complete. Show autodetected sinks of the output device. Some devices may provide system-dependent sink names that cannot be autodetected. The returned list cannot be assumed to be always complete. Set logging level and flags used by the library. The optional prefix can consist of the following values: Indicates that repeated log output should not be compressed to the first line and the \"Last message repeated n times\" line will be omitted. Indicates that log output should add a prefix to each message line. This can be used as an alternative to log coloring, e.g. when dumping the log to file. Indicates that log lines should be prefixed with time information. Indicates that log lines should be prefixed with date and time information. Flags can also be used alone by adding a ’+’/’-’ prefix to set/reset a single flag without affecting other or changing . When setting both and , a ’+’ separator is expected between the last value and before . is a string or a number containing one of the following values: Show nothing at all; be silent. Only show fatal errors which could lead the process to crash, such as an assertion failure. This is not currently used for anything. Only show fatal errors. These are errors after which the process absolutely cannot continue. Show all errors, including ones which can be recovered from. Show all warnings and errors. Any message related to possibly incorrect or unexpected events will be shown. Show informative messages during processing. This is in addition to warnings and errors. This is the default value. Same as , except more verbose. For example to enable repeated log output, add the prefix, and set to : Another example that enables repeated log output without affecting current state of prefix flag or : By default the program logs to stderr. If coloring is supported by the terminal, colors are used to mark errors and warnings. Log coloring can be disabled setting the environment variable , or can be forced setting the environment variable . Dump full command line and log output to a file named in the current directory. This file can be useful for bug reports. It also implies . Setting the environment variable to any value has the same effect. If the value is a ’:’-separated key=value sequence, these options will affect the report; option values must be escaped if they contain special characters or the options delimiter ’:’ (see the “Quoting and escaping” section in the ffmpeg-utils manual). The following options are recognized: set the file name to use for the report; is expanded to the name of the program, is expanded to a timestamp, is expanded to a plain set the log verbosity level using a numerical value (see ). For example, to output a report to a file named using a log level of (alias for log level ): Errors in parsing the environment variable are not fatal, and will not appear in the report. All FFmpeg tools will normally show a copyright notice, build options and library versions. This option can be used to suppress printing this information. Allows setting and clearing cpu flags. This option is intended for testing. Do not use it unless you know what you’re doing. Possible flags for this option are: Override detection of CPU count. This option is intended for testing. Do not use it unless you know what you’re doing. Set the maximum size limit for allocating a block on the heap by ffmpeg’s family of malloc functions. Exercise extreme caution when using this option. Don’t use if you do not understand the full consequence of doing so. Default is INT_MAX.\n\nThese options are provided directly by the libavformat, libavdevice and libavcodec libraries. To see the list of available AVOptions, use the option. They are separated into two categories:\n\nFor example to write an ID3v2.3 header instead of a default ID3v2.4 to an MP3 file, use the private option of the MP3 muxer:\n\nAll codec AVOptions are per-stream, and thus a stream specifier should be attached to them:\n\nIn the above example, a multichannel audio stream is mapped twice for output. The first instance is encoded with codec ac3 and bitrate 640k. The second instance is downmixed to 2 channels and encoded with codec aac. A bitrate of 128k is specified for it using absolute index of the output stream.\n\nNote: the syntax cannot be used for boolean AVOptions, use / .\n\nNote: the old undocumented way of specifying per-stream AVOptions by prepending v/a/s to the options name is now obsolete and will be removed soon.\n\nShow the unit of the displayed values. Use SI prefixes for the displayed values. Unless the \"-byte_binary_prefix\" option is used all the prefixes are decimal. Force the use of binary prefixes for byte values. Prettify the format of the displayed values, it corresponds to the options \"-unit -prefix -byte_binary_prefix -sexagesimal\". specifies the name of the writer, and specifies the options to be passed to the writer. For example for printing the output in JSON format, specify: For more details on the available output printing formats, see the Writers section below. Print sections structure and section information, and exit. The output is not meant to be parsed by a machine. Select only the streams specified by . This option affects only the options related to streams (e.g. , , etc.). For example to show only audio streams, you can use the command: To show only video packets belonging to the video stream with index 1: Show payload data, as a hexadecimal and ASCII dump. Coupled with , it will dump the packets’ data. Coupled with , it will dump the codec extradata. The dump is printed as the \"data\" field. It may contain newlines. Show a hash of payload data, for packets with and for codec extradata with . Show information about the error found when trying to probe the input. The error information is printed within a section with name \"ERROR\". Show information about the container format of the input multimedia stream. All the container format information is printed within a section with name \"FORMAT\". Like , but only prints the specified entry of the container format information, rather than all. This option may be given more than once, then all specified entries will be shown. This option is deprecated, use instead. Entries are specified according to the following syntax. contains a list of section entries separated by . Each section entry is composed by a section name (or unique name), optionally followed by a list of entries local to that section, separated by . If section name is specified but is followed by no , all entries are printed to output, together with all the contained sections. Otherwise only the entries specified in the local section entries list are printed. In particular, if is specified but the list of local entries is empty, then no entries will be shown for that section. Note that the order of specification of the local section entries is not honored in the output, and the usual display order will be retained. The formal syntax is given by: For example, to show only the index and type of each stream, and the PTS time, duration time, and stream index of the packets, you can specify the argument: To show all the entries in the section \"format\", but only the codec type in the section \"stream\", specify the argument: To show all the tags in the stream and format sections: To show only the tag (if available) in the stream sections: Show information about each packet contained in the input multimedia stream. The information for each single packet is printed within a dedicated section with name \"PACKET\". Show information about each frame and subtitle contained in the input multimedia stream. The information for each single frame is printed within a dedicated section with name \"FRAME\" or \"SUBTITLE\". Show logging information from the decoder about each frame according to the value set in , (see ). This option requires . The information for each log message is printed within a dedicated section with name \"LOG\". Show information about each media stream contained in the input multimedia stream. Each media stream information is printed within a dedicated section with name \"STREAM\". Show information about programs and their streams contained in the input multimedia stream. Each media stream information is printed within a dedicated section with name \"PROGRAM_STREAM\". Show information about stream groups and their streams contained in the input multimedia stream. Each media stream information is printed within a dedicated section with name \"STREAM_GROUP_STREAM\". Show information about chapters stored in the format. Each chapter is printed within a dedicated section with name \"CHAPTER\". Count the number of frames per stream and report it in the corresponding stream section. Count the number of packets per stream and report it in the corresponding stream section. Read only the specified intervals. must be a sequence of interval specifications separated by \",\". will seek to the interval starting point, and will continue reading from that. Each interval is specified by two optional parts, separated by \"%\". The first part specifies the interval start position. It is interpreted as an absolute position, or as a relative offset from the current position if it is preceded by the \"+\" character. If this first part is not specified, no seeking will be performed when reading this interval. The second part specifies the interval end position. It is interpreted as an absolute position, or as a relative offset from the current position if it is preceded by the \"+\" character. If the offset specification starts with \"#\", it is interpreted as the number of packets to read (not including the flushing packets) from the interval start. If no second part is specified, the program will read until the end of the input. Note that seeking is not accurate, thus the actual interval start point may be different from the specified position. Also, when an interval duration is specified, the absolute end time will be computed by adding the duration to the interval start point found by seeking the file, rather than to the specified start value. The formal syntax is given by:\n• Seek to time 10, read packets until 20 seconds after the found seek point, then seek to position (1 minute and thirty seconds) and read packets until position .\n• Read only 42 packets after seeking to position :\n• Read only the first 20 seconds from the start:\n• Read from the start until position : Show private data, that is data depending on the format of the particular shown element. This option is enabled by default, but you may need to disable it for specific uses, for example when creating XSD-compliant XML output. Version information is printed within a section with name \"PROGRAM_VERSION\". Version information for each library is printed within a section with name \"LIBRARY_VERSION\". Show information related to program and library versions. This is the equivalent of setting both and options. Show information about all pixel formats supported by FFmpeg. Pixel format information for each format is printed within a section with name \"PIXEL_FORMAT\". Some writers viz. JSON and XML, omit the printing of fields with invalid or non-applicable values, while other writers always print them. This option enables one to control this behaviour. Valid values are / , / and / . Default is . Analyze frames and/or their side data up to the provided read interval, providing additional information that may be useful at a stream level. Must be paired with the option or it will have no effect. Currently, the additional fields provided by this option when enabled are the and fields. For example, to analyze the first 20 seconds and populate these fields: Force bitexact output, useful to produce output which is not dependent on the specific build. Write output to . If not specified, the output is sent to stdout.\n\nA writer defines the output format adopted by , and will be used for printing all the parts of the output.\n\nA writer may accept one or more arguments, which specify the options to adopt. The options are specified as a list of = pairs, separated by \":\".\n\nAll writers support the following options:\n\nA description of the currently available writers follows.\n\nPrint each section in the form:\n\nMetadata tags are printed as a line in the corresponding FORMAT, STREAM, STREAM_GROUP_STREAM or PROGRAM_STREAM section, and are prefixed by the string \"TAG:\".\n\nA description of the accepted options follows.\n\nThe writer is equivalent to , but supports different defaults.\n\nEach section is printed on a single line. If no option is specified, the output has the form:\n\nMetadata tags are printed in the corresponding \"format\" or \"stream\" section. A metadata tag key, if printed, is prefixed by the string \"tag:\".\n\nThe description of the accepted options follows.\n\nA free-form output where each line contains an explicit key=value, such as \"streams.stream.3.tags.foo=bar\". The output is shell escaped, so it can be directly embedded in sh scripts as long as the separator character is an alphanumeric character or an underscore (see option).\n\nThe description of the accepted options follows.\n\nThe following conventions are adopted:\n• all key and values are UTF-8\n• newline, ‘ ’, ‘ ’, ‘ ’ and the following characters are escaped\n• ‘ ’ is not used but usually parsed as key/value separator\n\nThis writer accepts options as a list of = pairs, separated by ‘ ’.\n\nThe description of the accepted options follows.\n\nEach section is printed using JSON notation.\n\nThe description of the accepted options follows.\n\nFor more information about JSON, see http://www.json.org/.\n\nThe XML output is described in the XML schema description file installed in the FFmpeg datadir.\n\nAn updated version of the schema can be retrieved at the url http://www.ffmpeg.org/schema/ffprobe.xsd, which redirects to the latest schema committed into the FFmpeg development source code tree.\n\nNote that the output issued will be compliant to the schema only when no special global output options ( , , , etc.) are specified.\n\nThe description of the accepted options follows.\n\nFor more information about the XML format, see https://www.w3.org/XML/.\n• MPEG1/2 timecode is extracted from the GOP, and is available in the video stream details ( , see ).\n• MOV timecode is extracted from tmcd track, so is available in the tmcd stream metadata ( , see ).\n• DV, GXF and AVI timecodes are available in format metadata ( , see ).\n\nFor details about the authorship, see the Git history of the project (https://git.ffmpeg.org/ffmpeg), e.g. by typing the command in the FFmpeg source directory, or browsing the online repository at https://git.ffmpeg.org/ffmpeg.\n\nMaintainers for the specific components are listed in the file in the source code tree.\n\nThis document was generated on March 23, 2025 using makeinfo."
    },
    {
        "link": "https://stackoverflow.com/questions/8933053/check-duration-of-audio-files-on-the-command-line",
        "document": "I have the amazing SoX app which has an option called stats that generates a set of audio info including duration. I am looking for a way to get only duration. I am flexible on output format, could be any of sample length, hh:mm:ss, or seconds. The latter would be my preference.\n\nI need to check the duration of a group of audio files. Is there a simple way to do this on the unix command-line?\n\ngives you the length of the specified file in format (mm can be greater than 59). For just the total number of seconds in the file, you'd use: To get the total length of all the mp3 files in seconds, AWK can help:\n\nThe raw duration in seconds can be obtained with a high degree of precision with the use of of ffmpeg, as follows: The output, easy to use in further scripting, is formatted like this: Extending upon that, the following will measure the total duration in seconds of all .mp3 files in the current directory: LENGTH=0; for file in *.mp3; do if [ -f \"$file\" ]; then LENGTH=\"$LENGTH+$(ffprobe -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"$file\" 2>/dev/null)\"; fi; done; echo \"$LENGTH\" | bc And to measure the total length of audio files of several extensions, another wildcard may be appended: LENGTH=0; for file in *.mp3 *.ogg; do if [ -f \"$file\" ]; then LENGTH=\"$LENGTH+$(ffprobe -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"$file\" 2>/dev/null)\"; fi; done; echo \"$LENGTH\" | bc\n\nIf you are interested in finding total duration of wav files in a directory using soxi and python you can use this: soxi -D input_dir/*.wav | python -c \"import sys;print(sum(float(l) for l in sys.stdin))\" change according to your input directory. If you want to find max/min duration between all wav files feel free to change to or .\n\nI expanded on Navid Naderi's answer here, and created a bash function that will give you a summary of every file's running time, plus some time totals in various formats (seconds, minutes, or total running time). function sox_duration_total { if [[ \"$#\" -lt 1 ]]; then echo \"find files!\" echo \" sox_duration_total *.wav\" echo \"\" return fi for i in \"$@\"; do val=`soxi -d \"$i\"` echo \"$val | $i\" done soxi -D \"$@\" | python -c \"import sys;print(\\\"\n\ntotal sec: \\\" +str( sum(float(l) for l in sys.stdin)))\" soxi -D \"$@\" | python -c \"import sys;print(\\\"total min: \\\" +str( sum(float(l) for l in sys.stdin)/60 ))\" soxi -D \"$@\" | python -c \"import sys;import datetime;print(\\\"running time: \\\" +str( datetime.timedelta(seconds=sum(float(l) for l in sys.stdin)) ))\" }\n\nmediainfo can do this, but mediainfo is one of those useful tools that's so badly documented that you almost need to know how to use it in order to learn how to use it (happens a lot in the linux world). After hours of trials and reading high and low, I finally got it to generate a recursive comma-separated list of file names and their duration in milliseconds. cd to the starting directory and issue the following command: The results will be output to list.txt.\n\n(When you don't have at your disposal) I got it recursively for all my files # install mp3info if not yet installed with sudo apt-get install mp3info with the find command, put the total seconds to a csv file (go to the directory with your e.g. mp3 files first) Then open it in e.g. in LibreOffice and sum it up at the bottom (to get the hours) with\n\nExpanding on kevin meinert's answer, I have modified the script to:\n• Default to listing the duration of files in the current directory when no arguments are provided\n• Allow the flag to search recursively\n• Allow any number of specific arguments such as (same as previous behavior)\n• Added some QoL improvements like trimming duration and removed the running time as I didn't care about that I also added the script to rather than having it be a function, so I can call it anywhere by just running #!/bin/bash if [[ \"$#\" -eq 0 ]]; then echo \"No arguments provided, processing all WAV files in the current directory.\" files=$(find . -maxdepth 1 -type f -name \"*.wav\") elif [[ \"$1\" == \"-r\" ]]; then echo \"Recursive mode enabled, searching for WAV files recursively.\" files=$(find . -type f -name \"*.wav\") else files=\"$@\" fi if [[ -z \"$files\" ]]; then echo \"No WAV files found.\" exit 1 fi echo \"hh:mm:ss:ms | filepath\" echo \"----------------------\" for i in $files; do val=`soxi -d \"$i\"` echo \"$val | $i\" done echo \"$files\" | xargs soxi -D | python3 -c \"import sys;print(\\\"\n\ntotal sec: \\\" +str( sum(float(l) for l in sys.stdin)))\" echo \"$files\" | xargs soxi -D | python3 -c \"import sys;print(\\\"total min: \\\" +str( sum(float(l) for l in sys.stdin)/60 ))\" #echo \"$files\" | xargs soxi -D | python3 -c \"import sys;import datetime;print(\\\"running time: \\\" +str( datetime.timedelta(seconds=sum(float(l) for l in sys.stdin)) ))\""
    }
]