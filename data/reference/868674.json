[
    {
        "link": "https://elastic.co/guide/en/kibana/current/dashboard.html",
        "document": "Dashboards are the best way to visualize and share insights from your Elasticsearch data.\n\nA dashboard is made of one or more panels that you can organize as you like. Each panel can display various types of content: visualizations such as charts, tables, metrics, and maps, static annotations like text or images, or even specialized views for Machine Learning or Observability data.\n\nThere are several panel editors in Kibana that let you create and configure different types of visualizations.\n\nAt any time, you can share a dashboard you’ve created with your team, in Kibana or outside.\n\nSome dashboards are created and managed by the system, and are identified as in your list of dashboards. This generally happens when you set up an integration to add data. You can’t edit managed dashboards directly, but you can duplicate them and edit these duplicates."
    },
    {
        "link": "https://elastic.co/guide/en/kibana/current/create-a-dashboard-of-panels-with-web-server-data.html",
        "document": "Learn the most common ways to create a dashboard from your own data. The tutorial will use sample data from the perspective of an analyst looking at website logs, but this type of dashboard works on any type of data.\n\nWhen you’re done, you’ll have a complete overview of the sample web logs data.\n\nBefore you begin, you should be familiar with the Kibana concepts.\n\nAdd the sample web logs data, and create and set up the dashboard.\n\nOpen the visualization editor, then make sure the correct fields appear.\n\nTo create the visualizations in this tutorial, you’ll use the following fields:\n\nClick a field name to view more details, such as its top values and distribution.\n\nPick a field you want to analyze, such as clientip. To analyze only the clientip field, use the Metric visualization to display the field as a number.\n\nThe only number function that you can use with clientip is Unique count, also referred to as cardinality, which approximates the number of unique values.\n\nThere are two shortcuts you can use to view metrics over time. When you drag a numeric field to the workspace, the visualization editor adds the default time field from the data view. When you use the Date histogram function, you can replace the time field by dragging the field to the workspace.\n\nTo visualize the bytes field over time:\n\nTo emphasize the change in Median of bytes over time, change the visualization type to Line with one of the following options:\n\nTo save space on the dashboard, hide the axis labels.\n\nSince you removed the axis labels, add a panel title:\n\nCreate a visualization that displays the most frequent values of request.keyword on your website, ranked by the unique visitors. To create the visualization, use Top values of request.keyword ranked by Unique count of clientip, instead of being ranked by Count of records.\n\nThe Top values function ranks the unique values of a field by another function. The values are the most frequent when ranked by a Count function, and the largest when ranked by the Sum function.\n\nThe chart labels are unable to display because the request.keyword field contains long text fields. You could use one of the Suggestions, but the suggestions also have issues with long text. The best way to display long text fields is with the Table visualization.\n\nCreate a proportional visualization that helps you determine if your users transfer more bytes from documents under 10KB versus documents over 10Kb.\n\nTo select documents based on the number range of a field, use the Intervals function. When the ranges are non numeric, or the query requires multiple clauses, you could use the Filters function.\n\nTo display the values as a percentage of the sum of all values, use the Pie chart.\n\nThe distribution of a number can help you find patterns. For example, you can analyze the website traffic per hour to find the best time for routine maintenance.\n\nTable and Proportion visualizations support multiple functions. For example, to create visualizations that break down the data by website traffic sources and user geography, apply the Filters and Top values functions.\n\nRemove the documents that do not match the filter criteria:\n\nResize and move the panels so they all appear on the dashboard without scrolling.\n\nDecrease the size of the following panels, then move the panels to the first row:\n\nNow that you have a complete overview of your web server data, save the dashboard."
    },
    {
        "link": "https://devopscube.com/kibana-dashboard-tutorial",
        "document": "In this Kibana dashboard tutorial, we will look at the important Kibana concepts involved in the creation of different Kibana dashboards and visualizations.\n\nKibana is an open-source tool that helps in search and data visualization capabilities. We have covered the Kibana basics in our Kubernetes EFK stack tutorial. Take a look at the EFK stack guide to understand how Kibana interacts with Elasticsearch. Also, in our DevOps Engineers Guide, we have covered the importance of logging in an organization.\n\nWe will also look at some of the examples of Kibana dashboards to better understand all the visualizing concepts.\n\nWhen it comes to troubleshooting, it is essential to have logging dashboards and log search functionalities that help developers and DevOps engineers troubleshoot applications and infrastructure issues in real-time.\n\nIn this guide, we are going to learn the following.\n\nTo get started, we need data in Kibana. The good news is, Kibana comes pre-loaded with some sample data. We are going to use it to learn to create Kibana dashboards.\n\nLet's go ahead and import the sample data into Kibana to create visualizations. Follow the steps given below to import the sample data.\n\nStep 1: To get the sample data, go to the Kibana home. Sample URL: . Click the Load a data set option as shown below.\n\nStep 2: Load the 'Sample eCommerce orders' data by clicking the \"Add data\" option. This is the data that we are going to use for dashboards.\n\nOnce the data is added, you should see the installed option as shown below.\n\nLet's look at the popular visualizations and create some visualizations for our sample data. After this, we will create our dashboards.\n\n\n\nOpen the visualization section inside Kibana for this.\n\nLet's understand the concept of Gauge in Kibana.\n• What does Kibana gauge signify? It indicates the status of the metric.\n• How is it usable? It displays the metric against a 'range,' i.e., it depicts a comparison of current value to a predetermined lower and upper limit.\n• How will we use it? Let's use it to signify the volume of business in a given time frame. Next, we will use the 'range' settings to signify business state.\n\nSince each log indicates a transaction - we can assume that a higher no. of such logs means higher business transactions.\n\nWe will use colors such as Red, Yellow, and Green to indicate poor, average, and good business performance in the time frame.\n\nAdditional features: It has several other settings that help in creating better visualizations like:\n• color-schemes: to control the color for each range, try it by clicking on the color in the legend.\n• sub-levels: to control the text being displayed inside the visualization.\n\nLet's understand the concept of Metrics in Kibana.\n• What does Kibana metrics signify: It displays a calculation as a single figure.\n• How is it usable: It has the capability to display any metric and also perform calculations of it.\n• How will we use it: Let's use it to display the overall total revenue of the business (sum of ) and the total revenue from each customer (sum of grouped by customer's email address)\n\n\n\nThis will help the business find out which customer is spending more at their store! \n\n\n\nWe will display the revenue from the top 5 customers at a given time.\n• Add a metric. Select aggregation as 'sum' and field as 'taxful_total_price'.\n• Add a bucket - 'split group'. Select aggregation as 'terms' and field as 'email'.\n• This will now display results split by customer emails.\n• Save this visualization as a new visualization.\n\nIt has several other settings that help in creating better visualizations like:\n• buckets: to group data according to another field (e.g., email)\n• label: to control the text being displayed inside the visualization.\n• ranges: to treat values in a given range as per its significance,\n• e.g. total revenue in the range 0-100 is low and total revenue in the range of 100000-1000000 is high - so we can assign colors differently using this feature.\n• color-schema: to control the color displayed in the visualization as per the ranges.\n\nCreate a new visualization for total revenue and modify it -\n• Add aggregation of 'terms'. We use terms when we want to use a custom field like email.\n• What it signifies? It creates interactive controls for easy manipulation of the dashboard data.\n• How is it usable? It has the capability to apply filters to the visualizations inside a dashboard.\n• How will we use it? Let's use it to filter the logs based on important fields like manufacturer, category, and quantity.\n\nWe will display the logs by creating a saved query and importing it into our dashboard.\n• Go to the discover tab.\n• Add the required fields from the left-hand side options.\n• You should see only the added field in the logs panel now.\n• Save it by clicking on 'save.' (It's on the top side of the panel.\n• Now let's try to create a controller. Create a controller visualization.\n• Repeat step 6 for all the controller fields that you wish to add.\n\nAdditional features -\n\nIt has several other settings that help in better filtrations like:\n\nparentcontrol: It's used to control the values of a field. Let us understand it with the help of an example.\n\n\n\nThe 'category' controller field has 'manufacturer' as the parent control, and the current value for the 'manufacturer' field is 'Low Tide Media.'\n\n\n\nThis implies that the 'category' field will have only those values for which 'manufacturer' is 'Low Tide Media.'\n\n\n\nIn this way, filtering becomes easier.\n\n\n\nmultiselect: This enables/disables the functionality to have multiple selections in the field. This also helps in filtering out.\n\n\n\nE.g., let's say we want to look at logs where 'manufacturer' is 'Low Tide Media' and 'Spiritechnologies'. We can add both values if multiselect is enabled.\n\n\n\ndynamic options: dynamically updates the options list according to user inputs. \n\n\n\ncontrol label: It controls the text being displayed inside the visualization.\n\nControllers are usually added to dashboards along with a saved query (Yes, you can add a saved query from discover into a Kibana dashboard as well!)\n\nWhat it signifies? \n\nIt creates a visualization of trends in the data.\n\n\n\nHow is it usable? \n\nIt has the capability to plot various fields and apply basic calculations to the field such as sum, min, max.\n\n\n\nHow will we use it? \n\nLet's use it to display the total_quantity of products for various dates; let us also group this information according to the product type, such as 'men's shoes' or 'women's clothing.\n\n\n\nThis type of filtering can help us understand the trends easily.\n\n\n\nSteps to create:\n• In the Y-axis, we will plot the sum of quantities. So let's select aggregation as 'sum' and field as 'total_quantity.'\n• In the X-axis, we will display the trend as a histogram (It shows the distribution of data) with an interval of 1 hour. It means the data points will be taken on a per-hour basis.\n• We wish to understand the trend for each product category separately. So let's split the data according to the product category.\n\n\n\nTo do this, add a 'split series' option and add a 'sub-aggregation' of 'terms' and select 'category' in the field.\n• Apply the changes by clicking on the 'play' button at the top of the panels.\n\nSave it by clicking on 'save.' (It's on the top side of the panel.)\n\nAdditional features -\n\nIt has several other settings that help in better visualizations like:\n\ngrid: It's used to control the visibility of x and y-axis grids.\n\npositions: It's used to control the placement of the x and y-axis.\n\ncustomlabels: to control the text being displayed inside the visualization.\n\n\n\nApart from this, it has a lot of options for aggregations like average, min, max, etc.\n\n\n\nThis is the most commonly used visualization type! So I encourage you to explore this deeply.\n\nWhat it signifies? \n\nIt shows metrics on a map.\n\n\n\nHow is it usable? \n\nIt gives a visualization over the world map! You can visualize your metrics on a per-country basis.\n\n\n\nHow will we use it? \n\nLet's use it to display the transactions, i.e., the count of logs carried out in each country. \n\n\n\nThis type of filtering can help us get a 360' view.\n\n\n\nSteps to create:\n• In the metrics section, we want to plot the count of logs for the countries, so let us select 'count' as the aggregation.\n• We obviously want to see the count for each country separately. So let's create a bucket.\n• We have a field called 'country_iso_code' that tells Kibana about each transaction log's country. So select 'terms' in the bucket's 'aggregation' and field as 'country_iso_code'.\n• Increase the size to about 50 to see data for more countries.\n\nSave it by clicking on 'save'. (It's on the top side of the panel.)\n\nAdditional features -\n\nIt has several other settings that help in better visualizations like:\n\nvectormaps: It's used to specify different maps. \n\n\n\nBy default, the world map is displayed. But what if we want to visualize data related to just India. We can select 'India state and territories' in vectormap option.\n\n\n\nIt will display a detailed map of India with the state boundaries now.\n\n\n\nSimilarly, there are other options such as 'USA states' or 'Australia states'.\n\ncolor-schema: to control the color displayed in the visualization as per the ranges. \n\n\n\nborder thickness: to control the line size by which Kibana draws the boundaries in the map.\n\nA dashboard in Kibana is a collection of various visualizations. Here is an example dashboard.\n\n\n\n\n\nWe will create a similar dashboard from the visualizations in the above section.\n• Head to the dashboard section of Kibana and create a new dashboard.\n• Start adding the visualization and saved queries here.\n\nThat's it! You have created your very own first dashboard now! Save it for future use.\n\nIn production environments, teams usually try to keep a backup of these visualizations and dashboards in a JSON file for safekeeping.\n\n\n\nSteps to create a backup file:\n• Go to the saved objects tab.\n• Select your visualizations and dashboards from the list.\n• Export them. Kibana will create a JSON file for you and download it to your system.\n\nCreating a backup is not enough; we also need to understand importing the objects through the file.\n\nSteps to import an object using a backup file:\n• Under the saved objects click on the Import button.\n• Next, select your JSON backup file. Kibana will import them into its list now.\n\nBesides backups, Kibana also has tons of features like anomaly detection using Machine Learning and functionalities to receive email & slack alerts. So be sure to explore them too!\n\nEFK is a tool used by 100s of DevOps teams every day. In this EFK tutorial series, you have learned a very useful skill of creating a Kibana dashboard \n\n\n\nThere are many others; explore different options thoroughly to create the best visualizations!\n\nIn the next article, we will learn how to create Kibana dashboards for Kubernetes cluster monitoring.\n\nAlso, if you are learning about logging & monitoring, you might like my guide on setting up Prometheus on Kubernetes."
    },
    {
        "link": "https://coralogix.com/blog/kibana-dashboard-tutorial-spice-up-your-kibana-dashboards",
        "document": "When it comes to dashboarding, Kibana is king. Since its release Kibana has changed the way businesses visualize data. Kibana is a fairly intuitive platform and offers some seriously impressive methods of data analysis and visualization. In this kibana dashboard tutorial, we are going to help you unlock the full potential of the platform and help you become a Kibana guru. When it comes to visualizing data Kibana is well-suited for monitoring logs, application monitoring, and operational intelligence. Get ready to blow everyone away with your abilities to visualize data! So without further ado, let’s take a look at some outstanding examples of Kibana dashboards.\n\nIn this example, we are using logs from a Nginx server that is configured as a reverse proxy for a corporate website. Here we are providing visualization in the form of a heat map, breaking down the hours of a day, geo-IP addressing for the total number of requests and bytes, and finally geo-IP source and destination.\n\nThis example uses logs from a PostgreSQL server that is used in a web application. We are visualizing the total number of a given log type, logs by level over time, and a list of recent logs.\n\nThis dashboard is displaying the availability of several servers. The data is provided as ICMP logs which indicate when the server last responded. If a server does not respond for 60 seconds, it is classed as down.\n\nThis dashboard is using logs from Azure. It provides visibility into user activity. In this screen, we are showing a high-level summary of user activity levels, access requests, the top active users, and any resource groups that have been changed.\n\nThis example uses logs from a Redis server that is used in a web application. We are visualizing incoming logs, top commands, logs levels & roles, and finally logs over time.\n\nThis dashboard focuses on the availability of a set of servers. The Dashboard has a filter call out built-in and displays the instances uptime, CPU Utilisation, disk IO and goes on to show much more.\n\nThe most frequently used visualizations for Kibana are the Line charts, Area charts, Bar charts, Pie charts, data tables, metric, maps and gauges. When you click to create a new visualization in Kibana you are shown the new visualization screen to the left. Kibana ships with a vast array of visualizations and often this can be overwhelming. Remember the trick to effective dashboards is simplicity. This should enable you to select the correct Kibana visualization to achieve your desired dashboard. If you are unsure though we have put together a handy table that outlines the Kibana visualizations and their function. This should give you some context before we get into the kibana dashboard tutorial.\n\nBefore you jump in!\n\nBefore you start building any dashboards, it’s important to plan out what you are trying to achieve. Jumping in and just building a dashboard is a recipe for disaster. The key to information-rich dashboards is the quality of the data. With this in mind, you want to start from the objective of the dashboard and work backward. This allows you to map out your required data inputs and ensure your logs are configured to provide the right data. You will also need to think about how you are parsing the data into the platform. If you have allowed elastic to-do this automatically it’s highly probable you are going to need to tweak the fields to ensure they make sense at the dashboarding phase.\n\nTo help you think about this process and formulate the best possible plan to successfully implement your dashboard ask yourself the following questions:\n• What does your dashboard need to present?\n• What data do you need?\n• How will your data need to be queried to provide the correct outputs?\n• What visualizations will you be using?\n\nThe first step after you have Kibana deployed is to login! Kibana runs as a service on port 5061. As a result, you need to enter http://YOURADDRESS:5061. Navigating to this will present you with the below page:\n\nThis is the front page of Kibana which displays the menu bar which is used to navigate around the platform. The bottom left hand button will expand the menu bar to display the different menu options.\n\nThe first part of using Kibana is to set up your data once you have ingested it into Elastic. An index pattern tells Kibana what Elasticsearch index to analyze. The most common way to ingest logs is using Logstash. You can find out how to configure Logstash here.\n\nIn this example we have logstash shipping logs to our Elasticsearch cluster ready for Kibana. To set up your index patterns Click on the management button and then index patterns. Here you will configure how to group your indexes for use with Kibana. To set up your first index pattern click Create Index Pattern. Here you will need to enter the index pattern definition. In its rawest form this is the name in which the index pattern starts. You can use a * to add a wildcard to the end of your pattern. An example logstash-* will catch any indices that start with logstash-*. An example of this can be found below. You will need to create an index pattern for each data set you wish to visualize in Kibana.\n\nClick ‘Next step’ and then you will need to configure your timestamp. This is how Kibana will know what metrics to use for the time and date in the platform.\n\nIn our Logstash example we will be using @timestamp. This is the time and date that kibana takes from the log. You might want to use another field should your logs contain another date source for higher accuracy. Once configured click on the ‘Create index pattern’ button. Your index is now live and ready for usage in Kibana. To verify this, let’s see what data Kibana has access to. On the main menu click ‘Discover’.\n\nIf you don’t see any logs, make sure your time and date filter is set to a range that includes data like below (For our example we have it set to last 7 days):\n\nYou are now able to see and filter all the data available to Kibana.\n\nNow that we have data in Kibana, it’s time to build our searches. Searches help us focus our data for building visualizations. Using the ‘discover’ page from the above enables access to the raw data in the platform allowing you to note your available fields ready for building visualizations, which will ultimately make up your dashboard.\n\nThe discover page has a search and filter functionality. Once you have located your desired data set use the save option. You can use saved searches on different datasets and the advantage is changing the saved searches will also update all the linked visualizations negating the need to update them all individually!\n\nSearches that have been saved can also be inserted into the dashboards. Which will provide your dashboard with a quick link to jump into the discover tab allowing users to see the related logs to the dashboard.\n\nYour data is ready, your searches are saved, let’s build our first visualization! On the main menu bar select ‘Visualize’.\n\nClick on create visualization and select which visualization type you would like to use. In this example we are going to make a pie chart. The sources displayed will be the index patterns we configured earlier.\n\nIn this example we are going to use our Logstash indexes. Kibana will load with its defaults. You can open your previously created saved searches at the top like you would in the ‘Discover’ page. The left hand menu bar enables you to customize your visualization. Kibana offers two types of data aggregations. The first is Metric aggregations and the second is bucket aggregations.\n\nBuckets create aggregations of data based on a certain criteria. Depending on your aggregation type you can create buckets that provide filtering. This filtering can allow you to present your data in the form of value ranges and intervals for dates, ip ranges and more. You will want to apply a bucket to a pie chart to display the data that provides context. In the below example we have applied a bucket that uses an aggregation of terms on the field email.keyword and specified a size of 5. As a result we see the top five email addresses, in alphabetical order.\n\nMetric aggregations are used to calculate a value for each bucket based on the documents inside the bucket. Every visualization type has its own unique characteristics providing different ways to present buckets and their associated values. In our pie chart example below the slices are determined by the buckets. The size of the slice is determined by the metric aggregation.\n\nTip – When you make a change click the play button to refresh the chart.\n\nNow our visualization is starting to provide a meaningful chart. We are able to customise the chart further in the advanced tab. Should you wish to interrogate the raw data generating the graph then click on the inspect button.\n\nThe advanced tab will provide the capabilities to change the style of our visualization to suit our requirements. We can also add and remove items like labels. Each visualization is independently configurable. It’s best to experiment with the options to find what works best for your own visualizations.\n\nOnce you are happy the save button allows you to save your visualization. Enter a title and description. It’s important that these are specific, as you create multiple visualizations it can often be hard to find them without a good naming convention!\n\nFinally, a visualization can be shared. This allows you to provide a link or embed code into a website that will display the visualization. When you share your visualizations you have two options, you can either share a saved visualization or a snapshot of the current state. A saved visualization will allow users to see recent edits, whereas the snapshot option will not.\n\nTip: If you are looking to share visualizations and dashboards the users you are sharing them with need to have access to the Kibana server.\n\nLet’s take a look at creating another type of visualization. In this example we are going to look at creating an area chart. Much like with the pie chart you have your Metric and your Buckets. Our example is based on data from an ecommerce platform. Metrics have been configured as an aggregation type of Sum, a field of total_quantity, and our buckets have been configured with an aggregation type of Date Histogram, using a field of order_date, with a split aggregation of Terms, using the field category, ordered by the Metric: Sum of total_quantity. The results are as below:\n\nA visualization can be edited at any time. This is achieved by opening the visualization on the visualization page! What’s more when you edit a visualization it will also update on your dashboard.\n\nNow that we know how to create visualizations, it’s time to build a dashboard!\n\nNow let’s bring together all the work we have done creating exciting visualizations. Dashboards use multiple visualisations. Much like we have learnt putting together visualizations it’s important to plan out your dashboards goal. Adding visualizations is super simple as a result dashboards can become over crowded and lose their meaning. As we have discussed previously its import to focus on your dashboards goal and design accordingly.\n\nIt’s important to note that multiple data sources enable you to create advanced dashboards that display vast correlated data, however in doing so you lose the ability to drill down. When you drill down Kibana will add a filter to your data sources that will likely only be relevant to the selected source effectively making the visualizations useless. As a result for dashboards where drill down capabilities are required it is important to ensure your visualizations are based on the same data source!\n\nTIP – You should try to ensure you keep your dashboards to a single page to avoid scrolling.\n\nTo create your dashboard on the main menu select ‘Dashboard’ and then create dashboard. If you have already been in a dashboard it will load that up, click on the dashboards crumbs in the top left to return to the dashboards home page.\n\nClick on the Add button on the top left to add a visualization. This is where it will pay to have your visualizations well named with a naming convention. Click on the visualization you would like to add. This will add it to the dashboard but you will remain in the add visualization (add panels) page. This enables you to run through and select all of your required visualizations. Once you have added all the visualizations you require you can close the add panels page using the x in the top right corner. You will likely have a screen like the below:\n\nYou are now able to move and expand your panels. Once you have your dashboard laid out correctly remember to save it! Much like with our visualizations you can use the Share button in the corner to share your dashboard with other users. You can also change the data set using the search and dates fields at the top of the dashboard.\n\nTIP – The Options button will allow you to remove your visualization title and remove margins between panels.\n\nNow that we have explored much of the dashboarding capabilities of Kibana you have the power to make powerful dashboards to visualize your data. As demonstrated, Kiabana has many more use cases than just dashboarding, and hopefully, this article has got the creative juices flowing. After you’ve built your perfect dashboard start exploring the other feature, Kibana offers to fully supercharge your data visualization skills. Good luck dashboarding ninjas!"
    },
    {
        "link": "https://chaossearch.io/blog/how-to-create-kibana-dashboard",
        "document": "Wondering how to create a dashboard in Kibana to visualize and analyze your log data?\n\nIn this blog post, we’ll provide a step-by-step explanation of how to create a dashboard in Kibana. You’ll learn how to use Kibana to query indexed application and event log data, filter query results to highlight the most critical and actionable information, build Kibana visualizations using your log data, and incorporate those visualizations into a Kibana dashboard.\n\nKibana is an open-source data analytics and data visualization software used by data scientists, CloudOps and IT security teams to explore, monitor, and extract insights from structured log data. Once log data has been centralized (with a log aggregation tool, e.g. Fluentbit, Logstash or Amazon CloudWatch) and indexed (e.g., with Elasticsearch, Amazon OpenSearch Service, or ChaosSearch®), IT analysts can use Kibana to perform queries and filtering, construct data visualizations, and combine those visualizations into dashboards that reveal valuable insights into system security and application performance.\n\nSome organizations use Kibana in combination with Logstash and Elasticsearch as part of the open-source ELK stack log analytics solution. Logstash is used to aggregate, parse, and transform log data before indexing the data in Elasticsearch. Once data is indexed, Elasticsearch allows users to query the data in different ways and produce tables that can be visualized using Kibana.\n\nAt ChaosSearch, we’ve integrated the OpenSearch version of Kibana (a.k.a OpenSearch Dashboards) directly into our comprehensive log analytics solution. ChaosSearch transforms Amazon S3 into a data lake repository for log and event data, allowing DevSecOps teams to aggregate, index, and analyze log data in real-time, with no data movement and no ETL process.\n\nWith ChaosSearch, log data is aggregated in Amazon S3 buckets whose full functionality is available from within the ChaosSearch console. Log data in S3 buckets can be indexed using our proprietary Chaos Index® format, and DevSecOps teams can enable Live Indexing to index newly-created objects in specific Object Groups, enabling near real-time querying for data as it becomes available.\n\nLearn more about getting your data into S3:\n\nCloudFront Logs in Amazon S3, Quicker than Ever\n\nHow to Move Kubernetes Logs to S3 with Logstash\n\nCloudWatch Logs to S3: The Easy Way\n\nHow to Create an S3 Bucket with AWS CLI\n\nOnce log data has been indexed by ChaosSearch, users can leverage the ChaosSearch Refinery® to clean, prepare, and transform the data without writing or executing any code. This results in the creation of an Index View: a logical index, based on a physical index, ready for discovery, visualization, dashboarding, and analysis using Kibana.\n\nBefore we dive into all of the details and the how-to portion of this guide, let’s start with a brief overview of the main Kibana functionalities. Opening the left sidebar of Kibana, you’ll notice five key capabilities that you can leverage to support your log analytics program.\n\nHow Kibana Can Support Your Log Analytics\n\nKibana’s Discover tool is usually the starting point to data exploration. It gives you the ability to interact quickly with your data through search and filters. After choosing which Index View you’d like to work with, Kibana will allow you to submit search queries, filter the results, and view document data from your log files. If the data is timestamped, a histogram representation of your data will be automatically generated at the top of the page.\n\nKibana’s Visualize tool allows you to construct different kinds of visualizations using the ChaosSearch indices you have created.\n\nKibana’s Dashboard tool gives you the ability to combine log data visualizations built from your ChaosSearch indices into functional dashboards. Using dashboards with real-time indexing allows you to automatically update dashboards in near real-time as events are logged in your Amazon S3 buckets.\n\nWith the ChaosSearch Kibana integration, you’ll notice an added feature called “Alerting.” This feature gives you the ability to monitor log and event data in real-time, create customized alert triggers, and set a destination (Slack channel, email, AWS chime, etc.) where notifications may be sent when an alert is triggered.\n\nFinally, Kibana’s management interface gives you the ability to adjust Kibana’s runtime configuration and tweak advanced settings to change how Kibana behaves as you Discover, Visualize, and Dashboard your log data.\n\nNext, we’ll take you through the process of data discovery, visualization, and how to create a dashboard in Kibana.\n\nHow to Discover Your Data in Kibana\n\nKibana’s Discover screen is where you’ll be able to perform queries and create summary tables based on your indexed log data. As a prerequisite for using the Discover tool, you’ll need to have ingested some log data into your S3 buckets and have created ChaosSearch Index Views using the Refinery tool.\n\nHaving done so, you’ll be able to query those Index Views and apply filters on the Discover screen.\n\nIn this example, we’re using Kibana’s Discover screen to explore log data from AWS CloudTrail, which allows you to monitor and record all account activity across your AWS infrastructure.\n\nUsing the drop-down menu on Kibana’s left sidebar, we could select the Chaos View we created previously using the ChaosSearch Refinery® tool. Beneath our selection, Kibana shows us all the fields present in the View that we might choose to query from. For this analysis, we’re interested in exploring AWS console login events present in the CloudTrail.\n\nLooking under the heading “Available Fields,” we can search for a field called “Records.eventName,” which contains the name of the logged event. To set up our query, we’ll need to use the Query/Search Bar at the top of the Kibana Discover page to express our query using the Kibana Query Language (KQL). In this case, we’re looking for ConsoleLogin events, so our query in KQL reads Records.eventName: \"ConsoleLogin.\"\n\nWith this query, we can isolate all of the ConsoleLogin events from our CloudTrail logs. We can also specify an absolute or relative data range for our query - here, we’ve chosen to exclusively analyze data from the last 90 days. Finally, we hit the Refresh button to see the results from our query.\n\nOnce you’ve entered your query, Kibana will return a list of results that match your query parameters. You can click on each result to expand it and view all data from that log file in table or JSON format.\n\nWatch this quick demo to learn more about how to analyze CloudTrail logs with ChaosSearch:\n\nChaosSearch is built from the ground up for analyzing vast amounts of data, usually on the Terabyte and Petabyte scales. For this reason, our version of Kibana includes some additional features required when exploring these large datasets. The first one is a Query Progress Bar which monitors the status of your query. There is also a Cancel Query button used to abort queries when for example, you detect an error on your search syntax or search terms. Finally, there is a counter that indicates the Total Queried Data, which gives users instant feedback on how much data the query is running against.\n\nAnother useful feature of Discover is the ability to select specific fields to be listed on the event details table, giving you a clear view of the fields you're interested in for your query. All other fields for the event are now hidden but can still be accessed by expanding any particular event.\n\nWhen you’re happy with your query, you’ll need to click the Save button and enter a name for your query before you can turn your query results into a Visualization.\n\nHow to Build Visualizations in Kibana\n\nOnce you’ve queried your data, it’s time to transform your results into a visualization with Kibana's data visualization tools. Clicking the Visualize button on the left sidebar will bring you to the Visualizations screen, where you can create a new visualization or access visualizations you have previously created. To build a new data visualization, click on the Create Visualization button.\n\nKibana allows you to create many different types of charts, including area plots, data tables, gauges, goal trackers, heat maps, bar charts, line graphs, and more. The right option for you will depend on the specific goals of your analysis. You can read a short description of each visualization format to help determine the best choice for your intended use case.\n\nAfter choosing a format for your visualization, you’ll need to identify a data source. You can choose either a saved search (like the one we created above) or perform a new search by selecting the Chaos View with the data you want to visualize.\n\nIn this example, we’re using our indexed CloudTrail log data to create a visualization of event names generated over the past 30 days.\n\nIn our previous example, we focused specifically on Console Login events, but now we’re interested in all types of events and how frequently they appear in our log data. To shed light on that, we chose to organize our events data into a Pie Chart.\n\nAfter selecting a visualization format and data source, it’s time to configure the visualization builder to display your data in a useful way.\n\nConfiguration options in the visualization builder will vary depending on the visualization format you choose for your data. For a pie chart, you’ll need to use two aggregations: the Metrics aggregation and the Buckets aggregation.\n\nThe Metrics aggregation establishes how the size of each slice in your pie chart will be determined. In this example, we’re using the Count aggregation, so the size of each slice will reflect the total number of events for a particular event name within our source data (relative to the whole, since it’s a pie chart).\n\nThe Buckets aggregation determines what information is being retrieved from your data set to create the graph. In this example, we use the Terms aggregation, which allows us to display the top 10 most frequently occurring Event Names in our data.\n\nFinally, we created a filter to remove events without information on the EventName field from our pie chart related to CloudTrail log file integrity events.\n\nYou can modify visualizations by adding more filters, changing the date range, or even changing the colors. If you hover your mouse over a slice of the pie chart, you’ll see a small pop-up window with details about the data for that specific slice. When you’re done creating your visualization, you’ll need to click Save and give it a name before using it to create a dashboard in Kibana.\n\nHow to Create a Dashboard in Kibana\n\nCreating functional and informative Kibana dashboards is a great way to start monitoring applications and AWS services in real-time. To get started, you’ll need to click “Dashboard” on the left sidebar, then click “Create new dashboard.” Next, you’ll be asked to Add Panels to your Kibana dashboard.\n\nPanels are the building blocks of dashboards in Kibana. Any visualization you have previously created and saved in Kibana can be added as a Panel in your dashboard. You can also add a Text panel with static information about your dashboard or a Controls panel that allows you to filter your data in real-time. Panels can also be used to display a saved search table from the Kibana Discover tool, a table of live-streaming logs, or the results of machine-learning anomaly detection jobs.\n\nYou can also clone panels within a dashboard, copy panels to a separate dashboard, and even apply filters until every panel in your dashboard shows the data and information you want to see.\n\nOnce you’ve specified what data should be included in each panel on your dashboard, you’ll be able to arrange and reorganize panels and other dashboard elements and apply design options to make your dashboard more visually appealing and functional for users.\n\nOnce you have finished, you can click the Save as button to add a title to your completed dashboard and save it for future reference.\n\nYou can share your newly created Kibana dashboard in several ways: by embedding it as an iframe on a webpage, sharing a direct link, or generating a PDF or PNG report of your dashboard.\n\nAs you get more familiar with how to create dashboards in Kibana, you’ll be able to build more visually appealing and informative dashboards to support your application monitoring and security log analytics initiatives.\n\nUse Kibana for Real-Time Insights into Your Log Data\n\nNow that you’ve learned how to create a dashboard in Kibana, what’s next?\n\nIf you haven’t yet, now would be a great time to configure real-time indexing for the log data included in your dashboard. With real-time indexing, the log data you generate is used to automatically update your indices, visualizations, and dashboards in Kibana.\n\nAs a result, you’ll be able to gather real-time insights into network security and application performance, and you can even configure customized Alerts to notify you when anomalies are detected in your newly ingested log data.\n\nRead the Blog: How to Integrate BI and Data Visualization Tools with a Data Lake\n\nCheck out the Whitepaper: Digital Business Observability: Analyzing IT and Business Data Together"
    },
    {
        "link": "https://elastic.co/blog/using-painless-kibana-scripted-fields",
        "document": "Kibana provides powerful ways to search and visualize data stored in Elasticsearch. For the purpose of visualizations, Kibana looks for fields defined in Elasticsearch mappings and presents them as options to the user building a chart. But what happens if you forget to define an important value as a separate field in your schema? Or what if you want to combine two fields and treat them as one? This is where Kibana scripted fields come into play.\n\n\n\nScripted fields have actually been around since the early days of Kibana 4. At the time they were introduced, the only way to define them relied on Lucene Expressions, a scripting language in Elasticsearch which deals exclusively with numeric values. As a result, the power of scripted fields was limited to a subset of use cases. In 5.0, Elasticsearch introduced Painless, a safe and powerful scripting language that allows operating on a variety of data types, and as a result, scripted fields in Kibana 5.0 are that much more powerful.\n\nIn the rest of this blog, we'll walk you through how to create scripted fields for common use cases. We'll do so by relying on a dataset from Kibana Getting Started tutorial and use an instance of Elasticsearch and Kibana running in Elastic Cloud, which you can spin up for free.\n\nThe following video walks you through how to spin up a personal Elasticsearch and Kibana instance in Elastic Cloud and load a sample dataset into it. \n\n\n\nElasticsearch allows you to specify scripted fields on every request. Kibana improves on this by allowing you to define a scripted field once in the Management section, so it can be used in multiple places in the UI going forward. Note that while Kibana stores scripted fields alongside its other configuration in the .kibana index, this configuration is Kibana-specific, and Kibana scripted fields are not exposed to API users of Elasticsearch.\n\nWhen you go to define a scripted field in Kibana, you'll be given a choice of scripting language, allowing you to pick from all the languages installed on the Elasticsearch nodes that have dynamic scripting enabled. By default that is \"expression\" and \"painless\" in 5.0 and just \"expression\" in 2.x. You can install other scripting languages and enable dynamic scripting for them, but it is not recommended because they cannot be sufficiently sandboxed and have been deprecated.\n\nScripted fields operate on one Elasticsearch document at a time, but can reference multiple fields in that document. As a result, it is appropriate to use scripted fields to combine or transform fields within a single document, but not perform calculations based on on multiple documents (e.g. time-series math). Both Painless and Lucene expressions operate on fields stored in doc_values. So for string data, you will need to have the string to be stored in data type keyword. Scripted fields based on Painless also cannot operate directly on _source.\n\nOnce scripted fields are defined in \"Management\", user can interact with them the same way as with other fields in the rest of Kibana. Scripted fields automatically show up in the Discover field list and are available in Visualize for the purposes of creating visualizations. Kibana simply passes scripted field definitions to Elasticsearch at query time for evaluation. The resulting dataset is combined with other results coming back from Elasticsearch and presented to the user in a table or a chart.\n\nThere are a couple of known limitations when working with scripted fields at the time of writing this blog. You can apply most Elasticsearch aggregations available in Kibana visual builder to scripted fields, with the most notable exception of the significant terms aggregation. You can also filter on scripted fields via the filter bar in Discover, Visualize, and Dashboard, although you have to take care to write proper scripts that return well-defined values, as we show below. It is also important to refer to the \"Best Practices\" section below to ensure you do not destabilize your environment, when using scripted fields.\n\nThe following video shows how to use Kibana to create scripted fields.\n\nThis section presents a few examples of Lucene expressions and Painless scripted fields in Kibana in common scenarios. As mentioned above, these examples were developed on top of a dataset from Kibana Getting Started tutorial and assume you are using Elasticsearch and Kibana 5.1.1, as there are a couple of known issues related to filtering and sorting on certain types of scripted fields in earlier versions.\n\nFor the most part, scripted fields should work out of the box, as Lucene expressions and Painless are enabled by default in Elasticsearch 5.0. The only exception are scripts that require regex-based parsing of fields, which will require you to set the following setting in elasticsearch.yml to turn on regex matching for Painless: script.painless.regex.enabled: true\n\nNote: Keep in mind that Kibana scripted fields work on a single document at a time only, so there is no way to do time-series math in a scripted field.\n\n\n\nLucene expressions provide a whole host of date manipulation functions out-of-the-box. However, since Lucene expressions only return numerical values, we'll have to use Painless to return a string-based day-of-week (below).\n• Example: Combine source and destination or first and last name\n\nNote: Because scripted fields need to operate on fields in doc_values, we are using .keyword versions of strings above.\n• Example: Return label \"big download\" for any document with bytes over 10000\n\nNote: When introducing logic, ensure that every execution path has a well-defined return statement and a well-defined return value (not null). For instance, above scripted field will fail with a compile error when used in Kibana filters without the return statement at the end or if the statement returns null. Also keep in mind that breaking up logic into functions is not supported within Kibana scripted fields.\n• Example: Return the part after the last slash in the URL\n\nNote: Whenever possible, avoid using regex expressions to extract substrings, as indexOf() operations are less resource-intensive and less error-prone.\n\nMatch a string using regex, and take action on a match\n• Example: Return a string \"error\" if a substring \"error\" is found in field \"referer\", otherwise return a string \"no error\".\n\nNote: Simplified regex syntax is useful for conditionals based on a regex match.\n• Example: Return domain, the string after the last dot in the \"host\" field.\n\nNote: Defining an object via the regex matcher() functions allows you to extract groups of characters that matched the regex and return them.\n• Example: Return the first octet of the IP address (stored as a string) and treat it as a number.\n\nNote: It is important to return the right data type in a script. Regex match returns a string, even if a number is matched, so you should explicitly convert it to an integer on return.\n• Example: Parse date into day-of-week into string\n\nNote: Since Painless supports all of Java's native types, it provides access to native functions around those types, such as LocalDateTime(), useful in performing more advanced date math.\n\nAs you see, the Painless scripted language provides powerful ways of extracting useful information out of arbitrary fields stored in Elasticsearch via Kibana scripted fields. However, with great power comes great responsibility.\n\nBelow we outline a few best practices around using Kibana scripted fields.\n• Always use a development environment to experiment with scripted fields. Because scripted fields are immediately active after you save them in the Management section of Kibana (e.g. they appear in the Discover screen for that index pattern for all users), you should not develop scripted fields directly in production. We recommend that you try your syntax first in a development environment, evaluate the impact of scripted fields on realistic data sets and data volumes in staging, and only then promote them to production.\n• Once you gain confidence that the scripted field provides value to your users, consider modifying your ingest to extract the field at index time for new data. This will save Elasticsearch processing at query time and will result in faster response times for Kibana users. You can also use the _reindex API in Elasticsearch to re-index existing data."
    },
    {
        "link": "https://elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-painless.html",
        "document": "Painless is a performant, secure scripting language designed specifically for Elasticsearch. You can use Painless to safely write inline and stored scripts anywhere scripts are supported in Elasticsearch.\n\nPainless provides numerous capabilities that center around the following core principles:\n\nReady to start scripting with Painless? Learn how to write your first script.\n\nIf you’re already familiar with Painless, see the Painless Language Specification for a detailed description of the Painless syntax and language features."
    },
    {
        "link": "https://stackoverflow.com/questions/74111639/pass-a-dynamic-value-in-the-elasticsearch-painless-script-params-from-the-elasti",
        "document": "I am passing params map in the painless script, that map is type of . The key of the map defines the and the value defines the in epoch Long.\n\nThe elastic Document have the raw structure like this :\n\nI have calculate the average difference between the and (both are in epoch).\n\nI have build the ES Painless script as below:\n\nFor nesting on the array of & date I am using i.e. the nested path.\n\nI want to pass the dynamic value in the params map as it will give the from the map for the same in the Elasticsearch document.\n\nBut I am getting the parsing error from ES. I have tired with and but both are falling and giving parsing error.\n\nWhat is the correct way to pass the dynamic value in params map."
    },
    {
        "link": "https://stackoverflow.com/questions/57745685/kibana-painless-scripted-fields",
        "document": "I'm new to Kibana. I've been tasked with replicating an excel report into a dashboard.\n\nI want to use a data table to display my values and also have a calculated field for percentages. The percentiles metric is not what I am trying to use to perform my calculations.\n\nIn the excel report the percentages are calculated using a formula like this : =I20/$I$28*100\n\nIt's a relatively simple formula I want to replicate in Kibana. Is there a way to create a scripted field in Painless or is there an even easier way to do this?\n\nSome assistance would be much appreciated."
    },
    {
        "link": "https://medium.com/@samb333/advanced-painless-script-for-elasticsearch-ad194aeffd2d",
        "document": "We will explore the Painless script — hot search feature and use them during ingestion. This is advanced article please refer my Elasticsearch Blog posts for Elasticsearch\n\nTo access the value of index fields\n\nWe use Update By Query API . Ilustrated is Painless query and response from ES Cluster\n\nFor Ingest Pipelines need to transform, enrich data before indexing . We can use painless as Node Processor.Refer to my articles on advanced Ingestion using ES Nodes and how to configure\n\nWe can use Painless to perform Bucket aggregations tp perform percent like\n\nWithin a script, we can get or access the variable which represents the current relevance score of a document.\n\nPlease refer to my Posts on Scoring and how to calculate relevance scoring for Indexed Documents\n\nWe donot want to focus on fields returned rather on score augmentation and we get response from elastic cluster\n\nWe can create fields on fly using doc\n\nScripts errors are mostly parsing errors and ES will return and can be challenging . Lets see this its perfectly valid script as per ES guidelines but version and parser are external API so lets not be surprised all API have bugs\n\nUse emit() to out the value from script and lets see parsing error\n\nWe visited Painless Script and its advances usage in ElasticCluster got Query as well as ingest processor and dynamic fields after index are created."
    }
]