[
    {
        "link": "http://accord-framework.net/docs/html/T_Accord_Neuro_ActivationNetwork.htm",
        "document": "The ActivationNetwork type exposes the following members.\n\nDetermines whether the specified object is equal to the current object. (Inherited from .) Allows an object to try to free resources and perform other cleanup operations before it is reclaimed by garbage collection. (Inherited from .) Gets the of the current instance. (Inherited from .) Set new activation function for all neurons of the network. Top\n\nChecks whether an object implements a method with the given name. (Defined by ExtensionMethods.) Compares two objects for equality, performing an elementwise comparison if the elements are vectors or matrices. (Defined by Matrix.) Overloaded. Converts an object into another type, irrespective of whether the conversion can be done at compile time or not. This can be used to convert generic types to numeric types during runtime. (Defined by ExtensionMethods.) Overloaded. Converts an object into another type, irrespective of whether the conversion can be done at compile time or not. This can be used to convert generic types to numeric types during runtime. (Defined by ExtensionMethods.) Top\n\nActivation network is a base for multi-layer neural network with activation functions. It consists of activation layers."
    },
    {
        "link": "http://accord-framework.net/docs/html/M_Accord_Neuro_ActivationNetwork__ctor.htm",
        "document": "Initializes a new instance of the ActivationNetwork class.\n\nThe new network is randomized (see Randomize method) after it is created."
    },
    {
        "link": "https://github.com/accord-net/framework/issues/2232",
        "document": "It is sad that the examples of Accord.NET is extremely primitive and insufficient. Therefore, I am having hard time to figure out how to use the system properly.\n\nThe entire source code of my application is uploaded to here : https://github.com/FurkanGozukara/CSE419-Artificial-Intelligence-and-Machine-Learning-2020/tree/master/source%20codes/lecture%206%20perceptron%20example\n\nThe video of this lecture is here (3 hours 11 minutes) : https://youtu.be/qrklFBewlJA\n\nMy biggest question is about how to provide output classes?\n\nLets say for the dataset abalone (http://archive.ics.uci.edu/ml/datasets/Abalone) there are 28 output classes which are the age of abalone : https://github.com/FurkanGozukara/CSE419-Artificial-Intelligence-and-Machine-Learning-2020/blob/master/source%20codes/lecture%206%20perceptron%20example/lecture%206%20perceptron%20example/bin/Debug/netcoreapp3.1/abalone.data\n\nHow to compose the output class?\n\nLike this way?\n\nThis generates the output class like below\n\nAnd here the code of training part\n\nSo my second question is about calculating accuracy. The BackPropagationLearning algorithm generates weights for each output neuron. So I counted the highest one as the prediction. Is my approach and code correct?\n\nI am believe I am on the right track but I want to be sure by getting feedback of an Accord.NET library expert"
    },
    {
        "link": "https://stackoverflow.com/questions/50134740/difference-between-distance-and-activation-network-into-accord-net",
        "document": "Ð¡ould you explain the significant difference between distance and activation network into Accord.NET? In which cases is it best to use them?"
    },
    {
        "link": "http://accord-framework.net/docs/html/T_Accord_Neuro_Learning_BackPropagationLearning.htm",
        "document": "The BackPropagationLearning type exposes the following members.\n\nDetermines whether the specified object is equal to the current object. (Inherited from .) Allows an object to try to free resources and perform other cleanup operations before it is reclaimed by garbage collection. (Inherited from .) Gets the of the current instance. (Inherited from .) Top\n\nChecks whether an object implements a method with the given name. (Defined by ExtensionMethods.) Compares two objects for equality, performing an elementwise comparison if the elements are vectors or matrices. (Defined by Matrix.) Overloaded. Converts an object into another type, irrespective of whether the conversion can be done at compile time or not. This can be used to convert generic types to numeric types during runtime. (Defined by ExtensionMethods.) Overloaded. Converts an object into another type, irrespective of whether the conversion can be done at compile time or not. This can be used to convert generic types to numeric types during runtime. (Defined by ExtensionMethods.) Top\n\nThe class implements back propagation learning algorithm, which is widely used for training multi-layer neural networks with continuous activation functions."
    },
    {
        "link": "http://accord-framework.net/docs/html/T_Accord_Neuro_Learning_BackPropagationLearning.htm",
        "document": "The BackPropagationLearning type exposes the following members.\n\nDetermines whether the specified object is equal to the current object. (Inherited from .) Allows an object to try to free resources and perform other cleanup operations before it is reclaimed by garbage collection. (Inherited from .) Gets the of the current instance. (Inherited from .) Top\n\nChecks whether an object implements a method with the given name. (Defined by ExtensionMethods.) Compares two objects for equality, performing an elementwise comparison if the elements are vectors or matrices. (Defined by Matrix.) Overloaded. Converts an object into another type, irrespective of whether the conversion can be done at compile time or not. This can be used to convert generic types to numeric types during runtime. (Defined by ExtensionMethods.) Overloaded. Converts an object into another type, irrespective of whether the conversion can be done at compile time or not. This can be used to convert generic types to numeric types during runtime. (Defined by ExtensionMethods.) Top\n\nThe class implements back propagation learning algorithm, which is widely used for training multi-layer neural networks with continuous activation functions."
    },
    {
        "link": "http://accord-framework.net/docs/html/T_Accord_Neuro_Learning_ResilientBackpropagationLearning.htm",
        "document": "The ResilientBackpropagationLearning type exposes the following members.\n\nDetermines whether the specified object is equal to the current object. (Inherited from .) Allows an object to try to free resources and perform other cleanup operations before it is reclaimed by garbage collection. (Inherited from .) Gets the of the current instance. (Inherited from .) Top\n\nChecks whether an object implements a method with the given name. (Defined by ExtensionMethods.) Compares two objects for equality, performing an elementwise comparison if the elements are vectors or matrices. (Defined by Matrix.) Overloaded. Converts an object into another type, irrespective of whether the conversion can be done at compile time or not. This can be used to convert generic types to numeric types during runtime. (Defined by ExtensionMethods.) Overloaded. Converts an object into another type, irrespective of whether the conversion can be done at compile time or not. This can be used to convert generic types to numeric types during runtime. (Defined by ExtensionMethods.) Top\n\nThis class implements the resilient backpropagation (RProp) learning algorithm. The RProp learning algorithm is one of the fastest learning algorithms for feed-forward learning networks which use only first-order information."
    },
    {
        "link": "http://accord-framework.net/samples.html",
        "document": "The Harris sample application demonstrates how to perform corners detection using the Harris algorithm. The current implementation supports both Harris and Nobel corner measures. Harris can be enabled by checking the checkbox next to the ''k'' parameter, which is only needed for Harris. The Nobel measure does not require setting any parameters. A suitable choice for the threshold parameter when using Harris measure may range around 10,000 to 30,000, while when using Nobel it may range around 20 to 100. Best sigma values are usually higher than 0.3 and lesser than 5.0.\n\nThis sample application shows how to recreate the liblinear.exe command line application using the SVM algorithms provided by the framework. The framework can perform almost all liblinear algorithms in C#, except for one. Those include: The framework can perform also load to and from files stored in LibSVM's sparse format. This means it should be straightforward to create or learn your models using one tool and run it on the other, if that would be necessary. For example, given that Accord.NET can run on mobile applications, it is possible to create and learn your models in a computing grid using liblinear and then integrate it in your Windows Phone application by loading it in Accord.NET."
    },
    {
        "link": "https://visualstudiomagazine.com/articles/2015/04/01/back-propagation-using-c.aspx",
        "document": "Back-Propagation is the most common algorithm for training neural networks. Here's how to implement it in C#.\n\nBack-propagation is the most common algorithm used to train neural networks. There are many ways that back-propagation can be implemented. This article presents a code implementation, using C#, which closely mirrors the terminology and explanation of back-propagation given in the Wikipedia entry on the topic.\n\nYou can think of a neural network as a complex mathematical function that accepts numeric inputs and generates numeric outputs. The values of the outputs are determined by the input values, the number of so-called hidden processing nodes, the hidden and output layer activation functions, and a set of weights and bias values.\n\nA fully connected neural network with m inputs, h hidden nodes, and n outputs has (m * h) + h + (h * n) + n weights and biases. For example, a neural network with 4 inputs, 5 hidden nodes, and 3 outputs has (4 * 5) + 5 + (5 * 3) + 3 = 43 weights and biases. Training a neural network is the process of finding values for the weights and biases so that, for a set of training data with known input and output values, the computed outputs of the network closely match the known outputs.\n\nThe best way to see where this article is headed is to examine the demo program in Figure 1. The demo program begins by generating 1,000 synthetic data items. Each data item has four input values and three output values. For example, one of the synthetic data items is:\n\nThe four input values are all between -10.0 and +10.0 and correspond to predictor values that have been normalized so that values below zero are smaller than average, and values above zero are greater than average. The three output values correspond to a variable to predict that can take on one of three categorical values. For example, you might want to predict the political leaning of a person: conservative, moderate or liberal. Using 1-of-N encoding, conservative is (1, 0, 0), moderate is (0, 1, 0), and liberal is (0, 0, 1). So, for the example data item, if the predictor variables are age, income, education, and debt, the data item represents a person who is younger than average, has much lower income than average, is somewhat more educated than average, and has higher debt than average. The person has a liberal political view.\n\nAfter the 1,000 data items were generated, the demo program split the data randomly, into an 8,000-item training set and a 2,000-item test set. The training set is used to create the neural network model, and the test set is used to estimate the accuracy of the model.\n\nAfter the data was split, the demo program instantiated a neural network with five hidden nodes. The number of hidden nodes is arbitrary and in realistic scenarios must be determined by trial and error. Next, the demo sets the values of the back-propagation parameters. The maximum number of training iterations, maxEpochs, was set to 1,000. The learning rate, learnRate, controls how fast training works and was set to 0.05. The momentum rate, momentum, is an optional parameter to increase the speed of training and was set to 0.01. Training parameter values must be determined by trial and error.\n\nDuring training, the demo program calculated and printed the mean squared error, every 100 epochs. In general, the error decreased over time, but there were a few jumps in error. This is typical behavior when using back-propagation. When training finished, the demo displayed the values of the 43 weights and biases found. These values, along with the number of hidden nodes, essentially define the neural network model.\n\nThe demo concluded by using the weights and bias values to calculate the predictive accuracy of the model on the training data (99.13 percent, or 7,930 correct out of 8,000) and on the test data (98.50 percent, or 1,970 correct out of 2,000). The demo didn't use the resulting model to make a prediction. For example, if the model is fed input values (1.0, 2.0, 3.0, 4.0), the predicted output is (0, 1, 0), which corresponds to a political moderate.\n\nThis article assumes you have at least intermediate level developer skills and a basic understanding of neural networks but does not assume you are an expert using the back-propagation algorithm. The demo program is too long to present in its entirety here, but complete source code is available in the download that accompanies this article. All normal error checking has been removed to keep the main ideas as clear as possible.\n\nThe Demo Program\n\n To create the demo program, I launched Visual Studio, selected the C# console application program template, and named the project CodingBackProp. The demo has no significant Microsoft .NET Framework version dependencies, so any relatively recent version of Visual Studio should work. After the template code loaded, in the Solution Explorer window I renamed file Program.cs to BackPropProgram.cs and Visual Studio automatically renamed class Program for me.\n\nThe overall structure of the demo program is presented in Listing 1. I removed unneeded using statements that were generated by the Visual Studio console application template, leaving just the one reference to the top-level System namespace.\n\nAll the control logic is in the Main method and all the classification logic is in a program-defined NeuralNetwork class. Helper method MakeAllData generates a synthetic data set. Method SplitTrainTest splits the synthetic data into training and test sets. Methods ShowData and ShowVector are used to display training and test data, and neural network weights.\n\nThe Main method (with a few minor edits to save space) begins by preparing to create the synthetic data:\n\nNext, the synthetic data is created:\n\nTo create the 1,000-item synthetic data set, helper method MakeAllData creates a local neural network with random weights and bias values. Then, random input values are generated, the output is computed by the local neural network using the random weights and bias values, and then output is converted to 1-of-N format.\n\nNext, the demo program splits the synthetic data into training and test sets using these statements:\n\nNext, the neural network is instantiated like so:\n\nThe neural network has four inputs (one for each feature) and three outputs (because the Y variable can be one of three categorical values). The choice of five hidden processing units for the neural network is the same as the number of hidden units used to generate the synthetic data, but finding a good number of hidden units in a realistic scenario requires trial and error. Next, the back-propagation parameter values are assigned with these statements:\n\nDetermining when to stop neural network training is a difficult problem. Here, using 1,000 iterations was arbitrary. The learning rate controls how much each weight and bias value can change in each update step. Larger values increase the speed of training at the risk of overshooting optimal weight values. The momentum rate helps prevent training from getting stuck with local, non-optimal weight values and also prevents oscillation where training never converges to stable values.\n\nTraining using back-propagation is accomplished with these statements:\n\nThe Train method stores the best weights and bias values found internally in the NeuralNetwork object, and also returns those values, serialized into a single result array. In a production environment you would likely save the model weights and bias values to a text file so they could be retrieved later, if necessary.\n\nThe demo program concludes by calculating the prediction accuracy of the neural network model:\n\nThe accuracy of the model on the test data gives you a very rough estimate of how accurate the model will be when presented with new data that has unknown output values. The accuracy of the model on the training data is useful to determine if model over-fitting has occurred. If the prediction accuracy of the model on the training data is significantly greater than the accuracy on the test data, then there's a strong likelihood that over-fitting has occurred and re-training with new parameter values is necessary.\n\nImplementing Back-Propagation Training\n\n In many areas of computer science, Wikipedia articles have become de facto standard references. This is somewhat true for the neural network back-propagation algorithm. A major hurdle for many software engineers when trying to understand back-propagation, is the Greek alphabet soup of symbols used. Figure 2 presents 11 major symbols used in the Wikipedia explanation of back-propagation. Bear with me here; back-propagation is a complex algorithm but once you see the code implementation, understanding these symbols isn't quite as difficult as it might first appear.\n\nThe definition of the back-propagation training method begins by allocating space for gradient values:\n\nBack-propagation is based on calculus partial derivatives. Each weight and bias value has an associated partial derivative. You can think of a partial derivative as a value that contains information about how much, and in what direction, a weight value must be adjusted to reduce error. The collection of all partial derivatives is called a gradient. However, for simplicity, each partial derivative is commonly just called a gradient."
    },
    {
        "link": "https://stackoverflow.com/questions/41025186/why-am-i-getting-a-outofrangeexception-when-running-backpropagation-with-accord",
        "document": "I am messing with the different deep learning algorithms in Accord.NET. I decided to do this with spectra data I had lying around. I PCA transform the data so that it is reduced to 10 data points, using Accord's statistics toolbox. then follow the tutorial to the letter:\n\nI checked the inputted data and it is in the correct double[][] format fr both inputs and outputs. I also checked the original app:https://github.com/primaryobjects/deep-learning And that worked perfectly, so I am struggling to see what simply changing the inputted data is messing up so much. Any help would be greatly appreciated. The error I am getting is:\n\nAdditional information: Index was outside the bounds of the array."
    }
]