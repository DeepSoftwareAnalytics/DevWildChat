[
    {
        "link": "https://stackoverflow.com/questions/56414083/linux-bare-system-calls-not-glibc",
        "document": "Your article is probably obsolete.\n\nIf you code in C, there is no reason to avoid using the syscalls(2) (notice the plural) as documented. Be also aware of the vdso(7). You could use some other C standard library than the (e.g. musl-libc, dietlibc, etc...) and you might (but that is not recommended) statically link it.\n\nYou might use syscall(2) (notice the singular) instead. I see no reason to do that, e.g. use read(2) or mmap(2) without .\n\nThe Assembly HowTo might be an interesting read (beware, it might be too 32 bits centric, most Linux PCs today are 64 bits x86-64).\n\nBTW, some old Unixes (e.g. Solaris) had a providing just the syscalls, and their linked to it. I would like a too! But on current Linux systems, it does not really matter, since almost every process (running some dynamically linked ELF executable) is mmap(2)-ing, after ld-linux.so(8), several segments and sections of your ; for details, read Drepper's How to write a shared library (since it also explains in details how shared libraries actually work). Use also pmap(1) on some running process (e.g. in a shell).\n\nSome rare syscalls (e.g. userfaultfd(2) today 2Q2019) are not known by the . They are an exception, because most system calls are wrapped by your libc (the wrapping usually just deals with errno(3) setting on failure). Be aware of strace(1).\n\nAnd you also should read Operating Systems: Three Easy Pieces (it is a freely downloadable book, explaining the role of, and reason for, system calls)"
    },
    {
        "link": "https://stackoverflow.com/questions/28138720/minimal-c-library-with-only-linux-system-calls",
        "document": "I'm working on embedded systems using Linux as operating system. Those systems don't include dynamic linker, so all libraries must be included statically. To ensure small binary sizes I need an small C library. I know that there are many options, but my code only uses system calls, no standard C library calls, POSIX or any other extensions. Using a complete and general purpose library is an overkill.\n\nI can write this library in assembly, but my target hardware isn't homogeneous. I need some level of portability between architectures (ARM, i386 and x86_64) and versions of Linux kernels.\n\nAre there any small footprint C library that only includes Linux system calls?"
    },
    {
        "link": "https://gist.github.com/tcoppex/443d1dd45f873d96260195d6431b0989",
        "document": "You signed in with another tab or window. Reload to refresh your session.\n\nYou signed out in another tab or window. Reload to refresh your session.\n\nYou switched accounts on another tab or window. Reload to refresh your session."
    },
    {
        "link": "https://hero.handmade.network/forums/code-discussion/t/861-compiling_without_libc_on_linux",
        "document": "// returns negative value for error (for example, if error is EINVAL, then -EINVAL is returned) // for this example let's ignore result of write // but you should really handle it"
    },
    {
        "link": "https://reddit.com/r/C_Programming/comments/se3kgi/hello_world_without_libc",
        "document": "Yesterday I was a little bored and write a HelloWorld program in C without any libraries. Now I'm bored again and will post about it.\n\nCompiling a program without linking to libc is pretty trivial with gcc, just pass and you're set.\n\nI wrote this on my Linux machine which runs on a x86_64 CPU. In this case, this is important, because without libc to abstract this away, I had to get down to the nitty-gritty and make system calls myself using inline assembly. (This also means that my program is not cross-platform.)\n\nI wrote the following syscall-wrapper for write:\n\nIt puts the passed values into the corresponding syscall-argument-registers rax (the number of the syscall), rdi, rsi and rdx, and places the return value into the 'ret' variable.\n\nThen I wrote my main function and a quick 'strlen', because write expects the length of the buffer.\n\nAnd compiled, thinking I was ready to go, but ran into this error: . Then I remembered that ld doesn't really know 'main' to be the starting point of a C program. Libc actually defines '_start', which ld looks for and calls the user's 'main' in there.\n\nAnd voila, the words \"Hello, World!\" appeared on my screen ... quickly followed by . I remembered from experimenting with assembly that Linux expects a program to not just run out of instructions but call the 'exit' syscall, so I wrote that wrapper too:\n\nMy updated '_start' then looked like this:\n\nI compiled with and got the desired and a graceful exit.\n\nThis was a funny experiment and really showed me how much lives libc saves daily. Check out the code here!"
    },
    {
        "link": "https://man7.org/linux/man-pages/man2/pipe.2.html",
        "document": "Pages that refer to this page: eventfd(2), fork(2), getrlimit(2), ioctl_pipe(2), socketpair(2), statfs(2), syscall(2), syscalls(2), pmda(3), pmdaconnect(3), __pmprocesspipe(3), popen(3), capabilities(7), fifo(7), inode(7), man-pages(7), pipe(7), signal-safety(7)"
    },
    {
        "link": "https://linux.die.net/man/2/pipe",
        "document": ""
    },
    {
        "link": "https://man7.org/linux/man-pages/man2/syscalls.2.html",
        "document": "Pages that refer to this page: strace(1), intro(2), syscall(2), unimplemented(2), stapprobes(3stap), libc(7), man-pages(7), vdso(7)"
    },
    {
        "link": "https://commandlinux.com/man-page/man7/pipe.7.html",
        "document": "Section: Linux Programmer's Manual (7)Updated: 2014-07-08pipe - overview of pipes and FIFOs Pipes and FIFOs (also known as named pipes) provide a unidirectional interprocess communication channel. A pipe has aand a. Data written to the write end of a pipe can be read from the read end of the pipe.\n\nA pipe is created using pipe(2), which creates a new pipe and returns two file descriptors, one referring to the read end of the pipe, the other referring to the write end. Pipes can be used to create a communication channel between related processes; see pipe(2) for an example.\n\nA FIFO (short for First In First Out) has a name within the filesystem (created using mkfifo(3)), and is opened using open(2). Any process may open a FIFO, assuming the file permissions allow it. The read end is opened using the O_RDONLY flag; the write end is opened using the O_WRONLY flag. See fifo(7) for further details. Note: although FIFOs have a pathname in the filesystem, I/O on FIFOs does not involve operations on the underlying device (if there is one).\n\nThe only difference between pipes and FIFOs is the manner in which they are created and opened. Once these tasks have been accomplished, I/O on pipes and FIFOs has exactly the same semantics.\n\nIf a process attempts to read from an empty pipe, then read(2) will block until data is available. If a process attempts to write to a full pipe (see below), then write(2) blocks until sufficient data has been read from the pipe to allow the write to complete. Nonblocking I/O is possible by using the fcntl(2) F_SETFL operation to enable the O_NONBLOCK open file status flag.\n\nThe communication channel provided by a pipe is a byte stream: there is no concept of message boundaries.\n\nIf all file descriptors referring to the write end of a pipe have been closed, then an attempt to read(2) from the pipe will see end-of-file (read(2) will return 0). If all file descriptors referring to the read end of a pipe have been closed, then a write(2) will cause a SIGPIPE signal to be generated for the calling process. If the calling process is ignoring this signal, then write(2) fails with the error EPIPE. An application that uses pipe(2) and fork(2) should use suitable close(2) calls to close unnecessary duplicate file descriptors; this ensures that end-of-file and SIGPIPE/EPIPE are delivered when appropriate.\n\nIt is not possible to apply lseek(2) to a pipe.\n\nA pipe has a limited capacity. If the pipe is full, then a(2) will block or fail, depending on whether theflag is set (see below). Different implementations have different limits for the pipe capacity. Applications should not rely on a particular capacity: an application should be designed so that a reading process consumes data as soon as it is available, so that a writing process does not remain blocked.\n\nIn Linux versions before 2.6.11, the capacity of a pipe was the same as the system page size (e.g., 4096 bytes on i386). Since Linux 2.6.11, the pipe capacity is 65536 bytes. Since Linux 2.6.35, the default pipe capacity is 65536 bytes, but the capacity can be queried and set using the fcntl(2) F_GETPIPE_SZ and F_SETPIPE_SZ operations. See fcntl(2) for more information.\n\nPOSIX.1-2001 says that(2)s of less thanbytes must be atomic: the output data is written to the pipe as a contiguous sequence. Writes of more thanbytes may be nonatomic: the kernel may interleave the data with data written by other processes. POSIX.1-2001 requiresto be at least 512 bytes. (On Linux,is 4096 bytes.) The precise semantics depend on whether the file descriptor is nonblocking (), whether there are multiple writers to the pipe, and on, the number of bytes to be written:The only open file status flags that can be meaningfully applied to a pipe or FIFO areand\n\nSetting the O_ASYNC flag for the read end of a pipe causes a signal (SIGIO by default) to be generated when new input becomes available on the pipe (see fcntl(2) for details). On Linux, O_ASYNC is supported for pipes and FIFOs only since kernel 2.6.\n\nOn some systems (but not Linux), pipes are bidirectional: data can be transmitted in both directions between the pipe ends. According to POSIX.1-2001, pipes only need to be unidirectional. Portable applications should avoid reliance on bidirectional pipe semantics. (2),(2),(2),(2),(2),(2),(2),(2),(3),(7),(7) This page is part of release 3.74 of the Linuxproject. A description of the project, information about reporting bugs, and the latest version of this page, can be found at http://www.kernel.org/doc/man-pages/."
    },
    {
        "link": "https://en.wikipedia.org/wiki/Pipeline_(Unix)",
        "document": "In Unix-like computer operating systems, a pipeline is a mechanism for inter-process communication using message passing. A pipeline is a set of processes chained together by their standard streams, so that the output text of each process (stdout) is passed directly as input (stdin) to the next one. The second process is started as the first process is still executing, and they are executed concurrently.\n\nThe concept of pipelines was championed by Douglas McIlroy at Unix's ancestral home of Bell Labs, during the development of Unix, shaping its toolbox philosophy. It is named by analogy to a physical pipeline. A key feature of these pipelines is their \"hiding of internals\". This in turn allows for more clarity and simplicity in the system.\n\nThe pipes in the pipeline are anonymous pipes (as opposed to named pipes), where data written by one process is buffered by the operating system until it is read by the next process, and this uni-directional channel disappears when the processes are completed. The standard shell syntax for anonymous pipes is to list multiple commands, separated by vertical bars (\"pipes\" in common Unix verbiage).\n\nThe pipeline concept was invented by Douglas McIlroy[1] and first described in the man pages of Version 3 Unix.[2][3] McIlroy noticed that much of the time command shells passed the output file from one program as input to another. The concept of pipelines was championed by Douglas McIlroy at Unix's ancestral home of Bell Labs, during the development of Unix, shaping its toolbox philosophy.[4][5]\n\nHis ideas were implemented in 1973 when (\"in one feverish night\", wrote McIlroy) Ken Thompson added the system call and pipes to the shell and several utilities in Version 3 Unix. \"The next day\", McIlroy continued, \"saw an unforgettable orgy of one-liners as everybody joined in the excitement of plumbing.\" McIlroy also credits Thompson with the notation, which greatly simplified the description of pipe syntax in Version 4.[6][2]\n\nAlthough developed independently, Unix pipes are related to, and were preceded by, the 'communication files' developed by Ken Lochner [7] in the 1960s for the Dartmouth Time-Sharing System.[8]\n\nThis feature of Unix was borrowed by other operating systems, such as MS-DOS and the CMS Pipelines package on VM/CMS and MVS, and eventually came to be designated the pipes and filters design pattern of software engineering.\n\nIn Tony Hoare's communicating sequential processes (CSP), McIlroy's pipes are further developed.[9]\n\nA pipeline mechanism is used for inter-process communication using message passing. A pipeline is a set of processes chained together by their standard streams, so that the output text of each process (stdout) is passed directly as input (stdin) to the next one. The second process is started as the first process is still executing, and they are executed concurrently. It is named by analogy to a physical pipeline. A key feature of these pipelines is their \"hiding of internals\".[10] This in turn allows for more clarity and simplicity in the system.\n\nIn most Unix-like systems, all processes of a pipeline are started at the same time, with their streams appropriately connected, and managed by the scheduler together with all other processes running on the machine. An important aspect of this, setting Unix pipes apart from other pipe implementations, is the concept of buffering: for example a sending program may produce 5000 bytes per second, and a receiving program may only be able to accept 100 bytes per second, but no data is lost. Instead, the output of the sending program is held in the buffer. When the receiving program is ready to read data, the next program in the pipeline reads from the buffer. If the buffer is filled, the sending program is stopped (blocked) until at least some data is removed from the buffer by the receiver. In Linux, the size of the buffer is 65,536 bytes (64KiB). An open source third-party filter called bfr is available to provide larger buffers if required.\n\nTools like netcat and socat can connect pipes to TCP/IP sockets.\n\nAll widely used Unix shells have a special syntax construct for the creation of pipelines. In all usage one writes the commands in sequence, separated by the ASCII vertical bar character (which, for this reason, is often called \"pipe character\"). The shell starts the processes and arranges for the necessary connections between their standard streams (including some amount of buffer storage).\n\nThe pipeline uses anonymous pipes. For anonymous pipes, data written by one process is buffered by the operating system until it is read by the next process, and this uni-directional channel disappears when the processes are completed; this differs from named pipes, where messages are passed to or from a pipe that is named by making it a file, and remains after the processes are completed. The standard shell syntax for anonymous pipes is to list multiple commands, separated by vertical bars (\"pipes\" in common Unix verbiage):\n\nFor example, to list files in the current directory (ls), retain only the lines of ls output containing the string \"key\" ( grep), and view the result in a scrolling page ( less), a user types the following into the command line of a terminal:\n\nThe command is executed as a process, the output (stdout) of which is piped to the input (stdin) of the process for ; and likewise for the process for . Each process takes input from the previous process and produces output for the next process via standard streams. Each tells the shell to connect the standard output of the command on the left to the standard input of the command on the right by an inter-process communication mechanism called an (anonymous) pipe, implemented in the operating system. Pipes are unidirectional; data flows through the pipeline from left to right.\n\nBelow is an example of a pipeline that implements a kind of spell checker for the web resource indicated by a URL. An explanation of what it does follows.\n• obtains the HTML contents of a web page (could use on some systems).\n• replaces all characters (from the web page's content) that are not spaces or letters, with spaces. (Newlines are preserved.)\n• changes all of the uppercase letters into lowercase and converts the spaces in the lines of text to newlines (each 'word' is now on a separate line).\n• includes only lines that contain at least one lowercase alphabetical character (removing any blank lines).\n• sorts the list of 'words' into alphabetical order, and the switch removes duplicates.\n• finds lines in common between two files, suppresses lines unique to the second file, and those that are common to both, leaving only those that are found only in the first file named. The in place of a filename causes to use its standard input (from the pipe line in this case). sorts the contents of the file alphabetically, as expects, and outputs the results to a temporary file (via process substitution), which reads. The result is a list of words (lines) that are not found in /usr/share/dict/words.\n• allows the user to page through the results.\n\nBy default, the standard error streams (\"stderr\") of the processes in a pipeline are not passed on through the pipe; instead, they are merged and directed to the console. However, many shells have additional syntax for changing this behavior. In the csh shell, for instance, using instead of signifies that the standard error stream should also be merged with the standard output and fed to the next process. The Bash shell can also merge standard error with since version 4.0[11] or using , as well as redirect it to a different file.\n\nIn the most commonly used simple pipelines the shell connects a series of sub-processes via pipes, and executes external commands within each sub-process. Thus the shell itself is doing no direct processing of the data flowing through the pipeline.\n\nHowever, it's possible for the shell to perform processing directly, using a so-called mill or pipemill (since a command is used to \"mill\" over the results from the initial command). This construct generally looks something like:\n\nSuch pipemill may not perform as intended if the body of the loop includes commands, such as and , that read from :[12] on the loop's first iteration, such a program (let's call it the drain) will read the remaining output from , and the loop will then terminate (with results depending on the specifics of the drain). There are a couple of possible ways to avoid this behavior. First, some drains support an option to disable reading from (e.g. ). Alternatively, if the drain does not need to read any input from to do something useful, it can be given as input.\n\nAs all components of a pipe are run in parallel, a shell typically forks a subprocess (a subshell) to handle its contents, making it impossible to propagate variable changes to the outside shell environment. To remedy this issue, the \"pipemill\" can instead be fed from a here document containing a command substitution, which waits for the pipeline to finish running before milling through the contents. Alternatively, a named pipe or a process substitution can be used for parallel execution. GNU bash also has a option to disable forking for the last pipe component.[13]\n\nPipelines can be created under program control. The Unix system call asks the operating system to construct a new anonymous pipe object. This results in two new, opened file descriptors in the process: the read-only end of the pipe, and the write-only end. The pipe ends appear to be normal, anonymous file descriptors, except that they have no ability to seek.\n\nTo avoid deadlock and exploit parallelism, the Unix process with one or more new pipes will then, generally, call to create new processes. Each process will then close the end(s) of the pipe that it will not be using before producing or consuming any data. Alternatively, a process might create new threads and use the pipe to communicate between them.\n\nNamed pipes may also be created using or and then presented as the input or output file to programs as they are invoked. They allow multi-path pipes to be created, and are especially effective when combined with standard error redirection, or with .\n\nThe robot in the icon for Apple's Automator, which also uses a pipeline concept to chain repetitive commands together, holds a pipe in homage to the original Unix concept.\n• Everything is a file – describes one of the defining features of Unix; pipelines act on \"files\" in the Unix sense\n• History of Unix pipe notation Archived 2015-04-08 at the Wayback Machine\n• Doug McIlroy's original 1964 memo, proposing the concept of a pipe for the first time\n• : create an interprocess channel – System Interfaces Reference, The Single UNIX Specification, Version 4 from The Open Group\n• Pipes: A Brief Introduction by The Linux Information Project (LINFO)\n• Ad Hoc Data Analysis From The Unix Command Line at Wikibooks – Shows how to use pipelines composed of simple filters to do complex data analysis.\n• Use And Abuse Of Pipes With Audio Data – Gives an introduction to using and abusing pipes with netcat, nettee and fifos to play audio across a network."
    }
]