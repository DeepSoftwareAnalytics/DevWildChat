[
    {
        "link": "https://scikit-learn.org/stable/modules/naive_bayes.html",
        "document": "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. Bayes’ theorem states the following relationship, given class variable \\(y\\) and dependent feature vector \\(x_1\\) through \\(x_n\\), :\n\nUsing the naive conditional independence assumption that\n\nfor all \\(i\\), this relationship is simplified to\n\nSince \\(P(x_1, \\dots, x_n)\\) is constant given the input, we can use the following classification rule:\n\nand we can use Maximum A Posteriori (MAP) estimation to estimate \\(P(y)\\) and \\(P(x_i \\mid y)\\); the former is then the relative frequency of class \\(y\\) in the training set.\n\nThe different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of \\(P(x_i \\mid y)\\).\n\nIn spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters. (For theoretical reasons why naive Bayes works well, and on which types of data it does, see the references below.)\n\nNaive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.\n\nOn the flip side, although naive Bayes is known as a decent classifier, it is known to be a bad estimator, so the probability outputs from are not to be taken too seriously.\n\nimplements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors \\(\\theta_y = (\\theta_{y1},\\ldots,\\theta_{yn})\\) for each class \\(y\\), where \\(n\\) is the number of features (in text classification, the size of the vocabulary) and \\(\\theta_{yi}\\) is the probability \\(P(x_i \\mid y)\\) of feature \\(i\\) appearing in a sample belonging to class \\(y\\). The parameters \\(\\theta_y\\) is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting: where \\(N_{yi} = \\sum_{x \\in T} x_i\\) is the number of times feature \\(i\\) appears in all samples of class \\(y\\) in the training set \\(T\\), and \\(N_{y} = \\sum_{i=1}^{n} N_{yi}\\) is the total count of all features for class \\(y\\). The smoothing priors \\(\\alpha \\ge 0\\) accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting \\(\\alpha = 1\\) is called Laplace smoothing, while \\(\\alpha < 1\\) is called Lidstone smoothing.\n\nimplements the complement naive Bayes (CNB) algorithm. CNB is an adaptation of the standard multinomial naive Bayes (MNB) algorithm that is particularly suited for imbalanced data sets. Specifically, CNB uses statistics from the complement of each class to compute the model’s weights. The inventors of CNB show empirically that the parameter estimates for CNB are more stable than those for MNB. Further, CNB regularly outperforms MNB (often by a considerable margin) on text classification tasks. The procedure for calculating the weights is as follows: where the summations are over all documents \\(j\\) not in class \\(c\\), \\(d_{ij}\\) is either the count or tf-idf value of term \\(i\\) in document \\(j\\), \\(\\alpha_i\\) is a smoothing hyperparameter like that found in MNB, and \\(\\alpha = \\sum_{i} \\alpha_i\\). The second normalization addresses the tendency for longer documents to dominate parameter estimates in MNB. The classification rule is: i.e., a document is assigned to the class that is the poorest complement match.\n• None Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003). Tackling the poor assumptions of naive bayes text classifiers. In ICML (Vol. 3, pp. 616-623).\n\nimplements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. Therefore, this class requires samples to be represented as binary-valued feature vectors; if handed any other kind of data, a instance may binarize its input (depending on the parameter). The decision rule for Bernoulli naive Bayes is based on which differs from multinomial NB’s rule in that it explicitly penalizes the non-occurrence of a feature \\(i\\) that is an indicator for class \\(y\\), where the multinomial variant would simply ignore a non-occurring feature. In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier. might perform better on some datasets, especially those with shorter documents. It is advisable to evaluate both models, if time permits.\n• None A. McCallum and K. Nigam (1998). A comparison of event models for Naive Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.\n• None V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with Naive Bayes – Which Naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).\n\nimplements the categorical naive Bayes algorithm for categorically distributed data. It assumes that each feature, which is described by the index \\(i\\), has its own categorical distribution. For each feature \\(i\\) in the training set \\(X\\), estimates a categorical distribution for each feature i of X conditioned on the class y. The index set of the samples is defined as \\(J = \\{ 1, \\dots, m \\}\\), with \\(m\\) as the number of samples. The probability of category \\(t\\) in feature \\(i\\) given class \\(c\\) is estimated as: where \\(N_{tic} = |\\{j \\in J \\mid x_{ij} = t, y_j = c\\}|\\) is the number of times category \\(t\\) appears in the samples \\(x_{i}\\), which belong to class \\(c\\), \\(N_{c} = |\\{ j \\in J\\mid y_j = c\\}|\\) is the number of samples with class c, \\(\\alpha\\) is a smoothing parameter and \\(n_i\\) is the number of available categories of feature \\(i\\). assumes that the sample matrix \\(X\\) is encoded (for instance with the help of ) such that all categories for each feature \\(i\\) are represented with numbers \\(0, ..., n_i - 1\\) where \\(n_i\\) is the number of available categories of feature \\(i\\)."
    },
    {
        "link": "https://datacamp.com/tutorial/naive-bayes-scikit-learn",
        "document": "Master the basics of data analysis with Python in just four hours. This online course will introduce the Python interface and explore popular packages."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html",
        "document": "Can perform online updates to model parameters via . For details on algorithm used to update feature means and variance online, see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque.\n\nRead more in the User Guide.\n\nThis method is expected to be called several times consecutively on different chunks of a dataset so as to implement out-of-core or online learning. This is especially useful when the whole dataset is too big to fit in memory at once. This method has some performance and numerical stability overhead, hence it is better to call partial_fit on chunks of data that are as large as possible (as long as fitting in the memory budget) to hide the overhead. Training vectors, where is the number of samples and is the number of features. List of all the classes that can possibly appear in the y vector. Must be provided at the first call to partial_fit, can be omitted in subsequent calls.\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect.\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect.\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect."
    },
    {
        "link": "https://scikit-learn.org/stable/user_guide.html",
        "document": ""
    },
    {
        "link": "https://github.com/pb111/Naive-Bayes-Classification-Project/blob/master/Na%C3%AFve%20Bayes%20Classification%20with%20Python%20and%20Scikit-Learn.ipynb",
        "document": "To see all available qualifiers, see our documentation .\n\nSaved searches Use saved searches to filter your results more quickly\n\nWe read every piece of feedback, and take your input very seriously.\n\nYou signed in with another tab or window. Reload to refresh your session.\n\nYou signed out in another tab or window. Reload to refresh your session.\n\nYou switched accounts on another tab or window. Reload to refresh your session."
    },
    {
        "link": "https://quantstart.com/articles/training-the-perceptron-with-scikit-learn-and-tensorflow",
        "document": ""
    },
    {
        "link": "https://geeksforgeeks.org/multi-layer-perceptron-learning-in-tensorflow",
        "document": "Multi-Layer Perceptron (MLP) is an artificial neural network widely used for solving classification and regression tasks.\n\nMLP consists of fully connected dense layers that transform input data from one dimension to another. It is called “multi-layer” because it contains an input layer, one or more hidden layers, and an output layer. The purpose of an MLP is to model complex relationships between inputs and outputs, making it a powerful tool for various machine learning tasks.\n• Input Layer : Each neuron (or node) in this layer corresponds to an input feature. For instance, if you have three input features, the input layer will have three neurons.\n• Hidden Layers : An MLP can have any number of hidden layers, with each layer containing any number of nodes. These layers process the information received from the input layer.\n• Output Layer : The output layer generates the final prediction or result. If there are multiple outputs, the output layer will have a corresponding number of neurons.\n\nEvery connection in the diagram is a representation of the fully connected nature of an MLP. This means that every node in one layer connects to every node in the next layer. As the data moves through the network, each layer transforms it until the final output is generated in the output layer.\n\nLet’s delve in to the working of the multi-layer perceptron. The key mechanisms such as forward propagation, loss function, backpropagation, and optimization.\n\nIn forward propagation, the data flows from the input layer to the output layer, passing through any hidden layers. Each neuron in the hidden layers processes the input as follows:\n• Weighted Sum : The neuron computes the weighted sum of the inputs:\n• [Tex]z = \\sum_{i} w_i x_i + b[/Tex]\n• None\n• [Tex]w_i[/Tex] ​ is the corresponding weight.\n• Activation Function : The weighted sum z is passed through an activation function to introduce non-linearity. Common activation functions include:\n\nOnce the network generates an output, the next step is to calculate the loss using a loss function. In supervised learning, this compares the predicted output to the actual label.\n\nFor a classification problem, the commonly used binary cross-entropy loss function is:\n• [Tex]N[/Tex] is the number of samples.\n\nFor regression problems, the mean squared error (MSE) is often used:\n\nThe goal of training an MLP is to minimize the loss function by adjusting the network’s weights and biases. This is achieved through backpropagation:\n• Gradient Calculation : The gradients of the loss function with respect to each weight and bias are calculated using the chain rule of calculus.\n• Error Propagation : The error is propagated back through the network, layer by layer.\n• Gradient Descent : The network updates the weights and biases by moving in the opposite direction of the gradient to reduce the loss: [Tex]w = w – \\eta \\cdot \\frac{\\partial L}{\\partial w}[/Tex]\n• None\n• [Tex]\\frac{\\partial L}{\\partial w}​[/Tex] is the gradient of the loss function with respect to the weight.\n\nMLPs rely on optimization algorithms to iteratively refine the weights and biases during training. Popular optimization methods include:\n• Stochastic Gradient Descent (SGD) : Updates the weights based on a single sample or a small batch of data: [Tex]w = w – \\eta \\cdot \\frac{\\partial L}{\\partial w}[/Tex]\n• Adam Optimizer : An extension of SGD that incorporates momentum and adaptive learning rates for more efficient training:\n\nNow that we are done with the theory part of multi-layer perception, let’s go ahead and implement some code in python using the TensorFlow library.\n\nIn this section, we will guide through building a neural network using TensorFlow.\n\nFirst, we import necessary libraries such as TensorFlow, NumPy, and Matplotlib for visualizing the data. We also load the MNIST dataset.\n\nNext, we normalize the image data by dividing by 255 (since pixel values range from 0 to 255), which helps in faster convergence during training.\n\nTo understand the data better, we plot the first 100 training samples, each representing a digit.\n\nHere, we build a Sequential neural network model. The model consists of:\n• Dense Layers : Fully connected layers with 256 and 128 neurons, both using the relu activation function.\n• Output Layer : The final layer with 10 neurons representing the 10 classes of digits (0-9) with sigmoid\n\nOnce the model is defined, we compile it by specifying:\n• Loss Function : Sparse categorical crossentropy, which is suitable for multi-class classification.\n\nWe train the model on the training data using 10 epochs and a batch size of 2000. We also use 20% of the training data for validation to monitor the model’s performance on unseen data during training.\n\nAfter training, we evaluate the model on the test dataset to determine its performance.\n\nWe got the accuracy of our model 92% by using model.evaluate() on the test samples.\n\nThe model is learning effectively on the training set, but the validation accuracy and loss levels off, which might indicate that the model is starting to overfit (where it performs well on training data but not as well on unseen data).\n• Versatility : MLPs can be applied to a variety of problems, both classification and regression.\n• Non-linearity : Thanks to activation functions, MLPs can model complex, non-linear relationships in data.\n• Parallel Computation : With the help of GPUs, MLPs can be trained quickly by taking advantage of parallel computing.\n• Computationally Expensive : MLPs can be slow to train, especially on large datasets with many layers.\n• Prone to Overfitting : Without proper regularization techniques, MLPs can overfit the training data, leading to poor generalization.\n\nThe Multilayer Perceptron has the ability to learn complex patterns from data makes it a valuable tool in machine learning. Whether you’re working with structured data, images, or text, understanding how MLP works can open doors to solving a wide range of problems.\n\nWhat is the function of MLP?\n\nWhat are the applications of MLP?\n\nWhat are the advantages of multi layer perceptron?"
    },
    {
        "link": "https://tensorflow.org/model_optimization/guide",
        "document": "Save and categorize content based on your preferences.\n\nStay organized with collections Save and categorize content based on your preferences.\n\nThe TensorFlow Model Optimization Toolkit minimizes the complexity of optimizing machine learning inference.\n\nInference efficiency is a critical concern when deploying machine learning models because of latency, memory utilization, and in many cases power consumption. Particularly on edge devices, such as mobile and Internet of Things (IoT), resources are further constrained, and model size and efficiency of computation become a major concern.\n\nComputational demand for training grows with the number of models trained on different architectures, whereas the computational demand for inference grows in proportion to the number of users.\n\nModel optimization is useful, among other things, for:\n• Reducing latency and cost for inference for both cloud and edge devices (e.g. mobile, IoT).\n• Deploying models on edge devices with restrictions on processing, memory and/or power-consumption.\n\nThe area of model optimization can involve various techniques:\n• Update the original model topology to a more efficient one with reduced parameters or faster execution. For example, tensor decomposition methods and distillation\n\nOur toolkit supports post-training quantization, quantization aware training, pruning, and clustering. The toolkit also provides experimental support for collaborative optimization to combine various techniques.\n\nQuantized models are those where we represent the models with lower precision, such as 8-bit integers as opposed to 32-bit float. Lower precision is a requirement to leverage certain hardware.\n\nSparse models are those where connections in between operators (i.e. neural network layers) have been pruned, introducing zeros to the parameter tensors.\n\nClustered models are those where the original model's parameters are replaced with a smaller number of unique values.\n\nThe toolkit provides experimental support for collaborative optimization. This enables you to benefit from combining several model compression techniques and simultaneously achieve improved accuracy through quantization aware training."
    },
    {
        "link": "https://stackoverflow.com/questions/58137990/best-practices-in-tensorflow-2-0training-step",
        "document": "In tensorflow 2.0 you don't have to worry about training phase(batch size, number of epochs etc), because everything can be defined in method: .\n\nBut I have seen the following code style:\n\nSo here you can observe \"more detailed\" code, where you manually define by for loops you training procedure.\n\nI have following question: what is the best practice in Tensorflow 2.0? I haven't found a any complete tutorial."
    },
    {
        "link": "https://tensorflow.org/tutorials/customization/custom_training_walkthrough",
        "document": "This tutorial shows you how to train a machine learning model with a custom training loop to categorize penguins by species. In this notebook, you use TensorFlow to accomplish the following:\n• Use the trained model to make predictions\n\nThis tutorial demonstrates the following TensorFlow programming tasks:\n• Building models and layers with the Keras API\n\nImagine you are an ornithologist seeking an automated way to categorize each penguin you find. Machine learning provides many algorithms to classify penguins statistically. For instance, a sophisticated machine learning program could classify penguins based on photographs. The model you build in this tutorial is a little simpler. It classifies penguins based on their body weight, flipper length, and beaks, specifically the length and width measurements of their culmen.\n\nThere are 18 species of penguins, but in this tutorial you will only attempt to classify the following three:\n\nFortunately, a research team has already created and shared a dataset of 334 penguins with body weight, flipper length, beak measurements, and other data. This dataset is also conveniently available as the penguins TensorFlow Dataset.\n\nInstall the package for the penguins dataset. The package is the nightly released version of the TensorFlow Datasets (TFDS). For more information on TFDS, see TensorFlow Datasets overview.\n\nThen select Runtime > Restart Runtime from the Colab menu to restart the Colab runtime.\n\nDo not proceed with the rest of this tutorial without first restarting the runtime.\n\nImport TensorFlow and the other required Python modules.\n\nThe default penguins/processed TensorFlow Dataset is already cleaned, normalized, and ready for building a model. Before you download the processed data, preview a simplified version to get familiar with the original penguin survey data.\n\nDownload the simplified version of the penguins dataset ( ) using the TensorFlow Datasets method. There are 344 data records in this dataset. Extract the first five records into a object to inspect a sample of the values in this dataset:\n\nThe numbered rows are data records, one example per line, where:\n• The first six fields are features: these are the characteristics of an example. Here, the fields hold numbers representing penguin measurements.\n• The last column is the label: this is the value you want to predict. For this dataset, it's an integer value of 0, 1, or 2 that corresponds to a penguin species name.\n\nIn the dataset, the label for the penguin species is represented as a number to make it easier to work with in the model you are building. These numbers correspond to the following penguin species:\n\nCreate a list containing the penguin species names in this order. You will use this list to interpret the output of the classification model:\n\nFor more information about features and labels, refer to the ML Terminology section of the Machine Learning Crash Course.\n\nNow, download the preprocessed penguins dataset ( ) with the method, which returns a list of objects. Note that the dataset doesn't come with its own test set, so use an 80:20 split to slice the full dataset into the training and test sets. You will use the test dataset later to verify your model.\n\nNotice that this version of the dataset has been processed by reducing the data down to four normalized features and a species label. In this format, the data can be quickly used to train a model without further processing.\n\nYou can visualize some clusters by plotting a few features from the batch:\n\nA model is a relationship between features and the label. For the penguin classification problem, the model defines the relationship between the body mass, flipper and culmen measurements and the predicted penguin species. Some simple models can be described with a few lines of algebra, but complex machine learning models have a large number of parameters that are difficult to summarize.\n\nCould you determine the relationship between the four features and the penguin species without using machine learning? That is, could you use traditional programming techniques (for example, a lot of conditional statements) to create a model? Perhaps—if you analyzed the dataset long enough to determine the relationships between body mass and culmen measurements to a particular species. And this becomes difficult—maybe impossible—on more complicated datasets. A good machine learning approach determines the model for you. If you feed enough representative examples into the right machine learning model type, the program figures out the relationships for you.\n\nNext you need to select the kind of model to train. There are many types of models and picking a good one takes experience. This tutorial uses a neural network to solve the penguin classification problem. Neural networks can find complex relationships between features and the label. It is a highly-structured graph, organized into one or more hidden layers. Each hidden layer consists of one or more neurons. There are several categories of neural networks and this program uses a dense, or fully-connected neural network: the neurons in one layer receive input connections from every neuron in the previous layer. For example, Figure 2 illustrates a dense neural network consisting of an input layer, two hidden layers, and an output layer:\n\nWhen you train the model from Figure 2 and feed it an unlabeled example, it yields three predictions: the likelihood that this penguin is the given penguin species. This prediction is called inference. For this example, the sum of the output predictions is 1.0. In Figure 2, this prediction breaks down as: for Adelie, for Chinstrap, and for Gentoo species. This means that the model predicts—with 95% probability—that an unlabeled example penguin is a Chinstrap penguin.\n\nThe TensorFlow API is the preferred way to create models and layers. This makes it easy to build models and experiment while Keras handles the complexity of connecting everything together.\n\nThe model is a linear stack of layers. Its constructor takes a list of layer instances, in this case, two layers with 10 nodes each, and an output layer with 3 nodes representing your label predictions. The first layer's parameter corresponds to the number of features from the dataset, and is required:\n\nThe activation function determines the output shape of each node in the layer. These non-linearities are important—without them the model would be equivalent to a single layer. There are many , but ReLU is common for hidden layers.\n\nThe ideal number of hidden layers and neurons depends on the problem and the dataset. Like many aspects of machine learning, picking the best shape of the neural network requires a mixture of knowledge and experimentation. As a rule of thumb, increasing the number of hidden layers and neurons typically creates a more powerful model, which requires more data to train effectively.\n\nLet's have a quick look at what this model does to a batch of features:\n\nHere, each example returns a logit for each class.\n\nTo convert these logits to a probability for each class, use the softmax function:\n\nTaking the across classes gives us the predicted class index. But, the model hasn't been trained yet, so these aren't good predictions:\n\nTraining is the stage of machine learning when the model is gradually optimized, or the model learns the dataset. The goal is to learn enough about the structure of the training dataset to make predictions about unseen data. If you learn too much about the training dataset, then the predictions only work for the data it has seen and will not be generalizable. This problem is called overfitting—it's like memorizing the answers instead of understanding how to solve a problem.\n\nThe penguin classification problem is an example of supervised machine learning: the model is trained from examples that contain labels. In unsupervised machine learning, the examples don't contain labels. Instead, the model typically finds patterns among the features.\n\nBoth training and evaluation stages need to calculate the model's loss. This measures how off a model's predictions are from the desired label, in other words, how bad the model is performing. You want to minimize, or optimize, this value.\n\nYour model will calculate its loss using the function which takes the model's class probability predictions and the desired label, and returns the average loss across the examples.\n\nUse the context to calculate the gradients used to optimize your model:\n\nAn optimizer applies the computed gradients to the model's parameters to minimize the function. You can think of the loss function as a curved surface (refer to Figure 3) and you want to find its lowest point by walking around. The gradients point in the direction of steepest ascent—so you'll travel the opposite way and move down the hill. By iteratively calculating the loss and gradient for each batch, you'll adjust the model during training. Gradually, the model will find the best combination of weights and bias to minimize the loss. And the lower the loss, the better the model's predictions.\n\nTensorFlow has many optimization algorithms available for training. In this tutorial, you will use the that implements the stochastic gradient descent (SGD) algorithm. The parameter sets the step size to take for each iteration down the hill. This rate is a hyperparameter that you'll commonly adjust to achieve better results.\n\nInstantiate the optimizer with a learning rate of , a scalar value that is multiplied by the gradient at each iteration of the training:\n\nThen use this object to calculate a single optimization step:\n\nWith all the pieces in place, the model is ready for training! A training loop feeds the dataset examples into the model to help it make better predictions. The following code block sets up these training steps:\n• Iterate each epoch. An epoch is one pass through the dataset.\n• Within an epoch, iterate over each example in the training grabbing its features ( ) and label ( ).\n• Using the example's features, make a prediction and compare it with the label. Measure the inaccuracy of the prediction and use that to calculate the model's loss and gradients.\n• Use an to update the model's parameters.\n• Keep track of some stats for visualization.\n\nThe variable is the number of times to loop over the dataset collection. In the code below, is set to 201 which means this training loop will run 201 times. Counter-intuitively, training a model longer does not guarantee a better model. is a hyperparameter that you can tune. Choosing the right number usually requires both experience and experimentation:\n\nAlternatively, you could use the built-in Keras method to train your model.\n\nWhile it's helpful to print out the model's training progress, you can visualize the progress with TensorBoard - a visualization and metrics tool that is packaged with TensorFlow. For this simple example, you will create basic charts using the module.\n\nInterpreting these charts takes some experience, but in general you want to see the loss decrease and the accuracy increase:\n\nNow that the model is trained, you can get some statistics on its performance.\n\nEvaluating means determining how effectively the model makes predictions. To determine the model's effectiveness at penguin classification, pass some measurements to the model and ask the model to predict what penguin species they represent. Then compare the model's predictions against the actual label. For example, a model that picked the correct species on half the input examples has an accuracy of . Figure 4 shows a slightly more effective model, getting 4 out of 5 predictions correct at 80% accuracy:\n\nEvaluating the model is similar to training the model. The biggest difference is the examples come from a separate test set rather than the training set. To fairly assess a model's effectiveness, the examples used to evaluate a model must be different from the examples used to train the model.\n\nThe penguin dataset doesn't have a separate test dataset so in the previous Download the dataset section, you split the original dataset into test and train datasets. Use the dataset for the evaluation.\n\nEvaluate the model on the test dataset\n\nUnlike the training stage, the model only evaluates a single epoch of the test data. The following code iterates over each example in the test set and compare the model's prediction against the actual label. This comparison is used to measure the model's accuracy across the entire test set:\n\nYou can also use the keras function to get accuracy information on your test dataset.\n\nBy inspecting the last batch, for example, you can observe that the model predictions are usually correct.\n\nUse the trained model to make predictions\n\nYou've trained a model and \"proven\" that it's good—but not perfect—at classifying penguin species. Now let's use the trained model to make some predictions on unlabeled examples; that is, on examples that contain features but not labels.\n\nIn real-life, the unlabeled examples could come from lots of different sources including apps, CSV files, and data feeds. For this tutorial, manually provide three unlabeled examples to predict their labels. Recall, the label numbers are mapped to a named representation as:"
    },
    {
        "link": "https://mathworks.com/help/stats/classify.html",
        "document": ""
    },
    {
        "link": "https://mathworks.com/matlabcentral/fileexchange/38950-fisher-linear-dicriminant-analysis",
        "document": ""
    },
    {
        "link": "https://mathworks.com/help/stats/discriminant-analysis.html",
        "document": ""
    },
    {
        "link": "https://mathworks.com/help/stats/fitcdiscr.html",
        "document": ""
    },
    {
        "link": "https://mathworks.com/help/stats/classification-example.html",
        "document": ""
    }
]