[
    {
        "link": "https://tensorflow.org/guide/keras/sequential_model",
        "document": "Save and categorize content based on your preferences.\n\nStay organized with collections Save and categorize content based on your preferences.\n\nWhen to use a Sequential model\n\nA model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n\nis equivalent to this function:\n\nA Sequential model is not appropriate when:\n• Your model has multiple inputs or multiple outputs\n• Any of your layers has multiple inputs or multiple outputs\n• You need to do layer sharing\n\nYou can create a Sequential model by passing a list of layers to the Sequential constructor:\n\nIts layers are accessible via the attribute:\n\nYou can also create a Sequential model incrementally via the method:\n\nNote that there's also a corresponding method to remove layers: a Sequential model behaves very much like a list of layers.\n\nAlso note that the Sequential constructor accepts a argument, just like any layer or model in Keras. This is useful to annotate TensorBoard graphs with semantically meaningful names.\n\nSpecifying the input shape in advance\n\nGenerally, all layers in Keras need to know the shape of their inputs in order to be able to create their weights. So when you create a layer like this, initially, it has no weights:\n\nIt creates its weights the first time it is called on an input, since the shape of the weights depends on the shape of the inputs:\n\nNaturally, this also applies to Sequential models. When you instantiate a Sequential model without an input shape, it isn't \"built\": it has no weights (and calling results in an error stating just this). The weights are created when the model first sees some input data:\n\nOnce a model is \"built\", you can call its method to display its contents:\n\nHowever, it can be very useful when building a Sequential model incrementally to be able to display the summary of the model so far, including the current output shape. In this case, you should start your model by passing an object to your model, so that it knows its input shape from the start:\n\nNote that the object is not displayed as part of , since it isn't a layer:\n\nA simple alternative is to just pass an argument to your first layer:\n\nModels built with a predefined input shape like this always have weights (even before seeing any data) and always have a defined output shape.\n\nIn general, it's a recommended best practice to always specify the input shape of a Sequential model in advance if you know what it is.\n\nWhen building a new Sequential architecture, it's useful to incrementally stack layers with and frequently print model summaries. For instance, this enables you to monitor how a stack of and layers is downsampling image feature maps:\n\nWhat to do once you have a model\n\nOnce your model architecture is ready, you will want to:\n• Train your model, evaluate it, and run inference. See our guide to training & evaluation with the built-in loops\n• Save your model to disk and restore it. See our guide to serialization & saving.\n• Speed up model training by leveraging multiple GPUs. See our guide to multi-GPU and distributed training.\n\nOnce a Sequential model has been built, it behaves like a Functional API model. This means that every layer has an and attribute. These attributes can be used to do neat things, like quickly creating a model that extracts the outputs of all intermediate layers in a Sequential model:\n\nHere's a similar example that only extract features from one layer:\n\nTransfer learning consists of freezing the bottom layers in a model and only training the top layers. If you aren't familiar with it, make sure to read our guide to transfer learning.\n\nHere are two common transfer learning blueprint involving Sequential models.\n\nFirst, let's say that you have a Sequential model, and you want to freeze all layers except the last one. In this case, you would simply iterate over and set on each layer, except the last one. Like this:\n\nAnother common blueprint is to use a Sequential model to stack a pre-trained model and some freshly initialized classification layers. Like this:\n\nIf you do transfer learning, you will probably find yourself frequently using these two patterns.\n\nThat's about all you need to know about Sequential models!\n\nTo find out more about building models in Keras, see:\n• Guide to making new Layers & Models via subclassing"
    },
    {
        "link": "https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library",
        "document": "Keras is a Python library for deep learning that wraps the efficient numerical libraries TensorFlow and Theano.\n\nKeras allows you to quickly and simply design and train neural networks and deep learning models.\n\nIn this post, you will discover how to effectively use the Keras library in your machine learning project by working through a binary classification project step-by-step.\n\nAfter completing this tutorial, you will know:\n• How to load training data and make it available to Keras\n• How to design and train a neural network for tabular data\n• How to evaluate the performance of a neural network model in Keras on unseen data\n• How to perform data preparation to improve skill when using neural networks\n• How to tune the topology and configuration of neural networks in Keras\n\nKick-start your project with my new book Deep Learning With Python, including step-by-step tutorials and the Python source code files for all examples.\n\nThe dataset you will use in this tutorial is the Sonar dataset.\n\nThis is a dataset that describes sonar chirp returns bouncing off different services. The 60 input variables are the strength of the returns at different angles. It is a binary classification problem that requires a model to differentiate rocks from metal cylinders.\n\nYou can learn more about this dataset on the UCI Machine Learning repository. You can download the dataset for free and place it in your working directory with the filename sonar.csv.\n\nIt is a well-understood dataset. All the variables are continuous and generally in the range of 0 to 1. The output variable is a string “M” for mine and “R” for rock, which will need to be converted to integers 1 and 0.\n\nA benefit of using this dataset is that it is a standard benchmark problem. This means that we have some idea of the expected skill of a good model. Using cross-validation, a neural network should be able to achieve a performance of around 84% with an upper bound on accuracy for custom models at around 88%.\n\nLet’s create a baseline model and result for this problem.\n\nYou will start by importing all the classes and functions you will need.\n\nNow, you can load the dataset using pandas and split the columns into 60 input variables (X) and one output variable (Y). Use pandas to load the data because it easily handles strings (the output variable), whereas attempting to load the data directly using NumPy would be more difficult.\n\nThe output variable is string values. You must convert them into integer values 0 and 1.\n\nYou can do this using the LabelEncoder class from scikit-learn. This class will model the encoding required using the entire dataset via the fit() function, then apply the encoding to create a new output variable using the transform() function.\n\nYou are now ready to create your neural network model using Keras.\n\nYou will use scikit-learn to evaluate the model using stratified k-fold cross validation. This is a resampling technique that will provide an estimate of the performance of the model. It does this by splitting the data into k-parts and training the model on all parts except one, which is held out as a test set to evaluate the performance of the model. This process is repeated k-times, and the average score across all constructed models is used as a robust estimate of performance. It is stratified, meaning that it will look at the output values and attempt to balance the number of instances that belong to each class in the k-splits of the data.\n\nTo use Keras models with scikit-learn, you must use the KerasClassifier wrapper from the SciKeras module. This class takes a function that creates and returns our neural network model. It also takes arguments that it will pass along to the call to fit(), such as the number of epochs and the batch size.\n\nLet’s start by defining the function that creates your baseline model. Your model will have a single, fully connected hidden layer with the same number of neurons as input variables. This is a good default starting point when creating neural networks.\n\nThe weights are initialized using a small Gaussian random number. The Rectifier activation function is used. The output layer contains a single neuron in order to make predictions. It uses the sigmoid activation function in order to produce a probability output in the range of 0 to 1 that can easily and automatically be converted to crisp class values.\n\nFinally, you will use the logarithmic loss function (binary_crossentropy) during training, the preferred loss function for binary classification problems. The model also uses the efficient Adam optimization algorithm for gradient descent, and accuracy metrics will be collected when the model is trained.\n\nNow, it is time to evaluate this model using stratified cross validation in the scikit-learn framework.\n\nPass the number of training epochs to the KerasClassifier, again using reasonable default values. Verbose output is also turned off, given that the model will be created ten times for the 10-fold cross validation being performed.\n\nAfter tying this together, the complete example is listed below.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning this code produces the following output showing the mean and standard deviation of the estimated accuracy of the model on unseen data.\n\nThis is an excellent score without doing any hard work.\n\nIt is a good practice to prepare your data before modeling.\n\nNeural network models are especially suitable for having consistent input values, both in scale and distribution.\n\nStandardization is an effective data preparation scheme for tabular data when building neural network models. This is where the data is rescaled such that the mean value for each attribute is 0, and the standard deviation is 1. This preserves Gaussian and Gaussian-like distributions while normalizing the central tendencies for each attribute.\n\nYou can use scikit-learn to perform the standardization of your sonar dataset using the StandardScaler class.\n\nRather than performing the standardization on the entire dataset, it is good practice to train the standardization procedure on the training data within the pass of a cross-validation run and use the trained standardization to prepare the “unseen” test fold. This makes standardization a step in model preparation in the cross-validation process. It prevents the algorithm from having knowledge of “unseen” data during evaluation, knowledge that might be passed from the data preparation scheme like a crisper distribution.\n\nYou can achieve this in scikit-learn using a Pipeline. The pipeline is a wrapper that executes one or more models within a pass of the cross-validation procedure. Here, you can define a pipeline with the StandardScaler followed by your neural network model.\n\nAfter tying this together, the complete example is listed below.\n\nRunning this example provides the results below.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nYou now see a small but very nice lift in the mean accuracy.\n\n4. Tuning Layers and Number of Neurons in the Model\n\nThere are many things to tune on a neural network, such as weight initialization, activation functions, optimization procedure, and so on.\n\nOne aspect that may have an outsized effect is the structure of the network itself, called the network topology. In this section, you will look at two experiments on the structure of the network: making it smaller and making it larger.\n\nThese are good experiments to perform when tuning a neural network on your problem.\n\nNote that there is likely a lot of redundancy in the input variables for this problem.\n\nThe data describes the same signal from different angles. Perhaps some of those angles are more relevant than others. So you can force a type of feature extraction by the network by restricting the representational space in the first hidden layer.\n\nIn this experiment, you will take your baseline model with 60 neurons in the hidden layer and reduce it by half to 30. This will pressure the network during training to pick out the most important structure in the input data to model.\n\nYou will also standardize the data as in the previous experiment with data preparation and try to take advantage of the slight lift in performance.\n\nAfter tying this together, the complete example is listed below.\n\nRunning this example provides the following result. You can see that you have a very slight boost in the mean estimated accuracy and an important reduction in the standard deviation (average spread) of the accuracy scores for the model.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nThis is a great result because you are doing slightly better with a network half the size, which, in turn, takes half the time to train.\n\nA neural network topology with more layers offers more opportunities for the network to extract key features and recombine them in useful nonlinear ways.\n\nYou can easily evaluate whether adding more layers to the network improves the performance by making another small tweak to the function used to create our model. Here, you add one new layer (one line) to the network that introduces another hidden layer with 30 neurons after the first hidden layer.\n\nYour network now has the topology:\n\nThe idea here is that the network is given the opportunity to model all input variables before being bottlenecked and forced to halve the representational capacity, much like you did in the experiment above with the smaller network.\n\nInstead of squeezing the representation of the inputs themselves, you have an additional hidden layer to aid in the process.\n\nAfter tying this together, the complete example is listed below.\n\nRunning this example produces the results below.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nYou can see that you do not get a lift in the model performance. This may be statistical noise or a sign that further training is needed.\n\nWith further tuning of aspects like the optimization algorithm and the number of training epochs, it is expected that further improvements are possible. What is the best score that you can achieve on this dataset?\n\nIn this post, you discovered the Keras deep Learning library in Python.\n\nYou learned how you can work through a binary classification problem step-by-step with Keras, specifically:\n• How to load and prepare data for use in Keras\n• How to evaluate a Keras model using scikit-learn and stratified k-fold cross validation\n• How data preparation schemes can lift the performance of your models\n• How experiments adjusting the network topology can lift model performance\n\nDo you have any questions about deep learning with Keras or this post? Ask your questions in the comments, and I will do my best to answer."
    },
    {
        "link": "https://tensorflow.org/tutorials/keras/classification",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nThis guide trains a neural network model to classify images of clothing, like sneakers and shirts. It's okay if you don't understand all the details; this is a fast-paced overview of a complete TensorFlow program with the details explained as you go.\n\nThis guide uses tf.keras, a high-level API to build and train models in TensorFlow.\n\nThis guide uses the Fashion MNIST dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n\nFashion MNIST is intended as a drop-in replacement for the classic MNIST dataset—often used as the \"Hello, World\" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc.) in a format identical to that of the articles of clothing you'll use here.\n\nThis guide uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code.\n\nHere, 60,000 images are used to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow. Import and load the Fashion MNIST data directly from TensorFlow:\n• The and arrays are the training set—the data the model uses to learn.\n• The model is tested against the test set, the , and arrays.\n\nThe images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The labels are an array of integers, ranging from 0 to 9. These correspond to the class of clothing the image represents:\n\nEach image is mapped to a single label. Since the class names are not included with the dataset, store them here to use later when plotting the images:\n\nLet's explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, with each image represented as 28 x 28 pixels:\n\nLikewise, there are 60,000 labels in the training set:\n\nEach label is an integer between 0 and 9:\n\nThere are 10,000 images in the test set. Again, each image is represented as 28 x 28 pixels:\n\nAnd the test set contains 10,000 images labels:\n\nThe data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:\n\nScale these values to a range of 0 to 1 before feeding them to the neural network model. To do so, divide the values by 255. It's important that the training set and the testing set be preprocessed in the same way:\n\nTo verify that the data is in the correct format and that you're ready to build and train the network, let's display the first 25 images from the training set and display the class name below each image.\n\nBuilding the neural network requires configuring the layers of the model, then compiling the model.\n\nThe basic building block of a neural network is the layer. Layers extract representations from the data fed into them. Hopefully, these representations are meaningful for the problem at hand.\n\nMost of deep learning consists of chaining together simple layers. Most layers, such as , have parameters that are learned during training.\n\nThe first layer in this network, , transforms the format of the images from a two-dimensional array (of 28 by 28 pixels) to a one-dimensional array (of 28 * 28 = 784 pixels). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\n\nAfter the pixels are flattened, the network consists of a sequence of two layers. These are densely connected, or fully connected, neural layers. The first layer has 128 nodes (or neurons). The second (and last) layer returns a logits array with length of 10. Each node contains a score that indicates the current image belongs to one of the 10 classes.\n\nBefore the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n• Optimizer —This is how the model is updated based on the data it sees and its loss function.\n• Loss function —This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n• Metrics —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified.\n\nTraining the neural network model requires the following steps:\n• Feed the training data to the model. In this example, the training data is in the and arrays.\n• The model learns to associate images and labels.\n• You ask the model to make predictions about a test set—in this example, the array.\n• Verify that the predictions match the labels from the array.\n\nTo start training, call the method—so called because it \"fits\" the model to the training data:\n\nAs the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.91 (or 91%) on the training data.\n\nNext, compare how the model performs on the test dataset:\n\nIt turns out that the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy represents overfitting. Overfitting happens when a machine learning model performs worse on new, previously unseen inputs than it does on the training data. An overfitted model \"memorizes\" the noise and details in the training dataset to a point where it negatively impacts the performance of the model on the new data. For more information, see the following:\n\nWith the model trained, you can use it to make predictions about some images. Attach a softmax layer to convert the model's linear outputs—logits—to probabilities, which should be easier to interpret.\n\nHere, the model has predicted the label for each image in the testing set. Let's take a look at the first prediction:\n\nA prediction is an array of 10 numbers. They represent the model's \"confidence\" that the image corresponds to each of the 10 different articles of clothing. You can see which label has the highest confidence value:\n\nSo, the model is most confident that this image is an ankle boot, or . Examining the test label shows that this classification is correct:\n\nDefine functions to graph the full set of 10 class predictions.\n\nWith the model trained, you can use it to make predictions about some images.\n\nLet's look at the 0th image, predictions, and prediction array. Correct prediction labels are blue and incorrect prediction labels are red. The number gives the percentage (out of 100) for the predicted label.\n\nLet's plot several images with their predictions. Note that the model can be wrong even when very confident.\n\nFinally, use the trained model to make a prediction about a single image.\n\nmodels are optimized to make predictions on a batch, or collection, of examples at once. Accordingly, even though you're using a single image, you need to add it to a list:\n\nNow predict the correct label for this image:\n\nreturns a list of lists—one list for each image in the batch of data. Grab the predictions for our (only) image in the batch:\n\nAnd the model predicts a label as expected.\n\nTo learn more about building models with Keras, see the Keras guides.\n\n# Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the \"Software\"), # to deal in the Software without restriction, including without limitation # the rights to use, copy, modify, merge, publish, distribute, sublicense, # and/or sell copies of the Software, and to permit persons to whom the # Software is furnished to do so, subject to the following conditions: # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL # THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING # FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER"
    },
    {
        "link": "https://stackoverflow.com/questions/57867091/tf-keras-sequential-binary-classification-model-predicting-0-5-0-5-or-close-t",
        "document": "I am currently trying to build a model to classify whether or not the outcome of a given football match will be above or below 2.5 goals, based on the Home team, Away team & game league, using a model in TensorFlow 2.0RC.\n\nThe problem I am encountering is that my softmax results converge on when using the method. What makes this odd is that my validation & test accuracy and losses are about 0.94 & 0.12 respectively after 1000 epochs of training, otherwise I would have put this down to an overfitting problem. I am aware that 1000 epochs is extremely likely to overfit, however, I want to understand why my accuracy increases until about 800 epochs in. My loss flattens at about 300 epochs.\n\nI have tried to alter the number of layers, number of units in each layer, the activation functions, optimizers and loss functions, number of epochs and learning rates, but can only seem to increase the losses.\n\nThe results still seem to converge toward regardless.\n\nThe full code can be viewed at https://github.com/AhmUgEk/tensorflow_football_predictions, but below is an extract showing model composition.\n\nI would expect to receive a result of for example for an input of:\n\nbut as per the above, receive a result of say .\n\nAm I missing something obvious here?"
    },
    {
        "link": "https://deapsecure.gitlab.io/deapsecure-lesson04-nn/20-keras-intro/index.html",
        "document": "Keras is a high-level software framework for building, training, and deploying neural network (NN) models. Keras provides a high-level interface to another framework named TensorFlow, which implements the actual complex mathematical algorithms for NN training and inference. Keras offers an easy-to-use application programming interface (API) in the Python programming language. As we shall see shortly, expressing an NN model using Keras is quite intuitive; this makes Keras suitable for new users. They can focus on the structure of the network model without having to be concerned with low-level mathematical details.\n\nIn this episode, we will use Keras and the preprocessed dataset to build and train a simple NN model for a binary classification task, i.e. distinguishing two smartphone apps. This is an extremely simple model for such a powerful ML method, but with it we shall learn all the fundamentals of building, training, and validating an NN model. This fundamental knowledge and skillset will be applicable to all realistic and most complicated NN models.\n\nFirst, we need to import the necessary Python libraries. We will be using the sequential model as stated previously.\n\nThe Keras API is accessed via the Python module. Since Keras is quite a complex library, it is subdivided into many submodules. There are three basic Keras submodules that we should know:\n• contains the various layer objects that can be used to build NN models. represents the fully connected neuron layer, the most basic form of NN layers.\n• contains the model-focused objects and functions, including the and objects (representing the sequential and functional models, respectively), model training functions, and model saving/loading functions.\n• contains the various algorithms for training (optimizing) the Keras NN models.\n\nThese three submodules are interrelated and represent the three basic building blocks of a NN model in Keras: A Keras model consists of one or more layers that are arranged in a specific order; an optimizer defines an algorithm by which the model is trained according to the given training dataset.\n\nLet us load and prepare the data (features and labels) to train a classification NN model. The data are already preprocessed and only need to be split into training and testing datasets using , provided by scikit-learn:\n\nIn the split above, 80% of the data are used for training and 20% for validating (testing) the model.\n\nThere are two main ways we can programmatically build models in Keras, corresponding to two different APIs: sequential model API and functional model API.\n\nA sequential model creates models layer-by-layer, with the outputs of the previous layer connecting to the inputs of the subsequent layer. This easy-to-use API is useful to construct a straightforward NN model that has exactly one input data and one output data. (A single input could be a multi-dimensional array of values; similarly for the output.)\n• They cannot create multiple models that share layers\n• They cannot create models where layers have multiple inputs and outputs\n\nFigure: An example of a sequential NN model, visualized by the Keras API. This network may be trained to perform digit recognition by using the MNIST dataset (having 28x28 pixel input data and 10-category output data). Source: Keras Developer’s Guide.\n\nThe functional model provides a way to create arbitrarily complicated models that include shared layers, or layers with multiple inputs and/or outputs. In this lesson, we will focus on the sequential model. Once we understand how to build a network with the sequential model, it will be straightforward to learn the functional model.\n\nFigure: An example of a complex functional model from the Keras API. Source: Keras Developer’s Guide.\n\nA single layer of neurons is defined by creating a object. Let us create a single layer, with only one neuron, that takes in four input features and outputs one number (0 or 1, to indicate the app described by the input features):\n\nThe object declares a standard, fully-connected neuron layer, which can be a hidden layer or an output layer. The arguments have the following meaning:\n\nPlease refer to Keras documentation for the Dense layer for more information and additional parameters.\n\nKeras’ object defines an NN model that takes the form of a simple sequence of layers. The layers can be declared at the same time that the sequential model is constructed, as follows:\n\nIn this example, there is only one layer, which is the output layer.\n\nIn order to train the NN model we defined above, we need a few additional ingredients:\n• Loss function: quantifies the error of the network’s predictions when compared to the ground-truth labels in the training or validation data. The goal of the training phase is to minimize the loss function, i.e. to make the model predict the expected outcomes as accurately as possible.\n• Optimization algorithm: a predefined algorithm used to iteratively improve the model during the training phase. This will be explained further in this section.\n• Learning rate: a tuning hyperparameter in the optimization algorithm that determines the “step size” at each iteration while moving toward a minimum of the loss function.\n\nHow an Optimization Algorithm Works in Training a Neural Network Model Training an NN model involves progressively updating the weights of the neurons through backpropagation until the errors made by the model’s prediction are sufficiently minimized. These errors are quantified by a loss function, sometimes also called a cost function. Training (i.e. optimization) a neural network model is as much an art as it is science. The use of an appropriate optimization algorithm is critical for ensuring that we obtain the best model. When a new model is trained for the first time, the weights are typically initialized to random values. Among the various methods for optimizing NN models, gradient descent has become a general method of choice, optimizing an NN model by minimizing its error function. The following graphics illustrate how gradient descent works: Figure: An illustration of gradient descent for two-parameter optimization. Three optimization scenarios are indicated by the three black points and their tracks. Two points converge to the same terminal endpoint, but one converges to a different endpoint. The linked Wikimedia source includes animated graphics that demonstrate the progression of the optimization algorithm. Source: Jacopo Bertolotti, Wikimedia. Imagine a single-neuron model that has only two weight parameters (e.g. two inputs and no adjustable bias). The loss function, plotted as a function of these parameters, will look like the mountainous terrain shown in the picture above. The model will start with a set of parameters, represented as a tiny “dot” on this terrain. The slope (negative gradient) of the terrain plus the imaginary “downward gravity” will pull this “dot” along a certain trajectory until it meets the lowest point of a valley. Gradient descent is an iterative algorithm by which the “dot” is moved through successive steps until the lowest point is closely reached. This is, in a nutshell, how the gradient descent algorithm works! However, as the same illustration shows, not all the “dots” reach the same final point, as the parameters could get stuck in a higher valley, often called a local minimum. For this reason, it is often necessary to repeat the training process by starting at a different starting point to provide more confidence regarding the optimality of the trained model. In an actual network optimization scenario, the gradient descent algorithm must work in a high-dimensional space, according to the number of adjustable parameters in the network model. This can involve many millions, or even billions, of parameters. For example, the AlexNet model, which set the record for image classification accuracy in 2012, has over 62 million parameters. More recently, a powerful text-generation NN model called GPT-3, introduced in 2021, has over 175 billion parameters! The sheer dimensionality of the parameter space alone indicates that it can be very challenging to train an NN model correctly and optimally. There are many tricks to help reduce the amount of time needed to train a model. For example, weights saved from a previously trained model can often be used as the initial values to speed up the training. A detailed discussion on NN optimization algorithms is beyond the scope of this short training program; serious learners are advised to either take a reputable course on machine learning and deep learning or read a book on this subject. Some examples can be found in the reference section of this lesson.\n\nLet us now create an optimizer object, which encapsulates the optimization algorithm used to improve the network during the training process. The module contains many algorithms that can be used to train the network. Each algorithm has its own strengths and weaknesses that make it suitable for certain types of use cases.\n\nWe will be using the Adam optimizer, which was designed to effectively optimize neural networks in a wide range of scenarios:\n\nAdam is a specific variant of the gradient descent algorithm implementation and has recently seen broad adoption in many application areas, such as computer vision and natural language processing. An article by Jason Brownlee, Gentle Introduction to the Adam Optimization Algorithm for Deep Learning, provides a gentle introduction to the Adam optimizer. Interested readers can also learn more about this optimizer in a paper, “Adam: A Method for Stochastic Optimization”, written by Diederik P. Kingma and Jimmy Ba, the original inventors of this algorithm.\n\nThe learning rate (explained below) must be determined when the optimizer object is created. The Adam optimizer will automatically adjust the learning rate as the training progresses. The rate of this adjustment is determined by two hyperparameters: β and β . The following optional arguments to object creation can tweak the optimizer’s behavior:\n• : An exponential decay rate for the first moment estimates.\n• : An exponential decay rate for the second moment estimates.\n• : A boolean variable that controls whether to apply the AMSGrad variant of the optimizer. AMSGrad is a further refinement of Adam that is supposed to improve its convergence in some challenging cases.\n\nPlease consult Keras API for Adam optimizer for more information about the Keras API for the Adam optimizer.\n\nThe learning rate is a most important hyperparameter that controls how much the model parameters (neuron weights and biases) are changed according to the corrections estimated by backpropagation. The learning rate must be a real number between 0 and 1: 0 means that the model is not changed at all (i.e. ignoring changes estimated by backpropagation), whereas 1 means that an aggressive correction based on the backpropagation results is necessary. Typically, we use a value that is very small (closer to 0). For example, Keras’ default value for the learning rate is 0.001; we will start with a smaller value, which means a more conservative model update.\n\nIn choosing an appropriate learning rate, there is a trade-off between the rate of convergence (how fast the weights are changed from one iteration to the other) and the stability (whether the algorithm is making its way toward the lowest minimum of the loss function) of the algorithm. Additionally, a learning rate that is too small may lead to a suboptimal model because the parameters may become stuck in a local minimum. We must therefore find a “sweet spot” to obtain the most optimal model within a reasonable amount of time. For a more detailed explanation, see this article: Understand the Impact of Learning Rate on Neural Network Performance. We will revisit the issue of learning rate in a later episode.\n\nThe loss function determines how the error in the NN model’s prediction is quantified. Just like the optimizers, many kinds of loss functions are used in real applications, each tailored for a specific kind of machine learning task. There are two important functions that we must know when working with classification tasks:\n• Binary cross-entropy loss, designed for binary classification tasks such as the ML task at hand with the dataset.\n• Categorical cross-entropy loss, for multi-class classification tasks (i.e. classification involving more than two categories).\n\nThese loss functions are provided by Keras under the submodule. In Keras, the choice of the loss function is specified when we compile the NN model (see the argument of the function in the next section). The various loss functions provided by the Keras package can be found in Keras API documentation on Losses. The most frequently used loss functions and their specific applications are described briefly in this article: “Understanding Different Loss Functions for Neural Networks.\n\nCompiling the Model: Putting Them All Together\n\nAs a final step before training the model in Keras, we need to compile the model. The function of the model object ties three components together: the neural model itself, the optimizer, and the loss function. This step furnishes a workable model that can be trained and later used for inference. Here is the code:\n• : The optimizer object. We use , the Adam optimizer object already created earlier.\n• : The loss function object. See below for more details.\n• : The list of performance metrics to be evaluated and reported at the end of each epoch. The main metric we will use in this lesson is accuracy, although precision and recall may need to be considered for certain applications (e.g. malware detection).\n\nFor the loss function specification, Keras allows us to specify the function name as a string (as we did above) or the constructed object (i.e. ). The loss functions for classification problems can be specified in this way:\n\nCongratulations, now the model is truly ready for training! Let us now train the model and observe how well it performs.\n\nThe Keras NN model is trained by calling the function of the model object:\n\nThe key arguments to are as follows:\n• and : The features and labels, respectively, from the training dataset.\n• : The number of times (i.e. iterations) that the learning algorithm will work through the entire dataset.\n• : The number of samples from the training dataset to be processed at once (“as a single batch”) to compute the corrections to the network’s parameters.\n• : Dataset (features and labels) used to validate the model’s performance metrics. This validation is performed at the end of each epoch. This must be a 2-tuple containing the feature matrix ( ) and label arrays ( ), respectively. Obviously, this dataset must not overlap with the training dataset.\n\nHere is an example of the results of the training, printed by the function:\n\nKeras prints out a progress report at the end of every epoch (this behavior is determined by the argument to the function). Consider the first record printed, where the meaning of each reported field is as follows:\n• – The number of training sample batches processed (in this case, 15303 batches).\n• – The amount of time taken to complete this epoch (38 seconds).\n• – The loss function and accuracy of the NN model computed using the training data. This loss function is what is being minimized during the training.\n• – The loss function and accuracy of the NN model computed using the validation data. These metrics provide less unbiased estimates of the performance of the model.\n\nThe function’s output shows the results of training from 5 iterations or epochs. In each epoch, the training algorithm makes the model go through the all training data once, then adjusts the model parameters (neuron weights and biases) to better conform to the training data. As the result shows, each epoch took ~20 seconds to complete (this timing will vary based on the actual computer hardware used to run this training process). At the end, the loss function drops down to just below ~0.35, and the accuracy is ~0.85.\n\nInterestingly, Keras by default validates the model at the end of every epoch; therefore, we do not need to perform a separate validation.\n\nHow Did Our Model Perform?\n\nThe final accuracy of this model, according to the output printed above, is about 85% (the last value). The final loss function is just below 0.35, which did not drop much from the first value of 0.36. Considering that our model had only one neuron layer, which also serves as the output layer, this is actually not bad, though it is far from being a reliable model. Therefore, we should expect a meager accuracy outcome. The purpose of neural network modeling is to improve its performance by adding as many layers as possible to achieve the best result, within the bounds of what is computationally feasible.\n\nWe just finished constructing and testing our first NN model with Keras. It is instructive that we compare the performance results of our one-neuron model against the performance of traditional ML models.\n\nIn computer programming, writing functions is an important way to help us repeat a sequence of commands efficiently. A function is essentially a callable computer subprogram that may accept user-specified parameters to vary the actions of the subprogram’s behavior. Back to our NN modeling, it is very convenient to define a function to construct a ready-to-train NN model since we will use this many times in our experiments. As we shall see going forward, deep learning involves tedious trial-and-error runs. We will have to repeatedly modify, train, and validate our network models in order to find the most optimal one. As an exercise, let’s create a function to construct a one-neuron model that is ready to be trained to perform the binary classification task on the data. It must have four inputs and one output. Here’s the skeleton: After this function, one should be able to simply construct and train the Keras model in this way: Question 1: First, remind yourself: What are the required steps to include in the function above?\n• Compile the NN model to bring together the optimizer, loss function, and model evaluation metrics. Question 2: Now, write the contents (definition) of the function. Use the same choices as we used earlier for the optimizer, loss function, etc. Create a one-neuron binary classifier using Keras. `learning_rate` is the only adjustable hyperparameter here. Note: The importation of Keras objects ( , etc.) is optional, but this will allow the function to be used anywhere regardless of the availability of these objects as global symbols.\n\nIn this episode, we learned the basic mechanics of building and training a neural network model using Keras. We used a binary classification task to distinguish between only two apps: Facebook and WhatsApp. While this is a rather basic problem, we have learned a lot of important concepts that we must know to carry out a deep learning modeling.\n• Split the data into training and testing datasets;\n• Design the neural network model architecture (i.e. what types of layers to use, how many neurons per layer, and which activation function to choose);\n• Declare the neural network model using either a sequential or functional API;\n• Select an optimizer and determine its hyperparameters;\n• Fit the model using training data, and validate with testing data."
    },
    {
        "link": "https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_roc_curve_visualization_api.html",
        "document": "Go to the end to download the full example code. or to run this example in your browser via JupyterLite or Binder\n\nScikit-learn defines a simple API for creating visualizations for machine learning. The key features of this API is to allow for quick plotting and visual adjustments without recalculation. In this example, we will demonstrate how to use the visualization API by comparing ROC curves.\n\nFirst, we load the wine dataset and convert it to a binary classification problem. Then, we train a support vector classifier on a training dataset. In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. \n\nOn GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. \n\n \n\n\n\nNext, we plot the ROC curve with a single call to . The returned object allows us to continue using the already computed ROC curve for the SVC in future plots."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html",
        "document": "Compute confusion matrix to evaluate the accuracy of a classification.\n\nBy definition a confusion matrix \\(C\\) is such that \\(C_{i, j}\\) is equal to the number of observations known to be in group \\(i\\) and predicted to be in group \\(j\\).\n\nThus in binary classification, the count of true negatives is \\(C_{0,0}\\), false negatives is \\(C_{1,0}\\), true positives is \\(C_{1,1}\\) and false positives is \\(C_{0,1}\\).\n\nRead more in the User Guide.\n\nList of labels to index the matrix. This may be used to reorder or select a subset of labels. If is given, those that appear at least once in or are used in sorted order. Normalizes confusion matrix over the true (rows), predicted (columns) conditions or all the population. If None, confusion matrix will not be normalized. Confusion matrix whose i-th row and j-th column entry indicates the number of samples with true label being i-th class and predicted label being j-th class.\n\nIn the binary case, we can extract true positives, etc. as follows:"
    },
    {
        "link": "https://reddit.com/r/datascience/comments/1bln0ix/scikitlearn_visualization_guide_making_models",
        "document": "Use the Display API to replace complex Matplotlib code\n\nIn the journey of machine learning, explaining models with visualization is as important as training them.\n\nA good chart can show us what a model is doing in an easy-to-understand way. Here's an example:\n\nThis graph makes it clear that for the same dataset, the model on the right is better at generalizing.\n\nMost machine learning books prefer to use raw Matplotlib code for visualization, which leads to issues:\n• You have to learn a lot about drawing with Matplotlib.\n• Plotting code fills up your notebook, making it hard to read.\n• Sometimes you need third-party libraries, which isn't ideal in business settings.\n\nGood news! Scikit-learn now offers Display classes that let us use methods like from_estimator and from_predictions to make drawing graphs for different situations much easier.\n\nCurious? Let me show you these cool APIs.\n\nUse utils.discovery.all_displays to find available APIs\n\nScikit-learn (sklearn) always adds Display APIs in new releases, so it's key to know what's available in your version.\n\nSklearn's utils.discovery.all_displays lets you see which classes you can use.\n\nFor example, in my Scikit-learn 1.4.0, these classes are available:\n\nSince we mentioned it, let's start with decision boundaries.\n\nIf you use Matplotlib to draw them, it's a hassle:\n• Use plt.meshgrid to calculate the grid;\n• Use plt.contourf to draw the decision boundary fill;\n• Then use plt.scatter to plot data points.\n\nNow, with inspection.DecisionBoundaryDispla, you can simplify this process:\n\nSee the final effect in the figure:\n\nRemember, Display can only draw 2D, so make sure your data has only two features or reduced dimensions.\n\nTo compare classification models, probability calibration curves show how confident models are in their predictions.\n\nNote that CalibrationDisplay uses the model's predict_proba. If you use a support vector machine, set probability to True:\n\nWhen assessing classification models and dealing with imbalanced data, we look at precision and recall.\n\nThese break down into TP, FP, TN, and FN – a confusion matrix.\n\nTo draw one, use metrics.ConfusionMatrixDisplay. It's well-known, so I'll skip the details.\n\nThese two are together because they're often used to evaluate side by side.\n\nRocCurveDisplay compares TPR and FPR for the model.\n\nFor binary classification, you want low FPR and high TPR, so the upper left corner is best. The Roc curve bends towards this corner.\n\nBecause the Roc curve stays near the upper left, leaving the lower right empty, it's hard to see model differences.\n\nSo, we also use DetCurveDisplay to draw a Det curve with FNR and FPR. It uses more space, making it clearer than the Roc curve.\n\nThe perfect point for a Det curve is the lower left corner.\n\nWith imbalanced data, you might want to shift recall and precision.\n• For email fraud, you want high precision.\n• For disease screening, you want high recall to catch more cases.\n\nYou can adjust the threshold, but what's the right amount?\n\nHere, metrics.PrecisionRecallDisplay can help.\n\nThis shows that models following Scikit-learn's design can be drawn, like xgboost here. Handy, right?\n\nWe've talked about classification, now let's talk about regression.\n\nAs shown, it can draw two kinds of graphs. The left shows predicted vs. actual values – good for linear regression.\n\nHowever, not all data is perfectly linear. For that, use the right graph.\n\nThis plot's banana shape suggests our data might not fit linear regression.\n\nSwitching from a linear to an rbf kernel can help.\n\nSee, with rbf, the residual plot looks better.\n\nAfter assessing performance, let's look at optimization with LearningCurveDisplay.\n\nFirst up, learning curves – how well the model generalizes with different training and testing data, and if it suffers from variance or bias.\n\nAs shown below, we compare a DecisionTreeClassifier and a GradientBoostingClassifier to see how they do as training data changes.\n\nThe graph shows that although the tree-based GradientBoostingClassifier maintains good accuracy on the training data, its generalization capability on test data does not have a significant advantage over the DecisionTreeClassifier.\n\nSo, for models that don't generalize well, you might try adjusting the model's regularization parameters to tweak its performance.\n\nThe traditional approach is to use tools like GridSearchCV or Optuna to tune the model, but these methods only give you the overall best-performing model and the tuning process is not very intuitive.\n\nFor scenarios where you want to adjust a specific parameter to test its effect on the model, I recommend using model_selection.ValidationCurveDisplay to visualize how the model performs as the parameter changes.\n\nAfter trying out all these Displays, I must admit some regrets:\n• The biggest one is that most of these APIs lack detailed tutorials, which is probably why they're not well-known compared to Scikit-learn's thorough documentation.\n• These APIs are scattered across various packages, making it hard to reference them from a single place.\n• The code is still pretty basic. You often need to pair it with Matplotlib's APIs to get the job done. A typical example is DecisionBoundaryDisplay\n\n, where after plotting the decision boundary, you still need Matplotlib to plot the data distribution.\n• They're hard to extend. Besides a few methods validating parameters, it's tough to simplify my model visualization process with tools or methods; I end up rewriting a lot.\n\nI hope these APIs get more attention, and as versions upgrade, visualization APIs become even easier to use.\n\nIn the journey of machine learning, explaining models with visualization is as important as training them.\n\nThis article introduced various plotting APIs in the current version of scikit-learn.\n\nWith these APIs, you can simplify some Matplotlib code, ease your learning curve, and streamline your model evaluation process.\n\nDue to length, I didn't expand on each API. If interested, you can check the official documentation for more details.\n\nNow it's your turn. What are your expectations for visualizing machine learning methods? Feel free to leave a comment and discuss.\n\nThis article was originally published on my personal blog Data Leads Future."
    },
    {
        "link": "https://w3schools.com/python/python_ml_confusion_matrix.asp",
        "document": "On this page, W3schools.com collaborates with NYC Data Science Academy, to deliver digital training content to our students.\n\nIt is a table that is used in classification problems to assess where errors in the model were made.\n\nThe rows represent the actual classes the outcomes should have been. While the columns represent the predictions we have made. Using this table it is easy to see which predictions are wrong.\n\nConfusion matrixes can be created by predictions made from a logistic regression.\n\nFor now we will generate actual and predicted values by utilizing NumPy:\n\nNext we will need to generate the numbers for \"actual\" and \"predicted\" values.\n\nIn order to create the confusion matrix we need to import metrics from the sklearn module.\n\nOnce metrics is imported we can use the confusion matrix function on our actual and predicted values.\n\nTo create a more interpretable visual display we need to convert the table into a confusion matrix display.\n\nVizualizing the display requires that we import pyplot from matplotlib.\n\nFinally to display the plot we can use the functions plot() and show() from pyplot.\n\nSee the whole example in action:\n\nThe Confusion Matrix created has four different quadrants:\n\nTrue means that the values were accurately predicted, False means that there was an error or wrong prediction.\n\nNow that we have made a Confusion Matrix, we can calculate different measures to quantify the quality of the model. First, lets look at Accuracy.\n\nThe matrix provides us with many useful metrics that help us to evaluate our classification model.\n\nThe different measures include: Accuracy, Precision, Sensitivity (Recall), Specificity, and the F-score, explained below.\n\nAccuracy measures how often the model is correct.\n\nOf the positives predicted, what percentage is truly positive?\n\nPrecision does not evaluate the correctly predicted negative cases:\n\nOf all the positive cases, what percentage are predicted positive?\n\nSensitivity (sometimes called Recall) measures how good the model is at predicting positives.\n\nThis means it looks at true positives and false negatives (which are positives that have been incorrectly predicted as negative).\n\nSensitivity is good at understanding how well the model predicts something is positive:\n\nHow well the model is at prediciting negative results?\n\nSpecificity is similar to sensitivity, but looks at it from the persepctive of negative results.\n\nSince it is just the opposite of Recall, we use the recall_score function, taking the opposite position label:\n\nF-score is the \"harmonic mean\" of precision and sensitivity.\n\nIt considers both false positive and false negative cases and is good for imbalanced datasets.\n\nThis score does not take into consideration the True Negative values:\n\nAll calulations in one:"
    },
    {
        "link": "https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python",
        "document": "I am trying to plot a ROC curve to evaluate the accuracy of a prediction model I developed in Python using logistic regression packages. I have computed the true positive rate as well as the false positive rate; however, I am unable to figure out how to plot these correctly using matplotlib and calculate the AUC value. How could I do that?\n\nThis is the simplest way to plot an ROC curve, given a set of ground truth labels and predicted probabilities. Best part is, it plots the ROC curve for ALL classes, so you get multiple neat-looking curves as well import scikitplot as skplt import matplotlib.pyplot as plt y_true = # ground truth labels y_probas = # predicted probabilities generated by sklearn classifier skplt.metrics.plot_roc_curve(y_true, y_probas) plt.show() Here's a sample curve generated by plot_roc_curve. I used the sample digits dataset from scikit-learn so there are 10 classes. Notice that one ROC curve is plotted for each class. Disclaimer: Note that this uses the scikit-plot library, which I built.\n\nBased on multiple comments from stackoverflow, scikit-learn documentation and some other, I made a python package to plot ROC curve (and other metric) in a really simple way. To install package : (more info at the end of post) To plot a ROC Curve (example come from the documentation) : from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier(n_estimators=50, random_state=23) model = clf.fit(X_train, y_train) # Use predict_proba to predict probability of the class y_pred = clf.predict_proba(X_test)[:,1] You can now use plot_metric to plot ROC Curve : You can find more example of on the github and documentation of the package:\n\nThe previous answers assume that you indeed calculated TP/Sens yourself. It's a bad idea to do this manually, it's easy to make mistakes with the calculations, rather use a library function for all of this. the plot_roc function in scikit_lean does exactly what you need: http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html The essential part of the code is:\n\nThere is a library called metriculous that will do that for you: Let's first mock some data, this would usually come from the test dataset and the model(s): Now we can use metriculous to generate a table with various metrics and diagrams, including ROC curves: import metriculous metriculous.compare_classifiers( ground_truth=ground_truth, model_predictions=[perfect_model, noisy_model, random_model], model_names=[\"Perfect Model\", \"Noisy Model\", \"Random Model\"], class_names=class_names, one_vs_all_figures=True, # This line is important to include ROC curves in the output ).save_html(\"model_comparison.html\").display() The ROC curves in the output: The plots are zoomable and draggable, and you get further details when hovering with your mouse over the plot:\n\nI have made a simple function included in a package for the ROC curve. I just started practicing machine learning so please also let me know if this code has any problem! Have a look at the github readme file for more details! :) from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve import matplotlib.pyplot as plt import seaborn as sns import numpy as np def plot_ROC(y_train_true, y_train_prob, y_test_true, y_test_prob): ''' a funciton to plot the ROC curve for train labels and test labels. Use the best threshold found in train set to classify items in test set. ''' fpr_train, tpr_train, thresholds_train = roc_curve(y_train_true, y_train_prob, pos_label =True) sum_sensitivity_specificity_train = tpr_train + (1-fpr_train) best_threshold_id_train = np.argmax(sum_sensitivity_specificity_train) best_threshold = thresholds_train[best_threshold_id_train] best_fpr_train = fpr_train[best_threshold_id_train] best_tpr_train = tpr_train[best_threshold_id_train] y_train = y_train_prob > best_threshold cm_train = confusion_matrix(y_train_true, y_train) acc_train = accuracy_score(y_train_true, y_train) auc_train = roc_auc_score(y_train_true, y_train) print 'Train Accuracy: %s ' %acc_train print 'Train AUC: %s ' %auc_train print 'Train Confusion Matrix:' print cm_train fig = plt.figure(figsize=(10,5)) ax = fig.add_subplot(121) curve1 = ax.plot(fpr_train, tpr_train) curve2 = ax.plot([0, 1], [0, 1], color='navy', linestyle='--') dot = ax.plot(best_fpr_train, best_tpr_train, marker='o', color='black') ax.text(best_fpr_train, best_tpr_train, s = '(%.3f,%.3f)' %(best_fpr_train, best_tpr_train)) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.0]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('ROC curve (Train), AUC = %.4f'%auc_train) fpr_test, tpr_test, thresholds_test = roc_curve(y_test_true, y_test_prob, pos_label =True) y_test = y_test_prob > best_threshold cm_test = confusion_matrix(y_test_true, y_test) acc_test = accuracy_score(y_test_true, y_test) auc_test = roc_auc_score(y_test_true, y_test) print 'Test Accuracy: %s ' %acc_test print 'Test AUC: %s ' %auc_test print 'Test Confusion Matrix:' print cm_test tpr_score = float(cm_test[1][1])/(cm_test[1][1] + cm_test[1][0]) fpr_score = float(cm_test[0][1])/(cm_test[0][0]+ cm_test[0][1]) ax2 = fig.add_subplot(122) curve1 = ax2.plot(fpr_test, tpr_test) curve2 = ax2.plot([0, 1], [0, 1], color='navy', linestyle='--') dot = ax2.plot(fpr_score, tpr_score, marker='o', color='black') ax2.text(fpr_score, tpr_score, s = '(%.3f,%.3f)' %(fpr_score, tpr_score)) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.0]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('ROC curve (Test), AUC = %.4f'%auc_test) plt.savefig('ROC', dpi = 500) plt.show() return best_threshold\n\nWhen you need the probabilities as well... The following gets the AUC value and plots it all in one shot. When you have the probabilities... you can't get the auc value and plots in one shot. Do the following:\n\nIn my code, I have X_train and y_train and classes are 0 and 1. The method computes probabilities for both classes for every data point. I compare the probability of class1 with different values of threshold. probability = clf.predict_proba(X_train) def plot_roc(y_train, probability): threshold_values = np.linspace(0,1,100) #Threshold values range from 0 to 1 FPR_list = [] TPR_list = [] for threshold in threshold_values: #For every value of threshold y_pred = [] #Classify every data point in the test set #prob is an array consisting of 2 values - Probability of datapoint in Class0 and Class1. for prob in probability: if ((prob[1])<threshold): #Prob of class1 (positive class) y_pred.append(0) continue elif ((prob[1])>=threshold): y_pred.append(1) #Plot Confusion Matrix and Obtain values of TP, FP, TN, FN c_m = confusion_matrix(y, y_pred) TN = c_m[0][0] FP = c_m[0][1] FN = c_m[1][0] TP = c_m[1][1] FPR = FP/(FP + TN) #Obtain False Positive Rate TPR = TP/(TP + FN) #Obtain True Positive Rate FPR_list.append(FPR) TPR_list.append(TPR) fig = plt.figure() plt.plot(FPR_list, TPR_list) plt.ylabel('TPR') plt.xlabel('FPR') plt.show()"
    }
]