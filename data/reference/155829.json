[
    {
        "link": "https://threejs.org/docs/api/en/materials/ShaderMaterial.html",
        "document": "A material rendered with custom shaders. A shader is a small program written in [link:https://www.khronos.org/files/opengles_shading_language.pdf GLSL] that runs on the GPU. You may want to use a custom shader if you need to:\n• implement an effect not included with any of the built-in [page:Material materials]\n• combine many objects into a single [page:BufferGeometry] in order to improve performance\n• A `ShaderMaterial` will only be rendered properly by [page:WebGLRenderer], since the GLSL code in the [link:https://en.wikipedia.org/wiki/Shader#Vertex_shaders vertexShader] and [link:https://en.wikipedia.org/wiki/Shader#Pixel_shaders fragmentShader] properties must be compiled and run on the GPU using WebGL.\n• As of THREE r72, directly assigning attributes in a ShaderMaterial is no longer supported. A [page:BufferGeometry] instance must be used instead, using [page:BufferAttribute] instances to define custom attributes.\n• As of THREE r77, [page:WebGLRenderTarget] or [page:WebGLCubeRenderTarget] instances are no longer supposed to be used as uniforms. Their [page:Texture texture] property must be used instead.\n• Built in attributes and uniforms are passed to the shaders along with your code. If you don't want the [page:WebGLProgram] to add anything to your shader code, you can use [page:RawShaderMaterial] instead of this class.\n• You can use the directive #pragma unroll_loop_start and #pragma unroll_loop_end in order to unroll a `for` loop in GLSL by the shader preprocessor. The directive has to be placed right above the loop. The loop formatting has to correspond to a defined standard.\n• The loop has to be [link:https://en.wikipedia.org/wiki/Normalized_loop normalized].\n• The loop variable has to be *i*.\n• The value `UNROLLED_LOOP_INDEX` will be replaced with the explicitly value of *i* for the given iteration and can be used in preprocessor statements.\n\nYou can specify two different types of shaders for each material:\n• The vertex shader runs first; it receives `attributes`, calculates / manipulates the position of each individual vertex, and passes additional data (`varying`s) to the fragment shader.\n• The fragment ( or pixel ) shader runs second; it sets the color of each individual \"fragment\" (pixel) rendered to the screen. There are three types of variables in shaders: uniforms, attributes, and varyings:\n• `Uniforms` are variables that have the same value for all vertices - lighting, fog, and shadow maps are examples of data that would be stored in uniforms. Uniforms can be accessed by both the vertex shader and the fragment shader.\n• `Attributes` are variables associated with each vertex---for instance, the vertex position, face normal, and vertex color are all examples of data that would be stored in attributes. Attributes can `only` be accessed within the vertex shader.\n• `Varyings` are variables that are passed from the vertex shader to the fragment shader. For each fragment, the value of each varying will be smoothly interpolated from the values of adjacent vertices. Note that `within` the shader itself, uniforms and attributes act like constants; you can only modify their values by passing different values to the buffers from your JavaScript code.\n\nThe [page:WebGLRenderer] provides many attributes and uniforms to shaders by default; definitions of these variables are prepended to your `fragmentShader` and `vertexShader` code by the [page:WebGLProgram] when the shader is compiled; you don't need to declare them yourself. See [page:WebGLProgram] for details of these variables. Some of these uniforms or attributes (e.g. those pertaining lighting, fog, etc.) require properties to be set on the material in order for [page:WebGLRenderer] to copy the appropriate values to the GPU - make sure to set these flags if you want to use these features in your own shader. If you don't want [page:WebGLProgram] to add anything to your shader code, you can use [page:RawShaderMaterial] instead of this class.\n\nBoth custom attributes and uniforms must be declared in your GLSL shader code (within `vertexShader` and/or `fragmentShader`). Custom uniforms must be defined in `both` the `uniforms` property of your `ShaderMaterial`, whereas any custom attributes must be defined via [page:BufferAttribute] instances. Note that `varying`s only need to be declared within the shader code (not within the material). To declare a custom attribute, please reference the [page:BufferGeometry] page for an overview, and the [page:BufferAttribute] page for a detailed look at the `BufferAttribute` API. When creating your attributes, each typed array that you create to hold your attribute's data must be a multiple of your data type's size. For example, if your attribute is a [page:Vector3 THREE.Vector3] type, and you have 3000 vertices in your [page:BufferGeometry], your typed array value must be created with a length of 3000 * 3, or 9000 (one value per-component). A table of each data type's size is shown below for reference: Note that attribute buffers are `not` refreshed automatically when their values change. To update custom attributes, set the `needsUpdate` flag to true on the [page:BufferAttribute] of the geometry (see [page:BufferGeometry] for further details). To declare a custom [page:Uniform], use the `uniforms` property: uniforms: { time: { value: 1.0 }, resolution: { value: new THREE.Vector2() } } You're recommended to update custom [page:Uniform] values depending on [page:Object3D object] and [page:Camera camera] in [page:Object3D.onBeforeRender] because [page:Material] can be shared among [page:Mesh meshes], [page:Matrix4 matrixWorld] of [page:Scene] and [page:Camera] are updated in [page:WebGLRenderer.render], and some effects render a [page:Scene scene] with their own private [page:Camera cameras].\n\n[page:Object parameters] - (optional) an object with one or more properties defining the material's appearance. Any property of the material (including any property inherited from [page:Material]) can be passed in here.\n\nSee the base [page:Material] class for common properties.\n\nDefines whether this material supports clipping; true to let the renderer pass the clippingPlanes uniform. Default is false.\n\nWhen the rendered geometry doesn't include these attributes but the material does, these default values will be passed to the shaders. This avoids errors when buffer data is missing.\n\nDefines custom constants using `#define` directives within the GLSL code for both the vertex shader and the fragment shader; each key/value pair yields another directive: yields the lines in the GLSL code.\n\nAn object with the following properties: this.extensions = { clipCullDistance: false, // set to use vertex shader clipping multiDraw: false // set to use vertex shader multi_draw / enable gl_DrawID };\n\nDefine whether the material color is affected by global fog settings; true to pass fog uniforms to the shader. Default is false.\n\nFragment shader GLSL code. This is the actual code for the shader. In the example above, the `vertexShader` and `fragmentShader` code is extracted from the DOM; it could be passed as a string directly or loaded via AJAX instead.\n\nDefines the GLSL version of custom shader code. Valid values are `THREE.GLSL1` or `THREE.GLSL3`. Default is `null`.\n\nIf set, this calls [link:https://developer.mozilla.org/en-US/docs/Web/API/WebGLRenderingContext/bindAttribLocation gl.bindAttribLocation] to bind a generic vertex index to an attribute variable. Default is undefined.\n\nRead-only flag to check if a given object is of type [name].\n\nDefines whether this material uses lighting; true to pass uniform data related to lighting to this shader. Default is false.\n\nControls wireframe thickness. Default is `1`.\n\n\n\n Due to limitations of the [link:https://www.khronos.org/registry/OpenGL/specs/gl/glspec46.core.pdf OpenGL Core Profile] with the [page:WebGLRenderer WebGL] renderer on most platforms linewidth will always be `1` regardless of the set value.\n\nDefine whether the material is rendered with flat shading. Default is false.\n\nAn object of the form: specifying the uniforms to be passed to the shader code; keys are uniform names, values are definitions of the form where `value` is the value of the uniform. Names must match the name of the uniform, as defined in the GLSL code. Note that uniforms are refreshed on every frame, so updating the value of the uniform will immediately update the value available to the GLSL code.\n\nCan be used to force a uniform update while changing uniforms in [page:Object3D.onBeforeRender](). Default is `false`.\n\nDefines whether vertex coloring is used. Default is `false`.\n\nVertex shader GLSL code. This is the actual code for the shader. In the example above, the `vertexShader` and `fragmentShader` code is extracted from the DOM; it could be passed as a string directly or loaded via AJAX instead.\n\nRender geometry as wireframe (using GL_LINES instead of GL_TRIANGLES). Default is false (i.e. render as flat polygons).\n\nControls wireframe thickness. Default is `1`.\n\n\n\n Due to limitations of the [link:https://www.khronos.org/registry/OpenGL/specs/gl/glspec46.core.pdf OpenGL Core Profile] with the [page:WebGLRenderer WebGL] renderer on most platforms linewidth will always be `1` regardless of the set value.\n\nSee the base [page:Material] class for common methods.\n\nGenerates a shallow copy of this material. Note that the vertexShader and fragmentShader are copied `by reference`, as are the definitions of the `attributes`; this means that clones of the material will share the same compiled [page:WebGLProgram]. However, the `uniforms` are copied `by value`, which allows you to have different sets of uniforms for different copies of the material."
    },
    {
        "link": "https://dustinpfister.github.io/2023/01/13/threejs-shader-material",
        "document": "The Shader material in threejs is one way to go about getting started with custom shaders in threejs, the other general option would be to look into the raw shader material. The main difference between the two has to do with built-in uniforms and attributes when it comes to the starting state of the GLSL ( openGL Shader Language ) code. For this reason it might be best to start out with the Shader material rather than the raw shader material as there are some built in values that I will not have to worry about setting up myself when it comes to the raw shader material. Yet again it is a bit of a toss up with that as if one wants to learn a thing or two about GLSL alone then the raw material might prove to be a better starting point actually.\n\nIn any case the Shader material is what I am starting with, and that will be the main topic of this post today. Using the shader material alone is simple enough, but what is not so simple is coming up with custom GLSL code to use with this material. However one has to start somewhere so this post will start out with some very simply hello world style examples. Before moving on into one or more real examples when it comes to the topic of custom shaders which can end up getting pretty involved.\n\nThe Shader Material in threejs and what to know first\n\nThis is a post on getting started with making custom GLSL shaders in threejs by way of the THREE.ShaderMaterial class as well as many other core threejs features. This is then not in any way a kind of getting started post with threejs, client side javaScript, and many other skills that are needed before hand. Also on top of the usual set of skills that are needed for doing just about anything with threejs there is one new additional skill that one is going to need to start to scratch the surface with at least. As I mentioned in the opening of this post GLSL is what is used to write the code for these custom shaders. I will then not be getting into great detail about everything that you should know before hand here. However I do as always use this first opening section to outline some things that you might want to read up more on before reading the rest of this post.\n\nStart to learn at least a thing or two about GLSL\n\nIf you want to write custom shaders you will want to learn at least a little GLSL, and getting into every little thing about that may be outside the scope of this post. There is however learning just enough GLSL to start effectively hacking over shaders that have all ready been written though and that is more or less what I am doing in this post.\n\nStill there is the question of what is the Modzilla Developer Network equivalent site for GLSL? That is indeed a good question and thus far I can not say that I have found that. What I have found however is the khronos group pages that have provedn to be somewhat helpful. Much of what I have learned about GLSL though has been mainly just looking at the code that is used in the core of threejs itself as an example of core features of this language by looking at the source code files that compose the THREE.ShaderChunk library as well as just working with this Object in the process of creating a custom shader. Working closely with this object just strikes me as what the THREE.ShaderMaterial is all about anway.\n\nThere is learning a thing or two about the built in materials first of course\n\nIf you are still new to threejs, and also even if you do have a fair amount of experience there is still just sticking with the built in materials and moving on with life. Many of the material options will work just fine for just about all typical use case examples with threejs. It is just that there might be a few situations that call for writing a custom shader though as doing so with with built in materials will have to involve some kind of very complex trickery using canvas textures, so complex that it might be better to ditch that idea and just write a little GLSL code.\n\nBe aware of features that have to do with buffer geometry\n\nThere is a lot to be aware of when it comes to the state of buffer geometry as well of course. Once again this is very much an advanced post on threejs so I assume you know what a position attribute is, as well as many other such attributes both standard, and not so standard. There is not just getting into making custom shaders, but also custom attributes of geometry as well after all. In some cases I might want to create shaders that will make use of a color attribute, or one or more custom attributes that should be parked in the geometry.\n\nSource code examples are also on Github\n\nI also have the source code examples that I am writing about here up on my Github. This is also where I park the source code examples for the many other blog posts that I have wrote on threejs over the years as well.\n\nWhen I was first writing this post I was using r146 of threejs and thus I have followed the style rules I have set with that revision when making these. Threejs is still to this day a very fast moving project, and depending on how wide of a range of devices you want to target the latest is not always the greatest just yet. As of this writing there are a whole lot of code breaking changes up ahead. In any case when it comes to working out issues with code examples on the open web always be mindful of what version is being used on both ends.\n\n1 - Basic, hello world exmaples of THREE.ShaderMaterial in threejs\n\nThere is a lot to take in when it comes to even just simply getting started with the THREE.ShaderMaterial. However this is to be expected as this is without question one of the most, if not the most advanced topics there is when it comes to threejs. I have been working with threejs on and off for years and even I still find this to be a little intense for me, however I started taking a swing or two at it now and then because there is not much that remains when it comes to more ground to cover with threejs for me.\n\nAnyway not just for your sake, but very much for my own sake as well, this will be a collection of very basic, getting started type examples of custom shaders in threejs by way of THREE.ShaderMaterial. This might prove to be an easier alternative to that of the THREE.RawShaderMaterial but that will only help so much of course. Still I have found that if I just want to reproduce the functionally of the THREE.MeshBasicMatreial with just the color option, that is not so hard of a starting point. With that said these examples will just be focusing one various ways to go about doing that which will just result in a solid blob of color for the object rendered in the scene. Everything else should be something that I get to in more advanced sections.\n\n1.1 - Custom Shader hello world with gl_Position and gl_FragColor\n\nTo create an instance of the Shader material I will need to pass an object that contains three properties, unifroms, vertexShader, and fragmentShader. The uniforms property contains a set of values that will be the same for all vertices, for this getting started example I am dealing with just one uniform value that is a diffuse color. Each value of the uniforms object should itself be an object and the value property of this nested object is how to go about setting a value for the uniform value. The vertexShader and fragmentShader properties should both contain string values, and each string value should contain GLSL code for the vertex and fragment shaders that will provide the custom rendering logic.\n\nThe vertex shader runs first and the main job of this shader is to set the value of gl_Position. After the vertex shader there is the fragment shader there is the fragment shader which is what is used to set what the color should be for each pixel location. The main job of this fragment shader then is to set what the color should be for the gl_FragColor value.\n\nSo then the end result of this custom shader is that I have just a solid blob of color in the location of the mesh object that contains a Torus geometry. This is then just a very complex way to go about getting the same effect as just using the basic material and setting the color option to what I want for the solid diffuse color value. However there is of course expanding on this to do somersetting else that can not be done with the basic material, or any other material for that matter. It is just that this seems like a good starting point for me when it comes to getting into doing this sort of thing. However before getting into more advanced examples of this kind of custom material I will want to cover at least a few more options when it comes to just a basic getting started type point for this sort of thing.\n\nAlthough a big part of getting into this sort of thing is to write at least a little custom GLSL code, there are a lot of tools built into the core of threejs to help speed things along when using the THREE.ShaderMaterial class. One feature of threejs that I think I should cover right away that I have found is the THREE.ShaderChunk object. In this shader chunk object there is a meshbasic_vert, and meshbasic_frag properties that as you might guess contains GLSL code for the vertex shader, and fragment shader of the mesh basic material. So then another way to create a Shader material hello world would be to just use those values for the shaders.\n\nThat makes things for more concise, but it also totally defeats the purpose of bothering with the shader material because I could just use the basic material. Again the goal here is to just be aware fo what there is to work with to just get started with this sort of thing, and the THREE.ShaderChunk is just one thing to be aware of. Also in some cases I might want to start with functionality that is not all to different from that of the basic material, phong material, normal material and so forth. It is just that typically I will not want to do something like this, but rather start with the actual code rather than a reference to it as a start point.\n\nSo then in the last basic example I made use of the THREE.ShaderChunk object to create a material that is more or less about the same as the basic material. However I did so by just simply referencing the GLSL code. In this example I am doing the same thing, but now I am passing the GLSL code into the actually code of the example, rather than just referencing what is there. There is more than one way to do this however when it comes to the shader material #include can be used to import segements of GLSL code from the shader chunk library. This way if I just want to include the GLSL code that is baked into the core of threejs for somehting I can just go ahead and do so.\n\nNotice all the include statements that begin with a hashtag, followed by a name for what is some additional GLSL code? This is a way to include one or more parts of the THREE.ShaderChunk object in the GLSL code. So say that I quickly just want to add alpha map functionally to a material of mine. I could go threw the time consuming process of trying how to go about writing something like that from the ground up with GLSL code, but why other with that when I can just include what is baked into threejs to begin with?\n\nThis is still just a very complex way of just reproducing the functionally of the basic material, but the goal here was not to make a custom material to begin with, but rather to just get started with this. With these three examples I am not somewhat ready to start to move on to some real examples in which I am just hacking over and expanding from one of these kinds of start points.\n\n2 - Using Vertex Colors and a Base color to set frac color\n\nIn the basic section of this post I wrote a whole bunch of examples that do more or less the same thing, but in some very different ways. They all had to do with just simply drawing a single color for each pixel that is the same end result of just using the Mesh basic material with the color option. That’s okay considering that the goal in that section was to just get started, but not create some kind of final product.\n\nHowever in this section though the goal is to create an actual final result by making use of core threejs GLSL code to work with to create the same starting set of features there are to work with in the mesh basic material. Then hack over things a bit from there to create a material the will render a geometry by way of a ratio between a single solid base color, and whatever is going in in terms of any color attribute in the geometry that is used for what is called vertex coloring. For those of you not in the know with this one, vertex coloring is a way to define a color for each vertex in the position attribute of a buffer geometry. It is a nice quick way to have something other than a solid blob of color, that does not require light sources, or textures, but does require a color attribute in the geometry.\n\nThe first thing that I did when making this example is create a geometry using one of the built in geometry constructor functions. Then I would just get a reference to the position attribute of the geometry, and use the count value to know how many items I need for the color attribute that I will create for the geometry. For that I just worked out some quick code to create the color attribute by making an array and pushing values for the red, green, and blue color channels for each vertex. I then confirmed that everything is working okay by making use of the basic material with the vertexColors Boolean set to true. So now the next step is to just get the same result with the shader material by just using the same GLSL code as the basic material, but just commenting out everything that does not have to do with the features that I want.\n\nI really like the process of creating an array of strings for each line of GLSL code as this allows me to comment out lines of code one by one to get a quick idea of what I want to keep, and what might be unneeded bloat for what I want to do. Starting with the example in the basic section that has all the include statements I start to do just that until I have just the core features along that I want working. After that I started to go threw each of those include statements and just took a look at what the GLSL code is for each of them in the THREE.ShaderChunk object. Then I just replaced the includes with the actually GLSL lines, but with one exception which is the common lib.\n\nAfter a while I found what lines are used to add the vertex coloring feature, along with other core features that have to do with just the plain old diffuse color feature, and opacity. So then I ended up with just a few lines as I commented out, and removed lines that have to do with all kinds of features like alpha maps and so forth. Turns out that the mesh basic material is not so basic as there is a whole to to work with still. Anyway once I had just the features that I wanted from the basic feature I found ways to further reduce the complexity even more with this as all I want to do with this example at least is to color each fragment by way of a ratio between vertex coloring and a single uniform base color.\n\nThe end result is then just what I wanted for this example at least. I can set a base color, and a ratio in the form of a THREE.Vector2 object, to set the ratio between the base color and what is going on with vertex coloring as a way to render the geometry.\n\nOne of the first real examples of a custom shader material that I have made thus far is actually a hacked over version of one that I found on line. That is that I just did some quick google work to find a shader that does more or less what I wanted and found something that was close at least. I then hacked over it a lot, removing code that I did not want or need for the simple black and white look that I wanted and ended up with this.\n\nThe code that is was based on was made for a very old revision of threejs ( r57 ), so many of the changes had to do with having to just do things the way that they are done now. I also did not make use of additional shaders that where used in the example, and made a few more changes that allowed for things like thicker lines. Thus far this material works more or less the way that I would like it to, but I still would like to add at least a few more features to this such as transparency.\n\nThat will be it for now at least when it comes to custom shaders in threejs, not because there is not anything more to write about, but indeed very much the opposite. I have a lot more to look into when it comes to this sort of thing myself as there is a great deal more to learn about when it comes to working with the THREE.ShaderChunk library, as well as GLSL itself.\n\nThis was a kind of just getting started type post though, so maybe there is still only so much more to write about in terms of future edits of this post at least. However I am going to want to write additional blog posts on more examples for custom shaders, the shader chunk object, GLSL, and much more. There is just no way to do justice with this in the form of a single blog post, unless it is a real serious long form content piece that just goes on war and peace style maybe."
    },
    {
        "link": "https://threejs.org/docs#api/en/materials/ShaderMaterial.extensions",
        "document": ""
    },
    {
        "link": "https://dev.to/maniflames/creating-a-custom-shader-in-threejs-3bhi",
        "document": "3D stuff in the browser is awesome. After playing around with threejs for some time and making a mini-game at school I started to like it a lot. A classmate that is really into graphics programming told me a little bit about WebGL and shaders. It seemed really cool and I promised myself I would make my own shader. Of course some other shiny thing caught my attention and I forgot about it but, from today on I can finally say that I have created a shader and used it within threejs.\n\nBefore going all in on shaders it is probably a good idea to explain what three js is. Threejs is a javascript library to ease the process of creating 3D scenes on a canvas. Other popular solutions like a-frame and whitestorm js are build on top of it. If you have ever played around with those but want even more control definitely try it out! (If you are a TypeScript lover, three js has type definitions 😉).\n\nThe most popular intro to this library is creating a cube and making it spin. There is a written tutorial in the threejs documentation and a brilliant youtube tutorial by CJ Gammon that is part of his 'diving in: three js' series.\n\nCreating this cube is a basically preparing a film set and placing it inside of that set. You create a scene and a camera and pass these to a renderer to say: \"hey this is my movie set\". Then you can place mesh, which is basically an object, within the scene. This mesh consists of a geometry (the shape of the object) and a material (the color, behavior towards light and more). Based on the material you have chosen, you might want to add different kinds of lights to the scene. In order to animate the object and actually display everything you create a loop. Within this loop you tell the renderer to show the scene. Your code might look like this:\n\n\n\nShaders are basically functions or small scripts that are executed by the GPU. This is where WebGL and GLSL (OpenGL Shading Language) come into play. WebGL is a browser API that allows javascript to run code on the GPU. This can increase the performance of certain scripts because your GPU is optimized for doing graphics related calculations. WebGL even allows us to write code that will be executed directly by the GPU in the GLSL language. These pieces of GLSL code are our shaders and since threejs has a WebGL renderer we can write shaders to modify our mesh. In threejs you can create custom material by using the 'shader material'. This material accepts two shaders, a vertex shader and a fragment shader. Let's try to make 'gradient material'.\n\nA vertex shader is a function that is applied on every vertex (point) of a mesh. It is usually used to distort or animate the shape of a mesh. Within our script it looks something like this:\n\n\n\nThe first thing that you probably notice is that all our GLSL code is in a string. We do this because WebGL will pass this piece of code to our GPU and we have to pass the code to WebGL within javascript. The second thing you might notice is that we are using variables that we did not create. This is because threejs passes those variables to the GPU for us.\n\nWithin this piece of code we calculate where the points of our mesh should be placed. We do this by calculating where the points are in the scene by multiplying the position of the mesh in the scene (modelViewMatrix) and the position of the point. After that we multiply this value with the camera's relation to the scene (projectionMatrix) so the camera settings within threejs are respected by our shader. The gl_Position is the value that the GPU takes to draw our points.\n\nRight now this vertex shader doesn't change anything about our shape. So why even bother creating this at all? We will need the positions of parts of our mesh to create a nice gradient. By creating a 'varying' variable we can pass the position to another shader.\n\nA fragment shader is a function that is applied on every fragment of our mesh. A fragment is a result of a process called rasterization which turns the entire mesh into a collection of triangles. For every pixel that is covered by our mesh there will be at least one fragment. The fragment shader is usually used to do color transformations on pixels. Our fragment shader looks like this:\n\n\n\nAs you can see we take the value of the position that was passed by the vertex shader. We want to apply a mix of the colors A and B based on the position of the fragment on the z axis of our mesh. But where do the colors A and B come from? These are 'uniform' variables which means they are passed into the shader from the outside. The mix function will calculate the RGB value we want to draw for this fragment. This color and an additional value for the opacity are passed to gl_FragColor. Our GPU will set the color of a fragment to this color.\n\nNow that we've created the shaders we can finally build our threejs mesh with a custom material.\n\n\n\nThis is where everything comes together. Our 'uniforms' colorA and colorB are created and passed along with the vertex shader and fragment shader into the shader material. The material and geometry are used to create a mesh and the mesh is added to the scene.\n\n\n\n\n\n I build this in glitch. A friend recommended it and it is great! Some add blockers block you loading the embed though, so here is a direct link just in case.\n\nThe left cube is a cube using mesh lambert material, the right cube uses our own 'gradient material'. As you can see our material looks pretty sweet but ignores the light settings in the scene. This is because we didn't do the math in our fragment shader to take the light into account. This is hopefully something I figure out soon 😝.\n\nIt took some time to figure this out and if you liked this you should really check out the sources I have used to learn and understand this:"
    },
    {
        "link": "https://threejs.org/docs#api/materials/ShaderMaterial.uniforms",
        "document": ""
    },
    {
        "link": "https://threejs.org/docs#api/materials/ShaderMaterial.uniforms",
        "document": ""
    },
    {
        "link": "https://threejs.org/docs/api/en/materials/ShaderMaterial.html",
        "document": "A material rendered with custom shaders. A shader is a small program written in [link:https://www.khronos.org/files/opengles_shading_language.pdf GLSL] that runs on the GPU. You may want to use a custom shader if you need to:\n• implement an effect not included with any of the built-in [page:Material materials]\n• combine many objects into a single [page:BufferGeometry] in order to improve performance\n• A `ShaderMaterial` will only be rendered properly by [page:WebGLRenderer], since the GLSL code in the [link:https://en.wikipedia.org/wiki/Shader#Vertex_shaders vertexShader] and [link:https://en.wikipedia.org/wiki/Shader#Pixel_shaders fragmentShader] properties must be compiled and run on the GPU using WebGL.\n• As of THREE r72, directly assigning attributes in a ShaderMaterial is no longer supported. A [page:BufferGeometry] instance must be used instead, using [page:BufferAttribute] instances to define custom attributes.\n• As of THREE r77, [page:WebGLRenderTarget] or [page:WebGLCubeRenderTarget] instances are no longer supposed to be used as uniforms. Their [page:Texture texture] property must be used instead.\n• Built in attributes and uniforms are passed to the shaders along with your code. If you don't want the [page:WebGLProgram] to add anything to your shader code, you can use [page:RawShaderMaterial] instead of this class.\n• You can use the directive #pragma unroll_loop_start and #pragma unroll_loop_end in order to unroll a `for` loop in GLSL by the shader preprocessor. The directive has to be placed right above the loop. The loop formatting has to correspond to a defined standard.\n• The loop has to be [link:https://en.wikipedia.org/wiki/Normalized_loop normalized].\n• The loop variable has to be *i*.\n• The value `UNROLLED_LOOP_INDEX` will be replaced with the explicitly value of *i* for the given iteration and can be used in preprocessor statements.\n\nYou can specify two different types of shaders for each material:\n• The vertex shader runs first; it receives `attributes`, calculates / manipulates the position of each individual vertex, and passes additional data (`varying`s) to the fragment shader.\n• The fragment ( or pixel ) shader runs second; it sets the color of each individual \"fragment\" (pixel) rendered to the screen. There are three types of variables in shaders: uniforms, attributes, and varyings:\n• `Uniforms` are variables that have the same value for all vertices - lighting, fog, and shadow maps are examples of data that would be stored in uniforms. Uniforms can be accessed by both the vertex shader and the fragment shader.\n• `Attributes` are variables associated with each vertex---for instance, the vertex position, face normal, and vertex color are all examples of data that would be stored in attributes. Attributes can `only` be accessed within the vertex shader.\n• `Varyings` are variables that are passed from the vertex shader to the fragment shader. For each fragment, the value of each varying will be smoothly interpolated from the values of adjacent vertices. Note that `within` the shader itself, uniforms and attributes act like constants; you can only modify their values by passing different values to the buffers from your JavaScript code.\n\nThe [page:WebGLRenderer] provides many attributes and uniforms to shaders by default; definitions of these variables are prepended to your `fragmentShader` and `vertexShader` code by the [page:WebGLProgram] when the shader is compiled; you don't need to declare them yourself. See [page:WebGLProgram] for details of these variables. Some of these uniforms or attributes (e.g. those pertaining lighting, fog, etc.) require properties to be set on the material in order for [page:WebGLRenderer] to copy the appropriate values to the GPU - make sure to set these flags if you want to use these features in your own shader. If you don't want [page:WebGLProgram] to add anything to your shader code, you can use [page:RawShaderMaterial] instead of this class.\n\nBoth custom attributes and uniforms must be declared in your GLSL shader code (within `vertexShader` and/or `fragmentShader`). Custom uniforms must be defined in `both` the `uniforms` property of your `ShaderMaterial`, whereas any custom attributes must be defined via [page:BufferAttribute] instances. Note that `varying`s only need to be declared within the shader code (not within the material). To declare a custom attribute, please reference the [page:BufferGeometry] page for an overview, and the [page:BufferAttribute] page for a detailed look at the `BufferAttribute` API. When creating your attributes, each typed array that you create to hold your attribute's data must be a multiple of your data type's size. For example, if your attribute is a [page:Vector3 THREE.Vector3] type, and you have 3000 vertices in your [page:BufferGeometry], your typed array value must be created with a length of 3000 * 3, or 9000 (one value per-component). A table of each data type's size is shown below for reference: Note that attribute buffers are `not` refreshed automatically when their values change. To update custom attributes, set the `needsUpdate` flag to true on the [page:BufferAttribute] of the geometry (see [page:BufferGeometry] for further details). To declare a custom [page:Uniform], use the `uniforms` property: uniforms: { time: { value: 1.0 }, resolution: { value: new THREE.Vector2() } } You're recommended to update custom [page:Uniform] values depending on [page:Object3D object] and [page:Camera camera] in [page:Object3D.onBeforeRender] because [page:Material] can be shared among [page:Mesh meshes], [page:Matrix4 matrixWorld] of [page:Scene] and [page:Camera] are updated in [page:WebGLRenderer.render], and some effects render a [page:Scene scene] with their own private [page:Camera cameras].\n\n[page:Object parameters] - (optional) an object with one or more properties defining the material's appearance. Any property of the material (including any property inherited from [page:Material]) can be passed in here.\n\nSee the base [page:Material] class for common properties.\n\nDefines whether this material supports clipping; true to let the renderer pass the clippingPlanes uniform. Default is false.\n\nWhen the rendered geometry doesn't include these attributes but the material does, these default values will be passed to the shaders. This avoids errors when buffer data is missing.\n\nDefines custom constants using `#define` directives within the GLSL code for both the vertex shader and the fragment shader; each key/value pair yields another directive: yields the lines in the GLSL code.\n\nAn object with the following properties: this.extensions = { clipCullDistance: false, // set to use vertex shader clipping multiDraw: false // set to use vertex shader multi_draw / enable gl_DrawID };\n\nDefine whether the material color is affected by global fog settings; true to pass fog uniforms to the shader. Default is false.\n\nFragment shader GLSL code. This is the actual code for the shader. In the example above, the `vertexShader` and `fragmentShader` code is extracted from the DOM; it could be passed as a string directly or loaded via AJAX instead.\n\nDefines the GLSL version of custom shader code. Valid values are `THREE.GLSL1` or `THREE.GLSL3`. Default is `null`.\n\nIf set, this calls [link:https://developer.mozilla.org/en-US/docs/Web/API/WebGLRenderingContext/bindAttribLocation gl.bindAttribLocation] to bind a generic vertex index to an attribute variable. Default is undefined.\n\nRead-only flag to check if a given object is of type [name].\n\nDefines whether this material uses lighting; true to pass uniform data related to lighting to this shader. Default is false.\n\nControls wireframe thickness. Default is `1`.\n\n\n\n Due to limitations of the [link:https://www.khronos.org/registry/OpenGL/specs/gl/glspec46.core.pdf OpenGL Core Profile] with the [page:WebGLRenderer WebGL] renderer on most platforms linewidth will always be `1` regardless of the set value.\n\nDefine whether the material is rendered with flat shading. Default is false.\n\nAn object of the form: specifying the uniforms to be passed to the shader code; keys are uniform names, values are definitions of the form where `value` is the value of the uniform. Names must match the name of the uniform, as defined in the GLSL code. Note that uniforms are refreshed on every frame, so updating the value of the uniform will immediately update the value available to the GLSL code.\n\nCan be used to force a uniform update while changing uniforms in [page:Object3D.onBeforeRender](). Default is `false`.\n\nDefines whether vertex coloring is used. Default is `false`.\n\nVertex shader GLSL code. This is the actual code for the shader. In the example above, the `vertexShader` and `fragmentShader` code is extracted from the DOM; it could be passed as a string directly or loaded via AJAX instead.\n\nRender geometry as wireframe (using GL_LINES instead of GL_TRIANGLES). Default is false (i.e. render as flat polygons).\n\nControls wireframe thickness. Default is `1`.\n\n\n\n Due to limitations of the [link:https://www.khronos.org/registry/OpenGL/specs/gl/glspec46.core.pdf OpenGL Core Profile] with the [page:WebGLRenderer WebGL] renderer on most platforms linewidth will always be `1` regardless of the set value.\n\nSee the base [page:Material] class for common methods.\n\nGenerates a shallow copy of this material. Note that the vertexShader and fragmentShader are copied `by reference`, as are the definitions of the `attributes`; this means that clones of the material will share the same compiled [page:WebGLProgram]. However, the `uniforms` are copied `by value`, which allows you to have different sets of uniforms for different copies of the material."
    },
    {
        "link": "https://blog.maximeheckel.com/posts/the-study-of-shaders-with-react-three-fiber",
        "document": "When writing my first Three.js scene from start to finish in Building a Vaporwave scene with Three.js, I felt an immense sense of achievement. However, all I really did in this project was glue a couple of PNGs and maps I drew on Figma onto a plane and make the scene move. I'm being hard on myself here, I know 😅. At that point, I barely scratched the surface of the possibilities of creative coding on the web.\n\nAround the same time, as I was looking for inspiration for my next Three.js challenge, I kept finding gorgeous 3D scenes like this one:\n\nI had no clue how to build such dynamic meshes or make my geometries move, and my materials change colors. A few Google searches later: I got introduced to the concept of shaders that make scenes like the one above possible, and I wanted to know everything about them. However, shaders are incredibly difficult. Thus, I spent the past few weeks studying them, learned new techniques, created dozens of scenes from scratch, and hit as many roadblocks.\n\nIn this article, you'll find everything I learned about shaders during my experimentations, from how they work and use them with React Three Fiber to making them dynamic and interactive ✨. I included some of my own scenes/shaders as examples, as well as all the resources I used myself and tips on making your shaders composable and reusable.\n\nBefore jumping into the world of shaders and what they are, I want to introduce their use case. In Three.js and React Three Fiber, a 3D object is called a Mesh. And there's one thing you need to know and remember about meshes:\n• None The geometry is what defines the shape of the mesh.\n• None The material defines how the object looks and also what gives it some specific properties like reflection, metalness, roughness, etc. If you were to render the mesh defined by the React Three Fiber code above, you would see a white cube on your screen. That render is made possible by shaders. Three.js, and by extension React Three Fiber, is an abstraction on top of WebGL that uses shaders as its main component to render things on the screen: the materials bundled inside Three.js itself are implemented with shaders. So, if you've been tinkering around with Three.js or React Three Fiber, you've already used shaders without knowing it 🤯! These materials are pretty handy, but sometimes they are very limiting and put boundaries on our creativity. Defining your own material through shaders gives you absolute control over how your mesh looks within a scene. That is why a lot of creative developers decide to create their shaders from scratch! An icon representing the letter ‘i‘ in a circle If you're curious about how the bundled materials are built and want to go deeper: I highly encourage you to read through some of the shader implementations in the Three.js Github repository: A shader is a program, written in GLSL, that runs on the GPU. This program consists of two main functions that can output both 2D and 3D content: You can pass both functions to your React Three Fiber mesh's material via a to render your desired custom material. Basic definition of a React Three Fiber mesh with shaderMaterial Why do we need to pass these two functions separately? Simply because each has a very distinct purpose. Let's take a closer look at what they are doing. The role of the vertex shader is to position each vertex of a geometry. In simpler terms, this shader function allows you to programmatically alter the shape of your geometry and, potentially, \"make things move\". The code snippet below showcases how the default vertex shader looks. In this case, this function runs for every vertex and sets a property called that contains the x,y,z coordinates of a given vertex on the screen. For this first vertex shader example, I showcase how to edit the position of any vertex programmatically by changing their coordinate and make it a function of the coordinate. In this case, means that the \"height\" of our plane geometry follows a sine curve along the x-axis. What about those matrices? and are provided by React Three Fiber / Three.js. Thus, you don't need to worry about those too much to start playing with shaders. It would take a long time to explain why the formula looks the way it does, but if you want to read more about it, I found the WebGL model view projection documentation from MDN quite helpful. Once the GPU has run the vertex shader and placed all the vertices on the screen, i.e. when we have the overall \"shape\" of our geometry, and it can start processing the second function: the fragment shader. The role of the Fragment Shader is to set the color of each visible pixel of a geometry. This function sets the color in RGBA format, which we're already familiar with thanks to CSS (The only difference is that the values range from to instead of to : is and is ). Simple Fragment shader setting every pixel of the mesh to white Using Fragment Shader feels a lot like painting with computer code. Many creative coders, such as the author of the Book Of Shaders, draw a lot of stunning effects only through fragment shaders applied to a plane, like paint on a canvas. To demonstrate in a simple way how the fragment shader works, I built the little widget ✨ below that shows some simulated, low-resolution ( ) examples of fragment shaders. Notice how the fragment shader function runs for each pixel and outputs an RGBA color. For simplicity's sake, this demo only outputs grayscale colors: we set the color's red, green, and blue channels to the same value. You can hover over each pixel to see the value computed by the fragment shader function. As for your first (real) fragment shader example, why not play with some gradients 🎨! The scene below features a plane geometry with a shader material set to render of pink and yellow colors. In this specific fragment shader, we use the function that's bundled in the GLSL language along the x-axis of our plane. The x coordinates go from to , thus rendering a different color for each pixel along the x-axis, that color being a mix of pink and yellow. Why are shaders so hard to use?\n• None You have to learn a whole new language: GLSL. It is always challenging, but in this case, doing some C adjacent coding can feel far from pleasant, especially when coming from Javascript 😮‍💨. My advise here: go read The Book Of Shaders\n• None If you're used to fixing Javascript using , you are out of luck here: you can't log any values 😬. Debugging GLSL code is very tedious.\n• None Finally, the worst of all the reasons: when your code doesn't compile, nothing renders. You just get a blank screen 😵. All these downsides should not scare you away from learning shaders. Like when learning anything, it will take practice. Shaders will just require a bit more than usual. That's also the reason I'm writing this blog post: to give you some examples to put you on the right track!\n\nSo far, the shaders we saw are pretty static: we do not pass any external data, which is why we were only rendering some static colors and geometry. To make those dynamic, we need to add variables to our shaders and also be able to send data to the vertex and the fragment shader. This is where uniforms, varyings, and attributes come into the picture. To pass data from your Javascript code into your shader, we need to use uniforms. A uniform acts as an input to both vertex and fragment shader. The information passed is read-only and the same for each pixel and vertex of your mesh, hence the name \"uniform\". Diagram illustrating how to pass uniforms from our mesh to the vertex shader and fragment shader. You can picture a uniform as a bridge between your JS code and your shader code:\n• None Do you want to pass the x and y position of the mouse on the screen to your shader? That will be through a uniform.\n• None Do you want to pass the number of milliseconds since the scene rendered? That will be through a uniform as well.\n• None What about passing colors? Same: uniform! To declare uniforms, we need to place them at the top of your shaders, preceded by the variable type: , etc. Then we have to pass a uniforms object to our through the prop as follows: Example of passing a uniform to a shader I like to prefix my uniforms with :\n• None for the position of the mouse\n• None for the value of a color By accessing the uniforms object through the ref of our mesh within the hook and updating any values within that object, we can obtain dynamic uniforms that change their value through time/each frame. That is the technique featured below where the uniform is continuously given the elapsed time since the scene rendered, thus changing its value on every frame and resulting in the shape moving: If your scene contains some React state that can change and thus trigger a re-render: do not forget to memoize your uniform object!\n• None the resulting re-render will create a new uniform object\n• None our mesh will use this new object\n• None the hook will, unfortunately, keep updating the previous reference of our uniforms object Your shader will thus appear frozen on the screen until the next refresh. We saw that the vertex shader runs for every vertex. With uniforms, we can inject data into our vertex shader. However, this data is always the same for every vertex! What if we wanted to inject different data for each vertex? For this use case, we can use attributes. I'm not going to deep dive into those in this blog post as I haven't yet got to use them much, but I'm mentioning them so you know that they exist. Some attributes are used implicitly in every vertex shader like the variable or the variable (that will be mentioned below). Both are attributes that are always injected in any vertex shader. It's also important to keep in mind that attributes are only available in the vertex shader! We now know how to pass data from our React Three Fiber code to our shaders 🎉. But, what if we want to send information from one shader function to the other? Lucky us, we have varyings to do just that! A varying is a variable that can be declared and set in the vertex shader to be read by the fragment shader. Diagram illustrating how to pass the attributes from a geometry from the vertex shader to the fragment shader using varyings. In a nutshell, with varyings, we can \"link\" how we set the color of a given pixel based on the position of a vertex of the geometry. They are handy to pass attribute data to the fragment shader since, as we saw earlier, we can't pass attributes directly to the fragment shader. One way to do that is to:\n• None Assign the attribute to that varying variable.\n• None Read the varying in the fragment shader. Using varying to send the value of an attribute to the fragment shader In my own shader work, I use varyings to send my mesh's UV coordinates to my fragment shaders, especially when drawing shaders onto a plane. It allows me to simplify and normalize the coordinate system of my fragment shader. I've seen many fellow Three.js / React Three Fiber developers do so on their own shader work, and it's been working well for me. We're going to use this technique in our scenes going forward. UV coordinates is a coordinate system that allows you to position a 2D texture on a 3D object. Each UV coordinate references a pixel of a given texture and ranges from to . They act as a reference point to map a given point of the texture to a vertex of the mesh. This process of mapping a 2D texture on a 3D shape is also referred to as UV Mapping. UVs are given to us by Three.js/React Three Fiber out-of-the-box through an attribute in the vertex shader: In the code sandbox below we can see an example of such a technique:\n• None assign the UV coordinates in a varying in the vertex shader\n• None retrieve the UV coordinates back in the fragment shader.\n• None use the function against the x-axis of the vector. The result is this horizontal gradient going from pink to yellow: When using both uniforms and varyings within a shader, we can start seeing some magic happen 🪄. The code sandbox below showcases the implementation of the scene used as a teaser in the introduction:\n• None We use a combination of the hook from React Three Fiber and uniforms to pass the number of elapsed milliseconds since we rendered the scene.\n• None We apply a function to make the coordinate of a given vertex depend on the uniform and the / coordinates: the plane wobbles.\n• None We pass the coordinate as a varying to the fragment shader and colorize each pixel based on the value of : higher points are pink, lower points are more yellow.\n\nIn this part, we'll look at two examples of interactive React Three Fiber scenes with shaders that combine everything we've seen in the previous parts. But first, before we deep dive into those… I'm going to give you the one trick every creator developer uses to create those beautiful scenes with gradients, organic textures, clouds, and landscapes: noise. Sometimes you want to create a shader that is:\n• None random: it is not repetitive One could use an equivalent of in GLSL on every pixel or vertices, but that would not yield an appealing result. What we want is organic randomness, which is exactly what noise functions enable us to get! While noise is a fascinating subject, this article will not focus on it:\n• None It's a deep and complex subject that deserves its own article.\n• None already wrote an incredible article about this subject, better than I could have ever done 😄. already wrote an incredible article about this subject, better than I could have ever done 😄. @winkerVSbecks already wrote an incredible article about this subject, better than I could have ever done 😄. 👉 Also worth mentioning: The Book Of Shaders has a chapter entirely dedicated to noise. In the upcoming code sandboxes, we'll use only two types of noise: The full code for both noise functions will be featured in the code snippets (this was the only way I could make those work in Sandpack), it's long and very hard to follow but that's expected! You do not need to understand those functions. Most developers don't. In a normal setup, I'd recommend using the glsl-noise package and simply import the functions you need. The first shader we'll look at, named Blob, is a bit of a classic. It's an with the property (second argument) tuned to a high value to appear like a sphere. We apply a to this geometry with a custom shader:\n• None We use Perlin noise to \"displace\" vertices in the vertex shader.\n• None We use a uniform to make the organic randomness evolve through time.\n• None The displacement value for each vertex is set as a varying to be sent to the fragment shader.\n• None In the fragment shader, we set the color based on the value of that displacement varying, thus creating an organic-looking colored sphere. We also add a bit of interactivity to this scene:\n• None We use a uniform that sets the \"amplitude\" of our noise.\n• None We add hover listeners to increase the intensity of the noise when we hover the mesh.\n• None We lerp between the base value of our uniform and its final value, when hovered, to ease the transition between these two values in the hook. By combining uniforms, varyings, noise, and some hover effects, we created a pretty advanced shader for this scene that is both dynamic and interactive. For this second shader, I wanted to emphasize the \"painting\" aspect of shaders. When I feel like experimenting, I like to keep my geometries simple: I use a like I'd use an actual canvas to paint. An icon representing the letter ‘i‘ in a circle That's what developers sharing their creations on ShaderToy do: they only share a fragment shader applied on a plane. I love checking ShaderToy for inspiration: there are a lot of great techniques and GLSL code to read there!\n• None We do not touch anything in the vertex shader besides sending the UV coordinates as a varying to the fragment shader.\n• None We use the UV coordinates, the and uniforms as arguments for our Simplex noise. Instead of a hover effect like in the previous example, we directly send the cursor coordinates to the fragment shader!\n• None We use the function with color uniforms and our noise and assign the result to a variable several times to create a random gradient. The result is a dynamic gradient that changes when our cursor moves over the scene ✨:\n\nThroughout this article, we built our shaders from scratch on top of the material bundled in React Three Fiber. While it gives us almost unlimited possibilities, it also strips away a lot of work already done in some other materials. , for example, comes with props that allow us to tweak the reflectivity and interact with lights on a scene. However, if we want to get that effect along a custom shader, we're out of luck: we would have to reimplement the reflectivity and other physical properties of the material from scratch! It is possible to do just that, but for many developers getting started with shaders, including me, this feels out of reach at this stage. This is where Lamina comes into the picture 🍰. lamina lets you create materials with a declarative, system of layers. Layers make it incredibly easy to stack and blend effects. This approach was first made popular by the Spline Team. An icon representing the letter ‘i‘ in a circle I highly recommend taking a look at Lamina's README and also at some of the examples the maintainers included. Those are worth experimenting with! With Lamina, you can not only stack their pre-build layers (like , , or ) on top of existing material, but it also lets you declare your own custom layers (doc). And guess what? Those custom layers can be built using shaders! An icon representing an exclamation mark in an octogone Worth noting: you sadly can't just copy and paste your shader code into a class. You'll have to slightly tweak the code to get it to work:\n• None Uniforms must be prefixed by (this is also why I like to pick up this notation for my shaders in general).\n• None Varyings must be prefixed by .\n• None Local variables in the fragment shader must be prefixed by .\n• None You must the \"fragColor\" of the fragment shader and the \"position\" (only the position attribute, no need to return ) in the vertex shader. Apart from that, you can keep your shader code untouched! The result of that custom layer is a reusable and composable shader. Notice how the uniforms are automatically made available as props of the layer: our shader layer is easier to use and read ✨. Using a combination of custom shaders in Lamina can yield incredible results ✨. One such example is the Planet scene I created while learning shaders:\n• None Fractal Brownian Motion, a concept I learned about in I used, a concept I learned about in the dedicated chapter of The Book Of Shaders . This noise type can be changed more granularly and produce results that feel more organic, akin to clouds or mountains.\n• None I used this custom layer on top of a : this material can interact with light.\n• None Finally, I also used a layer to add that \"light pink atmospheric effect\" at the edge of the mesh 🍒. I provided the full implementation of this final example right below 👇, ready to be tweaked/forked:\n\nI hope this blog post gave you the little push you needed if you ever were on the fence about exploring shaders! There are a lot more aspects of shaders to cover, but this article sums up what I focused on while learning them. At this point, you have all the knowledge and techniques I gathered after spending several weeks working hard on many different shader scenes. From the fundamentals of shaders to building composable layers to use in your next creation, you now have all the tools to start experimenting on your own 🎉. If you are looking for a productive \"next step\" from this blog post, I would really encourage you to read The Book Of Shaders (I know, this is perhaps the third time I'm mentioning this website), go through all the examples, and even attempt to recreate some of the scene featured in the gallery. Or you can check out my creations and challenge yourself to reproduce them as closely as possible on your own 😄."
    },
    {
        "link": "https://stackoverflow.com/questions/41246878/how-to-get-the-same-result-as-with-using-pointsmaterial-but-with-using-shadermat",
        "document": "I want to use instead of .\n\n And have a problem with sizing or drawing of particles.\n\nIf I use , then sizes of particles depend on z coord of particle position (other words: depth of particle).\n\n But, if I use then I lose the needed effect.\n\nTry it here\n\nthe left shpere is \n\n the right shpere is\n\nHow you can see, have a big difference.\n\nMaybe someone knows how to get the same result of rendering with using ?"
    },
    {
        "link": "https://medium.com/@christianhelgeson/three-js-webgpurenderer-part-1-fragment-vertex-shaders-1070063447f0",
        "document": "If you've been judiciously following recent developments in Three.js, you may have found yourself wading into the confusing and uncharted woods of the WebGPURenderer without a mental map for how to deploy any of its new features. Indeed, while the Three.js project has largely moved away from the WebGLRenderer towards this new rendering paradigm, the WebGPURenderer, though capable of meeting most project requirements, technically remains in an unfinished and undocumented state. Avowed Three.js developers seeking to access familiar functionality may instead find themselves confused by unfamiliar terminology, such as Storage Buffer Nodes, automatic PassNode MRTs, Render Bundles, and a vast plethora of other inscrutable yet exciting new capabilities. Further still, while many features of the new renderer have been stable for some time, many others are still in a constant process of improvement, re-development, and tree shaking.\n\nIf you’re finding yourself in a similar place, you’re not alone. Like its namesake before it, the WebGPURenderer has a tendency to stymy beginners working from a dearth of resources, documentation, or charismatically produced Youtube videos. However, I believe that developers with a foothold of knowledge in the mature features of this new system will benefit greatly when the full feature set comes online. And once one masters the fundamentals, the new rendering paradigm proves itself to be both more intuitive and versatile than the previous version of the Three.js API. Thus, as someone who has waded through and cleared these woods myself, I believe I can vouch for my ability to navigate you around some of the same pitfalls I encountered when first experimenting with Nodes, TSL, and all the fancy new features that Three.js now offers.\n\nUltimately, the purpose of these tutorials is to provide a clear and comprehensive introduction to, and help develoers quickly implement, a large subset of Three.js’s latest features. While these tutorials should be useful for advanced developers, they are also intended to be accessible to aspiring developers who have minimal Three.js knowledge. Naturally, this means that the tutorials will include some basic project setup, explain seemingly obvious features, and demonstrate rudimentary effects with established WebGL-centric solutions, which an industrious developer might find more tedious than simply looking through the Three.js demos and figuring things out independently. However, my goal with these initial tutorials is not just to explain the new API but to foster a deeper comprehension of these new systems through play and experimentation. By doing so, I hope to lead beginners towards an underlying understanding of Three.js’ mechanics. After reading these tutorials, one should not only be able to replicate an effect by rote, but also embark on their own exploration of Three.js, even as the API continues to change.\n\nIn that spirit, we’ll begin our WebGPURenderer journey by using Nodes to create a fragment shader on the surface of a box mesh. From there, we’ll take it a step further by instancing the mesh and applying our newfound understanding of nodes to dynamically transform each instance, resulting in a visually simple, yet pleasing effect.\n\nOur initial web page setup is very similar to that of the official Three.js Examples, albeit simplified for clarity. For this tutorial, we’ll start by creating a Node project using Vite as our build tool. Our dependencies are listed below…\n\nIn the package.json, our Vite project requires us to apply a plugin for top-level await statements. The WebGPURenderer makes use of top level await statements to query for WebGPU compatible graphics resources on your computer. Consequently, this plugin will be necessary for our code to function properly. Fortunately, applying the plugin within our configuration file is straight forward. As shown below, we’ll also use our configuration file to define our import map for Vite, specifying that we only want to import from Three.js’s WebGPU build files.\n\nOnce the configuration is written, we can start adding code to our project, beginning with our index.html and main.css files. The main.css file is copied directly from the three.js examples directory, while the index.html file is pasted below.\n\nWith the styling and page layout in place, initialize the Three.js project with a script.js file. The initial Javascript file is a modified version of the Geometry Cube Example from the Three.js website, albeit one where we deploy the OrbitControls add-on to manipulate our camera, and where we substitute the existing renderer with the WebGPURenderer. In the starter file seen below, the program simply creates a scene containing a static cube mesh whose material is a texture map of a crate found at this link. Once downloaded, it can be placed into a ‘textures’ folder at the root of your project, and then imported into Javascript via the Three.TextureLoader.\n\nAnd voila, we have our scene:\n\nNow, our journey with nodes can truly begin.\n\nLet’s begin by defining what a node is conceptually. Essentially, a node is a placeholder for a unit of GPU computation, a representation of anything from a basic arithmetic operation, a lighting algorithm, a struct, a line of code, a shader, or a series of post-process passes. The specifics of how any given node works aren’t really important to understand. Nonetheless, they are the core building blocks used to write shaders and achieve a wide variety of effects within the WebGPU paradigm of the Three.js API.\n\nTypically, nodes are written and modified either inline or within TSL code blocks. TSL stands for Three Shading Language, an intermediary shader format that translates Three.js nodes into WGSL or GLSL code, depending on whether the WebGPURenderer deploys its WebGPU or WebGL backend.\n\nWait, the WebGPURenderer has a WebGL backend? Yes, it does! If the renderer detects that the device doesn’t support WebGPU, it will automatically fall back to WebGL, ensuring that projects built with WebGPU in mind can still run on a broader range of devices. TSL isn’t just a simple compatibility layer; it also abstracts much of the setup and syntax needed to deploy shaders. Consequently, unless you need to use specific features not yet supported by the node system, TSL is the recommended and way to write shaders in Three.js.\n\nSo how do we write a shader in TSL? Let’s start off with the simplest possible shader we can write, a fragment shader that outputs texture UVs to the surface of a mesh. To modify our mesh’s material using TSL shaders, we’ll need to change it from a MeshBasicMaterial to a MeshBasicNodeMaterial. As you can imagine, the MeshBasicNodeMaterial is just an extension of the feature set provided by its namesake class, allowing its properties to be define by nodes rather than traditional means. Accordingly, this change will not yet alter the visual output of your scene.\n\nWith this new material type, we can manipulate the fragment values the mesh material outputs by modifying the NodeMaterial’s fragmentNode property. First, import the uv() function from the ‘three/tsl’ directory to access a generic UV range. Then, write a TSL function which returns the value of uv(). Finally, assign that TSL function as the value of our material’s fragmentNode property. By assigning this function to fragmentNode, we apply our function as the new fragment shader of the material. Pay special attention to some of the syntax of how a TSL function is created, including the need to call the function as it is defined in order for the shader to properly execute.\n\nWith shorter shaders like these, we can actually simplify our syntax and return a value without explicit function brackets.\n\nWhenever possible, try to inline node operations to make them concise and readable.\n\nOnce we’ve applied this function to fragmentNode, our mesh’s surfaces will display uvs in the range of 0 to 1, overriding our material’s existing texture property.\n\nHmm, while these UVs are useful, they aren’t as visually interesting as our existing texture. What if we reapplied our crate texture to the surface of the material, only this time using the fragmentNode? To achieve this, we can import the texture() function from ‘three/tsl’, which will convert our existing texture into a TextureNode, allowing us to use it within our fragment shader.\n\nFrom here, we can apply various modifications to the output of our fragmentNode, including dynamically adjusting the texture's position based on the elapsed time of our application. Three.js provides four distinct timer nodes that can be used as uniforms within our TSL shader code. timerGlobal and timerLocal both represent elapsed time: timerGlobal tracks the time since the application started, while timerLocal tracks the time since the creation of the timer itself within the application. Additionally, timerDelta holds the elapsed time between the previous frame and the current frame, and timerFrame passes the current frame's ID. For our purposes, we want a simple variable that accumulates time, so we'll use timerLocal. By incorporating timerLocal, we can offset the UVs on our surface to create a scrolling texture effect.\"\n\nThat’s certainly a more dynamic result than our UVs! However, due to the nature of our fragment shader, the edges of our mesh are poorly defined. Although our mesh surfaces are dynamic, the mesh lacks any dimensionality without proper lighting. So, why not add some lights? Let’s begin with some basic lighting — nothing too fancy or chic, just a key light and a fill light. Then, we’ll convert our mesh’s material from a MeshBasicNodeMaterial to a material type that will react to our lighting.\n\nSurely these changes will give us the lighting we desire, right?\n\nThough there’s no lighting, this is actually the correct behavior given the code we’ve already written. There’s an important attribute of a material’s fragmentNode I’ve neglected to mention for the sake of example. In any instance where you apply a shader to the fragmentNode of your material, that shader will completely override the output fragment value of your mesh. It will not matter whether your scene has a complex lighting setup. It will not matter whether you have chosen a material that can properly receive light and shadow. Regardless of whatever elements you’ve added to your scene, the code of a material’s fragmentNode will completely override that material’s default fragment output.\n\nHowever, while a material’s fragmentNode will ignore the material’s internal shader logic and it’s interaction with outside elements, a material’s colorNode will not. The colorNode acts like it sounds, only modifying the base color value that a surface outputs wihtout affecting how that base color is affected by the larger lighting hierarchy of your scene. Therefore, if you want your mesh to properly integrate into your scene’s existing lighting setup, simply move your existing fragment shader from the fragmentNode to the colorNode.\n\nNow that we have a little bit of familiarity with the material system and TSL shaders, let’s create something a bit more exciting. We’ll stick with our basic cube and two lights, but leverage the power of vertex shaders to transform this basic cube into a dazzling dance of colorful, rotating cubes.\n\nTo give ourselves some motivation, let’s look at the effect our TSL vertex shader will produce:\n\nTo begin, we need to define some constants, such as how many concentric circles we want to create, and how many cubes we want in our scene. Populate your scene with eighty cube mesh instances placed within four concentric circles. For organizational purposes, place the code below above your material code, as you’ll need to use these constants within our material’s position shader.\n\nNext, shrink the scale of the cube geometry, remove the mesh’s default rotation, and switch the mesh over from a standard Mesh to an InstancedMesh. Instanced meshes leverage specific functionalities within the WebGPU graphics API that allows applications to draw multiple instances of the same mesh in a single draw call, improving overall rendering performance.\n\nFinally, we’ll import all the requisite Node functionality we need from the Three.js library into our project, create a set of uniforms, and make those uniforms accessible to the GUI.\n\nWith all those changes, our scene should look just like it does below, with our previous fragment shader still operating on our much smaller cube.\n\nWhen using the Three.js WebGPURenderer, users can apply a vertex shader to a mesh’s NodeMaterial by assigning a TSL function to the material’s positionNode or vertexNode property. Writing a TSL function for positionNode will still respect the standard model-view-projection (MVP) transformations that are already applied to your mesh. Essentially, this means that complex transformations to your mesh’s vertices or your mesh’s overall position can be executed similarly to how they are executed in Javascript. Moreover, since these operations will be executed in parallel within the material’s vertex shader, they will be much more performant than equivalent CPU operations.\n\nWhen using vertexNode, the behavior of a function is slightly different. This node will output the raw value returned by a TSL function directly to the vertex shader, bypassing the standard MVP transformation. Therefore, if you assign a TSL function to vertexNode, you must manually apply the MVP transformation within that function.\n\nTo demonstrate the difference, I’ve written two functions below, one for the vertexNode and one for the positionNode. Each shader performs the same operation: moving the mesh’s position along the x-axis.\n\nNote how in both shaders, we use positionLocal to acccess the vertices of our mesh in local space. While we Three.js, there are multiple convenience properties that access pre-transformed versions of your mesh vertices including\n• positionWorld: The position of a mesh geometry transformed by the modelWorldMatrix, which scales, rotates, and translates your mesh vertices.\n• positionView: The position of a mesh geometry transformed by the modelViewMatrix, which brings your mesh into viewSpace.\n• modelViewProjection: Performs the standard MVP transformation on the position passed as an argument.\n\nWith these additional nodes, we could modify our vertexNode shader to output the vertices of the mesh in myriad, without any changes to the shader’s visual output.\n\nSince we don’t want to mess with the standard MVP projection process, we’ll write our vertex shader for the material’s positionNode. Let’s start by first deleting any of the example position or vertex shaders we’ve created, and creating a new shader that we’ll later assign to our positionNode. Within this shader, let’s extract our uniforms and create some variables that we’ll use repeatedly across our shader.\n\nWe’ll then want to access some instance data within our position shader to properly coordinate the movement of each instance of our cube mesh. This means we’ll have to access the instance index that the current vertex belongs to. To do that, all we have to do is access the instanceIndex value we imported earlier. Within a function block assigned to either positionNode or vertexNode, the instanceIndex value will represent the index of the mesh instance that the current vertex belongs to. If you’re wondering why I explicitly make the distinction that this is its value within the context of a vertex shader, that’s because instanceIndex is a contextual node whose value changes depending on the context in which it is used. While it’s not important to know at the moment, other values that instanceIndex can represent will become essential down the road. For now, let’s continue by adding instanceIndex to our position shader, and deriving other indices from it’s value.\n\nWith these indices present, we’ll use them to separate and offset each mesh instance depending on these values. Below, we’ll add a short piece of code to our function to demonstrate that these values work, though this will not be our final effect.\n\nWith a successful demonstration showing we can manipulate our instances separately, it’s time to complete the shader. This following section will be heavy on code and light on text, but don’t worry, there will be plenty of code comments to help you follow along.\n\nFirst, set the position of the scene’s perspective camera back 15 units. Then, delete the sample code in the block above, and replace it with this line, which returns either a negative or positive value depending on the number’s parity.\n\nNext, create a variable representing the radius of one of the concentric circles. As circleIndex increases, the radius of each successive circle will also increase. The extent to which it increases is driven by the circle radius uniform we previously created and applied to our GUI.\n\nWe now need to move each instance of the cube mesh into it’s respective place. To do this, calculate each possible angle from the cube’s center to its circumference and move the cube along that angle into it’s circle. Additionally, we’ll scale the cubes in the outer circles such that they successively grow bigger than the cubes in the inner circles.\n\nHmm, we don’t really get a sense that our meshes are three-dimensional cubes and not two-dimensional planes. After the scale operation, let’s rotate each individual cube over time to reveal the mesh’s true nature.\n\nPerfect. Now, we can finish polishing our position shader by adding additional offsets to the position of each cube.\n\nAll that’s left to do now is apply randomized colors to each instance of our mesh, and our effect is complete!\n\nAnd there you have it, you’ve just taken your first steps into the world of the WebGPURenderer. If you’d like to see the final code for this tutorial, it can be found at this project’s Github repository. Feel free to comment on the tutorial, or message me on Github or the Three.js Forums if you have any questions or suggestions.\n\nIn the next tutorial, we’ll explore the new compute capabilities of Three.js, writing a compute shader that computes the velocity of multiple particles in parallel. Hope to see you there!"
    }
]