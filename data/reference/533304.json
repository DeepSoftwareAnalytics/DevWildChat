[
    {
        "link": "https://docs.docker.com/engine/install",
        "document": "This section describes how to install Docker Engine on Linux, also known as Docker CE. Docker Engine is also available for Windows, macOS, and Linux, through Docker Desktop. For instructions on how to install Docker Desktop, see: Overview of Docker Desktop.\n• If you use Debian derivatives such as \"BunsenLabs Linux\", \"Kali Linux\" or \"LMDE\" (Debian-based Mint) should follow the installation instructions for Debian, substitute the version of your distribution for the corresponding Debian release. Refer to the documentation of your distribution to find which Debian release corresponds with your derivative version.\n• Likewise, if you use Ubuntu derivatives such as \"Kubuntu\", \"Lubuntu\" or \"Xubuntu\" you should follow the installation instructions for Ubuntu, substituting the version of your distribution for the corresponding Ubuntu release. Refer to the documentation of your distribution to find which Ubuntu release corresponds with your derivative version.\n• Some Linux distributions provide a package of Docker Engine through their package repositories. These packages are built and maintained by the Linux distribution's package maintainers and may have differences in configuration or are built from modified source code. Docker isn't involved in releasing these packages and you should report any bugs or issues involving these packages to your Linux distribution's issue tracker.\n\nDocker provides binaries for manual installation of Docker Engine. These binaries are statically linked and you can use them on any Linux distribution.\n\nDocker Engine has two types of update channels, stable and test:\n• The stable channel gives you the latest versions released for general availability.\n• The test channel gives you pre-release versions that are ready for testing before general availability.\n\nUse the test channel with caution. Pre-release versions include experimental and early-access features that are subject to breaking changes.\n\nDocker Engine is an open source project, supported by the Moby project maintainers and community members. Docker doesn't provide support for Docker Engine. Docker provides support for Docker products, including Docker Desktop, which uses Docker Engine as one of its components.\n\nFor information about the open source project, refer to the Moby project website .\n\nPatch releases are always backward compatible with its major and minor version.\n\nDocker Engine is licensed under the Apache License, Version 2.0. See LICENSE for the full license text.\n\nIf you discover a security issue, we request that you bring it to our attention immediately.\n\nDO NOT file a public issue. Instead, submit your report privately to security@docker.com.\n\nSecurity reports are greatly appreciated, and Docker will publicly thank you for it.\n\nAfter setting up Docker, you can learn the basics with Getting started with Docker."
    },
    {
        "link": "https://docs.docker.com/engine/install/ubuntu",
        "document": "To get started with Docker Engine on Ubuntu, make sure you meet the prerequisites, and then follow the installation steps.\n• If you use ufw or firewalld to manage firewall settings, be aware that when you expose container ports using Docker, these ports bypass your firewall rules. For more information, refer to Docker and ufw.\n• Docker is only compatible with and . Firewall rules created with are not supported on a system with Docker installed. Make sure that any firewall rulesets you use are created with or , and that you add them to the chain, see Packet filtering and firewalls.\n\nTo install Docker Engine, you need the 64-bit version of one of these Ubuntu versions:\n\nDocker Engine for Ubuntu is compatible with x86_64 (or amd64), armhf, arm64, s390x, and ppc64le (ppc64el) architectures.\n\nBefore you can install Docker Engine, you need to uninstall any conflicting packages.\n\nYour Linux distribution may provide unofficial Docker packages, which may conflict with the official packages provided by Docker. You must uninstall these packages before you install the official version of Docker Engine.\n\nThe unofficial packages to uninstall are:\n\nMoreover, Docker Engine depends on and . Docker Engine bundles these dependencies as one bundle: . If you have installed the or previously, uninstall them to avoid conflicts with the versions bundled with Docker Engine.\n\nRun the following command to uninstall all conflicting packages:\n\nmight report that you have none of these packages installed.\n\nImages, containers, volumes, and networks stored in aren't automatically removed when you uninstall Docker. If you want to start with a clean installation, and prefer to clean up any existing data, read the uninstall Docker Engine section.\n\nYou can install Docker Engine in different ways, depending on your needs:\n• None Docker Engine comes bundled with Docker Desktop for Linux. This is the easiest and quickest way to get started.\n• None Set up and install Docker Engine from Docker's repository.\n• None Use a convenience script. Only recommended for testing and development environments.\n\nBefore you install Docker Engine for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository.\n• None To install a specific version of Docker Engine, start by listing the available versions in the repository:\n• None Verify that the installation is successful by running the image: This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.\n\nYou have now successfully installed and started Docker Engine.\n\nTo upgrade Docker Engine, follow step 2 of the installation instructions, choosing the new version you want to install.\n\nIf you can't use Docker's repository to install Docker Engine, you can download the file for your release and install it manually. You need to download a new file each time you want to upgrade Docker Engine.\n• None Select your Ubuntu version in the list.\n• None Go to and select the applicable architecture ( , , , or ).\n• None Download the following files for the Docker Engine, CLI, containerd, and Docker Compose packages:\n• None Install the packages. Update the paths in the following example to where you downloaded the Docker packages.\n• None Verify that the installation is successful by running the image: This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.\n\nYou have now successfully installed and started Docker Engine.\n\nTo upgrade Docker Engine, download the newer package files and repeat the installation procedure, pointing to the new files.\n\nDocker provides a convenience script at https://get.docker.com/ to install Docker into development environments non-interactively. The convenience script isn't recommended for production environments, but it's useful for creating a provisioning script tailored to your needs. Also refer to the install using the repository steps to learn about installation steps to install using the package repository. The source code for the script is open source, and you can find it in the repository on GitHub .\n\nAlways examine scripts downloaded from the internet before running them locally. Before installing, make yourself familiar with potential risks and limitations of the convenience script:\n• The script requires or privileges to run.\n• The script attempts to detect your Linux distribution and version and configure your package management system for you.\n• The script doesn't allow you to customize most installation parameters.\n• The script installs dependencies and recommendations without asking for confirmation. This may install a large number of packages, depending on the current configuration of your host machine.\n• By default, the script installs the latest stable release of Docker, containerd, and runc. When using this script to provision a machine, this may result in unexpected major version upgrades of Docker. Always test upgrades in a test environment before deploying to your production systems.\n• The script isn't designed to upgrade an existing Docker installation. When using the script to update an existing installation, dependencies may not be updated to the expected version, resulting in outdated versions.\n\nThis example downloads the script from https://get.docker.com/ and runs it to install the latest stable release of Docker on Linux:\n\nYou have now successfully installed and started Docker Engine. The service starts automatically on Debian based distributions. On based distributions, such as CentOS, Fedora, RHEL or SLES, you need to start it manually using the appropriate or command. As the message indicates, non-root users can't run Docker commands by default.\n\nDocker also provides a convenience script at https://test.docker.com/ to install pre-releases of Docker on Linux. This script is equal to the script at , but configures your package manager to use the test channel of the Docker package repository. The test channel includes both stable and pre-releases (beta versions, release-candidates) of Docker. Use this script to get early access to new releases, and to evaluate them in a testing environment before they're released as stable.\n\nTo install the latest version of Docker on Linux from the test channel, run:\n\nUpgrade Docker after using the convenience script\n\nIf you installed Docker using the convenience script, you should upgrade Docker using your package manager directly. There's no advantage to re-running the convenience script. Re-running it can cause issues if it attempts to re-install repositories which already exist on the host machine.\n• None Images, containers, volumes, or custom configuration files on your host aren't automatically removed. To delete all images, containers, and volumes:\n\nYou have to delete any edited configuration files manually."
    },
    {
        "link": "https://medium.com/@piyushkashyap045/comprehensive-guide-installing-docker-and-docker-compose-on-windows-linux-and-macos-a022cf82ac0b",
        "document": "Docker and Docker Compose have become essential tools for developers and system administrators, enabling the creation and management of containers to run applications in a consistent and isolated environment. Whether you’re a beginner or an experienced developer, getting Docker up and running on your system can sometimes be tricky. In this guide, we will walk you through the installation process for Docker and Docker Compose on Windows, Linux, and macOS, providing a detailed, step-by-step approach for each platform.\n\nWe’ll also touch on some post-installation tips to help you get the most out of Docker and Docker Compose once installed. This guide is designed to be simple and clear, even for beginners, and includes helpful screenshots to make the process easier.\n\nWhat is Docker and Docker Compose?\n\nBefore we dive into the installation process, let’s clarify what Docker and Docker Compose are:\n• Docker: Docker is an open-source platform that allows developers to package applications and their dependencies into containers. These containers can be easily moved, shared, and run on any system that has Docker installed. This simplifies the development process and ensures consistency across various environments.\n• Docker Compose: Docker Compose is a tool used to define and run multi-container Docker applications. With Compose, you can use a YAML file to configure all the services your app needs, and then spin up the entire environment with a single command.\n\nDocker Desktop is the easiest way to get started with Docker on Windows. It includes both Docker and Docker Compose, and works seamlessly with Windows Subsystem for Linux (WSL 2).\n• Choose the version compatible with your system (Windows 10/11 Pro or Enterprise with WSL 2 enabled is recommended). If you’re not sure whether your system is compatible, Docker will notify you during the installation if it’s not.\n• After the download finishes, open the installer file.\n• Follow the on-screen instructions in the installation wizard.\n• During installation, ensure you select the option to enable WSL 2 instead of Hyper-V. This option allows Docker to run in a lightweight Linux-based virtual machine, which is more efficient than Hyper-V for most use cases.\n• Once the installation completes, restart your system if prompted.\n\nStep 3: Enable WSL 2 (If Not Already Enabled)\n\nDocker Desktop requires WSL 2 (Windows Subsystem for Linux version 2) for optimal performance. If WSL 2 isn’t enabled on your system, follow these steps to set it up:\n• Run the following command to install WSL:\n\n3. Once installed, restart your computer if necessary.\n\nAfter restarting, open PowerShell or Command Prompt and run the following commands to check if Docker and Docker Compose are correctly installed:\n\nIf everything is working, you should see the version numbers of both Docker and Docker Compose.\n\nLinux users have more flexibility when it comes to Docker installation. Below, we’ll show you how to install Docker and Docker Compose on Ubuntu-based distributions, such as Ubuntu, Linux Mint, and Debian.\n\nStart by updating your system’s package list to ensure everything is up-to-date:\n\nNext, install the required dependencies to allow your system to fetch packages from Docker’s official repository:\n\nRun the following commands to add Docker’s official GPG key and the Docker repository to your system:\n\nNow, install Docker using the apt package manager:\n\nTo run Docker commands without , you need to add your user to the Docker group:\n\nImportant: After running this command, log out and log back in to apply the changes.\n\nDocker Compose allows you to manage multi-container applications easily. To install Docker Compose, run the following commands:\n\nYou should now see the version of Docker Compose that was installed.\n\nInstalling Docker Desktop on macOS is straightforward and similar to the Windows installation process.\n• Choose the appropriate version for your Mac (Intel or Apple Silicon).\n• Once the file is downloaded, open it.\n• Drag the Docker icon to the Applications folder.\n• Go to the Applications folder and launch Docker.\n• Once Docker starts, you will see a Docker icon in the macOS menu bar. This indicates Docker is running.\n\nOpen Terminal and run the following commands to check if Docker and Docker Compose are correctly installed:\n\nYou should now see the version numbers for both tools.\n\nAfter installing Docker and Docker Compose, there are a few things you can do to make your experience smoother:\n• Start Docker on Boot: On Windows and macOS, Docker Desktop is configured to start automatically. On Linux, you can enable Docker to start at boot using the following command:\n\n2. Use Docker Without (Linux Only): After adding your user to the Docker group, don’t forget to log out and log back in to apply the changes.\n\n3. Check Compatibility: If you’re using Docker Compose separately, make sure the version of Docker Compose is compatible with your Docker version. You can find this information in the Docker Compose release notes.\n\n4. Troubleshooting: If you encounter issues, the Docker community is large and active. Check the official Docker documentation or visit forums like Stack Overflow for solutions.\n\nWhether you’re using Windows, Linux, or macOS, installing Docker and Docker Compose is a crucial step in streamlining development workflows, and this guide has provided you with the steps needed to get these tools up and running on each platform. Docker’s powerful features allow for consistent environments and faster deployments, while Docker Compose simplifies the management of multi-container applications.\n\nBy following the steps above, you should now be equipped to begin using Docker in your projects. Happy containerizing!"
    },
    {
        "link": "https://docs.openvins.com/dev-docker.html",
        "document": "This will differ on which operating system you have installed, this guide is for linux-based systems. Please take a look at the official Docker Get Docker guide. There is also a guide from ROS called getting started with ROS and Docker. On Ubuntu one should be able to do the following to get docker: From there we can install NVIDIA Container Toolkit to allow for the docker to use our GPU and for easy GUI pass through. You might also want to check out this blogpost for some more details. From this point we should be able to \"test\" that everything is working ok. First on the host machine we need to allow for x11 windows to connect. We can now run the following command which should open gazebo GUI on your main desktop window. Alternatively we can launch directly into a bash shell and run commands from in there. This basically gives you a terminal in the docker container. rviz # you should be able to launch rviz once in bash\n\nClone the OpenVINS repository, build the container and then launch it. The Dockerfile will not build the repo by default, thus you will need to build the project. We have a few docker files for each version of ROS and operating system we support. In the following we will use the Dockerfile_ ros1_ 20_ 04 which is for a ROS1 install with a 20.04 system. Here it is important to note that we are going to create a dedicated ROS workspace which will then be loaded into the workspace. Thus if you are going to develop packages alongside OpenVINS you would make sure you have cloned your source code into the same workspace. The workspace local folder will be mounted to in the docker, thus all changes from the host are mirrored. export VERSION=ros1_20_04 # which docker file version you want (ROS1 vs ROS2 and ubuntu version) If the dockerfile breaks, you can remove the image and reinstall using the following: From here it is a good idea to create a nice helper command which will launch the docker and also pass the GUI to your host machine. Here you can append it to the bottom of the ~/.bashrc so that we always have it on startup or just run the two commands on each restart You will need to specify absolute directory paths to the workspace and dataset folders on the host you want to bind. Bind mounts are used to ensure that the host directory is directly used and all edits made on the host are sync'ed with the docker container. See the docker bind mounts documentation. You can add and remove mounts from this command as you see the need. source ~/.bashrc # after you save and exit Now we can launch RVIZ and also compile the OpenVINS codebase. From two different terminals on the host machine one can run the following (ROS 1): To actually get a bash environment that we can use to build and run things with we can do the following. Note that any install or changes to operating system variables will not persist, thus only edit within your workspace which is linked as a volume. Now once inside the docker with the bash shell we can build and launch an example simulation: And a version for ROS 2 we can do the following: On my machine running inside of the docker container is not real-time in nature. I am not sure why this is the case if someone knows if something is setup incorrectly please open a github issue. Thus it is recommended to only use the \"serial\" nodes which allows for the same parameters to be used as when installing directly on an OS.\n\nJetbrains provides some instructions on their side and a youtube video. Basically, Clion needs to be configured to use an external compile service and this service needs to be exposed from the docker container. I still recommend users compile with directly in the docker, but this will allow for debugging and syntax insights. After building the OpenVINS image (as above) we can do the following which will start a detached process in the docker. This process will allow us to connect Clion to it. export DOCKER_CATKINWS=/home/username/workspace/catkin_ws_ov # NOTE: should already be set in your bashrc export DOCKER_DATASETS=/home/username/datasets # NOTE: should already be set in your bashrc We can now change Clion to use the docker remote:\n• In short, you should add a new Toolchain entry in settings under Build, Execution, Deployment as a Remote Host type.\n• Click in the Credentials section and fill out the SSH credentials we set-up in the Dockerfile\n• Make sure the found CMake is the custom one installed and not the system one (greater than 3.12)\n• Add a CMake profile that uses this toolchain and you’re done.\n• Change build target to be this new CMake profile (optionally just edit / delete the default) To add support for ROS you will need to manually set environmental variables in the CMake profile. These were generated by going into the ROS workspace, building a package, and then looking at output. It should be under . This might be a brittle method, but not sure what else to do... (also see this blog post). You will need to edit the ROS version ( is used below) to fit whatever docker container you are using. When you build in Clion you should see in that the is building the files and maxing out the CPU during this process. Clion should send the source files to the remote server and then on build should build and run it remotely within the docker container. A user might also want to edit settings to exclude certain folders from copying over. See this jetbrains documentation page for more details."
    },
    {
        "link": "https://docker.com/get-started",
        "document": ""
    },
    {
        "link": "https://jetbrains.com/help/pycharm/docker-image-run-configuration.html",
        "document": "Use this type of configuration to run a Docker container from a locally existing image that you either pulled or built previously.\n\nDocker uses the docker run command with the following syntax:\n\nYou can set all the arguments for this command using the options of the Docker Image run configuration.\n\nBy default, the Docker Image configuration has the following options:\n\nSpecify a name for the run configuration to quickly identify it among others when editing or running. Save the file with the run configuration settings to share it with other team members. The default location is .idea/runConfigurations. However, if you do not want to share the .idea directory, you can save the configuration to any other directory within the project. By default, it is disabled, and PyCharm stores run configuration settings in .idea/workspace.xml. Select the Docker daemon connection to use for the run configuration. Specify the identifier or the name of the Docker image from which to create the container. Specify an optional name for the container. If empty, Docker will generate a random name for the container. This is similar to using the option with the command. Specify a list of tasks to perform before starting the run configuration. For example, run another configuration, build the necessary artifacts, run some external tool or a web browser, and so on. Click or press to add one of the available tasks. Move tasks in the list using and to change the order in which to perform the tasks. Select a task and click to edit the task. Click to remove the selected task from the list. Show the run configuration settings before actually starting it. Depending on the type of configuration, open the Run, Debug, or Services tool window when you start this run configuration. If this option is disabled, you can open the tool window manually:\n\nUse the Modify options menu to add advanced options to the run configuration:\n\nPublish all exposed container ports to random free ports on the host. This is similar to using the or option on the command line. Map specific container ports to specific ports on the host. This is similar to using the or option on the command line. Click in the Bind ports field and specify which ports on the host should be mapped to which ports in the container. You can also provide a specific host IP from which the port should be accessible (for example, you can set it to to make it accessible only locally, or set it to to open it for all computers in your network). Lets say you already have Django running on the host port 5432, and you want to run another instance of Django in a container and access it from the host via port 5433. Binding the host port 5433 to port 5432 in the container is similar to setting the following command-line option: You can set this option explicitly in the Run options field instead of configuring the Bind ports field. Override the default of the image. This is similar to using the option on the command line. Override the default of the image. This is similar to adding the command as an argument for . Mount files and directories on the host to a specific location in the container. This is similar to using the or option on the command line. Make sure that the necessary local paths are mapped to the virtual machine in the Docker connection settings (the Path mappings table). Click in the Bind mounts field and specify the host directory and the corresponding path in the container where it should be mounted. Select Read only if you want to disable writing to the container volume. For example, you can mount a local Django directory on the host (Users/Shared/django-data) to some directory inside the container (/var/lib/django-data). Mounting volumes in this manner is similar to setting the following command-line option: You can set this option explicitly in the Run options field instead of configuring the Bind mounts field. Specify environment variables. There are environment variables associated with the base image that you are using as defined by the instruction in the Dockerfile. There are also environment variables that Docker sets automatically for each new container. Use this field to override any of the variables or specify additional ones. This is similar to using the or option on the command line. Click in the Environment variables field to add names and values for variables. For example, if you want to connect to Django with a specific username by default (instead of the operating system name of the user running the application), you can set the variable to the necessary value. This is similar to setting the following command-line option: You can set this option explicitly in the Run options field instead of configuring the Environment variables field. For example, to connect the container to the network and set the alias for it, specify the following: Not all options are supported. If you would like to request support for some option, leave a comment in IDEA-181088. Attach to the container's standard input, output, and error streams. This is similar to using the or option on the command line. Preview the resulting command that will be used to execute the run configuration."
    },
    {
        "link": "https://jetbrains.com/help/pycharm/dockerfile-run-configuration.html",
        "document": "Specify a local directory that the daemon will use during the build process. All host paths in the Dockerfile will be processed relative to this directory. By default, if you leave it blank, Docker uses the same directory where the Dockerfile is located.\n\nSpecify the values for build-time variables that can be accessed like regular environment variables during the build process but do not persist in the intermediate or final images. This is similar to using the option with the command. These variables must be defined in the Dockerfile with the instruction. For example, you can define a variable for the version of the base image that you are going to use: The variable in this case will default to and the Dockerfile will produce an image with the latest available version of Python, unless you redefine it as a build-time argument. If you set, , Docker will pull instead, which will run a container with Python version 3.10. Redefining the argument is similar to setting the following command-line option: You can provide several arguments separated by spaces.\n\nMap specific container ports to specific ports on the host. This is similar to using the or option on the command line. Click in the Bind ports field and specify which ports on the host should be mapped to which ports in the container. You can also provide a specific host IP from which the port should be accessible (for example, you can set it to to make it accessible only locally, or set it to to open it for all computers in your network). Lets say you already have Django running on the host port 5432, and you want to run another instance of Django in a container and access it from the host via port 5433. Binding the host port 5433 to port 5432 in the container is similar to setting the following command-line option: You can set this option explicitly in the Run options field instead of configuring the Bind ports field.\n\nMount files and directories on the host to a specific location in the container. This is similar to using the or option on the command line. Make sure that the necessary local paths are mapped to the virtual machine in the Docker connection settings (the Path mappings table). Click in the Bind mounts field and specify the host directory and the corresponding path in the container where it should be mounted. Select Read only if you want to disable writing to the container volume. For example, you can mount a local Django directory on the host (Users/Shared/django-data) to some directory inside the container (/var/lib/django-data). Mounting volumes in this manner is similar to setting the following command-line option: You can set this option explicitly in the Run options field instead of configuring the Bind mounts field.\n\nSpecify environment variables. There are environment variables associated with the base image that you are using as defined by the instruction in the Dockerfile. There are also environment variables that Docker sets automatically for each new container. Use this field to override any of the variables or specify additional ones. This is similar to using the or option on the command line. Click in the Environment variables field to add names and values for variables. For example, if you want to connect to Django with a specific username by default (instead of the operating system name of the user running the application), you can set the variable to the necessary value. This is similar to setting the following command-line option: You can set this option explicitly in the Run options field instead of configuring the Environment variables field.\n\nFor example, to connect the container to the network and set the alias for it, specify the following: Not all options are supported. If you would like to request support for some option, leave a comment in IDEA-181088."
    },
    {
        "link": "https://jetbrains.com/help/pycharm/docker.html",
        "document": "Docker is a tool for deploying and running executables in isolated and reproducible environments. This may be useful, for example, to test code in an environment identical to production.\n\nPyCharm integrates the Docker functionality and provides assistance for creating Docker images, running Docker containers, managing Docker Compose applications, using public and private Docker registries, and much more directly from the IDE.\n\nYou can run and debug your Python code in various environments deployed in Docker containers. For more information, refer to Configure an interpreter using Docker.\n\nThis functionality relies on the Docker plugin, which is bundled and enabled in PyCharm by default. If the relevant features are not available, make sure that you did not disable the plugin. The Docker plugin is available by default only in PyCharm Professional. For PyCharm Community Edition, you need to install the Docker plugin as described in Install plugins.\n• None Press to open settings and then select Plugins.\n• None Open the Installed tab, find the Docker plugin, and select the checkbox next to the plugin name.\n• None Press to open settings and then select Build, Execution, Deployment | Docker.\n• None Click to add a Docker configuration and specify how to connect to the Docker daemon. The connection settings depend on your Docker version and operating system. For more information, refer to Docker connection settings. The Connection successful message should appear at the bottom of the dialog. For more information about mapping local paths to the virtual machine running the Docker daemon when using Docker on Windows or macOS, refer to Virtual machine path mappings for Windows and macOS hosts. You will not be able to use volumes and bind mounts for directories outside of the mapped local path. This table is not available on a Linux host, where Docker runs natively and you can mount any directory to the container.\n• None Open the Services tool window (View | Tool Windows | Services or ), select the configured Docker connection node and click , or select Connect from the context menu. To edit the Docker connection settings, select the Docker node and click on the toolbar, or select Edit Configuration from the context menu. You can also click and select Docker Connection to add a Docker connection directly from the Services tool window. If you have Docker contexts configured, you can select Docker Connections from Docker Contexts to add the corresponding connections.\n\nOnce you connect to the Docker daemon, you can use the Services tool window (View | Tool Windows | Services or ) to manage everything related to Docker, for example: pull and push images, create and run containers, and scale Docker Compose services. As with other tool windows, you can start typing the name of an image or container to highlight the matching items.\n\nFor more information, refer to the section about Docker in Services tool window."
    },
    {
        "link": "https://jetbrains.com/help/pycharm/docker-compose-run-configuration.html",
        "document": "Specify the path to a custom environment file that defines the Docker Compose environment variables. This is similar to using the option with the command. By default, the Docker Compose run configuration looks for a file named .env in the directory with the Docker Compose file.\n\nWhen stopping and removing containers, also delete named volumes declared in the Docker Compose file and anonymous volumes attached to containers. This is similar to using the or option with the command.\n\nConfigure which images should be removed when stopping and removing containers. You can choose to remove all images used by any service or only images that don't have a custom tag set in the field. This is similar to using the option with the command.\n\nSet a timeout in seconds to forcefully terminate containers that won't shutdown gracefully. Docker usually tries to gracefully terminate any container with , but it might end up running indefinitely. Set a timeout after which Docker should send to force the shutdown. This is similar to using the or option with the command.\n\nReturn the exit code of the selected service container. Whenever a container in the selected service stops, return its exit code and stop all other containers in the service. This is similar to using the option with the command.\n\nSet the number of containers to start for each service. This option overrides the parameter in the Docker Compose file, if it's present. This is similar to using the option with the command.\n• None Selected and dependencies: By default, Docker Compose starts all of the specified services and linked services.\n• None None: Don't start any services after creating them. This is similar to using the option with the command.\n• None Selected services: Don't start any of the linked services. This is similar to using the option with the command.\n\nConfigure for which containers to show output streams:\n• None Selected services: By default, Docker Compose attaches to all started containers of the specified services.\n• None None: Don't attach to any containers. This is similar to using the or option with the command.\n• None Selected and dependencies: Attach to containers of the specified services and linked services. This is similar to using the option with the command.\n\nConfigure which containers to stop and replace by new ones:\n• None Changed configuration: By default, Docker Compose recreates containers only if the corresponding configuration or image has changed.\n• None All: Recreate all containers in the services, even if the corresponding configuration or image hasn't changed. This is similar to using the option with the command.\n• None None: Don't recreate any containers in the services, even if the corresponding configuration has changed. This is similar to using the option with the command.\n\nConfigure which images to build before starting containers:\n• None Only missing images: By default, Docker Compose only builds images that are not available and uses previously built ones when possible.\n• None Never: Don't build any images. Always use previously built images or throw an error if some image is not available. This is similar to using the option with the command.\n• None Always: Always build images before starting containers. This is similar to using the option with the command.\n\nConfigure how to stop containers in a service. By default, Docker Compose doesn't stop other containers in a service. You have to stop them manually. However, you can choose to stop all containers if any container in a service stops. This is similar to using the option with the command."
    },
    {
        "link": "https://jetbrains.com/help/pycharm/using-docker-as-a-remote-interpreter.html",
        "document": "Make sure that the following prerequisites are met:\n• None Docker is installed, as described in the Docker documentation. You can install Docker on various platforms, but here we'll use the Windows installation. Note that you might want to repeat this tutorial on different platforms; then use Docker installations for macOS and Linux (Ubuntu, other distributions-related instructions are available as well).\n• None You have stable Internet connection, so that PyCharm can download and run (the latest version of the BusyBox Docker Official Image). Once you have successfully configured an interpreter using Docker, you can go offline.\n• None Before you start working with Docker, make sure that the Docker plugin is enabled. The plugin is bundled with PyCharm and is activated by default. If the plugin is not activated, enable it on the Plugins settings page as described in Install plugins. In the Settings dialog ( ) , select Build, Execution, Deployment | Docker, and select Docker for <your operating system> under Connect to Docker daemon with. For example, if you are on macOS, select Docker for Mac. See more detail in Docker settings. Note that you cannot install any Python packages into Docker-based project interpreters.\n\nCreate a Python project , add the Solver.py file, and copy and paste the following code: import math class Solver: def demo(self, a, b, c): d = b ** 2 - 4 * a * c if d > 0: disc = math.sqrt(d) root1 = (-b + disc) / (2 * a) root2 = (-b - disc) / (2 * a) return root1, root2 elif d == 0: return -b / (2 * a) else: return \"This equation has no roots\" if __name__ == '__main__': solver = Solver() while True: a = int(input(\"a: \")) b = int(input(\"b: \")) c = int(input(\"c: \")) result = solver.demo(a, b, c) print(result)\n• None\n• None Click the Python Interpreter selector and choose Add New Interpreter.\n• None Press to open Settings and go to Project: <project name> | Python Interpreter. Click the Add Interpreter link next to the list of the available interpreters.\n• None Click the Python Interpreter selector and choose Interpreter Settings. Click the Add Interpreter link next to the list of the available interpreters.\n• None Select an existing Docker configuration in the Docker server dropdown. Alternatively, click and perform the following steps to create a new Docker configuration: Click to add a Docker configuration and specify how to connect to the Docker daemon. The connection settings depend on your Docker version and operating system. For more information, refer to Docker connection settings. The Connection successful message should appear at the bottom of the dialog. For more information about mapping local paths to the virtual machine running the Docker daemon when using Docker on Windows or macOS, refer to Virtual machine path mappings for Windows and macOS hosts. You will not be able to use volumes and bind mounts for directories outside of the mapped local path. This table is not available on a Linux host, where Docker runs natively and you can mount any directory to the container.\n• None The following actions depend on whether you want to pull a pre-built image from a Docker registry or to build an image locally from a Dockerfile. Select Pull or use existing and specify the tag of the desired image in the Image tag field. Select Build and change the default values in the Dockerfile and Context folder fields if necessary. If required, expand the Optional section and specify the following: Specify an optional name and tag for the built image. This can be helpful for referring to the image in the future. If you leave the field blank, the image will have only a random unique identifier. For example, you can specify metadata for the built image with the option. Specify the values for build-time variables that can be accessed like regular environment variables during the build process but do not persist in the intermediate or final images. This is similar to using the option with the command. These variables must be defined in the Dockerfile with the instruction. For example, you can define a variable for the version of the base image that you are going to use: The variable in this case will default to and the Dockerfile will produce an image with the latest available version of Python, unless you redefine it as a build-time argument. If you set, , Docker will pull instead, which will run a container with Python version 3.10. Redefining the argument is similar to setting the following command-line option: You can provide several arguments separated by spaces.\n• None Wait for PyCharm to connect to the Docker daemon and complete the container introspection.\n• None Next, select an interpreter to use in the Docker container. You can choose any virtualenv or conda environment that is already configured in the container or select a system interpreter.\n• None The configured remote interpreter is added to the list.\n\nIn the gutter, next to the clause, click the icon, and choose Run 'solver': The script is launched in the Run tool window. Provide values to the script: Switch to the Services tool window to preview the container details. Expand the Containers node and select the running container. Note that the Log tab contains the same execution result.\n\nSee the following video tutorial for additional information:"
    },
    {
        "link": "https://testdriven.io/blog/docker-best-practices",
        "document": "This article looks at some best practices to follow when writing Dockerfiles and working with Docker in general. While most of the practices listed apply to all developers, regardless of the language, a few apply to only those developing Python-based applications.\n• Run Only One Process Per Container\n• Understand the Difference Between ENTRYPOINT and CMD\n• Lint and Scan Your Dockerfiles and Images\n\nTake advantage of multi-stage builds to create leaner, more secure Docker images.\n\nMulti-stage Docker builds allow you to break up your Dockerfiles into several stages. For example, you can have a stage for compiling and building your application, which can then be copied to subsequent stages. Since only the final stage is used to create the image, the dependencies and tools associated with building your application are discarded, leaving a lean and modular production-ready image.\n\nIn this example, the GCC compiler is required for installing certain Python packages, so we added a temp, build-time stage to handle the build phase. Since the final run-time image does not contain GCC, it's much lighter and more secure.\n\nIn summary, multi-stage builds can decrease the size of your production images, helping you save time and money. In addition, this will simplify your production containers. Also, due to the smaller size and simplicity, there's potentially a smaller attack surface.\n\nPay close attention to the order of your Dockerfile commands to leverage layer caching.\n\nDocker caches each step (or layer) in a particular Dockerfile to speed up subsequent builds. When a step changes, the cache will be invalidated not only for that particular step but all succeeding steps.\n\nIn this Dockerfile, we copied over the application code before installing the requirements. Now, each time we change sample.py, the build will reinstall the packages. This is very inefficient, especially when using a Docker container as a development environment. Therefore, it's crucial to keep the files that frequently change towards the end of the Dockerfile.\n\nSo, in the above Dockerfile, you should move the command to the bottom:\n• Always put layers that are likely to change as low as possible in the Dockerfile.\n• Combine and commands. (This also helps to reduce the image size. We'll touch on this shortly.)\n• If you want to turn off caching for a particular Docker build, add the flag.\n\nSmaller Docker images are more modular and secure.\n\nBuilding, pushing, and pulling images is quicker with smaller images. They also tend to be more secure since they only include the necessary libraries and system dependencies required for running your application.\n\nWhich Docker base image should you use?\n\nHere's a size comparison of various Docker base images for Python:\n\nWhile the Alpine flavor, based on Alpine Linux, is the smallest, it can often lead to increased build times if you can't find compiled binaries that work with it. As a result, you may end up having to build the binaries yourself, which can increase the image size (depending on the required system-level dependencies) and the build times (due to having to compile from the source).\n\nIn the end, it's all about balance. When in doubt, start with a flavor, especially in development mode, as you're building your application. You want to avoid having to continually update the Dockerfile to install necessary system-level dependencies when you add a new Python package. As you harden your application and Dockerfile(s) for production, you may want to explore using Alpine for the final image from a multi-stage build.\n\nIt's a good idea to combine the , , and commands as much as possible since they create layers. Each layer increases the size of the image since they are cached. Therefore, as the number of layers increases, the size also increases.\n\nYou can test this out with the command:\n\nTake note of the sizes. Only the , , and commands add size to the image. You can reduce the image size by combining commands wherever possible. For example:\n\nCan be combined into a single command:\n\nThus, creating a single layer instead of two, which reduces the size of the final image.\n\nWhile it's a good idea to reduce the number of layers, it's much more important for that to be less of a goal in itself and more a side-effect of reducing the image size and build times. In other words, focus more on the previous three practices -- multi-stage builds, order of your Dockerfile commands, and using a small base image -- than trying to optimize every single command.\n• Each layer contains the differences from the previous layer.\n• Layers increase the size of the final image.\n• Remove unnecessary files in the same step that created them.\n• Minimize the number of times is run since it upgrades all packages to the latest version.\n• With multi-stage builds, don't worry too much about overly optimizing the commands in temp stages.\n\nFinally, for readability, it's a good idea to sort multi-line arguments alphanumerically:\n\nAdditionally, it's crucial to perform clean-up actions within the same instruction to avoid unnecessary bloat in your Docker images. This approach ensures that temporary files or cache used during installation are not included in the final image layer, effectively reducing the image size. For example, after installing packages with , use to remove the package lists and any temporary files created during the installation process, as demonstrated above. This practice is essential for keeping your Docker images as lean and efficient as possible.\n\nBy default, Docker runs container processes as root inside of a container. However, this is a bad practice since a process running as root inside the container is running as root in the Docker host. Thus, if an attacker gains access to your container, they have access to all the root privileges and can perform several attacks against the Docker host, like-\n• copying sensitive info from the host's filesystem to the container\n\nTo prevent this, make sure to run container processes with a non-root user:\n\nYou can take it a step further and remove shell access and ensure there's no home directory as well:\n\nHere, the application within the container runs under a non-root user. However, keep in mind, the Docker daemon and the container itself still run with root privileges. Be sure to review Run the Docker daemon as a non-root user for help with running both the daemon and containers as a non-root user.\n\nUse unless you're sure you need the additional functionality that comes with .\n\nWhat's the difference between and ?\n\nBoth commands allow you to copy files from a specific location into a Docker image:\n\nWhile they look like they serve the same purpose, has some additional functionality:\n• is used for copying local files or directories from the Docker host to the image.\n• can be used for the same thing as well as downloading external files. Also, if you use a compressed file (tar, gzip, bzip2, etc.) as the parameter, will automatically unpack the contents to the given location.\n\nWhen a requirements file is changed, the image needs to be rebuilt to install the new packages. The earlier steps will be cached, as mentioned in Minimize the Number of Layers. Downloading all packages while rebuilding the image can cause a lot of network activity and takes a lot of time. Each rebuild takes up the same amount of time for downloading common packages across builds.\n\nYou can avoid this by mapping the pip cache directory to a directory on the host machine. So for each rebuild, the cached versions persist and can improve the build speed.\n\nAdd a volume to the docker run as or as a mapping in the Docker Compose file.\n\nMoving the cache from the docker image to the host can save you space in the final image.\n\nIf you're leveraging Docker BuildKit, use BuildKit cache mounts to manage the cache:\n\nRun Only One Process Per Container\n\nWhy is it recommended to run only one process per container?\n\nLet's assume your application stack consists of a two web servers and a database. While you could easily run all three from a single container, you should run each in a separate container to make it easier to reuse and scale each of the individual services.\n• Scaling - With each service in a separate container, you can scale one of your web servers horizontally as needed to handle more traffic.\n• Reusability - Perhaps you have another service that needs a containerized database. You can simply reuse the same database container without bringing two unnecessary services along with it.\n• Logging - Coupling containers makes logging much more complex. We'll address this in further detail later in this article.\n• Portability and Predictability - It's much easier to make security patches or debug an issue when there's less surface area to work with.\n\nYou can write the and commands in your Dockerfiles in both array (exec) or string (shell) formats:\n\nBoth are correct and achieve nearly the same thing; however, you should use the exec format whenever possible. From the Docker documentation:\n• Make sure you're using the exec form of and in your Dockerfile.\n• For example use not . Using the string form causes Docker to run your process using bash, which doesn't handle signals properly. Compose always uses the JSON form, so don't worry if you override the command or entrypoint in your Compose file.\n\nSo, since most shells don't process signals to child processes, if you use the shell format, (which generates a ) may not stop a child process.\n\nTry both of these. Take note that with the shell format flavor, won't kill the process. Instead, you'll see .\n\nAnother caveat is that the shell format carries the PID of the shell, not the process itself.\n\nUnderstand the Difference Between ENTRYPOINT and CMD\n\nShould I use ENTRYPOINT or CMD to run container processes?\n\nThere are two ways to run commands in a container:\n\nBoth essentially do the same thing: Start the application at with a Gunicorn server and bind it to .\n\nThe is easily overridden. If you run , the above gets replaced by the new arguments -- i.e., . Whereas to override the command, one must specify the option:\n\nHere, it's clear that we're overriding the entrypoint. So, it's recommended to use over to prevent accidentally overriding the command.\n\nThey can be used together as well.\n\nWhen used together like this, the command that is run to start the container is:\n\nAs discussed above, is easily overridden. Thus, can be used to pass arguments to the command. The number of workers can be easily changed like so:\n\nThis will start the container with six Gunicorn workers rather then four.\n\nUse a to determine if the process running in the container is not only up and running, but is \"healthy\" as well.\n\nDocker exposes an API for checking the status of the process running in the container, which provides much more information than just whether the process is \"running\" or not since \"running\" covers \"it is up and working\", \"still launching\", and even \"stuck in some infinite loop error state\". You can interact with this API via the HEALTHCHECK instruction.\n\nFor example, if you're serving up a web app, you can use the following to determine if the endpoint is up and can handle serving requests:\n\nIf you run , you can see the status of the .\n\nYou can take it a step further and set up a custom endpoint used only for health checks and then configure the to test against the returned data. For example, if the endpoint returns a JSON response of , you can instruct the to validate the response body.\n\nHere's how you view the status of the health check status using :\n\nYou can also add a health check to a Docker Compose file:\n• : The interval to test for -- e.g., test every unit of time.\n• : Max time to wait for the response.\n• : When to start the health check. It can be used when additional tasks are performed before the containers are ready, like running migrations.\n\nWhenever possible, avoid using the tag.\n\nIf you rely on the tag (which isn't really a \"tag\" since it's applied by default when an image isn't explicitly tagged), you can't tell which version of your code is running based on the image tag. It makes it challenging to do rollbacks and makes it easy to overwrite it (either accidentally or maliciously). Tags, like your infrastructure and deployments, should be immutable.\n\nRegardless of how you treat your internal images, you should never use the tag for base images since you could inadvertently deploy a new version with breaking changes to production.\n\nFor internal images, use descriptive tags to make it easier to tell which version of the code is running, handle rollbacks, and avoid naming collisions.\n\nFor example, you can use the following descriptors to make up a tag:\n\nHere, we used the following to form the tag:\n\nIt's essential to pick a tagging scheme and be consistent with it. Since commit hashes make it easy to tie an image tag back to the code quickly, it's highly recommended to include them in your tagging scheme.\n\nSecrets are sensitive pieces of information such as passwords, database credentials, SSH keys, tokens, and TLS certificates, to name a few. These should not be baked into your images without being encrypted since unauthorized users who gain access to the image can merely examine the layers to extract the secrets.\n\nDo not add secrets to your Dockerfiles in plaintext, especially if you're pushing the images to a public registry like Docker Hub:\n\nInstead, they should be injected via:\n• An orchestration tool like Docker Swarm (via Docker secrets) or Kubernetes (via Kubernetes secrets)\n\nAlso, you can help prevent leaking secrets by adding common secret files and folders to your .dockerignore file:\n\nFinally, be explicit about what files are getting copied over to the image rather than copying all files recursively:\n\nBeing explicit also helps to limit cache-busting.\n\nYou can pass secrets via environment variables, but they will be visible in all child processes, linked containers, and logs, as well as via . It's also difficult to update them.\n\nThis is the most straightforward approach to secrets management. While it's not the most secure, it will keep the honest people honest since it provides a thin layer of protection, helping to keep the secrets hidden from curious wandering eyes.\n\nPassing secrets in using a shared volume is a better solution, but they should be encrypted, via Vault or AWS Key Management Service (KMS), since they are saved to disc.\n\nYou can pass secrets in at build-time using build-time arguments, but they will be visible to those who have access to the image via .\n\nIf you only need to use the secrets temporarily as part of the build -- e.g., SSH keys for cloning a private repo or downloading a private package -- you should use a multi-stage build since the builder history is ignored for temporary stages:\n\nThe multi-stage build only retains the history for the final image. Keep in mind that you can use this functionality for permanent secrets that you need for your application, like a database credential.\n\nYou can also use the new option in Docker build to pass secrets to Docker images that do not get stored in the images.\n\nThis will mount the secret from the file.\n\nFinally, check the history to see if the secret is leaking:\n\nIf you're using Docker Swarm, you can manage secrets with Docker secrets.\n\nWhen a container is given access to the above secret, it will mount at . This file will contain the actual value of the secret in plaintext.\n• Google Kubernetes Engine - Using Secret Manager with other products\n\nWe've mentioned using a .dockerignore file a few times already. This file is used to specify the files and folders that you don't want to be added to the initial build context sent to the Docker daemon, which will then build your image. Put another way, you can use it to define the build context that you need.\n\nWhen a Docker image is built, the entire Docker context -- e.g., the root of your project -- is sent to the Docker daemon before the or commands are evaluated. This can be pretty expensive, especially if you have many dependencies, large data files, or build artifacts in your project. Plus, the Docker CLI and daemon may not be on the same machine. So, if the daemon is executed on a remote machine, you should be even more mindful of the size of the build context.\n\nWhat should you add to the .dockerignore file?\n\nIn summary, a properly structured .dockerignore can help:\n• Decrease the size of the Docker image\n\nLint and Scan Your Dockerfiles and Images\n\nLinting is the process of checking your source code for programmatic and stylistic errors and bad practices that could lead to potential flaws. Just like with programming languages, static files can also be linted. With your Dockerfiles specifically, linters can help ensure they are maintainable, avoid deprecated syntax, and adhere to best practices. Linting your images should be a standard part of your CI pipelines.\n\nHadolint is the most popular Dockerfile linter:\n\nYou can see it in action online at https://hadolint.github.io/hadolint/. There's also a VS Code Extension.\n\nYou can couple linting your Dockerfiles with scanning images and containers for vulnerabilities.\n• Scout is the exclusive provider of native vulnerability scanning for Docker. You can use the CLI command to scan images.\n• Trivy can be used to scan container images, file systems, git repositories, and other configuration files.\n• Clair is an open-source project used for the static analysis of vulnerabilities in application containers.\n• Docker Bench is a script that checks for dozens of common best-practices around deploying Docker containers in production.\n\nIn summary, lint and scan your Dockerfiles and images to surface any potential issues that deviate from best practices.\n\nHow do you know that the images used to run your production code have not been tampered with?\n\nTampering can come over the wire via man-in-the-middle (MITM) attacks or from the registry being compromised altogether.\n\nDocker Content Trust (DCT) enables the signing and verifying of Docker images from remote registries.\n\nTo verify the integrity and authenticity of an image, set the following environment variable:\n\nNow, if you try to pull an image that hasn't been signed, you'll receive the following error:\n\nYou can learn about signing images from the Signing Images with Docker Content Trust documentation.\n\nWhen downloading images from Docker Hub, make sure to use either official images or verified images from trusted sources. Larger teams should look to using their own internal private container registry.\n\nShould you use a virtual environment inside a container?\n\nIn most cases, virtual environments are unnecessary as long as you stick to running only a single process per container. Since the container itself provides isolation, packages can be installed system-wide. That said, you may want to use a virtual environment in a multi-stage build rather than building wheel files.\n\nIt's a good idea to limit the memory usage of your Docker containers, especially if you're running multiple containers on a single machine. This can prevent any of the containers from using all available memory and thereby crippling the rest.\n\nThe easiest way to limit memory usage is to use and options in the Docker CLI:\n\nThe above command limits the container usage to 2 CPUs and 512 megabytes of main memory.\n\nYou can do the same in a Docker Compose file like so:\n\nTake note of the field. It's used to set a soft limit, which takes priority when the host machine has low memory or CPU resources.\n\nApplications running within your Docker containers should write log messages to standard output (stdout) and standard error (stderr) rather than to a file.\n\nYou can then configure the Docker daemon to send your log messages to a centralized logging solution (like CloudWatch Logs or Papertrail).\n\nFor more, check out Treat logs as event streams from The Twelve-Factor App and Configure logging drivers from the Docker docs.\n\nGunicorn uses a file-based heartbeat system to ensure that all of the forked worker processes are alive.\n\nIn most cases, the heartbeat files are found in \"/tmp\", which is often in memory via tmpfs. Since Docker does not leverage tmpfs by default, the files will be stored on a disk-backed file system. This can cause problems, like random freezes since the heartbeat system uses , which may block a worker if the directory is in fact on a disk-backed filesystem.\n\nFortunately, there is a simple fix: Change the heartbeat directory to a memory-mapped directory via the flag.\n\nWhen a Docker daemon is exposed to the network or accessed over a network, securing the communication channel is crucial to prevent unauthorized access and ensure the integrity and confidentiality of the data being transmitted. Using TLS (Transport Layer Security) helps in encrypting the communication between the Docker client and the Docker daemon, making it significantly more secure.\n\nTo set up TLS for Docker, you'll need to generate SSL certificates: a CA (Certificate Authority) certificate, a server certificate for the Docker daemon, and a client certificate for the Docker client. These certificates are used to encrypt the communication and authenticate the client and server to each other.\n\nTo learn more, visit Use TLS to protect the Docker daemon.\n\nThis article looked at several best practices to make your Dockerfiles and images cleaner, leaner, and more secure.\n• Run Only One Process Per Container\n• Understand the Difference Between ENTRYPOINT and CMD\n• Lint and Scan Your Dockerfiles and Images"
    },
    {
        "link": "https://snyk.io/blog/best-practices-containerizing-python-docker",
        "document": "From reading many Python Docker container blogs, we’ve found that the majority of posts provide examples of how to containerize a Python application independent of its framework (Django, Flask, Falcon, etc.). For example, you might see something like this:\n\nWith this Dockerfile, we can build and run a Python Flask application:\n\nTwo simple steps and it works just fine, right?\n\nWhile this example is simple and useful for demos and getting started tutorials, it leaves many important concerns unattended. So with that in mind, in this post, we’ll attend to those concerns and take a look at some 6 best practices when containerizing Python applications with Docker. We’ll explore why you should:\n• None Use explicit and deterministic Docker base image tags for containerized Python applications.\n• None Run containers with least possible privilege (and never as root).\n• None Find and fix security vulnerabilities in your Python Docker application image.\n\n1. Use explicit and deterministic Docker base image tags for containerized Python applications\n\nWhile it may seem logical to use as the base image for our Dockerized Python application, it leaves the question of which version of Python is being used.\n\nAt the time of this article being written, the aforementioned base image in the refers to a base image with Python 3.10. The reason? Since we didn't add a specific tag, it defaulted to the version of that base image, which happens to be if we look on the official image page in Docker Hub\n\nSince we would like to control which versions of Python we are containerizing, we should always provide that version information in the Dockerfile.\n\nWell... Not exactly.\n\nThe Docker base image tag of is a full-fledged operating system with Python 3.10 installed, which contains a large amount of libraries that you will probably never use. Also, the fact that it contains such a large amount of software has a side effect: it will increase your security attack surface due to security vulnerabilities present in those libraries.\n\nIf you introduce a large Python Docker image, it will be harder for you to maintain and keep all those library versions up to date.\n\nIf we use the Snyk Advisor tool to examine a Python base image, we can see that the Docker base image has 12 high severity issues, 27 medium severity issues, and 132 low severity issues. So, by default, the Python Docker image will start with a minimum of 171 security vulnerabilities — and we haven't even added anything yet!\n\nAlso, since we are effectively introducing a complete operating system, the size of the base image is going to be quite large for our Python application server, which will lead to slower builds and that will require a bigger amount of space for it. Normally, there are few rules at the time of selecting the base image, but there are two that are key.\n• None Choose the minimum base image that meets all your requirements, and then build on top of that. Smaller images contain fewer vulnerabilities, are less resource intensive, and have fewer unnecessary packages..\n• None Using the named tag isn't enough to ensure that you will always use the same base image. The only way of ensuring this is by using the image digest.\n\nWith that knowledge, let's revisit Snyk Advisor and check which alternative tags it recommends. This tool offers a great overview of the vulnerabilities and size of the base images, which will greatly help us with our decision.\n\nSince we are interested in using Python 3.10 in our application, we will look for that specific tag.\n\nApart from the aforementioned vulnerabilities, we can see that this image has a base size of about 350 MB, it is based on Debian 11, and it has 427 installed packages. We would consider this image to be a bit too much for o ur little Python application.\n\nOn the other hand, there’s also the Docker base image for Python of with 1 high severity issue, 1 medium severity issue and 35 low severity issues. The Docker base size of 46.2 MB, an operating system that is also based on Debian 11, and with 106 installed packages. Simply selecting this Docker base image for our Python application server over the default one will reduce the amount of security vulnerabilities, the size on disk, and the installed libraries count — as well as satisfy our needs for the Python application version of .\n\nAlmost! We fulfilled the first rule — we have a small Docker base image for our requirements — but we still need to address the second issue. We need to ensure that we will actually use that exact Docker base image every single time we build our Python application server.\n\nTo do that, we have several options:\n• None Downloading the Docker image onto our computer with , which reveals the Docker image digest:\n\nIf we already have the Python Docker image on our computer, we can just get the image digest from the current existing image on disk with the command :\n\nOnce we have the base image digest, we can just add it to the aforementioned Dockerfile:\n\nThis practice ensures that every time we rebuild the Docker image for this Python application, the same underlying operating system and library versions are used. This provides a deterministic build.\n\nThis second best practice prevents one of the most common errors in any kind of Docker image that involves projects with dependencies. First, here’s the bad practice:\n• None Copy everything from our project folder into the image context.\n\nWell, it works, but there is a lot to improve on. For starters, when you're developing a project locally, you only install the dependencies when they change, right? So don’t force your Docker image to download and install them every single time that you change the slightest line of code.\n\nThis best practice is about optimizing the layers in a Docker image. If we want to take advantage of the caching system during a Docker build, we should always keep one thing in mind when writing a Dockerfile: the layers should be always ordered based on how prone they are to change.\n\nLet's take a look at the Dockerfile that we have been carrying up until now. Each time that we build that Python application image, the Docker software examines the different layers and answers the question: Did something change or can I just use what I did before?\n\nIn our current form of the Dockerfile contents, any change to our project folder will retrigger the instruction — and subsequently — the rest of the build layers. That doesn't really make sense and it leaves a lot of room for optimization and speed-up.\n\nLet's improve that with the following Dockerfile:\n\nWith this new Dockerfile, the next time Docker checks if layers can be reused, if it finds that there are no changes to the file, it will “jump” straight to the instruction, which will be resolved in a matter of seconds. With this tiny change, we speed up a lot of the build process — no more waiting for minutes between builds each time that we modify something in our code.\n\nIt’s important to note that it’s possible that not all your dependencies are packaged as wheels. In this case, a compiler would need to be installed on your image.\n\nYou’re right, and that's why we're going to show you another amazing Docker feature: the multi-stage builds.\n\nMulti-stage building means that we use a Docker image with more tools for compiling those required dependencies, and afterwards we just copy the artifacts needed to the actual Python Docker image that gets used.\n\nAn example with Node.js based applications would be the following:\n\nNote: This is a brief example to merely demonstrate the capabilities of multi-staging. If you want to learn best practices to containerize Node.js applications properly, consult this article from Liran Tal (that name looks familiar…) and Yoni Goldberg.\n\nMulti-staged builds for Node.js are quite easy to handle, as the folder will lay in the same folder as the actual project, however, this isn't the case when we're dealing with Python applications.\n\nIf we just run , we will install many things in many places, making it impossible to perform a multi-stage build. In order to solve this we have two potential solutions:\n\nUsing could seem like a good option since all the packages will be installed in the directory, so copying them from one stage to another is quite easy. But it creates another problem: you'd be adding all the system-level dependencies from the image we used for compiling the dependencies to the final Docker base image — and we don't want that to happen (remember our best practice to achieve as small a Docker base image as possible).\n\nWith that first option ruled out, let’s explore the second: using a . If we do that, we would end up with the following Dockerfile.\n\nNow, we have all of our required dependencies, but without the extra packages required to compile them.\n\nKnown issues with multi-stage builds for containerized Python applications\n\nCurrently, there's a known problem with pre-stages not being cached. The easiest way to solve this is by using BuildKit and adding the argument to the build process.\n\nWe’ll build normally the first time, but subsequent builds would use the following command:\n\nYou will need the environment variable so Docker knows to use BuildKit for the build process. It can also be enabled by adding the following settings in the Docker software configuration file at (then the environment variable wouldn't be needed):\n\nPython applications built for production that leave debug mode enabled is a big no-no — and a security incident waiting to happen. Unfortunately, it’s a common mistake that we’ve seen across many blog articles about containerized Python Flask applications, and other WSGI Python application frameworks.\n\nThe debugger allows executing arbitrary Python code from the browser, which creates a huge security risk. This can be protected up to a point, but it will always be a vulnerability. One more time for the sake of security: Do not run development servers or a debugger in a production environment! And this also applies to your containerized Python applications.\n\nWe understand that it’s easier to just deploy your Python Flask applications in debug mode than to set up a WSGI server and the web/proxy, but it’s only easier until you have to explain why your application got hacked.\n\nSo how to fix it? First of all, you should decide which WSGI server implementation you would like to use. There are mainly these four that are commonly used for Python applications:\n• None mod_wsgi — An Apache module which can host any Python web application which supports the Python WSGI specification.\n• None CherryPy — A Pythonic, object oriented HTTP framework that also functions as WSGI server.\n\nIn this article, we’ll use Gunicorn for our example, but feel free to read the documentation and information of all of them so you can choose the one that best fits your needs. In this post, we won’t look at configuration, as its entirely use case dependent.\n\nFor the containerization part, we will only need to:\n• None Include the dependency in the\n• None Change the entrypoint of the Python application container (see the instruction for this change):\n\nAfter we rebuild the Python application based on this new Dockerfile, we can run and test that the Flask application is ready to process requests:\n\nNote: We recommend that for actual production-ready deployments, you shouldn't bind directly the port exposed by Gunicorn to the host. Instead, we recommend that you deploy a reverse proxy server in the same network which handles all HTTP requests and serves static files.\n\n4. Run containerized Python applications with least possible privilege (and never as )\n\nThe principle of least privilege is a long-time security control from the early days of Unix — and we should always follow this when we’re running our containerized Python applications.\n\nThe official Docker image doesn't contain a privileged user by default, so we will need to create it in order to be able to run the process with a least privilege user.\n\nTo that end, we'll add the following commands to the Dockerfile in the final image (the second stage of the multi-staged build) which actually runs the process:\n\nThe problem, though, is that with the previous modifications the user owns the system process that gets executed by Docker... What about the file ownership of the copied files or the directory? By default, the Docker compiler will create the directory if it doesn't exist, yet it will create it with the system user as its owner, so any operation which would include writing to that directory could lead to a fatal error of our application. Also, the copied files will be owned by by default if we don't change their behavior, even if we already changed the user.\n\nWith the above updated instructions, we prevent unexpected behaviours to the directory in terms of file ownership, and we ensure that all the files are owned by the same user that is going to run the process.\n\nWhen you’re deploying an application, you need to be mindful of possible unhandled events or problems that could set your application to an unhealthy state where: 1) it won't work anymore, but 2) it won't kill the process. If this happens, your container won't be notified and you'll have a running Python application server that doesn't respond to HTTP requests anymore.\n\nTo avoid that, we want to implement a health check endpoint. In order to check the health of your containerized Python application, we always recommend to include a or HTTP endpoint, to ensure that the application is still able to successfully process user requests. With some web application frameworks for Python, like Flask, this would be an easy task (Flask example below), and in combination with Docker’s instruction, you will ensure that your application is well monitored for health status.\n\nThe following is a Python Flask application snippet that adds a HTTP endpoint:\n\n\"Once you have that endpoint in your application, you'd only need to include the instruction in your Dockerfile:\n\nIf you’re deploying on Kubernetes, then you should know that Dockerfile directives are ignored. A commensurate Kubernetes liveness, readiness and startup probe is needed in your YAML.\n\nSo, to address that for Kubernetes deployments of the above instruction:\n\nNote that this configuration would be nested in the part of the pod or deployment YAML file. Now, with a restart policy of or , our Python application container will always be restarted if it enters an unhealthy state.\n\n6. Find and fix security vulnerabilities in your Python Docker application image\n\nWe’ve already established that bigger Docker base images introduce many concerns, such as a large software stack that we would need to maintain, keep up-to-date with security fixes, and so on.\n\nWe’ve also explored using Snyk Advisor to determine the size and vulnerability metrics in the different base images. But Advisor is just the tip of the iceberg. Snyk is a free developer security platform that you can use to test anything from your own Python code, to your Python dependencies (such as those in ), Python container image running your application, and even the Terraform or Kubernetes configurations that orchestrate it all.\n\nThe best thing about Snyk is that it provides recommended fixes for the vulnerabilities it finds. So, instead of just telling you what security vulnerabilities exist, it will automatically create a fix PR or provide remediation advice. And it does it all from within your existing tools (IDE, CLI, Docker, etc.) and workflows (Git, CI/CD, etc.).\n\nExample: Scanning our containerized Python app with Snyk\n\nLet’s see how that works from the Snyk CLI when we use the Python Docker image of to build this sample Python Flask application.\n\nIf you install the Snyk CLI, you can use it to scan your Python project dependencies, your Python code, and more. So, first off, we install the Snyk CLI. If you have a Node.js environment, you can do it with the package manager as follows:\n\nOr if you’re on macOS or Linux, and are using Homebrew, you can get it installed like this:\n\nFor other installation methods see our guide on how to install the Snyk CLI.\n\nNext up, we need to authenticate from the CLI, to get a valid API token to query the vulnerabilities database:\n\nWith the above completed, we can continue to build a local Docker image of the Python application with the base image:\n\nAnd now, let’s scan it with Snyk by running:\n\nThe output yields the following results (intentionally shortened, as the output is large):\n\nSo, we have 427 dependencies introduced in the form of open source libraries as part of the Python 3.8 operating system’s contents. They introduce a total of 171 security vulnerabilities for this Python Flask application due to the fact that we have chosen that base image of .\n\nAt this point, you might be puzzled and ask, \"how do I fix it?\" Luckily for us, Snyk has some recommendations on which other base images we can upgrade to or switch to entirely, in order to lower the attack surface.\n\nNow we can make an informed, data-driven decision to secure our Python application. By choosing any of the alternative Docker images that Snyk recommended, we can significantly lower the attack surface of the software bundled in our application.\n\nTo take even more control of the security of your application, connect your repositories to the Snyk UI to import your source code and Dockerfile so you can not only find these vulnerabilities, but also continuously monitor them for new security issues. Here’s the same Docker base image report from the Snyk UI:\n\nWhat’s better than monitoring and finding security vulnerabilities? Fixing them! :-)\n\nIf you connect your Git repositories to Snyk, we can then also create pull requests in your repository (automatically!) to offer these Docker base image upgrades like you see here:\n\nIf you like this, check out this follow-up post about automating container security with Dockerfile pull requests.\n\nThose best practices should help you better create, manage, and secure your containerized Python apps. If you enjoyed reading up on these best practices and you value application security and championing for security overall, then we’d recommend the following resources as a follow-up reading material:\n• None Daniel Berman’s excellent getting started with Snyk for secure Python development\n• None Brian Vermeer’s secrets of the Snyk CLI with a great cheatsheet\n• None This comprehensive Node.js best practices for Docker base applications by Liran Tal and Yoni Golberg\n• None And finally, for your Java friends this Docker for Java developers: 5 things you need to know not to fail your security by Brian Vermeer.\n• None A fully featured and insights-rich report on common security issues in Python projects"
    },
    {
        "link": "https://docker.com/blog/how-to-dockerize-your-python-applications",
        "document": ""
    },
    {
        "link": "https://medium.com/@mjrod/deploying-a-python-application-to-docker-4478787c2add",
        "document": "I’ve been brushing up on some Python programming using Angela Yu’s 100 Days of Code. I’ve reached the section where we begin creating web servers using Flask, and building APIs with database backends.\n\nThis ties in well with the previous infrastructure studies I’ve been completing, and I’d like to be able to combine both skills to build out a cloud-based deployment pipeline for my Python projects.\n\nTo begin with, I’ve created a local development Docker host, with Jenkins installed for CI/CD, and Prometheus and Grafana installed for monitoring. My next step is following the Docker documentation for Pythonwhich details deploying a Flask application to a container. I’ve listed the steps I’ve taken below.\n\nI’ll be using the GitLab repo that I’ve previously stored files relating to this Docker host, creating a new branch for this project.\n• to create a folder within the file structure to hold project files\n• to create the code file structure and Flask server entry point, entering the following code:\n• Within the folder, open a terminal instance and create a Python virtual environment, selecting relevant OS specific commands:\n• Following this, in VSCode, press ⇧⌘P and select the Python interpreter\n• I added to the .gitignore file to prevent the folder from being committed to the repo\n• to start the server at URL http://localhost:5000\n• within the hello-py directory create a file to build the custom Docker image\n• specify the base Python image to use\n• use the copied requirements file to install pip dependencies\n• will copy the Python project files into the /hello-py image directory\n• supply the command to run that will launch the server, and make it accessible outside of the container\n\nThe final Dockerfile code should look like this:\n• in the local file’s terminal to build the image using the Dockerfile\n• on my local machine to test that the image was built successfully\n\nI’m using an external port of 5001 on the container on my local machine as I have been using port 5000 for running the Flask server through VSCode’s terminal, and want to make sure there are no conflicts. When we move the image to the remote Docker host I’ll use an external port of 5000\n\nPush to Docker Hub and Deploy to Remote Container\n• run this if required to connect CLI to Docker Hub account\n• tag the image with your Docker Hub username: <username>/<image-name>\n• to push a new repository to Docker Hub to store the image\n\nAfter this, log into the remote Docker host using SSH and run the following\n• to pull the previously uploaded image from Docker Hub\n\nNavigate to http://<remote-docker-ip>:5000 to see the Python application running in the remote container\n• Begin coding my Python application — now that I have the ability to package up my Python code and deploy it to a container, I’d like to begin building high-quality web applications to cement both my programming and infrastructure learnings.\n• Build a Jenkins pipeline to automate the rebuilding phases for each Docker images — each time I make code changes, the steps outlined in this document will need to be repeated to make the images reflect the code base. I’d like to link my Gitlab repo and my Docker host using Jenkins to automate some of this task away."
    },
    {
        "link": "https://docs.docker.com/build/building/best-practices",
        "document": "Multi-stage builds let you reduce the size of your final image, by creating a cleaner separation between the building of your image and the final output. Split your Dockerfile instructions into distinct stages to make sure that the resulting output only contains the files that are needed to run the application.\n\nUsing multiple stages can also let you build more efficiently by executing build steps in parallel.\n\nSee Multi-stage builds for more information.\n\nIf you have multiple images with a lot in common, consider creating a reusable stage that includes the shared components, and basing your unique stages on that. Docker only needs to build the common stage once. This means that your derivative images use memory on the Docker host more efficiently and load more quickly.\n\nIt's also easier to maintain a common base stage (\"Don't repeat yourself\"), than it is to have multiple different stages doing similar things.\n\nThe first step towards achieving a secure image is to choose the right base image. When choosing an image, ensure it's built from a trusted source and keep it small.\n• None Docker Official Images are some of the most secure and dependable images on Docker Hub. Typically, Docker Official images have few or no packages containing CVEs, and are thoroughly reviewed by Docker and project maintainers.\n• None Verified Publisher images are high-quality images published and maintained by the organizations partnering with Docker, with Docker verifying the authenticity of the content in their repositories.\n• None Docker-Sponsored Open Source are published and maintained by open source projects sponsored by Docker through an open source program.\n\nWhen you pick your base image, look out for the badges indicating that the image is part of these programs.\n\nWhen building your own image from a Dockerfile, ensure you choose a minimal base image that matches your requirements. A smaller base image not only offers portability and fast downloads, but also shrinks the size of your image and minimizes the number of vulnerabilities introduced through the dependencies.\n\nYou should also consider using two types of base image: one for building and unit testing, and another (typically slimmer) image for production. In the later stages of development, your image may not require build tools such as compilers, build systems, and debugging tools. A small image with minimal dependencies can considerably lower the attack surface.\n\nDocker images are immutable. Building an image is taking a snapshot of that image at that moment. That includes any base images, libraries, or other software you use in your build. To keep your images up-to-date and secure, make sure to rebuild your image often, with updated dependencies.\n\nTo ensure that you're getting the latest versions of dependencies in your build, you can use the option to avoid cache hits.\n\nThe following Dockerfile uses the tag of the image. Over time, that tag may resolve to a different underlying version of the image, as the publisher rebuilds the image with new security patches and updated libraries. Using the , you can avoid cache hits and ensure a fresh download of base images and dependencies.\n\nTo exclude files not relevant to the build, without restructuring your source repository, use a file. This file supports exclusion patterns similar to files.\n\nFor example, to exclude all files with the extension:\n\nFor information on creating one, see Dockerignore file.\n\nThe image defined by your Dockerfile should generate containers that are as ephemeral as possible. Ephemeral means that the container can be stopped and destroyed, then rebuilt and replaced with an absolute minimum set up and configuration.\n\nRefer to Processes under The Twelve-factor App methodology to get a feel for the motivations of running containers in such a stateless fashion.\n\nAvoid installing extra or unnecessary packages just because they might be nice to have. For example, you don’t need to include a text editor in a database image.\n\nWhen you avoid installing extra or unnecessary packages, your images have reduced complexity, reduced dependencies, reduced file sizes, and reduced build times.\n\nEach container should have only one concern. Decoupling applications into multiple containers makes it easier to scale horizontally and reuse containers. For instance, a web application stack might consist of three separate containers, each with its own unique image, to manage the web application, database, and an in-memory cache in a decoupled manner.\n\nLimiting each container to one process is a good rule of thumb, but it's not a hard and fast rule. For example, not only can containers be spawned with an init process, some programs might spawn additional processes of their own accord. For instance, Celery can spawn multiple worker processes, and Apache can create one process per request.\n\nUse your best judgment to keep containers as clean and modular as possible. If containers depend on each other, you can use Docker container networks to ensure that these containers can communicate.\n\nWhenever possible, sort multi-line arguments alphanumerically to make maintenance easier. This helps to avoid duplication of packages and make the list much easier to update. This also makes PRs a lot easier to read and review. Adding a space before a backslash ( ) helps as well.\n\nHere’s an example from the buildpack-deps image :\n\nWhen building an image, Docker steps through the instructions in your Dockerfile, executing each in the order specified. For each instruction, Docker checks whether it can reuse the instruction from the build cache.\n\nUnderstanding how the build cache works, and how cache invalidation occurs, is critical for ensuring faster builds. For more information about the Docker build cache and how to optimize your builds, see Docker build cache.\n\nImage tags are mutable, meaning a publisher can update a tag to point to a new image. This is useful because it lets publishers update tags to point to newer versions of an image. And as an image consumer, it means you automatically get the new version when you re-build your image.\n\nFor example, if you specify in your Dockerfile, resolves to the latest patch version for .\n\nAt one point in time, the tag might point to version 3.19.1 of the image. If you rebuild the image 3 months later, the same tag might point to a different version, such as 3.19.4. This publishing workflow is best practice, and most publishers use this tagging strategy, but it isn't enforced.\n\nThe downside with this is that you're not guaranteed to get the same for every build. This could result in breaking changes, and it means you also don't have an audit trail of the exact image versions that you're using.\n\nTo fully secure your supply chain integrity, you can pin the image version to a specific digest. By pinning your images to a digest, you're guaranteed to always use the same image version, even if a publisher replaces the tag with a new image. For example, the following Dockerfile pins the Alpine image to the same tag as earlier, , but this time with a digest reference as well.\n\nWith this Dockerfile, even if the publisher updates the tag, your builds would still use the pinned image version: .\n\nWhile this helps you avoid unexpected changes, it's also more tedious to have to look up and include the image digest for base image versions manually each time you want to update it. And you're opting out of automated security fixes, which is likely something you want to get.\n\nDocker Scout's default Up-to-Date Base Images policy checks whether the base image version you're using is in fact the latest version. This policy also checks if pinned digests in your Dockerfile correspond to the correct version. If a publisher updates an image that you've pinned, the policy evaluation returns a non-compliant status, indicating that you should update your image.\n\nDocker Scout also supports an automated remediation workflow for keeping your base images up-to-date. When a new image digest is available, Docker Scout can automatically raise a pull request on your repository to update your Dockerfiles to use the latest version. This is better than using a tag that changes the version automatically, because you're in control and you have an audit trail of when and how the change occurred.\n\nFor more information about automatically updating your base images with Docker Scout, see Remediation.\n\nBuild and test your images in CI\n\nWhen you check in a change to source control or create a pull request, use GitHub Actions or another CI/CD pipeline to automatically build and tag a Docker image and test it.\n\nFollow these recommendations on how to properly use the Dockerfile instructions to create an efficient and maintainable Dockerfile.\n\nWhenever possible, use current official images as the basis for your images. Docker recommends the Alpine image as it is tightly controlled and small in size (currently under 6 MB), while still being a full Linux distribution.\n\nFor more information about the instruction, see Dockerfile reference for the FROM instruction.\n\nYou can add labels to your image to help organize images by project, record licensing information, to aid in automation, or for other reasons. For each label, add a line beginning with with one or more key-value pairs. The following examples show the different acceptable formats. Explanatory comments are included inline.\n\nStrings with spaces must be quoted or the spaces must be escaped. Inner quote characters ( ), must also be escaped. For example:\n\nAn image can have more than one label. Prior to Docker 1.10, it was recommended to combine all labels into a single instruction, to prevent extra layers from being created. This is no longer necessary, but combining labels is still supported. For example:\n\nThe above example can also be written as:\n\nSee Understanding object labels for guidelines about acceptable label keys and values. For information about querying labels, refer to the items related to filtering in Managing labels on objects. See also LABEL in the Dockerfile reference.\n\nSplit long or complex statements on multiple lines separated with backslashes to make your Dockerfile more readable, understandable, and maintainable.\n\nFor example, you can chain commands with the operator, and use escape characters to break long commands into multiple lines.\n\nBy default, backslash escapes a newline character, but you can change it with the directive.\n\nYou can also use here documents to run multiple commands without chaining them with a pipeline operator:\n\nFor more information about , see Dockerfile reference for the RUN instruction.\n\nOne common use case for instructions in Debian-based images is to install software using . Because installs packages, the command has several counter-intuitive behaviors to look out for.\n\nAlways combine with in the same statement. For example:\n\nUsing alone in a statement causes caching issues and subsequent instructions to fail. For example, this issue will occur in the following Dockerfile:\n\nAfter building the image, all layers are in the Docker cache. Suppose you later modify by adding an extra package as shown in the following Dockerfile:\n\nDocker sees the initial and modified instructions as identical and reuses the cache from previous steps. As a result the isn't executed because the build uses the cached version. Because the isn't run, your build can potentially get an outdated version of the and packages.\n\nUsing ensures your Dockerfile installs the latest package versions with no further coding or manual intervention. This technique is known as cache busting. You can also achieve cache busting by specifying a package version. This is known as version pinning. For example:\n\nVersion pinning forces the build to retrieve a particular version regardless of what’s in the cache. This technique can also reduce failures due to unanticipated changes in required packages.\n\nBelow is a well-formed instruction that demonstrates all the recommendations.\n\nThe argument specifies a version . If the image previously used an older version, specifying the new one causes a cache bust of and ensures the installation of the new version. Listing packages on each line can also prevent mistakes in package duplication.\n\nIn addition, when you clean up the apt cache by removing it reduces the image size, since the apt cache isn't stored in a layer. Since the statement starts with , the package cache is always refreshed prior to .\n\nOfficial Debian and Ubuntu images automatically run , so explicit invocation is not required.\n\nSome commands depend on the ability to pipe the output of one command into another, using the pipe character ( ), as in the following example:\n\nDocker executes these commands using the interpreter, which only evaluates the exit code of the last operation in the pipe to determine success. In the example above, this build step succeeds and produces a new image so long as the command succeeds, even if the command fails.\n\nIf you want the command to fail due to an error at any stage in the pipe, prepend to ensure that an unexpected error prevents the build from inadvertently succeeding. For example:\n\nThe instruction should be used to run the software contained in your image, along with any arguments. should almost always be used in the form of . Thus, if the image is for a service, such as Apache and Rails, you would run something like . Indeed, this form of the instruction is recommended for any service-based image.\n\nIn most other cases, should be given an interactive shell, such as bash, python and perl. For example, , , or . Using this form means that when you execute something like , you’ll get dropped into a usable shell, ready to go. should rarely be used in the manner of in conjunction with , unless you and your expected users are already quite familiar with how works.\n\nFor more information about , see Dockerfile reference for the CMD instruction.\n\nThe instruction indicates the ports on which a container listens for connections. Consequently, you should use the common, traditional port for your application. For example, an image containing the Apache web server would use , while an image containing MongoDB would use and so on.\n\nFor external access, your users can execute with a flag indicating how to map the specified port to the port of their choice. For container linking, Docker provides environment variables for the path from the recipient container back to the source (for example, ).\n\nFor more information about , see Dockerfile reference for the EXPOSE instruction.\n\nTo make new software easier to run, you can use to update the environment variable for the software your container installs. For example, ensures that just works.\n\nThe instruction is also useful for providing the required environment variables specific to services you want to containerize, such as Postgres’s .\n\nLastly, can also be used to set commonly used version numbers so that version bumps are easier to maintain, as seen in the following example:\n\nSimilar to having constant variables in a program, as opposed to hard-coding values, this approach lets you change a single instruction to automatically bump the version of the software in your container.\n\nEach line creates a new intermediate layer, just like commands. This means that even if you unset the environment variable in a future layer, it still persists in this layer and its value can be dumped. You can test this by creating a Dockerfile like the following, and then building it.\n\nTo prevent this, and really unset the environment variable, use a command with shell commands, to set, use, and unset the variable all in a single layer. You can separate your commands with or . If you use the second method, and one of the commands fails, the also fails. This is usually a good idea. Using as a line continuation character for Linux Dockerfiles improves readability. You could also put all of the commands into a shell script and have the command just run that shell script.\n\nFor more information about , see Dockerfile reference for the ENV instruction.\n\nand are functionally similar. supports basic copying of files into the container, from the build context or from a stage in a multi-stage build. supports features for fetching files from remote HTTPS and Git URLs, and extracting tar files automatically when adding files from the build context.\n\nYou'll mostly want to use for copying files from one stage to another in a multi-stage build. If you need to add files from the build context to the container temporarily to execute a instruction, you can often substitute the instruction with a bind mount instead. For example, to temporarily add a file for a instruction:\n\nBind mounts are more efficient than for including files from the build context in the container. Note that bind-mounted files are only added temporarily for a single instruction, and don't persist in the final image. If you need to include files from the build context in the final image, use .\n\nThe instruction is best for when you need to download a remote artifact as part of your build. is better than manually adding files using something like and , because it ensures a more precise build cache. also has built-in support for checksum validation of the remote resources, and a protocol for parsing branches, tags, and subdirectories from Git URLs.\n\nThe following example uses to download a .NET installer. Combined with multi-stage builds, only the .NET runtime remains in the final stage, no intermediate files.\n\nFor more information about or , see the following:\n\nThe best use for is to set the image's main command, allowing that image to be run as though it was that command, and then use as the default flags.\n\nThe following is an example of an image for the command line tool :\n\nYou can use the following command to run the image and show the command's help:\n\nOr, you can use the right parameters to execute a command, like in the following example:\n\nThis is useful because the image name can double as a reference to the binary as shown in the command above.\n\nThe instruction can also be used in combination with a helper script, allowing it to function in a similar way to the command above, even when starting the tool may require more than one step.\n\nFor example, the Postgres Official Image uses the following script as its :\n\nThis script uses the Bash command so that the final running application becomes the container's PID 1. This allows the application to receive any Unix signals sent to the container. For more information, see the reference.\n\nIn the following example, a helper script is copied into the container and run via on container start:\n\nThis script lets you interact with Postgres in several ways.\n\nOr, you can use it to run Postgres and pass parameters to the server:\n\nLastly, you can use it to start a totally different tool, such as Bash:\n\nFor more information about , see Dockerfile reference for the ENTRYPOINT instruction.\n\nYou should use the instruction to expose any database storage area, configuration storage, or files and folders created by your Docker container. You are strongly encouraged to use for any combination of mutable or user-serviceable parts of your image.\n\nFor more information about , see Dockerfile reference for the VOLUME instruction.\n\nIf a service can run without privileges, use to change to a non-root user. Start by creating the user and group in the Dockerfile with something like the following example:\n\nAvoid installing or using as it has unpredictable TTY and signal-forwarding behavior that can cause problems. If you absolutely need functionality similar to , such as initializing the daemon as but running it as non- , consider using “gosu” .\n\nLastly, to reduce layers and complexity, avoid switching back and forth frequently.\n\nFor more information about , see Dockerfile reference for the USER instruction.\n\nFor clarity and reliability, you should always use absolute paths for your . Also, you should use instead of proliferating instructions like , which are hard to read, troubleshoot, and maintain.\n\nFor more information about , see Dockerfile reference for the WORKDIR instruction.\n\nAn command executes after the current Dockerfile build completes. executes in any child image derived the current image. Think of the command as an instruction that the parent Dockerfile gives to the child Dockerfile.\n\nA Docker build executes commands before any command in a child Dockerfile.\n\nis useful for images that are going to be built a given image. For example, you would use for a language stack image that builds arbitrary user software written in that language within the Dockerfile, as you can see in Ruby’s variants .\n\nImages built with should get a separate tag. For example, or .\n\nBe careful when putting or in . The onbuild image fails catastrophically if the new build's context is missing the resource being added. Adding a separate tag, as recommended above, helps mitigate this by allowing the Dockerfile author to make a choice.\n\nFor more information about , see Dockerfile reference for the ONBUILD instruction."
    }
]