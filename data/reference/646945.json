[
    {
        "link": "https://stackoverflow.com/questions/76511391/in-python-how-do-i-use-a-dictionary-to-translate-an-rna-sequence-to-a-protein-s",
        "document": "I'm able to import an RNA sequence (eg. AUGCCGACCCGCAGUCCCAGCG) and define a dictionary for translating it into a protein sequence (eg. AUG = M, CCG = Q, ACC = T). Every 3 letters of RNA sequence codes for one letter of protein sequence, and this translation is defined in the dictionary. How do I get Python to read the RNA sequence and output the protein sequence?\n\nWhen I run it, nothing happens. No error message.\n\nEdit: I forgot to add that I want the program to print the resulting protein sequence. I think RNA_sequence is defined as a string."
    },
    {
        "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11713021",
        "document": "Cyclic peptides, a class of peptides with cyclic structures, have garnered significant attention in drug design due to their unique drug-like properties. Cyclic peptides have exceptional biological stability and high selectivity for specific targets [1, 2] and exhibit various biological activities, including antibacterial, antitumor, and neurological applications [3–6]. As of now, the Food and Drug Administration (FDA) has approved over 40 cyclic peptide drugs [7], including zilucoplan in October 2023 for generalized myasthenia gravis [8]. Additionally, the early-approved cyclic peptide drug cyclosporine is widely used to prevent rejection in organ transplants and treat autoimmune diseases, making it a first-line treatment option worldwide for preventing graft rejection post-transplant [9]. Despite their significant potential, cyclic peptides encounter challenges in drug design arising from their complex amino acid composition and diverse cyclization methods (such as head-to-tail cyclization, side-chain linkage, and internal cyclization) [10, 11]. The advancements in computing power and artificial intelligence (AI) have empowered researchers to develop data-driven prediction algorithms, including AI-driven Drug Design (AIDD) and Computer-Aided Drug Design (CADD) [12–15]. However, utilizing these algorithms requires extensive standardized knowledge of cyclic peptides, such as amino acid composition and modifications, cyclization sites, and physicochemical properties, which introduces new challenges in cyclic peptide drug design [16–18]. In CADD, researchers commonly employ traditional cheminformatics toolkits, such as RDKit [19] and Biopython [20], for preliminary analyses. Nevertheless, these toolkits are primarily designed for small molecules and linear peptides and are suboptimal for cyclic peptide molecules. Emerging tools like pyPept demonstrate considerable promise in automating the generation of 2D and 3D representations of complex peptides, but its capacity to handle non-natural amino acids and predict the bioactive conformations of peptides remains constrained [21]. AI-driven cyclic peptide design tools such as AfCycDesign have also emerged; however, these tools currently focus on specific functions, for example, cyclic peptide generation [22]. While specialized tools for identifying cyclic peptide amino acid monomers exist—for example, Smiles2Monomers developed by Yoann Dufresne in 2015 [23]—these tools have limitations, including restricted functionality, cumbersome usage, and poor scalability, making them insufficient for current needs in cyclic peptide drug development. Therefore, we introduce cyclicpeptide, a Python package tailored for cyclic peptide drug design. This package offers functionalities including Structure2Sequence, Sequence2Structure, GraphAlignment, and PropertyAnalysis, covering various application scenarios in cyclic peptide drug development. The open-source code also allows users to extend its capabilities. By integrating these comprehensive tools, cyclicpeptide seeks to seamlessly incorporate cyclic peptides into modern AI-driven drug development workflows, thus expediting the advancement of cyclic peptide–based therapeutics.\n\ncyclicpeptide is a Python package developed based on RDKit [19] and NetworkX [24], specifically designed for the analysis and processing of cyclic peptide molecules. It includes tools such as Structure2Sequence, Sequence2Structure, GraphAlignment, PropertyAnalysis, StructureTransformer and SequenceTransformer (for format transformation), and SequenceGeneration. This package is open-source (https://github.com/dfwlab/cyclicpeptide) and provides detailed documentation (https://dfwlab.github.io/cyclicpeptide/) to support users in effectively utilizing tools (Fig. 1A). The workflow and methodology overview of the cyclicpeptide python package. (A) Overview of the cyclicpeptide framework. Cyclicpeptide is developed based on RDKit and NetworkX, which includes functionalities: Structure2Sequence, Sequence2Structure, GraphAlignment, format transformation (StructureTransformer and SequenceTransformer), PropertyAnalysis, and SequenceGeneration. (B) Workflow diagram of Structure2Sequence. The process begins with detecting the backbone from the cyclic peptide structure, and then expanding the backbone to locate each monomer. The sequence is generated via monomer alignment and assembly, with an option to customize the monomer library during alignment. Structure2Sequence can produce an output report in HTML format. (C) Workflow diagram of Sequence2Structure. By decoding the sequence, the order of each amino acid and its bonds are determined. Then, through monomer alignment, the peptide is assembled into a head-to-tail cyclic peptide chain. (D) Schematic diagram of the GraphAlignment tool. Amino acid sequences are converted into sets of nodes and edges and then visualized using NetworkX. Two algorithms are available for similarity calculation: Graph_similarity() for computing graph edit distance and mcs_similarity() for determining the maximum common subgraph. (E) Various physicochemical properties can be calculated in PropertyAnalysis. (F) Structural format and sequence format transformation. StructureTransformer also provides 3D structure prediction. (G) Schematic diagram of the SequenceGeneration tool. This tool automatically generates new cyclic peptide sequences based on the amino acid substitution table. (H) Comparison of functionalities across different tools. Cyclicpeptide offers comprehensive functionality and is suitable for various analyses and processing tasks in cyclic peptide development. This tool performs amino acid monomer identification on the SMILES (Simplified Molecular Input Line Entry System) representation of cyclic peptides, splitting them into amino acid residues and converting them into sequence format (Fig. 1B). The Structure2Sequence function is suitable for various cyclization methods and supports over 500 natural and non-natural amino acid monomers (Table S1). With Structure2Sequence, researchers can quickly infer possible sequences from cyclic peptides with known structures. To visualize the structure-to-sequence conversion process, the tool also offers an HyperText Markup Language (HTML) report, which includes graphs of the cyclic peptide structure and the conversion process (Fig. 1B). This report enables users to verify the accuracy of the conversion and make adjustments if necessary. This tool can convert known sequences (in one-letter code or amino acid chains) into cyclic structures (SMILES format). By converting cyclic peptide sequences into graphs, the monomers and their bonding relationships were decoded. Sequence2Structure then extracts the corresponding monomer structures from the monomer library and assembles the cyclic peptide structure based on these bonding connections (Fig. 1C). The GraphAlignment tool enables similarity comparison between cyclic peptide molecules (Fig. 1D). Leveraging NetworkX, it converts amino acid sequence format into graphs, allowing for an intuitive assessment of similarity between two cyclic peptide molecules in terms of nodes and edges. This approach overcomes the limitations of traditional linear sequence alignment algorithms when applied to cyclic structures. GraphAlignment offers two similarity metrics: graph edit distance and maximum common substructure. PropertyAnalysis computes physicochemical properties and provides metrics such as Topological Polar Surface Area, Complexity, Log(P), Hydrogen Bond Donor Count, and Hydrogen Bond Acceptor Count (Fig. 1E), assisting researchers in understanding the drug-like properties and application potential of cyclic peptides more effectively. cyclicpeptide also offers tools for format transformation of both structures (via StructureTransformer) and sequences (via SequenceTransformer) (Fig. 1F). The StructureTransformer supports multiple structural formats such as SMILES, InChI, and Protein Data Bank (PDB) for 3D structures, while the SequenceTransformer covers various sequence formats, including one-letter codes, IUPAC (International Union of Pure and Applied Chemistry) condensed format, amino acid chains, and graphical representations. These two tools can standardize the formats of cyclic peptide sequences and structures provided by users, ensuring compatibility with downstream tools such as Structure2Sequence, Sequence2Structure, and GraphAlignment. cyclicpeptide provides a sequence-based tool for generating cyclic peptides (SequenceGeneration, Fig. 1G). Users can automatically generate potential new cyclic peptide sequences using either a built-in amino acid substitution table or a custom table, which can be fed into other tools for further exploration. We compared cyclicpeptide with several existing tools for peptide analysis (Fig. 1H). For instance, smiles2monomers is a web-based visualization tool that primarily focuses on structure conversion. RDKit provides visualization along with mutual conversion between sequence and structure, property calculations, and structural format transformation; however, it is mainly designed for small molecules. Biopython offers various fundamental features but lacks functionality for cyclic peptide design. Tools like pyPept and AfCycDesign concentrate on cyclic peptide structure generation with limited additional functionality. In contrast, our cyclicpeptide offers a comprehensive suite of tools tailored for cyclic peptides, including mutual conversion between sequence and structure (via Structure2Sequence and Sequence2Structure), sequence comparison (via GraphAlignment), structure and sequence format transformation (via StructureTransformer and SequenceTransformer), cyclic peptide generation (via SequenceGeneration), property calculation (via PropertyAnalysis), and visualization. Together, these features provide robust support for cyclic peptide drug development.\n\nTo demonstrate the reliability of cyclicpeptide package, we extracted four sets of cyclic peptide data from the CyclicPepedia knowledge base [25]: (i) 830 cyclic peptides with both structure and sequence information, (ii) 484 monocyclic peptides with structure and sequence information, featuring head-to-tail cyclization, (iii) 670 monocyclic peptides with structural data and head-to-tail cyclization, and (iv) 4658 monocyclic peptides with sequence information and head-to-tail cyclization (4342 in one-letter code format and 316 in IUPAC condensed format) (Fig. 2A). These four datasets were used to validate the accuracy and stability of Structure2Sequence and Sequence2Structure (Fig. 2B). Accuracy is defined by whether the structure (or sequence) obtained from sequence-to-structure (or structure-to-sequence) conversion matches the observed structure (or sequence). Stability is defined by the consistency maintained when a sequence (or structure) is converted to a structure (or sequence) and then converted back to its original form. Validation of the cyclicpeptide python package. (A) Selection process of the validation datasets. From the 8745 cyclic peptides in the CyclicPepedia knowledge base, the following subsets were selected: 830 cyclic peptides with both structural and sequence information, 484 monocyclic peptides with both structure and sequence information as well as head-to-tail cyclization, 670 head-to-tail monocyclic peptides with solely structure information, and 4658 head-to-tail monocyclic peptides with only sequence information. (B) Reliability validation is divided into accuracy validation and stability validation. Accuracy validation involves converting structures (or sequences) into sequences (or structures) using conversion tools (Structure2Sequence and Sequence2Structure) and then comparing the results with their corresponding known counterparts. Stability validation involves comparing processed structures or sequences after two rounds of conversion with the originals to assess information loss during the conversion process. (C) Accuracy validation of the Structure2Sequence tool. Conformational differences refer to discrepancies in amino acid conformations between converted sequences and originals, often arising from unspecified conformations and mixed conformations in the original sequences. (D) Accuracy validation of the Sequence2Structure tool. Incorrect conversions were primarily due to special cyclization methods as well as the presence of prosthetic groups (e.g. metal ions and acetic acid). (E) Stability validation of structure conversion. (F) Stability validation of sequence conversion. Conformational differences appeared in the IUPAC condensed format. The one-letter code format does not specify conformations. The a set was divided into multiple subsets based on amino acid composition (natural and non-natural amino acids) and cyclization type (monocyclic and polycyclic) for validating the Structure2Sequence. The Structure2Sequence achieved an average amino acid detection rate of 99.6% based on the existing dataset (Fig. 2C, Table S2). Among the remaining 0.4%, 0.1% were due to inconsistencies between the original sequence and structure (Fig. S1A), and 0.3% were due to incorrect splitting of amino acids in ester-bond cyclization (Fig. S1B). Errors in the segmentation of ester-bond-cyclized amino acids require further judgment based on prior knowledge of chemical reactions. Currently, cyclicpeptide provides detailed reports of multiple segmentation strategies for users. The b set was used to validate Sequence2Structure, achieving a structure prediction accuracy of 97.7% (Fig. 2D). Incorrect conversion primarily arose from special cyclization methods, such as disulfide bonds and ester bonds, as well as the presence of prosthetic groups (e.g. metal ions and acetic acid). Discrepancies stem from the tool’s default assumption of head-to-tail peptide bond cyclization (Fig. S1C), making it less effective for predicting structures involving side-chain, ester-bond, or disulfide-bond cyclization. External editing tools are therefore necessary to modify these results. Furthermore, stability validation of Structure2Sequence and Sequence2Structure was performed using datasets c and d (Fig. 2B). As shown in Fig. 2E and F, both conversion tools demonstrated high stability (dataset c: 98.2% and dataset d: 97.5%), ensuring that no information about the cyclic peptides was lost during the conversion process, except for prosthetic groups. To further assess the practicality of cyclicpeptide, we compared it with several existing analytical tools, such as Smiles2Monomers, Biopython, RDKit, and pyPept, in terms of processing time and result accuracy. As shown in Table 1, both Smiles2Monomers and the Structure2Sequence tool in cyclicpeptide accurately identified amino acids during structure-to-sequence conversion. However, for the same data volume, Structure2Sequence was significantly faster than Smiles2Monomers—processing 452 cyclic peptides in 57.2 seconds. In addition, both pyPept and cyclicpeptide’s Sequence2Structure tool can convert cyclic peptide sequences into SMILES format, but Sequence2Structure demonstrated higher conversion efficiency. pyPept requires input sequences in Boehringer Ingelheim Line Notation (BILN) or Hierarchical Editing Language for Macromolecules (HELM) format, which adds an extra step before conversion. For the database lookup comparison, we used the head-to-tail cyclic peptide “AAGFPVFF” as the query (Table 2). The database (N = 452) included both the exact sequence “AAGFPVFF” and a rearranged cyclic variant, “PVFFAAGF.” We evaluated whether these tools could retrieve both “AAGFPVFF” and “PVFFAAGF” from the database. Biopython, which is designed for linear peptides, failed to identify the cyclic variant, yielding a similarity score of 0.54. In contrast, RDKit and GraphAlignment demonstrated robust performance; however, they each have limitations. RDKit requires complete structural information as it focuses on recognizing structural similarity (Table S2), while GraphAlignment has a lengthy processing time. To speed up GraphAlignment, we developed an estimation model for GraphAlignment using a graph convolutional network (GA_GCN, Fig. S2, Table 2). GA_GCN can accurately predict GraphAlignment scores (mean square error = 0.000295, Pearson r = 0.995, Spearman r = 0.985, Fig. S2C) and significantly reduces computation time (0.3 s, Table 2). Overall, each algorithm captures different cyclic peptide properties (Fig. S3), offering options for diverse research needs.\n\nCase study 1: mutual conversion between structure and sequence and similarity analysis In this case study, we used three examples to better illustrate the application of fundamental functions in cyclicpeptide (Fig. 3). Structure2Sequence was employed to convert the structure of “Alpha-Amanitin” into sequences (Fig. 3A). Users can view the matched amino acids and possible sequences through the output HTML report. Additionally, Structure2Sequence differentiates between peptide and nonpeptide bonds using solid and dashed lines, respectively. Example results of cyclicpeptide’s fundamental functions. (A) The SMILES string of “alpha-Amanitin” was converted into a sequence using Structure2Sequence. (B) The sequence “AIPFNSL” was converted to a SMILES string using Sequence2Structure with the parameter “cyclic” set to true. (C) The “graph_similarity” (graph edit distance) and “mcs_similarity” (maximum common subgraph) computed by GraphAlignment are shown. The Sequence2Structure supports sequence formats including amino acid chains and one-letter code. The sequence “AIPFNSL” was fed into Sequence2Structure for sequence-to-structure conversion (Fig. 3B). The tool split the sequence into individual amino acids and connection bonds and then matched them with the monomer library. The identified monomers were assembled into a cyclic peptide through head-to-tail cyclization. Using “Cys(1)(2)—Cys—OH-DL-Val(2)—4OH-Leu—OH-Ile(1)” as the Reference sequence for the GraphAlignment algorithm, three similar sequences were selected as queries. As shown in Fig. 3C, GraphAlignment correctly determined that Query1 is the same cyclic peptide as the Reference. It also accurately highlighted the differences in connection bonds and amino acids between the Reference and Query2, as well as between the Reference and Query3. Moreover, users can choose between the graph edit distance or the maximum common substructure algorithms to calculate similarity.\n\nTo demonstrate the role of the cyclicpeptide toolkit in cyclic peptide drug design, we conducted a case study on the design of cyclosporin A analogs. Sequence alignment of cyclosporin A and its analogs reveals that the amino acids at positions 2 and 3 may be critical for determining immunosuppressive activity [26] (Fig. 4A). Derivatives such as alisporivir and SCY-635 eliminate immunosuppressive effects while enhancing antiviral or other pharmacological activities (Fig. 4B). Design of novel cyclosporine A (CsA) analogs. (A) The workflow for drug design applications using the cyclicpeptide toolkit, exemplified by CsA. Structures of CsA and its analogs (such as Alisporivir) were obtained from drug databases like DrugBank. The Structure2Sequence tool was used to derive the amino acid chains, and GraphAlignment was employed to compare sequence similarities, identifying key sites for modification. Based on these findings, specific modification rules were established. SequenceGeneration was used to randomly generate cyclic peptide sequences according to the established criteria. These sequences were converted into 3D structures via Sequence2Structure and StructureTransformer and then docked with the known target, cyclophilin a (CypA) from the PDB database, using AutoDock. Analogs with better docking efficiency were selected, and their physicochemical properties were calculated using PropertyAnalysis for further study. (B) The sequence of CsA analogs, with highlighted amino acids indicating differences from the CsA sequence. (C) Illustration of the interaction between CsA and CypA, highlighting specific binding sites. (D) Illustration of the interaction between CsA analogs, such as alisporivir and other analogs, with CypA, along with their specific binding sites. Based on these observations, modification rules were established in SequenceGeneration, resulting in 66 cyclosporin A analog sequences with variations at positions 2 and 3 and random modifications at other positions (variation rate < 30%, Table S3). Using the Sequence2Structure and StructureTransformer, the structures of these sequences were predicted and then subjected to protein–target docking with cyclophilin A. Among them, three top-scoring analogs (CsA_RP47, CsA_RP29, and CsA_RP48) exhibited the highest docking activity with cyclophilin A (Fig. 4B and D). Compared to cyclosporine A (Fig. 4C), these three analogs displayed a spatial conformation similar to that of Alisporivir, suggesting that they may also possess nonimmunosuppressive properties (Fig. 4D). Once the cyclicpeptide toolkit has been successfully installed, users need to call specific modules (Structure2Sequence, Sequence2Structure, GraphAlignment, etc.) to utilize its functions. For structure-to-sequence conversion, users can import the Structure2Sequence module from the cyclicpeptide package and call the transform() function to convert structures (SMILES). The “monomers_path” parameter defaults to the built-in monomer library, but users can specify a custom monomer library through this parameter. The “report” parameter controls whether to output a results report (Fig. 5A). Code usage examples. (A) Example usage of the Structure2Sequence tool. The input structure for the transform() function must be in SMILES format; other formats can be converted using the StructureTransformer tool. (B) Example usage of the Sequence2Structure tool. The sequence input for seq2stru_non_naturalAA() must be in amino acid chain format, while the sequence input for seq2stru_naturalAA() can be in either one-letter code or amino acid chain format. Other formats can be converted using the SequenceTransformer tool. (C) Example usage of the GraphAlignment tool. The sequence input for sequence_to_node_edge() must be in amino acid chain format. For sequence-to-structure conversion, first use the reference_aa_monomer() function to read the monomer library and then use seq2stru_non_naturalAA() to convert the sequence (amino acid chain) into a structure (SMILES) (Fig. 5B). For sequence similarity comparison, first use the sequence_to_node_edge() function from GraphAlignment module to obtain nodes and edges information from sequences (amino acid chain). Then, create a NetworkX graph using the create_graph() function and choose a similarity calculation method (graph_similarity(), mcs_similarity()) to compute the similarity value (Fig. 5C). In addition, cyclicpeptide provides the IOManager module for drawing and exporting images, such as plot_smiles() (Fig. 5B) and graph2svg() (Fig. 5C). Detailed usage instructions for the toolkit can be found in the documentation (Supplemental file, https://dfwlab.github.io/cyclicpeptide/).\n\nDespite cyclic peptides becoming a significant focus in drug development, there remains a lack of analytical tools specifically developed for cyclic peptide drug design. To address this, we developed a Python package named cyclicpeptide to assist in cyclic peptide drug design. This package provides tools such as Structure2Sequence, Sequence2Structure, GraphAlignment, and other functionalities, offering technical support for early-stage drug design of cyclic peptides. AIDD and CADD are becoming integral in drug design, complementing traditional experimental methods. These new techniques, particularly through the use of deep learning and machine learning, can predict the membrane permeability of cyclic peptides and facilitate the design of target-specific cyclic peptide drugs for specified targets and diseases, significantly accelerating the drug design process [27–29]. These data-driven drug design approaches heavily rely on the quality and quantity of datasets [30, 31]. cyclicpeptide effectively addresses this issue in the early stages of cyclic peptide drug development by providing standardized datasets through its functionalities, including Structure2Sequence, Sequence2Structure, SequenceGeneration, and format transformation. The GraphAlignment tool in cyclicpeptide facilitates the identification of structural similarities and differences in cyclic peptides, including amino acid composition and bond types, which is crucial for discovering new drug mechanisms and potential targets. However, due to the computational complexity of graph operations, we recommend optimizing performance through parallel computing, our GraphAlignment estimation model (GA_GCN), or by integrating GraphAlignment with other tools (e.g. Biopython, RDKit). Moreover, SequenceGeneration can be used to generate new cyclic peptides, and the PropertyAnalysis tool offers comprehensive physicochemical analysis, enabling researchers to discover new cyclic peptide drugs and assess their drug-like properties and application potential. Our cyclicpeptide offers various functionalities for cyclic peptide design but comes with certain limitations that require further optimization. In particular, the GraphAlignment algorithm may result in a notable increase in computation time and memory usage when applied to large-scale datasets. To address this issue, we introduce a graph convolution model to approximate the GraphAlignment scoring. Furthermore, the synthesis of cyclic peptides is a complex and precise process involving various chemical reactions and molecular interactions. However, the current toolkit can only generate 2D structures of cyclic peptides and predict potential 3D structures based on energy minimization, which may differ from the actual experimental data. Users can employ tools like AfCycDesign [22] to generate 3D structures and verify the results. In the future version, we plan to incorporate methods to support the analysis of cyclic peptide synthesis, further expanding the applications of cyclicpeptide. In summary, cyclicpeptide is a powerful analytical tool that significantly accelerates the early-stage design of cyclic peptide therapeutics, with ongoing improvements aimed at enhancing its computational efficiency and user accessibility.\n\nThe Structure2sequence tool is based on the RDKit package (version 2023.9.5), enabling the conversion of cyclic peptide molecules from SMILES format to sequence format. The process involves identifying the cyclic peptide backbone from the SMILES string, followed by expanding the backbone to locate and isolate each amino acid. The isolated amino acid residues are then matched with a monomers library to generate the corresponding amino acid sequence (Fig. 1B). For greater flexibility, a “monomers_path” parameter is provided. By modifying this parameter, users can specify a custom amino acid monomer library to accommodate their research needs. The Sequence2Structure tool, built on the RDKit package, can convert cyclic peptide molecules from sequence format to SMILES format. It first splits the cyclic peptide sequence into an amino acid set and a connection bond set and then retrieves the SMILES string for each amino acid from the monomer library and assembles them sequentially into a complete SMILES structure (Fig. 1C). The function also provides a “monomers_path” parameter. A Boolean parameter named “cyclic” can be set to specify whether a cyclic peptide should be generated. GraphAlignment is used to screen sequences similar to the given amino acid sequence. Specifically, the target amino acid sequences (Reference and Query) are first split into sets of nodes and edges and then visualized using NetworkX (version 3.2.1). To calculate the similarity between sequences, two algorithms can be utilized: graph_similarity() and mcs_similarity() (Fig. 1E). The graph_similarity() algorithm evaluates similarity by counting the minimum number of edit operations (i.e. adding, deleting, or replacing nodes and edges) required to transform one graph into another. In contrast, the mcs_similarity() algorithm measures similarity by identifying the largest subgraph shared by both graphs, that is, the substructure containing the most shared nodes and edges. To accelerate the speed of alignment, we employed a graph convolutional network to approximate the GraphAlignment score (graph edit distance calculated by graph_similarity()). Each cyclic peptide sequence was represented by a graph, with nodes representing amino acid residues and edges representing peptide bonds. Each residue was converted into a vector using an amino acid dictionary. The two input graphs were processed through multiple convolutional layers (Conv1 and Conv2), which aggregated local structural information from neighboring nodes to capture specific patterns and features within each peptide. The encoded information of the two cyclic peptides was then passed through fully connected layers to calculate a similarity score. The model was trained in PyTorch (version 12.1) for 200 epochs. GraphAlignment results of ~80 000 cyclic peptide pairs were used for model construction and testing (with a random 20% reserved for testing). Mean squared error loss was used during training and served as the evaluation metric for the model. StructureTransformer is based on the RDKit package and supports mutual conversion between diverse structural formats, including SMILES, InChI, InChIKey, mol block, and PDB block. It is important to note that to generate a PDB block, the 3D conformation must first be predicted using predict_3d_conformation(mol). SequenceTransformer can read various formats of cyclic peptide sequences, including Graph presentation, IUPAC condensed, amino acid chain, and one-letter code, using the read_sequence(seq) function. It outputs the nodes and edges of the cyclic peptide, which can then be used with the create_sequence(nodes, edges) function to generate sequences in different formats. This allows for convenient conversion between multiple sequence formats. PropertyAnalysis, based on the RDKit, integrates the calculation of various physicochemical properties, for example, exact mass, topological polar surface area, complexity, and hydrogen bond donor count. It also supports the generation of molecular fingerprints in different formats, such as RDKit fingerprints, topological fingerprints, Morgan fingerprints, and MACCSkeys fingerprints. The SequenceGeneration tool can generate new cyclic peptides from a base sequence using either a built-in amino acid substitution table or custom user-defined substitution rules. SequenceGeneration randomly replaces specified amino acids to create novel cyclic peptide sequences. Four validation datasets were selected from the CyclicPepedia knowledge base based on criteria such as structural, sequence information, number of rings, and the cyclization method (head-to-tail cyclization). These sets are designated as follows: (i) cyclic peptides with both structural and sequence information, (ii) cyclic peptides with both structural and sequence information and head-to-tail cyclization, (iii) cyclic peptides with structural information and head-to-tail cyclization, and (iv) cyclic peptides with sequence information (in one-letter code or IUPAC condensed format) and head-to-tail cyclization (Fig. 2A). The reliability of cyclicpeptide was validated from both accuracy and stability perspectives. For accuracy, structures from the a set were converted to sequences using Structure2Sequence, and the resulting sequences were compared to their original sequences using GraphAlignment. The sequences of the b set were converted to structures using Sequence2Structure, and these structures were then compared to the original structures using RDKit to determine consistency. Stability refers to the error-free conversion between cyclic peptide structural and sequence information during transformation. For stability validation, structures from the c set were first converted to sequences using Structure2Sequence. These sequences were then converted back to structures using Sequence2Structure, and the resulting structures were compared to the originals for accuracy. Similarly, for the d set, sequences were first converted to structures and then converted back to sequences, with the final sequences being compared to the original sequences to ensure consistency. (Fig. 2B). Information on cyclosporin A and its known analogs, including SMILES, mechanism of action, targets, and structural details, was collected from databases such as DrugBank and PDB. Using the Structure2Sequence tool, we converted the structural information (SMILES) of cyclosporin A and its analogs into sequences. Sequence similarity was calculated using GraphAlignment. Based on the results of manual alignment, modification rules were established and then applied in SequenceGeneration. Amino acids in the cyclosporin A sequence were randomly substituted to generate multiple analog sequences. The Sequence2Structure and StructureTransformer tools were utilized to predict the 3D structures of these analogs, which were subsequently used as ligands for docking. We conducted batch protein–ligand docking using AutoDock Vina [32]. The crystal structure of cyclophilin A, downloaded from the PDB, was prepared by removing the original ligand, dehydrating, removing nonstandard residues, decharging, and adding hydrogens. The active binding sites were automatically calculated from the protein complex. Ligand structures were minimized for energy using AutoDock Vina. Protein and cyclic peptide ligands were docked in AutoDock Vina, and the docking energy scores were ranked. The top-performing cyclosporin A analogs were selected for further analysis and visualized as complexes in PyMol (V3.1.0) [33]. Their physicochemical properties were calculated using PropertyAnalysis, providing insights for subsequent studies.\n• None Cyclic peptides have enormous potential as therapeutic drugs, but the lack of standardized tools has slowed their development.\n• None We introduced a Python package named cyclicpeptide that provides multiple standardized tools for cyclic peptide drug design.\n• None cyclicpeptide can efficiently process and convert cyclic peptide structure and sequence, which may speed up the early stages of cyclic peptide drug design."
    },
    {
        "link": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00748-2",
        "document": "Peptides as therapeutic or diagnostic agents are a modality with proven translational success in clinical applications; more than 80 drugs on the market are peptide molecules, with others actively in clinical trials [1]. In an accompanying fashion, suitable in silico tools to represent, process, and analyze peptides have been steadily published [2,3,4,5]. Among the available open source tools, many are restricted to natural amino acids, but some also support enhancement by a selected set of non-natural amino acids (NNAAs). This is the case for packages available in the Rosetta Commons project [6, 7], the PEPstrMOD webserver [8], and the SwissSideChain database [9]. Some tools, inspired by full proteins, can fail when dealing with more complex peptidic structures, including staples (e.g., a hydrocarbon chain attached to two amino acids in order to help maintain alpha-helical conformation), non-stapled cyclic peptides, or multi-chain branched peptides [10]. For the representation of peptides, the FASTA format is widely used and can be employed, e.g., to calculate peptide properties based on experimentally available physicochemical properties of natural monomers [11]. However, it is restricted to natural amino acids and simple peptides without extra bonds such as those involved in formation of cyclic peptides. For example, the natural hormone oxytocin can be represented in FASTA by its sequence CYIQNCPLG, but the ring formed by the disulfide bond between the first and sixth cysteine monomers is not accounted for. For more complex macromolecular entities, line notations that go beyond the simple FASTA format have been developed. A prominent example is the Hierarchical Editing Language for Macromolecules (HELM) [12, 13]. It relies on a monomer library that defines the individual monomers and their possible connection points. Together with the information on how these monomers are connected, this allows an unambiguous representation of even very complex biomolecular entities in a string. Recently, we described an intuitive line notation termed BILN (Boehringer Ingelheim Line Notation) [14], where a simple but, critically, human-readable and robust format allows the representation and manipulation of complex multi-chain peptides including staples and cycles. In principle, small molecule cheminformatics tools are also applicable to peptides [15]. However, they look at the peptide as a whole and neglect its construction principle that it is an ordered sequence of monomers, something that potentially could be used for more efficient computations. In addition, even when these tools offer monomer support, this is usually limited to a hard-coded list of amino acids. RDKit, a widely used open-source package for cheminformatics tasks [16], contains functions to process HELM inputs, but only for a set of 48 residues, consisting of the coding L-amino acids, their D-counterparts, and analogs of natural amino acids, specifically norleucine, selenomethionine, ornithine, norvaline, l- and d-alpha aminobutyric acid, pyroglutamic acid, and the acetyl capping group. Another largely unsolved issue is the generation of suitable 3D conformations for peptides that can be used as starting points for MD simulations [17], mutation pipeline analysis [18], or structure-based modeling and drug design. Currently, most tools either focus on structure prediction for whole proteins or small molecules, but they do not cover the middle occupied by medium- to large-sized peptides [5]. Finally, pharmaceutical companies generally require the all-atom representation of a molecule for its registration in various databases, and historically molecules have been drawn manually before registration. Even for short peptides such as the 9-mer oxytocin, manual drawing of these structures can be error-prone, and the rate of error grows as peptide molecules evolve to contain longer main chains, staples, cycles, or fatty-acid chains connected to a peptide main chain. There is a clear need for a platform that can take line formats of peptides, including arbitrary NNAAs, and generate correct atomistic representations with no manual intervention. To fill the void of a toolkit for working with peptides containing arbitrary monomers, unusual connections between monomers, or branching, we developed the python-language toolbox pyPept for handling complex peptides. pyPept is internally based on BILN, but it also can accept FASTA or HELM representations as input. Using a monomer library and the information contained in the input string, atom-level logical connections are validated, and the molecule can optionally be converted into a molecule object. This object can then be used to run typical cheminformatics analyses and predict conformers according to structural restraints. Figure 1 shows a general overview of the package. Summary of pyPept architecture and interfaces. Each monomer is mapped to chemical structure through a monomer dictionary, and monomers are connected by bonds defined for each monomer’s R-groups to yield a sanitized Sequence object. Information from the Sequence is used to create a Molecule object, and two options for 2D depictions are provided. Further, one can predict Conformer objects using additional secondary structure restraints. An executable driver program (run_pyPept.py) encapsulates the sequence-to-structure conversion, offering a non-programmatic way to obtain conformers directly from line notations. The solid lines indicate the default run_pyPept.py execution, with supported options shown by dotted arrows Briefly, the package works as follows. The pyPept Sequence class converts the input line notation into a Sequence object, which is an ordered list of monomers together with all of the connectivity information necessary to accurately build the molecule. The Molecule class takes this Sequence object and creates a molecule object with a sanitized 2D representation. The Conformer class leverages distance geometry functionality to generate a 3D conformer. Here we found it necessary to provide secondary structure constraints in the 3D generation to obtain conformations that can be close to a bioactive one. Therefore, we developed a method to predict peptide secondary structure elements, which we packaged into the SecStructPredictor class. In addition, we developed a helper class Converter which can be used to translate from HELM to BILN and back, or to convert a FASTA string into BILN. A wrapper script (run_pyPept.py) is also provided that automates the sequence-to-structure/conformer conversion of a general peptide, thus demonstrating how to connect the individual components, and providing a non-programmatic way to use pyPept by simple command line execution.\n\nFrom the BIOLIP database (version 04.2022) [20], we extracted the 8112 bioactive peptides for which secondary structure annotations were returned by the DSSP software [21]. The peptides, composed of natural amino acids, are unique sequences showing a diverse set of possible bound conformations, including 30% of helical peptides and 10% forming parallel or anti-parallel beta sheets, even for small peptides of five or six amino acids. They were used to develop a matching algorithm between a query sequence and the bioactive conformers. Our method compares the query peptide by matching its amino acids to those contained in database sequences, where a substitution matrix generates the matching score [22]. The selected matrix was fitted to capture the similarity between known protein structures and is available in the BioPython package [19]. We chose to not allow alignment gaps, thus, the comparisons are made between fragments of the same length. Therefore, if the query sequence is shorter than the database sequence, we compare it with fragments of the database sequence. Inversely, when the database sequence is shorter than the query sequence, the query sequence is fragmented to obtain peptides of the same length. In practical applications, we recommend a peptide query length in the range of 5–20 amino acids, given that the reference set of bound peptides from BIOLIP has a maximum length of 30 amino acids. For each comparison between sequences A and B, we calculate a similarity score using the selected substitution matrix and normalize it by: where \\(score_{AB}\\) is the alignment score between the two peptides, and \\(score_{AA}\\) and \\(score_{BB}\\) are the alignment scores for each peptide with itself. After finding matches above a similarity threshold, a profile with the hits is created, and each amino acid in the query sequence is assigned the most frequent secondary structure element. Using a set of experimental peptide structures with different secondary structure categories, we found a threshold for \\(S_{AB}\\) in the range of 0.6–0.7 to be suitable (see Additional file 1). We also compared the predictions of our method against various state-of-the-art tools such as PSIPRED [23], ModPep [24], and AlphaFold2 [25]. Specifically, we selected a list of 38 peptides available in the PDB with lengths between 8 and 17 amino acids and a diverse set of secondary structure motifs to test the predictions (see Additional file 1: Table S2). We found that the approach described here correctly predicts the secondary structure for most peptides, with 8 of 10 correct predictions for complex motifs based on co-occurring alpha-helix and beta-sheet conformations. This result was on par or better than the methods tested (see Additional file 1: Table S3). Our method can be easily embedded with the rest of the pyPept functionalities. Still further, the user can include secondary structure restraints from any other method or simply by manually providing the expected conformation as exemplified in the “Typical workflow” section. Allowing arbitrary monomers in a peptide sequence affords monomer definitions that connect the identifiers in the line notation with the underlying chemical structure. In pyPept, we use a dictionary which, for each monomer, contains the following information:\n• possible leaving groups (that are removed when the respective connection points are used in a bond between monomers)\n• the abbreviation to be used in the peptide line notation\n• the natural analog of a monomer, if applicable\n• additional information about a specific monomer such as its role, i.e., amino acid or capping group, As per the extensible definition of BILN [14], these monomers can be any non-natural amino acid or non-amino acid with annotated leaving groups that will allow the formation of inter-monomer bonds. No assumptions on the type or nature of monomers, or their connections, are made. This allows the formation of additional bonds to describe branched peptides, as well as cyclic peptides using peptide, disulfide, or potentially other types of bonds (see examples in Table 1). The Pistoia Alliance maintains a dataset of 322 HELM monomers [26]. This dataset closely follows the monomer entry (dictionary) format described above; thus, the 322 HELM monomers are adaptable for use in pyPept with only minor conversion effort. For convenience and to remove an entry barrier to apply pyPept, we provide a python script in the repository to convert the HELM monomer dataset into a format suitable for pyPept, as well as a structure definition file (SDF) with this modified monomer information. This can also be used to add proprietary monomers, as long as they are provided as a SDF file in the Pistoia monomer format. Specifically, the user can add the SDF format of the new monomer in the monomers.sdf file, which is located in the “data” folder of pyPept. The SDF requires some tags to allow the correct mapping into the dictionary, including the name of the monomer, the type of monomer (amino acid or capping group), the abbreviated symbol, if the monomer has a natural analog, and the corresponding leaving R-groups to bond other monomers. An example of a monomer entry using the SDF format is provided in the data folder with the name example_preProcessed_monomer.sdf. We note that in the Pistoia monomer set, no PDB residue names are provided. We chose to use the names reported in the chemical component dictionary [27]. If a monomer is not contained in this dictionary, a new random, though nonetheless unique, PDB code is created.\n\nThis is the main class of the pyPept package. It converts the input BILN string (or HELM/FASTA transformed by the Converter class) into a Sequence object. In Table 1 we show some examples of peptides using the three input formats, where the FASTA format can be used only to represent natural amino acids, and it includes no information on branching or cyclization. Table 1 Examples of peptides using the three input formats BILN [14], HELM [13], and FASTA The Sequence object holds a list of dictionaries, with each dictionary containing the necessary information for one monomer in the peptide sequence (see above, Monomer Library). In addition, a Sequence object stores the information about which monomers are connected and which atoms form these bonds. A Sequence object goes beyond a pair of bonds found in two adjacent amino acids of a linear FASTA sequence and also manages the information necessary for cycles, branches, staples, and other peptide-specific bond structure. In all monomer structures, the R-groups at connection points that are not involved in bonds are replaced by their correct leaving group (e.g., the R2-group at the C-terminal end of the peptide is replaced by an OH forming the C-terminal carboxylic acid). During this procedure, checks guarantee that the input BILN string is not malformed, that the correct number of bond identifiers are present, and that it only contains monomers included in the monomer library. As a final processing step, we change the names of the atoms that are part of an amino acid residue and those of the capping groups to follow the IUPAC convention which appends greek letters to the element symbol (e.g., C\\(\\alpha\\) as CA, C\\(\\beta\\) as CB, hydrogens HB2 and HB3 attached to C\\(\\beta\\)). To achieve these changes, the greekify method from the rdkit-to-params package [28] was adapted for our needs. A class method reads the monomer information and stores it in a Pandas DataFrame [29] object to allow easy access for the various Sequence methods. The Molecule class contains methods to convert the Sequence object into an RDKit ROMol molecule object. To accomplish this, we sequentially take each monomer in the Sequence object, merge its RDKit representation with the growing peptide and then add, if applicable, the appropriate bond(s) between the new monomer and the peptide. To obtain an extended conformation of the peptide without overlapping atoms, the rdDepictor module from RDKit is used [30]. Alternatively, we have developed a procedure which changes the phi/psi angles in the protein backbone to obtain an extended 2D conformation and adjusts the torsion between C\\(\\alpha\\) and C\\(\\beta\\) to obtain an aesthetically pleasing 2D depiction of the peptide without overlapping atoms (see Fig. 1 for an example). At this point, the 2D peptide object can be exported by a Molecule object method to different molecular formats, such as SMILES or SDF. Initial tests showed that the inclusion of secondary structure information is necessary to have a chance of obtaining a 3D structure that is close to the experimental conformation and is suitable for 3D modeling tasks. As this experimental information is often unavailable, and the existing secondary structure prediction tools did not return results sufficiently accurate enough for our purposes when applied to short and medium-length peptides, we decided to develop a similarity-based tool to assign secondary structure motifs to the peptides based on a dataset of bioactive conformers available in the PDB (see Methods section). The SecStructPredictor class collects the functionality to obtain, for a given peptide, a prediction of its secondary structure. Since experimental peptide structures are mostly of natural amino acids, in this protocol, non-natural amino acids are first mutated into their natural analogs, then this mutated peptide is compared with all sequences in the database. To assign the natural analog, pyPept checks first if the information about a natural analog was included in the dictionary. If not, a fingerprint-based similarity run is performed between the monomer of interest and the 20 standard natural residues. A potential natural analog is assigned based on the highest Tanimoto score above a threshold of 0.5. Otherwise, the non-natural amino acid is replaced by an alanine. After this mapping and search for matching contexts, the secondary structure element for each residue in the original peptide is returned. The secondary structure categories are: B (beta bridge), H (alpha helix), E (beta strand), S (bend), T (turn), and G (3/10 helix). Of course, any other secondary structure prediction tool can be used to generate these annotations and use them to drive pyPept’s conformer generator. The Conformer class is used to generate a 3D conformer of the peptide. We employ the ETKDGv3 (Experimental-Torsion Knowledge Distance Geometry) method from RDKit followed by minimization of the structure [31, 32]. Using distance geometry without any constraints usually leads to random coil 3D structures. To end up with peptide conformations that are helical, for example, one needs secondary structure information as constraints for the algorithm. As this information often is not available experimentally, we suggest to use a tool to predict the peptide secondary structure. This can be the SecStructPredictor class presented above, or any other method. Based on the input secondary structure elements, fixed distances are assigned in the RDKit-defined distance bounds matrix to force the formation of \\(\\alpha\\)-helix or \\(\\beta\\)-sheet conformations, which is not a feature available in small molecule-oriented packages such as RDKit. The constraints are complemented by the ETKDGv3 knowledge-based potential to predict the peptide conformers. In the case of non-natural amino acids, the natural analogs (if available in the monomer dictionary) are used to assign the secondary structure element. If no natural analog is available, alanines are used instead. At the end of this processing pipeline, a PDB file can be generated with unique 3-letter residue codes and atom names conforming to the IUPAC rules. In our experience, this procedure is suitable for sequences shorter than 20 amino acids; for longer sequences, many well-established protein modeling tools are available as well [33]. We note that AlphaFold [25, 34] can also predict the conformations of even short peptides, which are often surprisingly close to the experimental bound or free structure. However, this is again a tool that can only deal with natural amino acids. Thus, pre- and post-processing steps are necessary: first, replace the NNAA with a close natural analog; second, conduct the AlphaFold prediction; third, mutate the analog monomer back to the NNAA using the conformer obtained. The native input format of the Sequence class is BILN. To allow one to start from a HELM or FASTA representation, we also provide a format conversion class [14]. The Converter class allows a two-way conversion between HELM and BILN, and from FASTA to BILN.\n\nWe envision a typical use case in which one wishes to obtain a 2D representation stored in SDF format, starting from a BILN sequence. With the aforementioned pyPept classes, this could look as follows: From there, a few lines of additional code would then generate a PDB file with a 3D representation of the input peptide, based on the prediction or specified input of its secondary structure: A graphical summary of this workflow is shown in Fig. 2. Detailed description of peptide 2D/3D generation from sequence. a Main components of the monomer dictionary used to define each BILN component and to allow the generation of peptide bonds between them. In addition, the monomer atoms are named according to the IUPAC convention. b Example of a peptide with a non-natural amino acid and the 2D depiction of the RDKit molecular object with modified peptide bond dihedrals to minimize overlapping atoms. c Scheme showing the prediction of the secondary structure of the example peptide in (b), the addition of restrained distances into the RDKit bound matrix, and the subsequent prediction of the most probable conformer using the ETKDGv3 method The workflow above is a routine task. To automate this workflow and remove the need for one to implement it themselves, we provide a command line wrapper script which takes the peptide representation and additional options as command-line arguments: The sequence-to-conformer protocol can be run all at once by executing the provided wrapper script. An example execution using a randomly-generated peptide sequence is as follows: where the capped peptide in BILN format is used as input (with quotation to avoid any mis-processing by the host operating system), and the RDKit built-in function is used to generate the 2D depiction. As examples, we ran the method with a set of peptides having different features, including the presence of non-natural amino acids, capping groups, and the presence of multiple chains (Fig. 3). In the second case (Fig. 3b), the peptide main chain is predicted as partly \\(\\alpha\\)-helical based on our conformer prediction method. In the third case (Fig. 3c), a branched peptide with a bond connection between a lysine and a glutamic acid is shown. Example of peptides formatted with pyPept, sequences are shown in BILN format. a Capped peptide with acetyl group at the N-terminal part and an amino group at the C-terminal part. b A peptide with three non-natural amino acids highlighted in green (Iva: Isovaline), red (meT: N-Methyl-Threonine) and blue (Aib: Alpha-aminoisobutyric acid). In this case, the main peptide was predicted as an \\(\\alpha\\)-helix. c A peptide with a branch generated between a lysine and a glutamic acid through the third R-group located in their side chains. The bridge is identifiable in both the 2D and 3D representations\n\nWith clinical precedent for their therapeutic benefit, peptides have been and will continue to be actively developed, with increasingly complex topologies, necessitating a complementary infrastructure for peptide information communication (e.g., BILN in presentations or patent applications), automated conversion of the human-communicable format into formats that can be directly submitted to compound registration systems, and for computational chemistry purposes. The pyPept package provides a publicly accessible collaborative effort to achieve these goals, with a low barrier to entry which enables less tech-savvy experimental and design research organizations to maximally benefit. pyPept facilitates the generation of 2D and 3D conformations of a peptide even in the presence of non-natural amino acids, non-amino-acid monomers, branches, and cyclic structures, which are certain to increase as peptide synthesis technologies have continued to improve. For peptide design teams, they can easily convert a series of peptides stored in a spreadsheet with one monomer per column into matching BILNs. Then, it is straightforward to directly apply the pyPept 2D depiction pipeline and generate 2D representations for all peptides. Since 2D representation is still at the core of the compound registration process for many companies, use of pyPept for systematic representation generation avoids the error-prone manual drawing of peptide structures. The 3D pipeline produces a peptide structure that can be used as a starting point for MD simulations, structure-based modeling efforts, or other methods to obtain low-energy conformations of the peptide [35]. It remains to be clarified, admittedly, how well our procedure predicts the bioactive conformations of peptides. One of the issues is that all secondary structure predictors (as other peptide/protein conformer predictors, including AlphaFold) work based on natural amino acids. The introduction of NNAAs in a post-processing step may completely alter the local conformation and, thus, the overall structure of the peptide. We fully acknowledge the need for a more systematic analysis. Despite such an aspect, we believe that pyPept is a framework that will facilitate the generation of 2D and 3D structures of complex peptides, reducing human error and accelerating not only drug discovery but all research fields involving peptides.\n• Other requirements: RDKit 2020 or later; Biopython 1.7.9 recommended. The code is available as a Github repository. Any questions related to the implementation can be directed to the authors’ email accounts."
    },
    {
        "link": "http://disi.unitn.it/~teso/courses/sciprog/python_dictionaries.html",
        "document": "A dictionary represents a map between objects: it maps from a key to the corresponding value.\n\nThe abstract syntax for defining a dictionary is:\n\nExample. In order to define a dictionary implementing the genetic code, we write:\n\nHere maps from three-letter nucleotide strings (the keys) to the corresponding amino acid character (the value).\n\nTo use a dictionary, we resort to the usual extraction operator, as follows:\n\nI can use the dictionary to “simulate” the process of translation and convert an RNA sequence into the corresponding aminoacid sequence. For instance, starting from the following mRNA sequence:\n\nI can split it in triples:\n\nAt this point, I can translate the triples with:\n\nOf course, this is a very simple functional model of translation. The most obvious difference is that the above Python code does not care about termination codons. Support for them will be added in due time.\n\nKeys are unique: the same key can not be used more than once. Value are not unique: different keys can map to the same value. In the genetic code example, each key is a unique three-letter string, which is associated to a given value; the same value (for instance is associated to multiple keys:\n\nExample. Let’s build a dictionary that maps from amino acids to their (approximated) volume in cubic Amstrongs:\n\nHere the keys are strings and the values are floats.\n\nThere are no restrictions on the kinds of objects that can appear as values. The keys, however, must be immutable objects. This means that and objects can not be used as keys. More formally, here is what object types you can use where: This restriction is due to how dictionaries are implemented in Python (and most other programming languages, really).\n\nExample. Let’s create a dictionary that maps from amino acids to a list of two properties, mass and volume:\n\nHere the keys are (immutable) and the values are (mutable). Can I create the inverse dictionary (from property list to amino acids)?\n\nNo. Let’s write the very first key-value pair:\n\nIdeally, this dictionary would map from the list:\n\nto . However Python raises an error:\n\nTo solve the problem, I can use tuples in place of lists:\n\nNow that the keys are immutable, everything works.\n\nExtracts the value associated to a key The only major behavioral difference lies in the assignment operator (the last one). The syntax is the same as for lists, and the meaning too; however, for dictionaries it can be used to add entirely new key-value pairs. Let’s see a few examples. Example. Starting from an empty dictionary: I want to build (this time, incrementally) the genetic code dictionary I introduced in the very first example of this chapter. Let’s add the key-value pairs one by one with the assignment operator: Here I am adding new key-value pairs to a dictionary. Whoops, I made a mistake! should map to an , not to an ! I can solve the problem by replacing the value associated to the key , using the same syntax as above: So, if the key was already there, the assignment operator simply replaces the value it is associated with. It does not make sense, however, to extract values associated to keys not present in the dictionary. For instance: # the dictionary contains no such key\n\nif the object is a key of the dict Get the keys as a list Get the values as a list Get the key-value pairs as a list of pairs I can get the list of keys: and the list of both keys and values: Now that I have a bunch of lists, I can apply any of the list operations/methods to perform the tasks I need! Finally, to check whether a given object appears as a key, I can write: Key-value pairs are stored in the dictionary in a seemingly arbitrary order. In other words, Python does not guarantee that the order in which the key-value pairs are inserted is preserved. Here I inserted first, and second. However, the order in which they are stored in the dictionary is the exact opposite! Example. We can use a dictionary to represent a complex structured object, for instance the properties of a protein chain: Of course, writing a dictionary like this by hand is inconvenient. We will later see how such dictionaries can be created by reading the data automatically out of some biological database. Example. Given the following FASTA sequence (cut down to a reasonable size) describing the primary sequence of the HIV-1 retrotranscriptase protein (taken from the PDB): We’d like to compile a dictionary with the same information: From this dictionary, it’s very easy to extract the sequence of every individual chain: as well as computing how many chains there are: Example. Dictionaries can be used to describe histograms. For instance, let’s take a sequence: We compute the number of the various nucleotides: It is easy to write down a corresponding histogram into a dictionary: Let’s say we are now interested in the proportion of adenosine; we can write: We can also check whether the histogram represents a “true” multinomial distribution by checking whether the sum of the probabilities is (approximately) 1: Example. Dictionaries are also very useful for describing protein interaction networks (physical, functional, genomic, you name it): which represents the following network: Here is a tuple with all of the binding partners of protein 1A3A. We can use the dictionary to compute the n-step neighborhood of 1A3A: # 2-step neighborhood of 1A3A (it may include repeated elements) \\ # 3-step neighborhood of 1A3A (again, may include repeated elements) \\ Note that the very same idea can be used to encode social networks (Facebook, Twitter, Google+), to see who-is-friends-with-who, and to discover communities in said networks. Dictionaries are just one of the many ways to encode a network (or graph). An alternative is to use an adjacency matrix, which can be implemented (as usual) as a list of lists."
    },
    {
        "link": "https://academic.oup.com/bib/article/26/1/bbae714/7948245",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/19487149/parameter-validation-best-practices-in-python",
        "document": "even though already answered, this is too long for a comment, so i'll just add an another answer.\n\nIn general, type checking is done for two reasons: making sure your function actually completes, and avoiding difficult-to-debug downstream failures from bad output.\n\nFor the first problem, the answer is always appropriate - EAFP is the normal method. and you don't worry about bad inputs.\n\nFor the second... the answer depends on your normal use cases, and you DO worry about bad inputs/bugs. EAFP is still appropriate (and it's easier, and more debuggable) when bad inputs always generate exceptions (where 'bad inputs' can be limited to the types of bad inputs that your app expects to produce, possibly). But if there's a possibility bad inputs could create a valid output, then LYBL may make your life easier later.\n\nExample: let's say that you call square(), put this value into a dictionary, and then (much) later extract this value from the dictionary and use it as an index. Indexes, of course, must be integers.\n\nsquare(2) == 4, and is a valid integer, and so is correct. square('a') will always fail, because 'a'*'a' is invalid, and will always throw an exception. If these are the only two possibilities, then you can safely use EAFP. if you do get bad data, it will throw an exception, generate a traceback, and you can restart with pdb and get a good indication of what's wrong.\n\nhowever... let's say that your app uses some FP. and it's possible (assuming you have a bug! not normal operation of course) for you to accidentally call square(1.43). This will return a valid value - 2.0449 or so. you will NOT get exception here, and so your app will happily take that 2.0449 and put it in the dictionary for you. Much later, your app will pull this value back out of the dictionary, use it as an index into a list and - crash. You'll get a traceback, you'll restart with pdb, and realize that doesn't help you at all, because that value was calculated a long time ago, and you no longer have the inputs, or any idea how that data got there. And those are not fun to debug.\n\nIn those cases, you can use asserts (special form of LYBL) to move detection of those sorts of bugs earlier, or you can do it explicitly. If you don't ever have a bug calling that function, then either one will work. But if you do... then you'll really be glad you checked the inputs artificially close to the failure, rather than naturally some random later place in your app."
    },
    {
        "link": "https://endjin.com/blog/2023/03/a-look-into-pandera-and-great-expectations-for-data-validation?ref=sangkon.com",
        "document": "Data validation is the process of verifying that data is in an acceptable state for downstream processing (analytics, visualisations, machine learning, etc). It's important for data to conform to the expectations of downstream consumers so that they can use it with confidence; poor data quality issues that go unresolved can have significant deleterious impact on production systems.\n\nPandera and Great Expectations are popular Python libraries for performing data validation. In this blog post I'll provide a broad overview of the features of each library, demonstrate how to create some basic validation tests with them, and provide some thoughts as to which one you should use.\n\nThe type of scenario we're going to look at is one that is typical in the data processing world (i.e.: data science & engineering), for which these libraries are designed. Generally, it looks as follows: you've ingested some raw data that could have quality issues, before going on to do useful things with the data, like analytics and visualisation, you need to verify that it's of adequate quality and meets the expectations of downstream consumers.\n\nTake the following boat sales data, an edited extract from this Kaggle dataset, containing 7 columns: 'Id', 'Price', 'Currency' 'Boat Type', 'Year Built' 'Length', 'Number of visits in the last 7 days'.\n\nSome validation rules immediately come to mind:\n• The values in the 'Id' column should be distinct from one another\n• The values in the 'Price' column should be of type int\n• The 'Currency' and 'Boat Type' are categorical variables, therefore those columns should only take values from defined sets\n• None of the columns should have missing data\n\nThese are the basic sorts of the validation rules you want to check your data against. You could go on to define slightly more sophisticated rules, such as:\n• The number values in the 'Year Built', 'Length' and 'Number of view last 7 days' columns should fall within some sensible range for each column\n\nWhat do you want in a data validation tool?\n\nAt a basic level, you want to be able to define validation rules (i.e. expectations of the data) - like the ones described, validate data against the rules, and have the tool inform you about cases that do not pass the validation. Based on the output of the validation, you can then go on to address the failing cases by whatever means you like. As we'll see though, the two packages under question can do quite a bit more than that.\n\nLet's take a look at the two tools under question in this blog post, starting with Pandera.\n\nFrom their official documentation:\n\nYou can download Pandera from PyPI. When I did this, it resulted in a total of 12 packages being installed on my machine. Pandera provides a type, which provides an easy way for you to define a set of validation checks against the columns in your data. After creating the schema object you can use it to validate against data frame types; the library supports validating against data frame type objects from multiple providers, however we'll just looking at the Pandas in this blog.\n\nThe code above captures the data validation tests suggested at the start of the post in a Pandera schema. The code is quite succinct and doesn't look too dissimilar to Pandas code, which is nice. It could also have been written in YAML or JSON; the library provides functions for converting schemas between the different languages, too.\n\nPandera provides lots of built in checks, some of which I've used above, like and . You can define your own by passing a lambda with boolean return type - see the example above (which could have also been accomplished with the built-in ). You can also define your own extension methods so that they appear in the namespace.\n\nYou can validate your data against tests by simply passing your to the method on the object.\n\nPandera schemas can be written from scratch using Python, as shown above, however you can see how that would become quite tedious and time consuming. To help with this, Pandera provides an function that scans your data and generates a with some basic checks; this is intended to provide a starting point for you to tailor and further develop.\n\nYou can use the function, then the method on the resulting , which converts it to a Python script.\n\nI've copied the autogenerated schema script shown above and placed it in its own Python file ( ) within a folder ( ). Now I'm going to import the schema into a Jupyter Notebook and see how we can use it to better identify issues in our data.\n\nIf you use the default method, you may get a very long error message from which it can be quite difficult to ascertain an understanding of the underlying issue. This is especially true if you're working in notebooks where a cell's output size is limited, causing the error message to be truncated. Also, the default behaviour of is eager, meaning the validation will be halted as soon as an issue is found, this way you don't get to see all the validation failures.\n\nTo work around this, I suggest you set in , and use a try-except to catch any exceptions; you can then pull out the failure cases from the exception into a for better readability.\n\nThe GIF below shows the different approaches described in a Jupyter Notebook.\n\nIn summary Pandera is a nice library, with great documentation, that provides a familiar, easy-to-use API for writing validation tests in Python. The learning curve is shallow, allowing you to become productive quickly; it didn't take me very long at all to download the package and get to a state where I had some basic tests to run against my data.\n\nFrom their official documentation:\n\nGreat Expectations (GE) can also be downloaded from PyPI; I counted a total of 107 package installs when installing this into a fresh environment, which took some time; the package also comes with its own CLI. So, it's clear from the start that this is a pretty hefty package. The official documentation is great, and plentiful. A quick scan of the documentation reveals a whole host of terminology, definitions and concepts, as such, the package demands quite a bit of effort from the new user to get started.\n\nFirst you need to create a Data Context, which is like a GE project inside your Python project that houses everything to do with data validation (configuration, validation results, etc). Next, you create a DataSource, which is like a reference from the data context to a dataset for use later on; the actual dataset can be a file on disk or a database connection (which is pretty neat) - I've just pointed it to local csv file. You're guided through this setup process via the CLI, after a number of multiple choice selections, GE will open a Jupyter notebook in the browser for further configuration of the Data Source. Running all cells in the notebook (i.e. following the default config) results in some YAML being added to the file, which is like the master configuration for the GE project.\n\nNext, you need to create an Expectation Suite. An expectation is just a validation test (i.e.: a specific expectation of the data) and a suite is a collection of these. GE provides multiple paths for creating expectations suites; for getting started, they recommend using the Data Assistant (one of the options provided when creating an expectation via the CLI), which profiles your data and automatically generates validation tests. Once again, GE will launch a notebook in the browser for you to configure the Data Assistant - shown in the GIF below.\n\nUpon running all cells, an expectation suite JSON file is generated, this file defines the expectations that GE generated for us. Running those notebook cells also caused a HTML file to be generated, this is what GE calls a Data Doc. A Data Doc contains the results from running an expectation suite against a batch of data, i.e.: it's a local web page showing the test results. In this case, the Data Doc has been generated by validating the Expectation Suite against the data batch that was used to automatically generate the suite, so we expect all tests to have passed.\n\nPart of the generated Data Doc is shown in the GIF above. You can see that GE has generated some table-level expectations for us, as well as column-level ones. Let's take a look at some of these. For the column we have: \"values must never be null.\", \"values must always be greater than or equal to 6 and less than or equal to 42 characters long.\", and \"values must match this regular expression: .\". The second of these is interesting; my immediate response was that this is useless - you wouldn't be interested in checking that the number of characters in the name of the boat type is within some range, however perhaps a name with 1 or 2 characters would be suspicious; I think in this case you would want to consult your stakeholder with the domain expertise, which may be necessary in many cases anyway. The generated regex doesn't offer much either, it will match against more-or-less any any sequence of characters; for example, it will match the string . Moreover, I don't think it's necessary to check that the name of a boat type conforms to some pattern, again though, this may be one for the domain expert.\n\nFor the column we have: \"values must belong to this set: EUR CHF Â£ DKK.\", which is one of the tests that I suggested at the beginning of the post, so that's good. We also have: \"fraction of unique values must be exactly 0.0006743088334457181.\", that's plain wrong - it's not valid to expect the fraction of boat sales that use a given currency to be exactly equal to some value.\n\nSo, although we can see GE has generated plenty of expectations for us, many of them don't make much sense or are overly stringent for this data. I think that's fine, though. The point of the data assistant (or schema inference in the case of Pandera) is to generate a suite for you to refine and develop; it removes the need to write lots of boiler plate and provides a platform for you to build on. I'd rather have too many generated expectations than too few, since it's easier and quicker to edit and delete than it is to write new ones.\n\nSo, at this point, we'd like to go in and edit and delete some of those expectations. There's a couple of ways you can do this:\n• You can edit the JSON definition of the expectations directly (below is the JSON object representing the \"values should not be null\" expectation on the column, to give you an idea of what they look like)\n• You can edit the expectations interactively with a batch of data.\n\nThe second option is what's recommended in the docs. By 'interactively', they mean that you can edit some python code in a notebook, run it against a batch of data, and see the result. Let's see what that looks like.\n\nNow we've got our desired expectation suite, let's use it to validate a new batch of data; this is simulating the situation where you've got a new data drop incoming that you need to validate. To do this we need to create what GE call a Checkpoint; a Checkpoint associates an expectation suite with a batch of data, you can then run the Checkpoint any number of times in the future to validate the batch of data against the Expectation Suite. Again, by default, GE will launch a Jupyter notebook for you to configure a checkpoint.\n\nYou can see in the GIF shown above that I'm creating a Checkpoint using the Expectation Suite created earlier and with as the batch. Towards the end of the GIF, you can see a new tab in the browser being opened, this is showing the results of running the checkpoint.\n\nA number of the expectations have failed on this new batch of data. At this point (assuming we're happy with the expectations), we'd starting cleaning the data to get those expectations to pass.\n\nGoing further with Great Expectations\n\nThe example I've demonstrated here is quite basic, and makes GE feel a bit overkill. Although it may seem cumbersome, many of the concepts and features of GE are much richer than what the example demonstrates, and I think GE will begin to shine when you want to do data validation in a more complex, real-world, production context. I'm not going to get into that in detail here, but the documentation is great; you can find lots of how-tos for configuring GE in more sophisticated ways and integrating it with other data tools, including cloud-based ones.\n\nTake the Checkpoint concept. The example I've shown uses the SimpleCheckpoint (the default), which points at a specific csv and has some default configuration, like what post validation actions to take; with that configuration, I'd have to create a new Checkpoint if I wanted to run my expectations suite against a new file. But, you can configure a Checkpoint so that it receives a Batch Request at runtime; a Batch Request is like a query which you pass to your Data Source, which then produces a Batch of data. I'm envisioning scenarios where you'd want to integrate GE with your ETL/ELT process, and it seems like this type of functionality would come in very handy.\n\nGreat Expectations is a heavy-weight package with a design that is clearly focused around integration and building production-ready validation systems. It introduces some of its own terminology and concepts, and feels opinionated in how it's helping you do validation. Whilst it's not quick and easy to get up and running, and even though I was frustrated multiple times throughout the process, I suspect all these things will be beneficial when developing in a more complex production setting.\n\nThe key aspect that captures the differences between the two tools is that GE's design seems to have been strongly guided by the idea of building production-ready validation systems that integrate with other data tools, and can form part of a larger automated system. Therefore, naturally, the tool and API has to be larger and more complex to handle that additional complexity. This is reflected in the number of package dependencies: 107 for Great Expectations; 12 for Pandera.\n\nOn the other hand, Pandera seems to have been designed primarily with data scientists in mind, who are typically less concerned with production systems and engineering. Consequently, the tool is much more simple and concise, and I expect that building with Pandera in a production setting and integrating it with other data tools would require more effort. How much more? I don't know. That being said, Pandera does offer integrations with FastAPI; also, you may want to build your own validation system from the \"lower level\" components that Pandera offers.\n\nSo, which one should you choose?\n\nIt depends. Given what I've said above, roughly speaking, if you want to get started writing basic tests quickly or want lower level control over you validation system, then choose Pandera. On the other hand, if you know you need to build a comprehensive validation system that needs to integrate with other tools and want sound help doing so, then Great Expectations is probably the way to go."
    },
    {
        "link": "https://labex.io/tutorials/python-how-to-validate-sequence-type-418550",
        "document": "In the world of Python programming, understanding how to validate sequence types is crucial for writing robust and error-resistant code. This tutorial explores comprehensive techniques for identifying and verifying different sequence types, providing developers with essential skills to ensure data integrity and type safety in their Python applications.\n\n%%%%{init: {'theme':'neutral'}}%%%% flowchart RL python((\"Python\")) -.-> python/DataStructuresGroup([\"Data Structures\"]) python((\"Python\")) -.-> python/FunctionsGroup([\"Functions\"]) python((\"Python\")) -.-> python/PythonStandardLibraryGroup([\"Python Standard Library\"]) python((\"Python\")) -.-> python/BasicConceptsGroup([\"Basic Concepts\"]) python/BasicConceptsGroup -.-> python/numeric_types(\"Numeric Types\") python/BasicConceptsGroup -.-> python/type_conversion(\"Type Conversion\") python/DataStructuresGroup -.-> python/lists(\"Lists\") python/DataStructuresGroup -.-> python/tuples(\"Tuples\") python/FunctionsGroup -.-> python/function_definition(\"Function Definition\") python/FunctionsGroup -.-> python/arguments_return(\"Arguments and Return Values\") python/FunctionsGroup -.-> python/build_in_functions(\"Build-in Functions\") python/PythonStandardLibraryGroup -.-> python/data_collections(\"Data Collections\") subgraph Lab Skills python/numeric_types -.-> lab-418550{{\"How to validate sequence type\"}} python/type_conversion -.-> lab-418550{{\"How to validate sequence type\"}} python/lists -.-> lab-418550{{\"How to validate sequence type\"}} python/tuples -.-> lab-418550{{\"How to validate sequence type\"}} python/function_definition -.-> lab-418550{{\"How to validate sequence type\"}} python/arguments_return -.-> lab-418550{{\"How to validate sequence type\"}} python/build_in_functions -.-> lab-418550{{\"How to validate sequence type\"}} python/data_collections -.-> lab-418550{{\"How to validate sequence type\"}} end"
    },
    {
        "link": "https://dataengineeracademy.com/blog/how-to-validate-datatypes-in-python",
        "document": "How to Validate Datatypes in Python\n\nThis article isn’t just about the ‘how’ — it’s an exploration of the best practices and methodologies seasoned data engineers employ to enforce data types rigorously. We’ll dissect the spectrum of techniques available in Python, from native type checking to leverage robust third-party libraries and distill these into actionable insights and patterns you can readily apply to your projects.\n\nStepping beyond mere syntax, we’ll delve into the realm of designing validation strategies that align with real-world data engineering scenarios — strategies that are proactive rather than reactive, preventing problems before they ever have a chance to manifest.\n\nWhat are the data types of Python?\n\nPython, a dynamically-typed language, offers a variety of data types to handle different kinds of data. Understanding these data types is essential for writing robust code and validating data effectively. Here are the primary data types in Python:\n\nlist: Represents an ordered collection of items, which can be of different types, e.g., [1, 2, 3], [‘a’, ‘b’, ‘c’].\n\ntuple: Represents an ordered collection of items similar to a list, but tuples are immutable, e.g., (1, 2, 3), (‘a’, ‘b’, ‘c’).\n\nNoneType: Represents the absence of a value, e.g., None.\n\nmemoryview: Allows memory access to byte data without copying, e.g., memoryview(b’abc’).\n\nPython provides a variety of built-in methods associated with its data types. These methods allow you to perform common operations and manipulate data efficiently. Here’s a look at some of the key methods for the primary data types in Python:\n\nStrings are sequences of characters and come with many useful methods for text manipulation:\n• str.upper(): Converts all characters in the string to uppercase.\n• str.lower(): Converts all characters in the string to lowercase.\n• str.strip(): Removes leading and trailing whitespace from the string.\n• str.replace(old, new): Replaces all occurrences of old with new in the string.\n• str.split(separator): Splits the string into a list of substrings based on the given separator.\n\nLists are ordered collections of items and support various methods for adding, removing, and modifying elements:\n• list.append(item): Adds an item to the end of the list.\n• list.extend(iterable): Extends the list by appending all the items from the iterable.\n• list.insert(index, item): Inserts an item at a specified index.\n• list.remove(item): Removes the first occurrence of the specified item.\n• list.pop(index): Removes and returns the item at the specified index.\n\nDictionaries are collections of key-value pairs with methods for accessing, adding, and modifying entries:\n• dict.get(key, default): Returns the value for the specified key, or default if the key is not found.\n• dict.update([other]): Updates the dictionary with key-value pairs from other, overwriting existing keys.\n• dict.pop(key, default): Removes and returns the value for the specified key, or default if the key is not found.\n\nSets are collections of unique items with methods for set operations:\n• set.add(item): Adds an item to the set.\n• set.remove(item): Removes the specified item from the set. Raises a KeyError if the item is not found.\n• set.union(other_set): Returns a new set with elements from both sets.\n• set.difference(other_set): Returns a new set with elements in the first set but not in the second.\n\nThese methods associated with various data types in Python provide powerful tools for manipulating and interacting with data, allowing you to write more efficient and effective code.\n\nWhen handling data in Python, validating datatypes is a process we weave into our workflow to avoid the domino effect of type-related errors. Our toolkit is rich with Python’s built-in capabilities and bolstered by third-party libraries that give us flexibility and power. Here’s a breakdown of some core techniques for datatype validation that are essential in the repertoire of any data engineer.\n\nOne of the simplest ways to validate datatypes is using the type() function. However, it’s quite rigid as it doesn’t account for subtype polymorphism. That’s where isinstance() comes in, offering a more flexible approach that can check for class inheritance, which is particularly useful when working with custom classes or when type hierarchy matters.\n\nFor complex data pipelines, we often build custom validation functions that encapsulate the logic for our specific data structures. These functions might combine type checks with additional logic to ensure the data conforms in type and value, format, or structure — like checking a string to be a valid date.\n\nWhen we move beyond Python’s native capabilities, we find robust libraries tailored for data validation like Pandas, Pydantic, and Voluptuous. These tools come with their own mechanisms for ensuring datatype integrity. For example, Pandas ensures columns of a DataFrame retain their datatype, while Pydantic validates data against a predefined schema with support for complex types and custom validation logic.\n\nIn our data pipelines, we often validate data as it’s ingested from various sources — be it a CSV file where we need to ensure numeric columns aren’t inadvertently read as strings or an API call where we verify the data structure before processing.\n\nImplementing custom validation functions in Python allows us to check and ensure data types align with our expectations throughout our data pipelines. These functions are critical when dealing with data ingestion, transformation, and loading (ETL) processes where the integrity of data is paramount.\n\nExample of how to write custom validation functions:\n\nThe first step is defining what constitutes valid data for your application. For instance, if you’re expecting a dictionary with specific key-value pairs where the values need to be of certain types, your validation logic should reflect this.\n\n\n\nStep 2: Create the Validation Function\n\nNext, you’ll want to encapsulate this logic in a function. This function takes the data as input and checks it against the expected format and types.\n\nStep 3: Use the Function in Your Data Pipeline\n\nWith your validation function in place, you can call it whenever you process a new record.\n\nTo make this function reusable, you might parameterize it further, such as passing the required_fields as an argument or designing it to work with various data structures.\n\nBy incorporating these custom validation functions into your data pipelines, you establish a strong defensive programming practice that can significantly reduce the risk of type-related errors in your data processing applications.\n\n\n\nElevate your data engineering skills and learn how to implement custom validation functions to new heights with DE Academy’s comprehensive Python courses.\n\nPandas is a cornerstone in the data engineer’s toolkit, primarily for data manipulation and analysis. It includes features for data validation, especially useful when working with tabular data in DataFrames.\n\nFor example, you can define a schema for a DataFrame to ensure that each column contains data of the expected type using the dtypes attribute. Here’s a brief snippet demonstrating this:\n\nPydantic is a type validation and settings management library that uses Python type annotations. It excels in creating data models with fields corresponding to your expected data types, automatically validating incoming data.\n\nVoluptuous, another Python data validation library, allows for the composition of validation schemas that are simple yet expressive. It is especially useful for validating JSON-like data, configuration settings, or form data in web applications.\n\nA basic example of using Voluptuous is as follows:\n\nEach of these libraries offers a unique set of features that can simplify the process of data validation. Whether you need to enforce data types, ensure the presence of certain keys or fields, or check for more complex conditions, these tools can greatly reduce the effort required and help you maintain the integrity of your data pipelines.\n\nTesting and debugging are integral to ensuring your data validation logic is foolproof. A robust suite of tests can catch errors before they infiltrate your pipelines, while systematic debugging can resolve unexpected behavior swiftly.\n\nUtilize pytest, a powerful testing framework, to create tests for your validation functions. Begin by crafting simple test cases that confirm expected behavior for correct data types and then move on to tests that feed incorrect types to ensure they’re rejected as expected.\n\nHere’s an example of a basic test using pytest for a hypothetical validation function:\n\nWhen it comes to debugging, especially in complex data pipelines, logging is your first line of defense. Implement detailed logging within your validation functions to capture the state of your data and any errors. Tools like Python’s built-in logging module can be configured to provide varying levels of detail depending on the environment (development vs. production).\n\nWhen you encounter a type-related issue, isolate the problem by:\n• Applying Python’s debugger (pdb) to step through code execution and inspect variables at different stages.\n• Printing or logging type information at various points in the data pipeline to trace where a type mismatch occurs.\n\nRemember to test not only the ‘happy path’ but also edge cases and failure modes. Consider type edge cases — such as empty strings or lists, which are technically the correct type but may not be valid in context.\n\nThe field of data engineering is ever-evolving, and staying ahead requires continuous learning and adaptation. Whether you’re just starting or looking to deepen your expertise, DE Academy offers a wealth of coaching, courses, and community support to help you.\n\n\n\nStart for free ans explore DE Academy’s offerings and take the next step in your data engineering career."
    },
    {
        "link": "https://quanthub.com/validating-data-with-python-ensuring-integrity-in-your-data-pipeline",
        "document": "Data validation is a pivotal step in building data pipelines to ensure that the data being ingested, processed, and outputted maintains its quality, accuracy, and consistency. Using Python for data validation when building data pipelines is a wise choice due to its rich library ecosystem and flexibility. With tools ranging from built-in functions to specialized libraries like Pandas, Python makes it easy to enforce data quality at every step of your data pipeline, ensuring the reliability and accuracy of your analyses and applications.\n\nDefine Data Validation Rules\n\n Determine the criteria your data must meet before it’s processed. This is the foundation of your validation checks.\n• Documentation and Comments: A well-documented set of validation rules using inline comments ensures clarity for the team.\n• Data Contracts: Libraries like ‘Pydanti’ or ‘Marshmallow’ can be used to create data models with built-in validation.\n\nPerform Data Type Validations\n\n Ensure each data element conforms to its expected data type.\n• ‘ Using the attribute to verify the datatype of columns in a dataframe. method to cast or convert the data type of columns or the entire DataFrame/Series to a specified data type. Using the method to infer better data types for object columns.\n\nCheck for Missing Values\n\n Identify and handle null or missing values.\n• ‘Pandas’: The or functions can identify missing values in dataframes.\n• Handling: Depending on context, handle by imputation, removal, or setting default values.\n\nValidate Value Ranges\n\n Ensure data elements like numbers are within acceptable ranges.\n• ‘Pandas’ Filtering: Using querying methods to filter out values outside valid ranges.\n• Regular Expressions: The module in Python offers robust pattern matching using ‘regex,’ ideal for string validations.\n• Pandas: The function can help identify duplicate values in dataframes.\n\nPerform Cross-field and Custom Validations\n\n Enforce more complex validation rules that span multiple fields or don’t fit standard patterns.\n• Custom Functions: Write tailored Python functions that process rows or columns to enforce complex business rules.\n• Pandas’ Function: A versatile function that can be used to apply custom validation functions across rows or columns in a dataframe.\n\nData Serialization and Storage\n\n After validation, it’s essential to store or serialize the data for future use.\n• Pandas Serialization: method to serialize the DataFrame into a file format for efficient storage and later retrieval.\n\nValidate early and provide detailed error messages\n\n Example: Instead of waiting for the data to pass through multiple transformations, check its validity as soon as you receive it. This can save computation time and resources.\n\nImplement checksums or hash checks for validating the integrity of data files.\n\n Example: Verifying that a downloaded file hasn’t been tampered with using its MD5 checksum.\n\nUse libraries like ‘erberus’ for schema-based validations.\n\n Example: Ensuring that a dictionary has specific keys and value types.\n\nAlways account for edge cases in validations.\n\n Example: If you’re validating date strings, consider leap years or varying month lengths.\n\nMistake: Relying solely on descriptive statistics for validation.\n\n Countermeasure: While measures like mean, median, and mode provide insights, they may not capture anomalies or outliers. Use visual tools like histograms and boxplots, and apply tests for normality, to get a more comprehensive view of your data.\n\nMistake: Ignoring domain-specific constraints.\n\n Countermeasure: Data validation isn’t just about types and missing values. For example, a negative value might be invalid for a field that denotes “number of products sold.” Always apply domain knowledge to set specific validation rules.\n\nMistake: Overlooking temporal inconsistencies.\n\n Countermeasure: Check for date-related issues. This includes checking for future dates where they shouldn’t exist or ensuring sequences of dates are consistent.\n\nMistake: Relying on manual validation processes.\n\n Countermeasure: Automate data validation as much as possible. While manual checks might be necessary occasionally, they aren’t scalable or reliable for large datasets. Use scripts, libraries, and tools to automate these tasks.\n\nMistake: Not considering validation performance.\n\n Countermeasure: Validation can become a bottleneck, especially for large datasets. Optimize validation steps, employ parallel processing where possible, and be wary of computationally expensive operations."
    }
]