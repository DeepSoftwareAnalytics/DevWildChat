[
    {
        "link": "https://docs.gitlab.com/ci/jobs",
        "document": "CI/CD jobs are the fundamental elements of a GitLab CI/CD pipeline. Jobs are configured in the file with a list of commands to execute to accomplish tasks like building, testing, or deploying code.\n• Execute on a runner, for example in a Docker container.\n• Have a job log with the full execution log for the job.\n\nJobs are defined with YAML keywords that define all aspects of the job’s execution, including keywords that:\n• Control how and when jobs run.\n• Group jobs together in collections called stages. Stages run in sequence, while all jobs in a stage can run in parallel.\n• Save files as artifacts which can be used by other jobs.\n\nTo add a job to a pipeline, add it into your file. The job must:\n• Be defined at the top-level of the YAML configuration.\n• Have either a section defining commands to run, or a section to trigger a downstream pipeline to run.\n\nYou can’t use these keywords as job names:\n\nAdditionally, these names are valid when quoted, but are not recommended as they can make pipeline configuration unclear:\n\nJob names must be 255 characters or fewer.\n\nUse unique names for your jobs. If multiple jobs have the same name in a file, only one is added to the pipeline, and it’s difficult to predict which one is chosen. If the same job name is used in one or more included files, parameters are merged.\n\nTo temporarily disable a job without deleting it from the configuration file, add a period ( ) to the start of the job name. Hidden jobs do not need to contain the or keywords, but must contain valid YAML configuration.\n\nHidden jobs are not processed by GitLab CI/CD, but they can be used as templates for reusable configuration with:\n\nYou can use the keyword to set default job keywords and values, which are used by default by all jobs in a pipeline.\n\nWhen the pipeline runs, the job uses the default keywords:\n\nControl the inheritance of default keywords and variables\n\nYou can control the inheritance of:\n\nIn this example:\n• :\n• inherits: the default and the variable.\n• does not inherit: the default and the variable.\n• :\n• does not inherit: the and variables.\n• :\n• inherits: the default and , and the variable.\n\nWhen you access a pipeline, you can see the related jobs for that pipeline.\n\nThe order of jobs in a pipeline depends on the type of pipeline graph.\n• None For full pipeline graphs, jobs are sorted by name.\n• None For pipeline mini graphs, jobs are sorted by status, and then by name. The job status order is:\n\nSelecting an individual job shows you its job log, and allows you to:\n• Retry the job, if it failed.\n• Run the job again, if it passed.\n\nFiltering jobs by name is an experiment. For more information about the development of this feature, see issue 387547.\n\nTo view the full list of jobs that ran in a project:\n• On the left sidebar, select Search or go to and find your project.\n\nYou can filter the list by job status and job name.\n\nIf you have many similar jobs, your pipeline graph becomes long and hard to read.\n\nYou can automatically group similar jobs together. If the job names are formatted in a certain way, they are collapsed into a single group in regular pipeline graphs (not the mini graphs).\n\nYou can recognize when a pipeline has grouped jobs if you see a number next to a job name instead of the retry or cancel buttons. The number indicates the amount of grouped jobs. Hovering over them shows you if all jobs have passed or any has failed. Select to expand them.\n\nTo create a group of jobs, in the file, separate each job name with a number and one of the following:\n\nYou can use these symbols interchangeably.\n\nIn the example below, these three jobs are in a group named :\n\nThe pipeline graph displays a group named with three jobs.\n\nThe jobs are ordered by comparing the numbers from left to right. You usually want the first number to be the index and the second number to be the total.\n\nThis regular expression evaluates the job names: . One or more , , , or sequences are removed from the end of job names only. Matching substrings found at the beginning or in the middle of job names are not removed.\n\nWhen a pipeline fails or is allowed to fail, there are several places where you can find the reason:\n• In the pipeline graph, in the pipeline details view.\n• In the pipeline widgets, in the merge requests and commit pages.\n• In the job views, in the global and detailed views of a job.\n\nIn each place, if you hover over the failed job you can see the reason it failed.\n\nYou can also see the reason it failed on the Job detail page.\n\nYou can use GitLab Duo Root Cause Analysis in GitLab Duo Chat to troubleshoot failed CI/CD jobs.\n\nDeployment jobs are CI/CD jobs that use environments. A deployment job is any job that uses the keyword and the environment . Deployment jobs do not need to be in the stage. The following job is an example of a deployment job. is the default behavior and is defined here for clarity, but you can omit it:\n\nThe behavior of deployment jobs can be controlled with deployment safety settings like preventing outdated deployment jobs and ensuring only one deployment job runs at a time."
    },
    {
        "link": "https://docs.gitlab.com/ci/yaml",
        "document": "This document lists the configuration options for the GitLab file. This file is where you define the CI/CD jobs that make up your pipeline.\n• If you are already familiar with basic CI/CD concepts, try creating your own file by following a tutorial that demonstrates a simple or complex pipeline.\n• For a collection of examples, see GitLab CI/CD examples.\n• To view a large file used in an enterprise, see the file for .\n\nWhen you are editing your file, you can validate it with the CI Lint tool.\n\nIf you are editing content on this page, follow the instructions for documenting keywords.\n• None The names and order of the pipeline stages.\n• None Override a set of commands that are executed after job. Allow job to fail. A failed job does not cause the pipeline to fail. List of files and directories to attach to a job on success. Override a set of commands that are executed before job. List of files that should be cached between subsequent runs. Use configuration from DAST profiles on a job level. Restrict which artifacts are passed to a specific job by providing a list of jobs to fetch artifacts from. Name of an environment to which the job deploys. Configuration entries that this job inherits from. Authenticate with third party services using identity federation. Defines if a job can be canceled when made redundant by a newer run. Upload the result of a job to use with GitLab Pages. How many instances of a job should be run in parallel. When and how many times a job can be auto-retried in case of a failure. List of conditions to evaluate and determine selected attributes of a job, and whether or not it’s created. Shell script that is executed by a runner. Run configuration that is executed by a runner. The CI/CD secrets the job needs. List of tags that are used to select a runner. Define a custom job-level timeout that takes precedence over the project-wide setting.\n• None Define default CI/CD variables for all jobs in the pipeline.\n\nSome keywords are not defined in a job. These keywords control pipeline behavior or import additional pipeline configuration.\n\nYou can set global defaults for some keywords. Each default keyword is copied to every job that doesn’t already have it defined. If the job already has a keyword defined, that default is not used.\n\nSupported values: These keywords can have custom defaults:\n• , though due to issue 213634 this keyword has no effect.\n\nIn this example:\n• and are the default keywords for all jobs in the pipeline.\n• The job does not have or defined, so it uses the defaults of and .\n• The job does not have defined, but it does have explicitly defined. It uses the default , but ignores the default and uses the defined in the job.\n• Control inheritance of default keywords in jobs with .\n• Global defaults are not passed to downstream pipelines, which run independently of the upstream pipeline that triggered the downstream pipeline.\n\nUse to include external YAML files in your CI/CD configuration. You can split one long file into multiple files to increase readability, or reduce duplication of the same configuration in multiple places.\n\nYou can also store template files in a central repository and include them in projects.\n• Merged with those in the file.\n• Always evaluated first and then merged with the content of the file, regardless of the position of the keyword.\n\nThe time limit to resolve all files is 30 seconds.\n• Only certain CI/CD variables can be used with keywords.\n• Use merging to customize and override included CI/CD configurations with local\n• You can override included configuration by having the same job name or global keyword in the file. The two configurations are merged together, and the configuration in the file takes precedence over the included configuration.\n• If you rerun a:\n• Job, the files are not fetched again. All jobs in a pipeline use the configuration fetched when the pipeline was created. Any changes to the source files do not affect job reruns.\n• Pipeline, the files are fetched again. If they changed after the last pipeline run, the new pipeline uses the changed configuration.\n• You can have up to 150 includes per pipeline by default, including nested. Additionally:\n• In GitLab 16.0 and later users on GitLab Self-Managed can change the maximum includes value.\n• In GitLab 15.10 and later you can have up to 150 includes. In nested includes, the same file can be included multiple times, but duplicated includes count towards the limit.\n• From GitLab 14.9 to GitLab 15.9, you can have up to 100 includes. The same file can be included multiple times in nested includes, but duplicates are ignored.\n\nUse to add a CI/CD component to the pipeline configuration.\n\nSupported values: The full address of the CI/CD component, formatted as .\n\nUse to include a file that is in the same repository and branch as the configuration file containing the keyword. Use instead of symbolic links.\n• The YAML file must have the extension or .\n• You can use and wildcards in the file path.\n• You can use certain CI/CD variables.\n\nYou can also use shorter syntax to define the path:\n• The file and the local file must be on the same branch.\n• configuration is always evaluated based on the location of the file containing the keyword, not the project running the pipeline. If a nested is in a configuration file in a different project, checks that other project for the file.\n\nTo include files from another private project on the same GitLab instance, use and .\n• A full file path, or array of file paths, relative to the root directory ( ). The YAML files must have the or extension.\n• : Optional. The ref to retrieve the file from. Defaults to the of the project when not specified.\n• You can use certain CI/CD variables.\n\nYou can also specify a :\n• configuration is always evaluated based on the location of the file containing the keyword, not the project running the pipeline. If a nested is in a configuration file in a different project, checks that other project for the file.\n• When the pipeline starts, the file configuration included by all methods is evaluated. The configuration is a snapshot in time and persists in the database. GitLab does not reflect any changes to the referenced file configuration until the next pipeline starts.\n• When you include a YAML file from another private project, the user running the pipeline must be a member of both projects and have the appropriate permissions to run pipelines. A error may be displayed if the user does not have access to any of the included files.\n• Be careful when including another project’s CI/CD configuration file. No pipelines or notifications trigger when CI/CD configuration files change. From a security perspective, this is similar to pulling a third-party dependency. For the , consider:\n• Using a specific SHA hash, which should be the most stable option. Use the full 40-character SHA hash to ensure the desired commit is referenced, because using a short SHA hash for the might be ambiguous.\n• Applying both protected branch and protected tag rules to the in the other project. Protected tags and branches are more likely to pass through change management before changing.\n\nUse with a full URL to include a file from a different location.\n• Authentication with the remote URL is not supported.\n• The YAML file must have the extension or .\n• You can use certain CI/CD variables.\n• All nested includes are executed without context as a public user, so you can only include public projects or templates. No variables are available in the section of nested includes.\n• Be careful when including another project’s CI/CD configuration file. No pipelines or notifications trigger when the other project’s files change. From a security perspective, this is similar to pulling a third-party dependency. To verify the integrity of the included file, consider using the keyword. If you link to another GitLab project you own, consider the use of both protected branches and protected tags to enforce change management rules.\n• All templates can be viewed in . Not all templates are designed to be used with , so check template comments before using one.\n• You can use certain CI/CD variables.\n• All nested includes are executed without context as a public user, so you can only include public projects or templates. No variables are available in the section of nested includes.\n\nUse to set the values for input parameters when the included configuration uses and is added to the pipeline.\n\nIn this example:\n• The configuration contained in is added to the pipeline, with a input set to a value of for the included configuration.\n• If the included configuration file uses , the input value must match the defined type.\n• If the included configuration file uses , the input value must match one of the listed options.\n\nYou can use with to conditionally include other configuration files.\n\nIn this example, if the variable is:\n• , the configuration is included in the pipeline.\n• Not or does not exist, the configuration is not included in the pipeline.\n• Examples of using with:\n\nUse with to specifiy a SHA256 hash of the included remote file. If does not match the actual content, the remote file is not processed and the pipeline fails.\n\nUse to define stages that contain groups of jobs. Use in a job to configure the job to run in a specific stage.\n\nIf is not defined in the file, the default pipeline stages are:\n\nThe order of the items in defines the execution order for jobs:\n• Jobs in the same stage run in parallel.\n• Jobs in the next stage run after the jobs from the previous stage complete successfully.\n\nIf a pipeline contains only jobs in the or stages, it does not run. There must be at least one other job in a different stage.\n\nIn this example:\n• All jobs in execute in parallel.\n• If all jobs in succeed, the jobs execute in parallel.\n• If all jobs in succeed, the jobs execute in parallel.\n• If all jobs in succeed, the pipeline is marked as .\n\nIf any job fails, the pipeline is marked as and jobs in later stages do not start. Jobs in the current stage are not stopped and continue to run.\n• If a job does not specify a , the job is assigned the stage.\n• If a stage is defined but no jobs use it, the stage is not visible in the pipeline, which can help compliance pipeline configurations:\n• Stages can be defined in the compliance configuration but remain hidden if not used.\n• The defined stages become visible when developers use them in job definitions.\n• To make a job start earlier and ignore the stage order, use the keyword.\n\nYou can use some predefined CI/CD variables in configuration, but not variables that are only defined when jobs start.\n\nUse to configure the behavior of the auto-cancel redundant pipelines feature.\n• : Cancel the pipeline, but only if no jobs with have started yet. Default when not defined.\n• : Do not auto-cancel any jobs.\n\nIn this example:\n• When a new commit is pushed to a branch, GitLab creates a new pipeline and and start.\n• If a new commit is pushed to the branch before the jobs complete, only is canceled.\n\nUse to configure which jobs should be canceled as soon as one job fails.\n• : Cancel the pipeline and all running jobs as soon as one job fails.\n• : Do not auto-cancel any jobs.\n\nIn this example, if fails, is canceled if it is still running and does not start.\n\nYou can use in to define a name for pipelines.\n\nAll pipelines are assigned the defined name. Any leading or trailing spaces in the name are removed.\n\nA configuration with different pipeline names depending on the pipeline conditions:\n• If the name is an empty string, the pipeline is not assigned a name. A name consisting of only CI/CD variables could evaluate to an empty string if all the variables are also empty.\n• become default variables available in all jobs, including jobs which forward variables to downstream pipelines by default. If the downstream pipeline uses the same variable, the variable is overwritten by the upstream variable value. Be sure to either:\n• Use a unique variable name in every project’s pipeline configuration, like .\n• Use in the trigger job and list the exact variables you want to forward to the downstream pipeline.\n\nThe keyword in is similar to defined in jobs, but controls whether or not a whole pipeline is created.\n\nWhen no rules evaluate to true, the pipeline does not run.\n\nSupported values: You can use some of the same keywords as job-level :\n• , can only be or when used with .\n\nIn this example, pipelines run if the commit title (first line of the commit message) does not end with and the pipeline is for either:\n• If your rules match both branch pipelines (other than the default branch) and merge request pipelines, duplicate pipelines can occur.\n• , , and are not supported in , but do not cause a syntax violation. Though they have no effect, do not use them in as it could cause syntax failures in the future. See issue 436473 for more details.\n\nYou can use in to define variables for specific pipeline conditions.\n\nWhen the condition matches, the variable is created and can be used by all jobs in the pipeline. If the variable is already defined at the top level as a default variable, the variable takes precedence and overrides the default variable.\n\nSupported values: Variable name and value pairs:\n• The name can use only numbers, letters, and underscores ( ).\n• The value must be a string.\n\nWhen the branch is the default branch:\n\nWhen the branch is :\n• job1’s is , and is .\n• job2’s is , and is .\n\nWhen the branch is something else:\n• become default variables available in all jobs, including jobs which forward variables to downstream pipelines by default. If the downstream pipeline uses the same variable, the variable is overwritten by the upstream variable value. Be sure to either:\n• Use unique variable names in every project’s pipeline configuration, like .\n• Use in the trigger job and list the exact variables you want to forward to the downstream pipeline.\n\nUse to configure the behavior of the or the features.\n\nIn this example, is set to and is set to for all jobs by default. But if a pipeline runs for a protected branch, the rule overrides the default with and . For example, if a pipeline is running for:\n• A non-protected branch and a new commit is pushed, continues to run and is canceled.\n• A protected branch and a new commit is pushed, both and continue to run.\n\nSome keywords must be defined in a header section of a YAML configuration file. The header must be at the top of the file, separated from the rest of the configuration with .\n\nAdd a section to the header of a YAML file to configure the behavior of a pipeline when a configuration is added to the pipeline with the keyword.\n\nYou can use to define input parameters for the CI/CD configuration you intend to add to a pipeline with . Use to define the values to use when the pipeline runs.\n\nUse the inputs to customize the behavior of the configuration when included in CI/CD configuration.\n\nUse the interpolation format to reference the values outside of the header section. Inputs are evaluated and interpolated when the configuration is fetched during pipeline creation, but before the configuration is merged with the contents of the file.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n• Inputs are mandatory unless you use to set a default value.\n• Inputs expect strings unless you use to set a different input type.\n• A string containing an interpolation block must not exceed 1 MB.\n• The string inside an interpolation block must not exceed 1 KB.\n\nInputs are mandatory when included, unless you set a default value with .\n\nUse to have no default value.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n\nSupported values: A string representing the default value, or .\n\nIn this example:\n• is mandatory and must be defined.\n• is optional. If not defined, the value is .\n• is optional. If not defined, it has no value.\n• The pipeline fails with a validation error when the input:\n• Uses both and , but the default value is not one of the listed options.\n• Uses both and , but the default value does not match the regular expression.\n• Value does not match the .\n\nUse to give a description to a specific input. The description does not affect the behavior of the input and is only used to help users of the file understand the input.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n\nInputs can use to specify a list of allowed values for an input. The limit is 50 options per input.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n\nIn this example:\n• is mandatory and must be defined with one of the values in the list.\n• The pipeline fails with a validation error when:\n• The input uses both and , but the default value is not one of the listed options.\n• Any of the input options do not match the , which can be either or , but not when using .\n\nUse to specify a regular expression that the input must match.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n\nIn this example, inputs of or match the regular expression and pass validation. An input of does not match the regular expression and fails validation.\n• can only be used with a of , not or .\n• Do not enclose the regular expression with the character. For example, use , not .\n\nBy default, inputs expect strings. Use to set a different required type for inputs.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n\nSupported values: Can be one of:\n• , to accept an array of inputs.\n• , to accept string inputs (default when not defined).\n• , to only accept or inputs.\n\nThe following topics explain how to use keywords to configure CI/CD pipelines.\n\nUse to define an array of commands to run last, after a job’s and sections complete. commands also run when:\n• The job is canceled while the or sections are still running.\n• The job fails with failure type of , but not other failure types.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nScripts you specify in execute in a new shell, separate from any or commands. As a result, they:\n• Have the current working directory set back to the default (according to the variables which define how the runner processes Git requests).\n• Don’t have access to changes done by commands defined in the or , including:\n• Changes outside of the working tree (depending on the runner executor), like software installed by a or script.\n• Have a separate timeout. For GitLab Runner 16.4 and later, this defaults to 5 minutes, and can be configured with the variable. In GitLab 16.3 and earlier, the timeout is hard-coded to 5 minutes.\n• Don’t affect the job’s exit code. If the section succeeds and the times out or fails, the job exits with code ( ).\n• There is a known issue with using CI/CD job tokens with . You can use a job token for authentication in commands, but the token immediately becomes invalid if the job is canceled. See issue for more details.\n\nFor jobs that time out:\n• commands do not execute by default.\n• You can configure timeout values to ensure runs by setting appropriate and values that don’t exceed the job’s timeout.\n• Use with to define a default array of commands that should run after all jobs.\n• You can configure a job to skip commands if the job is canceled.\n• Use color codes with to make job logs easier to review.\n• You can ignore errors in .\n\nUse to determine whether a pipeline should continue running when a job fails.\n• To let the pipeline continue running subsequent jobs, use .\n• To stop the pipeline from running subsequent jobs, use .\n\nWhen jobs are allowed to fail ( ) an orange warning ( ) indicates that a job failed. However, the pipeline is successful and the associated commit is marked as passed with no warnings.\n\nThis same warning is displayed when:\n• All other jobs in the stage are successful.\n• All other jobs in the pipeline are successful.\n\nThe default value for is:\n• for jobs that use inside .\n• in all other cases.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nIn this example, and run in parallel:\n• If fails, jobs in the stage do not start.\n• If fails, jobs in the stage can still start.\n• You can use as a subkey of .\n• If is set, the job is always considered successful, and later jobs with don’t start if this job fails.\n• You can use with a manual job to create a blocking manual job. A blocked pipeline does not run any jobs in later stages until the manual job is started and completes successfully.\n\nUse to control when a job should be allowed to fail. The job is for any of the listed exit codes, and false for any other exit code.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nUse to specify which files to save as job artifacts. Job artifacts are a list of files and directories that are attached to the job when it succeeds, fails, or always.\n\nThe artifacts are sent to GitLab after the job finishes. They are available for download in the GitLab UI if the size is smaller than the maximum artifact size.\n\nBy default, jobs in later stages automatically download all the artifacts created by jobs in earlier stages. You can control artifact download behavior in jobs with .\n\nWhen using the keyword, jobs can only download artifacts from the jobs defined in the configuration.\n\nJob artifacts are only collected for successful jobs by default, and artifacts are restored after caches.\n\nPaths are relative to the project directory ( ) and can’t directly link outside it.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• An array of file paths, relative to the project directory.\n• You can use Wildcards that use glob patterns and:\n• In GitLab Runner 13.0 and later, .\n• For GitLab Pages job:\n• In GitLab 17.10 and later, the path is automatically appended to , so you don’t need to specify it again.\n• In GitLab 17.10 and later, when the path is not specified, the directory is automatically appended to .\n\nThis example creates an artifact with and all the files in the directory.\n• If not used with , the artifacts file is named , which becomes when downloaded.\n• To restrict which jobs a specific job fetches artifacts from, see .\n\nUse to prevent files from being added to an artifacts archive.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• An array of file paths, relative to the project directory.\n• You can use Wildcards that use glob or patterns.\n\nThis example stores all files in , but not files located in subdirectories of .\n• Files matched by can be excluded using too.\n\nUse to specify how long job artifacts are stored before they expire and are deleted. The setting does not affect:\n• Artifacts from the latest job, unless keeping the latest job artifacts is disabled at the project level or instance-wide.\n\nAfter their expiry, artifacts are deleted hourly by default (using a cron job), and are not accessible anymore.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nSupported values: The expiry time. If no unit is provided, the time is in seconds. Valid values include:\n• The expiration time period begins when the artifact is uploaded and stored on GitLab. If the expiry time is not defined, it defaults to the instance wide setting.\n• To override the expiration date and protect artifacts from being automatically deleted:\n• Select Keep on the job page.\n• Set the value of to .\n• If the expiry time is too short, jobs in later stages of a long pipeline might try to fetch expired artifacts from earlier jobs. If the artifacts are expired, jobs that try to fetch them fail with a could not retrieve the needed artifacts error. Set the expiry time to be longer, or use in later jobs to ensure they don’t try to fetch expired artifacts.\n\nUse the keyword to expose job artifacts in the merge request UI.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• The name to display in the merge request UI for the artifacts download link. Must be combined with .\n• Artifacts are saved, but do not display in the UI if the values:\n• Define a directory, but do not end with . For example, works with , but does not.\n• Start with . For example, works with , but does not.\n• A maximum of 10 job artifacts per merge request can be exposed.\n• If a directory is specified and there is more than one file in the directory, the link is to the job artifacts browser.\n• If GitLab Pages is enabled, GitLab automatically renders the artifacts when the artifacts is a single file with one of these extensions:\n\nUse the keyword to define the name of the created artifacts archive. You can specify a unique name for every archive.\n\nIf not defined, the default name is , which becomes when downloaded.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• The name of the artifacts archive. CI/CD variables are supported. Must be combined with .\n\nTo create an archive with a name of the current job:\n• Use CI/CD variables to define the artifacts configuration\n\nUse to determine whether the job artifacts should be publicly available.\n\nWhen is (default), the artifacts in public pipelines are available for download by anonymous, guest, and reporter users.\n\nTo deny read access to artifacts in public pipelines for anonymous, guest, and reporter users, set to :\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• (default if not defined) or .\n\nUse to determine who can access the job artifacts from the GitLab UI or API. This option does not prevent you from forwarding artifacts to downstream pipelines.\n\nYou cannot use and in the same job.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• (default): Artifacts in a job in public pipelines are available for download by anyone, including anonymous, guest, and reporter users.\n• : Artifacts in the job are only available for download by users with the Developer role or higher.\n• : Artifacts in the job are not available for download by anyone.\n• affects all too, so you can also restrict access to artifacts for reports.\n\nUse to collect artifacts generated by included templates in jobs.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• See list of available artifacts reports types.\n• Combining reports in parent pipelines using artifacts from child pipelines is not supported. Track progress on adding support in this issue.\n• To be able to browse and download the report output files, include the keyword. This uploads and stores the artifact twice.\n• Artifacts created for are always uploaded, regardless of the job results (success or failure). You can use to set an expiration date for the artifacts.\n\nUse to add all Git untracked files as artifacts (along with the paths defined in ). ignores configuration in the repository’s , so matching artifacts in are included.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• or (default if not defined).\n\nUse to upload artifacts on job failure or despite the failure.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• (default): Upload artifacts only when the job succeeds.\n• : Upload artifacts only when the job fails.\n• : Always upload artifacts (except when jobs time out). For example, when uploading artifacts required to troubleshoot failing tests.\n• The artifacts created for are always uploaded, regardless of the job results (success or failure). does not change this behavior.\n\nUse to define an array of commands that should run before each job’s commands, but after artifacts are restored.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• Scripts you specify in are concatenated with any scripts you specify in the main . The combined scripts execute together in a single shell.\n• Using at the top level, but not in the section, is deprecated.\n• Use with to define a default array of commands that should run before the commands in all jobs.\n• Use color codes with to make job logs easier to review.\n\nUse to specify a list of files and directories to cache between jobs. You can only use paths that are in the local working copy.\n• By default, not shared between protected and unprotected branches.\n• Limited to a maximum of four different caches.\n\nYou can disable caching for specific jobs, for example to override:\n• The configuration for a job added with .\n\nFor more information about caches, see Caching in GitLab CI/CD.\n\nUse the keyword to choose which files or directories to cache.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• An array of paths relative to the project directory ( ). You can use wildcards that use glob patterns:\n• In GitLab Runner 13.0 and later, .\n\nCache all files in that end in and the file:\n• The keyword includes files even if they are untracked or in your file.\n• See the common use cases for more examples.\n\nUse the keyword to give each cache a unique identifying key. All jobs that use the same cache key use the same cache, including in different pipelines.\n\nIf not set, the default key is . All jobs with the keyword but no share the cache.\n\nMust be used with , or nothing is cached.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• None If you use Windows Batch to run your shell scripts you must replace with . For example:\n• None The value can’t contain:\n• The character, or the equivalent URI-encoded .\n• Only the character (any number), or the equivalent URI-encoded .\n• None The cache is shared between jobs, so if you’re using different paths for different jobs, you should also set a different . Otherwise cache content can be overwritten.\n• You can specify a fallback cache key to use if the specified is not found.\n• You can use multiple cache keys in a single job.\n• See the common use cases for more examples.\n\nUse the keyword to generate a new key when one or two specific files change. lets you reuse some caches, and rebuild them less often, which speeds up subsequent pipeline runs.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• An array of one or two file paths.\n\nThis example creates a cache for Ruby and Node.js dependencies. The cache is tied to the current versions of the and files. When one of these files changes, a new cache key is computed and a new cache is created. Any future job runs that use the same and with use the new cache, instead of rebuilding the dependencies.\n• The cache is a SHA computed from the most recent commits that changed each listed file. If neither file is changed in any commits, the fallback key is .\n\nUse to combine a prefix with the SHA computed for .\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nFor example, adding a of causes the key to look like . If a branch changes , that branch has a new SHA checksum for . A new cache key is generated, and a new cache is created for that key. If is not found, the prefix is added to , so the key in the example would be .\n• If no file in is changed in any commits, the prefix is added to the key.\n\nUse to cache all files that are untracked in your Git repository. Untracked files include files that are:\n• Created, but not added to the checkout with .\n\nCaching untracked files can create unexpectedly large caches if the job downloads:\n• Dependencies, like gems or node modules, which are usually untracked.\n• Artifacts from a different job. Files extracted from the artifacts are untracked by default.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• None You can combine with to cache all untracked files, as well as files in the configured paths. Use to cache any specific files, including tracked files, or files that are outside of the working directory, and use to also cache all untracked files. For example: In this example, the job caches all untracked files in the repository, as well as all the files in . If there are untracked files in , they are covered by both keywords.\n\nUse to set a cache to be shared between protected and unprotected branches.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nUse to define when to save the cache, based on the status of the job.\n\nMust be used with , or nothing is cached.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• (default): Save the cache only when the job succeeds.\n• : Save the cache only when the job fails.\n\nThis example stores the cache whether or not the job fails or succeeds.\n\nTo change the upload and download behavior of a cache, use the keyword. By default, the job downloads the cache when the job starts, and uploads changes to the cache when the job ends. This caching style is the policy (default).\n\nTo set a job to only download the cache when the job starts, but never upload changes when the job finishes, use .\n\nTo set a job to only upload a cache when the job finishes, but never download the cache when the job starts, use .\n\nUse the policy when you have many jobs executing in parallel that use the same cache. This policy speeds up job execution and reduces load on the cache server. You can use a job with the policy to build the cache.\n\nMust be used with , or nothing is cached.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• You can use a variable to control a job’s cache policy.\n\nUse to specify a list of keys to try to restore cache from if there is no cache found for the . Caches are retrieved in the order specified in the section.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nUse with a custom regular expression to configure how code coverage is extracted from the job output. The coverage is shown in the UI if at least one line in the job output matches the regular expression.\n\nTo extract the code coverage value from the match, GitLab uses this smaller regular expression: .\n• An RE2 regular expression. Must start and end with . Must match the coverage number. May match surrounding text as well, so you don’t need to use a regular expression character group to capture the exact number. Because it uses RE2 syntax, all groups must be non-capturing.\n\nIn this example:\n• GitLab checks the job log for a match with the regular expression. A line like would match.\n• GitLab then checks the matched fragment to find a match to . The sample matching line above gives a code coverage of .\n• You can find regex examples in Code Coverage.\n• If there is more than one matched line in the job output, the last line is used (the first result of reverse search).\n• If there are multiple matches in a single line, the last match is searched for the coverage number.\n• If there are multiple coverage numbers found in the matched fragment, the first number is used.\n• Coverage output from child pipelines is not recorded or displayed. Check the related issue for more details.\n\nUse the keyword to specify a site profile and scanner profile to be used in a CI/CD configuration. Both profiles must first have been created in the project. The job’s stage must be .\n\nKeyword type: Job keyword. You can use only as part of a job.\n\nSupported values: One each of and .\n• Use to specify the site profile to be used in the job.\n• Use to specify the scanner profile to be used in the job.\n\nIn this example, the job extends the configuration added with the keyword to select a specific site profile and scanner profile.\n• Settings contained in either a site profile or scanner profile take precedence over those contained in the DAST template.\n\nUse the keyword to define a list of specific jobs to fetch artifacts from. The specified jobs must all be in earlier stages. You can also set a job to download no artifacts at all.\n\nWhen is not defined in a job, all jobs in earlier stages are considered dependent and the job fetches all artifacts from those jobs.\n\nTo fetch artifacts from a job in the same stage, you must use . You should not combine with in the same job.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The names of jobs to fetch artifacts from.\n• An empty array ( ), to configure the job to not download any artifacts.\n\nIn this example, two jobs have artifacts: and . When is executed, the artifacts from are downloaded and extracted in the context of the build. The same thing happens for and artifacts from .\n\nThe job downloads artifacts from all previous jobs because of the stage precedence.\n• The job status does not matter. If a job fails or it’s a manual job that isn’t triggered, no error occurs.\n• If the artifacts of a dependent job are expired or deleted, then the job fails.\n\nUse to define the environment that a job deploys to.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: The name of the environment the job deploys to, in one of these formats:\n• CI/CD variables, including predefined, project, group, instance, or variables defined in the file. You can’t use variables defined in a section.\n• If you specify an and no environment with that name exists, an environment is created.\n\nSet a name for an environment.\n\nCommon environment names are , , and , but you can use any name.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: The name of the environment the job deploys to, in one of these formats:\n• CI/CD variables, including predefined, project, group, instance, or variables defined in the file. You can’t use variables defined in a section.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: A single URL, in one of these formats:\n• CI/CD variables, including predefined, project, group, instance, or variables defined in the file. You can’t use variables defined in a section.\n• After the job completes, you can access the URL by selecting a button in the merge request, environment, or deployment pages.\n\nClosing (stopping) environments can be achieved with the keyword defined under . It declares a different job that runs to close the environment.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• See for more details and an example.\n\nUse the keyword to specify how the job interacts with the environment.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: One of the following keywords:\n\nThe keyword specifies the lifetime of the environment. When an environment expires, GitLab automatically stops it.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: A period of time written in natural language. For example, these are all equivalent:\n\nWhen the environment for is created, the environment’s lifetime is set to . Every time the review app is deployed, that lifetime is also reset to .\n\nThe keyword can be used for all environment actions except . Some actions can be used to reset the scheduled stop time for the environment. For more information, see Access an environment for preparation or verification purposes.\n\nUse the keyword to configure the dashboard for Kubernetes for an environment.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : A string specifying the GitLab agent for Kubernetes. The format is .\n• : A string representing the Kubernetes namespace. It needs to be set together with the keyword.\n• : A string representing the path to the Flux resource. This must be the full resource path. It needs to be set together with the and keywords.\n\nThis configuration sets up the job to deploy to the environment, associates the agent named with the environment, and configures the dashboard for Kubernetes for an environment with the namespace and the set to .\n• To use the dashboard, you must install the GitLab agent for Kubernetes and configure for the environment’s project or its parent group.\n• The user running the job must be authorized to access the cluster agent. Otherwise, it will ignore , and attributes.\n\nUse the keyword to specify the tier of the deployment environment.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: One of the following:\n• Environments created from this job definition are assigned a tier based on this value.\n• Existing environments don’t have their tier updated if this value is added later. Existing environments must have their tier updated via the Environments API.\n\nUse CI/CD variables to dynamically name environments.\n\nThe job is marked as a deployment to dynamically create the environment. is a CI/CD variable set by the runner. The variable is based on the environment name, but suitable for inclusion in URLs. If the job runs in a branch named , this environment would be accessible with a URL like .\n\nThe common use case is to create dynamic environments for branches and use them as review apps. You can see an example that uses review apps at https://gitlab.com/gitlab-examples/review-apps-nginx/.\n\nUse to reuse configuration sections. It’s an alternative to YAML anchors and is a little more flexible and readable.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The name of another job in the pipeline.\n• A list (array) of names of other jobs in the pipeline.\n\nIn this example, the job uses the configuration from the template job. When creating the pipeline, GitLab:\n• Merges the content with the job.\n• Doesn’t merge the values of the keys.\n\nThe combined configuration is equivalent to these jobs:\n• You can use multiple parents for .\n• The keyword supports up to eleven levels of inheritance, but you should avoid using more than three levels.\n• In the example above, is a hidden job, but you can extend configuration from regular jobs as well.\n• Use to reuse configuration from included configuration files.\n\nUse to specify lists of commands to execute on the runner at certain stages of job execution, like before retrieving the Git repository.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• A hash of hooks and their commands. Available hooks: .\n\nUse to specify a list of commands to execute on the runner before cloning the Git repository and any submodules. You can use it for example to:\n\nThis feature is in beta.\n\nUse to authenticate with third party services using identity federation.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• : Google Cloud. Must be configured with the Google Cloud IAM integration.\n\nUse to create JSON web tokens (JWT) to authenticate with third party services. All JWTs created this way support OIDC authentication. The required sub-keyword is used to configure the claim for the JWT.\n\nUse to specify a Docker image that the job runs in.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nSupported values: The name of the image, including the registry path if needed, in one of these formats:\n\nIn this example, the image is the default for all jobs in the pipeline. The job does not use the default, because it overrides the default with a job-specific section.\n\nThe name of the Docker image that the job runs in. Similar to used by itself.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nSupported values: The name of the image, including the registry path if needed, in one of these formats:\n\nCommand or script to execute as the container’s entry point.\n\nWhen the Docker container is created, the is translated to the Docker option. The syntax is similar to the Dockerfile directive, where each shell token is a separate string in the array.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• Override the entrypoint of an image.\n\nUse to pass options to the Docker executor runner. This keyword does not work with other executor types.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nA hash of options for the Docker executor, which can include:\n• : Selects the architecture of the image to pull. When not specified, the default is the same platform as the host runner.\n• : Specify the username or UID to use when running the container.\n\nThe pull policy that the runner uses to fetch the Docker image.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• A single pull policy, or multiple pull policies in an array. Can be , , or .\n• If the runner does not support the defined pull policy, the job fails with an error similar to: ERROR: Job failed (system failure): the configured PullPolicies ([always]) are not allowed by AllowedPullPolicies ([never]) .\n\nUse to control inheritance of default keywords and variables.\n\nUse to control the inheritance of default keywords.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• (default) or to enable or disable the inheritance of all default keywords.\n• You can also list default keywords to inherit on one line:\n\nUse to control the inheritance of default variables keywords.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• (default) or to enable or disable the inheritance of all default variables.\n• You can also list default variables to inherit on one line:\n\nUse to configure the auto-cancel redundant pipelines feature to cancel a job before it completes if a new pipeline on the same ref starts for a newer commit. If the feature is disabled, the keyword has no effect. The new pipeline must be for a commit with new changes. For example, the Auto-cancel redundant pipelines feature has no effect if you select New pipeline in the UI to run a pipeline for the same commit.\n\nThe behavior of the Auto-cancel redundant pipelines feature can be controlled by the setting.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nExample of with the default behavior:\n\nIn this example, a new pipeline causes a running pipeline to be:\n• Canceled, if only is running or pending.\n\nExample of with the setting:\n\nIn this example, a new pipeline causes a running pipeline to cancel and if they are running or pending.\n• Only set if the job can be safely canceled after it has started, like a build job. Deployment jobs usually shouldn’t be canceled, to prevent partial deployments.\n• When using the default behavior or :\n• A job that has not started yet is always considered , regardless of the job’s configuration. The configuration is only considered after the job starts.\n• Running pipelines are only canceled if all running jobs are configured with or no jobs configured with have started at any time. After a job with starts, the entire pipeline is no longer considered interruptible.\n• If the pipeline triggered a downstream pipeline, but no job with in the downstream pipeline has started yet, the downstream pipeline is also canceled.\n• You can add an optional manual job with in the first stage of a pipeline to allow users to manually prevent a pipeline from being automatically canceled. After a user starts the job, the pipeline cannot be canceled by the Auto-cancel redundant pipelines feature.\n• When using with a trigger job:\n• The triggered downstream pipeline is never affected by the trigger job’s configuration.\n• If is set to , the trigger job’s configuration has no effect.\n• If is set to , a trigger job with can be automatically canceled.\n\nUse to execute jobs out-of-order. Relationships between jobs that use can be visualized as a directed acyclic graph.\n\nYou can ignore stage ordering and run some jobs without waiting for others to complete. Jobs in multiple stages can run concurrently.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• An array of jobs (maximum of 50 jobs).\n• An empty array ( ), to set the job to start as soon as the pipeline is created.\n\nThis example creates four paths of execution:\n• Linter: The job runs immediately without waiting for the stage to complete because it has no needs ( ).\n• Linux path: The job runs as soon as the job finishes, without waiting for to finish.\n• macOS path: The jobs runs as soon as the job finishes, without waiting for to finish.\n• The job runs as soon as all previous jobs finish: , , , , .\n• The maximum number of jobs that a single job can have in the array is limited:\n• For GitLab.com, the limit is 50. For more information, see issue 350398.\n• For GitLab Self-Managed, the default limit is 50. This limit can be changed.\n• If refers to a job that uses the keyword, it depends on all jobs created in parallel, not just one job. It also downloads artifacts from all the parallel jobs by default. If the artifacts have the same name, they overwrite each other and only the last one downloaded is saved.\n• To have refer to a subset of parallelized jobs (and not all of the parallelized jobs), use the keyword.\n• You can refer to jobs in the same stage as the job you are configuring.\n• If refers to a job that might not be added to a pipeline because of , , or , the pipeline might fail to create. Use the keyword to resolve a failed pipeline creation.\n• If a pipeline has jobs with and jobs in the stage, they will all start as soon as the pipeline is created. Jobs with start immediately, and jobs in the stage also start immediately.\n\nWhen a job uses , it no longer downloads all artifacts from previous stages by default, because jobs with can start before earlier stages complete. With you can only download artifacts from the jobs listed in the configuration.\n\nUse (default) or to control when artifacts are downloaded in jobs that use .\n\nKeyword type: Job keyword. You can use it only as part of a job. Must be used with .\n\nIn this example:\n• The job does not download the artifacts.\n• The job downloads the artifacts from all three , because is , or defaults to , for all three needed jobs.\n• You should not combine with in the same job.\n\nUse to download artifacts from up to five jobs in other pipelines. The artifacts are downloaded from the latest successful specified job for the specified ref. To specify multiple jobs, add each as separate array items under the keyword.\n\nIf there is a pipeline running for the ref, a job with does not wait for the pipeline to complete. Instead, the artifacts are downloaded from the latest successful run of the specified job.\n\nmust be used with , , and .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : The job to download artifacts from.\n• : The ref to download artifacts from.\n• : Must be to download artifacts.\n\nIn this example, downloads the artifacts from the latest successful and jobs on the branches in the and projects.\n\nYou can use CI/CD variables in , for example:\n• To download artifacts from a different pipeline in the current project, set to be the same as the current project, but use a different ref than the current pipeline. Concurrent pipelines running on the same ref could override the artifacts.\n• The user running the pipeline must have at least the Reporter role for the group or project, or the group/project must have public visibility.\n• You can’t use in the same job as .\n• When using to download artifacts from another pipeline, the job does not wait for the needed job to complete. Using to wait for jobs to complete is limited to jobs in the same pipeline. Make sure that the needed job in the other pipeline completes before the job that needs it tries to download the artifacts.\n• You can’t download artifacts from jobs that run in .\n• To download artifacts between parent-child pipelines, use .\n\nA child pipeline can download artifacts from a job in its parent pipeline or another child pipeline in the same parent-child pipeline hierarchy.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : A pipeline ID. Must be a pipeline present in the same parent-child pipeline hierarchy.\n• : The job to download artifacts from.\n\nIn this example, the job in the parent pipeline creates some artifacts. The job triggers a child pipeline, and passes the variable to the child pipeline as a new variable. The child pipeline can use that variable in to download artifacts from the parent pipeline.\n• The attribute does not accept the current pipeline ID ( ). To download artifacts from a job in the current pipeline, use .\n• You cannot use in a trigger job, or to fetch artifacts from a multi-project pipeline. To fetch artifacts from a multi-project pipeline use .\n\nTo need a job that sometimes does not exist in the pipeline, add to the configuration. If not defined, is the default.\n\nJobs that use , , or and that are added with might not always be added to a pipeline. GitLab checks the relationships before starting a pipeline:\n• If the entry has and the needed job is present in the pipeline, the job waits for it to complete before starting.\n• If the needed job is not present, the job can start when all other needs requirements are met.\n• If the section contains only optional jobs, and none are added to the pipeline, the job starts immediately (the same as an empty entry: ).\n• If a needed job has , but it was not added to the pipeline, the pipeline fails to start with an error similar to: 'job1' job needs 'job2' job, but it was not added to the pipeline .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nIn this example:\n• When the branch is the default branch, is added to the pipeline, so:\n• waits for both and to complete.\n• When the branch is not the default branch, is not added to the pipeline, so:\n• waits for only to complete, and does not wait for the missing .\n• has no other needed jobs and starts immediately (at the same time as ), like .\n\nYou can mirror the pipeline status from an upstream pipeline to a job by using the keyword. The latest pipeline status from the default branch is replicated to the job.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• A full project path, including namespace and group. If the project is in the same group or namespace, you can omit them from the keyword. For example: or .\n• If you add the keyword to , the job no longer mirrors the pipeline status. The behavior changes to .\n\nJobs can use to run a job multiple times in parallel in a single pipeline, but with different variable values for each instance of the job.\n\nUse to execute jobs out-of-order depending on parallelized jobs.\n\nKeyword type: Job keyword. You can use it only as part of a job. Must be used with .\n\nSupported values: An array of hashes of variables:\n• The variables and values must be selected from the variables and values defined in the job.\n\nThe above example generates the following jobs:\n\nThe job runs as soon as the job finishes.\n• Specify a parallelized job using needs with multiple parallelized jobs.\n• None The order of the matrix variables in must match the order of the matrix variables in the needed job. For example, reversing the order of the variables in the job in the earlier example above would be invalid: - : app1 # The variable order does not match `linux:build` and is invalid.\n\nUse to define a GitLab Pages job that uploads static content to GitLab. The content is then published as a website.\n• Alternatively, define if want to use a different content directory.\n\nKeyword type: Job keyword or Job name (deprecated). You can use it only as part of a job.\n• A boolean. Uses the default configuration when set to\n• A hash of configuration options, see the following sections for details.\n\nThis example renames the directory to . This directory is exported as an artifact and published with GitLab Pages.\n\nThis example does not move the directory, but uses the property directly. It also configures the pages deployment to be unpublished after a week.\n\nDeprecated: Use as a job name\n\nUsing as a job name results in the same behavior as specifying the Pages property . This method is available for backwards compatibility, but might not receive all future improvements to the Pages job configuration.\n\nExample using as a job name:\n\nTo use as a job name without triggering a Pages deployment, set the property to false:\n\nUse to configure the content directory of a job. The top-level keyword is deprecated as of GitLab 17.9 and must now be nested under the keyword.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: A path to a directory containing the Pages content. In GitLab 17.10 and later, if not specified, the default directory is used and if specified, this path is automatically appended to .\n\nThis example uses Eleventy to generate a static website and output the generated HTML files into a the directory. This directory is exported as an artifact and published with GitLab Pages.\n\nIt is also possible to use variables in the field. For example:\n\nThe publish path specified must be relative to the build root.\n\nUse to configure a path prefix for parallel deployments of GitLab Pages.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nThe given value is converted to lowercase and shortened to 63 bytes. Everything except alphanumeric characters or periods is replaced with a hyphen. Leading and trailing hyphens or periods are not permitted.\n\nIn this example, a different pages deployment is created for each branch.\n\nUse to specify how long a deployment should be available before it expires. After the deployment is expired, it’s deactivated by a cron job running every 10 minutes.\n\nBy default, parallel deployments expire automatically after 24 hours. To disable this behavior, set the value to .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: The expiry time. If no unit is provided, the time is in seconds. Valid values include:\n\nUse to run a job multiple times in parallel in a single pipeline.\n\nMultiple runners must exist, or a single runner must be configured to run multiple jobs concurrently.\n\nParallel jobs are named sequentially from to .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• A numeric value from to .\n\nThis example creates 5 jobs that run in parallel, named to .\n• Every parallel job has a and predefined CI/CD variable set.\n• A pipeline with jobs that use might:\n• Create more jobs running in parallel than available runners. Excess jobs are queued and marked while waiting for an available runner.\n• Create too many jobs, and the pipeline fails with a error. The maximum number of jobs that can exist in active pipelines is limited at the instance-level.\n\nUse to run a job multiple times in parallel in a single pipeline, but with different variable values for each instance of the job.\n\nMultiple runners must exist, or a single runner must be configured to run multiple jobs concurrently.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: An array of hashes of variables:\n• The variable names can use only numbers, letters, and underscores ( ).\n• The values must be either a string, or an array of strings.\n• The number of permutations cannot exceed 200.\n\nThe example generates 10 parallel jobs, each with different values for and :\n• None jobs add the variable values to the job names to differentiate the jobs from each other, but large values can cause names to exceed limits:\n• Job names must be 255 characters or fewer.\n• When using , job names must be 128 characters or fewer.\n• None You cannot create multiple matrix configurations with the same variable values but different variable names. Job names are generated from the variable values, not the variable names, so matrix entries with identical values generate identical job names that overwrite each other. For example, this configuration would try to create two series of identical jobs, but the versions overwrite the versions:\n• There’s a known issue when using tags with .\n• Select different runner tags for each parallel matrix job.\n\nThe release job must have access to the , which must be in the .\n\nIf you use the Docker executor, you can use this image from the GitLab container registry:\n\nIf you use the Shell executor or similar, install on the server where the runner is registered.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• When you add a Git tag in the UI at Code > Tags.\n• None All release jobs, except trigger jobs, must include the keyword. A release job can use the output from script commands. If you don’t need the script, you can use a placeholder: An issue exists to remove this requirement.\n• None The section executes after the keyword and before the .\n• None A release is created only if the job’s main script succeeds.\n• None If the release already exists, it is not updated and the job with the keyword fails.\n• CI/CD example of the keyword.\n\nRequired. The Git tag for the release.\n\nIf the tag does not exist in the project yet, it is created at the same time as the release. New tags use the SHA associated with the pipeline.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nTo create a release when a new tag is added to the project:\n• Use the CI/CD variable as the .\n• Use to configure the job to run only for new tags.\n\nTo create a release and a new tag at the same time, your should not configure the job to run only for new tags. A semantic versioning example:\n\nIf the tag does not exist, the newly created tag is annotated with the message specified by . If omitted, a lightweight tag is created.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nThe release name. If omitted, it is populated with the value of .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nThe long description of the release.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The path to a file that contains the description.\n• The file location must be relative to the project directory ( ).\n• If the file is a symbolic link, it must be in the .\n• The and filename can’t contain spaces.\n• The is evaluated by the shell that runs . You can use CI/CD variables to define the description, but some shells use different syntax to reference variables. Similarly, some shells might require special characters to be escaped. For example, backticks ( ) might need to be escaped with a backslash ( ).\n\nThe for the release, if the doesn’t exist yet.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• A commit SHA, another tag name, or a branch name.\n\nThe title of each milestone the release is associated with.\n\nThe date and time when the release is ready.\n• A date enclosed in quotes and expressed in ISO 8601 format.\n• If it is not defined, the current date and time is used.\n\nUse to include asset links in the release.\n\nUse to create a resource group that ensures a job is mutually exclusive across different pipelines for the same project.\n\nFor example, if multiple jobs that belong to the same resource group are queued simultaneously, only one of the jobs starts. The other jobs wait until the is free.\n\nResource groups behave similar to semaphores in other programming languages.\n\nYou can choose a process mode to strategically control the job concurrency for your deployment preferences. The default process mode is . To change the process mode of a resource group, use the API to send a request to edit an existing resource group.\n\nYou can define multiple resource groups per environment. For example, when deploying to physical devices, you might have multiple physical devices. Each device can be deployed to, but only one deployment can occur per device at any given time.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• Only letters, digits, , , , , , , , and spaces. It can’t start or end with . CI/CD variables are supported.\n\nIn this example, two jobs in two separate pipelines can never run at the same time. As a result, you can ensure that concurrent deployments never happen to the production environment.\n\nUse to configure how many times a job is retried if it fails. If not defined, defaults to and jobs do not retry.\n\nWhen a job fails, the job is processed up to two more times, until it succeeds or reaches the maximum number of retries.\n\nBy default, all failure types cause the job to be retried. Use or to select which failures to retry on.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nwill be retried up to 2 times if the exit code is or if it had a runner system failure.\n\nUse with to retry jobs for only specific failure cases. is the maximum number of retries, like , and can be , , or .\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• A single failure type, or an array of one or more failure types:\n• : Retry when the failure reason is unknown.\n• : Retry when:\n• The runner failed to pull the Docker image. For , , executors.\n• : Retry when the job got stuck or timed out.\n• : Retry if there is a runner system failure (for example, job setup failed).\n• : Retry if the runner is unsupported.\n• : Retry if a delayed job could not be executed.\n• : Retry if the script exceeded the maximum execution time set for the job.\n• : Retry if the job is archived and can’t be run.\n• : Retry if the job failed to complete prerequisite tasks.\n• : Retry if the scheduler failed to assign the job to a runner.\n• : Retry if there is an unknown job problem.\n\nIf there is a failure other than a runner system failure, the job is not retried.\n\nExample of (array of failure types):\n\nUse with to retry jobs for only specific failure cases. is the maximum number of retries, like , and can be , , or .\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nYou can specify the number of retry attempts for certain stages of job execution using variables.\n\nUse to include or exclude jobs in pipelines.\n\nRules are evaluated when the pipeline is created, and evaluated in order. When a match is found, no more rules are checked and the job is either included or excluded from the pipeline depending on the configuration. If no rules match, the job is not added to the pipeline.\n\naccepts an array of rules. Each rules must have at least one of:\n\nRules can also optionally be combined with:\n\nYou can combine multiple keywords together for complex rules.\n\nThe job is added to the pipeline:\n• If an , , or rule matches, and is configured with (default if not defined), , or .\n• If a rule is reached that is only , , or .\n\nThe job is not added to the pipeline:\n• If a rule matches and has .\n\nFor additional examples, see Specify when jobs run with .\n\nUse clauses to specify when to add a job to a pipeline:\n• If an statement is true, add the job to the pipeline.\n• If an statement is true, but it’s combined with , do not add the job to the pipeline.\n• If an statement is false, check the next item (if any more exist).\n• Based on the values of CI/CD variables or predefined CI/CD variables, with some exceptions.\n\nKeyword type: Job-specific and pipeline-specific. You can use it as part of a job to configure the job behavior, or with to configure the pipeline behavior.\n• You cannot use nested variables with . See issue 327780 for more details.\n• If a rule matches and has no defined, the rule uses the defined for the job, which defaults to if not defined.\n• You can mix at the job-level with in rules. configuration in takes precedence over at the job-level.\n• Unlike variables in sections, variables in rules expressions are always formatted as .\n• You can use with to conditionally include other configuration files.\n• CI/CD variables on the right side of and expressions are evaluated as regular expressions.\n\nUse to specify when to add a job to a pipeline by checking for changes to specific files.\n\nFor new branch pipelines or when there is no Git event, always evaluates to true and the job always runs. Pipelines like tag pipelines, scheduled pipelines, and manual pipelines, all do not have a Git event associated with them. To cover these cases, use to specify the branch to compare against the pipeline ref.\n\nIf you do not use , you should use only with branch pipelines or merge request pipelines, though still evaluates to true when creating a new branch. With:\n• Merge request pipelines, compares the changes with the target MR branch.\n• Branch pipelines, compares the changes with the previous commit on the branch.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nAn array including any number of:\n• Paths to files. The file paths can include CI/CD variables.\n• Wildcard paths for:\n• A directory and all its subdirectories, for example .\n• Wildcard glob paths for all files with the same extension or multiple extensions, for example or .\n• Wildcard paths to files in the root directory, or all directories, wrapped in double quotes. For example or .\n\nIn this example:\n• If the pipeline is a merge request pipeline, check and the files in for changes.\n• If has changed, add the job to the pipeline as a manual job, and the pipeline continues running even if the job is not triggered ( ).\n• If a file in has changed, add the job to the pipeline.\n• If no listed files have changed, do not add either job to any pipeline (same as ).\n• Glob patterns are interpreted with Ruby’s with the flags .\n• A maximum of 50 patterns or file paths can be defined per section.\n• resolves to if any of the matching files are changed (an operation).\n• For additional examples, see Specify when jobs run with .\n• You can use the character for both variables and paths. For example, if the variable exists, its value is used. If it does not exist, the is interpreted as being part of a path.\n• Jobs or pipelines can run unexpectedly when using .\n\nUse to specify that a job only be added to a pipeline when specific files are changed, and use to specify the files.\n\nis the same as using without any subkeys. All additional details and related topics are the same.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• Same as above.\n\nIn this example, both jobs have the same behavior.\n\nUse to specify which ref to compare against for changes to the files listed under .\n\nKeyword type: Job keyword. You can use it only as part of a job, and it must be combined with .\n• A branch name, like , , or .\n• A tag name, like or .\n\nIn this example, the job is only included when the has changed relative to and the pipeline source is a merge request event.\n• Using with merged results pipelines can cause unexpected results, because the comparison base is an internal commit that GitLab creates.\n• You can use to skip a job if the branch is empty.\n\nUse to run a job when certain files exist in the repository.\n\nKeyword type: Job keyword. You can use it as part of a job or an .\n• An array of file paths. Paths are relative to the project directory ( ) and can’t directly link outside it. File paths can use glob patterns and CI/CD variables.\n\nIn this example:\n• runs if a exists in the root directory of the repository.\n• runs if a exists anywhere in the repository.\n• Glob patterns are interpreted with Ruby’s with the flags .\n• For performance reasons, GitLab performs a maximum of 50,000 checks against patterns or file paths. After the 50,000th check, rules with patterned globs always match. In other words, the rule always assumes a match in projects with more than 50,000 files, or if there are fewer than 50,000 files but the rules are checked more than 50,000 times.\n• If there are multiple patterned globs, the limit is 50,000 divided by the number of globs. For example, a rule with 5 patterned globs has file limit of 10,000.\n• A maximum of 50 patterns or file paths can be defined per section.\n• resolves to if any of the listed files are found (an operation).\n• With job-level , GitLab searches for the files in the project and ref that runs the pipeline. When using with , GitLab searches for the files in the project and ref of the file that contains the section. The project containing the section can be different than the project running the pipeline when using:\n• cannot search for the presence of artifacts, because evaluation happens before jobs run and artifacts are fetched.\n\nis the same as using without any subkeys. All additional details are the same.\n\nKeyword type: Job keyword. You can use it as part of a job or an .\n\nIn this example, both jobs have the same behavior.\n• In some cases you cannot use or in a CI/CD variable with . See issue 386595 for more details.\n\nUse to specify the location in which to search for the files listed under . Must be used with .\n\nKeyword type: Job keyword. You can use it as part of a job or an , and it must be combined with .\n• : Optional. The commit ref to use to search for the file. The ref can be a tag, branch name, or SHA. Defaults to the of the project when not specified.\n\nIn this example, the job is only included when the exists in the project on the commit tagged with .\n\nUse alone or as part of another rule to control conditions for adding a job to a pipeline. is similar to , but with slightly different input options.\n\nIf a rule is not combined with , , or , it always matches if reached when evaluating a job’s rules.\n\nKeyword type: Job-specific. You can use it only as part of a job.\n• (default): Run the job only when no jobs in earlier stages fail.\n• : Run the job only when at least one job in an earlier stage fails.\n• : Don’t run the job regardless of the status of jobs in earlier stages.\n• : Run the job regardless of the status of jobs in earlier stages.\n• : Add the job to the pipeline as a manual job. The default value for changes to .\n• : Add the job to the pipeline as a delayed job.\n\nIn this example, is added to pipelines:\n• For the default branch, with which is the default behavior when is not defined.\n• In all other cases as a manual job.\n• When evaluating the status of jobs for and :\n• Jobs with in earlier stages are considered successful, even if they failed.\n• Skipped jobs in earlier stages, for example manual jobs that have not been started, are considered successful.\n• When using to add a manual job:\n• becomes by default. This default is the opposite of using to add a manual job.\n• To achieve the same behavior as defined outside of , set to .\n\nUse in to allow a job to fail without stopping the pipeline.\n\nYou can also use with a manual job. The pipeline continues running without waiting for the result of the manual job. combined with in rules causes the pipeline to wait for the manual job to run before continuing.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• or . Defaults to if not defined.\n\nIf the rule matches, then the job is a manual job with .\n• The rule-level overrides the job-level , and only applies when the specific rule triggers the job.\n\nUse in rules to update a job’s for specific conditions. When a condition matches a rule, the job’s configuration is completely replaced with the in the rule.\n\nKeyword type: Job-specific. You can use it only as part of a job.\n• An array of job names as strings.\n• A hash with a job name, optionally with additional attributes.\n• An empty array ( ), to set the job needs to none when the specific condition is met.\n\nIn this example:\n• If the pipeline runs on a branch that is not the default branch, and therefore the rule matches the first condition, the job needs the job.\n• If the pipeline runs on the default branch, and therefore the rule matches the second condition, the job needs the job.\n• in rules override any defined at the job-level. When overridden, the behavior is same as job-level .\n• in rules can accept and .\n\nUse in to define variables for specific conditions.\n\nKeyword type: Job-specific. You can use it only as part of a job.\n• A hash of variables in the format .\n\nUse in rules to update a job’s value for specific conditions.\n\nKeyword type: Job-specific. You can use it only as part of a job.\n• The rule-level overrides the job-level , and only applies when the specific rule triggers the job.\n\nUse to define a series of steps to be executed in a job. Each step can be either a script or a predefined step.\n\nYou can also provide optional environment variables and inputs.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• An array of hashes, where each hash represents a step with the following possible keys:\n• : A string representing the name of the step.\n• : A string or array of strings containing shell commands to execute.\n• : Optional. A hash of environment variables specific to this step.\n\nEach array entry must have a , and one or (but not both).\n\nIn this example, the job has two steps:\n• uses a predefined step with an environment variable and an input parameter.\n• A step can have either a or a key, but not both.\n• A configuration cannot be used together with existing keyword.\n• Multi-line scripts can be defined using YAML block scalar syntax.\n\nUse to specify commands for the runner to execute.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• When you use these special characters in , you must use single quotes ( ) or double quotes ( ).\n• Use color codes with to make job logs easier to review.\n\nUse to specify CI/CD secrets to:\n• Make available in the job as CI/CD variables ( type by default).\n\nUse to specify secrets provided by a HashiCorp Vault.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : Name of the secrets engine. Can be one of (default), , or .\n• : Name of the field where the password is stored.\n\nTo specify all details explicitly and use the KV-V2 secrets engine:\n\nYou can shorten this syntax. With the short syntax, and both default to :\n\nTo specify a custom secrets engine path in the short syntax, add a suffix that starts with :\n\nUse to specify secrets provided by GCP Secret Manager.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : Name of the secret.\n\nUse to specify secrets provided by a Azure Key Vault.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : Name of the secret.\n\nUse to configure the secret to be stored as either a or type CI/CD variable\n\nBy default, the secret is passed to the job as a type CI/CD variable. The value of the secret is stored in the file and the variable contains the path to the file.\n\nIf your software can’t use type CI/CD variables, set to store the secret value directly in the variable.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The keyword is a setting for the CI/CD variable and must be nested under the CI/CD variable name, not in the section.\n\nUse to explicitly select a token to use when authenticating with Vault by referencing the token’s CI/CD variable.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The name of an ID token\n• When the keyword is not set, the first ID token is used to authenticate.\n\nUse to specify any additional Docker images that your scripts require to run successfully. The image is linked to the image specified in the keyword.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nSupported values: The name of the services image, including the registry path if needed, in one of these formats:\n\nCI/CD variables are supported, but not for .\n\nIn this example, GitLab launches two containers for the job:\n• A PostgreSQL container. The commands in the Ruby container can connect to the PostgreSQL database at the hostname.\n\nUse to pass options to the Docker executor of a GitLab Runner.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nA hash of options for the Docker executor, which can include:\n• : Selects the architecture of the image to pull. When not specified, the default is the same platform as the host runner.\n• : Specify the username or UID to use when running the container.\n\nThe pull policy that the runner uses to fetch the Docker image.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• A single pull policy, or multiple pull policies in an array. Can be , , or .\n• If the runner does not support the defined pull policy, the job fails with an error similar to: ERROR: Job failed (system failure): the configured PullPolicies ([always]) are not allowed by AllowedPullPolicies ([never]) .\n\nUse to define which stage a job runs in. Jobs in the same can execute in parallel (see Additional details).\n\nIf is not defined, the job uses the stage by default.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: A string, which can be a:\n• The stage name must be 255 characters or fewer.\n• Jobs can run in parallel if they run on different runners.\n• If you have only one runner, jobs can run in parallel if the runner’s setting is greater than .\n\nUse the stage to make a job run at the start of a pipeline. By default, is the first stage in a pipeline. User-defined stages execute after . You do not have to define in .\n\nIf a pipeline contains only jobs in the or stages, it does not run. There must be at least one other job in a different stage.\n\nKeyword type: You can only use it with a job’s keyword.\n• If a pipeline has jobs with and jobs in the stage, they will all start as soon as the pipeline is created. Jobs with start immediately, ignoring any stage configuration.\n• A pipeline execution policy can define a stage which runs before .\n\nUse the stage to make a job run at the end of a pipeline. By default, is the last stage in a pipeline. User-defined stages execute before . You do not have to define in .\n\nIf a pipeline contains only jobs in the or stages, it does not run. There must be at least one other job in a different stage.\n\nKeyword type: You can only use it with a job’s keyword.\n• A pipeline execution policy can define a stage which runs after .\n\nUse to select a specific runner from the list of all runners that are available for the project.\n\nWhen you register a runner, you can specify the runner’s tags, for example , , or . To pick up and run a job, a runner must be assigned every tag listed in the job.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• An array of tag names, which are case-sensitive.\n\nIn this example, only runners with both the and tags can run the job.\n• The number of tags must be less than .\n• Use tags to control which jobs a runner can run\n• Select different runner tags for each parallel matrix job\n\nUse to configure a timeout for a specific job. If the job runs for longer than the timeout, the job fails.\n\nThe job-level timeout can be longer than the project-level timeout, but can’t be longer than the runner’s timeout.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nSupported values: A period of time written in natural language. For example, these are all equivalent:\n\nUse to declare that a job is a “trigger job” which starts a downstream pipeline that is either:\n\nTrigger jobs can use only a limited set of GitLab CI/CD configuration keywords. The keywords available for use in trigger jobs are:\n• (only with a value of , , or ).\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• For multi-project pipelines, the path to the downstream project. CI/CD variables are supported in GitLab 15.3 and later, but not job-only variables. Alternatively, use .\n• You can use in the same job as , but you cannot use the API to start trigger jobs. See issue 284086 for more details.\n• You cannot manually specify CI/CD variables before running a manual trigger job.\n• CI/CD variables defined in a top-level section (globally) or in the trigger job are forwarded to the downstream pipeline as trigger variables.\n• Pipeline variables are not passed to downstream pipelines by default. Use trigger:forward to forward these variables to downstream pipelines.\n• Job-only variables are not available in trigger jobs.\n• Environment variables defined in the runner’s are not available to trigger jobs and are not passed to downstream pipelines.\n• You cannot use in a trigger job.\n• To run a pipeline for a specific branch, tag, or commit, you can use a trigger token to authenticate with the pipeline triggers API. The trigger token is different than the keyword.\n\nUse to declare that a job is a “trigger job” which starts a child pipeline.\n• to set the inputs when the downstream pipeline configuration uses .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The path to the child pipeline’s configuration file.\n\nUse to declare that a job is a “trigger job” which starts a multi-project pipeline.\n\nBy default, the multi-project pipeline triggers for the default branch. Use to specify a different branch.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The path to the downstream project. CI/CD variables are supported in GitLab 15.3 and later, but not job-only variables.\n\nExample of for a different branch:\n• To run a pipeline for a specific branch, tag, or commit, you can also use a trigger token to authenticate with the pipeline triggers API. The trigger token is different than the keyword.\n\nUse to force the job to wait for the downstream pipeline to complete before it is marked as success.\n\nThis behavior is different than the default, which is for the job to be marked as success as soon as the downstream pipeline is created.\n\nThis setting makes your pipeline execution linear rather than parallel.\n\nIn this example, jobs from subsequent stages wait for the triggered pipeline to successfully complete before starting.\n• Optional manual jobs in the downstream pipeline do not affect the status of the downstream pipeline or the upstream trigger job. The downstream pipeline can complete successfully without running any optional manual jobs.\n• Blocking manual jobs in the downstream pipeline must run before the trigger job is marked as successful or failed. The trigger job shows pending ( ) if the downstream pipeline status is waiting for manual action ( ) due to manual jobs. By default, jobs in later stages do not start until the trigger job completes.\n• If the downstream pipeline has a failed job, but the job uses , the downstream pipeline is considered successful and the trigger job shows success.\n\nUse to set the inputs when the downstream pipeline configuration uses .\n\nUse to specify what to forward to the downstream pipeline. You can control what is forwarded to both parent-child pipelines and multi-project pipelines.\n\nForwarded variables do not get forwarded again in nested downstream pipelines by default, unless the nested downstream trigger job also uses .\n• : (default), or . When , variables defined in the trigger job are passed to downstream pipelines.\n• : or (default). When , pipeline variables are passed to the downstream pipeline.\n\nRun this pipeline manually, with the CI/CD variable :\n• CI/CD variables forwarded to downstream pipelines with are pipeline variables, which have high precedence. If a variable with the same name is defined in the downstream pipeline, that variable is usually overwritten by the forwarded variable.\n\nUse to configure the conditions for when jobs run. If not defined in a job, the default value is .\n\nKeyword type: Job keyword. You can use it as part of a job. and can also be used in .\n• (default): Run the job only when no jobs in earlier stages fail.\n• : Run the job only when at least one job in an earlier stage fails.\n• : Don’t run the job regardless of the status of jobs in earlier stages. Can only be used in a section or .\n• : Run the job regardless of the status of jobs in earlier stages.\n• : Add the job to the pipeline as a manual job.\n• : Add the job to the pipeline as a delayed job.\n\nIn this example, the script:\n• Always executes as the last step in pipeline regardless of success or failure.\n• Executes when you run it manually in the GitLab UI.\n• When evaluating the status of jobs for and :\n• Jobs with in earlier stages are considered successful, even if they failed.\n• Skipped jobs in earlier stages, for example manual jobs that have not been started, are considered successful.\n• The default value for is with . The default value changes to with .\n• can be used with for more dynamic job control.\n• can be used with to control when a pipeline can start.\n\nUse with to define a custom confirmation message for manual jobs. If there is no manual job defined with , this keyword has no effect.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nVariables can be defined in a CI/CD job, or as a top-level (global) keyword to define default CI/CD variables for all jobs.\n• All YAML-defined variables are also set to any linked Docker service containers.\n• YAML-defined variables are meant for non-sensitive project configuration. Store sensitive information in protected variables or CI/CD secrets.\n• Manual pipeline variables and scheduled pipeline variables are not passed to downstream pipelines by default. Use trigger:forward to forward these variables to downstream pipelines.\n• Predefined variables are variables the runner automatically creates and makes available in the job.\n• You can configure runner behavior with variables.\n\nYou can use job variables in commands in the job’s , , or sections, and also with some job keywords. Check the Supported values section of each job keyword to see if it supports variables.\n\nYou cannot use job variables as values for global keywords like .\n\nSupported values: Variable name and value pairs:\n• The name can use only numbers, letters, and underscores ( ). In some shells, the first character must be a letter.\n• The value must be a string.\n\nIn this example:\n• has and job variables defined. Both job variables can be used in the section.\n\nVariables defined in a top-level section act as default variables for all jobs.\n\nEach default variable is made available to every job in the pipeline, except when the job already has a variable defined with the same name. The variable defined in the job takes precedence, so the value of the default variable with the same name cannot be used in the job.\n\nLike job variables, you cannot use default variables as values for other global keywords, like .\n\nSupported values: Variable name and value pairs:\n• The name can use only numbers, letters, and underscores ( ). In some shells, the first character must be a letter.\n• The value must be a string.\n\nIn this example:\n• has no variables defined. The default variable is copied to the job and can be used in the section.\n• already has a variable defined, so the default is not copied to the job. The job also has a job variable defined. Both job variables can be used in the section.\n\nUse the keyword to define a description for a default variable. The description displays with the prefilled variable name when running a pipeline manually.\n\nKeyword type: You can only use this keyword with default , not job .\n• When used without , the variable exists in pipelines that were not triggered manually, and the default value is an empty string ( ).\n\nUse the keyword to define a pipeline-level (default) variable’s value. When used with , the variable value is prefilled when running a pipeline manually.\n\nKeyword type: You can only use this keyword with default , not job .\n• If used without , the behavior is the same as .\n\nUse to define an array of values that are selectable in the UI when running a pipeline manually.\n\nMust be used with , and the string defined for :\n• Must also be one of the strings in the array.\n\nIf there is no , this keyword has no effect.\n\nKeyword type: You can only use this keyword with default , not job .\n\nUse the keyword to configure a variable to be expandable or not.\n\nKeyword type: You can use this keyword with both default and job .\n• : The variable is not expandable.\n• The result of is .\n• The result of is .\n• The keyword can only be used with default and job keywords. You can’t use it with or .\n\nThe following keywords are deprecated.\n\nDefining , , , , and globally is deprecated. Using these keywords at the top level is still possible to ensure backwards compatibility, but could be scheduled for removal in a future milestone.\n\nUse instead. For example:\n\nYou can use and to control when to add jobs to pipelines.\n• Use to define when a job runs.\n• Use to define when a job does not run.\n\nYou can use the and keywords to control when to add jobs to a pipeline based on branch names or pipeline types.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: An array including any number of:\n• Branch names, for example or .\n• Regular expressions that match against branch names, for example .\n• The following keywords: For pipelines triggered by the pipelines API. When the Git reference for a pipeline is a branch. For pipelines created by using a GitLab ChatOps command. When you use CI services other than GitLab. When an external pull request on GitHub is created or updated (See Pipelines for external pull requests). For pipelines created when a merge request is created or updated. Enables merge request pipelines, merged results pipelines, and merge trains. For multi-project pipelines created by using the API with , or the keyword. For pipelines triggered by a event, including for branches and tags. When the Git reference for a pipeline is a tag. For pipelines created by using a trigger token. For pipelines created by selecting New pipeline in the GitLab UI, from the project’s Build > Pipelines section.\n\nExample of and :\n• None Scheduled pipelines run on specific branches, so jobs configured with run on scheduled pipelines too. Add to prevent jobs with from running on scheduled pipelines.\n• None or used without any other keywords are equivalent to or . For example, the following two jobs configurations have the same behavior:\n• None If a job does not use , , or , then is set to and by default. For example, and are equivalent:\n\nYou can use the or keywords to control when to add jobs to a pipeline, based on the status of CI/CD variables.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nUse the keyword with to run a job, or with to skip a job, when a Git push event modifies a file.\n\nUse in pipelines with the following refs:\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: An array including any number of:\n• Wildcard paths for:\n• A directory and all its subdirectories, for example .\n• Wildcard glob paths for all files with the same extension or multiple extensions, for example or .\n• Wildcard paths to files in the root directory, or all directories, wrapped in double quotes. For example or .\n• resolves to if any of the matching files are changed (an operation).\n• Glob patterns are interpreted with Ruby’s with the flags .\n• If you use refs other than , , or , can’t determine if a given file is new or old and always returns .\n• If you use with other refs, jobs ignore the changes and always run.\n• If you use with other refs, jobs ignore the changes and never run.\n• Jobs or pipelines can run unexpectedly when using .\n\nUse or to control if jobs are added to the pipeline when the Kubernetes service is active in the project.\n\nKeyword type: Job-specific. You can use it only as part of a job.\n• The strategy accepts only the keyword.\n\nIn this example, the job runs only when the Kubernetes service is active in the project."
    },
    {
        "link": "https://docs.gitlab.com/ci",
        "document": "CI/CD is a continuous method of software development, where you continuously build, test, deploy, and monitor iterative code changes.\n\nThis iterative process helps reduce the chance that you develop new code based on buggy or failed previous versions. GitLab CI/CD can catch bugs early in the development cycle, and help ensure that the code deployed to production complies with your established code standards.\n\nThis process is part of a larger workflow:\n\nTo use GitLab CI/CD, you start with a file at the root of your project. This file specifies the stages, jobs, and scripts to be executed during your CI/CD pipeline. It is a YAML file with its own custom syntax.\n\nIn this file, you define variables, dependencies between jobs, and specify when and how each job should be executed.\n\nYou can name this file anything you want, but is the most common name, and the product documentation refers to it as the file or the CI/CD configuration file.\n\nFor more information, see:\n• The CI/CD YAML syntax reference, which lists all possible keywords\n\nRunners are the agents that run your jobs. These agents can run on physical machines or virtual instances. In your file, you can specify a container image you want to use when running the job. The runner loads the image, clones your project, and runs the job either locally or in the container.\n\nIf you use GitLab.com, runners on Linux, Windows, and macOS are already available for use. And you can register your own runners on GitLab.com if you’d like.\n\nIf you don’t use GitLab.com, you can:\n• Register runners or use runners already registered for your GitLab Self-Managed instance.\n\nFor more information, see:\n\nA pipeline is what you’re defining in the file, and is what happens when the contents of the file are run on a runner.\n\nPipelines are made up of jobs and stages:\n• Stages define the order of execution. Typical stages might be , , and .\n• Jobs specify the tasks to be performed in each stage. For example, a job can compile or test code.\n\nPipelines can be triggered by various events, like commits or merges, or can be on schedule. In your pipeline, you can integrate with a wide range of tools and platforms.\n\nFor more information, see:\n• Pipeline editor, which you use to edit your configuration\n\nStep 4: Use CI/CD variables as part of jobs\n\nGitLab CI/CD variables are key-value pairs you use to store and pass configuration settings and sensitive information, like passwords or API keys, to jobs in a pipeline.\n\nUse CI/CD variables to customize jobs by making values defined elsewhere accessible to jobs. You can hard-code CI/CD variables in your file, set them in your project settings, or generate them dynamically. You can define them for the project, group, or instance.\n\nTwo types of variables exist: custom variables and predefined.\n• Custom variables are user-defined. Create and manage them in the GitLab UI, API, or in configuration files.\n• Predefined variables are automatically set by GitLab and provide information about the current job, pipeline, and environment.\n\nVariables can be marked as “protected” or “masked” for added security.\n• Protected variables are only available to jobs running on protected branches or tags.\n• Masked variables have their values hidden in job logs to prevent sensitive information from being exposed.\n\nFor more information, see:\n\nA CI/CD component is a reusable pipeline configuration unit. Use a CI/CD component to compose an entire pipeline configuration or a small part of a larger pipeline.\n\nYou can add a component to your pipeline configuration with .\n\nReusable components help reduce duplication, improve maintainability, and promote consistency across projects. Create a component project and publish it to the CI/CD Catalog to share your component across multiple projects.\n\nGitLab also has CI/CD component templates for common tasks and integrations.\n\nFor more information, see:"
    },
    {
        "link": "https://docs.gitlab.com/ci/pipelines",
        "document": "CI/CD pipelines are the fundamental component of GitLab CI/CD. Pipelines are configured in a file by using YAML keywords.\n\nPipelines can run automatically for specific events, like when pushing to a branch, creating a merge request, or on a schedule. When needed, you can also run pipelines manually.\n• Global YAML keywords that control the overall behavior of the project’s pipelines.\n• Jobs that execute commands to accomplish a task. For example, a job could compile, test, or deploy code. Jobs run independently from each other, and are executed by runners.\n• Stages, which define how to group jobs together. Stages run in sequence, while the jobs in a stage run in parallel. For example, an early stage could have jobs that lint and compile code, while later stages could have jobs that test and deploy code. If all jobs in a stage succeed, the pipeline moves on to the next stage. If any job in a stage fails, the next stage is not (usually) executed and the pipeline ends early.\n\nA small pipeline could consist of three stages, executed in the following order:\n• A stage, with a job called that compiles the project’s code.\n• A stage, with two jobs called and that run various tests on the code. These tests would only run if the job completed successfully.\n• A stage, with a job called . This job would only run if both jobs in the stage started and completed successfully.\n\nTo get started with your first pipeline, see Create and run your first GitLab CI/CD pipeline.\n\nPipelines can be configured in many different ways:\n• Basic pipelines run everything in each stage concurrently, followed by the next stage.\n• Pipelines that use the keyword run based on dependencies between jobs and can run more quickly than basic pipelines.\n• Merge request pipelines run for merge requests only (rather than for every commit).\n• Merged results pipelines are merge request pipelines that act as though the changes from the source branch have already been merged into the target branch.\n• Merge trains use merged results pipelines to queue merges one after the other.\n• Parent-child pipelines break down complex pipelines into one parent pipeline that can trigger multiple child sub-pipelines, which all run in the same project and with the same SHA. This pipeline architecture is commonly used for mono-repos.\n• Multi-project pipelines combine pipelines for different projects together.\n\nPipelines and their component jobs and stages are defined with YAML keywords in the CI/CD pipeline configuration file for each project. When editing CI/CD configuration in GitLab, you should use the pipeline editor.\n\nYou can also configure specific aspects of your pipelines through the GitLab UI:\n\nIf you use VS Code to edit your GitLab CI/CD configuration, the GitLab Workflow extension for VS Code helps you validate your configuration and view your pipeline status.\n\nPipelines can be manually executed, with predefined or manually-specified variables.\n\nYou might do this if the results of a pipeline (for example, a code build) are required outside the standard operation of the pipeline.\n• On the left sidebar, select Search or go to and find your project.\n• In the Run for branch name or tag field, select the branch or tag to run the pipeline for.\n• Enter any CI/CD variables required for the pipeline to run. You can set specific variables to have their values prefilled in the form.\n\nThe pipeline now executes the jobs as configured.\n\nYou can use the and keywords to define pipeline-level (global) variables that are prefilled when running a pipeline manually. Use the description to explain information such as what the variable is used for, and what the acceptable values are.\n\nIn manually-triggered pipelines, the New pipeline page displays all pipeline-level variables that have a defined in the file. The description displays below the variable.\n\nYou can change the prefilled value, which overrides the value for that single pipeline run. Any variables overridden by using this process are expanded and not masked. If you do not define a for the variable in the configuration file, the variable name is still listed, but the value field is blank.\n\nIn this example:\n• is listed in the New pipeline page, but with no value set. The user is expected to define the value each time the pipeline is run manually.\n• is pre-filled in the New pipeline page with as the default value, and the message explains the other options.\n\nYou can define an array of CI/CD variable values the user can select from when running a pipeline manually. These values are in a dropdown list in the New pipeline page. Add the list of value options to and set the default value with . The string in must also be included in the list.\n\nYou can use a query string to pre-populate the New pipeline page. For example, the query string pre-populates the New pipeline page with:\n\nThe format of the URL is:\n\nThe following parameters are supported:\n• : specify the branch to populate the Run for field with.\n\nFor each or , a key and value are required.\n\nManual jobs, allow you to require manual interaction before moving forward in the pipeline.\n\nYou can do this straight from the pipeline graph. Select Run ( ) to execute that particular job.\n\nFor example, your pipeline can start automatically, but require a manual action to deploy to production. In the example below, the stage has a job with a manual action:\n\nIf a stage contains only manual jobs, you can start all the jobs at the same time by selecting Run all manual ( ) above the stage. If the stage contains non-manual jobs, the option is not displayed.\n\nTo push a commit without triggering a pipeline, add or , using any capitalization, to your commit message.\n\nAlternatively, with Git 2.10 or later, use the Git push option. The push option does not skip merge request pipelines.\n\nUsers with the Owner role for a project can delete a pipeline:\n• On the left sidebar, select Search or go to and find your project.\n• Select either the pipeline ID (for example ) or the pipeline status icon (for example Passed) of the pipeline to delete.\n• In the top right of the pipeline details page, select Delete.\n\nDeleting a pipeline does not automatically delete its child pipelines. See issue 39503 for more details.\n\nA strict security model is enforced when pipelines are executed on protected branches.\n\nThe following actions are allowed on protected branches if the user is allowed to merge or push to that specific branch:\n• Run manual pipelines (using the Web UI or pipelines API).\n• Retry or cancel existing jobs (using the Web UI or pipelines API).\n\nVariables marked as protected are accessible to jobs that run in pipelines for protected branches. Only assign users the right to merge to protected branches if they have permission to access sensitive information like deployment credentials and tokens.\n\nRunners marked as protected can run jobs only on protected branches, preventing untrusted code from executing on the protected runner and preserving deployment keys and other credentials from being unintentionally accessed. To ensure that jobs intended to be executed on protected runners do not use regular runners, they must be tagged accordingly.\n\nReview the deployment safety page for additional security recommendations for securing your pipelines.\n\nTrigger a pipeline when an upstream project is rebuilt (deprecated)\n\nYou can set up your project to automatically trigger a pipeline based on tags in a different project. When a new tag pipeline in the subscribed project finishes, it triggers a pipeline on your project’s default branch, regardless of the tag pipeline’s success, failure, or cancellation.\n• The upstream project must be public.\n• The user must have the Developer role in the upstream project.\n\nTo trigger the pipeline when the upstream project is rebuilt:\n• On the left sidebar, select Search or go to and find your project.\n• Enter the project you want to subscribe to, in the format . For example, if the project is , use .\n\nThe maximum number of upstream pipeline subscriptions is 2 by default, for both the upstream and downstream projects. On GitLab Self-Managed, an administrator can change this limit.\n\nThe total running time for a given pipeline excludes:\n• The duration of the initial run for any job that is retried or manually re-run.\n\nThat means that if a job is retried or manually re-run, only the duration of the latest run is included in the total running time.\n\nEach job is represented as a , which consists of:\n\nIn the example:\n• A begins at 0 and ends at 2.\n• A’ begins at 2 and ends at 4.\n• B begins at 1 and ends at 3.\n• C begins at 6 and ends at 7.\n\nVisually, it can be viewed as:\n\nBecause A is retried, we ignore it and count only job A’. The union of B, A’, and C is (1, 4) and (6, 7). Therefore, the total running time is:\n\nTo view all the pipelines that ran for your project:\n• On the left sidebar, select Search or go to and find your project.\n\nYou can filter the Pipelines page by:\n\nSelect Pipeline ID in the dropdown list in the top right to display the pipeline IDs (unique ID across the instance). Select pipeline IID to display the pipeline IIDs (internal ID, unique across the project only).\n\nTo view the pipelines that relate to a specific merge request, go to the Pipelines tab in the merge request.\n\nSelect a pipeline to open the pipeline details page which shows every job in the pipeline. From this page you can cancel a running pipeline, retry failed jobs, or delete a pipeline.\n\nThe pipeline details page displays a graph of all the jobs in the pipeline:\n\nYou can use a standard URL to access the details for specific pipelines:\n• : The details page for the latest pipeline for the most recent commit on the default branch in the project.\n• : The details page for the latest pipeline for the most recent commit on branch in the project.\n\nWhen you configure jobs with the keyword, you have two options for how to group the jobs in the pipeline details page. To group the jobs by stage configuration, select stage in the Group jobs by section:\n\nTo group the jobs by configuration, select Job dependencies. You can optionally select Show dependencies to render lines between dependent jobs.\n\nJobs in the leftmost column run first, and jobs that depend on them are grouped in the next columns. In this example:\n• is configured with and depends on no jobs, so it displays in the first column, despite being in the stage.\n• depends on , and depends on both and , so both test jobs display in the second column.\n• Both jobs depend on jobs in second column (which themselves depend on other earlier jobs), so the deploy jobs display in the third column.\n\nWhen you hover over a job in the Job dependencies view, every job that must run before the selected job is highlighted:\n\nPipeline mini graphs take less space and can tell you at a quick glance if all jobs passed or something failed. They show all related jobs for a single commit and the net result of each stage of your pipeline. You can quickly see what failed and fix it.\n\nThe pipeline mini graph always group jobs by stage, and display throughout GitLab when displaying pipeline or commit details.\n\nStages in pipeline mini graphs are expandable. Hover your mouse over each stage to see the name and status, and select a stage to expand its jobs list.\n\nWhen a pipeline contains a job that triggers a downstream pipeline, you can see the downstream pipeline in the pipeline details view and mini graphs.\n\nIn the pipeline details view, a card displays for every triggered downstream pipeline on the right of the pipeline graph. Hover over a card to see which job triggered the downstream pipeline. Select a card to display the downstream pipeline to the right of the pipeline graph.\n\nIn the pipeline mini graph, the status of every triggered downstream pipeline displays as additional status icons to the right of the mini graph. Select a downstream pipeline status icon to go to the detail page of that downstream pipeline.\n\nPipeline analytics are available on the CI/CD Analytics page.\n\nPipeline status and test coverage report badges are available and configurable for each project. For information on adding pipeline badges to projects, see Pipeline badges.\n• Perform basic functions. For more information, see Pipelines API.\n• Maintain pipeline schedules. For more information, see Pipeline schedules API.\n• Trigger pipeline runs. For more information, see:\n\nWhen a runner picks a pipeline job, GitLab provides that job’s metadata. This includes the Git refspecs, which indicate which ref (such as branch or tag) and commit (SHA1) are checked out from your project repository.\n\nThis table lists the refspecs injected for each pipeline type:\n\nThe refs and exist in your project repository. GitLab generates the special ref during a running pipeline job. This ref can be created even after the associated branch or tag has been deleted. It’s therefore useful in some features such as automatically stopping an environment, and merge trains that might run pipelines after branch deletion.\n\nWhen a user deletes their GitLab.com account, the deletion does not occur for seven days. During this period, any pipeline subscriptions created by that user continue to run with the user’s original permissions. To prevent unauthorized pipeline executions, immediately update pipeline subscription settings for the deleted user."
    },
    {
        "link": "https://docs.gitlab.com/ci/quick_start",
        "document": "This tutorial shows you how to configure and run your first CI/CD pipeline in GitLab.\n\nIf you are already familiar with basic CI/CD concepts, you can learn about common keywords in Tutorial: Create a complex pipeline.\n\nBefore you start, make sure you have:\n• A project in GitLab that you would like to use CI/CD for.\n• The Maintainer or Owner role for the project.\n\nIf you don’t have a project, you can create a public project for free on https://gitlab.com.\n\nTo create and run your first pipeline:\n• None Ensure you have runners available to run your jobs. If you’re using GitLab.com, you can skip this step. GitLab.com provides instance runners for you.\n• None Create a file at the root of your repository. This file is where you define the CI/CD jobs.\n\nWhen you commit the file to your repository, the runner runs your jobs. The job results are displayed in a pipeline.\n\nEnsure you have runners available\n\nIn GitLab, runners are agents that run your CI/CD jobs.\n\nIf you’re using GitLab.com, you can skip this step. GitLab.com provides instance runners for you.\n• On the left sidebar, select Search or go to and find your project.\n\nAs long as you have at least one runner that’s active, with a green circle next to it, you have a runner available to process your jobs.\n\nIf you don’t have access to these settings, contact your GitLab administrator.\n\nIf you don’t have a runner\n\nIf you don’t have a runner:\n• Register the runner for your project. Choose the executor.\n\nWhen your CI/CD jobs run, in a later step, they will run on your local machine.\n\nNow create a file. It is a YAML file where you specify instructions for GitLab CI/CD.\n\nIn this file, you define:\n• The structure and order of jobs that the runner should execute.\n• The decisions the runner should make when specific conditions are encountered.\n\nTo create a file in your project:\n• None On the left sidebar, select Search or go to and find your project.\n• None Above the file list, select the branch you want to commit to. If you’re not sure, leave or . Then select the plus icon ( ) and New file:\n• None For the Filename, type and in the larger window, paste this sample code: - echo \"This job tests something, but takes more time than test-job1.\" - echo \"After the echo commands complete, it runs the sleep command for 20 seconds\" - echo \"which simulates a test that runs 20 seconds longer than test-job1\" - echo \"This job deploys something from the $CI_COMMIT_BRANCH branch.\" This example shows four jobs: , , , and . The comments listed in the commands are displayed in the UI when you view the jobs. The values for the predefined variables and are populated when the jobs run.\n\nThe pipeline starts and runs the jobs you defined in the file.\n\nView the status of your pipeline and jobs\n\nNow take a look at your pipeline and the jobs within.\n• None Go to Build > Pipelines. A pipeline with three stages should be displayed:\n• None View a visual representation of your pipeline by selecting the pipeline ID:\n• None View details of a job by selecting the job name. For example, :\n\nYou have successfully created your first CI/CD pipeline in GitLab. Congratulations!\n\nNow you can get started customizing your and defining more advanced jobs.\n\nHere are some tips to get started working with the file.\n\nFor the complete syntax, see the full CI/CD YAML syntax reference.\n• Use the pipeline editor to edit your file.\n• Each job contains a script section and belongs to a stage:\n• describes the sequential execution of jobs. If there are runners available, jobs in a single stage run in parallel.\n• Use the keyword to run jobs out of stage order, to increase pipeline speed and efficiency.\n• You can set additional configuration to customize how your jobs and stages perform:\n• Use the keyword to specify when to run or skip jobs. The and legacy keywords are still supported, but can’t be used with in the same job.\n• Keep information across jobs and stages persistent in a pipeline with and . These keywords are ways to store dependencies and job output, even when using ephemeral runners for each job.\n• Use the keyword to specify additional configurations that are applied to all jobs. This keyword is often used to define and sections that should run on every job.\n• First time GitLab & CI/CD. This includes a quick introduction to GitLab, the first steps with CI/CD, building a Go project, running tests, using the CI/CD pipeline editor, detecting secrets and security vulnerabilities and offers more exercises for asynchronous practice.\n• Intro to GitLab CI. This workshop uses the Web IDE to quickly get going with building source code using CI/CD, and run unit tests."
    },
    {
        "link": "https://docs.gitlab.com/ci/docker/using_docker_images",
        "document": "You can run your CI/CD jobs in Docker containers hosted on dedicated CI/CD build servers or your local machine.\n\nTo run CI/CD jobs in a Docker container, you need to:\n• Register a runner and configure it to use the Docker executor.\n• Specify the container image where you want to run the CI/CD jobs in the file.\n• Optional. Run other services, like MySQL, in containers. Do this by specifying services in your file.\n\nRegister a runner that uses the Docker executor\n\nTo use GitLab Runner with Docker you need to register a runner that uses the Docker executor.\n\nThis example shows how to set up a temporary template to supply services:\n\nThen use this template to register the runner:\n\nThe registered runner uses the Docker image and runs two services, and , both of which are accessible during the build process.\n\nWhat is an image\n\nThe keyword is the name of the Docker image the Docker executor uses to run CI/CD jobs.\n\nBy default, the executor pulls images from Docker Hub. However, you can configure the registry location in the file. For example, you can set the Docker pull policy to use local images.\n\nFor more information about images and Docker Hub, see the Docker overview.\n\nAny image used to run a CI/CD job must have the following applications installed:\n\nYou can define an image that’s used for all jobs, and a list of services that you want to use during runtime:\n\nThe image name must be in one of the following formats:\n\nYou can use a string or a map for the or entries:\n• Strings must include the full image name (including the registry, if you want to download the image from a registry other than Docker Hub).\n• Maps must contain at least the option, which is the same image name as used for the string setting.\n\nFor example, the following two definitions are equal:\n• None A map for and . The is required:\n\nWhen a CI job runs in a Docker container, the , , and commands run in the directory. Your image may have a different default defined. To move to your , save the as an environment variable so you can reference it in the container during the job’s runtime.\n\nOverride the entrypoint of an image\n\nBefore explaining the available entrypoint override methods, let’s describe how the runner starts. It uses a Docker image for the containers used in the CI/CD jobs:\n• The runner starts a Docker container using the defined entrypoint. The default from that may be overridden in the file.\n• The runner attaches itself to a running container.\n• The runner prepares a script (the combination of , , and ).\n• The runner sends the script to the container’s shell and receives the output.\n\nTo override the entrypoint of a Docker image, in the file:\n• For Docker 17.06 and later, set to an empty value.\n• For Docker 17.03 and earlier, set to , , or an equivalent shell available in the image.\n\nThe syntax of is similar to Dockerfile .\n\nLet’s assume you have a image with a SQL database in it. You want to use it as a base image for your job because you want to execute some tests with this database binary. Let’s also assume that this image is configured with as an entrypoint. When the container starts without additional options, it runs the database’s process. The runner expects that the image has no entrypoint or that the entrypoint is prepared to start a shell command.\n\nWith the extended Docker configuration options, instead of:\n• Creating your own image based on .\n• Using the new image in your CI job.\n\nYou can now define an in the file.\n\nFor Docker 17.06 and later:\n\nIn the file, you can define:\n• In the section, the container image used to run CI/CD jobs\n• In the section, the services container\n\nThe image and services defined this way are added to all jobs run by that runner.\n\nTo access private container registries, the GitLab Runner process can use:\n• Credentials Store. For more information, see the relevant Docker documentation.\n• Credential Helpers. For more information, see the relevant Docker documentation.\n\nWhen you use the GitLab Container Registry on the same GitLab instance, GitLab provides default credentials for this registry. With these credentials, the is used for authentication. To use the job token, the user starting the job must have at least the Developer role for the project where the private image is hosted. The project hosting the private image must also allow the other project to authenticate with the job token. This access is disabled by default. For more details, see CI/CD job token.\n\nTo define which option should be used, the runner process reads the configuration in this order:\n• A file in directory of the user running the process. If the flag is provided to run the child processes as unprivileged user, the home directory of the main runner process user is used.\n• Available for Kubernetes executor in GitLab Runner 13.1 and later.\n• Credentials Store and Credential Helpers require binaries to be added to the GitLab Runner , and require access to do so. Therefore, these features are not available on instance runners, or any other runner where the user does not have access to the environment where the runner is installed.\n\nYou can access a private registry using two approaches. Both require setting the CI/CD variable with appropriate authentication information.\n• Per-job: To configure one job to access a private registry, add as a CI/CD variable.\n• Per-runner: To configure a runner so all its jobs can access a private registry, add as an environment variable in the runner’s configuration.\n\nSee below for examples of each.\n\nAs an example, let’s assume you want to use the image. This image is private and requires you to sign in to a private container registry.\n\nLet’s also assume that these are the sign-in credentials:\n\nUse one of the following methods to determine the value for :\n• None Do a on your local machine: Then copy the content of . If you don’t need access to the registry from your computer, you can do a :\n• None In some setups, it’s possible the Docker client uses the available system key store to store the result of . In that case, it’s impossible to read , so you must prepare the required base64-encoded version of and create the Docker configuration JSON manually. Open a terminal and execute the following command: # The use of printf (as opposed to echo) prevents encoding a newline in the password. If your username includes special characters like , you must escape them with a backslash ( ) to prevent authentication problems. Create the Docker JSON configuration content as follows:\n\nTo configure a single job with access for , follow these steps:\n• None Create a CI/CD variable with the content of the Docker configuration file as the value:\n• None You can now use any private image from defined in or in your file: In the example above, GitLab Runner looks at for the image .\n\nYou can add configuration for as many registries as you want, adding more registries to the hash as described above.\n\nThe full combination is required everywhere for the runner to match the . For example, if is specified in the file, then the must also specify . Specifying only does not work.\n\nIf you have many pipelines that access the same registry, you should set up registry access at the runner level. This allows pipeline authors to have access to a private registry just by running a job on the appropriate runner. It also helps simplify registry changes and credential rotations.\n\nThis means that any job on that runner can access the registry with the same privilege, even across projects. If you need to control access to the registry, you need to be sure to control access to the runner.\n• None Modify the runner’s file as follows:\n• The double quotes included in the data must be escaped with backslashes. This prevents them from being interpreted as TOML.\n• The option is a list. Your runner may have existing entries and you should add this to the list, not replace it.\n• None To use a Credentials Store, you need an external helper program to interact with a specific keychain or external store. Make sure the helper program is available in the GitLab Runner .\n• None Make GitLab Runner use it. You can accomplish this by using one of the following options:\n• None Create a CI/CD variable with the content of the Docker configuration file as the value:\n• None Or, if you’re running self-managed runners, add the above JSON to . GitLab Runner reads this configuration file and uses the needed helper for this specific repository.\n\nis used to access all the registries. If you use both images from a private registry and public images from Docker Hub, pulling from Docker Hub fails. Docker daemon tries to use the same credentials for all the registries.\n\nAs an example, let’s assume that you want to use the image. This image is private and requires you to sign in to a private container registry.\n\nTo configure access for , follow these steps:\n• None Make sure is available in the GitLab Runner .\n• None Have any of the following AWS credentials setup. Make sure that GitLab Runner can access the credentials.\n• None Make GitLab Runner use it. You can accomplish this by using one of the following options:\n• None Create a CI/CD variable with the content of the Docker configuration file as the value: This configures Docker to use the Credential Helper for a specific registry. Instead, you can configure Docker to use the Credential Helper for all Amazon Elastic Container Registry (ECR) registries: If you use , set the region explicitly in the AWS shared configuration file ( ). The region must be specified when the ECR Credential Helper retrieves the authorization token.\n• None Or, if you’re running self-managed runners, add the previous JSON to . GitLab Runner reads this configuration file and uses the needed helper for this specific repository.\n• None You can now use any private image from defined in and/or in your file: In the example, GitLab Runner looks at for the image .\n\nYou can add configuration for as many registries as you want, adding more registries to the hash.\n\nUse checksum to keep your image secure\n\nUse the image checksum in your job definition in your file to verify the integrity of the image. A failed image integrity verification prevents you from using a modified container.\n\nTo use the image checksum you have to append the checksum at the end:\n\nTo get the image checksum, on the image tab, view the column. For example, view the Ruby image. The checksum is a random string, like .\n\nYou can also get the checksum of any image on your system with the command :\n\nYou can create a custom GitLab Runner Docker image to package AWS CLI and Amazon ECR Credential Helper. This setup facilitates secure and streamlined interactions with AWS services, especially for containerized applications. For example, use this setup to manage, deploy, and update Docker images on Amazon ECR. This setup helps avoid time consuming, error-prone configurations, and manual credential management.\n• None Create a with the following content:\n• None To build the custom GitLab Runner Docker image in a , include the following example below:"
    },
    {
        "link": "https://cmakkaya.medium.com/gitlab-ci-cd-1-building-a-java-project-using-maven-and-docker-within-the-gitlab-ci-pipeline-278feaf7ee12",
        "document": "GitLab CI/CD - 1 : Building a Java Project using Maven and Docker within the GitLab CI pipeline.\n\n1. a. The benefits of using\n\n1. b. Basic concepts and key terms used in it\n\n3. Creating .gitlab-ci.yml (Configuration File) and explanation of it"
    },
    {
        "link": "https://forum.gitlab.com/t/create-docker-images-in-gitlab-ci/5810",
        "document": "I want to create an docker image from an Java Maven project.\n\n We use Gitlab CI Docker Runner for CI Builds and in my opinion there are 2 methods to do this:\n• create docker image directly with maven (this requires an DOCKER_HOST)\n• create docker image in CI Process with an own stage and Dockerfile\n\nI tried this 2 options, but now i describe the second option.\n\nSo I added an stage for docker image creation.\n\n Here is my gitlab-ci.yml:\n\nBut it doesnt works, because to create an docker image you have to install an docker host.\n\n How can i install Docker Host and set ENV Variables?\n\nHere is the error:\n\nCan anyone help?"
    },
    {
        "link": "https://stackoverflow.com/questions/33430487/how-to-use-gitlab-ci-to-build-a-java-maven-project",
        "document": "I'll start answering the Java build question, then the Runners one.\n\nI'll start from the most basic Java build configuration, and progressively add features.\n\nThis configuration will hopefully run your Maven build (and only the build, explicitly excluding the unit tests):\n• declares Maven build output as GitLab artifacts (for later use in the downstream pipeline),\n• takes benefit of GitLab's cache to cache local Maven repository (in ),\n• also enforces some recommended Maven options to use in CI/CD context.\n\nThere are two options when integrating you unit tests in your CI/CD pipeline:\n• run them in the same job as the build\n\nAs a matter of pipeline execution speed and green-IT considerations, I definitely prefer option 1, but I admit people could prefer the second one.\n\nHere is the new version of the file, also implementing GitLab unit tests integration:\n\nFrom this step, the build job could still be enhanced further, for instance with code coverage computation and integration, but that requires a little more code.\n\nAnother way of implementing state-of-the-art pipeline with much less efforts is using GitLab CI/CD template. For example:\n\nto be continuous is an open-source project that provides a collection of ready-to-use, configurable, extensible, composable templates.\n\nGitLab architecture is very versatile, with the notion of Runners. Runners are the basic executors pool that make it possible to execute GitLab CI/CD jobs.\n\n2 things of interest to know about runners\n\n1. you can specialise your runners\n\nWith GitLab, you can register several kind of runners, made for special and complementary purpose.\n\nIn order to segregate them, GitLab supports the notion of Tags. When registering your runners, you shall associate them with functional tag names, that will help developers select the most appropriate one in their file.\n\nFor example, let's imagine you have 4 runners:\n\nWith this setup, a monorepo project with both Java and .NET code may declare the following file:"
    },
    {
        "link": "https://docs.gitlab.com/ci/docker/using_docker_build",
        "document": "You can use GitLab CI/CD with Docker to create Docker images. For example, you can create a Docker image of your application, test it, and push it to a container registry.\n\nTo run Docker commands in your CI/CD jobs, you must configure GitLab Runner to support commands. This method requires mode.\n\nIf you want to build Docker images without enabling mode on the runner, you can use a Docker alternative.\n\nTo enable Docker commands for your CI/CD jobs, you can use:\n\nTo include Docker commands in your CI/CD jobs, you can configure your runner to use the executor. In this configuration, the user runs the Docker commands, but needs permission to do so.\n• None Register a runner. Select the executor. For example:\n• None On the server where GitLab Runner is installed, install Docker Engine. View a list of supported platforms.\n• None Add the user to the group:\n• None Verify that has access to Docker:\n• None In GitLab, add to to verify that Docker is working:\n\nYou can now use commands (and install Docker Compose if needed).\n\nWhen you add to the group, you effectively grant full root permissions. For more information, see security of the group.\n• Your registered runner uses the Docker executor or the Kubernetes executor.\n• The executor uses a container image of Docker, provided by Docker, to run your CI/CD jobs.\n\nThe Docker image includes all of the tools and can run the job script in context of the image in privileged mode.\n\nYou should use Docker-in-Docker with TLS enabled, which is supported by GitLab.com instance runners.\n\nYou should always pin a specific version of the image, like . If you use a tag like , you have no control over which version is used. This can cause incompatibility problems when new versions are released.\n\nUse the Docker executor with Docker-in-Docker\n\nYou can use the Docker executor to run jobs in a Docker container.\n\nDocker-in-Docker with TLS enabled in the Docker executor\n\nThe Docker daemon supports connections over TLS. TLS is the default in Docker 19.03.12 and later.\n\nTo use Docker-in-Docker with TLS enabled:\n• None Register GitLab Runner from the command line. Use and mode:\n• This command registers a new runner to use the image (if none is specified at the job level). To start the build and service containers, it uses the mode. If you want to use Docker-in-Docker, you must always use in your Docker containers.\n• This command mounts for the service and build container, which is needed for the Docker client to use the certificates in that directory. For more information, see the Docker image documentation. The previous command creates a entry similar to the following example:\n• None You can now use in the job script. You should include the service: # When you use the dind service, you must instruct Docker to talk with # the daemon started inside of the service. The daemon is available # with a network connection instead of the default # by setting the DOCKER_HOST in # The 'docker' hostname is the alias of the service container as described at # Specify to Docker where to create the certificates. Docker # creates them automatically on boot, and creates # `/certs/client` to share between the service and job # container, thanks to volume mount from config.toml\n\nUse a Unix socket on a shared volume between Docker-in-Docker and build container\n\nDirectories defined in in the Docker-in-Docker with TLS enabled in the Docker executor approach are persistent between builds. If multiple CI/CD jobs using a Docker executor runner have Docker-in-Docker services enabled, then each job writes to the directory path. This approach might result in a conflict.\n\nTo address this conflict, use a Unix socket on a volume shared between the Docker-in-Docker service and the build container. This approach improves performance and establishes a secure connection between the service and client.\n\nThe following is a sample with temporary volume shared between build and service containers:\n\nThe Docker-in-Docker service creates a . The Docker client connects to through a Docker Unix socket volume.\n\nDocker-in-Docker with TLS disabled in the Docker executor\n\nSometimes there are legitimate reasons to disable TLS. For example, you have no control over the GitLab Runner configuration that you are using.\n\nAssuming that the runner’s is similar to:\n\nYou can now use in the job script. You should include the service:\n\nDocker-in-Docker with proxy enabled in the Docker executor\n\nYou might need to configure proxy settings to use the command.\n\nFor more information, see Proxy settings when using dind service.\n\nUse the Kubernetes executor with Docker-in-Docker\n\nYou can use the Kubernetes executor to run jobs in a Docker container.\n\nTo use Docker-in-Docker with TLS enabled in Kubernetes:\n• None Using the Helm chart, update the file to specify a volume mount.\n• None You can now use in the job script. You should include the service: # When using dind service, you must instruct Docker to talk with # the daemon started inside of the service. The daemon is available # with a network connection instead of the default # The 'docker' hostname is the alias of the service container as described at # If you're using GitLab Runner 12.7 or earlier with the Kubernetes executor and Kubernetes 1.6 or earlier, # the variable must be set to tcp://localhost:2376 because of how the # Specify to Docker where to create the certificates. Docker # creates them automatically on boot, and creates # `/certs/client` to share between the service and job # container, thanks to volume mount from config.toml # These are usually specified by the entrypoint, however the\n\nTo use Docker-in-Docker with TLS disabled in Kubernetes, you must adapt the example above to:\n• Remove the section from the file.\n• Change the port from to with .\n• Instruct Docker to start with TLS disabled with .\n• None Using the Helm chart, update the file:\n• None You can now use in the job script. You should include the service: # When using dind service, you must instruct Docker to talk with # the daemon started inside of the service. The daemon is available # with a network connection instead of the default # The 'docker' hostname is the alias of the service container as described at # If you're using GitLab Runner 12.7 or earlier with the Kubernetes executor and Kubernetes 1.6 or earlier, # the variable must be set to tcp://localhost:2376 because of how the # This instructs Docker not to start over TLS.\n\nDocker-in-Docker is the recommended configuration, but you should be aware of the following issues:\n• None The command: This command is not available in this configuration by default. To use in your job scripts, follow the Docker Compose installation instructions.\n• None Cache: Each job runs in a new environment. Because every build gets its own instance of the Docker engine, concurrent jobs do not cause conflicts. However, jobs can be slower because there’s no caching of layers. See Docker layer caching.\n• None Storage drivers: By default, earlier versions of Docker use the storage driver, which copies the file system for each job. Docker 17.09 and later use , which is the recommended storage driver. See Using the OverlayFS driver for details.\n• None Root file system: Because the container and the runner container do not share their root file system, you can use the job’s working directory as a mount point for child containers. For example, if you have files you want to share with a child container, you could create a subdirectory under and use it as your mount point. For a more detailed explanation, see issue #41227.\n\nUse the Docker executor with Docker socket binding\n\nTo use Docker commands in your CI/CD jobs, you can bind-mount into the container. Docker is then available in the context of the image.\n\nIf you bind the Docker socket you can’t use as a service. Volume bindings also affect services, making them incompatible.\n\nTo make Docker available in the context of the image, you need to mount into the launched containers. To do this with the Docker executor, add to the Volumes in the section.\n\nYour configuration should look similar to this example:\n\nTo mount while registering your runner, include the following options:\n\nFor the executor, use a configuration similar to this example:\n\nFor complex Docker-in-Docker setups like Code Quality scanning using CodeClimate, you must match host and container paths for proper execution. For more details, see Use private runners for CodeClimate-based scanning.\n\nWhen the Docker daemon starts inside the service container, it uses the default configuration. You might want to configure a registry mirror for performance improvements and to ensure you do not exceed Docker Hub rate limits.\n\nThe service in the file\n\nYou can append extra CLI flags to the service to set the registry mirror:\n\nThe service in the GitLab Runner configuration file\n\nIf you are a GitLab Runner administrator, you can specify the to configure the registry mirror for the Docker daemon. The service must be defined for the Docker or Kubernetes executor.\n\nThe Docker executor in the GitLab Runner configuration file\n\nIf you are a GitLab Runner administrator, you can use the mirror for every service. Update the configuration to specify a volume mount.\n\nFor example, if you have a file with the following content:\n\nUpdate the file to mount the file to . This mounts the file for every container created by GitLab Runner. The configuration is detected by the service.\n\nThe Kubernetes executor in the GitLab Runner configuration file\n\nIf you are a GitLab Runner administrator, you can use the mirror for every service. Update the configuration to specify a ConfigMap volume mount.\n\nFor example, if you have a file with the following content:\n\nCreate a ConfigMap with the content of this file. You can do this with a command like:\n\nAfter the ConfigMap is created, you can update the file to mount the file to . This update mounts the file for every container created by GitLab Runner. The service detects this configuration.\n\nWhen you use Docker socket binding, you avoid running Docker in privileged mode. However, the implications of this method are:\n• None By sharing the Docker daemon, you effectively disable all the container’s security mechanisms and expose your host to privilege escalation. This can cause container breakout. For example, if a project ran , it would remove the GitLab Runner containers.\n• None Concurrent jobs might not work. If your tests create containers with specific names, they might conflict with each other.\n• None Any containers created by Docker commands are siblings of the runner, rather than children of the runner. This might cause complications for your workflow.\n• None Sharing files and directories from the source repository into containers might not work as expected. Volume mounting is done in the context of the host machine, not the build container. For example:\n\nYou do not need to include the service, like you do when you use the Docker-in-Docker executor:\n\nWhen you use Docker-in-Docker, the standard authentication methods do not work, because a fresh Docker daemon is started with the service. You should authenticate with registry.\n\nWhen using Docker-in-Docker, Docker downloads all layers of your image every time you create a build. You can make your builds faster with Docker layer caching.\n\nBy default, when using , Docker uses the storage driver, which copies the file system on every run. You can avoid this disk-intensive operation by using a different driver, for example .\n• None Check whether the module is loaded: If you see no result, then the module is not loaded. To load the module, use: If the module loaded, you must make sure the module loads on reboot. On Ubuntu systems, do this by adding the following line to :\n\nUse the OverlayFS driver per project\n\nYou can enable the driver for each project individually by using the CI/CD variable in :\n\nUse the OverlayFS driver for every project\n\nIf you use your own runners, you can enable the driver for every project by setting the environment variable in the section of the file:\n\nIf you’re running multiple runners, you must modify all configuration files.\n\nRead more about the runner configuration and using the OverlayFS storage driver.\n\nTo build Docker images without enabling privileged mode on the runner, you can use one of these alternatives:\n\nTo use Buildah with GitLab CI/CD, you need a runner with one of the following executors:\n\nIn this example, you use Buildah to:\n\nIn the last step, Buildah uses the under the root directory of the project to build the Docker image. Finally, it pushes the image to the project’s container registry:\n\nIf you are using GitLab Runner Operator deployed to an OpenShift cluster, try the tutorial for using Buildah to build images in rootless container.\n\nAfter you’ve built a Docker image, you can push it to the GitLab container registry.\n\nError: docker: Cannot connect to the Docker daemon at tcp://docker:2375\n\nThis error is common when you are using Docker-in-Docker v19.03 or later:\n\nThis error occurs because Docker starts on TLS automatically.\n• If this is your first time setting it up, see use the Docker executor with the Docker image.\n• If you are upgrading from v18.09 or earlier, see the upgrade guide.\n\nThis error can also occur with the Kubernetes executor when attempts are made to access the Docker-in-Docker service before it has fully started up. For a more detailed explanation, see issue 27215.\n\nYou might get an error that says docker: error during connect: Post https://docker:2376/v1.40/containers/create: dial tcp: lookup docker on x.x.x.x:53: no such host .\n\nThis issue can occur when the service’s image name includes a registry hostname. For example:\n\nA service’s hostname is derived from the full image name. However, the shorter service hostname is expected. To allow service resolution and access, add an explicit alias for the service name :\n\nError: Cannot connect to the Docker daemon at unix:///var/run/docker.sock\n\nYou might get the following error when trying to run a command to access a service:\n\nMake sure your job has defined these environment variables:\n\nYou may also want to update the image that provides the Docker client. For example, the images are obsolete and should be replaced with .\n\nAs described in runner issue 30944, this error can happen if your job previously relied on environment variables derived from the deprecated Docker parameter, such as . Your job fails with this error if:\n• Your CI/CD image relies on a legacy variable, such as .\n• The runner feature flag is set to .\n\nThis error appears when you use the deprecated variable, :\n\nTo prevent users from receiving this error, you should:\n\nThis error appears when the service has failed to start:\n\nCheck the job log to see if appears. For example:\n\nThis indicates the GitLab Runner does not have permission to start the service:\n• Check that is set in the .\n• Make sure the CI job has the right Runner tags to use these privileged runners.\n\nThere is a known incompatibility introduced by Docker Engine 20.10.\n\nWhen the host uses Docker Engine 20.10 or newer, then the service in a version older than 20.10 does not work as expected.\n\nWhile the service itself will start without problems, trying to build the container image results in the error:\n\nTo resolve this issue, update the container to version at least 20.10.x, for example .\n\nThe opposite configuration ( service and Docker Engine on the host in version 19.06.x or older) works without problems. For the best strategy, you should to frequently test and update job environment versions to the newest. This brings new features, improved security and - for this specific case - makes the upgrade on the underlying Docker Engine on the runner’s host transparent for the job.\n\nThis error can appear when Docker commands like or are executed in a Docker-in-Docker environment where custom or private certificates are used (for example, Zscaler certificates):\n\nThis error occurs because Docker commands in a Docker-in-Docker environment use two separate containers:\n• The build container runs the Docker client ( ) and executes your job’s script commands.\n• The service container (often named ) runs the Docker daemon that processes most Docker commands.\n\nWhen your organization uses custom certificates, both containers need these certificates. Without proper certificate configuration in both containers, Docker operations that connect to external registries or services will fail with certificate errors.\n• None Store your root certificate as a CI/CD variable named . The certificate should be in this format:\n• None Configure your pipeline to install the certificate in the service container before starting the Docker daemon. For example:"
    },
    {
        "link": "https://medium.com/@tuananhbk1996/7-tips-to-optimize-your-dockerfile-for-faster-builds-and-smaller-images-e33ea8d94d4e",
        "document": "Source: 7 Tips to Optimize Your Dockerfile for Faster Builds and Smaller Images\n\nChoosing the right base image is the first and perhaps the most crucial step in Dockerfile optimization. A smaller base image reduces the size of the final image and minimizes potential attack surfaces.\n\nBase images are the foundation of your Docker image. They determine the initial size and capabilities of your container. Common choices include ubuntu, debian, alpine, and language-specific images like python or node.\n\nOne of the most popular lightweight base images is Alpine Linux, which is significantly smaller than other images. For example, an Ubuntu image might be around 70MB, whereas an Alpine image can be as small as 5MB.\n\nUsing Alpine Linux as your base image can drastically reduce the size of your Docker images, leading to faster download times and more efficient storage usage.\n\nWhile Alpine is great for many applications, it may not be suitable for all. If your application requires specific libraries or tools that are not easily available on Alpine, you might need to consider other base images.\n\nMulti-stage builds are a powerful feature in Docker that allows you to use multiple FROM statements in a single Dockerfile, helping you to create smaller, more efficient images.\n\nIn a multi-stage build, you create an intermediate container to build your application, and then copy only the necessary artifacts into the final container. This ensures that your final image contains only the essential components.\n\nMulti-stage builds significantly reduce the size of the final Docker image by excluding unnecessary files and dependencies, leading to faster deployment times and reduced security risks.\n\n2.3 When to Use Multi-Stage Builds\n\nMulti-stage builds are particularly useful for compiled languages like Go, Java, or C++, where the build environment is much larger than the runtime environment.\n\nDocker caches the layers of your images, which can drastically speed up the build process. However, improper use of caching can lead to inefficient builds.\n\nEach line in your Dockerfile creates a new image layer. Docker caches these layers to avoid rebuilding them if they haven’t changed.\n\nTo optimize layer caching, put the commands that are least likely to change at the top of your Dockerfile. This way, these layers can be reused across builds.\n\nProperly optimizing your Dockerfile with caching in mind can lead to significantly faster build times, especially when iterating frequently during development.\n\nIn some cases, layer caching alone might not be sufficient. For example, if your application has large dependencies that change frequently, you might need to explore other optimization strategies.\n\nEvery command in your Dockerfile creates a new layer. Minimizing the number of layers helps to reduce the final image size and speeds up the build process.\n\nYou can combine multiple commands into a single RUN statement to reduce the number of layers.\n\nFewer layers result in smaller images, quicker builds, and more efficient storage and deployment.\n\nWhile combining commands can reduce layers, it can also make debugging more challenging if an error occurs. Be sure to strike a balance between optimization and maintainability.\n\nJust as you use .gitignore to exclude files from version control, you can use .dockerignore to prevent unnecessary files from being copied into your Docker image.\n\nThe .dockerignore file works similarly to .gitignore, excluding files and directories that are not needed in the Docker image.\n\nBy excluding unnecessary files, you can reduce the size of your Docker context, leading to faster builds and smaller images.\n\nTypically, you should ignore files like logs, temporary files, and dependency directories (e.g., node_modules for Node.js projects).\n\n6. Keep Your Dockerfile Simple and Clean\n\nA simple and clean Dockerfile is easier to maintain, debug, and optimize. Avoid complex and unnecessary commands that can bloat your image or slow down the build process.\n\nFollow best practices such as using explicit versions for base images, commenting your Dockerfile, and avoiding unnecessary layers and commands.\n\nA clean Dockerfile is easier to read, understand, and maintain. It also reduces the risk of errors and simplifies the optimization process.\n\nAvoid using latest tags for base images, as they can lead to unpredictable builds. Also, refrain from running commands that produce large amounts of unnecessary data.\n\nDocker and the tools you use are constantly evolving. Regularly reviewing and updating your Dockerfile ensures it remains optimized and compatible with the latest best practices.\n\nOutdated Dockerfiles can lead to security vulnerabilities, larger image sizes, and slower builds. Regular updates help you take advantage of new features and improvements.\n\nPeriodically review your Dockerfile to remove any deprecated commands, update base images, and refactor for better efficiency.\n\nConsider using tools like Hadolint to analyze your Dockerfile for potential issues and optimizations.\n\nUpdate your Dockerfile whenever you change your application’s dependencies, switch base images, or adopt new best practices.\n\nOptimizing your Dockerfile is an ongoing process that can significantly impact your application’s performance, security, and reliability. By following these seven tips, you’ll be well on your way to creating Docker images that are both efficient and effective.\n\nIf you have any questions or need further clarification, feel free to leave a comment below!\n\nRead more at : 7 Tips to Optimize Your Dockerfile for Faster Builds and Smaller Images"
    },
    {
        "link": "https://dev.to/rajeshgheware/dockerfile-best-practices-the-ultimate-guide-to-optimizing-your-container-builds-2d0p",
        "document": "Dockerfile Best Practices: The Ultimate Guide to Optimizing Your Container Builds\n• Understand the fundamental role of Dockerfiles in container development.\n• Implement best practices for writing efficient and secure Dockerfiles.\n• Explore advanced optimization techniques like multi-stage builds and using Alpine Linux.\n• Avoid common pitfalls to improve your Docker workflow and application performance.\n\nA Dockerfile is essentially a blueprint for building Docker images—a text file containing a series of instructions that define how your application environment should be constructed. These instructions create layers that, when combined, form your final Docker image.\n\nStart with official, trusted base images to ensure security and reliability. These images:\n\nMulti-stage builds separate your build environment from your runtime environment, significantly reducing final image size.\n\nPlace infrequently changing commands at the top of your Dockerfile to maximize cache usage:\n\nRemove unnecessary files in the same layer they’re created:\n\n\n\nInstead, specify exact versions for reproducibility.\n\nOnly install what your application needs.\n\nAlways scan images for vulnerabilities and use non-root users when possible.\n\nLints Dockerfiles for best practices and catches common mistakes.\n\nImplementing these Dockerfile best practices will help you create more efficient, secure, and maintainable container images. Remember to:\n\nBy following these best practices and continuously learning about new optimization techniques, you’ll be well-equipped to create high-quality Docker images that serve your applications effectively.\n\nQ: Why should I use multi-stage builds?\n\nA: Multi-stage builds help reduce the size of your final image by separating the build environment from the runtime environment. This leads to leaner images and improved security.\n\nQ: What are the benefits of using Alpine Linux base images?\n\nA: Alpine Linux base images are significantly smaller than standard images, reducing your image size by up to 90%. They provide a minimal environment that lowers the attack surface and speeds up deployment times.\n\nQ: How can I ensure my Docker images are secure?\n\nA: Use official base images, implement security best practices like running as a non-root user, regularly update your images, and scan for vulnerabilities using tools like Clair or Trivy.\n\nAbout the Author:Rajesh Gheware, with over two decades of industry experience and a strong background in cloud computing and Kubernetes, is an expert in guiding startups and enterprises through their digital transformation journeys. As a mentor and community contributor, Rajesh is committed to sharing knowledge and insights on cutting-edge technologies."
    },
    {
        "link": "https://medium.com/@reach2shristi.81/real-life-proven-strategies-to-reduce-docker-image-size-0415b756f886",
        "document": "Reducing the size of Docker images is crucial for optimizing resource usage, improving deployment times, and enhancing security.\n\nHere are some proven strategies to reduce Docker image size:\n\n- Alpine Linux is a lightweight and security-focused distribution. Choosing Alpine-based images as your base image can significantly reduce the image size compared to larger distributions.\n• Use multi-stage builds to separate the build environment from the runtime environment. This helps discard unnecessary build dependencies and only include the final artifacts.\n\n- Combine multiple commands into a single RUN instruction to minimize the number of layers. This reduces the size of the image and improves caching efficiency.\n• Remove unnecessary files and cleanup package managers to reduce image size. This is important for package managers like apt, yum, or apk.\n\n- Create a `.dockerignore` file to exclude unnecessary files and directories from being copied into the Docker image. This reduces the amount of data transferred to the image.\n\n- Be mindful of the order of instructions in your Dockerfile. Place instructions that are less likely to change towards the top to leverage Docker’s layer caching mechanism.\n• Select the smallest base image that meets your application’s requirements. For example, if you need Python, choose a slim version or Alpine-based image.\n• Use build arguments to parameterize your Dockerfile. This can allow you to conditionally include/exclude certain dependencies during the build process.\n\nRemember that the effectiveness of these strategies may vary depending on the specific requirements and nature of your application. It’s a good practice to experiment and profile your images to find the best combination of optimizations for your use case."
    },
    {
        "link": "https://blacksmith.sh/blog/how-to-optimize-dockerfile-faster-docker-builds",
        "document": "Docker has truly changed how we package, distribute, and deploy applications. At its core is the Docker’s build process, where a Docker image is created from a Dockerfile. In this post, we’ll explore several strategies to optimize Docker builds, making them faster and the resulting images more space efficient.\n\nBefore diving into optimization strategies, let's understand the most essential instructions that make up a Dockerfile:\n• FROM: Specifies the base image to use as the starting point for the build. This is typically the first instruction in a Dockerfile.\n• RUN: Executes a command inside the image during the build process. Each RUN instruction creates a new layer in the image.\n• COPY: Copies files or directories from the host machine to the image. It's the preferred instruction for copying files.\n• CMD: Specifies the default command to run when a container is started from the image. There can only be one CMD instruction in a Dockerfile.\n\nThese four instructions form the core of most Dockerfiles. They allow you to select a base image, execute commands to install dependencies and configure the image, copy necessary files, and define the default command to run when a container is started.\n\nDocker uses a build cache to speed up the build process. Each instruction in your Dockerfile is treated as a separate layer. If the layer hasn't changed, docker caches these layers and reuses them in subsequent builds. This means that if you modify an instruction in your Dockerfile, Docker will use the cache for all the layers before the modified instruction and rebuild the layers after it. This happens because each layer depends on the layers before it. Modifying an instruction changes the state of the image, and subsequent layers need to be rebuilt to ensure consistency and include the changes.\n\nHow to Structure Your Dockerfile\n\nThe order of instructions in your Dockerfile matters. You should structure your Dockerfile in a way that maximizes cache utilization. Here are a few ways to improve that:\n\n1. Order from least frequently changing to most\n\n‍Place instructions less likely to change at the top of your Dockerfile. For example, if your application dependencies don't change frequently, install them early in the Dockerfile. This way, Docker can reuse the cached layers for these instructions in subsequent builds.\n\nLet’s consider the following example\n• We copy the and files first before copying the entire project directory. This allows Docker to cache the instruction separately from the application code.\n• If the and files haven't changed, Docker can reuse the cached layer for , even if the application code has changed.\n• The instruction is placed after , so changes to the application code won't invalidate the cache for the dependency installation step.\n\n‍Group related commands together in a single RUN instruction. Each RUN instruction creates a new layer, so combining related commands reduces the number of layers and improves build efficiency. More layers lead to a higher image size and would require a higher number of cache hits.\n\n\n\nConsider this example\n\n\n\n‍Multi-stage builds allow you to create smaller and more efficient Docker images by separating the build and runtime environments. This is particularly useful when you have build dependencies that are not needed at runtime.\n\n\n\nBefore multi-stage builds, you might have a Dockerfile like this\n• The stage uses the full Node.js image to install dependencies and build the application.\n• The stage uses a minimal Node.js image ( ) and copies only the built files and production dependencies from the stage.\n\nThe resulting image is significantly smaller because it doesn't include the build tools and intermediate artifacts.\n\nBuildKit is the default Docker backend and it enables parallelizing builds for a multi-stage build.\n\n\n\nConsider a Dockerfile that builds a Go application with multiple components\n• The stage sets up the common dependencies by copying and files and downloading the required packages.\n• The and stages can be executed in parallel as they depend only on the stage. Each stage builds a specific component of the application.\n• The final stage uses a minimal base image ( ) and copies the compiled binaries from the and stages.\n\nWith BuildKit, Docker will automatically detect the independent stages and execute them in parallel, speeding up the overall build process.\n\nSmaller Docker images are faster to build, push, and pull. They also consume less disk space and memory. Here are some strategies to minimize your image size:\n• Use the right base image: Choose a minimal base image that includes only the necessary dependencies for your application. For example, if you're running a Go application, consider using the official image instead of the full-fledged image.\n\nAlpine Linux images have the additional benefit of generally containing fewer security vulnerabilities due to their reduced attack surface. It only includes essential packages by default, reducing the number of pre-installed components that might not be needed for your specific application. With fewer packages and a smaller codebase, there's less surface area for potential vulnerabilities.\n• Combine RUN commands: As we mentioned before, each RUN instruction in your Dockerfile creates a new layer. More layers result in more overhead and a larger image size. Combine multiple RUN instructions into a single instruction to reduce the number of layers.\n• Clean up after package installations: When installing packages in your Dockerfile, it's useful to clean up the package cache and temporary files in the same instruction. This practice helps reduce the size of your final Docker image by removing unnecessary files that are not needed at runtime.\n\n\n\nConsider the following example\n\n In this Dockerfile, we update the package lists using and install the package using . However, after the installation, we remove the package lists stored in using . This cleanup step removes the cached package lists, which are not required once the packages are installed.\n\n\n\nRemoving the package cache and temporary files prevents them from being included in the final image layer. This can significantly reduce the image size, especially when installing multiple packages or dealing with large package repositories.\n\n\n\nThe multi-stage builds solution we mentioned before circumvents this problem entirely by copying only what’s required at run time instead of build time.\n\nUse the Right File-Copying Strategy\n\nWhen copying files into your Docker image, use the appropriate instructions\n• COPY vs. ADD: Use for most cases, as it’s straightforward and only copies files from the host to the image. can perform extra tasks such as extracting a local tar file directly into the container’s filesystem or reading from remote URLs. This can result in unwanted artifacts ending up in your Docker image.\n• Use .dockerignore: Create a .dockerignore file in your build context to exclude unnecessary files and directories from being copied into the image. This reduces the build context size, speeds up the build process, and keeps your images lean. Some common files to ignore are:\n• Version Control Directories: Exclude directories such as , , or that contain version control metadata.\n• Build Artifacts: Exclude files and directories generated by build processes, such as , , , or directories.\n• Temporary Files: Exclude temporary files and directories like , , or .\n• Configuration Files: Exclude files that are not needed in the production environment, such as files containing development configurations or secrets.\n• Documentation: Exclude documentation files like , unless they are needed in the image.\n\nTo summarize, the order of your Dockerfile can drastically improve your cache efficiency. Your Docker image only needs to contain runtime dependencies, not build dependencies. Having a smaller image size can also help you pull/push faster and contribute to improving developer productivity."
    },
    {
        "link": "https://stackoverflow.com/questions/68682375/optimizing-dockerfile-image-size-what-more-how-can-i-reduce-size-of-this-imag",
        "document": "Its my first question so hello world\n\nSo i'am beginner, unfortunately in both GNU/Linux and dockerizing things. I got an image that reason to exist is having all in one image for bitbucket-pipelines and azure-pipelines. (Multi-project image).\n\nDuring forced update (added groovy and changed nodejs source due to problems with ssl) Image size went up form 1GB to 1.5GB.\n\nWith my tweaks i managed to free 150MB to current 1.35GB\n• merged many layers to as few as i could done"
    }
]