[
    {
        "link": "https://learn.microsoft.com/en-us/azure/devops/pipelines/process/variables?view=azure-devops",
        "document": "Variables give you a convenient way to get key bits of data into various parts of the pipeline. The most common use of variables is to define a value that you can then use in your pipeline. All variables are strings and are mutable. The value of a variable can change from run to run or job to job of your pipeline.\n\nWhen you define the same variable in multiple places with the same name, the most locally scoped variable takes precedence. So, a variable defined at the job level can override a variable set at the stage level. A variable defined at the stage level overrides a variable set at the pipeline root level. A variable set in the pipeline root level overrides a variable set in the Pipeline settings UI. To learn more how to work with variables defined at the job, stage, and root level, see Variable scope.\n\nYou can use variables with expressions to conditionally assign values and further customize pipelines.\n\nWhen you define a variable, you can use different syntaxes (macro, template expression, or runtime) and what syntax you use determines where in the pipeline your variable renders.\n\nIn YAML pipelines, you can set variables at the root, stage, and job level. You can also specify variables outside of a YAML pipeline in the UI. When you set a variable in the UI, that variable can be encrypted and set as secret.\n\nUser-defined variables can be set as read-only. There are naming restrictions for variables (example: you can't use at the start of a variable name).\n\nYou can use a variable group to make variables available across multiple pipelines.\n\nUse templates to define variables in one file that are used in multiple pipelines.\n\nAzure DevOps supports multi-line variables but there are a few limitations.\n\nDownstream components such as pipeline tasks might not handle the variable values correctly.\n\nAzure DevOps won't alter user-defined variable values. Variable values need to be formatted correctly before being passed as multi-line variables. When formatting your variable, avoid special characters, don't use restricted names, and make sure you use a line ending format that works for the operating system of your agent.\n\nMulti-line variables behave differently depending on the operating system. To avoid this, make sure that you format multi-line variables correctly for the target operating system.\n\nAzure DevOps never alters variable values, even if you provide unsupported formatting.\n\nIn addition to user-defined variables, Azure Pipelines has system variables with predefined values. For example, the predefined variable Build.BuildId gives the ID of each build and can be used to identify different pipeline runs. You can use the variable in scripts or tasks when you need to a unique value.\n\nIf you're using YAML or classic build pipelines, see predefined variables for a comprehensive list of system variables.\n\nIf you're using classic release pipelines, see release variables.\n\nSystem variables get set with their current value when you run the pipeline. Some variables are set automatically. As a pipeline author or end user, you change the value of a system variable before the pipeline runs.\n\nEnvironment variables are specific to the operating system you're using. They're injected into a pipeline in platform-specific ways. The format corresponds to how environment variables get formatted for your specific scripting platform.\n\nOn UNIX systems (macOS and Linux), environment variables have the format . On Windows, the format is for batch and in PowerShell.\n\nSystem and user-defined variables also get injected as environment variables for your platform. When variables convert into environment variables, variable names become uppercase, and periods turn into underscores. For example, the variable name becomes the variable name .\n\nThere are variable naming restrictions for environment variables (example: you can't use at the start of a variable name).\n\nUser-defined and environment variables can consist of letters, numbers, , and characters. Don't use variable prefixes reserved by the system. These are: , , , , and . Any variable that begins with one of these strings (regardless of capitalization) won't be available to your tasks and scripts.\n\nAzure Pipelines supports three different ways to reference variables: macro, template expression, and runtime expression. You can use each syntax for a different purpose and each have some limitations.\n\nIn a pipeline, template expression variables ( ) get processed at compile time, before runtime starts. Macro syntax variables ( ) get processed during runtime before a task runs. Runtime expressions ( ) also get processed during runtime but are intended to be used with conditions and expressions. When you use a runtime expression, it must take up the entire right side of a definition.\n\nIn this example, you can see that the template expression still has the initial value of the variable after the variable is updated. The value of the macro syntax variable updates. The template expression value doesn't change because all template expression variables get processed at compile time before tasks run. In contrast, macro syntax variables evaluate before each task runs.\n\nMost documentation examples use macro syntax ( ). Macro syntax is designed to interpolate variable values into task inputs and into other variables.\n\nVariables with macro syntax get processed before a task executes during runtime. Runtime happens after template expansion. When the system encounters a macro expression, it replaces the expression with the contents of the variable. If there's no variable by that name, then the macro expression doesn't change. For example, if can't be replaced, won't be replaced by anything.\n\nMacro syntax variables remain unchanged with no value because an empty value like might mean something to the task you're running and the agent shouldn't assume you want that value replaced. For example, if you use to reference variable in a Bash task, replacing all expressions in the input to the task could break your Bash scripts.\n\nMacro variables are only expanded when they're used for a value, not as a keyword. Values appear on the right side of a pipeline definition. The following is valid: . The following isn't valid: . Macro variables aren't expanded when used to display a job name inline. Instead, you must use the property.\n\nThis example uses macro syntax with Bash, PowerShell, and a script task. The syntax for calling a variable with macro syntax is the same for all three.\n\nYou can use template expression syntax to expand both template parameters and variables ( ). Template variables process at compile time, and get replaced before runtime starts. Template expressions are designed for reusing parts of YAML as templates.\n\nTemplate variables silently coalesce to empty strings when a replacement value isn't found. Template expressions, unlike macro and runtime expressions, can appear as either keys (left side) or values (right side). The following is valid: .\n\nYou can use runtime expression syntax for variables that are expanded at runtime ( ). Runtime expression variables silently coalesce to empty strings when a replacement value isn't found. Use runtime expressions in job conditions, to support conditional execution of jobs, or whole stages.\n\nRuntime expression variables are only expanded when they're used for a value, not as a keyword. Values appear on the right side of a pipeline definition. The following is valid: . The following isn't valid: . The runtime expression must take up the entire right side of a key-value pair. For example, is valid but isn't.\n\nWhat syntax should I use?\n\nUse macro syntax if you're providing a secure string or a predefined variable input for a task.\n\nChoose a runtime expression if you're working with conditions and expressions. However, don't use a runtime expression if you don't want your empty variable to print (example: ). For example, if you have conditional logic that relies on a variable having a specific value or no value. In that case, you should use a macro expression.\n\nTypically a template variable is the standard to use. By leveraging template variables, your pipeline will fully inject the variable value into your pipeline at pipeline compilation. This is helpful when attempting to debug pipelines. You can download the log files and evaluate the fully expanded value that is being substituted in. Since the variable is substituted in, you shouldn't leverage template syntax for sensitive values.\n\nIn the most common case, you set the variables and use them within the YAML file. This allows you to track changes to the variable in your version control system. You can also define variables in the pipeline settings UI (see the Classic tab) and reference them in your YAML. Here's an example that shows how to set two variables, and , and use them later in steps. To use a variable in a YAML statement, wrap it in . Variables can't be used to define a in a YAML statement. # Set variables once variables: configuration: debug platform: x64 steps: # Use them once - task: MSBuild@1 inputs: solution: solution1.sln configuration: $(configuration) # Use the variable platform: $(platform) # Use them again - task: MSBuild@1 inputs: solution: solution2.sln configuration: $(configuration) # Use the variable platform: $(platform) In the YAML file, you can set a variable at various scopes:\n• At the root level, to make it available to all jobs in the pipeline.\n• At the stage level, to make it available only to a specific stage.\n• At the job level, to make it available only to a specific job. When you define a variable at the top of a YAML, the variable is available to all jobs and stages in the pipeline and is a global variable. Global variables defined in a YAML aren't visible in the pipeline settings UI. Variables at the job level override variables at the root and stage level. Variables at the stage level override variables at the root level. variables: global_variable: value # this is available to all jobs jobs: - job: job1 pool: vmImage: 'ubuntu-latest' variables: job_variable1: value1 # this is only available in job1 steps: - bash: echo $(global_variable) - bash: echo $(job_variable1) - bash: echo $JOB_VARIABLE1 # variables are available in the script environment too - job: job2 pool: vmImage: 'ubuntu-latest' variables: job_variable2: value2 # this is only available in job2 steps: - bash: echo $(global_variable) - bash: echo $(job_variable2) - bash: echo $GLOBAL_VARIABLE The output from both jobs looks like this: # job1 value value1 value1 # job2 value value2 value In the preceding examples, the keyword is followed by a list of key-value pairs. The keys are the variable names and the values are the variable values. There's another syntax, useful when you want to use templates for variables or variable groups. With templates, variables can be defined in one YAML and included in another YAML file. Variable groups are a set of variables that you can use across multiple pipelines. They allow you to manage and organize variables that are common to various stages in one place. Use this syntax for variable templates and variable groups at the root level of a pipeline. In this alternate syntax, the keyword takes a list of variable specifiers. The variable specifiers are for a regular variable, for a variable group, and to include a variable template. The following example demonstrates all three. variables: # a regular variable - name: myvariable value: myvalue # a variable group - group: myvariablegroup # a reference to a variable template - template: myvariabletemplate.yml Learn more about variable reuse with templates. Notice that variables are also made available to scripts through environment variables. The syntax for using these environment variables depends on the scripting language. The name is upper-cased, and the is replaced with the . This is automatically inserted into the process environment. Here are some examples: Predefined variables that contain file paths are translated to the appropriate styling (Windows style C:\\foo\\ versus Unix style /foo/) based on agent host type and shell type. If you are running bash script tasks on Windows, you should use the environment variable method for accessing these variables rather than the pipeline variable method to ensure you have the correct file path styling. You can set a variable for a build pipeline by following these steps:\n• Go to the Pipelines page, select the appropriate pipeline, and then select Edit.\n• Locate the Variables for this pipeline.\n• To mark the variable as secret, select Keep this value secret. After setting the variable, you can use it as an input to a task or within the scripts in your pipeline. To use a variable as an input to a task, wrap it in . Notice that variables are also made available to scripts through environment variables. The syntax for using these environment variables depends on the scripting language. The name is upper-cased, and the is replaced with the . This is automatically inserted into the process environment. Here are some examples: Predefined variables that contain file paths are translated to the appropriate styling (Windows style C:\\foo\\ versus Unix style /foo/) based on agent host type and shell type. If you are running bash script tasks on Windows, you should use the environment variable method for accessing these variables rather than the pipeline variable method to ensure you have the correct file path styling. Using the Azure DevOps CLI, you can create and update variables for the pipeline runs in your project. You can also delete the variables if you no longer need them.\n• You've installed the Azure DevOps CLI extension as described in Get started with Azure DevOps CLI.\n• For the examples in this article, set the default organization using . You can create variables in your pipeline with the az pipelines variable create command. To get started, see Get started with Azure DevOps CLI.\n• name: Required. Name of the variable.\n• allow-override: Optional. Indicates whether the value can be set at queue time. Accepted values are false and true.\n• org: Azure DevOps organization URL. You can configure the default organization using . Required if not configured as default or picked up using . Example: .\n• pipeline-id: Required if pipeline-name isn't supplied. ID of the pipeline.\n• pipeline-name: Required if pipeline-id isn't supplied, but ignored if pipeline-id is supplied. Name of the pipeline.\n• project: Name or ID of the project. You can configure the default project using . Required if not configured as default or picked up using .\n• secret: Optional. Indicates whether the variable's value is a secret. Accepted values are false and true.\n• value: Required for non-secret variable. Value of the variable. For secret variables, if value parameter isn't provided, it's picked from environment variable prefixed with or user is prompted to enter it via standard input. For example, a variable named MySecret can be input using the environment variable . The following command creates a variable in MyFirstProject named Configuration with the value platform in the pipeline with ID 12. It shows the result in table format. az pipelines variable create --name Configuration --pipeline-id 12 --value platform --project MyFirstProject --output table Name Allow Override Is Secret Value ---------- ---------------- ----------- -------- Configuration False False platform You can update variables in your pipeline with the az pipelines variable update command. To get started, see Get started with Azure DevOps CLI.\n• name: Required. Original name of the variable.\n• allow-override: Optional. Indicates whether the value can be set at queue time. Accepted values are false and true.\n• new-name: Optional. Specify to change the name of the variable.\n• org: Azure DevOps organization URL. You can configure the default organization using . Required if not configured as default or picked up using . Example: .\n• pipeline-id: Required if pipeline-name isn't supplied. ID of the pipeline.\n• pipeline-name: Required if pipeline-id isn't supplied, but ignored if pipeline-id is supplied. Name of the pipeline.\n• project: Name or ID of the project. You can configure the default project using . Required if not configured as default or picked up using .\n• prompt-value: Set to true to update the value of a secret variable using environment variable or prompt via standard input. Accepted values are false and true.\n• secret: Indicates whether the variable's value is a secret. Accepted values are false and true.\n• value: Updates the value of the variable. For secret variables, use the prompt-value parameter to be prompted to enter it via standard input. For non-interactive consoles, it can be picked from environment variable prefixed with . For example, a variable named MySecret can be input using the environment variable . The following command updates the Configuration variable with the new value config.debug in the pipeline with ID 12. It specifies that the variable isn't a secret and shows the result in table format. az pipelines variable update --name Configuration --pipeline-id 12 --secret false --value config.debug --output table Name Allow Override Is Secret Value ------------- ---------------- ----------- ------------ Configuration False False config.debug You can delete variables in your pipeline with the az pipelines variable delete command. To get started, see Get started with Azure DevOps CLI.\n• name: Required. Name of the variable you want to delete.\n• org: Azure DevOps organization URL. You can configure the default organization using . Required if not configured as default or picked up using . Example: .\n• pipeline-id: Required if pipeline-name isn't supplied. ID of the pipeline.\n• pipeline-name: Required if pipeline-id isn't supplied, but ignored if pipeline-id is supplied. Name of the pipeline.\n• project: Name or ID of the project. You can configure the default project using . Required if not configured as default or picked up using . The following command deletes the Configuration variable from the pipeline with ID 12 and doesn't prompt for confirmation.\n\nDon't set secret variables in your YAML file. Operating systems often log commands for the processes that they run, and you wouldn't want the log to include a secret that you passed in as an input. Use the script's environment or map the variable within the block to pass secrets to your pipeline. Azure Pipelines makes an effort to mask secrets when emitting data to pipeline logs, so you may see additional variables and data masked in output and logs that are not set as secrets. You need to set secret variables in the pipeline settings UI for your pipeline. These variables are scoped to the pipeline where they're set. You can also set secret variables in variable groups. To set secrets in the web interface, follow these steps:\n• Go to the Pipelines page, select the appropriate pipeline, and then select Edit.\n• Locate the Variables for this pipeline.\n• Select the option to Keep this value secret to store the variable in an encrypted manner. Secret variables are encrypted at rest with a 2048-bit RSA key. Secrets are available on the agent for tasks and scripts to use. Be careful about who has access to alter your pipeline. We make an effort to mask secrets from appearing in Azure Pipelines output, but you still need to take precautions. Never echo secrets as output. Some operating systems log command line arguments. Never pass secrets on the command line. Instead, we suggest that you map your secrets into environment variables. We never mask substrings of secrets. If, for example, \"abc123\" is set as a secret, \"abc\" isn't masked from the logs. This is to avoid masking secrets at too granular of a level, making the logs unreadable. For this reason, secrets should not contain structured data. If, for example, \"{ \"foo\": \"bar\" }\" is set as a secret, \"bar\" isn't masked from the logs. Unlike a normal variable, they are not automatically decrypted into environment variables for scripts. You need to explicitly map secret variables. The following example shows how to map and use a secret variable called in PowerShell and Bash scripts. Two global variables are defined. is assigned the value of a secret variable , and is assigned the value of a non-secret variable . Unlike a normal pipeline variable, there's no environment variable called . The PowerShell task runs a script to print the variables.\n• : This is a direct reference to the secret variable and works.\n• : This attempts to access the secret variable as an environment variable, which does not work because secret variables are not automatically mapped to environment variables.\n• : This attempts to access the secret variable through a global variable, which also does not work because secret variables cannot be mapped this way.\n• : This accesses the non-secret variable through a global variable, which works.\n• : This accesses the secret variable through a task-specific environment variable, which is the recommended way to map secret variables to environment variables. variables: GLOBAL_MYSECRET: $(mySecret) # this will not work because the secret variable needs to be mapped as env GLOBAL_MY_MAPPED_ENV_VAR: $(nonSecretVariable) # this works because it's not a secret. steps: - powershell: | Write-Host \"Using an input-macro works: $(mySecret)\" Write-Host \"Using the env var directly does not work: $env:MYSECRET\" Write-Host \"Using a global secret var mapped in the pipeline does not work either: $env:GLOBAL_MYSECRET\" Write-Host \"Using a global non-secret var mapped in the pipeline works: $env:GLOBAL_MY_MAPPED_ENV_VAR\" Write-Host \"Using the mapped env var for this task works and is recommended: $env:MY_MAPPED_ENV_VAR\" env: MY_MAPPED_ENV_VAR: $(mySecret) # the recommended way to map to an env variable - bash: | echo \"Using an input-macro works: $(mySecret)\" echo \"Using the env var directly does not work: $MYSECRET\" echo \"Using a global secret var mapped in the pipeline does not work either: $GLOBAL_MYSECRET\" echo \"Using a global non-secret var mapped in the pipeline works: $GLOBAL_MY_MAPPED_ENV_VAR\" echo \"Using the mapped env var for this task works and is recommended: $MY_MAPPED_ENV_VAR\" env: MY_MAPPED_ENV_VAR: $(mySecret) # the recommended way to map to an env variable The output from both tasks in the preceding script would look like this: Using an input-macro works: *** Using the env var directly does not work: Using a global secret var mapped in the pipeline does not work either: Using a global non-secret var mapped in the pipeline works: foo Using the mapped env var for this task works and is recommended: *** You can also use secret variables outside of scripts. For example, you can map secret variables to tasks using the definition. This example shows how to use secret variables and in an Azure file copy task. This example shows how to reference a variable group in your YAML file, and also how to add variables within the YAML. There are two variables used from the variable group: and . The variable is secret, and is mapped to the environment variable so that it can be referenced in the YAML. This YAML makes a REST call to retrieve a list of releases, and outputs the result. variables: - group: 'my-var-group' # variable group - name: 'devopsAccount' # new variable defined in YAML value: 'contoso' - name: 'projectName' # new variable defined in YAML value: 'contosoads' steps: - task: PowerShell@2 inputs: targetType: 'inline' script: | # Encode the Personal Access Token (PAT) # $env:USER is a normal variable in the variable group # $env:MY_MAPPED_TOKEN is a mapped secret variable $base64AuthInfo = [Convert]::ToBase64String([Text.Encoding]::ASCII.GetBytes((\"{0}:{1}\" -f $env:USER,$env:MY_MAPPED_TOKEN))) # Get a list of releases $uri = \"https://vsrm.dev.azure.com/$(devopsAccount)/$(projectName)/_apis/release/releases?api-version=5.1\" # Invoke the REST call $result = Invoke-RestMethod -Uri $uri -Method Get -ContentType \"application/json\" -Headers @{Authorization=(\"Basic {0}\" -f $base64AuthInfo)} # Output releases in JSON Write-Host $result.value env: MY_MAPPED_TOKEN: $(token) # Maps the secret variable $(token) from my-var-group By default with GitHub repositories, secret variables associated with your pipeline aren't made available to pull request builds of forks. For more information, see Contributions from forks. To set secrets in the web interface, follow these steps:\n• Go to the Pipelines page, select the appropriate pipeline, and then select Edit.\n• Locate the Variables for this pipeline.\n• Select the option to Keep this value secret to store the variable in an encrypted manner. Secret variables are encrypted at rest with a 2048-bit RSA key. Secrets are available on the agent for tasks and scripts to use. Be careful about who has access to alter your pipeline. We make an effort to mask secrets from appearing in Azure Pipelines output, but you still need to take precautions. Never echo secrets as output. Some operating systems log command line arguments. Never pass secrets on the command line. Instead, we suggest that you map your secrets into environment variables. We never mask substrings of secrets. If, for example, \"abc123\" is set as a secret, \"abc\" isn't masked from the logs. This is to avoid masking secrets at too granular of a level, making the logs unreadable. For this reason, secrets should not contain structured data. If, for example, \"{ \"foo\": \"bar\" }\" is set as a secret, \"bar\" isn't masked from the logs. Unlike a normal variable, they are not automatically decrypted into environment variables for scripts. You need to explicitly map secret variables. Each task that needs to use the secret as an environment variable does remapping. If you want to use a secret variable called from a script, use the section of the scripting task's input variables. Set the environment variable name to , and set the value to . By default with GitHub repositories, secret variables associated with your pipeline aren't made available to pull request builds of forks. For more information, see Contributions from forks. To set secret variables using the Azure DevOps CLI, see Create a variable or Update a variable.\n\nTo share variables across multiple pipelines in your project, use the web interface. Under Library, use variable groups.\n• To reference a variable from a different task within the same job, use .\n• To reference a variable from a task from a different job, use .\n\nScripts can define variables that are later consumed in subsequent steps in the pipeline. All variables set by this method are treated as strings. To set a variable from a script, you use a command syntax and print to stdout.\n\nTo set a variable from a script, you use the logging command. This updates the environment variables for subsequent jobs. Subsequent jobs have access to the new variable with macro syntax and in tasks as environment variables. When is true, the value of the variable will be saved as secret and masked from the log. For more information on secret variables, see logging commands. steps: # Create a variable - bash: | echo \"##vso[task.setvariable variable=sauce]crushed tomatoes\" # remember to use double quotes # Use the variable # \"$(sauce)\" is replaced by the contents of the `sauce` variable by Azure Pipelines # before handing the body of the script to the shell. - bash: | echo my pipeline variable is $(sauce) Subsequent steps will also have the pipeline variable added to their environment. You can't use the variable in the step that it's defined. steps: # Create a variable # Note that this does not update the environment of the current script. - bash: | echo \"##vso[task.setvariable variable=sauce]crushed tomatoes\" # An environment variable called `SAUCE` has been added to all downstream steps - bash: | echo \"my environment variable is $SAUCE\" - pwsh: | Write-Host \"my environment variable is $env:SAUCE\" The output from the preceding pipeline. my environment variable is crushed tomatoes my environment variable is crushed tomatoes If you want to make a variable available to future jobs, you must mark it as an output variable by using . Then you can map it into future jobs by using the syntax and including the step name that set the variable. Multi-job output variables only work for jobs in the same stage. To pass variables to jobs in different stages, use the stage dependencies syntax. By default, each stage in a pipeline depends on the one just before it in the YAML file. Therefore, each stage can use output variables from the prior stage. To access further stages, you will need to alter the dependency graph, for instance, if stage 3 requires a variable from stage 1, you will need to declare an explicit dependency on stage 1. When you create a multi-job output variable, you should assign the expression to a variable. In this YAML, is assigned to the variable . jobs: # Set an output variable from job A - job: A pool: vmImage: 'windows-latest' steps: - powershell: echo \"##vso[task.setvariable variable=myOutputVar;isOutput=true]this is the value\" name: setvarStep - script: echo $(setvarStep.myOutputVar) name: echovar # Map the variable into job B - job: B dependsOn: A pool: vmImage: 'ubuntu-latest' variables: myVarFromJobA: $[ dependencies.A.outputs['setvarStep.myOutputVar'] ] # map in the variable # remember, expressions require single quotes steps: - script: echo $(myVarFromJobA) name: echovar The output from the preceding pipeline. this is the value this is the value If you're setting a variable from one stage to another, use . If you're setting a variable from a matrix or slice, then to reference the variable when you access it from a downstream job, you must include:\n• The name of the job. jobs: # Set an output variable from a job with a matrix - job: A pool: vmImage: 'ubuntu-latest' strategy: maxParallel: 2 matrix: debugJob: configuration: debug platform: x64 releaseJob: configuration: release platform: x64 steps: - bash: echo \"##vso[task.setvariable variable=myOutputVar;isOutput=true]this is the $(configuration) value\" name: setvarStep - bash: echo $(setvarStep.myOutputVar) name: echovar # Map the variable from the debug job - job: B dependsOn: A pool: vmImage: 'ubuntu-latest' variables: myVarFromJobADebug: $[ dependencies.A.outputs['debugJob.setvarStep.myOutputVar'] ] steps: - script: echo $(myVarFromJobADebug) name: echovar jobs: # Set an output variable from a job with slicing - job: A pool: vmImage: 'ubuntu-latest' parallel: 2 # Two slices steps: - bash: echo \"##vso[task.setvariable variable=myOutputVar;isOutput=true]this is the slice $(system.jobPositionInPhase) value\" name: setvarStep - script: echo $(setvarStep.myOutputVar) name: echovar # Map the variable from the job for the first slice - job: B dependsOn: A pool: vmImage: 'ubuntu-latest' variables: myVarFromJobsA1: $[ dependencies.A.outputs['job1.setvarStep.myOutputVar'] ] steps: - script: \"echo $(myVarFromJobsA1)\" name: echovar Be sure to prefix the job name to the output variables of a deployment job. In this case, the job name is : jobs: # Set an output variable from a deployment - deployment: A pool: vmImage: 'ubuntu-latest' environment: staging strategy: runOnce: deploy: steps: - bash: echo \"##vso[task.setvariable variable=myOutputVar;isOutput=true]this is the deployment variable value\" name: setvarStep - bash: echo $(setvarStep.myOutputVar) name: echovar # Map the variable from the job for the first slice - job: B dependsOn: A pool: vmImage: 'ubuntu-latest' variables: myVarFromDeploymentJob: $[ dependencies.A.outputs['A.setvarStep.myOutputVar'] ] steps: - bash: \"echo $(myVarFromDeploymentJob)\" name: echovar To set a variable from a script, use the logging command. This doesn't update the environment variables, but it does make the new variable available to downstream steps within the same job. You can run a script on a:\n• Windows agent using either a Batch script task or PowerShell script task. @echo off set sauceArgument=%~1 set secretSauceArgument=%~2 @echo No problem reading %sauceArgument% or %SAUCE% @echo But I cannot read %SECRET_SAUCE% @echo But I can read %secretSauceArgument% (but the log is redacted so I do not spoil the secret) Param( [string]$sauceArgument, [string]$secretSauceArgument ) Write-Host No problem reading $env:SAUCE or $sauceArgument Write-Host But I cannot read $env:SECRET_SAUCE Write-Host But I can read $secretSauceArgument \"(but the log is redacted so I do not spoil the secret)\" #!/bin/bash echo \"No problem reading $1 or $SAUCE\" echo \"But I cannot read $SECRET_SAUCE\" echo \"But I can read $2 (but the log is redacted so I do not spoil the secret)\" No problem reading crushed tomatoes or crushed tomatoes But I cannot read But I can read ******** (but the log is redacted so I do not spoil the secret) In order to use a variable as a task input, you must make the variable an output variable, and you must give the producing task a reference name. You can set a task's reference name on the Output Variables section of the task editor. For instance, a script task whose output variable reference name is might have the following contents: The output variable can be referenced in the input of a downstream task as . You can't pass a variable from one job to another job of a build pipeline, unless you use YAML. There's no az pipelines command that applies to setting variables in scripts."
    },
    {
        "link": "https://stackoverflow.com/questions/75185600/variable-from-expression-in-yaml-pipeline-with-dynamic-variable-group",
        "document": "I am trying to assign value of the variable by coalescing (via expression) from parameter and variable that is in variable group.\n\nProblem is, that the variable group is dynamic, depending on another parameter (like DEV, QA,...)\n\nPipeline can't see the value of the variable ResourceGroupName which comes from variable group MyAPIVarGroup-*. It comes empty.\n\nIs this even achievable and I'm just doing something wrong? Or is this not possible with the current implementation of expressions and variable groups in YAML pipelines?"
    },
    {
        "link": "https://stackoverflow.com/questions/76973596/dynamic-variable-assignment-in-azure-devops-yaml-based-on-conditions",
        "document": "I'm facing an issue with dynamically assigning variables in Azure DevOps YAML based on conditions related to the branch name. My goal is to assign specific variables that can later be utilized within tasks. However, I'm encountering problems when attempting to set the AZURE_SUBSCRIPTION variable using conditions in my YAML file.\n\nHere's the relevant part of my YAML:\n\nThe trick with ${{ if eq(variables['Build.SourceBranchName'], 'dev') }} gets the job done, but it's a no-go for me because it can't handle branches with '/'.\n\nMy issue lies in the fact that the AZURE_SUBSCRIPTION variable doesn't seem to be assigned as expected. I suspect there might be an error in my condition logic or how I'm referencing the variable. Could someone please review my YAML and provide insights into why the AZURE_SUBSCRIPTION variable isn't being assigned correctly?"
    },
    {
        "link": "https://learn.microsoft.com/en-us/azure/devops/pipelines/process/templates?view=azure-devops",
        "document": "Use YAML templates in pipelines for reusable and secure processes\n\nTemplates let you define reusable content, logic, and parameters in YAML pipelines. To work with templates effectively, you need to have a basic understanding of Azure Pipelines key concepts such as stages, steps, and jobs. Templates can help you speed up development. For example, you can have a series of the same tasks in a template and then include the template multiple times in different stages of your YAML pipeline. Templates can also help you secure your pipeline. When a template controls what is allowed in a pipeline, the template defines logic that another file must follow. For example, you might want to restrict what tasks are allowed to run. For that scenario, you can use template to prevent someone from successfully running a task that violates your organization's security policies. There are two types of templates: includes and extends.\n• Includes templates let you insert reusable content with a template. If a template is used to include content, it functions like an include directive in many programming languages. Content from template is inserted into the pipeline or template that includes it.\n• Extends templates let you control what is allowed in a pipeline. When an extends template controls what is allowed in a pipeline, the template defines logic that a pipeline must follow. For example, an extends template can be used in the context of extending a pipeline to perform stages or jobs. To take full advantage of templates, you should also use template expressions and template parameters. Templates and template expressions can cause explosive growth to the size and complexity of a pipeline. To help prevent runaway growth, Azure Pipelines imposes the following limits:\n• No more than 100 separate YAML files may be included (directly or indirectly)\n• No more than 100 levels of template nesting (templates including other templates)\n• No more than 20 megabytes of memory consumed while parsing the YAML (in practice, this is typically between 600 KB - 2 MB of on-disk YAML, depending on the specific features used)\n\nYou can copy content from one YAML and reuse it in a different YAML. Copying content from one YAML to another saves you from having to manually include the same logic in multiple places. The file template contains steps that are reused in . Template files need to exist on your filesystem at the start of a pipeline run. You can't reference templates in an artifact. You can insert a template to reuse one or more steps across several jobs. In addition to the steps from the template, each job can define more steps. # File: azure-pipelines.yml jobs: - job: Linux pool: vmImage: 'ubuntu-latest' steps: - template: templates/npm-steps.yml # Template reference - job: macOS pool: vmImage: 'macOS-latest' steps: - template: templates/npm-steps.yml # Template reference - job: Windows pool: vmImage: 'windows-latest' steps: - script: echo This script runs before the template's steps, only on Windows. - template: templates/npm-steps.yml # Template reference - script: echo This step runs after the template's steps. Much like steps, jobs can be reused with templates. When working with multiple jobs, remember to remove the name of the job in the template file, so as to avoid conflict Stages can also be reused with templates. In the following templates:\n• defines two parameters: and and creates a job with the name parameter for the job name and the vmImage parameter for the VM image.\n• The pipeline ( ) references the template three times, each with different parameter values referring to the operating system and VM image names.\n• The built pipeline runs on a different VM image and named according to the specified OS. Each job performs npm install and npm test steps. # File: templates/npm-with-params.yml parameters: - name: name # defaults for any parameters that aren't specified default: '' - name: vmImage default: '' jobs: - job: ${{ parameters.name }} pool: vmImage: ${{ parameters.vmImage }} steps: - script: npm install - script: npm test When you consume the template in your pipeline, specify values for the template parameters. # File: azure-pipelines.yml jobs: - template: templates/npm-with-params.yml # Template reference parameters: name: Linux vmImage: 'ubuntu-latest' - template: templates/npm-with-params.yml # Template reference parameters: name: macOS vmImage: 'macOS-latest' - template: templates/npm-with-params.yml # Template reference parameters: name: Windows vmImage: 'windows-latest' In the following templates:\n• The template defines four parameters: , , , and , all of type string. The template creates a stage using the parameter to set the stage name, defines a job with , and includes a step to run a script.\n• The pipeline, , then dynamically define stages and jobs using parameters and runs a job that executes a script, . # stage-template.yml parameters: - name: stageName type: string - name: jobName type: string - name: vmImage type: string - name: scriptPath type: string stages: - stage: ${{ parameters.stageName }} jobs: - job: ${{ parameters.jobName }} pool: vmImage: ${{ parameters.vmImage }} steps: - script: ./${{ parameters.scriptPath }} # azure-pipelines.yml trigger: - main stages: - template: stage-template.yml parameters: stageName: 'BuildStage' jobName: 'BuildJob' scriptPath: 'build-script.sh' # replace with script in your repository vmImage: 'ubuntu-latest' You can also use parameters with step or stage templates. In the following templates:\n• The template ( ) defines a parameter named with a default value of false.\n• The pipeline ( ) runs and because the parameter is true. # File: templates/steps-with-params.yml parameters: - name: 'runExtendedTests' # defaults for any parameters that aren't specified type: boolean default: false steps: - script: npm test - ${{ if eq(parameters.runExtendedTests, true) }}: - script: npm test --extended When you consume the template in your pipeline, specify values for the template parameters. Scalar parameters without a specified type are treated as strings. For example, will return , even if the parameter is the word , if is not explicitly made . Non-empty strings are cast to in a Boolean context. That expression could be rewritten to explicitly compare strings: . Parameters aren't limited to scalar strings. See the list of data types. For example, using the type: # azure-pipelines.yml jobs: - template: process.yml parameters: pool: # this parameter is called `pool` vmImage: ubuntu-latest # and it's a mapping rather than a string # process.yml parameters: - name: 'pool' type: object default: {} jobs: - job: build pool: ${{ parameters.pool }} Variables can be defined in one YAML and included in another template. This could be useful if you want to store all of your variables in one file. If you're using a template to include variables in a pipeline, the included template can only be used to define variables. You can use steps and more complex logic when you're extending from a template. Use parameters instead of variables when you want to restrict type. In this example, the variable is included in . You can pass parameters to variables with templates. In this example, you're passing the parameter to a variable. # File: templates/package-release-with-params.yml parameters: - name: DIRECTORY type: string default: \".\" # defaults for any parameters that specified with \".\" (current directory) variables: - name: RELEASE_COMMAND value: grep version ${{ parameters.DIRECTORY }}/package.json | awk -F \\\" '{print $4}' When you consume the template in your pipeline, specify values for the template parameters.\n\nExtend from a template and use an include template with variables\n\nOne common scenario is to have a pipeline with stages for development, testing, and production that uses both an includes template for variables and an extends template for stages and jobs.\n\nIn the following example, defines a set of virtual machine variables that are then used in .\n\nThe following file, defines a reusable stage configuration with three parameters ( , , ) and a job named .\n\nThe following pipeline, , imports variables from , and then uses the template for each stage. Each stage (Dev, Test, Prod) is defined with the same template but with different parameters, leading to consistency across stages while allowing for customization. The 'Prod' stage includes an environment variable as an example of something you might use for authentication. To learn more about defining parameters, see Template parameters.\n\nTemplate paths can be an absolute path within the repository or relative to the file that does the including.\n\nTo use an absolute path, the template path must start with a . All other paths are considered relative.\n\nThen, in you can reference and like this.\n\nIf is your starting point, you can include and like this.\n\nWhen is your starting point, you can include and like this.\n\nAlternatively, could refer to and using absolute paths like this.\n\nYou can keep your templates in other repositories. For example, suppose you have a core pipeline that you want all of your app pipelines to use. You can put the template in a core repo and then refer to it from each of your app repos:\n\nNow you can reuse this template in multiple pipelines. Use the specification to provide the location of the core repo. When you refer to the core repo, use and the name you gave it in .\n\nFor , is as in the preceding example. For (Azure Repos), is . If that project is in a separate Azure DevOps organization, you need to configure a service connection of type with access to the project and include that in YAML:\n\nRepositories are resolved only once, when the pipeline starts up. After that, the same resource is used during the pipeline run. Only the template files are used. Once the templates are fully expanded, the final pipeline runs as if it were defined entirely in the source repo. This means that you can't use scripts from the template repo in your pipeline.\n\nIf you want to use a particular, fixed version of the template, be sure to pin to a . The are either branches ( ) or tags ( ). If you want to pin a specific commit, first create a tag pointing to that commit, then pin to that tag.\n\nYou can also pin to a specific commit in Git with the SHA value for a repository resource. The SHA value is a 40-character checksum hash that uniquely identifies the commit.\n\nYou can also use to refer to the repository where the original pipeline was found. This is convenient for use in templates if you want to refer back to contents in the extending pipeline's repository. For example:\n\nHow can I use variables inside of templates?\n\nThere are times when it's useful to set parameters to values based on variables. Parameters are expanded early in processing a pipeline run so not all variables are available. To see what predefined variables are available in templates, see Use predefined variables.\n\nIn this example, the predefined variables and are used in conditions in template.yml."
    },
    {
        "link": "https://medium.com/@ivansla/dynamic-selection-of-variables-in-azure-pipelines-with-audit-trail-189e629d254d",
        "document": "\n• Variables in pipeline are not a solution\n• Parameters with variable groups is not a solution. But getting there…\n• Dynamic selection of variables at runtime, with audit trail\n\nLet's say you have three environments, DEV, TEST and PROD. Each environment has different configuration, like hostname, port, level of debugging or some other custom specific variable. You want to create a CI/CD pipeline that will be able to properly build, test and deploy your application to each environment.\n\nVariables in pipeline are not solution\n\nIf you specify variables in azure-pipelines.yaml file, they act like constants you can’t modify them.\n\nYou can add variables to your pipeline via UI, just before you run your pipeline.\n\nThen refer to them with $(var_name) expression in your pipelines yaml file.\n\nToo much of clicking and not mention you don’t have datatype constraints nor enum. Might I remind you, that you need to configure each value every time you want to deploy to a different environment?\n\nDatatype checks and enums are possible with parameters. This gives us at least some type safety and knowledge of possible values. The problem remains with configuring parameter every time you want to deploy to a different environment.\n\nIn addition, if you have many parameters, your pipelines yaml file will become too long. And if there is anything that we developers hate, it’s seeing a long file.\n\nIt took 19 lines just to define three simple parameters.\n\nVariable groups, help us to reduce clutter in pipeline yaml file, because all you need to do is refer to their name.\n\nHowever, you still need to change a value in each variable to accommodate the environment you want to deploy to.\n\nParameters with variable groups is not a solution. But getting there…\n\nA combination of parameters and variable groups definitely helps us remove the problem of modifying values each time we want to deploy to a different environment.\n\nFirst, we need to specify three different variable groups for each environment. Where the values will already be set to accommodate each environment.\n\nThen we define a dynamic selection of variable group in our pipeline yaml, based on environment parameter.\n\nThis allows us to modify only one parameter at the time of running the pipeline…\n\n…while using values from a chosen variable group at the runtime.\n\nThis would be a perfect solution for our CI/CD pipelines, if not for two problems. One, there is no audit trail on whom, when and how modified values in our variable groups. Two, there is no automatic selection of correct variable groups for automatically triggered pipelines.\n\nDynamic selection of variables at runtime, with audit trail\n\nIn order to get audit of our variables, we need to switch from variable groups to variable templates. Templates let you define reusable content, logic, and parameters in YAML pipelines. Microsoft has a very good documentation on this topic.\n\nIn the folder where you have your azure-pipelines.yaml file, create the following folder structure. These folders will contain variable template for each environment.\n\nInside each folder, create variables.yaml file with contents required by that specific environment.\n\nStoring variables this way gives us an audit trail of their modifications through GIT versioning system.\n\nIf you are using git branching strategy, in your development, you probably have these three branches that are connected to the following environments.\n\nAs developers, we are a lazy bunch, and we really dislike unnecessary clicking, therefore, we let pipeline do the triggering of build/deploy. However, in order for pipeline to work correctly, it requires variables specific to each environment. Adding automatic selection of variables to azure-pipelines.yaml file is rather simple:\n\nNow if you create and push a branch, which name begins with release/, variables are extracted from a fore mentioned variable template file and used at runtime.\n\nThis was a quick overview of different ways how to use variables and parameters in your CI/CD pipelines. I know that pipelines can remain for over 90 days, and that you can see with which variables/parameters it was run, but why rely on retention period when you have GIT which contains the whole history?\n\nThank you for reading."
    },
    {
        "link": "https://learn.microsoft.com/en-us/azure/devops/pipelines/packages/nuget-restore?view=azure-devops",
        "document": "With NuGet Package Restore you can install all your project's dependency without needing to store them in source control. This allows for a cleaner development environment and a smaller repository size. You can restore your NuGet packages using the NuGet restore task, the NuGet CLI, or the .NET Core CLI. This article will guide you through restoring your NuGet packages using both Classic and YAML Pipelines.\n• None Create an Azure DevOps organization and a project if you haven't already.\n• None Create a new feed if you don't have one already.\n• None If you're using a self-hosted agent, make sure that it has the .NET Core SDK (2.1.400+) and NuGet (4.8.0.5385+) installed.\n\nIf you're using Ubuntu 24.04 or higher, you must use the task with the .NET CLI instead of the nuget.exe. See Support for newer Ubuntu hosted images for more details.\n\nRestore NuGet packages from a feed in the same organization\n• None Sign in to your Azure DevOps organization, and then navigate to your project.\n• None Select Pipelines, and then select your pipeline definition.\n• None Select Edit, and then add the following snippet to your YAML pipeline.\n• None Sign in to your Azure DevOps organization, and then navigate to your project.\n• None Select Pipelines, select your pipeline definition, and then select Edit.\n• None Select + to add a new task. Add the NuGet tool installer, NuGet Authenticate, and Command line tasks to your pipeline. Leave the NuGet tool installer and NuGet Authenticate tasks with their default settings and configure the Command line task as follows:\n\nMake sure that The NuGet Gallery upstream is enabled in your feed. See Enable upstream sources in an existing feed for details.\n\nRestore NuGet packages from a feed in another organization\n\nTo restore NuGet packages from a feed in a different Azure DevOps organization, you must first create a personal access token then use it to set up a NuGet service connection.\n• None Navigate to your Azure DevOps organization, and then select User settings > Personal Access Tokens.\n• None Create a new personal access token with Packaging* > Read scope. Copy your PAT as you'll need it in the following section.\n• None Sign in to the Azure DevOps organization where your pipeline will run, and then navigate to your project.\n• None Select New service connection, select NuGet, and then select Next.\n• None Select External Azure DevOps Server as the Authentication method, and then enter your target Feed URL. Paste the Personal Access Token you created earlier, provide a name for your service connection, and check Grant access permission to all pipelines if applicable to your scenario."
    },
    {
        "link": "https://learn.microsoft.com/en-us/azure/devops/pipelines/ecosystems/dotnet-core?view=azure-devops",
        "document": "Use an Azure Pipeline to automatically build, test, and deploy your .NET Core projects. This article shows you how to do the following tasks:\n\nAre you new to Azure Pipelines? If so, then we recommend you try the following section first.\n\nIf you don't have a .NET project to work with, create a new one on your local system. Start by installing the latest .NET 8.0 SDK .\n• None Create a project directory and navigate to it.\n• None From the same terminal session, run the application locally using the command from your project directory.\n• None Once the application has started, press Ctrl-C to shut it down.\n\nCreate a git repo and connect it to GitHub\n• None From the project directory, create a local git repository and commit the application code to the main branch.\n\nYou can use the YAML pipeline editor or the classic editor to create your pipeline. To use the classic editor, select Use the classic editor.\n\nYou now have a working pipeline that's ready for you to customize! Read further to learn some of the common ways to customize your pipeline.\n\nNuGet is a popular way to depend on code that you don't build. You can download NuGet packages and project-specific tools that are specified in the project file by running the command either through the .NET Core task or directly in a script in your pipeline. For more information, see .NET Core task (DotNetCoreCLI@2).\n\nYou can download NuGet packages from Azure Artifacts, NuGet.org, or some other external or internal NuGet repository. The .NET Core task is especially useful to restore packages from authenticated NuGet feeds. If your feed is in the same project as your pipeline, you don't need to authenticate.\n\nThis pipeline uses an Azure Artifact feed for in the DotNetCoreCLI@2 task.\n\nThe command uses the packaged with the .NET Core SDK and can only restore packages specified in the .NET Core project files.\n\nIf you also have a Microsoft .NET Framework project in your solution or use to specify your dependencies, use the NuGet task to restore those dependencies.\n\nIn .NET Core SDK version 2.0 and newer, packages are restored automatically when running commands such as . However, you would still need to use the .NET Core task to restore packages if you use an authenticated feed.\n\nYour builds can fail because of connection issues when you restore packages from NuGet.org. You can use Azure Artifacts with upstream sources to cache the packages. The credentials of the pipeline are automatically used when it connects to Azure Artifacts. These credentials are typically derived from the Project Collection Build Service account. To learn more about using Azure Artifacts to cache your NuGet packages, see Connect to Azure Artifact feeds.\n\nTo specify a NuGet repository, put the URL in a file in your repository. If your feed is authenticated, manage its credentials by creating a NuGet service connection in the Services tab under Project Settings.\n\nFor more information about NuGet service connections, see publish to NuGet feeds.\n\nDo the following to restore packages from an external feed.\n\nBuild your .NET Core projects by running the command. You can add the command to your pipeline as a command line script or by using the .NET Core task.\n\nYAML example to build using the DotNetCoreCLI@2 task:\n\nYAML example to build using as a script:\n\nYou can add .NET SDK commands to your project as a script or using the .NET Core task. The .NET Core task (DotNetCoreCLI@2) task allows you to easily add dotnet CLI commands to your pipeline. You can add .NET Core tasks by editing your YAML file or using the classic editor.\n\nYou can add .NET Core CLI commands as a in your file.\n\nTo install a .NET Core global tool like dotnetsay in your build running on Windows, take the following steps:\n• Add the .NET Core task and set the following properties:\n• To run the tool, add a Command Line and set the following properties:\n\nWhen you have test projects in your repository, you can use the .NET Core task to run unit tests by using testing frameworks like MSTest, xUnit, and NUnit. The test project must reference Microsoft.NET.Test.SDK version 15.8.0 or higher. Test results are automatically published to the service. These results are available to you in the build summary and can be used for troubleshooting failed tests and test-timing analysis.\n\nYou can add a test task to your pipeline using the DotNetCoreCLI@2 task or add the following snippet to your file:\n\nWhen using the .NET Core task editor, set Command to test and Path to projects should refer to the test projects in your solution.\n\nAlternatively, you can run the command with a specific logger and then use the Publish Test Results task:\n\nWhen you're building on the Windows platform, code coverage metrics can be collected by using the built-in coverage data collector. The test project must reference Microsoft.NET.Test.SDK version 15.8.0 or higher.\n\nWhen you use the .NET Core task to run tests, coverage data is automatically published to the server. The file can be downloaded from the build summary for viewing in Visual Studio.\n\nAdd the following snippet to your file:\n\nTo add the .NET Core task through the task editor:\n• None Add the .NET Core task to your build job and set the following properties:\n• Path to projects: Should refer to the test projects in your solution.\n\nIf you choose to run the command, specify the test results logger and coverage options. Then use the Publish Test Results task:\n\nIf you're building on Linux or macOS, you can use Coverlet or a similar tool to collect code coverage metrics.\n\nYou can publish code coverage results to the server with the Publish Code Coverage Results (PublishCodeCoverageResults@2 task. The coverage tool must be configured to generate results in Cobertura or JaCoCo coverage format.\n\nTo run tests and publish code coverage with Coverlet, do the following tasks:\n• None Add the following snippet to your file: - task: UseDotNet@2 inputs: version: '8.x' includePreviewVersions: true # Required for preview versions - task: DotNetCoreCLI@2 displayName: 'dotnet build' inputs: command: 'build' configuration: $(buildConfiguration) - task: DotNetCoreCLI@2 displayName: 'dotnet test' inputs: command: 'test' arguments: '--configuration $(buildConfiguration) --collect:\"XPlat Code Coverage\" -- DataCollectionRunSettings.DataCollectors.DataCollector.Configuration.Format=cobertura' publishTestResults: true projects: 'MyTestLibrary' # update with your test project directory - task: PublishCodeCoverageResults@2 displayName: 'Publish code coverage report' inputs: codeCoverageTool: 'Cobertura' summaryFileLocation: '$(Agent.TempDirectory)/**/coverage.cobertura.xml'\n\nYou can publish your build artifacts by:\n• Creating a NuGet package and publish to your NuGet feed.\n\nTo create a NuGet package and publish it to your NuGet feed, add the following snippet:\n\nFor more information about versioning and publishing NuGet packages, see publish to NuGet feeds.\n\nYou can publish your NuGet packages to your Azure Artifacts feed by using the NuGetCommand@2 to push to your Azure Artifact feed. For example, see Publish NuGet packages with Azure Pipelines.\n\nTo create a .zip file archive that's ready to publish to a web app, add the following snippet:\n\nTo publish this archive to a web app, see Azure Web Apps deployment.\n\nBuild an image and push to container registry\n\nYou can also build an image for your app and push it to a container registry.\n\nYou can use the PublishSymbols@2 task to publish symbols to an Azure Artifacts symbol server or a file share.\n\nFor example, to publish symbols to a file share, add the following snippet to your file:\n\nWhen using the classic editor, select Index sources publish symbols from the task catalog to add to your pipeline.\n\nFor more information, see Publish symbols.\n\nIf you can build your project on your development machine, but you're having trouble building it on Azure Pipelines, explore the following potential causes and corrective actions:\n• None Check the .NET Core SDK versions and runtime on your development machine and make sure they match the agent. You can include a command-line script in your pipeline to print the version of the .NET Core SDK. Either use the .NET Core Tool Installer to deploy the same version on the agent, or update your projects and development machine to the newer version of the .NET Core SDK.\n• None You might be using some logic in the Visual Studio IDE that isn't encoded in your pipeline. Azure Pipelines runs each of the commands you specify in the tasks one after the other in a new process. Examine the logs from the pipelines build to see the exact commands that ran as part of the build. Repeat the same commands in the same order on your development machine to locate the problem.\n• None If you have a mixed solution that includes some .NET Core projects and some .NET Framework projects, you should also use the NuGet task to restore packages specified in files. Add the MSBuild or Visual Studio Build task to build the .NET Framework projects.\n• None Your builds might fail intermittently while restoring packages: either NuGet.org is having issues or there are networking problems between the Azure data center and NuGet.org. You can explore whether using Azure Artifacts with NuGet.org as an upstream source improves the reliability of your builds, as it's not in our control.\n• None Occasionally, a when new version of the .NET Core SDK or Visual Studio is rolled out, your build might break. For example, a newer version or feature of the NuGet tool is shipped with the SDK could break your build. To isolate this issue, use the .NET Core Tool Installer task to specify the version of the .NET Core SDK used in your build.\n\nQ: Where can I learn more about Azure Artifacts?\n\nQ: Where can I learn more about .NET Core commands?\n\nQ: Where can I learn more about running tests in my solution?\n\nQ: Where can I learn more about tasks?"
    },
    {
        "link": "https://learn.microsoft.com/en-us/dotnet/core/tools/dotnet-publish",
        "document": "This article applies to: ✔️ .NET Core 3.1 SDK and later versions\n\n- Publishes the application and its dependencies to a folder for deployment to a hosting system.\n\ncompiles the application, reads through its dependencies specified in the project file, and publishes the resulting set of files to a directory. The output includes the following assets:\n• Intermediate Language (IL) code in an assembly with a dll extension.\n• A .deps.json file that includes all of the dependencies of the project.\n• A .runtimeconfig.json file that specifies the shared runtime that the application expects, as well as other configuration options for the runtime (for example, garbage collection type).\n• The application's dependencies, which are copied from the NuGet cache into the output folder.\n\nThe command's output is ready for deployment to a hosting system (for example, a server, PC, Mac, laptop) for execution. It's the only officially supported way to prepare the application for deployment. Depending on the type of deployment that the project specifies, the hosting system may or may not have the .NET shared runtime installed on it. For more information, see Publish .NET apps with the .NET CLI.\n\nYou don't have to run because it's run implicitly by all commands that require a restore to occur, such as , , , , , and . To disable implicit restore, use the option.\n\nThe command is still useful in certain scenarios where explicitly restoring makes sense, such as continuous integration builds in Azure DevOps Services or in build systems that need to explicitly control when the restore occurs.\n\nFor information about how to manage NuGet feeds, see the documentation.\n\nThe command calls MSBuild, which invokes the target. If the property is set to for a particular project, the target can't be invoked, and the command only runs the implicit dotnet restore on the project.\n\nAny parameters passed to are passed to MSBuild. The and parameters map to MSBuild's and properties, respectively.\n\nThe command accepts MSBuild options, such as for setting properties and to define a logger. For example, you can set an MSBuild property by using the format: .\n\nYou can also set publish-related properties by referring to a .pubxml file. For example:\n\nThe preceding example uses the FolderProfile.pubxml file that is found in the <project_folder>/Properties/PublishProfiles folder. If you specify a path and file extension when setting the property, they're ignored. MSBuild by default looks in the Properties/PublishProfiles folder and assumes the pubxml file extension. To specify the path and filename including extension, set the property instead of the property.\n• is used by Visual Studio to denote the Publish target.\n• is used by the CLI to denote the Publish target.\n\nIf you want the scenario to work in all places, you can initialize both these properties to the same value in the .pubxml file. When GitHub issue dotnet/sdk#20931 is resolved, only one of these properties will need to be set.\n\nSome properties in the .pubxml file are honored only by Visual Studio and have no effect on . We're working to bring the CLI more into alignment with Visual Studio's behavior. But some properties will never be used by the CLI. The CLI and Visual Studio both do the packaging aspect of publishing, and dotnet/sdk#29817 plans to add support for more properties related to that. But the CLI doesn't do the deployment automation aspect of publishing, and properties related to that aren't supported. The most notable .pubxml properties that aren't supported by are the following ones that impact the build:\n\nThe following MSBuild properties change the output of .\n• Compiles application assemblies as ReadyToRun (R2R) format. R2R is a form of ahead-of-time (AOT) compilation. For more information, see ReadyToRun images. To see warnings about missing dependencies that could cause runtime failures, use . We recommend that you specify in a publish profile rather than on the command line.\n• Packages the app into a platform-specific single-file executable. For more information about single-file publishing, see the single-file bundler design document. We recommend that you specify this option in the project file rather than on the command line.\n• Trims unused libraries to reduce the deployment size of an app when publishing a self-contained executable. For more information, see Trim self-contained deployments and executables. Available since .NET 6 SDK. We recommend that you specify this option in the project file rather than on the command line.\n\nFor more information, see the following resources:\n\nWhen you run this command, it initiates an asynchronous background download of advertising manifests for workloads. If the download is still running when this command finishes, the download is stopped. For more information, see Advertising manifests.\n• The project or solution to publish.\n• None is the path and filename of a C#, F#, or Visual Basic project file, or the path to a directory that contains a C#, F#, or Visual Basic project file. If the directory is not specified, it defaults to the current directory.\n• None is the path and filename of a solution file (.sln or .slnx extension), or the path to a directory that contains a solution file. If the directory is not specified, it defaults to the current directory.\n• Specifies the target architecture. This is a shorthand syntax for setting the Runtime Identifier (RID), where the provided value is combined with the default RID. For example, on a machine, specifying sets the RID to . If you use this option, don't use the option. Available since .NET 6 Preview 7.\n• All build output files from the executed command will go in subfolders under the specified path, separated by project. For more information see Artifacts Output Layout. Available since .NET 8 SDK.\n• Defines the build configuration. If you're developing with the .NET 8 SDK or a later version, the command uses the configuration by default for projects whose TargetFramework is set to or a later version. The default build configuration is for earlier versions of the SDK and for earlier target frameworks. You can override the default in project settings or by using this option. For more information, see 'dotnet publish' uses Release configuration and 'dotnet pack' uses Release configuration.\n• Forces the command to ignore any persistent build servers. This option provides a consistent way to disable all use of build caching, which forces a build from scratch. A build that doesn't rely on caches is useful when the caches might be corrupted or incorrect for some reason. Available since .NET 7 SDK.\n• Publishes the application for the specified target framework. You must specify the target framework in the project file.\n• Forces all dependencies to be resolved even if the last restore was successful. Specifying this flag is the same as deleting the project.assets.json file.\n• Prints out a description of how to use the command.\n• Allows the command to stop and wait for user input or action. For example, to complete authentication. Available since .NET Core 3.0 SDK.\n• Specifies one or several target manifests to use to trim the set of packages published with the app. The manifest file is part of the output of the command. To specify multiple manifests, add a option for each manifest.\n• Doesn't build the project before publishing. It also implicitly sets the flag.\n• Ignores project-to-project references and only restores the root project.\n• Doesn't display the startup banner or the copyright message.\n• Doesn't execute an implicit restore when running the command.\n• Specifies the path for the output directory. If not specified, it defaults to [project_file_folder]/bin/[configuration]/[framework]/publish/ for a framework-dependent executable and cross-platform binaries. It defaults to [project_file_folder]/bin/[configuration]/[framework]/[runtime]/publish/ for a self-contained executable. In a web project, if the output folder is in the project folder, successive commands result in nested output folders. For example, if the project folder is myproject, and the publish output folder is myproject/publish, and you run twice, the second run puts content files such as .config and .json files in myproject/publish/publish. To avoid nesting publish folders, specify a publish folder that isn't directly under the project folder, or exclude the publish folder from the project. To exclude a publish folder named publishoutput, add the following element to a element in the .csproj file:\n• If you specify the option when running this command on a solution, the CLI will emit a warning (an error in 7.0.200) due to the unclear semantics of the output path. The option is disallowed because all outputs of all built projects would be copied into the specified directory, which isn't compatible with multi-targeted projects, as well as projects that have different versions of direct and transitive dependencies. For more information, see Solution-level option no longer valid for build-related commands.\n• If you specify a relative path when publishing a project, the generated output directory is relative to the current working directory, not to the project file location. If you specify a relative path when publishing a solution, all output for all projects goes into the specified folder relative to the current working directory. To make publish output go to separate folders for each project, specify a relative path by using the msbuild property instead of the option. For example, sends publish output for each project to a folder under the folder that contains the project file.\n• If you specify a relative path when publishing a project, the generated output directory is relative to the project file location, not to the current working directory. If you specify a relative path when publishing a solution, each project's output goes into a separate folder relative to the project file location. If you specify an absolute path when publishing a solution, all publish output for all projects goes into the specified folder.\n• Specifies the target operating system (OS). This is a shorthand syntax for setting the Runtime Identifier (RID), where the provided value is combined with the default RID. For example, on a machine, specifying sets the RID to . If you use this option, don't use the option. Available since .NET 6.\n• Publishes the .NET runtime with your application so the runtime doesn't need to be installed on the target machine. Default is if a runtime identifier is specified and the project is an executable project (not a library project). For more information, see .NET application publishing and Publish .NET apps with the .NET CLI. If this option is used without specifying or , the default is . In that case, don't put the solution or project argument immediately after , because or is expected in that position.\n• The URI of the NuGet package source to use during the restore operation.\n• Publishes the application for a given runtime. For a list of Runtime Identifiers (RIDs), see the RID catalog. For more information, see .NET application publishing and Publish .NET apps with the .NET CLI. If you use this option, use or also.\n• Specifies whether the terminal logger should be used for the build output. The default is , which first verifies the environment before enabling terminal logging. The environment check verifies that the terminal is capable of using modern output features and isn't using a redirected standard output before enabling the new logger. skips the environment check and enables terminal logging. skips the environment check and uses the default console logger. The terminal logger shows you the restore phase followed by the build phase. During each phase, the currently building projects appear at the bottom of the terminal. Each project that's building outputs both the MSBuild target currently being built and the amount of time spent on that target. You can search this information to learn more about the build. When a project is finished building, a single \"build completed\" section is written that captures:\n• The name of the built project.\n• The status of that build.\n• The primary output of that build (which is hyperlinked).\n• Any diagnostics generated for that project. This option is available starting in .NET 8.\n• Sets the to a platform portable based on the one of your machine. This happens implicitly with properties that require a , such as , , , , and . If the property is set to false, that implicit resolution will no longer occur.\n• Sets the verbosity level of the command. Allowed values are , , , , and . The default is . For more information, see LoggerVerbosity.\n• Defines the version suffix to replace the asterisk ( ) in the version field of the project file.\n• None Create a framework-dependent cross-platform binary for the project in the current directory: Starting with .NET Core 3.0 SDK, this example also creates a framework-dependent executable for the current platform.\n• None Create a self-contained executable for the project in the current directory, for a specific runtime: The RID must be in the project file.\n• None Create a framework-dependent executable for the project in the current directory, for a specific platform: The RID must be in the project file. This example applies to .NET Core 3.0 SDK and later versions.\n• None Publish the project in the current directory, for a specific runtime and target framework:\n• None Publish the current application but don't restore project-to-project (P2P) references, just the root project during the restore operation:"
    },
    {
        "link": "https://stackoverflow.com/questions/67892603/azure-devops-build-pipeline-seems-to-restore-nuget-packages-twice",
        "document": "I have a build pipeline, where I have a Nuget restore step using NuGetCommand which works fine.\n\nBut the next step where the build is performed fails on missing nuget packages.\n\nIt seems the build step tries to restore the nuget packages a second time, which does not work (It doesn't have the credentials to do so)\n\nThe yaml file for the build definition is as follows:\n\nI also have a nuGet.config file which looks like this:\n\nI don't understand why the build step needs to restore the nuget packages again.\n\nIs there a way to point the DotNetCoreCli to the already restored packages?\n\nAs suggested by @Ibrahim I added the --no-restore to the DotNetCoreCLI task. I then received the following error message:\n\nC:\\Program Files\\dotnet\\sdk\\5.0.300\\Sdks\\Microsoft.NET.Sdk\\targets\\Microsoft.PackageDependencyResolution.targets(241,5): error NETSDK1047: Assets file 'D:\\a\\1\\s\\ServiceHosts\\TestProject\\obj\\project.assets.json' doesn't have a target for 'netcoreapp3.1/win-x64'. Ensure that restore has run and that you have included 'netcoreapp3.1' in the TargetFrameworks for your project. You may also need to include 'win-x64' in your project's RuntimeIdentifiers.\n\nThis was mitigated by adding a task to the yaml file to install the newest version of nuget using\n\nAnd adding RuntimeIdentifier win-x64 to the csproj files."
    },
    {
        "link": "https://stackoverflow.com/questions/55759914/dotnet-restore-using-local-and-server-sources",
        "document": "I would like to use dotnet restore command to provide two sources using --source flag. First one is in local folder inside project files and second one is on remote server (actually it's official nuget source).\n\nI used inside teamcity and passed sources as parameters as follow:\n\nHowever when teamcity builds my pipeline, dotnet restore search for packages firstly inside C:\\BuildAgent\\work\\LocalFolder, which is fine - that's what I wanted. But in next step it's looking for second source in C:\\BuildAgent\\work\\https://api.nuget.org/ which is obviously wrong.\n\nI dont know how to combine two sources within one dotnet restore command when one of the sources is local and second one is hosted on the server. Is there any workaround for that?\n\n@Edit I know I can create two build steps with seperate commands. One for local source and second one for server. However It would be nice to combine it within one command."
    }
]