[
    {
        "link": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html",
        "document": "Right-hand side of the system: the time derivative of the state at time . The calling signature is , where is a scalar and is an ndarray with . Additional arguments need to be passed if is used (see documentation of argument). must return an array of the same shape as . See vectorized for more information.\n• None ‘RK45’ (default): Explicit Runge-Kutta method of order 5(4) [1]. The error is controlled assuming accuracy of the fourth-order method, but steps are taken using the fifth-order accurate formula (local extrapolation is done). A quartic interpolation polynomial is used for the dense output [2]. Can be applied in the complex domain.\n• None ‘RK23’: Explicit Runge-Kutta method of order 3(2) [3]. The error is controlled assuming accuracy of the second-order method, but steps are taken using the third-order accurate formula (local extrapolation is done). A cubic Hermite polynomial is used for the dense output. Can be applied in the complex domain.\n• None ‘DOP853’: Explicit Runge-Kutta method of order 8 [13]. Python implementation of the “DOP853” algorithm originally written in Fortran [14]. A 7-th order interpolation polynomial accurate to 7-th order is used for the dense output. Can be applied in the complex domain.\n• None ‘Radau’: Implicit Runge-Kutta method of the Radau IIA family of order 5 [4]. The error is controlled with a third-order accurate embedded formula. A cubic polynomial which satisfies the collocation conditions is used for the dense output.\n• None ‘BDF’: Implicit multi-step variable-order (1 to 5) method based on a backward differentiation formula for the derivative approximation [5]. The implementation follows the one described in [6]. A quasi-constant step scheme is used and accuracy is enhanced using the NDF modification. Can be applied in the complex domain.\n• None ‘LSODA’: Adams/BDF method with automatic stiffness detection and switching [7], [8]. This is a wrapper of the Fortran solver from ODEPACK. Explicit Runge-Kutta methods (‘RK23’, ‘RK45’, ‘DOP853’) should be used for non-stiff problems and implicit methods (‘Radau’, ‘BDF’) for stiff problems [9]. Among Runge-Kutta methods, ‘DOP853’ is recommended for solving with high precision (low values of rtol and atol). If not sure, first try to run ‘RK45’. If it makes unusually many iterations, diverges, or fails, your problem is likely to be stiff and you should use ‘Radau’ or ‘BDF’. ‘LSODA’ can also be a good universal choice, but it might be somewhat less convenient to work with as it wraps old Fortran code. You can also pass an arbitrary class derived from which implements the solver.\n\nEvents to track. If None (default), no events will be tracked. Each event occurs at the zeros of a continuous function of time and state. Each function must have the signature where additional argument have to be passed if is used (see documentation of argument). Each function must return a float. The solver will find an accurate value of t at which using a root-finding algorithm. By default, all zeros will be found. The solver looks for a sign change over each step, so if multiple zero crossings occur within one step, events may be missed. Additionally each event function might have the following attributes: When boolean, whether to terminate integration if this event occurs. When integral, termination occurs after the specified the number of occurrences of this event. Implicitly False if not assigned. Direction of a zero crossing. If direction is positive, event will only trigger when going from negative to positive, and vice versa if direction is negative. If 0, then either direction will trigger event. Implicitly 0 if not assigned. You can assign attributes like to any function in Python.\n\nWhether fun can be called in a vectorized fashion. Default is False. If is False, fun will always be called with of shape , where . If is True, fun may be called with of shape , where is an integer. In this case, fun must behave such that (i.e. each column of the returned array is the time derivative of the state corresponding with a column of ). Setting allows for faster finite difference approximation of the Jacobian by methods ‘Radau’ and ‘BDF’, but will result in slower execution for other methods and for ‘Radau’ and ‘BDF’ in some circumstances (e.g. small ).\n\nAdditional arguments to pass to the user-defined functions. If given, the additional arguments are passed to all user-defined functions. So if, for example, fun has the signature , then jac (if given) and any event functions must have the same signature, and args must be a tuple of length 3.\n\nRelative and absolute tolerances. The solver keeps the local error estimates less than . Here rtol controls a relative accuracy (number of correct digits), while atol controls absolute accuracy (number of correct decimal places). To achieve the desired rtol, set atol to be smaller than the smallest value that can be expected from so that rtol dominates the allowable error. If atol is larger than the number of correct digits is not guaranteed. Conversely, to achieve the desired atol set rtol such that is always smaller than atol. If components of y have different scales, it might be beneficial to set different atol values for different components by passing array_like with shape (n,) for atol. Default values are 1e-3 for rtol and 1e-6 for atol.\n\nJacobian matrix of the right-hand side of the system with respect to y, required by the ‘Radau’, ‘BDF’ and ‘LSODA’ method. The Jacobian matrix has shape (n, n) and its element (i, j) is equal to . There are three ways to define the Jacobian:\n• None If array_like or sparse_matrix, the Jacobian is assumed to be constant. Not supported by ‘LSODA’.\n• None If callable, the Jacobian is assumed to depend on both t and y; it will be called as , as necessary. Additional arguments have to be passed if is used (see documentation of argument). For ‘Radau’ and ‘BDF’ methods, the return value might be a sparse matrix.\n• None If None (default), the Jacobian will be approximated by finite differences. It is generally recommended to provide the Jacobian rather than relying on a finite-difference approximation.\n\nDefines a sparsity structure of the Jacobian matrix for a finite- difference approximation. Its shape must be (n, n). This argument is ignored if jac is not None. If the Jacobian has only few non-zero elements in each row, providing the sparsity structure will greatly speed up the computations [10]. A zero entry means that a corresponding element in the Jacobian is always zero. If None (default), the Jacobian is assumed to be dense. Not supported by ‘LSODA’, see lband and uband instead.\n\nParameters defining the bandwidth of the Jacobian for the ‘LSODA’ method, i.e., . Default is None. Setting these requires your jac routine to return the Jacobian in the packed format: the returned array must have columns and rows in which Jacobian diagonals are written. Specifically . The same format is used in (check for an illustration). These parameters can be also used with to reduce the number of Jacobian elements estimated by finite differences."
    },
    {
        "link": "https://docs.scipy.org/doc/scipy-1.7.1/reference/reference/generated/scipy.integrate.solve_ivp.html",
        "document": "Solve an initial value problem for a system of ODEs.\n\nThis function numerically integrates a system of ordinary differential equations given an initial value:\n\nHere t is a 1-D independent variable (time), y(t) is an N-D vector-valued function (state), and an N-D vector-valued function f(t, y) determines the differential equations. The goal is to find y(t) approximately satisfying the differential equations, given an initial value y(t0)=y0.\n\nSome of the solvers support integration in the complex domain, but note that for stiff ODE solvers, the right-hand side must be complex-differentiable (satisfy Cauchy-Riemann equations [11]). To solve a problem in the complex domain, pass y0 with a complex data type. Another option always available is to rewrite your problem for real and imaginary parts separately.\n\nRight-hand side of the system. The calling signature is . Here t is a scalar, and there are two options for the ndarray y: It can either have shape (n,); then fun must return array_like with shape (n,). Alternatively, it can have shape (n, k); then fun must return an array_like with shape (n, k), i.e., each column corresponds to a single column in y. The choice between the two options is determined by vectorized argument (see below). The vectorized implementation allows a faster approximation of the Jacobian by finite differences (required for stiff solvers). Interval of integration (t0, tf). The solver starts with t=t0 and integrates until it reaches t=tf. Initial state. For problems in the complex domain, pass y0 with a complex data type (even if the initial value is purely real).\n• None ‘RK45’ (default): Explicit Runge-Kutta method of order 5(4) [1]. The error is controlled assuming accuracy of the fourth-order method, but steps are taken using the fifth-order accurate formula (local extrapolation is done). A quartic interpolation polynomial is used for the dense output [2]. Can be applied in the complex domain.\n• None ‘RK23’: Explicit Runge-Kutta method of order 3(2) [3]. The error is controlled assuming accuracy of the second-order method, but steps are taken using the third-order accurate formula (local extrapolation is done). A cubic Hermite polynomial is used for the dense output. Can be applied in the complex domain.\n• None ‘DOP853’: Explicit Runge-Kutta method of order 8 [13]. Python implementation of the “DOP853” algorithm originally written in Fortran [14]. A 7-th order interpolation polynomial accurate to 7-th order is used for the dense output. Can be applied in the complex domain.\n• None ‘Radau’: Implicit Runge-Kutta method of the Radau IIA family of order 5 [4]. The error is controlled with a third-order accurate embedded formula. A cubic polynomial which satisfies the collocation conditions is used for the dense output.\n• None ‘BDF’: Implicit multi-step variable-order (1 to 5) method based on a backward differentiation formula for the derivative approximation [5]. The implementation follows the one described in [6]. A quasi-constant step scheme is used and accuracy is enhanced using the NDF modification. Can be applied in the complex domain.\n• None ‘LSODA’: Adams/BDF method with automatic stiffness detection and switching [7], [8]. This is a wrapper of the Fortran solver from ODEPACK. Explicit Runge-Kutta methods (‘RK23’, ‘RK45’, ‘DOP853’) should be used for non-stiff problems and implicit methods (‘Radau’, ‘BDF’) for stiff problems [9]. Among Runge-Kutta methods, ‘DOP853’ is recommended for solving with high precision (low values of rtol and atol). If not sure, first try to run ‘RK45’. If it makes unusually many iterations, diverges, or fails, your problem is likely to be stiff and you should use ‘Radau’ or ‘BDF’. ‘LSODA’ can also be a good universal choice, but it might be somewhat less convenient to work with as it wraps old Fortran code. You can also pass an arbitrary class derived from which implements the solver. Times at which to store the computed solution, must be sorted and lie within t_span. If None (default), use points selected by the solver. Whether to compute a continuous solution. Default is False. Events to track. If None (default), no events will be tracked. Each event occurs at the zeros of a continuous function of time and state. Each function must have the signature and return a float. The solver will find an accurate value of t at which using a root-finding algorithm. By default, all zeros will be found. The solver looks for a sign change over each step, so if multiple zero crossings occur within one step, events may be missed. Additionally each event function might have the following attributes: Whether to terminate integration if this event occurs. Implicitly False if not assigned. Direction of a zero crossing. If direction is positive, event will only trigger when going from negative to positive, and vice versa if direction is negative. If 0, then either direction will trigger event. Implicitly 0 if not assigned. You can assign attributes like to any function in Python. Whether fun is implemented in a vectorized fashion. Default is False. Additional arguments to pass to the user-defined functions. If given, the additional arguments are passed to all user-defined functions. So if, for example, fun has the signature , then jac (if given) and any event functions must have the same signature, and args must be a tuple of length 3. Options passed to a chosen solver. All options available for already implemented solvers are listed below. Initial step size. Default is None which means that the algorithm should choose. Maximum allowed step size. Default is np.inf, i.e., the step size is not bounded and determined solely by the solver. Relative and absolute tolerances. The solver keeps the local error estimates less than . Here rtol controls a relative accuracy (number of correct digits). But if a component of y is approximately below atol, the error only needs to fall within the same atol threshold, and the number of correct digits is not guaranteed. If components of y have different scales, it might be beneficial to set different atol values for different components by passing array_like with shape (n,) for atol. Default values are 1e-3 for rtol and 1e-6 for atol. Jacobian matrix of the right-hand side of the system with respect to y, required by the ‘Radau’, ‘BDF’ and ‘LSODA’ method. The Jacobian matrix has shape (n, n) and its element (i, j) is equal to . There are three ways to define the Jacobian:\n• None If array_like or sparse_matrix, the Jacobian is assumed to be constant. Not supported by ‘LSODA’.\n• None If callable, the Jacobian is assumed to depend on both t and y; it will be called as , as necessary. For ‘Radau’ and ‘BDF’ methods, the return value might be a sparse matrix.\n• None If None (default), the Jacobian will be approximated by finite differences. It is generally recommended to provide the Jacobian rather than relying on a finite-difference approximation. Defines a sparsity structure of the Jacobian matrix for a finite- difference approximation. Its shape must be (n, n). This argument is ignored if jac is not None. If the Jacobian has only few non-zero elements in each row, providing the sparsity structure will greatly speed up the computations [10]. A zero entry means that a corresponding element in the Jacobian is always zero. If None (default), the Jacobian is assumed to be dense. Not supported by ‘LSODA’, see lband and uband instead. Parameters defining the bandwidth of the Jacobian for the ‘LSODA’ method, i.e., . Default is None. Setting these requires your jac routine to return the Jacobian in the packed format: the returned array must have columns and rows in which Jacobian diagonals are written. Specifically . The same format is used in (check for an illustration). These parameters can be also used with to reduce the number of Jacobian elements estimated by finite differences. The minimum allowed step size for ‘LSODA’ method. By default min_step is zero. Bunch object with the following fields defined: Values of the solution at t. Found solution as instance; None if dense_output was set to False. Contains for each event type a list of arrays at which an event of that type event was detected. None if events was None. For each value of t_events, the corresponding value of the solution. None if events was None. Number of evaluations of the right-hand side. Number of evaluations of the Jacobian.\n• None 0: The solver successfully reached the end of tspan. True if the solver reached the interval end or a termination event occurred ( ).\n\nSpecifying points where the solution is desired.\n\nCannon fired upward with terminal event upon impact. The and fields of an event are applied by monkey patching a function. Here is position and is velocity. The projectile starts at position 0 with velocity +10. Note that the integration never reaches t=100 because the event is terminal.\n\nUse dense_output and events to find position, which is 100, at the apex of the cannonball’s trajectory. Apex is not defined as terminal, so both apex and hit_ground are found. There is no information at t=20, so the sol attribute is used to evaluate the solution. The sol attribute is returned by setting . Alternatively, the y_events attribute can be used to access the solution at the time of the event.\n\nAs an example of a system with additional parameters, we’ll implement the Lotka-Volterra equations [12].\n\nWe pass in the parameter values a=1.5, b=1, c=3 and d=1 with the args argument."
    },
    {
        "link": "https://sundnes.github.io/solving_odes_in_python/ode_book.pdf",
        "document": ""
    },
    {
        "link": "https://py-pde.readthedocs.io/_/downloads/en/0.26.0/pdf",
        "document": ""
    },
    {
        "link": "https://py-pde.readthedocs.io/_/downloads/en/v0.21.0/pdf",
        "document": ""
    },
    {
        "link": "https://csc.ucdavis.edu/~cmg/Group/readings/pythonissue_3of4.pdf",
        "document": ""
    },
    {
        "link": "https://w3resource.com/python-exercises/numpy/create-a-2d-grid-and-solve-a-pde-with-numpy-and-scipy.php",
        "document": "Create a 2D grid and solve a PDE with NumPy and SciPy\n\nWrite a NumPy program to create a 2D grid of data points and solve a partial differential equation (PDE) using SciPy's integrated module.\n• Import the necessary modules from NumPy, SciPy, and Matplotlib.\n• Define the partial differential equation as a system of first-order ordinary differential equations (ODEs).\n• Define the boundary conditions for the PDE.\n• Set an initial guess for the solution.\n• Use SciPy's solve_bvp function to solve the boundary value problem.\n• Plot the solution using Matplotlib for visualization.\n\nPrevious: Generate and analyze synthetic data with NumPy and SciPy.\n\n Next: Perform Geometric transformations on Synthetic data with NumPy."
    },
    {
        "link": "https://py-pde.readthedocs.io/en/latest/manual/advanced_usage.html",
        "document": "A crucial aspect of partial differential equations are boundary conditions, which need to be specified at the domain boundaries. For the simple domains contained in , all boundaries are orthogonal to one of the axes in the domain, so boundary conditions need to be applied to both sides of each axis. Here, the lower side of an axis can have a different condition than the upper side. For instance, one can enforce the value of a field to be at the lower side and its derivative (in the outward direction) to be on the upper side using the following code: Here, the Laplace operator applied to the field in the last line will respect the boundary conditions. Note that both sides of the axis can be specified together if their conditions are the same. For instance, to enforce a value of on both side, one could simply use . Vectorial boundary conditions, e.g., to calculate the vector gradient or tensor divergence, can have vectorial values for the boundary condition. Generally, only the normal components at a boundary need to be specified if an operator reduces the rank of a field, e.g., for divergences. Otherwise, e.g., for gradients and Laplacians, the full field needs to be specified at the boundary. Boundary values that depend on space can be set by specifying a mathematical expression, which may depend on the coordinates of all axes: # two different conditions for lower and upper end of x-axis # the same condition on the lower and upper end of the y-axis To interpret arbitrary expressions, the package uses . It should therefore not be used in a context where malicious input could occur. Heterogeneous values can also be specified by directly supplying an array, whose shape needs to be compatible with the boundary, i.e., it needs to have the same shape as the grid but with the dimension of the axis along which the boundary is specified removed. There also exist special boundary conditions that impose a more complex value of the field ( ) or its derivative ( ). Beyond the spatial coordinates that are already supported for the constant conditions above, the expressions of these boundary conditions can depend on the time variable . Moreover, these boundary conditions also except python functions with signature , thus greatly enlarging the flexibility with which boundary conditions can be expressed. Note that PDEs need to supply the current time when setting the boundary conditions, e.g., when applying the differential operators. The pre-defined PDEs and the general class already support time-dependent boundary conditions. To specify the same boundary conditions for many sides, the wildcard specifier can be used: For example, specifies Dirichlet conditions for all axes, except the upper y-axis, where a Neumann condition is imposed instead. If all axes have the same condition, the outer dictionary can be skipped, so that imposes the same conditions as . Moreover, many boundaries have convenient names, so that for instance can be replaced by , and can be replaced by . One important aspect about boundary conditions is that they need to respect the periodicity of the underlying grid. For instance, in a 2d grid with one periodic axis, the following boundary condition can be used: For convenience, this typical situation can be described with the special boundary condition , e.g., calling the Laplace operator using is identical to the example above. Similarly, the special condition enforces periodic boundary conditions or Dirichlet boundary condition (vanishing value), depending on the periodicity of the underlying grid. In summary, we have the following options for boundary conditions on a field \\(c\\) Here, \\(\\partial_n\\) denotes a derivative in outward normal direction, \\(f\\) denotes an arbitrary function given by an expression (see next section), \\(x\\) denotes coordinates along the boundary, \\(t\\) denotes time. Finally, we support the advanced technique of setting the virtual points at the boundary manually. This can be achieved by passing a python function that takes as its first argument a , which contains the full field data including the virtual points, and a second, optional argument, which is a dictionary containing additional parameters, like the current time point in case of a simulation; see for more details.\n\nExpressions are strings that describe mathematical expressions. They can be used in several places, most prominently in defining PDEs using , in creating fields using , and in defining boundary conditions; see section above. Expressions are parsed using , so the expected syntax is defined by this python package. While we describe some common use cases below, it might be best to test the abilities using the function. To interpret arbitrary expressions, the package uses . It should therefore not be used in a context where malicious input could occur. Simple expressions can contain many standard mathematical functions, e.g., is a valid expression. and furthermore accept differential operators defined in this package. Note that operators need to be specified with their full name, i.e., for a scalar Laplacian and for a Laplacian operating on a vector field. Moreover, the dot product between two vector fields can be denoted by using in the expression, and calculates an outer product. In this case, boundary conditions for the operators can be specified using the argument, in which case the same boundary conditions are applied to all operators. The additional argument provides a more fine-grained control, where conditions for each individual operator can be specified. Field expressions can also directly depend on spatial coordinates. For instance, if a field is defined on a two-dimensional Cartesian grid, the variables and denote the local coordinates. To initialize a step profile in the \\(x\\)-direction, one can use either or , where the second argument denotes the returned value in case the first argument is . For convenience, Cartesian coordinates are also available when using curvilinear grids. The respective coordinate values at a point can be accessed using , where is an index, e.g., for the first axis (normally the x-axis). Finally, expressions for equations in can explicitly depend on time, which is denoted by the variable . Expressions also support user-defined functions via the argument, which is a dictionary that maps the name of a function to an actual implementation. Finally, constants can be defined using the argument. Constants can either be individual numbers or spatially extended data, which provide values for each grid point. Note that in the latter case only the actual grid data should be supplied, i.e., the attribute of a potential field class.\n\nTo implement a new PDE in a way that all of the machinery of can be used, one needs to subclass and overwrite at least the method. A simple implementation for the Kuramoto–Sivashinsky equation could read \"\"\"Evaluate the right hand side of the evolution equation.\"\"\" A slightly more advanced example would allow for attributes that for instance define the boundary conditions and the diffusivity: \"\"\"Initialize the class with a diffusivity and boundary conditions.\"\"\" \"\"\"Evaluate the right hand side of the evolution equation.\"\"\" We here replaced the call to by a dot product with itself (using the notation), which is equivalent. Note that the numpy implementation of the right hand side of the PDE is rather slow since it runs mostly in pure python and constructs a lot of intermediate field classes. While such an implementation is helpful for testing initial ideas, actual computations should be performed with compiled PDEs as described below. Another feature of custom PDE classes is a special function that is called after every time step. This function is defined by and allows direct manipulation of the state data and also abortion of the simulation by raising . \"\"\"Create a hook function that is called after every time step.\"\"\" # hook function and initial value for data \"\"\"Evaluate the right hand side of the evolution equation.\"\"\" We here use a simple constant evolution equation. The hook defined by the first method does two things: First, it limits the state to the interval using . Second, it evaluates the standard deviation across the entire data, aborting the simulation when the value exceeds one. Note that the hook always receives the data always as a and not as a full field class. The hook can also keep track of additional data via , which is a that can be updated in place.\n\nThis section explains how to use the low-level version of the field operators. This is necessary for the numba-accelerated implementations described above and it might be necessary to use parts of the package in other packages. Applying a differential operator to an instance of is a simple as calling , where denotes the boundary conditions. Calling this method returns another , which in this case contains the discretized Laplacian of the original field. The equivalent call using the low-level interface is Here, the first line creates a function for the given grid and the boundary conditions . This function can be applied to instances, e.g. . Note that the result of this call is again a . Note that this example does not even use the field classes. Instead, it directly defines a and the respective gradient operator. This operator is then applied to a random field and the resulting represents the 2-dimensional vector field. The method of the grids generally supports the following differential operators: , , , , , , and . Moreover, generic operators that perform a derivative along a single axis are supported: Specifying for instance performs a single derivative along the -direction, uses a forward derivative along the -direction, and performs a second derivative in -direction. A complete list of operators supported by a certain grid class can be obtained from the class property . New operators can be added using the class method . The integral of an instance of is usually determined by accessing the property . Since the integral of a discretized field is basically a sum weighted by the cell volumes, calculating the integral using only is easy: Note that is a simple number for Cartesian grids, but is an array for more complicated grids, where the cell volume is not uniform. The fields defined in the package also support linear interpolation by calling . Similarly to the differential operators discussed above, this call can also be translated to code that does not use the full package: We first create a function , which is then used to interpolate the field data at a certain point. Note that the coordinates of the point need to be supplied as a and that only the interpolation at single points is supported. However, iteration over multiple points can be fast when the loop is compiled with . For vector and tensor fields, defines inner products that can be accessed conveniently using the -syntax: determines the scalar product between the two fields. The package also provides an implementation for an dot-operator: Here, is the data of the scalar field resulting from the dot product.\n\nThe compiled operators introduced in the previous section can be used to implement a compiled method for the evolution rate of PDEs. As an example, we now extend the class introduced above: \"\"\" initialize the class with a diffusivity and boundary conditions for the actual field and its second derivative \"\"\" To activate the compiled implementation of the evolution rate, we simply have to overwrite the method. This method expects an example of the state class (e.g., an instance of ) and returns a function that calculates the evolution rate. The argument is necessary to define the grid and the dimensionality of the data that the returned function is supposed to be handling. The implementation of the compiled function is split in several parts, where we first copy the attributes that are required by the implementation. This is necessary, since freezes the values when compiling the function, so that in the example above the diffusivity cannot be altered without recompiling. In the next step, we create all operators that we need subsequently. Here, we use the boundary conditions defined by the attributes, which requires two different laplace operators, since their boundary conditions might differ. In the last step, we define the actual implementation of the evolution rate as a local function that is compiled using the decorator. Here, we use the implementation shipped with , which sets some default values. However, we could have also used the usual numba implementation. It is important that the implementation of the evolution rate only uses python constructs that numba can compile. One advantage of the numba compiled implementation is that we can now use loops, which will be much faster than their python equivalents. For instance, we could have written the dot product in the last line as an explicit loop: Here, we extract the total number of elements in the state using its attribute and we obtain the dimensionality of the space from the grid attribute . Note that we access numpy arrays using their attribute to provide an implementation that works for all dimensions."
    },
    {
        "link": "https://stackoverflow.com/questions/43742613/how-to-simulate-coupled-pde-in-python",
        "document": "I'm trying to simulate in time and space the following system of partial differential equations. I'm using python 3 for that.\n\nHere is a link to the set of equations with their boundary conditions\n\nMy ideas was to transform all the equations to the discrete form (forward Euler as the simplest starting point) and then run the code. Forward Euler implies: Here lin to image i = 0,...,Nx - mesh for n = 0,1,...,Nt Here what I have (by the means of numpy)\n\nMy first problem I'm encountering is different warnings:\n\nMy main question is: it possible to solve this set with the forward Euler Method Iam trying at the moment ? Thank you everyone in advance!"
    },
    {
        "link": "http://kevinpmooney.blogspot.com/2019/06/solving-partial-differential-equations.html",
        "document": "# Import numpy and the ODE solver from Scipy solve_ivp (T, t, D, N, L, Tl, Tr): delta (L) (N) (t, T): # Boundary conditions set explicitly. This is probably redundant # as these numbers are set before the function is called and the # code takes the time derivative at these points to be zero, but # we'll ensure the proper boundary conditions anyway. T[ ] Tl T[ ] Tr Tprime np zeros( (N) ) # We are being lazy and forcing the temperature of the boundaries # not to change by setting the derivative to zero at those points Tprime[ ] Tprime[ ] # Implement the diffusion equation in the interior points using the # central difference formula. This can be vectorized for better # performance, but I am just using a loop here. i ( , N ): Tprime[i] D (T[i ] T[i] T[i ]) delta Tprime # Solve the equation by calling solve_ivp and return the solution sol solve_ivp(equations, [tspan[ ], tspan[ ]], T, t_eval tspan) sol # Divide the rod into N points, 100 in this case N L D Tl Tr # We are interested in t = 2 to 2 = 2 seconds tspan np linspace( , ) T np ones( (N) ) # These are the boundary conditions. T[ ] Tl T[ ] Tr # Solve the problem by calling our function which invokes the solver sol diffusion(T, tspan, D, N, L, Tl, Tr) plt plot(np linspace( , , N), sol y[:, ], ) plt xlabel( ) plt ylabel( ) plt grid( ) plt show()\n\nIDA Implicit_Problem : ( , t, T, D, N, L, Tl, Tr): delta (L) (N) # Assign the variables passed into the constructor to class N N D D L L Tl Tl Tr Tr # Initial guesses for T and Tprime (T should be OK as that is just T0 T Tprime0 np zeros( T shape ) t0 # Set which variables are to be treated as differential or algvar [ ] T shape[ ] algvar[ ] algvar[ ] # Create the model object and pass initial guesses at T and # Tprime. The code will then solve for consistent initial model Implicit_Problem( equations, T0, Tprime0, t0) # Create a simulation object from the IDA DAE solve and pass in sim IDA(model) # Bind our algvar variable to the sim object sim algvar algvar sim make_consistent( ) # Integrate the system and bind the results to class variables. t, T, Tprime sim simulate(t[ ], ) ( , t, T, Tprime): res np zeros( T shape ) res[ ] T[ ] Tl res[ ] T[ ] Tr i ( , N ): res[i] Tprime[i] D (T[i ] T[i] T[i ]) \\ delta res # Divide the rod into N points, 100 in this case N L D Tl Tr t np linspace( , ) T np ones( (N) ) # Run the code to solve the problem problem diffusion(t, T, D, N, L, Tl, Tr) x np linspace( , L, N) plt plot(x, problem T[ , :], ) plt xlabel( ) plt ylabel( ) plt grid( ) plt show()\n\nIDA Implicit_Problem : ( , t, T, D, N, L, Tl, Tr): delta (L) (N) # Assign the variables passed into the constructor to class N N D D L L Tl Tl Tr Tr # Initial guesses for T and Tprime (T should be OK as that is just T0 T Tprime0 np zeros( T shape ) t0 # Set which variables are to be treated as differential or algvar [ ] T shape[ ] algvar[ ] algvar[ ] # Create the model object and pass initial guesses at T and # Tprime. The code will then solve for consistent initial model Implicit_Problem( equations, T0, Tprime0, t0) # Create a simulation object from the IDA DAE solve and pass in sim IDA(model) # Bind our algvar variable to the sim object sim algvar algvar sim make_consistent( ) # Integrate the system and bind the results to class variables. t, T, Tprime sim simulate(t[ ], ) ( , t, T, Tprime): res np zeros( T shape ) res[ ] T[ ] Tl res[ ] T[ ] Tr i ( , N ): res[i] Tprime[i] D (T[i ] T[i] T[i ]) \\ delta res : ( , t, T, D, N, L, Tl, Tr): delta (L) (N) # Assign the variables passed into the constructor to class N N D D L L Tl Tl Tr Tr diagonals [ np ones(( N ,)), np ones(( N,)), np ones(( N ,)) ] M scipy sparse diags(diagonals, [ , , ]) # Initial guesses for T and Tprime (T should be OK as that is just T0 T Tprime0 np zeros( T shape ) t0 # Set which variables are to be treated as differential or algvar [ ] T shape[ ] algvar[ ] algvar[ ] # Create the model object and pass initial guesses at T and # Tprime. The code will then solve for consistent initial model Implicit_Problem( equations, T0, Tprime0, t0) # Create a simulation object from the IDA DAE solve and pass in sim IDA(model) # Bind our algvar variable to the sim object sim algvar algvar sim make_consistent( ) # Integrate the system and bind the results to class variables. t, T, Tprime sim simulate(t[ ], ) ( , t, T, Tprime): res Tprime D delta M T res[ ] T[ ] Tl res[ ] T[ ] Tr res # Divide the rod into N points, 100 in this case N L D Tl Tr t np linspace( , ) T np ones( (N) ) # Run the code with the loop to solve the problem. Also calculate the # time taken to run the code. start_time time time() problem diffusion(t, T, D, N, L, Tl, Tr) end_time time time() ( , end_time start_time) # Run the vectorized version and calculate the time. start_time time time() problem_vectorized vectorized_diffusion(t, T, D, N, L, Tl, Tr) end_time time time() ( , end_time start_time) x np linspace( , L, N) plt plot(x, problem_vectorized T[ , :], , label ) plt plot(x, problem T[ , :], , label ) plt xlabel( ) plt ylabel( ) plt grid( ) plt legend() plt show()\n\nDespite having a plan in mind on the subjects of these posts, I tend to write them based on what is going on at the moment rather than sticking to the original schedule. The last article was inspired by a couple of curve-fitting questions that came up at work within short succession, and this one, also inspired by questions from our scientists and engineers, is based on questions on using Python for solving ordinary and partial differential equations (ODEs and PDEs). One question involved needing to estimate how long a cylindrical battery cell would take to cool down given various thermal contacts with a heat sink and electrical pulsing criteria. They wanted to run though a bunch of scenarios quickly and didn't have the time or desire to set something up with a full-blown finite-element package like Ansys or COMSOL. That exact problem is beyond the scope of this post, but we can still look at solving simple heat-transfer problems.We'll look at a couple examples of solving the diffusion equation for different geometries and boundary conditions. We'll start off with the common Python libraries numpy and scipy and solve these problems in an somewhat \"hacky\" sort of way. This will enable us to solve Dirichlet boundary value problems. You'll see why I say this is a bit of a hack as we go set up the problem. Then we'll look at solving the same types of problems using the Assimulo package which a Python interface built around the Sundials differential algebraic equation solves put out by Lawrence Livermore National Laboratory. This will enable us to solve problems with Neumann boundary conditions as well.The reason for not just using Assimulo from the start is scipy is a pretty standard library while Assimulo is more specialized and needs a C and FORTRAN compiler to install on Linux. I haven't yet tried installing it on Windows. I wanted this to be useful for those who don't have the tools available, or the permission to buld the package from scratch such as might be the case in a company's work environment.In the interest of brevity, I'll only do 1-dimensional problems here and in a short follow-up, expand the ideas here to more complicated 2-dimensional problems.As a last bit of administrivia, I will build the time derivative vectors using a for-loop for the main part of this code. I think this a bit clearer for the most part, but in general is not as computationally efficient. The building of the derivatives and residuals should ideally be done in a vectorized manner. I'll present code that does this in the appendix with a little description, but will stick to the for-loop for the most part.So, with all that said, let's begin.We'll start off with a 1-dimensional diffusion equation and look to solve for the temperature distribution in a rod whose end points are clamped at different fixed temperatures. For the sake of simplicity, we'll assume the diffusion coefficient is constant.Our strategy will be to approximate the PDE by a system of ordinary differential equations which can be solved by existing ODE solvers. This is done by discretizing the spatial derivatives leading to an ordinary differential equation that describes the time evolution of the temperature at each grid point. Boundary and initial conditions gives us some algebraic equations that provide constraints on this system.The temperature as a function of position and time is given by the 1-d diffusion equation,\\begin{equation}\\frac{\\partial T}{\\partial t} = D\\frac{\\partial^2 T}{\\partial x^2},\\label{eq:1d_diffusion}\\end{equation}where $t$ is time, $x$ is the position along the rod, $D$ is the diffusion coefficient, and $T$ is the temperature at a given position and time.We will impose the boundary conditions $T(t, x = 0) = T_L$ and $T(t, x = L) = T_R$ where $L$ is the length of the rod. These are Dirichlet boundary conditions where the value of the function is specified at the boundaries. Later on, we'll touch briefly on Neumann conditions, where the derivative at the boundaries are known.We will use a finite difference method discretizing the spatial component of our equation. We will use a central difference formula and approximate the second derivative at the $i$th point as,\\begin{equation}\\frac{\\partial^2 T}{\\partial x^2_i} \\approx \\frac{ T_{i+1} - 2T_i + T_{i-1}}{\\Delta^2}.\\label{eq:central_difference}\\end{equation}The subscript $i$ in Eq. \\ref{eq:central_difference} again reffers to the $i$th grid point on our rod, and $\\Delta$ is the distance between grid points. Here, we've assumed a uniform spacing for simplicity.If there are a total of $N$ points, we can rewrite Eq. \\ref{eq:1d_diffusion} asa system of $N-2$ ODEs and two algebraic constrain equations for our boundary conditions.\\begin{align}T_1 & = T_l \n\nonumber \\\\\\frac{dT_2}{dt} & = & D\\left(\\frac{ T_3 - 2T_2 + T_1}{\\Delta^2}\\right)\n\nonumber \\\\\\frac{dT_3}{dt} & = & D\\left(\\frac{ T_4 - 2T_3 + T_2}{\\Delta^2}\\right)\n\nonumber \\\\&\\vdots& \\\\\\frac{dT_{N-2}}{dt} & = & D\\left(\\frac{ T_{N-1} - 2T_{N-2} + T_{N-3}}{\\Delta^2}\\right)\n\nonumber \\\\\\frac{dT_{N-1}}{dt} & = & D\\left(\\frac{ T_{N} - 2T_{N-1} + T_{N-2}}{\\Delta^2}\\right) \n\nonumber \\\\T_N & = T_r\n\nonumber.\\label{eq:difference_loop}\\end{align}Now to code this. Scipy has a built-in differential equation solverincluded in the scipy.integrate package. It solves equations of the form $dy/dt = f(t, y)$, and uses a Runge-Kutta type algorithm by default. The function passed to the solver does not take addition arguments. Therefore, to pass other parameters to the solver, we need to use either a nested function or create a class and use class variables to hold additional information. For the first examples, we will use the first method, and when we look at Assimulo later, we will use the second.Consider the following code;The is rather simple, so I've just presented the entire thing here in one block. We took the diffusion constant $D$ to be 0.01, the length of the rod $L$ to be 1m, and the initial temperature to be 5 degrees. The important part of this is the functionwhich does the discretization and gives an ODE for each point of our grid. I've set the boundary conditions explicitly and set the time derivative at the two end point to zero to ensure they do not change.Running this gives the following result for the temperature profile after two seconds.Simple enough. You can also see why it is a bit of a hack. The solver assumes each equation is an ODE, so I had to set the time derivatives equal to zero and explicitly set the boundary temperatures in the code. This works well enough in this case, but with more complicated boundary conditions, such as constant flux at the boundary, we can't use this trick.The proper way to approach this is to use a differential algebraic equation (DAE) solver. DAEs, as the name implies, are systems of both ODEs and algebraic equations. They are generally written in the form of,\\begin{align}\\frac{dx}{dt} & = f(x(t), y(y), t) \\\\\\label{eq:DAE}0 & = g(x(t), y(t), t). \n\nonumber\\end{align}Assimulo comes with a DAE solver as part of the package and we can make use of that directly. This will enable us to handle more involved boundary conditions.The code is more or less the same as above with a couple important exceptions. The first is the form of the function that defines Eq. 4. The solver expects an equation in the form,\\begin{equation}0 = F\\left( t, y, \\frac{dy}{dt} \\right).\\label{eq:DAE_res}\\end{equation}So we write our function to have the form of r. where the variableis the residual. As I noted earlier, I wrapped the solver function in a class this time rather than using nested functions.The other issue is that unlike ODEs, where for a first order equation we need only provide the initial values, in the case of DAEs, we need to provide both the initial values as well as the initial values of the first derivatives. We don't know the value of these derivatives which presents a problem We could write our own code to calculate these, but fortunately, the IDA solver can work this out for us without us having to much other than tell the solver which variables are to be treated as algebraic and which are differential.The line,instructs the solver to calculate the initial values of the algrabraic compoents of $Y$ and the differential components of $y^\\prime$. Therefore we need to tell the solver which variables are algebraic and which are differential. This is done in the following lines.andSee the Sundials documentation for more information.The rest of the code is essentially the same as the previous. Running this gives,The results are identical to those presented in Fig. 1.You can see the utility of using this method as opposed to Scipy's internal solvers right away. Say we had the boundary constraint,\\begin{equation}\\frac{dT}{dx}_{x = L} = C.\\end{equation}We can handle this by replacing the current boundary condition for the right-hand side with this line of code:It's that simple.I have said in other articles, one wants to avoid looping over arrays and matrices when using libraries like Numpy and Scipy. These packages are typically written in highly optimized C or FORTRAN code and compiled into libraries called by Python. As these binary libraries are much faster than code running through an interpreter, it is a good idea to try to write things to take advantage of this fact. When I learned numerical techniques, we were writing the code in C or C++, so this was less of an issue. I presented the code above using loops because I find that a little more intuitive, but I would be remiss if I didn't at least give a vectorized version.Consider Eqs. \\ref{eq:central_difference} and 3. With the exception of the first and last row, these equations can be though of as the following matrix equation,\\begin{equation}\\vec{T^\\prime} = \\frac{D}{\\Delta^2} \\overline{\\overline{\\sigma}} \\quad \\vec{T},\\label{vecotr}\\end{equation}where $\\vec{T^\\prime}$ and $\\vec{T}$ are column vecrtors of the time derivative of the temperature and the temperature, respectively. The matrix $\\overline{\\overline{\\sigma}}$ is a sparse, tridiaonal matrix whose elements are,\\begin{equation}\\left(\\begin{matrix}\\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots\\\\1 & -2 & 1 & 0 & 0 & 0 & \\cdots& 0 \\\\0 & 1 & -2 & 1 & 0 & 0 & \\cdots & 0 \\\\\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\0 & 0 & 0 & 0 & 0 & 1 & -2 & 1 \\\\\\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots\\\\\\end{matrix}\\right).\\label{eq:matrix}\\end{equation}The first and last rows of the matrix are where we'd set the boundary conditions. When we build the matrix, we explicitly tell Scipy to use a sparse matrix representation. Most of the entries in Eq. \\ref{eq:matrix} are zeros. Scipy is smart enough to not allocate memory for all the zero entries and to not multiply out all the zeros when evaluating Eq. 7. This is very useful, as these matrices can get large fast.The code is essentially the same as before, just with the loop replaced by matrix multiplication.For small numbers of grid points, the vectorization speedup is between a factor of three and four. With $N = 1000$, the vectorized code takes 72 seconds to run vs. 6000 seconds for the nonvectorized code. So use vectorization if you can!For more information on finite difference methods, check out the Wikipedia article on the subject The Wikipedia article on DAEs has some basic information The Assimulo projectThe SUNDIALS page at Lawrence Livermore National Laboratory."
    }
]