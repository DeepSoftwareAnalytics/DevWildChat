[
    {
        "link": "https://geeksforgeeks.org/std-shared_mutex-in-cpp",
        "document": "In C++, std::mutex is a mechanism that locks access to the shared resource when some other thread is working on it so that errors such as race conditions can be avoided and threads can be synchronized. But in some cases, several threads need to read the data from shared resources at the same time. Here, the std::shared_mutex comes into play. In this article, we will discuss the std::shared_mutex, its associated methods, and how it is different from the std::mutex in C++.\n\nPrerequisite: C++ Multithreading, std::mutex in C++.\n\nIn C++, std::shared_mutex is a synchronization primitive that lets several threads use a shared resource simultaneously for reading while guaranteeing exclusive writing access. It is helpful in situations where many threads need read-only access to the same data structure, but write operations aren't often used.\n\nThe std::shared_mutex provides two types of locks:\n• Unique Lock: It is a unique lock that can only be acquired by a single thread at a time. This lock only allows a single thread to modify the shared resources while blocking the other threads. It is similar to the lock provided by the std::mutex.\n• Shared Lock: The shared lock is a non-exclusive lock that several threads can acquire at once. This gives multiple threads read-only access to the shared resource.\n\nThe std::shared_mutex contains several member methods that are required to perform different operations. Some of the commonly used functions are:\n\nSimilar to std::mutex, std::shared_mutex is a synchronization primitive that prevents concurrent access to shared resources. Both have their own advantages and disadvantages.\n\nThe std::shared_mutex has the following advantages over std::mutex:\n• Enhanced Parallelism: Multiple threads can access the shared data for reading operation at once enhancing the parallelism of the program where only read operation is required.\n• Limited Flexibility: There are only two types of locks that the std::shared_mutex supports: shared locks and unique locks. Additional lock types, such as recursive and postponed locks, are supported via the std::mutex.\n• Increased Complexity: Compared to std::mutex, std::shared_mutex has greater complexity. It may become more challenging to understand and apply.\n\nFollowing are some main applications of std::shared_mutex:\n• When reading data is the primary operation and writing data is uncommon, shared mutexes offer effective parallelism.\n• Shared mutexes can be used in caching algorithms to enable simultaneous updating of the cache by one thread while allowing several threads to read data that has been cached.\n• Database connection pooling allows for simultaneous reading and updating of connection information by several threads, but not for simultaneous modification.\n\nstd::shared_mutex is one of the useful mechanisms for access synchronization in multithreaded environments. It works especially effectively in scenarios where the shared data is mostly read-only. But before utilizing the std::shared_mutex, it's crucial to be aware of its limitations."
    },
    {
        "link": "https://en.cppreference.com/w/cpp/thread/shared_mutex",
        "document": "The class is a synchronization primitive that can be used to protect shared data from being simultaneously accessed by multiple threads. In contrast to other mutex types which facilitate exclusive access, a shared_mutex has two levels of access:\n• shared - several threads can share ownership of the same mutex.\n• exclusive - only one thread can own the mutex.\n\nIf one thread has acquired the exclusive lock (through lock, try_lock), no other threads can acquire the lock (including the shared).\n\nIf one thread has acquired the shared lock (through lock_shared, try_lock_shared), no other thread can acquire the exclusive lock, but can acquire the shared lock.\n\nOnly when the exclusive lock has not been acquired by any thread, the shared lock can be acquired by multiple threads.\n\nWithin one thread, only one lock (shared or exclusive) can be acquired at the same time.\n\nShared mutexes are especially useful when shared data can be safely read by any number of threads simultaneously, but a thread may only write the same data when no other thread is reading or writing at the same time.\n\nThe class satisfies all requirements of SharedMutex and StandardLayoutType.\n\nThe output below was generated on a single-core machine. When starts, it enters the loop for the first time and calls followed by . However, before it can print the returned value to std::cout, the scheduler puts to sleep and wakes up , which obviously has time enough to run all three loop iterations at once. Back to , still in the first loop iteration, it finally prints its local copy of the counter's value, which is 1, to and then runs the remaining two loop iterations. On a multi-core machine, none of the threads is put to sleep and the output is more likely to be in ascending order. ThreadSafeCounter ThreadSafeCounter // Multiple threads/readers can read the counter's value at the same time. get lock mutex_ value_ // Only one thread/writer can increment/write the counter's value. increment lock mutex_ value_ // Only one thread/writer can reset/write the counter's value. reset lock mutex_ value_ mutable std mutex_ value_ main ThreadSafeCounter counter increment_and_print counter i i i counter. counter. thread1 increment_and_print thread2 increment_and_print thread1. thread2."
    },
    {
        "link": "https://stackoverflow.com/questions/14306797/c11-equivalent-to-boost-shared-mutex",
        "document": "Simple... There isn't one. There is no standard C++ implementation of a readers-writer lock.\n\nBut, you have a few options here.\n• You are left at your own devices to make your own readers-writer lock.\n• Use a platform-specific implementation such as Win32's, POSIX's, or Boost's as you mention.\n• Don't use one at all -- use a mutex which already exists in C++11.\n\nGoing with #1 and implementing your own is a scary undertaking and it is possible to riddle your code with race conditions if you don't get it right. There is a reference implemenation that may make the job a bit easier.\n\nIf you want platform independent code or don't want to include any extra libraries in your code for something as simple as a reader-writer lock, you can throw #2 out the window.\n\nAnd, #3 has a couple caveats that most people don't realize: Using a reader-writer lock is often less performant, and has more difficult-to-understand code than an equivalent implementation using a simple mutex. This is because of the extra book-keeping that has to go on behind the scenes of a readers-writer lock implementation.\n\nI can only present you your options, really it is up to you to weigh the costs and benefits of each and pick which works best.\n\nEdit: C++17 now has a type for situations where the benefits of having multiple concurrent readers outweigh the performance cost of the itself."
    },
    {
        "link": "https://en.cppreference.com/w/cpp/thread",
        "document": "C++ includes built-in support for threads, atomic operations, mutual exclusion, condition variables, and futures.\n\nThreads enable programs to execute across several processor cores.\n\nThe components stop source, stop token, and stop callback can be used to asynchronously request that an operation stops execution in a timely manner, typically because the result is no longer required. Such a request is called a stop request.\n\nThese components specify the semantics of shared access to a stop state. Any object modeling any of these components that refer to the same stop state is an associated stop source, stop token, or stop callback, respectively.\n• to cooperatively cancel the execution such as for ,\n\nIn fact, they do not even need to be used to \"stop\" anything, but can instead be used for a thread-safe one-time function(s) invocation trigger, for example.\n\nThese components are provided for fine-grained atomic operations allowing for lockless concurrent programming. Each atomic operation is indivisible with regards to any other atomic operation that involves the same object. Atomic objects are free of data races.\n\nNeither the macro, nor any of the non-macro global namespace declarations are provided by any C++ standard library header other than .\n\nMutual exclusion algorithms prevent multiple threads from simultaneously accessing shared resources. This prevents data races and provides support for synchronization between threads.\n\nA condition variable is a synchronization primitive that allows multiple threads to communicate with each other. It allows some number of threads to wait (possibly with a timeout) for notification from another thread that they may proceed. A condition variable is always associated with a mutex.\n\nA semaphore is a lightweight synchronization primitive used to constrain concurrent access to a shared resource. When either would suffice, a semaphore can be more efficient than a condition variable.\n\nLatches and barriers are thread coordination mechanisms that allow any number of threads to block until an expected number of threads arrive. A latch cannot be reused, while a barrier can be used repeatedly.\n\nThe standard library provides facilities to obtain values that are returned and to catch exceptions that are thrown by asynchronous tasks (i.e. functions launched in separate threads). These values are communicated in a shared state, in which the asynchronous task may write its return value or store an exception, and which may be examined, waited for, and otherwise manipulated by other threads that hold instances of std::future or std::shared_future that reference that shared state.\n\nSafe-reclamation techniques are most frequently used to straightforwardly resolve access-deletion races."
    },
    {
        "link": "https://stackoverflow.com/questions/36471955/c11-threads-with-shared-mutex-and-class-instance",
        "document": "Hello I am experimenting with C++11 and threads. I am facing some problems I can't figure out how to solve. I want to do stuff of a class in another thread and need to use a mutex in several functions and the class. Yes I know std::mutex is not copyable but I do not know how to solve my problem.\n\nWhat I have so far is main.cpp:"
    },
    {
        "link": "https://stackoverflow.com/questions/14306797/c11-equivalent-to-boost-shared-mutex",
        "document": "Simple... There isn't one. There is no standard C++ implementation of a readers-writer lock.\n\nBut, you have a few options here.\n• You are left at your own devices to make your own readers-writer lock.\n• Use a platform-specific implementation such as Win32's, POSIX's, or Boost's as you mention.\n• Don't use one at all -- use a mutex which already exists in C++11.\n\nGoing with #1 and implementing your own is a scary undertaking and it is possible to riddle your code with race conditions if you don't get it right. There is a reference implemenation that may make the job a bit easier.\n\nIf you want platform independent code or don't want to include any extra libraries in your code for something as simple as a reader-writer lock, you can throw #2 out the window.\n\nAnd, #3 has a couple caveats that most people don't realize: Using a reader-writer lock is often less performant, and has more difficult-to-understand code than an equivalent implementation using a simple mutex. This is because of the extra book-keeping that has to go on behind the scenes of a readers-writer lock implementation.\n\nI can only present you your options, really it is up to you to weigh the costs and benefits of each and pick which works best.\n\nEdit: C++17 now has a type for situations where the benefits of having multiple concurrent readers outweigh the performance cost of the itself."
    },
    {
        "link": "https://stackoverflow.com/questions/7241993/is-it-smart-to-replace-boostthread-and-boostmutex-with-c11-equivalents",
        "document": "There are several differences between Boost.Thread and the C++11 standard thread library:\n• C++11 supports , but Boost does not\n• Boost has a for multiple-reader/single-writer locking. The analogous is available only since C++14 (N3891), while is available only since C++17 (N4508).\n• C++11 timeouts are different to Boost timeouts (though this should soon change now Boost.Chrono has been accepted).\n• Some of the names are different (e.g. vs )\n• The argument-passing semantics of are different to --- Boost uses , which requires copyable arguments. allows move-only types such as to be passed as arguments. Due to the use of , the semantics of placeholders such as in nested bind expressions can be different too.\n• If you don't explicitly call or then the destructor and assignment operator will call on the thread object being destroyed/assigned to. With a C++11 object, this will result in a call to and abort the application.\n\nTo clarify the point about move-only parameters, the following is valid C++11, and transfers the ownership of the from the temporary to the parameter of when the new thread is started. However, if you use then it won't work, as it uses internally, and cannot be copied. There is also a bug in the C++11 thread library provided with GCC that prevents this working, as it uses in the implementation there too.\n\nIf you are using Boost then you can probably switch to C++11 threads relatively painlessly if your compiler supports it (e.g. recent versions of GCC on linux have a mostly-complete implementation of the C++11 thread library available in mode).\n\nIf your compiler doesn't support C++11 threads then you may be able to get a third-party implementation such as Just::Thread, but this is still a dependency."
    },
    {
        "link": "https://codeproject.com/Articles/1183423/We-Make-a-std-shared-mutex-10-Times-Faster",
        "document": "This article details the atomic operations and C++11 memory barriers and the assembler instructions generated by it on x86_64 CPUs.\n• Thread-safe std::map with the speed of lock-free map\n\nIn this article, we will detail the atomic operations and C++11 memory barriers and the assembler instructions generated by it on x86_64 CPUs.\n\nNext, we’ll show how to speed up the work of up to the level of complex and optimized lock-free data structures that are similar to in terms of their functionality, for example: and from library (Concurrent Data Structures library): https://github.com/khizmax/libcds.\n\nAnd we can get such multi-threaded performance for any of your initially non-thread-safe -class used as – it is class, where is own optimized shared-mutex.\n\nNamely, we will show how to realize your own high-performance contention-free shared-mutex, which almost does not conflict on readings. We implement our own active locks - spinlock and recursive-spinlock - to lock the rows in the update operations. We will create RAII-blocking pointers to avoid the cost of multiple locking. Here are the results of performance tests.\n\nAnd as «Just for fun» bonus, we will demonstrate the way of realization of our own class of simplified partitioned type , which is even more optimized for multithreading, consisting of several , in analogy with partition table from RDBMS, when the boundaries of each section are known initially.\n\nLet’s consider the fundamentals of multithreading, atomicity and memory barriers.\n\nIf we change the same data from a set of threads, i.e., if we run the function simultaneously in several threads:\n\nthen each thread calling the function adds to the ordinary shared variable In the general case, such a code will not be thread-safe, because compound operations (RMW - read-modify-write) with the ordinary variables consist of many small operations, between which another thread can change the data. The operation consists of at least three mini-operations:\n• Load the value of the variable “ ” into the CPU register\n• Add to the value in the register\n• Write the value of the register back into the variable “ ”\n\nFor non-atomic , if initially and 2 threads perform the operation then the result should be But the following can happen (step by step):\n\nTwo threads added to the same global variable with initial value , but in the end, the variable - such a problem is called Data-Races.\n\nThere are three ways that help to avoid such a problem:\n• To use atomic instructions with atomic variables - but there is one disadvantage, the number of atomic functions is very small - therefore, it is difficult to realize complex logic with the help of such functions: http://en.cppreference.com/w/cpp/atomic/atomic\n• To develop your own complex lock-free algorithms for each new container.\n• To use locks ( , , …) – they admit 1 thread, one by one, to the locked code, so the problem of data-races does not arise and we can use arbitrary complex logic by using any normal not thread-safe objects.\n\nFor : if initially and 2 threads perform the operation then the result always will be\n\nThe following always happens on x86_64 CPU (step by step):\n\nOn processors with LL/SC-support, for example on ARM, other steps occur, but with the same correct result: .\n\nAtomic variables are introduced into the standard C++11: http://en.cppreference.com/w/cpp/atomic/atomic\n\nHere are three fragments of the correct code identical in meaning:\n• It is identical to example 1:\n• It is identical to example 1:\n\nIt means that for :\n• and is equal to and\n• and is equal to and\n• Sequential Consistency is the default memory barrier (the most strict and reliable, but also the slowest with regard to others).\n\nLet’s note the difference between and volatile in C++11: http://www.drdobbs.com/parallel/volatile-vs-volatile/212701484\n\nThere are five main differences:\n• Optimizations: For two optimizations are possible, which are impossible for\n• Optimization of the fusion: ; can be replaced by compiler with\n• Optimization of substitution by a constant: can be replaced by the compiler\n• Reordering: operations can limit reordering around themselves for operations with the ordinary variables and operations with other atomic variables in accordance with the used memory barrier _... In contrast, does not affect the order of regular variables (non-atomic / non-volatile), but the calls to all volatile variables always preserve a strict mutual order, i.e., the order of execution of any two volatile-operations cannot be changed by the compiler, but this order can be changed by the CPU/PCI-express/other device where the buffer with this volatile variable is physically located.\n• Spilling: , , memory barriers, which are specified for operations, initiate spilling of all regular variables before execution of an atomic operation. That is to say that these barriers upload the regular variables from the CPU registers into the main memory/ cache, except when the compiler can guarantee 100% that this local variable cannot be used in other threads.\n• Atomicity / alignment: For other threads see that operation has been performed entirely or not performed at all. For Integral-types , this is achieved by alignment of the atomic variable location in memory by compiler - at least, the variable is in a single cache line, so that the variable can be changed or loaded by one operation of the CPU. Conversely, the compiler does not guarantee the alignment of the volatile variables. Volatile-variables are commonly used to access the memory of devices (or in other cases), so an API of the device driver returns a pointer to volatile variables, and this API ensures alignment if necessary.\n• Atomicity of RMW-operations (read-modify-write): For operations ( ++, --, += , -= , *=, /=, CAS, exchange) are performed atomically, i.e., if two threads do operation then the -variable will always be increased by . This is achieved by locking cache-line ( ) or by marking the cache line on CPUs that support LL/SC (ARM, PowerPC) for the duration of the RMW-operation. Volatile variables do not ensure atomicity of compound RMW-operations.\n\nThere is one general rule for the variables and : each read or write operation always calls the memory/ cache, i.e., the values are never cached in the CPU registers.\n\nAlso, we note that any optimizations and any reordering of independent instructions relative to each other done by the compiler or CPU are possible for ordinary variables and objects (non-atomic/non-volatile).\n\nRecall that operations of writing to memory with atomic variables with , and memory barriers guarantee spilling (writing to the memory from the registers) of all non-atomic/non-volatile variables, which are in the CPU registers at the moment, at once: https://en.wikipedia.org/wiki/Register_allocation#Spilling.\n\nThe compiler and processor change the order of instructions to optimize the program and to improve its performance.\n\nHere is an example of optimizations that are done by the GCC compiler and CPU x86_64: https://godbolt.org/g/n91hpt.\n• GCC 7.0 compiler reordering:\n• It interchanges writing to the memory and uploading from the memory to the register . This allows you to ask for the “ ” value as early as possible, and while the CPU is waiting for this long operation, the CPU pipeline allows the execution of operation as these 2 operations do not depend on each other.\n• It combines uploading from the memory into the register and the addition operation - in the result, we have one operation add , instead of two operations.\n• x86_64 CPU reordering: The CPU can exchange the actual writing to the memory and reading from the memory, which is combined with the addition operation - .\n\nUpon initiating the writing to the memory through instruction, the following occurs: first, the value of and the address of are placed in the Store-buffer queue, the cache lines containing the address in all the CPU cores are expected to be invalidated and a response from them is being waited, then CPU-Core-0 sets the “eXclusive” status for the cache line containing . Only after that, the actual writing of the value of from the Store-buffer is carried out into this cache line at . For more details about the cache-coherency protocols on , changes in which are visible in terms of all the cores immediately, see this link.\n\nIn order not to wait all this time - immediately after “ ” is placed in the Store-Buffer, without waiting for the actual cache entry, we can start execution of the following instructions: reading from the memory or registers operations. This is what the x86_64 CPU performs:\n\nCPUs of the family have a strong memory model. And CPUs that have a weak memory model, for example, PowerPC and ARM v7/v8, can perform even more reorderings.\n\nHere are the examples of possible reordering of an entry in the memory of ordinary variables, volatile-variables and atomic variables.\n\nThis code with ordinary variables can be reordered by the compiler or by the CPU, because its meaning does not change within one thread. But within a set of threads, such reordering can affect the logic of the program.\n\nIf two variables are volatile, then the following reordering is possible. The compiler cannot reorder operations on volatile variables at compile-time, but the compiler allows the CPU to do this reordering at run-time.\n\nTo prevent all or only some of the reordering, there are atomic operations (recall that atomic operations use the most stringent memory barrier - by default).\n\nAnother thread can see the changes of the variables in the memory exactly in this modified order.\n\nIf we do not specify the memory barrier for an atomic operation, then the most strict barrier is used by default, and no atomic or non-atomic operations can be reordered with such operations (but there are exceptions that we will consider later).\n\nIn the above case, we first write to the ordinary variables and , and then - to the atomic variables and , and this order cannot be changed. Also, writing to memory cannot occur earlier than writing to memory . But writing to the variables and can be reordered relative to each other.\n\nWhen we say “can be reordered”, this means that they can, but not necessarily. It depends on how the compiler decides to optimize the C++ code during compilation, or how the CPU decides to do that at runtime.\n\nBelow, we will consider more weak memory barriers, which allow reordering the instructions in the allowed directions. This allows the compiler and CPU to better optimize the code and increase the performance.\n\nThe C++11 standard memory model provides us with 6 types of memory barriers that correspond to the capabilities of modern CPUs for speculative execution. By using them, we do not completely prohibit the change of the order, but we prohibit it only in the necessary directions. This allows the compiler and the CPU to optimize the code as much as possible. And the forbidden directions of reordering allow us to keep correctness of our code. http://en.cppreference.com/w/cpp/atomic/memory_order\n\n. Immediately, we note that we practically will not use barrier, because in the standard, there are doubts about the practicability of its usage - a quotation from the standard.\n\n. Also, we note that barrier is used only for atomic compound operations of RMW (Read-Modify-Write), such as: / , , fetch_(add, sub, and, or, xor) or their corresponding operators: http://en.cppreference.com/w/cpp/atomic/atomic\n\nThe remaining four memory barriers can be used for any operations, except for the following: “ ” is not used for , and “ ” is not used for .\n\nDepending on the chosen memory barrier, for the compiler and the CPU, it is prohibited to move the executable code relative to the barrier in different directions.\n\nNow let’s see what the arrows specify and what can be interchanged and what cannot be interchanged:\n\nFor two instructions to be interchanged, it is necessary that the barriers of both the instructions allow such an interchange. Because “other any code” means the ordinary non-atomic variables that do not have any barriers, then they allow any order changes. By the example of Relaxed-Release-Relaxed, as you can see, the possibility to change the order of the same memory barriers depends on the sequence of their appearance.\n\nLet’s consider, what do these memory barriers mean and what advantages they give us, by the example of realization of the simplest “spin-lock” type lock, which requires the most common Acquire-Release reordering semantics. Spin-lock is a lock that is similar to in terms of its way of use.\n\nFirst, we realize the spin-lock concept direct in the body of our program. And then, we realized a separate spin-lock class. To realize locks ( , ...), you should use Acquire-Release semantics, C++11 Standard § 1.10.1 (3).\n\nThe main point of the Acquire-Release semantics is that: Thread-2 after performing the operation should see all the changes to any variables/structures/classes (not even atomic ones) that have been made by Thread-1 before it executed the operation.\n\nThe basic purpose of locks ( , ...) is creation of a part of code that can be executed only by one thread at the same time. Such a code area is called the critical section. Inside it, you can use any normal code, including those without .\n\nMemory barriers prevent the compiler from optimizing the program so that no operation from the critical section goes beyond its limits.\n\nThe thread, which captures the lock first, executes this lock of the code, and the remaining threads wait in the cycle. When the first thread releases the lock, the CPU decides which next of the waiting threads will capture it, etc.\n\nHere are two identical examples:\n\nExample-1 is more preferable. Therefore, for it, we will schematically show the intent of using memory barriers - solid blue shows atomic operations:\n\nThe purpose of the barriers is very simple - the compiler optimizer is not allowed to move instructions from the critical section to the outside:\n• No instruction placed after can be executed before it.\n• No instruction placed before can be executed after it.\n\nAny other changes in the order of execution of independent instructions can be performed by the compiler (compile-time) or by the CPU (run-time) in order to optimize the performance.\n\nFor example, the line can be executed before . Such reordering is acceptable and does not create data-races, because the entire code, which accesses the data that is common for multiple threads, is always enclosed within two barriers - “ ” and “ ”. And outside, there is a code that works only with the thread local data, and it does not matter in which order it will be executed. The thread local dependencies are always stored in a way similar to that of single-threaded execution. That is why, int cannot be executed before\n\nWhat exactly is the compiler doing in :\n• 1, 6: The compiler generates the assembler instructions acquire-barrier for the load operation and the release-barrier for the store operation, if these barriers are necessary for the given CPU architecture\n• 2: The compiler cancels the previous caching of variables in the CPU registers in order to reload the values ​​of these variables changed by another thread - after the ( ) operation\n• 5: The compiler saves the values ​​of all variables from the CPU registers to the memory so that they can be seen by other threads, i.e., it executes spilling (link) - up to store(release)\n• 3, 4: The compiler prevents the optimizer from changing the order of the instructions in the forbidden directions - indicated by red arrows\n\nAnd now let’s see what happens, if we use Relaxed-Release semantics instead of Acquire-Release:\n• On the left. In case of Acquire-Release, everything is executed correctly.\n• On the right. In case of Relaxed-Release, part of the code inside the critical section, which is protected by a lock, can be moved outside by the compiler or CPU. Then, there will be a problem with Data Races - a lot of threads will start to work simultaneously with the data, which are not processed by atomic operations, before locking.\n\nNote that it is usually impossible to implement all logic in relation to general data only with the help of atomic operations, since there are very few of them and they are rather slow. Therefore, it is simpler and faster to do the following actions: to set the flag “closed” by means of one atomic operation, to perform all non-atomic operations in relation to the data common to the threads, and to set the flag “open”.\n\nWe will show this process schematically in time:\n\nFor example, two threads start executing the function:\n• Thread-1 comes a little earlier and performs 2 operations at a time by means of one atomic instruction : it performs checkup and , then it sets the “ ” value to it (that is, locks spin-lock) and returns “ ”. Therefore, the expression ends immediately and the code of the critical section starts to be executed.\n• At this point, thread-2 also starts executing this atomic instruction : it performs checkup and , then it sets the “ ” value. Otherwise, it does not change anything and returns the current “ ” value. Therefore, the expression will be executed until,\n• Thread-1 performs the addition operation and then sets the value by means of an atomic operation (that is, unlocks spin-lock).\n• Finally, thread-2, after finishing waiting for the condition , assigns , returns “ ” and completes the cycle. Then it performs the addition of and assigns (unlocks spin-lock).\n\nAt the beginning of this chapter, we gave 2 examples:\n• by using and test_and_set():[21] http://coliru.stacked-crooked.com/a/1ec9a0c2b10ce864\n• by using and :[22] http://coliru.stacked-crooked.com/a/03c019596b65199a\n\nFor more details, see: http://en.cppreference.com/w/cpp/atomic/atomic\n\nLet’s see how the assembler code for x86_64, which is generated by GCC compiler, differs from these two examples:\n\nTo be convenient in use, it can be combined into one class:\n\nExample of usage of : [23] http://coliru.stacked-crooked.com/a/92b8b9a89115f080\n\nThe following table will help you to understand which barrier you should use and what it will be compiled into:\n\nYou should necessarily know the following information, only if you write a code in assembler , when the compiler cannot interchange the assembler instructions for optimization:\n• . The main difference (Clang and MSVC) from GCC is when you use the Store operation for the Sequential Consistency semantics, namely: ( , ); - in this case, Clang and MSVC generate the [LOCK] XCHG reg, [addr] instruction, which cleans the CPU-Store-buffer in the same way as the barrier does. And GCC in this case uses two instructions , and .\n• . As all atomic RMW (Read-Modify-Write) instructions on x86_64 have the prefix, which cleans the Store-Buffer, they all correspond to the Sequential-Consistency semantics at the assembler code level. Any for RMW generate an identical code, including .\n• . As you can see, on x86_64, the first 4 memory barriers ( , , , ) generate an identical assembler code - i.e., x86_64 architecture provides the acquire-release semantics automatically. Besides, it is provided by the MESIF (Intel) / MOESI (AMD) cache coherency protocols. This is only for the memory allocated by the usual compiler tools, which is marked by default as Write Back (but it’s not true for the memory marked as Un-cacheable or Write Combined, which is used for work with the Mapped Memory Area from Device - only Acquire- Semantic is automatically provided in it).\n\nAs we know, dependent operations cannot ever be reordered anywhere, for example: (Read-X, Write-X) or (Write-X, Read-X)\n• Slides from speech of Herb Sutter for x86_64: https://onedrive.live.com/view.aspx?resid=4E86B0CF20EF15AD!24884&app=WordPdf&authkey=!AMtj_EflYn2507c In x86_64, any of the following cannot be reordered:\n• In x86_64, any of the following can be reordered: Write-X <–> Read-Y. To prevent it, the barrier, which can generate four options of a code in x86_64, depending on the compiler, is used: http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html\n\nA summary table of correspondence between the memory barriers and the CPU instructions for architectures (x86_64, PowerPC, ARMv7, ARMv8, AArch64, Itanium): http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html\n\nYou can have a look at a real assembler code for different compilers by clicking the following links. And also, you can choose other architectures: ARM, ARM64, AVR, PowerPC.\n\nBesides, we will demonstrate briefly in a table, what effect the various memory barriers have on the order in relation to CAS (Compare-and-swap) - instructions: http://en.cppreference.com/w/cpp/atomic/atomic/compare_exchange\n\nNow we’ll show more complex examples of reordering from four consecutive operations: , , , .\n• Dark blue rectangles inside light blue ones (in examples 3 and 4) are parts of the compound atomic Read-Modify-Write (RMW) instructions that consist of several operations.\n\nWe’ll give four examples. Each example will demonstrate possible reordering of operations with ordinary variables around the operations with atomic variables. But only in examples 1 and 3, some reordering of atomic operations among themselves is also possible.\n\n1st case is interesting in that several critical sections can be fused into one.\n\nThe compiler cannot perform such a reordering at compile-time, but the compiler allows the CPU to do this reordering at run-time. Therefore, the fusion of critical sections that run in different sequences in different threads can not lead to a deadlock, because the initial sequence of instructions will be visible to the processor.\n\n Therefore, the processor will try to enter into the second critical section in advance, but if it cannot, it will continue completing the first critical section.\n\n3rd case is interesting in that the parts of two compound atomic instructions can be reordered:\n• This is possible with PowerPC architecture, where compound operations consist of 3 separate ASM instructions ( , , ), and their atomicity is achieved by means of LL / SC technique (wiki-link); for the assembler code, see: godbolt.org/g/j8uw7n The compiler cannot perform such a reordering at compile-time, but the compiler allows the CPU to do this reordering at run-time.\n\n2nd case is interesting in that is the strongest barrier and would seem to prohibit any reordering of atomic or non-atomic operations around itself. But have only one additional property compared to acquire / release - only if both atomic operations have a , then the sequence of operations STORE-A ( ); LOAD-B ( ); cannot be reordered. Here are two quotes from C++ standard:\n\n1. Strict mutual order of execution is retained only for operations with the barrier , Standard C++11 § 29.3 (3): http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/n4606.pdf\n\n2. If an atomic operation has a barrier different from , then it can be reordered with operations in allowed directions, Standard C++11 § 29.3 (8): http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/n4606.pdf\n\nThe directions allowed for reordering atomic operations (atomic and non-atomic) around atomic operations are the same as for acquire / release:\n• – allows the same reordering as for other non-seq_cst atomic operations\n• – allows the same reordering as for other non-seq_cst atomic operations\n\nA stricter order is possible, but is not guaranteed.\n\nIn case of , as well as for acquire and release, nothing going to can be executed after it, and nothing going after can be executed before it.\n\nBut in the opposite direction, reordering is possible, examples of C++ code and generated code on Assembler x86_64 and PowerPC.\n\nNow we’ll show the changes in the instructions order that are permitted for CPUs by the compilers, for example, GCC for x86_64 and PowerPC.\n\nThe following order changes around memory_order_seq_cst operations are possible:\n• On x86_64, at the CPU level, the Store-Load reordering is permitted. That is to say, the following sequence can be reordered: ⇒ , as in x86_64 architecture, is added only after , but for , the assembler instructions are identical for and : https://godbolt.org/g/BsLqas\n• On PowerPC, at the CPU level, Store-Load, Store-Store reordering and others are permitted. That is to say, the following sequence can be reordered: ⇒ , as on PowerPC architecture, for , is added only before and before . Thus, all the instructions that are between and can be executed before : https://godbolt.org/g/dm7tWd\n\nFor more detail, see the following example with three variables with semantics: and relaxed.\n• C++ to ASM for PowerPC: https://godbolt.org/g/mEM8T8\n• C++ to ASM for x86_64: https://godbolt.org/g/Tft2vw\n\nWhy are such changes of order possible? Because C++ compiler generates the assembler code, which allows you to do such reordering in relation to x86_64 and PowerPC CPUs:\n\nReordering permitted by different CPUs in the absence of assembler-barriers between the instructions\n\nThere is one more feature of the data exchange between the threads, which is manifested upon interaction of four threads or more. If at least one of the following operations does not use the most stringent barrier , then different threads can see the same changes in different order. For example:\n• If thread-1 changed some value first\n• And thread-2 changed some value second\n• Then thread-3 can first see the changes made by thread-2, and only after that it will see the changes made by thread-1\n• And thread-4, on the contrary, can first see the changes made by thread-1, and only after that, it will see the changes made by thread-2\n\nThis is possible because of the hardware features of the cache-coherent protocol and the topology of location of the cores in the CPUs. In this case, some two cores can see the changes made by each other before they see the changes made by other cores. In order that all threads could see the changes in the same order, i.e., they would have a single total order (C++ 11 § 29.3 (3)), it is necessary that all operations ( , , ) would be performed with the memory barrier : http://en.cppreference.com/w/cpp/atomic/memory_order\n\nIn the following example, the program never finish with error; Since all operations use the most stringent memory barrier - : http://coliru.stacked-crooked.com/a/52726a5ad01f6529\n\nThis code in the online compiler taken by the link: http://en.cppreference.com/w/cpp/atomic/memory_order\n\nIn the figure, we’ll show what order change is possible for Acquire-Release semantics and for Sequential semantics by the example of 4 threads:\n• Acquire-Release\n• It is possible to change the order of ordinary variables in the allowed directions\n• It is possible to change the order of atomic variables with Acquire-Release semantics\n• Sequential\n• It is possible to change the order of ordinary variables in the allowed directions\n• It is to change the order of atomic variables with Sequential semantics\n\nWe will show how the memory barriers are applied for atomic operations by the example of realization of own active locks: spin-lock and recursive-spin-lock.\n\nFurther, we will need these locks for locking separate rows (row-lock) in the table instead of locking the whole table (table-lock). This will increase the degree of parallelism and increase the performance, because different threads can work with different rows in parallel, without locking the whole table.\n\nThe number of synchronization objects provided by the operating system can be limited. The number of rows in a table can be equal to millions or billions, many operating systems do not allow you to create so many mutexes. And the amount of spin-locks can be any, as far as RAM allows it. That is why they can be used for locking every single row.\n\nIn fact, is +1 byte to each row, or +17 bytes upon using recursive-spin-lock (1 byte for the flag + 8 bytes for the recursion counter + 8 bytes for the thread number).\n• We have already given an example of realization and use of : [24] http://coliru.stacked-crooked.com/a/92b8b9a89115f080\n• Now we will give an example of realization and use of : [25] http://coliru.stacked-crooked.com/a/eae6b7da971810a3\n\nThe main difference between and ordinary spinlock is that can be repeatedly locked by the same thread, i.e., it supports recursive nested locks. Similarly, is different from .\n• – eternal deadlock occurs – does not withdraw the result: [26] http://coliru.stacked-crooked.com/a/d3b93315270fd367\n\nIf you try to run this code in the MSVC 2013 compiler, you will get a very strong slowdown due to the function: https://connect.microsoft.com/VisualStudio/feedback/details/1558211\n\nWe will modify the class so that it caches the thread-id in the variable (thread) - this is the analog of the C++11 standard. This example will show good performance in MSVC 2013: [28] http://coliru.stacked-crooked.com/a/3090a9778d02f6ea\n\nThis is a patch for the old MSVC 2013, so we will not think about the beauty of this solution.\n\nIt is believed that in most cases, the recursive locking of a mutex is a design error, but in our case, this can be a slow but working code. Secondly, all can be mistaken, and with the nested recursive locks, will work much slower, and will hang forever - which is better, it’s up to you.\n\nIn case of using our thread-safe pointer , both examples are quite logical, but the second one will work only with :\n• // can be used: &\n\nIt is well-known that new types of mutexes gradually appeared in new C++ standards: http://en.cppreference.com/w/cpp/thread\n\nis a that allows many threads to read the same data simultaneously, if at that time, there are no threads that change these data. was not created in one day. There were disputes about its performance in comparison with the usual .\n\nThe classic realization of with a count of the number of readers showed an advantage in speed only when many readers held the lock for a long time - i.e., when they had been reading for a long time. With short reads, only slowed down the program and complicated the code.\n\nBut are all the realizations so slow in short reads?\n\nThe main reason for slow operation of and is the atomic counter of readers. Each reader thread increments the counter during locking and decrements it upon unlocking. As a result, threads constantly drive one cache line between the cores (namely, drive its exclusive-state (E)). According to the logic of this realization, each reader thread counts the number of readers at the moment, but this is absolutely not important for the reader thread, because it is important only that there is not a single writer. And, since the increment and decrement are RMW operations, they always generate Store-Buffer cleaning (MFENCE x86_64) and, at x86_64 level, asm actually correspond to the slowest semantics of the Sequential Consistency.\n\nLet’s try to solve this problem.\n\nThere is a type of algorithm that is classified as write contention-free - when there is not a single memory cell in which it would be possible to write more than one thread. In a more general case, there is not a single cache line in which it would be possible to write more than one thread. In order to have our shared-mutex be classified as write contention-free only in the presence of readers, it is necessary that readers do not interfere with each other - i.e., each reader should write a flag (which is read by it) to its own cell and remove the flag in the same cell at the end of reading - without RMW operations.\n\nWrite contention-free is the most productive guarantee, which is more productive than wait-free or lock-free.\n\nIt is possible that each cell is located in a separate cache line to exclude false-sharing, and it is possible that cells lie tightly - 16 in one cache line - the performance loss will depend on the CPU and the number of threads.\n\nBefore setting the flag, the reader checks if there is a writer - i.e., if there is an exclusive lock. And since shared-mutex is used in cases where there are very few writers, then all the used cores can have a copy of this value in their cache-L1 in shared-state (S), where they will receive the value of the writer’s flag for 3 cycles, until it changes.\n\nFor all writers, as usually, there is the same flag - it means that there is a writer at the moment. The writer threads set and remove it by using RMW-operations.\n\nBut to make the readers not interfere with each other, they shall write information about their shared-locks in different memory cells. We will allocate an array for these locks, the size of which we will set with the default template parameter of 20. At the first call, the thread will be automatically registered, taking a certain place in this array. If the number of threads is larger than the size of the array, then the remaining threads, upon calling the , will call an exclusive lock of the writer. The threads are rarely created, and the time spent by the operating system for their creation is so long that the time of registration of a new thread in our object will be negligible.\n\nWe will make sure that there are no obvious errors - by examples, we will show that everything is working correctly, and after that, we will demonstrate schematically the correctness of our approach.\n\nHere is an example of such a fast shared lock in which readers do not interfere with each other: [30] http://coliru.stacked-crooked.com/a/b78467b7a3885e5b\n• None During the shared lock, there can be no object changes . This line of two recursive shared-locks shows this: - the values ​​of both lines should always be equal, because we change 2 lines in under one eXclusive-lock .\n• None During reading, we really call the lock_shared() function. Let's reduce the cycle to two iterations, remove the lines modifying the data and leave only the first two inserts in in the function. Now we add the output of S letter to the function, and X letter - to the function. We see that there are two X inserts going first, and only after that we see S - indeed, when reading const-objects, we call : [31] http://coliru.stacked-crooked.com/a/515ba092a46135ae\n• None During the changes, we really call the lock() function . Now we’ll comment out the reading and leave only the operations of changing the array. Now only X letters are displayed: [32] http://coliru.stacked-crooked.com/a/882eb908b22c98d6\n\nThe main task is to ensure that there can only be one of the two states at the same time:\n• Any number of threads successfully executed . Thereat, all threads trying to execute should change to standby mode.\n• One of the threads successfully executed . Thereat, all other threads trying to execute or should change to standby mode.\n\nSchematically, the table of compatible states looks like below:\n\nLet’s consider the algorithm of our for the cases when two threads are trying to perform operations at a time:\n• T1-read & T2-read: The reader threads lock the mutex by using - these threads do not interfere with each other, because they write states about locks in the memory cells separate for each thread. And at this time, there should not be an exclusive locking of the writer thread ( ). Except when the number of threads is bigger than that of the dedicated cells. In such a case, even the reader threads are locked exclusively by using the CAS-function: .\n• T1-write & T2-write: The writer threads compete with each other for the same flag ( ) and try to set it to “ ” by using the atomic CAS-function: Here, everything is simple, as in the usual , which we have discussed above.\n• T1-read & T2-write: The reader thread T1 writes the lock flag to its cell, and only after that, it checks if the flag ( ) is set. If it is set ( ), then it cancels its lock and after that, waits for the state ( ) and repeats this algorithm from the beginning.\n\nThe writer thread sets the flag ( ), and then waits until all reader threads remove the locking flags from their cells.\n\nThe writer thread has a higher priority than the reader. And if they simultaneously set their locking flags, then the reader thread, by means of the following operation, checks if there is a writer thread ( ). If there is a writer thread, then the reader cancels its lock. The writer thread sees that there are no more locks from the readers and successfully completes the locking function. The global order of these locks is met due to the semantics of Sequential Consistency ( ).\n\nBelow, the interaction of two threads (the reader and the writer) is shown schematically.\n\nIn both functions, and , for both marked operations 1 and 2 used - i.e., for these operations, there shall be a single total order for all threads.\n\nAutomatic Deregistration of the Thread in Cont-Free Shared-Mutex\n\nWhen a thread first accesses our lock, it is registered. And when this thread is terminated or the lock is deleted, then registration should be canceled.\n\nBut now let’s see what happens if 20 threads work with our mutex and then these threads terminate and try to register new 20 threads, provided that the array of locks is 20: [33] http://coliru.stacked-crooked.com/a/f1ac55beedd31966\n\nAs you can see, the first 20 threads were successfully registered, but the following 20 threads could not register ( ) and had to use the writer’s exclusive lock, despite the fact that the previous 20 threads had already disappeared and no longer use the lock.\n\nTo solve this problem, we add automatic cancellation of the thread registration when the thread is deleted. If the thread has worked with a lot of such locks, then at the time of thread termination, registration should be canceled in all such locks. And if, at the time of thread removal, there are locks, which are already deleted, no error is supposed to occur due to an attempt to cancel the registration in a non-existent lock.\n\nAs you can see, first, 20 threads were registered. After they were removed and 20 new threads were created, they also could register - under the same numbers (see the example output).\n\nNow we’ll show that even if the threads have worked with locking, and then the lock was removed, then upon termination of the threads, there will be no error due to an attempt to cancel the registration in a non-existent lock object: [35] http://coliru.stacked-crooked.com/A/d2e5a4ba1cd787da\n\nWe set the timers so that 20 threads could be created, perform reading by using our lock and fall asleep at 500ms; and at that time, in 100ms, the object, which contains our lock inside, could be removed; and only after that 20 threads could wake up and terminate. Upon their termination, there was no error due to deregistering in a remote lock object.\n\nAs a small addition, we will support MSVS2013 so that the owners of the old compiler can also see the example. We add simplified support for registering the threads, but without the possibility of deregistration of threads.\n\nThe final result will be shown as an example in which all the above thoughts are taken into account.\n\nThe Correct Functioning of the Algorithm and Selected Barriers\n\nAbove we have performed tests that showed no obvious errors. But in order to prove the working capacity, it is necessary to create a scheme of possible changes in the order of operations and possible states of variables.\n\nExclusive / is almost as simple as in the case of . So, we will not consider it in detail.\n\nThe reader thread competition for and the writer thread competition for has been discussed in detail above.\n\nNow the main task is to show that uses at least Acquire-semantic in all cases; and uses at least Release-semantic in all cases. But this is not a prerequisite for repeated recursive locking/unlocking.\n\nNow let’s see how these barriers are implemented in our code.\n\nThe barriers for are shown schematically. The red crossed arrows indicate the directions, in which the change of the order is forbidden:\n\nWe also show a block diagram of the same function :\n\nIn the oval blocks, strict sequences of operations are indicated:\n\nGreen color indicates the changes that can be executed in any order, because these changes do not lock our “shared-mutex”, but the only increase the recursion nesting count - these changes are important only for local use. That is to say, these operations are not the actual entrance to a lock.\n\nSince 2 conditions are executed, it is assumed that all the necessary side-effects of the multithreading are taken into account:\n• The moment of making a decision to enter a lock always has semantics of at least “acquire” level:\n• First, we always lock (1-red) and only after that we check (2-purple), whether we can enter the lock. Further, the correctness of the algorithm can be verified by a simple comparison of these operations and their sequence with the logic of the algorithm. All other functions of our lock “ ” are more obvious from the point of view of the logic of multi-threaded execution. Also, in recursive locks, atomic operations do not need to have a barrier (as is done in the figure), there is enough to set the , because this is not the actual entrance to a lock - we are already in the lock. But this does not add much speed and can complicate understanding.\n\nHow to Use\n\nAn example of how to use in C++ as highly optimized .\n• Download this code for Linux (GCC 6.3.0) and Windows (MSVS 2015/13): contfree_shared_mutex.zip - 8.4 KB\n\nTo compile in compiler Clang++ 3.8.0 on Linux, you should change Makefile.\n\nThis code is in the online compiler: http://coliru.stacked-crooked.com/a/11c191b06aeb5fb6\n\nAs you see, our is used in the same way as a standard .\n\nHere is an example of testing on 16 threads for a single server CPU Intel Xeon E5-2660 v3 2.6 GHz. First of all, we are interested in the blue and purple lines:\n\nYou can download this benchmark for Linux (GCC 6.3.0) and Windows (MSVS 2015): bench_contfree.zip - 8.5 KB\n\nTo compile in compiler Clang++ 3.8.0 on Linux, you should change Makefile.\n\nThe performance of various locks for different ratios of lock types: shared-lock and exclusive-lock:\n\nPerformance (the bigger – the better)\n• For 0% of modifications – our shared-mutex (as part of «contfree_safe_ptr<map>») shows performance 34.60 Mops, that is 14 times faster than standard (as part of ) that shows only 2.44 Mops.\n• For 15% of modifications – our shared-mutex (as part of ) shows performance 5.16 Mops, that is 7 times faster than standard (as part of ) that shows only 0.74 Mops.\n\nOur lock «contention-free shared-mutex» works for any number of threads: for the first 36 threads as contention-free, and for the next threads as exclusive-lock. As can be seen from the graphs – even exclusive-lock « » works faster than « » for 15% of modifications.\n\nThe number of threads 36 for the contention-free-locks is specified by the template parameter, and can be changed.\n\nNow we will show the median latency for different ratios of lock types: shared-lock and exclusive-lock.\n\nIn the test code main.cpp, you should to set:\n\nMedian-latency (the lower – the better), microseconds\n\nThus, we created a shared lock, in which the readers do not interfere with each other during locking and unlocking, unlike and . But we have the following additional allocations for each thread: 64 bytes in the locks array + 24 bytes are occupied by the for deregistration + an element pointing to this structure from the . Totally, it is about ~100 bytes per thread.\n\nA more serious problem is the scaling feature. For example, if you have 8 CPUs (Intel® Xeon® Processor E7-8890 v4) with 24 cores each (48 HyperThreading threads each), then this is a total of 384 logical cores. Each writer thread must read 24576 bytes (64 bytes from each of 384 cores) before writing. But they can by read in parallel. It's certainly better than waiting until one cache line consistently goes from each of 384 threads to each one, like in ordinary and (for any type of unique/shared lock). But paralleling by 1000 cores and more is usually realized by a different approach, rather than by calling an atomic operation to process each message. All the options discussed above - atomic operations, active locks, lock-free data structures - are necessary for small latencies (0.5-10 usec) of individual messages.\n\nFor high indicators of the number of operations per second (i.е., for high system performance in general and scalability for tens of thousands of logical cores), they use mass parallelism approaches, “hide latency” and “batch processing” - batch processing when messages are sorted (for map) or grouped (for ) and fused with the already available sorted or grouped array for 50 - 500 usec. As a result, each operation has ten-hundredfold latency, but this latency occurs in a great number of threads at a time. As a result, hide latency occurs because of using “Fine-grained Temporal multithreading”.\n\nLet’s assume: each message has hundredfold latency, but the number of messages being processed is 10,000 times more. This is hundredfold more efficient per time unit. Such principles are used when developing on the GPU. Perhaps, in the following articles, we will discuss this in more detail.\n\nConclusion: We have developed our own “shared-mutex”, which does not require that the reader threads synchronize with each other, as it is seen in the standard . We have strictly proved the correctness of the work of our “ ”. And also we studied atomic operations, memory barriers and allowed reordering directions for maximum optimization of performance in detail. Next, we’ll see how much we were able to increase the performance of multithreaded programs, compared to the standard ."
    },
    {
        "link": "https://boost.org/doc/libs/1_60_0/doc/html/thread/changes.html",
        "document": "...one of the most highly regarded and expertly designed C++ library projects in the world. — Herb Sutter and Andrei Alexandrescu , C++ Coding Standards\n\nPlease take a look at thread Know Bugs to see the current state.\n\nPlease take a look at thread trunk regression test to see the last regression test snapshot.\n\nPlease take a look at thread Know Bugs to see the current state.\n\nPlease take a look at thread trunk regression test to see the last regression test snapshot.\n\nThere are some severe bugs that prevent the use of the library on concrete contexts, in particular:\n\nPlease take a look at thread Know Bugs to see the current state.\n\nPlease take a look at thread trunk regression test to see the last snapshot.\n\nThere are some severe bugs that prevent the use of the library on concrete contexts, in particular:\n\nPlease take a look at thread Know Bugs to see the current state.\n\nPlease take a look at thread trunk regression test to see the last snapshot.\n\nThere are some severe bugs that prevent the use of the library on concrete contexts, in particular:\n\nPlease take a look at thread trunk regression test to see the current state.\n\nThere are some severe bugs that prevent the use of the library on concrete contexts, in particular:\n\nDeprecated features since boost 1.50 available only until boost 1.55:\n\nThese deprecated features will be provided by default up to boost 1.52. If you don't want to include the deprecated features you could define BOOST_THREAD_DONT_PROVIDE_DEPRECATED_FEATURES_SINCE_V3_0_0. Since 1.53 these features will not be included any more by default. Since this version, if you want to include the deprecated features yet you could define BOOST_THREAD_PROVIDE_DEPRECATED_FEATURES_SINCE_V3_0_0. These deprecated features will be only available until boost 1.55, that is you have yet 1 year to move to the new features.\n\nBreaking changes when BOOST_THREAD_VERSION==3 (Default value since Boost 1.53):\n\nThere are some new features which share the same interface but with different behavior. These breaking features are provided by default when BOOST_THREAD_VERSION is 3, but the user can however choose the version 2 behavior by defining the corresponding macro. As for the deprecated features, these broken features will be only available until boost 1.55.\n\nDeprecated features since boost 1.50 available only until boost 1.55:\n\nThese deprecated features will be provided by default up to boost 1.52. If you don't want to include the deprecated features you could define BOOST_THREAD_DONT_PROVIDE_DEPRECATED_FEATURES_SINCE_V3_0_0. Since 1.53 these features will not be included any more by default. Since this version, if you want to include the deprecated features yet you could define BOOST_THREAD_PROVIDE_DEPRECATED_FEATURES_SINCE_V3_0_0. These deprecated features will be only available until boost 1.55, that is you have 1 year and a half to move to the new features.\n\nThere are some new features which share the same interface but with different behavior. These breaking features are provided by default when BOOST_THREAD_VERSION is 3, but the user can however choose the version 2 behavior by defining the corresponding macro. As for the deprecated features, these broken features will be only available until boost 1.55.\n\nThe 1.41.0 release of Boost adds futures to the thread library. There are also a few minor changes.\n\nThe 1.36.0 release of Boost includes a few new features in the thread library:\n\nAlmost every line of code in Boost.Thread has been changed since the 1.34 release of boost. However, most of the interface changes have been extensions, so the new code is largely backwards-compatible with the old code. The new features and breaking changes are described below.\n\nThe list below should cover all changes to the public interface which break backwards compatibility."
    },
    {
        "link": "https://boost.org/doc/libs/1_63_0/doc/html/thread/changes.html",
        "document": "...one of the most highly regarded and expertly designed C++ library projects in the world. — Herb Sutter and Andrei Alexandrescu , C++ Coding Standards\n\nPlease define BOOST_THREAD_PATCH to apply the patch that could unfortunately results is a regression as described in #12049.\n\nPlease take a look at thread Know Bugs to see the current state.\n\nPlease take a look at thread trunk regression test to see the last regression test snapshot.\n\nPlease define BOOST_THREAD_PATCH to apply the patch that could unfortunately results is a regression as described in #12049.\n\nPlease take a look at thread Know Bugs to see the current state.\n\nPlease take a look at thread trunk regression test to see the last regression test snapshot.\n\nPlease take a look at thread Know Bugs to see the current state.\n\nPlease take a look at thread trunk regression test to see the last regression test snapshot.\n\nPlease take a look at thread Know Bugs to see the current state.\n\nPlease take a look at thread trunk regression test to see the last regression test snapshot.\n\nThere are some severe bugs that prevent the use of the library on concrete contexts, in particular:\n\nPlease take a look at thread Know Bugs to see the current state.\n\nPlease take a look at thread trunk regression test to see the last snapshot.\n\nThere are some severe bugs that prevent the use of the library on concrete contexts, in particular:\n\nPlease take a look at thread Know Bugs to see the current state.\n\nPlease take a look at thread trunk regression test to see the last snapshot.\n\nThere are some severe bugs that prevent the use of the library on concrete contexts, in particular:\n\nPlease take a look at thread trunk regression test to see the current state.\n\nThere are some severe bugs that prevent the use of the library on concrete contexts, in particular:\n\nDeprecated features since boost 1.50 available only until boost 1.55:\n\nThese deprecated features will be provided by default up to boost 1.52. If you don't want to include the deprecated features you could define BOOST_THREAD_DONT_PROVIDE_DEPRECATED_FEATURES_SINCE_V3_0_0. Since 1.53 these features will not be included any more by default. Since this version, if you want to include the deprecated features yet you could define BOOST_THREAD_PROVIDE_DEPRECATED_FEATURES_SINCE_V3_0_0. These deprecated features will be only available until boost 1.55, that is you have yet 1 year to move to the new features.\n\nBreaking changes when BOOST_THREAD_VERSION==3 (Default value since Boost 1.53):\n\nThere are some new features which share the same interface but with different behavior. These breaking features are provided by default when BOOST_THREAD_VERSION is 3, but the user can however choose the version 2 behavior by defining the corresponding macro. As for the deprecated features, these broken features will be only available until boost 1.55.\n\nDeprecated features since boost 1.50 available only until boost 1.55:\n\nThese deprecated features will be provided by default up to boost 1.52. If you don't want to include the deprecated features you could define BOOST_THREAD_DONT_PROVIDE_DEPRECATED_FEATURES_SINCE_V3_0_0. Since 1.53 these features will not be included any more by default. Since this version, if you want to include the deprecated features yet you could define BOOST_THREAD_PROVIDE_DEPRECATED_FEATURES_SINCE_V3_0_0. These deprecated features will be only available until boost 1.55, that is you have 1 year and a half to move to the new features.\n\nThere are some new features which share the same interface but with different behavior. These breaking features are provided by default when BOOST_THREAD_VERSION is 3, but the user can however choose the version 2 behavior by defining the corresponding macro. As for the deprecated features, these broken features will be only available until boost 1.55.\n\nThe 1.41.0 release of Boost adds futures to the thread library. There are also a few minor changes.\n\nThe 1.36.0 release of Boost includes a few new features in the thread library:\n\nAlmost every line of code in Boost.Thread has been changed since the 1.34 release of boost. However, most of the interface changes have been extensions, so the new code is largely backwards-compatible with the old code. The new features and breaking changes are described below.\n\nThe list below should cover all changes to the public interface which break backwards compatibility."
    }
]