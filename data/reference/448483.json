[
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html",
        "document": "Applies an affine linear transformation to the incoming data: y=xAT+b.\n\nOn certain ROCm devices, when using float16 inputs this module will use different precision for backward.\n• None bias (bool) – If set to , the layer will not learn an additive bias. Default:\n• None Input: (∗,Hin​) where ∗ means any number of dimensions including none and Hin​=in_features.\n• None Output: (∗,Hout​) where all but the last dimension are the same shape as the input and Hout​=out_features.\n• None weight (torch.Tensor) – the learnable weights of the module of shape (out_features,in_features). The values are initialized from U(−k ​,k ​), where k=in_features1​\n• None bias – the learnable bias of the module of shape (out_features). If is , the values are initialized from U(−k ​,k ​) where k=in_features1​"
    },
    {
        "link": "https://pytorch.org/docs/stable/nn.html",
        "document": "These are the basic building blocks for graphs:\n\nApplies a 1D convolution over an input signal composed of several input planes. Applies a 2D convolution over an input signal composed of several input planes. Applies a 3D convolution over an input signal composed of several input planes. Applies a 1D transposed convolution operator over an input image composed of several input planes. Applies a 2D transposed convolution operator over an input image composed of several input planes. Applies a 3D transposed convolution operator over an input image composed of several input planes. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. Combines an array of sliding local blocks into a large containing tensor.\n\nApplies a 1D max pooling over an input signal composed of several input planes. Applies a 2D max pooling over an input signal composed of several input planes. Applies a 3D max pooling over an input signal composed of several input planes. Applies a 1D average pooling over an input signal composed of several input planes. Applies a 2D average pooling over an input signal composed of several input planes. Applies a 3D average pooling over an input signal composed of several input planes. Applies a 2D fractional max pooling over an input signal composed of several input planes. Applies a 3D fractional max pooling over an input signal composed of several input planes. Applies a 1D power-average pooling over an input signal composed of several input planes. Applies a 2D power-average pooling over an input signal composed of several input planes. Applies a 3D power-average pooling over an input signal composed of several input planes. Applies a 1D adaptive max pooling over an input signal composed of several input planes. Applies a 2D adaptive max pooling over an input signal composed of several input planes. Applies a 3D adaptive max pooling over an input signal composed of several input planes. Applies a 1D adaptive average pooling over an input signal composed of several input planes. Applies a 2D adaptive average pooling over an input signal composed of several input planes. Applies a 3D adaptive average pooling over an input signal composed of several input planes.\n\nPads the input tensor using the reflection of the input boundary. Pads the input tensor using the reflection of the input boundary. Pads the input tensor using the reflection of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with a constant value. Pads the input tensor boundaries with a constant value. Pads the input tensor boundaries with a constant value. Pads the input tensor using circular padding of the input boundary. Pads the input tensor using circular padding of the input boundary. Pads the input tensor using circular padding of the input boundary.\n\nCreates a criterion that measures the mean absolute error (MAE) between each element in the input x and target y. Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input x and target y. This criterion computes the cross entropy loss between input logits and target. Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities: This loss combines a layer and the in one single class. Creates a criterion that measures the loss given inputs x1, x2, two 1D mini-batch or 0D , and a label 1D mini-batch or 0D y (containing 1 or -1). Measures the loss given an input tensor x and a labels tensor y (containing 1 or -1). Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input x (a 2D mini-batch ) and output y (which is a 2D of target class indices). Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. Creates a criterion that optimizes a two-class classification logistic loss between input tensor x and target tensor y (containing 1 or -1). Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input x and target y of size (N,C). Creates a criterion that measures the loss given input tensors x1​, x2​ and a label y with values 1 or -1. Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input x (a 2D mini-batch ) and output y (which is a 1D tensor of target class indices, 0≤y≤x.size(1)−1): Creates a criterion that measures the triplet loss given an input tensors x1, x2, x3 and a margin with a value greater than 0. Creates a criterion that measures the triplet loss given input tensors a, p, and n (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\"distance function\") used to compute the relationship between the anchor and positive example (\"positive distance\") and the anchor and negative example (\"negative distance\").\n\nClip the gradient norm of an iterable of parameters. Clip the gradient norm of an iterable of parameters. Clip the gradients of an iterable of parameters at specified value. Compute the norm of an iterable of tensors. Scale the gradients of an iterable of parameters given a pre-calculated total norm and desired max norm. Utility functions to flatten and unflatten Module parameters to and from a single vector. Flatten an iterable of parameters into a single vector. Copy slices of a vector into an iterable of parameters. Fuse a convolutional module and a BatchNorm module into a single, new convolutional module. Fuse convolutional module parameters and BatchNorm module parameters into new convolutional module parameters. Fuse a linear module and a BatchNorm module into a single, new linear module. Fuse linear module parameters and BatchNorm module parameters into new linear module parameters. Convert of to The conversion recursively applies to nested , including . Utility functions to apply and remove weight normalization from Module parameters. Apply weight normalization to a parameter in the given module. Apply spectral normalization to a parameter in the given module. Given a module class object and args / kwargs, instantiate the module without initializing parameters / buffers. Abstract base class for creation of new pruning techniques. Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. Prune (currently unpruned) units in a tensor at random. Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. Prune entire (currently unpruned) channels in a tensor at random. Prune entire (currently unpruned) channels in a tensor based on their L -norm. Prune tensor by removing units with the lowest L1-norm. Prune tensor by removing random channels along the specified dimension. Prune tensor by removing channels with the lowest L -norm along the specified dimension. Globally prunes tensors corresponding to all parameters in by applying the specified . Prune tensor corresponding to parameter called in by applying the pre-computed mask in . Remove the pruning reparameterization from a module and the pruning method from the forward hook. Check if a module is pruned by looking for pruning pre-hooks. Parametrizations implemented using the new parametrization functionality in . Apply an orthogonal or unitary parametrization to a matrix or a batch of matrices. Apply weight normalization to a parameter in the given module. Apply spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules. Note that these functions can be used to parametrize a given Parameter or Buffer given a specific function that maps from an input space to the parametrized space. They are not parameterizations that would transform an object into a parameter. See the Parametrizations tutorial for more information on how to implement your own parametrizations. Remove the parametrizations on a tensor in a module. Context manager that enables the caching system within parametrizations registered with . A sequential container that holds and manages the original parameters or buffers of a parametrized . Utility functions to call a given Module in a stateless manner. Perform a functional call on the module by replacing the module parameters and buffers with the provided ones. Holds the data and list of of a packed sequence."
    },
    {
        "link": "https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/pytorch_cheatsheet.ipynb",
        "document": "To see all available qualifiers, see our documentation .\n\nSaved searches Use saved searches to filter your results more quickly\n\nWe read every piece of feedback, and take your input very seriously.\n\nYou signed in with another tab or window. Reload to refresh your session.\n\nYou signed out in another tab or window. Reload to refresh your session.\n\nYou switched accounts on another tab or window. Reload to refresh your session."
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/nn_tutorial.html",
        "document": "Authors: Jeremy Howard, fast.ai. Thanks to Rachel Thomas and Francisco Ingham.\n\nWe recommend running this tutorial as a notebook, not a script. To download the notebook ( ) file, click the link at the top of the page.\n\nPyTorch provides the elegantly designed modules and classes torch.nn , torch.optim , Dataset , and DataLoader to help you create and train neural networks. In order to fully utilize their power and customize them for your problem, you need to really understand exactly what they’re doing. To develop this understanding, we will first train basic neural net on the MNIST data set without using any features from these models; we will initially only use the most basic PyTorch tensor functionality. Then, we will incrementally add one feature from , , , or at a time, showing exactly what each piece does, and how it works to make the code either more concise, or more flexible.\n\nThis tutorial assumes you already have PyTorch installed, and are familiar with the basics of tensor operations. (If you’re familiar with Numpy array operations, you’ll find the PyTorch tensor operations used here nearly identical).\n\nWe will use the classic MNIST dataset, which consists of black-and-white images of hand-drawn digits (between 0 and 9). We will use pathlib for dealing with paths (part of the Python 3 standard library), and will download the dataset using requests. We will only import modules when we use them, so you can see exactly what’s being used at each point. This dataset is in numpy array format, and has been stored using pickle, a python-specific format for serializing data. Each image is 28 x 28, and is being stored as a flattened row of length 784 (=28x28). Let’s take a look at one; we need to reshape it to 2d first. # ``pyplot.show()`` only if not on Colab PyTorch uses , rather than numpy arrays, so we need to convert our data.\n\nWe will now refactor our code, so that it does the same thing as before, only we’ll start taking advantage of PyTorch’s classes to make it more concise and flexible. At each step from here, we should be making our code one or more of: shorter, more understandable, and/or more flexible. The first and easiest step is to make our code shorter by replacing our hand-written activation and loss functions with those from (which is generally imported into the namespace by convention). This module contains all the functions in the library (whereas other parts of the library contain classes). As well as a wide range of loss and activation functions, you’ll also find here some convenient functions for creating neural nets, such as pooling functions. (There are also functions for doing convolutions, linear layers, etc, but as we’ll see, these are usually better handled using other parts of the library.) If you’re using negative log likelihood loss and log softmax activation, then Pytorch provides a single function that combines the two. So we can even remove the activation function from our model. Note that we no longer call in the function. Let’s confirm that our loss and accuracy are the same as before:"
    },
    {
        "link": "https://docs.kanaries.net/topics/Python/nn-linear",
        "document": "Deep learning has revolutionized the field of artificial intelligence, enabling machines to mimic human intelligence in an unprecedented manner. At the heart of this revolution is PyTorch, a popular open-source machine learning library that provides two high-level features: tensor computation with strong GPU acceleration and deep neural networks built on a tape-based autograd system. One of the fundamental components of PyTorch is , a module that applies a linear transformation to the incoming data. This article provides a comprehensive guide to understanding in PyTorch, its role in neural networks, and how it compares to other linear transformation methods.\n\nis a linear layer used in neural networks that applies a linear transformation to input data using weights and biases. It is a critical component in the architecture of many deep learning models. This guide will delve into the details of , including its definition, how it works, and its applications in deep learning. We will also address frequently asked questions and related queries, providing a thorough understanding of this essential PyTorch module.\n\nIn the context of neural networks, is a module provided by PyTorch that applies a linear transformation to the incoming data. This transformation is represented by the formula , where is the input, is the weight, is the bias, and is the output.\n\nThe module takes two parameters: and , which represent the number of input and output features, respectively. When an object is created, it randomly initializes a weight matrix and a bias vector. The size of the weight matrix is , and the size of the bias vector is .\n\nIn the code snippet above, we create an instance of with three input features and one output feature. This results in a 3x1 weight matrix and a 1x1 bias vector.\n\nworks by performing a matrix multiplication of the input data with the weight matrix and adding the bias term. This operation is applied to each layer in a feed-forward neural network.\n\nIn the code snippet above, we pass a tensor of size 3 (matching the number of input features) to the . The output is a tensor of size 1 (matching the number of output features), which is the result of the linear transformation.\n\nThe weights and biases in are parameters that the model learns during training. Initially, they are set to random values. You can view the weights and biases using the and attributes.\n\nThe code snippet above prints the weight matrix and bias vector of\n\nWhile PyTorch initializes these parameters randomly, you can also set them manually or use different initialization methods. For instance, you can use the module to apply specific initialization methods to the weights and biases. Here's an example of using the Xavier uniform initialization:\n\nIn the code snippet above, we use the function from to initialize the weights of our . The bias is initialized to zero using the function. These initialization methods can help improve the learning process of the neural network.\n\nand are both fundamental modules in PyTorch used for different purposes. While applies a linear transformation to the incoming data, applies a 2D convolution over an input signal composed of several input planes.\n\nThe main difference between and lies in their application. is typically used in fully connected layers where each input neuron is connected to each output neuron. On the other hand, is used in convolutional layers, which are primarily used in convolutional neural networks (CNNs) for tasks like image processing.\n\nIn terms of their parameters, requires the number of input features and the number of output features. requires the number of input channels (or depth of the input), the number of output channels, and the kernel size.\n\nis a versatile module in PyTorch and finds numerous applications in deep learning. Here are a few examples:\n• Multi-Layer Perceptron (MLP): MLPs are a type of feed-forward neural network that consist of at least three layers of nodes: an input layer, a hidden layer, and an output layer. Each layer is fully connected to the next one, and is used to implement these connections.\n• Linear Regression: In linear regression tasks, can be used to implement the linear equation that the model learns.\n• Data Transformation: can be used to transform input data into a higher dimension for more complex tasks.\n• Deep Learning Models: Many deep learning models, such as autoencoders, use in their architecture.\n\nIn the next segment, we will delve into more details about the use of in a PyTorch model, including how to initialize weights and biases, and how to use it in a model. We will also provide examples of its applications in deep learning.\n\nIncorporating into a PyTorch model involves defining the layer in the model's constructor and then applying it to the input data in the forward method. Here's an example of a simple feed-forward neural network that uses :\n\nIn the code snippet above, we define a network with two linear layers ( and ). The method defines the forward pass of the input through the network. The function applies the ReLU activation function to the output of the first linear layer before passing it to the second linear layer.\n\nCommon Errors and Solutions for nn.Linear in PyTorch\n\nWhile using , you might encounter some common errors. Here are a few of them along with their solutions:\n• Mismatched Input Size: The input size must match the parameter of . If they don't match, you'll get a runtime error. To fix this, ensure that the size of the input tensor matches the parameter.\n• Incorrect Weight and Bias Sizes: The weight matrix and bias vector must have sizes that match the and parameters. If they don't, you'll get a runtime error. To fix this, ensure that the sizes of the weight matrix and bias vector are correct.\n• Using nn.Linear with 3D Input: expects a 2D input, but sometimes you might mistakenly pass a 3D input (for example, from a convolutional layer in a CNN). This will result in a runtime error. To fix this, you can use or to reshape the input to 2D.\n\nIn conclusion, is a fundamental component in PyTorch and deep learning. It plays a crucial role in implementing linear transformations in neural networks, and understanding it can significantly aid in building and troubleshooting deep learning models. Whether you're a beginner just starting out with PyTorch or an experienced practitioner, mastering is a valuable skill in your deep learning toolkit.\n\nWhat is the purpose of a bias vector in nn.Linear?\n\nThe bias vector in allows the model to shift the output of the linear transformation along the y-axis. This can be crucial for fitting the model to the data, especially when the data is not centered around the origin. Without the bias, the model would always go through the origin, which could limit its capacity to fit the data.\n\nHow do you initialize weights and biases for a linear layer in PyTorch?\n\nWeights and biases for a linear layer in PyTorch can be initialized when an object is created. By default, they are initialized to random values. However, you can manually set them or use different initialization methods provided by the module.\n\nWhat is the difference between nn.Linear and nn.Conv2d in PyTorch?\n\nand are both used to implement layers in neural networks, but they serve different purposes. applies a linear transformation to the incoming data and is typically used in fully connected layers. On the other hand, applies a 2D convolution over an input signal and is primarily used in convolutional layers for tasks like image processing."
    },
    {
        "link": "https://discuss.pytorch.org/t/example-on-how-to-use-batch-norm/216",
        "document": "TLDR: What exact size should I give the batch_norm layer here if I want to apply it to a CNN? output? In what format?\n• So far I have only this link here, that shows how to use batch-norm. My first question is, is this the proper way of usage? For example Is this the correct intended usage? Maybe an example of the syntax for it’s usage with a CNN?\n• I know that there are sometimes caveats with usage of batch-norm during training and inference time - (for example, the original paper will compute running averages and variances of the training data AFTER the net has fully trained, and then use that in the inference equation), however I am guessing the batch-norm usage in pyTorch already does this under the hood, and so I can call forward_prop at test time the same way I would call it at train time? Cannot Set Batch Size to Anything Aside from 1 on my CNN Model\n\n@moskomule Thanks! I will try this. On an unrelated note, one thing I noticed though is that you are doing ReLUs AFTER your max_pool, however the canonical way it is done is usually the reverse. (ReLU and then max-pool).\n• As @moskomule pointed out, you have to specify how many feature channels will your input have (because that’s the number of BatchNorm parameters). Batch and spatial dimensions don’t matter.\n• BatchNorm will only update the running averages in mode, so if you want the model to keep updating them in test time, you will have to keep modules in the training mode. See the C implementation for details (it should be readable).\n• About ReLU and MaxPool - if you think about it for a moment both ReLU + MaxPool and MaxPool + ReLU are equivalent operations, with the second option being 37.5% more efficient ( in first case in the second case, where is the number of elements in the tensor). That’s why the example has a different order.\n\n@apaszke Thanks for the batchnorm info! For the ReLUs/Max-pool, I agree that in this case they are equivalent, however in the more general case of an arbitrary activation function they will not necessarily be - consider a “relu” activation function that is flipped and exists on the second and forth quadrants instead of just the first and border between third and second, in this case the two operation orders would matter. (Granted they are not that popular, I just wanted to point out the subtlety )\n\nAt test time, I would like to freeze both the weights, (lambda and beta), as well as freeze the running averages that is has computed. (Ostensibly because it has a good estimate for those from training already). So I basically expect that I would want all 4 of those values frozen.\n\nI have a pretrained model whose parameters are available as csv files. This model has batch norm layers which has got weight, bias, mean and variance parameters. I want to copy these parameters to layers of a similar model I have created in pytorch. But the Batch norm layer in pytorch has only two parameters namely weight and bias. How do I deal with mean and variance so that during eval all these four parameters are used?\n\nI have been using BN to train audoencoders over a large number of image patches (50K/image) of different architectures recently. There is indeed gotcha whenever BN is used with the dataset as follows. After training a long time (70 epochs or more with 4K batches each), the validation loss suddenly increases significantly and never comes back while the training loss remains stable. Decreasing the learning rate only postpones the phenomenon. The trained model at this point is not usable if model.eval() is called as it is supposed to be. But if the output is normalized to the regular pixel range, the results seem alright. After several trials, the cause is likely the default epsilon which may be too small (1e-5) for long term stability. However, increasing the epsilon leads to a slightly higher validation loss. Alternatively, the running mean and var computed by pytorch under the hood may have something to do with since fixing BN at the training mode also alleviates the issue for inference time. In view of the popularity of BN, I am a bit surprised everyone seems happy with it but is it really the case?"
    },
    {
        "link": "https://discuss.pytorch.org/t/instancenorm-vs-batchnorm-num-batches-tracked/178590",
        "document": "Is there a reason why gets updated in BN but not in IN? # Create an instance normalization layer with track_running_stats=True norm_layer = torch.nn.InstanceNorm1d(2, track_running_stats=True, affine=False) # Process the input data using the normalization layer y = norm_layer(x) # Print the running statistics of the layer print(norm_layer.state_dict()) # OrderedDict([('running_mean', tensor([-0.0022, 0.0019])), ('running_var', tensor([1.0003, 1.0025])), ('num_batches_tracked', tensor(0))]) # Create an instance normalization layer with track_running_stats=True norm_layer = torch.nn.BatchNorm1d(2, track_running_stats=True, affine=False) # Process the input data using the normalization layer y = norm_layer(x) # Print the running statistics of the layer print(norm_layer.state_dict()) # OrderedDict([('running_mean', tensor([-0.0022, 0.0019])), ('running_var', tensor([0.9972, 1.0007])), ('num_batches_tracked', tensor(1))])\n\nis simply not relevant for InstanceNorm since it doesn’t need to keep track of running statistics from multiple batches and can be applied independently per instance\n\nThanks Mark! I have noticed that and gets updated but does not. My question is as one can get the and , why not ? The documentation mentions keeping running estimates. What am I missing here? If is set to , during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. For example, in the following code, the after the first run should be 1 as other buffers are updated.\n\nis used in batchnorm layers to update the running stats with the cumulative moving average, which is used if the argument is set to .\n\n This option is not supported in instancenorm layers as these expect a valid floating point value for the and don’t accept .\n\nUnfortunately, I don’t know why it’s disallowed or if it was just never implemented.\n\n Internally would be used as seen here so I would assume a cumulative moving average usage should be possible, but also don’t know if it wouldn’t make sense."
    },
    {
        "link": "https://medium.com/thedeephub/batch-normalization-for-training-neural-networks-328112bda3ae",
        "document": "Training neural networks is an art rather than a process with a fixed outcome. You don’t know whether you’ll end up with working models, and there are many factors that may induce failure for your machine learning project.\n\nHowever, over time, you’ll also learn a certain set of brush strokes which significantly improve the odds that you’ll succeed.\n\nIn modern neural network theory, Batch Normalization is likely one of the key concepts that you’ll encounter during your quest for information.\n\nIt has something to do with normalizing based on batches of data… right? Yeah, but that’s actually repeating the name in different words.\n\nBatch Normalization, in fact, helps you overcome a phenomenon called internal covariate shift. What it this? And how does Batch Normalization work? We’ll answer those questions in this blog.\n\nThat discussion is followed by the introduction of Batch Normalization. Here, we’ll also take a look at what it is, how it works, what it does and why it matters. This way, you’ll understand how it can be used to speed up your training, or to even save you from situations with non-convergence.\n• What Batch Normalization does at a high level.\n• The differences between and in PyTorch.\n• How you can implement Batch Normalization with PyTorch.\n\nIt also includes a test run to see whether it can really perform better compared to not applying it.\n\nAre you ready? Let’s go!\n\nInternal Covariate Shift: a possible explanation of slow training and non-convergence\n\nSuppose that you have a neural network such as this one that has been equipped with Dropout neurons:\n\nAs you might recall from the high-level supervised machine learning process, training a neural network includes a feedforward operation on your training set. During this operation, the data is fed to the neural network, which generates a prediction for each sample that can be compared to the target data, a.k.a. the ground truth.\n\nThis results in a loss value that is computed by some loss function.\n\nBased on the loss function, backpropagation will compute what is known as the gradient to improve the loss, while gradient descent or an adaptive optimizer will actually change the weights of the neurons of your neural network. Based on this change, the model is expected to perform better during the next iteration, in which the process is repeated.\n\nNow, let’s change our viewpoint. Most likely, you’ll have read the previous secion while visualizing the neural network as a whole. Perfectly fine, as this was intended, but now focus on the network as if it is a collection of stacked, but individual, layers.\n\nEach layer takes some input, transforms this input through interaction with its weights, and outputs the result, to be consumed by the first layer downstream. Obviously, this is not true for the input layer (with the original sample as input) and the output layer (with no subsequent layer), but you get the point.\n\nNow suppose that we feed the entire training set to the neural network. The first layer will transform this data into something else. Statistically, however, this is also a sample, which thus has a sample mean and a sample standard deviation. This process repeats itself for each individual layer: the input data can be represented as some statistical sample with mean μ and standard deviation σ.\n• Firstly, the discussion above implies, as a consequence, that the distribution of input data for some particular layer depends on all the interactions happening in all the upstream layers.\n• Secondly, this means that a change in how one or more of the upstream layer(s) process data will change the input distribution for this layer.\n\n…and what happens when you train your model? Indeed, you change how the layers process data, by changing their weights.\n\nIoffe & Szegedy (2015), in their paper “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift” call this process the “internal covariate shift”. They define it as follows:\n\nWhy is this bad?\n\nPut plainly and simply: It slows down training.\n\nIf you were using a very strict approach towards defining a supervised machine learning model, you would, for example, say that machine learning produces a function which maps some input to some output based on some learned mapping, which equals the mapping made by the true, underlying mapping in your data.\n\nThis is also true for each layer: each layer is essentially a function which learns to map some input to some output, so that the system as a whole maps the original input to the desired output.\n\nNow imagine that you’re looking at the training process from some distance. Slowly but surely, each layer learns to represent the internal mapping and the system as a whole starts to show the desired behavior. Perfect, isn’t it?\n\nYes, except that you also see some oscillation during the process. Indeed, you see that the layers make tiny mistakes during training, because they expect the inputs to be of some kind, while they are slightly different. They do know how to handle this, as the changes are very small, but they have to readjust each time they encounter such a change. As a result, the process as a whole takes a bit longer.\n\nThe same is true for the actual machine learning process. The internal covariance shift, or the changing distributions of the input data for each hidden layer, mean that each layer requires some extra time to learn the weights which allow the system as a whole to minimize the loss value of the entire neural network. In extreme cases, although this does not happen too often, this shift may even result in non-convergence, or the impossibility of learning the mapping as a whole. This especially occurs in datasets which have not been normalized and are, as a consequence, a poor fit for ML.\n\nSpeaking of such normalization: rather than leaving it to the machine learning engineer, can’t we (at least partially) fix the problem in the neural network itself?\n\nThat’s the thought process that led Ioffe & Szegedy (2015) to conceptualize the concept of Batch Normalization: by normalizing the inputs to each layer to a learned representation likely close to (μ= 0.0, σ = 1.0), the internal covariance shift is reduced substantially. As a result, it is expected that the speed of the training process is increased significantly.\n\nBut how does it work?\n\nThe first important thing to understand about Batch Normalization is that it works on a per-feature basis.\n\nThis means that, for example, for feature vector x = [0.23, 1.26, -2.41], normalization is not performed equally for each dimension. Rather, each dimension is normalized individually, based on the sample parameters of the dimension.\n\nThe second important thing to understand about Batch Normalization is that it makes use of minibatches for performing the normalization process (Ioffe & Szegedy, 2015). It avoids the computational burden of using the entire training set, while assuming that minibatches approach the dataset’s sample distribution if sufficiently large. This is a very good idea.\n\nNow, the algorithm. For each feature x_B^{(k)} in your feature vector x_B (which, for your hidden layers, doesn’t contain your features but rather the inputs for that particular layer), Batch Normalization normalizes the values with a four-step process on your minibatch B (Ioffe & Szegedy, 2015):\n• Computing the mean of your minibatch:\n\n2. Computing the variance of your minibatch:\n\nThe first two steps are simple and are very common as well as required in a normalization step: computing the mean, μ, and variance, σ², of the k-th dimension of your minibatch sample x_B.\n\nThese are subsequently used in the normalization step, in which the expected distribution is (0, 1) as long as samples in the minibatch have the same distribution and the value for ε is neglected (Ioffe & Szegedy, 2015).\n\nYou may ask: indeed, this ε, why is it there?\n\nIt’s for numerical stability (Ioffe & Szegedy, 2015). If the variance σ² were zero, one would get a division by zero error. This means that the model would become numerically unstable. The value for ε resolves this by taking a very small but nonzero value to counter this effect.\n\nNow, finally, the fourth step: scaling and shifting the normalized input value. I can get why this is weird, as we already completed normalization in the third step.\n\nLinear regime of the nonlinearity? Represent the identity transform? What are these?\n\nLet’s simplify the rather academic English into a plainer variant.\n\nFirst, the “linear regime of the nonlinearity”. Suppose that we’re using the Sigmoid activation function, which is a nonlinear activation function (a “nonlinearity”) and was still quite common in 2015, when the Ioffe & Szegedy paper was written.\n\nIt looks like this:\n\nSuppose that we’ve added it to some arbitrary layer.\n\nWithout Batch Normalization, the inputs of this layer do not have a distribution of approximately (0, 1), and hence could theoretically be likelier to take rather large values (e.g. 2.5623423…).\n\nSuppose that our layer does nothing but pass the data (to make our case simpler), the activations of those input values produce outputs that have a nonlinear slope: as you can see in the plot above, for inputs to the activation function in the domain [2, 4], the output bends a bit.\n\nHowever, for inputs of ≈ 0, this is not the case: the outputs for the input domain of approximately [-0.5, 0.5] don’t bend and actually seem to represent a linear function. This entirely reduces the effect of nonlinear activation, and by consequence the performance of our model, and might not be what we want!\n\n…and wait: didn’t we normalize to (0, 1), meaning that the inputs to our activation function are likely in the domain [-1, 1] for every layer? Oops!\n\nThis is why the authors introduce a scaling and shifting operation with some parameters γ and β, with which the normalization can be adapted during training, in extreme cases even to “represent the identity transform” (a.k.a., what goes in, comes out again — entirely removing the Batch Normalization step).\n\nThe parameters are learned during training, together with the other parameters (Ioffe & Szegedy, 2015).\n\nNow, let’s revise our small example from above, with our feature vector x = [0.23, 1.26, -2.41].\n\nSay we used a minibatch approach with 2 samples per batch (a bit scant, I know, but it’s sufficient for the explanation), with another vector x_a = [0.56, 0.75, 1.00] in the set, our Batch Normalization step would go as follows (assuming γ = β = 1):\n\nAs we can see, with γ =β=1 , our values are normalized to a distribution of approximately (0, 1) — with some ε term.\n\nTheoretically, there are some assumed benefits when using Batch Normalization in your neural network (Ioffe & Szegedy, 2015):\n• The model is less sensitive to hyperparameter tuning. That is, whereas larger learning rates led to non-useful models previously, larger LRs are acceptable now.\n• Weight initialization is a tad less important now.\n• Dropout, which is used to add noise to benefit training, can be removed.\n\nWhile a minibatch approach speeds up the training process, it is “neither necessary nor desirable during inference” (Ioffe & Szegedy, 2015). When inferring, e.g., the class for a new sample, you wish to normalize it based on the entire training set, as it produces better estimates and is computationally feasible.\n\nHence, during inference, the Batch Normalization step goes as follows:\n\nWhere x ε X and X represents the full training data, rather than some minibatch X_b.\n\nHere, you will continue implementing Batch Normalization with the PyTorch library for deep learning. This involves a few steps:\n• Taking a look at the differences between and .\n• Writing your neural network and constructing your Batch Normalization-impacted training loop.\n• Consolidating everything in the full code.\n\nFirst of all, the differences between two-dimensional and one-dimensional Batch Normalization in PyTorch.\n• Two-dimensional Batch Normalization is made available by .\n• For one-dimensional Batch Normalization, you can use\n\nOne-dimensional Batch Normalization is defined as follows on the PyTorch website:\n\n…this is how two-dimensional Batch Normalization is described:\n• One-dimensional BatchNormalization ( ) applies Batch Normalization over a 2D or 3D input (a batch of 1D inputs with a possible channel dimension).\n• Two-dimensional BatchNormalization ( ) applies it over a 4D input (a batch of 2D inputs with a possible channel dimension).\n\nCreate a file — e.g. - and open it in your code editor. Also make sure that you have Python, PyTorch and installed onto your system (or available within your Python environment). Let's go!\n\nFirstly, we’re going to state our imports.\n• We’re going to need based definitions for downloading the dataset properly.\n• All based imports are required for PyTorch: itself, the (a.k.a. neural network) module and the for loading the dataset we're going to use in today's neural network.\n• From , we load the dataset - as well as some (primarily image normalization) that we will apply on the dataset before training the neural network.\n\nNext up is defining the . Indeed, we're not using layers today - which will likely improve your neural network. Instead, we're immediately flattening the 32x32x3 input, then further processing it into a 10-class outcome (because CIFAR10 has 10 classes).\n\nAs you can see, we’re applying here because we use densely-connected/fully connected (a.k.a. ) layers. Note that the number of inputs to the BatchNorm layer must equal the number of outputs of the layer.\n\nIt clearly shows how Batch Normalization must be applied with PyTorch.\n\nNext up is writing the training loop. We’re not going to cover it to a great extent here, because already wrote about it in our dedicated article about getting started with a first PyTorch model:\n\nHowever, to summarize briefly what happens, here you go:\n• First, we set the seed vector of our random number generator to a fixed number. This ensures that any differences are due to the stochastic nature of the number generation process, and not due to pseudorandomness of the number generator itself.\n• We then prepare the CIFAR-10 dataset, initialize the MLP and define the loss function and optimizer.\n• This is followed by iterating over the epochs, where we set current loss to 0.0 and start iterating over the data loader. We set the gradients to zero, perform the forward pass, compute the loss, and perform the backwards pass followed by optimization. Indeed this is what happens in the supervised ML process.\n• We print statistics per mini batch fed forward through the model.\n\nAlso available in my Github repository.\n\nThese are the results after training our MLP for 5 epochs on the CIFAR-10 dataset, with Batch Normalization:\n\nThe same, but then without Batch Normalization:\n\nClearly, but unsurprisingly, the Batch Normalization based model performs better.\n\nIn this blog post, we’ve looked at the problem of a relatively slow and non-convergent training process, and noted that Batch Normalization may help reduce the issues with your neural network. By reducing the distribution of the input data to (0, 1), and doing so on a per-layer basis, Batch Normalization is theoretically expected to reduce what is known as the “internal covariance shift”, resulting in faster learning.\n\nIn my next post, we’ll implement Batch Normalizing with TensorFlow and Keras and see if we can draw some empirical evidence to back up the need for Batch Normalization. I hope you’ve learned something from today’s post. Any comments, suggestions or questions are welcome. Thank you for reading!!\n\nReddit. (n.d.). Question about Batch Normalization. Retrieved from https://www.reddit.com/r/MachineLearning/comments/3k4ecb/question_about_batch_normalization/"
    },
    {
        "link": "https://discuss.pytorch.org/t/batch-normalisation-value-overflow/186037",
        "document": "throws error\n\n \n\n when using batch size of or or so, but worked fine when using or batch size. now its strange, as the data is not so big, and I have 24gb vram. what could be the cause ? and what can I do here ?\n\nHi, Thank you for your response. class BatchNorm(nn.Module): def __init__(self, input_dim: int, use_batch_normalization: bool = True, momentum: float = 0.1, track_running_stats: bool = True) -> None: super().__init__() self.input_dim = input_dim self.momentum = momentum self.use_batch_normalization = use_batch_normalization if self.use_batch_normalization: self.batch_norm = nn.BatchNorm1d(input_dim, momentum=momentum, track_running_stats=track_running_stats) else: self.bias = Parameter(torch.zeros(input_dim, dtype=torch.float32), requires_grad=True) def forward(self, x: torch.Tensor) -> torch.Tensor: \"\"\" Forward pass of batch normalization block. :param x: Input of shape `(N, D)` or `(N, K, D)` where `N = number of points`, `K = number of neighbors`, and `D = number of feature channels`. :type x: torch.Tensor :return: Normalized output of the same shape as the input. :rtype: torch.Tensor \"\"\" if self.use_batch_normalization: if x.dim() == 2: return self.batch_norm(x) if x.dim() == 3: # (N, K, D) -> (N, D, K) x = x.transpose(1, 2).contiguous() # (N, D, K) output = self.batch_norm(x) # (N, D, K) -> (N, K, D) return output.transpose(1, 2).contiguous() raise ValueError(f\"Input dimension of batch normalization block should be 2 or 3, got {x.dim()}.\") else: return x + self.bias def __repr__(self) -> str: return 'BatchNormBlock(in_feat: {:d},' \\ ' momentum: {:.3f}, only_bias: {:s})'.format(self.input_dim, self.momentum, str(not self.use_batch_normalization)) File \"x:\\Transformer.py\", line 206, in forward peb = self.linear_pos_bias(pos) File \"x:\\lib\\site-packages\\torch\n\nn\\modules\\module.py\", line 1190, in _call_impl return forward_call(*input, **kwargs) File \"x:\\lib\\site-packages\\torch\n\nn\\modules\\container.py\", line 204, in forward input = module(input) File \"x:\\lib\\site-packages\\torch\n\nn\\modules\\module.py\", line 1190, in _call_impl return forward_call(*input, **kwargs) File \"x:\\blocks\\batch_norm.py\", line 65, in forward output = self.batch_norm(x) File \"x:\\lib\\site-packages\\torch\n\nn\\modules\\module.py\", line 1190, in _call_impl return forward_call(*input, **kwargs) File \"x:\\lib\\site-packages\\torch\n\nn\\modules\\batchnorm.py\", line 171, in forward return F.batch_norm( File \"x:\\lib\\site-packages\\torch\n\nn\\functional.py\", line 2450, in batch_norm return torch.batch_norm( RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR\n\n1.13.1+cu117 is the highest i can go making sure that all the tests pass in my code base, as the code base is yet to be migrated to the latest PyTorch version. which has to be done eventually, but till then if you could think of another workaround, please let me know! PyTorch version: 1.13.1+cu117 Is debug build: False CUDA used to build PyTorch: 11.7 ROCM used to build PyTorch: N/A OS: Microsoft Windows 11 Pro GCC version: Could not collect Clang version: Could not collect CMake version: version 3.23.3 Libc version: N/A Python version: 3.9.12 (tags/v3.9.12:b28265d, Mar 23 2022, 23:52:46) [MSC v.1929 64 bit (AMD64)] (64-bit runtime) Python platform: Windows-10-10.0.22621-SP0 Is CUDA available: True CUDA runtime version: 11.7.64 CUDA_MODULE_LOADING set to: LAZY GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Ti Nvidia driver version: 522.06 cuDNN version: Could not collect HIP runtime version: N/A MIOpen runtime version: N/A Is XNNPACK available: True Versions of relevant libraries: [pip3] mypy==1.5.0 [pip3] mypy-extensions==1.0.0 [pip3] numpy==1.25.2 [pip3] torch==1.13.1+cu117 [pip3] torch-cluster==1.6.1+pt113cu117 [pip3] torch-geometric==2.3.1 [pip3] torch-scatter==2.1.1+pt113cu117 [pip3] torch-sparse==0.6.17+pt113cu117 [pip3] torch-spline-conv==1.2.2+pt113cu117 [pip3] torchaudio==0.13.1+cu117 [pip3] torchvision==0.14.1+cu117 [conda] Could not collect"
    },
    {
        "link": "https://stackoverflow.com/questions/75477709/how-to-use-pytorch-nn-batchnorm1d-to-get-equal-normalization-across-features",
        "document": "i would like to ask a question regarding the nn.BatchNorm1d in PyTorch.\n\nI have one main tensor, which has shape . Then, i have two additional tensors which have shape and . I will concatenate the main tensor with the two tensors separately, to construct new tensors and .\n\nI pass my tensors to a plain MLP (consists of conv1d and batchnorm1d). Ideally, i want to predict something \"point-wise\", like no matter what the number of dimension 2, it has some consistent prediction only given the value. However, the batchnorm1d will have different results given input and , while i am only focusing on first N points in 2nd dimension.\n\nIs there any possible way to have consistent prediction of first N points?"
    }
]