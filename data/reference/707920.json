[
    {
        "link": "https://he.kendallhunt.com/sites/default/files/uploadedFiles/Kendall_Hunt/Content/Higher_Education/Uploads/ChenTsai_ProgramLanguages_4e_Chapter1.pdf",
        "document": ""
    },
    {
        "link": "https://utdallas.edu/~cid021000/CS-4337_13F/slides/CS-4337_03_Chapter3.pdf",
        "document": ""
    },
    {
        "link": "http://siek.blogspot.com/2012/07/crash-course-on-notation-in-programming.html",
        "document": "\n• For any natural numbers and , if , then .\n• For any , if , then .\n• For any and , if and , then .\n• For any , if , then .\n\nType Systems (with the Lambda Calculus as an example)\n\nThis blog post is meant to help my friends get started in reading my other blog posts, that is, this post is a crash course on the notation used in programming language theory (\"PL theory\" for short). For a much more thorough introduction, I recommendby Benjamin C. Pierce andby Felleisen, Findler, and Flatt. I'll assume the reader is an experienced programmer but not an experienced mathematician or PL theorist. I'll start with the most basic definitions and try to build up quickly.I suspect many readers will already be familiar with sets, tuples, and relations and thus may want to skip the next few subsections, but if you are not familiar with inductive definitions, then please make sure to read the subsection below titled Definition by Rules.The main building block that we use in PL theory is the, a collection of objects (also called elements), such as the set containing the first three natural numbers: The only thing that matters is whether an object is in the set or not; it doesn't make sense to ask if there are duplicates of an object or in what order the objects appear in the set. For example, the set is the same set as the set listed above. The notation means \"in\", so is true and is false. Sets may have an infinite number of elements, such as the set of all natural numbers (non-negative integers), written .Another building block is the tuple, which is an ordered collection of objects. So is a tuple of three elements and it is different from the tuple . The subscript notation retrieves the ith element of tuple . For example, if , then . Tuples contain only a finite number of elements and usually less than a handful. Sometimes angle brackets are used for tuples instead of parentheses, such as .Putting tuples and sets together we get relations. That is, ais a set of tuples. We often use relations to represent a mapping from input to output. For example, the above relation can be thought of as mapping a natural number to its successor, that is, to the next greater natural number. The above definition is rather imprecise because of the elipses ( ). Fortunately, there are more precise notations for describing infinite sets and relations.The main way that we define infinite sets in PL theory is by giving a list of rules for what is in the set. Let's use the name for the above relation. Then the following two rules give a precise definition of . Notice that the second rule is recursive in that it refers to itself. That's ok and quite common.When we use rules to define a set, we implicitly mean that an element is not in the set if there is no way to use the given rules to justify that the element should be in the set. So is not in because there is no way to use the above two rules to conclude that is in .Some sets of rules are nonsensical and do not define a set. For example, rules should not be contradictory as in the following.A textbook on set theory will give the restrictions on what constitutes a \"good\" bunch of rules, but we won't go into that here, other than to point out that you need at least one non-recursive rule and that logical negation should be avoided.A common notation for rules such as the above uses a horizontal line in place of \"if\" and \"then\". For example, an equivalent definition of is given by the following. We have dropped the \"For any natural numbers and \" part of rule 2. The convention is that variables such as and that appear in a rule can be replaced byobject of the correct type, in this case, a natural number. Often the \"correct type\" is something you can deduce from the context of the discussion, in this case, the natural numbers.Suppose that I claim that a particular element is in , say . You might respond by saying that you don't believe me. To convince you, I need to show you how the rules justify that , that is, I need to show you a. A derivation is a chaining together of rules, replacing variables such as and with particular objects and replacing premises such as with sub-derivations. I've labelled each step in the derivation with the rule number.The fancy name for what I'm calling Definition by Rules is. (My daughter loves the Fancy Nancy series of books.)It turns out that using rules to defines sets, as we did above, is how we define the syntax of a programming language. Suppose we'd like to define a simple language of integer arithmetic, call it , including expressions such asand. Recall that is the set of all integers. Then here's a bunch of rules that we might use to define :(BNF) is another common notation for writing rules that define the syntax of a language, but the meaning is the same. (There are several variations on BNF; I forget which one I'm using here.) The bunch of rules is referred to as aA vertical bar (meaning \"or\") is often used to make such syntax definitions more concise, as follows.In PL theory, we use a peculiar variation on BNF that replaces the name of the language being defined, in this case , with the variable that is used to range over elements of . So suppose we are using the variable as a placeholder for any integer and as a placeholder for elements of . Then we would write the above grammar as Note that I've dropped the parentheses. It's generally understood that parentheses are allowed in any language.The notion of derivation coincides with that of a parse tree, they both demonstrate why a particular element is in a set.A language is brought to life by defining what it means to run a program in the language, that is, the operational semantics of a language. In the case of , we just need to specify the integer output of each program. As discussed above, relations can be used to map inputs to outputs, and indeed, we typically use relations for this purpose in PL theory. There's several different styles of relations, the first we'll discuss is astyle of semantics that maps a program directly to its output.Let's define a relation that maps elements of to integers. For example, we'd like to have . This relation will be infinite (because there are an infinite number of programs in ), so again we'll use a bunch of rules to define . But before we start, it's common to introduce some shorthand: means . Below we state the rules that define using the horizontal notation. To make sure we don't leave out any programs, we create one rule for each syntactic rule of (there are three). We say that the rules arewhen there is one rule for each syntactic rule in the language. It may seem a little odd that I'm definingin terms of , and similarly for. Isn't that circular? No, the and are the usual arithmetic operators for integers that everyone learned in grade school. In this way, the language is rather odd in not using 32 or 64-bit arithmetic. An implementor of would have to use a big-integer package to properly handle the arithmetic.The second, and perhaps more common, style of operational semantics issemantics. In this style, the relation doesn't map a program to its output, but instead it maps a program to a slightly simplified program in which one subexpression has been computed. This style of semantics can be thought of as textual rewriting. To give an example of this style, let's define a relation named . We'll want this relation to have the following elements, among many others: Again, we'll introduce shorthand: means . Also, we'll chain together steps, so means and . The termis a synonym for step. The above example of two steps can now be written as OK, on to the rules that define the relation. There are five rules, which we explain below. Rules (1) and (2) are the most interesting; they perform the arithmetic. We call themreduction rules. Rules (3-5) allow us to reach inside of sub-expressions to perform computation. They are often calledrules for reasons we won't go into. The use of the variable in rule (5) means that reduction proceeds from left to right. In particular, we're not allowed to reduce the right-hand expression of a plus until we've already reduced the left-hand side to an integer.This left-to-right ordering is a choice that I made as the designer of this example language. I could have not specified an ordering, letting it be non-deterministic. However, this example language doesn't have side-effects, so the ordering doesn't matter! However, most language do have side-effects and they do specify an ordering (but not all!), so I thought to include an example of how ordering is typically specified.Time for an example: let's see the derivation of the step .We've defined a single step of computation, the relation, but we're not quite done. We still need to specify what it means to run a program to completion. We'll do this by defining a relation in terms of the relation as follows. In plain Engilish, the relation will contain any pair if expression reduces to integer in zero or more steps. Some of the notation here is new and is explained below. The notation is theornotation for defining a set. The stuff to the left of the vertical bar is a template for a typical element of the set and the stuff to the right of the vertical bar places restrictions on the elements in the set. The notation means zero or more steps. I like to define this multi-step relation with the following rules: (My brain is good at reasoning about Lisp-style lists, so I think of the first rule as nil and the second rule as cons.)Many programming languages are statically typed, that is, the compiler performs some sanity checking before proceeding with the actual work of compiling. The checking usually involves make sure that objects are only used as intended, for example, not trying to treat an integer as if it were a function. The way a programming language designer (PL theorist) specifies what kind of sanity checking should be performed is by defining afor the language. The language is so simple that there is no interesting type checking to be performed. Let's consider a slightly larger language that also happens to be used over and over again in PL theory, the lambda calculus (technically, the simply-typed lambda calculus). The lambda calculus just consists of first-class anonymous functions. Here we'll extend the lambda calculus to also include our arithmetic expressions. So now our example language is defined by the following grammar. The variable ranges over parameter names, such asand. Two expressions right next to each other denote function application (i.e., function call). So if you're familiar with the C language, read as . In the lambda calculus, functions only take one parameter, so function calls only require one argument. The syntax creates a function with one parameter named of type (types will be defined shortly) and whose body is the expression . (A common point of confusion is to think that is the name of the function. It instead is the parameter name. The function is anonymous, i.e. it doesn't have a name.) The return value of the function will be whatever the expression evaluates to.Now let's consider what kind of objects will be alive when we run the program: there's integers and functions. We'll create a set ofto describe the kinds of objects, using to range over the set of types. In a function type , the is the type of the parameter and is the return type.The job of a type system is to predict which type of value will be produced by an expression. For example, the expressionshould have the typebecause the result ofis, which is an integer. As with the syntax and operational semantics of a language, PL theorists use relations and rules to define a type system. We'll define a relation named that, as a first approximation, maps expressions to types, so for example, we'll have .However, because the lambda calculus includes variables, we'll need something analogous to a symbol table, a relation called a, to keep track of which variables have which types. The Greek letter (gamma) is traditionally used for this purpose. We'll need to be able to create new type environments from old ones, potentially overshadowing variable definitions from outer scopes. To set up the mathematical machinery for that, we define to be the relation just like except that any tuple starting with is removed. (The way the type system will be defined, there may be 0 or 1 tuple that starts with , making the type environment a special kind of relation called a partial function.) We'll write for the operation of extending a type environment with variable, possibly overriding a previous definition, and define it as follows: Suppose we have ThenOne way in which type environments are different from the global symbol table in a compiler is that there isn't just one type environment, there will be lots of them, one for each scope. Further, we won't ever update a type environment in place, we'll keep creating new ones that differ a little bit from the old ones. From a programming perspective, the mathematical metalanguage we're using here is pure functional, that is, it doesn't use state or side effects. The reader might worry that this might lead to inefficiency, but remember, we're not writing a program here, we're writing a specification! Clarity is what matters most in this setting, and staying pure helps to make things clear.Getting back to the relation, instead of containing 2-tuples (pairs) it will contain 3-tuples (triples) of the form , so we'll be assigning types to expressions in the context of a type environment. As yet more shorthand (PL theorists love shorthand!), we'll write instead of . We're now ready to write down the rules that define . To sum up the above rules, the arithmetic operators work on integers, variables get their types from the environment, lambdas are given function types based on their parameter type and their deduced return type, the body of a lambda is checked using the environment from the point of creation (this is lexical scoping) extended with the lambda's parameter, and function application is sane so long as the argument's type is the same as the parameter type.Thus ends this crash course on the notation used in programming language theory. This blog post only scratches the surface, but much of the additional notation that you'll need is variations on what's covered here. Happy reading! And P.S., feel free to ask questions in the form of comments to this blog."
    },
    {
        "link": "https://he.kendallhunt.com/sites/default/files/heupload/pdfs/Chen_Ch1.pdf",
        "document": ""
    },
    {
        "link": "https://personal.utdallas.edu/~cid021000/CS-4337_13F/slides/CS-4337_03_Chapter3.pdf",
        "document": ""
    },
    {
        "link": "https://craftinginterpreters.com/introduction.html",
        "document": "I’m really excited we’re going on this journey together. This is a book on implementing interpreters for programming languages. It’s also a book on how to design a language worth implementing. It’s the book I wish I’d had when I first started getting into languages, and it’s the book I’ve been writing in my head for nearly a decade.\n\nIn these pages, we will walk step-by-step through two complete interpreters for a full-featured language. I assume this is your first foray into languages, so I’ll cover each concept and line of code you need to build a complete, usable, fast language implementation.\n\nIn order to cram two full implementations inside one book without it turning into a doorstop, this text is lighter on theory than others. As we build each piece of the system, I will introduce the history and concepts behind it. I’ll try to get you familiar with the lingo so that if you ever find yourself at a cocktail party full of PL (programming language) researchers, you’ll fit in.\n\nBut we’re mostly going to spend our brain juice getting the language up and running. This is not to say theory isn’t important. Being able to reason precisely and formally about syntax and semantics is a vital skill when working on a language. But, personally, I learn best by doing. It’s hard for me to wade through paragraphs full of abstract concepts and really absorb them. But if I’ve coded something, run it, and debugged it, then I get it.\n\nThat’s my goal for you. I want you to come away with a solid intuition of how a real language lives and breathes. My hope is that when you read other, more theoretical books later, the concepts there will firmly stick in your mind, adhered to this tangible substrate.\n\nEvery introduction to every compiler book seems to have this section. I don’t know what it is about programming languages that causes such existential doubt. I don’t think ornithology books worry about justifying their existence. They assume the reader loves birds and start teaching.\n\nBut programming languages are a little different. I suppose it is true that the odds of any of us creating a broadly successful, general-purpose programming language are slim. The designers of the world’s widely used languages could fit in a Volkswagen bus, even without putting the pop-top camper up. If joining that elite group was the only reason to learn languages, it would be hard to justify. Fortunately, it isn’t.\n\nFor every successful general-purpose language, there are a thousand successful niche ones. We used to call them “little languages”, but inflation in the jargon economy led to the name “domain-specific languages”. These are pidgins tailor-built to a specific task. Think application scripting languages, template engines, markup formats, and configuration files.\n\nAlmost every large software project needs a handful of these. When you can, it’s good to reuse an existing one instead of rolling your own. Once you factor in documentation, debuggers, editor support, syntax highlighting, and all of the other trappings, doing it yourself becomes a tall order.\n\nBut there’s still a good chance you’ll find yourself needing to whip up a parser or other tool when there isn’t an existing library that fits your needs. Even when you are reusing some existing implementation, you’ll inevitably end up needing to debug and maintain it and poke around in its guts.\n\nLong distance runners sometimes train with weights strapped to their ankles or at high altitudes where the atmosphere is thin. When they later unburden themselves, the new relative ease of light limbs and oxygen-rich air enables them to run farther and faster.\n\nImplementing a language is a real test of programming skill. The code is complex and performance critical. You must master recursion, dynamic arrays, trees, graphs, and hash tables. You probably use hash tables at least in your day-to-day programming, but do you really understand them? Well, after we’ve crafted our own from scratch, I guarantee you will.\n\nWhile I intend to show you that an interpreter isn’t as daunting as you might believe, implementing one well is still a challenge. Rise to it, and you’ll come away a stronger programmer, and smarter about how you use data structures and algorithms in your day job.\n\nThis last reason is hard for me to admit, because it’s so close to my heart. Ever since I learned to program as a kid, I felt there was something magical about languages. When I first tapped out BASIC programs one key at a time I couldn’t conceive how BASIC itself was made.\n\nLater, the mixture of awe and terror on my college friends’ faces when talking about their compilers class was enough to convince me language hackers were a different breed of human—some sort of wizards granted privileged access to arcane arts.\n\nIt’s a charming image, but it has a darker side. I didn’t feel like a wizard, so I was left thinking I lacked some inborn quality necessary to join the cabal. Though I’ve been fascinated by languages ever since I doodled made-up keywords in my school notebook, it took me decades to muster the courage to try to really learn them. That “magical” quality, that sense of exclusivity, excluded me.\n\nWhen I did finally start cobbling together my own little interpreters, I quickly learned that, of course, there is no magic at all. It’s just code, and the people who hack on languages are just people.\n\nThere are a few techniques you don’t often encounter outside of languages, and some parts are a little difficult. But not more difficult than other obstacles you’ve overcome. My hope is that if you’ve felt intimidated by languages and this book helps you overcome that fear, maybe I’ll leave you just a tiny bit braver than you were before.\n\nAnd, who knows, maybe you will make the next great language. Someone has to.\n\nThis book is broken into three parts. You’re reading the first one now. It’s a couple of chapters to get you oriented, teach you some of the lingo that language hackers use, and introduce you to Lox, the language we’ll be implementing.\n\nEach of the other two parts builds one complete Lox interpreter. Within those parts, each chapter is structured the same way. The chapter takes a single language feature, teaches you the concepts behind it, and walks you through an implementation.\n\nIt took a good bit of trial and error on my part, but I managed to carve up the two interpreters into chapter-sized chunks that build on the previous chapters but require nothing from later ones. From the very first chapter, you’ll have a working program you can run and play with. With each passing chapter, it grows increasingly full-featured until you eventually have a complete language.\n\nAside from copious, scintillating English prose, chapters have a few other delightful facets:\n\nWe’re about crafting interpreters, so this book contains real code. Every single line of code needed is included, and each snippet tells you where to insert it in your ever-growing implementation.\n\nMany other language books and language implementations use tools like Lex and Yacc, so-called compiler-compilers, that automatically generate some of the source files for an implementation from some higher-level description. There are pros and cons to tools like those, and strong opinions—some might say religious convictions—on both sides.\n\nWe will abstain from using them here. I want to ensure there are no dark corners where magic and confusion can hide, so we’ll write everything by hand. As you’ll see, it’s not as bad as it sounds, and it means you really will understand each line of code and how both interpreters work.\n\nA book has different constraints from the “real world” and so the coding style here might not always reflect the best way to write maintainable production software. If I seem a little cavalier about, say, omitting or declaring a global variable, understand I do so to keep the code easier on your eyes. The pages here aren’t as wide as your IDE and every character counts.\n\nAlso, the code doesn’t have many comments. That’s because each handful of lines is surrounded by several paragraphs of honest-to-God prose explaining it. When you write a book to accompany your program, you are welcome to omit comments too. Otherwise, you should probably use a little more than I do.\n\nWhile the book contains every line of code and teaches what each means, it does not describe the machinery needed to compile and run the interpreter. I assume you can slap together a makefile or a project in your IDE of choice in order to get the code to run. Those kinds of instructions get out of date quickly, and I want this book to age like XO brandy, not backyard hooch.\n\nSince the book contains literally every line of code needed for the implementations, the snippets are quite precise. Also, because I try to keep the program in a runnable state even when major features are missing, sometimes we add temporary code that gets replaced in later snippets.\n\nA snippet with all the bells and whistles looks like this:\n\nIn the center, you have the new code to add. It may have a few faded out lines above or below to show where it goes in the existing surrounding code. There is also a little blurb telling you in which file and where to place the snippet. If that blurb says “replace _ lines”, there is some existing code between the faded lines that you need to remove and replace with the new snippet.\n\nAsides contain biographical sketches, historical background, references to related topics, and suggestions of other areas to explore. There’s nothing that you need to know in them to understand later parts of the book, so you can skip them if you want. I won’t judge you, but I might be a little sad.\n\nEach chapter ends with a few exercises. Unlike textbook problem sets, which tend to review material you already covered, these are to help you learn more than what’s in the chapter. They force you to step off the guided path and explore on your own. They will make you research other languages, figure out how to implement features, or otherwise get you out of your comfort zone.\n\nVanquish the challenges and you’ll come away with a broader understanding and possibly a few bumps and scrapes. Or skip them if you want to stay inside the comfy confines of the tour bus. It’s your book.\n\nMost “programming language” books are strictly programming language implementation books. They rarely discuss how one might happen to design the language being implemented. Implementation is fun because it is so precisely defined. We programmers seem to have an affinity for things that are black and white, ones and zeroes.\n\nPersonally, I think the world needs only so many implementations of FORTRAN 77. At some point, you find yourself designing a new language. Once you start playing that game, then the softer, human side of the equation becomes paramount. Things like which features are easy to learn, how to balance innovation and familiarity, what syntax is more readable and to whom.\n\nAll of that stuff profoundly affects the success of your new language. I want your language to succeed, so in some chapters I end with a “design note”, a little essay on some corner of the human aspect of programming languages. I’m no expert on this—I don’t know if anyone really is—so take these with a large pinch of salt. That should make them tastier food for thought, which is my main aim.\n\nWe’ll write our first interpreter, jlox, in Java. The focus is on concepts. We’ll write the simplest, cleanest code we can to correctly implement the semantics of the language. This will get us comfortable with the basic techniques and also hone our understanding of exactly how the language is supposed to behave.\n\nJava is a great language for this. It’s high level enough that we don’t get overwhelmed by fiddly implementation details, but it’s still pretty explicit. Unlike in scripting languages, there tends to be less complex machinery hiding under the hood, and you’ve got static types to see what data structures you’re working with.\n\nI also chose Java specifically because it is an object-oriented language. That paradigm swept the programming world in the ’90s and is now the dominant way of thinking for millions of programmers. Odds are good you’re already used to organizing code into classes and methods, so we’ll keep you in that comfort zone.\n\nWhile academic language folks sometimes look down on object-oriented languages, the reality is that they are widely used even for language work. GCC and LLVM are written in C++, as are most JavaScript virtual machines. Object-oriented languages are ubiquitous, and the tools and compilers for a language are often written in the same language.\n\nAnd, finally, Java is hugely popular. That means there’s a good chance you already know it, so there’s less for you to learn to get going in the book. If you aren’t that familiar with Java, don’t freak out. I try to stick to a fairly minimal subset of it. I use the diamond operator from Java 7 to make things a little more terse, but that’s about it as far as “advanced” features go. If you know another object-oriented language, like C# or C++, you can muddle through.\n\nBy the end of part II, we’ll have a simple, readable implementation. It’s not very fast, but it’s correct. However, we are only able to accomplish that by building on the Java virtual machine’s own runtime facilities. We want to learn how Java itself implements those things.\n\nSo in the next part, we start all over again, but this time in C. C is the perfect language for understanding how an implementation really works, all the way down to the bytes in memory and the code flowing through the CPU.\n\nA big reason that we’re using C is so I can show you things C is particularly good at, but that does mean you’ll need to be pretty comfortable with it. You don’t have to be the reincarnation of Dennis Ritchie, but you shouldn’t be spooked by pointers either.\n\nIf you aren’t there yet, pick up an introductory book on C and chew through it, then come back here when you’re done. In return, you’ll come away from this book an even stronger C programmer. That’s useful given how many language implementations are written in C: Lua, CPython, and Ruby’s MRI, to name a few.\n\nIn our C interpreter, clox, we are forced to implement for ourselves all the things Java gave us for free. We’ll write our own dynamic array and hash table. We’ll decide how objects are represented in memory, and build a garbage collector to reclaim them.\n\nOur Java implementation was focused on being correct. Now that we have that down, we’ll turn to also being fast. Our C interpreter will contain a compiler that translates Lox to an efficient bytecode representation (don’t worry, I’ll get into what that means soon), which it then executes. This is the same technique used by implementations of Lua, Python, Ruby, PHP, and many other successful languages.\n\nWe’ll even try our hand at benchmarking and optimization. By the end, we’ll have a robust, accurate, fast interpreter for our language, able to keep up with other professional caliber implementations out there. Not bad for one book and a few thousand lines of code."
    },
    {
        "link": "https://reddit.com/r/ProgrammingLanguages/comments/11qees1/advice_for_a_firsttime_designer_of_my_own",
        "document": "This post is me just looking for advice from people who've tread the path I'm now on before me. What are common pitfalls I must avoid? What might save me hours of pain? Like avoiding a complete rewrite of my entire project I'm working on for the interpreter. And, so on.\n\nI have no official education on the means of how to design a programming language, or how to write either a compiler or interpreter. While I am a software engineer, I never studied this particular subject in college, or read a book on the subject. However, I have now designed my own original language! It's been a long time in the works, conceptually. I'm not ready to share it with the world yet, though. I want a working interpreter or compiler first, so I can \"prove\" that I actually did it, and that it works in some capacity.\n\nMy thought process was that an interpreter would be easier to write than a compiler. Not really sure if that is true or not. But the interpreter itself is being written in Java. Simply because Java is the language I am most proficient in. I've nearly completed the lexer phase, after 2 weeks and 80 hours of dedicated work. Oh, and lots of tests to ensure accuracy for the various features I want my language to support.\n\nSo far I've just let Wikipedia be my guide. So I have tabs open on lexical analysis, parsing, and abstract syntax trees, for example.\n\nAs far as what principles are guiding the language design... well, without getting into the actual weeds of it all here and now, basically I want it to look and feel as original as it most possibly can. So while it certainly has many features similar to languages I'm familiar with (Java, Python, C), I am attempting to keep the syntax and operators used to define everything as unique and original as I can. So that it looks and feels original, as you are writing it. Less keywords, more symbols. But still (hopefully) easily readable.\n\nA key design principle is if I cannot easily remember what a given feature or operator is meant to be doing, I need to rework it.\n\nI want to look at the language design itself as \"art.\" Functional art, of course. But like I want to push things into a direction of looking less similar than other languages used to solve actual problems. But without falling into esoteric or obfuscated programming language territory. Some things are familiar, like the dot . operator accesses members of types, and parentheses () announces groups of function parameters. And I don't want to mess with numeric literal representation, or change what the otherwise default mathematical operators are (+,-,*,/,%). But it is fun to consider \"Should I support raising to a power with the caret? (10^4) Or, factorials? (10!) Probably not, as ! has special meaning in my language for something else. But I digress.\n\nBut still. When \" and ' aren't used to quote strings, () aren't used to denote order of operations, and commonly-used keywords like if, for, and while aren't used... it just feels like a breath of fresh air to me. Hopefully others enjoy what I create, once it gets released into the wild!\n\nIn particular, any resources in helping me understand how to move from lists of string tokens (or actually, proper objects denoting the type of each string token, like a string literal, a particular operator, a number, and soon) to an abstract syntax tree might be helpful. Since that's where I'm about to be headed next in implementing the parser for my project.\n\nThank you, everyone!"
    },
    {
        "link": "https://dev.to/brainbuzzer/building-my-own-interpreter-part-1-1m5d",
        "document": "I've been programming for several years now and have tackled working with many languages. But one thing that always bugged me was how do these programming languages really work? I know there are compilers and interpreters that do most of the work of converting a simple text file into a working program that the computer can understand, but how exactly do they work?\n\nThis had become increasingly important that I understand the inner workings of a language for what we are doing at Hyperlog. We are building a scoring system that analyzes the skillsets of a programmer by going through multiple metrics. A couple of these metrics are tightly coupled with how you write and structure a program. So in order to get deeper insight, I decided to implement one of my own interpreters.\n\nThat is when I embarked on my journey. I know what you're thinking, why interpreter rather than compiler? I like to take the top-down approach to learning new stuff. The major inspiration behind this was a book by Kulrose, Ross Computer Networking - Top Down Approach. I learned a lot about computer networking in a neat manner while reading that book. And ever since, I've been doing a similar form of learning as often as possible.\n\nWhat tech to use?\n\nI guess this is the most common dilemma of a programmer. While writing this interpreter, I wanted to focus on the inner workings rather than learning a whole new language like assembly.\n\nFor this adventure, I settled on using Golang. Golang gives you the most basic barebones of talking with the computer, and you can write the programs that won't require any imports and installs from external libraries just to make the basic code usable. (Looking at you, JavaScript).\n\nWhat would the interpreter be interpreting?\n\nIn order to properly implement my interpreter, I need some basic syntax for the language. I decided to settle on this syntax for my interpreter which is inspired by a bit of C++, and a bit of Golang.\n\n\n\nThe above program should run successfully using my interpreter. If you notice, there are some pretty basic things there. Let's go through them one by one.\n• Recursive function - The Fibonacci function written above is a recursive function.\n• Implicit returns - When you closely notice add and Fibonacci functions, they do not have a return statement. This part was inspired by my favorite language, ruby.\n• Function as a parameter to other functions - The last part of this program gets a function as a parameter.\n\nWhat really goes in an interpreter?\n\nIf you've been in the programming sphere for a couple of years now, you may have heard of the words \"Lexer\", \"Parser\", and \"Evaluator\". These are the most important parts of an interpreter. So what exactly are they?\n\nLexer converts our source code into a series of tokens. This is particularly helpful to define the basic structure of words that can be used in your program, and classifying those words. All the keywords, variable names, variable types, operators are put in their own token in this step.\n\nOnce your program passes through lexer, the interpreter needs to make sure that you have written the tokens in the correct syntax. A parser basically declares the grammar of the language. The parser is also responsible for building the abstract syntax tree (AST) of your program. Note that the parser does not actually evaluate and run the code, it basically just checks for the grammar. Evaluation happens in the next steps after the parser makes sure that the code is in the correct syntax.\n\nThis is the part that actually looks at how to execute the program. After the program goes through the lexer and parser, evaluator steps in.\n\nStarting out, I built a token system that defined what each character would mean in the language. In order to get there, firstly, I needed a token system that defined the type of the token and the actual token itself. This is particularly useful for throwing error messages like \"Expected token to be an int, found string\".\n\n\n\nThen, there are actual token types:\n\n\n\nIn this block, I think the not so apparent ones are , , and . Illegal token type is assigned whenever we encounter some character that does not fit our accepted string type. Since the interpreter will be using ASCII character set rather than Unicode(for the sake of simplicity), this is important. EOF is do determine the end of file, so that we can hand over the code to our parser in the next step. And IDENT is used for getting the identifier. These are variable and function names that can be declared by the user.\n\nTDD approach never fails. So I first wrote the tests for what exactly do I want as output from the lexer. Below is a snippet from the .\n\n\n\nHere, we're invoking the function for the given input which is of type string. We are invoking the function that helps us get the next token available in the given program.\n\nAlright, so first things first, we are invoking the function, which returns a lexer. But what does a lexer contain?\n\n\n\nHere is the given input. is the current position our lexer is tokenizing, and is just . And lastly, is the character at the current position. Why are all these declared in such a way? Because we'll keep updating our lexer itself, while keeping track of the position we are analyzing at any moment, and adding tokens to a separate array.\n\nPretty easy. Should be self-explanatory. Now, what about the NextToken function? Behold, as there's a ton of code ahead. All of it is explained in the comments. So do read them.\n\n\n\n// Reads the next character and sets the lexer to that position. func (l *Lexer) readChar() { // If the character is last in the file, set the current character // to 0. This is helpful for determining the end of file. if l.readPosition >= len(l.input) { l.ch = 0 } else { l.ch = l.input[l.readPosition] } l.position = l.readPosition l.readPosition += 1 } // Major function ahead! func (l *Lexer) NextToken() token.Token { // This will be the token for our current character. var tok token.Token // We don't want those stinky whitespaces to be counted in our program. // This might not be very useful if we were writing ruby or python-like language. l.skipWhitespace() // Let's determine the token for each character // I think most of it is self explanatory, but I'll just go over once. switch l.ch { case '=': // Here, we are peeking at the next character because we also want to check for `==` operator. // If the next immediate character is not `=`, we just classify this as ASSIGN operator. if l.peekChar() == '=' { ch := l.ch l.readChar() tok = token.Token{Type: token.EQ, Literal: string(ch) + string(l.ch)} } else { tok = newToken(token.ASSIGN, l.ch) } case '+': tok = newToken(token.PLUS, l.ch) case '(': tok = newToken(token.LPAREN, l.ch) case ')': tok = newToken(token.RPAREN, l.ch) case '{': tok = newToken(token.LBRACE, l.ch) case '}': tok = newToken(token.RBRACE, l.ch) case ',': tok = newToken(token.COMMA, l.ch) case ';': tok = newToken(token.SEMICOLON, l.ch) case '/': tok = newToken(token.SLASH, l.ch) case '*': tok = newToken(token.ASTERICKS, l.ch) case '-': tok = newToken(token.MINUS, l.ch) case '<': tok = newToken(token.LT, l.ch) case '>': tok = newToken(token.GT, l.ch) case '!': // Again, we are peeking at the next character because we also want to check for `!=` operator. if l.peekChar() == '=' { ch := l.ch l.readChar() tok = token.Token{Type: token.NOT_EQ, Literal: string(ch) + string(l.ch)} } else { tok = newToken(token.BANG, l.ch) } case 0: // This is important. Remember how we set our character code to 0 if there were no more tokens to be seen? // This is where we declare that the end of file has reached. tok.Literal = \"\" tok.Type = token.EOF default: // Now, why this default case? If you notice above, we have never really declared how do we determine // keywords, identifiers and int. So we go on a little adventure of checking if the identifier or number // has any next words that match up in our token file. // If yes, we give the type exactly equals to the token. // If not, we give it a simple identifier. if isLetter(l.ch) { tok.Literal = l.readIdentifier() tok.Type = token.LookupIdent(tok.Literal) // Notice how we are returning in this function right here. // This is because we don't want to read the next character without returning // this particular token. If this behavior wasn't implemented, there would be a lot // of bugs. return tok } else if isDigit(l.ch) { tok.Type = token.INT tok.Literal = l.readNumber() return tok } else { // If nothing else matches up, we declare that character as illegal. tok = newToken(token.ILLEGAL, l.ch) } } // We keep reading the next characters. l.readChar() return tok } // Look above for how exactly this is used. // It simply reads the complete identifier and // passes it token's LookupIdent function. func (l *Lexer) readIdentifier() string { position := l.position for isLetter(l.ch) { l.readChar() } return l.input[position:l.position] } // We take a peek at the next char. // Helpful for determining the operators. func (l *Lexer) peekChar() byte { if l.readPosition >= len(l.input) { return 0 } else { return l.input[l.readPosition] } } func (l *Lexer) readNumber() string { position := l.position for isDigit(l.ch) { l.readChar() } return l.input[position:l.position] } func isLetter(ch byte) bool { return 'a' <= ch && ch <= 'z' || 'A' <= ch && ch <= 'Z' || ch == '_' } func isDigit(ch byte) bool { return '0' <= ch && ch <= '9' } func newToken(tokenType token.TokenType, ch byte) token.Token { return token.Token{Type: tokenType, Literal: string(ch)} } // Note how we check not just for whitespace, but also for tabs, newlines and // windows based end of lines. func (l *Lexer) skipWhitespace() { for l.ch == ' ' || l.ch == '\\t' || l.ch == '\n\n' || l.ch == '\\r' { l.readChar() } }\n\nOkay, cool, but what about that function? Well, here's the code for that.\n\n\n\nGet it? We are just mapping it the proper , and returning the type accordingly.\n\nAnd voila! That is the lexer to our basic interpreter. I know it seems like I skipped over a large portion of explaining, but if you want to learn more, I highly recommend picking up the Interpreter Book.\n\nStay tuned for part 2 where I'll be implementing the parser for this program."
    },
    {
        "link": "https://reddit.com/r/ProgrammingLanguages/comments/tvwghd/crafting_interpreters_and_compiler_design",
        "document": "Hi! I am a first year undergrad who is very interested in compiler design. I started by reading(and coding) this amazing book called \"Crafting Interpreters\" and so far it has been great. My question is - is this book related to compiler design, or is that a totally different field from interpreters(the name of the book)? I did not dive much about it online before starting the book. The book explains clearly the difference between compilers and interpreters and I understood that, but my question remains that can complete this book and say that I know the basics of compiler design? I am asking because I saw an internship opportunity coming my way and was thinking if I am qualified enough to even apply for it."
    },
    {
        "link": "https://toptal.com/scala/writing-an-interpreter",
        "document": "Some say that “everything boils down to ones and zeros”—but do we really understand how our programs get translated into those bits?\n\nCompilers and interpreters both take a raw string representing a program, parse it, and make sense of it. Though interpreters are the simpler of the two, writing even a very simple interpreter (that only does addition and multiplication) will be instructive. We’ll focus on what compilers and interpreters have in common: lexing and parsing the input.\n\nThe Dos and Don’ts of Writing Your Own Interpreter\n\nReaders may wonder What’s wrong with a regex? Regular expressions are powerful, but source code grammars aren’t simple enough to be parsed by them. Neither are domain-specific languages (DSLs), and a client might need a custom DSL for authorization expressions, for example. But even without applying this skill directly, writing an interpreter makes it much easier to appreciate the effort behind many programming languages, file formats, and DSLs.\n\nWriting parsers correctly by hand can be challenging with all the edge cases involved. That’s why there are popular tools, like ANTLR, that can generate parsers for many popular programming languages. There are also libraries called parser combinators, that enable developers to write parsers directly in their preferred programming languages. Examples include FastParse for Scala and Parsec for Python.\n\nWe recommend that, in a professional context, readers use such tools and libraries to avoid reinventing the wheel. Still, understanding the challenges and possibilities of writing an interpreter from scratch will help developers leverage such solutions more effectively.\n\nAn interpreter is a complex program, so there are multiple stages to it:\n• A lexer is the part of an interpreter that turns a sequence of characters (plain text) into a sequence of tokens.\n• A parser, in turn, takes a sequence of tokens and produces an abstract syntax tree (AST) of a language. The rules by which a parser operates are usually specified by a formal grammar.\n• An interpreter is a program that interprets the AST of the source of a program on the fly (without compiling it first).\n\nWe won’t build a specific, integrated interpreter here. Instead, we’ll explore each of these parts and their common issues with separate examples. In the end, the user code will look like this:\n\nFollowing the three stages, we’ll expect this code to calculate a final value and print . This tutorial happens to use Scala because it’s:\n• Very concise, fitting a lot of code in one screen.\n• Expression oriented, with no need for uninitialized/null variables.\n\nSpecifically, the code here is written in Scala3 optional-braces syntax (a Pythonlike, indentation-based syntax). But none of the approaches are Scala-specific, and Scala is similar to many other languages: Readers will find it straightforward to convert these code samples into other languages. Barring that, the examples can be run online using Scastie.\n\nLastly, the Lexer, Parser, and Interpreter sections have different example grammars. As the corresponding GitHub repo shows, the dependencies in later examples change slightly to implement these grammars, but the overall concepts stay the same.\n\nLet’s say we want to lex this string: . It contains different types of tokens:\n\nWhitespace between tokens will be skipped in this example.\n\nAt this stage, expressions don’t have to make sense; the lexer simply converts the input string into a list of tokens. (The job of “making sense of tokens” is left to the parser.)\n\nWe’ll use this code to represent a token:\n\nEvery token has a type, textual representation, and position in the original input. The position can help end users of the lexer with debugging.\n\nThe token is a special token that marks the end of input. It doesn’t exist in the source text; we only use it to simplify the parser stage.\n\nThis will be the output of our lexer:\n\nWe start with an empty list of tokens, then we go through the string and add tokens as they come.\n\nWe use the lookahead character to decide the type of the next token. Note that the lookahead character is not always the furthest character ahead being examined. Based on the lookahead, we know what the token looks like and use to scan all of the expected characters in the current token, then add the token to the list:\n\nIf the lookahead is whitespace, we skip it. Single letter tokens are trivial; we add them and increment the index. For integers, we only need to take care of the index.\n\nNow we get to something a bit complicated: identifiers versus literals. The rule is that we take the longest possible match and check if it’s a literal; if it’s not, it’s an identifier.\n\nTake care when handling operators like and . There you have to look ahead one more character and see if it’s before concluding that it’s a operator. Otherwise, it’s just a .\n\nWith that, our lexer has produced a list of tokens.\n\nWe have to give some structure to our tokens—we can’t do much with a list alone. For example, we need to know:\n\nWhich expressions are nested? Which operators are applied in what order? Which scoping rules apply, if any?\n\nA tree structure supports nesting and ordering. But first, we have to define some rules for constructing trees. We would like our parser to be unambiguous—to always return the same structure for a given input.\n\nNote that the following parser doesn’t use the previous lexer example. This one is for adding numbers, so its grammar has only two tokens, and :\n\nAn equivalent, using a pipe character ( ) as an “or” symbol like in regular expressions, is:\n\nEither way, we have two rules: one that says that we can sum two s and another that says that can be a token, which here will mean a nonnegative integer.\n\nRules are usually specified with a formal grammar. A formal grammar consists of: The rules themselves, as shown above A starting rule (the first rule specified, per convention) Two types of symbols to define the rules with: Terminals: the “letters” (and other characters) of our language—the irreducible symbols that make up tokens Nonterminals: intermediate constructs used for parsing (i.e., symbols that can be replaced)\n\nOnly a nonterminal can be on the left side of a rule; the right side can have both terminals and nonterminals. In the example above, the terminals are and , and the only nonterminal is . For a wider example, in the Java language, we have terminals like , , , and , and nonterminals like , , and .\n\nThere are many ways we could implement this parser. Here, we’ll use a “recursive descent” parsing technique. It’s the most common type because it’s the simplest to understand and implement.\n\nA recursive descent parser uses one function for each nonterminal in the grammar. It starts from the starting rule and goes down from there (hence “descent”), figuring out which rule to apply in each function. The “recursive” part is vital because we can nest nonterminals recursively! Regexes can’t do that: They can’t even handle balanced parentheses. So we need a more powerful tool.\n\nA parser for the first rule would look something like this (full code):\n\nThe function checks if the lookahead matches the expected token and then moves the lookahead index. Unfortunately, this won’t work yet because we need to fix some problems with our grammar.\n\nThe first issue is the ambiguity of our grammar, which may not be apparent at first glance:\n\nGiven the input , our parser could choose to calculate either the left or the right first in the resulting AST:\n\nThat’s why we need to introduce some asymmetry:\n\nThe set of expressions we can represent with this grammar hasn’t changed since its first version. Only now it is unambiguous: The parser always goes left. Just what we needed!\n\nThis makes our operation left associative, but this will become apparent when we get to the Interpreter section.\n\nUnfortunately, the above fix doesn’t solve our other problem, left recursion:\n\nWe have infinite recursion here. If we were to step into this function, we’d eventually get a stack-overflow error. But parsing theory can help!\n\nSuppose we have a grammar like this, where could be any sequence of terminals and nonterminals:\n\nWe can rewrite this grammar as:\n\nThere, is an empty string—nothing, no token.\n\nLet’s take the current revision of our grammar:\n\nFollowing the method of rewriting parsing rules detailed above, with being our tokens, our grammar becomes:\n\nNow the grammar is OK, and we can parse it with a recursive descent parser. Let’s see how such a parser would look for this latest iteration of our grammar:\n\nHere we use the token to simplify our parser. We are always sure that there is at least one token in our list, so we don’t need to handle a special case of an empty list.\n\nAlso, if we switch to a streaming lexer, we wouldn’t have an in-memory list but an iterator, so we need a marker to know when we come to the end of the input. When we come to the end, the token should be the last token remaining.\n\nWalking through the code, we can see that an expression can be just a number. If there’s nothing left, the next token wouldn’t be a , so we would stop parsing. The last token would be , and we would be done.\n\nIf the input string has more tokens, then these would have to look like . That’s where recursion on kicks in!\n\nNow that we successfully parsed our expression, it’s hard to do anything with it as is. We could put some callbacks in our parser, but that would be very cumbersome and unreadable. Instead, we will return an AST, a tree representing the input expression:\n\nThis resembles our rules, using simple data classes.\n\nOur parser now returns a useful data structure:\n\nFor , , and other implementation details, please see the accompanying GitHub repo.\n\nOur nonterminal can still be improved:\n\nIt’s hard to recognize the pattern it represents in our grammar just by looking at it. It turns out that we can replace this recursion with a simpler construct:\n\nThis construct simply means occurs zero or more times.\n\nNow our full grammar looks like this:\n\nAnd our AST looks nicer:\n\nThe resulting parser is the same length but simpler to understand and use. We’ve eliminated , which is now implied by starting with an empty structure.\n\nWe didn’t even need the class here. We could have just put , or in grammar format, . So why didn’t we?\n\nConsider that if we had multiple possible operators, like or , then we’d have a grammar like this:\n\nIn this case, the AST then needs to accommodate the operator type:\n\nNote that the syntax in the grammar means the same thing as in regular expressions: “one of those three characters.” We’ll see this in action soon.\n\nOur interpreter will make use of our lexer and parser to get the AST of our input expression and then evaluate that AST whichever way we want. In this case, we’re dealing with numbers, and we want to evaluate their sum.\n\nIn the implementation of our interpreter example, we will use this simple grammar:\n\nNow we’ll see how to write an interpreter for the above grammar:\n\nIf we parsed our input into an AST without encountering an error, we’re sure that we’ll always have at least one . Then we take the optional numbers and add them to (or subtract them from) our result.\n\nThe note from the beginning about the left associativity of is now clear: We start from the leftmost number and add others, from left to right. This may seem unimportant for addition, but consider subtraction: The expression is evaluated as and not as !\n\nBut if we want to go beyond interpreting plus and minus operators, there’s another rule to define.\n\nWe know how to parse a simple expression like , but when it comes to , we have a bit of a problem.\n\nMost people agree on the convention that multiplication has higher precedence than addition. But the parser doesn’t know that. We can’t just evaluate it as . Instead we want .\n\nThis means that we need to evaluate multiplication first. Multiplication needs to be further from the root of the AST to force it to be evaluated before addition. For this, we need to introduce yet another layer of indirection.\n\nThis is our original, left-recursive grammar, which has no precedence rules:\n\nFirst, we give it rules of precedence and remove its ambiguity:\n\nThen it gets non-left-recursive rules:\n\nThis leaves us with a concise interpreter implementation:\n\nAs before, the ideas in the requisite lexer and grammar were covered earlier, but readers can find them in the repo if needed.\n\nWe didn’t cover this, but error handling and reporting are crucial features of any parser. As developers, we know how frustrating it can be when a compiler produces confusing or misleading errors. It’s an area that has many interesting problems to solve, like giving correct and precise error messages, not deterring the user with more messages than necessary, and recovering gracefully from errors. It’s up to the developers writing an interpreter or compiler to ensure their future users have a better experience.\n\nIn walking through our example lexers, parsers, and interpreters, we only scratched the surface of the theories behind compilers and interpreters, which cover topics like:\n\nFor further reading, I recommend these resources:\n• Intro to Grammars and Parsing by Paul Klint\n• The notes from the East Carolina University course “Program Translation and Compiling”"
    }
]