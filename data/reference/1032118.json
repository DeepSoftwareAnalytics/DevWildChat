[
    {
        "link": "https://docs.opencv.org/4.x/d2/d96/tutorial_py_table_of_contents_imgproc.html",
        "document": "\n• Learn to change images between different color spaces. Plus learn to track a colored object in a video.\n• Learn to apply different geometric transformations to images like rotation, translation etc.\n• Learn to convert images to binary images using global thresholding, Adaptive thresholding, Otsu's binarization etc\n• Learn to blur the images, filter the images with custom kernels etc.\n• Learn about morphological transformations like Erosion, Dilation, Opening, Closing etc\n• Learn about image pyramids and how to use them for image blending\n• All about Contours in OpenCV\n• All about histograms in OpenCV\n• Meet different Image Transforms in OpenCV like Fourier Transform, Cosine Transform etc.\n• Learn to search for an object in an image using Template Matching\n• Learn to detect lines in an image\n• Learn to detect circles in an image"
    },
    {
        "link": "https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html",
        "document": "\n• Learn how to setup OpenCV-Python on your computer!\n• Here you will learn how to display and save images and videos, control mouse events and create trackbar.\n• In this section you will learn basic operations on image like pixel editing, geometric transformations, code optimization, some mathematical tools etc.\n• In this section you will learn different image processing functions inside OpenCV.\n• In this section you will learn about feature detectors and descriptors\n• In this section you will learn different techniques to work with videos like object tracking etc.\n• In this section we will learn about camera calibration, stereo imaging etc.\n• In this section you will learn different image processing functions inside OpenCV.\n• In this section you will learn different computational photography techniques like image denoising etc.\n• In this section you will learn object detection techniques like face detection etc.\n• In this section, we will see how OpenCV-Python bindings are generated"
    },
    {
        "link": "https://geeksforgeeks.org/opencv-python-tutorial",
        "document": ""
    },
    {
        "link": "https://medium.com/@nimritakoul01/image-processing-using-opencv-python-9c9b83f4b1ca",
        "document": "OpenCV(Open Source Computer Vision Library) is an open source, platform independent library for image processing and computer vision. OpenCV can be used with Python, C++, Java. It was developed by Intel. The library has more than 2500 optimized algorithms, which can be used to detect and recognize faces, identify objects, classify human actions in videos, track camera movements, track moving objects, extract 3D models of objects, produce 3D point clouds from stereo cameras, stitch images together to produce a high resolution image of an entire scene, find similar images from an image database, remove red eyes from images taken using flash, follow eye movements, recognize scenery and establish markers to overlay it with augmented reality, etc.\n\nUse Python 3.* for this notebook. I am executing this code in Google Colab.\n\nA digital image is a grid of dots or picture elements (pixels).\n\nPixel: A pixel (picture element) is a single dot in a digital image. It is the smallest portion (building block) of a digital image.\n\nResolution: Resolution of an image is often measured in dots-per-inch or pixels-per-inch(ppi). Higher the resolution of an image, better it looks. You can zoom into an image to see the individual pixels.\n\nEach image has a certain number of pixels along the width and the height of the image. E.g., an 18x18 image has 18 pixels along the width (18 columns) and 18 pixels along the height (18 rows), with a total of 18x18 = 324 pixels in the image.\n\nEach pixel has a color value(black, white, shades of gray or color) that determines how the pixel looks on screen. Color value is 8 bits or 24 bits depending on the color of the pixel.\n\nIn OpenCV, images are represented as 3 dimensional numpy arrays. First two dimensions represent the number of pixels along the width and the height of the image (number of columns and rows respectively in the numpy array) and the third dimension represents the depth of color for the image.\n\nColor of a pixel can be represented in one of the color models like RGB (Red, Green, Blue), grayscale, CMYK(cyan, magenta, yellow,key/black). In the RGB model, we represent the color of a pixel in terms of the combination of three separate values, each representing the value of red, gree and blue colors at that pixel.\n\nBased on the information represented in each pixel, there are 4 main types of images:\n• Binary or black and white images: Each pixel has one of the two possible values (0 or 1).(One-bit images). In such images, the third dimension of numpy array is 1.\n• Grayscale images: Each pixel uses 8 bits to store gray value of that pixel. It uses 2 to 8 bits to represent the shade information. They shades of gray are determined by values between 0 and 255. They shades of gray are determined by values between 0 and 255.\n• Color images: Every pixel in a color image uses three color values red, green and blue to determine its color. Each color value is represented by 8 bits(0 to 255). Thus every pixel has 24 bit color information. The possible range of colors for every pixel in RGB images is 256256256 = 16777216.\n\nCommon steps in a deep learning project that uses image data:\n\na. Image Loading: Load the input images using OpenCV or other libraries.\n\nc. Feature Extraction: Extract relevant features from images, often using pre-trained deep learning models (e.g., CNNs).\n\nd. Data Splitting: Divide the dataset into training, validation, and test sets.\n\ne. Model Training: Train the deep learning model using the training dataset.\n\nf. Model Evaluation: Evaluate the model performance on the validation set.\n\nh. Testing: Assess the model’s performance on the test set to gauge its generalization capability.\n\ni. Post-processing: Apply any necessary post-processing techniques on the model’s output.\n\nCommon types of image preprocessing that need to be done during a computer vision project\n• Resizing: Adjust the image size to meet the input requirements of the deep learning model.\n• Normalization: Scale pixel values to a standard range (e.g., [0, 1] or [-1, 1]) to enhance model convergence.\n• Data Augmentation: Generate new training samples by applying random transformations like rotation, flipping, and zooming to increase dataset diversity.\n• Cropping: Crop images to focus on the region of interest or to obtain consistent input sizes.\n• Gray Scaling: Convert color images to grayscale, reducing computational complexity and focusing on intensity information.\n• Image Rotation: Rotate images to account for variations in orientation within the dataset.\n• Noise Reduction:Remove or reduce noise in images through techniques like median filtering or denoising algorithms.\n• Edge Detection:Highlight edges in images using techniques like the Sobel or Canny edge detectors.\n• Color Space Conversion: Convert images between color spaces (e.g., RGB to HSV) to emphasize or extract specific color information.\n• Normalization: Standardize pixel values by subtracting mean and dividing by standard deviation.\n• Thresholding: Convert images to binary format by setting a threshold, useful for segmenting objects from the background.\n• Morphological Transformations:Perform operations like dilation and erosion to manipulate image structures.\n• Image Inversion: Invert pixel values to highlight different aspects of the image.\n• Centering:Center images or objects within the frame for improved consistency.\n• Hue, Saturation, and Value (HSV) Adjustment:Modify color components in the HSV color space to control brightness, saturation, and hue.\n\nAbove steps of image processing help with computer vision tasks like\n• Image enhancement: To make images more readable for machines or humans. E.g., improving brightness, contrast, color balancing or correction.\n• Image restoration: To recover the obscure parts of image like those caused by motion blur, noise etc.\n• Segmentation: Partitioning an image into multiple objects present in the image.\n• Representation and description of objects in an image: based on boundaries or pixel values.\n\nAlright, next we will see the code.\n\nThe first step is to install OpenCV Python. You can do this using pip.\n\nThen you can import the library opencv-python using the statement\n\nis the name of opencv-python library.\n\nAlso import other required libraries like matplotlib.pyplot, numpy etc.\n\nImportant Note: opencv-python function cv2.imshow() sometimes faces challenges like kernel crash in Jupyter notebook and Colab.\n\nTo prevent this, I will use cv2_imshow() from the package google.colab.patches in this notebook.\n\nIf you are executing this in your local machine you can use the below function in place of cv2.imshow() to display images using matplotlib.pyplot.\n\nSince I am running this code in Colab, I don’t need to use cv2.waitKey() and cv2.destroyAllWindows() methods as the images are not displayed in separate windows in our case.\n• Reading an image file and converting to grayscale and black and white:\n\nYou can also use opencv to read and manipulate video files. A video is just a sequence of picture frames which are quickly changing.\n\nUse cv2.VideoCapture() to open a video file.Below cell demonstrates opening a video file and displaying each of its frames separately. In the remaining part of this notebook, we will focus on image related operations only.\n\nYou can also display a cv2 image using matplotlib.\n\nSee the shape of numpy array that corresponds to your image\n\nAn image is internally represented as a numerical array (3D), let us see an example below:\n\nLet us print intensity values of a few pixels in the image\n\nLet us see some image pre-processing operations\n\n2. Image resizing — downscaling, reducing the size or upscaling , increasing the size\n\nA color space is a specific way of representing the color information of an image. It defines how colors are encoded and stored as numerical values within the image data.\n\nOpenCV supports various color spaces. The default color space of OpenCV is BGR (Blue, Green, Red).\n• RGB (Red, Green, Blue): This is the most common color space used in computer vision and image processing. Each pixel is represented by three values (intensity levels) corresponding to the Red, Green, and Blue color channels.\n• BGR (Blue, Green, Red): Each pixel in a BGR image is represented by three channels: one for blue, one for green, and one for red, each ranging in value from 0 to 255. BGR is suitable for general image processing tasks but can be less intuitive for tasks like color detection or manipulation. BGR is default color space for OpenCV.\n• HSV (Hue, Saturation, Value): This color space separates the color information into three channels: hue, saturation, and value. Hue represents the “color tint” (e.g., red, green, blue), saturation represents the intensity of the color (e.g., vivid vs. dull), and value represents the brightness (e.g., light vs. dark). HSV is often preferred for tasks like object tracking, color thresholding, and image segmentation.\n• Lab (CIELAB): In this color space the distance between two points in the roughly corresponds to the perceived difference in color between those points. This makes it ideal for tasks like color matching and image similarity comparison. However, it is computationally more expensive than other color spaces. It consists of three channels: L* (luminance), a* (green to red), and b* (blue to yellow).\n• YCrCb: This color space represents an image using its luminance (Y) and two chrominance channels (Cr and Cb). YCrCb is commonly used in video compression and processing.\n• Grayscale: Grayscale is a single-channel color space where pixel values represent the intensity of light.\n\nOpenCV allows you to convert images from one color space to another using functions like cv2.cvtColor().\n\nFirst, let us again open an image, since default color space of OpenCV is BGR, this image is opened in BGR:\n\nNext, we will convert the image to LAB color space\n\nLet us segment the image using color spaces\n\nTry some more thresholding methods:\n\nThe Sobel operator is a fundamental edge detection algorithm. It operates by convolving the image with a pair of 3x3 convolution kernels,one for detecting edges in the horizontal (X) direction and the other for the vertical (Y) direction.\n\nThese kernels are used to compute the gradient of the image intensity in the X and Y directions, respectively.\n\nIn OpenCV, the cv2.Sobel function is used to apply the Sobel operator.\n\nThe Sobel operator is often used as a pre-processing step for more advanced edge detection methods or as a feature in various computer vision applications. It helps to highlight the regions in an image where the intensity changes rapidly, which often corresponds to edges or boundaries between different objects or textures.\n\nContour detection means identification and extraction of boundaries of objects in an image. They represent shape and structure of objects.\n\nIn OpenCv, cv2.findContours() is used for contour detection. It takes a binary image as input and outputs a list of contours along with hierarchy information."
    },
    {
        "link": "https://docs.opencv.org/4.x/d3/df2/tutorial_py_basic_ops.html",
        "document": "Almost all the operations in this section are mainly related to Numpy rather than OpenCV. A good knowledge of Numpy is required to write better optimized code with OpenCV.\n\nYou can access a pixel value by its row and column coordinates. For BGR image, it returns an array of Blue, Green, Red values. For grayscale image, just corresponding intensity is returned.\n\nYou can modify the pixel values the same way.\n\nNumpy is an optimized library for fast array calculations. So simply accessing each and every pixel value and modifying it will be very slow and it is discouraged.\n\nImage properties include number of rows, columns, and channels; type of image data; number of pixels; etc.\n\nThe shape of an image is accessed by img.shape. It returns a tuple of the number of rows, columns, and channels (if the image is color):\n\nTotal number of pixels is accessed by :\n\nSometimes, you will have to play with certain regions of images. For eye detection in images, first face detection is done over the entire image. When a face is obtained, we select the face region alone and search for eyes inside it instead of searching the whole image. It improves accuracy (because eyes are always on faces :D ) and performance (because we search in a small area).\n\nROI is again obtained using Numpy indexing. Here I am selecting the ball and copying it to another region in the image:\n\nSometimes you will need to work separately on the B,G,R channels of an image. In this case, you need to split the BGR image into single channels. In other cases, you may need to join these individual channels to create a BGR image. You can do this simply by:\n\nSuppose you want to set all the red pixels to zero - you do not need to split the channels first. Numpy indexing is faster:\n\ncv.split() is a costly operation (in terms of time). So use it only if necessary. Otherwise go for Numpy indexing.\n\nIf you want to create a border around an image, something like a photo frame, you can use cv.copyMakeBorder(). But it has more applications for convolution operation, zero padding etc. This function takes following arguments:\n• top, bottom, left, right - border width in number of pixels in corresponding directions\n• borderType - Flag defining what kind of border to be added. It can be following types:\n• cv.BORDER_CONSTANT - Adds a constant colored border. The value should be given as next argument.\n• cv.BORDER_REFLECT - Border will be mirror reflection of the border elements, like this : fedcba|abcdefgh|hgfedcb\n• cv.BORDER_REFLECT_101 or cv.BORDER_DEFAULT - Same as above, but with a slight change, like this : gfedcb|abcdefgh|gfedcba\n• cv.BORDER_REPLICATE - Last element is replicated throughout, like this: aaaaaa|abcdefgh|hhhhhhh\n• cv.BORDER_WRAP - Can't explain, it will look like this : cdefgh|abcdefgh|abcdefg\n• value - Color of border if border type is cv.BORDER_CONSTANT\n\nBelow is a sample code demonstrating all these border types for better understanding:\n\nSee the result below. (Image is displayed with matplotlib. So RED and BLUE channels will be interchanged):"
    },
    {
        "link": "https://geeksforgeeks.org/dividing-images-into-equal-parts-using-opencv-in-python",
        "document": "In this article, we are going to see how to divide images into equal parts using OpenCV in Python. We are familiar with python lists and list slicing in one dimension. But here in the case of images, we will be using 2D list comprehension since images are a 2D matrix of pixel intensity.\n\nFirst, select an image as input and read it using the cv2.imread() function. Then extract the dimensions of the image by using img.shape command and store the value into h, w, channels respectively. After that performing list slicing on the image will produce the desired result. Why? As mentioned earlier images are nothing but a 2D matrix of colour pixel intensities. Hence dividing images into two or more parts means basically slicing a matrix into two or more parts.\n\nFor horizontal division, we need to divide the width by a factor of 2 and then take the matrix till that index and store it in a variable ‘left_part’ in this case. It goes something like this:\n\nAfter storing the two parts in different variables, we are displaying them using the cv2.imshow() function which takes two arguments 1st, the title of the image which is a string, and 2nd is the image to be displayed. Example:\n\nTo save the resultant image on the computer we use the cv2.imwrite() function which also takes two arguments one is a string and the other is the image to be saved. Example:\n\nThe last function cv2.waitKey() takes only one argument that is time in ms. cv2.waitKey(1) means all the cv2.imshow() will display the images for 1ms and then will close the window automatically, whereas cv2.waitKey(0) will display the window till infinity, i.e. unless the user presses exit."
    },
    {
        "link": "https://stackoverflow.com/questions/19181485/splitting-image-using-opencv-in-python",
        "document": "As mentioned in the documentation tutorial, the cv2.split() is a costly operation in terms of performance(time) if you don't want to operate on all the channels but only one/two, so the numpy indexing is preferred:\n\nRemember that opencv reads the images as BGR instead of RGB\n\nEdit: @Mitch McMabers, Thanks for pointing this out. Please use this method for max efficiency if you want to work on just one/two channels separately. If you want to operate on all three channels, access the channels using cv2.split() as mentioned in @jabaldeno's answer."
    },
    {
        "link": "https://stackoverflow.com/questions/66397938/splitting-an-image-in-python",
        "document": "For the splitting part, I will just reference this Q&A. Instead of saving the coordinates of each subimage in the final list, I saved the actual subimages there.\n\nOn the reconstruction part: If the reconstruction method knows the final (or original) image size, you can simply reconstruct the number of subimages in each direction and their corresponding overlaps; it's basically the same code as for the splitting. From that information, you can easily reconstruct the original image.\n\nIf the reconstruction method only gets a list of subimages, it'll get complicated! From my point of view, for arbitrary images (and, following, their subimages), you'll need advanced image stitching techniques.\n• which will be stored lossless (internally or externally), and\n\nI came up with the following idea:\n• From the first two subimages you can brute-force search the first vertical overlap in the first column. That works, because the above splitting method saves subimages in that order, and because of the above two assumptions.\n• Then, you can use that vertical overlap (or the decrement) to find the remainding subimages of the first column. That works, because the above splitting method guarantees the most equal overlap (within +/- 1), and the first vertical overlap in each column is always the larger one.\n• Now, when the end of the first column is reached, you won't find an appropriate vertical overlap with the next subimage, since it's the first image of the second row. Notice, that only works, if the above assumption 2. holds. Looking at the image in alexzander's answer, this procedure will fail. The lower part of the last subimage of the first column (all zeros) is equal to the top part of the first subimage of the second column.\n• When reached the end of the first column, we can determine the number of vertical overlaps, thus the number of subimages per column. From that, we also know the number of subimages per row. So, now, we do the whole thing again for finding the proper horizontal overlaps.\n• Having all the horizontal and vertical overlaps, we can cut all subimages accordingly and stack them to reconstruct the original image.\n\nYes, that's a lot of work to do, but you don't need advanded image stitching, and I assume, that this reconstruction method will work for most real world images. Any large single-color backgrounds (in combination with small overlaps) or any compression between generating the subimages and reconstructing the original image will lead to malfunction.\n\nAs usual, this is my test image:\n\nAnd, that's the output of the code:\n\nYou see, all subimages have the desired shape of and the reconstructed image is the same as the original image."
    },
    {
        "link": "https://analyticsvidhya.com/blog/2021/09/a-beginners-guide-to-image-processing-with-opencv-and-python",
        "document": "Ever wondered what stories an image holds? Images contain valuable information, but can machines interpret them like humans? This post explores Computer Vision (CV), a branch of AI that helps machines analyze images and videos. If you’re ready to start, make sure you have Python set up in a tool like Jupyter Notebook, PyCharm, or VS Code. We’ll begin by installing OpenCV and running code in Jupyter Notebook.\n\nThis article was published as a part of the Data Science Blogathon.\n\nOpenCV, a widely utilized pre-built open-source CPU-only library, plays a crucial role in computer vision, machine learning, and image processing applications. It boasts compatibility with several programming languages, including Python.\n\nRun any of these commands on your terminal or if you are using Anaconda Navigator – Jupyter Notebook, you can change “pip” with the “conda” command and install the same.\n\nWhat is a package in Python? A package in Python is a collection of modules that contain pre-written scripts. These packages help us to import modules entirely or even individually. We can import the package by calling the “cv2” module like this:\n\nDigital images could be classified into; colour images, grey-scale images, binary images, and multispectral images. A color image includes the color information for each pixel. Images having shades of grey as their only color are grayscale images while a binary image has exactly two colors, mostly black and white pixels. Multispectral images are images that capture image data ranging across the electromagnetic spectrum within some specific wavelength.\n\nLet’s get back to the coding part and read an image, for example, the image is shown below:\n\nThis is an image of a mandrill. I am currently reading the image from my local directory.\n\nThe usage code look likes this:\n\nHere we are using the “imread” method of the cv2 package to read the image and the first parameter corresponds to the path of the image with its filename and extension, and the second one is the flag that you can set which tells the way, how to read in the image. If you like, you can replace the absolute path to the image here and try reading it from your local computer or even from the internet! If the image is present in your current working directory, you only need to specify the image name with its extension type.\n\nAs far as the second parameter is concerned, if you like to read it as a grayscale image, you can specify the parameter to 0, -1 for reading the image as unchanged (reads the image as alpha or transparency channel if it is there) and by default, it is 1, as a color image. You can also try other parameters from this link under the ImreadModes.\n\nEvery image has a shape. The length of boundaries exhibited by the picture might be referred to as the shape i.e, the height and width. Now that you know how to read an image, how about we check the shape of the image?\n\nis the basic way to print the shape of the image, but we can extract the shape using:\n\nto get a better understanding.\n\nFor the color and unchanged mode, it would return 3 values including the height, width, and the number of channels present in the image. If you have used the grayscale mode, the shape would be 2, which will return the height and width of the image, but then you just have to use h and w variables only (exclude using “c” ), else you might be getting a value error saying “not enough values to unpack (expected 3, got 2)”.\n\nWe can know the type of the image using the “type” method. Using this method helps us to know how the image data is represented. Run the code as follows:\n\nThe result might be like this:\n\nas the image type. It is a multidimensional container of items of the same type and size. You can refer more to the N-dimensional array at this link.\n\nData type of the image you have just read\n\nSince the image is an N-dimensional array, we can check the data type of the image:\n\nWe can think of an image as a set of small samples. These samples are called pixels. For a better understanding, try zoom in on an image as much as possible. We can see the same divided into different squares. These are the pixels and when they are combined together they form an image.\n\nOne of the simple ways to represent an image is, in the form of a matrix. We can even create an image using a matrix and save it! Will show you how, later in this post. Take a look at this picture below:\n\nThis picture is rather an example of how the matrix representation of an image looks like. On the left, is an image of Lincoln, in the middle, the pixel values are labeled with numbers from 0 to 255, denoting their intensity or brightness and on the right, the numbers in matrix form themselves. Each value in the matrix corresponds to a pixel, which is the smallest element of information present in an image. Check out the image pixel values by just printing the variable that you loaded the image!\n\nImage resolution could be defined as the number of pixels present in an image. The quality of the image increases when the number of pixels increases. We have seen earlier, the shape of the image which gives the number of rows and columns. This could be said as the resolution of that image.\n• Multiply rows × columns to find the total number of pixels in an image.\n\nLet us see how to display the image in a window. For that, we have to create a GUI window to display the image on the screen. The first parameter has to be the title of the GUI window screen, specified in string format. We can show the image in a pop-up window using the cv2.imshow() method. But, when you try to close it, you might feel stuck with its window. So to combat that, we can use a simple “waitKey” method. Try out this code part in new a cell:\n\nHere, we have specified the parameter ‘0’ in the “waitKey” to keep the window open until we close it. (You can also give the time in milliseconds, instead of 0, specifying how much time it should be opened.) After that, we can assign the variable to act for closing the window when we press the ‘ESC’ key or the key ‘q’. The cv2.destroAllWindows() method is used for closing or deleting the GUI windows from the screen/memory.\n\nBefore saving the image, how about converting the image to grayscale and then save it? Convert the image to grayscale using:\n\nNow we can save the image:\n\nand check the image being saved in the current working directory. The first parameter corresponds to the name of the file in which the image is to be saved and the second parameter is the variable that contains the image (pixel information).\n\nExtracting the image bit planes and reconstructing them\n\nWe can divide an image into different levels of bit planes. For example, divide an image into 8-bit (0-7) planes, where the last few planes contain the majority of information for an image.\n\nFor doing this, we can import two more packages:\n\nIf you get an error while importing any of the packages, you can install them using:\n\nNow we are defining a function to extract each of the 8 level bit planes of the image.\n\nNow we are ready to call the function.\n\nWe have reconstructed the image (almost similar) using the last three bit planes i.e, the sixth, seventh, and eighth planes. And the result looks like this:\n\nHow about we construct a small image on our own? Let’s try it now!\n\nWe can try to generate a synthetic image containing four concentric squares with four different pixel intensity values,\n\nThe resulting image would be looking like this:\n\nWe can take look at using various filters and transformations on images in another article.\n\nThank you for reading the article.\n\nFeel free to add your suggestions under the comments and do share if you like this article. The complete Python programming code in Jupyter Notebook, mentioned in this article is available on my Github repository."
    },
    {
        "link": "https://pyimagesearch.com/2021/01/23/splitting-and-merging-channels-with-opencv",
        "document": "In this tutorial, you will learn how to split and merge channels with OpenCV.\n\nAs we know, an image is represented by three components: a Red, Green, and Blue channel.\n\nAnd while we’ve briefly discussed grayscale and binary representations of an image, you may be wondering:\n\nSince images in OpenCV are internally represented as NumPy arrays, accessing each channel can be accomplished in multiple ways, implying multiple ways to skin this cat. However, we’ll focus on the two main methods that you should use: and .\n\nBy the end of this tutorial, you will have a good understanding of how to split images into channels using and merge the individual channels back together with .\n\nTo learn how to split and merge channels with OpenCV, just keep reading.\n\nIn the first part of this tutorial, we will configure our development environment and review our project structure.\n\nWe’ll then implement a Python script that will:\n• Split it into its respective Red, Green, and Blue channels\n• Display each channel onto our screen for visualization purposes\n• Merge the individual channels back together to form the original image\n\nTo follow this guide, you need to have the OpenCV library installed on your system.\n\nIf you need help configuring your development environment for OpenCV, I highly recommend that you read my pip install OpenCV guide — it will have you up and running in a matter of minutes.\n\nAll that said, are you:\n• Wanting to skip the hassle of fighting with the command line, package managers, and virtual environments?\n• Ready to run the code right now on your Windows, macOS, or Linux system?\n\nGain access to Jupyter Notebooks for this tutorial and other PyImageSearch guides that are pre-configured to run on Google Colab’s ecosystem right in your web browser! No installation required.\n\nAnd best of all, these Jupyter Notebooks will run on Windows, macOS, and Linux!\n\nLet’s start by reviewing our project directory structure. Be sure to use the “Downloads” section of this tutorial to download the source code and example images:\n\nInside our project, you’ll see that we have a single Python script, , which will show us:\n• How to split an input image ( and ) into their respective Red, Green, and Blue channels\n• Visualize each of the RGB channels\n• Merge the RGB channels back into the original image\n\nHow to split and merge channels with OpenCV\n\nA color image consists of multiple channels: a Red, a Green, and a Blue component. We have seen that we can access these components via indexing into NumPy arrays. But what if we wanted to split an image into its respective components?\n\nAs you’ll see, we’ll make use of the function.\n\nBut for the time being, let’s take a look at an example image in Figure 2:\n\nHere, we have (in the order of appearance) Red, Green, Blue, and original image of myself on a trip to Florida.\n\nBut given these representations, how do we interpret the different channels of the image?\n\nLet’s take a look at the sky’s color in the original image (bottom-right). Notice how the sky is a slightly blue tinge. And when we look at the blue channel image (bottom-left), we see that the blue channel is very light in the region that corresponds to the sky. This is because the blue channel pixels are very bright, indicating that they contribute heavily to the output image.\n\nThen, take a look at the black hoodie that I am wearing. In each of the Red, Green, and Blue channels of the image, my black hoodie is very dark — indicating that each of these channels contributes very little to the hoodie region of the output image (giving it a very dark black color).\n\nWhen you investigate each channel individually rather than as a whole, you can visualize how much each channel contributes to the overall output image. Performing this exercise is extremely helpful, especially when applying methods such as thresholding and edge detection, which we’ll cover later in this module.\n\nNow that we have visualized our channels, let’s examine some code to accomplish this for us:\n\nLines 2-4 import our required Python packages. We then parse our command line arguments on Lines 7-10.\n\nWe only need a single argument here, , which points to our input image residing on disk.\n\nLet’s now load this image and split it into its respective channels:\n\nLine 15 loads our from disk. We then split it into its Red, Green, and Blue channel components on Line 16 with a call to .\n\nUsually, we think of images in the RGB color space — the red pixel first, the green pixel second, and the blue pixel third. However, OpenCV stores RGB images as NumPy arrays in reverse channel order. Instead of storing an image in RGB order, it stores the image in BGR order. Thus we unpack the tuple in reverse order.\n\nLines 19-22 then show each channel individually, as in Figure 2.\n\nWe can also merge the channels back together again using the function:\n\nWe simply specify our channels, again in BGR order, and then takes care of the rest for us (Line 25)!\n\nNotice how we reconstruct our original input image from each of the individual RGB channels:\n\nThere is also a second method to visualize each channel’s color contribution. In Figure 3, we simply examine the single-channel representation of an image, which looks like a grayscale image.\n\nHowever, we can also visualize the color contribution of the image as a full RGB image, like this:\n\nUsing this method, we can visualize each channel in “color” rather than “grayscale.” This is strictly a visualization technique and not something we would use in a standard computer vision or image processing application.\n\nBut that said, let’s investigate the code to see how to construct this representation:\n\nTo show the actual “color” of the channel, we first need to take apart the image using . We need to reconstruct the image, but this time, having all pixels but the current channel as zero.\n\nOn Line 31, we construct a NumPy array of zeros, with the same width and height as our original .\n\nThen, to construct the Red channel representation of the image, we make a call to , specifying our array for the Green and Blue channels.\n\nWe take similar approaches to the other channels in Lines 33 and 34.\n\nYou can refer to Figure 5 for this code’s output visualization.\n\nTo split and merge channels with OpenCV, be sure to use the “Downloads” section of this tutorial to download the source code.\n\nLet’s execute our script to split each of the individual channels and visualize them:\n\nYou can refer to the previous section to see the script’s output.\n\nIf you wish to supply a different image to the script, all you need to do is supply the command line argument:\n\nHere, you can see that we’ve taken the input image and split it into its respective Red, Green, and Blue channel components:\n\nAnd here is the second visualization of each channel:\n\nIn this tutorial, you learned how to split and merge image channels using OpenCV and the and functions.\n\nWhile there are NumPy functions you can use for splitting and merging, I strongly encourage you to use the and functions — they tend to be easier to read and understand from a code perspective.\n\nTo download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), simply enter your email address in the form below!"
    }
]