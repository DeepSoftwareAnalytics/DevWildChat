[
    {
        "link": "https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html",
        "document": "Go to the end to download the full example code. or to run this example in your browser via JupyterLite or Binder\n\nDemonstration of several covariances types for Gaussian mixture models.\n\nSee Gaussian mixture models for more information on the estimator.\n\nAlthough GMM are often used for clustering, we can compare the obtained clusters with the actual classes from the dataset. We initialize the means of the Gaussians with the means of the classes from the training set to make this comparison valid.\n\nWe plot predicted labels on both training and held out test data using a variety of GMM covariance types on the iris dataset. We compare GMMs with spherical, diagonal, full, and tied covariance matrices in increasing order of performance. Although one would expect full covariance to perform best in general, it is prone to overfitting on small datasets and does not generalize well to held out test data.\n\nOn the plots, train data is shown as dots, while test data is shown as crosses. The iris dataset is four-dimensional. Only the first two dimensions are shown here, and thus some points are separated in other dimensions."
    },
    {
        "link": "https://reddit.com/r/reinforcementlearning/comments/xp0by9/why_we_use_diagonal_gaussian_rather_than",
        "document": "Is that proved empirically better? or is there any theory related to that?"
    },
    {
        "link": "https://labex.io/tutorials/ml-gaussian-mixture-model-covariances-49134",
        "document": "This tutorial demonstrates the use of different covariance types for Gaussian mixture models (GMMs). GMMs are often used for clustering, and we can compare the obtained clusters with the actual classes from the dataset. We initialize the means of the Gaussians with the means of the classes from the training set to make this comparison valid. We plot predicted labels on both training and held-out test data using a variety of GMM covariance types on the iris dataset. We compare GMMs with spherical, diagonal, full, and tied covariance matrices in increasing order of performance.\n\nAlthough one would expect full covariance to perform best in general, it is prone to overfitting on small datasets and does not generalize well to held-out test data.\n\nOn the plots, train data is shown as dots, while test data is shown as crosses. The iris dataset is four-dimensional. Only the first two dimensions are shown here, and thus some points are separated in other dimensions.\n\nAfter the VM startup is done, click the top left corner to switch to the Notebook tab to access Jupyter Notebook for practice.\n\nSometimes, you may need to wait a few seconds for Jupyter Notebook to finish loading. The validation of operations cannot be automated because of limitations in Jupyter Notebook.\n\nIf you face issues during learning, feel free to ask Labby. Provide feedback after the session, and we will promptly resolve the problem for you.\n\n%%%%{init: {'theme':'neutral'}}%%%% flowchart RL sklearn((\"Sklearn\")) -.-> sklearn/CoreModelsandAlgorithmsGroup([\"Core Models and Algorithms\"]) sklearn((\"Sklearn\")) -.-> sklearn/ModelSelectionandEvaluationGroup([\"Model Selection and Evaluation\"]) ml((\"Machine Learning\")) -.-> ml/FrameworkandSoftwareGroup([\"Framework and Software\"]) sklearn/CoreModelsandAlgorithmsGroup -.-> sklearn/mixture(\"Gaussian Mixture Models\") sklearn/ModelSelectionandEvaluationGroup -.-> sklearn/model_selection(\"Model Selection\") ml/FrameworkandSoftwareGroup -.-> ml/sklearn(\"scikit-learn\") subgraph Lab Skills sklearn/mixture -.-> lab-49134{{\"Gaussian Mixture Model Covariances\"}} sklearn/model_selection -.-> lab-49134{{\"Gaussian Mixture Model Covariances\"}} ml/sklearn -.-> lab-49134{{\"Gaussian Mixture Model Covariances\"}} end"
    },
    {
        "link": "https://ethen8181.github.io/machine-learning/clustering/GMM/GMM.html",
        "document": "Clustering methods such as K-means have hard boundaries, meaning a data point either belongs to that cluster or it doesn't. On the other hand, clustering methods such as Gaussian Mixture Models (GMM) have soft boundaries, where data points can belong to multiple cluster at the same time but with different degrees of belief. e.g. a data point can have a 60% of belonging to cluster 1, 40% of belonging to cluster 2. Apart from using it in the context of clustering, one other thing that GMM can be useful for is outlier detection: Due to the fact that we can compute the likelihood of each point being in each cluster, the points with a \"relatively\" low likelihood (where \"relatively\" is a threshold that we just determine ourselves) can be labeled as outliers. But here we'll focus on the clustering application.\n\nBut that was in one dimesion, what about two, three, four ... It turns out the univariate (one-dimensional) gaussian can be extended to the multivariate (multi-dimensional) case. The form of a d-dimensional gaussian: $$N(x \\mid \\mu,\\Sigma) = \\frac{1}{(2\\pi)^{d/2}\\sqrt{|\\Sigma|}}exp(-\\dfrac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu))$$ In higher dimensions, a Gaussian is fully specified by a mean vector $\\boldsymbol{\\mu}$ and a d-by-d covariance matrix, $\\boldsymbol{\\Sigma}$ (do not confused this symbol with $\\sum$, which is used for denoting summing a bunch of stuff). $|\\Sigma|$ refers to the determinant of the covariance matrix e.g. In two dimension, the Gaussian's parameters might look like this: $$N \\begin{bmatrix} \\begin{pmatrix} \\mu_1\\\\ \\mu_2 \\end{pmatrix}\\, , \\begin{pmatrix} \\sigma^2_1 & \\sigma_{12} \\\\ \\sigma_{21} & \\sigma^2_2 \\end{pmatrix} \\end{bmatrix} $$ The mean vector, containing elements $\\mu_1$ and $\\mu_1$ centers the distribution along every dimension. On the other hand, the covariance matrix specifies the spread and orientation of the distribution. Along the diagonal of this covariance matrix we have the variance terms $\\sigma^2_1$ and $\\sigma^2_2$ representing the shape (spread) along each of the dimensions. But then we also have the off-diagonal terms, $\\sigma_{12}$ and $\\sigma_{21}$ (these two thing actually take the same value because this a symmetric matrix) that specify the correlation structure of the distribution. Let's look at a few examples of covariance structures that we could specify.\n\nOne way to view a Gaussian distribution in two dimensions is what's called a contour plot. The coloring represents the region's intensity, or how high it was in probability. So in the plot above, the center area that has dark red color is the region of highest probability, while the blue area corresponds to a low probability. The first plot is refered to as a Spherical Gaussian, since the probability distribution has spherical (circular) symmetry. The covariance matrix is a diagonal covariance with equal elements along the diagonal. By specifying a diagonal covariance, what we're seeing is that there's no correlation between our two random variables, because the off-diagonal correlations takes the value of 0. Furthermore, by having equal values of the variances along the diagonal, we end up with a circular shape to the distribution because we are saying that the spread along each one of these two dimensions is exactly the same. In contrast, the middle plot's covariance matrix is also a diagonal one, but we can see that if we were to specify different variances along the diagonal, then the spread in each of these dimensions is different and so what we end up with are these axis-aligned ellipses. This is refered to as a Diagonal Gaussian. Finally, we have the Full Gaussian. A full covariance matrix allows for correlation between our two random variables (non zero off diagonal value) we can provide these non-axis aligned ellipses. So in this example that we're showing here, these two variables are negatively correlated, meaning if one variable is high, it's more likely that the other value is low.\n\nThat's great!! But this is all based on knowing which points came from which distribution. Now, what if we have just a bunch of data points, we don't know which one came from which source. Can we trace back these guassian sources? Hmm ..., a bit trickier isn't it? On the other hand, what if someone came along and actually told us the parameters for the Gaussian, then we could actually figure out which points is more likely to come from which Gaussian. Given these information, we know have a chicken and egg problem. If someone told us which point came from which source, we can easily estimate the means and variance. Or if someone told us the mean and the variance for the Gaussians then we can figure out the probability of each point coming from each Gaussians. Unfortunately, we have neither ..... This is the exact situation we're in when doing GMM. We have a bunch of data points, we suspect that they came from $K$ different guassians, but we have no clue which data points came from which guassian. To solve this problem, we use the EM algorithm. The way it works is that it will start by placing guassians randomly (generate random mean and variance for the guassian). Then it will iterate over these two steps until it converges.\n• E step: With the current means and variances, it's going to figure out the probability of each data point $x_i$ coming from each guassian.\n• M step: Once it computed these probability assignments it will use these numbers to re-estimate the guassians' mean and variance to better fit the data points.\n\nWe'll now formalize this. Recall that GMM's goal is to output a set of soft assignments per data point (allocating the probability of that data point belonging to each one of the clusters). To begin with, let's just assume we actually know the parameters $\\pi_k$, $\\mu_k$ and $\\Sigma_k$ (from some random initialization) and we need a formula to compute the soft assignments having fixed the values of all the other parameters. $$r_{ik} = \\frac{ \\pi_k N(x_i \\mid \\mu_k,\\Sigma_k) }{ \\sum_{j=1}^K \\pi_j N(x_i \\mid \\mu_j,\\Sigma_j) }$$ Let's break this down piece by piece. The soft assignments are quantified by the responsibility vector $r$. For each observation $i$, we form a responsibility vector with elements $r_{i1}$, $r_{i2}$, all the way up to $r_{iK}$. Where $K$ is the total number of clusters, or often referred to as the number of components. The cluster responsibilities for a single data point $i$ should sum to 1. The name Mixture of Gaussians comes from the notion that, in order to model more complex data distribution, we can use a linear combination of several Gaussians instead of using just one. To compute the mixture of Gaussians, we introduce a set of cluster weights, $\\pi_k$, one for each cluster $k$. Where $\\sum_{k=1}^K \\pi_k = 1$ and $0 \\leq \\pi_k \\leq 1$ (meaning that the sum must add up to one and each of them is between 0 and 1). This parameter tells us what's the prior probability that the data point in our data set $x$ comes from the $k_{th}$ cluster. We can think it as controlling each cluster's size. The next part of the equation, $N(x_i \\mid \\mu_k,\\Sigma_k)$ tells us: Given that we knew that the observation comes from the $k_{th}$ cluster, what is the likelihood of observing our data point $x_i$ coming from this cluster. To compute this part, the scipy package provides a convenient function that computes the likelihood of seeing a data point in a multivariate Gaussian distribution. After multiplying the prior and the likelihood, we need to normalize over all possible cluster assignments so that the responsibility vector becomes a valid probability. And this is essentially the computation that's done for the E step.\n\nApart from training the model, we also want a way to monitor the convergence of the algorithm. We do so by computing the log likelihood of the data given the current estimates of our model parameters and responsibilities. Recall that during the E step of the algorithm, we used the formula: $$\\sum_{j=1}^K \\pi_j N(x_i \\mid \\mu_j,\\Sigma_j)$$ To compute the weighted probability of our data point $x_i$ coming from each cluster $j$ and summed up all the weighted probability. If we were to assume the observed data points were generated independently, the likelihood of the data can be written as: $$p(X \\mid \\pi, \\mu,\\Sigma)=\\prod_{n=1}^{N} \\sum_{j=1}^K \\pi_j N(x_i \\mid \\mu_j,\\Sigma_j)$$ This basically means that we multiply all the probability for every data point together to obtain a single number that estimates the likelihood of the data fitted under the model's parameter. We can take the log of this likelihood so that the product becomes a sum and it makes the computation a bit easier: $$ ln \\left( p(X \\mid \\pi,\\mu,\\Sigma) \\right) = \\sum^N_{i=1} ln\\{\\sum^K_{j=1}\\pi_j N (x_i \\mid \\mu_j,\\Sigma_j)\\} $$ Given this formula, we can use it and say: If the log likelihood of the data occuring under the current model's parameter does not improve by a tolerance value that we've pre-specified, then the algorithm is deemed converged."
    },
    {
        "link": "https://stats.stackexchange.com/questions/105140/gaussian-naive-bayes-really-equivalent-to-gmm-with-diagonal-covariance-matrices",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/mixture.html",
        "document": "is a package which enables one to learn Gaussian Mixture Models (diagonal, spherical, tied and full covariance matrices supported), sample them, and estimate them from data. Facilities to help determine the appropriate number of components are also provided.\n\nA Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians.\n\nScikit-learn implements different classes to estimate Gaussian mixture models, that correspond to different estimation strategies, detailed below.\n\nThe object implements the expectation-maximization (EM) algorithm for fitting mixture-of-Gaussian models. It can also draw confidence ellipsoids for multivariate models, and compute the Bayesian Information Criterion to assess the number of clusters in the data. A method is provided that learns a Gaussian Mixture Model from train data. Given test data, it can assign to each sample the Gaussian it most probably belongs to using the method. The comes with different options to constrain the covariance of the difference classes estimated: spherical, diagonal, tied or full covariance.\n• None See GMM covariances for an example of using the Gaussian mixture as clustering on the iris dataset.\n• None See Density Estimation for a Gaussian mixture for an example on plotting the density estimation. It is the fastest algorithm for learning mixture models As this algorithm maximizes only the likelihood, it will not bias the means towards zero, or bias the cluster sizes to have specific structures that might or might not apply. When one has insufficiently many points per mixture, estimating the covariance matrices becomes difficult, and the algorithm is known to diverge and find solutions with infinite likelihood unless one regularizes the covariances artificially. This algorithm will always use all the components it has access to, needing held-out data or information theoretical criteria to decide how many components to use in the absence of external cues. Selecting the number of components in a classical Gaussian Mixture model # The BIC criterion can be used to select the number of components in a Gaussian Mixture in an efficient way. In theory, it recovers the true number of components only in the asymptotic regime (i.e. if much data is available and assuming that the data was actually generated i.i.d. from a mixture of Gaussian distribution). Note that using a Variational Bayesian Gaussian mixture avoids the specification of the number of components for a Gaussian mixture model.\n• None See Gaussian Mixture Model Selection for an example of model selection performed with classical Gaussian mixture. The main difficulty in learning Gaussian mixture models from unlabeled data is that one usually doesn’t know which points came from which latent component (if one has access to this information it gets very easy to fit a separate Gaussian distribution to each set of points). Expectation-maximization is a well-founded statistical algorithm to get around this problem by an iterative process. First one assumes random components (randomly centered on data points, learned from k-means, or even just normally distributed around the origin) and computes for each point a probability of being generated by each component of the model. Then, one tweaks the parameters to maximize the likelihood of the data given those assignments. Repeating this process is guaranteed to always converge to a local optimum. There is a choice of four initialization methods (as well as inputting user defined initial means) to generate the initial centers for the model components: This applies a traditional k-means clustering algorithm. This can be computationally expensive compared to other initialization methods. This uses the initialization method of k-means clustering: k-means++. This will pick the first center at random from the data. Subsequent centers will be chosen from a weighted distribution of the data favouring points further away from existing centers. k-means++ is the default initialization for k-means so will be quicker than running a full k-means but can still take a significant amount of time for large data sets with many components. This will pick random data points from the input data as the initial centers. This is a very fast method of initialization but can produce non-convergent results if the chosen points are too close to each other. Centers are chosen as a small perturbation away from the mean of all data. This method is simple but can lead to the model taking longer to converge.\n• None See GMM Initialization Methods for an example of using different initializations in Gaussian Mixture.\n\nThe object implements a variant of the Gaussian mixture model with variational inference algorithms. The API is similar to the one defined by . Variational inference is an extension of expectation-maximization that maximizes a lower bound on model evidence (including priors) instead of data likelihood. The principle behind variational methods is the same as expectation-maximization (that is both are iterative algorithms that alternate between finding the probabilities for each point to be generated by each mixture and fitting the mixture to these assigned points), but variational methods add regularization by integrating information from prior distributions. This avoids the singularities often found in expectation-maximization solutions but introduces some subtle biases to the model. Inference is often notably slower, but not usually as much so as to render usage unpractical. Due to its Bayesian nature, the variational algorithm needs more hyperparameters than expectation-maximization, the most important of these being the concentration parameter . Specifying a low value for the concentration prior will make the model put most of the weight on a few components and set the remaining components’ weights very close to zero. High values of the concentration prior will allow a larger number of components to be active in the mixture. The parameters implementation of the class proposes two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data. The next figure compares the results obtained for the different type of the weight concentration prior (parameter ) for different values of . Here, we can see the value of the parameter has a strong impact on the effective number of active components obtained. We can also notice that large values for the concentration weight prior lead to more uniform weights when the type of prior is ‘dirichlet_distribution’ while this is not necessarily the case for the ‘dirichlet_process’ type (used by default). The examples below compare Gaussian mixture models with a fixed number of components, to the variational Gaussian mixture models with a Dirichlet process prior. Here, a classical Gaussian mixture is fitted with 5 components on a dataset composed of 2 clusters. We can see that the variational Gaussian mixture with a Dirichlet process prior is able to limit itself to only 2 components whereas the Gaussian mixture fits the data with a fixed number of components that has to be set a priori by the user. In this case the user has selected which does not match the true generative distribution of this toy dataset. Note that with very little observations, the variational Gaussian mixture models with a Dirichlet process prior can take a conservative stand, and fit only one component. On the following figure we are fitting a dataset not well-depicted by a Gaussian mixture. Adjusting the , parameter of the controls the number of components used to fit this data. We also present on the last two plots a random sampling generated from the two resulting mixtures.\n• None See Gaussian Mixture Model Ellipsoids for an example on plotting the confidence ellipsoids for both and .\n• None Gaussian Mixture Model Sine Curve shows using and to fit a sine wave.\n• None See Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture for an example plotting the confidence ellipsoids for the with different for different values of the parameter . Pros and cons of variational inference with BayesianGaussianMixture # When is small enough and is larger than what is found necessary by the model, the Variational Bayesian mixture model has a natural tendency to set some mixture weights values close to zero. This makes it possible to let the model choose a suitable number of effective components automatically. Only an upper bound of this number needs to be provided. Note however that the “ideal” number of active components is very application specific and is typically ill-defined in a data exploration setting. Less sensitivity to the number of parameters Unlike finite models, which will almost always use all components as much as they can, and hence will produce wildly different solutions for different numbers of components, the variational inference with a Dirichlet process prior ( ) won’t change much with changes to the parameters, leading to more stability and less tuning. Due to the incorporation of prior information, variational solutions have less pathological special cases than expectation-maximization solutions. The extra parametrization necessary for variational inference makes inference slower, although not by much. This algorithm needs an extra hyperparameter that might need experimental tuning via cross-validation. There are many implicit biases in the inference algorithms (and also in the Dirichlet process if used), and whenever there is a mismatch between these biases and the data it might be possible to fit better models using a finite mixture. Here we describe variational inference algorithms on Dirichlet process mixture. The Dirichlet process is a prior probability distribution on clusterings with an infinite, unbounded, number of partitions. Variational techniques let us incorporate this prior structure on Gaussian mixture models at almost no penalty in inference time, comparing with a finite Gaussian mixture model. An important question is how can the Dirichlet process use an infinite, unbounded number of clusters and still be consistent. While a full explanation doesn’t fit this manual, one can think of its stick breaking process analogy to help understanding it. The stick breaking process is a generative story for the Dirichlet process. We start with a unit-length stick and in each step we break off a portion of the remaining stick. Each time, we associate the length of the piece of the stick to the proportion of points that falls into a group of the mixture. At the end, to represent the infinite mixture, we associate the last remaining piece of the stick to the proportion of points that don’t fall into all the other groups. The length of each piece is a random variable with probability proportional to the concentration parameter. Smaller values of the concentration will divide the unit-length into larger pieces of the stick (defining more concentrated distribution). Larger concentration values will create smaller pieces of the stick (increasing the number of components with non zero weights). Variational inference techniques for the Dirichlet process still work with a finite approximation to this infinite mixture model, but instead of having to specify a priori how many components one wants to use, one just specifies the concentration parameter and an upper bound on the number of mixture components (this upper bound, assuming it is higher than the “true” number of components, affects only algorithmic complexity, not the actual number of components used)."
    },
    {
        "link": "https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html",
        "document": "Go to the end to download the full example code. or to run this example in your browser via JupyterLite or Binder\n\nDemonstration of several covariances types for Gaussian mixture models.\n\nSee Gaussian mixture models for more information on the estimator.\n\nAlthough GMM are often used for clustering, we can compare the obtained clusters with the actual classes from the dataset. We initialize the means of the Gaussians with the means of the classes from the training set to make this comparison valid.\n\nWe plot predicted labels on both training and held out test data using a variety of GMM covariance types on the iris dataset. We compare GMMs with spherical, diagonal, full, and tied covariance matrices in increasing order of performance. Although one would expect full covariance to perform best in general, it is prone to overfitting on small datasets and does not generalize well to held out test data.\n\nOn the plots, train data is shown as dots, while test data is shown as crosses. The iris dataset is four-dimensional. Only the first two dimensions are shown here, and thus some points are separated in other dimensions."
    },
    {
        "link": "https://ethen8181.github.io/machine-learning/clustering/GMM/GMM.html",
        "document": "Clustering methods such as K-means have hard boundaries, meaning a data point either belongs to that cluster or it doesn't. On the other hand, clustering methods such as Gaussian Mixture Models (GMM) have soft boundaries, where data points can belong to multiple cluster at the same time but with different degrees of belief. e.g. a data point can have a 60% of belonging to cluster 1, 40% of belonging to cluster 2. Apart from using it in the context of clustering, one other thing that GMM can be useful for is outlier detection: Due to the fact that we can compute the likelihood of each point being in each cluster, the points with a \"relatively\" low likelihood (where \"relatively\" is a threshold that we just determine ourselves) can be labeled as outliers. But here we'll focus on the clustering application.\n\nBut that was in one dimesion, what about two, three, four ... It turns out the univariate (one-dimensional) gaussian can be extended to the multivariate (multi-dimensional) case. The form of a d-dimensional gaussian: $$N(x \\mid \\mu,\\Sigma) = \\frac{1}{(2\\pi)^{d/2}\\sqrt{|\\Sigma|}}exp(-\\dfrac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu))$$ In higher dimensions, a Gaussian is fully specified by a mean vector $\\boldsymbol{\\mu}$ and a d-by-d covariance matrix, $\\boldsymbol{\\Sigma}$ (do not confused this symbol with $\\sum$, which is used for denoting summing a bunch of stuff). $|\\Sigma|$ refers to the determinant of the covariance matrix e.g. In two dimension, the Gaussian's parameters might look like this: $$N \\begin{bmatrix} \\begin{pmatrix} \\mu_1\\\\ \\mu_2 \\end{pmatrix}\\, , \\begin{pmatrix} \\sigma^2_1 & \\sigma_{12} \\\\ \\sigma_{21} & \\sigma^2_2 \\end{pmatrix} \\end{bmatrix} $$ The mean vector, containing elements $\\mu_1$ and $\\mu_1$ centers the distribution along every dimension. On the other hand, the covariance matrix specifies the spread and orientation of the distribution. Along the diagonal of this covariance matrix we have the variance terms $\\sigma^2_1$ and $\\sigma^2_2$ representing the shape (spread) along each of the dimensions. But then we also have the off-diagonal terms, $\\sigma_{12}$ and $\\sigma_{21}$ (these two thing actually take the same value because this a symmetric matrix) that specify the correlation structure of the distribution. Let's look at a few examples of covariance structures that we could specify.\n\nOne way to view a Gaussian distribution in two dimensions is what's called a contour plot. The coloring represents the region's intensity, or how high it was in probability. So in the plot above, the center area that has dark red color is the region of highest probability, while the blue area corresponds to a low probability. The first plot is refered to as a Spherical Gaussian, since the probability distribution has spherical (circular) symmetry. The covariance matrix is a diagonal covariance with equal elements along the diagonal. By specifying a diagonal covariance, what we're seeing is that there's no correlation between our two random variables, because the off-diagonal correlations takes the value of 0. Furthermore, by having equal values of the variances along the diagonal, we end up with a circular shape to the distribution because we are saying that the spread along each one of these two dimensions is exactly the same. In contrast, the middle plot's covariance matrix is also a diagonal one, but we can see that if we were to specify different variances along the diagonal, then the spread in each of these dimensions is different and so what we end up with are these axis-aligned ellipses. This is refered to as a Diagonal Gaussian. Finally, we have the Full Gaussian. A full covariance matrix allows for correlation between our two random variables (non zero off diagonal value) we can provide these non-axis aligned ellipses. So in this example that we're showing here, these two variables are negatively correlated, meaning if one variable is high, it's more likely that the other value is low.\n\nThat's great!! But this is all based on knowing which points came from which distribution. Now, what if we have just a bunch of data points, we don't know which one came from which source. Can we trace back these guassian sources? Hmm ..., a bit trickier isn't it? On the other hand, what if someone came along and actually told us the parameters for the Gaussian, then we could actually figure out which points is more likely to come from which Gaussian. Given these information, we know have a chicken and egg problem. If someone told us which point came from which source, we can easily estimate the means and variance. Or if someone told us the mean and the variance for the Gaussians then we can figure out the probability of each point coming from each Gaussians. Unfortunately, we have neither ..... This is the exact situation we're in when doing GMM. We have a bunch of data points, we suspect that they came from $K$ different guassians, but we have no clue which data points came from which guassian. To solve this problem, we use the EM algorithm. The way it works is that it will start by placing guassians randomly (generate random mean and variance for the guassian). Then it will iterate over these two steps until it converges.\n• E step: With the current means and variances, it's going to figure out the probability of each data point $x_i$ coming from each guassian.\n• M step: Once it computed these probability assignments it will use these numbers to re-estimate the guassians' mean and variance to better fit the data points.\n\nWe'll now formalize this. Recall that GMM's goal is to output a set of soft assignments per data point (allocating the probability of that data point belonging to each one of the clusters). To begin with, let's just assume we actually know the parameters $\\pi_k$, $\\mu_k$ and $\\Sigma_k$ (from some random initialization) and we need a formula to compute the soft assignments having fixed the values of all the other parameters. $$r_{ik} = \\frac{ \\pi_k N(x_i \\mid \\mu_k,\\Sigma_k) }{ \\sum_{j=1}^K \\pi_j N(x_i \\mid \\mu_j,\\Sigma_j) }$$ Let's break this down piece by piece. The soft assignments are quantified by the responsibility vector $r$. For each observation $i$, we form a responsibility vector with elements $r_{i1}$, $r_{i2}$, all the way up to $r_{iK}$. Where $K$ is the total number of clusters, or often referred to as the number of components. The cluster responsibilities for a single data point $i$ should sum to 1. The name Mixture of Gaussians comes from the notion that, in order to model more complex data distribution, we can use a linear combination of several Gaussians instead of using just one. To compute the mixture of Gaussians, we introduce a set of cluster weights, $\\pi_k$, one for each cluster $k$. Where $\\sum_{k=1}^K \\pi_k = 1$ and $0 \\leq \\pi_k \\leq 1$ (meaning that the sum must add up to one and each of them is between 0 and 1). This parameter tells us what's the prior probability that the data point in our data set $x$ comes from the $k_{th}$ cluster. We can think it as controlling each cluster's size. The next part of the equation, $N(x_i \\mid \\mu_k,\\Sigma_k)$ tells us: Given that we knew that the observation comes from the $k_{th}$ cluster, what is the likelihood of observing our data point $x_i$ coming from this cluster. To compute this part, the scipy package provides a convenient function that computes the likelihood of seeing a data point in a multivariate Gaussian distribution. After multiplying the prior and the likelihood, we need to normalize over all possible cluster assignments so that the responsibility vector becomes a valid probability. And this is essentially the computation that's done for the E step.\n\nApart from training the model, we also want a way to monitor the convergence of the algorithm. We do so by computing the log likelihood of the data given the current estimates of our model parameters and responsibilities. Recall that during the E step of the algorithm, we used the formula: $$\\sum_{j=1}^K \\pi_j N(x_i \\mid \\mu_j,\\Sigma_j)$$ To compute the weighted probability of our data point $x_i$ coming from each cluster $j$ and summed up all the weighted probability. If we were to assume the observed data points were generated independently, the likelihood of the data can be written as: $$p(X \\mid \\pi, \\mu,\\Sigma)=\\prod_{n=1}^{N} \\sum_{j=1}^K \\pi_j N(x_i \\mid \\mu_j,\\Sigma_j)$$ This basically means that we multiply all the probability for every data point together to obtain a single number that estimates the likelihood of the data fitted under the model's parameter. We can take the log of this likelihood so that the product becomes a sum and it makes the computation a bit easier: $$ ln \\left( p(X \\mid \\pi,\\mu,\\Sigma) \\right) = \\sum^N_{i=1} ln\\{\\sum^K_{j=1}\\pi_j N (x_i \\mid \\mu_j,\\Sigma_j)\\} $$ Given this formula, we can use it and say: If the log likelihood of the data occuring under the current model's parameter does not improve by a tolerance value that we've pre-specified, then the algorithm is deemed converged."
    },
    {
        "link": "https://stackoverflow.com/questions/37300698/gaussian-mixture-model-fit-in-python-with-sklearn-is-too-slow-any-alternative",
        "document": "I need to use Gaussian Mixture Models on an RGB image, and therefore the dataset is quite big. This needs to run on real time (from a webcam feed). I first coded this with Matlab and I was able to achieve a running time of 0.5 seconds for an image of 1729 × 866. The images for the final application will be smaller and therefore the timing will be faster.\n\nHowever, I need to implement this with Python and OpenCV for the final application (I need it to run on an embedded board). I translated all my code and used sklearn.mixture.GMM to replace fitgmdist in Matlab. The line of code calculating the GMM model itself is performed in only 7.7e-05 seconds, but the one to fit the model takes 19 seconds. I have tried other types of covariance, such as 'diag' or 'spherical', and the time does reduce a little but the results are worse and the time is still not good enough, not even close.\n\nI was wondering if there is any other library I can use, or if it would be worth it to translate the functions from Matlab to Python.\n\nHere is my example:\n\nThank you very much for your help"
    },
    {
        "link": "https://geeksforgeeks.org/gaussian-mixture-models-gmm-covariances-in-scikit-learn",
        "document": "The Gaussian Mixture Model (GMM) is a flexible clustering technique that models data as a mixture of multiple Gaussian distributions. Unlike k-means which assumes spherical clusters GMM allows clusters to take various shapes making it more effective for complex datasets.\n\nScikit-Learn provides a built-in implementation of Gaussian Mixture Models (GMM) through the class allowing easy application of GMM for clustering, density estimation, and anomaly detection. Here’s how it works conceptually:\n\nTo use GMM in Scikit-Learn, we start by defining a GaussianMixture object, where we specify the number of components (clusters). Each component represents a Gaussian distribution with its own mean and covariance.\n\nThe model is then trained (or \"fitted\") on the dataset. During this process, Expectation-Maximization (EM) is used to iteratively adjust the means, covariances, and weights of the Gaussian distributions to best fit the data.\n\nOnce trained, the model assigns soft cluster probabilities to each data point, meaning that a point can belong to multiple clusters with different probabilities, unlike k-means which assigns hard labels.\n\nTo determine the best number of clusters, we can use model selection techniques like:\n• Bayesian Information Criterion (BIC) - how well a model fits the data stronger penalty for complexity Formula:\n\nwhere L is the likelihood, k is the number of parameters, and N is the number of data points.\n\nBoth help compare different GMM models by balancing goodness-of-fit with model complexity.\n\nGMM is not just for clustering, it can also be used for density estimation, where it models the probability distribution of the dataset. This is useful in anomaly detection, where points with low probability densities are flagged as outliers.\n\nOnce trained, GMM can predict the probability of new data points belonging to each Gaussian component, making it useful for tasks such as customer segmentation, image segmentation, and speech recognition\n\nIn Gaussian Mixture Models (GMMs), the covariance matrix plays a crucial role in shaping the individual Gaussian components of the mixture. The class in Scikit-Learn provides an important parameter, , which determines how the covariance of each Gaussian component is estimated. Selecting the right covariance type is essential for effectively modeling the structure and relationships within the data.\n\nEach component has its own full covariance matrix. Allows each component to have a unique shape, orientation, and size in all dimensions. Provides the most flexibility but also increases computational cost.\n\nAll components share a single common full covariance matrix. Enforces all clusters to have the same shape and orientation, making it more restrictive. Useful when components are expected to be similar in spread.\n\nEach component has its own diagonal covariance matrix (non-zero values only on the diagonal). Allows each component to have different variances along each dimension but assumes no correlation between dimensions. Computationally efficient and useful for high-dimensional data.\n\nEach component has a single variance value across all dimensions. Assumes that all clusters are spherical and identical in all directions. The simplest model but often too restrictive for real-world data.\n\nEach covariance type offers different levels of flexibility and constraints, impacting how GMM models the data. Choosing the right depends on the dataset's characteristics and the balance between model complexity and computational efficiency.\n\nTo work with GMM covariances in Scikit-Learn, let's delve deeper into the model with in-built wine dataset.\n\nBefore using Gaussian Mixture Models (GMM) in Scikit-Learn, we need to import the necessary libraries.\n• Scikit-Learn – This is the main library that provides the\n• NumPy – Used for handling and manipulating numerical data efficiently.\n\nBefore importing, ensure that Scikit-Learn is installed in your Python environment to avoid errors. If it’s not installed, you can install it using:\n\nData preparation is a crucial step in our program. Ensuring that our data is in the correct format for GMM is essential. We must prepare our data accordingly. To begin, let's load wine dataset for this task, we will leverage the NumPy library.\n\nIn this step, we will initialize a Gaussian Mixture Model. To do this, we specify two key parameters:\n• Number of Components: This parameter determines the number of components in our model, and it reflects the number of clusters or distributions the data will be divided into.\n• Covariance Type: The covariance type defines the structure of the covariance matrix for each component. It can be to one of the four options: full, tied, diag, or spherical.\n\nTo work with Gaussian Mixture in Scikit-Learn, we will use the module. Within this module, we will set the number of components and the desired covariance type as per our analysis needs.\n\nIn this step, we will fit our GMM model using our prepared data. This fitting process will estimate the model's parameters, including the specified covariance type, along with other essential components. Fitting the GMM model is a crucial step that helps us understand the underlying structure of the data and how it relates to the chosen covariance type.\n\nYou can access the covariance matrices of the components through the \"covariances_\" attribute of our fitted GMM model. The shape of these covariance matrices depends on the specified 'covariance_type'. This will help accessing the covariances.\n\nUsing the GMM Model for Clustering or Predictions\n\nWith our GMM model fully prepared, the final step is to utilize the model for clustering or making predictions, depending on the specific task at hand.\n\nThe plot shows how different covariance types affect GMM clustering. Full covariance offers flexibility, tied enforces shared shapes, diagonal assumes no feature correlation, and spherical enforces equal variance. The choice depends on the dataset's structure and the balance between complexity and efficiency\n\nFor datasets with varying shapes and orientations, 'full' covariance might be more suitable. In cases where components are expected to have similar shapes, 'tied' or 'spherical' covariance might be more appropriate.\n\nGaussian Mixture Models offer flexible clustering with diverse covariance structures. Model selection, especially covariance type, impacts performance; choose wisely for optimal results."
    }
]