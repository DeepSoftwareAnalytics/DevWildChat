[
    {
        "link": "http://docs.aiohttp.org/en/stable/client_quickstart.html",
        "document": "Eager to get started? This page gives a good introduction in how to get started with aiohttp client API.\n\nFirst, make sure that aiohttp is installed and up-to-date\n\nLet’s get started with some simple examples.\n\nBegin by importing the aiohttp module, and asyncio: Now, let’s try to get a web-page. For example let’s query : Now, we have a called and a object called . We can get all the information we need from the response. The mandatory parameter of coroutine is an HTTP url ( or class: instance). In order to make an HTTP POST request use coroutine: Other HTTP methods are available as well: To make several requests to the same site more simple, the parameter of constructor can be used. For example to request different endpoints of can be used the following code: Don’t create a session per request. Most likely you need a session per application which performs all requests together. More complex cases may require a session per site, e.g. one for Github and other one for Facebook APIs. Anyway making a session for every request is a very bad idea. A session contains a connection pool inside. Connection reusage and keep-alive (both are on by default) may speed up total performance. A session context manager usage is not mandatory but method should be called in this case, e.g.:\n\nYou often want to send some sort of data in the URL’s query string. If you were constructing the URL by hand, this data would be given as key/value pairs in the URL after a question mark, e.g. . Requests allows you to provide these arguments as a , using the keyword argument. As an example, if you wanted to pass and to , you would use the following code: You can see that the URL has been correctly encoded by printing the URL. For sending data with multiple values for the same key may be used; the library support nested lists ( ) alternative as well. It is also possible to pass a list of 2 item tuples as parameters, in that case you can specify multiple values for each key: You can also pass content as param, but beware – content is not encoded by library. Note that is not encoded: Canonicalization encodes host part by IDNA codec and applies requoting to path and query parts. For example is converted to . Sometimes canonicalization is not desirable if server accepts exact representation and does not requote URL itself. To disable canonicalization use parameter for URL construction: Passing params overrides , never use both options.\n\nWhile methods , and are very convenient you should use them carefully. All these methods load the whole response in memory. For example if you want to download several gigabyte sized files, these methods will load all the data in memory. Instead you can use the attribute. It is an instance of the class. The and transfer-encodings are automatically decoded for you: In general, however, you should use a pattern like this to save what is being streamed to a file: It is not possible to use , and after explicit reading from .\n\nTypically, you want to send some form-encoded data – much like an HTML form. To do this, simply pass a dictionary to the data argument. Your dictionary of data will automatically be form-encoded when the request is made: If you want to send data that is not form-encoded you can do it by passing a instead of a . This data will be posted directly and content-type set to ‘application/octet-stream’ by default: If you want to send JSON data: To send text with appropriate content-type just use argument:\n\nsupports multiple types of streaming uploads, which allows you to send large files without reading them into memory. As a simple case, simply provide a file-like object for your body: Or you can use asynchronous generator: # Then you can use file_sender as a data provider: Because the attribute is a (provides async iterator protocol), you can chain get and post requests together: Python 3.5 has no native support for asynchronous generators, use library as workaround. Deprecated since version 3.1: still supports decorator but this approach is deprecated in favor of asynchronous generators as shown above.\n\nYou have to use the coroutine for client websocket connection. It accepts a url as a first parameter and returns , with that object you can communicate with websocket server using response’s methods: You must use the only websocket task for both reading (e.g. or ) and writing but may have multiple writer tasks which can only send data asynchronously (by for example).\n\nBy default aiohttp uses a total 300 seconds (5min) timeout, it means that the whole operation should finish in 5 minutes. In order to allow time for DNS fallback, the default timeout is 30 seconds. The value could be overridden by timeout parameter for the session (specified in seconds): Timeout could be overridden for a request like : The maximal number of seconds for the whole operation including connection establishment, request sending and response reading. The maximal number of seconds for connection establishment of a new connection or for waiting for a free connection from a pool if pool connection limits are exceeded. The maximal number of seconds for connecting to a peer for a new connection, not given from a pool. The maximal number of seconds allowed for period between reading a new data portion from a peer. The threshold value to trigger ceiling of absolute timeout values. All fields are floats, or disables a particular timeout check, see the reference for defaults and additional details. Thus the default timeout is: aiohttp ceils timeout if the value is equal or greater than 5 seconds. The timeout expires at the next integer second greater than . The ceiling is done for the sake of optimization, when many concurrent tasks are scheduled to wake-up at the almost same but different absolute times. It leads to very many event loop wakeups, which kills performance. The optimization shifts absolute wakeup times by scheduling them to exactly the same time as other neighbors, the loop wakes up once-per-second for timeout expiration. Smaller timeouts are not rounded to help testing; in the real life network timeouts usually greater than tens of seconds. However, the default threshold value of 5 seconds can be configured using the parameter."
    },
    {
        "link": "https://docs.aiohttp.org",
        "document": "For speeding up DNS resolving by client API you may install aiodns as well. This option is highly recommended: Installing all speedups in one command¶ The following will get you along with aiodns and in one bundle. No need to type separate commands anymore!\n\nWhen writing your code, we recommend enabling Python’s development mode ( ). In addition to the extra features enabled for asyncio, aiohttp will:\n• None Use a strict parser in the client code (which can help detect malformed responses from a server).\n• None Enable some additional checks (resulting in warnings in certain situations).\n\nGo to What’s new in aiohttp 3.0 page for aiohttp 3.0 major release changes.\n\nThe project is hosted on GitHub Please feel free to file an issue on the bug tracker if you have found a bug or have some suggestion in order to improve the library.\n\nFeel free to post your questions and ideas here. We support Stack Overflow. Please add aiohttp tag to your question there.\n\nThe package is written mostly by Nikolay Kim and Andrew Svetlov. Feel free to improve this package and send a pull request to GitHub.\n\nAfter deprecating some Public API (method, class, function argument, etc.) the library guaranties the usage of deprecated API is still allowed at least for a year and half after publishing new release with deprecation. All deprecations are reflected in documentation and raises . Sometimes we are forced to break the own rule for sake of very strong reason. Most likely the reason is a critical bug which cannot be solved without major API change, but we are working hard for keeping these changes as rare as possible."
    },
    {
        "link": "http://docs.aiohttp.org/en/stable/client_reference.html",
        "document": "Client session is the recommended interface for making HTTP requests. Session encapsulates a connection pool (connector instance) and supports keepalives by default. Unless you are connecting to a large, unknown number of different servers over the lifetime of your application, it is suggested you use a single session for the lifetime of your application to benefit from connection pooling. The client session supports the context manager protocol for self closing. The class for creating client sessions and making requests.\n• None Base part of the URL (optional) If set, allows to join a base part to relative URLs in request calls. If the URL has a path it must have a trailing (as in https://docs.aiohttp.org/en/stable/). Note that URL joining follows RFC 3986. This means, in the most common case the request URLs should have no leading slash, e.g.:\n• None cookies (dict) – Cookies to send with the request (optional)\n• None HTTP Headers to send with every request (optional). May be either iterable of key-value pairs or (e.g. , ).\n• None set of headers for which autogeneration should be skipped. aiohttp autogenerates headers like or if these headers are not explicitly passed. Using parameter allows to skip that generation. Note that autogeneration can’t be skipped.\n• None auth (aiohttp.BasicAuth) – an object that represents HTTP Basic Authorization (optional). It will be included with any request. However, if the parameter is set, the request URL’s origin must match the base URL’s origin; otherwise, the default auth will not be included.\n• None request_class (aiohttp.ClientRequest) – Custom class to use for client requests.\n• None response_class (ClientResponse) – Custom class to use for client responses.\n• None ws_response_class (ClientWebSocketResponse) – Custom class to use for websocket responses.\n• By default every session instance has own private cookie jar for automatic cookies processing but user may redefine this behavior by providing own jar implementation. One example is not processing cookies at all when working in proxy mode. If no cookie processing is needed, a instance can be provided.\n• Setting the parameter to allows to share connection pool between sessions without sharing session state: cookies etc.\n• None Automatically call for each response, by default. This parameter can be overridden when making a request, e.g.: Set the parameter to if you need for most of cases but override for those requests where you need to handle responses with status 400 or higher. You can also provide a coroutine which takes the response as an argument and can raise an exception based on custom logic, e.g.: 'I wanted to see \"apple pie\" in response' As with boolean values, you’re free to set this on the session and/or overwrite it on a per-request basis.\n• Changed in version 3.10.9: The default value for the timeout has been changed to 30 seconds.\n• None Trust environment settings for proxy configuration if the parameter is ( by default). See Proxy support for more information. Get proxy credentials from file if present. Get HTTP Basic Auth credentials from file if present. If environment variable is set, read from file specified there rather than from . Changed in version 3.9: Added support for reading HTTP Basic Auth credentials from file.\n• None trace_configs – A list of instances used for client tracing. (default) is used for request tracing disabling. See Tracing Reference for more information.\n• None A callable that accepts a and the contents, and returns a which will be used as the encoding parameter to . This function will be called when the charset is not known (e.g. not specified in the Content-Type header). The default function simply defaults to . if the session has been closed, otherwise. derived instance used for the session. Gives access to cookie jar’s content and modifiers. aiohttp re quote’s redirect urls by default, but some servers require exact url from location header. To disable re-quote system set attribute to . Deprecated since version 3.5: The attribute modification is deprecated. Default client timeouts, instance. The value can be tuned by passing timeout parameter to constructor. HTTP Headers that sent with every request May be either iterable of key-value pairs or (e.g. , ). Set of headers for which autogeneration skipped. Should connector be closed on session closing Should be called for each response Should the body response be automatically decompressed Trust environment settings for proxy configuration or ~/.netrc file if present. See Proxy support for more information. A list of instances used for client tracing. (default) is used for request tracing disabling. See Tracing Reference for more information. Performs an asynchronous HTTP request. Returns a response object that should be used as an async context manager.\n• None url – Request URL, or that will be encoded with (see to skip encoding).\n• None Mapping, iterable of tuple of key/value pairs or string to be sent as parameters in the query string of the new request. Ignored for subsequent redirected requests (optional)\n• None with preferably url-encoded content (Warning: content will not be encoded by aiohttp)\n• None data – The data to send in the body of the request. This can be a object or anything that can be passed into , e.g. a dictionary, bytes, or file-like object. (optional)\n• None json – Any json compatible python object (optional). json and data parameters could not be used at the same time.\n• Global session cookies and the explicitly set cookies will be merged when sending the request.\n• None headers (dict) – HTTP Headers to send with the request (optional)\n• None set of headers for which autogeneration should be skipped. aiohttp autogenerates headers like or if these headers are not explicitly passed. Using parameter allows to skip that generation.\n• None allow_redirects (bool) – Whether to process redirects or not. When , redirects are followed (up to times) and logged into and . When , the original response is returned. by default (optional).\n• None max_redirects (int) – Maximum number of redirects to follow. is raised if the number is exceeded. Ignored when . by default.\n• None compress (bool) – Set to if request has to be compressed with deflate encoding. If can not be combined with a Content-Encoding and Content-Length headers. by default (optional).\n• None chunked (int) – Enable chunked transfer encoding. It is up to the developer to decide how to chunk data streams. If chunking is enabled, aiohttp encodes the provided chunks in the “Transfer-encoding: chunked” format. If chunked is set, then the Transfer-encoding and content-length headers are disallowed. by default (optional).\n• None response if set to . If set to value from will be used. by default (optional).\n• None read_until_eof (bool) – Read response until EOF if response does not have Content-Length header. by default (optional).\n• Changed in version 3.3: The parameter is instance, is still supported for sake of backward compatibility. If is passed it is a total timeout (in seconds).\n• None ( is used), for skip SSL certificate validation, for fingerprint validation, for custom SSL certificate validation.\n• None Sets or overrides the host name that the target server’s certificate will be matched against. See for more information.\n• None HTTP headers to send to the proxy if the parameter proxy has been provided.\n• None Object used to give as a kw param for each new object instantiated, used to give information to the tracers that is only available at request time.\n• None by default, it means that the session global value is used.\n• None auto_decompress (bool) – Automatically decompress response body. Overrides . May be used to enable/disable auto decompression on a per-request basis. In order to modify inner parameters, provide .\n• None allow_redirects (bool) – Whether to process redirects or not. When , redirects are followed and logged into . When , the original response is returned. by default (optional). In order to modify inner parameters, provide .\n• None data – Data to send in the body of the request; see for details (optional) In order to modify inner parameters, provide .\n• None data – Data to send in the body of the request; see for details (optional) In order to modify inner parameters, provide . In order to modify inner parameters, provide .\n• None allow_redirects (bool) – Whether to process redirects or not. When , redirects are followed and logged into . When , the original response is returned. by default (optional). In order to modify inner parameters, provide .\n• None allow_redirects (bool) – Whether to process redirects or not. When , redirects are followed and logged into . When , the original response is returned. by default (optional). In order to modify inner parameters, provide .\n• None data – Data to send in the body of the request; see for details (optional)\n• None url – Websocket server url, or that will be encoded with (see to skip encoding).\n• None timeout – a timeout for websocket. By default, the value is used ( seconds for the websocket to close). means no timeout will be used.\n• None autoclose (bool) – Automatically close websocket connection on close message from server. If autoclose is False then close procedure has to be handled manually. by default\n• None autoping (bool) – automatically send pong on ping message from server. by default\n• None heartbeat (float) – Send ping message every heartbeat seconds and wait pong response, if pong response is not received then close connection. The timer is reset on any data reception.(optional)\n• None Mapping, iterable of tuple of key/value pairs or string to be sent as parameters in the query string of the new request. Ignored for subsequent redirected requests (optional)\n• None with preferably url-encoded content (Warning: content will not be encoded by aiohttp)\n• None headers (dict) – HTTP Headers to send with the request (optional)\n• None ( is used), for skip SSL certificate validation, for fingerprint validation, for custom SSL certificate validation.\n• None Perform SSL certificate validation for HTTPS requests (enabled by default). May be disabled to skip validation for sites with invalid certificates.\n• None Pass the SHA256 digest of the expected certificate in DER format to verify that the certificate the server presents matches. Useful for certificate pinning. Note: use of MD5 or SHA1 digests is insecure and deprecated.\n• ssl_context may be used for configuring certification authority channel, supported SSL options etc.\n• None HTTP headers to send to the proxy if the parameter proxy has been provided.\n• None 0 for disable, 9 to 15 for window bit support. Default value is 0.\n• None 4 MB by default. To disable the size limit use . Detach connector from session without closing the former. Session is switched to closed state anyway.\n\nWhile we encourage usage we also provide simple coroutines for making HTTP requests. Basic API is good for performing simple HTTP requests without keepaliving, cookies and complex connection stuff like properly configured SSL certification chaining. Asynchronous context manager for performing an asynchronous HTTP request. Returns a response object. Use as an async context manager.\n• None url – Request URL, or that will be encoded with (see to skip encoding).\n• None Mapping, iterable of tuple of key/value pairs or string to be sent as parameters in the query string of the new request. Ignored for subsequent redirected requests (optional)\n• None data – The data to send in the body of the request. This can be a object or anything that can be passed into , e.g. a dictionary, bytes, or file-like object. (optional)\n• None json – Any json compatible python object (optional). json and data parameters could not be used at the same time.\n• None cookies (dict) – HTTP Cookies to send with the request (optional)\n• None headers (dict) – HTTP Headers to send with the request (optional)\n• None set of headers for which autogeneration should be skipped. aiohttp autogenerates headers like or if these headers are not explicitly passed. Using parameter allows to skip that generation.\n• None allow_redirects (bool) – Whether to process redirects or not. When , redirects are followed (up to times) and logged into and . When , the original response is returned. by default (optional).\n• None max_redirects (int) – Maximum number of redirects to follow. is raised if the number is exceeded. Ignored when . by default.\n• None compress (bool) – Set to if request has to be compressed with deflate encoding. If can not be combined with a Content-Encoding and Content-Length headers. by default (optional).\n• None chunked (int) – Enables chunked transfer encoding. It is up to the developer to decide how to chunk data streams. If chunking is enabled, aiohttp encodes the provided chunks in the “Transfer-encoding: chunked” format. If chunked is set, then the Transfer-encoding and content-length headers are disallowed. by default (optional).\n• None for response if set to . If set to value from will be used. by default (optional).\n• None read_until_eof (bool) – Read response until EOF if response does not have Content-Length header. by default (optional).\n• None SSL validation mode. for default SSL check ( is used), for skip SSL certificate validation, for fingerprint validation, for custom SSL certificate validation.\n• None Sets or overrides the host name that the target server’s certificate will be matched against. See for more information.\n• None proxy_headers (collections.abc.Mapping) – HTTP headers to send to the proxy if the parameter proxy has been provided.\n• None trace_request_ctx – Object used to give as a kw param for each new object instantiated, used to give information to the tracers that is only available at request time.\n• None by default, it means that the session global value is used.\n• None auto_decompress (bool) – Automatically decompress response body. May be used to enable/disable auto decompression on a per-request basis.\n• None for regular TCP sockets (both HTTP and HTTPS schemes supported).\n• None for connecting via UNIX socket (it’s used mostly for testing purposes). All connector classes should be derived from . By default all connectors support keep-alive connections (behavior is controlled by force_close constructor’s parameter).\n• None keepalive_timeout (float) – timeout for connection reusing after releasing (optional). Values . For disabling keep-alive feature use flag.\n• None limit (int) – total number simultaneous connections. If limit is the connector has no limit (default: 100).\n• None limit_per_host (int) – limit simultaneous connections to the same endpoint. Endpoints are the same if they are have equal triple. If limit is the connector has no limit (default: 0).\n• None some SSL servers do not properly complete SSL shutdown process, in that case asyncio leaks SSL connections. If this parameter is set to True, aiohttp additionally aborts underlining transport after 2 seconds. It is off by default. For Python version 3.12.7+, or 3.13.1 and later, this parameter is ignored because the asyncio SSL connection leak is fixed in these versions of Python.\n• None event loop used for handling connections. If param is , is used for getting default event loop. Read-only property, if connector should ultimately close connections on releasing. The total number for simultaneous connections. If limit is 0 the connector has no limit. The default limit size is 100. The limit for simultaneous connections to the same endpoint. Endpoints are the same if they are have equal triple. If limit_per_host is the connector has no limit per host. Get a free connection from pool or create new one if connection is absent in the pool. The call may be paused if is exhausted until used connections returns to pool. Abstract method for actual connection establishing, should be overridden in subclasses. Connector for working with HTTP and HTTPS via TCP sockets. The most common transport. When you don’t know what connector type to use, use a instance. Constructor accepts all parameters suitable for plus several TCP-specific ones: ( is used), for skip SSL certificate validation, for fingerprint validation, for custom SSL certificate validation.\n• None perform SSL certificate validation for HTTPS requests (enabled by default). May be disabled to skip validation for sites with invalid certificates. Deprecated since version 2.3: Pass verify_ssl to etc.\n• None pass the SHA256 digest of the expected certificate in DER format to verify that the certificate the server presents matches. Useful for certificate pinning. Note: use of MD5 or SHA1 digests is insecure and deprecated. Deprecated since version 2.3: Pass verify_ssl to etc.\n• None use internal cache for DNS lookups, by default. Enabling an option may speedup connection establishing a bit but may introduce some side effects also.\n• None expire after some seconds the DNS entries, means cached forever. By default 10 seconds (optional). In some environments the IP addresses related to a specific HOST can change after a specific time. Use this option to keep the DNS cache updated refreshing each entry after N seconds.\n• None limit (int) – total number simultaneous connections. If limit is the connector has no limit (default: 100).\n• None limit_per_host (int) – limit simultaneous connections to the same endpoint. Endpoints are the same if they are have equal triple. If limit is the connector has no limit (default: 0).\n• None custom resolver instance to use. by default (asynchronous if is installed). Custom resolvers allow to resolve hostnames differently than the way the host is configured. The resolver is by default, asynchronous version is pretty robust but might fail in very rare cases.\n• None TCP socket family, both IPv4 and IPv6 by default. For IPv4 only use , for IPv6 only – . family is by default, that means both IPv4 and IPv6 are accepted. To specify only concrete version please pass or explicitly.\n• ssl_context may be used for configuring certification authority channel, supported SSL options etc.\n• None local_addr (tuple) – tuple of used to bind socket locally if specified.\n• None enable_cleanup_closed (bool) – Some ssl servers do not properly complete SSL shutdown process, in that case asyncio leaks SSL connections. If this parameter is set to True, aiohttp additionally aborts underlining transport after 2 seconds. It is off by default.\n• None The amount of time in seconds to wait for a connection attempt to complete, before starting the next attempt in parallel. This is the “Connection Attempt Delay” as defined in RFC 8305. To disable Happy Eyeballs, set this to . The default value recommended by the RFC is 0.25 (250 milliseconds).\n• None controls address reordering when a host name resolves to multiple IP addresses. If or unspecified, no reordering is done, and addresses are tried in the order returned by the resolver. If a positive integer is specified, the addresses are interleaved by address family, and the given integer is interpreted as “First Address Family Count” as defined in RFC 8305. The default is if happy_eyeballs_delay is not specified, and if it is. Use quick lookup in internal DNS cache for host names if . The cache of resolved hosts if is enabled. Remove specific entry if both host and port are specified, clear all cache otherwise. Use for sending HTTP/HTTPS requests through UNIX Sockets as underlying transport. UNIX sockets are handy for writing tests and making very fast connections between processes on the same host. Constructor accepts all parameters suitable for plus UNIX-specific one: End user should never create instances manually but get it by coroutine. read-only property, if connection was closed, released or detached. Underlying socket is not closed, the connection may be reused later if timeout (30 seconds by default) for connection was not expired.\n\nUser never creates the instance of ClientResponse class but gets it from API calls. After exiting from block response object will be released (see method). Boolean representation of HTTP status code ( ). if is less than ; otherwise, . Payload stream, which contains response’s BODY ( ). It supports various reading methods depending on the expected format. When chunked transfer encoding is used by the server, allows retrieving the actual http chunks. Reading from the stream may raise if the response object is closed before response receives all data or in case if any transfer encoding related errors like malformed chunked encoding of broken compression data. Unmodified HTTP headers of response as unconverted bytes, a sequence of pairs. For each link, key is link param when it exists, or link url as otherwise, and value is of link params and url at key as instance. Returns value is if no Content-Type header present in HTTP headers according to RFC 2616. To make sure Content-Type header is not present in the server reply, use or , e.g. . Read-only property that specifies the encoding for the request’s BODY. The value is parsed from the Content-Type HTTP header. Returns like or if no Content-Type header present in HTTP headers or it has no charset information. Read-only property that specified the Content-Disposition HTTP header. Instance of or if no Content-Disposition header present in HTTP headers. A of objects of preceding requests (earliest request first) if there were redirects, an empty sequence otherwise. Read the whole response’s body as . Close underlying connection if data reading gets an error, release connection otherwise. Raise an if the data can’t be read. It is not required to call on the response object. When the client fully receives the payload, the underlying connection automatically returns back to pool. If the payload is not fully read, the connection is closed Raise an if the response status is 400 or higher. Do nothing for success responses (less than 400). Read response’s body and return decoded using specified encoding parameter. If encoding is content encoding is determined from the Content-Type header, or using the function. Close underlying connection if data reading gets an error, release connection otherwise. encoding (str) – text encoding used for BODY decoding, or for encoding autodetection (default). if decoding fails. See also . Read response’s body as JSON, return using specified encoding and loader. If data is not still available a call will be done. If response’s does not match parameter get raised. To disable content type check pass value.\n• None text encoding used for BODY decoding, or for encoding autodetection (default). By the standard JSON encoding should be but practice beats purity: some servers return non-UTF responses. Autodetection works pretty fine anyway.\n• None loads (collections.abc.Callable) – callable used for loading JSON data, by default.\n• None content_type (str) – specify response’s content-type, if content type does not match raise . To disable check, pass as value. (default: ). BODY as JSON data parsed by loads parameter or if BODY is empty or contains white-spaces only. A with request URL and headers from object, instance. Retrieve content encoding using info in HTTP header. If no charset is present or the charset is not understood by Python, the function associated with the is called.\n\nTotal number of seconds for the whole request. Maximal number of seconds for acquiring a connection from pool. The time consists connection establishment for a new connection or waiting for a free connection from a pool if pool connection limits are exceeded. Maximal number of seconds for connecting to a peer for a new connection, not given from a pool. See also . Maximal number of seconds for reading a portion of data from a peer. A timeout for the websocket to close. Timeouts of 5 seconds or more are rounded for scheduling on the next second boundary (an absolute time where microseconds part is zero) for the sake of performance. E.g., assume a timeout is , absolute time when timeout should expire is , and it points to which is equal to . The absolute time for the timeout cancellation is . It leads to grouping all close scheduled timeout expirations to exactly the same time to reduce amount of loop wakeups. Changed in version 3.7: Rounding to the next seconds boundary is disabled for timeouts smaller than 5 seconds for the sake of easy debugging. In turn, tiny timeouts can lead to significant performance degradation on production environment. Value of corresponding etag without quotes. Flag indicates that etag is weak (has prefix). A data class to represent the Content-Disposition header, available as attribute. A instance. Value of Content-Disposition header itself, e.g. . A instance. Content filename extracted from parameters. May be . A with request URL and headers from object, available as attribute. Should be used for specifying authorization data in client API, e.g. auth parameter for . credentials data, or is credentials are not provided. Encode credentials into string suitable for header etc. The cookie jar instance is available as . The jar contains items for storing internal cookie data. These cookies may be iterated over:\n• None unsafe (bool) – (optional) Whether to accept cookies from IPs.\n• None (optional) Whether to quote cookies according to RFC 2109. Some backend systems (not compatible with RFC mentioned above) does not support quoted cookies.\n• None for cookies marked as Secured. Possible types are\n• None or of or\n• None cookies – a (e.g. , ) or iterable of pairs with cookies returned by server’s response.\n• None response_url (URL) – URL of response, for shared cookies. Regular cookies are coupled with server’s URL and are sent only to this server, shared ones are sent in every client request. Return jar’s cookies acceptable for URL and available in header for sending client requests for given URL. response_url (URL) – request’s URL for which cookies are asked. with filtered cookies for given URL. Write a pickled representation of cookies into the file at provided path. file_path – Path to file where cookies will be serialized, or instance. Load a pickled representation of cookies from the file at provided path. file_path – Path to file from where cookies will be imported, or instance. Removes all cookies from the jar if the predicate is . Otherwise remove only those that returns . callable that gets as a parameter and returns if this must be deleted from the jar. Remove all cookies from the jar that belongs to the specified domain or its subdomains. domain (str) – domain for which cookies must be deleted from the jar. Dummy cookie jar which does not store cookies but ignores them. Could be useful e.g. for web crawlers to iterate over Internet without blowing up with saved cookies information. To install dummy cookie jar pass it into session instance: digest (bytes) – SHA256 digest for certificate in DER-encoded binary form (see ). To check fingerprint pass the object into call, e.g.: A object contains the form data and also handles encoding it into a body that is either or . is used if at least one field is an object or was added with at least one optional argument to ( , , or ). Otherwise, is used. instances are callable and return a on being called. A container for the key/value pairs of this form. If it is a or , it must be a valid argument for . For , , and , the keys and values must be valid and arguments to , respectively.\n• None name (str) – Name of the field\n• If this is not set and is a , , or object, the argument is used as the filename unless is specified. If is not set and is an object, the filename is extracted from the object if possible. Add one or more fields to the form.\n• None or of length two, containing a name-value pair\n\nException hierarchy has been significantly modified in version 2.0. aiohttp defines only exceptions that covers connection handling and server response misbehaviors. For developer specific mistakes, aiohttp uses python standard exceptions like or . Reading a response content may raise a exception. This exception indicates errors specific to the payload encoding. Such as invalid compressed data, malformed chunked-encoded chunks or not enough data that satisfy the content-length header. All exceptions are available as members of aiohttp module. This exception can only be raised while reading the response payload if one of these errors occurs:\n• None not enough data that satisfy HTTP header. URL used for fetching is malformed, e.g. it does not contain host part. Base class for all errors related to client url. Base class for all errors related to client redirects. Base class for all errors related to non http client urls. Redirect URL is malformed, e.g. it does not contain host part. Redirect URL does not contain http schema. These exceptions could happen after we get response from server. Instance of object, contains information about request. History from failed response, if available, else empty tuple. A of objects used for handle redirection responses. Client was redirected too many times. Maximum number of redirects can be configured by using parameter in . Subset of connection errors that are initiated by an exception. To catch all timeouts, including the timeout, use ."
    },
    {
        "link": "http://docs.aiohttp.org/en/stable/client_advanced.html",
        "document": "is the heart and the main entry point for all client API operations. Create the session first, use the instance for performing HTTP requests and initiating WebSocket connections. The session contains a cookie storage and connection pool, thus cookies and connections are shared between HTTP requests sent by the same session.\n\nIf you need to add HTTP headers to a request, pass them in a to the headers parameter. For example, if you want to specify the content-type directly: You also can set default headers for all session requests: Typical use case is sending JSON body. You can specify content type directly as shown above, but it is more convenient to use special keyword :\n\nInstead of setting the header directly, and individual request methods provide an argument. An instance of can be passed in like this: Note that if the request is redirected and the redirect URL contains credentials, those credentials will supersede any previously set credentials. In other words, if redirects to , the second request will be authenticated as . Providing both the parameter and authentication in the initial URL will result in a . For other authentication flows, the header can be set directly: The authentication header for a session may be updated as and when required. For example: Note that a copy of the headers dictionary is set as an attribute when creating a instance (as a object). Updating the original dictionary does not have any effect. In cases where the authentication header value expires periodically, an task may be used to update the session’s default headers in the background. header will be removed if you get redirected to a different host or protocol.\n\nWe can view the server’s response using a : The dictionary is special, though: it’s made just for HTTP headers. According to RFC 7230, HTTP Header names are case-insensitive. It also supports multiple values for the same key as HTTP protocol does. So, we can access the headers using any capitalization we want: All headers are converted from binary data using UTF-8 with option. That works fine on most cases but sometimes unconverted data is needed if a server uses nonstandard encoding. While these headers are malformed from RFC 7230 perspective they may be retrieved by using property: If a response contains some HTTP Cookies, you can quickly access them: Response cookies contain only values, that were in headers of the last request in redirection chain. To gather cookies between all redirection requests please use aiohttp.ClientSession object.\n\nThe execution flow of a specific request can be followed attaching listeners coroutines to the signals provided by the instance, this instance will be used as a parameter for the constructor having as a result a client that triggers the different signals supported by the . By default any instance of class comes with the signals ability disabled. The following snippet shows how the start and the end signals of a request flow can be followed: The is a list that can contain instances of class that allow run the signals handlers coming from different instances. The following example shows how two different that have a different nature are installed to perform their job in each signal handle: All signals take as a parameters first, the instance used by the specific request related to that signals and second, a instance called . The object can be used to share the state through to the different signals that belong to the same request and to the same class, perhaps: The param is by default a that is initialized at the beginning of the request flow. However, the factory used to create this object can be overwritten using the constructor param of the class. The param can given at the beginning of the request execution, accepted by all of the HTTP verbs, and will be passed as a keyword argument for the factory. This param is useful to pass data that is only available at request time, perhaps: Tracing Reference section for more information about the different signals supported.\n\nTo tweak or change transport layer of requests you can pass a custom connector to and family. For example: By default session object takes the ownership of the connector, among other things closing the connections once the session is closed. If you are keen on share the same connector through different session instances you must give the connector_owner parameter as False for each session instance. Connectors section for more information about different connector types and configuration options. To limit amount of simultaneously opened connections you can pass limit parameter to connector: The example limits total amount of parallel connections to . If you explicitly want not to have limits, pass . For example: To limit amount of simultaneously opened connection to the same endpoint ( triple) you can pass limit_per_host parameter to connector: The example limits amount of parallel connections to the same to . The default is (no limit on per host bases). By default comes with the DNS cache table enabled, and resolutions will be cached by default for seconds. This behavior can be changed either to change of the TTL for a resolution, as can be seen in the following example: or disabling the use of the DNS cache table, meaning that all requests will end up making a DNS resolution, as the following example shows: In order to specify the nameservers to when resolving the hostnames, aiodns is required: If your HTTP server uses UNIX domain sockets you can use : If your HTTP server uses Named pipes you can use : It will only work with the ProactorEventLoop\n\nBy default aiohttp uses strict checks for HTTPS protocol. Certification checks can be relaxed by setting ssl to : If you need to setup custom ssl parameters (use own certification files for example) you can create a instance and pass it into the methods or set it for the entire session with . There are explicit errors when ssl verification fails If you need to skip both ssl related errors By default, Python uses the system CA certificates. In rare cases, these may not be installed or Python is unable to find them, resulting in a error like One way to work around this problem is to use the package: If you need to verify self-signed certificates, you need to add a call to with the key pair: You may also verify certificates via SHA256 fingerprint: Note that this is the fingerprint of the DER-encoded certificate. If you have the certificate in PEM format, you can convert it to DER with e.g: Tip: to convert from a hexadecimal digest to a binary byte-string, you can use . ssl parameter could be passed to as default, the value from and others override default.\n\naiohttp supports plain HTTP proxies and HTTP proxies that can be upgraded to HTTPS via the HTTP CONNECT method. aiohttp has a limited support for proxies that must be connected to via — see the info box below for more details. To connect, use the proxy parameter: Authentication credentials can be passed in proxy URL: And you may set default proxy: Contrary to the library, it won’t read environment variables by default. But you can do so by passing into constructor.: aiohttp uses for reading the proxy configuration (e.g. from the HTTP_PROXY etc. environment variables) and applies them for the HTTP, HTTPS, WS and WSS schemes. Hosts defined in will bypass the proxy. Added in version 3.8: WS_PROXY and WSS_PROXY are supported since aiohttp v3.8. Proxy credentials are given from file if present (see for more details). As of now (Python 3.10), support for TLS in TLS is disabled for the transports that uses. If the further release of Python (say v3.11) toggles one attribute, it’ll just work™. aiohttp v3.8 and higher is ready for this to happen and has code in place supports TLS-in-TLS, hence sending HTTPS requests over HTTPS proxy tunnels. ⚠️ For as long as your Python runtime doesn’t declare the support for TLS-in-TLS, please don’t file bugs with aiohttp but rather try to help the CPython upstream enable this feature. Meanwhile, if you really need this to work, there’s a patch that may help you make it happen, include it into your app’s code base: https://github.com/aio-libs/aiohttp/discussions/6044#discussioncomment-1432443. When supplying a custom instance, bear in mind that it will be used not only to establish a TLS session with the HTTPS endpoint you’re hitting but also to establish a TLS tunnel to the HTTPS proxy. To avoid surprises, make sure to set up the trust chain that would recognize TLS certificates used by both the endpoint and the proxy.\n\nWhen closes at the end of an block (or through a direct call), the underlying connection remains open due to asyncio internal details. In practice, the underlying connection will close after a short while. However, if the event loop is stopped before the underlying connection is closed, a warning is emitted (when warnings are enabled). To avoid this situation, a small delay must be added before closing the event loop to allow any open underlying connections to close. For a without SSL, a simple zero-sleep ( ) will suffice: # Zero-sleep to allow underlying connections to close For a with SSL, the application must wait a short duration before closing: # Wait 250 ms for the underlying SSL connections to close Note that the appropriate amount of time to wait will vary from application to application. All if this will eventually become obsolete when the asyncio internals are changed so that aiohttp itself can wait on the underlying connection to close. Please follow issue #1925 for the progress on this."
    },
    {
        "link": "https://pypi.org/project/aiohttp",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://docs.python.org/3/library/asyncio.html",
        "document": "asyncio is a library to write concurrent code using the async/await syntax.\n\nasyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\nasyncio is often a perfect fit for IO-bound and high-level structured network code.\n\nasyncio provides a set of high-level APIs to:\n• None run Python coroutines concurrently and have full control over their execution;\n\nAdditionally, there are low-level APIs for library and framework developers to:\n• None create and manage event loops, which provide asynchronous APIs for networking, running subprocesses, handling OS signals, etc;\n\nYou can experiment with an concurrent context in the REPL:\n\nRaises an auditing event with no arguments."
    },
    {
        "link": "https://docs.python.org/3/library/asyncio-task.html",
        "document": "This section outlines high-level asyncio APIs to work with coroutines and Tasks.\n\nCoroutines declared with the async/await syntax is the preferred way of writing asyncio applications. For example, the following snippet of code prints “hello”, waits 1 second, and then prints “world”: Note that simply calling a coroutine will not schedule it to be executed: To actually run a coroutine, asyncio provides the following mechanisms:\n• None The function to run the top-level entry point “main()” function (see the above example.)\n• None Awaiting on a coroutine. The following snippet of code will print “hello” after waiting for 1 second, and then print “world” after waiting for another 2 seconds:\n• None The function to run coroutines concurrently as asyncio . Let’s modify the above example and run two coroutines concurrently: # Wait until both tasks are completed (should take Note that expected output now shows that the snippet runs 1 second faster than before:\n• None The class provides a more modern alternative to . Using this API, the last example becomes: # The await is implicit when the context manager exits. The timing and output should be the same as for the previous version.\n\nWe say that an object is an awaitable object if it can be used in an expression. Many asyncio APIs are designed to accept awaitables. There are three main types of awaitable objects: coroutines, Tasks, and Futures. Python coroutines are awaitables and therefore can be awaited from other coroutines: # Nothing happens if we just call \"nested()\". # A coroutine object is created but not awaited, # so it *won't run at all*. # Let's do it differently now and await it: In this documentation the term “coroutine” can be used for two closely related concepts: Tasks are used to schedule coroutines concurrently. When a coroutine is wrapped into a Task with functions like the coroutine is automatically scheduled to run soon: # \"task\" can now be used to cancel \"nested()\", or # can simply be awaited to wait until it is complete: A is a special low-level awaitable object that represents an eventual result of an asynchronous operation. When a Future object is awaited it means that the coroutine will wait until the Future is resolved in some other place. Future objects in asyncio are needed to allow callback-based code to be used with async/await. Normally there is no need to create Future objects at the application level code. Future objects, sometimes exposed by libraries and some asyncio APIs, can be awaited: A good example of a low-level function that returns a Future object is .\n\nTask groups combine a task creation API with a convenient and reliable way to wait for all tasks in the group to finish. An asynchronous context manager holding a group of tasks. Tasks can be added to the group using . All tasks are awaited when the context manager exits. Create a task in this task group. The signature matches that of . If the task group is inactive (e.g. not yet entered, already finished, or in the process of shutting down), we will close the given . Changed in version 3.13: Close the given coroutine if the task group is not active. \"Both tasks have completed now: The statement will wait for all tasks in the group to finish. While waiting, new tasks may still be added to the group (for example, by passing into one of the coroutines and calling in that coroutine). Once the last task has finished and the block is exited, no new tasks may be added to the group. The first time any of the tasks belonging to the group fails with an exception other than , the remaining tasks in the group are cancelled. No further tasks can then be added to the group. At this point, if the body of the statement is still active (i.e., hasn’t been called yet), the task directly containing the statement is also cancelled. The resulting will interrupt an , but it will not bubble out of the containing statement. Once all tasks have finished, if any tasks have failed with an exception other than , those exceptions are combined in an or (as appropriate; see their documentation) which is then raised. Two base exceptions are treated specially: If any task fails with or , the task group still cancels the remaining tasks and waits for them, but then the initial or is re-raised instead of or . If the body of the statement exits with an exception (so is called with an exception set), this is treated the same as if one of the tasks failed: the remaining tasks are cancelled and then waited for, and non-cancellation exceptions are grouped into an exception group and raised. The exception passed into , unless it is , is also included in the exception group. The same special case is made for and as in the previous paragraph. Task groups are careful not to mix up the internal cancellation used to “wake up” their with cancellation requests for the task in which they are running made by other parties. In particular, when one task group is syntactically nested in another, and both experience an exception in one of their child tasks simultaneously, the inner task group will process its exceptions, and then the outer task group will receive another cancellation and process its own exceptions. In the case where a task group is cancelled externally and also must raise an , it will call the parent task’s method. This ensures that a will be raised at the next , so the cancellation is not lost. Changed in version 3.13: Improved handling of simultaneous internal and external cancellations and correct preservation of cancellation counts. While terminating a task group is not natively supported by the standard library, termination can be achieved by adding an exception-raising task to the task group and ignoring the raised exception: \"\"\"Used to force termination of a task group.\"\"\" # add an exception-raising task to force the group to terminate\n\nReturn an asynchronous context manager that can be used to limit the amount of time spent waiting on something. delay can either be , or a float/int number of seconds to wait. If delay is , no time limit will be applied; this can be useful if the delay is unknown when the context manager is created. In either case, the context manager can be rescheduled after creation using . If takes more than 10 seconds to complete, the context manager will cancel the current task and handle the resulting internally, transforming it into a which can be caught and handled. The context manager is what transforms the into a , which means the can only be caught outside of the context manager. \"The long operation timed out, but we've handled it.\" \"This statement will run regardless.\" The context manager produced by can be rescheduled to a different deadline and inspected. should be an absolute time at which the context should time out, as measured by the event loop’s clock:\n• None If is , the timeout will never trigger.\n• None If , the timeout will trigger on the next iteration of the event loop. Return the current deadline, or if the current deadline is not set. Return whether the context manager has exceeded its deadline (expired). # We do not know the timeout when starting, so we pass ``None``. # We know the timeout now, so we reschedule it. \"Looks like we haven't finished on time.\" Similar to , except when is the absolute time to stop waiting, or . \"The long operation timed out, but we've handled it.\" \"This statement will run regardless.\" Wait for the aw awaitable to complete with a timeout. If aw is a coroutine it is automatically scheduled as a Task. timeout can either be or a float or int number of seconds to wait for. If timeout is , block until the future completes. If a timeout occurs, it cancels the task and raises . To avoid the task , wrap it in . The function will wait until the future is actually cancelled, so the total wait time may exceed the timeout. If an exception happens during cancellation, it is propagated. If the wait is cancelled, the future aw is also cancelled. # Wait for at most 1 second Changed in version 3.7: When aw is cancelled due to a timeout, waits for aw to be cancelled. Previously, it raised immediately. Changed in version 3.11: Raises instead of .\n\nRun and instances in the aws iterable concurrently and block until the condition specified by return_when. The aws iterable must not be empty. timeout (a float or int), if specified, can be used to control the maximum number of seconds to wait before returning. Note that this function does not raise . Futures or Tasks that aren’t done when the timeout occurs are simply returned in the second set. return_when indicates when this function should return. It must be one of the following constants: The function will return when any future finishes or is cancelled. The function will return when any future finishes by raising an exception. If no future raises an exception then it is equivalent to . The function will return when all futures finish or are cancelled. Unlike , does not cancel the futures when a timeout occurs. Changed in version 3.11: Passing coroutine objects to directly is forbidden. Run awaitable objects in the aws iterable concurrently. The returned object can be iterated to obtain the results of the awaitables as they finish. The object returned by can be iterated as an asynchronous iterator or a plain iterator. When asynchronous iteration is used, the originally-supplied awaitables are yielded if they are tasks or futures. This makes it easy to correlate previously-scheduled tasks with their results. Example: # earliest_connect is done. The result can be obtained by During asynchronous iteration, implicitly-created tasks will be yielded for supplied awaitables that aren’t tasks or futures. When used as a plain iterator, each iteration yields a new coroutine that returns the result or raises the exception of the next completed awaitable. This pattern is compatible with Python versions older than 3.13: # next_connect is not one of the original task objects. It must be # awaited to obtain the result value or raise the exception of the A is raised if the timeout occurs before all awaitables are done. This is raised by the loop during asynchronous iteration or by the coroutines yielded during plain iteration. Deprecated since version 3.10: Deprecation warning is emitted if not all awaitable objects in the aws iterable are Future-like objects and there is no running event loop. Changed in version 3.13: The result can now be used as either an asynchronous iterator or as a plain iterator (previously it was only a plain iterator).\n\nTasks are used to run coroutines in event loops. If a coroutine awaits on a Future, the Task suspends the execution of the coroutine and waits for the completion of the Future. When the Future is done, the execution of the wrapped coroutine resumes. Event loops use cooperative scheduling: an event loop runs one Task at a time. While a Task awaits for the completion of a Future, the event loop runs other Tasks, callbacks, or performs IO operations. Use the high-level function to create Tasks, or the low-level or functions. Manual instantiation of Tasks is discouraged. To cancel a running Task use the method. Calling it will cause the Task to throw a exception into the wrapped coroutine. If a coroutine is awaiting on a Future object during cancellation, the Future object will be cancelled. can be used to check if the Task was cancelled. The method returns if the wrapped coroutine did not suppress the exception and was actually cancelled. inherits from all of its APIs except and . An optional keyword-only context argument allows specifying a custom for the coro to run in. If no context is provided, the Task copies the current context and later runs its coroutine in the copied context. An optional keyword-only eager_start argument allows eagerly starting the execution of the at task creation time. If set to and the event loop is running, the task will start executing the coroutine immediately, until the first time the coroutine blocks. If the coroutine returns or raises without blocking, the task will be finished eagerly and will skip scheduling to the event loop. Changed in version 3.7: Added support for the module. Changed in version 3.8: Added the name parameter. Deprecated since version 3.10: Deprecation warning is emitted if loop is not specified and there is no running event loop. Return if the Task is done. A Task is done when the wrapped coroutine either returned a value, raised an exception, or the Task was cancelled. Return the result of the Task. If the Task is done, the result of the wrapped coroutine is returned (or if the coroutine raised an exception, that exception is re-raised.) If the Task has been cancelled, this method raises a exception. If the Task’s result isn’t yet available, this method raises an exception. Return the exception of the Task. If the wrapped coroutine raised an exception that exception is returned. If the wrapped coroutine returned normally this method returns . If the Task has been cancelled, this method raises a exception. If the Task isn’t done yet, this method raises an exception. Add a callback to be run when the Task is done. This method should only be used in low-level callback-based code. See the documentation of for more details. This method should only be used in low-level callback-based code. See the documentation of for more details. Return the list of stack frames for this Task. If the wrapped coroutine is not done, this returns the stack where it is suspended. If the coroutine has completed successfully or was cancelled, this returns an empty list. If the coroutine was terminated by an exception, this returns the list of traceback frames. The frames are always ordered from oldest to newest. Only one stack frame is returned for a suspended coroutine. The optional limit argument sets the maximum number of frames to return; by default all available frames are returned. The ordering of the returned list differs depending on whether a stack or a traceback is returned: the newest frames of a stack are returned, but the oldest frames of a traceback are returned. (This matches the behavior of the traceback module.) Print the stack or traceback for this Task. This produces output similar to that of the traceback module for the frames retrieved by . The limit argument is passed to directly. The file argument is an I/O stream to which the output is written; by default output is written to . Return the coroutine object wrapped by the . This will return for Tasks which have already completed eagerly. See the Eager Task Factory. Changed in version 3.12: Newly added eager task execution means result may be . Return the object associated with the task. Return the name of the Task. If no name has been explicitly assigned to the Task, the default asyncio Task implementation generates a default name during instantiation. Set the name of the Task. The value argument can be any object, which is then converted to a string. In the default Task implementation, the name will be visible in the output of a task object. Request the Task to be cancelled. This arranges for a exception to be thrown into the wrapped coroutine on the next cycle of the event loop. The coroutine then has a chance to clean up or even deny the request by suppressing the exception with a … … … block. Therefore, unlike , does not guarantee that the Task will be cancelled, although suppressing cancellation completely is not common and is actively discouraged. Should the coroutine nevertheless decide to suppress the cancellation, it needs to call in addition to catching the exception. Changed in version 3.11: The parameter is propagated from cancelled task to its awaiter. The following example illustrates how coroutines can intercept the cancellation request: Return if the Task is cancelled. The Task is cancelled when the cancellation was requested with and the wrapped coroutine propagated the exception thrown into it. Decrement the count of cancellation requests to this Task. Note that once execution of a cancelled task completed, further calls to are ineffective. This method is used by asyncio’s internals and isn’t expected to be used by end-user code. In particular, if a Task gets successfully uncancelled, this allows for elements of structured concurrency like Task Groups and to continue running, isolating cancellation to the respective structured block. For example: # Outer code not affected by the timeout: While the block with and might get cancelled due to the timeout, should continue running even in case of the timeout. This is implemented with . context managers use in a similar fashion. If end-user code is, for some reason, suppressing cancellation by catching , it needs to call this method to remove the cancellation state. When this method decrements the cancellation count to zero, the method checks if a previous call had arranged for to be thrown into the task. If it hasn’t been thrown yet, that arrangement will be rescinded (by resetting the internal flag). Changed in version 3.13: Changed to rescind pending cancellation requests upon reaching zero. Return the number of pending cancellation requests to this Task, i.e., the number of calls to less the number of calls. Note that if this number is greater than zero but the Task is still executing, will still return . This is because this number can be lowered by calling , which can lead to the task not being cancelled after all if the cancellation requests go down to zero. This method is used by asyncio’s internals and isn’t expected to be used by end-user code. See for more details."
    },
    {
        "link": "https://stackoverflow.com/questions/76321884/asyncio-create-task-not-running-function-in-background-in-python-3-11-3",
        "document": "I've created a Python 3.11.3 program that uses asyncio to run an async function in the background. I've tried using to run the function in the background, but the program keeps waiting for the function to finish before continuing execution.\n\nI've also tried adding the syntax for async context managers introduced in Python 3.11, but I'm not sure if it's relevant to my problem.\n\nWhat could be causing my program to wait for the function to finish? How can I run the async function in the background correctly using asyncio in Python 3.11.3?\n\nThank you in advance for your help.\n\nFrom the output I expect something like this:"
    },
    {
        "link": "https://docs.python.org/3/library/asyncio-dev.html",
        "document": "Asynchronous programming is different from classic “sequential” programming.\n\nThis page lists common mistakes and traps and explains how to avoid them.\n\nBy default asyncio runs in production mode. In order to ease the development asyncio has a debug mode. There are several ways to enable asyncio debug mode: In addition to enabling the debug mode, consider also:\n• None setting the log level of the asyncio logger to , for example the following snippet of code can be run at startup of the application:\n• None configuring the module to display warnings. One way of doing that is by using the command line option. When the debug mode is enabled:\n• None asyncio checks for coroutines that were not awaited and logs them; this mitigates the “forgotten await” pitfall.\n• None Many non-threadsafe asyncio APIs (such as and methods) raise an exception if they are called from a wrong thread.\n• None The execution time of the I/O selector is logged if it takes too long to perform an I/O operation.\n• None Callbacks taking longer than 100 milliseconds are logged. The attribute can be used to set the minimum execution duration in seconds that is considered “slow”.\n\nAn event loop runs in a thread (typically the main thread) and executes all callbacks and Tasks in its thread. While a Task is running in the event loop, no other Tasks can run in the same thread. When a Task executes an expression, the running Task gets suspended, and the event loop executes the next Task. To schedule a callback from another OS thread, the method should be used. Example: Almost all asyncio objects are not thread safe, which is typically not a problem unless there is code that works with them from outside of a Task or a callback. If there’s a need for such code to call a low-level asyncio API, the method should be used, e.g.: To schedule a coroutine object from a different OS thread, the function should be used. It returns a to access the result: # Later in another OS thread: To handle signals the event loop must be run in the main thread. The method can be used with a to execute blocking code in a different OS thread without blocking the OS thread that the event loop runs in. There is currently no way to schedule coroutines or callbacks directly from a different process (such as one started with ). The Event Loop Methods section lists APIs that can read from pipes and watch file descriptors without blocking the event loop. In addition, asyncio’s Subprocess APIs provide a way to start a process and communicate with it from the event loop. Lastly, the aforementioned method can also be used with a to execute code in a different process.\n\nBlocking (CPU-bound) code should not be called directly. For example, if a function performs a CPU-intensive calculation for 1 second, all concurrent asyncio Tasks and IO operations would be delayed by 1 second. An executor can be used to run a task in a different thread or even in a different process to avoid blocking the OS thread with the event loop. See the method for more details.\n\nasyncio uses the module and all logging is performed via the logger. The default log level is , which can be easily adjusted: Network logging can block the event loop. It is recommended to use a separate thread for handling logs or use non-blocking IO. For example, see Dealing with handlers that block.\n\nIf a is called but the Future object is never awaited on, the exception would never be propagated to the user code. In this case, asyncio would emit a log message when the Future object is garbage collected. Example of an unhandled exception: Enable the debug mode to get the traceback where the task was created:"
    },
    {
        "link": "https://dev.to/koladev/creating-and-managing-tasks-with-asyncio-4kjl",
        "document": "Asyncio allows developers to write asynchronous programs in Python without hassle. The module also provides many ways asynchronous tasks and with the multitude of ways to do it, it can become confusing on which one to use.\n\nIn this article, we will discuss the many ways you can create and manage tasks with asyncio.\n\nWhat is an asyncio task?\n\nIn , a task is an object that wraps a coroutine and schedules it to run within the event loop. Simply put, a task is a way to run a coroutine concurrently with other tasks. Once a task is created, the event loop runs it, pausing and resuming it as necessary to allow other tasks to run.\n\nNow, we can discuss methods for creating and managing tasks. First, to create a task in Python using asyncio, you use the method which takes the following arguments:\n• None (required): The coroutine object to be scheduled. This is the function you want to run asynchronously.\n• (optional): A name for the task that can be useful for debugging or logging purposes. You can assign a string to this parameter.\n• You can also set or get the name later using and .\n• (optional): Introduced in Python 3.11, this is used to set a context variable for the task, enabling task-local storage. It’s similar to thread-local storage but for asyncio tasks.\n• This argument is not commonly used unless you're dealing with advanced scenarios that require context management.\n\nHere is an example of the usage of :\n\n\n\nWhen you create a task, you can execute many methods such as:\n• None : to add a callback function that runs when the task is done.\n• None : to check if the task is completed.\n• None : to retrieve the result of the task after it’s completed.\n\nNow that we understand how to create a task, let's see how to handle waiting for one task or a multitude of tasks.\n\nIn this section, we will discuss how to wait for a task completion, for one or many tasks. Asynchronous programming is based on the fact that we can continue the execution of a program if we have an asynchronous task running. There might be times when you want to control better the flow and want to ensure that you have a result that you can work with before safely continuing the execution of the program.\n\nTo wait for a single task completion, you can use . It takes two arguments:\n• None (required): This is the coroutine, task, or future that you want to wait for. It can be any object that can be awaited, like a coroutine function call, an , or an .\n• None (optional): This specifies the maximum number of seconds to wait for the to complete. If the is reached and the awaitable has not completed, raises a . If is set to , the function will wait indefinitely for the awaitable to complete.\n\nHere is an example where this method is used:\n\n\n\nIn the code above, is a coroutine that simulates a long-running task by sleeping for 5 seconds. The line waits for the task to complete but limits the wait to 2 seconds, causing a timeout since the task takes longer. When the timeout is exceeded, a is raised, the task is canceled, and the exception is handled by printing a message indicating the task took too long.\n\nWe can also wait for multiple or a group of tasks to complete. This is possible using , or . Let's explore each method.\n\nThe method waits for a collection of tasks and returns two sets: one for completed tasks and one for pending tasks. It takes the following arguments:\n• None (required, iterable of awaitables): A collection of coroutine objects, tasks, or futures that you want to wait for.\n• None (float or None, optional): The maximum number of seconds to wait. If not provided, it waits indefinitely.\n• \n• (default): Returns when all tasks are complete.\n• : Returns when the first task is completed.\n• : Returns when the first task raises an exception.\n\nLet's see how it is used in an example.\n\n\n\nIn the code above, waits for a group of tasks and returns two sets: one with completed tasks and another with those still pending. You can control when it returns, such as after the first task is completed or after all tasks are done. In the example, returns when the first task is completed, leaving the rest in the pending set.\n\nThe method runs multiple awaitable objects concurrently and returns a list of their results, optionally handling exceptions. Let's see the arguments it takes.\n• None (required, multiple awaitables): A variable number of awaitable objects (like coroutines, tasks, or futures) to run concurrently.\n• None (bool, optional): If , exceptions in the tasks will be returned as part of the results list instead of being raised.\n\nLet's see how it can be used in an example.\n\n\n\nIn the code above, runs multiple awaitable objects concurrently and returns a list of their results in the order they were passed in. It allows you to handle exceptions gracefully if is set to . In the example, three tasks are run simultaneously, and their results are returned in a list once all tasks are complete.\n\nThe method is used to return an iterator that yields tasks as they are completed, allowing results to be processed immediately. It takes the following arguments:\n• None (iterable of awaitables): A collection of coroutine objects, tasks, or futures.\n• None (float or None, optional): The maximum number of seconds to wait for tasks to complete. If not provided, it waits indefinitely.\n\nIn the example above, returns an iterator that yields results as each task completes, allowing you to process them immediately. This is useful when you want to handle results as soon as they're available, rather than waiting for all tasks to finish. In the example, the tasks are run simultaneously, and their results are printed as each one finishes, in the order they complete.\n\nSo to make a summary, you use:\n• None : when you need to handle multiple tasks and want to track which tasks are completed and which are still pending. It's useful when you care about the status of each task separately.\n• None : when you want to run multiple tasks concurrently and need the results in a list, especially when the order of results matters or you need to handle exceptions gracefully.\n• None : when you want to process results as soon as each task finishes, rather than waiting for all tasks to complete. It’s useful for handling results in the order they become available.\n\nHowever, these methods don't take atomic task management with built-in error handling. In the next section, we will see about and how to use it to manage a group of tasks.\n\nis a context manager introduced in Python 3.11 that simplifies managing multiple tasks as a group. It ensures that if any task within the group fails, all other tasks are canceled, providing a way to handle complex task management with robust error handling. The class has one method called used to create and add tasks to the task group. You pass a coroutine to this method, and it returns an object that is managed by the group.\n\nHere is an example of how it is used:\n\n\n\nmanages multiple tasks and ensures that if any task fails, all other tasks in the group are canceled. In the example, a task with an error causes the entire group to be canceled, and only the results of completed tasks are printed.\n\nUsage for this can be in web scraping. You can use to handle multiple concurrent API requests and ensure that if any request fails, all other requests are canceled to avoid incomplete data.\n\nWe are at the end of the article and we have learned the multiple methods provides to create and manage tasks. Here is a summary of the methods:\n• None : Handle tasks as they are completed.\n• None : Manage a group of tasks with automatic cancellation on failure.\n\nAsynchronous programming can transform the way you handle concurrent tasks in Python, making your code more efficient and responsive. In this article, we've navigated through the various methods provided by to create and manage tasks, from simple timeouts to sophisticated task groups. Understanding when and how to use each method— , , , , and —will help you harness the full potential of asynchronous programming, making your applications more robust and scalable.\n\nFor a deeper dive into asynchronous programming and more practical examples, explore our detailed guide here.\n\nIf you enjoyed this article, consider subscribing to my newsletter so you don't miss out on future updates."
    },
    {
        "link": "https://docs.python.org/3/library/asyncio-task.html",
        "document": "This section outlines high-level asyncio APIs to work with coroutines and Tasks.\n\nCoroutines declared with the async/await syntax is the preferred way of writing asyncio applications. For example, the following snippet of code prints “hello”, waits 1 second, and then prints “world”: Note that simply calling a coroutine will not schedule it to be executed: To actually run a coroutine, asyncio provides the following mechanisms:\n• None The function to run the top-level entry point “main()” function (see the above example.)\n• None Awaiting on a coroutine. The following snippet of code will print “hello” after waiting for 1 second, and then print “world” after waiting for another 2 seconds:\n• None The function to run coroutines concurrently as asyncio . Let’s modify the above example and run two coroutines concurrently: # Wait until both tasks are completed (should take Note that expected output now shows that the snippet runs 1 second faster than before:\n• None The class provides a more modern alternative to . Using this API, the last example becomes: # The await is implicit when the context manager exits. The timing and output should be the same as for the previous version.\n\nWe say that an object is an awaitable object if it can be used in an expression. Many asyncio APIs are designed to accept awaitables. There are three main types of awaitable objects: coroutines, Tasks, and Futures. Python coroutines are awaitables and therefore can be awaited from other coroutines: # Nothing happens if we just call \"nested()\". # A coroutine object is created but not awaited, # so it *won't run at all*. # Let's do it differently now and await it: In this documentation the term “coroutine” can be used for two closely related concepts: Tasks are used to schedule coroutines concurrently. When a coroutine is wrapped into a Task with functions like the coroutine is automatically scheduled to run soon: # \"task\" can now be used to cancel \"nested()\", or # can simply be awaited to wait until it is complete: A is a special low-level awaitable object that represents an eventual result of an asynchronous operation. When a Future object is awaited it means that the coroutine will wait until the Future is resolved in some other place. Future objects in asyncio are needed to allow callback-based code to be used with async/await. Normally there is no need to create Future objects at the application level code. Future objects, sometimes exposed by libraries and some asyncio APIs, can be awaited: A good example of a low-level function that returns a Future object is .\n\nTask groups combine a task creation API with a convenient and reliable way to wait for all tasks in the group to finish. An asynchronous context manager holding a group of tasks. Tasks can be added to the group using . All tasks are awaited when the context manager exits. Create a task in this task group. The signature matches that of . If the task group is inactive (e.g. not yet entered, already finished, or in the process of shutting down), we will close the given . Changed in version 3.13: Close the given coroutine if the task group is not active. \"Both tasks have completed now: The statement will wait for all tasks in the group to finish. While waiting, new tasks may still be added to the group (for example, by passing into one of the coroutines and calling in that coroutine). Once the last task has finished and the block is exited, no new tasks may be added to the group. The first time any of the tasks belonging to the group fails with an exception other than , the remaining tasks in the group are cancelled. No further tasks can then be added to the group. At this point, if the body of the statement is still active (i.e., hasn’t been called yet), the task directly containing the statement is also cancelled. The resulting will interrupt an , but it will not bubble out of the containing statement. Once all tasks have finished, if any tasks have failed with an exception other than , those exceptions are combined in an or (as appropriate; see their documentation) which is then raised. Two base exceptions are treated specially: If any task fails with or , the task group still cancels the remaining tasks and waits for them, but then the initial or is re-raised instead of or . If the body of the statement exits with an exception (so is called with an exception set), this is treated the same as if one of the tasks failed: the remaining tasks are cancelled and then waited for, and non-cancellation exceptions are grouped into an exception group and raised. The exception passed into , unless it is , is also included in the exception group. The same special case is made for and as in the previous paragraph. Task groups are careful not to mix up the internal cancellation used to “wake up” their with cancellation requests for the task in which they are running made by other parties. In particular, when one task group is syntactically nested in another, and both experience an exception in one of their child tasks simultaneously, the inner task group will process its exceptions, and then the outer task group will receive another cancellation and process its own exceptions. In the case where a task group is cancelled externally and also must raise an , it will call the parent task’s method. This ensures that a will be raised at the next , so the cancellation is not lost. Changed in version 3.13: Improved handling of simultaneous internal and external cancellations and correct preservation of cancellation counts. While terminating a task group is not natively supported by the standard library, termination can be achieved by adding an exception-raising task to the task group and ignoring the raised exception: \"\"\"Used to force termination of a task group.\"\"\" # add an exception-raising task to force the group to terminate\n\nReturn an asynchronous context manager that can be used to limit the amount of time spent waiting on something. delay can either be , or a float/int number of seconds to wait. If delay is , no time limit will be applied; this can be useful if the delay is unknown when the context manager is created. In either case, the context manager can be rescheduled after creation using . If takes more than 10 seconds to complete, the context manager will cancel the current task and handle the resulting internally, transforming it into a which can be caught and handled. The context manager is what transforms the into a , which means the can only be caught outside of the context manager. \"The long operation timed out, but we've handled it.\" \"This statement will run regardless.\" The context manager produced by can be rescheduled to a different deadline and inspected. should be an absolute time at which the context should time out, as measured by the event loop’s clock:\n• None If is , the timeout will never trigger.\n• None If , the timeout will trigger on the next iteration of the event loop. Return the current deadline, or if the current deadline is not set. Return whether the context manager has exceeded its deadline (expired). # We do not know the timeout when starting, so we pass ``None``. # We know the timeout now, so we reschedule it. \"Looks like we haven't finished on time.\" Similar to , except when is the absolute time to stop waiting, or . \"The long operation timed out, but we've handled it.\" \"This statement will run regardless.\" Wait for the aw awaitable to complete with a timeout. If aw is a coroutine it is automatically scheduled as a Task. timeout can either be or a float or int number of seconds to wait for. If timeout is , block until the future completes. If a timeout occurs, it cancels the task and raises . To avoid the task , wrap it in . The function will wait until the future is actually cancelled, so the total wait time may exceed the timeout. If an exception happens during cancellation, it is propagated. If the wait is cancelled, the future aw is also cancelled. # Wait for at most 1 second Changed in version 3.7: When aw is cancelled due to a timeout, waits for aw to be cancelled. Previously, it raised immediately. Changed in version 3.11: Raises instead of .\n\nRun and instances in the aws iterable concurrently and block until the condition specified by return_when. The aws iterable must not be empty. timeout (a float or int), if specified, can be used to control the maximum number of seconds to wait before returning. Note that this function does not raise . Futures or Tasks that aren’t done when the timeout occurs are simply returned in the second set. return_when indicates when this function should return. It must be one of the following constants: The function will return when any future finishes or is cancelled. The function will return when any future finishes by raising an exception. If no future raises an exception then it is equivalent to . The function will return when all futures finish or are cancelled. Unlike , does not cancel the futures when a timeout occurs. Changed in version 3.11: Passing coroutine objects to directly is forbidden. Run awaitable objects in the aws iterable concurrently. The returned object can be iterated to obtain the results of the awaitables as they finish. The object returned by can be iterated as an asynchronous iterator or a plain iterator. When asynchronous iteration is used, the originally-supplied awaitables are yielded if they are tasks or futures. This makes it easy to correlate previously-scheduled tasks with their results. Example: # earliest_connect is done. The result can be obtained by During asynchronous iteration, implicitly-created tasks will be yielded for supplied awaitables that aren’t tasks or futures. When used as a plain iterator, each iteration yields a new coroutine that returns the result or raises the exception of the next completed awaitable. This pattern is compatible with Python versions older than 3.13: # next_connect is not one of the original task objects. It must be # awaited to obtain the result value or raise the exception of the A is raised if the timeout occurs before all awaitables are done. This is raised by the loop during asynchronous iteration or by the coroutines yielded during plain iteration. Deprecated since version 3.10: Deprecation warning is emitted if not all awaitable objects in the aws iterable are Future-like objects and there is no running event loop. Changed in version 3.13: The result can now be used as either an asynchronous iterator or as a plain iterator (previously it was only a plain iterator).\n\nTasks are used to run coroutines in event loops. If a coroutine awaits on a Future, the Task suspends the execution of the coroutine and waits for the completion of the Future. When the Future is done, the execution of the wrapped coroutine resumes. Event loops use cooperative scheduling: an event loop runs one Task at a time. While a Task awaits for the completion of a Future, the event loop runs other Tasks, callbacks, or performs IO operations. Use the high-level function to create Tasks, or the low-level or functions. Manual instantiation of Tasks is discouraged. To cancel a running Task use the method. Calling it will cause the Task to throw a exception into the wrapped coroutine. If a coroutine is awaiting on a Future object during cancellation, the Future object will be cancelled. can be used to check if the Task was cancelled. The method returns if the wrapped coroutine did not suppress the exception and was actually cancelled. inherits from all of its APIs except and . An optional keyword-only context argument allows specifying a custom for the coro to run in. If no context is provided, the Task copies the current context and later runs its coroutine in the copied context. An optional keyword-only eager_start argument allows eagerly starting the execution of the at task creation time. If set to and the event loop is running, the task will start executing the coroutine immediately, until the first time the coroutine blocks. If the coroutine returns or raises without blocking, the task will be finished eagerly and will skip scheduling to the event loop. Changed in version 3.7: Added support for the module. Changed in version 3.8: Added the name parameter. Deprecated since version 3.10: Deprecation warning is emitted if loop is not specified and there is no running event loop. Return if the Task is done. A Task is done when the wrapped coroutine either returned a value, raised an exception, or the Task was cancelled. Return the result of the Task. If the Task is done, the result of the wrapped coroutine is returned (or if the coroutine raised an exception, that exception is re-raised.) If the Task has been cancelled, this method raises a exception. If the Task’s result isn’t yet available, this method raises an exception. Return the exception of the Task. If the wrapped coroutine raised an exception that exception is returned. If the wrapped coroutine returned normally this method returns . If the Task has been cancelled, this method raises a exception. If the Task isn’t done yet, this method raises an exception. Add a callback to be run when the Task is done. This method should only be used in low-level callback-based code. See the documentation of for more details. This method should only be used in low-level callback-based code. See the documentation of for more details. Return the list of stack frames for this Task. If the wrapped coroutine is not done, this returns the stack where it is suspended. If the coroutine has completed successfully or was cancelled, this returns an empty list. If the coroutine was terminated by an exception, this returns the list of traceback frames. The frames are always ordered from oldest to newest. Only one stack frame is returned for a suspended coroutine. The optional limit argument sets the maximum number of frames to return; by default all available frames are returned. The ordering of the returned list differs depending on whether a stack or a traceback is returned: the newest frames of a stack are returned, but the oldest frames of a traceback are returned. (This matches the behavior of the traceback module.) Print the stack or traceback for this Task. This produces output similar to that of the traceback module for the frames retrieved by . The limit argument is passed to directly. The file argument is an I/O stream to which the output is written; by default output is written to . Return the coroutine object wrapped by the . This will return for Tasks which have already completed eagerly. See the Eager Task Factory. Changed in version 3.12: Newly added eager task execution means result may be . Return the object associated with the task. Return the name of the Task. If no name has been explicitly assigned to the Task, the default asyncio Task implementation generates a default name during instantiation. Set the name of the Task. The value argument can be any object, which is then converted to a string. In the default Task implementation, the name will be visible in the output of a task object. Request the Task to be cancelled. This arranges for a exception to be thrown into the wrapped coroutine on the next cycle of the event loop. The coroutine then has a chance to clean up or even deny the request by suppressing the exception with a … … … block. Therefore, unlike , does not guarantee that the Task will be cancelled, although suppressing cancellation completely is not common and is actively discouraged. Should the coroutine nevertheless decide to suppress the cancellation, it needs to call in addition to catching the exception. Changed in version 3.11: The parameter is propagated from cancelled task to its awaiter. The following example illustrates how coroutines can intercept the cancellation request: Return if the Task is cancelled. The Task is cancelled when the cancellation was requested with and the wrapped coroutine propagated the exception thrown into it. Decrement the count of cancellation requests to this Task. Note that once execution of a cancelled task completed, further calls to are ineffective. This method is used by asyncio’s internals and isn’t expected to be used by end-user code. In particular, if a Task gets successfully uncancelled, this allows for elements of structured concurrency like Task Groups and to continue running, isolating cancellation to the respective structured block. For example: # Outer code not affected by the timeout: While the block with and might get cancelled due to the timeout, should continue running even in case of the timeout. This is implemented with . context managers use in a similar fashion. If end-user code is, for some reason, suppressing cancellation by catching , it needs to call this method to remove the cancellation state. When this method decrements the cancellation count to zero, the method checks if a previous call had arranged for to be thrown into the task. If it hasn’t been thrown yet, that arrangement will be rescinded (by resetting the internal flag). Changed in version 3.13: Changed to rescind pending cancellation requests upon reaching zero. Return the number of pending cancellation requests to this Task, i.e., the number of calls to less the number of calls. Note that if this number is greater than zero but the Task is still executing, will still return . This is because this number can be lowered by calling , which can lead to the task not being cancelled after all if the cancellation requests go down to zero. This method is used by asyncio’s internals and isn’t expected to be used by end-user code. See for more details."
    },
    {
        "link": "https://stackoverflow.com/questions/29269370/how-to-properly-create-and-run-concurrent-tasks-using-pythons-asyncio-module",
        "document": "I am trying to properly understand and implement two concurrently running objects using Python 3's relatively new module.\n\nIn a nutshell, asyncio seems designed to handle asynchronous processes and concurrent execution over an event loop. It promotes the use of (applied in async functions) as a callback-free way to wait for and use a result, without blocking the event loop. (Futures and callbacks are still a viable alternative.)\n\nIt also provides the class, a specialized subclass of designed to wrap coroutines. Preferably invoked by using the method. The intended use of asyncio tasks is to allow independently running tasks to run 'concurrently' with other tasks within the same event loop. My understanding is that are connected to the event loop which then automatically keeps driving the coroutine between statements.\n\nI like the idea of being able to use concurrent Tasks without needing to use one of the classes, but I haven't found much elaboration on implementation.\n\nThis is how I'm currently doing it:\n\nIn the case of trying to concurrently run two looping Tasks, I've noticed that unless the Task has an internal expression, it will get stuck in the loop, effectively blocking other tasks from running (much like a normal loop). However, as soon the Tasks have to (a)wait, they seem to run concurrently without an issue.\n\nThus, the statements seem to provide the event loop with a foothold for switching back and forth between the tasks, giving the effect of concurrency.\n\nDoes this implementation pass for a 'proper' example of concurrent looping Tasks in ?\n\nIs it correct that the only way this works is for a to provide a blocking point ( expression) in order for the event loop to juggle multiple tasks?\n\n2022 UPDATE: Please note that the API has changed fairly substantially since this question was asked. See the newly marked as correct answer which now shows the correct use of the API given Python 3.10. I still recommend the answer from @dano for broader knowledge of how this works under the hood."
    },
    {
        "link": "https://medium.com/@obaff/asyncio-in-python-a-comprehensive-guide-with-examples-3a3f854017f9",
        "document": "Python’s asyncio library is designed for writing concurrent code using the async/await syntax. It provides a foundation for writing asynchronous programs, handling I/O-bound operations efficiently by non-blocking calls, making it suitable for programs that deal with network connections, file I/O, or databases. This tutorial will introduce the basics of asyncio, gradually moving from simple to more complex examples.\n\nAsynchronous programming allows a program to handle tasks concurrently without waiting for one task to finish before starting another. In synchronous programming, if a function call takes time (like downloading a file), the whole program waits. In contrast, asynchronous code can perform multiple tasks seemingly simultaneously by switching between them while waiting for responses (e.g., I/O operations).\n\nAsyncio is Python’s built-in library for writing asynchronous code. It allows the execution of non-blocking functions that can yield control to the event loop when waiting for I/O operations, freeing up the program to execute other tasks.\n\nThe key components of asyncio are:\n• Event Loop: The core of asyncio that executes coroutines, scheduling them as needed.\n• Futures/Tasks: Objects representing coroutines or other computations that are yet to be completed.\n\nThe basic structure of asyncio is built around coroutines. A coroutine is a function that can suspend execution and allows other coroutines to run. Coroutines are defined with the async def keyword, and execution is paused with await.\n\nExplanation:\n\n- `say_hello` is a coroutine, denoted by async def.\n\n- await pauses the coroutine, allowing the event loop to execute other tasks while waiting for asyncio.sleep(1) to finish.\n\nCoroutines are at the heart of asyncio. These functions return a coroutine object and need to be awaited or scheduled to run.\n\nHere, task1 and task2 run sequentially. The main() coroutine orchestrates the flow by awaiting each task in sequence.\n\nTo run multiple coroutines concurrently, we can use:\n\n- asyncio.gather(): Runs a collection of coroutines concurrently.\n\n- asyncio.create_task(): Schedules a coroutine as a separate task.\n\nExplanation:\n\n- Both task1 and task2 run concurrently, so the total execution time is determined by the longest-running task (2 seconds in this case).\n\nasyncio.create_task() creates separate tasks that run in the background, allowing you to start a coroutine without blocking the main flow.\n\nExplanation:\n\nHere, both tasks are scheduled and run concurrently. The await statements ensure that the main function waits for both tasks to complete.\n\nasyncio.wait_for() allows you to set a timeout for coroutines. If the coroutine doesn’t complete within the specified time, a `TimeoutError` is raised.\n\nExplanation:\n\nThe task takes 5 seconds to complete, but the wait_for timeout is set to 3 seconds, causing a TimeoutError.\n\nasyncio provides several synchronization primitives to coordinate multiple coroutines, including:\n\n- Locks: To prevent multiple coroutines from accessing shared resources simultaneously.\n\n- Event: Signals one or more coroutines when some event has occurred.\n\nThe asyncio.Lock() ensures that only one task at a time accesses the shared resource.\n\nWhen dealing with asynchronous code, exceptions can still occur. You can handle them using try/except blocks within the coroutine.\n\nLet’s put everything together and build a simple web scraper using aiohttp for asynchronous HTTP requests.\n\nExample: Web Scraper with asyncio and aiohttp\n\nExplanation:\n\n- This example uses aiohttp to fetch multiple URLs asynchronously.\n\n- asyncio.gather() is used to collect the responses concurrently, which significantly speeds up the process compared to making sequential requests.\n\nAsyncio is a powerful library in Python for writing concurrent code. It’s well-suited for I/O-bound and high-level structured network code. We’ve covered the basic and advanced features of asyncio, from simple coroutines and task scheduling to handling timeouts and synchronization. By understanding these concepts, you can build highly efficient applications that handle multiple tasks concurrently.\n\nNote: There are affiliate links in the article and if you buy something, I’ll get a commission at no extra cost to you.\n\nThis content is free, and by using the links, You’ll be supporting my work & that means a whole lot to me."
    },
    {
        "link": "https://stackoverflow.com/questions/62528272/what-does-asyncio-create-task-do",
        "document": "It submits the coroutine to run \"in the background\", i.e. concurrently with the current task and all other tasks, switching between them at points. It returns an awaitable handle called a \"task\" which you can also use to cancel the execution of the coroutine.\n\nIt's one of the central primitives of asyncio, the asyncio equivalent of starting a thread. (In the same analogy, awaiting the task with is the equivalent of joining a thread.)\n\nNo, because you explicitly used to start in the background. Had you written something like:\n\n...indeed would not run because no one would have yet submitted it to the event loop. But does exactly that: submit it to the event loop for execution concurrently with other tasks, the point of switching being any .\n\nis not the only way to achieve concurrency in asyncio. It's just a utility function that makes it easier to wait for a number of coroutines to all complete, and submit them to the event loop at the same time. does just the submitting, it should have probably been called or something like that.\n\nWe don't have to, it just serves to wait for both coroutines to finish cleanly. The code could have also awaited or something like that. Returning from (and the event loop) immediately with some tasks still pending would have worked as well, but it would have printed a warning message indicating a possible bug. Awaiting (or canceling) the task before stopping the event loop is just cleaner.\n\nIt's an asyncio construct that tracks execution of a coroutine in a concrete event loop. When you call , you submit a coroutine for execution and receive back a handle. You can await this handle when you actually need the result, or you can never await it, if you don't care about the result. This handle is the task, and it inherits from , which makes it awaitable and also provides the lower-level callback-based interface, such as ."
    },
    {
        "link": "https://docs.python.org/3/library/asyncio.html",
        "document": "asyncio is a library to write concurrent code using the async/await syntax.\n\nasyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\nasyncio is often a perfect fit for IO-bound and high-level structured network code.\n\nasyncio provides a set of high-level APIs to:\n• None run Python coroutines concurrently and have full control over their execution;\n\nAdditionally, there are low-level APIs for library and framework developers to:\n• None create and manage event loops, which provide asynchronous APIs for networking, running subprocesses, handling OS signals, etc;\n\nYou can experiment with an concurrent context in the REPL:\n\nRaises an auditing event with no arguments."
    }
]