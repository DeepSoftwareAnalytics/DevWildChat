[
    {
        "link": "https://rdocumentation.org/packages/brms/versions/2.22.0/topics/bayes_R2.brmsfit",
        "document": "Optional names of response variables. If specified, predictions are performed only for the specified response variables.\n\nShould summary statistics be returned instead of the raw values? Default is .\n\nIf (the default) the mean is used as the measure of central tendency and the standard deviation as the measure of variability. If , the median and the median absolute deviation (MAD) are applied instead. Only used if is .\n\nThe percentiles to be computed by the function. Only used if is .\n\nFurther arguments passed to , which is used in the computation of the R-squared values."
    },
    {
        "link": "https://rdrr.io/cran/brms/man/bayes_R2.brmsfit.html",
        "document": "Optional names of response variables. If specified, predictions are performed only for the specified response variables. Should summary statistics be returned instead of the raw values? Default is . If (the default) the mean is used as the measure of central tendency and the standard deviation as the measure of variability. If , the median and the median absolute deviation (MAD) are applied instead. Only used if is . The percentiles to be computed by the function. Only used if is . Further arguments passed to , which is used in the computation of the R-squared values.\n\nFor an introduction to the approach, see Gelman et al. (2018) and https://github.com/jgabry/bayes_R2/.\n\nIf , an M x C matrix is returned (M = number of response variables and c = ) containing summary statistics of the Bayesian R-squared values. If , the posterior draws of the Bayesian R-squared values are returned in an S x M matrix (S is the number of draws).\n\nAndrew Gelman, Ben Goodrich, Jonah Gabry & Aki Vehtari. (2018). R-squared for Bayesian regression models, The American Statistician. (Preprint available at https://stat.columbia.edu/~gelman/research/published/bayes_R2_v3.pdf)"
    },
    {
        "link": "https://discourse.mc-stan.org/t/bayes-r2-and-conditioning-on-random-effects/20461",
        "document": "I have a question regarding . From the docs, I understand that can be used to condition or refrain from conditioning on random effects in mixed models: The default, NULL, indicates that all estimated group-level parameters are conditioned on. However, I don‚Äôt quite understand the output. Similar to a frequentist framework, I would expect the conditional R2, i.e. the R2 for the model taking random effects into account, to be higher than the ‚Äúmarginal‚Äù R2 that only considers fixed effects. In other word, I would expect the point estimate for to be higher than for . library(rstanarm) m <- stan_lmer(Sepal.Length ~ Petal.Length + (1 | Species), data = iris, refresh = 0, seed = 333) # The default, NULL, indicates that all estimated # group-level parameters are conditioned on. mean(bayes_R2(m, re.form = NULL)) #> [1] 0.8278072 # \"marginal\" R2? mean(bayes_R2(m, re.form = NA)) #> [1] 0.9515886 Is there anything I am missing?\n\nI‚Äôm not exactly sure of what it is supposed to do, but at least it seems to give similar results to lme4: Created on 2021-02-01 by the reprex package (v0.3.0) The original arg explanation says that: re.form specify which random effects to condition on when predicting. If , include all random effects; if or , include no random effects. If I‚Äôm not wrong, the predictions are mostly driven by the population effects than the individual levels (see this thread).\n\nThanks Paul! If I understand you correctly, the above R2 is conditioned on an average random effect level. But how should we interpret it, in comparison with ? In the frequentist marginal/conditional R2 framework, the two R2s can be interpreted as the explanatory power of the full model and that of only the fixed effects, from which one can derive the part of the random effects. What is the correct interpretation for the two Bayesian R2s?\n\nIt seems like the ‚Äúbug‚Äù (see bellow) is specifically in , as it is the only place where : y <- iris$Sepal.Length ## lme4::lmer ---- m <- lme4::lmer(Sepal.Length ~ Petal.Length + (1 | Species), data = iris) p1 <- predict(m, re.form = NULL) p2 <- predict(m, re.form = NA) c( \"with random\" = cor(p1, y), \"no random\" = cor(p2, y) ) #> with random no random #> 0.9146892 0.8717538 ## nlme::lme ---- m <- nlme::lme(Sepal.Length ~ Petal.Length, random = ~ 1 | Species, data = iris) p1 <- predict(m, level = 1) p2 <- predict(m, level = 0) c( \"with random\" = cor(p1, y), \"no random\" = cor(p2, y) ) #> with random no random #> 0.9146892 0.8717538 ## brms::brm ----- m <- brms::brm(Sepal.Length ~ Petal.Length + (1 | Species), data = iris, chains = 1, refresh = 0) p1 <- rstantools::posterior_epred(m, re.form = NULL) p1 <- apply(p1, 2, mean) p2 <- rstantools::posterior_epred(m, re.form = NA) p2 <- apply(p2, 2, mean) c( \"with random\" = cor(p1, y), \"no random\" = cor(p2, y) ) #> with random no random #> 0.9144018 0.8718073 c( \"with random\" = median(rstantools::bayes_R2(m, re.form = NULL)), \"no random\" = median(rstantools::bayes_R2(m, re.form = NA)) ) #> with random no random #> 0.820947 0.724174 ## rstanarm::stan_(g)lmer ----- m <- rstanarm::stan_glmer(Sepal.Length ~ Petal.Length + (1 | Species), data = iris, chains = 1, refresh = 0) p1 <- rstantools::posterior_epred(m, re.form = NULL) p1 <- apply(p1, 2, mean) p2 <- rstantools::posterior_epred(m, re.form = NA) p2 <- apply(p2, 2, mean) c( \"with random\" = cor(p1, y), \"no random\" = cor(p2, y) ) #> with random no random #> 0.9145250 0.8717538 c( \"with random\" = median(rstantools::bayes_R2(m, re.form = NULL)), \"no random\" = median(rstantools::bayes_R2(m, re.form = NA)) ) #> with random no random #> 0.8276229 0.9516522 This seems to be because while estimates R2 as:\n\n If we use the sigma method for , we get the same results as ! ## lme4::lmer ---- m <- lme4::lmer(Sepal.Length ~ Petal.Length + (1 | Species), data = iris) p1 <- predict(m, re.form = NULL) p2 <- predict(m, re.form = NA) c( \"with random\" = var(p1) / (var(p1) + sigma(m)^2), \"no random\" = var(p2) / (var(p2) + sigma(m)^2) ) #> with random no random #> 0.8317347 0.9533562 Where do I collect my trophy? üòÅ"
    },
    {
        "link": "https://paulbuerkner.com/brms/reference/bayes_R2.brmsfit.html",
        "document": ""
    },
    {
        "link": "https://cran.r-project.org/web/packages/brms/brms.pdf",
        "document": ""
    },
    {
        "link": "https://tem11010.github.io/regression_brms",
        "document": "There are many good reasons to analyse your data using Bayesian methods. Historically, however, these methods have been computationally intensive and difficult to implement, requiring knowledge of sometimes challenging coding platforms and languages, like WinBUGS, JAGS, or Stan. Newer R packages, however, including, r2jags, rstanarm, and brms have made building Bayesian regression models in R relatively straightforward. For some background on Bayesian statistics, there is a Powerpoint presentation here. Here I will introduce code to run some simple regression models using the brms package. This package offers a little more flexibility than rstanarm, although the both offer many of the same functionality. I encourage you to check out the extremely helpful vignettes written by Paul Buerkner. Paul‚Äôs Github page is also a useful resource. I won‚Äôt go into too much detail on prior selection, or demonstrating the full flexibility of the brms package (for that, check out the vignettes), but I will try to add useful links where possible. I will also go a bit beyond the models themselves to talk about model selection using loo, and model averaging\n\nBecause these analyses can sometimes be a little sluggish, it is recommended to set the number of cores you use to the maximum number available. You can check how many cores you have available with the following code. We‚Äôll use this bit of code again when we are running our models and doing model selection."
    },
    {
        "link": "https://rensvandeschoot.com/tutorials/r-linear-regression-bayesian-using-brms",
        "document": "This tutorial provides the reader with a basic tutorial how to perform a Bayesian regression in brms, using Stan instead of as the MCMC sampler. Throughout this tutorial, the reader will be guided through importing data files, exploring summary statistics and regression analyses. Here, we will exclusively focus on Bayesian statistics.\n\nIn this tutorial, we start by using the default prior settings of the software. In a second step, we will apply user-specified priors, and if you really want to use Bayes for your own data, we recommend to follow the WAMBS-checklist, also available in other software.\n\nWe are continuously improving the tutorials so let me know if you discover mistakes, or if you have additional resources I can refer to. The source code is available via Github. If you want to be the first to be informed about updates, follow me on Twitter.\n\nThe data we will be using for this exercise is based on a study about predicting PhD-delays (Van de Schoot, Yerkes, Mouw and Sonneveld 2013).The data can be downloaded here. Among many other questions, the researchers asked the Ph.D. recipients how long it took them to finish their Ph.D. thesis (n=333). It appeared that Ph.D. recipients took an average of 59.8 months (five years and four months) to complete their Ph.D. trajectory. The variable B3_difference_extra measures the difference between planned and actual project time in months (mean=9.97, minimum=-31, maximum=91, sd=14.43). For more information on the sample, instruments, methodology and research context we refer the interested reader to the paper. For the current exercise we are interested in the question whether age (M = 31.7, SD = 6.86) of the Ph.D. recipients is related to a delay in their project. The relation between completion time and age is expected to be non-linear. This might be due to that at a certain point in your life (i.e., mid thirties), family life takes up more of your time than when you are in your twenties or when you are older. So, in our model the \\(gap\\) (B3_difference_extra) is the dependent variable and \\(age\\) (E22_Age) and \\(age^2\\)(E22_Age_Squared ) are the predictors. The data can be found in the file . Question: Write down the null and alternative hypotheses that represent this question. Which hypothesis do you deem more likely? \\(H_0:\\) \\(age\\) is not related to a delay in the PhD projects. \\(H_1:\\) \\(age\\) is related to a delay in the PhD projects. \\(H_0:\\) \\(age^2\\) is not related to a delay in the PhD projects. \\(H_1:\\) \\(age^2\\)is related to a delay in the PhD projects.\n\n# if you dont have these packages installed yet, please use the install.packages(\"package_name\") command. library(rstan) library(brms) library(psych) #to get some extended summary statistics library(tidyverse) # needed for data manipulation and plotting You can find the data in the file , which contains all variables that you need for this analysis. Although it is a .csv-file, you can directly load it into R using the following syntax: Alternatively, you can directly download them from GitHub into your R work space using the following command: GitHub is a platform that allows researchers and developers to share code, software and research and to collaborate on projects (see https://github.com/) Once you loaded in your data, it is advisable to check whether your data import worked well. Therefore, first have a look at the summary statistics of your data. you can do this by using the function. Question: Have all your data been loaded in correctly? That is, do all data points substantively make sense? If you are unsure, go back to the .csv-file to inspect the raw data.\n\nIn this exercise you will investigate the impact of Ph.D. students‚Äô \\(age\\) and \\(age^2\\) on the delay in their project time, which serves as the outcome variable using a regression analysis (note that we ignore assumption checking!). As you know, Bayesian inference consists of combining a prior distribution with the likelihood obtained from the data. Specifying a prior distribution is one of the most crucial points in Bayesian inference and should be treated with your highest attention (for a quick refresher see e.g. Van de Schoot et al. 2017). In this tutorial, we will first rely on the default prior settings, thereby behaving a ‚Äònaive‚Äô Bayesians (which might not always be a good idea). To run a multiple regression with brms, you first specify the model, then fit the model and finally acquire the summary (similar to the frequentist model using ). The model is specified as follows:\n‚Ä¢ A dependent variable we want to predict.\n‚Ä¢ A ‚Äú~‚Äù, that we use to indicate that we now give the other variables of interest. (comparable to the ‚Äò=‚Äô of the regression equation).\n‚Ä¢ The different independent variables separated by the summation symbol ‚Äò+‚Äô.\n‚Ä¢ Finally, we insert that the dependent variable has a variance and that we want an intercept.\n‚Ä¢ We do set a seed to make the results exactly reproducible. There are many other options we can select, such as the number of chains how many iterations we want and how long of a warm-up phase we want, but we will just use the defaults for now. For more information on the basics of brms, see the website and vignettes The following code is how to specify the regression model: Now we will have a look at the summary by using or for more precise estimates of the coefficients ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: diff ~ age + age2 ## Data: dataPHD (Number of observations: 333) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -47.31 12.66 -71.99 -23.49 2068 1.00 ## age 2.66 0.60 1.53 3.84 2030 1.00 ## age2 -0.03 0.01 -0.04 -0.01 2038 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 14.05 0.56 13.01 15.19 2914 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The results that stem from a Bayesian analysis are genuinely different from those that are provided by a frequentist model. The key difference between Bayesian statistical inference and frequentist statistical methods concerns the nature of the unknown parameters that you are trying to estimate. In the frequentist framework, a parameter of interest is assumed to be unknown, but fixed. That is, it is assumed that in the population there is only one true population parameter, for example, one true mean or one true regression coefficient. In the Bayesian view of subjective probability, all unknown parameters are treated as uncertain and therefore are be described by a probability distribution. Every parameter is unknown, and everything unknown receives a distribution. This is why in frequentist inference, you are primarily provided with a point estimate of the unknown but fixed population parameter. This is the parameter value that, given the data, is most likely in the population. An accompanying confidence interval tries to give you further insight in the uncertainty that is attached to this estimate. It is important to realize that a confidence interval simply constitutes a simulation quantity. Over an infinite number of samples taken from the population, the procedure to construct a (95%) confidence interval will let it contain the true population value 95% of the time. This does not provide you with any information how probable it is that the population parameter lies within the confidence interval boundaries that you observe in your very specific and sole sample that you are analyzing. In Bayesian analyses, the key to your inference is the parameter of interest‚Äôs posterior distribution. It fulfils every property of a probability distribution and quantifies how probable it is for the population parameter to lie in certain regions. On the one hand, you can characterize the posterior by its mode. This is the parameter value that, given the data and its prior probability, is most probable in the population. Alternatively, you can use the posterior‚Äôs mean or median. Using the same distribution, you can construct a 95% credibility interval, the counterpart to the confidence interval in frequentist statistics. Other than the confidence interval, the Bayesian counterpart directly quantifies the probability that the population value lies within certain limits. There is a 95% probability that the parameter value of interest lies within the boundaries of the 95% credibility interval. Unlike the confidence interval, this is not merely a simulation quantity, but a concise and intuitive probability statement. For more on how to interpret Bayesian analysis, check Van de Schoot et al. 2014. Question: Interpret the estimated effect, its interval and the posterior distribution. \\(Age\\) seems to be a relevant predictor of PhD delays, with a posterior mean regression coefficient of 2.67, 95% Credibility Interval [1.53, 3.83]. Also, \\(age^2\\) seems to be a relevant predictor of PhD delays, with a posterior mean of -0.0259, and a 95% credibility Interval of [-0.038, -0.014]. The 95% Credibility Interval shows that there is a 95% probability that these regression coefficients in the population lie within the corresponding intervals, see also the posterior distributions in the figures below. Since 0 is not contained in the Credibility Interval we can be fairly sure there is an effect. Question: Every Bayesian model uses a prior distribution. Describe the shape of the prior distributions of the regression coefficients. To check which default priors are being used by brms, you can use the function or check the brms documentation, which states that, ‚ÄúThe default prior for population-level effects (including monotonic and category specific effects) is an improper flat prior over the reals‚Äù This means, that there an uninformative prior was chosen. We also see that a student-t distribution was chosen for the intercept.\n\nIn brms, you can also manually specify your prior distributions. Be aware that usually, this has to be done BEFORE peeking at the data, otherwise you are double-dipping (!). In theory, you can specify your prior knowledge using any kind of distribution you like. However, if your prior distribution does not follow the same parametric form as your likelihood, calculating the model can be computationally intense. Conjugate priors avoid this issue, as they take on a functional form that is suitable for the model that you are constructing. For your normal linear regression model, conjugacy is reached if the priors for your regression parameters are specified using normal distributions (the residual variance receives an inverse gamma distribution, which is neglected here). In brms, you are quite flexible in the specification of informative priors. Let‚Äôs re-specify the regression model of the exercise above, using conjugate priors. We leave the priors for the intercept and the residual variance untouched for the moment. Regarding your regression parameters, you need to specify the hyperparameters of their normal distribution, which are the mean and the variance. The mean indicates which parameter value you deem most likely. The variance expresses how certain you are about that. We try 4 different prior specifications, for both the \\(\\beta_{age}\\) regression coefficient, and the \\(\\beta_{age^2}\\) coefficient. First, we use the following prior specifications: In brms, the priors are set using the function. Be careful, Stan uses standard deviations instead of variance in the normal distribution. The standard deviations is the square root of the variance, so a variance of 0.1 corresponds to a standard deviation of 0.316 and a variance of 0.4 corresponds to a standard deviation of 0.632. The priors are presented in code as follows: Now we can run the model again, but with the included. Now fit the model again and request for summary statistics. ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: diff ~ age + age2 ## Data: dataPHD (Number of observations: 333) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -50.14 9.45 -68.37 -30.86 1733 1.00 ## age 2.80 0.45 1.90 3.70 1658 1.00 ## age2 -0.03 0.00 -0.04 -0.02 1698 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 14.02 0.54 13.03 15.09 2550 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Question Fill in the results in the table below: Next, try to adapt the code, using the prior specifications of the other columns and then complete the table. Question: Compare the results over the different prior specifications. Are the results comparable with the default model? Question: Do we end up with similar conclusions, using different prior specifications? To answer these questions, proceed as follows: We can calculate the relative bias to express this difference. (\\(bias= 100*\\frac{(model \\; informative\\; priors\\;-\\;model \\; uninformative\\; priors)}{model \\;uninformative \\;priors}\\)). In order to preserve clarity we will just calculate the bias of the two regression coefficients and only compare the default (uninformative) model with the model that uses the \\(\\mathcal{N}(20, .4)\\) and \\(\\mathcal{N}(20, .1)\\) priors. Copy Paste the following code to R: #1) get the estimates estimatesuninformative <- posterior_summary(model)[c(\"b_age\", \"b_age2\") , \"Estimate\"] estimatesinformative <- posterior_summary(model4)[c(\"b_age\", \"b_age2\") , \"Estimate\"] #2) calculate the bias round(100*((estimatesinformative-estimatesuninformative)/estimatesuninformative), 2) The and indices stand for the \\(\\beta_{age}\\) and \\(\\beta_{age^2}\\) respectively. We can also plot these differences by plotting both the posterior and priors for the five different models we ran. In this example we only plot the regression of coefficient of age \\(\\beta_{age}\\) First we extract the MCMC chains of the 5 different models for only this one parameter (\\(\\beta_{age}\\)=beta[1,2,1]). Copy-past the following code to R: posterior1.2.3.4.5 <- bind_rows(\"uninformative prior\" = as_tibble(as.mcmc(model, pars = \"b_age\", exact_match = TRUE ,combine_chains = TRUE)), \"informative prior 1\" = as_tibble(as.mcmc(model2, pars = \"b_age\", exact_match = TRUE ,combine_chains = TRUE)), \"informative prior 2\" = as_tibble(as.mcmc(model3, pars = \"b_age\", exact_match = TRUE ,combine_chains = TRUE)), \"informative prior 3\" = as_tibble(as.mcmc(model4, pars = \"b_age\", exact_match = TRUE ,combine_chains = TRUE)), \"informative prior 4\" = as_tibble(as.mcmc(model5, pars = \"b_age\", exact_match = TRUE ,combine_chains = TRUE)), .id = \"id1\") prior1.2.3.4.5 <- bind_rows(\"uninformative prior\" = enframe(rnorm(10000, mean=0, sd=sqrt(1/1e-2))), \"informative prior 1\" = enframe(rnorm(10000, mean=3, sd=sqrt(0.4))), \"informative prior 2\" = enframe(rnorm(10000, mean=3, sd=sqrt(1000))), \"informative prior 3\" = enframe(rnorm(10000, mean=20, sd=sqrt(0.4))), \"informative prior 4\" = enframe(rnorm(10000, mean=20, sd=sqrt(1000))), .id = \"id1\") %>% rename(b_age = value)# here we sample a large number of values from the prior distributions to be able to plot them. priors.posterior <- bind_rows(\"posterior\" = posterior1.2.3.4.5, \"prior\" = prior1.2.3.4.5, .id = \"id2\") instead of sampling the priors like this, you could also get the actual prior values sampled by Stan by adding the command to the function, this would save the priors as used by stan. With enough samples this would yield the same results Then, we can plot the different posteriors and priors by using the following code: ggplot(data = priors.posterior, mapping = aes(x = b_age, fill = id1, colour = id2, linetype = id2, alpha = id2))+ geom_density(size=1)+ scale_x_continuous(limits=c(0, 23))+ scale_colour_manual(name = 'Posterior/Prior', values = c(\"black\",\"red\"), labels = c(\"posterior\", \"prior\"))+ scale_linetype_manual(name = 'Posterior/Prior', values = c(\"solid\",\"dotted\"), labels = c(\"posterior\", \"prior\"))+ scale_alpha_discrete(name ='Posterior/Prior', range = c(.7,.3), labels = c(\"posterior\", \"prior\"))+ scale_fill_manual(name = \"Densities\", values = c(\"Yellow\",\"darkred\",\"blue\", \"green\", \"pink\"))+ labs(title = expression(\"Influence of (Informative) Priors on\" ~ beta[Age]), subtitle = \"5 different densities of priors and posteriors\") Now, with the information from the table, the bias estimates and the plot you can answer the two questions about the influence of the priors on the results. #1) get the estimates estimatesuninformative <- posterior_summary(model)[c(\"b_age\", \"b_age2\") , \"Estimate\"] estimatesinformative <- posterior_summary(model4)[c(\"b_age\", \"b_age2\") , \"Estimate\"] #2) calculate the bias round(100*((estimatesinformative - estimatesuninformative)/estimatesuninformative), 2) We see that the influence of this highly informative prior is around 386% and 406% on the two regression coefficients respectively. This is a large difference and we thus certainly would not end up with similar conclusions. The results change with different prior specifications, but are still comparable. Only using \\(\\mathcal{N}(20, .4)\\) for age, results in a really different coefficients, since this prior mean is far from the mean of the data, while its variance is quite certain. However, in general the other results are comparable. Because we use a big dataset the influence of the prior is relatively small. If one would use a smaller dataset the influence of the priors are larger. To check this you can use these lines to sample roughly 20% of all cases and redo the same analysis. The results will of course be different because we use many fewer cases (probably too few!). Use this code. We made a new dataset with randomly chosen 60 of the 333 observations from the original dataset. You can repeat the analyses with the same code and only changing the name of the dataset to see the influence of priors on a smaller dataset. Run the model model.informative.priors2 with this new dataset If you really want to use Bayes for your own data, we recommend to follow the WAMBS-checklist, which you are guided through by this exercise. Greenland, S., Senn, S. J., Rothman, K. J., Carlin, J. B., Poole, C., Goodman, S. N. Altman, D. G. (2016). Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. European Journal of Epidemiology 31 (4). https://doi.org/10.1007/s10654-016-0149-3 Hoffman, M. D., & Gelman, A. (2014). The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15(1), 1593-1623. van de Schoot R, Yerkes MA, Mouw JM, Sonneveld H (2013) What Took Them So Long? Explaining PhD Delays among Doctoral Candidates. PLoS ONE 8(7): e68839. https://doi.org/10.1371/journal.pone.0068839 Trafimow D, Amrhein V, Areshenkoff CN, Barrera-Causil C, Beh EJ, Bilgi? Y, Bono R, Bradley MT, Briggs WM, Cepeda-Freyre HA, Chaigneau SE, Ciocca DR, Carlos Correa J, Cousineau D, de Boer MR, Dhar SS, Dolgov I, G?mez-Benito J, Grendar M, Grice J, Guerrero-Gimenez ME, Guti?rrez A, Huedo-Medina TB, Jaffe K, Janyan A, Karimnezhad A, Korner-Nievergelt F, Kosugi K, Lachmair M, Ledesma R, Limongi R, Liuzza MT, Lombardo R, Marks M, Meinlschmidt G, Nalborczyk L, Nguyen HT, Ospina R, Perezgonzalez JD, Pfister R, Rahona JJ, Rodr?guez-Medina DA, Rom?o X, Ruiz-Fern?ndez S, Suarez I, Tegethoff M, Tejo M, ** van de Schoot R** , Vankov I, Velasco-Forero S, Wang T, Yamada Y, Zoppino FC, Marmolejo-Ramos F. (2017) Manipulating the alpha level cannot cure significance testing ‚Äì comments on ‚ÄúRedefine statistical significance‚Äù PeerJ reprints 5:e3411v1 https://doi.org/10.7287/peerj.preprints.3411v1"
    },
    {
        "link": "https://andrewheiss.com/blog/2023/05/15/fancy-bayes-diffs-props",
        "document": "Use R, Stan, and {brms} to calculate differences between categorical proportions in a principled Bayesian way"
    },
    {
        "link": "https://santiagobarreda.com/bmmrmd/fitting-bayesian-regression-models-with-brms.html",
        "document": ""
    },
    {
        "link": "https://rensvandeschoot.com/tutorials/generalised-linear-models-with-brms",
        "document": "This tutorial provides an introduction to Bayesian GLM (genearlised linear models) with non-informative priors using the package in R. If you have not followed the Intro to Frequentist (Multilevel) Generalised Linear Models (GLM) in R with glm and lme4 tutorial, we highly recommend that you do so, because it offers more extensive information about GLM. If you are not familar with Bayesian inference, we also recommend that you read this tutorial Building a Multilevel Model in BRMS Tutorial: Popularity Data prior to using this tutorial.\n\nThe current tutorial specifically focuses on the use of Bayesian logistic regression in both binary-outcome and count/porportion-outcome scenarios, and the respective approaches to model evaluation. The tutorial uses the Thai Educational Data example in Chapter 6 of the book Multilevel analysis: Techniques and applications. Furthermore, the tutorial briefly demonstrates the multilevel extension of Bayesian GLM models.\n\nThis tutorial follows this structure:\n\n 1. Preparation;\n\n 2. Introduction to GLM;\n\n 3. Thai Educational Data;\n\n 4. Data Preparation;\n\n 5. Bayesian Binary (Bernoulli) Logistic Regression;\n\n 6. Bayesian Binomial Logistic Regression;\n\n 7. Bayesian Multilevel Logistic Regression.\n\nNote that this tutorial is meant for beginners and therefore does not delve into technical details and complex models. For a detailed introduction into frequentist multilevel models, see this LME4 Tutorial. For an extensive overview of GLM models, see here. If you want to use the Bayesian approach for your own research, we recommend that you follow the WAMBS-checklist.\n\nIf you are already familar with generalised linear models (GLM), you can proceed to the next section. Otherwise, click ‚ÄúRead More‚Äù to learn about GLM. Recall that in a linear regression model, the object is to model the expected value of a continuous variable, \\(Y\\), as a linear function of the predictor, \\(\\eta = X\\beta\\). The model structure is thus: \\(E(Y) = X\\beta + e\\), where \\(e\\) refers to the residual error term. The linear regression model assumes that \\(Y\\) is continous and comes from a normal distribution, that \\(e\\) is normally distributed and that the relationship between the linear predictor \\(\\eta\\) and the expected outcome \\(E(Y)\\) is strictly linear. However, these assumptions are easily violated in many real world data examples, such as those with binary or proportional outcome variables and those with non-linear relationships between the predictors and the outcome variable. In these scenarios where linear regression models are clearly inappropriate, generalised linear models (GLM) are needed. The GLM is the genearlised version of linear regression that allows for deviations from the assumptions underlying linear regression. The GLM generalises linear regression by assuming the dependent variable \\(Y\\) to be generated from any particular distribution in an exponential family (a large class of probability distributions that includes the normal, binomial, Poisson and gamma distributions, among others). In this way, the distribution of \\(Y\\) does not necessarily have to be normal. In addition, the GLM allows the linear predictor \\(\\eta\\) to be connected to the expected value of the outcome variable, \\(E(Y)\\), via a link function \\(g(.)\\). The outcome variable, \\(Y\\), therefore, depends on \\(\\eta\\) through \\(E(Y) = g^{-1}(\\eta) = g^{-1}(X\\beta)\\). In this way, the model does not assume a linear relationship between \\(E(Y)\\) and \\(\\eta\\); instead, the model assumes a linear relationship between \\(E(Y)\\) and the transformed \\(g^{-1}(\\eta)\\). This tutorial focuses on the Bayesian version of the probably most popular example of GLM: logistic regression. Logistic regression has two variants, the well-known binary logistic regression that is used to model binary outcomes (1 or 0; ‚Äúyes‚Äù or ‚Äúno‚Äù), and the less-known binomial logistic regression suited to model count/proportion data. Binary logistic regression assumes that \\(Y\\) comes from a Bernoulli distribution, where \\(Y\\) only takes a value of 1 (target event) or 0 (non-target event). Binary logistic regression connects \\(E(Y)\\) and \\(\\eta\\) via the logit link \\(\\eta = logit(\\pi) = log(\\pi/(1-\\pi))\\), where \\(\\pi\\) refers to the probability of the target event (\\(Y = 1\\)). Binomial logistic regression, in contrast, assumes a binomial distribution underlying \\(Y\\), where \\(Y\\) is interpreted as the number of target events, can take on any non-negative integer value and is binomially distributed with regards to \\(n\\) number of trials and \\(\\pi\\) probability of the target event. The link function is the same as that of binary logistic regression. The next section details the exampler data (Thai Educational Data) in this tutorial, followed by the demonstration of the use of Bayesian binary, Bayesian binomial logistic regression and Bayesian multilevel binary logistic regression. For the frequentist versions of these models, see the Intro to Frequentist (Multilevel) Generalised Linear Models (GLM) in R with glm and lme4 tutorial.\n\n5.1. Explore Data: number of by and It seems that the number of pupils who repeated a grade differs quite a bit between the two genders, with more male pupils having to repeat a grade. More pupils who did not have preschool education repeated a grade. This observation suggests that and might be predictive of . The function from the package performs Bayesian GLM. The has three basic arguments that are identical to those of the function: , and . However, note that in the argument, we need to specify (rather than ) for a binary logistic regression. The function has a few more additional (and necessary) arguments that does not offer: specifies the burn-in period (i.e. number of iterations that should be discarded); specifies the total number of iterations (including the burn-in iterations); specifies the number of chains; specifies the starting values of the iterations (normally you can either use the maximum likelihood esimates of the parameters as starting values, or simply ask the algorithm to start with zeros); specifies the number of cores used for the algorithm; specifies the random seed, allowing for replication of results. See below the specification of the binary logistic regression model with two predictors, without using informative priors. Before looking at the model summary, we should check whether there is evidence of non-convergence for the two chains. To do so, we can use the function from the package. First, we plot the caterpillar plot for each parameter of interest. The plot only shows the iterations after the burn-in period. The two chains mix well for all of the parameters and therefore, we can conclude no evidence of non-convergence. We can also check autocorrelation, considering that the presence of strong autocorrelation would bias variance estimates. The plot shows no evidence of autocorrelation for all model variables in both chains, as the autocorrelation parameters all quickly diminish to around zero. Now, we can safely proceed to the interpretation of the model. Below is the model summary of the Bayesian binary logistic regression model. ## Family: bernoulli ## Links: mu = logit ## Formula: REPEAT ~ SEX + PPED ## Data: ThaiEdu_New (Number of observations: 7516) ## Samples: 2 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 3000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -1.77 0.06 -1.88 -1.65 2621 1.00 ## SEXboy 0.43 0.07 0.30 0.57 2470 1.00 ## PPEDyes -0.61 0.07 -0.74 -0.48 2451 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). For comparison, below is the model summary of the frequentist binary logistic regression model. ## ## Call: ## glm(formula = REPEAT ~ SEX + PPED, family = binomial(link = \"logit\"), ## data = ThaiEdu_New) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.6844 -0.5630 -0.5170 -0.4218 2.2199 ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -1.76195 0.05798 -30.387 < 2e-16 *** ## SEXboy 0.42983 0.06760 6.358 2.04e-10 *** ## PPEDyes -0.61298 0.06833 -8.971 < 2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 6140.8 on 7515 degrees of freedom ## Residual deviance: 6016.2 on 7513 degrees of freedom ## AIC: 6022.2 ## ## Number of Fisher Scoring iterations: 4 From the model summary above, we can see that the Bayesian model estimates are almost identical to those of the frequentist model. The interpretation of these estimates are the same in both frequentist and Bayesian models. Nevertheless, note that the interpretation of the uncertainty intervals is not the same between the two models. In the frequentist model, the idea behind using a 95% uncertainty interval (confidence interval) is that, under repeated sampling, 95% of the resulting uncertainy intervals would cover the true population value. That allows us to say that, for a given 95% confidence interval, we are 95% confident that this confidence interval contains the true population value. However, it does not allow us to say that there is a 95% chance that the confidence interval contains the true population value (i.e. frequentist uncertainty intervals are not probability statements). In contrast, in the Bayesian model, the 95% uncertainty interval (called credibility interval), which is more interpretable, states that there is 95% chance that the true population value falls within this interval. When the 95% credibility intervals do not contain zero, we conclude that the respective model parameters are likely meaningful. Let‚Äôs visualise the point estimates and their associated uncertainty intervals, using the function. The plot above shows the densities of the parameter estimates. The dark blue line in each density represents the point estimate, while the light-blue area indicates the 95% credibility intervals. We can easily see that both and are meaningful predictors, as their credibility intervals do not contain zero and their densities have a very narrow shape. positively predicts a pupil‚Äôs probability of repeating a grade, while negatively so. Specifically, in comparison to being a girl, being a boy is more likely to repeat a grade, assuming everything else stays constant. Having previous schooling is less likely to result in repeating a grade, assuming everything else stays constant. To interpret the value of the parameter estimates, we need to exponentiate the estimates. See below. We can also plot densities of these parameter estimates. For this, we again use the function from . Note that the interpretation of the parameter estimates is linked to the odds rather than probabilities. The definition of odds is: P(event occurring)/P(event not occurring). In this analysis, assuming everything else stays the same, being a boy increases the odds of repeating a grade by 54%, in comparison to being a girl; having preschool education lowers the odds of repeating a grade by (1 ‚Äì 0.54)% = 46%, in comparison to not having preschool education, assuming everything else stays constant. The baseline odds (indicated by the intercept term) of repeating a grade, namely if you‚Äôre a girl with no previous schooling, is about 17%. We can plot the marginal effects (i.e. estimated probabilities of repeating a grade) of the variables in the model. Below, we show how different combinations of and result in different probability estimates. The advantage of this approach is that probabilities are more interpretable than odds. As we can see, being a male pupil with no preschool education has the highest probability (~0.21), followed by being a girl with no preschool education (~0.15), being a boy with preschool education (~0.13), and lastly, being a girl with preschool education (~0.09). Note that both 68% (thicker inner lines) and 95% (thinner outer lines) credibility intervals for the estimates are included to give us some idea of the uncertainties of the estimates. In the Intro to Frequentist (Multilevel) Generalised Linear Models (GLM) in R with glm and lme4 tutorial, we learn that we can use the likelihood ratio test and AIC to assess the goodness of fit of the model(s). However, these two approaches do not apply to Bayesian models. Instead, Bayesian models make use of so-called Posterior Predictive P-values (PPPs) to assess the fit of the model. In addition, many also use Bayes factors to quantify support from the data for the model. This tutorial does not delve into PPPs or Bayes factors because of the complexity of the topics. The other two measures mentioned in Intro to Frequentist (Multilevel) Generalised Linear Models (GLM) in R with glm and lme4 are correct classification rate and area under the curve (AUC). They are model-agnostic, meaning they can be applied to both frequentist and Bayesian models. The percentage of correct classification is a useful measure to see how well the model fits the data. #use the `predict()` function to calculate the predicted probabilities of pupils in the original data from the fitted model Pred <- predict(Bayes_Model_Binary, type = \"response\") Pred <- if_else(Pred[,1] > 0.5, 1, 0) ConfusionMatrix <- table(Pred, pull(ThaiEdu_New, REPEAT)) #`pull` results in a vector #correct classification rate sum(diag(ConfusionMatrix))/sum(ConfusionMatrix) We can see that the model correctly classifies 85.8% of all the observations. However, a closer look at the confusion matrix reveals that the model predicts all of the observations to belong to class ‚Äú0‚Äù, meaning that all pupils are predicted not to repeat a grade. Given that the majority category of the variable is 0 (No), the model does not perform better in classification than simply assigning all observations to the majority class 0 (No). An alternative to using correct classification rate is the Area under the Curve (AUC) measure. The AUC measures discrimination, that is, the ability of the test to correctly classify those with and without the target response. In the current data, the target response is repeating a grade. We randomly pick one pupil from the ‚Äúrepeating a grade‚Äù group and one from the ‚Äúnot repeating a grade‚Äù group. The pupil with the higher predicted probability should be the one from the ‚Äúrepeating a grade‚Äù group. The AUC is the percentage of randomly drawn pairs for which this is true. This procedure sets AUC apart from the correct classification rate because the AUC is not dependent on the imblance of the proportions of classes in the outcome variable. A value of 0.50 means that the model does not classify better than chance. A good model should have an AUC score much higher than 0.50 (preferably higher than 0.80). # Compute AUC for predicting Class with the model Prob <- predict(Bayes_Model_Binary, type=\"response\") Prob <- Prob[,1] Pred <- prediction(Prob, as.vector(pull(ThaiEdu_New, REPEAT))) AUC <- performance(Pred, measure = \"auc\") AUC <- AUC@y.values[[1]] AUC With an AUC score of close to 0.60, the model does not discriminate well.\n\nAs explained in the Intro to Frequentist (Multilevel) Generalised Linear Models (GLM) in R with glm and lme4 tutorial, logistic regression can also be used to model count or proportion data. Binary logistic regression assumes that the outcome variable comes from a bernoulli distribution (which is a special case of binomial distributions) where the number of trial \\(n\\) is 1 and thus the outcome variable can only be 1 or 0. In contrast, binomial logistic regression assumes that the number of the target events follows a binomial distribution with \\(n\\) trials and probability \\(q\\). In this way, binomial logistic regression allows the outcome variable to take any non-negative integer value and thus is capable of handling count data. The information about individual pupils that are clustered within schools. By aggregating the number of pupils who repeated a grade by school, we obtain a new data set where each row represents a school, with information about the proportion of pupils repeating a grade in that school. The (mean SES score) is also on the school level; therefore, it can be used to predict proportion or count of pupils who repeat a grade in a particular school. See below. In this new data set, refers to the number of pupils who repeated a grade; refers to the total number of students in a particular school. We can see that the proportion of students who repeated a grade is (moderately) negatively related to the inverse-logit of . Note that we model the variable as its inverse-logit because in a binomial regression model, we assume a linear relationship between the inverse-logit of the linear predictor and the outcome (i.e. proportion of events), not linearity between the predictor itself and the outcome. To fit a Bayesian binomial logistic regression model, we also use the function like we did with the previous Bayesian binary logistic regression model. There are, however, two differences: First, to specify the outcome variable in the formula, we need to specify both the number of target events ( ) and the total number of trials ( ) wrapped in , which are separated by . In addition, the should be ‚Äúbinomial‚Äù instead of ‚Äúbernoulli‚Äù. ## Family: binomial ## Links: mu = logit ## Formula: REPEAT | trials(TOTAL) ~ MSESC ## Data: ThaiEdu_Prop (Number of observations: 356) ## Samples: 2 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 3000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -1.81 0.03 -1.87 -1.74 2743 1.00 ## MSESC -0.44 0.09 -0.62 -0.26 2478 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). ## ## Call: ## glm(formula = cbind(REPEAT, TOTAL - REPEAT) ~ MSESC, family = binomial(logit), ## data = ThaiEdu_Prop) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.3629 -1.8935 -0.5083 1.1674 6.9494 ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -1.80434 0.03324 -54.280 < 2e-16 *** ## MSESC -0.43644 0.09164 -4.763 1.91e-06 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1480.7 on 355 degrees of freedom ## Residual deviance: 1457.3 on 354 degrees of freedom ## AIC: 2192 ## ## Number of Fisher Scoring iterations: 5 We can see that the model estimates between the Bayesian and the frequentist binomial logistic regression models are very similar. Note that we skipped the step of checking model convergence, for the sake of keeping this tutorial shorter. You can use the same codes we showed before (with the binary logistic regression model) to check the convergence of this model. The parameter interpretation in a binomial regression model is the same as that in a binary logistic regression model. We know from the model summary above that the mean SES score of a school is negatively related to the odds of students repeating a grade in that school. To enhance interpretability, we again calculate the exponentiated coefficient estimate of . Since is a continous variable, we can standardise the exponentiated estimate (by multiplying the original estimate with the SD of the variable, and then then exponentiating the resulting number). We can see that with a SD increase in , the odds of students repeating a grade is lowered by about (1 ‚Äì 85%) = 15%. ‚ÄúQ2.5‚Äù and ‚ÄúQ97.5‚Äù refer to the lower bound and the upper bound of the uncertainty interval, respectively. This credibility interval does not contain zero, suggesting that the variable is likely meaningful. We can visualise the effect of . Bayes_Model_Prop %>% spread_draws(b_Intercept, b_MSESC) %>% mutate(MSESC = list(seq(-0.77, 1.49, 0.01))) %>% #the observed value range of MSESC unnest(MSESC) %>% mutate(pred = exp(b_Intercept + b_MSESC*MSESC)/(1+exp(b_Intercept + b_MSESC*MSESC))) %>% group_by(MSESC) %>% summarise(pred_m = mean(pred, na.rm = TRUE), pred_low = quantile(pred, prob = 0.025), pred_high = quantile(pred, prob = 0.975)) %>% ggplot(aes(x = MSESC, y = pred_m)) + geom_line() + geom_ribbon(aes(ymin = pred_low, ymax = pred_high), alpha=0.2) + ylab(\"Predicted Probability of Repeating a Grade\") + scale_y_continuous(breaks = seq(0, 0.22, 0.01)) The plot above shows the expected influence of on the probability of a pupil repeating a grade. Holding everything else constant, as increases, the probability of a pupil repeating a grade lowers (from 0.19 to 0.08). The grey shaded areas indicate the 95% credibility intervals of the predicted values at each value of . Similar to the Bayesian binary logistic regression model, we can use the PPPS and Bayes factor (which are not discussed in this tutorial) to evaluate the fit of a Bayesian binomial logistic regression model. Correct classification rate and AUC are not suited here, as the model is not concerned with classification.\n\nThe Bayesian binary logistic regression model introduced earlier is limited to modelling the effects of pupil-level predictors; the Bayesian binomial logistic regression is limited to modelling the effects of school-level predictors. To incorporate both pupil-level and school-level predictors, we can use multilevel models, specifically, Bayesian multilevel binary logistic regression. If you are unfamiliar with multilevel models, you can use Multilevel analysis: Techniques and applications for reference and this tutorial for a good introduction to multilevel models with the package in R. In addition to the motivation above, there are more reasons to use multilevel models. For instance, as the data are clustered within schools, it is likely that pupils from the same school are more similar to each other than those from other schools. Because of this, in one school, the probability of a pupil repeating a grade may be high, while in another school, low. Furthermore, even the relationship between the outcome (i.e. repeating a grade) and the predictor variabales (e.g. gender, preschool education, SES) may be different across schools. Also note that there are missing values in the variable. Using multilevel models can appropriately address these issues. See the following plot as an example. The plot shows the proportions of students repeating a grade across schools. We can see vast differences across schools. Therefore, we need multilevel models. We can also plot the relationship between and by , to see whether the relationship between gender and repeating a grade differs by school. In the plot above, different colors represent different schools. We can see that the relationship between and appears to be quite different across schools. We can make the same plot for and . The relationship between and also appears to be quite different across schools. However, we can also see that most of the relationships follow a downward trend, going from 0 (no previous schooling) to 1 (with previous schooling), indicating a negative relationship between and . Because of the observations above, we can conclude that there is a need for multilevel modelling in the current data, with not only a random intercept ( ) but potentially also random slopes of the and . Prior to fitting a multilevel model, it is necessary to center the predictors by using an appropriately chosen centering method (i.e. grand-mean centering or within-cluster centering), because the centering approach matters for the interpretation of the model estimates. Following the advice of Enders and Tofighi (2007), we should use within-cluster centering for the first-level predictors and , and grand-mean centering for the second-level predictor . ## # A tibble: 6 x 5 ## SCHOOLID SEX PPED REPEAT MSESC ## <fct> <dbl> <dbl> <dbl+lbl> <dbl> ## 1 10103 -0.647 -0.882 0 [no] 0.870 ## 2 10103 -0.647 -0.882 0 [no] 0.870 ## 3 10103 -0.647 0.118 0 [no] 0.870 ## 4 10103 -0.647 0.118 0 [no] 0.870 ## 5 10103 -0.647 0.118 0 [no] 0.870 ## 6 10103 -0.647 0.118 0 [no] 0.870 To specify a multilevel model, we again use the function from the package. Note that the random effect term should be included in parentheses. In addition, within the parentheses, the random slope term(s) and the cluster terms should be separated by . We start by specifying an intercept-only model, in order to assess the impact of the clustering structure of the data. Note that we will skip the step of model convergence diagnostics. Below we calculate the ICC (intra-class correlation) of the intercept-only model. Note that for non-Gaussian Bayesian models (e.g. logistic regression), we need to set ‚Äúppd = T‚Äù such that the variance calculation is based on the posterior predictive distribution. ## ## # Random Effect Variances and ICC ## ## Family: bernoulli (logit) ## Conditioned on: all random effects ## ## ## Variance Ratio (comparable to ICC) ## Ratio: 0.29 HDI 89%: [0.20 0.37] ## ## ## Variances of Posterior Predicted Distribution ## Conditioned on fixed effects: 0.09 HDI 89%: [0.08 0.10] ## Conditioned on rand. effects: 0.12 HDI 89%: [0.12 0.13] ## ## ## Difference in Variances ## Difference: 0.03 HDI 89%: [0.02 0.05] A variance ratio (comparable to ICC) of 0.29 means that 29% of the variation in the outcome variable can be accounted for by the clustering stucture of the data. This provides evidence that a multilevel model may make a difference to the model estimates, in comparison with a non-multilevel model. Therefore, the use of multilevel models is necessary and warrantied. It is good practice to build a multilevel model step by step. However, as this tutorial‚Äôs focus is not on muitilevel modelling, we go directly from the intercept-only model to the full-model that we are ultimately interested in. In the full model, we include not only fixed effect terms of , and and a random intercept term, but also random slope terms for and . Note that we specify , as this model is essentially a binary logistic regression model. ## Family: bernoulli ## Links: mu = logit ## Formula: REPEAT ~ SEX + PPED + MSESC + (1 + SEX + PPED | SCHOOLID) ## Data: ThaiEdu_Center (Number of observations: 7516) ## Samples: 2 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 3000 ## ## Group-Level Effects: ## ~SCHOOLID (Number of levels: 356) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 1.34 0.08 1.19 1.50 851 1.00 ## sd(SEX) 0.38 0.18 0.05 0.73 492 1.00 ## sd(PPED) 0.26 0.18 0.01 0.69 800 1.00 ## cor(Intercept,SEX) 0.42 0.29 -0.22 0.93 1298 1.00 ## cor(Intercept,PPED) -0.20 0.43 -0.90 0.75 2392 1.00 ## cor(SEX,PPED) 0.01 0.49 -0.86 0.87 1502 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -2.29 0.09 -2.47 -2.12 801 1.00 ## SEX 0.45 0.11 0.22 0.66 1615 1.00 ## PPED -0.60 0.13 -0.86 -0.34 2826 1.00 ## MSESC -0.51 0.22 -0.94 -0.07 743 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We can plot the densities of the relevant model parameter estimates. The results (pertaining to the fixed effects) are similar to the results of the previous Bayesian binary logistic regression and binomial logistic regression models. On the pupil-level, has a positive influence on the odds of a pupil repeating a grade, while has a negative influence. On the school-level, has a negative effect on the outcome variable. Among three predictors, and have credibility intervals (indicated by the shaded light blue regions in the densities) that clearly do not contain zero. Therefore, they should be treated as meaningful predictors. In contrast, , despite having a 95% credibility interval without zero, the upper bound of the credibility interval is very close to zero, and its density only contains zero. Because of this, is likely a less relevant predictor than and . Now let‚Äôs look at the random effect terms ( , and ). The density of in the plot is clearly away from zero, indicating the relevance of including this random intercept term in the model. The variance of the random slope of is \\(0.38^2 = 0.14\\), and that of is \\(0.26^2 = 0.07\\). Both variances are not negligible. However, if we look at the density plot, the lower bounds of the credibility intervals of both and are very close to zero, and their densities also not clearly separate from zero. This suggests that including these two random slope terms may not be necessary. We can also plot the random effect terms across schools. #extract posterior distributions of all the random effect terms data_RandomEffect <- ranef(Bayes_Model_Multi_Full) #extract posterior distributions of `sd(Intercept)` r_Intercept <- data_RandomEffect$SCHOOLID[, , 1] %>% as_tibble() %>% rownames_to_column(var = \"SCHOOLID\") %>% mutate(Variable = \"sd(Intercept)\") #extract posterior distributions of `sd(SEX)` r_SEX <- data_RandomEffect$SCHOOLID[, , 2] %>% as_tibble() %>% rownames_to_column(var = \"SCHOOLID\") %>% mutate(Variable = \"sd(SEX)\") #extract posterior distributions of `sd(PPED)` r_PPED <- data_RandomEffect$SCHOOLID[, , 3] %>% as_tibble() %>% rownames_to_column(var = \"SCHOOLID\") %>% mutate(Variable = \"sd(PPED)\") #plot r_Intercept %>% bind_rows(r_SEX) %>% bind_rows(r_PPED) %>% mutate(Contain_Zero = if_else(Q2.5*Q97.5 > 0, \"no\", \"yes\")) %>% ggplot(aes(x = SCHOOLID, y = Estimate, col = Contain_Zero)) + geom_point() + geom_errorbar(aes(ymin=Q2.5, ymax=Q97.5)) + facet_grid(. ~ Variable, scale = \"free\") + coord_flip() + theme(legend.position = \"top\") Again, we can see that the posterior distributions of the random intercept term ( ) have a large variance across schools. Quite a number of them are also away from zero. Therefore, we can conclude that the inclusion of the random intercept is necessary. In comparison, all of the posterior distributions of and go through zero, suggesting that there is probably no need to include the two random slopes in the model. To interpret the fixed-effect terms, we can calculate the exponentiated coefficient estimates. We can see that the effects of , , and are very similar to the prevoius model results."
    }
]