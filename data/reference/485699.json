[
    {
        "link": "https://community.fabric.microsoft.com/t5/Desktop/Resolving-Data-Source-Credential-Issues-in-Power-BI/td-p/3475456",
        "document": "I am new here and am encountering the following issue:\n\n\n\n\n\nThe \"Data source error\" indicates that the scheduled refresh has been disabled because at least one data source is missing its credentials. Furthermore, the option for data source credentials is grayed out.\n\n\n\n\n\nMy Power BI utilizes OData feed triggers from Azure Logic Apps and a web URL from https://learn.microsoft.com/en-us/azure/active-directory/enterprise-users/licensing-service-plan-ref.... All my connections do not require authentication as the credentials are set to \"Anonymous\" for the type.\n\n\n\n\n\nRegarding the table for the web URL: When I remove this table, the issue seems to be resolved.\n\nHow can I address this problem?"
    },
    {
        "link": "https://community.powerbi.com/t5/Desktop/How-to-solve-the-Power-BI-Service-credential-issue/td-p/2530726",
        "document": "I recently encountered the dataset refresh credential issues (on-demand and auto refresher both). the working scenario and issue are described below.\n\n1) data source: the ms forms excel data stored in the SharePoint folder\n\n2) create a Power BI report with the Power BI Desktop and connect it to excel through a \"web\" data source. The credential I used Organizational account\n\n3) Power BI Report work fine, and my report data update once there is a data change in the SharePoint excel file (In power BI desktop)\n\n4) publish the report to my workspace. and trying to refresh the dataset in my workspace. However, I try to put the dataset credential. The credential type has anonymous, basic and OAth2. I picked OAth2 with privacy level \"Organizational\", however, no matter how I try, it always fails \"The OAuth authentication method isn't supported for this data source. Contact your admin....\". I also tried other combinations and they don't work either.\n\nCan you please let me know how could I resolve the issue? Howe could I keep the published dataset up-to-date?\n\nBeyond that, how could I set the web power BI page publish to our whole organization?\n\nThank you in advance for any comments/suggestions."
    },
    {
        "link": "https://learn.microsoft.com/en-us/power-bi",
        "document": "The unified platform to meet your organization's data and analytics needs. Discover the capabilities Fabric has to offer, understand how it works, and how to use it."
    },
    {
        "link": "https://powerbigate.com/understanding-power-bi-data-source-credentials-why-they-may-be-greyed-out",
        "document": "Power BI is a powerful tool for data visualization, analysis, and reporting that allows users to seamlessly connect to a wide array of data sources. However, one of the most common challenges users face when working with Power BI is the issue of greyed-out data source credentials, which prevents them from editing or refreshing their datasets. This problem can be frustrating, especially when it disrupts workflows, making it impossible to generate updated reports or maintain data pipelines.\n\nIn this article, we’ll explore in detail why data source credentials may be greyed out in Power BI and how to resolve this issue effectively. We’ll break down the common causes and offer step-by-step troubleshooting solutions, as well as cover advanced techniques for users dealing with complex Power BI deployments.\n\nThe Importance of Data Source Credentials in Power BI\n\nBefore addressing why data source credentials might be greyed out in Power BI, it’s important to understand their role in the platform. Power BI allows users to connect to a variety of data sources, including on-premises databases, cloud-based systems, and external services. To retrieve and update data from these sources, Power BI requires valid credentials to authenticate the connection securely.\n\nData source credentials are crucial because they allow Power BI to access the necessary information and ensure that the data is retrieved in real-time or refreshed according to a set schedule. This authentication process ensures that only authorized users can access sensitive data from the source, maintaining the integrity and security of the information.\n\nCredentials in Power BI can vary depending on the type of data source and authentication method. These methods include Windows authentication, OAuth2 for cloud services, and API keys for external web services. Without properly configured credentials, Power BI cannot refresh datasets or connect to live data sources, which can disrupt reporting and analysis workflows.\n\nWhen credentials are greyed out, it indicates a potential issue with the configuration, permissions, or the data source itself, which prevents users from editing or refreshing the connection. Understanding the importance of these credentials is key to troubleshooting and resolving greyed-out credential issues efficiently.\n\nThese credentials come in different forms, depending on the type of authentication the data source requires. Power BI supports various authentication methods, such as:\n• Windows Authentication: Used for services like SQL Server that are within the same network domain.\n• Basic Authentication: Requires a username and password, often for web-based services or cloud platforms.\n• OAuth2: Common for modern cloud services like SharePoint, Azure SQL Database, and Salesforce.\n• API Key: Used for web APIs and other external sources that use token-based authentication.\n\nWithout these credentials, Power BI cannot access or refresh datasets, which can affect real-time data retrieval, scheduled refreshes, and overall reporting accuracy. When these credentials become greyed out, it’s a signal that something is wrong with how Power BI is managing or accessing the data source.\n\nWhy Credentials are Essential for Data Refreshes\n\nIn many Power BI workflows, data refreshes are set to run automatically, ensuring reports are always up-to-date. For example, a daily sales report that connects to a SQL database will refresh every morning to reflect the latest data. Without valid credentials, Power BI cannot connect to the data source, causing the refresh to fail.\n\nFor organizations relying on real-time or frequently updated data, the inability to refresh datasets can have significant operational consequences. This underscores the importance of ensuring that data source credentials are configured correctly.\n\nNow that we’ve established the importance of data source credentials, let’s explore the most common reasons why they might become greyed out in Power BI. Understanding these reasons will provide the foundation for troubleshooting the issue.\n\nOne of the most common reasons for greyed-out data source credentials is insufficient permissions. Power BI operates on a permission-based system, meaning that only users with specific roles or privileges can edit certain settings, including credentials.\n\nImagine you’re working in a Power BI workspace that’s shared across your team. You notice that the data source credentials for one of the datasets are greyed out. This could happen if the dataset belongs to another user who has set restrictive permissions. If you’re not the dataset owner or have not been granted administrative rights within the workspace, you won’t have the ability to modify those credentials.\n\nTo resolve this, you’ll need to request access or have the dataset owner modify the permissions to grant you administrative or member access.\n• Navigate to the Power BI Service: Go to the Power BI workspace where the dataset resides.\n• Review Workspace Permissions: Click on the workspace name, then go to “Manage Permissions.”\n• Check Your Role: Verify if you are an Admin, Member, or Contributor. Only Admins and Members usually have the ability to edit credentials.\n• Request Elevated Permissions: If you do not have the necessary permissions, request the workspace Admin to either grant you higher access or modify the credentials themselves.\n\nIn organizations with strict security protocols, it’s common for datasets to be locked down to certain individuals or roles. IT administrators may enforce role-based access control (RBAC) policies that limit who can edit data source credentials. In these cases, modifying the data source permissions may require coordination with your IT department or Power BI administrators.\n\nIf you’re using Power BI to connect to on-premises data sources, you’ll likely need a Power BI Gateway to facilitate the connection. A misconfigured gateway can cause issues with credential management, resulting in greyed-out options.\n\nSuppose your dataset connects to an on-premises SQL Server through a data gateway. If the gateway hasn’t been correctly configured or if you don’t have administrative access to the gateway, you may not be able to modify the credentials.\n• Check Gateway Status: In the Power BI Service, navigate to “Manage Gateways” under the settings.\n• Verify Gateway Configuration: Ensure the gateway is properly configured for the data source. Check that the data source connection settings within the gateway are accurate.\n• Administrator Rights: Ensure that you have been added as an administrator for the gateway. Without admin rights, you won’t be able to modify the data source credentials.\n• Update Gateway Software: Outdated versions of the Power BI Gateway can cause various issues. Ensure that the gateway software is up-to-date.\n\nIn cases where multiple users or data sources share the same gateway, conflicts can arise if different authentication methods are required for each data source. Ensuring that the correct authentication mode (Windows, Basic, OAuth, etc.) is configured for each data source will prevent issues with greyed-out credentials. Additionally, network firewalls or proxy settings can sometimes interfere with the gateway’s ability to connect to the Power BI service, so verifying the network configuration is also essential.\n\nAnother common reason for greyed-out credentials is an authentication mode mismatch between Power BI Desktop and Power BI Service. For example, if you set up your data source using Windows authentication in Power BI Desktop but then attempt to refresh the dataset in Power BI Service with OAuth2, this mismatch can lead to credential issues.\n\nConsider a scenario where you’re working on a report in Power BI Desktop, using Windows authentication to connect to a local SQL Server. After publishing the report to the Power BI Service, the credentials may become greyed out because the service expects a different authentication method, such as OAuth2, for cloud-based operations.\n• Open Power BI Desktop: Go to “Transform Data” and select “Data Source Settings.”\n• Check Authentication Method: Verify the authentication mode that’s being used for each data source.\n• Align Authentication: Ensure that the authentication mode in Power BI Desktop matches what’s required in Power BI Service. This often means switching from Windows authentication to OAuth2 for cloud-based services.\n\nFor organizations with hybrid infrastructures (a mix of on-premises and cloud-based systems), it’s essential to establish a consistent authentication strategy. Ideally, all data sources should use a common authentication method to prevent issues when moving reports from development to production environments. Implementing single sign-on (SSO) or using Azure Active Directory (AAD) for authentication can streamline the process.\n\nThe type of data source you’re connecting to and how it’s configured in Power BI can also affect whether the credentials are editable. Data sources can be connected via different modes, such as Import Mode (where data is copied into Power BI) or DirectQuery/Live Connection (where data is queried in real-time from the source).\n\nSuppose you’ve connected to an Excel file stored in OneDrive and have set it up in Import Mode. Because the data is imported and stored within Power BI, you may not need to refresh the credentials unless the file location or access permissions change. As a result, the credentials for this data source might be greyed out.\n• Check the Connection Mode: In Power BI Service, go to the dataset settings and verify if it’s using Import Mode or DirectQuery/Live Connection.\n• Adjust the Mode: If needed, switch between Import and DirectQuery modes in Power BI Desktop to control whether credentials are editable.\n• Live Connection Authentication: For Live Connections, ensure that the correct authentication method (e.g., OAuth2) is set up to allow real-time data access.\n\nUsing an outdated version of Power BI can lead to several issues, including greyed-out data source credentials. Microsoft regularly releases updates to Power BI that include bug fixes, new features, and security patches. Failing to update the software can cause compatibility issues, especially when working with newer data sources or authentication methods.\n\nHow to Ensure Your Power BI is Up to Date:\n• Check for Updates: In Power BI Desktop, go to “File” > “Options and Settings” > “Options” and check for updates.\n• Install the Latest Version: Download and install the latest version of Power BI Desktop from the official Microsoft website.\n• Update the Gateway: If using an on-premises gateway, ensure that it is also updated to the latest version to avoid compatibility issues.\n\nIn addition to the common troubleshooting steps, there are several advanced techniques users can implement when facing persistent issues with greyed-out data source credentials. These techniques are particularly useful for larger organizations with complex data infrastructures.\n\nSometimes, cached credentials stored in Power BI Desktop or Service can cause issues when they become outdated or corrupted. Clearing these credentials and re-entering them can often resolve the problem.\n• Open Power BI Desktop: Go to “File” > “Options and Settings” > “Data Source Settings.”\n• Clear Cached Credentials: Select the data source and click “Clear Permissions.” This will remove any stored credentials for that data source.\n• Re-enter Credentials: When prompted, enter the correct credentials for the data source.\n\nIf clearing cached credentials doesn’t resolve the issue, the next step is to delete and recreate the data source connection altogether. This can help resolve persistent credential issues by starting with a clean slate.\n• Delete the Existing Connection: In Power BI Service, go to the dataset settings and delete the existing data source connection.\n• Recreate the Connection: Return to Power BI Desktop, and recreate the data source connection with the correct credentials and authentication settings.\n• Publish the Dataset: Once the new connection is set up, publish the dataset to Power BI Service and verify if the credentials are still greyed out.\n\nFor organizations using cloud-based services, switching to Azure Active Directory (AAD) for authentication can simplify credential management. AAD provides a centralized platform for managing users, groups, and access to cloud resources, including Power BI.\n• Single Sign-On (SSO): AAD allows users to sign in once and access all authorized resources without re-entering credentials.\n• Conditional Access: IT administrators can enforce conditional access policies, such as requiring multi-factor authentication (MFA) for sensitive datasets.\n• Improved Security: AAD supports advanced security features, such as risk-based authentication and device compliance checks.\n\nEven after following all of the above steps, some users may continue to face challenges with greyed-out data source credentials. In these cases, there are a few additional things to consider:\n\nIn large organizations, IT departments often enforce strict policies on how data is accessed and shared. These policies may prevent users from editing credentials, even if they have administrative access within Power BI. It’s important to check with your IT department to ensure that there are no organizational policies in place that are restricting your access.\n\nSometimes refreshing the dataset locally in Power BI Desktop before publishing it to the service can resolve issues with credentials. This approach forces Power BI to revalidate the credentials during the refresh process.\n• Open Power BI Desktop: Open the report in Power BI Desktop and manually refresh the dataset by clicking “Refresh.”\n• Check Credentials: If prompted, re-enter your credentials for the data source.\n• Publish the Report: Once the dataset has refreshed successfully, publish it to the Power BI Service.\n\nIf you’ve exhausted all options and are still unable to resolve the issue, it may be time to reach out to Microsoft support. Power BI has a dedicated support team that can assist with more complex issues, including credential problems. Additionally, community forums like the Power BI Community and Stack Overflow are great resources for finding solutions to common issues.\n\nEncountering greyed-out data source credentials in Power BI can be frustrating, but the issue can often be resolved with the right troubleshooting approach. The key to solving this problem is understanding the underlying causes, which typically fall into a few common categories.\n\nOne frequent issue is permission restrictions. If you don’t have the necessary admin or member access to a dataset or workspace, you may be unable to edit the credentials. Another common cause is gateway configuration problems, especially when working with on-premises data sources. If the gateway isn’t properly configured or you lack administrative rights, the credentials may appear greyed out. Additionally, authentication mode mismatches can prevent you from editing credentials, particularly if the mode used in Power BI Desktop differs from what’s required in Power BI Service.\n\nTo address these issues, start by checking your permissions and ensuring that the correct authentication method is being used. If you’re using a gateway, confirm that it’s configured correctly and that you have the right level of access. In more complex scenarios, advanced troubleshooting may be necessary, such as clearing cached credentials or switching to Azure Active Directory (AAD) for authentication, which can simplify credential management and improve security.\n\nBy following these steps, users can typically resolve greyed-out credentials and restore access to their data sources, allowing for smooth and uninterrupted reporting in Power BI.\n\nPower BI is a powerful and versatile tool for data analytics, and ensuring that data source credentials are properly configured is key to maintaining smooth workflows and up-to-date reports. Have you faced challenges with greyed-out data source credentials? What solutions worked for you? Share your thoughts and experiences in the comments below!"
    },
    {
        "link": "https://community.powerbi.com/t5/Power-Query/Data-Source-Credentials-Issue/td-p/93332",
        "document": "Hi @DarenD2,\n\n\n\nAre you able to refresh your dataset after republish reports?\n\n\n\nThanks,\n\nLydia Zhang\n\nCommunity Support Team _ Lydia Zhang\n\nIf this post helps, then please consider Accept it as the solution to help the other members find it more quickly."
    },
    {
        "link": "https://learn.microsoft.com/en-us/power-bi/connect-data/desktop-data-sources",
        "document": "With Power BI Desktop, you can connect to data from many different sources. For a full list of available data sources, see Power BI data sources.\n\nTo see available data sources, in the Home group of the Power BI Desktop ribbon, select the Get data button label or down arrow to open the Common data sources list. If the data source you want isn't listed under Common data sources, select More to open the Get Data dialog box.\n\nOr, open the Get Data dialog box directly by selecting the Get data icon itself.\n\nThis article provides an overview of the available data sources in Power BI Desktop and explains how to connect to them. It also describes how to export or use data sources as PBIDS files to make it easier to build new reports from the same data.\n\nThe Get Data dialog box organizes data types in the following categories:\n\nThe All category includes all data connection types from all categories.\n\nThe File category provides the following data connections:\n\nThe Database category provides the following data connections:\n\nThe Microsoft Fabric category provides the following data connections:\n\nThe Power Platform category provides the following data connections:\n\nThe Azure category provides the following data connections:\n\nThe Online Services category provides the following data connections:\n\nThe Other category provides the following data connections:\n\nYou can find template apps for your organization by selecting the Template Apps link near the bottom of the Get data window.\n\nAvailable Template Apps may vary based on your organization.\n• None To connect to a data source, select the data source from the Get data window and select Connect. The following screenshot shows Web selected from the Other data connection category.\n• None A connection window appears. Enter the URL or resource connection information, and then select OK. The following screenshot shows a URL entered in the From Web connection dialog box.\n• None Depending on the data connection, you might be prompted to provide credentials or other information. After you provide all required information, Power BI Desktop connects to the data source and presents the available data sources in the Navigator dialog box.\n• None Select the tables and other data that you want to load. To load the data, select the Load button at the bottom of the Navigator pane. To transform or edit the query in Power Query Editor before loading the data, select the Transform Data button.\n\nConnecting to data sources in Power BI Desktop is that easy. Try connecting to data from our growing list of data sources, and check back often. We continue to add to this list all the time.\n\nUse PBIDS files to get data\n\nPBIDS files are Power BI Desktop files that have a specific structure and a .pbids extension to identify them as Power BI data source files.\n\nYou can create a PBIDS file to streamline the Get Data experience for new or beginner report creators in your organization. If you create the PBIDS file from existing reports, it's easier for beginning report authors to build new reports from the same data.\n\nWhen an author opens a PBIDS file, Power BI Desktop prompts the user for credentials to authenticate and connect to the data source that the file specifies. The Navigator dialog box appears, and the user must select the tables from that data source to load into the model. Users might also need to select the database and connection mode if none was specified in the PBIDS file.\n\nFrom that point forward, the user can begin building visualizations or select Recent Sources to load a new set of tables into the model.\n\nCurrently, PBIDS files only support a single data source in one file. Specifying more than one data source results in an error.\n\nIf you have an existing Power BI Desktop PBIX file already connected to the data you’re interested in, you can export the connection files from within Power BI Desktop. This method is recommended, since the PBIDS file can be autogenerated from Desktop. You can also still edit or manually create the file in a text editor.\n• None To create the PBIDS file, select File > Options and settings > Data source settings.\n• None In the dialog that appears, select the data source you want to export as a PBIDS file, and then select Export PBIDS.\n• None In the Save As dialog box, give the file a name, and select Save. Power BI Desktop generates the PBIDS file, which you can rename and save in your directory, and share with others.\n\nYou can also open the file in a text editor, and modify the file further, including specifying the mode of connection in the file itself. The following image shows a PBIDS file open in a text editor.\n\nIf you prefer to manually create your PBIDS files in a text editor, you must specify the required inputs for a single connection and save the file with the .pbids extension. Optionally, you can also specify the connection as either or . If is missing or in the file, the user who opens the file in Power BI Desktop is prompted to select DirectQuery or Import.\n\nThis section provides some examples from commonly used data sources. The PBIDS file type only supports data connections that are also supported in Power BI Desktop, with the following exceptions: Wiki URLs, Live Connect, and Blank Query.\n\nThe PBIDS file doesn't include authentication information and table and schema information.\n\nThe following code snippets show several common examples for PBIDS files, but they aren't complete or comprehensive. For other data sources, you can refer to the git Data Source Reference (DSR) format for protocol and address information.\n\nIf you're editing or manually creating the connection files, these examples are for convenience only, aren't meant to be comprehensive, and don't include all supported connectors in DSR format.\n\nThe URL must point to the SharePoint site itself, not to a list within the site. Users get a navigator that allows them to select one or more lists from that site, each of which becomes a table in the model.\n\nYou can do all sorts of things with Power BI Desktop. For more information on its capabilities, check out the following resources:"
    },
    {
        "link": "https://learn.microsoft.com/en-us/power-bi/guidance/power-bi-optimization",
        "document": "This article provides guidance that enables developers and administrators to produce and maintain optimized Power BI solutions. You can optimize your solution at different architectural layers. Layers include:\n• The environment, including capacities, data gateways, and the network\n\nThe data model supports the entire visualization experience. Data models are either hosted in the Power BI ecosystem or externally (by using DirectQuery or Live Connection), and in Power BI they are referred to as semantic models. It's important to understand your options, and to choose the appropriate semantic model type for your solution. There are three semantic model table storage modes: Import, DirectQuery, and Composite. For more information, see Semantic models in the Power BI service, and Semantic model modes in the Power BI service.\n\nThe semantic model is the foundation of all reporting in Power BI. Consumers of the semantic model can create Power BI reports in Power BI Desktop by connecting to a published semantic model or connecting to data and creating a local semantic model. The semantic model can also be used to create Power BI reports in the browser, create Power BI explorations, create paginated reports, create DAX queries, and create reports in Excel with Analyze in Excel, connecting to Power BI in Excel, or exporting data from a report visual, as well as many other reporting tools. A semantic model author can help semantic model consumers understand and utilize the semantic model with how they build the model.\n• Names: Tables, columns, and measures in the semantic model with descriptive names. For example, 'Store Sales' as a table name is more intuitive than 'Table1'.\n• Descriptions: Tables, columns, and measures in the model can have descriptions added to them to provide more detail than can fit in the name. Explain not only what they include but how they should be used.\n• Hide: You can hide tables, columns, and measures in the model to show only what you expect them to use in a report. For example, relationship columns may be an ID that is not necessary for reporting and can be hidden as it's not expected to be used in a report, or data columns that have a measure to aggregate the column could be hidden to encourage use of the measure instead. Hidden objects can always be unhidden later by the model consumer, so they will still be available, but hiding can provide focus.\n• Hierarchies: You can create hierarchies to convey the hierarchy across multiple columns. For example, a Calendar hierarchy may contain Year, Month, Day columns, and a Product hierarchy may contain Category, Sub-Category, Product columns. Right-click a column to create a hierarchy.\n• Measures: You can use measures to aggregate data columns in the semantic model to provide consistency across reports. Measures can range from the SUM of a column, to a health index combining multiple aggregations in a specific way or comparing aggregations across time periods, such as daily average this month compared to the daily average of the same month last year. Measures can also be surfaced in Power BI search and other features, such as Metrics and Scorecards.\n• Formats: You can specify how a column or measure is displayed in a visual, by default. Values in visuals can be customized further in the visual. Format options include if it has a thousands comma, how many decimal places, how a date is shown, etc. You can also apply custom or dynamic formats.\n• Data category: You can specify a column data category, such as if it's a Country or Web URL.\n\nThese are common features of Power BI semantic model that can be leveraged to help your report authors and model consumers. There are many others, such as calculation groups, field parameters, what if parameters, and grouping and binning columns, which should be evaluated to see if they apply your specific reporting needs.\n\nPower BI visualizations can be dashboards, Power BI reports, or Power BI paginated reports. Each has different architectures, and so each has their own guidance.\n\nIt's important to understand that Power BI maintains a cache for your dashboard tiles—except live report tiles, and streaming tiles. If your semantic model enforces dynamic row-level security (RLS), be sure to understand performance implications as tiles will cache on a per-user basis.\n\nWhen you pin live report tiles to a dashboard, they're not served from the query cache. Instead, they behave like reports, and make queries to v-cores on the fly.\n\nAs the name suggests, retrieving the data from the cache provides better and more consistent performance than relying on the data source. One way to take advantage of this functionality is to have dashboards be the first landing page for your users. Pin often-used and highly requested visuals to the dashboards. In this way, dashboards become a valuable \"first line of defense\", which delivers consistent performance with less load on the capacity. Users can still click through to a report to analyze details.\n\nFor DirectQuery and live connection semantic models, the cache is updated on a periodic basis by querying the data source. By default, it happens every hour, though you can configure a different frequency in the semantic model settings. Each cache update will send queries to the underlying data source to update the cache. The number of queries that generate depends on the number of visuals pinned to dashboards that rely on the data source. Notice that if row-level security is enabled, queries are generated for each different security context. For example, consider there are two different roles that categorize your users, and they have two different views of the data. During query cache refresh, Power BI generates two sets of queries.\n\nThere are several recommendations for optimizing Power BI report designs.\n\nThe more data that a visual needs to display, the slower that visual is to load. While this principle seems obvious, it's easy to forget. For example: suppose you have a large semantic model. Based on that semantic model, you build a report with a table. End users use slicers on the page to get to the rows they want—typically, they're only interested in a few dozen rows.\n\nA common mistake is to leave the default view of the table unfiltered—that is, all 100M+ rows. The data for these rows loads into memory and is uncompressed at every refresh. This processing creates huge demands for memory. The solution: use the \"Top N\" filter to reduce the max number of items that the table displays. You can set the max item to larger than what users would need, for example, 10,000. The result is the end-user experience doesn't change, but memory use drops greatly. And most importantly, performance improves.\n\nA similar design approach to the above is suggested for every visual in your report. Ask yourself, is all the data in this visual needed? Are there ways to filter the amount of data shown in the visual with minimal impact to the end-user experience? Remember, tables in particular can be expensive.\n\nThe above principle applies equally to the number of visuals added to a report page. It's highly recommended you limit the number of visuals on a particular report page to only what is necessary. Drillthrough pages and report page tooltips are great ways to provide additional details without jamming more visuals onto the page.\n\nBe sure to put each custom visual through its paces to ensure high performance. Poorly optimized Power BI visuals can negatively affect the performance of the entire report.\n\nPower BI paginated report designs can be optimized by applying best practice design to the report's data retrieval. For more information, see Data retrieval guidance for paginated reports.\n\nAlso, ensure your capacity has sufficient memory allocated to the paginated reports workload.\n\nYou can optimize the Power BI environment by configuring capacity settings, sizing data gateways, and reducing network latency.\n\nWhen using capacities—available with Power BI Premium (P SKUs), Premium Per User (PPU) licenses, or Power BI Embedded (A SKUs, A4-A6)—you can manage capacity settings. For more information, see Microsoft Fabric capacity licenses and Managing Premium capacities.\n\nA gateway is required whenever Power BI must access data that isn't accessible directly over the Internet. You can install the on-premises data gateway on a server on-premises, or VM-hosted Infrastructure-as-a-Service (IaaS).\n\nTo understand gateway workloads and sizing recommendations, see On-premises data gateway sizing.\n\nNetwork latency can impact report performance by increasing the time required for requests to reach the Power BI service, and for responses to be delivered. Tenants in Power BI are assigned to a specific region.\n\nWhen users from a tenant access the Power BI service, their requests always route to this region. As requests reach the Power BI service, the service may then send additional requests—for example, to the underlying data source, or a data gateway—which are also subject to network latency.\n\nTools such as Azure Speed Test provide an indication of network latency between the client and the Azure region. In general, to minimize the impact of network latency, strive to keep data sources, gateways, and your Power BI capacity as close as possible. Preferably, they reside within the same region. If network latency is an issue, try locating gateways and data sources closer to your Power BI capacity by placing them inside cloud-hosted virtual machines.\n\nYou can monitor performance to identify bottlenecks. Slow queries—or report visuals—should be a focal point of continued optimization. Monitoring can be done at design time in Power BI Desktop, or on production workloads in Power BI Premium capacities. For more information, see Monitoring report performance in Power BI.\n\nFor more information about this article, check out the following resources:\n• Questions? Try asking the Fabric Community"
    },
    {
        "link": "https://pragmaticworks.com/blog/best-practice-for-your-power-bi-data-sources",
        "document": "We all know there are lots of data sources; flat files like Excel files, CSVs that you import from another data source, connecting directly to a SQL Server or to SharePoint, or using a data warehouse architecture, as well as many others. The question is, are you using too many data sources with Power BI?\n\nOne problem we see as consultants is some companies use far too many data sources. Let’s say a company is connecting to a couple different tables off of SQL Server, they’ve got on view on SQL Server and they’re connecting directly to a SharePoint list to pull in some data and exporting files from Excel files; you get the picture, right?\n\nThis can be problematic when trying to put all these different data sources into one report. If you have multiple people creating reports based on a hodgepodge of different data sources, there may be some very different types of filtering that gets done, along with different types of DAX or M code that gets used. This can cause serious ‘one version of the truth’ problems.\n\nI suggest a best practice is to have a long-term goal – this could be months or even a year or more – for getting your data sources in one place. This long term goal can be things such as connecting to a data warehouse or Tabular model or getting a multi-dimensional cube spun up; something that contains all your data and data sources in one spot.\n\nThat one spot is where you:\n• Apply all your changes to these sources like any kind of DAX calculation that anyone would need is already there.\n• Any kind of relationship that needs to be built, active or inactive, between any table is already set up.\n• Ways to navigate through relationships that are inactive using certain calculations, DAX or others, are ready.\n\nDoing this will give you one version of the truth throughout your company. Also, you will be connecting to one single data source when connecting to these via live connection. It will help prevent users from having access to data that others don’t (assuming they are supposed to have access).\n\nIn short, set your company up with a long-term goal for a type of data warehouse platform. Any person that creates reports will be able to connect to this and know they are getting one version of the truth, so they can create the reports they want with confidence.\n\nIf you want to learn more about data warehousing or how to set up a long-term goal for your data sources or anything about Power BI or Azure, we can help. Our experts can give you the guidance you need, help you create a plan and help you with implementation. Contact us or click the link below and start a conversation today."
    },
    {
        "link": "https://reddit.com/r/PowerBI/comments/pmhhm1/ideas_for_best_practices_on_where_to_keep_data",
        "document": "I have 4 Excel files containing 7 years of operational data that are appended with new data every day. The files range in size from approximately 60k-300k rows. I am planning on creating my first Power BI report using the data contained in the Excel workbooks. Any recommendations for best practices on data storage for Power BI sourcing? I had the idea of moving the data to an Access database hosted on an internal SharePoint site and pointing Power BI to the Access Db for refreshes instead of referencing to Excel.\n\nBackground: I’m manage Operational Excellence for a mid-sized manufacturing facility that’s part of a larger corporate network of global facilities. There are no true data analysts within our organization. Additionally, since I’m not a part of the IT team I’m not allowed direct access to the source data so I’ve had to download batch records every day through a SEQUEL web server interface and grunt through Excel pivot tables to analyze and present data to my GM and internal teams. I’ve done this type of work for nearly 22 years, although I’ve only been with this employer for close to 3 years. I’m wanting to switch careers to something that lines up closer to a true data analytics role instead of the continuous improvement and quality management type of roles I’ve mostly done. I have a degree in Mechanical Engineering, am currently enrolled in the Google Data Analytics certificate program, and just recently completed a few courses covering Power BI."
    },
    {
        "link": "https://community.databricks.com/t5/data-engineering/what-are-the-best-practices-for-optimizing-power-bi-reports-and/td-p/84297",
        "document": "Hi @jamson ,\n\n\n\nHere's a good article that answers your question. I think author did a pretty good job - many of his advice I apply on everyday job.\n\n\n\nHow I Tuned Databricks Query Performance from Power BI Desktop: A Personal Journey. | by Brahmareddy...\n\nThe first thing I learned was the importance of optimizing queries directly in Databricks. Here’s what I did:\n\nFilter Early: Initially, I was pulling entire datasets into Power BI and then applying filters. This was a big mistake. Instead, I started applying filters directly in my Databricks SQL queries. For example, instead of pulling all sales data and then filtering by region in Power BI, I added a WHERE clause in my Databricks query to filter by region before the data even left the server. This simple change drastically reduced the amount of data being transferred and sped up my reports.\n\nAggregate Data: I also realized I didn’t always need granular data in Power BI. For instance, instead of pulling every transaction, I aggregated data at the monthly level directly in Databricks. This not only reduced the data size but also made the subsequent analysis much quicker.\n\nPower BI’s DirectQuery mode allows you to work with large datasets without importing them into Power BI, which sounds great on paper. However, I quickly learned that it comes with its own set of challenges.\n\nMinimize Data in DirectQuery: At first, I was pulling in large tables, thinking I might need all the data. But this led to painfully slow reports. I started being more selective about the data I queried. For example, instead of querying a full year’s worth of data, I limited it to the last three months, which was sufficient for most of my analyses.\n\nOptimize Relationships: Another lesson was to simplify relationships in my Power BI data model. Initially, I had multiple tables with complex joins, which significantly slowed down performance. I restructured my model to reduce the number of relationships and avoid many-to-many relationships whenever possible. This made a noticeable difference in query times.\n\nAnother area that needed attention was the complexity of my Power BI reports.\n\nReduce the Number of Visuals: I realized that each visual in Power BI sends its own query to Databricks. My reports were overloaded with visuals, many of which were redundant or could be combined. By reducing the number of visuals and focusing only on the most essential ones, I reduced the query load significantly.\n\nSimplify Measures: I also found that complex measures were dragging down performance. Initially, I had measures that calculated results on the fly with intricate DAX formulas. By simplifying these measures and doing more of the heavy lifting in Databricks, I was able to speed up my reports.\n\nFor example, instead of calculating a rolling average in Power BI, I calculated it directly in Databricks and passed the results to Power BI.\n\nTo fine-tune everything, I used Power BI Desktop’s Performance Analyzer. This tool became my best friend.\n\nUsing Performance Analyzer: I ran the Performance Analyzer on my reports to see exactly where the slowdowns were happening. It allowed me to pinpoint which visuals or queries were the bottlenecks. Armed with this information, I could go back and optimize those specific areas, leading to faster overall performance.\n\nImproving query performance wasn’t just about tweaking Power BI; I also needed to make sure my Databricks environment was up to the task.\n\nRight-Size the Cluster: I realized that my Databricks cluster wasn’t properly configured for the size of the datasets I was working with. After upgrading to a larger cluster with more resources, query processing sped up considerably.\n\nUse Delta Tables: I switched to using Delta Tables in Databricks, which are specifically optimized for performance. Delta Tables handle large datasets efficiently, and I noticed an immediate improvement in query times.\n\nFinally, I learned the importance of regularly maintaining my data environment.\n\nOptimize Tables: I made it a routine to run the OPTIMIZE command on my Delta Tables. This ensures that data is stored in the most efficient way possible, which helps keep queries running smoothly.\n\nVacuum Old Data: I also used the VACUUM command to clean up old files that were no longer needed. This reduced storage clutter and further improved performance.\n\nAfter implementing these strategies, the difference was night and day. My Power BI reports now load much faster, and the overall experience is far smoother. While it took some time to figure out the right combination of optimizations, the effort was well worth it.\n\nIf you’re struggling with slow query performance when using Databricks with Power BI, I highly recommend trying out these tips. They made a huge difference for me, and I’m confident they can help you too. Happy reporting!\n\nAppreciate your time spent here!\n\nIf you’ve found value in my work and want to show your support…"
    }
]