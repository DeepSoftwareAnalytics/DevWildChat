[
    {
        "link": "https://bisqwit.iki.fi/story/howto/openmp",
        "document": ""
    },
    {
        "link": "https://openmp.org/wp-content/uploads/omp-hands-on-SC08.pdf",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/33117014/synchronization-in-openmp",
        "document": "If the vector modifications are very simple single-value assignment operations and are not actually function calls, what you need are probably atomic reads and writes. With atomic operations, a read from an array element that is simultaneously being written to will either return the new value or the previous value; it will never return some kind of a bit mash of the old and the new value. OpenMP provides the construct for that purpose. On certain architectures, including x86, atomics are far more lightweight than critical sections.\n\nWith more complex modifications you have to use critical sections. Those could be either named or anonymous. The latter are created using\n\nAnonymous critical sections all map to the same synchronisation object, no matter what the position of the construct in the source code, therefore it is possible for unrelated code sections to get synchronised with all possible ill effects, like performance degradation or even unexpected deadlocks. That's why it is advisable to always use named critical sections. For example, the following two code segments will not get synchronised:\n\nNote that critical sections bind to all threads of the program, without regard to the team to which the threads belong. It means that even code that executes in different parallel regions (a situation that mainly arises when nested parallelism is used) gets synchronised."
    },
    {
        "link": "https://intel.com/content/www/us/en/docs/advisor/user-guide/2024-0/advanced-openmp-atomic-operations.html",
        "document": ""
    },
    {
        "link": "https://tildesites.bowdoin.edu/~ltoma/teaching/cs3225-GIS/fall16/Lectures/openmp.html",
        "document": "Parallel code with OpenMP marks, through a special directive, sections to be executed in parallel. The part of the code that’s marked to run in parallel will cause threads to form. The main tread is the master thread. The slave threads all run in parallel and run the same code. Each thread executes the parallelized section of the code independently. When a thread finishes, it joins the master. When all threads finished, the master continues with code following the parallel section.\n\nEach thread has an ID attached to it that can be obtained using a runtime library function (called omp_get_thread_num()). The ID of the master thread is 0.\n\nOpenMP supports C, C++ and Fortran.\n\nThe OpenMP functions are included in a header file called The OpenMP parts in the code are specified using #pragmas OpenMP has directives that allow the programmer to:\n• specify how to parallelize loops\n• specify the scope of the variables in the parallel section (private and shared)\n• specify if the threads are to be synchronized\n• specify how the works is divided between threads (scheduling) OpenMP hides the low-level details and allows the programmer to describe the parallel code with high-level constructs, which is as simple as it can get. The public linux machines dover and foxcroft have gcc/g++ installed with OpenMP support. All you need to do is use the -fopenmp flag on the command line: It’s also pretty easy to get OpenMP to work on a Mac. A quick search with google reveals that the native apple compiler clang is installed without openmp support. When you installed gcc it probably got installed without openmp support. To test, go to the terminal and try to compile something: If you get an error message saying that “omp.h” is unknown, that mans your compiler does not have openmp support. Here’s what I did: 1. I installed Homebrew, the missing package manager for MacOS, http://brew.sh/index.html 2. Then I asked brew to install gcc: 3. Then type ‘gcc’ and press tab; it will complete with all the versions of gcc installed: 4. The obvious guess here is that gcc-6 is the latest version, so I use it to compile: Works! The basic directive is: This is used to fork additional threads to carry out the work enclosed in the block following the #pragma construct. The block is executed by all threads in parallel. The original thread will be denoted as master thread with thread-id 0. Example (C program): Display \"Hello, world.\" using multiple threads. Use flag -fopenmp to compile using gcc: Output on a computer with two cores, and thus two threads: On dover, I got 24 hellos, for 24 threads. On my desktop I get (only) 8. How many do you get? Note that the threads are all writing to the standard output, and there is a race to share it. The way the threads are interleaved is completely arbitrary, and you can get garbled output: In a parallel section variables can be private (each thread owns a copy of the variable) or shared among all threads. Shared variables must be used with care because they cause race conditions.\n• shared: the data within a parallel region is shared, which means visible and accessible by all threads simultaneously. By default, all variables in the work sharing region are shared except the loop iteration counter.\n• private: the data within a parallel region is private to each thread, which means each thread will have a local copy and use it as a temporary variable. A private variable is not initialized and the value is not maintained for use outside the parallel region. By default, the loop iteration counters in the OpenMP loop constructs are private. The type of variables is specified following the #pragma omp int main (int argc, char *argv[]) { int th_id, nthreads; #pragma omp parallel private(th_id) // th_id is declared above. It is is specified as private; so each thread will have its own copy of th_id { th_id = omp_get_thread_num(); printf(\"Hello World from thread %d\n\n\", th_id); } Sharing variables is sometimes what you want, other times its not, and can lead to race conditions. Put differently, some variables need to be shared, some need to be private, and you the programmer have to specify what you want. OpenMP lets you specify how to synchronize the threads. Here’s what’s available:\n• critical: the enclosed code block will be executed by only one thread at a time, and not simultaneously executed by multiple threads. It is often used to protect shared data from race conditions.\n• atomic: the memory update (write, or read-modify-write) in the next instruction will be performed atomically. It does not make the entire statement atomic; only the memory update is atomic. A compiler might use special hardware instructions for better performance than when using critical.\n• ordered: the structured block is executed in the order in which iterations would be executed in a sequential loop\n• barrier: each thread waits until all of the other threads of a team have reached this point. A work-sharing construct has an implicit barrier synchronization at the end.\n• nowait: specifies that threads completing assigned work can proceed without waiting for all threads in the team to finish. In the absence of this clause, threads encounter a barrier synchronization at the end of the work sharing construct. More on barriers: If we wanted all threads to be at a specific point in their execution before proceeding, we would use a barrier. A barrier basically tells each thread, \"wait here until all other threads have reached this point...\". int main (int argc, char *argv[]) { int th_id, nthreads; #pragma omp parallel private(th_id) { th_id = omp_get_thread_num(); printf(\"Hello World from thread %d\n\n\", th_id); #pragma omp barrier <----------- master waits until all threads finish before printing if ( th_id == 0 ) { nthreads = omp_get_num_threads(); printf(\"There are %d threads\n\n\",nthreads); } } }//main Note above the function omp_get_num_threads(). Can you guess what it’s doing? Some other runtime functions are: Parallelizing loops with OpenMP is straightforward. One simply denotes the loop to be parallelized and a few parameters, and OpenMP takes care of the rest. Can't be easier! #pragma omp for //specify a for loop to be parallelized; no curly braces The “#pragma omp for” distributes the loop among the threads. It must be used inside a parallel block: #pragma omp parallel { … #pragma omp for //for loop to parallelize … }//end of parallel block Example: //compute the sum of two arrays in parallel #include < stdio.h > #include < omp.h > #define N 1000000 int main(void) { float a[N], b[N], c[N]; int i; /* Initialize arrays a and b */ for (i = 0; i < N; i++) { a[i] = i * 2.0; b[i] = i * 3.0; } /* Compute values of array c = a+b in parallel. */ #pragma omp parallel shared(a, b, c) private(i) { #pragma omp for for (i = 0; i < N; i++) { c[i] = a[i] + b[i]; printf (\"%f\n\n\", c[10]); } } } Another example (here): adding all elements in an array. //example4.c: add all elements in an array in parallel #include < stdio.h > int main() { const int N=100; int a[N]; //initialize for (int i=0; i < N; i++) a[i] = i; //compute sum int local_sum, sum; #pragma omp parallel private(local_sum) shared(sum) { local_sum =0; //the array is distributde statically between threads #pragma omp for schedule(static,1) for (int i=0; i< N; i++) { local_sum += a[i]; } //each thread calculated its local_sum. ALl threads have to add to //the global sum. It is critical that this operation is atomic. #pragma omp critical sum += local_sum; } printf(\"sum=%d should be %d\n\n\", sum, N*(N-1)/2); } There exists also a “parallel for” directive which combines a parallel and a for (no need to nest a for inside a parallel): Exactly how the iterations are assigned to ecah thread, that is specified by the schedule (see below). Note:Since variable i is declared inside the parallel for, each thread will have its own private version of i. OpenMP lets you control how the threads are scheduled. The type of schedule available are:\n• static: Each thread is assigned a chunk of iterations in fixed fashion (round robin). The iterations are divided among threads equally. Specifying an integer for the parameter chunk will allocate chunk number of contiguous iterations to a particular thread. Note: is this the default? check.\n• dynamic: Each thread is initialized with a chunk of threads, then as each thread completes its iterations, it gets assigned the next set of iterations. The parameter chunk defines the number of contiguous iterations that are allocated to a thread at a time.\n• guided: Iterations are divided into pieces that successively decrease exponentially, with chunk being the smallest size. This is specified by appending after the pragma for directive: ...which you probably won't need.\n• can request that iterations of a loop are executed in order\n• specify a block to be executed only by the master thread\n• specify a block to be executed only by the first thread that reaches it\n• define a section to be “critical”: will be executed by each thread, but can be executed only by a single thread at a time. This forces threads to take turns, not interrupt each other.\n• define a section to be “atomic”: this forces threads to write to a shared memory location in a serial manner to avoid race conditions #include < stdio.h > #include < omp.h > int main(void) { int count = 0; #pragma omp parallel shared(count) { #pragma omp atomic count++; // count is updated by only a single thread at a time } printf_s(\"Number of threads: %d\n\n\", count); } Critical sections and atomic sections serialize the execution and eliminate the concurrent execution of threads. If used unwisely, OpenMP code can be worse than serial code because of all the thread overhead. OpenMP is not magic. A loop must be obviously parallelizable in order for OpenMP to unroll it and facilitate the assignment of iterations among threads. If there are any data dependencies from one iteration to the next, then OpenMP can't parallelize it. The for loop cannot exit early, for example: // BAD - can;t parallelize with OpenMP for (int i=0;i < 100; i++) { if (i > 50) break; <----- breaking when i greater than 50 } Values of the loop control expressions must be the same for all iterations of the loop. For example: // BAD - can;t parallelize with OpenMP for (int i=0;i < 100; i++) { if (i == 50) i = 0; }"
    },
    {
        "link": "https://geeksforgeeks.org/rand-and-srand-in-ccpp",
        "document": "The std::rand() is a built-in function that is used to generate a series of random numbers. It will generate random numbers in the range [0, RAND_MAX) where RAND_MAX is a constant whose default value may vary but is granted to be at least 32767. It is defined inside <cstdlib> and <stdlib.h> header files.\n\nIn this article, we will learn about rand() function in C++ and how to use it to generate random numbers.\n• None This function does not take any parameters.\n• None Returns a pseudo-random number in the range of [0, RAND_MAX).\n\nThe random number generated by rand() function is generated using an algorithm that gives a series of non-related numbers by taking 1 as the starting point or seed. Due to this, the random number series will aways be same for different function calls. To resolve this problem, we use srand() function.\n\nThe srand() function changes the “seed” or the starting point of the algorithm. A seed is an integer used to initialize the random number generator.\n• seed: Integer used to initialize the random number generator. It is common to use the current time ( from ) to ensure a different sequence of random numbers.\n• None This function does not return anything return.\n\nApplications of rand() in C++\n\nrand() function is mainly used to generate random numbers in our program which can be used in the following applications:"
    },
    {
        "link": "https://cplusplus.com/reference/cstdlib/rand",
        "document": ""
    },
    {
        "link": "https://en.cppreference.com/w/cpp/numeric/random/rand",
        "document": "Returns a pseudo-random integral value from the range ​0​ RAND_MAX .\n\nstd::srand() seeds the pseudo-random number generator used by . If is used before any calls to std::srand(), behaves as if it was seeded with std::srand(1).\n\nEach time is seeded with std::srand(), it must produce the same sequence of values on successive calls.\n\nOther functions in the standard library may call . It is implementation-defined which functions do so.\n\nIt is implementation-defined whether is thread-safe.\n\nPseudo-random integral value between ​0​ and RAND_MAX.\n\nThere are no guarantees as to the quality of the random sequence produced. In the past, some implementations of have had serious shortcomings in the randomness, distribution and period of the sequence produced (in one well-known example, the low-order bit simply alternated between 1 and ​0​ between calls).\n\nis not recommended for serious random-number generation needs. It is recommended to use C++11's random number generation facilities to replace .(since C++11)"
    },
    {
        "link": "https://stackoverflow.com/questions/24005459/implementation-of-the-random-number-generator-in-c-c",
        "document": "I am a bit confused by the implementation of the random number generator in C, which is also apparently different from that in C++\n\nIf I understand correctly, a call to 'srand(seed)' somehow initializes a hidden variable (the seed) that is accessible by 'rand()', which in turn points the function to a pre-generated sequence, like for example this one. Each successive call to 'rand()' advances the sequence (and apparently there are other ways to advance in C++), which also suggests the use of an internal hidden pointer or counter to keep track of the advance.\n\nI have found many discussions on how the algorithms for pseudo-random number generation work and the documentation of the functions rand() and srand(), but haven't been able to find information about these hidden parameters and their behavior, except for the fact that according to this source, they are not thread-safe.\n• None Could anybody here please shed some light as to how are these parameters defined and what should be their defined behavior according to the standards, or if their behavior is implementation-defined?\n• None Are they expected to be local to the function/method that calls rand() and srand()? If so, is there a way to communicate them to another function/method?\n\nIf your answer is specific to either C or C++, please be so kind to point it out. Any information will be much appreciated. Please bear in mind that this question is not about the predictability of data generated by rand() and srand(), but about the requirements, status and functioning of their internal variables as well as their accessibility and scope."
    },
    {
        "link": "https://isocpp.org/files/papers/n3551.pdf",
        "document": ""
    }
]