[
    {
        "link": "https://stackoverflow.com/questions/20909448/stdshared-ptr-best-practice-for-passing-shared-pointer-as-parameter",
        "document": "I've been away from serious C++ for about ten years. I'm coming back in to the fold and am currently working on a project to fully familiarize myself with C++11. I'm having a bit of an existential crisis about how to best pass std::shared_ptr's around.\n\nFor a simple example, take the following setup:\n\nNotice here that ServiceA requires a reference to ServiceB. I'd like to keep that reference tied up in a shared_ptr. Am I okay to do what I did here, which is simply pass the shared_ptr down as a reference and let the std::shared_ptr copy constructor do the work for me? Does this properly increment the reference count on the shared_ptr?\n\nIf this is not the best way to do this, what is the common \"best practice\" for passing around std::shared_ptr's?"
    },
    {
        "link": "https://learn.microsoft.com/en-us/cpp/cpp/how-to-create-and-use-shared-ptr-instances?view=msvc-170",
        "document": "How to: Create and Use shared_ptr instances\n\nThe type is a smart pointer in the C++ standard library that is designed for scenarios in which more than one owner needs to manage the lifetime of an object. After you initialize a you can copy it, pass it by value in function arguments, and assign it to other instances. All the instances point to the same object, and share access to one \"control block\" that increments and decrements the reference count whenever a new is added, goes out of scope, or is reset. When the reference count reaches zero, the control block deletes the memory resource and itself.\n\nThe following illustration shows several instances that point to one memory location.\n\nThe examples that follow all assume that you've included the required headers and declared the required types, as shown here:\n\nWhenever possible, use the function to create a when the memory resource is created for the first time. is exception-safe. It uses the same call to allocate the memory for the control block and the resource, which reduces the construction overhead. If you don't use , then you have to use an explicit expression to create the object before you pass it to the constructor. The following example shows various ways to declare and initialize a together with a new object.\n\nThe following example shows how to declare and initialize instances that take on shared ownership of an object that was allocated by another . Assume that is an initialized .\n\nis also helpful in C++ Standard Library containers when you're using algorithms that copy elements. You can wrap elements in a , and then copy it into other containers with the understanding that the underlying memory is valid as long as you need it, and no longer. The following example shows how to use the algorithm on instances in a vector.\n\nYou can use , , and to cast a . These functions resemble the , , and operators. The following example shows how to test the derived type of each element in a vector of of base classes, and then copies the elements and display information about them.\n\nYou can pass a to another function in the following ways:\n• None Pass the by value. This invokes the copy constructor, increments the reference count, and makes the callee an owner. There's a small amount of overhead in this operation, which may be significant depending on how many objects you're passing. Use this option when the implied or explicit code contract between the caller and callee requires that the callee be an owner.\n• None Pass the by reference or const reference. In this case, the reference count isn't incremented, and the callee can access the pointer as long as the caller doesn't go out of scope. Or, the callee can decide to create a based on the reference, and become a shared owner. Use this option when the caller has no knowledge of the callee, or when you must pass a and want to avoid the copy operation for performance reasons.\n• None Pass the underlying pointer or a reference to the underlying object. This enables the callee to use the object, but it doesn't share ownership of the object with the caller's . Beware the case of the callee creating a from the passed raw pointer because the callee's has an independent reference count from the caller's . When the in the callee goes out of scope, it will delete the object, leaving the pointer in the caller's 'shared_ptr' pointing at released memory. When the caller's then goes out of scope, a double-free results. Only use this option when the contract between the caller and callee clearly specifies that the caller retains ownership of the lifetime.\n• None When you're deciding how to pass a , determine whether the callee has to share ownership of the underlying resource. An \"owner\" is an object or function that can keep the underlying resource alive for as long as it needs it. If the caller has to guarantee that the callee can extend the life of the pointer beyond its (the function's) lifetime, use the first option. If you don't care whether the callee extends the lifetime, then pass by reference and let the callee copy it or not.\n• None If you have to give a helper function access to the underlying pointer, and you know that the helper function uses the pointer and return before the calling function returns, then that function doesn't have to share ownership of the underlying pointer. It just has to access the pointer within the lifetime of the caller's . In this case, it's safe to pass the by reference, or pass the raw pointer or a reference to the underlying object. Passing this way provides a small performance benefit, and may also help you express your programming intent.\n• None Sometimes, for example in a , you may have to pass each to a lambda expression body or named function object. If the lambda or function doesn't store the pointer, then pass the by reference to avoid invoking the copy constructor for each element.\n\nThe following example shows how overloads various comparison operators to enable pointer comparisons on the memory that is owned by the instances."
    },
    {
        "link": "https://reddit.com/r/cpp_questions/comments/17tl7oh/best_practice_smart_pointer_use",
        "document": "I wonder what’s your strategy on smart pointer use. I have gone through some phases in my programming career:\n\nPhase 1) use no smart pointers at all Phase 2) use shared_ptr everywhere Phase 3) use barely any smart pointers other than unique_ptr\n\nWe don’t have to talk about phase 1. Phase 2 was quite convenient, because it was easy to slap shared_ptr on everything and be good with it. But the more complex my code became, the more I realised it is dangerous not to think about ownership at all. This lead me to phase 3. Now I use unique_ptr almost exclusively and only in rare events a shared_ptr.\n\nWhile this also seems to be the agreed “best practice” when scanning through the expert discussions, I wonder if I have gone a bit too far in this direction. Or put in other words: when do I actually want to share ownership in a multi-threaded application?\n\nIn my app I have bunch of data which is heavily shared across threads. There is one class where I can very clearly say: this class owns the data. Yet, other threads temporarily get access to it, perform operations on it and are expected to return their claim on the data. Currently I have implemented this by only allowing other classes to get the raw pointers to my unique_ptr. So it is clear they are not guaranteed any life-time on it. This works well, as long as I keep an eye on the synchronisation between the threads. So that the owner is not deleting anything while there is still others doing computations. I like that it forces me think about the ownership, program flow and the overall structure. But it’s also a slippery slope to miss out on a case which may lead to a segfault.\n\nWhat’s your approach? Do you always pass shared_ptr if multiple threads access the same data? Or do you prefer the unique_ptr + Synchronisation approach?"
    },
    {
        "link": "https://stackoverflow.com/questions/3476938/example-to-use-shared-ptr",
        "document": "Learning to use smart pointers is in my opinion one of the most important steps to become a competent C++ programmer. As you know whenever you new an object at some point you want to delete it.\n\nOne issue that arise is that with exceptions it can be very hard to make sure a object is always released just once in all possible execution paths.\n\nThis is the reason for RAII: http://en.wikipedia.org/wiki/RAII\n\nMaking a helper class with purpose of making sure that an object always deleted once in all execution paths.\n\nExample of a class like this is: std::auto_ptr\n\nBut sometimes you like to share objects with other. It should only be deleted when none uses it anymore.\n\nIn order to help with that reference counting strategies have been developed but you still need to remember addref and release ref manually. In essence this is the same problem as new/delete.\n\nThat's why boost has developed boost::shared_ptr, it's reference counting smart pointer so you can share objects and not leak memory unintentionally.\n\nWith the addition of C++ tr1 this is now added to the c++ standard as well but its named std::tr1::shared_ptr<>.\n\nI recommend using the standard shared pointer if possible. ptr_list, ptr_dequeue and so are IIRC specialized containers for pointer types. I ignore them for now.\n\nSo we can start from your example:\n\nThe problem here is now that whenever G goes out scope we leak the 2 objects added to G. Let's rewrite it to use std::tr1::shared_ptr\n\nWhen G goes out of scope the memory is automatically reclaimed.\n\nAs an exercise which I plagued newcomers in my team with is asking them to write their own smart pointer class. Then after you are done discard the class immedietly and never use it again. Hopefully you acquired crucial knowledge on how a smart pointer works under the hood. There's no magic really."
    },
    {
        "link": "https://geeksforgeeks.org/shared_ptr-in-cpp",
        "document": "std::shared_ptr is one of the smart pointers introduced in C++11. Unlike a simple pointer, it has an associated control block that keeps track of the reference count for the managed object. This reference count is shared among all the copies of the shared_ptr instances pointing to the same object, ensuring proper memory management and deletion.\n\nPrerequisites: Pointers in C++, Smart Pointers in C++.\n\nThe shared_ptr of type T can be declared as:\n\nWe can initialize the shared_ptr using the following methods:\n\nFollowing are some members associated with shared_ptr:\n\nResets the std::shared_ptr to empty, releasing ownership of the managed object. Returns the current reference count, indicating how many std::shared_ptr instances share ownership. Check if there is only one std::shared_ptr owning the object (reference count is 1). Returns a raw pointer to the managed object. Be cautious when using this method. swaps the contents (ownership) of two std::shared_ptr instances.\n\n// Printing the address of the managed object // referring to the same managed object // Relinquishes ownership of p1 on the object // This will print nullptr or 0 These lines demonstrate that p1 no longer manages an object (get() returns nullptr), but p2 still manages the same object, so its reference count is 1."
    },
    {
        "link": "https://edw.is/learning-vulkan",
        "document": "tl;dr: I learned some Vulkan and made a game engine with two small game demos in 3 months.\n\nThe code for the engine and the games can be found here: https://github.com/eliasdaler/edbr\n\nThis article documents my experience of learning Vulkan and writing a small game/engine with it. It took me around 3 months to do it without any previous knowledge of Vulkan (I had previous OpenGL experience and some experience with making game engines, though).\n\nThe engine wasn’t implemented as a general purpose engine, which is probably why it took me a few months (and not years) to achieve this. I started by making a small 3D game and separated reusable parts into the “engine” afterwards. I can recommend everyone to follow the same process to not get stuck in the weeds (see “Bike-shedding” section below for more advice).\n\nI’m a professional programmer, but I’m self-taught in graphics programming. I started studying graphics programming around 1.5 years ago by learning OpenGL and writing a 3D engine in it.\n\nThe engine I wrote in Vulkan is mostly suited for smaller level-based games. I’ll explain things which worked for me, but they might not be the most efficient. My implementation would probably still be a good starting point for many people.\n\nIf you haven’t done any graphics programming before, you should start with OpenGL. It’s much easier to learn it and not get overwhelmed by all the complexity that Vulkan has. A lot of your OpenGL and graphics programming knowledge will be useful when you start doing things with Vulkan later.\n\nIdeally, you should at least get a textured model displayed on the screen with some simple Blinn-Phong lighting. I can also recommend doing some basic shadow mapping too, so that you learn how to render your scene from a different viewpoint and to a different render target, how to sample from depth textures and so on.\n\nI can recommend using the following resources to learn OpenGL:\n• Thorsten Thormählen’s lectures lectures (watch the first 6 videos, the rest might be a bit too advanced)\n\nSadly, most OpenGL resources don’t teach the latest OpenGL 4.6 practices. They make writing OpenGL a lot more enjoyable. If you learn them, transitioning to Vulkan will be much easier (I only learned about OpenGL 3.3 during my previous engine development, though, so it’s not a necessity).\n\nHere are some resources which teach you the latest OpenGL practices:\n\nBike-shedding and how to avoid it\n\nAh, bike-shedding… Basically, it’s a harmful pattern of overthinking and over-engineering even the simplest things. It’s easy to fall into this trap when doing graphics programming (especially when doing Vulkan since you need to make many choices when implementing an engine with it).\n• Always ask yourself “Do I really need this?”, “Will this thing ever become a bottleneck?”.\n• Remember that you can always rewrite any part of your game/engine later.\n• Don’t implement something unless you need it right now. Don’t think “Well, a good engine needs X, right…?”.\n• Don’t try to make a general purpose game engine. It’s probably even better to not think about “the engine” at first and write a simple game.\n• Make a small game first - a Breakout clone, for example. Starting your engine development by doing a Minecraft clone with multiplayer support is probably not a good idea.\n• Be wary of people who tend to suggest complicated solutions to simple problems.\n• Don’t look too much at what other people do. I’ve seen many over-engineered engines on GitHub - sometimes they’re that complex for a good reason (and there are years of work behind them). But you probably don’t need most of that complexity, especially for simpler games.\n• Don’t try to make magical wrappers around Vulkan interfaces prematurely, especially while you’re still learning Vulkan.\n\nGet it working first. Leave “TODO”/“FIXME” comments in some places. Then move on to the next thing. Try to fix “TODO”/“FIXME” places only when they really become problematic or bottleneck your performance. You’ll be surprised to see how many things won’t become a problem at all.\n\nThe situation with graphic APIs in 2024 is somewhat complicated. It all depends on the use case: DirectX seems like the most solid choice for most AAA games. WebGL or WebGPU are the only two choices for doing 3D graphics on the web. Metal is the go-to graphics API on macOS and iOS (though you can still do Vulkan there via MoltenVK).\n\nMy use case is simple: I want to make small 3D games for desktop platforms (Windows and Linux mostly). I also love open source technology and open standards. So, it was a choice between OpenGL and Vulkan for me.\n\nOpenGL is a good enough choice for many small games. But it’s very unlikely that it’ll get new versions in the future (so you can’t use some newest GPU capabilities like ray tracing), it’s deprecated on macOS and its future is uncertain.\n\nWebGPU was also a possible choice. Before learning Vulkan, I learned some of it. It’s a pretty solid API, but I had some problems with it:\n• It’s still not stable and there’s not a lot of tutorials and examples for it. This tutorial is fantastic, though.\n• WGSL is an okay shading language, but I just find its syntax not as pleasant as GLSL’s (note that you can write in GLSL and then load compiled SPIR-V on WebGPU native).\n• On desktop, it’s essentially a wrapper around other graphic APIs (DirectX, Vulkan, Metal).This introduces additional problems for me:\n• It can’t do things some things that Vulkan or DirectX can do.\n• It has more limitations than native graphic APIs since it needs to behave similarly between them.\n• RenderDoc captures become confusing as they differ between the platforms (you can get DirectX capture on Windows and Vulkan capture on Linux) and you don’t have 1-to-1 mapping between WebGPU calls and native API calls.\n• Using Dawn and WGPU feels like using bgfx or sokol. You don’t get the same degree of control over the GPU and some of the choices/abstractions might not be the most pleasant for you.\n\nStill, I think that WebGPU is a better API than OpenGL/WebGL and can be more useful to you than Vulkan in some use cases:\n• Validation errors are much better than in OpenGL/WebGL and not having global state helps a lot.\n• It’s also kind of similar to Vulkan in many things, so learning a bit of it before diving into Vulkan also helped me a lot.\n• It requires a lot less boilerplate to get things on the screen (compared to Vulkan).\n• You don’t have to deal with explicit synchronization which makes things much simpler.\n• You can make your games playable inside the browser.\n\nLearning Vulkan seemed like an impossible thing for me previously. It felt like you needed to have many years of AAA game graphics programming experience to be able to do things in it. You also hear people saying “you’re basically writing a graphics driver when writing in Vulkan” which also made Vulkan sounds like an incredibly complicated thing.\n\nI have also checked out some engines written in Vulkan before and was further demotivated by seeing tons of scary abstractions and files named like or which had thousands of lines of scary C++ code.\n\nThe situation has changed over the years. Vulkan is not as complicated as it was before. First of all, Khronos realized that some parts of Vulkan were indeed very complex and introduced some newer features which made many things much simpler (for example, dynamic rendering). Secondly, some very useful libraries which reduce boilerplate were implemented. And finally, there are a lot of fantastic resources which make learning Vulkan much easier than it was before.\n\nThe best Vulkan learning resource which helped me get started was vkguide. If you’re starting from scratch, just go through it all (you might stop at “GPU driver rendering” chapter at first - many simple games probably won’t need this level of complexity)\n\nVulkan Lecture Series by TU Wien also nicely teaches Vulkan basics (you can probably skip “Real-Time Ray Tracing” chapter for now). I especially found a lecture on synchronization very helpful.\n\nHere are some more advanced Vulkan books that also helped me:\n• 3D Graphics Rendering Cookbook by Sergey Kosarevsky and Viktor Latypov. There is the second edition in the writing and it’s promising to be better than the first one. The second edition is not released yet, but the source code for it can be found here: https://github.com/PacktPublishing/3D-Graphics-Rendering-Cookbook-Second-Edition\n• Mastering Graphics Programming with Vulkan by Marco Castorina, Gabriel Sassone. Very advanced book which explains some of the “cutting edge” graphics programming concepts (I mostly read it to understand where to go further, but didn’t have time to implement most of it). The source code for it can be found here: https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan\n\nHere’s the result of my first month of learning Vulkan:\n\nBy this point I had:\n\nOf course, doing it for the 3rd time (I had it implemented it all in OpenGL and WebGPU before) certainly helped. Once you get to this point, Vulkan won’t seem as scary anymore.\n\nLet’s see how the engine works and some useful things I learned.\n\nMy engine is called EDBR (Elias Daler’s Bikeshed Engine) and was initially started as a project for learning Vulkan. It quickly grew into a somewhat usable engine which I’m going to use for my further projects.\n\nI copy-pasted some non-graphics related stuff from my previous engine (e.g. input handling and audio system) but all of the graphics and many other core systems were rewritten from scratch. I feel like it was a good way to do it instead of trying to cram Vulkan into my old OpenGL abstractions.\n\nLet’s see how this frame in rendered:\n\nFirst, models with skeletal animations are skinned in the compute shader. The compute shader takes unskinned mesh and produces a buffer of vertices which are then used instead of the original mesh in later rendering steps. This allows me to treat static and skinned meshes similarly in shaders and not do skinning repeatedly in different rendering steps.\n\nI use a 4096x4096 depth texture with 3 slices for cascaded shadow mapping. The first slice looks like this:\n\nAll the models are drawn and shading is calculated using the shadow map and light info. I use a PBR model which is almost identical to the one described in Physically Based Rendering in Filament. The fragment shader is quite big and does calculation for all the lights affecting the drawn mesh in one draw call:\n\nEverything is drawn into a multi-sampled texture. Here’s how it looks after resolve:\n\nDepth resolve step is performed manually via a fragment shader. I just go through all the fragments of multi-sample depth texture and write the minimum value into the non-MS depth texture (it’ll be useful in the next step).\n\nSome post FX is applied - right now it’s only depth fog (I use “depth resolve” texture from the previous step here), afterwards tone-mapping and bloom will also be done here.\n\nDialogue UI is drawn. Everything is done in one draw call (more is explained in “Drawing many sprites” section)\n\nAnd that’s it! It’s pretty basic right now and would probably become much more complex in the future (see “Future work” section).\n\nThere are a couple of libraries which greatly improve the experience of writing Vulkan. Most of them are already used in vkguide, but I still want to highlight how helpful they were to me.\n\nvk-bootstrap simplifies a lot of Vulkan boilerplate: physical device selection, swapchain creation and so on.\n\nI don’t like big wrappers around graphic APIs because they tend to be very opinionated. Plus, you need to keep a mental map of “wrapper function vs function in the API spec” in your head at all times.\n\nThankfully, vk-bootstrap is not like this. It mostly affects the initialization step of your program and doesn’t attempt to be a wrapper around every Vulkan function.\n\nI’ll be honest, I used VMA without even learning about how to allocate memory in Vulkan manually. I read about it in the Vulkan spec later - I’m glad that I didn’t have to do it on my own.\n\nVolk was very useful for me for simplifying extension function loading. For example, if you want to use very useful for setting debug names for your objects (useful for RenderDoc captures and validation errors), you’ll need to do this if you don’t use volk:\n\nWith volk, all the extensions are immediately loaded after you call and you don’t need to store these pointers everywhere. You just include and call - beautiful!\n\nI have a class which encapsulates most of the commonly used functionality and stores many objects that you need for calling Vulkan functions ( , and so on). A single instance is created on the startup and then gets passed around.\n• returns a new which is later used in all the drawing steps.\n• does drawing to the swapchain and does sync between the frames.\n\nThat’s… a lot of things. However, it’s not that big: is only 714 lines at the time of writing this article. It’s more convenient to pass one object into the function instead of many ( , , and so on).\n\nIn Vulkan, you can use any shading language which compiles to SPIR-V - that means that you can use GLSL, HLSL and others. I chose GLSL because I already knew it from my OpenGL experience.\n\nYou can pre-compile your shaders during the build step or compile them on the fly. I do it during the build so that my shader loading runtime code is simpler. I also don’t have an additional runtime dependency on the shader compiler. Also, shader errors are detected during the build step and I don’t get compile errors during the runtime.\n\nI use glslc (from shaderc project, it’s included in Vulkan SDK) which allows you to specify a in CMake which is incredibly useful when you use shader includes. If you change a shader file, all files which include it are recompiled automatically. Without the , CMake won’t be able to see which files shader files need to be recompiled and will only recompile the file which was changed.\n\nMy CMake script for building shaders looks like this:\n\nand then in the main CMakeLists file:\n\nNow, when you build a target, shaders get built automatically and the resulting SPIR-V files are put into the binary directory.\n\nPassing data to shaders in OpenGL is much simpler than it is in Vulkan. In OpenGL, you could just do this:\n\nYou can also use explicit uniform location like this.\n\nIn Vulkan, you need to group your uniforms into “descriptor sets”:\n\nNow, this makes things a lot more complicated, because you need to specify descriptor set layout beforehand, use descriptor set pools and allocate descriptor sets with them, do the whole + thing, call for each descriptor set and so on.\n\nI’ll explain later how I avoided using descriptor sets by using bindless descriptors and buffer device access. Basically, I only have one “global” descriptor set for bindless textures and samplers, and that’s it. Everything else is passed via push constants which makes everything much easier to handle.\n\nMost of them look like this:\n\nThe function is usually called once during the engine initialization. abstraction is described in vkguide here. I modified it a bit to use the Builder pattern to be able to chain the calls.\n• does all the needed cleanup. It usually simply destroys the pipeline and its layout:\n• is called each frame and all the needed inputs are passed as arguments. It’s assumed that the sync is performed outside of the call (see “Synchronization” section below). Some pipelines are only called once per frame - some either take of objects to draw or are called like this:\n\nThe typical function looks like this:\n\nNote another thing: it’s assumed that is called between and - the render pass itself doesn’t care what texture it renders to - the caller of is responsible for that. It makes things simpler and allows you to do several draws to the same render target, e.g.:\n\nI have one vertex type for all the meshes. It looks like this:\n\nThe vertices are accessed in the shader like this:\n\nPVP frees you from having to define vertex format (no more VAOs like in OpenGL or + in Vulkan). BDA also frees you from having to bind a buffer to a descriptor set - you just pass an address to your buffer which contains vertices in push constants and that’s it.\n\nTextures were painful to work with even in OpenGL - you had “texture slots” which were awkward to work with. You couldn’t just sample any texture from the shader if it wasn’t bound to a texture slot beforehand. changed that and made many things easier.\n\nVulkan doesn’t have the exact same functionality, but it has something similar. You can create big descriptor sets which look like this:\n\nYou’ll need to maintain a list of all your textures using some “image manager” and when a new texture is loaded, you need to insert it into the array. The index at which you inserted it becomes a bindless “texture id” which then can be used to sample it in shaders. Now you can pass these ids in your push constants like this:\n\nand then you can sample your texture in the fragment shader like this:\n• I chose separate image samplers so that I could sample any texture using different samplers. Common samplers (nearest, linear with anisotropy, depth texture samplers) are created and put into array on the startup.\n• The wrapper function makes the process of sampling a lot more convenient.\n\nI use bindless ids for the mesh material buffer which looks like this:\n\nNow I can only pass material ID in my push constants and then sample texture like this in the fragment shader:\n\nNeat! No more bulky descriptor sets, just one int per material in the push constants.\n\nYou can also put different texture types into the same set like this (this is needed for being able to access textures of types other than ):\n\nAnd here’s how you can sample with a linear sampler (note that we use here instead of ):\n\nHere’s a very good article on using bindless textures in Vulkan:\n\nHandling dynamic data which needs to be uploaded every frame\n\nI find it useful to pre-allocate big arrays of things and push stuff to them in every frame. Basically, you can pre-allocate an array of N structs (or matrices) and then start at index 0 at each new frame and push things to it from the CPU. Then, you can access all these items in your shaders. For example, I have all joint matrices stored in one big array and the skinning compute shader accesses joint matrices of a particular mesh using start index passed via push constants (more about it will be explained later).\n\nHere are two ways of doing this:\n• \n• Have N buffers on GPU and swap between them.\n\nvkguide explains the concept of “in flight” frames pretty well. To handle this parallelism properly, you need to have one buffer for the “currently drawing” frame and one buffer for “currently recording new drawing commands” frame to not have races. (If you have more frames in flight, you’ll need to allocate more than 2 buffers)\n\nThis means that you need to preallocate 2 buffers on GPU. You write data from CPU to GPU to the first buffer during the first frame. While you record the second frame, GPU reads from the first buffer while you write new data to the second buffer. On the third frame, GPU reads from the second buffer and you write new info to the first buffer… and so on.\n• \n• One buffer on GPU and N “staging” buffers on CPU\n\nThis might be useful if you need to conserve some memory on the GPU.\n\nLet’s see how it works in my engine:\n\nNote how staging buffers are created using VMA’s flag and the “main” buffer from which we read in the shader is using the flag.\n\nHere’s how new data is uploaded (full implementation):\n\nI’d go with the first approach for most cases (more data on GPU, but no need for manual sync) unless you need to conserve GPU memory for some reason. I’ve found no noticeable difference in performance between two approaches, but it might matter if you are uploading huge amounts of data to GPU on each frame.\n\nNow, this might be somewhat controversial… but I didn’t find much use of the deletion queue pattern used in vkguide. I don’t really need to allocated/destroy new objects on every frame.\n\nUsing C++ destructors for Vulkan object cleanup is not very convenient either. You need to wrap everything in custom classes, add move constructors and move … It adds an additional layer of complexity.\n\nIn most cases, the cleanup of Vulkan objects happens in one place - and you don’t want to accidentally destroy some in-use object mid-frame by accidentally destroying some wrapper object.\n\nIt’s also harder to manage lifetimes when you have cleanup in happening in the destructor. For example, suppose you have a case like this:\n\nIf you want to cleanup resources (e.g. the instance of has a object) during , you can’t do that if the cleanup of is performed in its destructor.\n\nOf course, you can do this:\n\n… but I don’t like how it introduces a dynamic allocation and requires you to do write more code (and it’s not that much different from calling a function manually).\n\nRight now, I prefer to clean up stuff directly, e.g.\n\nThis approach is not perfect - first of all, it’s easy to forget to call function, This is not a huge problem since you get a validation error in case you forget to cleanup some Vulkan resources on shutdown:\n\nVMA also triggers asserts if you forget to free some buffer/image allocated with it.\n\nI find it convenient to have all the Vulkan cleanup happening explicitly in one place. It makes it easy to track when the objects get destroyed.\n\nSynchronization in Vulkan is difficult. OpenGL and WebGPU do it for you - if you read from some texture/buffer, you know that it will have the correct data and you won’t get problems with data races. With Vulkan, you need to be explicit and this is usually where things tend to get complicated.\n\nRight now I manage most of the complexities of sync manually in one place. I separate my drawing into “passes”/pipelines (as described above) and then insert barriers between them. For example, the skinning pass writes new vertex data into GPU memory. Shadow mapping pass reads this data to render skinned meshes into the shadow map. Sync in my code looks like this:\n\nOf course, this can be automated/simplified using render graphs. This is something that I might implement in the future. Right now I’m okay with doing manual sync. vkconfig’s “synchronization” validation layer also helps greatly in finding sync errors.\n\nThe following resources were useful for understanding synchronization:\n\nWith bindless textures, it’s easy to draw many sprites using one draw call without having to allocate vertex buffers at all.\n\nFirst of all, you can emit vertex coordinates and UVs using in your vertex shader like this:\n\nThis snippet produces this set of values:\n\nAll the sprite draw calls are combined into which looks like this in GLSL:\n\nOn CPU/C++ side, it looks almost the same:\n\nI create two fixed size buffers on the GPU and then upload the contents of (using techniques described above in the “Handling dynamic data” section).\n\nThe sprite renderer is used like this:\n\nAnd finally, here’s how the command to do the drawing looks like inside :\n\nThe complete sprite.vert looks like this:\n\nAll the parameters of the sprite draw command are self-explanatory, but needs a bit of clarification. Currently, I use it to branch inside the fragment shader:\n\nThis allows me to draw sprites differently depending on this ID without having to change pipelines. Of course, it can be potentially bad for the performance. This can be improved by drawing sprites with the same shader ID in batches. You’ll only need to switch pipelines when you encounter a draw command with a different shader ID.\n\nThe sprite renderer is very efficient: it can draw 10 thousand sprites in just 315 microseconds.\n\nI do skinning for skeletal animation in a compute shader. This allows me to have the same vertex format for all the meshes.\n\nBasically, I just take the mesh’s vertices (not skinned) and joint matrices and produce a new buffer of vertices which are used in later rendering stages.\n\nSuppose you spawn three cats with identical meshes:\n\nAll three of them can have different animations. They all have an identical “input” mesh. But the “output” vertex buffer will differ between them, which means that you need to pre-allocate a vertex buffer for each instance of the mesh.\n\nHere’s how the skinning compute shader looks like:\n• I store all joint matrices in a big array and populate it every frame (and also pass the starting index in the array for each skinned mesh, ).\n• Skinning data is not stored inside each mesh vertex, a separate buffer of elements is used.\n\nAfter the skinning is performed, all the later rendering stages use this set of vertices Thee rendering process for static and skinned meshes becomes identical, thanks to that.\n\nI have a game/renderer separation which uses a simple concept of “draw commands”. In the game logic, I use entt, but the renderer doesn’t know anything about entities or “game objects”. It only knows about the lights, some scene parameters (like fog, which skybox texture to use etc) and meshes it needs to draw.\n\nThe renderer’s API looks like this in action:\n\nWhen you call or , the renderer creates a mesh draw command and puts it in which are then iterated through during the drawing process. The looks like this:\n• is used for looking up static meshes in - it’s a simple of references to vertex buffers on GPU.\n• If the mesh has a skeleton, is used during compute skinning and is used for all the rendering afterwards (instead of )\n• is used for frustum culling.\n\nThis separation is nice because the renderer is clearly separated from the game logic. You can also do something more clever as described here if sorting draw commands becomes a bottleneck.\n\nI use Blender as a level editor and export it as glTF. It’s easy to place objects, colliders and lights there. Here’s how it looks like:\n\nWriting your own level editor would probably take months (years!), so using Blender instead saved me quite a lot of time.\n\nIt’s important to mention how I use node names for spawning some objects. For example, you can see an object named selected in the screenshot above. The part before the first dot is the prefab name (in this case “Interact”). The “Sphere” part is used by the physics system to create a sphere physics body for the object (“Capsule” and “Box” can also be used, otherwise the physics shape is created using mesh vertices).\n\nSome models are pretty complex and I don’t want to place them directly into the level glTF file as it’ll greatly increase each level’s size. I just place an “Empty->Arrows” object and name it something like “Cat.NearStore”. This will spawn “Cat” prefab and attach “NearStore” tag to it for runtime identification.\n\nPrefabs are written in JSON and look like this:\n\nDuring the level loading process, if the node doesn’t have a corresponding prefab, it’s loaded as-is and its mesh data is taken from the glTF file itself (this is mostly used for static geometry). If the node has a corresponding prefab loaded, it’s created instead. Its mesh data is loaded from the external glTF file - only transform is copied from the original glTF node (the one in the level glTF file).\n\nUsing forward rendering allowed me to easily implement MSAA. Here’s a comparison of how the game looks without AA and with MSAA on:\n\nMSAA is explained well here: https://vulkan-tutorial.com/Multisampling\n\nHere’s another good article about MSAA: https://therealmjp.github.io/posts/msaa-overview/ and potential problems you can have with it (especially with HDR and tone-mapping).\n\nMy UI system was inspired by Roblox’s UI API: https://create.roblox.com/docs/ui\n\nBasically, the UI can calculate its own layout without me having to hard code each individual element’s size and position. Basically it relies on the following concepts:\n• Origin is an anchor around which the UI element is positioned. If origin is , setting UI element’s position to be will make its upper-left pixel have (x,y) pixel coordinate. If the origin is , then the element’s bottom-right corner will be positioned at . If the origin is (0.5, 1) then it will be positioned using bottom-center point as the reference.\n• Relative size makes the children’s be proportional to parent’s size. If (1,1) then the child element will have the same size as the parent element. If it’s (0.5, 0.5) then it’ll have half the size of the parent. If the parent uses children’s size as a guide, then if a child has (0.5, 0.25) relative size, the parent’s width will be 2x larger and the height will be 4x larger.\n• Relative position uses parent’s size as a guide for positioning. It’s useful for centering elements, for example if you have an element with (0.5, 0.5) origin and (0.5, 0.5) relative position, it’ll be centered inside its parent element.\n• You can also set pixel offsets for both position and size separately (they’re called and in my codebase).\n• You can also set a fixed size for the elements if you don’t want them to ever be resized.\n• The label/image element size is determined using its content.\n\nHere are some examples of how it can be used to position child elements:\n\na) The child (yellow) has relative size (0.5, 1), relative position of (0.5, 0.5) and origin (0.5, 0.5) (alternatively, the relative position can be (0.5, 0.0) and origin at (0.5, 0.0) in this case). Its parent (green) will be two times wider, but will have the same height. The child element will be centered inside the parent.\n\nb) The child (yellow) has origin (1, 1), fixed size (w,h) and absolute offset of (x,y) - this way, the item can be positioned relative to the bottom-right corner of its parent (green)\n\nLet’s see how sizes and positions of UI elements are calculated (implementation in EDBR).\n\nFirst, sizes of all elements are calculated recursively. Then positions are computed based on the previously computed sizes and specified offset positions. Afterwards all elements are drawn recursively - parent element first, then its children etc.\n\nWhen calculating the size, most elements either have a “fixed” size (which you can set manually, e.g. you can set some button to always be 60x60 pixels) or their size is computed based on their content. For example, for label elements, their size is computed using the text’s bounding box. For image elements, their size equals the image size and so on.\n\nIf an element has an “Auto-size” property, it needs to specify which child will be used to calculate its size. For example, the menu nine-slice can have several text labels inside the “vertical layout” element - the bounding boxes will be calculated first, then their sizes will be summed up - then, the parent’s size is calculated.\n\nLet’s take a look at a simple menu with bounding boxes displayed:\n\nHere, root is marked as “Auto-size”. To compute its size, it first computes the size of its child ( ). This recursively computes the sizes of each button, sums them up and adds some padding ( also makes the width of each button the same based on the maximum width in the list).\n\nI love Dear ImGui. I used it to implement many useful dev and debug tools (open the image in a new tab to see them better):\n\nIt has some problems with sRGB, though. I won’t explain it in detail, but basically if you use sRGB framebuffer, Dear ImGui will look wrong in many ways, see the comparison:\n\nSometimes you can see people doing hacks by doing with Dear ImGui’s colors but it still doesn’t work properly with alpha and produces incorrect color pickers.\n\nI ended up writing my own Dear ImGui backend and implementing DilligentEngine’s workaround which is explained in detail here and here.\n\nThere are some additional benefits of having my own backend:\n• It supports bindless texture ids, so I can draw images by simply calling . Dear ImGui’s Vulkan backend requires you to “register” textures by calling for each texture before you can call .\n• It can properly draw linear and non-linear images by passing their format into backend (so that sRGB images are not gamma corrected twice when they’re displayed)\n• Initializing and dealing with it is easier as it does Vulkan things in the same way as the rest of my engine.\n\nThere are many parts of the engine not covered there because they’re not related to Vulkan. I still feel like it’s good to mention them briefly for the sake of completion.\n\nIntegrating it into the engine was pretty easy. Right now I mostly use it for collision resolution and basic character movement.\n\nThe samples are fantastic. The docs are very good too.\n\nI especially want to point out how incredible is. It handles basic character movement so well. I remember spending days trying to get proper slope movement in Bullet to work. With Jolt, it just worked “out of the box”.\n\nHere’s how it basically works (explaining how it works properly would probably require me to write quite a big article):\n• You add your shapes to Jolt’s world.\n• You get new positions of your physics objects and use these positions to render objects in their current positions.\n• I use entt for the entity-component-system part.\n\nIt has worked great for me so far. Previously I had my own ECS implementation, but decided to experiment with a 3rd party ECS library to have less code to maintain.\n• I use openal-soft, libogg and libvorbis for audio.\n\nThe audio system is mostly based on these articles: https://indiegamedev.net/2020/02/15/the-complete-guide-to-openal-with-c-part-1-playing-a-sound/\n\nIntegrating it was very easy (read the PDF doc, it’s fantastic!) and it helped me avoid tons of bike-shedding by seeing how little time something, which I thought was “inefficient”, really took.\n\nWhat I gained from switching to Vulkan\n\nThere are many nice things I got after switching to Vulkan:\n\nThis makes abstractions a lot easier. With OpenGL abstractions/engines, you frequently see “shader.bind()” calls, state trackers, magic RAII, which automatically binds/unbinds objects and so on. There’s no need for that in Vulkan - it’s easy to write functions which take some objects as an input and produce some output - stateless, more explicit and easier to reason about.\n• API is more pleasant to work with overall - I didn’t like “binding” things and the whole “global state machine” of OpenGL.\n• You need to write less abstractions overall.\n\nWith OpenGL, you need to write a lot of abstractions to make it all less error-prone… Vulkan’s API requires a lot less of this, in my experience. And usually the abstractions that you write map closer to Vulkan’s “raw” functions, compared to OpenGL abstractions which hide manipulation of global state and usually call several functions (and might do some stateful things for optimization).\n\nValidation errors are very good in Vulkan. While OpenGL has , it doesn’t catch that many issues and you’re left wondering why your texture looks weird, why your lighting is broken and so on. Vulkan has more extensive validation which makes the debugging process much better.\n\nI can now debug shaders in RenderDoc. It looks like this:\n\nWith OpenGL I had to output the values to some texture and color-pick them… which took a lot of time. But now I can debug vertex and fragment shaders easily.\n• More consistent experience across different GPUs and OSes.\n\nWith OpenGL, drivers on different GPUs and OSes worked differently from each other which made some bugs pop up only on certain hardware configurations. It made the process of debugging them hard. I still experienced some slight differences between different GPUs in Vulkan, but it’s much less prevalent compared to OpenGL.\n• Ability to use better shading languages in the future\n\nGLSL is a fine shading language, but there are some new shading languages which promise to be more feature-complete, convenient and readable, for example:\n\nI might explore them in the future and see if they offer me something that GLSL lacks.\n• More control over every aspect of the graphics pipeline.\n\nMy first OpenGL engine was written during the process of learning graphics programming from scratch. Many abstractions were not that good and rewriting them with some graphics programming knowledge (and some help from vkguide) helped me implement a much cleaner system.\n\nAnd finally, it makes me proud to be able to say “I have a custom engine written in Vulkan and it works”. Sometimes people start thinking about you as a coding wizard and it makes me happy and proud of my work. :)\n\nThere are many things that I plan to do in the future, here’s a list of some of them:\n• Loading many images and generating mipmaps in parallel (or use image formats which already have mipmaps stored inside of them)\n\nOverall, I’m quite satisfied with what I managed to accomplish. Learning Vulkan was quite difficult, but it wasn’t as hard as I imagined. It taught me a lot about graphics programming and modern APIs and now I have a strong foundation to build my games with."
    },
    {
        "link": "https://reddit.com/r/vulkan/comments/8gph83/best_practices_for_storing_textures",
        "document": "In opengl, they say you should5 use as few textures as possible, and instead stuff em with all the individual images stitched together. is it the same for vulkan? should I keep one big image top fill with textures or are many smaller images better? I would probably allocate a single memory object to hold them all either way.\n\nEDIT what I want to know is, if I should have a single VkImage or multiple VkImages to store stuff."
    },
    {
        "link": "https://therookies.co/entries/18223",
        "document": "My Vulkan renderer that I built from scratch in C++ as a learning exercise. Although it initially started as a project to practice my skills, it grew into something bigger and in my opinion worth showing off.\n\nThis is a Vulkan renderer I wrote from scratch in C++. I started the project back in 2020, after I finished a tutorial on Vulkan. At the time I understood how the main concepts of the API work, but couldn't quite wrap my head around how to use it in a more complex project, which is why I started this project.\n\n In the end, I ended up going on a long and deep dive into more of the details of the Vulkan API and how to write a game engine of sorts. I absolutely loved working on this project, and loved learning a lot from all the little challenges that stood in my way.\n\nBelow the overview video I go into more details on some of the technical aspects of the project.\n\nThis was a fun little side-quest to figure out, because going into this I had no clue where to start. In the end I realized that all I had to do was simply destroy the current render pass and graphics pipeline, reload the shader files and recreate the render pass and graphics pipeline. Because of my small abstraction layer, this was fairly easy to implement, and the end code resulted in something like this:\n\nA major pain point I identified was setting up pipelines in Vulkan. They require a lot of boilerplate structs to be filled out with the necessary information. To make this easier, I made a PipelineBuilder helper class. With simply calling a few functions, these structs get filled and the pipeline gets created. Another benefit I found was that I could simply re-use the builder, change some properties and create a different unique pipeline without too much extra code. This is roughly how to use this PipelineBuilder:\n\nI ended up writing a small asset manager, but in its current implementation it only manages textures. The main reason behind this is that I didn't want to have to load the same texture multiple times. This was especially important to me when I worked on model loading, and needed a fallback default texture if a material didn't have a specific texture. Internally, the asset manager simply holds a map of strings and asset references. An asset reference is nothing more than a pointer to the asset, and a count to how many references there are still alive.\n\nWhen we request a texture through the asset manager, it first checks if it has already loaded that one before. If it has, it increases the refCount for that asset and returns the asset. Otherwise, it will load in the texture on-the-fly, store it to keep track of it and then return it. Upon releasing a texture, it first checks if there are other references currently alive. Only when you release the last instance of that asset, does the asset effectively get destroyed.\n\nI really wanted to implement an ECS in this project, because I had not implemented one before. I chose EnTT to do the heavy-lifting, after hearing quite a few other people recommend it after they implemented it in their own projects and games. In its current implementation, my scene system is just a thin wrapper around EnTT to make it play nicely in my project. Although this was useful to do through code, I could see this becoming quite annoying to work with over time, which is why I added a serialization/deserialization system that can save and load these scenes from and to a JSON file."
    },
    {
        "link": "https://medium.com/@jjspira/uploading-textures-with-vulkan-dfcbfca0a09c",
        "document": "Everything we need in an “image” we’re going to slap into a single struct. We won’t use all these fields here, but the reason to keep them bundled is simple — we don’t want Rust to automatically deconstruct any of them, so we need to haul them around. I call this “loaded image”… . It's also a badass name 😎.\n\nNote those wrappers, which allows us to pass a function to free this memory, since they really represent assets on the GPU.\n\nFirst things first, let’s be good C-citizens (when we’re this in Rust, we're not far from just writing C) and add our destructor:\n\nIt’s difficult to force the compiler to force us to use the method when we make an image -- normally that's a thing Rust handles easily, but we've basically \"turned that off\" by using , so I guess we'll just have to use our dumb brains to remember to do it (fun fact -- while writing this article, I forgot to include the section about dropping some memory, ironically showing why this kind of memory management can be error prone).\n\n, by the way, is a simple convenience macro because I got tired of writing this all the time:\n\nOkay, now we’re onto the good bits! Let’s walk through what actually making an image looks like.\n\nNotice that we pass in our . We only have two options as far as I can tell -- or . Simple choice, really -- if you're doing pixel art, do , otherwise, do . In your game, you might feel free to hardcode this.\n\nThere’s two other things to point out here of note: and . These refer to the texel size of the image. If you've never heard the term , bless your heart, because it's a terrible word. A is to a texture like a is to a... ....which is what a is...oh no!\n\nOkay, so what is a ? For us, we're using a very simple definition (mipmaps complicate this!): a texture is a 2D grid of , and a color is 4 s in a row, forming an image (u8’s can represent 256 numbers, which is why colors are 0–255!).\n\nHere’s an example of a texture written out…\n\nAnd here is it in picture form:\n\nSo when we say the and of a texture, we're really asking about this grid.\n\nThis whole section is largely boilerplate, but let’s run through it quickly.\n\nFirst, we say, “Hey, GPU, make me an image please” and it says “sure, here ya go”:\n\nWe’ll also need to find the requirements for how much memory the GPU is going to need. This ultimately is up to the GPU to tell us, since GPUs might pad memory differently, but it’s going to be in the ballpark of , which reflects the we wrote out above.\n\nNext, we’re going to get that memory requirement, ask the GPU to allocate that memory, and then we bind that memory to our image object. I’m not exactly sure what means in this Vulkan context for the GPU, but I assume this is essentially giving our on the GPU side a pointer to its memory. The code to do that looks like this:\n\nNext, we make our image_view and our sampler. It’s difficult for me to get into too much detail, as these things are bound to your which come from the you'll create in your , but for me, a simple 2D man with a simple 2D game, it looks like this:\n\nAnd finally, we create our like this:\n\nOkay! So now we have a . You'll notice we bound it to a before we returned it out of its constructor, and that's because we're not done yet. It's time to actually edit the image so it looks like what we want.\n\nTo edit any image, we need to create a buffer, which we’ll fill with our colors, turning it into a flat representation of that grid which we wrote out above, and then we need to put that buffer in our pipeline to send into our image!\n\nFirst, we’re going to need to do some pointer funtime math! Here’s what we’re going to need to do:\n\nWhat’s that function? It's exactly like how we made an image object, but just slightly tweaked to be about buffers instead of images.\n\nThe returned looks like this, just to keep it all out there:\n\nIt also has a method, like the before it:\n\nNow here’s the real meat of the problem — we need to write the stream of image data we have to the buffer. This code is dense, so read over it a few times for clarification. For me, grabbing a piece of paper and doing it myself gave me a good feel, but basically, we’re trying to convert a flat array to grid, copying each row at a time to the GPU. When we send it to the GPU, we’ll tell it how long each row is, which the GPU will use to re-assemble the grid the later.\n\nAnd with that, our is good to go! We need one last piece of data, and that's simple:\n\nI have this all bound in as as a function which returns a tuple of , which is good enough. See the linked repository for more.\n\nUploading Our Buffer to the GPU\n\nOkay, so when you want to upload data to the GPU, you need two things:\n• The data you want to operate on in some sort of buffer. We just made ours when we made our “staging buffer” and prepared it with our image data.\n• A “command buffer” which is just another buffer that you upload to the GPU which has references to the buffer(s) you want to operate on, and…well…commands to the GPU, as to what to do with those buffers.\n\nTo make our command buffer, we ask the GPU for one out of our , which we make in our creation (see the learn gfx_hal tutorials above for that!):\n\nOur image is in some state right now (as in, I personally don't know what state it's in!), so we'll need to transfer it to a state where we can write to it. We do this with a barrier, and we create on like this:\n\nNext, we do what we actually want to be doing here, which is copying the buffer over! We do it like this:\n\nImportant note here: if you instead want to make a dynamic texture (which I may cover in a brief addendum in the future), where you edit a part of a texture after creating it, you can easily do that by making and only a section of the image, and then specify some into the image. You can also just re-edit the entire texture at once, but that's awfully wasteful!\n\nNow, we need to transition our back to being in the state of and the layout of . We do that with...you guessed it, another barrier, like so:\n\nAnd now we’re done adding to our . We'll have to submit it to the GPU to actually do all those commands, but before we do that, we make a . For those who don't know, a , in Vulkan speak, is similar to a semaphore, but a fence is used between the CPU and the GPU and a semaphore is used between different parts of the CPU. (Check the Vulkan docs for a better explanation of the difference -- in practice, sometimes Vulkan wants a fence, sometimes it wants a semaphore. I just do what the specs tell me to do).\n\nAs always, we need to do our cleanup here too! First, we wait on our fence to make sure that our command buffer has finished being uploaded to the GPU, and then we free it and destroy the fence. Afterwards, we cleanup everything else.\n\nAnd, with that, we are done!\n\nLet’s take a step back and let’s see how this code look in our wider program.\n\nI made a wrapper function called which requires my , which is where my , , , , and live, and an . This is a struct provided by the image crate. In the repository with all this code, I’ve just mocked this up, because otherwise you’re going to have to look at all five thousand lines or so of Vulkan rendering code, and I don’t think anyone wants that.\n\nThe function looks like this:\n\nThat looks pretty good to me!"
    },
    {
        "link": "https://intel.com/content/www/us/en/developer/articles/training/api-without-secrets-introduction-to-vulkan-part-6.html",
        "document": ""
    },
    {
        "link": "https://glfw.org/docs/latest/vulkan_guide.html",
        "document": "This guide is intended to fill the gaps between the official Vulkan resources and the rest of the GLFW documentation and is not a replacement for either. It assumes some familiarity with Vulkan concepts like loaders, devices, queues and surfaces and leaves it to the Vulkan documentation to explain the details of Vulkan functions.\n\nTo develop for Vulkan you should download the LunarG Vulkan SDK for your platform. Apart from headers and link libraries, they also provide the validation layers necessary for development.\n\nThe Vulkan Tutorial has more information on how to use GLFW and Vulkan. The Khronos Vulkan Samples also use GLFW, although with a small framework in between.\n\nFor details on a specific Vulkan support function, see the Vulkan support reference. There are also guides for the other areas of the GLFW API.\n\nGLFW itself does not ever need to be linked against the Vulkan loader.\n\nBy default, GLFW will load the Vulkan loader dynamically at runtime via its standard name: on Windows, on Linux and other Unix-like systems and on macOS.\n\nmacOS: GLFW will also look up and search the subdirectory of your application bundle.\n\nIf your code is using a Vulkan loader with a different name or in a non-standard location you will need to direct GLFW to it. Pass your version of to glfwInitVulkanLoader before initializing GLFW and it will use that function for all Vulkan entry point retrieval. This prevents GLFW from dynamically loading the Vulkan loader.\n\nmacOS: To make your application be redistributable you will need to set up the application bundle according to the LunarG SDK documentation. This is explained in more detail in the SDK documentation for macOS.\n\nTo have GLFW include the Vulkan header, define GLFW_INCLUDE_VULKAN before including the GLFW header.\n\nIf you instead want to include the Vulkan header from a custom location or use your own custom Vulkan header then do this before the GLFW header.\n\nUnless a Vulkan header is included, either by the GLFW header or above it, the following GLFW functions will not be declared, as depend on Vulkan types.\n\nThe macros do not need to be defined for the Vulkan part of GLFW to work. Define them only if you are using these extensions directly.\n\nIf you are linking directly against the Vulkan loader then you can skip this section. The canonical desktop loader library exports all Vulkan core and Khronos extension functions, allowing them to be called directly.\n\nIf you are loading the Vulkan loader dynamically instead of linking directly against it, you can check for the availability of a loader and ICD with glfwVulkanSupported.\n\nThis function returns if the Vulkan loader and any minimally functional ICD was found.\n\nIf one or both were not found, calling any other Vulkan related GLFW function will generate a GLFW_API_UNAVAILABLE error.\n\nTo load any Vulkan core or extension function from the found loader, call glfwGetInstanceProcAddress. To load functions needed for instance creation, pass as the instance.\n\nOnce you have created an instance, you can load from it all other Vulkan core functions and functions from any instance extensions you enabled.\n\nThis function in turn calls . If that fails, the function falls back to a platform-specific query of the Vulkan loader (i.e. or ). If that also fails, the function returns . For more information about , see the Vulkan documentation.\n\nVulkan also provides for loading device-specific versions of Vulkan function. This function can be retrieved from an instance with glfwGetInstanceProcAddress.\n\nDevice-specific functions may execute a little faster, due to not having to dispatch internally based on the device passed to them. For more information about , see the Vulkan documentation.\n\nTo do anything useful with Vulkan you need to create an instance. If you want to use Vulkan to render to a window, you must enable the instance extensions GLFW requires to create Vulkan surfaces.\n\nThese extensions must all be enabled when creating instances that are going to be passed to glfwGetPhysicalDevicePresentationSupport and glfwCreateWindowSurface. The set of extensions will vary depending on platform and may also vary depending on graphics drivers and other factors.\n\nIf it fails it will return and GLFW will not be able to create Vulkan window surfaces. You can still use Vulkan for off-screen rendering and compute work.\n\nIf successful the returned array will always include , so if you don't require any additional extensions you can pass this list directly to the struct.\n\nAdditional extensions may be required by future versions of GLFW. You should check whether any extensions you wish to enable are already in the returned array, as it is an error to specify an extension more than once in the struct.\n\nmacOS: MoltenVK is (as of July 2022) not yet a fully conformant implementation of Vulkan. As of Vulkan SDK 1.3.216.0, this means you must also enable the instance extension and set the bit in the instance creation info flags for MoltenVK to show up in the list of physical devices. For more information, see the Vulkan and MoltenVK documentation.\n\nNot every queue family of every Vulkan device can present images to surfaces. To check whether a specific queue family of a physical device supports image presentation without first having to create a window and surface, call glfwGetPhysicalDevicePresentationSupport.\n\nThe extension additionally provides the function, which performs the same test on an existing Vulkan surface.\n\nUnless you will be using OpenGL or OpenGL ES with the same window as Vulkan, there is no need to create a context. You can disable context creation with the GLFW_CLIENT_API hint.\n\nSee Windows without contexts for more information.\n\nYou can create a Vulkan surface (as defined by the extension) for a GLFW window with glfwCreateWindowSurface.\n\nIf an OpenGL or OpenGL ES context was created on the window, the context has ownership of the presentation on the window and a Vulkan surface cannot be created.\n\nIt is your responsibility to destroy the surface. GLFW does not destroy it for you. Call function from the same extension to destroy it."
    },
    {
        "link": "https://glfw.org/docs/3.3/vulkan_guide.html",
        "document": "This guide is intended to fill the gaps between the official Vulkan resources and the rest of the GLFW documentation and is not a replacement for either. It assumes some familiarity with Vulkan concepts like loaders, devices, queues and surfaces and leaves it to the Vulkan documentation to explain the details of Vulkan functions.\n\nTo develop for Vulkan you should download the LunarG Vulkan SDK for your platform. Apart from headers and link libraries, they also provide the validation layers necessary for development.\n\nThe Vulkan Tutorial has more information on how to use GLFW and Vulkan. The Khronos Vulkan Samples also use GLFW, although with a small framework in between.\n\nFor details on a specific Vulkan support function, see the Vulkan support reference. There are also guides for the other areas of the GLFW API.\n\nBy default, GLFW will look for the Vulkan loader on demand at runtime via its standard name ( on Windows, on Linux and other Unix-like systems and on macOS). This means that GLFW does not need to be linked against the loader. However, it also means that if you are using the static library form of the Vulkan loader GLFW will either fail to find it or (worse) use the wrong one.\n\nThe GLFW_VULKAN_STATIC CMake option makes GLFW call the Vulkan loader directly instead of dynamically loading it at runtime. Not linking against the Vulkan loader will then be a compile-time error.\n\nmacOS: To make your application be redistributable you will need to set up the application bundle according to the LunarG SDK documentation. This is explained in more detail in the SDK documentation for macOS.\n\nTo include the Vulkan header, define GLFW_INCLUDE_VULKAN before including the GLFW header.\n\nIf you instead want to include the Vulkan header from a custom location or use your own custom Vulkan header then do this before the GLFW header.\n\nUnless a Vulkan header is included, either by the GLFW header or above it, any GLFW functions that take or return Vulkan types will not be declared.\n\nThe macros do not need to be defined for the Vulkan part of GLFW to work. Define them only if you are using these extensions directly.\n\nIf you are linking directly against the Vulkan loader then you can skip this section. The canonical desktop loader library exports all Vulkan core and Khronos extension functions, allowing them to be called directly.\n\nIf you are loading the Vulkan loader dynamically instead of linking directly against it, you can check for the availability of a loader and ICD with glfwVulkanSupported.\n\nThis function returns if the Vulkan loader and any minimally functional ICD was found.\n\nIf one or both were not found, calling any other Vulkan related GLFW function will generate a GLFW_API_UNAVAILABLE error.\n\nTo load any Vulkan core or extension function from the found loader, call glfwGetInstanceProcAddress. To load functions needed for instance creation, pass as the instance.\n\nOnce you have created an instance, you can load from it all other Vulkan core functions and functions from any instance extensions you enabled.\n\nThis function in turn calls . If that fails, the function falls back to a platform-specific query of the Vulkan loader (i.e. or ). If that also fails, the function returns . For more information about , see the Vulkan documentation.\n\nVulkan also provides for loading device-specific versions of Vulkan function. This function can be retrieved from an instance with glfwGetInstanceProcAddress.\n\nDevice-specific functions may execute a little faster, due to not having to dispatch internally based on the device passed to them. For more information about , see the Vulkan documentation.\n\nTo do anything useful with Vulkan you need to create an instance. If you want to use Vulkan to render to a window, you must enable the instance extensions GLFW requires to create Vulkan surfaces.\n\nThese extensions must all be enabled when creating instances that are going to be passed to glfwGetPhysicalDevicePresentationSupport and glfwCreateWindowSurface. The set of extensions will vary depending on platform and may also vary depending on graphics drivers and other factors.\n\nIf it fails it will return and GLFW will not be able to create Vulkan window surfaces. You can still use Vulkan for off-screen rendering and compute work.\n\nIf successful the returned array will always include , so if you don't require any additional extensions you can pass this list directly to the struct.\n\nAdditional extensions may be required by future versions of GLFW. You should check whether any extensions you wish to enable are already in the returned array, as it is an error to specify an extension more than once in the struct.\n\nmacOS: MoltenVK is (as of July 2022) not yet a fully conformant implementation of Vulkan. As of Vulkan SDK 1.3.216.0, this means you must also enable the instance extension and set the bit in the instance creation info flags for MoltenVK to show up in the list of physical devices. For more information, see the Vulkan and MoltenVK documentation.\n\nNot every queue family of every Vulkan device can present images to surfaces. To check whether a specific queue family of a physical device supports image presentation without first having to create a window and surface, call glfwGetPhysicalDevicePresentationSupport.\n\nThe extension additionally provides the function, which performs the same test on an existing Vulkan surface.\n\nUnless you will be using OpenGL or OpenGL ES with the same window as Vulkan, there is no need to create a context. You can disable context creation with the GLFW_CLIENT_API hint.\n\nSee Windows without contexts for more information.\n\nYou can create a Vulkan surface (as defined by the extension) for a GLFW window with glfwCreateWindowSurface.\n\nIf an OpenGL or OpenGL ES context was created on the window, the context has ownership of the presentation on the window and a Vulkan surface cannot be created.\n\nIt is your responsibility to destroy the surface. GLFW does not destroy it for you. Call function from the same extension to destroy it."
    },
    {
        "link": "https://glfw.org/docs/latest/input_guide.html",
        "document": "This guide introduces the input related functions of GLFW. For details on a specific function in this category, see the Input reference. There are also guides for the other areas of GLFW.\n\nGLFW provides many kinds of input. While some can only be polled, like time, or only received via callbacks, like scrolling, many provide both callbacks and polling. Callbacks are more work to use than polling but is less CPU intensive and guarantees that you do not miss state changes.\n\nAll input callbacks receive a window handle. By using the window user pointer, you can access non-global structures or objects from your callbacks.\n\nTo get a better feel for how the various events callbacks behave, run the test program. It registers every callback supported by GLFW and prints out all arguments provided for every event, along with time and sequence information.\n\nGLFW needs to poll the window system for events both to provide input to the application and to prove to the window system that the application hasn't locked up. Event processing is normally done each frame after buffer swapping. Even when you have no windows, event polling needs to be done in order to receive monitor and joystick connection events.\n\nThere are three functions for processing pending events. glfwPollEvents, processes only those events that have already been received and then returns immediately.\n\nThis is the best choice when rendering continuously, like most games do.\n\nIf you only need to update the contents of the window when you receive new input, glfwWaitEvents is a better choice.\n\nIt puts the thread to sleep until at least one event has been received and then processes all received events. This saves a great deal of CPU cycles and is useful for, for example, editing tools.\n\nIf you want to wait for events but have UI elements or other tasks that need periodic updates, glfwWaitEventsTimeout lets you specify a timeout.\n\nIt puts the thread to sleep until at least one event has been received, or until the specified number of seconds have elapsed. It then processes any received events.\n\nIf the main thread is sleeping in glfwWaitEvents, you can wake it from another thread by posting an empty event to the event queue with glfwPostEmptyEvent.\n\nDo not assume that callbacks will only be called in response to the above functions. While it is necessary to process events in one or more of the ways above, window systems that require GLFW to register callbacks of its own can pass events to GLFW in response to many window system function calls. GLFW will pass those events on to the application callbacks before returning.\n\nFor example, on Windows the system function that glfwSetWindowSize is implemented with will send window size events directly to the event callback that every window has and that GLFW implements for its windows. If you have set a window size callback GLFW will call it in turn with the new size before everything returns back out of the glfwSetWindowSize call.\n\nGLFW divides keyboard input into two categories; key events and character events. Key events relate to actual physical keyboard keys, whereas character events relate to the text that is generated by pressing some of them.\n\nKeys and characters do not map 1:1. A single key press may produce several characters, and a single character may require several keys to produce. This may not be the case on your machine, but your users are likely not all using the same keyboard layout, input method or even operating system as you.\n\nIf you wish to be notified when a physical key is pressed or released or when it repeats, set a key callback.\n\nThe callback function receives the keyboard key, platform-specific scancode, key action and modifier bits.\n\nThe action is one of , or . Events with and actions are emitted for every key press. Most keys will also emit events with actions while a key is held down.\n\nNote that many keyboards have a limit on how many keys being simultaneous held down that they can detect. This limit is called key rollover.\n\nKey events with actions are intended for text input. They are emitted at the rate set in the user's keyboard settings. At most one key is repeated even if several keys are held down. actions should not be relied on to know which keys are being held down or to drive animation. Instead you should either save the state of relevant keys based on and actions, or call glfwGetKey, which provides basic cached key state.\n\nThe key will be one of the existing key tokens, or if GLFW lacks a token for it, for example E-mail and Play keys.\n\nThe scancode is unique for every key, regardless of whether it has a key token. Scancodes are platform-specific but consistent over time, so keys will have different scancodes depending on the platform but they are safe to save to disk. You can query the scancode for any key token supported on the current platform with glfwGetKeyScancode.\n\nThe last reported state for every physical key with a key token is also saved in per-window state arrays that can be polled with glfwGetKey.\n\nThe returned state is one of or .\n\nThis function only returns cached key event state. It does not poll the system for the current state of the physical key. It also does not provide any key repeat information.\n\nWhenever you poll state, you risk missing the state change you are looking for. If a pressed key is released again before you poll its state, you will have missed the key press. The recommended solution for this is to use a key callback, but there is also the input mode.\n\nWhen sticky keys mode is enabled, the pollable state of a key will remain until the state of that key is polled with glfwGetKey. Once it has been polled, if a key release event had been processed in the meantime, the state will reset to , otherwise it will remain .\n\nIf you wish to know what the state of the Caps Lock and Num Lock keys was when input events were generated, set the input mode.\n\nWhen this input mode is enabled, any callback that receives modifier bits will have the GLFW_MOD_CAPS_LOCK bit set if Caps Lock was on when the event occurred and the GLFW_MOD_NUM_LOCK bit set if Num Lock was on.\n\nThe constant holds the highest value of any key token.\n\nGLFW supports text input in the form of a stream of Unicode code points, as produced by the operating system text input system. Unlike key input, text input is affected by keyboard layouts and modifier keys and supports composing characters using dead keys. Once received, you can encode the code points into UTF-8 or any other encoding you prefer.\n\nBecause an is 32 bits long on all platforms supported by GLFW, you can treat the code point argument as native endian UTF-32.\n\nIf you wish to offer regular text input, set a character callback.\n\nThe callback function receives Unicode code points for key events that would have led to regular text input and generally behaves as a standard text field on that platform.\n\nIf you wish to refer to keys by name, you can query the keyboard layout dependent name of printable keys with glfwGetKeyName.\n\nThis function can handle both keys and scancodes. If the specified key is then the scancode is used, otherwise it is ignored. This matches the behavior of the key callback, meaning the callback arguments can always be passed unmodified to this function.\n\nMouse input comes in many forms, including mouse motion, button presses and scrolling offsets. The cursor appearance can also be changed, either to a custom image or a standard cursor shape from the system theme.\n\nIf you wish to be notified when the cursor moves over the window, set a cursor position callback.\n\nThe callback functions receives the cursor position, measured in screen coordinates but relative to the top-left corner of the window content area. On platforms that provide it, the full sub-pixel cursor position is passed on.\n\nThe cursor position is also saved per-window and can be polled with glfwGetCursorPos.\n\nThe input mode provides several cursor modes for special forms of mouse motion input. By default, the cursor mode is , meaning the regular arrow cursor (or another cursor set with glfwSetCursor) is used and cursor motion is not limited.\n\nIf you wish to implement mouse motion based camera controls or other input schemes that require unlimited mouse movement, set the cursor mode to .\n\nThis will hide the cursor and lock it to the specified window. GLFW will then take care of all the details of cursor re-centering and offset calculation and providing the application with a virtual cursor position. This virtual position is provided normally via both the cursor position callback and through polling.\n\nIf you only wish the cursor to become hidden when it is over a window but still want it to behave normally, set the cursor mode to .\n\nThis mode puts no limit on the motion of the cursor.\n\nIf you wish the cursor to be visible but confined to the content area of the window, set the cursor mode to .\n\nThe cursor will behave normally inside the content area but will not be able to leave unless the window loses focus.\n\nTo exit out of either of these special modes, restore the cursor mode.\n\nIf the cursor was disabled, this will move it back to its last visible position.\n\nWhen the cursor is disabled, raw (unscaled and unaccelerated) mouse motion can be enabled if available.\n\nRaw mouse motion is closer to the actual motion of the mouse across a surface. It is not affected by the scaling and acceleration applied to the motion of the desktop cursor. That processing is suitable for a cursor while raw motion is better for controlling for example a 3D camera. Because of this, raw mouse motion is only provided when the cursor is disabled.\n\nCall glfwRawMouseMotionSupported to check if the current machine provides raw motion and set the input mode to enable it. It is disabled by default.\n\nIf supported, raw mouse motion can be enabled or disabled per-window and at any time but it will only be provided when the cursor is disabled.\n\nGLFW supports creating both custom and system theme cursor images, encapsulated as GLFWcursor objects. They are created with glfwCreateCursor or glfwCreateStandardCursor and destroyed with glfwDestroyCursor, or glfwTerminate, if any remain.\n\nA custom cursor is created with glfwCreateCursor, which returns a handle to the created cursor object. For example, this creates a 16x16 white square cursor with the hot-spot in the upper-left corner:\n\nIf cursor creation fails, will be returned, so it is necessary to check the return value.\n\nThe image data is 32-bit, little-endian, non-premultiplied RGBA, i.e. eight bits per channel with the red channel first. The pixels are arranged canonically as sequential rows, starting from the top-left corner.\n\nA cursor with a standard shape from the current system cursor theme can be created with glfwCreateStandardCursor.\n\nThese cursor objects behave in the exact same way as those created with glfwCreateCursor except that the system cursor theme provides the actual image.\n\nA few of these shapes are not available everywhere. If a shape is unavailable, is returned. See glfwCreateStandardCursor for details.\n\nWhen a cursor is no longer needed, destroy it with glfwDestroyCursor.\n\nCursor destruction always succeeds. If the cursor is current for any window, that window will revert to the default cursor. This does not affect the cursor mode. All remaining cursors are destroyed when glfwTerminate is called.\n\nA cursor can be set as current for a window with glfwSetCursor.\n\nOnce set, the cursor image will be used as long as the system cursor is over the content area of the window and the cursor mode is set to .\n\nA single cursor may be set for any number of windows.\n\nTo revert to the default cursor, set the cursor of that window to .\n\nWhen a cursor is destroyed, any window that has it set will revert to the default cursor. This does not affect the cursor mode.\n\nIf you wish to be notified when the cursor enters or leaves the content area of a window, set a cursor enter/leave callback.\n\nThe callback function receives the new classification of the cursor.\n\nYou can query whether the cursor is currently inside the content area of the window with the GLFW_HOVERED window attribute.\n\nIf you wish to be notified when a mouse button is pressed or released, set a mouse button callback.\n\nThe callback function receives the mouse button, button action and modifier bits.\n\nThe action is one of or .\n\nThe last reported state for every supported mouse button is also saved in per-window state arrays that can be polled with glfwGetMouseButton.\n\nThe returned state is one of or .\n\nThis function only returns cached mouse button event state. It does not poll the system for the current state of the mouse button.\n\nWhenever you poll state, you risk missing the state change you are looking for. If a pressed mouse button is released again before you poll its state, you will have missed the button press. The recommended solution for this is to use a mouse button callback, but there is also the input mode.\n\nWhen sticky mouse buttons mode is enabled, the pollable state of a mouse button will remain until the state of that button is polled with glfwGetMouseButton. Once it has been polled, if a mouse button release event had been processed in the meantime, the state will reset to , otherwise it will remain .\n\nThe constant holds the highest value of any supported mouse button.\n\nIf you wish to be notified when the user scrolls, whether with a mouse wheel or touchpad gesture, set a scroll callback.\n\nA normal mouse wheel, being vertical, provides offsets along the Y-axis.\n\nThe joystick functions expose connected joysticks and controllers, with both referred to as joysticks. It supports up to sixteen joysticks, ranging from , up to and including or . You can test whether a joystick is present with glfwJoystickPresent.\n\nEach joystick has zero or more axes, zero or more buttons, zero or more hats, a human-readable name, a user pointer and an SDL compatible GUID.\n\nDetected joysticks are added to the beginning of the array. Once a joystick is detected, it keeps its assigned ID until it is disconnected or the library is terminated, so as joysticks are connected and disconnected, there may appear gaps in the IDs.\n\nJoystick axis, button and hat state is updated when polled and does not require a window to be created or events to be processed. However, if you want joystick connection and disconnection events reliably delivered to the joystick callback then you must process events.\n\nTo see all the properties of all connected joysticks in real-time, run the test program.\n\nThe positions of all axes of a joystick are returned by glfwGetJoystickAxes. See the reference documentation for the lifetime of the returned array.\n\nEach element in the returned array is a value between -1.0 and 1.0.\n\nThe states of all buttons of a joystick are returned by glfwGetJoystickButtons. See the reference documentation for the lifetime of the returned array.\n\nEach element in the returned array is either or .\n\nFor backward compatibility with earlier versions that did not have glfwGetJoystickHats, the button array by default also includes all hats. See the reference documentation for glfwGetJoystickButtons for details.\n\nThe states of all hats are returned by glfwGetJoystickHats. See the reference documentation for the lifetime of the returned array.\n\nEach element in the returned array is one of the following:\n\nThe diagonal directions are bitwise combinations of the primary (up, right, down and left) directions and you can test for these individually by ANDing it with the corresponding direction.\n\nFor backward compatibility with earlier versions that did not have glfwGetJoystickHats, all hats are by default also included in the button array. See the reference documentation for glfwGetJoystickButtons for details.\n\nThe human-readable, UTF-8 encoded name of a joystick is returned by glfwGetJoystickName. See the reference documentation for the lifetime of the returned string.\n\nJoystick names are not guaranteed to be unique. Two joysticks of the same model and make may have the same name. Only the joystick ID is guaranteed to be unique, and only until that joystick is disconnected.\n\nEach joystick has a user pointer that can be set with glfwSetJoystickUserPointer and queried with glfwGetJoystickUserPointer. This can be used for any purpose you need and will not be modified by GLFW. The value will be kept until the joystick is disconnected or until the library is terminated.\n\nThe initial value of the pointer is .\n\nIf you wish to be notified when a joystick is connected or disconnected, set a joystick callback.\n\nThe callback function receives the ID of the joystick that has been connected and disconnected and the event that occurred.\n\nFor joystick connection and disconnection events to be delivered on all platforms, you need to call one of the event processing functions. Joystick disconnection may also be detected and the callback called by joystick functions. The function will then return whatever it returns for a disconnected joystick.\n\nOnly glfwGetJoystickName and glfwGetJoystickUserPointer will return useful values for a disconnected joystick and only before the monitor callback returns.\n\nThe joystick functions provide unlabeled axes, buttons and hats, with no indication of where they are located on the device. Their order may also vary between platforms even with the same device.\n\nTo solve this problem the SDL community crowdsourced the SDL_GameControllerDB project, a database of mappings from many different devices to an Xbox-like gamepad.\n\nGLFW supports this mapping format and contains a copy of the mappings available at the time of release. See Gamepad mappings for how to update this at runtime. Mappings will be assigned to joysticks automatically any time a joystick is connected or the mappings are updated.\n\nYou can check whether a joystick is both present and has a gamepad mapping with glfwJoystickIsGamepad.\n\nIf you are only interested in gamepad input you can use this function instead of glfwJoystickPresent.\n\nYou can query the human-readable name provided by the gamepad mapping with glfwGetGamepadName. This may or may not be the same as the joystick name.\n\nTo retrieve the gamepad state of a joystick, call glfwGetGamepadState.\n\nThe GLFWgamepadstate struct has two arrays; one for button states and one for axis states. The values for each button and axis are the same as for the glfwGetJoystickButtons and glfwGetJoystickAxes functions, i.e. or for buttons and -1.0 to 1.0 inclusive for axes.\n\nThe sizes of the arrays and the positions within each array are fixed.\n\nThe button indices are , , , , , , , , , , , , , and .\n\nFor those who prefer, there are also the , , and aliases for the A, B, X and Y button indices.\n\nThe axis indices are , , , , and .\n\nThe and constants equal the largest available index for each array.\n\nGLFW contains a copy of the mappings available in SDL_GameControllerDB at the time of release. Newer ones can be added at runtime with glfwUpdateGamepadMappings.\n\nThis function supports everything from single lines up to and including the unmodified contents of the whole file.\n\nIf you are compiling GLFW from source with CMake you can update the built-in mappings by building the update_mappings target. This runs the CMake script, which downloads and regenerates the header file.\n\nBelow is a description of the mapping format. Please keep in mind that this description is not authoritative. The format is defined by the SDL and SDL_GameControllerDB projects and their documentation and code takes precedence.\n\nEach mapping is a single line of comma-separated values describing the GUID, name and layout of the gamepad. Lines that do not begin with a hexadecimal digit are ignored.\n\nThe first value is always the gamepad GUID, a 32 character long hexadecimal string that typically identifies its make, model, revision and the type of connection to the computer. When this information is not available, the GUID is generated using the gamepad name. GLFW uses the SDL 2.0.5+ GUID format but can convert from the older formats.\n\nThe second value is always the human-readable name of the gamepad.\n\nAll subsequent values are in the form and describe the layout of the mapping. These fields may not all be present and may occur in any order.\n\nThe button fields are , , , , , , , , , , , , , and .\n\nThe axis fields are , , , , and .\n\nThe value of an axis or button field can be a joystick button, a joystick axis, a hat bitmask or empty. Joystick buttons are specified as , for example for the third button. Joystick axes are specified as , for example for the eighth button. Joystick hat bit masks are specified as , for example for left on the first hat. More than one bit may be set in the mask.\n\nBefore an axis there may be a or range modifier, for example for the positive half of the fourth axis. This restricts input to only the positive or negative halves of the joystick axis. After an axis or half-axis there may be the inversion modifier, for example or . This negates the values of the gamepad axis.\n\nThe hat bit mask match the hat states in the joystick functions.\n\nThere is also the special field that specifies which platform the mapping is valid for. Possible values are , and .\n\nBelow is an example of what a gamepad mapping might look like. It is the one built into GLFW for Xbox controllers accessed via the XInput API on Windows. This example has been broken into several lines to fit on the page, but real gamepad mappings must be a single line.\n\nGLFW provides high-resolution time input, in seconds, with glfwGetTime.\n\nIt returns the number of seconds since the library was initialized with glfwInit. The platform-specific time sources used typically have micro- or nanosecond resolution.\n\nYou can modify the base time with glfwSetTime.\n\nThis sets the time to the specified time, in seconds, and it continues to count from there.\n\nYou can also access the raw timer used to implement the functions above, with glfwGetTimerValue.\n\nThis value is in 1 / frequency seconds. The frequency of the raw timer varies depending on the operating system and hardware. You can query the frequency, in Hz, with glfwGetTimerFrequency.\n\nIf the system clipboard contains a UTF-8 encoded string or if it can be converted to one, you can retrieve it with glfwGetClipboardString. See the reference documentation for the lifetime of the returned string.\n\nIf the clipboard is empty or if its contents could not be converted, is returned.\n\nThe contents of the system clipboard can be set to a UTF-8 encoded string with glfwSetClipboardString.\n\nIf you wish to receive the paths of files and/or directories dropped on a window, set a file drop callback.\n\nThe callback function receives an array of paths encoded as UTF-8.\n\nThe path array and its strings are only valid until the file drop callback returns, as they may have been generated specifically for that event. You need to make a deep copy of the array if you want to keep the paths."
    },
    {
        "link": "https://glfw.org/docs/3.3",
        "document": "GLFW is a free, Open Source, multi-platform library for OpenGL, OpenGL ES and Vulkan application development. It provides a simple, platform-independent API for creating windows, contexts and surfaces, reading input, handling events, etc.\n\nRelease notes for version 3.3 list new features, caveats and deprecations.\n\nGetting started is a guide for users new to GLFW. It takes you through how to write a small but complete program.\n\nThere are guides for each section of the API:\n• Introduction to the API – initialization, error handling and high-level design\n• Window guide – creating and working with windows and framebuffers\n• Monitor guide – enumerating and working with monitors and video modes\n\nOnce you have written a program, see Compiling GLFW and Building applications.\n\nThe reference documentation provides more detailed information about specific functions.\n\nMoving from GLFW 2 to 3 explains what has changed and how to update existing code to use the new API.\n\nThere is a section on Guarantees and limitations for pointer lifetimes, reentrancy, thread safety, event order and backward and forward compatibility.\n\nThe FAQ answers many common questions about the design, implementation and use of GLFW.\n\nFinally, Standards conformance explains what APIs, standards and protocols GLFW uses and what happens when they are not present on a given machine.\n\nThis documentation was generated with Doxygen. The sources for it are available in both the source distribution and GitHub repository."
    },
    {
        "link": "https://glfw.org/docs/latest",
        "document": "GLFW is a free, Open Source, multi-platform library for OpenGL, OpenGL ES and Vulkan application development. It provides a simple, platform-independent API for creating windows, contexts and surfaces, reading input, handling events, etc.\n\nRelease notes for version 3.4 list new features, caveats and deprecations.\n\nGetting started is a guide for users new to GLFW. It takes you through how to write a small but complete program.\n\nThere are guides for each section of the API:\n• Introduction to the API – initialization, error handling and high-level design\n• Window guide – creating and working with windows and framebuffers\n• Monitor guide – enumerating and working with monitors and video modes\n\nOnce you have written a program, see Compiling GLFW and Building applications.\n\nThe reference documentation provides more detailed information about specific functions.\n\nMoving from GLFW 2 to 3 explains what has changed and how to update existing code to use the new API.\n\nThere is a section on Guarantees and limitations for pointer lifetimes, reentrancy, thread safety, event order and backward and forward compatibility.\n\nFinally, Standards conformance explains what APIs, standards and protocols GLFW uses and what happens when they are not present on a given machine.\n\nThis documentation was generated with Doxygen. The sources for it are available in both the source distribution and GitHub repository."
    }
]