[
    {
        "link": "https://scikit-learn.org/stable/modules/grid_search.html",
        "document": "Tuning the hyper-parameters of an estimator#\n\nHyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include , and for Support Vector Classifier, for Lasso, etc.\n\nIt is possible and recommended to search the hyper-parameter space for the best cross validation score.\n\nAny parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, use:\n• None an estimator (regressor or classifier such as );\n\nTwo generic approaches to parameter search are provided in scikit-learn: for given values, exhaustively considers all parameter combinations, while can sample a given number of candidates from a parameter space with a specified distribution. Both these tools have successive halving counterparts and , which can be much faster at finding a good parameter combination.\n\nAfter describing these tools we detail best practices applicable to these approaches. Some models allow for specialized, efficient parameter search strategies, outlined in Alternatives to brute force parameter search.\n\nNote that it is common that a small subset of those parameters can have a large impact on the predictive or computation performance of the model while others can be left to their default values. It is recommended to read the docstring of the estimator class to get a finer understanding of their expected behavior, possibly by reading the enclosed reference to the literature.\n\nWhile using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favorable properties. implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n• None A budget can be chosen independent of the number of parameters and possible values.\n• None Adding parameters that do not influence the performance does not decrease efficiency. Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for . Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified: This example uses the module, which contains many useful distributions for sampling parameters, such as , , , or . In principle, any function can be passed that provides a (random variate sample) method to sample a value. A call to the function should provide independent random samples from possible parameter values on consecutive calls. The distributions in prior to version scipy 0.16 do not allow specifying a random state. Instead, they use the global numpy random state, that can be seeded via or set using . However, beginning scikit-learn 0.18, the module sets the random state provided by the user if scipy >= 0.16 is also available. For continuous parameters, such as above, it is important to specify a continuous distribution to take full advantage of the randomization. This way, increasing will always lead to a finer search. A continuous log-uniform random variable is the continuous version of a log-spaced parameter. For example to specify the equivalent of from above, can be used instead of . Mirroring the example above in grid search, we can specify a continuous random variable that is log-uniformly distributed between and :\n• None Comparing randomized search and grid search for hyperparameter estimation compares the usage and efficiency of randomized search and grid search.\n• None Bergstra, J. and Bengio, Y., Random search for hyper-parameter optimization, The Journal of Machine Learning Research (2012)\n\nScikit-learn also provides the and estimators that can be used to search a parameter space using successive halving . Successive halving (SH) is like a tournament among candidate parameter combinations. SH is an iterative selection process where all candidates (the parameter combinations) are evaluated with a small amount of resources at the first iteration. Only some of these candidates are selected for the next iteration, which will be allocated more resources. For parameter tuning, the resource is typically the number of training samples, but it can also be an arbitrary numeric parameter such as in a random forest. The resource increase chosen should be large enough so that a large improvement in scores is obtained when taking into account statistical significance. As illustrated in the figure below, only a subset of candidates ‘survive’ until the last iteration. These are the candidates that have consistently ranked among the top-scoring candidates across all iterations. Each iteration is allocated an increasing amount of resources per candidate, here the number of samples. We here briefly describe the main parameters, but each parameter and their interactions are described more in detail in the dropdown section below. The (> 1) parameter controls the rate at which the resources grow, and the rate at which the number of candidates decreases. In each iteration, the number of resources per candidate is multiplied by and the number of candidates is divided by the same factor. Along with and , is the most important parameter to control the search in our implementation, though a value of 3 usually works well. effectively controls the number of iterations in and the number of candidates (by default) and iterations in . can also be used if the number of available resources is small. More control is available through tuning the parameter. These estimators are still experimental: their predictions and their API might change without any deprecation cycle. To use them, you need to explicitly import : The sections below dive into technical aspects of successive halving. By default, the resource is defined in terms of number of samples. That is, each iteration will use an increasing amount of samples to train on. You can however manually specify a parameter to use as the resource with the parameter. Here is an example where the resource is defined in terms of the number of estimators of a random forest: Note that it is not possible to budget on a parameter that is part of the parameter grid. As mentioned above, the number of resources that is used at each iteration depends on the parameter. If you have a lot of resources available but start with a low number of resources, some of them might be wasted (i.e. not used): The search process will only use 80 resources at most, while our maximum amount of available resources is . Here, we have . For , by default, the parameter is set to ‘exhaust’. This means that is automatically set such that the last iteration can use as many resources as possible, within the limit: was here automatically set to 250, which results in the last iteration using all the resources. The exact value that is used depends on the number of candidate parameter, on and on . For , exhausting the resources can be done in 2 ways:\n• None by setting , just like for ; Both options are mutually exclusive: using requires knowing the number of candidates, and symmetrically requires knowing . In general, exhausting the total number of resources leads to a better final candidate parameter, and is slightly more time-intensive. The attribute contains useful information for analyzing the results of a search. It can be converted to a pandas dataframe with . The attribute of and is similar to that of and , with additional information related to the successive halving process. Each row corresponds to a given parameter combination (a candidate) and a given iteration. The iteration is given by the column. The column tells you how many resources were used. In the example above, the best parameter combination is since it has reached the last iteration (3) with the highest score: 0.96.\n\nBy default, parameter search uses the function of the estimator to evaluate a parameter setting. These are the for classification and for regression. For some applications, other scoring functions are better suited (for example in unbalanced classification, the accuracy score is often uninformative). An alternative scoring function can be specified via the parameter of most parameter search tools. See The scoring parameter: defining model evaluation rules for more details. and allow specifying multiple metrics for the parameter. Multimetric scoring can either be specified as a list of strings of predefined scores names or a dict mapping the scorer name to the scorer function and/or the predefined scorer name(s). See Using multiple metric evaluation for more details. When specifying multiple metrics, the parameter must be set to the metric (string) for which the will be found and used to build the on the whole dataset. If the search should not be refit, set . Leaving refit to the default value will result in an error when using multiple metrics. See Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV for an example usage. and do not support multimetric scoring. and allow searching over parameters of composite or nested estimators such as , , or using a dedicated syntax: Here, is the parameter name of the nested estimator, in this case . If the meta-estimator is constructed as a collection of estimators as in , then refers to the name of the estimator, see Access to nested parameters. In practice, there can be several levels of nesting: Please refer to Pipeline: chaining estimators for performing parameter searches over pipelines. Model selection by evaluating various parameter settings can be seen as a way to use the labeled data to “train” the parameters of the grid. When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process: it is recommended to split the data into a development set (to be fed to the instance) and an evaluation set to compute performance metrics. This can be done by using the utility function. The parameter search tools evaluate each parameter combination on each data fold independently. Computations can be run in parallel by using the keyword . See function signature for more details, and also the Glossary entry for n_jobs. Some parameter settings may result in a failure to one or more folds of the data. By default, the score for those settings will be . This can be controlled by setting to raise an exception if one fit fails, or for example to set another value for the score of failing parameter combinations."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html",
        "document": "Exhaustive search over specified parameter values for an estimator.\n\nGridSearchCV implements a “fit” and a “score” method. It also implements “score_samples”, “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n\nThe parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.\n\nRead more in the User Guide.\n\nThis is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a function, or must be passed. Dictionary with parameters names ( ) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings. Strategy to evaluate the performance of the cross-validated model on the test set. If represents a single score, one can use:\n• None a callable (see Callable scorers) that returns a single value. If represents multiple scores, one can use:\n• None a callable returning a dictionary where the keys are the metric names and the values are the metric scores;\n• None a dictionary with metric names as keys and callables as values. See Specifying multiple metrics for evaluation for an example. Number of jobs to run in parallel. means 1 unless in a context. means using all processors. See Glossary for more details. Changed in version v0.20: default changed from 1 to None Refit an estimator using the best found parameters on the whole dataset. For multiple metric evaluation, this needs to be a denoting the scorer that would be used to find the best parameters for refitting the estimator at the end. Where there are considerations other than maximum score in choosing a best estimator, can be set to a function which returns the selected given . In that case, the and will be set according to the returned while the attribute will not be available. The refitted estimator is made available at the attribute and permits using directly on this instance. Also for multiple metric evaluation, the attributes , and will only be available if is set and all of them will be determined w.r.t this specific scorer. See parameter to know more about multiple metric evaluation. See Custom refit strategy of a grid search with cross-validation to see how to design a custom selection strategy using a callable via . Determines the cross-validation splitting strategy. Possible inputs for cv are:\n• None None, to use the default 5-fold cross validation,\n• None integer, to specify the number of folds in a ,\n• None An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if the estimator is a classifier and is either binary or multiclass, is used. In all other cases, is used. These splitters are instantiated with so the splits will be the same across calls. Refer User Guide for the various cross-validation strategies that can be used here. Changed in version 0.22: default value if None changed from 3-fold to 5-fold. Controls the verbosity: the higher, the more messages.\n• None >1 : the computation time for each fold and parameter candidate is displayed;\n• None >2 : the score is also displayed;\n• None >3 : the fold and candidate parameter indexes are also displayed together with the starting time of the computation. Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:\n• None None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs\n• None An int, giving the exact number of total jobs that are spawned\n• None A str, giving an expression as a function of n_jobs, as in ‘2*n_jobs’ Value to assign to the score if an error occurs in estimator fitting. If set to ‘raise’, the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. If , the attribute will not include training scores. Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off. However computing the scores on the training set can be computationally expensive and is not strictly required to select the parameters that yield the best generalization performance. Changed in version 0.21: Default value was changed from to A dict with keys as column headers and values as columns, that can be imported into a pandas . For instance the below given table will be represented by a dict of: The key is used to store a list of parameter settings dicts for all the parameter candidates. The , , and are all in seconds. For multi-metric evaluation, the scores for all the scorers are available in the dict at the keys ending with that scorer’s name ( ) instead of shown above. (‘split0_test_precision’, ‘mean_train_precision’ etc.) Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if . See parameter for more information on allowed values. Mean cross-validated score of the best_estimator For multi-metric evaluation, this is present only if is specified. This attribute is not available if is a function. Parameter setting that gave the best results on the hold out data. For multi-metric evaluation, this is present only if is specified. The index (of the arrays) which corresponds to the best candidate parameter setting. The dict at gives the parameter setting for the best model, that gives the highest mean score ( ). For multi-metric evaluation, this is present only if is specified. Scorer function used on the held out data to choose the best parameters for the model. For multi-metric evaluation, this attribute holds the validated dict which maps the scorer key to the scorer callable. Seconds used for refitting the best model on the whole dataset. This is present only if is not False. Whether or not the scorers compute several metrics. Number of features seen during fit. Names of features seen during fit. Only defined if is defined (see the documentation for the parameter for more details) and that exposes when fit.\n\nThe parameters selected are those that maximize the score of the left out data, unless an explicit score is passed in which case it is used instead.\n\nIf was set to a value higher than one, the data is copied for each point in the grid (and not times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available. A workaround in this case is to set . Then, the memory is copied only many times. A reasonable value for is ."
    },
    {
        "link": "https://analyticsvidhya.com/blog/2021/06/tune-hyperparameters-with-gridsearchcv",
        "document": "business problem, the professional never rely on one algorithm. One always applies multiple relevant algorithms based on the problem and selects the best model based on the best performance metrics shown by the models. But this is not the end. One can increase the model performance using hyperparameters. Thus, finding the optimal hyperparameters would help us achieve the best-performing model. There are several techniques for choosing a model’s hyperparameters, including Random Search, sklearn’s GridSearchCV, Manual Search, and Bayesian Optimization. Among these, gridsearchcv is widely recognized for its efficiency in tuning parameters. In this article, we will learn about GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n\nThis article was published as a part of the Data Science Blogathon\n\nGridSearchCV acts as a valuable tool for identifying the optimal parameters for a machine learning model. Imagine you have a machine learning model with adjustable settings, known as hyperparameters, that can enhance its performance. GridSearchCV aids in pinpointing the best combination of these hyperparameters automatically.\n\nHere’s the process: You provide GridSearchCV with a set of Scoring parameter to experiment with, and it systematically explores each possible combination. For every combination, it evaluates the model’s performance by testing it on various sections of the dataset to gauge its accuracy.\n\nAfter exhaustively trying out all the combinations, GridSearchCV presents you with the combination of settings that yielded the most favorable outcomes. This streamlines the process of fine-tuning your model, ensuring it operates optimally for your specific task without incurring excessive computational expenses.In this article we have thoroughly exapling about the grid search cv and how grid search cv in machine learning works . what is grid search. so you will get a full understanding of this tutorial.\n\nParameters and Hyper parameters both are associated with the Machine Learning model, but both are meant for different tasks. Let’s understand how they are different from each other in the context of Machine Learning.\n\nParameters are the variables that are used by the Machine Learning algorithm for predicting the results based on the input historic data. These are estimated by using an optimization algorithm by the Machine Learning algorithm itself. Thus, these variables are not set or hardcoded by the user or professional. These variables are served as a part of model training. Example of best Parameters: Coefficient of independent variables Linear Regression and Logistic Regression.\n\nHyperparameters are the variables that the user specifies, usually when building the Machine Learning model. They are distinct from parameters and play a crucial role in determining the model’s performance. Hyperparameters are chosen prior to defining the parameters, and they are instrumental in finding the optimal parameter combinations. One common approach to finding the best hyperparameters is through methods like grid search, where a parameter grid is defined, and various combinations of Specified parameter values are evaluated against a specified evaluation metric. The beauty of hyperparameters lies in the user’s ability to tailor them to the specific needs of the model being built. For instance, in Random Forest Algorithms, the user might adjust the max_depth hyperparameter, or in a KNN Classifier, the k hyperparameter can be tuned to enhance performance.\n\nNow we know what hyperparameters are the multiple combinations, our goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n\nFor this reason, methods like Random Search, GridSearch were introduced. Here, we will discuss how Grid Seach is performed and how it is executed with cross-validation in GridSearchCV.\n\nGrid Search employs an exhaustive search strategy, systematically exploring various combinations of specified hyperparameters and their Default values. This approach involves tuning parameters, such as learning rate, through a cross-validated model, which assesses performance across different parameter settings. However, due to its exhaustive nature, Grid Search can become time-consuming and resource-intensive, particularly as the number of hyperparameters increases.\n\nGrid Search across Two Parameters (Image by Alexander Elvers from WikiMedia)\n\nIn GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model. As we know that before training the model with data, we divide the data into two parts – train data and test data. In cross-validation, the process divides the train data further into two parts – the train data and the validation data.\n\nThe most popular type of Cross-validation is K-fold Cross-Validation. It is an iterative process that divides the train data into k partitions. Each iteration keeps one partition for testing and the remaining k-1 partitions for training the model. The next iteration will set the next partition as test data and the remaining k-1 as train data and so on. In each iteration, it will record the performance of the model and at the end give the average of all the performance. Thus, it is also a time-consuming process.\n\nThus, GridSearch along with cross-validation takes huge time cumulatively to evaluate the best hyperparameters. Now we will see how to use GridSearchCV in our Machine Learning problem.\n\nRead More about the Hyperparameter Optimization Techniques\n\n.GridSearchCV() method is available in the scikit-learn class model_selection. It can be initiated by creating an object of GridSearchCV():\n\nPrimarily, it takes 4 arguments i.e. estimator, param_grid, cv, and scoring. The description of the arguments is as follows:\n• param_grid – A dictionary with parameter names as keys and lists of parameter values.\n• scoring – The performance measure. For example, ‘r2’ for regression models, ‘precision’ for classification models.\n• cv – An integer that is the number of folds for K-fold cross-validation.\n\nGridSearchCV can be used on several hyperparameters to get the best values for the specified hyperparameters.\n\nAlso, Read about the 5 Hyperparameter Optimization Techniques\n\nHere we are going to use the HeartDiseaseUCI dataset.\n\nSplitting the data into train and test set\n\nHere, we created the object rfc of RandomForestClassifier().\n\nInitializing GridSearchCV() object and fitting it with hyperparameters\n\nHere, we passed the estimator object rfc, param_grid as forest_params, cv = 5 and scoring method as accuracy in to GridSearchCV() as arguments.\n\nGetting the Best Hyperparameters\n\nThis will give the combination of hyperparameters along with values that give the best performance of our estimate specified.\n\nPutting it all together\n\nOn executing the above code, we get:\n\nBest Params and Best Score of the Random Forest Classifier\n\nThus, clf.best_params_ gives the best combination of tuned hyperparameters, and clf.best_score_ gives the average cross-validated score of our Random Forest Classifier.\n\nGridSearchCV is a tool from the scikit-learn library used for hyperparameter tuning in machine learning. It essentially automates the process of finding the optimal combination of hyperparameters for a given machine learning model.\n\nHere is How it Works:\n\nHyperparameters refer to configurations in a machine learning model that manage how it learns. Instances could be the quantity of trees in a haphazard forest or the pace of learning in a support vector machine. Hyperparameters are determined before training, while model parameters are learned from data.\n• Grid Search: GridSearchCV methodically explores various combinations of hyperparameter values within a predetermined grid. This grid establishes the potential values for each hyperparameter.\n• Cross-Validation is used by GridSearchCV to assess the performance of each combination of hyperparameters. The data is divided into folds, the model is trained on certain folds using a particular hyperparameter configuration, and tested on the rest of the folds. All folds and hyperparameter combinations undergo this process again.\n\nThus, in this article, we learned about Grid Search, K-fold Cross-Validation, Grid Search CV, and how to make good use of Grid Search CV. Grid search cv is a crucial model selection step that should be performed after Data Processing tasks. Leveraging sklearn grid search cv ensures that the grid search is exhaustive, providing the best possible model performance. It is always good to compare the performances of Tuned and Untuned Models. This will cost us the time and expense but will surely give us the best results. The scikit-learn API is a great resource in case of any help. It’s always good to learn by doing.\n\nHope you will find this explanation helpful! Grid Search CV is a powerful tool in scikit-learn for hyperparameter tuning, particularly with models like RandomForestClassifier. It systematically searches for optimal parameters, enhancing performance through effective cross-validation (CV) in Random Forest hyperparameter tuning.\n\nWe you read about the article majorly you get about the grid search hyperparameter tuning and how it being used and its being classified by the grid search and its impact realible on grid search , also collapse that grid search cv in machine learning plays and important role in it."
    },
    {
        "link": "https://medium.com/dvt-engineering/hyper-parameter-tuning-for-scikit-learn-ml-models-860747bc3d72",
        "document": "A frequent obstacle with putting together machine learning models is how to tweak them to be just right. ML algorithms can always be tweaked to alter their behaviour. A good example of this is the regularization parameter C in an SVM model. Higher values of C incentivize the model to fit the training data more tightly, but lead to overfitting if you make it too high (the noisier the data, the worse it gets).\n\nLower values are more tolerant to noisy data and make the SVM favour a simpler decision boundary, which may cause your model to generalize better to data it hasn’t seen before. Of course, there’s a catch. If C is too low, your model could underfit, and not perform well on the training set or the test set.\n\nFor the uninitiated, these kinds of parameters are called hyper-parameters for machine learning models. Hyper-parameters influence model performance, but are not learned as part of the training process for the model. So, how do we pick the best ones? How do we optimize our models? That’s what this post is all about.\n\nClone the Python project of this blog at https://github.com/rdbruyn/dvt-optimise if you’d like, and let’s get into it.\n\nSay we want to optimize an SVM model, with a proper value of C. A naive approach may be to use inspection; manually set a few values of C, and see how it goes. We can try 0.5, 1, and 2. That’s not so bad, right? We’ll have to train the model 3 times. Not too much of a hassle. That’s only one of the hyper-parameters we have available to us, however. There are other ones, like kernel type and tolerance. The kernel dictates what kind of decision boundary the SVM trains, and the tolerance tells the model when it’s done training. Let’s say we want to optimize these hyper-parameters along with C. We want to try 6 values of C, 4 different kernels, and 2 values of tolerance. That’s 6x4x2=48 combinations to try. Somewhat more cumbersome, yes? Fortunately, scikit-learn comes with a way to check model performance with different hyper-parameters, called . It’s an exhaustive grid search algorithm, which means that it tries all combinations, and lets you know how it went. In a Python script called , let’s do the following:\n\nIn the code above, we use scikit-learn’s GridSearchCV class to do the heavy lifting for us. The argument is a dictionary with all the hyper-parameters we want to try (we’ll get to that in a second). The object just tries all the combinations and keeps the results, so we can see which parameters do best. It does this by using crossvalidation. In a nutshell, crossvalidation is a way of kinda seeing how well a model performs, without actually involving the test set.\n\nIn our case, we’re using 5-fold crossvalidation ( ). So the model is trained 5 times, and the training data set is divided into 5 splits. Every iteration the model trains is on 4/5 splits. Then, the “test score” is taken on the remaining split. After training like this 5 times with the different split combinations, the model is scored by its average accuracy. While you certainly can test model performance on the test set instead of crossvalidation, this is considered cheating. By doing that, you “leak” knowledge of the test set to the model by using the test set for tuning. Exactly how harmful this is depends on your data, but in principle, you may end up tuning your model so much to get great testing accuracy, that your model overfits on the test data, and ends up not generalizing well (defeating the purpose of having a test set to begin with). By using parts of your training data as a validation set in crossvalidation, you get a rough idea of how well a model performs, without actually exposing it to the test set.\n\nWe save the data in a pandas , and then save it to CSV. There’s a lot of different data in there, which you can check out yourself. I filter the columns for the most relevant ones and sort them so the best results show up first. After you find the hyper-parameters that perform best during crossvalidation, you can train the model on the test set, and see how it goes. Let’s go ahead and do that in a file called :\n\nOkay, first things first. We start by making our very own dataset using . It generates 3000 samples of data with 20 features that are clustered in a normal distribution. Check out the documentation for the specifics. All we need to know is, it’s a binary classification problem, and we don’t have to worry about scaling or normalizing the data for our models. This is then split into training and test sets, as is tradition. We then feed it our parameter grid, testing 48 different combinations of hyper-parameters. Running this, my terminal output looks as such:\n\nAs mentioned above, there are many more columns in the grid search results, these are the ones I filtered as they are the most relevant. It looks like our best performing model had crossvalidation accuracy of about 86.44%, with hyper-parameters as shown. The second part of the output is below:\n\nSo the grid search algorithm took 17.71 seconds to train all the various combinations, and the best one had a test set accuracy of 85.6%, which is close to the crossvalidation accuracy. I’ll mention again, crossvalidation accuracy has nothing to do with test set accuracy. A good crossvalidation accuracy is an indicator that a model may have good test set accuracy, but you’ll have to see the test set accuracy to confirm this. The two measures are taken on different parts of the dataset.\n\nSo. This is all good and well. But maybe a little slow. We trained 48 different models on the full dataset. On my machine, giving the dataset 10 000 samples takes almost 2 minutes to optimize. The time taken to do this ramps up pretty quickly, the more data you have, and the more combinations you have in your hyper-parameter grid. Many datasets for real-world applications could have you training a model for hours or days. Imagine our SVM took an hour to train on such a dataset. Doing that, but times 48, will have you waiting two days to find the best model. Fortunately, an experimental feature from scikit-learn can cut down on training time.\n\nThis relatively new grid search implementation in the scikit-learn library, called successive halving grid search, can find optimal parameters fairly quickly, but not quite with the same diligence as . It does this by training all the combinations in your parameter grid but on a small subset of the training data. Once this is done, it only takes the best ones. Then, it retrains those best ones, but with a bit more data. Then it takes the best ones out of those and trains them on more data once again. This is a rinse and repeat process, and only the last iteration uses the entire dataset for crossvalidation. So instead of going all out on every combination, it just kind of roughly checks out the combination to gauge how good it is. The assumption is that if one model is better than another on 100 points of data, surely it must be better when you use 3000 data points. This is not a perfect assumption, but it makes for a decent heuristic. That’s why it picks the best combinations at the end of every iteration and then tries again, but with more data. Check out the documentation and this example page to take a closer look. Let’s see how it works. In the file we made earlier, , add the following code:\n\nNote the . Because this is an experimental feature at the time of writing, you need this to make it work. This is basically the same code as the grid search function. I’ve added the most important function arguments for the object. The argument is the reduction factor, set to 3 (default value). This means that we keep the best third combinations at every iteration. For the first iteration on our dataset, it should do 48 combinations first, and only use 48/3=16 combinations in the second iteration. The argument dictates how many resources (data points) you start with for each combination in the first iteration. The default value, , automatically sets the initial value so that the last iteration uses the whole dataset, which is reasonable default behaviour. If your dataset is particularly big, or you’re happy with not using all of it for crossvalidation in the end, you can set the and to suit your needs.\n\nOne last thing: looking at the results, be careful not to look for the highest to find the best hyper-parameters. This works for the normal exhaustive grid search object because everything is trained with the same data. With the successive halving grid search results, it’s possible that one of the candidates got an amazing crossvalidation score on 100 datapoints, but just got lucky, and does poorly on subsequent iterations. It may not even have made it to the last iteration. When you look at these results, only look at the combinations that made it to the last iteration, since those are the best ones, and their crossvalidation score is based on the whole training set. The other entries simply don’t give you the full picture.\n\nLet’s run both these algorithms side by side, and see what we get. The new should look something like this:\n\nHere are the results after running it. Terminal output:\n\nYou can see that the best crossvalidation score from the had a crossvalidation score of 86.34%, very close to the 86.44% of exhaustive grid search. Of real interest is the second part of the data output:\n\nThe final hyper-parameters are somewhat similar. The exhaustive grid search scored 85.6% accuracy, while the halving grid search got 85.47%. Not bad. Especially if you consider that the halving grid search only took 1.4 seconds. That’s more than 12 times faster. For comparison’s sake, when running the optimization with 10 000 samples, I get the following output:\n\nOnce again, similar crossvalidation scores, similar test set accuracy, similar optimal parameters. About 10 times faster.\n\nWe went over some practical examples of how to optimize hyper-parameters for scikit-learn ML models. If you were not familiar with the process before, you should be able to take it from here. It’s fairly easy and intuitive to apply it to any of the other estimators in the scikit-learn library. Personally, I really like the successive halving grid search that scikit-learn introduced, and I hope it stays in the API without too many changes. I recommend using it to test out a broad range of hyper-parameters first, to get a feel for which ones tend to work better. You can then use the exhaustive grid search algorithm on a narrower set of parameters if you want to try and squeeze that last bit of accuracy out of your model."
    },
    {
        "link": "https://stackoverflow.com/questions/35164310/random-forest-hyperparameter-tuning-scikit-learn-using-gridsearchcv",
        "document": "I am trying to use Random forest for my problem (below is a sample code for boston datasets, not for my data). I am planning to use for hyperparameter tuning but what should be the range of values for different parameters? How will I know that the range I am selecting is the correct one?\n\nI was reading about it on the internet and someone suggested to try \"zoom in\" on the optimum in a second grid-search (e.g. if it was 10 then try [5, 20, 50]).\n\nIs this the right approach? Shall I use this approach for ALL the parameters required for random forest? This approach may miss a \"good\" combination, right?"
    },
    {
        "link": "https://xgboost.readthedocs.io/en/release_1.5.0/python/python_api.html",
        "document": "This page gives the Python API reference of xgboost, please also refer to Python Package Introduction for more information about the Python package.\n\nDMatrix is an internal data structure that is used by XGBoost, which is optimized for both memory efficiency and training speed. You can construct DMatrix from multiple different sources of data.\n• None data (os.PathLike/string/numpy.array/scipy.sparse/pd.DataFrame/) – dt.Frame/cudf.DataFrame/cupy.array/dlpack Data source of DMatrix. When data is string or os.PathLike type, it represents the path libsvm format txt file, csv file (by specifying uri parameter ‘path_to_csv?format=csv’), or binary file that xgboost can read from.\n• In ranking task, one weight is assigned to each group (not each data point). This is because we only care about the relative ordering of data points within each group, so it doesn’t make sense to assign weights to individual data points.\n• None base_margin (array_like) – Base margin used for boosting from existing model.\n• None missing (float, optional) – Value in the input data which needs to be present as a missing value. If None, defaults to np.nan.\n• None feature_types (Optional[List[str]]) – Set types for features. When is set to , string “c” represents categorical data type.\n• None nthread (integer, optional) – Number of threads to use for loading data when parallelization is applicable. If -1, uses maximum threads available on the system.\n• None qid (array_like) – Query ID for data samples, used for ranking.\n• Experimental support of specializing for categorical features. Do not set to True unless you are interested in development. Currently it’s only available for tree method with 1 vs rest (one hot) categorical split. Also, JSON serialization format is required. Get the base margin of the DMatrix. Get float property from the DMatrix. field (str) – The field name of the information info – a numpy array of float information of the data Get the label of the DMatrix. Get unsigned integer property from the DMatrix. field (str) – The field name of the information info – a numpy array of unsigned integer information of the data Get the weight of the DMatrix. Get the number of columns (features) in the DMatrix. Get the number of rows in the DMatrix. Save DMatrix to an XGBoost buffer. Saved binary can be later loaded by providing the path to as input.\n• None fname (string or os.PathLike) – Name of the output buffer file.\n• None silent (bool (optional; default: True)) – If set, the output is suppressed. Set base margin of booster to start from. This can be used to specify a prediction value of existing model to be base_margin However, remember margin is needed, instead of transformed prediction e.g. for logistic regression: need to put in value before logistic transformation see also example/demo.py margin (array like) – Prediction margin of each datapoint\n• None field (str) – The field name of the information\n• None data (numpy array) – The array of data to be set\n• None field (str) – The field name of the information\n• None data (numpy array) – The array of data to be set Set group size of DMatrix (used for ranking). group (array like) – Group size of each group Set meta info for DMatrix. See doc string for . label (array like) – The label information to be set into DMatrix\n• None field (str) – The field name of the information\n• None data (numpy array) – The array of data to be set In ranking task, one weight is assigned to each group (not each data point). This is because we only care about the relative ordering of data points within each group, so it doesn’t make sense to assign weights to individual data points. Slice the DMatrix and return a new DMatrix that only contains .\n• None rindex (Union[List[int], numpy.ndarray]) – List of indices to be selected.\n• None allow_groups (bool) – Allow slicing of a matrix with a groups attribute A new DMatrix containing only selected indices. Device memory Data Matrix used in XGBoost for training with tree_method=’gpu_hist’. Do not use this for test/validation tasks as some information may be lost in quantisation. This DMatrix is primarily designed to save memory in training from device memory inputs by avoiding intermediate storage. Set max_bin to control the number of bins during quantisation. See doc string in for documents on meta info. You can construct DeviceQuantileDMatrix from cupy/cudf/dlpack.\n• None data (os.PathLike/string/numpy.array/scipy.sparse/pd.DataFrame/) – dt.Frame/cudf.DataFrame/cupy.array/dlpack Data source of DMatrix. When data is string or os.PathLike type, it represents the path libsvm format txt file, csv file (by specifying uri parameter ‘path_to_csv?format=csv’), or binary file that xgboost can read from.\n• In ranking task, one weight is assigned to each group (not each data point). This is because we only care about the relative ordering of data points within each group, so it doesn’t make sense to assign weights to individual data points.\n• None base_margin (array_like) – Base margin used for boosting from existing model.\n• None missing (float, optional) – Value in the input data which needs to be present as a missing value. If None, defaults to np.nan.\n• None feature_types – Set types for features. When is set to , string “c” represents categorical data type.\n• None nthread (integer, optional) – Number of threads to use for loading data when parallelization is applicable. If -1, uses maximum threads available on the system.\n• None qid (array_like) – Query ID for data samples, used for ranking.\n• Experimental support of specializing for categorical features. Do not set to True unless you are interested in development. Currently it’s only available for tree method with 1 vs rest (one hot) categorical split. Also, JSON serialization format is required. Booster is the model of xgboost, that contains low level routines for training, prediction and evaluation.\n• None model_file (string/os.PathLike/Booster/bytearray) – Path to the model file if it’s string or PathLike. Get attribute string from the Booster. key (str) – The key to get attribute from. value – The attribute value of the key, returns None if attribute do not exist. Get attributes stored in the Booster as a dictionary. result – Returns an empty dict if there’s no attributes. Boost the booster for one iteration, with customized gradient statistics. Like , this function should not be called directly by users.\n• None grad (list) – The first order of gradient.\n• None hess (list) – The second order of gradient. Dump model into a text or JSON file. Unlike , the output format is primarily used for visualization or interpretation, hence it’s more human readable but cannot be loaded back to XGBoost.\n• None fmap (string or os.PathLike, optional) – Name of the file containing feature map names.\n• None with_stats (bool, optional) – Controls whether the split statistics are output.\n• None dump_format (string, optional) – Format of model dump file. Can be ‘text’ or ‘json’.\n• None name (str, optional) – The name of the dataset.\n• None evals (list of tuples (DMatrix, string)) – List of items to be evaluated. Feature names for this booster. Can be directly set by input data or by assignment. Feature types for this booster. Can be directly set by input data or by assignment. Returns the model dump as a list of strings. Unlike , the output format is primarily used for visualization or interpretation, hence it’s more human readable but cannot be loaded back to XGBoost.\n• None fmap (string or os.PathLike, optional) – Name of the file containing feature map names.\n• None with_stats (bool, optional) – Controls whether the split statistics are output.\n• None dump_format (string, optional) – Format of model dump. Can be ‘text’, ‘json’ or ‘dot’. Get feature importance of each feature. Zero-importance features will not be included Keep in mind that this function does not include zero-importance feature, i.e. those features that have not been used in any split conditions. fmap (str or os.PathLike (optional)) – The name of feature map file Get feature importance of each feature. For tree model Importance type can be defined as:\n• None ‘weight’: the number of times a feature is used to split the data across all trees.\n• None ‘gain’: the average gain across all splits the feature is used in.\n• None ‘cover’: the average coverage across all splits the feature is used in.\n• None ‘total_gain’: the total gain across all splits the feature is used in.\n• None ‘total_cover’: the total coverage across all splits the feature is used in. For linear model, only “weight” is defined and it’s the normalized coefficients without bias. Zero-importance features will not be included Keep in mind that this function does not include zero-importance feature, i.e. those features that have not been used in any split conditions.\n• None fmap (str or os.PathLike (optional)) – The name of feature map file.\n• None importance_type (str, default 'weight') – One of the importance types defined above.\n• None A map between feature names and their scores. When is used for\n• None multi-class classification the scores for each feature is a list with length Get split value histogram of a feature\n• None feature (str) – The name of the feature.\n• None fmap (str or os.PathLike (optional)) – The name of feature map file.\n• None bin (int, default None) – The maximum number of bins. Number of bins equals number of unique split values n_unique, if bins == None or bins > n_unique.\n• None as_pandas (bool, default True) – Return pd.DataFrame when pandas is installed. If False or pandas is not installed, return numpy ndarray.\n• None a histogram of used splitting values for the specified feature\n• None either as numpy array or pandas DataFrame. Run prediction in-place, Unlike method, inplace prediction does not cache the prediction result. Calling only in multiple threads is safe and lock free. But the safety does not hold when used in conjunction with other methods. E.g. you can’t train the booster in one thread and perform prediction in the other.\n• None data (numpy.ndarray/scipy.sparse.csr_matrix/cupy.ndarray/) – cudf.DataFrame/pd.DataFrame The input data, must not be a view for numpy array. Set to for running prediction on CuPy array or CuDF DataFrame. prediction – The prediction result. When input data is on GPU, prediction result is stored in a cupy array. Load the model from a file or bytearray. Path to file can be local or as an URI. The model is loaded from XGBoost format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be loaded when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. fname (Union[str, bytearray, os.PathLike]) – Input file name or memory buffer(see also save_raw) Get number of boosted rounds. For gblinear this is reset to 0 after serializing the model. Predict with data. The full model will be used unless is specified, meaning user have to either slice the model or use the attribute to get prediction from best model returned from early stopping. See Prediction for issues like thread safety and a summary of outputs from this function.\n• None output_margin (bool) – Whether to output the raw untransformed margin value.\n• None pred_leaf (bool) – When this option is on, the output will be a matrix of (nsample, ntrees) with each record indicating the predicted leaf index of each sample in each tree. Note that the leaf index of a tree is unique per tree, so you may find leaf 1 in both tree 1 and tree 0.\n• None pred_contribs (bool) – When this is True the output will be a matrix of size (nsample, nfeats + 1) with each record indicating the feature contributions (SHAP values) for that prediction. The sum of all feature contributions is equal to the raw untransformed margin value of the prediction. Note the final column is the bias term.\n• None approx_contribs (bool) – Approximate the contributions of each feature. Used when or is set to True. Changing the default of this parameter (False) is not recommended.\n• None pred_interactions (bool) – When this is True the output will be a matrix of size (nsample, nfeats + 1, nfeats + 1) indicating the SHAP interaction values for each pair of features. The sum of each row (or column) of the interaction values equals the corresponding SHAP value (from pred_contribs), and the sum of the entire matrix equals the raw untransformed margin value of the prediction. Note the last row and column correspond to the bias term.\n• None validate_features (bool) – When this is True, validate that the Booster’s and data’s feature_names are identical. Otherwise, it is assumed that the feature_names are the same.\n• None Whether the prediction value is used for training. This can effect booster, which performs dropouts during training iterations but use all trees for inference. If you want to obtain result with dropouts, set this parameter to . Also, the parameter is set to true when obtaining prediction for custom objective function.\n• None Specifies which layer of trees are used in prediction. For example, if a random forest is trained with 100 rounds. Specifying , then only the forests built during [10, 20) (half open set) rounds are used in this prediction.\n• None When set to True, output shape is invariant to whether classification is used. For both value and margin prediction, the output shape is (n_samples, n_groups), n_groups == 1 when multi-class is not used. Default to False, in which case the output shape can be (n_samples, ) if multi-class is not used. The model is saved in an XGBoost internal format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be saved when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. Save the model to a in memory buffer representation instead of file. a in memory buffer representation of the model Set the attribute of the Booster.\n• None **kwargs – The attributes to set. Setting a value to None deletes an attribute.\n• None params (dict/list/str) – list of key,value pairs, dict of key to value or simply str key\n• None value (optional) – value of the specified parameter, when params is str key This feature is only defined when the decision tree model is chosen as base learner ( ). It is not defined for other base learner types, such as linear learners ( ). fmap (str or os.PathLike (optional)) – The name of feature map file. Update for one iteration, with objective function calculated internally. This function should not be called directly by users.\n\nImplementation of the scikit-learn API for XGBoost regression.\n• None n_estimators (int) – Number of gradient boosted trees. Equivalent to number of boosting rounds.\n• None verbosity (Optional[int]) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n• None objective (typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n• None booster (Optional[str]) – Specify which booster to use: gbtree, gblinear or dart.\n• None tree_method (Optional[str]) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It’s recommended to study this option from the parameters document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n• None n_jobs (Optional[int]) – Number of parallel threads used to run xgboost. When used with other Scikit-Learn algorithms like grid search, you may choose which algorithm to parallelize and balance the threads. Creating thread contention will significantly slow down both algorithms.\n• None gamma (Optional[float]) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n• None max_delta_step (Optional[float]) – Maximum delta step we allow each tree’s weight estimation to be.\n• None colsample_bytree (Optional[float]) – Subsample ratio of columns when constructing each tree.\n• None colsample_bylevel (Optional[float]) – Subsample ratio of columns for each level.\n• None colsample_bynode (Optional[float]) – Subsample ratio of columns for each split.\n• None base_score (Optional[float]) – The initial prediction score of all instances, global bias.\n• Using gblinear booster with shotgun updater is nondeterministic as it uses Hogwild algorithm.\n• None missing (float, default np.nan) – Value in the data which needs to be present as a missing value.\n• None monotone_constraints (Optional[Union[Dict[str, int], str]]) – Constraint of variable monotonicity. See tutorial for more information.\n• None interaction_constraints (Optional[Union[str, List[Tuple[str]]]]) – Constraints for interaction representing permitted interactions. The constraints must be specified in the form of a nest list, e.g. [[0, 1], [2, 3, 4]], where each inner list is a group of indices of features that are allowed to interact with each other. See tutorial for more information\n• None The feature importance type for the feature_importances_ property:\n• None For tree model, it’s either “gain”, “weight”, “cover”, “total_gain” or “total_cover”.\n• None For linear model, only “weight” is defined and it’s the normalized coefficients without bias.\n• None predictor (Optional[str]) – Force XGBoost to use specific predictor, available choices are [cpu_predictor, gpu_predictor].\n• Experimental support for categorical data. Do not set to true unless you are interested in development. Only valid when and dataframe are used.\n• None Keyword arguments for XGBoost Booster object. Full documentation of parameters can be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst. Attempting to set a parameter via the constructor args and **kwargs dict simultaneously will result in a TypeError. **kwargs is unsupported by scikit-learn. We do not guarantee that parameters passed via this argument will interact properly with scikit-learn. A custom objective function can be provided for the parameter. In this case, it should have the signature : The value of the gradient for each sample point. The value of the second derivative for each sample point Return the predicted leaf every tree for each sample. If the model is trained with early stopping, then is used automatically. X_leaves – For each datapoint x in X and for each tree, return the index of the leaf x ends up in. Leaves are numbered within , possibly with gaps in the numbering. Coefficients are defined only for linear learners Coefficients are only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). If eval_set is passed to the function, you can call to get evaluation results for all passed eval_sets. When eval_metric is also passed to the function, the evals_result will contain the eval_metrics passed to the function. The variable evals_result will contain:\n• None feature_importances_ (array of shape except for multi-class)\n• None linear model, which returns an array with shape Note that calling multiple times will cause the model object to be re-fit from scratch. To resume training from a previous checkpoint, explicitly pass argument.\n• None eval_set (Optional[List[Tuple[Any, Any]]]) – A list of (X, y) tuple pairs to use as validation sets, for which metrics will be computed. Validation metrics will help us track the performance of the model.\n• None If a str, should be a built-in evaluation metric to use. See doc/parameter.rst. If a list of str, should be the list of multiple built-in evaluation metrics to use. If callable, a custom evaluation metric. The call signature is where will be a DMatrix object such that you may need to call the method. It must return a str, value pair where the str is a name for the evaluation and value is the value of the evaluation function. The callable custom objective is always minimized.\n• None Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. Requires at least one item in eval_set. The method returns the model from the last iteration (not the best one). If there’s more than one item in eval_set, the last entry will be used for early stopping. If there’s more than one metric in eval_metric, the last metric will be used for early stopping. If early stopping occurs, the model will have three additional fields: , .\n• None verbose (Optional[bool]) – If and an evaluation set is used, writes the evaluation metric measured on the validation set to stderr.\n• None xgb_model (Optional[Union[xgboost.core.Booster, xgboost.sklearn.XGBModel, str]]) – file name of stored XGBoost model or ‘Booster’ instance XGBoost model to be loaded before training (allows training continuation).\n• None sample_weight_eval_set (Optional[List[Any]]) – A list of the form [L_1, L_2, …, L_n], where each L_i is an array like object storing instance weights for the i-th validation set.\n• None base_margin_eval_set (Optional[List[Any]]) – A list of the form [M_1, M_2, …, M_n], where each M_i is an array like object storing base margin for the i-th validation set.\n• None feature_weights (Optional[Any]) – Weight for each feature, defines the probability of each feature being selected when colsample is being used. All values must be greater than 0, otherwise a is thrown. Only available for , and tree methods.\n• None List of callback functions that are applied at end of each iteration. It is possible to use predefined callbacks by using Callback API. Example: Get the underlying xgboost Booster of this model. This will raise an exception when fit was not called Gets the number of xgboost boosting rounds. Intercept is defined only for linear learners Intercept (bias) is only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). Load the model from a file or bytearray. Path to file can be local or as an URI. The model is loaded from XGBoost format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be loaded when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. fname (Union[str, bytearray, os.PathLike]) – Input file name or memory buffer(see also save_raw) Predict with . If the model is trained with early stopping, then is used automatically. For tree models, when data is on GPU, like cupy array or cuDF dataframe and is not specified, the prediction is run on GPU automatically, otherwise it will run on CPU. This function is only thread safe for and .\n• None X (Any) – Data to predict with.\n• None output_margin (bool) – Whether to output the raw untransformed margin value.\n• None validate_features (bool) – When this is True, validate that the Booster’s and data’s feature_names are identical. Otherwise, it is assumed that the feature_names are the same.\n• None Specifies which layer of trees are used in prediction. For example, if a random forest is trained with 100 rounds. Specifying , then only the forests built during [10, 20) (half open set) rounds are used in this prediction. The model is saved in an XGBoost internal format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be saved when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. Set the parameters of this estimator. Modification of the sklearn method to allow unknown kwargs. This allows using the full range of xgboost parameters that are not defined as member variables in sklearn grid search. Implementation of the scikit-learn API for XGBoost classification.\n• None use_label_encoder (bool) – (Deprecated) Use the label encoder from scikit-learn to encode the labels. For new code, we recommend that you set this parameter to False.\n• None verbosity (Optional[int]) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n• None objective (typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n• None booster (Optional[str]) – Specify which booster to use: gbtree, gblinear or dart.\n• None tree_method (Optional[str]) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It’s recommended to study this option from the parameters document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n• None n_jobs (Optional[int]) – Number of parallel threads used to run xgboost. When used with other Scikit-Learn algorithms like grid search, you may choose which algorithm to parallelize and balance the threads. Creating thread contention will significantly slow down both algorithms.\n• None gamma (Optional[float]) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n• None max_delta_step (Optional[float]) – Maximum delta step we allow each tree’s weight estimation to be.\n• None colsample_bytree (Optional[float]) – Subsample ratio of columns when constructing each tree.\n• None colsample_bylevel (Optional[float]) – Subsample ratio of columns for each level.\n• None colsample_bynode (Optional[float]) – Subsample ratio of columns for each split.\n• None base_score (Optional[float]) – The initial prediction score of all instances, global bias.\n• Using gblinear booster with shotgun updater is nondeterministic as it uses Hogwild algorithm.\n• None missing (float, default np.nan) – Value in the data which needs to be present as a missing value.\n• None monotone_constraints (Optional[Union[Dict[str, int], str]]) – Constraint of variable monotonicity. See tutorial for more information.\n• None interaction_constraints (Optional[Union[str, List[Tuple[str]]]]) – Constraints for interaction representing permitted interactions. The constraints must be specified in the form of a nest list, e.g. [[0, 1], [2, 3, 4]], where each inner list is a group of indices of features that are allowed to interact with each other. See tutorial for more information\n• None The feature importance type for the feature_importances_ property:\n• None For tree model, it’s either “gain”, “weight”, “cover”, “total_gain” or “total_cover”.\n• None For linear model, only “weight” is defined and it’s the normalized coefficients without bias.\n• None predictor (Optional[str]) – Force XGBoost to use specific predictor, available choices are [cpu_predictor, gpu_predictor].\n• Experimental support for categorical data. Do not set to true unless you are interested in development. Only valid when and dataframe are used.\n• None Keyword arguments for XGBoost Booster object. Full documentation of parameters can be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst. Attempting to set a parameter via the constructor args and **kwargs dict simultaneously will result in a TypeError. **kwargs is unsupported by scikit-learn. We do not guarantee that parameters passed via this argument will interact properly with scikit-learn. A custom objective function can be provided for the parameter. In this case, it should have the signature : The value of the gradient for each sample point. The value of the second derivative for each sample point Return the predicted leaf every tree for each sample. If the model is trained with early stopping, then is used automatically. X_leaves – For each datapoint x in X and for each tree, return the index of the leaf x ends up in. Leaves are numbered within , possibly with gaps in the numbering. Coefficients are defined only for linear learners Coefficients are only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). If eval_set is passed to the function, you can call to get evaluation results for all passed eval_sets. When eval_metric is also passed to the function, the evals_result will contain the eval_metrics passed to the function. The variable evals_result will contain\n• None feature_importances_ (array of shape except for multi-class)\n• None linear model, which returns an array with shape Note that calling multiple times will cause the model object to be re-fit from scratch. To resume training from a previous checkpoint, explicitly pass argument.\n• None eval_set (Optional[List[Tuple[Any, Any]]]) – A list of (X, y) tuple pairs to use as validation sets, for which metrics will be computed. Validation metrics will help us track the performance of the model.\n• None If a str, should be a built-in evaluation metric to use. See doc/parameter.rst. If a list of str, should be the list of multiple built-in evaluation metrics to use. If callable, a custom evaluation metric. The call signature is where will be a DMatrix object such that you may need to call the method. It must return a str, value pair where the str is a name for the evaluation and value is the value of the evaluation function. The callable custom objective is always minimized.\n• None Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. Requires at least one item in eval_set. The method returns the model from the last iteration (not the best one). If there’s more than one item in eval_set, the last entry will be used for early stopping. If there’s more than one metric in eval_metric, the last metric will be used for early stopping. If early stopping occurs, the model will have three additional fields: , .\n• None verbose (Optional[bool]) – If and an evaluation set is used, writes the evaluation metric measured on the validation set to stderr.\n• None xgb_model (Optional[Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel]]) – file name of stored XGBoost model or ‘Booster’ instance XGBoost model to be loaded before training (allows training continuation).\n• None sample_weight_eval_set (Optional[List[Any]]) – A list of the form [L_1, L_2, …, L_n], where each L_i is an array like object storing instance weights for the i-th validation set.\n• None base_margin_eval_set (Optional[List[Any]]) – A list of the form [M_1, M_2, …, M_n], where each M_i is an array like object storing base margin for the i-th validation set.\n• None feature_weights (Optional[Any]) – Weight for each feature, defines the probability of each feature being selected when colsample is being used. All values must be greater than 0, otherwise a is thrown. Only available for , and tree methods.\n• None List of callback functions that are applied at end of each iteration. It is possible to use predefined callbacks by using Callback API. Example: Get the underlying xgboost Booster of this model. This will raise an exception when fit was not called Gets the number of xgboost boosting rounds. Intercept is defined only for linear learners Intercept (bias) is only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). Load the model from a file or bytearray. Path to file can be local or as an URI. The model is loaded from XGBoost format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be loaded when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. fname (Union[str, bytearray, os.PathLike]) – Input file name or memory buffer(see also save_raw) Predict with . If the model is trained with early stopping, then is used automatically. For tree models, when data is on GPU, like cupy array or cuDF dataframe and is not specified, the prediction is run on GPU automatically, otherwise it will run on CPU. This function is only thread safe for and .\n• None X (Any) – Data to predict with.\n• None output_margin (bool) – Whether to output the raw untransformed margin value.\n• None validate_features (bool) – When this is True, validate that the Booster’s and data’s feature_names are identical. Otherwise, it is assumed that the feature_names are the same.\n• None Specifies which layer of trees are used in prediction. For example, if a random forest is trained with 100 rounds. Specifying , then only the forests built during [10, 20) (half open set) rounds are used in this prediction. Predict the probability of each example being of a given class. This function is only thread safe for and .\n• None validate_features (bool) – When this is True, validate that the Booster’s and data’s feature_names are identical. Otherwise, it is assumed that the feature_names are the same.\n• None iteration_range (Optional[Tuple[int, int]]) – Specifies which layer of trees are used in prediction. For example, if a random forest is trained with 100 rounds. Specifying , then only the forests built during [10, 20) (half open set) rounds are used in this prediction. a numpy array of shape array-like of shape (n_samples, n_classes) with the probability of each data example being of a given class. The model is saved in an XGBoost internal format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be saved when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. Set the parameters of this estimator. Modification of the sklearn method to allow unknown kwargs. This allows using the full range of xgboost parameters that are not defined as member variables in sklearn grid search. Implementation of the Scikit-Learn API for XGBoost Ranking.\n• None n_estimators (int) – Number of gradient boosted trees. Equivalent to number of boosting rounds.\n• None verbosity (Optional[int]) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n• None objective (typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n• None booster (Optional[str]) – Specify which booster to use: gbtree, gblinear or dart.\n• None tree_method (Optional[str]) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It’s recommended to study this option from the parameters document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n• None n_jobs (Optional[int]) – Number of parallel threads used to run xgboost. When used with other Scikit-Learn algorithms like grid search, you may choose which algorithm to parallelize and balance the threads. Creating thread contention will significantly slow down both algorithms.\n• None gamma (Optional[float]) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n• None max_delta_step (Optional[float]) – Maximum delta step we allow each tree’s weight estimation to be.\n• None colsample_bytree (Optional[float]) – Subsample ratio of columns when constructing each tree.\n• None colsample_bylevel (Optional[float]) – Subsample ratio of columns for each level.\n• None colsample_bynode (Optional[float]) – Subsample ratio of columns for each split.\n• None base_score (Optional[float]) – The initial prediction score of all instances, global bias.\n• Using gblinear booster with shotgun updater is nondeterministic as it uses Hogwild algorithm.\n• None missing (float, default np.nan) – Value in the data which needs to be present as a missing value.\n• None monotone_constraints (Optional[Union[Dict[str, int], str]]) – Constraint of variable monotonicity. See tutorial for more information.\n• None interaction_constraints (Optional[Union[str, List[Tuple[str]]]]) – Constraints for interaction representing permitted interactions. The constraints must be specified in the form of a nest list, e.g. [[0, 1], [2, 3, 4]], where each inner list is a group of indices of features that are allowed to interact with each other. See tutorial for more information\n• None The feature importance type for the feature_importances_ property:\n• None For tree model, it’s either “gain”, “weight”, “cover”, “total_gain” or “total_cover”.\n• None For linear model, only “weight” is defined and it’s the normalized coefficients without bias.\n• None predictor (Optional[str]) – Force XGBoost to use specific predictor, available choices are [cpu_predictor, gpu_predictor].\n• Experimental support for categorical data. Do not set to true unless you are interested in development. Only valid when and dataframe are used.\n• None Keyword arguments for XGBoost Booster object. Full documentation of parameters can be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst. Attempting to set a parameter via the constructor args and **kwargs dict simultaneously will result in a TypeError. **kwargs is unsupported by scikit-learn. We do not guarantee that parameters passed via this argument will interact properly with scikit-learn. A custom objective function is currently not supported by XGBRanker. Likewise, a custom metric function is not supported either. Query group information is required for ranking tasks by either using the parameter or parameter in method. Before fitting the model, your data need to be sorted by query group. When fitting the model, you need to provide an additional array that contains the size of each query group. For example, if your original data look like: then your group array should be . Sometimes using query id ( ) instead of group can be more convenient. Return the predicted leaf every tree for each sample. If the model is trained with early stopping, then is used automatically. X_leaves – For each datapoint x in X and for each tree, return the index of the leaf x ends up in. Leaves are numbered within , possibly with gaps in the numbering. Coefficients are defined only for linear learners Coefficients are only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). If eval_set is passed to the function, you can call to get evaluation results for all passed eval_sets. When eval_metric is also passed to the function, the evals_result will contain the eval_metrics passed to the function. The variable evals_result will contain:\n• None feature_importances_ (array of shape except for multi-class)\n• None linear model, which returns an array with shape Note that calling multiple times will cause the model object to be re-fit from scratch. To resume training from a previous checkpoint, explicitly pass argument.\n• None group (Optional[Any]) – Size of each query group of training data. Should have as many elements as the query groups in the training data. If this is set to None, then user must provide qid.\n• None qid (Optional[Any]) – Query ID for each training sample. Should have the size of n_samples. If this is set to None, then user must provide group.\n• In ranking task, one weight is assigned to each query group/id (not each data point). This is because we only care about the relative ordering of data points within each group, so it doesn’t make sense to assign weights to individual data points.\n• None eval_set (Optional[List[Tuple[Any, Any]]]) – A list of (X, y) tuple pairs to use as validation sets, for which metrics will be computed. Validation metrics will help us track the performance of the model.\n• None eval_group (Optional[List[Any]]) – A list in which is the list containing the sizes of all query groups in the -th pair in eval_set.\n• None eval_qid (Optional[List[Any]]) – A list in which is the array containing query ID of -th pair in eval_set.\n• None eval_metric (Optional[Union[str, List[str], Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]]]]) – If a str, should be a built-in evaluation metric to use. See doc/parameter.rst. If a list of str, should be the list of multiple built-in evaluation metrics to use. The custom evaluation metric is not yet supported for the ranker.\n• None early_stopping_rounds (Optional[int]) – Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. Requires at least one item in eval_set. The method returns the model from the last iteration (not the best one). If there’s more than one item in eval_set, the last entry will be used for early stopping. If there’s more than one metric in eval_metric, the last metric will be used for early stopping. If early stopping occurs, the model will have three additional fields: , and .\n• None verbose (Optional[bool]) – If and an evaluation set is used, writes the evaluation metric measured on the validation set to stderr.\n• None xgb_model (Optional[Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel]]) – file name of stored XGBoost model or ‘Booster’ instance XGBoost model to be loaded before training (allows training continuation).\n• None A list of the form [L_1, L_2, …, L_n], where each L_i is a list of group weights on the i-th validation set. In ranking task, one weight is assigned to each query group (not each data point). This is because we only care about the relative ordering of data points within each group, so it doesn’t make sense to assign weights to individual data points.\n• None base_margin_eval_set (Optional[List[Any]]) – A list of the form [M_1, M_2, …, M_n], where each M_i is an array like object storing base margin for the i-th validation set.\n• None feature_weights (Optional[Any]) – Weight for each feature, defines the probability of each feature being selected when colsample is being used. All values must be greater than 0, otherwise a is thrown. Only available for , and tree methods.\n• None List of callback functions that are applied at end of each iteration. It is possible to use predefined callbacks by using Callback API. Example: Get the underlying xgboost Booster of this model. This will raise an exception when fit was not called Gets the number of xgboost boosting rounds. Intercept is defined only for linear learners Intercept (bias) is only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). Load the model from a file or bytearray. Path to file can be local or as an URI. The model is loaded from XGBoost format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be loaded when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. fname (Union[str, bytearray, os.PathLike]) – Input file name or memory buffer(see also save_raw) Predict with . If the model is trained with early stopping, then is used automatically. For tree models, when data is on GPU, like cupy array or cuDF dataframe and is not specified, the prediction is run on GPU automatically, otherwise it will run on CPU. This function is only thread safe for and .\n• None X (Any) – Data to predict with.\n• None output_margin (bool) – Whether to output the raw untransformed margin value.\n• None validate_features (bool) – When this is True, validate that the Booster’s and data’s feature_names are identical. Otherwise, it is assumed that the feature_names are the same.\n• None Specifies which layer of trees are used in prediction. For example, if a random forest is trained with 100 rounds. Specifying , then only the forests built during [10, 20) (half open set) rounds are used in this prediction. The model is saved in an XGBoost internal format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be saved when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. Set the parameters of this estimator. Modification of the sklearn method to allow unknown kwargs. This allows using the full range of xgboost parameters that are not defined as member variables in sklearn grid search.\n• None n_estimators (int) – Number of trees in random forest to fit.\n• None verbosity (Optional[int]) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n• None objective (typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n• None booster (Optional[str]) – Specify which booster to use: gbtree, gblinear or dart.\n• None tree_method (Optional[str]) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It’s recommended to study this option from the parameters document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n• None n_jobs (Optional[int]) – Number of parallel threads used to run xgboost. When used with other Scikit-Learn algorithms like grid search, you may choose which algorithm to parallelize and balance the threads. Creating thread contention will significantly slow down both algorithms.\n• None gamma (Optional[float]) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n• None max_delta_step (Optional[float]) – Maximum delta step we allow each tree’s weight estimation to be.\n• None colsample_bytree (Optional[float]) – Subsample ratio of columns when constructing each tree.\n• None colsample_bylevel (Optional[float]) – Subsample ratio of columns for each level.\n• None colsample_bynode (Optional[float]) – Subsample ratio of columns for each split.\n• None base_score (Optional[float]) – The initial prediction score of all instances, global bias.\n• Using gblinear booster with shotgun updater is nondeterministic as it uses Hogwild algorithm.\n• None missing (float, default np.nan) – Value in the data which needs to be present as a missing value.\n• None monotone_constraints (Optional[Union[Dict[str, int], str]]) – Constraint of variable monotonicity. See tutorial for more information.\n• None interaction_constraints (Optional[Union[str, List[Tuple[str]]]]) – Constraints for interaction representing permitted interactions. The constraints must be specified in the form of a nest list, e.g. [[0, 1], [2, 3, 4]], where each inner list is a group of indices of features that are allowed to interact with each other. See tutorial for more information\n• None The feature importance type for the feature_importances_ property:\n• None For tree model, it’s either “gain”, “weight”, “cover”, “total_gain” or “total_cover”.\n• None For linear model, only “weight” is defined and it’s the normalized coefficients without bias.\n• None predictor (Optional[str]) – Force XGBoost to use specific predictor, available choices are [cpu_predictor, gpu_predictor].\n• Experimental support for categorical data. Do not set to true unless you are interested in development. Only valid when and dataframe are used.\n• None Keyword arguments for XGBoost Booster object. Full documentation of parameters can be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst. Attempting to set a parameter via the constructor args and **kwargs dict simultaneously will result in a TypeError. **kwargs is unsupported by scikit-learn. We do not guarantee that parameters passed via this argument will interact properly with scikit-learn. A custom objective function can be provided for the parameter. In this case, it should have the signature : The value of the gradient for each sample point. The value of the second derivative for each sample point Return the predicted leaf every tree for each sample. If the model is trained with early stopping, then is used automatically. X_leaves – For each datapoint x in X and for each tree, return the index of the leaf x ends up in. Leaves are numbered within , possibly with gaps in the numbering. Coefficients are defined only for linear learners Coefficients are only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). If eval_set is passed to the function, you can call to get evaluation results for all passed eval_sets. When eval_metric is also passed to the function, the evals_result will contain the eval_metrics passed to the function. The variable evals_result will contain:\n• None feature_importances_ (array of shape except for multi-class)\n• None linear model, which returns an array with shape Note that calling multiple times will cause the model object to be re-fit from scratch. To resume training from a previous checkpoint, explicitly pass argument.\n• None eval_set (Optional[List[Tuple[Any, Any]]]) – A list of (X, y) tuple pairs to use as validation sets, for which metrics will be computed. Validation metrics will help us track the performance of the model.\n• None If a str, should be a built-in evaluation metric to use. See doc/parameter.rst. If a list of str, should be the list of multiple built-in evaluation metrics to use. If callable, a custom evaluation metric. The call signature is where will be a DMatrix object such that you may need to call the method. It must return a str, value pair where the str is a name for the evaluation and value is the value of the evaluation function. The callable custom objective is always minimized.\n• None Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. Requires at least one item in eval_set. The method returns the model from the last iteration (not the best one). If there’s more than one item in eval_set, the last entry will be used for early stopping. If there’s more than one metric in eval_metric, the last metric will be used for early stopping. If early stopping occurs, the model will have three additional fields: , .\n• None verbose (Optional[bool]) – If and an evaluation set is used, writes the evaluation metric measured on the validation set to stderr.\n• None xgb_model (Optional[Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel]]) – file name of stored XGBoost model or ‘Booster’ instance XGBoost model to be loaded before training (allows training continuation).\n• None sample_weight_eval_set (Optional[List[Any]]) – A list of the form [L_1, L_2, …, L_n], where each L_i is an array like object storing instance weights for the i-th validation set.\n• None base_margin_eval_set (Optional[List[Any]]) – A list of the form [M_1, M_2, …, M_n], where each M_i is an array like object storing base margin for the i-th validation set.\n• None feature_weights (Optional[Any]) – Weight for each feature, defines the probability of each feature being selected when colsample is being used. All values must be greater than 0, otherwise a is thrown. Only available for , and tree methods.\n• None List of callback functions that are applied at end of each iteration. It is possible to use predefined callbacks by using Callback API. Example: Get the underlying xgboost Booster of this model. This will raise an exception when fit was not called Gets the number of xgboost boosting rounds. Intercept is defined only for linear learners Intercept (bias) is only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). Load the model from a file or bytearray. Path to file can be local or as an URI. The model is loaded from XGBoost format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be loaded when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. fname (Union[str, bytearray, os.PathLike]) – Input file name or memory buffer(see also save_raw) Predict with . If the model is trained with early stopping, then is used automatically. For tree models, when data is on GPU, like cupy array or cuDF dataframe and is not specified, the prediction is run on GPU automatically, otherwise it will run on CPU. This function is only thread safe for and .\n• None X (Any) – Data to predict with.\n• None output_margin (bool) – Whether to output the raw untransformed margin value.\n• None validate_features (bool) – When this is True, validate that the Booster’s and data’s feature_names are identical. Otherwise, it is assumed that the feature_names are the same.\n• None Specifies which layer of trees are used in prediction. For example, if a random forest is trained with 100 rounds. Specifying , then only the forests built during [10, 20) (half open set) rounds are used in this prediction. The model is saved in an XGBoost internal format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be saved when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. Set the parameters of this estimator. Modification of the sklearn method to allow unknown kwargs. This allows using the full range of xgboost parameters that are not defined as member variables in sklearn grid search.\n• None n_estimators (int) – Number of trees in random forest to fit.\n• None use_label_encoder (bool) – (Deprecated) Use the label encoder from scikit-learn to encode the labels. For new code, we recommend that you set this parameter to False.\n• None verbosity (Optional[int]) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n• None objective (typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n• None booster (Optional[str]) – Specify which booster to use: gbtree, gblinear or dart.\n• None tree_method (Optional[str]) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It’s recommended to study this option from the parameters document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n• None n_jobs (Optional[int]) – Number of parallel threads used to run xgboost. When used with other Scikit-Learn algorithms like grid search, you may choose which algorithm to parallelize and balance the threads. Creating thread contention will significantly slow down both algorithms.\n• None gamma (Optional[float]) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n• None max_delta_step (Optional[float]) – Maximum delta step we allow each tree’s weight estimation to be.\n• None colsample_bytree (Optional[float]) – Subsample ratio of columns when constructing each tree.\n• None colsample_bylevel (Optional[float]) – Subsample ratio of columns for each level.\n• None colsample_bynode (Optional[float]) – Subsample ratio of columns for each split.\n• None base_score (Optional[float]) – The initial prediction score of all instances, global bias.\n• Using gblinear booster with shotgun updater is nondeterministic as it uses Hogwild algorithm.\n• None missing (float, default np.nan) – Value in the data which needs to be present as a missing value.\n• None monotone_constraints (Optional[Union[Dict[str, int], str]]) – Constraint of variable monotonicity. See tutorial for more information.\n• None interaction_constraints (Optional[Union[str, List[Tuple[str]]]]) – Constraints for interaction representing permitted interactions. The constraints must be specified in the form of a nest list, e.g. [[0, 1], [2, 3, 4]], where each inner list is a group of indices of features that are allowed to interact with each other. See tutorial for more information\n• None The feature importance type for the feature_importances_ property:\n• None For tree model, it’s either “gain”, “weight”, “cover”, “total_gain” or “total_cover”.\n• None For linear model, only “weight” is defined and it’s the normalized coefficients without bias.\n• None predictor (Optional[str]) – Force XGBoost to use specific predictor, available choices are [cpu_predictor, gpu_predictor].\n• Experimental support for categorical data. Do not set to true unless you are interested in development. Only valid when and dataframe are used.\n• None Keyword arguments for XGBoost Booster object. Full documentation of parameters can be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst. Attempting to set a parameter via the constructor args and **kwargs dict simultaneously will result in a TypeError. **kwargs is unsupported by scikit-learn. We do not guarantee that parameters passed via this argument will interact properly with scikit-learn. A custom objective function can be provided for the parameter. In this case, it should have the signature : The value of the gradient for each sample point. The value of the second derivative for each sample point Return the predicted leaf every tree for each sample. If the model is trained with early stopping, then is used automatically. X_leaves – For each datapoint x in X and for each tree, return the index of the leaf x ends up in. Leaves are numbered within , possibly with gaps in the numbering. Coefficients are defined only for linear learners Coefficients are only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). If eval_set is passed to the function, you can call to get evaluation results for all passed eval_sets. When eval_metric is also passed to the function, the evals_result will contain the eval_metrics passed to the function. The variable evals_result will contain\n• None feature_importances_ (array of shape except for multi-class)\n• None linear model, which returns an array with shape Note that calling multiple times will cause the model object to be re-fit from scratch. To resume training from a previous checkpoint, explicitly pass argument.\n• None eval_set (Optional[List[Tuple[Any, Any]]]) – A list of (X, y) tuple pairs to use as validation sets, for which metrics will be computed. Validation metrics will help us track the performance of the model.\n• None If a str, should be a built-in evaluation metric to use. See doc/parameter.rst. If a list of str, should be the list of multiple built-in evaluation metrics to use. If callable, a custom evaluation metric. The call signature is where will be a DMatrix object such that you may need to call the method. It must return a str, value pair where the str is a name for the evaluation and value is the value of the evaluation function. The callable custom objective is always minimized.\n• None Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. Requires at least one item in eval_set. The method returns the model from the last iteration (not the best one). If there’s more than one item in eval_set, the last entry will be used for early stopping. If there’s more than one metric in eval_metric, the last metric will be used for early stopping. If early stopping occurs, the model will have three additional fields: , .\n• None verbose (Optional[bool]) – If and an evaluation set is used, writes the evaluation metric measured on the validation set to stderr.\n• None xgb_model (Optional[Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel]]) – file name of stored XGBoost model or ‘Booster’ instance XGBoost model to be loaded before training (allows training continuation).\n• None sample_weight_eval_set (Optional[List[Any]]) – A list of the form [L_1, L_2, …, L_n], where each L_i is an array like object storing instance weights for the i-th validation set.\n• None base_margin_eval_set (Optional[List[Any]]) – A list of the form [M_1, M_2, …, M_n], where each M_i is an array like object storing base margin for the i-th validation set.\n• None feature_weights (Optional[Any]) – Weight for each feature, defines the probability of each feature being selected when colsample is being used. All values must be greater than 0, otherwise a is thrown. Only available for , and tree methods.\n• None List of callback functions that are applied at end of each iteration. It is possible to use predefined callbacks by using Callback API. Example: Get the underlying xgboost Booster of this model. This will raise an exception when fit was not called Gets the number of xgboost boosting rounds. Intercept is defined only for linear learners Intercept (bias) is only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). Load the model from a file or bytearray. Path to file can be local or as an URI. The model is loaded from XGBoost format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be loaded when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. fname (Union[str, bytearray, os.PathLike]) – Input file name or memory buffer(see also save_raw) Predict with . If the model is trained with early stopping, then is used automatically. For tree models, when data is on GPU, like cupy array or cuDF dataframe and is not specified, the prediction is run on GPU automatically, otherwise it will run on CPU. This function is only thread safe for and .\n• None X (Any) – Data to predict with.\n• None output_margin (bool) – Whether to output the raw untransformed margin value.\n• None validate_features (bool) – When this is True, validate that the Booster’s and data’s feature_names are identical. Otherwise, it is assumed that the feature_names are the same.\n• None Specifies which layer of trees are used in prediction. For example, if a random forest is trained with 100 rounds. Specifying , then only the forests built during [10, 20) (half open set) rounds are used in this prediction. Predict the probability of each example being of a given class. This function is only thread safe for and .\n• None validate_features (bool) – When this is True, validate that the Booster’s and data’s feature_names are identical. Otherwise, it is assumed that the feature_names are the same.\n• None iteration_range (Optional[Tuple[int, int]]) – Specifies which layer of trees are used in prediction. For example, if a random forest is trained with 100 rounds. Specifying , then only the forests built during [10, 20) (half open set) rounds are used in this prediction. a numpy array of shape array-like of shape (n_samples, n_classes) with the probability of each data example being of a given class. The model is saved in an XGBoost internal format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be saved when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. Set the parameters of this estimator. Modification of the sklearn method to allow unknown kwargs. This allows using the full range of xgboost parameters that are not defined as member variables in sklearn grid search.\n\nDask extensions for distributed training. See https://xgboost.readthedocs.io/en/latest/tutorials/dask.html for simple tutorial. Also xgboost/demo/dask for some examples. There are two sets of APIs in this module, one is the functional API including and methods. Another is stateful Scikit-Learner wrapper inherited from single-node Scikit-Learn interface. The implementation is heavily influenced by dask_xgboost: https://github.com/dask/dask-xgboost DMatrix holding on references to Dask DataFrame or Dask Array. Constructing a forces all lazy computation to be carried out. Wait for the input data explicitly if you want to see actual computation of constructing . See doc for constructor for other parameters. DaskDMatrix accepts only dask collection. DaskDMatrix does not repartition or move data between workers. It’s the caller’s responsibility to balance the data.\n• None client (distributed.Client) – Specify the dask client used for training. Use default client returned from dask if it’s set to None. Specialized data type for tree method. This class is used to reduce the memory usage by eliminating data copies. Internally the all partitions/chunks of data are merged by weighted GK sketching. So the number of partitions from dask may affect training accuracy as GK generates bounded error for each merge. See doc string for and for other parameters. Other parameters are the same as except for , which is returned as part of function return value instead of argument.\n• None client (distributed.Client) – Specify the dask client used for training. Use default client returned from dask if it’s set to None. results – A dictionary containing trained booster and evaluation history. field is the same as from . Using might be faster when some features are not needed. See for details on various parameters. When output has more than 2 dimensions (shap value, leaf with strict_shape), input should be or .\n• None client (distributed.Client) – Specify the dask client used for training. Use default client returned from dask if it’s set to None.\n• None model (Union[Dict[str, Any], xgboost.core.Booster, distributed.Future]) – The trained model. It can be a distributed.Future so user can pre-scatter it onto all workers.\n• None data (Union[xgboost.dask.DaskDMatrix, da.Array, dd.DataFrame, dd.Series]) – Input data used for prediction. When input is a dataframe object, prediction output is a series.\n• None missing (float) – Used when input data is not DaskDMatrix. Specify the value considered as missing. prediction – When input data is or , the return value is an array, when input data is , return value can be , , depending on the output shape. Inplace prediction. See doc in for details.\n• None client (distributed.Client) – Specify the dask client used for training. Use default client returned from dask if it’s set to None.\n• None model (Union[Dict[str, Any], xgboost.core.Booster, distributed.Future]) – See for details.\n• None missing (float) – Value in the input data which needs to be present as a missing value. If None, defaults to np.nan.\n• None See for details. Right now classifier is not well supported with base_margin as it requires the size of base margin to be . When input data is , the return value is an array, when input data is , return value can be , , depending on the output shape. Implementation of the scikit-learn API for XGBoost classification.\n• None n_estimators (int) – Number of gradient boosted trees. Equivalent to number of boosting rounds.\n• None verbosity (Optional[int]) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n• None objective (typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n• None booster (Optional[str]) – Specify which booster to use: gbtree, gblinear or dart.\n• None tree_method (Optional[str]) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It’s recommended to study this option from the parameters document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n• None n_jobs (Optional[int]) – Number of parallel threads used to run xgboost. When used with other Scikit-Learn algorithms like grid search, you may choose which algorithm to parallelize and balance the threads. Creating thread contention will significantly slow down both algorithms.\n• None gamma (Optional[float]) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n• None max_delta_step (Optional[float]) – Maximum delta step we allow each tree’s weight estimation to be.\n• None colsample_bytree (Optional[float]) – Subsample ratio of columns when constructing each tree.\n• None colsample_bylevel (Optional[float]) – Subsample ratio of columns for each level.\n• None colsample_bynode (Optional[float]) – Subsample ratio of columns for each split.\n• None base_score (Optional[float]) – The initial prediction score of all instances, global bias.\n• Using gblinear booster with shotgun updater is nondeterministic as it uses Hogwild algorithm.\n• None missing (float, default np.nan) – Value in the data which needs to be present as a missing value.\n• None monotone_constraints (Optional[Union[Dict[str, int], str]]) – Constraint of variable monotonicity. See tutorial for more information.\n• None interaction_constraints (Optional[Union[str, List[Tuple[str]]]]) – Constraints for interaction representing permitted interactions. The constraints must be specified in the form of a nest list, e.g. [[0, 1], [2, 3, 4]], where each inner list is a group of indices of features that are allowed to interact with each other. See tutorial for more information\n• None The feature importance type for the feature_importances_ property:\n• None For tree model, it’s either “gain”, “weight”, “cover”, “total_gain” or “total_cover”.\n• None For linear model, only “weight” is defined and it’s the normalized coefficients without bias.\n• None predictor (Optional[str]) – Force XGBoost to use specific predictor, available choices are [cpu_predictor, gpu_predictor].\n• Experimental support for categorical data. Do not set to true unless you are interested in development. Only valid when and dataframe are used.\n• None Keyword arguments for XGBoost Booster object. Full documentation of parameters can be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst. Attempting to set a parameter via the constructor args and **kwargs dict simultaneously will result in a TypeError. **kwargs is unsupported by scikit-learn. We do not guarantee that parameters passed via this argument will interact properly with scikit-learn. Return the predicted leaf every tree for each sample. If the model is trained with early stopping, then is used automatically. X_leaves – For each datapoint x in X and for each tree, return the index of the leaf x ends up in. Leaves are numbered within , possibly with gaps in the numbering. The dask client used in this model. The object can not be serialized for transmission, so if task is launched from a worker instead of directly from the client process, this attribute needs to be set at that worker. Coefficients are defined only for linear learners Coefficients are only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). If eval_set is passed to the function, you can call to get evaluation results for all passed eval_sets. When eval_metric is also passed to the function, the evals_result will contain the eval_metrics passed to the function. The variable evals_result will contain:\n• None feature_importances_ (array of shape except for multi-class)\n• None linear model, which returns an array with shape Note that calling multiple times will cause the model object to be re-fit from scratch. To resume training from a previous checkpoint, explicitly pass argument.\n• None eval_set (Optional[List[Tuple[Union[da.Array, dd.DataFrame, dd.Series], Union[da.Array, dd.DataFrame, dd.Series]]]]) – A list of (X, y) tuple pairs to use as validation sets, for which metrics will be computed. Validation metrics will help us track the performance of the model.\n• None If a str, should be a built-in evaluation metric to use. See doc/parameter.rst. If a list of str, should be the list of multiple built-in evaluation metrics to use. If callable, a custom evaluation metric. The call signature is where will be a DMatrix object such that you may need to call the method. It must return a str, value pair where the str is a name for the evaluation and value is the value of the evaluation function. The callable custom objective is always minimized.\n• None Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. Requires at least one item in eval_set. The method returns the model from the last iteration (not the best one). If there’s more than one item in eval_set, the last entry will be used for early stopping. If there’s more than one metric in eval_metric, the last metric will be used for early stopping. If early stopping occurs, the model will have three additional fields: , .\n• None verbose (bool) – If and an evaluation set is used, writes the evaluation metric measured on the validation set to stderr.\n• None xgb_model (Optional[Union[xgboost.core.Booster, xgboost.sklearn.XGBModel]]) – file name of stored XGBoost model or ‘Booster’ instance XGBoost model to be loaded before training (allows training continuation).\n• None sample_weight_eval_set (Optional[List[Union[da.Array, dd.DataFrame, dd.Series]]]) – A list of the form [L_1, L_2, …, L_n], where each L_i is an array like object storing instance weights for the i-th validation set.\n• None base_margin_eval_set (Optional[List[Union[da.Array, dd.DataFrame, dd.Series]]]) – A list of the form [M_1, M_2, …, M_n], where each M_i is an array like object storing base margin for the i-th validation set.\n• None feature_weights (Optional[Union[da.Array, dd.DataFrame, dd.Series]]) – Weight for each feature, defines the probability of each feature being selected when colsample is being used. All values must be greater than 0, otherwise a is thrown. Only available for , and tree methods.\n• None List of callback functions that are applied at end of each iteration. It is possible to use predefined callbacks by using Callback API. Example: Get the underlying xgboost Booster of this model. This will raise an exception when fit was not called Gets the number of xgboost boosting rounds. Intercept is defined only for linear learners Intercept (bias) is only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). Load the model from a file or bytearray. Path to file can be local or as an URI. The model is loaded from XGBoost format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be loaded when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. fname (Union[str, bytearray, os.PathLike]) – Input file name or memory buffer(see also save_raw) Predict with . If the model is trained with early stopping, then is used automatically. For tree models, when data is on GPU, like cupy array or cuDF dataframe and is not specified, the prediction is run on GPU automatically, otherwise it will run on CPU. This function is only thread safe for and .\n• None output_margin (bool) – Whether to output the raw untransformed margin value.\n• None validate_features (bool) – When this is True, validate that the Booster’s and data’s feature_names are identical. Otherwise, it is assumed that the feature_names are the same.\n• None Specifies which layer of trees are used in prediction. For example, if a random forest is trained with 100 rounds. Specifying , then only the forests built during [10, 20) (half open set) rounds are used in this prediction. Predict the probability of each example being of a given class. This function is only thread safe for and .\n• None validate_features (bool) – When this is True, validate that the Booster’s and data’s feature_names are identical. Otherwise, it is assumed that the feature_names are the same.\n• None iteration_range (Optional[Tuple[int, int]]) – Specifies which layer of trees are used in prediction. For example, if a random forest is trained with 100 rounds. Specifying , then only the forests built during [10, 20) (half open set) rounds are used in this prediction. a numpy array of shape array-like of shape (n_samples, n_classes) with the probability of each data example being of a given class. The model is saved in an XGBoost internal format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be saved when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. Set the parameters of this estimator. Modification of the sklearn method to allow unknown kwargs. This allows using the full range of xgboost parameters that are not defined as member variables in sklearn grid search. Implementation of the Scikit-Learn API for XGBoost.\n• None n_estimators (int) – Number of gradient boosted trees. Equivalent to number of boosting rounds.\n• None verbosity (Optional[int]) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n• None objective (typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n• None booster (Optional[str]) – Specify which booster to use: gbtree, gblinear or dart.\n• None tree_method (Optional[str]) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It’s recommended to study this option from the parameters document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n• None n_jobs (Optional[int]) – Number of parallel threads used to run xgboost. When used with other Scikit-Learn algorithms like grid search, you may choose which algorithm to parallelize and balance the threads. Creating thread contention will significantly slow down both algorithms.\n• None gamma (Optional[float]) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n• None max_delta_step (Optional[float]) – Maximum delta step we allow each tree’s weight estimation to be.\n• None colsample_bytree (Optional[float]) – Subsample ratio of columns when constructing each tree.\n• None colsample_bylevel (Optional[float]) – Subsample ratio of columns for each level.\n• None colsample_bynode (Optional[float]) – Subsample ratio of columns for each split.\n• None base_score (Optional[float]) – The initial prediction score of all instances, global bias.\n• Using gblinear booster with shotgun updater is nondeterministic as it uses Hogwild algorithm.\n• None missing (float, default np.nan) – Value in the data which needs to be present as a missing value.\n• None monotone_constraints (Optional[Union[Dict[str, int], str]]) – Constraint of variable monotonicity. See tutorial for more information.\n• None interaction_constraints (Optional[Union[str, List[Tuple[str]]]]) – Constraints for interaction representing permitted interactions. The constraints must be specified in the form of a nest list, e.g. [[0, 1], [2, 3, 4]], where each inner list is a group of indices of features that are allowed to interact with each other. See tutorial for more information\n• None The feature importance type for the feature_importances_ property:\n• None For tree model, it’s either “gain”, “weight”, “cover”, “total_gain” or “total_cover”.\n• None For linear model, only “weight” is defined and it’s the normalized coefficients without bias.\n• None predictor (Optional[str]) – Force XGBoost to use specific predictor, available choices are [cpu_predictor, gpu_predictor].\n• Experimental support for categorical data. Do not set to true unless you are interested in development. Only valid when and dataframe are used.\n• None Keyword arguments for XGBoost Booster object. Full documentation of parameters can be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst. Attempting to set a parameter via the constructor args and **kwargs dict simultaneously will result in a TypeError. **kwargs is unsupported by scikit-learn. We do not guarantee that parameters passed via this argument will interact properly with scikit-learn. Return the predicted leaf every tree for each sample. If the model is trained with early stopping, then is used automatically. X_leaves – For each datapoint x in X and for each tree, return the index of the leaf x ends up in. Leaves are numbered within , possibly with gaps in the numbering. The dask client used in this model. The object can not be serialized for transmission, so if task is launched from a worker instead of directly from the client process, this attribute needs to be set at that worker. Coefficients are defined only for linear learners Coefficients are only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). If eval_set is passed to the function, you can call to get evaluation results for all passed eval_sets. When eval_metric is also passed to the function, the evals_result will contain the eval_metrics passed to the function. The variable evals_result will contain:\n• None feature_importances_ (array of shape except for multi-class)\n• None linear model, which returns an array with shape Note that calling multiple times will cause the model object to be re-fit from scratch. To resume training from a previous checkpoint, explicitly pass argument.\n• None eval_set (Optional[List[Tuple[Union[da.Array, dd.DataFrame, dd.Series], Union[da.Array, dd.DataFrame, dd.Series]]]]) – A list of (X, y) tuple pairs to use as validation sets, for which metrics will be computed. Validation metrics will help us track the performance of the model.\n• None If a str, should be a built-in evaluation metric to use. See doc/parameter.rst. If a list of str, should be the list of multiple built-in evaluation metrics to use. If callable, a custom evaluation metric. The call signature is where will be a DMatrix object such that you may need to call the method. It must return a str, value pair where the str is a name for the evaluation and value is the value of the evaluation function. The callable custom objective is always minimized.\n• None Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. Requires at least one item in eval_set. The method returns the model from the last iteration (not the best one). If there’s more than one item in eval_set, the last entry will be used for early stopping. If there’s more than one metric in eval_metric, the last metric will be used for early stopping. If early stopping occurs, the model will have three additional fields: , .\n• None verbose (bool) – If and an evaluation set is used, writes the evaluation metric measured on the validation set to stderr.\n• None xgb_model (Optional[Union[xgboost.core.Booster, xgboost.sklearn.XGBModel]]) – file name of stored XGBoost model or ‘Booster’ instance XGBoost model to be loaded before training (allows training continuation).\n• None sample_weight_eval_set (Optional[List[Union[da.Array, dd.DataFrame, dd.Series]]]) – A list of the form [L_1, L_2, …, L_n], where each L_i is an array like object storing instance weights for the i-th validation set.\n• None base_margin_eval_set (Optional[List[Union[da.Array, dd.DataFrame, dd.Series]]]) – A list of the form [M_1, M_2, …, M_n], where each M_i is an array like object storing base margin for the i-th validation set.\n• None feature_weights (Optional[Union[da.Array, dd.DataFrame, dd.Series]]) – Weight for each feature, defines the probability of each feature being selected when colsample is being used. All values must be greater than 0, otherwise a is thrown. Only available for , and tree methods.\n• None List of callback functions that are applied at end of each iteration. It is possible to use predefined callbacks by using Callback API. Example: Get the underlying xgboost Booster of this model. This will raise an exception when fit was not called Gets the number of xgboost boosting rounds. Intercept is defined only for linear learners Intercept (bias) is only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). Load the model from a file or bytearray. Path to file can be local or as an URI. The model is loaded from XGBoost format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be loaded when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. fname (Union[str, bytearray, os.PathLike]) – Input file name or memory buffer(see also save_raw) Predict with . If the model is trained with early stopping, then is used automatically. For tree models, when data is on GPU, like cupy array or cuDF dataframe and is not specified, the prediction is run on GPU automatically, otherwise it will run on CPU. This function is only thread safe for and .\n• None output_margin (bool) – Whether to output the raw untransformed margin value.\n• None validate_features (bool) – When this is True, validate that the Booster’s and data’s feature_names are identical. Otherwise, it is assumed that the feature_names are the same.\n• None Specifies which layer of trees are used in prediction. For example, if a random forest is trained with 100 rounds. Specifying , then only the forests built during [10, 20) (half open set) rounds are used in this prediction. The model is saved in an XGBoost internal format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be saved when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. Set the parameters of this estimator. Modification of the sklearn method to allow unknown kwargs. This allows using the full range of xgboost parameters that are not defined as member variables in sklearn grid search. Implementation of the Scikit-Learn API for XGBoost Ranking.\n• None n_estimators (int) – Number of gradient boosted trees. Equivalent to number of boosting rounds.\n• None verbosity (Optional[int]) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n• None objective (typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n• None booster (Optional[str]) – Specify which booster to use: gbtree, gblinear or dart.\n• None tree_method (Optional[str]) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It’s recommended to study this option from the parameters document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n• None n_jobs (Optional[int]) – Number of parallel threads used to run xgboost. When used with other Scikit-Learn algorithms like grid search, you may choose which algorithm to parallelize and balance the threads. Creating thread contention will significantly slow down both algorithms.\n• None gamma (Optional[float]) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n• None max_delta_step (Optional[float]) – Maximum delta step we allow each tree’s weight estimation to be.\n• None colsample_bytree (Optional[float]) – Subsample ratio of columns when constructing each tree.\n• None colsample_bylevel (Optional[float]) – Subsample ratio of columns for each level.\n• None colsample_bynode (Optional[float]) – Subsample ratio of columns for each split.\n• None base_score (Optional[float]) – The initial prediction score of all instances, global bias.\n• Using gblinear booster with shotgun updater is nondeterministic as it uses Hogwild algorithm.\n• None missing (float, default np.nan) – Value in the data which needs to be present as a missing value.\n• None monotone_constraints (Optional[Union[Dict[str, int], str]]) – Constraint of variable monotonicity. See tutorial for more information.\n• None interaction_constraints (Optional[Union[str, List[Tuple[str]]]]) – Constraints for interaction representing permitted interactions. The constraints must be specified in the form of a nest list, e.g. [[0, 1], [2, 3, 4]], where each inner list is a group of indices of features that are allowed to interact with each other. See tutorial for more information\n• None The feature importance type for the feature_importances_ property:\n• None For tree model, it’s either “gain”, “weight”, “cover”, “total_gain” or “total_cover”.\n• None For linear model, only “weight” is defined and it’s the normalized coefficients without bias.\n• None predictor (Optional[str]) – Force XGBoost to use specific predictor, available choices are [cpu_predictor, gpu_predictor].\n• Experimental support for categorical data. Do not set to true unless you are interested in development. Only valid when and dataframe are used.\n• None Keyword arguments for XGBoost Booster object. Full documentation of parameters can be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst. Attempting to set a parameter via the constructor args and **kwargs dict simultaneously will result in a TypeError. **kwargs is unsupported by scikit-learn. We do not guarantee that parameters passed via this argument will interact properly with scikit-learn. For dask implementation, group is not supported, use qid instead. Return the predicted leaf every tree for each sample. If the model is trained with early stopping, then is used automatically. X_leaves – For each datapoint x in X and for each tree, return the index of the leaf x ends up in. Leaves are numbered within , possibly with gaps in the numbering. The dask client used in this model. The object can not be serialized for transmission, so if task is launched from a worker instead of directly from the client process, this attribute needs to be set at that worker. Coefficients are defined only for linear learners Coefficients are only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). If eval_set is passed to the function, you can call to get evaluation results for all passed eval_sets. When eval_metric is also passed to the function, the evals_result will contain the eval_metrics passed to the function. The variable evals_result will contain:\n• None feature_importances_ (array of shape except for multi-class)\n• None linear model, which returns an array with shape Note that calling multiple times will cause the model object to be re-fit from scratch. To resume training from a previous checkpoint, explicitly pass argument.\n• None group (Optional[Union[da.Array, dd.DataFrame, dd.Series]]) – Size of each query group of training data. Should have as many elements as the query groups in the training data. If this is set to None, then user must provide qid.\n• None qid (Optional[Union[da.Array, dd.DataFrame, dd.Series]]) – Query ID for each training sample. Should have the size of n_samples. If this is set to None, then user must provide group.\n• In ranking task, one weight is assigned to each query group/id (not each data point). This is because we only care about the relative ordering of data points within each group, so it doesn’t make sense to assign weights to individual data points.\n• None eval_set (Optional[List[Tuple[Union[da.Array, dd.DataFrame, dd.Series], Union[da.Array, dd.DataFrame, dd.Series]]]]) – A list of (X, y) tuple pairs to use as validation sets, for which metrics will be computed. Validation metrics will help us track the performance of the model.\n• None eval_group (Optional[List[Union[da.Array, dd.DataFrame, dd.Series]]]) – A list in which is the list containing the sizes of all query groups in the -th pair in eval_set.\n• None eval_qid (Optional[List[Union[da.Array, dd.DataFrame, dd.Series]]]) – A list in which is the array containing query ID of -th pair in eval_set.\n• None eval_metric (Optional[Union[str, List[str], Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]]]]) – If a str, should be a built-in evaluation metric to use. See doc/parameter.rst. If a list of str, should be the list of multiple built-in evaluation metrics to use. The custom evaluation metric is not yet supported for the ranker.\n• None early_stopping_rounds (int) – Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. Requires at least one item in eval_set. The method returns the model from the last iteration (not the best one). If there’s more than one item in eval_set, the last entry will be used for early stopping. If there’s more than one metric in eval_metric, the last metric will be used for early stopping. If early stopping occurs, the model will have three additional fields: , and .\n• None verbose (bool) – If and an evaluation set is used, writes the evaluation metric measured on the validation set to stderr.\n• None xgb_model (Optional[Union[xgboost.core.Booster, xgboost.sklearn.XGBModel]]) – file name of stored XGBoost model or ‘Booster’ instance XGBoost model to be loaded before training (allows training continuation).\n• None A list of the form [L_1, L_2, …, L_n], where each L_i is a list of group weights on the i-th validation set. In ranking task, one weight is assigned to each query group (not each data point). This is because we only care about the relative ordering of data points within each group, so it doesn’t make sense to assign weights to individual data points.\n• None base_margin_eval_set (Optional[List[Union[da.Array, dd.DataFrame, dd.Series]]]) – A list of the form [M_1, M_2, …, M_n], where each M_i is an array like object storing base margin for the i-th validation set.\n• None feature_weights (Optional[Union[da.Array, dd.DataFrame, dd.Series]]) – Weight for each feature, defines the probability of each feature being selected when colsample is being used. All values must be greater than 0, otherwise a is thrown. Only available for , and tree methods.\n• None List of callback functions that are applied at end of each iteration. It is possible to use predefined callbacks by using Callback API. Example: Get the underlying xgboost Booster of this model. This will raise an exception when fit was not called Gets the number of xgboost boosting rounds. Intercept is defined only for linear learners Intercept (bias) is only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). Load the model from a file or bytearray. Path to file can be local or as an URI. The model is loaded from XGBoost format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be loaded when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. fname (Union[str, bytearray, os.PathLike]) – Input file name or memory buffer(see also save_raw) Predict with . If the model is trained with early stopping, then is used automatically. For tree models, when data is on GPU, like cupy array or cuDF dataframe and is not specified, the prediction is run on GPU automatically, otherwise it will run on CPU. This function is only thread safe for and .\n• None output_margin (bool) – Whether to output the raw untransformed margin value.\n• None validate_features (bool) – When this is True, validate that the Booster’s and data’s feature_names are identical. Otherwise, it is assumed that the feature_names are the same.\n• None Specifies which layer of trees are used in prediction. For example, if a random forest is trained with 100 rounds. Specifying , then only the forests built during [10, 20) (half open set) rounds are used in this prediction. The model is saved in an XGBoost internal format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be saved when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. Set the parameters of this estimator. Modification of the sklearn method to allow unknown kwargs. This allows using the full range of xgboost parameters that are not defined as member variables in sklearn grid search. Implementation of the Scikit-Learn API for XGBoost Random Forest Regressor.\n• None n_estimators (int) – Number of trees in random forest to fit.\n• None verbosity (Optional[int]) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n• None objective (typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n• None booster (Optional[str]) – Specify which booster to use: gbtree, gblinear or dart.\n• None tree_method (Optional[str]) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It’s recommended to study this option from the parameters document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n• None n_jobs (Optional[int]) – Number of parallel threads used to run xgboost. When used with other Scikit-Learn algorithms like grid search, you may choose which algorithm to parallelize and balance the threads. Creating thread contention will significantly slow down both algorithms.\n• None gamma (Optional[float]) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n• None max_delta_step (Optional[float]) – Maximum delta step we allow each tree’s weight estimation to be.\n• None colsample_bytree (Optional[float]) – Subsample ratio of columns when constructing each tree.\n• None colsample_bylevel (Optional[float]) – Subsample ratio of columns for each level.\n• None colsample_bynode (Optional[float]) – Subsample ratio of columns for each split.\n• None base_score (Optional[float]) – The initial prediction score of all instances, global bias.\n• Using gblinear booster with shotgun updater is nondeterministic as it uses Hogwild algorithm.\n• None missing (float, default np.nan) – Value in the data which needs to be present as a missing value.\n• None monotone_constraints (Optional[Union[Dict[str, int], str]]) – Constraint of variable monotonicity. See tutorial for more information.\n• None interaction_constraints (Optional[Union[str, List[Tuple[str]]]]) – Constraints for interaction representing permitted interactions. The constraints must be specified in the form of a nest list, e.g. [[0, 1], [2, 3, 4]], where each inner list is a group of indices of features that are allowed to interact with each other. See tutorial for more information\n• None The feature importance type for the feature_importances_ property:\n• None For tree model, it’s either “gain”, “weight”, “cover”, “total_gain” or “total_cover”.\n• None For linear model, only “weight” is defined and it’s the normalized coefficients without bias.\n• None predictor (Optional[str]) – Force XGBoost to use specific predictor, available choices are [cpu_predictor, gpu_predictor].\n• Experimental support for categorical data. Do not set to true unless you are interested in development. Only valid when and dataframe are used.\n• None Keyword arguments for XGBoost Booster object. Full documentation of parameters can be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst. Attempting to set a parameter via the constructor args and **kwargs dict simultaneously will result in a TypeError. **kwargs is unsupported by scikit-learn. We do not guarantee that parameters passed via this argument will interact properly with scikit-learn. A custom objective function can be provided for the parameter. In this case, it should have the signature : The value of the gradient for each sample point. The value of the second derivative for each sample point Return the predicted leaf every tree for each sample. If the model is trained with early stopping, then is used automatically. X_leaves – For each datapoint x in X and for each tree, return the index of the leaf x ends up in. Leaves are numbered within , possibly with gaps in the numbering. The dask client used in this model. The object can not be serialized for transmission, so if task is launched from a worker instead of directly from the client process, this attribute needs to be set at that worker. Coefficients are defined only for linear learners Coefficients are only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). If eval_set is passed to the function, you can call to get evaluation results for all passed eval_sets. When eval_metric is also passed to the function, the evals_result will contain the eval_metrics passed to the function. The variable evals_result will contain:\n• None feature_importances_ (array of shape except for multi-class)\n• None linear model, which returns an array with shape Note that calling multiple times will cause the model object to be re-fit from scratch. To resume training from a previous checkpoint, explicitly pass argument.\n• None eval_set (Optional[List[Tuple[Union[da.Array, dd.DataFrame, dd.Series], Union[da.Array, dd.DataFrame, dd.Series]]]]) – A list of (X, y) tuple pairs to use as validation sets, for which metrics will be computed. Validation metrics will help us track the performance of the model.\n• None If a str, should be a built-in evaluation metric to use. See doc/parameter.rst. If a list of str, should be the list of multiple built-in evaluation metrics to use. If callable, a custom evaluation metric. The call signature is where will be a DMatrix object such that you may need to call the method. It must return a str, value pair where the str is a name for the evaluation and value is the value of the evaluation function. The callable custom objective is always minimized.\n• None Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. Requires at least one item in eval_set. The method returns the model from the last iteration (not the best one). If there’s more than one item in eval_set, the last entry will be used for early stopping. If there’s more than one metric in eval_metric, the last metric will be used for early stopping. If early stopping occurs, the model will have three additional fields: , .\n• None verbose (bool) – If and an evaluation set is used, writes the evaluation metric measured on the validation set to stderr.\n• None xgb_model (Optional[Union[xgboost.core.Booster, xgboost.sklearn.XGBModel]]) – file name of stored XGBoost model or ‘Booster’ instance XGBoost model to be loaded before training (allows training continuation).\n• None sample_weight_eval_set (Optional[List[Union[da.Array, dd.DataFrame, dd.Series]]]) – A list of the form [L_1, L_2, …, L_n], where each L_i is an array like object storing instance weights for the i-th validation set.\n• None base_margin_eval_set (Optional[List[Union[da.Array, dd.DataFrame, dd.Series]]]) – A list of the form [M_1, M_2, …, M_n], where each M_i is an array like object storing base margin for the i-th validation set.\n• None feature_weights (Optional[Union[da.Array, dd.DataFrame, dd.Series]]) – Weight for each feature, defines the probability of each feature being selected when colsample is being used. All values must be greater than 0, otherwise a is thrown. Only available for , and tree methods.\n• None List of callback functions that are applied at end of each iteration. It is possible to use predefined callbacks by using Callback API. Example: Get the underlying xgboost Booster of this model. This will raise an exception when fit was not called Gets the number of xgboost boosting rounds. Intercept is defined only for linear learners Intercept (bias) is only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). Load the model from a file or bytearray. Path to file can be local or as an URI. The model is loaded from XGBoost format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be loaded when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. fname (Union[str, bytearray, os.PathLike]) – Input file name or memory buffer(see also save_raw) Predict with . If the model is trained with early stopping, then is used automatically. For tree models, when data is on GPU, like cupy array or cuDF dataframe and is not specified, the prediction is run on GPU automatically, otherwise it will run on CPU. This function is only thread safe for and .\n• None output_margin (bool) – Whether to output the raw untransformed margin value.\n• None validate_features (bool) – When this is True, validate that the Booster’s and data’s feature_names are identical. Otherwise, it is assumed that the feature_names are the same.\n• None Specifies which layer of trees are used in prediction. For example, if a random forest is trained with 100 rounds. Specifying , then only the forests built during [10, 20) (half open set) rounds are used in this prediction. The model is saved in an XGBoost internal format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be saved when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. Set the parameters of this estimator. Modification of the sklearn method to allow unknown kwargs. This allows using the full range of xgboost parameters that are not defined as member variables in sklearn grid search. Implementation of the Scikit-Learn API for XGBoost Random Forest Classifier.\n• None n_estimators (int) – Number of trees in random forest to fit.\n• None verbosity (Optional[int]) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n• None objective (typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n• None booster (Optional[str]) – Specify which booster to use: gbtree, gblinear or dart.\n• None tree_method (Optional[str]) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It’s recommended to study this option from the parameters document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n• None n_jobs (Optional[int]) – Number of parallel threads used to run xgboost. When used with other Scikit-Learn algorithms like grid search, you may choose which algorithm to parallelize and balance the threads. Creating thread contention will significantly slow down both algorithms.\n• None gamma (Optional[float]) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n• None max_delta_step (Optional[float]) – Maximum delta step we allow each tree’s weight estimation to be.\n• None colsample_bytree (Optional[float]) – Subsample ratio of columns when constructing each tree.\n• None colsample_bylevel (Optional[float]) – Subsample ratio of columns for each level.\n• None colsample_bynode (Optional[float]) – Subsample ratio of columns for each split.\n• None base_score (Optional[float]) – The initial prediction score of all instances, global bias.\n• Using gblinear booster with shotgun updater is nondeterministic as it uses Hogwild algorithm.\n• None missing (float, default np.nan) – Value in the data which needs to be present as a missing value.\n• None monotone_constraints (Optional[Union[Dict[str, int], str]]) – Constraint of variable monotonicity. See tutorial for more information.\n• None interaction_constraints (Optional[Union[str, List[Tuple[str]]]]) – Constraints for interaction representing permitted interactions. The constraints must be specified in the form of a nest list, e.g. [[0, 1], [2, 3, 4]], where each inner list is a group of indices of features that are allowed to interact with each other. See tutorial for more information\n• None The feature importance type for the feature_importances_ property:\n• None For tree model, it’s either “gain”, “weight”, “cover”, “total_gain” or “total_cover”.\n• None For linear model, only “weight” is defined and it’s the normalized coefficients without bias.\n• None predictor (Optional[str]) – Force XGBoost to use specific predictor, available choices are [cpu_predictor, gpu_predictor].\n• Experimental support for categorical data. Do not set to true unless you are interested in development. Only valid when and dataframe are used.\n• None Keyword arguments for XGBoost Booster object. Full documentation of parameters can be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst. Attempting to set a parameter via the constructor args and **kwargs dict simultaneously will result in a TypeError. **kwargs is unsupported by scikit-learn. We do not guarantee that parameters passed via this argument will interact properly with scikit-learn. A custom objective function can be provided for the parameter. In this case, it should have the signature : The value of the gradient for each sample point. The value of the second derivative for each sample point Return the predicted leaf every tree for each sample. If the model is trained with early stopping, then is used automatically. X_leaves – For each datapoint x in X and for each tree, return the index of the leaf x ends up in. Leaves are numbered within , possibly with gaps in the numbering. The dask client used in this model. The object can not be serialized for transmission, so if task is launched from a worker instead of directly from the client process, this attribute needs to be set at that worker. Coefficients are defined only for linear learners Coefficients are only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). If eval_set is passed to the function, you can call to get evaluation results for all passed eval_sets. When eval_metric is also passed to the function, the evals_result will contain the eval_metrics passed to the function. The variable evals_result will contain:\n• None feature_importances_ (array of shape except for multi-class)\n• None linear model, which returns an array with shape Note that calling multiple times will cause the model object to be re-fit from scratch. To resume training from a previous checkpoint, explicitly pass argument.\n• None eval_set (Optional[List[Tuple[Union[da.Array, dd.DataFrame, dd.Series], Union[da.Array, dd.DataFrame, dd.Series]]]]) – A list of (X, y) tuple pairs to use as validation sets, for which metrics will be computed. Validation metrics will help us track the performance of the model.\n• None If a str, should be a built-in evaluation metric to use. See doc/parameter.rst. If a list of str, should be the list of multiple built-in evaluation metrics to use. If callable, a custom evaluation metric. The call signature is where will be a DMatrix object such that you may need to call the method. It must return a str, value pair where the str is a name for the evaluation and value is the value of the evaluation function. The callable custom objective is always minimized.\n• None Activates early stopping. Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training. Requires at least one item in eval_set. The method returns the model from the last iteration (not the best one). If there’s more than one item in eval_set, the last entry will be used for early stopping. If there’s more than one metric in eval_metric, the last metric will be used for early stopping. If early stopping occurs, the model will have three additional fields: , .\n• None verbose (bool) – If and an evaluation set is used, writes the evaluation metric measured on the validation set to stderr.\n• None xgb_model (Optional[Union[xgboost.core.Booster, xgboost.sklearn.XGBModel]]) – file name of stored XGBoost model or ‘Booster’ instance XGBoost model to be loaded before training (allows training continuation).\n• None sample_weight_eval_set (Optional[List[Union[da.Array, dd.DataFrame, dd.Series]]]) – A list of the form [L_1, L_2, …, L_n], where each L_i is an array like object storing instance weights for the i-th validation set.\n• None base_margin_eval_set (Optional[List[Union[da.Array, dd.DataFrame, dd.Series]]]) – A list of the form [M_1, M_2, …, M_n], where each M_i is an array like object storing base margin for the i-th validation set.\n• None feature_weights (Optional[Union[da.Array, dd.DataFrame, dd.Series]]) – Weight for each feature, defines the probability of each feature being selected when colsample is being used. All values must be greater than 0, otherwise a is thrown. Only available for , and tree methods.\n• None List of callback functions that are applied at end of each iteration. It is possible to use predefined callbacks by using Callback API. Example: Get the underlying xgboost Booster of this model. This will raise an exception when fit was not called Gets the number of xgboost boosting rounds. Intercept is defined only for linear learners Intercept (bias) is only defined when the linear model is chosen as base learner ( ). It is not defined for other base learner types, such as tree learners ( ). Load the model from a file or bytearray. Path to file can be local or as an URI. The model is loaded from XGBoost format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be loaded when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. fname (Union[str, bytearray, os.PathLike]) – Input file name or memory buffer(see also save_raw) Predict with . If the model is trained with early stopping, then is used automatically. For tree models, when data is on GPU, like cupy array or cuDF dataframe and is not specified, the prediction is run on GPU automatically, otherwise it will run on CPU. This function is only thread safe for and .\n• None output_margin (bool) – Whether to output the raw untransformed margin value.\n• None validate_features (bool) – When this is True, validate that the Booster’s and data’s feature_names are identical. Otherwise, it is assumed that the feature_names are the same.\n• None Specifies which layer of trees are used in prediction. For example, if a random forest is trained with 100 rounds. Specifying , then only the forests built during [10, 20) (half open set) rounds are used in this prediction. Predict the probability of each example being of a given class. This function is only thread safe for and .\n• None validate_features (bool) – When this is True, validate that the Booster’s and data’s feature_names are identical. Otherwise, it is assumed that the feature_names are the same.\n• None iteration_range (Optional[Tuple[int, int]]) – Specifies which layer of trees are used in prediction. For example, if a random forest is trained with 100 rounds. Specifying , then only the forests built during [10, 20) (half open set) rounds are used in this prediction. a numpy array of shape array-like of shape (n_samples, n_classes) with the probability of each data example being of a given class. The model is saved in an XGBoost internal format which is universal among the various XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as feature_names) will not be saved when using binary format. To save those attributes, use JSON instead. See: Model IO for more info. Set the parameters of this estimator. Modification of the sklearn method to allow unknown kwargs. This allows using the full range of xgboost parameters that are not defined as member variables in sklearn grid search."
    },
    {
        "link": "https://xgboost.readthedocs.io/en/latest/python/python_api.html",
        "document": "This page gives the Python API reference of xgboost, please also refer to Python Package Introduction for more information about the Python package.\n\nSparkXGBClassifier is a PySpark ML estimator. It implements the XGBoost classification algorithm based on XGBoost python library, and it can be used in PySpark Pipeline and PySpark ML meta algorithms like - / - / - SparkXGBClassifier automatically supports most of the parameters in constructor and most of the parameters used in and method. To enable GPU support, set to or . SparkXGBClassifier doesn’t support setting explicitly as well, but support another param called . see doc below for more details. SparkXGBClassifier doesn’t support setting , but we can get output margin from the raw prediction column. See param doc below for more details. SparkXGBClassifier doesn’t support setting xgboost param, instead, the param for each xgboost worker will be set equal to config value.\n• None features_col (str | List[str]) – When the value is string, it requires the features column name to be vector type. When the value is a list of string, it requires all the feature columns to be numeric types.\n• None probability_col (str) – Column name for predicted class conditional probabilities. Default to probabilityCol\n• None raw_prediction_col (str) – The is implicitly supported by the output column, which is always returned with the predicted margin values.\n• None validation_indicator_col (str | None) – For params related to training with evaluation dataset’s supervision, set parameter instead of setting the parameter in fit method.\n• None weight_col (str | None) – To specify the weight of the training and validation dataset, set parameter instead of setting and parameter in fit method.\n• None base_margin_col (str | None) – To specify the base margins of the training and validation dataset, set parameter instead of setting and in the fit method.\n• None num_workers (int) – How many XGBoost workers to be used to train. Each XGBoost worker corresponds to one spark task.\n• Device for XGBoost workers, available options are , , and .\n• None force_repartition (bool) – Boolean value to specify if forcing the input dataset to be repartitioned before XGBoost training.\n• None repartition_random_shuffle (bool) – Boolean value to specify if randomly shuffling the dataset when repartitioning is required.\n• None enable_sparse_data_optim (bool) – Boolean value to specify if enabling sparse data optimization, if True, Xgboost DMatrix object will be constructed from sparse matrix instead of dense matrix.\n• None launch_tracker_on_driver (bool) – Boolean value to indicate whether the tracker should be launched on the driver side or the executor side.\n• None coll_cfg (Config | None) – The collective configuration. See\n• None kwargs (Any) – A dictionary of xgboost parameters, please refer to https://xgboost.readthedocs.io/en/stable/parameter.html The Parameters chart above contains parameters that need special handling. For a full list of parameters, see entries with below. Clears a param from the param map if it has been explicitly set. Creates a copy of this instance with the same uid and some extra params. The default implementation creates a shallow copy using , and then copies the embedded and extra parameters over and returns the copy. Subclasses should override this method if the default approach is not sufficient.\n• None extra (dict, optional) – Extra parameters to copy to the new instance Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string. Returns the documentation of all params with their optionally default values and user-supplied values. Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values < user-supplied values < extra. Fits a model to the input dataset with optional parameters.\n• None params (dict or list or tuple, optional) – an optional param map that overrides embedded params. If a list/tuple of param maps is given, this calls fit on each param map and returns a list of models. Fits a model to the input dataset for each param map in . A thread safe iterable which contains one model for each param map. Each call to will return where model was fit using . values may not be sequential. Gets the value of featuresCol or its default value. Gets the value of labelCol or its default value. Gets the value of a param in the user-supplied param map or its default value. Raises an error if neither is set. Gets a param by its name. Gets the value of predictionCol or its default value. Gets the value of probabilityCol or its default value. Gets the value of rawPredictionCol or its default value. Gets the value of validationIndicatorCol or its default value. Gets the value of weightCol or its default value. Checks whether a param has a default value. Tests whether this instance contains a param with a given (string) name. Checks whether a param is explicitly set by user or has a default value. Checks whether a param is explicitly set by user. Reads an ML instance from the input path, a shortcut of . Returns all params ordered by name. The default implementation uses to get all attributes of type . Return the reader for loading the estimator. Save this ML instance to the given path, a shortcut of ‘write().save(path)’. Return the writer for saving the estimator. Clears a param from the param map if it has been explicitly set. Creates a copy of this instance with the same uid and some extra params. The default implementation creates a shallow copy using , and then copies the embedded and extra parameters over and returns the copy. Subclasses should override this method if the default approach is not sufficient.\n• None extra (dict, optional) – Extra parameters to copy to the new instance Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string. Returns the documentation of all params with their optionally default values and user-supplied values. Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values < user-supplied values < extra. Gets the value of featuresCol or its default value. Gets the value of labelCol or its default value. Gets the value of a param in the user-supplied param map or its default value. Raises an error if neither is set. Gets a param by its name. Gets the value of predictionCol or its default value. Gets the value of probabilityCol or its default value. Gets the value of rawPredictionCol or its default value. Gets the value of validationIndicatorCol or its default value. Gets the value of weightCol or its default value. Get feature importance of each feature. Importance type can be defined as:\n• None ‘weight’: the number of times a feature is used to split the data across all trees.\n• None ‘gain’: the average gain across all splits the feature is used in.\n• None ‘cover’: the average coverage across all splits the feature is used in.\n• None ‘total_gain’: the total gain across all splits the feature is used in.\n• None ‘total_cover’: the total coverage across all splits the feature is used in. importance_type (str, default 'weight') – One of the importance types defined above. Checks whether a param has a default value. Tests whether this instance contains a param with a given (string) name. Checks whether a param is explicitly set by user or has a default value. Checks whether a param is explicitly set by user. Reads an ML instance from the input path, a shortcut of . Returns all params ordered by name. The default implementation uses to get all attributes of type . Return the reader for loading the model. Save this ML instance to the given path, a shortcut of ‘write().save(path)’. Return the writer for saving the model. SparkXGBRegressor is a PySpark ML estimator. It implements the XGBoost regression algorithm based on XGBoost python library, and it can be used in PySpark Pipeline and PySpark ML meta algorithms like - / - / - SparkXGBRegressor automatically supports most of the parameters in constructor and most of the parameters used in and method. To enable GPU support, set to or . SparkXGBRegressor doesn’t support setting explicitly as well, but support another param called . see doc below for more details. SparkXGBRegressor doesn’t support setting xgboost param, instead, the param for each xgboost worker will be set equal to config value.\n• None features_col (str | List[str]) – When the value is string, it requires the features column name to be vector type. When the value is a list of string, it requires all the feature columns to be numeric types.\n• None validation_indicator_col (str | None) – For params related to training with evaluation dataset’s supervision, set parameter instead of setting the parameter in fit method.\n• None weight_col (str | None) – To specify the weight of the training and validation dataset, set parameter instead of setting and parameter in fit method.\n• None base_margin_col (str | None) – To specify the base margins of the training and validation dataset, set parameter instead of setting and in the fit method.\n• None num_workers (int) – How many XGBoost workers to be used to train. Each XGBoost worker corresponds to one spark task.\n• Device for XGBoost workers, available options are , , and .\n• None force_repartition (bool) – Boolean value to specify if forcing the input dataset to be repartitioned before XGBoost training.\n• None repartition_random_shuffle (bool) – Boolean value to specify if randomly shuffling the dataset when repartitioning is required.\n• None enable_sparse_data_optim (bool) – Boolean value to specify if enabling sparse data optimization, if True, Xgboost DMatrix object will be constructed from sparse matrix instead of dense matrix.\n• None launch_tracker_on_driver (bool) – Boolean value to indicate whether the tracker should be launched on the driver side or the executor side.\n• None coll_cfg (Config | None) – The collective configuration. See\n• None kwargs (Any) – A dictionary of xgboost parameters, please refer to https://xgboost.readthedocs.io/en/stable/parameter.html The Parameters chart above contains parameters that need special handling. For a full list of parameters, see entries with below. Clears a param from the param map if it has been explicitly set. Creates a copy of this instance with the same uid and some extra params. The default implementation creates a shallow copy using , and then copies the embedded and extra parameters over and returns the copy. Subclasses should override this method if the default approach is not sufficient.\n• None extra (dict, optional) – Extra parameters to copy to the new instance Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string. Returns the documentation of all params with their optionally default values and user-supplied values. Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values < user-supplied values < extra. Fits a model to the input dataset with optional parameters.\n• None params (dict or list or tuple, optional) – an optional param map that overrides embedded params. If a list/tuple of param maps is given, this calls fit on each param map and returns a list of models. Fits a model to the input dataset for each param map in . A thread safe iterable which contains one model for each param map. Each call to will return where model was fit using . values may not be sequential. Gets the value of featuresCol or its default value. Gets the value of labelCol or its default value. Gets the value of a param in the user-supplied param map or its default value. Raises an error if neither is set. Gets a param by its name. Gets the value of predictionCol or its default value. Gets the value of validationIndicatorCol or its default value. Gets the value of weightCol or its default value. Checks whether a param has a default value. Tests whether this instance contains a param with a given (string) name. Checks whether a param is explicitly set by user or has a default value. Checks whether a param is explicitly set by user. Reads an ML instance from the input path, a shortcut of . Returns all params ordered by name. The default implementation uses to get all attributes of type . Return the reader for loading the estimator. Save this ML instance to the given path, a shortcut of ‘write().save(path)’. Return the writer for saving the estimator. Clears a param from the param map if it has been explicitly set. Creates a copy of this instance with the same uid and some extra params. The default implementation creates a shallow copy using , and then copies the embedded and extra parameters over and returns the copy. Subclasses should override this method if the default approach is not sufficient.\n• None extra (dict, optional) – Extra parameters to copy to the new instance Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string. Returns the documentation of all params with their optionally default values and user-supplied values. Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values < user-supplied values < extra. Gets the value of featuresCol or its default value. Gets the value of labelCol or its default value. Gets the value of a param in the user-supplied param map or its default value. Raises an error if neither is set. Gets a param by its name. Gets the value of predictionCol or its default value. Gets the value of validationIndicatorCol or its default value. Gets the value of weightCol or its default value. Get feature importance of each feature. Importance type can be defined as:\n• None ‘weight’: the number of times a feature is used to split the data across all trees.\n• None ‘gain’: the average gain across all splits the feature is used in.\n• None ‘cover’: the average coverage across all splits the feature is used in.\n• None ‘total_gain’: the total gain across all splits the feature is used in.\n• None ‘total_cover’: the total coverage across all splits the feature is used in. importance_type (str, default 'weight') – One of the importance types defined above. Checks whether a param has a default value. Tests whether this instance contains a param with a given (string) name. Checks whether a param is explicitly set by user or has a default value. Checks whether a param is explicitly set by user. Reads an ML instance from the input path, a shortcut of . Returns all params ordered by name. The default implementation uses to get all attributes of type . Return the reader for loading the model. Save this ML instance to the given path, a shortcut of ‘write().save(path)’. Return the writer for saving the model. SparkXGBRanker is a PySpark ML estimator. It implements the XGBoost ranking algorithm based on XGBoost python library, and it can be used in PySpark Pipeline and PySpark ML meta algorithms like / / SparkXGBRanker automatically supports most of the parameters in constructor and most of the parameters used in and method. To enable GPU support, set to or . SparkXGBRanker doesn’t support setting explicitly as well, but support another param called . see doc below for more details. SparkXGBRanker doesn’t support setting , but we can get output margin from the raw prediction column. See param doc below for more details. SparkXGBRanker doesn’t support setting xgboost param, instead, the param for each xgboost worker will be set equal to config value.\n• None features_col (str | List[str]) – When the value is string, it requires the features column name to be vector type. When the value is a list of string, it requires all the feature columns to be numeric types.\n• None validation_indicator_col (str | None) – For params related to training with evaluation dataset’s supervision, set parameter instead of setting the parameter in fit method.\n• None weight_col (str | None) – To specify the weight of the training and validation dataset, set parameter instead of setting and parameter in fit method.\n• None base_margin_col (str | None) – To specify the base margins of the training and validation dataset, set parameter instead of setting and in the fit method.\n• None num_workers (int) – How many XGBoost workers to be used to train. Each XGBoost worker corresponds to one spark task.\n• Device for XGBoost workers, available options are , , and .\n• None force_repartition (bool) – Boolean value to specify if forcing the input dataset to be repartitioned before XGBoost training.\n• None repartition_random_shuffle (bool) – Boolean value to specify if randomly shuffling the dataset when repartitioning is required.\n• None enable_sparse_data_optim (bool) – Boolean value to specify if enabling sparse data optimization, if True, Xgboost DMatrix object will be constructed from sparse matrix instead of dense matrix.\n• None launch_tracker_on_driver (bool) – Boolean value to indicate whether the tracker should be launched on the driver side or the executor side.\n• None coll_cfg (Config | None) – The collective configuration. See\n• None kwargs (Any) – A dictionary of xgboost parameters, please refer to https://xgboost.readthedocs.io/en/stable/parameter.html\n• None Note: (..) – The Parameters chart above contains parameters that need special handling.: For a full list of parameters, see entries with below. Clears a param from the param map if it has been explicitly set. Creates a copy of this instance with the same uid and some extra params. The default implementation creates a shallow copy using , and then copies the embedded and extra parameters over and returns the copy. Subclasses should override this method if the default approach is not sufficient.\n• None extra (dict, optional) – Extra parameters to copy to the new instance Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string. Returns the documentation of all params with their optionally default values and user-supplied values. Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values < user-supplied values < extra. Fits a model to the input dataset with optional parameters.\n• None params (dict or list or tuple, optional) – an optional param map that overrides embedded params. If a list/tuple of param maps is given, this calls fit on each param map and returns a list of models. Fits a model to the input dataset for each param map in . A thread safe iterable which contains one model for each param map. Each call to will return where model was fit using . values may not be sequential. Gets the value of featuresCol or its default value. Gets the value of labelCol or its default value. Gets the value of a param in the user-supplied param map or its default value. Raises an error if neither is set. Gets a param by its name. Gets the value of predictionCol or its default value. Gets the value of validationIndicatorCol or its default value. Gets the value of weightCol or its default value. Checks whether a param has a default value. Tests whether this instance contains a param with a given (string) name. Checks whether a param is explicitly set by user or has a default value. Checks whether a param is explicitly set by user. Reads an ML instance from the input path, a shortcut of . Returns all params ordered by name. The default implementation uses to get all attributes of type . Return the reader for loading the estimator. Save this ML instance to the given path, a shortcut of ‘write().save(path)’. Return the writer for saving the estimator. Clears a param from the param map if it has been explicitly set. Creates a copy of this instance with the same uid and some extra params. The default implementation creates a shallow copy using , and then copies the embedded and extra parameters over and returns the copy. Subclasses should override this method if the default approach is not sufficient.\n• None extra (dict, optional) – Extra parameters to copy to the new instance Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string. Returns the documentation of all params with their optionally default values and user-supplied values. Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values < user-supplied values < extra. Gets the value of featuresCol or its default value. Gets the value of labelCol or its default value. Gets the value of a param in the user-supplied param map or its default value. Raises an error if neither is set. Gets a param by its name. Gets the value of predictionCol or its default value. Gets the value of validationIndicatorCol or its default value. Gets the value of weightCol or its default value. Get feature importance of each feature. Importance type can be defined as:\n• None ‘weight’: the number of times a feature is used to split the data across all trees.\n• None ‘gain’: the average gain across all splits the feature is used in.\n• None ‘cover’: the average coverage across all splits the feature is used in.\n• None ‘total_gain’: the total gain across all splits the feature is used in.\n• None ‘total_cover’: the total coverage across all splits the feature is used in. importance_type (str, default 'weight') – One of the importance types defined above. Checks whether a param has a default value. Tests whether this instance contains a param with a given (string) name. Checks whether a param is explicitly set by user or has a default value. Checks whether a param is explicitly set by user. Reads an ML instance from the input path, a shortcut of . Returns all params ordered by name. The default implementation uses to get all attributes of type . Return the reader for loading the model. Save this ML instance to the given path, a shortcut of ‘write().save(path)’. Return the writer for saving the model."
    },
    {
        "link": "https://xgboost.readthedocs.io/en/stable/python/python_api.html",
        "document": "This page gives the Python API reference of xgboost, please also refer to Python Package Introduction for more information about the Python package.\n\nSparkXGBClassifier is a PySpark ML estimator. It implements the XGBoost classification algorithm based on XGBoost python library, and it can be used in PySpark Pipeline and PySpark ML meta algorithms like - / - / - SparkXGBClassifier automatically supports most of the parameters in constructor and most of the parameters used in and method. To enable GPU support, set to or . SparkXGBClassifier doesn’t support setting explicitly as well, but support another param called . see doc below for more details. SparkXGBClassifier doesn’t support setting , but we can get output margin from the raw prediction column. See param doc below for more details. SparkXGBClassifier doesn’t support setting xgboost param, instead, the param for each xgboost worker will be set equal to config value.\n• None features_col (str | List[str]) – When the value is string, it requires the features column name to be vector type. When the value is a list of string, it requires all the feature columns to be numeric types.\n• None probability_col (str) – Column name for predicted class conditional probabilities. Default to probabilityCol\n• None raw_prediction_col (str) – The is implicitly supported by the output column, which is always returned with the predicted margin values.\n• None validation_indicator_col (str | None) – For params related to training with evaluation dataset’s supervision, set parameter instead of setting the parameter in fit method.\n• None weight_col (str | None) – To specify the weight of the training and validation dataset, set parameter instead of setting and parameter in fit method.\n• None base_margin_col (str | None) – To specify the base margins of the training and validation dataset, set parameter instead of setting and in the fit method.\n• None num_workers (int) – How many XGBoost workers to be used to train. Each XGBoost worker corresponds to one spark task.\n• Device for XGBoost workers, available options are , , and .\n• None force_repartition (bool) – Boolean value to specify if forcing the input dataset to be repartitioned before XGBoost training.\n• None repartition_random_shuffle (bool) – Boolean value to specify if randomly shuffling the dataset when repartitioning is required.\n• None enable_sparse_data_optim (bool) – Boolean value to specify if enabling sparse data optimization, if True, Xgboost DMatrix object will be constructed from sparse matrix instead of dense matrix.\n• None launch_tracker_on_driver (bool) – Boolean value to indicate whether the tracker should be launched on the driver side or the executor side.\n• None coll_cfg (Config | None) – The collective configuration. See\n• None kwargs (Any) – A dictionary of xgboost parameters, please refer to https://xgboost.readthedocs.io/en/stable/parameter.html The Parameters chart above contains parameters that need special handling. For a full list of parameters, see entries with below. Clears a param from the param map if it has been explicitly set. Creates a copy of this instance with the same uid and some extra params. The default implementation creates a shallow copy using , and then copies the embedded and extra parameters over and returns the copy. Subclasses should override this method if the default approach is not sufficient.\n• None extra (dict, optional) – Extra parameters to copy to the new instance Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string. Returns the documentation of all params with their optionally default values and user-supplied values. Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values < user-supplied values < extra. Fits a model to the input dataset with optional parameters.\n• None params (dict or list or tuple, optional) – an optional param map that overrides embedded params. If a list/tuple of param maps is given, this calls fit on each param map and returns a list of models. Fits a model to the input dataset for each param map in . A thread safe iterable which contains one model for each param map. Each call to will return where model was fit using . values may not be sequential. Gets the value of featuresCol or its default value. Gets the value of labelCol or its default value. Gets the value of a param in the user-supplied param map or its default value. Raises an error if neither is set. Gets a param by its name. Gets the value of predictionCol or its default value. Gets the value of probabilityCol or its default value. Gets the value of rawPredictionCol or its default value. Gets the value of validationIndicatorCol or its default value. Gets the value of weightCol or its default value. Checks whether a param has a default value. Tests whether this instance contains a param with a given (string) name. Checks whether a param is explicitly set by user or has a default value. Checks whether a param is explicitly set by user. Reads an ML instance from the input path, a shortcut of . Returns all params ordered by name. The default implementation uses to get all attributes of type . Return the reader for loading the estimator. Save this ML instance to the given path, a shortcut of ‘write().save(path)’. Return the writer for saving the estimator. Clears a param from the param map if it has been explicitly set. Creates a copy of this instance with the same uid and some extra params. The default implementation creates a shallow copy using , and then copies the embedded and extra parameters over and returns the copy. Subclasses should override this method if the default approach is not sufficient.\n• None extra (dict, optional) – Extra parameters to copy to the new instance Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string. Returns the documentation of all params with their optionally default values and user-supplied values. Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values < user-supplied values < extra. Gets the value of featuresCol or its default value. Gets the value of labelCol or its default value. Gets the value of a param in the user-supplied param map or its default value. Raises an error if neither is set. Gets a param by its name. Gets the value of predictionCol or its default value. Gets the value of probabilityCol or its default value. Gets the value of rawPredictionCol or its default value. Gets the value of validationIndicatorCol or its default value. Gets the value of weightCol or its default value. Get feature importance of each feature. Importance type can be defined as:\n• None ‘weight’: the number of times a feature is used to split the data across all trees.\n• None ‘gain’: the average gain across all splits the feature is used in.\n• None ‘cover’: the average coverage across all splits the feature is used in.\n• None ‘total_gain’: the total gain across all splits the feature is used in.\n• None ‘total_cover’: the total coverage across all splits the feature is used in. importance_type (str, default 'weight') – One of the importance types defined above. Checks whether a param has a default value. Tests whether this instance contains a param with a given (string) name. Checks whether a param is explicitly set by user or has a default value. Checks whether a param is explicitly set by user. Reads an ML instance from the input path, a shortcut of . Returns all params ordered by name. The default implementation uses to get all attributes of type . Return the reader for loading the model. Save this ML instance to the given path, a shortcut of ‘write().save(path)’. Return the writer for saving the model. SparkXGBRegressor is a PySpark ML estimator. It implements the XGBoost regression algorithm based on XGBoost python library, and it can be used in PySpark Pipeline and PySpark ML meta algorithms like - / - / - SparkXGBRegressor automatically supports most of the parameters in constructor and most of the parameters used in and method. To enable GPU support, set to or . SparkXGBRegressor doesn’t support setting explicitly as well, but support another param called . see doc below for more details. SparkXGBRegressor doesn’t support setting xgboost param, instead, the param for each xgboost worker will be set equal to config value.\n• None features_col (str | List[str]) – When the value is string, it requires the features column name to be vector type. When the value is a list of string, it requires all the feature columns to be numeric types.\n• None validation_indicator_col (str | None) – For params related to training with evaluation dataset’s supervision, set parameter instead of setting the parameter in fit method.\n• None weight_col (str | None) – To specify the weight of the training and validation dataset, set parameter instead of setting and parameter in fit method.\n• None base_margin_col (str | None) – To specify the base margins of the training and validation dataset, set parameter instead of setting and in the fit method.\n• None num_workers (int) – How many XGBoost workers to be used to train. Each XGBoost worker corresponds to one spark task.\n• Device for XGBoost workers, available options are , , and .\n• None force_repartition (bool) – Boolean value to specify if forcing the input dataset to be repartitioned before XGBoost training.\n• None repartition_random_shuffle (bool) – Boolean value to specify if randomly shuffling the dataset when repartitioning is required.\n• None enable_sparse_data_optim (bool) – Boolean value to specify if enabling sparse data optimization, if True, Xgboost DMatrix object will be constructed from sparse matrix instead of dense matrix.\n• None launch_tracker_on_driver (bool) – Boolean value to indicate whether the tracker should be launched on the driver side or the executor side.\n• None coll_cfg (Config | None) – The collective configuration. See\n• None kwargs (Any) – A dictionary of xgboost parameters, please refer to https://xgboost.readthedocs.io/en/stable/parameter.html The Parameters chart above contains parameters that need special handling. For a full list of parameters, see entries with below. Clears a param from the param map if it has been explicitly set. Creates a copy of this instance with the same uid and some extra params. The default implementation creates a shallow copy using , and then copies the embedded and extra parameters over and returns the copy. Subclasses should override this method if the default approach is not sufficient.\n• None extra (dict, optional) – Extra parameters to copy to the new instance Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string. Returns the documentation of all params with their optionally default values and user-supplied values. Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values < user-supplied values < extra. Fits a model to the input dataset with optional parameters.\n• None params (dict or list or tuple, optional) – an optional param map that overrides embedded params. If a list/tuple of param maps is given, this calls fit on each param map and returns a list of models. Fits a model to the input dataset for each param map in . A thread safe iterable which contains one model for each param map. Each call to will return where model was fit using . values may not be sequential. Gets the value of featuresCol or its default value. Gets the value of labelCol or its default value. Gets the value of a param in the user-supplied param map or its default value. Raises an error if neither is set. Gets a param by its name. Gets the value of predictionCol or its default value. Gets the value of validationIndicatorCol or its default value. Gets the value of weightCol or its default value. Checks whether a param has a default value. Tests whether this instance contains a param with a given (string) name. Checks whether a param is explicitly set by user or has a default value. Checks whether a param is explicitly set by user. Reads an ML instance from the input path, a shortcut of . Returns all params ordered by name. The default implementation uses to get all attributes of type . Return the reader for loading the estimator. Save this ML instance to the given path, a shortcut of ‘write().save(path)’. Return the writer for saving the estimator. Clears a param from the param map if it has been explicitly set. Creates a copy of this instance with the same uid and some extra params. The default implementation creates a shallow copy using , and then copies the embedded and extra parameters over and returns the copy. Subclasses should override this method if the default approach is not sufficient.\n• None extra (dict, optional) – Extra parameters to copy to the new instance Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string. Returns the documentation of all params with their optionally default values and user-supplied values. Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values < user-supplied values < extra. Gets the value of featuresCol or its default value. Gets the value of labelCol or its default value. Gets the value of a param in the user-supplied param map or its default value. Raises an error if neither is set. Gets a param by its name. Gets the value of predictionCol or its default value. Gets the value of validationIndicatorCol or its default value. Gets the value of weightCol or its default value. Get feature importance of each feature. Importance type can be defined as:\n• None ‘weight’: the number of times a feature is used to split the data across all trees.\n• None ‘gain’: the average gain across all splits the feature is used in.\n• None ‘cover’: the average coverage across all splits the feature is used in.\n• None ‘total_gain’: the total gain across all splits the feature is used in.\n• None ‘total_cover’: the total coverage across all splits the feature is used in. importance_type (str, default 'weight') – One of the importance types defined above. Checks whether a param has a default value. Tests whether this instance contains a param with a given (string) name. Checks whether a param is explicitly set by user or has a default value. Checks whether a param is explicitly set by user. Reads an ML instance from the input path, a shortcut of . Returns all params ordered by name. The default implementation uses to get all attributes of type . Return the reader for loading the model. Save this ML instance to the given path, a shortcut of ‘write().save(path)’. Return the writer for saving the model. SparkXGBRanker is a PySpark ML estimator. It implements the XGBoost ranking algorithm based on XGBoost python library, and it can be used in PySpark Pipeline and PySpark ML meta algorithms like / / SparkXGBRanker automatically supports most of the parameters in constructor and most of the parameters used in and method. To enable GPU support, set to or . SparkXGBRanker doesn’t support setting explicitly as well, but support another param called . see doc below for more details. SparkXGBRanker doesn’t support setting , but we can get output margin from the raw prediction column. See param doc below for more details. SparkXGBRanker doesn’t support setting xgboost param, instead, the param for each xgboost worker will be set equal to config value.\n• None features_col (str | List[str]) – When the value is string, it requires the features column name to be vector type. When the value is a list of string, it requires all the feature columns to be numeric types.\n• None validation_indicator_col (str | None) – For params related to training with evaluation dataset’s supervision, set parameter instead of setting the parameter in fit method.\n• None weight_col (str | None) – To specify the weight of the training and validation dataset, set parameter instead of setting and parameter in fit method.\n• None base_margin_col (str | None) – To specify the base margins of the training and validation dataset, set parameter instead of setting and in the fit method.\n• None num_workers (int) – How many XGBoost workers to be used to train. Each XGBoost worker corresponds to one spark task.\n• Device for XGBoost workers, available options are , , and .\n• None force_repartition (bool) – Boolean value to specify if forcing the input dataset to be repartitioned before XGBoost training.\n• None repartition_random_shuffle (bool) – Boolean value to specify if randomly shuffling the dataset when repartitioning is required.\n• None enable_sparse_data_optim (bool) – Boolean value to specify if enabling sparse data optimization, if True, Xgboost DMatrix object will be constructed from sparse matrix instead of dense matrix.\n• None launch_tracker_on_driver (bool) – Boolean value to indicate whether the tracker should be launched on the driver side or the executor side.\n• None coll_cfg (Config | None) – The collective configuration. See\n• None kwargs (Any) – A dictionary of xgboost parameters, please refer to https://xgboost.readthedocs.io/en/stable/parameter.html\n• None Note: (..) – The Parameters chart above contains parameters that need special handling.: For a full list of parameters, see entries with below. Clears a param from the param map if it has been explicitly set. Creates a copy of this instance with the same uid and some extra params. The default implementation creates a shallow copy using , and then copies the embedded and extra parameters over and returns the copy. Subclasses should override this method if the default approach is not sufficient.\n• None extra (dict, optional) – Extra parameters to copy to the new instance Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string. Returns the documentation of all params with their optionally default values and user-supplied values. Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values < user-supplied values < extra. Fits a model to the input dataset with optional parameters.\n• None params (dict or list or tuple, optional) – an optional param map that overrides embedded params. If a list/tuple of param maps is given, this calls fit on each param map and returns a list of models. Fits a model to the input dataset for each param map in . A thread safe iterable which contains one model for each param map. Each call to will return where model was fit using . values may not be sequential. Gets the value of featuresCol or its default value. Gets the value of labelCol or its default value. Gets the value of a param in the user-supplied param map or its default value. Raises an error if neither is set. Gets a param by its name. Gets the value of predictionCol or its default value. Gets the value of validationIndicatorCol or its default value. Gets the value of weightCol or its default value. Checks whether a param has a default value. Tests whether this instance contains a param with a given (string) name. Checks whether a param is explicitly set by user or has a default value. Checks whether a param is explicitly set by user. Reads an ML instance from the input path, a shortcut of . Returns all params ordered by name. The default implementation uses to get all attributes of type . Return the reader for loading the estimator. Save this ML instance to the given path, a shortcut of ‘write().save(path)’. Return the writer for saving the estimator. Clears a param from the param map if it has been explicitly set. Creates a copy of this instance with the same uid and some extra params. The default implementation creates a shallow copy using , and then copies the embedded and extra parameters over and returns the copy. Subclasses should override this method if the default approach is not sufficient.\n• None extra (dict, optional) – Extra parameters to copy to the new instance Explains a single param and returns its name, doc, and optional default value and user-supplied value in a string. Returns the documentation of all params with their optionally default values and user-supplied values. Extracts the embedded default param values and user-supplied values, and then merges them with extra values from input into a flat param map, where the latter value is used if there exist conflicts, i.e., with ordering: default param values < user-supplied values < extra. Gets the value of featuresCol or its default value. Gets the value of labelCol or its default value. Gets the value of a param in the user-supplied param map or its default value. Raises an error if neither is set. Gets a param by its name. Gets the value of predictionCol or its default value. Gets the value of validationIndicatorCol or its default value. Gets the value of weightCol or its default value. Get feature importance of each feature. Importance type can be defined as:\n• None ‘weight’: the number of times a feature is used to split the data across all trees.\n• None ‘gain’: the average gain across all splits the feature is used in.\n• None ‘cover’: the average coverage across all splits the feature is used in.\n• None ‘total_gain’: the total gain across all splits the feature is used in.\n• None ‘total_cover’: the total coverage across all splits the feature is used in. importance_type (str, default 'weight') – One of the importance types defined above. Checks whether a param has a default value. Tests whether this instance contains a param with a given (string) name. Checks whether a param is explicitly set by user or has a default value. Checks whether a param is explicitly set by user. Reads an ML instance from the input path, a shortcut of . Returns all params ordered by name. The default implementation uses to get all attributes of type . Return the reader for loading the model. Save this ML instance to the given path, a shortcut of ‘write().save(path)’. Return the writer for saving the model."
    },
    {
        "link": "https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html",
        "document": "The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that tries to accurately predict a target variable by combining multiple estimates from a set of simpler models. The XGBoost algorithm performs well in machine learning competitions for the following reasons:\n\nYou can use XGBoost for regression, classification (binary and multiclass), and ranking problems.\n\nYou can use the new release of the XGBoost algorithm as either:\n\nThis implementation has a smaller memory footprint, better logging, improved hyperparameter validation, and an bigger set of metrics than the original versions. It provides an XGBoost that runs a training script in a managed XGBoost environment. The current release of SageMaker AI XGBoost is based on the original XGBoost versions 1.0, 1.2, 1.3, 1.5, and 1.7.\n\nFor more information about the Amazon SageMaker AI XGBoost algorithm, see the following blog posts:\n\nSageMaker AI XGBoost supports CPU and GPU training and inference. Instance recommendations depend on training and inference needs, as well as the version of the XGBoost algorithm. Choose one of the following options for more information:\n\nSageMaker AI XGBoost 1.0-1 or earlier only trains using CPUs. It is a memory-bound (as opposed to compute-bound) algorithm. So, a general-purpose compute instance (for example, M5) is a better choice than a compute-optimized instance (for example, C4). Further, we recommend that you have enough total memory in selected instances to hold the training data. It supports the use of disk space to handle data that does not fit into main memory. This is a result of the out-of-core feature available with the libsvm input mode. Even so, writing cache files onto disk slows the algorithm processing time.\n\nSageMaker AI XGBoost version 1.2-2 or later supports GPU training. Despite higher per-instance costs, GPUs train more quickly, making them more cost effective.\n\nSageMaker AI XGBoost version 1.2-2 or later supports P2, P3, G4dn, and G5 GPU instance families.\n\nSageMaker AI XGBoost version 1.7-1 or later supports P3, G4dn, and G5 GPU instance families. Note that due to compute capacity requirements, version 1.7-1 or later does not support the P2 instance family.\n\nTo take advantage of GPU training:\n\nTo run CPU training on multiple instances, set the parameter for the estimator to a value greater than one. The input data must be divided between the total number of instances.\n\nDivide the input data using the following steps:\n\nYou can use distributed training with either single-GPU or multi-GPU instances.\n\nSageMaker AI XGBoost versions 1.2-2 through 1.3-1 only support single-GPU instance training. This means that even if you select a multi-GPU instance, only one GPU is used per instance.\n\nYou must divide your input data between the total number of instances if:\n\nFor more information, see Divide input data across instances.\n\nStarting with version 1.5-1, SageMaker AI XGBoost offers distributed GPU training with Dask . With Dask you can utilize all GPUs when using one or more multi-GPU instances. Dask also works when using single-GPU instances.\n\nTrain with Dask using the following steps:\n\nThere are a few considerations to be aware of when training SageMaker AI XGBoost with Dask. Be sure to split your data into smaller files. Dask reads each Parquet file as a partition. There is a Dask worker for every GPU. As a result, the number of files should be greater than the total number of GPUs (instance count * number of GPUs per instance). Having a very large number of files can also degrade performance. For more information, see Dask Best Practices .\n\nThe specified hyperparameter determines the algorithm that is used for XGBoost training. The tree methods , and are all approximate methods and use sketching for quantile calculation. For more information, see Tree Methods in the XGBoost documentation. Sketching is an approximate algorithm. Therefore, you can expect variations in the model depending on factors such as the number of workers chosen for distributed training. The significance of the variation is data-dependent.\n\nSageMaker AI XGBoost supports CPU and GPU instances for inference. For information about the instance types for inference, see Amazon SageMaker AI ML Instance Types ."
    },
    {
        "link": "https://xgboost.readthedocs.io/en/release_1.5.0/python/python_intro.html",
        "document": "This document gives a basic walkthrough of the xgboost package for Python.\n\nThe XGBoost python module is able to load data from many types of different formats, including: The data is stored in a object.\n• None Missing values can be replaced by a default value in the constructor:\n• None Weights can be set when needed: When performing ranking tasks, the number of weights should be equal to number of groups.\n• None To load a LIBSVM text file or a XGBoost binary file into : The parser in XGBoost has limited functionality. When using Python interface, it’s recommended to use sklearn or other similar utilites than XGBoost’s builtin parser.\n• # label_column specifies the index of the column containing the true label The parser in XGBoost has limited functionality. When using Python interface, it’s recommended to use pandas or other similar utilites than XGBoost’s builtin parser.\n\nAfter training, the model can be saved. The model and its feature map can also be dumped to a text file. A saved model can be loaded as follows: Methods including and from are designed for internal usage only. The wrapper function does some pre-configuration including setting up caches and some other parameters.\n\nIf you have a validation set, you can use early stopping to find the optimal number of boosting rounds. Early stopping requires at least one set in . If there’s more than one, it will use the last. The model will train until the validation score stops improving. Validation error needs to decrease at least every to continue training. If early stopping occurs, the model will have two additional fields: , . Note that will return a model from the last iteration, not the best one. This works with both metrics to minimize (RMSE, log loss, etc.) and to maximize (MAP, NDCG, AUC). Note that if you specify more than one evaluation metric the last one in is used for early stopping.\n\nA model that has been trained or loaded can perform predictions on data sets. If early stopping is enabled during training, you can get predictions from the best iteration with :\n\nYou can use plotting module to plot importance and output tree. To plot importance, use . This function requires to be installed. To plot the output tree via , use , specifying the ordinal number of the target tree. This function requires and . When you use , you can use the function, which converts the target tree to a instance. The instance is automatically rendered in ."
    }
]