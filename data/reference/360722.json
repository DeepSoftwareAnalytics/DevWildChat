[
    {
        "link": "https://json-schema.org",
        "document": "While JSON is probably the most popular format for exchanging data, JSON Schema is the vocabulary that enables JSON data consistency, validity, and interoperability at scale. Simplify your validation logic to reduce your code’s complexity and save time on development. Define constraints for your data structures to catch and prevent errors, inconsistencies, and invalid data. Establish a common language for data exchange, no matter the scale or complexity of your project. Define precise validation rules for your data structures to create shared understanding and increase interoperability across different systems and platforms. Create a clear, standardized representation of your data to improve understanding and collaboration among developers, stakeholders, and collaborators. Adopt JSON Schema with an expansive range of community-driven tools, libraries, and frameworks across many programming languages.\n\nDiscover JSON Schema tooling to help your organization leverage the benefits of JSON Schema. Because JSON Schema is much more than a Specification, it is a vibrant ecosystem of Validators, Generators, Linters, and other JSON Schema Utilities made by this amazing Community.\n\nThe following companies support us by letting us use their products.\n\nEmail us for more info!"
    },
    {
        "link": "https://json-schema.org/draft/2020-12/json-schema-core",
        "document": "JSON Schema is defined to be platform-independent. As such, to increase compatibility across platforms, implementations SHOULD conform to a standard validation output format. This section describes the minimum requirements that consumers will need to properly interpret validation results.¶ JSON Schema output is defined using the JSON Schema data instance model as described in section 4.2.1. Implementations MAY deviate from this as supported by their specific languages and platforms, however it is RECOMMENDED that the output be convertible to the JSON format defined herein via serialization or other means.¶ This specification defines four output formats. See the \"Output Structure\" section for the requirements of each format.¶\n• Flag - A boolean which simply indicates the overall validation result with no further details.¶\n• Detailed - Provides validation information in a condensed hierarchical structure based on the structure of the schema.¶\n• Verbose - Provides validation information in an uncondensed hierarchical structure that matches the exact structure of the schema.¶ An implementation SHOULD provide at least one of the \"flag\", \"basic\", or \"detailed\" format and MAY provide the \"verbose\" format. If it provides one or more of the \"detailed\" or \"verbose\" formats, it MUST also provide the \"flag\" format. Implementations SHOULD specify in their documentation which formats they support.¶ Beyond the simplistic \"flag\" output, additional information is useful to aid in debugging a schema or instance. Each sub-result SHOULD contain the information contained within this section at a minimum.¶ A single object that contains all of these components is considered an output unit.¶ The relative location of the validating keyword that follows the validation path. The value MUST be expressed as a JSON Pointer, and it MUST include any by-reference applicators such as \"$ref\" or \"$dynamicRef\".¶ Note that this pointer may not be resolvable by the normal JSON Pointer process due to the inclusion of these by-reference applicator keywords.¶ The JSON key for this information is \"keywordLocation\".¶ The absolute, dereferenced location of the validating keyword. The value MUST be expressed as a full URI using the canonical URI of the relevant schema resource with a JSON Pointer fragment, and it MUST NOT include by-reference applicators such as \"$ref\" or \"$dynamicRef\" as non-terminal path components. It MAY end in such keywords if the error or annotation is for that keyword, such as an unresolvable reference. Note that \"absolute\" here is in the sense of \"absolute filesystem path\" (meaning the complete location) rather than the \"absolute-URI\" terminology from RFC 3986 (meaning with scheme but without fragment). Keyword absolute locations will have a fragment in order to identify the keyword. ¶ This information MAY be omitted only if either the dynamic scope did not pass over a reference or if the schema does not declare an absolute URI as its \"$id\".¶ The JSON key for this information is \"absoluteKeywordLocation\".¶ The location of the JSON value within the instance being validated. The value MUST be expressed as a JSON Pointer.¶ The JSON key for this information is \"instanceLocation\".¶ The error or annotation that is produced by the validation.¶ For errors, the specific wording for the message is not defined by this specification. Implementations will need to provide this.¶ For annotations, each keyword that produces an annotation specifies its format. By default, it is the keyword's value.¶ The JSON key for failed validations is \"error\"; for successful validations it is \"annotation\".¶ For the two hierarchical structures, this property will hold nested errors and annotations.¶ The JSON key for nested results in failed validations is \"errors\"; for successful validations it is \"annotations\". Note the plural forms, as a keyword with nested results can also have a local error or annotation.¶ The output MUST be an object containing a boolean property named \"valid\". When additional information about the result is required, the output MUST also contain \"errors\" or \"annotations\" as described below.¶\n• \"valid\" - a boolean value indicating the overall validation success or failure¶\n• \"errors\" - the collection of errors or annotations produced by a failed validation¶\n• \"annotations\" - the collection of errors or annotations produced by a successful validation¶ For these examples, the following schema and instance will be used.¶ This instance will fail validation and produce errors, but it's trivial to deduce examples for passing schemas that produce annotations.¶ Specifically, the errors it will produce are:¶\n• The second object is missing a \"y\" property.¶\n• The second object has a disallowed \"z\" property.¶\n• There are only two objects, but three are required.¶ Note that the error message wording as depicted in these examples is not a requirement of this specification. Implementations SHOULD craft error messages tailored for their audience or provide a templating mechanism that allows their users to craft their own messages.¶ In the simplest case, merely the boolean result for the \"valid\" valid property needs to be fulfilled.¶ Because no errors or annotations are returned with this format, it is RECOMMENDED that implementations use short-circuiting logic to return failure or success as soon as the outcome can be determined. For example, if an \"anyOf\" keyword contains five sub-schemas, and the second one passes, there is no need to check the other three. The logic can simply return with success.¶ The \"Basic\" structure is a flat list of output units.¶ { \"valid\": false, \"errors\": [ { \"keywordLocation\": \"\", \"instanceLocation\": \"\", \"error\": \"A subschema had errors.\" }, { \"keywordLocation\": \"/items/$ref\", \"absoluteKeywordLocation\": \"https://example.com/polygon#/$defs/point\", \"instanceLocation\": \"/1\", \"error\": \"A subschema had errors.\" }, { \"keywordLocation\": \"/items/$ref/required\", \"absoluteKeywordLocation\": \"https://example.com/polygon#/$defs/point/required\", \"instanceLocation\": \"/1\", \"error\": \"Required property 'y' not found.\" }, { \"keywordLocation\": \"/items/$ref/additionalProperties\", \"absoluteKeywordLocation\": \"https://example.com/polygon#/$defs/point/additionalProperties\", \"instanceLocation\": \"/1/z\", \"error\": \"Additional property 'z' found but was invalid.\" }, { \"keywordLocation\": \"/minItems\", \"instanceLocation\": \"\", \"error\": \"Expected at least 3 items but found 2\" } ] } ¶ The \"Detailed\" structure is based on the schema and can be more readable for both humans and machines. Having the structure organized this way makes associations between the errors more apparent. For example, the fact that the missing \"y\" property and the extra \"z\" property both stem from the same location in the instance is not immediately obvious in the \"Basic\" structure. In a hierarchy, the correlation is more easily identified.¶ The following rules govern the construction of the results object:¶\n• All applicator keywords (\"*Of\", \"$ref\", \"if\"/\"then\"/\"else\", etc.) require a node.¶\n• Nodes that have no children are removed.¶\n• Nodes that have a single child are replaced by the child.¶ Branch nodes do not require an error message or an annotation.¶ { \"valid\": false, \"keywordLocation\": \"\", \"instanceLocation\": \"\", \"errors\": [ { \"valid\": false, \"keywordLocation\": \"/items/$ref\", \"absoluteKeywordLocation\": \"https://example.com/polygon#/$defs/point\", \"instanceLocation\": \"/1\", \"errors\": [ { \"valid\": false, \"keywordLocation\": \"/items/$ref/required\", \"absoluteKeywordLocation\": \"https://example.com/polygon#/$defs/point/required\", \"instanceLocation\": \"/1\", \"error\": \"Required property 'y' not found.\" }, { \"valid\": false, \"keywordLocation\": \"/items/$ref/additionalProperties\", \"absoluteKeywordLocation\": \"https://example.com/polygon#/$defs/point/additionalProperties\", \"instanceLocation\": \"/1/z\", \"error\": \"Additional property 'z' found but was invalid.\" } ] }, { \"valid\": false, \"keywordLocation\": \"/minItems\", \"instanceLocation\": \"\", \"error\": \"Expected at least 3 items but found 2\" } ] } ¶ The \"Verbose\" structure is a fully realized hierarchy that exactly matches that of the schema. This structure has applications in form generation and validation where the error's location is important.¶ The primary difference between this and the \"Detailed\" structure is that all results are returned. This includes sub-schema validation results that would otherwise be removed (e.g. annotations for failed validations, successful validations inside a `not` keyword, etc.). Because of this, it is RECOMMENDED that each node also carry a `valid` property to indicate the validation result for that node.¶ Because this output structure can be quite large, a smaller example is given here for brevity. The URI of the full output structure of the example above is: https://json-schema.org/draft/2020-12/output/verbose-example.¶ // schema { \"$id\": \"https://example.com/polygon\", \"$schema\": \"https://json-schema.org/draft/2020-12/schema\", \"type\": \"object\", \"properties\": { \"validProp\": true, }, \"additionalProperties\": false } // instance { \"validProp\": 5, \"disallowedProp\": \"value\" } // result { \"valid\": false, \"keywordLocation\": \"\", \"instanceLocation\": \"\", \"errors\": [ { \"valid\": true, \"keywordLocation\": \"/type\", \"instanceLocation\": \"\" }, { \"valid\": true, \"keywordLocation\": \"/properties\", \"instanceLocation\": \"\" }, { \"valid\": false, \"keywordLocation\": \"/additionalProperties\", \"instanceLocation\": \"\", \"errors\": [ { \"valid\": false, \"keywordLocation\": \"/additionalProperties\", \"instanceLocation\": \"/disallowedProp\", \"error\": \"Additional property 'disallowedProp' found but was invalid.\" } ] } ] } ¶ For convenience, JSON Schema has been provided to validate output generated by implementations. Its URI is: https://json-schema.org/draft/2020-12/output/schema.¶\n\nD.1. Best practices for vocabulary and meta-schema authors Vocabulary authors should take care to avoid keyword name collisions if the vocabulary is intended for broad use, and potentially combined with other vocabularies. JSON Schema does not provide any formal namespacing system, but also does not constrain keyword names, allowing for any number of namespacing approaches.¶ Vocabularies may build on each other, such as by defining the behavior of their keywords with respect to the behavior of keywords from another vocabulary, or by using a keyword from another vocabulary with a restricted or expanded set of acceptable values. Not all such vocabulary re-use will result in a new vocabulary that is compatible with the vocabulary on which it is built. Vocabulary authors should clearly document what level of compatibility, if any, is expected.¶ Meta-schema authors should not use \"$vocabulary\" to combine multiple vocabularies that define conflicting syntax or semantics for the same keyword. As semantic conflicts are not generally detectable through schema validation, implementations are not expected to detect such conflicts. If conflicting vocabularies are declared, the resulting behavior is undefined.¶ Vocabulary authors SHOULD provide a meta-schema that validates the expected usage of the vocabulary's keywords on their own. Such meta-schemas SHOULD not forbid additional keywords, and MUST not forbid any keywords from the Core vocabulary.¶ It is recommended that meta-schema authors reference each vocabulary's meta-schema using the \"allOf\" (Section 10.2.1.1) keyword, although other mechanisms for constructing the meta-schema may be appropriate for certain use cases.¶ The recursive nature of meta-schemas makes the \"$dynamicAnchor\" and \"$dynamicRef\" keywords particularly useful for extending existing meta-schemas, as can be seen in the JSON Hyper-Schema meta-schema which extends the Validation meta-schema.¶ Meta-schemas may impose additional constraints, including describing keywords not present in any vocabulary, beyond what the meta-schemas associated with the declared vocabularies describe. This allows for restricting usage to a subset of a vocabulary, and for validating locally defined keywords not intended for re-use.¶ However, meta-schemas should not contradict any vocabularies that they declare, such as by requiring a different JSON type than the vocabulary expects. The resulting behavior is undefined.¶ Meta-schemas intended for local use, with no need to test for vocabulary support in arbitrary implementations, can safely omit \"$vocabulary\" entirely.¶ This meta-schema explicitly declares both the Core and Applicator vocabularies, together with an extension vocabulary, and combines their meta-schemas with an \"allOf\". The extension vocabulary's meta-schema, which describes only the keywords in that vocabulary, is shown after the main example meta-schema.¶ The main example meta-schema also restricts the usage of the Unevaluated vocabulary by forbidding the keywords prefixed with \"unevaluated\", which are particularly complex to implement. This does not change the semantics or set of keywords defined by the other vocabularies. It just ensures that schemas using this meta-schema that attempt to use the keywords prefixed with \"unevaluated\" will fail validation against this meta-schema.¶ Finally, this meta-schema describes the syntax of a keyword, \"localKeyword\", that is not part of any vocabulary. Presumably, the implementors and users of this meta-schema will understand the semantics of \"localKeyword\". JSON Schema does not define any mechanism for expressing keyword semantics outside of vocabularies, making them unsuitable for use except in a specific environment in which they are understood.¶ This meta-schema combines several vocabularies for general use.¶ { \"$schema\": \"https://json-schema.org/draft/2020-12/schema\", \"$id\": \"https://example.com/meta/general-use-example\", \"$dynamicAnchor\": \"meta\", \"$vocabulary\": { \"https://json-schema.org/draft/2020-12/vocab/core\": true, \"https://json-schema.org/draft/2020-12/vocab/applicator\": true, \"https://json-schema.org/draft/2020-12/vocab/validation\": true, \"https://example.com/vocab/example-vocab\": true }, \"allOf\": [ {\"$ref\": \"https://json-schema.org/draft/2020-12/meta/core\"}, {\"$ref\": \"https://json-schema.org/draft/2020-12/meta/applicator\"}, {\"$ref\": \"https://json-schema.org/draft/2020-12/meta/validation\"}, {\"$ref\": \"https://example.com/meta/example-vocab\"} ], \"patternProperties\": { \"^unevaluated\": false }, \"properties\": { \"localKeyword\": { \"$comment\": \"Not in vocabulary, but validated if used\", \"type\": \"string\" } } } ¶ As shown above, even though each of the single-vocabulary meta-schemas referenced in the general-use meta-schema's \"allOf\" declares its corresponding vocabulary, this new meta-schema must re-declare them.¶ The standard meta-schemas that combine all vocabularies defined by the Core and Validation specification, and that combine all vocabularies defined by those specifications as well as the Hyper-Schema specification, demonstrate additional complex combinations. These URIs for these meta-schemas may be found in the Validation and Hyper-Schema specifications, respectively.¶ While the general-use meta-schema can validate the syntax of \"minDate\", it is the vocabulary that defines the logic behind the semantic meaning of \"minDate\". Without an understanding of the semantics (in this example, that the instance value must be a date equal to or after the date provided as the keyword's value in the schema), an implementation can only validate the syntactic usage. In this case, that means validating that it is a date-formatted string (using \"pattern\" to ensure that it is validated even when \"format\" functions purely as an annotation, as explained in the Validation specification [json-schema-validation].¶"
    },
    {
        "link": "https://github.com/Swimminschrage/5e-schema",
        "document": "A JSON schema definition for modules within the 5e of the \"worlds greatest role-playing game\".\n\nContains definitions for the following artifacts:\n\nYou've spent time building the perfect adventure or a new set of monsters for the game, but why not make it easily consumable by tools and applications that DM's use at the table. By creating a representation of your content in JSON, you can expose it in a way a book or PDF never could!\n\nNo more trying to import or work with unstructured 5e data that changes from implementation to implementation. A standard makes it easy to build against and verify content that your app consumes providing a better experience for both yourselves and your users. Use as much or as little as needed to build the app you've always wanted to have around the gaming table."
    },
    {
        "link": "https://medium.com/game-development-stuff/how-to-apply-json-schema-in-game-development-6bb7c9ccdd2e",
        "document": "JSON Schema is a vocabulary that allows you to annotate and validate JSON documents.\n\nValidates data which is useful for:\n\nHow does JSON Schema fit in game development\n\nIf you provide any data through JSON to your game, you can apply JSON Schema on them. Sceneries, items, characters customization, assets, game levels parameters… You can define a schema for any object definition you need or that you’re currently using in your game.\n• The JSON’s structure is checked: If a JSON file is created manually or automatically by a script, you can detect errors coming from the script generator or typos from the person who edited it.\n• The data structure is defined: Every person working on the project can check the schemas related and know what is the data needed and how it is structured, no more misleads.\n• Control on data parameters and parameters format: You can define that an object follows compulsory the schema definitions you’ve defined.\n\nFor instance, imagine we have a game title text definition, this text only needs a font family, size and color. We can define that no additional properties are added to this case, otherwise JSON Schema will alert an error. \n\nAnother example, if we need that size property has to be defined as an integer, it can also be required by the schema definition, so, if the incoming value is a string, an error will be displayed.\n\nI used JSON Schema in a project I developed where designers can preview the graphics and layout of the next game without needing the real game project to be ready and working to that point.\n\nOn this case, the purpose of JSON Schema is to define the type of objects that the layout can contain, such as sprites, buttons, texts and layout containers, and its limitations, if any limitation is crossed, the tool is prepared to show logs of the errors and warnings on Chrome Developers Tools Console.\n\nThis is an example of the schemaToLoad.json I defined on my project. On patternProperties you define the main objects your layout is composed of. You can use particular names for unique cases (ResolutionToLoad object) or names following regular expressions to apply to different classes of objects (sprites, buttons, texts and layout containers).\n\nThese main objects definitions relate to each own case at schemaDefinitionsToLoad.json, where its needed properties and limitations are defined.\n\nadditionalProperties to false indicates that there can’t exist more main objects definitions besides the defined on patternProperties, if the layout file contains an object definition which doesn’t fit in any of these cases, an error will be thrown.\n\nThis is an example of the whole schemaDefinitionsToLoad.json:\n\nResolutionToLoad case was defined at schemaToLoad.json like:\n\nAt schemaDefinitionsToLoad.json, we find its complete definition:\n\nOn type parameter we define if it’s a string, integer, object… at properties parameter we define the properties that ResolutionToLoad object should contain, on this case, w from width and h from height which are defined as required. Apart from w and h, ResolutionToLoad must not contain any additional properties.\n\nThis is an example of ResolutionToLoad object from the game’s layout, which must follow the rules we’ve defined on schemaToLoad.json and schemaDefinitionsToLoad.json:\n\nThe case for layout containers was defined using a regular expression at schemaToLoad.json:\n\nAt schemaDefinitionsToLoad.json, we find its complete definition:\n\nThis case is also a type object and has properties like x, y, z, w, h and scale. As you can see, scale property is defined as an object and “nested” with its own properties. The special thing of this case is that it also uses patternProperties like at schemaToLoad.json and referencing the definitions of the same file. The reason for this is because a layout container can also contain texts, sprites, buttons or others layouts.\n\nExample of a layout element from the game’s layout:\n\nLooking at this example should show you how flexible can be using JSON Schema. At first, it might seem confusing, but after reading the docs and playing a while you get how powerful it is and how you can apply it to your projects.\n\nJSON Schema is available in JavaScript, .NET, C, Go, Java, PHP, Python… You can get the version you need through this page: http://json-schema.org/implementations.html\n\nIn my case I used JavaScript ‘ajv’: https://github.com/epoberezkin/ajv\n\nThis is the main function implementation I did on my project, giving user’s feedback of the layout checking process through Chrome’s console:"
    },
    {
        "link": "https://stackoverflow.com/questions/40483028/how-do-i-define-a-json-schema-containing-definitions-in-code",
        "document": "I am attempting to replicate the following example, by defining the schema in code using :\n\nThis is as close as I've got so far. (Example is in F# but might just as well be in C#.)\n\nAs you can see, only one of the two addresses is defined using a reference to another schema, and the address schema is in \"properties\" rather than \"definitions\". What's the trick to defining a schema in \"definitions\" and referencing it elsewhere?"
    },
    {
        "link": "https://learn.microsoft.com/en-us/sql/relational-databases/json/json-data-sql-server?view=sql-server-ver16",
        "document": "Applies to: SQL Server 2016 (13.x) and later versions Azure SQL Database Azure SQL Managed Instance Azure Synapse Analytics SQL database in Microsoft Fabric\n\nJSON is a popular textual data format that's used for exchanging data in modern web and mobile applications. JSON is also used for storing unstructured data in log files or NoSQL databases such as Microsoft Azure Cosmos DB. Many REST web services return results that are formatted as JSON text or accept data that's formatted as JSON. For example, most Azure services, such as Azure Search, Azure Storage, and Azure Cosmos DB, have REST endpoints that return or consume JSON. JSON is also the main format for exchanging data between webpages and web servers by using AJAX calls.\n\nJSON functions, first introduced in SQL Server 2016 (13.x), enable you to combine NoSQL and relational concepts in the same database. You can combine classic relational columns with columns that contain documents formatted as JSON text in the same table, parse and import JSON documents in relational structures, or format relational data to JSON text.\n\nHere's an example of JSON text:\n\nBy using SQL Server built-in functions and operators, you can do the following things with JSON text:\n• Run any Transact-SQL query on the converted JSON objects.\n• Format the results of Transact-SQL queries in JSON format.\n\nThe next sections discuss the key capabilities that SQL Server provides with its built-in JSON support.\n\nThe new json data type that stores JSON documents in a native binary format that provides the following benefits over storing JSON data in varchar/nvarchar:\n• More efficient reads, as the document is already parsed\n• More efficient writes, as the query can update individual values without accessing the entire document\n• No change in compatibility with existing code\n\nUsing the JSON same functions described in this article remain the most efficient way to query the json data type. For more information on the native json data type, see JSON data type.\n\nExtract values from JSON text and use them in queries\n\nIf you have JSON text that's stored in database tables, you can read or modify values in the JSON text by using the following built-in functions:\n• JSON_QUERY (Transact-SQL) extracts an object or an array from a JSON string.\n• JSON_MODIFY (Transact-SQL) changes a value in a JSON string.\n\nIn the following example, the query uses both relational and JSON data (stored in a column named ) from a table called :\n\nApplications and tools see no difference between the values taken from scalar table columns and the values taken from JSON columns. You can use values from JSON text in any part of a Transact-SQL query (including WHERE, ORDER BY, or GROUP BY clauses, window aggregates, and so on). JSON functions use JavaScript-like syntax for referencing values inside JSON text.\n\nFor more information, see Validate, Query, and Change JSON Data with Built-in Functions (SQL Server), JSON_VALUE (Transact-SQL), and JSON_QUERY (Transact-SQL).\n\nIf you must modify parts of JSON text, you can use the JSON_MODIFY (Transact-SQL) function to update the value of a property in a JSON string and return the updated JSON string. The following example updates the value of a property in a variable that contains JSON:\n\nYou don't need a custom query language to query JSON in SQL Server. To query JSON data, you can use standard T-SQL. If you must create a query or report on JSON data, you can easily convert JSON data to rows and columns by calling the rowset function. For more information, see Parse and Transform JSON Data with OPENJSON.\n\nThe following example calls and transforms the array of objects that is stored in the variable to a rowset that can be queried with a standard Transact-SQL statement:\n\ntransforms the array of JSON objects into a table in which each object is represented as one row, and key/value pairs are returned as cells. The output observes the following rules:\n• converts JSON values to the types that are specified in the clause.\n• can handle both flat key/value pairs and nested, hierarchically organized objects.\n• You don't have to return all the fields that are contained in the JSON text.\n• You can optionally specify a path after the type specification to reference a nested property or to reference a property by a different name.\n• The optional prefix in the path specifies that values for the specified properties must exist in the JSON text.\n\nFor more information, see Parse and Transform JSON Data with OPENJSON and OPENJSON (Transact-SQL).\n\nJSON documents might have sub-elements and hierarchical data that can't be directly mapped into the standard relational columns. In this case, you can flatten JSON hierarchy by joining parent entity with sub-arrays.\n\nIn the following example, the second object in the array has sub-array representing person skills. Every sub-object can be parsed using additional function call:\n\nThe array is returned in the first as original JSON text fragment and passed to another function using operator. The second function parses JSON array and return string values as single column rowset that will be joined with the result of the first .\n\njoins first-level entity with sub-array and return flatten resultset. Due to JOIN, the second row is repeated for every skill.\n\nFormat SQL Server data or the results of SQL queries as JSON by adding the clause to a statement. Use to delegate the formatting of JSON output from your client applications to SQL Server. For more information, see Format query results as JSON with FOR JSON.\n\nThe following example uses PATH mode with the clause:\n\nThe clause formats SQL results as JSON text that can be provided to any app that understands JSON. The PATH option uses dot-separated aliases in the SELECT clause to nest objects in the query results.\n\nFor more information, see Format query results as JSON with FOR JSON and FOR Clause (Transact-SQL).\n\nJSON aggregate functions enable construction of JSON objects or arrays based on an aggregate from SQL data.\n• JSON_OBJECTAGG constructs a JSON object from an aggregation of SQL data or columns.\n• JSON_ARRAYAGG constructs a JSON array from an aggregation of SQL data or columns.\n\nUse cases for JSON data in SQL Server\n\nJSON support in SQL Server and Azure SQL Database lets you combine relational and NoSQL concepts. You can easily transform relational to semi-structured data and vice-versa. JSON isn't a replacement for existing relational models, however. Here are some specific use cases that benefit from the JSON support in SQL Server and in SQL Database.\n\nConsider denormalizing your data model with JSON fields in place of multiple child tables.\n\nStore info about products with a wide range of variable attributes in a denormalized model for flexibility.\n\nLoad, query, and analyze log data stored as JSON files with all the power of the Transact-SQL language.\n\nWhen you need real-time analysis of IoT data, load the incoming data directly into the database instead of staging it in a storage location.\n\nTransform relational data from your database easily into the JSON format used by the REST APIs that support your web site.\n\nSQL Server provides a hybrid model for storing and processing both relational and JSON data by using standard Transact-SQL language. You can organize collections of your JSON documents in tables, establish relationships between them, combine strongly typed scalar columns stored in tables with flexible key/value pairs stored in JSON columns, and query both scalar and JSON values in one or more tables by using full Transact-SQL.\n\nJSON text is stored in or columns and is indexed as plain text. Any SQL Server feature or component that supports text supports JSON, so there are almost no constraints on interaction between JSON and other SQL Server features. You can store JSON in In-memory or Temporal tables, apply Row-Level Security predicates on JSON text, and so on.\n\nHere are some use cases that show how you can use the built-in JSON support in SQL Server.\n\nJSON is a textual format so the JSON documents can be stored in columns in a SQL Database. Since type is supported in all SQL Server subsystems you can put JSON documents in tables with clustered columnstore indexes, memory optimized tables, or external files that can be read using OPENROWSET or PolyBase.\n\nTo learn more about your options for storing, indexing, and optimizing JSON data in SQL Server, see the following articles:\n\nYou can format information that's stored in files as standard JSON or line-delimited JSON. SQL Server can import the contents of JSON files, parse it by using the or functions, and load it into tables.\n• None If your JSON documents are stored in local files, on shared network drives, or in Azure Files locations that can be accessed by SQL Server, you can use bulk import to load your JSON data into SQL Server.\n• None If your line-delimited JSON files are stored in Azure Blob storage or the Hadoop file system, you can use PolyBase to load JSON text, parse it in Transact-SQL code, and load it into tables.\n\nIf you must load JSON data from an external service into SQL Server, you can use to import the data into SQL Server instead of parsing the data in the application layer.\n\nIn supported platforms, use the native json data type instead of nvarchar(max) for improved performance and more efficient storage.\n\nYou can provide the content of the JSON variable by an external REST service, send it as a parameter from a client-side JavaScript framework, or load it from external files. You can easily insert, update, or merge results from JSON text into a SQL Server table.\n\nIf you must filter or aggregate JSON data for reporting purposes, you can use to transform JSON to relational format. You can then use standard Transact-SQL and built-in functions to prepare the reports.\n\nYou can use both standard table columns and values from JSON text in the same query. You can add indexes on the expression to improve the performance of the query. For more information, see Index JSON data.\n\nIf you have a web service that takes data from the database layer and returns it in JSON format, or if you have JavaScript frameworks or libraries that accept data formatted as JSON, you can format JSON output directly in a SQL query. Instead of writing code or including a library to convert tabular query results and then serialize objects to JSON format, you can use to delegate the JSON formatting to SQL Server.\n\nFor example, you might want to generate JSON output that's compliant with the OData specification. The web service expects a request and response in the following format:\n\nThis OData URL represents a request for the ProductID and ProductName columns for the product with 1. You can use to format the output as expected in SQL Server.\n\nThe output of this query is JSON text that's fully compliant with the OData spec. Formatting and escaping are handled by SQL Server. SQL Server can also format query results in any format, such as OData JSON or GeoJSON.\n\nTo get the AdventureWorks sample database, download at least the database file and the samples and scripts file from GitHub.\n\nAfter you restore the sample database to an instance of SQL Server, extract the samples file, and then open the file from the JSON folder. Run the scripts in this file to reformat some existing data as JSON data, test sample queries and reports over the JSON data, index the JSON data, and import and export JSON.\n\nHere's what you can do with the scripts that are included in the file:\n• None Denormalize the existing schema to create columns of JSON data.\n• None Store information from , , , , and other tables that contain information related to sales order into JSON columns in the table.\n• None Store information from and tables in the table as arrays of JSON objects.\n• None Import and export JSON. Create and run procedures that export the content of the and the tables as JSON results, and import and update the and the tables by using JSON input.\n• None Run query examples. Run some queries that call the stored procedures and views that you created in steps 2 and 4.\n• None Clean up scripts. Don't run this part if you want to keep the stored procedures and views that you created in steps 2 and 4."
    },
    {
        "link": "https://stackoverflow.com/questions/17688349/sql-like-operator-to-find-words-in-stored-json",
        "document": "I have this JSON stored in a MySQL DB, column name:\n\nI want to make a search using operator to find all categories with \"Category\" word:\n\nAt the moment I'm doing it this way, but it only return a complete phrase:\n\nHow can I build a query that returns all categories containing the word \"Category\"?"
    },
    {
        "link": "https://red-gate.com/simple-talk/databases/sql-server/t-sql-programming-sql-server/effective-strategies-for-storing-and-parsing-json-in-sql-server",
        "document": "Like XML, JSON is an open standard storage format for data, metadata, parameters, or other unstructured or semi-structured data. Because of its heavy usage in applications today, it inevitably will make its way into databases where it will need to be stored, compressed, modified, searched, and retrieved.\n\nEven though a relational database is not the ideal place to store and manage less structured data, application requirements can oftentimes override an “optimal” database design. There is a convenience in having JSON data close to related relational data and architecting its storage effectively from the start can save significant time and resources in the future.\n\nThis article delves into how JSON is stored in SQL Server and the different ways in which it can be written, read, and maintained.\n\nJSON and Strings are Not the Same!\n\nData stored as JSON will likely enter and exit a database in the JSON format. While stored in SQL Server, JSON should be queried in its native format.\n\nFor most data engineers and application developers, working with strings is fast, easy, and second nature. Functions such as , , , and are well-documented and standard across many platforms. For this reason, it is a common mistake when working with unstructured or semi-structured data to convert it to a / format and using string-manipulation to work with it.\n\nBecause JSON is stored in SQL Server in / formats, it is even easier to use this strategy than when working with XML, which is stored in XML-typed columns.\n\nThere are performance benefits and cost-savings when using the built-in JSON functionality. This article will focus on using SQL Server’s JSON functionality and omit demonstrations that work around it.\n\nLastly, be sure to only use JSON where it is needed by an application. Only store JSON data in SQL Server when required by an application and do not parse it unless component data from within it is required and cannot be obtained in an easier fashion.\n\nStarting in 2024, in Azure SQL Database, JSON can be stored natively using the newly-provided JSON data type. This will be demonstrated later in this article. For now, in on-premises SQL Server, JSON is stored in / columns. In both scenarios, JSON functions may be used to validate, read, and write the underlying data as JSON.\n\nNote that native JSON support is likely to be added to SQL Server in the future, but as of the writing of this article, there is currently no timeline available for that feature. It is also very likely that the techniques provided here will still be useful, much like the XML functionality that operates on string data is.\n\nJSON support was introduced with SQL Server 2016. This equates to compatibility level 130. If any JSON functions are used in older versions of SQL Server, they will fail with an error that they are not found.\n\nNote that JSON functions are available, even if a database is in a compatibility level less than 130, but SQL Server itself must be on at least SQL Server 2016 for this to work. For example, if any of the demonstrations in this article are run on compatibility level 100, but on SQL Server 2022, they would work. While I cannot think of any reason to do this, I suspect someone out there can!\n\nThe ideal use-case for JSON in the database is to be written and read in-place, without any complex I/O requirements to be managed along the way. Whether its use is this simple or not, reading JSON from the database can be accomplished relatively easily.\n\nJSON can be stored and parsed from scalar values, such as parameters or locally declared variables, or it can be written and read to a column within a table. These first examples work with a simple variable that describes some (somewhat) hypothetical metadata about people:\n\nThis JSON document describes a single person using a list of attributes. Note the syntax used in creating the document as it is important to create JSON that is well-formed with valid syntax. Forgetting a detail such as a quotation mark or comma can render the JSON impossible to validate or read using SQL Server’s built in functions.\n\nFor this reason, if there is any question at all as to the validity of JSON that is to be read, validate it first. This may be done using the function, like this:\n\nThe result is either a zero if invalid, one if valid, and if the value of the input, or in the case, is :\n\nThis is a great way to avoid unexpected errors. If zero is returned, then code can be executed to handle that scenario in whatever way makes the most sense. For example, if the T-SQL should throw an error, then this would be a simple way to do so:\n\nIf the JSON had not been well-formed, then the error would have been thrown immediately and would ensure that the calling process would end immediately. Alternatively, the error could be handled less viciously with a log entry, quiet skipping of invalid data, or some other method that doesn’t end processing immediately.\n\nJSON can be written as a set of values, as well, like this:\n\nThe following is a small table that includes a JSON-containing string column:\n\nIn addition to the two examples already provided of person data, a few additional rows will be inserted into the table:\n\nThe validity of each JSON document can be verified similarly to the scalar example from earlier:\n\nFour rows are returned, confirming that each JSON document is valid:\n\nData can be read from JSON documents by either brute-force string searching or by using native JSON functions. Generally speaking, using native JSON functions will result in more reliable results and better performance. This is especially significant if using the native JSON data type in Azure SQL Database as it automatically offers performance improvements for native operations that are forfeited if string-searching is used instead.\n\nFull-Text Indexing can be used on JSON columns that are typed as or , though this is not advisable unless you already use Full-Text Indexing and/or all other search solutions have been exhausted.\n\nIt is important to repeat here: JSON documents are ideally stored as pass-through data for basic writes and reads. Since JSON columns cannot be natively indexed: searching, updating, and otherwise complex parsing will be potentially slow and challenging to code.\n\nAll demonstrations here use native functions and do not show string manipulation as a way to parse JSON data.\n\nThe function can be used to return a value from a JSON document. It can also be used for filtering, grouping, etc…The following example returns the values for and for each row in the table:\n\nThe results are what we expect, knowing the data in the table:\n\nIf the path requested in doesn’t exist, will be returned. There will not be an error or indication of why is returned, though. Similarly, , when evaluated against will simply return , with no further fanfare. The following code provides an example of this:\n\nThe results show the expected for the expression:\n\nIf the inputs to a JSON expression like this may not be valid, ensure that they are tested (if needed) to catch bad data, since there will not be an error or other indication of a problem (other than all values returned).\n\nAn expression like can be placed in the clause for a query. Keep in mind that this will result in an index/table scan on a relatively large wide column value to be fully evaluated. Therefore, be sure to include additional filters if the table is particularly large, or if queries get slow.\n\nThe following example shows a query that returns all rows for people with a defined as Albany:\n\nThe results show a single row returned that matches the filter:\n\nIf there is a need to test whether a path exists within a JSON document, the function may be used. It will return a 1 if the path exists, 0 if it does not, or if the input it . Like , this function does not return errors, regardless of whether the path exists or not. The following query can provide a test as to whether a person has a path exists for :\n\nThe results show that the path doesn’t exist for any of our examples:\n\nRemove the from the string and you will see that does exist. This can be a good way to understand how the paths work in JSON documents.\n\nThis can also be used in the clause to ensure that only rows are returned that include a given path, or that values are only returned when a specified path exists.\n\nWhile JSON documents can be updated en masse, that can be an expensive process if the documents are large, or if there are downstream calculations made based on documents. Being able to tactically make changes when and where needed can reduce IO and be a faster and more efficient way to update, add, or remove paths. Similarly, they may also be a need to modify documents after being read.\n\nThe simplest way to make changes to a JSON document is using the function. This allows properties to be added, removed, or updated with relative ease. For example, the following query updates the and columns for any matching a specific City/State filter:\n\nThe result is an updated document with London replacing Fortuneswell:\n\nNote that this was not an statement and that does not write to the underlying table. It modifies the string value to include the new data you are asking it to. To permanently make this change to the table, the needs to be adjusted to an , like this:\n\nThis query can be used to validate the result:\n\nThe result shows that the change has now been made permanent:\n\nA list may be updated with new elements by using the option for . This example adds the skill “Mustaches” to the list for a specific person:\n\nThe results confirm that the list has been returned with the added element:\n\nLike before, the underlying document has not been updated, but could be if the is adjusted into an , like this:\n\nWe can query to confirm the existence of the new skill in the list by using :\n\nThis query creates a full list of people with one row per person and skill. Removing the clause shows the full output of the which in effect normalizes the data in the skills column to another table, just like if you were joining to a table:\n\nThe filter checks the [value] column returned by and returns only the rows that are filtered for.\n\nThe following syntax can also be used to search for one or many list elements:\n\nWhether mustaches are a skill is up for grabs, but there are at least easy ways to search for them within a JSON list!\n\nA property may be deleted by setting it to . The following example shows the attribute being removed from the document:\n\nThe results show that no longer exists for any rows in the table:\n\nA property can be renamed, if needed. This can be helpful when system or product names change, or simply to correct a mistake within a document. This cannot be accomplished in a single step, though, and is done via a deletion and an insertion, like this:\n\nNote that to maintain the existing value, is used to set the value for the new attribute equal to its previous value. The result shows that has been renamed to :\n\nThe new attribute is appended to the end of the document and retains its original value prior to the rename.\n\nThe final example shows how updating JSON can become complex and difficult to read in T-SQL. If the application that manages this data can perform updates as it would update a value to any other column, the result would be simpler code and likely code that is more efficient. Since JSON is not relational data, even simple operations such as an attribute rename can become convoluted.\n\nThe longer and more complex a query is, the more likely it is for a mistake to be made when writing or modifying it. Consider this when deciding how documents will be updated, and what application will be used to do so.\n\nWhile no native indexing exists (yet) for JSON columns, hope is not lost! If a JSON document is often searched based on a specific attribute, then a computed column may be added that evaluates that attribute. Once a computed column exists, it may be indexed, just like any other computed column.\n\nConsider the example earlier of a JSON document that contains a variety of attributes. Let’s say that a very common search occurs against the attribute. To begin to solve this problem, a computed column will be created on the table that isolates\n\nThe resulting column shows that it is set equal to the value of the attribute.\n\nNow there is a new column added.\n\nWhile the new column is convenient, it provides no performance help as it is just an embedded query against the JSON document. Indexing it provides the ability to search it:\n\nThis executes successfully, but does return a warning message:\n\nis 1700 bytes. The index 'IX_PersonInfo_CityJSON' has maximum length of 8000 bytes. For some combination of\n\nViewing the column’s definition shows that it indeed inherited a biggie-sized column length:\n\nThat column size is unnecessarily massive and can both impact performance, as well as confuse developers who are guaranteed to scratch their heads at a City that can be 4000 multi-byte characters long. To address this, we will remove the index and column and recreate it with a minor adjustment:\n\nThe new column includes a that forces the column length to be . Assuming that does not require double-byte characters and will always be under 100 characters, then this is a beneficial change. The new column size can be quickly validated, like before:\n\nWith that problem out of the way, an index can be placed on the column, this time without a warning message:\n\nThe final test of this indexed computed column is to select a row using a simple JSON search and then the indexed City column:\n\nThe search against the unindexed JSON column requires a table scan as the function must be evaluated against every JSON doc in the table prior to returning results. Alternatively, the search against the City column can use the newly created index and a seek to get its result.\n\nAn alternative to this strategy would be for the application that uses this table to persist the column itself. Instead of a computed column, the could be maintained completely by the application, ensuring the correct value is updated whenever the JSON document is created, updated, or deleted. This would be less convoluted but would require that the application be solely responsible for always maintaining the column. Depending on the app, this may or may not be a simple request.\n\nConsider the cost of maintaining a persisted computed column when implementing a performance change like this. The cost for improving search performance for a column is that write operations against the JSON document will be slightly slower as each update will need to re-evaluate the definition for the computed column and update the index. Adding many columns like this can quickly become expensive and cause write operations to both perform slower and be more likely to introduce contention. Carefully weigh the importance and frequency of a search query against the IO needed to maintain a persisted column with its value.\n\nThere is no unique way to compress a JSON column, regardless of whether it is stored in / format, or in native JSON format.\n\nThe option available for compressing JSON documents is to use the and functions, which use the Gzip compression algorithm to decrease the size of text columns that are stored as off-row data. These functions are not unique to JSON and are also used to compress large text or binary data that row/page compression will not handle by default.\n\nIt is important to note that compression has ramifications that go beyond simply reducing the table’s size and therefore the size of the data file for its database. Compressed data remains compressed in memory and is only decompressed at runtime, when needed by SQL Server. If you are running SQL Server in Web Edition, or if backup compression is unavailable, then compressing data translates to compressing data within backups. This results in smaller backup files. Therefore, data compression can save storage space in multiple locations, as well as memory.\n\nSchema changes are required to use and as the output of compression is a data type, and not a / data type. This can be observed by selecting JSON data and compressing it prior to displaying it:\n\nThe result of this query is a string that is visually meaningless:\n\nTo use compression via this method, there are some basic steps to follow:\n• Ensure the table has a column available to store the compressed data.\n• When writing data to the table, use on the string data prior to inserting it into the column.\n• When reading data from the table, use on the binary data.\n• When the data is ready to be used, cast the output of as the / that the column is intended to be.\n\nTo begin the walkthrough of this process, a new table will be created that uses a column, instead of a .\n\nThe column is now a column now, rather than .\n\nWith this table created, it can be populated similarly to before:\n\nThe big difference is that each JSON document is wrapped in a call to the function. This is now a necessary step, and attempting to insert the JSON data directly into the column will result in a conversion error. Selecting data from the table will show the same results as the compression test in the previous demonstration.\n\nTo read data from this column, it needs to have the function applied, like this:\n\nTo convert this into meaningful data, it needs to be converted to its intended format:\n\nThis time, the output is the JSON document, as it was originally inserted:\n\nThere is value in keeping the JSON document compressed for as long as possible, prior to decompressing and converting to a string that can be read or parsed. The longer it is compressed, the longer computing resources are saved in moving it around.\n\nBecause the compressed JSON document is stored in format, there is no simple way to search the column that will not involve a table scan. This is because all rows need to be decompressed and converted to a string first, prior to searching. If there is a frequent need to search the compressed JSON column, consider persisting the search column as a separate indexed column.\n\nNote that and are deterministic and will always yield the same results. There is no random element to the algorithm, nor does the compression algorithm/results change for different versions or editions of SQL Server.\n\nAzure SQL Database added native JSON support in May 2024, allowing columns and variables to be declared as JSON, rather than or This support is available regardless of database compatibility level. Native support provides significantly improved performance as the document is stored in an already-parsed format. This reduces its storage size, IO needed to read and write JSON data, and allows for efficient compression.\n\nAs a bonus, no code changes are required to take advantage of the native JSON data type. A column’s data type may be altered from to JSON and existing functions will continue to work. As always, it is important to test schema changes before deploying to a production environment, but the ability to improve JSON performance without code changes is hugely beneficial to any organization managing JSON documents in SQL Server.\n\nThese demos are executed against a small test database in Azure SQL Database that contains the default test objects from Microsoft.\n\nConsider the simple example from earlier in this article, rewritten slightly to use the native JSON data type:\n\nThis code executes exactly the same way as it did earlier, returning a “1”, indicating that this is valid JSON. All the other code in the chapter will work the same way as well, just in a more efficient manner internally.\n\nFor columns that use the native JSON data type, constraints may be placed against them, just like any other column data type. While may be used in the column’s definition, allowing JSON to only be stored if it is valid and well-formed, this check is already performed as part of the JSON data type. An attempt to insert JSON that is not properly formed will result in an error.\n\nThe following is a simple table that contains a JSON column:\n\nNote the constraint on the column. This constraint will check the document to ensure that is present, and if not, the document will not be allowed. For example, the following T-SQL will result in an error:\n\nThe error is the standard check constraint failure error:\n\nThe INSERT statement conflicted with the CHECK constraint \"CK_JSONTest_Check_FirstName\". The conflict occurred in database \"EdTest\", table \"dbo.JSONTest\", column 'JSONDocument'.\n\nThe statement has been terminated.\n\nThe ability to embed JSON functions into check constraints can help to add firm data validation to documents before they enter the database. This can be exceptionally valuable in scenarios where an application is unable to ensure different criteria up-front.\n\nThere are no native indexes supported (yet) for JSON data types, though a JSON column may be part of the columns in an index definition. The JSON data type is compatible as a stored procedure parameter, in triggers, views, and as a return type in functions.\n\nIf using Azure SQL Database and JSON, then the native data type is an easy way to improve performance and data integrity.\n\nJSON documents can be stored in SQL Server and accessed similarly to data stored in any other typed column using specialized JSON functions. This provides applications the ability to store JSON in SQL Server alongside relational data when needed.\n\nWhile there are many considerations for how to store, index, compress, and manage JSON data that differ from traditional data types, the ability to maintain this data in SQL Server allows applications to manage it conveniently, rather than needing another storage location for it.\n\nThe JSON data type is a game-changer for these applications as it allows JSON documents to be stored natively in SQL Server, where they can be stored and read far more efficiently than as a string (or compressed string) column. While this feature is only available in Azure SQL Database as of June 2024, it is very likely to be available in future versions of SQL Server. Similarly, native JSON indexing is also a likely future feature addition. If this is an important feature to your development team, then keep an eye out for its availability in the near future.\n\nLastly, consider how and why JSON is stored in a relational database. Part of planning good data architecture is understanding the WHY and HOW behind data decisions, as well as what the future of that data will be. The ideal scenario for JSON is to be stored, maybe compressed, but filtered and manipulated as little as possible. While this may not always be possible, it is important to remember that a relational database engine is not optimized for document storage. Even with a native JSON data type, compression, and other (really cool) trickery, it is still a bit of an edge-case that should be managed carefully.\n\nThanks for reading, and hopefully this information helps you manage JSON documents more efficiently in the world of SQL Server!"
    },
    {
        "link": "https://stackoverflow.com/questions/57135701/sql-like-query-on-json-data",
        "document": "I have JSON data (no schema) stored in a SQL Server column and need to run search queries on it.\n\nSQL Server 2017 has JSON_XXXX methods but they work on pre-known schema. In my case, the schema of objects is not defined precisely and could change.\n\nCurrently to search the columns e.g. find Make=Mercedes-Benz. I'm using a search phrase \"%\\\"Make\\\":\\\"Mercedes-Benz\\\"%\". This works quite well IF exact make name is used. I'd like user to be able to search using partial names as well e.g. just typing 'Benz' or 'merc'.\n\nIs it possible to structure a SQL query using wild cards that'll work for me? Any other options?"
    },
    {
        "link": "https://reddit.com/r/webdev/comments/1fvcox2/best_practices_for_generating_nested_json",
        "document": "Hey all! I wanted to kick off a bit of a discussion on creating nested JSON responses for an API.\n\nI'd like to produce a nested response to an API request. Let's imagine something like this from `/api/books/:id`\n\nMy main question is, what's the best way to produce this data from our relational database?\n\nWe could build a single SQL query that makes this, and the API end point could just pass along the response.\n• Let's the database handle the data and does it in a single query\n• The shape of a response is kinda sorta business logic so maybe it doesn't belong at the database level?\n• Typing is thrown out the window using the JSON function\n• Depending on the nesting complexity, performance of query could suffer and hog a connection\n\nWe could keep the queries separate and simple, and assemble them in the API\n• Endpoint performance and maintainability suffer by some amount\n• May require more management of DB connections/clients, using transactions or otherwise, and async management (Promise.all, etc)"
    }
]