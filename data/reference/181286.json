[
    {
        "link": "https://huggingface.co/docs/accelerate/v0.3.0/installation.html",
        "document": "ü§ó Accelerate is tested on Python 3.6+, and PyTorch 1.6.0+.\n\nYou should install ü§ó Accelerate in a virtual environment. If you‚Äôre unfamiliar with Python virtual environments, check out the user guide. Create a virtual environment with the version of Python you‚Äôre going to use and activate it.\n\nNow, if you want to use ü§ó Accelerate, you can install it with pip.\n\nFirst you need to install PyTorch. Please refer to the PyTorch installation page regarding the specific install command for your platform. When PyTorch has been installed, ü§ó Accelerate can be installed using pip as follows: Alternatively, for CPU-support only, you can install ü§ó Accelerate and PyTorch in one line with: To check ü§ó Accelerate is properly installed, run the following command:\n\nHere is how to quickly install from source: Note that this will install not the latest released version, but the bleeding edge version, which you may want to use in case a bug has been fixed since the last official release and a new release hasn‚Äôt been yet rolled out. While we strive to keep operational at all times, if you notice some issues, they usually get fixed within a few hours or a day and and you‚Äôre more than welcome to help us detect any problems by opening an Issue and this way, things will get fixed even sooner.\n\nIf you want to constantly use the bleeding edge version of the source code, or if you want to contribute to the library and need to test the changes in the code you‚Äôre making, you will need an editable install. This is done by cloning the repository and installing with the following commands: This command performs a magical link between the folder you cloned the repository to and your python library paths, and it‚Äôll look inside this folder in addition to the normal library-wide paths. So if normally your python packages get installed into: now this editable install will reside where you clone the folder to, e.g. and python will search it too. Do note that you have to keep that folder around and not delete it to continue using the ü§ó Accelerate library. Now, let‚Äôs get to the real benefit of this installation approach. Say, you saw some new feature has been just committed into . If you have already performed all the steps above, to update your accelerate repo to include all the latest commits, all you need to do is to into that cloned repository folder and update the clone to the latest version: There is nothing else to do. Your python environment will find the bleeding edge version of ü§ó Accelerate on the next run."
    },
    {
        "link": "https://huggingface.co/docs/accelerate/en/basic_tutorials/install",
        "document": "and get access to the augmented documentation experience\n\nBefore you start, you will need to setup your environment, install the appropriate packages, and configure Accelerate. Accelerate is tested on Python 3.8+.\n\nAccelerate is available on pypi and conda, as well as on GitHub. Details to install from each are below:\n\nAccelerate can also be installed with conda with:\n\nNew features are added every day that haven‚Äôt been released yet. To try them out yourself, install from the GitHub repository:\n\nIf you‚Äôre working on contributing to the library or wish to play with the source code and see live results as you run the code, an editable version can be installed from a locally-cloned version of the repository:\n\nAfter installing, you need to configure Accelerate for how the current system is setup for training. To do so run the following and answer the questions prompted to you:\n\nTo write a barebones configuration that doesn‚Äôt include options such as DeepSpeed configuration or running on TPUs, you can quickly run:\n\nAccelerate will automatically utilize the maximum number of GPUs available and set the mixed precision mode.\n\nTo check that your configuration looks fine, run:\n\nAn example output is shown below, which describes two GPUs on a single machine with no mixed precision being used:"
    },
    {
        "link": "https://github.com/huggingface/accelerate/blob/main/docs/source/basic_tutorials/install.md",
        "document": "Before you start, you will need to setup your environment, install the appropriate packages, and configure Accelerate. Accelerate is tested on Python 3.8+.\n\nAccelerate is available on pypi and conda, as well as on GitHub. Details to install from each are below:\n\nAccelerate can also be installed with conda with:\n\nNew features are added every day that haven't been released yet. To try them out yourself, install from the GitHub repository:\n\nIf you're working on contributing to the library or wish to play with the source code and see live results as you run the code, an editable version can be installed from a locally-cloned version of the repository:\n\nAfter installing, you need to configure Accelerate for how the current system is setup for training. To do so run the following and answer the questions prompted to you:\n\nTo write a barebones configuration that doesn't include options such as DeepSpeed configuration or running on TPUs, you can quickly run:\n\nAccelerate will automatically utilize the maximum number of GPUs available and set the mixed precision mode.\n\nTo check that your configuration looks fine, run:\n\nAn example output is shown below, which describes two GPUs on a single machine with no mixed precision being used:"
    },
    {
        "link": "https://pypi.org/project/accelerate",
        "document": "A required part of this site couldn‚Äôt load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://acceleratehs.org/get-started/install-from-github.html",
        "document": ""
    },
    {
        "link": "https://huggingface.co/blog/4bit-transformers-bitsandbytes",
        "document": "Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA\n\nThis article is also available in Chinese ÁÆÄ‰Ωì‰∏≠Êñá.\n\nWe present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimizers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.\n\nThis blogpost and release come with several resources to get started with 4bit models and QLoRA:\n‚Ä¢ Basic usage Google Colab notebook - This notebook shows how to use 4bit models in inference with all their variants, and how to run GPT-neo-X (a 20B parameter model) on a free Google Colab instance ü§Ø\n‚Ä¢ Fine tuning Google Colab notebook - This notebook shows how to fine-tune a 4bit model on a downstream task using the Hugging Face ecosystem. We show that it is possible to fine tune GPT-neo-X 20B on a Google Colab instance!\n‚Ä¢ Guanaco 33b playground - or check the playground section below\n\nIf you are not familiar with model precisions and the most common data types (float16, float32, bfloat16, int8), we advise you to carefully read the introduction in our first blogpost that goes over the details of these concepts in simple terms with visualizations.\n\nFor more information we recommend reading the fundamentals of floating point representation through this wikibook document.\n\nThe recent QLoRA paper explores different data types, 4-bit Float and 4-bit NormalFloat. We will discuss here the 4-bit Float data type since it is easier to understand.\n\nFP8 and FP4 stand for Floating Point 8-bit and 4-bit precision, respectively. They are part of the minifloats family of floating point values (among other precisions, the minifloats family also includes bfloat16 and float16).\n\nLet‚Äôs first have a look at how to represent floating point values in FP8 format, then understand how the FP4 format looks like.\n\nAs discussed in our previous blogpost, a floating point contains n-bits, with each bit falling into a specific category that is responsible for representing a component of the number (sign, mantissa and exponent). These represent the following.\n\nThe FP8 (floating point 8) format has been first introduced in the paper ‚ÄúFP8 for Deep Learning‚Äù with two different FP8 encodings: E4M3 (4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa).\n\nAlthough the precision is substantially reduced by reducing the number of bits from 32 to 8, both versions can be used in a variety of situations. Currently one could use Transformer Engine library that is also integrated with HF ecosystem through accelerate.\n\nThe potential floating points that can be represented in the E4M3 format are in the range -448 to 448, whereas in the E5M2 format, as the number of bits of the exponent increases, the range increases to -57344 to 57344 - but with a loss of precision because the number of possible representations remains constant. It has been empirically proven that the E4M3 is best suited for the forward pass, and the second version is best suited for the backward computation\n\nThe sign bit represents the sign (+/-), the exponent bits a base two to the power of the integer represented by the bits (e.g. ), and the fraction or mantissa is the sum of powers of negative two which are ‚Äúactive‚Äù for each bit that is ‚Äú1‚Äù. If a bit is ‚Äú0‚Äù the fraction remains unchanged for that power of where i is the position of the bit in the bit-sequence. For example, for mantissa bits 1010 we have . To get a value, we add 1 to the fraction and multiply all results together, for example, with 2 exponent bits and one mantissa bit the representations 1101 would be:\n\nFor FP4 there is no fixed format and as such one can try combinations of different mantissa/exponent combinations. In general, 3 exponent bits do a bit better in most cases. But sometimes 2 exponent bits and a mantissa bit yield better performance.\n\nQLoRA paper, a new way of democratizing quantized large transformer models\n\nIn few words, QLoRA reduces the memory usage of LLM finetuning without performance tradeoffs compared to standard 16-bit model finetuning. This method enables 33B model finetuning on a single 24GB GPU and 65B model finetuning on a single 46GB GPU.\n\nMore specifically, QLoRA uses 4-bit quantization to compress a pretrained language model. The LM parameters are then frozen and a relatively small number of trainable parameters are added to the model in the form of Low-Rank Adapters. During finetuning, QLoRA backpropagates gradients through the frozen 4-bit quantized pretrained language model into the Low-Rank Adapters. The LoRA layers are the only parameters being updated during training. Read more about LoRA in the original LoRA paper.\n\nQLoRA has one storage data type (usually 4-bit NormalFloat) for the base model weights and a computation data type (16-bit BrainFloat) used to perform computations. QLoRA dequantizes weights from the storage data type to the computation data type to perform the forward and backward passes, but only computes weight gradients for the LoRA parameters which use 16-bit bfloat. The weights are decompressed only when they are needed, therefore the memory usage stays low during training and inference.\n\nQLoRA tuning is shown to match 16-bit finetuning methods in a wide range of experiments. In addition, the Guanaco models, which use QLoRA finetuning for LLaMA models on the OpenAssistant dataset (OASST1), are state-of-the-art chatbot systems and are close to ChatGPT on the Vicuna benchmark. This is an additional demonstration of the power of QLoRA tuning.\n\nFor a more detailed reading, we recommend you read the QLoRA paper.\n\nHow to use it in transformers?\n\nIn this section let us introduce the transformers integration of this method, how to use it and which models can be effectively quantized.\n\nAs a quickstart, load a model in 4bit by (at the time of this writing) installing accelerate and transformers from source, and make sure you have installed the latest version of bitsandbytes library (0.39.0).\n\nThe basic way to load a model in 4bit is to pass the argument when calling the method by providing a device map (pass to get a device map that will be automatically inferred).\n\nThat's all you need!\n\nAs a general rule, we recommend users to not manually set a device once the model has been loaded with . So any device assignment call to the model, or to any model‚Äôs submodules should be avoided after that line - unless you know what you are doing.\n\nKeep in mind that loading a quantized model will automatically cast other model's submodules into dtype. You can change this behavior, (if for example you want to have the layer norms in ), by passing to the method.\n\nYou can play with different variants of 4bit quantization such as NF4 (normalized float 4 (default)) or pure FP4 quantization. Based on theoretical considerations and empirical results from the paper, we recommend using NF4 quantization for better performance.\n\nOther options include which uses a second quantization after the first one to save an additional 0.4 bits per parameter. And finally, the compute type. While 4-bit bitsandbytes stores weights in 4-bits, the computation still happens in 16 or 32-bit and here any combination can be chosen (float16, bfloat16, float32 etc).\n\nThe matrix multiplication and training will be faster if one uses a 16-bit compute dtype (default torch.float32). One should leverage the recent from transformers to change these parameters. An example to load a model in 4bit using NF4 quantization below with double quantization with the compute dtype bfloat16 for faster training:\n\nAs mentioned above, you can also change the compute dtype of the quantized model by just changing the argument in .\n\nFor enabling nested quantization, you can use the argument in . This will enable a second quantization after the first one to save an additional 0.4 bits per parameter. We also use this feature in the training Google colab notebook.\n\nAnd of course, as mentioned in the beginning of the section, all of these components are composable. You can combine all these parameters together to find the optimial use case for you. A rule of thumb is: use double quant if you have problems with memory, use NF4 for higher precision, and use a 16-bit dtype for faster finetuning. For instance in the inference demo, we use nested quantization, bfloat16 compute dtype and NF4 quantization to fit gpt-neo-x-20b (40GB) entirely in 4bit in a single 16GB GPU.\n\nIn this section, we will also address some common questions anyone could have regarding this integration.\n\nDoes FP4 quantization have any hardware requirements?\n\nNote that this method is only compatible with GPUs, hence it is not possible to quantize models in 4bit on a CPU. Among GPUs, there should not be any hardware requirement about this method, therefore any GPU could be used to run the 4bit quantization as long as you have CUDA>=11.2 installed. Keep also in mind that the computation is not done in 4bit, the weights and activations are compressed to that format and the computation is still kept in the desired or native dtype.\n\nWhat are the supported models?\n\nSimilarly as the integration of LLM.int8 presented in this blogpost the integration heavily relies on the library. Therefore, any model that supports accelerate loading (i.e. the argument when calling ) should be quantizable in 4bit. Note also that this is totally agnostic to modalities, as long as the models can be loaded with the argument, it is possible to quantize them.\n\nFor text models, at this time of writing, this would include most used architectures such as Llama, OPT, GPT-Neo, GPT-NeoX for text models, Blip2 for multimodal models, and so on.\n\nAt this time of writing, the models that support accelerate are:\n\nNote that if your favorite model is not there, you can open a Pull Request or raise an issue in transformers to add the support of accelerate loading for that architecture.\n\nIt is not possible to perform pure 4bit training on these models. However, you can train these models by leveraging parameter efficient fine tuning methods (PEFT) and train for example adapters on top of them. That is what is done in the paper and is officially supported by the PEFT library from Hugging Face. We also provide a training notebook and recommend users to check the QLoRA repository if they are interested in replicating the results from the paper.\n\nWhat other consequences are there?\n\nThis integration can open up several positive consequences to the community and AI research as it can affect multiple use cases and possible applications. In RLHF (Reinforcement Learning with Human Feedback) it is possible to load a single base model, in 4bit and train multiple adapters on top of it, one for the reward modeling, and another for the value policy training. A more detailed blogpost and announcement will be made soon about this use case.\n\nWe have also made some benchmarks on the impact of this quantization method on training large models on consumer hardware. We have run several experiments on finetuning 2 different architectures, Llama 7B (15GB in fp16) and Llama 13B (27GB in fp16) on an NVIDIA T4 (16GB) and here are the results\n\nWe have used the recent from TRL library, and the benchmarking script can be found here\n\nTry out the Guananco model cited on the paper on the playground or directly below\n\nThe HF team would like to acknowledge all the people involved in this project from University of Washington, and for making this available to the community.\n\nThe authors would also like to thank Pedro Cuenca for kindly reviewing the blogpost, Olivier Dehaene and Omar Sanseviero for their quick and strong support for the integration of the paper's artifacts on the HF Hub."
    },
    {
        "link": "https://huggingface.co/docs/transformers/main/en//quantization",
        "document": "and get access to the augmented documentation experience\n\nQuantization techniques focus on representing data with less information while also trying to not lose too much accuracy. This often means converting a data type to represent the same information with fewer bits. For example, if your model weights are stored as 32-bit floating points and they‚Äôre quantized to 16-bit floating points, this halves the model size which makes it easier to store and reduces memory-usage. Lower precision can also speedup inference because it takes less time to perform calculations with fewer bits.\n\nTransformers supports several quantization schemes to help you run inference with large language models (LLMs) and finetune adapters on quantized models. This guide will show you how to use Activation-aware Weight Quantization (AWQ), AutoGPTQ, and bitsandbytes.\n\nü§ó Quanto library is a versatile pytorch quantization toolkit. The quantization method used is the linear quantization. Quanto provides several unique features such as:\n\nBefore you begin, make sure the following libraries are installed:\n\nNow you can quantize a model by passing QuantoConfig object in the from_pretrained() method. This works for any model in any modality, as long as it contains layers.\n\nThe integration with transformers only supports weights quantization. For the more complex use case such as activation quantization, calibration and quantization aware training, you should use quanto library instead.\n\nNote that serialization is not supported yet with transformers but it is coming soon! If you want to save the model, you can use quanto library instead.\n\nQuanto library uses linear quantization algorithm for quantization. Even though this is a basic quantization technique, we get very good results! Have a look at the following becnhmark (llama-2-7b on perplexity metric). You can find more benchamarks here\n\nThe library is versatible enough to be compatible with most PTQ optimization algorithms. The plan in the future is to integrate the most popular algorithms in the most seamless possible way (AWQ, Smoothquant).\n\nAdditive Quantization of Language Models (AQLM) is a Large Language Models compression method. It quantizes multiple weights together and take advantage of interdependencies between them. AQLM represents groups of 8-16 weights as a sum of multiple vector codes.\n\nInference support for AQLM is realised in the library. Make sure to install it to run the models (note aqlm works only with python>=3.10):\n\nThe library provides efficient kernels for both GPU and CPU inference and training.\n\nThe instructions on how to quantize models yourself, as well as all the relevant code can be found in the corresponding GitHub repository.\n\nStarting with version , AQLM supports Parameter-Efficient Fine-Tuning in a form of LoRA integrated into the PEFT library.\n\nAQLM quantization setups vary mainly on the number of codebooks used as well as codebook sizes in bits. The most popular setups, as well as inference kernels they support are:\n\nActivation-aware Weight Quantization (AWQ) doesn‚Äôt quantize all the weights in a model, and instead, it preserves a small percentage of weights that are important for LLM performance. This significantly reduces quantization loss such that you can run models in 4-bit precision without experiencing any performance degradation.\n\nThere are several libraries for quantizing models with the AWQ algorithm, such as llm-awq, autoawq or optimum-intel. Transformers supports loading models quantized with the llm-awq and autoawq libraries. This guide will show you how to load models quantized with autoawq, but the process is similar for llm-awq quantized models.\n\nMake sure you have autoawq installed:\n\nAWQ-quantized models can be identified by checking the attribute in the model‚Äôs config.json file:\n\nA quantized model is loaded with the from_pretrained() method. If you loaded your model on the CPU, make sure to move it to a GPU device first. Use the parameter to specify where to place the model:\n\nLoading an AWQ-quantized model automatically sets other weights to fp16 by default for performance reasons. If you want to load these other weights in a different format, use the parameter:\n\nAWQ quantization can also be combined with FlashAttention-2 to further accelerate inference:\n\nFused modules offers improved accuracy and performance and it is supported out-of-the-box for AWQ modules for Llama and Mistral architectures, but you can also fuse AWQ modules for unsupported architectures.\n\nRecent versions of supports exllama-v2 kernels for faster prefill and decoding. To get started, first install the latest version of by running:\n\nGet started by passing an with .\n\nThe AutoGPTQ library implements the GPTQ algorithm, a post-training quantization technique where each row of the weight matrix is quantized independently to find a version of the weights that minimizes the error. These weights are quantized to int4, but they‚Äôre restored to fp16 on the fly during inference. This can save your memory-usage by 4x because the int4 weights are dequantized in a fused kernel rather than a GPU‚Äôs global memory, and you can also expect a speedup in inference because using a lower bitwidth takes less time to communicate.\n\nBefore you begin, make sure the following libraries are installed:\n\nTo quantize a model (currently only supported for text models), you need to create a GPTQConfig class and set the number of bits to quantize to, a dataset to calibrate the weights for quantization, and a tokenizer to prepare the dataset.\n\nYou could also pass your own dataset as a list of strings, but it is highly recommended to use the same dataset from the GPTQ paper.\n\nLoad a model to quantize and pass the to the from_pretrained() method. Set to automatically offload the model to a CPU to help fit the model in memory, and allow the model modules to be moved between the CPU and GPU for quantization.\n\nIf you‚Äôre running out of memory because a dataset is too large, disk offloading is not supported. If this is the case, try passing the parameter to allocate the amount of memory to use on your device (GPU and CPU):\n\nOnce your model is quantized, you can push the model and tokenizer to the Hub where it can be easily shared and accessed. Use the push_to_hub() method to save the GPTQConfig:\n\nYou could also save your quantized model locally with the save_pretrained() method. If the model was quantized with the parameter, make sure to move the entire model to a GPU or CPU before saving it. For example, to save the model on a CPU:\n\nReload a quantized model with the from_pretrained() method, and set to automatically distribute the model on all available GPUs to load the model faster without using more memory than needed.\n\nExLlama is a Python/C++/CUDA implementation of the Llama model that is designed for faster inference with 4-bit GPTQ weights (check out these benchmarks). The ExLlama kernel is activated by default when you create a GPTQConfig object. To boost inference speed even further, use the ExLlamaV2 kernels by configuring the parameter:\n\nThe ExLlama kernels are only supported when the entire model is on the GPU. If you‚Äôre doing inference on a CPU with AutoGPTQ (version > 0.4.2), then you‚Äôll need to disable the ExLlama kernel. This overwrites the attributes related to the ExLlama kernels in the quantization config of the config.json file.\n\nbitsandbytes is the easiest option for quantizing a model to 8 and 4-bit. 8-bit quantization multiplies outliers in fp16 with non-outliers in int8, converts the non-outlier values back to fp16, and then adds them together to return the weights in fp16. This reduces the degradative effect outlier values have on a model‚Äôs performance. 4-bit quantization compresses a model even further, and it is commonly used with QLoRA to finetune quantized LLMs.\n\nTo use bitsandbytes, make sure you have the following libraries installed:\n\nNow you can quantize a model with the or parameters in the from_pretrained() method. This works for any model in any modality, as long as it supports loading with Accelerate and contains layers.\n\nYou can check your memory footprint with the method:\n\nQuantized models can be loaded from the from_pretrained() method without needing to specify the or parameters:\n\nThis section explores some of the specific features of 8-bit models, such as offloading, outlier thresholds, skipping module conversion, and finetuning.\n\n8-bit models can offload weights between the CPU and GPU to support fitting very large models into memory. The weights dispatched to the CPU are actually stored in float32, and aren‚Äôt converted to 8-bit. For example, to enable offloading for the bigscience/bloom-1b7 model, start by creating a BitsAndBytesConfig:\n\nDesign a custom device map to fit everything on your GPU except for the , which you‚Äôll dispatch to the CPU:\n\nNow load your model with the custom and :\n\nAn ‚Äúoutlier‚Äù is a hidden state value greater than a certain threshold, and these values are computed in fp16. While the values are usually normally distributed ([-3.5, 3.5]), this distribution can be very different for large models ([-60, 6] or [6, 60]). 8-bit quantization works well for values ~5, but beyond that, there is a significant performance penalty. A good default threshold value is 6, but a lower threshold may be needed for more unstable models (small models or finetuning).\n\nTo find the best threshold for your model, we recommend experimenting with the parameter in BitsAndBytesConfig:\n\nFor some models, like Jukebox, you don‚Äôt need to quantize every module to 8-bit which can actually cause instability. With Jukebox, there are several modules that should be skipped using the parameter in BitsAndBytesConfig:\n\nWith the PEFT library, you can finetune large models like flan-t5-large and facebook/opt-6.7b with 8-bit quantization. You don‚Äôt need to pass the parameter for training because it‚Äôll automatically load your model on a GPU. However, you can still customize the device map with the parameter if you want to ( should only be used for inference).\n\nThis section explores some of the specific features of 4-bit models, such as changing the compute data type, using the Normal Float 4 (NF4) data type, and using nested quantization.\n\nTo speedup computation, you can change the data type from float32 (the default value) to bf16 using the parameter in BitsAndBytesConfig:\n\nNF4 is a 4-bit data type from the QLoRA paper, adapted for weights initialized from a normal distribution. You should use NF4 for training 4-bit base models. This can be configured with the parameter in the BitsAndBytesConfig:\n\nFor inference, the does not have a huge impact on performance. However, to remain consistent with the model weights, you should use the and values.\n\nNested quantization is a technique that can save additional memory at no additional performance cost. This feature performs a second quantization of the already quantized weights to save an addition 0.4 bits/parameter. For example, with nested quantization, you can finetune a Llama-13b model on a 16GB NVIDIA T4 GPU with a sequence length of 1024, a batch size of 1, and enabling gradient accumulation with 4 steps.\n\nOnce quantized, you can dequantize the model to the original precision. Note this might result in a small quality loss of the model. Make also sure to have enough GPU RAM to fit the dequantized model. Below is how to perform dequantization on a 4-bit model using .\n\nThe EETQ library supports int8 per-channel weight-only quantization for NVIDIA GPUS. The high-performance GEMM and GEMV kernels are from FasterTransformer and TensorRT-LLM. It requires no calibration dataset and does not need to pre-quantize your model. Moreover, the accuracy degradation is negligible owing to the per-channel quantization.\n\nMake sure you have eetq installed from the relase page\n\nor via the source code https://github.com/NetEase-FuXi/EETQ. EETQ requires CUDA capability <= 8.9 and >= 7.0\n\nAn unquantized model can be quantized via ‚Äúfrom_pretrained‚Äù.\n\nA quantized model can be saved via ‚Äúsaved_pretrained‚Äù and be reused again via the ‚Äúfrom_pretrained‚Äù.\n\nThe Optimum library supports quantization for Intel, Furiosa, ONNX Runtime, GPTQ, and lower-level PyTorch quantization functions. Consider using Optimum for quantization if you‚Äôre using specific and optimized hardware like Intel CPUs, Furiosa NPUs or a model accelerator like ONNX Runtime.\n\nTo compare the speed, throughput, and latency of each quantization scheme, check the following benchmarks obtained from the optimum-benchmark library. The benchmark was run on a NVIDIA A1000 for the TheBloke/Mistral-7B-v0.1-AWQ and TheBloke/Mistral-7B-v0.1-GPTQ models. These were also tested against the bitsandbytes quantization methods as well as a native fp16 model.\n\nThe benchmarks indicate AWQ quantization is the fastest for inference, text generation, and has the lowest peak memory for text generation. However, AWQ has the largest forward latency per batch size. For a more detailed discussion about the pros and cons of each quantization method, read the Overview of natively supported quantization schemes in ü§ó Transformers blog post.\n\nThe TheBloke/Mistral-7B-OpenOrca-AWQ model was benchmarked with with and without fused modules.\n\nThe speed and throughput of fused and unfused modules were also tested with the optimum-benchmark library.\n\nHalf-Quadratic Quantization (HQQ) implements on-the-fly quantization via fast robust optimization. It doesn‚Äôt require calibration data and can be used to quantize any model.\n\n Please refer to the official package for more details.\n\nFor installation, we recommend you use the following approach to get the latest version and build its corresponding CUDA kernels:\n\nTo quantize a model, you need to create an HqqConfig. There are two ways of doing it:\n\nThe second approach is especially interesting for quantizing Mixture-of-Experts (MoEs) because the experts are less affected by lower quantization settings.\n\nThen you simply quantize the model as follows\n\nHQQ supports various backends, including pure Pytorch and custom dequantization CUDA kernels. These backends are suitable for older gpus and peft/QLoRA training. For faster inference, HQQ supports 4-bit fused kernels (TorchAO and Marlin), reaching up to 200 tokens/sec on a single 4090. For more details on how to use the backends, please refer to https://github.com/mobiusml/hqq/?tab=readme-ov-file#backend"
    },
    {
        "link": "https://github.com/huggingface/blog/blob/main/4bit-transformers-bitsandbytes.md",
        "document": "LLMs are known to be large, and running or training them in consumer hardware is a huge challenge for users and accessibility. Our LLM.int8 blogpost showed how the techniques in the LLM.int8 paper were integrated in transformers using the library. As we strive to make models even more accessible to anyone, we decided to collaborate with bitsandbytes again to allow users to run models in 4-bit precision. This includes a large majority of HF models, in any modality (text, vision, multi-modal, etc.). Users can also train adapters on top of 4bit models leveraging tools from the Hugging Face ecosystem. This is a new method introduced today in the QLoRA paper by Dettmers et al. The abstract of the paper is as follows:\n\nThis blogpost and release come with several resources to get started with 4bit models and QLoRA:\n‚Ä¢ Basic usage Google Colab notebook - This notebook shows how to use 4bit models in inference with all their variants, and how to run GPT-neo-X (a 20B parameter model) on a free Google Colab instance ü§Ø\n‚Ä¢ Fine tuning Google Colab notebook - This notebook shows how to fine-tune a 4bit model on a downstream task using the Hugging Face ecosystem. We show that it is possible to fine tune GPT-neo-X 20B on a Google Colab instance!\n‚Ä¢ Guanaco 33b playground - or check the playground section below\n\nIf you are not familiar with model precisions and the most common data types (float16, float32, bfloat16, int8), we advise you to carefully read the introduction in our first blogpost that goes over the details of these concepts in simple terms with visualizations.\n\nFor more information we recommend reading the fundamentals of floating point representation through this wikibook document.\n\nThe recent QLoRA paper explores different data types, 4-bit Float and 4-bit NormalFloat. We will discuss here the 4-bit Float data type since it is easier to understand.\n\nFP8 and FP4 stand for Floating Point 8-bit and 4-bit precision, respectively. They are part of the minifloats family of floating point values (among other precisions, the minifloats family also includes bfloat16 and float16).\n\nLet‚Äôs first have a look at how to represent floating point values in FP8 format, then understand how the FP4 format looks like.\n\nAs discussed in our previous blogpost, a floating point contains n-bits, with each bit falling into a specific category that is responsible for representing a component of the number (sign, mantissa and exponent). These represent the following.\n\nThe FP8 (floating point 8) format has been first introduced in the paper ‚ÄúFP8 for Deep Learning‚Äù with two different FP8 encodings: E4M3 (4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa).\n\nAlthough the precision is substantially reduced by reducing the number of bits from 32 to 8, both versions can be used in a variety of situations. Currently one could use Transformer Engine library that is also integrated with HF ecosystem through accelerate.\n\nThe potential floating points that can be represented in the E4M3 format are in the range -448 to 448, whereas in the E5M2 format, as the number of bits of the exponent increases, the range increases to -57344 to 57344 - but with a loss of precision because the number of possible representations remains constant. It has been empirically proven that the E4M3 is best suited for the forward pass, and the second version is best suited for the backward computation\n\nThe sign bit represents the sign (+/-), the exponent bits a base two to the power of the integer represented by the bits (e.g. ), and the fraction or mantissa is the sum of powers of negative two which are ‚Äúactive‚Äù for each bit that is ‚Äú1‚Äù. If a bit is ‚Äú0‚Äù the fraction remains unchanged for that power of where i is the position of the bit in the bit-sequence. For example, for mantissa bits 1010 we have . To get a value, we add 1 to the fraction and multiply all results together, for example, with 2 exponent bits and one mantissa bit the representations 1101 would be:\n\nFor FP4 there is no fixed format and as such one can try combinations of different mantissa/exponent combinations. In general, 3 exponent bits do a bit better in most cases. But sometimes 2 exponent bits and a mantissa bit yield better performance.\n\nIn few words, QLoRA reduces the memory usage of LLM finetuning without performance tradeoffs compared to standard 16-bit model finetuning. This method enables 33B model finetuning on a single 24GB GPU and 65B model finetuning on a single 46GB GPU.\n\nMore specifically, QLoRA uses 4-bit quantization to compress a pretrained language model. The LM parameters are then frozen and a relatively small number of trainable parameters are added to the model in the form of Low-Rank Adapters. During finetuning, QLoRA backpropagates gradients through the frozen 4-bit quantized pretrained language model into the Low-Rank Adapters. The LoRA layers are the only parameters being updated during training. Read more about LoRA in the original LoRA paper.\n\nQLoRA has one storage data type (usually 4-bit NormalFloat) for the base model weights and a computation data type (16-bit BrainFloat) used to perform computations. QLoRA dequantizes weights from the storage data type to the computation data type to perform the forward and backward passes, but only computes weight gradients for the LoRA parameters which use 16-bit bfloat. The weights are decompressed only when they are needed, therefore the memory usage stays low during training and inference.\n\nQLoRA tuning is shown to match 16-bit finetuning methods in a wide range of experiments. In addition, the Guanaco models, which use QLoRA finetuning for LLaMA models on the OpenAssistant dataset (OASST1), are state-of-the-art chatbot systems and are close to ChatGPT on the Vicuna benchmark. This is an additional demonstration of the power of QLoRA tuning.\n\nFor a more detailed reading, we recommend you read the QLoRA paper.\n\nIn this section let us introduce the transformers integration of this method, how to use it and which models can be effectively quantized.\n\nAs a quickstart, load a model in 4bit by (at the time of this writing) installing accelerate and transformers from source, and make sure you have installed the latest version of bitsandbytes library (0.39.0).\n\nThe basic way to load a model in 4bit is to pass the argument when calling the method by providing a device map (pass to get a device map that will be automatically inferred).\n\nThat's all you need!\n\nAs a general rule, we recommend users to not manually set a device once the model has been loaded with . So any device assignment call to the model, or to any model‚Äôs submodules should be avoided after that line - unless you know what you are doing.\n\nKeep in mind that loading a quantized model will automatically cast other model's submodules into dtype. You can change this behavior, (if for example you want to have the layer norms in ), by passing to the method.\n\nYou can play with different variants of 4bit quantization such as NF4 (normalized float 4 (default)) or pure FP4 quantization. Based on theoretical considerations and empirical results from the paper, we recommend using NF4 quantization for better performance.\n\nOther options include which uses a second quantization after the first one to save an additional 0.4 bits per parameter. And finally, the compute type. While 4-bit bitsandbytes stores weights in 4-bits, the computation still happens in 16 or 32-bit and here any combination can be chosen (float16, bfloat16, float32 etc).\n\nThe matrix multiplication and training will be faster if one uses a 16-bit compute dtype (default torch.float32). One should leverage the recent from transformers to change these parameters. An example to load a model in 4bit using NF4 quantization below with double quantization with the compute dtype bfloat16 for faster training:\n\nAs mentioned above, you can also change the compute dtype of the quantized model by just changing the argument in .\n\nFor enabling nested quantization, you can use the argument in . This will enable a second quantization after the first one to save an additional 0.4 bits per parameter. We also use this feature in the training Google colab notebook.\n\nAnd of course, as mentioned in the beginning of the section, all of these components are composable. You can combine all these parameters together to find the optimial use case for you. A rule of thumb is: use double quant if you have problems with memory, use NF4 for higher precision, and use a 16-bit dtype for faster finetuning. For instance in the inference demo, we use nested quantization, bfloat16 compute dtype and NF4 quantization to fit gpt-neo-x-20b (40GB) entirely in 4bit in a single 16GB GPU.\n\nIn this section, we will also address some common questions anyone could have regarding this integration.\n\nNote that this method is only compatible with GPUs, hence it is not possible to quantize models in 4bit on a CPU. Among GPUs, there should not be any hardware requirement about this method, therefore any GPU could be used to run the 4bit quantization as long as you have CUDA>=11.2 installed. Keep also in mind that the computation is not done in 4bit, the weights and activations are compressed to that format and the computation is still kept in the desired or native dtype.\n\nSimilarly as the integration of LLM.int8 presented in this blogpost the integration heavily relies on the library. Therefore, any model that supports accelerate loading (i.e. the argument when calling ) should be quantizable in 4bit. Note also that this is totally agnostic to modalities, as long as the models can be loaded with the argument, it is possible to quantize them.\n\nFor text models, at this time of writing, this would include most used architectures such as Llama, OPT, GPT-Neo, GPT-NeoX for text models, Blip2 for multimodal models, and so on.\n\nAt this time of writing, the models that support accelerate are:\n\nNote that if your favorite model is not there, you can open a Pull Request or raise an issue in transformers to add the support of accelerate loading for that architecture.\n\nIt is not possible to perform pure 4bit training on these models. However, you can train these models by leveraging parameter efficient fine tuning methods (PEFT) and train for example adapters on top of them. That is what is done in the paper and is officially supported by the PEFT library from Hugging Face. We also provide a training notebook and recommend users to check the QLoRA repository if they are interested in replicating the results from the paper.\n\nThis integration can open up several positive consequences to the community and AI research as it can affect multiple use cases and possible applications. In RLHF (Reinforcement Learning with Human Feedback) it is possible to load a single base model, in 4bit and train multiple adapters on top of it, one for the reward modeling, and another for the value policy training. A more detailed blogpost and announcement will be made soon about this use case.\n\nWe have also made some benchmarks on the impact of this quantization method on training large models on consumer hardware. We have run several experiments on finetuning 2 different architectures, Llama 7B (15GB in fp16) and Llama 13B (27GB in fp16) on an NVIDIA T4 (16GB) and here are the results\n\nWe have used the recent from TRL library, and the benchmarking script can be found here\n\nTry out the Guananco model cited on the paper on the playground or directly below\n\nThe HF team would like to acknowledge all the people involved in this project from University of Washington, and for making this available to the community.\n\nThe authors would also like to thank Pedro Cuenca for kindly reviewing the blogpost, Olivier Dehaene and Omar Sanseviero for their quick and strong support for the integration of the paper's artifacts on the HF Hub."
    },
    {
        "link": "https://github.com/huggingface/transformers/blob/main/docs/source/en/peft.md",
        "document": "PEFT, a library of parameter-efficient fine-tuning methods, enables training and storing large models on consumer GPUs. These methods only fine-tune a small number of extra model parameters, also known as adapters, on top of the pretrained model. A significant amount of memory is saved because the GPU doesn't need to store the optimizer states and gradients for the pretrained base model. Adapters are very lightweight, making it convenient to share, store, and load them.\n\nThis guide provides a short introduction to the PEFT library and how to use it for training with Transformers. For more details, refer to the PEFT documentation.\n\nInstall PEFT with the command below.\n\nLow-Rank Adaptation (LoRA) is a very common PEFT method that decomposes the weight matrix into two smaller trainable matrices. Start by defining a LoraConfig object with the parameters shown below.\n\nAdd LoraConfig to the model with [ ]. The model is now ready to be passed to [ ] for training.\n\nTo add an additional trainable adapter on top of a model with an existing adapter attached, specify the modules you want to train in modules_to_save().\n\nFor example, to train the module on top of a causal language model with a LoRA adapter attached, set . Add the adapter to the model as shown below, and then pass it to [ ].\n\nSave your adapter with [ ] to reuse it.\n\nTo load an adapter with Transformers, the Hub repository or local directory must contain an file and the adapter weights. Load the adapter with [ ] or with [ ].\n\nFor very large models, it is helpful to load a quantized version of the model in 8 or 4-bit precision to save memory. Transformers supports quantization with its bitsandbytes integration. Specify in [ ] whether you want to load a model in 8 or 4-bit precision.\n\nFor multiple devices, add to automatically distribute the model across your hardware.\n\n[ ] adds a new adapter to a model. To add a second adapter, the new adapter must be the same type as the first adapter. Use the parameter to assign a name to the adapter.\n\nOnce added, use [ ] to force a model to use the specified adapter and disable the other adapters.\n\n[ ] is a broader function that enables all adapters attached to a model, and [ ] disables all attached adapters."
    },
    {
        "link": "https://medium.com/@rakeshrajpurohit/model-quantization-with-hugging-face-transformers-and-bitsandbytes-integration-b4c9983e8996",
        "document": "This blog post explores the integration of Hugging Face‚Äôs Transformers library with the Bitsandbytes library, which simplifies the process of model quantization, making it more accessible and user-friendly.\n\nQuantization is a technique used to reduce the precision of numerical values in a model. Instead of using high-precision data types, such as 32-bit floating-point numbers, quantization represents values using lower-precision data types, such as 8-bit integers. This process significantly reduces memory usage and can speed up model execution while maintaining acceptable accuracy.\n\nHugging Face‚Äôs Transformers library is a go-to choice for working with pre-trained language models. To make the process of model quantization more accessible, Hugging Face has seamlessly integrated with the Bitsandbytes library. This integration simplifies the quantization process and empowers users to achieve efficient models with just a few lines of code.\n\nOne of the key features of this integration is the ability to load models in 4-bit quantization. This can be done by setting the argument when calling the method. By doing so, you can reduce memory usage by approximately fourfold.\n\nFor further memory optimization, you can load a model in 8-bit quantization. This can be achieved by using the argument when calling . This reduces the memory footprint by approximately half.\n\nYou can even check the memory footprint of your model using the method:\n\nThe Hugging Face and Bitsandbytes integration goes beyond basic quantization techniques. Here are some use cases you can explore:\n\nYou can modify the data type used during computation by setting the to a different value, such as . This can result in speed improvements in specific scenarios. Here's an example:\n\nThe NF4 data type is designed for weights initialized using a normal distribution. You can use it by specifying :\n\nThe integration also recommends using the nested quantization technique for even greater memory efficiency without sacrificing performance. This technique has proven beneficial, especially when fine-tuning large models:\n\nA quantized model can be loaded with ease using the method. Make sure the saved weights are quantized by checking the attribute in the model configuration:\n\nIn this case, you don‚Äôt need to specify the argument, but you must have both Bitsandbytes and Accelerate library installed.\n\nThere are additional techniques and configurations to consider:\n\nOne advanced use case involves loading a model and distributing weights between the CPU and GPU. This can be achieved by setting . This feature is beneficial for users who need to fit large models and distribute them between the GPU and CPU.\n\nExperiment with the argument to change the threshold for outliers. This parameter impacts inference speed and can be fine-tuned to suit your specific use case.\n\nSkipping the Conversion of Some Modules\n\nIn certain situations, you may want to skip the conversion of specific modules to 8-bit. You can do this using the argument.\n\nWith the support of adapters in the Hugging Face ecosystem, can fine-tune models loaded in 8-bit quantization, enabling the fine-tuning of large models with ease.\n\nQuantization is a powerful technique for optimizing machine learning models. The integration of Hugging Face‚Äôs Transformers library with the Bitsandbytes library makes this technique accessible to a broader audience. Whether you‚Äôre looking to reduce memory usage, speed up model execution, or share quantized models with the community, this integration provides the tools and flexibility you need to do so. It‚Äôs a significant step towards making efficient machine learning models available to all."
    },
    {
        "link": "https://pypi.org/project/bitsandbytes",
        "document": "A required part of this site couldn‚Äôt load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://github.com/bitsandbytes-foundation/bitsandbytes/releases",
        "document": "PR #1401 brings full LLM.int8() support for NVIDIA Hopper GPUs such as the H100, H200, and H800!\n\nAs part of the compatibility enhancements, we've rebuilt much of the LLM.int8() code in order to simplify for future compatibility and maintenance. We no longer use the or architecture-specific tensor layout formats while maintaining backwards compatibility. We additionally bring performance improvements targeted for inference scenarios.\n\nThis release includes broad performance improvements for a wide variety of inference scenarios. See this X thread for a detailed explanation.\n\nThe improvements were measured using the ü§óoptimum-benchmark tool.\n\nFor more benchmark results, see benchmarking/README.md.\n‚Ä¢ Turing/Ampere/Ada: The observed per-token throughput is improved by 60-85%, while latency is decreased by 40-45%.\n‚Ä¢ H100: With our benchmarking of Llama 3.1 70B, we observed the new LLM.int8() to consistently outperform NF4 at batch size >= 8.\n\nExample throughput improvement for Qwen 2.5 14B Instruct on RTX 4090:\n\nExample throughput improvement for Qwen 2.5 3B Instruct on T4:\n‚Ä¢ Turing/Ampere/Ada: With batch size of 1, per-token throughput is improved by 10-25% and per-token latency is decreased by 10-20%.\n‚Ä¢ H100: Across all batch sizes, per-token throughput is improved by up to 28% and per-token latency is decreased by up to 22%.\n\nExample throughput improvement for Qwen 2.5 14B Instruct on RTX 4090:\n\nExample throughput improvement for Qwen 2.5 3B Instruct on T4:\n\nThe size of our wheel has been reduced by ~43.5% from 122.4 MB to 69.1 MB! This results in an on-disk size decrease from ~396MB to ~224MB.\n‚Ä¢ Binaries built with CUDA Toolkit 12.6.2 are now included in the PyPI distribution.\n‚Ä¢ The CUDA 12.5.0 build has been updated to CUDA Toolkit 12.5.1.\n\nü§óPEFT users wishing to merge adapters with 8-bit weights will need to upgrade to .\n‚Ä¢ A new public API for int8 dequantization has been added: . This functionality is being integrated into ü§óPEFT and ü§ótransformers.\n‚Ä¢ We've continued to make documentation updates. The module now has an API documentation page.\n\nA number of public API functions have been marked for deprecation and will emit when used. These functions will become unavailable in future releases. This should have minimal impact on most end-users.\n\nThe k-bit quantization features are deprecated in favor of blockwise quantization. For all optimizers, using is not recommended and support will be removed in a future release.\n\nAs part of the refactoring process, we've implemented many new 8bit operations. These operations no longer use specialized data layouts.\n\nThe following relevant functions from are now deprecated :\n\nAdditionally the following functions from are deprecated:\n‚Ä¢ refine docs for multi-backend alpha release by @Titus-von-Koeller in #1380\n‚Ä¢ README: Replace special Unicode text symbols with regular characters by @akx in #1385\n‚Ä¢ Fix invalid escape sequence warning in Python 3.12 by @oshiteku in #1420\n‚Ä¢ [Build] Add CUDA 12.6.2 build; update 12.5.0 to 12.5.1 by @matthewdouglas in #1431\n‚Ä¢ @oshiteku made their first contribution in #1420"
    },
    {
        "link": "https://huggingface.co/docs/bitsandbytes/v0.42.0/en/index",
        "document": "The bitsandbytes is a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions.\n\nIn some cases it can happen that you need to compile from source. If this happens please consider submitting a bug report with information. What now follows is some short instructions which might work out of the box if is installed. If these do not work see further below.\n\nA more detailed example, can be found in examples/int8_inference_huggingface.py.\n‚Ä¢ Add 8-bit optimizer of your choice (arguments stay the same)\n‚Ä¢ There are two modes:\n‚Ä¢ To use the full LLM.int8() method, use the argument. We recommend .\n‚Ä¢ Stable Embedding Layer: Improved stability through better initialization, and normalization\n‚Ä¢ Fast quantile estimation: Up to 100x faster than other algorithms\n‚Ä¢ LLM.int8(): NVIDIA Turing (RTX 20xx; T4) or Ampere GPU (RTX 30xx; A4-A100); (a GPU from 2018 or newer).\n\nThe bitsandbytes library is currently only supported on Linux distributions. Windows is not supported at the moment.\n\nThe requirements can best be fulfilled by installing pytorch via anaconda. You can install PyTorch by following the ‚ÄúGet Started‚Äù instructions on the official website.\n\nFor straight Int8 matrix multiplication with mixed precision decomposition you can use . To enable mixed precision decomposition, use the threshold parameter:\n\nFor instructions how to use LLM.int8() inference layers in your own code, see the TL;DR above or for extended instruction see this blog post.\n\nWith bitsandbytes 8-bit optimizers can be used by changing a single line of code in your codebase. For NLP models we recommend also to use the StableEmbedding layers (see below) which improves results and helps with stable 8-bit optimization. To get started with 8-bit optimizers, it is sufficient to replace your old optimizer with the 8-bit optimizer in the following way:\n\nNote that by default all parameter tensors with less than 4096 elements are kept at 32-bit even if you initialize those parameters with 8-bit optimizers. This is done since such small tensors do not save much memory and often contain highly variable parameters (biases) or parameters that require high precision (batch norm, layer norm). You can change this behavior like so:\n\nChange Bits and other Hyperparameters for Individual Parameters\n\nIf you want to optimize some unstable parameters with 32-bit Adam and others with 8-bit Adam, you can use the . With this, we can also configure specific hyperparameters for particular layers, such as embedding layers. To do that, we need two things: (1) register the parameter while they are still on the CPU, (2) override the config with the new desired hyperparameters (anytime, anywhere). See our guide for more details\n\nTo use the Stable Embedding Layer, override the respective function of your model. Make sure to also use the flag to disable scaling of the word embedding layer (nor replaced with layer norm). You can use the optimizers by replacing the optimizer in the respective file ( etc.).\n\nFor upcoming features and changes and full history see Patch Notes.\n‚Ä¢ RuntimeError: CUDA error: no kernel image is available for execution on the device. Solution\n\nTo compile from source, you need an installation of CUDA. If is not installed, you can install the CUDA Toolkit with nvcc through the following commands.\n\nTo use a specific CUDA version just for a single compile run, you can set the variable , for example the following command compiles using compiler flags for cuda11x with the cuda version at :\n\nFor more detailed instruction, please follow the compile_from_source.md instructions.\n\nThe majority of bitsandbytes is licensed under MIT, however portions of the project are available under separate license terms: Pytorch is licensed under the BSD license.\n\nWe thank Fabio Cannizzo for his work on FastBinarySearch which we use for CPU quantization.\n\nHow to cite us\n\nIf you found this library and found LLM.int8() useful, please consider citing our work:\n\nFor 8-bit optimizers or quantization routines, please consider citing the following work:"
    },
    {
        "link": "https://github.com/fa0311/bitsandbytes-windows",
        "document": "The bitsandbytes is a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions.\n\nLooking for something old? here\n\nIn some cases it can happen that you need to compile from source. If this happens please consider submitting a bug report with information. What now follows is some short instructions which might work out of the box if is installed. If these do not work see further below.\n\nA more detailed example, can be found in examples/int8_inference_huggingface.py.\n‚Ä¢ Add 8-bit optimizer of your choice (arguments stay the same)\n‚Ä¢ There are two modes:\n‚Ä¢ To use the full LLM.int8() method, use the argument. We recommend .\n‚Ä¢ Stable Embedding Layer: Improved stability through better initialization, and normalization\n‚Ä¢ Fast quantile estimation: Up to 100x faster than other algorithms\n‚Ä¢ LLM.int8(): NVIDIA Turing (RTX 20xx; T4) or Ampere GPU (RTX 30xx; A4-A100); (a GPU from 2018 or older).\n\nThe bitsandbytes library is currently only supported on Linux distributions. Windows is not supported at the moment.\n\nThe requirements can best be fulfilled by installing pytorch via anaconda. You can install PyTorch by following the \"Get Started\" instructions on the official website.\n\nFor straight Int8 matrix multiplication with mixed precision decomposition you can use . To enable mixed precision decomposition, use the threshold parameter:\n\nFor instructions how to use LLM.int8() inference layers in your own code, see the TL;DR above or for extended instruction see this blog post.\n\nWith bitsandbytes 8-bit optimizers can be used by changing a single line of code in your codebase. For NLP models we recommend also to use the StableEmbedding layers (see below) which improves results and helps with stable 8-bit optimization. To get started with 8-bit optimizers, it is sufficient to replace your old optimizer with the 8-bit optimizer in the following way:\n\nNote that by default all parameter tensors with less than 4096 elements are kept at 32-bit even if you initialize those parameters with 8-bit optimizers. This is done since such small tensors do not save much memory and often contain highly variable parameters (biases) or parameters that require high precision (batch norm, layer norm). You can change this behavior like so:\n\nIf you want to optimize some unstable parameters with 32-bit Adam and others with 8-bit Adam, you can use the . With this, we can also configure specific hyperparameters for particular layers, such as embedding layers. To do that, we need two things: (1) register the parameter while they are still on the CPU, (2) override the config with the new desired hyperparameters (anytime, anywhere). See our guide for more details\n\nTo use the Stable Embedding Layer, override the respective function of your model. Make sure to also use the flag to disable scaling of the word embedding layer (nor replaced with layer norm). You can use the optimizers by replacing the optimizer in the respective file ( etc.).\n\nFor upcoming features and changes and full history see Patch Notes.\n‚Ä¢ RuntimeError: CUDA error: no kernel image is available for execution on the device. Solution\n\nTo compile from source, you need an installation of CUDA. If is not installed, you can install the CUDA Toolkit with nvcc through the following commands.\n\nTo use a specific CUDA version just for a single compile run, you can set the variable , for example the following command compiles using compiler flags for cuda11x with the cuda version at :\n\nFor more detailed instruction, please follow the compile_from_source.md instructions.\n\nThe majority of bitsandbytes is licensed under MIT, however portions of the project are available under separate license terms: Pytorch is licensed under the BSD license.\n\nWe thank Fabio Cannizzo for his work on FastBinarySearch which we use for CPU quantization.\n\nIf you found this library and found LLM.int8() useful, please consider citing our work:\n\nFor 8-bit optimizers or quantization routines, please consider citing the following work:"
    },
    {
        "link": "https://huggingface.co/blog/4bit-transformers-bitsandbytes",
        "document": "Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA\n\nThis article is also available in Chinese ÁÆÄ‰Ωì‰∏≠Êñá.\n\nWe present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimizers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.\n\nThis blogpost and release come with several resources to get started with 4bit models and QLoRA:\n‚Ä¢ Basic usage Google Colab notebook - This notebook shows how to use 4bit models in inference with all their variants, and how to run GPT-neo-X (a 20B parameter model) on a free Google Colab instance ü§Ø\n‚Ä¢ Fine tuning Google Colab notebook - This notebook shows how to fine-tune a 4bit model on a downstream task using the Hugging Face ecosystem. We show that it is possible to fine tune GPT-neo-X 20B on a Google Colab instance!\n‚Ä¢ Guanaco 33b playground - or check the playground section below\n\nIf you are not familiar with model precisions and the most common data types (float16, float32, bfloat16, int8), we advise you to carefully read the introduction in our first blogpost that goes over the details of these concepts in simple terms with visualizations.\n\nFor more information we recommend reading the fundamentals of floating point representation through this wikibook document.\n\nThe recent QLoRA paper explores different data types, 4-bit Float and 4-bit NormalFloat. We will discuss here the 4-bit Float data type since it is easier to understand.\n\nFP8 and FP4 stand for Floating Point 8-bit and 4-bit precision, respectively. They are part of the minifloats family of floating point values (among other precisions, the minifloats family also includes bfloat16 and float16).\n\nLet‚Äôs first have a look at how to represent floating point values in FP8 format, then understand how the FP4 format looks like.\n\nAs discussed in our previous blogpost, a floating point contains n-bits, with each bit falling into a specific category that is responsible for representing a component of the number (sign, mantissa and exponent). These represent the following.\n\nThe FP8 (floating point 8) format has been first introduced in the paper ‚ÄúFP8 for Deep Learning‚Äù with two different FP8 encodings: E4M3 (4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa).\n\nAlthough the precision is substantially reduced by reducing the number of bits from 32 to 8, both versions can be used in a variety of situations. Currently one could use Transformer Engine library that is also integrated with HF ecosystem through accelerate.\n\nThe potential floating points that can be represented in the E4M3 format are in the range -448 to 448, whereas in the E5M2 format, as the number of bits of the exponent increases, the range increases to -57344 to 57344 - but with a loss of precision because the number of possible representations remains constant. It has been empirically proven that the E4M3 is best suited for the forward pass, and the second version is best suited for the backward computation\n\nThe sign bit represents the sign (+/-), the exponent bits a base two to the power of the integer represented by the bits (e.g. ), and the fraction or mantissa is the sum of powers of negative two which are ‚Äúactive‚Äù for each bit that is ‚Äú1‚Äù. If a bit is ‚Äú0‚Äù the fraction remains unchanged for that power of where i is the position of the bit in the bit-sequence. For example, for mantissa bits 1010 we have . To get a value, we add 1 to the fraction and multiply all results together, for example, with 2 exponent bits and one mantissa bit the representations 1101 would be:\n\nFor FP4 there is no fixed format and as such one can try combinations of different mantissa/exponent combinations. In general, 3 exponent bits do a bit better in most cases. But sometimes 2 exponent bits and a mantissa bit yield better performance.\n\nQLoRA paper, a new way of democratizing quantized large transformer models\n\nIn few words, QLoRA reduces the memory usage of LLM finetuning without performance tradeoffs compared to standard 16-bit model finetuning. This method enables 33B model finetuning on a single 24GB GPU and 65B model finetuning on a single 46GB GPU.\n\nMore specifically, QLoRA uses 4-bit quantization to compress a pretrained language model. The LM parameters are then frozen and a relatively small number of trainable parameters are added to the model in the form of Low-Rank Adapters. During finetuning, QLoRA backpropagates gradients through the frozen 4-bit quantized pretrained language model into the Low-Rank Adapters. The LoRA layers are the only parameters being updated during training. Read more about LoRA in the original LoRA paper.\n\nQLoRA has one storage data type (usually 4-bit NormalFloat) for the base model weights and a computation data type (16-bit BrainFloat) used to perform computations. QLoRA dequantizes weights from the storage data type to the computation data type to perform the forward and backward passes, but only computes weight gradients for the LoRA parameters which use 16-bit bfloat. The weights are decompressed only when they are needed, therefore the memory usage stays low during training and inference.\n\nQLoRA tuning is shown to match 16-bit finetuning methods in a wide range of experiments. In addition, the Guanaco models, which use QLoRA finetuning for LLaMA models on the OpenAssistant dataset (OASST1), are state-of-the-art chatbot systems and are close to ChatGPT on the Vicuna benchmark. This is an additional demonstration of the power of QLoRA tuning.\n\nFor a more detailed reading, we recommend you read the QLoRA paper.\n\nHow to use it in transformers?\n\nIn this section let us introduce the transformers integration of this method, how to use it and which models can be effectively quantized.\n\nAs a quickstart, load a model in 4bit by (at the time of this writing) installing accelerate and transformers from source, and make sure you have installed the latest version of bitsandbytes library (0.39.0).\n\nThe basic way to load a model in 4bit is to pass the argument when calling the method by providing a device map (pass to get a device map that will be automatically inferred).\n\nThat's all you need!\n\nAs a general rule, we recommend users to not manually set a device once the model has been loaded with . So any device assignment call to the model, or to any model‚Äôs submodules should be avoided after that line - unless you know what you are doing.\n\nKeep in mind that loading a quantized model will automatically cast other model's submodules into dtype. You can change this behavior, (if for example you want to have the layer norms in ), by passing to the method.\n\nYou can play with different variants of 4bit quantization such as NF4 (normalized float 4 (default)) or pure FP4 quantization. Based on theoretical considerations and empirical results from the paper, we recommend using NF4 quantization for better performance.\n\nOther options include which uses a second quantization after the first one to save an additional 0.4 bits per parameter. And finally, the compute type. While 4-bit bitsandbytes stores weights in 4-bits, the computation still happens in 16 or 32-bit and here any combination can be chosen (float16, bfloat16, float32 etc).\n\nThe matrix multiplication and training will be faster if one uses a 16-bit compute dtype (default torch.float32). One should leverage the recent from transformers to change these parameters. An example to load a model in 4bit using NF4 quantization below with double quantization with the compute dtype bfloat16 for faster training:\n\nAs mentioned above, you can also change the compute dtype of the quantized model by just changing the argument in .\n\nFor enabling nested quantization, you can use the argument in . This will enable a second quantization after the first one to save an additional 0.4 bits per parameter. We also use this feature in the training Google colab notebook.\n\nAnd of course, as mentioned in the beginning of the section, all of these components are composable. You can combine all these parameters together to find the optimial use case for you. A rule of thumb is: use double quant if you have problems with memory, use NF4 for higher precision, and use a 16-bit dtype for faster finetuning. For instance in the inference demo, we use nested quantization, bfloat16 compute dtype and NF4 quantization to fit gpt-neo-x-20b (40GB) entirely in 4bit in a single 16GB GPU.\n\nIn this section, we will also address some common questions anyone could have regarding this integration.\n\nDoes FP4 quantization have any hardware requirements?\n\nNote that this method is only compatible with GPUs, hence it is not possible to quantize models in 4bit on a CPU. Among GPUs, there should not be any hardware requirement about this method, therefore any GPU could be used to run the 4bit quantization as long as you have CUDA>=11.2 installed. Keep also in mind that the computation is not done in 4bit, the weights and activations are compressed to that format and the computation is still kept in the desired or native dtype.\n\nWhat are the supported models?\n\nSimilarly as the integration of LLM.int8 presented in this blogpost the integration heavily relies on the library. Therefore, any model that supports accelerate loading (i.e. the argument when calling ) should be quantizable in 4bit. Note also that this is totally agnostic to modalities, as long as the models can be loaded with the argument, it is possible to quantize them.\n\nFor text models, at this time of writing, this would include most used architectures such as Llama, OPT, GPT-Neo, GPT-NeoX for text models, Blip2 for multimodal models, and so on.\n\nAt this time of writing, the models that support accelerate are:\n\nNote that if your favorite model is not there, you can open a Pull Request or raise an issue in transformers to add the support of accelerate loading for that architecture.\n\nIt is not possible to perform pure 4bit training on these models. However, you can train these models by leveraging parameter efficient fine tuning methods (PEFT) and train for example adapters on top of them. That is what is done in the paper and is officially supported by the PEFT library from Hugging Face. We also provide a training notebook and recommend users to check the QLoRA repository if they are interested in replicating the results from the paper.\n\nWhat other consequences are there?\n\nThis integration can open up several positive consequences to the community and AI research as it can affect multiple use cases and possible applications. In RLHF (Reinforcement Learning with Human Feedback) it is possible to load a single base model, in 4bit and train multiple adapters on top of it, one for the reward modeling, and another for the value policy training. A more detailed blogpost and announcement will be made soon about this use case.\n\nWe have also made some benchmarks on the impact of this quantization method on training large models on consumer hardware. We have run several experiments on finetuning 2 different architectures, Llama 7B (15GB in fp16) and Llama 13B (27GB in fp16) on an NVIDIA T4 (16GB) and here are the results\n\nWe have used the recent from TRL library, and the benchmarking script can be found here\n\nTry out the Guananco model cited on the paper on the playground or directly below\n\nThe HF team would like to acknowledge all the people involved in this project from University of Washington, and for making this available to the community.\n\nThe authors would also like to thank Pedro Cuenca for kindly reviewing the blogpost, Olivier Dehaene and Omar Sanseviero for their quick and strong support for the integration of the paper's artifacts on the HF Hub."
    }
]