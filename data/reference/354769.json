[
    {
        "link": "https://kernel.org/doc/html/v5.18/index.html",
        "document": "This is the top level of the kernel’s documentation tree. Kernel documentation, like the kernel itself, is very much a work in progress; that is especially true as we work to integrate our many scattered documents into a coherent whole. Please note that improvements to the documentation are welcome; join the linux-doc list at vger.kernel.org if you want to help out.\n\nThe following describes the license of the Linux kernel source code (GPLv2), how to properly mark the license of individual files in the source tree, as well as links to the full license text."
    },
    {
        "link": "https://kernel.org/doc/html/v5.7",
        "document": "This is the top level of the kernel’s documentation tree. Kernel documentation, like the kernel itself, is very much a work in progress; that is especially true as we work to integrate our many scattered documents into a coherent whole. Please note that improvements to the documentation are welcome; join the linux-doc list at vger.kernel.org if you want to help out.\n\nThe following describes the license of the Linux kernel source code (GPLv2), how to properly mark the license of individual files in the source tree, as well as links to the full license text."
    },
    {
        "link": "https://geeksforgeeks.org/inter-process-communication-ipc",
        "document": "Processes need to communicate with each other in many situations, for example, to count occurrences of a word in text file, output of grep command needs to be given to wc command, something like grep -o -i <word> <file> | wc -l. Inter-Process Communication or IPC is a mechanism that allows processes to communicate. It helps processes synchronize their activities, share information, and avoid conflicts while accessing shared resources.\n\nLet us first talk about types of types of processes.\n• Independent process: An independent process is not affected by the execution of other processes. Independent processes are processes that do not share any data or resources with other processes. No inte-process communication required here.\n• Co-operating process: Interact with each other and share data or resources. A co-operating process can be affected by other executing processes. Inter-process communication (IPC) is a mechanism that allows processes to communicate with each other and synchronize their actions. The communication between these processes can be seen as a method of cooperation between them.\n\nInter process communication (IPC) allows different programs or processes running on a computer to share information with each other. IPC allows processes to communicate by using different techniques like sharing memory, sending messages, or using files. It ensures that processes can work together without interfering with each other. Cooperating processes require an Inter Process Communication (IPC) mechanism that will allow them to exchange data and information.\n\nThe two fundamental models of Inter Process Communication are:\n\nFigure 1 below shows a basic structure of communication between processes via the shared memory method and via the message passing method.\n\nAn operating system can implement both methods of communication. First, we will discuss the shared memory methods of communication and then message passing. Communication between processes using shared memory requires processes to share some variable, and it completely depends on how the programmer will implement it. One way of communication using shared memory can be imagined like this: Suppose process1 and process2 are executing simultaneously, and they share some resources or use some information from another process. Process1 generates information about certain computations or resources being used and keeps it as a record in shared memory. When process2 needs to use the shared information, it will check in the record stored in shared memory and take note of the information generated by process1 and act accordingly. Processes can use shared memory for extracting information as a record from another process as well as for delivering any specific information to other processes.\n\nFigure 1 below shows a basic structure of communication between processes via the shared memory method and via the message passing method.\n\nLet’s discuss an example of communication between processes using the shared memory method.\n\nInter-Process Communication refers to the techniques and methods that allow processes to exchange data and coordinate their activities. Since processes typically operate independently in a multitasking environment, IPC is essential for them to communicate effectively without interfering with one another. There are several methods of IPC, each designed to suit different scenarios and requirements. These methods include shared memory, message passing, semaphores, and signals, etc.\n\nTo read more refer – methods of Inter Process Communication\n\nIn IPC, synchronization is essential for controlling access to shared resources and guaranteeing that processes do not conflict with one another. Data consistency is ensured and problems like race situations are avoided with proper synchronization.\n• None Enables processes to communicate with each other and share resources, leading to increased efficiency and flexibility.\n• None Facilitates coordination between multiple processes, leading to better overall system performance.\n• None Allows for the creation of distributed systems that can span multiple computers or networks.\n• None Can be used to implement various and communication protocols, such as semaphores, pipes, and sockets.\n• None Increases system complexity, making it harder to design, implement, and debug.\n• None Can introduce security vulnerabilities, as processes may be able to access or modify data belonging to other processes.\n• None Requires careful management of system resources, such as memory and time, to ensure that IPC operations do not degrade overall system performance. \n\n Can lead to data inconsistencies if multiple processes try to access or modify the same data at the same time.\n• None Overall, the advantages of IPC outweigh the disadvantages, as it is a necessary mechanism for modern operating systems and enables processes to work together and share resources in a flexible and efficient manner. However, care must be taken to design and implement IPC systems carefully, in order to avoid potential security vulnerabilities and performance issues.\n\nA fundamental component of contemporary operating systems, IPC allows processes to efficiently coordinate operations, share resources, and communicate. IPC is beneficial for developing adaptable and effective systems, despite its complexity and possible security threats.\n\nWhy is synchronization important in IPC?\n\nWhat are the main methods of IPC?"
    },
    {
        "link": "https://tldp.org/LDP/tlk/ipc/ipc.html",
        "document": "Processes communicate with each other and with the kernel to coordinate their activities. Linux supports a number of Inter-Process Communication (IPC) mechanisms. Signals and pipes are two of them but Linux also supports the System V IPC mechanisms named after the Unix release in which they first appeared.\n\nThere are a set of defined signals that the kernel can generate or that can be generated by other processes in the system, provided that they have the correct privileges. You can list a system's set of signals using the command (kill -l), on my Intel Linux box this gives:\n\nThe numbers are different for an Alpha AXP Linux box. Processes can choose to ignore most of the signals that are generated, with two notable exceptions: neither the signal which causes a process to halt its execution nor the signal which causes a process to exit can be ignored. Otherwise though, a process can choose just how it wants to handle the various signals. Processes can block the signals and, if they do not block them, they can either choose to handle them themselves or allow the kernel to handle them. If the kernel handles the signals, it will do the default actions required for this signal. For example, the default action when a process receives the (floating point exception) signal is to core dump and then exit. Signals have no inherent relative priorities. If two signals are generated for a process at the same time then they may be presented to the process or handled in any order. Also there is no mechanism for handling multiple signals of the same kind. There is no way that a process can tell if it received 1 or 42 signals.\n\nLinux implements signals using information stored in the for the process. The number of supported signals is limited to the word size of the processor. Processes with a word size of 32 bits can have 32 signals whereas 64 bit processors like the Alpha AXP may have up to 64 signals. The currently pending signals are kept in the field with a mask of blocked signals held in . With the exception of and , all signals can be blocked. If a blocked signal is generated, it remains pending until it is unblocked. Linux also holds information about how each process handles every possible signal and this is held in an array of data structures pointed at by the for each process. Amongst other things it contains either the address of a routine that will handle the signal or a flag which tells Linux that the process either wishes to ignore this signal or let the kernel handle the signal for it. The process modifies the default signal handling by making system calls and these calls alter the for the appropriate signal as well as the mask.\n\nNot every process in the system can send signals to every other process, the kernel can and super users can. Normal processes can only send signals to processes with the same uid and gid or to processes in the same process group1. Signals are generated by setting the appropriate bit in the 's field. If the process has not blocked the signal and is waiting but interruptible (in state Interruptible) then it is woken up by changing its state to Running and making sure that it is in the run queue. That way the scheduler will consider it a candidate for running when the system next schedules. If the default handling is needed, then Linux can optimize the handling of the signal. For example if the signal (the X window changed focus) and the default handler is being used then there is nothing to be done.\n\nSignals are not presented to the process immediately they are generated., they must wait until the process is running again. Every time a process exits from a system call its and fields are checked and, if there are any unblocked signals, they can now be delivered. This might seem a very unreliable method but every process in the system is making system calls, for example to write a character to the terminal, all of the time. Processes can elect to wait for signals if they wish, they are suspended in state Interruptible until a signal is presented. The Linux signal processing code looks at the structure for each of the current unblocked signals.\n\nIf a signal's handler is set to the default action then the kernel will handle it. The signal's default handler will change the current process's state to Stopped and then run the scheduler to select a new process to run. The default action for the signal will core dump the process and then cause it to exit. Alternatively, the process may have specfied its own signal handler. This is a routine which will be called whenever the signal is generated and the structure holds the address of this routine. The kernel must call the process's signal handling routine and how this happens is processor specific but all CPUs must cope with the fact that the current process is running in kernel mode and is just about to return to the process that called the kernel or system routine in user mode. The problem is solved by manipulating the stack and registers of the process. The process's program counter is set to the address of its signal handling routine and the parameters to the routine are added to the call frame or passed in registers. When the process resumes operation it appears as if the signal handling routine were called normally.\n\nLinux is POSIX compatible and so the process can specify which signals are blocked when a particular signal handling routine is called. This means changing the mask during the call to the processes signal handler. The mask must be returned to its original value when the signal handling routine has finished. Therefore Linux adds a call to a tidy up routine which will restore the original mask onto the call stack of the signalled process. Linux also optimizes the case where several signal handling routines need to be called by stacking them so that each time one handling routine exits, the next one is called until the tidy up routine is called.\n\npipes the output from the command listing the directory's files into the standard input of the command which paginates them. Finally the standard output from the command is piped into the standard input of the command which prints the results on the default printer. Pipes then are unidirectional byte streams which connect the standard output from one process into the standard input of another process. Neither process is aware of this redirection and behaves just as it would normally. It is the shell which sets up these temporary pipes between the processes.\n\nIn Linux, a pipe is implemented using two data structures which both point at the same temporary VFS inode which itself points at a physical page within memory. Figure 5.1 shows that each data structure contains pointers to different file operation routine vectors; one for writing to the pipe, the other for reading from the pipe.\n\nThis hides the underlying differences from the generic system calls which read and write to ordinary files. As the writing process writes to the pipe, bytes are copied into the shared data page and when the reading process reads from the pipe, bytes are copied from the shared data page. Linux must synchronize access to the pipe. It must make sure that the reader and the writer of the pipe are in step and to do this it uses locks, wait queues and signals.\n\nWhen the writer wants to write to the pipe it uses the standard write library functions. These all pass file descriptors that are indices into the process's set of data structures, each one representing an open file or, as in this case, an open pipe. The Linux system call uses the write routine pointed at by the data structure describing this pipe. That write routine uses information held in the VFS inode representing the pipe to manage the write request.\n\nIf there is enough room to write all of the bytes into the pipe and, so long as the pipe is not locked by its reader, Linux locks it for the writer and copies the bytes to be written from the process's address space into the shared data page. If the pipe is locked by the reader or if there is not enough room for the data then the current process is made to sleep on the pipe inode's wait queue and the scheduler is called so that another process can run. It is interruptible, so it can receive signals and it will be woken by the reader when there is enough room for the write data or when the pipe is unlocked. When the data has been written, the pipe's VFS inode is unlocked and any waiting readers sleeping on the inode's wait queue will themselves be woken up.\n\nReading data from the pipe is a very similar process to writing to it.\n\nProcesses are allowed to do non-blocking reads (it depends on the mode in which they opened the file or pipe) and, in this case, if there is no data to be read or if the pipe is locked, an error will be returned. This means that the process can continue to run. The alternative is to wait on the pipe inode's wait queue until the write process has finished. When both processes have finished with the pipe, the pipe inode is discarded along with the shared data page.\n\nLinux also supports named pipes, also known as FIFOs because pipes operate on a First In, First Out principle. The first data written into the pipe is the first data read from the pipe. Unlike pipes, FIFOs are not temporary objects, they are entities in the file system and can be created using the command. Processes are free to use a FIFO so long as they have appropriate access rights to it. The way that FIFOs are opened is a little different from pipes. A pipe (its two data structures, its VFS inode and the shared data page) is created in one go whereas a FIFO already exists and is opened and closed by its users. Linux must handle readers opening the FIFO before writers open it as well as readers reading before any writers have written to it. That aside, FIFOs are handled almost exactly the same way as pipes and they use the same data structures and operations.\n\nAll Linux data structures representing System V IPC objects in the system include an\n\nstructure which contains the owner and creator process's user and group identifiers. The access mode for this object (owner, group and other) and the IPC object's key. The key is used as a way of locating the System V IPC object's reference identifier. Two sets of keys are supported: public and private. If the key is public then any process in the system, subject to rights checking, can find the reference identifier for the System V IPC object. System V IPC objects can never be referenced with a key, only by their reference identifier.\n\ndata structure contains an data structure and pointers to the messages entered onto this queue. In addition, Linux keeps queue modification times such as the last time that this queue was written to and so on. The also contains two wait queues; one for the writers to the queue and one for the readers of the message queue.\n\nEach time a process attempts to write a message to the write queue its effective user and group identifiers are compared with the mode in this queue's data structure. If the process can write to the queue then the message may be copied from the process's address space into a\n\ndata structure and put at the end of this message queue. Each message is tagged with an application specific type, agreed between the cooperating processes. However, there may be no room for the message as Linux restricts the number and length of messages that can be written. In this case the process will be added to this message queue's write wait queue and the scheduler will be called to select a new process to run. It will be woken up when one or more messages have been read from this message queue.\n\nReading from the queue is a similar process. Again, the processes access rights to the write queue are checked. A reading process may choose to either get the first message in the queue regardless of its type or select messages with particular types. If no messages match this criteria the reading process will be added to the message queue's read wait queue and the scheduler run. When a new message is written to the queue this process will be woken up and run again.\n\nIn its simplest form a semaphore is a location in memory whose value can be tested and set by more than one process. The test and set operation is, so far as each process is concerned, uninterruptible or atomic; once started nothing can stop it. The result of the test and set operation is the addition of the current value of the semaphore and the set value, which can be positive or negative. Depending on the result of the test and set operation one process may have to sleep until the semphore's value is changed by another process. Semaphores can be used to implement critical regions, areas of critical code that only one process at a time should be executing.\n\nSay you had many cooperating processes reading records from and writing records to a single data file. You would want that file access to be strictly coordinated. You could use a semaphore with an initial value of 1 and, around the file operating code, put two semaphore operations, the first to test and decrement the semaphore's value and the second to test and increment it. The first process to access the file would try to decrement the semaphore's value and it would succeed, the semaphore's value now being 0. This process can now go ahead and use the data file but if another process wishing to use it now tries to decrement the semaphore's value it would fail as the result would be -1. That process will be suspended until the first process has finished with the data file. When the first process has finished with the data file it will increment the semaphore's value, making it 1 again. Now the waiting process can be woken and this time its attempt to increment the semaphore will succeed.\n\nSystem V IPC semaphore objects each describe a semaphore array and Linux uses the\n\ndata structure to represent this. All of the data structures in the system are pointed at by the , a vector of pointers. There are in each semaphore array, each one described by a data structure pointed at by . All of the processes that are allowed to manipulate the semaphore array of a System V IPC semaphore object may make system calls that perform operations on them. The system call can specify many operations and each operation is described by three inputs; the semaphore index, the operation value and a set of flags. The semaphore index is an index into the semaphore array and the operation value is a numerical value that will be added to the current value of the semaphore. First Linux tests whether or not all of the operations would succeed. An operation will succeed if the operation value added to the semaphore's current value would be greater than zero or if both the operation value and the semaphore's current value are zero. If any of the semaphore operations would fail Linux may suspend the process but only if the operation flags have not requested that the system call is non-blocking. If the process is to be suspended then Linux must save the state of the semaphore operations to be performed and put the current process onto a wait queue. It does this by building a data structure on the stack and filling it out. The new data structure is put at the end of this semaphore object's wait queue (using the and pointers). The current process is put on the wait queue in the data structure ( ) and the scheduler called to choose another process to run.\n\nIf all of the semaphore operations would have succeeded and the current process does not need to be suspended, Linux goes ahead and applies the operations to the appropriate members of the semaphore array. Now Linux must check that any waiting, suspended, processes may now apply their semaphore operations. It looks at each member of the operations pending queue ( ) in turn, testing to see if the semphore operations will succeed this time. If they will then it removes the data structure from the operations pending list and applies the semaphore operations to the semaphore array. It wakes up the sleeping process making it available to be restarted the next time the scheduler runs. Linux keeps looking through the pending list from the start until there is a pass where no semaphore operations can be applied and so no more processes can be woken.\n\nThere is a problem with semaphores, deadlocks. These occur when one process has altered the semaphores value as it enters a critical region but then fails to leave the critical region because it crashed or was killed. Linux protects against this by maintaining lists of adjustments to the semaphore arrays. The idea is that when these adjustments are applied, the semaphores will be put back to the state that they were in before the a process's set of semaphore operations were applied. These adjustments are kept in data structures queued both on the data structure and on the data structure for the processes using these semaphore arrays.\n\nEach individual semaphore operation may request that an adjustment be maintained. Linux will maintain at most one data structure per process for each semaphore array. If the requesting process does not have one, then one is created when it is needed. The new data structure is queued both onto this process's data structure and onto the semaphore array's data structure. As operations are applied to the semphores in the semaphore array the negation of the operation value is added to this semphore's entry in the adjustment array of this process's data structure. So, if the operation value is 2, then -2 is added to the adjustment entry for this semaphore.\n\nWhen processes are deleted, as they exit Linux works through their set of data structures applying the adjustments to the semaphore arrays. If a semaphore set is deleted, the data structures are left queued on the process's but the semaphore array identifier is made invalid. In this case the semaphore clean up code simply discards the data structure.\n\nShared memory allows one or more processes to communicate via memory that appears in all of their virtual address spaces. The pages of the virtual memory is referenced by page table entries in each of the sharing processes' page tables. It does not have to be at the same address in all of the processes' virtual memory. As with all System V IPC objects, access to shared memory areas is controlled via keys and access rights checking. Once the memory is being shared, there are no checks on how the processes are using it. They must rely on other mechanisms, for example System V semaphores, to synchronize access to the memory.\n\nEach newly created shared memory area is represented by a data structure. These are kept in the vector.\n\nThe data structure decribes how big the area of shared memory is, how many processes are using it and information about how that shared memory is mapped into their address spaces. It is the creator of the shared memory that controls the access permissions to that memory and whether its key is public or private. If it has enough access rights it may also lock the shared memory into physical memory.\n\nEach process that wishes to share the memory must attach to that virtual memory via a system call. This creates a new data structure describing the shared memory for this process. The process can choose where in its virtual address space the shared memory goes or it can let Linux choose a free area large enough. The new structure is put into the list of pointed at by the . The and pointers are used to link them together. The virtual memory is not actually created during the attach; it happens when the first process attempts to access it.\n\nThe first time that a process accesses one of the pages of the shared virtual memory, a page fault will occur. When Linux fixes up that page fault it finds the data structure describing it. This contains pointers to handler routines for this type of shared virtual memory. The shared memory page fault handling code looks in the list of page table entries for this to see if one exists for this page of the shared virtual memory. If it does not exist, it will allocate a physical page and create a page table entry for it. As well as going into the current process's page tables, this entry is saved in the . This means that when the next process that attempts to access this memory gets a page fault, the shared memory fault handling code will use this newly created physical page for that process too. So, the first process that accesses a page of the shared memory causes it to be created and thereafter access by the other processes cause that page to be added into their virtual address spaces.\n\nWhen processes no longer wish to share the virtual memory, they detach from it. So long as other processes are still using the memory the detach only affects the current process. Its is removed from the data structure and deallocated. The current process's page tables are updated to invalidate the area of virtual memory that it used to share. When the last process sharing the memory detaches from it, the pages of the shared memory current in physical memory are freed, as is the data structure for this shared memory.\n\nFurther complications arise when shared virtual memory is not locked into physical memory. In this case the pages of the shared memory may be swapped out to the system's swap disk during periods of high memory usage. How shared memory memory is swapped into and out of physical memory is described in Chapter mm-chapter."
    },
    {
        "link": "https://medium.com/@adilrk/interprocess-communication-and-synchronization-305922b39daa",
        "document": "A running Linux system consists of numerous processes, many of which operate independently of each other. However, some processes cooperate to achieve their intended purposes, and these processes need methods of communicating with one another and synchronizing their actions.\n\nOne way for processes to communicate is by reading and writing information in disk files. However, this is often too slow and inflexible for applications that require real-time communication or that need to share data between processes that are running on different hosts.\n\nTo address these limitations, Linux provides a rich set of mechanisms for interprocess communication (IPC), including:\n• Signals: Signals are a lightweight way for processes to send notifications to each other. Signals can be used to indicate that an event has occurred, such as the arrival of a new message or the termination of another process.\n• Pipes and FIFOs: Pipes and FIFOs are unidirectional channels that can be used to transfer data between processes. Pipes are created by the kernel, while FIFOs are created by userspace processes.\n• Sockets: Sockets are bidirectional channels that can be used to transfer data between processes on the same host or on different hosts connected by a network. Sockets are a more general-purpose IPC mechanism than pipes or FIFOs, and they are often used for applications that require reliable and high-performance communication.\n• File locking: File locking allows a process to lock regions of a file in order to prevent other processes from reading or updating the file contents. File locking is often used to implement synchronization between processes that are sharing a file.\n• Message queues: Message queues are a way for processes to exchange messages (packets of data) with each other. Message queues are a more flexible IPC mechanism than pipes or FIFOs, and they can be used to implement a variety of communication patterns.\n• Semaphores: Semaphores are a way for processes to synchronize their actions. Semaphores can be used to implement mutual exclusion, counting semaphores, and barriers.\n• Shared memory: Shared memory is a way for two or more processes to share a piece of memory. When one process changes the contents of the shared memory, all of the other processes can immediately see the changes.Shared memory is a very efficient IPC mechanism, but it can be difficult to use correctly.\n\nThe wide variety of IPC mechanisms available on Linux systems reflects the different needs of different applications. For example, pipes and FIFOs are a good choice for applications that need to transfer small amounts of data quickly, while sockets are a better choice for applications that need to transfer large amounts of data reliably over a network.\n\nThe choice of IPC mechanism also depends on the specific requirements of the application. For example, if an application needs to implement mutual exclusion, then a semaphore is the best choice. If an application needs to exchange messages with other processes, then a message queue is the best choice.\n\nIn general, the best way to choose an IPC mechanism is to consider the specific requirements of the application and to choose the mechanism that best meets those requirements.\n\nThe hardware-based solutions to the critical-section problems are complicated as well as generally inaccessible to application programmers. Instead, operating-system designers build higher-level software tools to solve the critical-section problem. The simplest of these tools is the mutex lock. (In fact, the term mutex is short for mutual exclusion.) We use the mutex lock to protect critical sections and thus prevent race conditions. That is, a process must acquire the lock before entering a critical section; it releases the lock when it exits the critical section. The acquire()function acquires the lock, and the release() function releases the lock.\n\nA mutex lock has a boolean variable available whose value indicates if the lock is available or not. If the lock is available, a call to acquire() succeeds, and the lock is then considered unavailable. A process that attempts to acquire an unavailable lock is blocked until the lock is released.\n\nMutex locks, as we mentioned earlier, are generally considered the simplest of synchronization tools. In this section, we examine a more robust tool that can behave similarly to a mutex lock but can also provide more sophisticated ways for processes to synchronize their activities.\n\nA semaphore S is an integer variable that, apart from initialization, is accessed only through two standard atomic operations: wait() and signal(). Semaphores were introduced by the Dutch computer scientist Edsger Dijk- stra, and such, the wait() operation was originally termed P (from the Dutch proberen, “to test”); signal() was originally called V (from verhogen, “to increment”). The definition of wait() is as follows:\n\nThe definition of signal() is as follows:\n\nAll modifications to the integer value of the semaphore in the wait() and signal() operations must be executed atomically. That is, when one process modifies the semaphore value, no other process can simultaneously modify that same semaphore value. In addition, in the case of wait(S), the testing of the integer value of S (S ≤ 0), as well as its possible modification (S — ), must be executed without interruption.\n\nImplementation of mutex locks suffers from busy waiting. The definitions of the wait() and signal() semaphore operations just described present the same problem. To overcome this problem, we can modify the definition of the wait() and signal() operations as follows: When a process executes the wait() operation and finds that the semaphore value is not positive, it must wait. However, rather than engaging in busy waiting, the process can suspend itself. The suspend operation places a process into a waiting queue associated with the semaphore, and the state of the process is switched to the waiting state. Then control is transferred to the CPU scheduler, which selects another process to execute."
    },
    {
        "link": "https://stackoverflow.com/questions/28406304/inter-process-communication-in-c",
        "document": "I have a scenario, where one process should wait for a signal from another process, and this wait should be blocking wait, and as soon as it gets a signal, it should wake up.\n\nHowever, with mechanisms like kill() or raise(), the first process goes to wait state, but periodically checks after a specified amount of time, whether the even/signal occurred or not, and decides to wait or go on. My requirement is a bit stringent, I want that process should wake up at the same instant as signal is received."
    },
    {
        "link": "https://wiki.luckfox.com/Core3566/Linux-Systems-Programming/4.Inter-process-communication",
        "document": "\n• None In the Linux system, signals are an asynchronous notification mechanism used for passing events and information between processes or between the operating system and processes. Processes can respond to external events, such as user input, hardware exceptions, or actions from other processes, through signals.\n• None User processes have three ways to respond to signals:\n• Ignore the signal: The process takes no action when the signal is received. However, there are two signals that cannot be ignored, namely SIGKILL and SIGSTOP. SIGKILL is used to immediately terminate the execution of a process, while SIGSTOP is used to pause a process's execution\n• Catch the signal: Define a signal handler function to execute a custom action when the signal is received. Applications can use system calls like signal() or sigaction() to register signal handler functions and respond to specific signals\n• Perform the default action: Linux defines default actions for each type of signal. For example, when a process receives the SIGTERM signal, Linux defaults to terminating the execution of that process. Applications can choose not to define a signal handler function and let the system perform the default action for specific signals\n• None The process from signal generation to handling includes the following steps:\n• Signal generation: Signals can be triggered by various events, such as hardware interrupts, software exceptions, or user key presses. When an event occurs, the Linux kernel automatically generates the corresponding signal and sends it to the target process\n• Signal delivery: The process generating the signal sends it to a specific target process, usually using system calls like kill() or sigqueue(). The sending process needs to know the target process's Process ID (PID) and specify the signal type and other parameters\n• Signal reception: When the target process receives the signal, the operating system checks how this process handles the signal. If the signal is blocked or ignored by the process, the operating system stores the signal in the process's signal queue, waiting for the target process to unblock or stop ignoring the signal before processing it\n• Signal handling: If the target process has not handled the signal specifically or the signal is not blocked or ignored, the operating system calls the target process's signal handler function to process the signal. Each process can set its own signal handler function, and when the process receives a signal, the operating system automatically calls the corresponding signal handler function\n• Signal handling options: Each process can set its own signal handling options, including the signal handler function, the way to block signals, and the way to ignore signals. Some signals cannot be blocked or ignored, such as SIGKILL and SIGSTOP\n• Signal priorities: Signals in Linux have priorities, where signals with lower numbers have higher priorities. For example, SIGKILL has a priority of 9, while SIGINT has a priority of 2. When a process receives multiple signals simultaneously, the operating system prioritizes signals based on their priorities to decide which signal to handle first\n• Default signal handling: For each signal type, the Linux kernel defines a default handling action. For example, for the SIGINT signal (usually generated by the user pressing Ctrl+C on the terminal), the default action is to terminate the target process. However, processes can set their own signal handling options, including defining signal handler functions and blocking or ignoring signals, by calling functions like sigaction\n• None If you want to view the signals already defined in Linux, enter the following in the terminal:\n• None Commonly used signals in the system: Write on a pipe with no reader Software termination signal, can be caught by the process I/O is available for asynchronous system calls (Ctrl-C)\n\nFunctions for sending signals mainly include , , , and .\n• None\n• The parameter specifies the process ID of the recipient of the signal\n• The parameter specifies the type of signal to be sent\n• None The function allows a process to send a specified signal to itself. Function declaration:\n• The parameter specifies the type of signal to be sent.\n• The program prints \"raise before,\" uses the function to send a stop signal, and then the process terminates.\n• None The function can send a SIGALRM signal to the current process after a specified time. Function declaration:\n• The parameter specifies the number of seconds for the timer. The function will send a SIGALRM signal to the current process after the specified seconds.\n• None \"Waiting for alarm to go off...\n\n\"\n• None alarm to go off\n• In the above example program, we first registered the signal handling function , and then called to set a 5-second timer. During the waiting period for the timer to trigger, we use the function to block the process and wait for the signal to be triggered. When the timer is triggered, it automatically sends the SIGALRM signal to the process, triggering the signal handling function, and the program prints the received signal number before exiting.\n• To receive signals, the process that receives the signal must not stop. Typically, while loops, sleep, and pause are used.\n• None In Linux, the system call is used to set a new signal handler for a signal, which can be set as a user-specified function. The function declaration is as follows:\n• : The signal we want to handle. You can check the system's signals by typing in the terminal.\n• : The method of handling the signal (system default, ignore, or capture).\n• None Customizing the handling of the SIGINT signal:\n• When Ctrl+C is pressed, the function is executed instead of exiting the program.\n\nIn the Linux system, pipes are used to establish communication between reading and writing processes by sharing a file, hence they are also known as pipe files.there are two types of pipes: anonymous pipes and named pipes.\n• None Anonymous Pipes: An anonymous pipe is a simple half-duplex pipe that can only be used for communication between processes with a parent-child relationship. Once created, the anonymous pipe becomes shared file descriptors between the two processes, where one process writes data to the pipe and the other process reads data from it. The lifetime of an anonymous pipe is associated with the processes that created it, and when the creating process terminates, the anonymous pipe is destroyed.\n• None Named Pipes: Named pipes, also known as FIFO (First In, First Out), are a special type of file that allows any process to access it using its filename at any time. It provides a communication mechanism between processes that are not necessarily related and can allow multiple processes to access it simultaneously, making it suitable for interprocess communication over networks. Named pipes persist in the file system until they are deleted or the system is shut down. Any process with permission to access the named pipe can write to and read from it.\n\nIn summary, anonymous pipes are suitable for communication between related processes, while named pipes are suitable for communication between unrelated processes and have persistence.\n• None\n• Pipes use half-duplex communication, allowing data transmission in only one direction at a time\n• Pipes are actually fixed-size buffers, and if a process writes to a full pipe, the system will block that process until there is space in the pipe to receive the data\n• The pipe communication mechanism must provide synchronization between the reading and writing processes\n• None To create an anonymous pipe, the function is used with the following function declaration:\n• The function parameter is an array containing two elements, used to return two file descriptors; represents the read end, and represents the write end of the pipe. After calling the function, the two processes can communicate using these two file descriptors to transfer data.\n• None Example of parent-child processes communicating through a pipe:\n• None Step 1: The parent process calls the function to create a pipe and obtains two file descriptors, and , representing the read and write ends of the pipe\n• None Step 2: The parent process uses the function to create a child process, which also has two file descriptors pointing to the same pipe\n• None Step 3: The parent process closes the read end of the pipe, and the child process closes the write end. The parent process can write data to the pipe, and the child process can read data from it\n• None To create a named pipe, the function is used with the following function declaration:\n• The parameter specifies the path of the named pipe, and the parameter specifies the permission mask. Calling function will create a named pipe at the specified path, and the return value of 0 indicates successful creation, while -1 indicates failure.\n• To read and write from a named pipe, you can use ordinary and functions, or you can use the function to open the pipe file and then use and functions for communication. After using the named pipe, it is necessary to use the function to delete the pipe file.\n• None In Linux, the function from C language can be used to check if a process has the permission to access a file or directory. The function declaration is as follows:\n• The parameter specifies the path of the file or directory to be checked, and the parameter specifies the access permissions to be checked. Common access permissions include:\n• : Check if the file is readable\n• : Check if the file is writable\n• : Check if the file is executable\n• The function returns 0 if the check is successful and -1 if it fails. If function returns -1, the variable can be used to obtain the error code, which usually includes:\n• None Example of communication using named pipes:\n• None Compile the programs and run them:\n\nIn the Linux system, a message queue is an inter-process communication (IPC) mechanism used for data transmission between different processes. It is a first-in-first-out (FIFO) data structure that allows one or multiple processes to communicate by sending and receiving messages in the message queue. The message queue is identified by a message queue identifier (mqid), similar to a file descriptor, which ensures the uniqueness of the message queue. When creating a message queue, a unique key needs to be specified. This key is used to identify the message queue within the system, ensuring that multiple processes can access the same message queue using the same key. Message queues enable data sharing between different processes, making them a powerful inter-process communication mechanism.\n\nUsing message queues for inter-process communication offers the following advantages:\n• It supports a many-to-many communication model, where multiple processes can simultaneously read and write to the same message queue.\n• Messages in the message queue can persist in the system, even if the sending process has exited. The receiving process can still read these messages.\n• Message queues have a certain fault tolerance. If the receiving process is temporarily unable to process a message, the message can be retained in the queue and processed later when the receiving process is ready.\n\nIn summary, message queues are a highly practical inter-process communication mechanism widely used in various applications, including network communication, multi-threaded programming, distributed systems, and more.In the Linux system, message queue operations primarily rely on the following functions: , , , and .\n\nThe function is used to create or retrieve a message queue. Its function prototype is as follows:\n• None The parameter is the key value of the message queue, used to uniquely identify a message queue\n• None The parameter is the flag used to specify the attributes of the message queue\n• None In the Linux system, several macros can be used to generate the parameter value for the function. Commonly used macros include:\n• None : Represents creating a new private message queue accessible only to the current process. This flag is generally used when sharing data between parent and child processes without sharing among different processes.\n• None : Generates a unique key value based on a given file path and an integer. When using , you need to pass an accessible file path and a user-defined integer as parameters, for example:\n• None This code will generate a key value for the function based on the path name and the character 'a'.\n• None : Used to create a new message queue. If the specified message queue does not exist, a new one will be created; otherwise, the function returns the identifier of the existing message queue.\n• None : Specifies that if both and flags are set, only create a new message queue if it does not already exist; otherwise, return an error.\n• None The parameter is used to specify the attributes of the message queue and can be set using multiple flags, including:\n• : If the specified message queue does not exist, create a new one; otherwise, return the identifier of the existing message queue.\n• : If both and flags are set, only create a new message queue if it does not already exist; otherwise, return an error.\n• : Represents creating a new private message queue accessible only to the current process. This flag is generally used when sharing data between parent and child processes without sharing among different processes.\n• : Represents the permission of the created message queue, using the same representation method as file permissions. Here, 6 represents read and write permissions, 4 represents read permissions, 2 represents write permissions, and 0 represents no permissions.\n\nThe function is used to send a message to the message queue. Its function prototype is as follows:\n• None The parameter is the identifier of the message queue (obtained from the function)\n• None The parameter is a pointer to the buffer where the message to be sent is temporarily stored. A generic structure can be used to represent the message:\n• None The parameter is the length (in bytes) of the message to be sent. The actual length of the message (excluding the message type field) can be calculated using the formula :\n• The total size of the structure is bytes\n• None The parameter is used to specify the behavior of sending the message and can take the following values:\n• : Represents the blocking mode, where the thread will be blocked until the message can be written\n• : Represents the non-blocking mode, where the function immediately returns with an error if the message queue is full or other conditions prevent sending the message\n• None If the function executes successfully, it returns ; otherwise, it returns .\n\nThe function is used to receive a message from the message queue. Its function prototype is as follows:\n• None The parameter is the identifier of the message queue\n• None The parameter is a pointer to the buffer where the received message will be stored. A generic structure can be used to represent the message:\n• None The parameter is the length of the received message\n• None The parameter specifies the type of the message to receive\n• None The parameter is used to specify the behavior of receiving the message and can take the following values:\n• : Represents the blocking mode, where the function waits until the message queue is not empty\n• : Represents the non-blocking mode, where the function immediately returns with an error if the message queue is empty\n• None If the function executes successfully, it returns the number of bytes received into the array.\n\nThe function is used to control the state of the message queue. Its function prototype is as follows:\n• The parameter is the identifier of the message queue.\n• The parameter specifies the operation to be executed, including deleting the message queue, querying the status of the message queue, etc. It can take the following values:\n• : Read the attributes of the message queue and save them in the buffer pointed to by\n• : Set the attributes of the message queue based on the values from the parameter\n• : Delete the queue from the system kernel\n• The parameter is a pointer to the structure used to save the status information of the message queue.\n\nBelow is a simple example of a message queue program, including sender and receiver processes:\n\nSystem V Semaphores are a mechanism for inter-process synchronization and mutual exclusion. They use a counter-based approach to control access to shared resources, allowing multiple processes to access the resource simultaneously while limiting the number of concurrent accesses. This mechanism ensures that each process can smoothly acquire control of the resource, achieving synchronization and mutual exclusion between multiple processes.\n\nIn System V IPC (Inter-Process Communication), use the function to create or get a semaphore set. The function declaration is as follows:\n• None\n• : The key value that identifies the semaphore set. You can use the function to generate a unique key value\n• : Specifies the number of semaphores in the semaphore set to be created or obtained\n• : Specifies the operation flags, which can be one or a combination of several flags\n• None Function return value: The function returns a non-negative integer if successful, representing the semaphore set identifier (also known as the semaphore set descriptor). If the function fails, it returns -1.\n\nIn System V IPC, use the function to perform P/V operations (increment and decrement operations) on semaphores. The function declaration is as follows:\n• None\n• : The semaphore set identifier (returned by the function)\n• : A pointer to an array of structures, with each structure describing a P/V operation\n• : Specifies the number of structures in the array\n• None Function return value: The function returns 0 if successful, and -1 if it fails\n• None The structure is defined as follows:\n• None There are two basic ways to operate on semaphores:\n• P operation, also known as the wait operation or decrement operation, is used to acquire control of the semaphore and decrement its value. Specifically, when a process needs to access a shared resource, it calls the P operation to wait for the semaphore. If the semaphore's value is greater than or equal to 1, the process will decrement the semaphore's value and continue execution. Otherwise, the process will be blocked and wait until the semaphore's value becomes greater than or equal to 1\n• V operation, also known as the signal operation or increment operation, is used to release control of the semaphore and increment its value. Specifically, when a process finishes accessing a shared resource, it calls the V operation to release the semaphore. This allows other processes waiting for the semaphore to regain control and continue execution\n• PV operations are the most basic and commonly used operations in System V Semaphores. They can be used to implement various synchronization and mutual exclusion mechanisms, such as mutexes and read-write locks. Due to the atomic nature of PV operations, they ensure that access to shared resources does not lead to race conditions and other issues, thus ensuring synchronization and mutual exclusion between multiple processes\n\nIn System V IPC, use the function to control semaphore set attributes. The function declaration is as follows:\n• None\n• None : The semaphore set identifier (returned by the function)\n• None : The index of the semaphore in the semaphore set. For most commands, this parameter is ignored and can be set to 0\n• None : Specifies the operation command to be executed, which can be one of the following commands:\n• : Get the current value of the semaphore\n• : Set the value of the semaphore\n• : Set the values of all semaphores in the set\n• None Function return value: The function returns a non-negative integer if successful and -1 if it fails.\n• None When using the function, depending on the command, additional parameters may be required. For example, for the command, you need to provide a structure as the fourth parameter, which contains the value to be set. The structure is defined as follows:\n\nUsing semaphores to control the execution order between two processes (parent and child):\n\nShared memory refers to a mechanism where multiple processes share the same physical memory. This mechanism allows multiple processes to access the same memory area, enabling data sharing between processes.\n\nUnder the shared memory mechanism, multiple processes can map the same memory region to their address space to achieve data sharing. This allows multiple processes to read and write the same memory, achieving communication and synchronization between them without the need for any data copying or inter-process communication operations. Therefore, shared memory is an efficient inter-process communication mechanism.\n\nIn Linux, use the function to create shared memory. The function declaration is as follows:\n• None In the function, specifies the key value for identifying the shared memory, specifies the size of the shared memory, and specifies the creation flags, which can be one or a combination of the following constants:\n• : If the shared memory does not exist, create it; otherwise, return the identifier of the existing shared memory\n• : Used together with , return an error if the shared memory already exists\n• : Use a random value as the key to create shared memory\n• None Function return value: The function returns an integer indicating the identifier of the shared memory segment. If the creation fails, the function returns -1 and sets the variable to indicate the error type\n\nBelow is an example code that demonstrates how to use the function to create shared memory:\n\nThe function is used in Linux to map shared memory to the process's address space. The function declaration is as follows:\n• In the function, is the identifier of the shared memory, which is returned by the function\n• The parameter is used to specify the starting address of the mapping. If the value of is 0, the operating system will choose a suitable address for mapping, usually written as\n• The parameter is used to specify the mapping flags, and is typically used to indicate default options for operating on shared memory:\n• : Remap the shared memory into the process's address space\n• : Allow the shared memory content to be executable. Generally, the content of shared memory is not allowed to be executed\n• : When the function is called to detach shared memory, it will be destroyed\n• Function return value: The function returns a pointer that points to the starting address of the shared memory mapping. If the mapping fails, the function returns -1 and sets the variable to indicate the error type.\n\nThe function is used in Linux to detach shared memory from the process's address space. The function declaration is as follows:\n• The parameter is the starting address of the shared memory mapping, usually obtained from the pointer returned by the function\n• Function return value: The function returns 0 on success and -1 on failure\n• Note: The function detaches the shared memory from the process's address space but does not delete the shared memory region. It just makes the current process unable to access the shared memory.\n\nThe function in Linux is used to control shared memory. It can be used to get or modify shared memory attributes. The function declaration is as follows:\n• None The parameter is the identifier of the shared memory, which is returned by the function. The parameter specifies the operation command to be executed. It can be one of the following commonly used commands:\n• : Get the status information of the shared memory and store the result in the structure pointed to by\n• : Set the status information of the shared memory using the data in the structure\n• : Remove the shared memory and release its resources\n• None The parameter is a pointer to a structure used to store or pass the status information of the shared memory\n• None Function return value: The function returns 0 on success and -1 on failure."
    }
]