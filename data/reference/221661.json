[
    {
        "link": "https://learn.microsoft.com/en-us/sql/relational-databases/databases/create-a-database?view=sql-server-ver16",
        "document": "This article describes how to create a database in SQL Server by using SQL Server Management Studio or Transact-SQL.\n\nTo create a database in Azure SQL Database using T-SQL, see CREATE DATABASE.\n\nA maximum of 32,767 databases can be specified on an instance of SQL Server.\n\nThe statement must run in autocommit mode (the default transaction management mode) and isn't allowed in an explicit or implicit transaction.\n\nThe master database should be backed up whenever a user database is created, modified, or dropped.\n\nWhen you create a database, make the data files as large as possible based on the maximum amount of data you expect in the database.\n\nRequires permission in the database, or requires , or permission.\n\nTo maintain control over disk use on an instance of SQL Server, permission to create databases is typically limited to a few SQL Server logins.\n• None In Object Explorer, connect to an instance of the SQL Server Database Engine and then expand that instance.\n• None Right-click Databases, and then select New Database.\n• None In New Database, enter a database name.\n• None To create the database by accepting all default values, select OK; otherwise, continue with the following optional steps.\n• None To change the owner name, select (...) to select another owner. The Use full-text indexing option is always checked and dimmed because all user databases are full-text enabled.\n• None To change the default values of the primary data and transaction log files, in the Database files grid, select the appropriate cell and enter the new value. For more information, see Add Data or Log Files to a Database.\n• None To change the collation of the database, select the Options page, and then select a collation from the list.\n• None To change the recovery model, select the Options page and select a recovery model from the list.\n• None To change database options, select the Options page, and then modify the database options. For a description of each option, see ALTER DATABASE SET options.\n• None To add a new filegroup, select the Filegroups page. Select Add and then enter the values for the filegroup.\n• None To add an extended property to the database, select the Extended Properties page.\n• None In the Name column, enter a name for the extended property.\n• None In the Value column, enter the extended property text. For example, enter one or more statements that describe the database.\n• None To create the database, select OK.\n• None From the Standard bar, select New Query.\n• None Copy and paste the following example into the query window and select Execute. This example creates the database . Because the keyword isn't used, the first file ( ) becomes the primary file. Because or aren't specified in the parameter for the file, it uses and is allocated in megabytes. The file is allocated in megabytes because the suffix is explicitly stated in the parameter.\n\nFor more examples, see CREATE DATABASE."
    },
    {
        "link": "https://virtual-dba.com/blog/best-practices-database-creation-sql-server",
        "document": "Creating a new database in SQL server is a very common and general task for professionals dealing with SQL in an environment. There could be different ways of creating a new database. You can literally run a basic create DB statement, which will follow the default settings of your instance, or you can pinpoint every single thing related to the creation of the database (like number of data and log files, their locations, growth rate, max growth size, filegroup etc.) After creating a new database, it is very important to have it configured in such a way that it meets the need of the user with performance, security, availability, and recoverability. I have described the points below that definitely need to be considered while creating a new database.\n• Create the needed database with default data and log file location as per database settings\n\nOr Create the needed database with specifying the location of data and log files with specific file growth\n\nNote: This is always a best practice creating a database with data and log files in a different drive for better performance in the long run.\n\nChange the owner of the database to ‘sa’/service account\n• For better performance and Recoverability you also need to make sure to add the databases in your maintenance plans. This should include database backup, index rebuild/reorg, update stats, and database consistency check.\n• Depending on the criticality of the data in the newly created database, you must consider if you need to take advantage of Transparent Data Encryption(TDE), which can protect your data, log and backup files from being stolen and restored without the key. Note: Special care should be given while configuring TDE. If you lose your encryption key and you don’t have a backup of the key you won’t be able to restore the database in the result of a disaster.\n• You also need to check if this database needs to be a part of any high availability solution. Check for AlwaysOn, mirroring, log shipping, or replication being used in your environment and if this database needs to be included on them.\n• A lot of organizations use third-party tools these days to monitor the availability and performance of the databases. If that is the case for your environment, make sure to include it in your monitoring tool and configure the different settings for how you want it to be monitored."
    },
    {
        "link": "https://learn.microsoft.com/en-us/sql/relational-databases/security/sql-server-security-best-practices?view=sql-server-ver16",
        "document": "This article provides information about best practices and guidelines that help establish security for SQL Server. For a comprehensive review of SQL Server security features, see Securing SQL Server.\n\nFor specific product security best practices, see Azure SQL Database and SQL Managed Instance and SQL Server on Azure VMs.\n\nA layered security methodology provides a defense-in-depth solution by using multiple security capabilities targeted at different security scopes. The security features made available in SQL Server 2016, and improved in subsequent releases, help counter security threats and provide well-secured database applications.\n\nAzure complies with several industry regulations and standards that can enable you to build a compliant solution with SQL Server running in a virtual machine. For information about regulatory compliance with Azure, see Azure Trust Center.\n\nOrganizations often need to protect data at the column level as data regarding customers, employees, trade secrets, product data, healthcare, financial, and other sensitive data is often stored in SQL Server databases. Sensitive columns often include identification/social security numbers, mobile phone numbers, first name, family name, financial account identification, and any other data that could be deemed personal data.\n\nThe methods and features mentioned in this section raise the level of protection at the column level with minimal overhead, and without requiring extensive changes to application code.\n\nUse Always Encrypted to encrypt data at rest and over the wire. Encrypted data is only decrypted by client libraries at the application client level. Use randomized encryption over deterministic where possible. Always Encrypted with secure enclaves can improve performance for comparison operations such as BETWEEN, IN, LIKE, DISTINCT, Joins, and more for randomized encryption scenarios.\n\nUse Dynamic Data Masking (DDM) to obfuscate data at the column level when Always Encrypted isn't an available option. Dynamic Data Masking (DDM) isn't compatible with Always Encrypted. Use Always Encrypted over dynamic data masking whenever possible.\n\nYou can also GRANT permissions at the column level to a table, view, or table-valued function. Consider the following: - Only , , and permissions can be granted on a column. - A table-level doesn't take precedence over a column-level .\n\nRow-Level Security (RLS) enables the ability to use user execution context in order to control access to rows in a database table. RLS ensures that users can only see the record that pertains to them. This gives your application 'record level' security without having to make significant changes to your application.\n\nThe business logic is encapsulated within table-valued functions controlled by a security policy that toggles the RLS functionality on and off. The security policy also controls the and predicates that are bound to the tables RLS operates against. Use Row-Level Security (RLS) to limit the records that are returned to the user making the call. Use SESSION_CONTEXT (T-SQL) for users who connect to the database through a middle-tier application where application users share the same SQL Server user account. For optimal performance and manageability, follow the Row-Level Security best practices.\n\nTransparent Data Encryption (TDE) protects the data at the file level by providing encryption-at-rest to the database files. Transparent Data Encryption (TDE) ensures that database files, backup files, and files can't be attached and read without proper certificates decrypting database files. Without Transparent Data Encryption (TDE), it's possible for an attacker to take the physical media (drives or backup tapes) and restore or attach the database to read the contents. Transparent Data Encryption (TDE) is supported to work with all other security capabilities in SQL Server. Transparent Data Encryption (TDE) provides real-time I/O encryption and decryption of the data and log files. TDE encryption uses a database encryption key (DEK) is stored in the user database. The database encryption key can also be protected using a certificate, which is protected by the database master key of the database.\n\nUse TDE to protect data at rest, backups, and .\n\nTo audit SQL Server, create an audit policy at either the server or database level. Server policies apply to all existing and newly created databases on the server. For simplicity, enable server-level auditing and allow the database-level auditing to inherit the server-level property for all databases.\n\nAudit tables and columns with sensitive data that have security measures applied to them. If a table or column is important enough to need protection by a security capability, then it should be considered important enough to audit. It's especially important to audit and regularly review tables that contain sensitive information but where it's not possible to apply desired security measures due to some kind of application or architectural limitation.\n\nSQL Server supports two authentication modes, Windows authentication mode and 'SQL Server and Windows Authentication mode' (mixed mode).\n\nLogins are separate from database users. First, map logins or Windows groups to database users or roles separately. Next, grant permissions to users, server roles, and/or database roles to access database objects.\n\nSQL Server supports the following types of logins:\n• A local Windows user account or Active Directory domain account - SQL Server relies on Windows to authenticate the Windows user accounts.\n• Windows group - Granting access to a Windows group grants access to all Windows user logins that are members of the group. Removing a user from a group removes the rights from the user that came from the group. Group membership is the preferred strategy.\n• SQL Server login - SQL Server stores the username and a hash of the password in the database.\n• Contained database users authenticate SQL Server connections at the database level. A contained database is a database that is isolated from other databases and from the instance of SQL Server (and the database) that hosts the database. SQL Server supports contained database users for both Windows and SQL Server authentication.\n\nThe following recommendations and best practices help secure your identities and authentication methods:\n• \n• It's standard to place Active Directory users in AD groups, AD groups should exist in SQL Server roles, and SQL Server roles should be granted the minimum permissions required by the application.\n• None In Azure, use least-privilege security by using role-based access (RBAC) controls\n• None Choose Active Directory over SQL Server authentication whenever possible, and especially choose Active Directory over storing the security at the application or database level.\n• If a user leaves the company, it's easy to disable the account.\n• It's also easy to remove users from groups when users change roles or leave the organization. Group security is considered a best practice.\n• None Use multifactor authentication for accounts that have machine-level access, including accounts that use RDP to log into the machine. This helps guard against credential theft or leaks, as single-factor password-based authentication is a weaker form of authentication with credentials at risk of being compromised or mistakenly given away.\n• None Require strong and complex passwords that can't be easily guessed, and aren't used for any other accounts or purposes. Regularly update passwords and enforce Active Directory policies.\n• None Group-Managed Service Accounts (gMSA) provide automatic password management, simplified service principal name (SPN) management and delegate the management to other administrators.\n• With gMSA, the Windows operating system manages passwords for the account instead of relying on the administrator to manage the password.\n• gMSA reduces the administrative surface-level and improves the separation of duties.\n• None Minimize the rights granted to the AD account of the DBA; Consider a separation of duties that limit access to the virtual machine, the ability to log into the operating system, the ability to modify error and auditing logs, and the ability to install applications and/or features.\n• None Consider removing DBA accounts from the sysadmin role and granting CONTROL SERVER to DBA accounts rather than making them a member of the sysadmin role. The system admin role doesn't respect while CONTROL SERVER does.\n\nKeeping historical records of data changes over time can be beneficial to address accidental changes to the data. It can also be useful for application-change auditing and can recover data elements when a bad actor introduces data changes that weren't authorized.\n• Use temporal tables to preserve record versions over time, and to see data as it was over the record's life span to provide a historical view of your application's data.\n• Temporal Tables can be used to supply a version of the current table at any point in time.\n\nThe following configuration and assessment tools address surface-area security, identify data security opportunities, and provide a best practice assessment of the security of your SQL Server environment at the instance level.\n• Surface area configuration - You should enable only the features required by your environment, to minimize the number of features that can be attacked by a malicious user.\n• Vulnerability assessment for SQL Server (SSMS) - SQL vulnerability assessment is a tool in SSMS v17.4+ that helps discover, track, and remediate potential database vulnerabilities. The vulnerability assessment is a valuable tool to improve your database security and is executed at the database level, per database.\n• SQL Data Discovery and Classification (SSMS) - It's common for DBAs to manage servers and databases and not be aware of sensitivity of the data that is contained in the database. Data Discovery & Classification adds the capability to discover, classify, label, and report on the sensitivity level of your data. Data Discovery & Classification is supported starting with SSMS 17.5.\n\nIt helps to know what are some common threats that risk SQL Server:\n• SQL injection - SQL injection is a type of attack where malicious code is inserted into strings that are passed to an instance of SQL Server for execution.\n• The injection process works by terminating a text string and appending a new command. Because the inserted command might have more strings appended to it before it executes, the attacker terminates the injected string with a comment mark .\n• SQL Server executes any syntactically valid query that is received.\n• Be aware of Side-channel attacks, malware & other threats.\n\nTo minimize the risk of a SQL injection, consider the following items:\n• Review any SQL process that constructs SQL statements for injection vulnerabilities.\n• Developers and security admins should review all code that calls , , or .\n• Disallow the following input characters:\n• : Catalog-extended stored procedures, such as .\n• It isn't recommended to use on any SQL Server environment. Use SQLCLR instead, or look for other alternatives due to the risks can introduce.\n• Always validate user inputs and scrub error outputs from being spilled and exposed to the attacker.\n\nTo minimize the risk of a side-channel attack, consider the following:\n• Ensure the latest application and operating system patches are applied.\n• For hybrid workloads, ensure the latest firmware patches are applied for any hardware on-premises.\n• In Azure, for highly sensitive applications and workloads, you can add additional protection against side-channel attacks with isolated virtual machines, dedicated hosts, or by using Confidential Compute virtual machines such as the DC-series and Virtual Machines that use 3rd Gen AMD EPYC processors.\n\nConsider the following common infrastructure threats:\n• Brute force access - the attacker attempts to authenticate with multiple passwords on different accounts until a correct password is found.\n• Password cracking / password spray - attackers try a single carefully crafted password against all of the known user accounts (one password to many accounts). If the initial password spray fails, they try again, utilizing a different carefully crafted password, normally waiting a set amount of time between attempts to avoid detection.\n• Ransomware attacks is a type of targeted attack where malware is used to encrypt data and files, preventing access to important content. The attackers then attempt to extort money from victims, usually in the form of cryptocurrencies, in exchange for the decryption key. Most ransomware infections start with email messages with attachments that try to install ransomware, or websites hosting exploit kits that attempt to use vulnerabilities in web browsers and other software to install ransomware.\n\nSince you don't want attackers to easily guess account names, or passwords, the following steps help reduce the risk of passwords being discovered:\n• Use complex strong passwords for all your accounts. For more information about how to create a strong password, see Create a strong password article.\n• By default, Azure selects Windows Authentication during SQL Server virtual machine setup. Therefore, the SA login is disabled and a password is assigned by setup. We recommend that the SA login shouldn't be used or enabled. If you must have a SQL login, use one of the following strategies:\n• None Create a SQL account with a unique name that has sysadmin membership. You can do this from the portal by enabling SQL Authentication during provisioning. If you don't enable SQL Authentication during provisioning, you must manually change the authentication mode to SQL Server and Windows Authentication Mode. For more information, see Change server authentication mode.\n• None If you must use the SA login, enable the login after provisioning and assign a new strong password.\n\nConsider the following to minimize ransomware risks:\n• The best strategy to guard against ransomware is to pay particular attention to RDP and SSH vulnerabilities. Additionally, consider the following recommendations:\n• Use firewalls and lock down ports\n• Ensure the latest operating system and application security updates are applied\n• Improve Surface Area Security by avoiding installing tools including sysinternals and SSMS on the local machine\n• Additionally, there should be a regular full backup scheduled that is separately secured from a common administrator account so it can't delete copies of the databases."
    },
    {
        "link": "https://blog.idera.com/database-tools/best-practices-for-writing-sql-server-scripts",
        "document": "When it comes to database management, SQL Server scripts are essential for automating repetitive tasks, querying data, and performing administrative functions. A well-written SQL script can boost performance, simplify maintenance, and reduce the likelihood of errors. As database administrators (DBAs) and developers, mastering SQL script writing is crucial to maintaining high-performing, secure, and reliable databases.\n\nThis blog will cover a few best practices for writing SQL Server scripts that are efficient, maintainable, and optimized for performance. Whether you’re a seasoned DBA or just getting started, these tips can help you write scripts that enhance the performance and security of your SQL Server environment.\n\nProper formatting in SQL scripts is critical for readability and maintainability. Writing SQL code that is easy to read helps teams collaborate more effectively and troubleshoot problems faster. Consistently use indentation to differentiate sections of your code, such as SELECT, FROM, and WHERE clauses. Use spaces between SQL keywords and expressions to improve clarity.\n\nReadable code reduces confusion, especially in complex queries, and makes your scripts easier to debug. For example, instead of writing everything in one line, break the code into logical blocks, allowing others to understand the purpose of each section quickly.\n\nComments are essential for explaining the logic behind your code. SQL scripts can quickly become complex, and adding comments ensures that other developers (or even yourself in the future) can understand the script’s intent. Use comments to explain sections that aren’t immediately obvious and to document assumptions or constraints.\n\nThis practice also helps when debugging issues or making updates in the future. While comments won’t impact performance, they contribute to long-term script maintainability, especially in team environments.\n\nUsing SELECT * in queries may seem convenient, but it can lead to performance issues and unnecessary data retrieval. Instead, explicitly list the columns you need in your SELECT statement. This minimizes the amount of data returned and reduces the workload on the SQL Server.\n\nBy avoiding SELECT *, you also protect your queries from breaking if the table structure changes. Including only the necessary columns leads to more efficient queries, reducing both network traffic and query execution time.\n\nWhen writing SQL scripts involving multiple tables, optimizing your JOIN statements is critical. Ensure you’re using appropriate indexes on the columns involved in JOIN conditions. Without proper indexing, SQL Server may need to perform full table scans, slowing down query performance.\n\nAlso, consider the order of JOIN operations and the size of the datasets involved. Large datasets should be filtered as much as possible before joining to avoid unnecessary resource consumption. Always test your query performance using execution plans to identify any bottlenecks.\n\nParameterized queries help protect your SQL Server from SQL injection attacks, which are one of the most common security threats. Instead of embedding raw user input directly into your SQL statements, use parameters to pass values. This practice not only enhances security but can also improve query execution performance by allowing SQL Server to reuse execution plans.\n\nBy making your queries parameterized, you also improve the scalability of your scripts, particularly in applications where queries are frequently executed with different input values.\n\nError handling is crucial in any SQL Server script to ensure that issues are detected and addressed without causing further problems. Use TRY…CATCH blocks to manage errors and ensure that your script responds appropriately when something goes wrong. This approach helps prevent partial data processing and ensures that any failed operations are rolled back to avoid inconsistencies.\n\nBy incorporating proper error handling, you improve the reliability of your SQL scripts, reducing downtime and potential data corruption.\n\nTest Your Scripts Thoroughly Before Deployment\n\nBefore deploying any SQL script in a production environment, it is essential to test it thoroughly in a development or staging environment. Test your script with various datasets and edge cases to ensure it behaves as expected under different conditions. This will help identify performance bottlenecks, potential errors, or unexpected behavior.\n\nRunning tests allows you to optimize your script further and ensure it meets both performance and functionality requirements before going live.\n\nWriting efficient SQL Server scripts is a core skill for DBAs and developers. By following these best practices, you can ensure your scripts are performant, maintainable, and secure, ultimately improving the overall health of your SQL Server environment.\n\nRelated products with the capability to write SQL Server scripts:"
    },
    {
        "link": "https://medium.com/@BrandonSouthern/sql-best-practices-e1c61e96ee27",
        "document": "In this post I’m going to share some best practices for formatting SQL that I’ve learned and used over the last 20 years. Many of these tips are things that I’ve collected along the way out of frustration with inheriting code, needing to perform updates, chasing down bugs, and performing code reviews for others. Also, when it comes to training new employees, I have found that it is much more difficult for people to understand the domain and code when dealing with poorly written SQL.\n\nIn each section of this article, I’ll try to focus in on one area of practice. I’ll provide you with an example of a bad practice and example(s) of best practices along with my reasons behind writing code a specific way. As you’re reading you might think, “Well that isn’t as performant of code” and sometimes this might be the case because I’m trying to hone in on a specific point and avoid excess potential for confusion. Also, sometimes I prefer to have slightly less performant code (depending on how often it is used, the purpose of the code, and the performance hit) if it makes the code easier to read and maintain.\n\nI’m guessing that some people reading this might say, “Well my code executes just fine so I don’t care. I understand what it does.” To that (because I’ve heard these comments many times over the years) I would challenge that assertion. It takes significantly more time to find and read sloppy “bad practice” code than it does when working with clean code. These practices will also help to reduce bugs and your code will be appreciated by everyone that reads it.\n\nWhen we’re talking about programming languages, you can pretty much write statements as you see fit and the code will execute. Obviously there are some exceptions to this (such as indentations in Python) but that’s for another day. What this means is that when I’m talking about SQL best practices, one could make the argument that the this is just a matter of opinion and writing style.\n\nHow you write it matters. Think about a book for example. Sure, you could write an entire book without paragraphs, line returns, standard spacing between sentences, and more. Yes, it would still be a book and yes, the reader could probably understand it. However, how easily would it be for you find a specific section if there weren’t paragraphs? How easy would it be to have confidence that you’re actually in the right section if you were asked to edit something? For these reasons any many more, I’m outlining some best practices.\n\nPlease note that while all of these practices could be considered “opinions”, they are practices that I’ve used after 20 years of making mistakes, challenges with efficiently reading, editing, and understanding code, and watching my team members face the same challenges. I’ll try my best to tell you about the practice and the logic behind using them. I’ll leave it up to you to consider these practices, see what works for you, and what works for your team.\n\nThe first thing that I’d like to talk about is formatting. Code should be well formatted and visually appealing, which makes it very easy to read. Having properly formatted code we pay off when it comes to debugging, troubleshooting, and modifying your code.\n\nWhen it comes to formatting, there are a number of things that should be considered such as intentions, alignment, comma positions, and text case. If you made it this far in the reading you’ve probably noticed how well aligned my code is and that most items are found on a single line. This make a tremendous difference in readability.\n\nBelow is an example of code that has been written and is quite unreadable. In this example you’ll notice a number of things go against best practices such as:\n• grouping by number instead of name\n\nIn this article I’ll discuss these practices and more to help you write clean and bug-free code that you can be proud of.\n\nLook at the code below and compare to the code above. Which one is easier to read? Which one provides the best context about what the intent of the code is and what some of the conditional values mean? Which version provides a cleaner UI that will allow you to quickly spot bugs or avoid bugs all together? I’m hoping that you’ve agreed that the code below serves to overcome all of the challenges found in the above code.\n\nThe counter argument that I’ve heard people say is, “Well, you had to write more lines of code.” This is irrelevant. The computer doesn’t care and it’s a couple of key strokes to have a line return and spaces or tabs.\n\nIn the following sections I will discuss each of these issues and provide examples of the good and bad practices.\n\nIf you look at the above “best practice” code, notice how well everything is aligned to the left. All commas, spaces, and indentations make the code very easy to read.\n\nMy general rule is one item per line. This could be one element in your select statement or one condition in a join statement or one case statement. Again, look at the above code on formatting to see the difference of readability when writing with one item per line. The key here is to be consistent. I’ve seen code that is written with one item per line but then every so often there will be a *join* clause that has an and and an or statement that are on the same line. This can be immensely frustrating when reading and debugging because it is very easy to overlook the addition condition because it was written to the same line.\n\nHere we see a case statement that is all in one line. This is a bad practice because it make the code hard to read and quickly pick up on all of the conditions that are being evaluated. Also, it is really challenging if not impossible to properly comment the code. I know that in the example, ‘main_reporting’ isn’t descriptive and doesn’t appear consistent with the other values but hey, sometimes you’re told to output values this way and can’t logic to others.\n\nHere we see a case statement that is written on multiple lines with comments to help provide clarity.\n\nPlease comment your code. I’ll write another post about code comments in the future but comments are important. I feel like I see a post on LinkedIn or some other site on a daily basis where someone says something along the lines of, “You don’t need code comments. The code is a comment. What’s the matter? Don’t you know how to read code?” Seriously, I’ve heard this sort of thing for years. But here’s the reality. While code is a language and if proficient in the language, a reader can understand <strong>what</strong> the code is doing. But the code <strong>never</strong> tells the reader <strong>why</strong> someone wanted to code to function that way. The possibilities are endless as to why someone wanted to code to work a certain way. Sometimes you could be coding around a bug in the back-end data or maybe there is business logic that dictates how the code should function.\n\nWhile it’s true that you could read the code and possibly look up the documentation on certain tables, it’s that a lot more work than typing a few characters. Below are some examples of good and bad commenting practices.\n\nLook at the code below. We can see that the code only wants to return results where u.id > 1000. That’s pretty obvious in this very simple example. But the more important question is why did someone do this?\n\nMaybe they are test users prior to u.id = 1000. Or maybe the code is filtering out all users that are from Michigan, because for some reason someone thought that all users less than u.id 1000 are from the state of Michigan. That might sound like a horrible idea to actually have code that would be written that way but it executes all the same. The point here is that as new users, we don’t know and chances are that in six months you probably won’t know either.\n\nHere we have an in-line comment that tells us a bit more about why we added the u.id > 1000 condition. We evidently have test users that should be scrubbed out of the result set.\n\nLook at the code below. We can see here that the query is going to return users that are considered to be non-test users. The in-line comment helps us to understand that the desire is to scrub out these test users from the result set. But we had to read a few lines of code. You may be saying, well, it isn’t worth a comment block at the top. It’s just 8 lines of code and it’s obvious what is happening. Simple or not, the reader doesn’t know the <strong>why</strong> behind this code. But what if the code wasn’t as simple? You’d surely appreciate some comments. For these reasons and to have better planning before you start writing code (more on that later),\n\nBelow is the same code that we just looked at by now we are telling the user why we\n\nwant to run this code and things to look out for.\n\nCommon table expressions or CTEs are a way of creating an in-memory table of your query results. This table can then be used throughout the rest of your SQL script. The benefit to using a CTE is that you can reduce code duplication, make your code more readable, and increase your ability to perform QA checks on your results.\n\nAlso note the really good block comment.\n\nExample of Code Not Using CTE’s\n\nIn the code below we can see that there are two sub queries that are returning results. These two subqueries are then joined together to produce the final result set. While this code will execute, there are a few concerns:\n\n1. It’s really hard to perform a QA on the sub queries and inspect the results. For example, what if we wanted to run a some counts on the number of users that have multiple records for default screens? We can’t easily just execute some sql against the sub query. We’d have to copy/paste the sub query and then modify it to perform this qa. It would be much better if we could avoid changing code during our QA process.\n\n2. If we need to utilize this users sub-query elsewhere in our code, we’d have to re-write or copy/paste that block of code to other places in our script. This would not be a DRY (don’t repeat yourself)\n\nprocess and exposes more potential to bugs. How so? Assume for a moment that you’ve used the users subquery in 5 locations in your script. Also assume that the code that you are working with is not easy to read because it doesn’t follow best practices. If you are asked to update the code to add another condition to scrub out additional test users, there’s a good chance that you could miss adding this condition\n\nto at least one of the 5 uses of the subquery.\n\n3. More cycles on the database. Each time that the subquery is execute it performs table scans to return results. With our users subquery containing wildcard conditions, the database is going to have a fair amount of work to do. It’s much cheaper (CPU cycles and dollars if you’re using cloud databases) to perform the subquery once, store it in memory and then just re-use the result set as needed in your code.\n\n4. More complex to read the entire block of code and understand what is being performed and why. While it is possible to scroll through the code, it may be hard to easily comprehend what is happening. Generally speaking, if you have to vertically scroll your code on your monitor, your code is too way too long and should be refactored to smaller components.\n\nExample of Using CTEs\n\nBelow we see an example of using CTEs. While CTE’s can be great and help to overcome some of the challenges that we previously pointed out, CTEs typically don’t stay in memory after the final result set has been displayed.\n\nFor example, if you were to run this entire block of code, it would execute. But then if you wanted to select all of the results from the users CTE a few minutes later, this data wouldn’t be available to query. To get around this issue you can use volatile tables or temporary in-memory tables that typically live as long as your session (database connection) remains active. More on this topic later.\n\nYou should never write queries with “select *”. I think the only exception to this rule is if you are trying to inspect a table and in such a case, you should always limit the number of results that are returned. Writing queries this way is a bad idea of many reasons:\n\n1. Database performance. Returning columns that aren’t needed is more expensive than querying only the columns that you care about.\n\n2. Challenges debugging. Assuming that you’re using CTEs as described in the previous section, it can be very challenging to trace the origins of certain attributes.\n\n3. Tables change. Even if you actually need to select all columns, there’s no guarantee that your table won’t change over time. And as the table changes you’ll be querying new data that was never intended which could possibly break code elsewhere, cause confusion, or impact database performance and costs.\n\nAliasing is very important to help readers understand where elements reside and what tables are being used. When aliases aren’t used or poor naming conventions are used, complexity is increased, and the reading/comprehension of code is reduced.\n\nBad Practice — No Alias Used on Fields\n\nBelow you can see that the tables have an alias of ‘u’ and ‘p’ but the selected elements don’t utilize the alias. This can be very frustrating and can cause run-time errors if more than one of the tables contains a field with the same name. Eg. user_id is found in both the ‘users’ table and the ‘preferences’ table.\n\nBest Practice — Alias Used on Fields\n\nBelow you can see that the tables and selected elements utilize the table alias name. This makes the code very readable for the end user. Even if you only have a single table it is a good practice to use an alias on the table and field name. Good habits make for good code.\n\nBelow we have a CTE that has been created but the table name that has been assigned is called ‘cte’. This is a very generic name and tells the end-user absolutely nothing about the data in the table. If you were a user reading the select statement that follows the CTE, you wouldn’t have any indication of what table was being used.\n\nBelow we have a CTE that has been created with a more descriptive name. The name give the user some indication of what data is contained within the able.\n\nIn select statements I prefer to have a leading comma as opposed to a trailing comma and this is one of those cases where I would say my personal opinion comes into play. I’ve seen a lot of people write their selected elements with trailing comma’s and in other languages it is common practice to have a trailing comma. However, with other languages, it isn’t common to have a large number of arguments past into a function whereas in SQL it’s quite common to have a large number of elements being selected (and declared in the code). While you may think that I’m being overly opinionated about this usage, here’s a couple of reasons why I find leading comma’s to be beneficial.\n\n1. Clean looking UI. When you look at the best practice example, look at how nicely the commas are aligned. It is very easy to see that a comma is missing and avoid a run-time error when compared to using trailing commas\n\n2. No confusion when working with longer case statements that wrap lines. Looking at the example below, it’s difficult to tell if the end of the line is the end of an element or statement or if it is specifying the end of an argument that is being passed into a function.\n\nTo add some additional frustration around this bad practice of trailing commas, BigQuery query formatter actually reformats your code to display everything a trailing comma. :(\n\nIn this example we can see that leading commas are not used, making it very hard to spot the missing comma.\n\nIn this example we can see that all of the commas are aligned, making it easy to ensure that no comma is ever missing.\n\nIn this example we can see that leading commas are not used. We have a line break that ends with a comma so it is vary hard to tell if this ‘max’ line is really one single statement or if it is part of a longer statement. One could argue that you shouldn’t use line breaks like this in your code and while I support line breaks in the right places (because it makes code easier to read), you would still encounter the same issue when looking at your word-wrapped code in your editor or in a Git diff comparison.\n\nBest Practice — Leading Comma with Indentation on Wrapped Text\n\nIn the example below it’s easy to see that a comma is missing. You might immediately wonder if someone forgot the comma, but because it’s so easy to read code with leading commas, there’s a higher probability that the missing leading comma is by design and there isn’t actually a comma that should be in front of the ‘cast’ statement. Also, by adding indentation to the cast statement, the code appears to have a more obvious intent as to why a comma isn’t necessary.\n\nHad someone performed a carriage return after “desc)” in line 3, you’d now have a common beginning on line 4. Normally, we want to have all of the commas as leading characters. But that logic only applies if we are talking about an attribute (column) being returned. In our case, that comma at the end of line 3 is part of a case statement so things would get really confusing.\n\nIn a lot of SQL code in the old days, this was pretty common practice and it might have to do with the fact that SQL has been around for very long time, probably longer than most text editors with syntax highlighting. Today, most people are using (or should be using) syntax highlighting in their editors so the capitalization shouldn’t be as necessary to spot the reserve words.\n\nWhile I have a personal opinion to not use uppercase you may disagree and have a different opinion. Here’s my justification:\n\n1. I don’t like my code yelling at me. In social context and written communication, using uppercase is synonymous with yelling so I try to avoid writing this way.\n\n2. The code doesn’t read as fluidly. Psychologic studies have shown that word recognition is easier with lower case words that upper case words. This is because there is more shape variation with lower case words compared to upper case words. Reading speed can decrease by 13–20% when all upper case is used. Also, when you mix upper case and lower case words\n\n3. It’s extra key strokes to hold down the shift key while I type or lock/unlock the caps key. Not a good argument, I know.\n\nIn this example we can see that reserve words are uppercased and all other words are lowercased.\n\nIn this example we can see that all of the commas are aligned, making it easy to ensure that no comma is ever missing.\n\nI almost always perform my group by with explicit field names as opposed to the position number in the select statement. While this doesn’t have any bering on the results I have found that it saves me time when when it comes to typing and debugging. Normally I’d prefer to just use the numbers because it’s less text on the page but it has caused too many issues and time spent chasing down run-time bugs.\n\nHere you can see that we are performing a group-by with position numbers based on the selected items. What I don’t like about this practice is that:\n\n1. If someone puts an aggregate function(s) anywhere but the first or last item in the select statement then you have to skip a position number in the group by. This creates frustrations if you later decide to re-order your selected elements.\n\n2. You have to count out how many elements you have, minus the aggregated items, and then manually type the position number.\n\nBest Practice — Group by field name\n\nHere you can see that we are performing a group-by with explicit field names. While it looks like this would be a lot more typing and work versus using position numbers, it is actually faster to type than using numbers. How so? Because all that you have to do is copy what is in the select statement (minus the aggregated field) and paste those values in the group-by. When you use numbers you actually have to type the numbers."
    },
    {
        "link": "https://learn.microsoft.com/en-us/sql/relational-databases/tables/create-foreign-key-relationships?view=sql-server-ver16",
        "document": "Applies to: SQL Server 2016 (13.x) and later versions Azure SQL Database Azure SQL Managed Instance SQL database in Microsoft Fabric\n\nThis article describes how to create foreign key relationships in SQL Server by using SQL Server Management Studio or Transact-SQL. You create a relationship between two tables when you want to associate rows of one table with rows of another.\n\nCreating a new table with a foreign key requires CREATE TABLE permission in the database, and ALTER SCHEMA permission on the schema in which the table is being created.\n\nCreating a foreign key in an existing table requires ALTER TABLE permission on the table.\n• None A foreign key constraint doesn't have to be linked only to a primary key constraint in another table. Foreign keys can also be defined to reference the columns of a constraint in another table.\n• None When a value other than is entered into the column of a constraint, the value must exist in the referenced column. Otherwise, a foreign key violation error message is returned. To make sure that all values of a composite foreign key constraint are verified, specify on all the participating columns.\n• None constraints can reference only tables within the same database on the same server. Cross-database referential integrity must be implemented through triggers. For more information, see CREATE TRIGGER (Transact-SQL).\n• None constraints can reference another column in the same table, and is referred to as a self-reference.\n• None A constraint specified at the column level can list only one reference column. This column must have the same data type as the column on which the constraint is defined.\n• None A constraint specified at the table level must have the same number of reference columns as the number of columns in the constraint column list. The data type of each reference column must also be the same as the corresponding column in the column list.\n• None The Database Engine doesn't have a predefined limit on the number of constraints a table can contain that reference other tables. The Database Engine also doesn't limit the number of constraints owned by other tables that reference a specific table. However, the actual number of constraints used is limited by the hardware configuration, and by the design of the database and application. A table can reference a maximum of 253 other tables and columns as foreign keys (outgoing references). SQL Server 2016 (13.x) and later versions increase the limit for the number of other tables and columns that can reference columns in a single table (incoming references), from 253 to 10,000. (Requires at least 130 compatibility level.) The increase has the following restrictions:\n• None Greater than 253 foreign key references are supported for and DML operations. operations aren't supported.\n• None A table with a foreign key reference to itself is still limited to 253 foreign key references.\n• None Greater than 253 foreign key references aren't currently available for columnstore indexes, or memory-optimized tables.\n• None If a foreign key is defined on a CLR user-defined type column, the implementation of the type must support binary ordering. For more information, see CLR User-Defined Types.\n• None A column of type varchar(max) can participate in a constraint only if the primary key it references is also defined as type varchar(max).\n• None In Object Explorer, right-click the table that will be on the foreign-key side of the relationship and select Design. The table opens in Create and update database tables.\n• None From the Table Designer menu, select Relationships. (See the Table Designer menu in the header, or, right-click in the empty space of the table definition, then select Relationships....)\n• The relationship appears in the Selected Relationship list with a system-provided name in the format , where the first tablename is the name of the foreign key table, and the second tablename is the name of the primary key table. This is just a default and common naming convention for the (Name) field of the foreign key object.\n• None Select the relationship in the Selected Relationship list.\n• None Select Tables and Columns Specification in the grid to the right and select the ellipses (...) to the right of the property.\n• None In the Tables and Columns dialog box, in the Primary Key dropdown list, choose the table that will be on the primary-key side of the relationship.\n• None In the grid beneath the dialog box, choose the columns contributing to the table's primary key. In the adjacent grid cell to the right of each column, choose the corresponding foreign-key column of the foreign-key table. Table Designer suggests a name for the relationship. To change this name, edit the contents of the Relationship Name text box.\n• None Choose OK to create the relationship.\n• None Close the table designer window and Save your changes for the foreign key relationship change to take effect.\n\nThe following example creates a table and defines a foreign key constraint on the column that references the column in the table in the database. The and clauses are used to ensure that changes made to table are automatically propagated to the table.\n\nThe following example creates a foreign key on the column and references the column in the table in the database."
    },
    {
        "link": "https://learn.microsoft.com/th-th/sql/relational-databases/tables/create-foreign-key-relationships?view=sql-server-2017",
        "document": "Applies to: SQL Server 2016 (13.x) and later versions Azure SQL Database Azure SQL Managed Instance SQL database in Microsoft Fabric\n\nThis article describes how to create foreign key relationships in SQL Server by using SQL Server Management Studio or Transact-SQL. You create a relationship between two tables when you want to associate rows of one table with rows of another.\n\nCreating a new table with a foreign key requires CREATE TABLE permission in the database, and ALTER SCHEMA permission on the schema in which the table is being created.\n\nCreating a foreign key in an existing table requires ALTER TABLE permission on the table.\n• None A foreign key constraint doesn't have to be linked only to a primary key constraint in another table. Foreign keys can also be defined to reference the columns of a constraint in another table.\n• None When a value other than is entered into the column of a constraint, the value must exist in the referenced column. Otherwise, a foreign key violation error message is returned. To make sure that all values of a composite foreign key constraint are verified, specify on all the participating columns.\n• None constraints can reference only tables within the same database on the same server. Cross-database referential integrity must be implemented through triggers. For more information, see CREATE TRIGGER (Transact-SQL).\n• None constraints can reference another column in the same table, and is referred to as a self-reference.\n• None A constraint specified at the column level can list only one reference column. This column must have the same data type as the column on which the constraint is defined.\n• None A constraint specified at the table level must have the same number of reference columns as the number of columns in the constraint column list. The data type of each reference column must also be the same as the corresponding column in the column list.\n• None The Database Engine doesn't have a predefined limit on the number of constraints a table can contain that reference other tables. The Database Engine also doesn't limit the number of constraints owned by other tables that reference a specific table. However, the actual number of constraints used is limited by the hardware configuration, and by the design of the database and application. A table can reference a maximum of 253 other tables and columns as foreign keys (outgoing references). SQL Server 2016 (13.x) and later versions increase the limit for the number of other tables and columns that can reference columns in a single table (incoming references), from 253 to 10,000. (Requires at least 130 compatibility level.) The increase has the following restrictions:\n• None Greater than 253 foreign key references are supported for and DML operations. operations aren't supported.\n• None A table with a foreign key reference to itself is still limited to 253 foreign key references.\n• None Greater than 253 foreign key references aren't currently available for columnstore indexes, or memory-optimized tables.\n• None If a foreign key is defined on a CLR user-defined type column, the implementation of the type must support binary ordering. For more information, see CLR User-Defined Types.\n• None A column of type varchar(max) can participate in a constraint only if the primary key it references is also defined as type varchar(max).\n• None In Object Explorer, right-click the table that will be on the foreign-key side of the relationship and select Design. The table opens in Create and update database tables.\n• None From the Table Designer menu, select Relationships. (See the Table Designer menu in the header, or, right-click in the empty space of the table definition, then select Relationships....)\n• The relationship appears in the Selected Relationship list with a system-provided name in the format , where the first tablename is the name of the foreign key table, and the second tablename is the name of the primary key table. This is just a default and common naming convention for the (Name) field of the foreign key object.\n• None Select the relationship in the Selected Relationship list.\n• None Select Tables and Columns Specification in the grid to the right and select the ellipses (...) to the right of the property.\n• None In the Tables and Columns dialog box, in the Primary Key dropdown list, choose the table that will be on the primary-key side of the relationship.\n• None In the grid beneath the dialog box, choose the columns contributing to the table's primary key. In the adjacent grid cell to the right of each column, choose the corresponding foreign-key column of the foreign-key table. Table Designer suggests a name for the relationship. To change this name, edit the contents of the Relationship Name text box.\n• None Choose OK to create the relationship.\n• None Close the table designer window and Save your changes for the foreign key relationship change to take effect.\n\nThe following example creates a table and defines a foreign key constraint on the column that references the column in the table in the database. The and clauses are used to ensure that changes made to table are automatically propagated to the table.\n\nThe following example creates a foreign key on the column and references the column in the table in the database."
    },
    {
        "link": "https://learn.microsoft.com/en-us/sql/relational-databases/tables/modify-foreign-key-relationships?view=sql-server-ver16",
        "document": "In Object Explorer , expand the table with the foreign key and then expand Keys .\n\nRight-click the foreign key to be modified and select Modify .\n\nIn the Foreign Key Relationships dialog box, you can make the following modifications.\n\nSelected Relationship\n\n Lists existing relationships. Select a relationship to show its properties in the grid to the right. If the list is empty, no relationships have been defined for the table.\n\nAdd\n\n Create a new relationship. The Tables and Columns Specifications must be set before the relationship will be valid.\n\nDelete\n\n Delete the relationship selected in the Selected Relationships list. To cancel the addition of a relationship, use this button to remove the relationship.\n\nGeneral Category\n\n Expand to show Check Existing Data on Creation or RE-Enabling and Tables and Columns Specifications.\n\nCheck Existing Data on Creation or Re-Enabling\n\n Verify all existing data in the table before the constraint was created or re-enabled, against the constraint.\n\nTables and Columns Specifications Category\n\n Expand to show which columns from which tables act as the foreign key and primary (or unique) key in the relationship. To edit or define these values, click the ellipsis button (...) to the right of the property field.\n\nForeign Key Base Table\n\n Shows which table contains the column acting as a foreign key in the selected relationship.\n\nForeign Key Columns\n\n Shows which column acts as a foreign key in the selected relationship.\n\nPrimary/Unique Key Base Table\n\n Shows which table contains the column acting as a primary (or unique) key in the selected relationship.\n\nPrimary/Unique Key Columns\n\n Shows which column acts as a primary (or unique) key in the selected relationship.\n\nIdentity Category\n\n Expand to show the property fields for Name and Description.\n\nName\n\n Shows the name of the relationship. When a new relationship is created, it is given a default name based on the table in the active window in Table Designer. You can change the name at any time.\n\nDescription\n\n Describe the relationship. To write a more detailed description, click Description and then click the ellipsis (...) that appears to the right of the property field. This provides a larger area in which to write text.\n\nTable Designer Category\n\n Expand to show information for Check Existing Data on Creation or Re-Enabling and Enforce for Replication.\n\nEnforce For Replication\n\n Indicates whether to enforce the constraint when a replication agent performs an insert, update, or delete on this table.\n\nEnforce Foreign Key Constraint\n\n Specify whether changes are allowed to the data of the columns in the relationship if those changes would invalidate the integrity of the foreign key relationship. Choose Yes if you do not want to allow such changes, and choose No if you do want to allow them.\n\nINSERT and UPDATE Specification Category\n\n Expand to show information for the Delete Rule and the Update Rule for the relationship.\n\nDelete Rule\n\n Specify what happens if a user tries to delete a row with data that is involved in a foreign key relationship:\n• None No Action An error message tells the user that the deletion is not allowed and the DELETE is rolled back.\n• None Cascade Deletes all rows containing data involved in the foreign key relationship. Do not specify CASCADE if the table will be included in a merge publication that uses logical records.\n• None Set Null Sets the value to null if all foreign key columns for the table can accept null values.\n• None Set Default Sets the value to the default value defined for the column if all foreign key columns for the table have defaults defined for them.\n\nUpdate Rule\n\n Specify what occurs if a user tries to update a row with data that is involved in a foreign key relationship:\n• None No Action An error message tells the user that the update is not allowed and the UPDATE is rolled back.\n• None Cascade Updates all rows that contain data involved in the foreign key relationship. Do not specify CASCADE if the table will be included in a merge publication that uses logical records.\n• None Set Null Sets the value to null if all foreign key columns for the table can accept null values.\n• None Set Default Sets the value to the default value that is defined for the column if all foreign key columns for the table have defaults defined for them."
    },
    {
        "link": "https://learn.microsoft.com/en-us/ef/core/modeling/relationships",
        "document": "This document provides a simple introduction to the representation of relationships in object models and relational databases, including how EF Core maps between the two.\n\nA relationship defines how two entities relate to each other. For example, when modeling posts in a blog, each post is related to the blog it is published on, and the blog is related to all the posts published on that blog.\n\nIn an object-oriented language like C#, the blog and post are typically represented by two classes: and . For example:\n\nIn the classes above, there is nothing to indicate that and are related. This can be added to the object model by adding a reference from to the on which it is published:\n\nLikewise, the opposite direction of the same relationship can be represented as a collection of objects on each :\n\nThis connection from to and, inversely, from back to is known as a \"relationship\" in EF Core.\n\nRelational databases represent relationships using foreign keys. For example, using SQL Server or Azure SQL, the following tables can be used to represent our and classes:\n\nIn this relational model, the and tables are each given a \"primary key\" column. The value of the primary key uniquely identifies each post or blog. In addition, the table is given a \"foreign key\" column. The primary key column is referenced by the foreign key column of the table. This column is \"constrained\" such that any value in the column of must match a value in the column of . This match determines which blog every post is related to. For example, if the value in one row of the table is 7, then the post represented by that row is published in the blog with the primary key 7.\n\nEF Core relationship mapping is all about mapping the primary key/foreign key representation used in a relational database to the references between objects used in an object model.\n\nIn the most basic sense, this involves:\n• Associating the references between entity types with the primary and foreign keys to form a single relationship configuration.\n\nOnce this mapping is made, EF changes the foreign key values as needed when the references between objects change, and changes the references between objects as needed when the foreign key values change.\n\nFor example, the entity types shown above can be updated with primary and foreign key properties:\n\nThe primary key property of , , and the foreign key property of , , can then be associated with the references (\"navigations\") between the entity types ( and ). This is done automatically by EF when building a simple relationship like this, but can also be specified explicitly when overriding the method of your . For example:\n\nNow all these properties will behave coherently together as a representation of a single relationship between and .\n\nEF supports many different types of relationships, with many different ways these relationships can be represented and configured. To jump into examples for different kinds of relationships, see:\n• One-to-many relationships, in which a single entity is associated with any number of other entities.\n• One-to-one relationships, in which a single entity is associated with another single entity.\n• Many-to-many relationships, in which any number of entities are associated with any number of other entities.\n\nIf you are new to EF, then trying the examples linked in in the bullet points above is a good way to get a feel for how relationships work.\n\nTo dig deeper into the properties of entity types involved in relationship mapping, see:\n• Foreign and principal keys in relationships, which covers how foreign keys map to the database.\n• Relationship navigations, which describes how navigations are layered over a foreign key to provide an object-oriented view of the relationship.\n\nEF models are built using a combination of three mechanisms: conventions, mapping attributes, and the model builder API. Most of the examples show the model building API. To find out more about other options, see:\n• Relationship conventions, which discover entity types, their properties, and the relationships between the types.\n• Relationship mapping attributes, which can be used an alternative to the model building API for some aspects of relationship configuration.\n• Cascade deletes, which describe how related entities can be automatically deleted when or is called.\n• Owned entity types use a special type of \"owning\" relationship that implies a stronger connection between the two types than the \"normal\" relationships discussed here. Many of the concepts described here for normal relationships are carried over to owned relationships. However, owned relationships also have their own special behaviors.\n\nRelationships defined in the model can be used in various ways. For example:\n• Relationships can be used to query related data in any of three ways:\n• Eagerly as part of a LINQ query, using .\n• Lazily using lazy-loading proxies, or lazy-loading without proxies.\n• Explicitly using the or methods.\n• Relationships can be used in data seeding through matching of PK values to FK values.\n• Relationships can be used to track graphs of entities. Relationships are then used by the change tracker to:\n• Detect changes in relationships and perform fixup\n• Send foreign key updates to the database with or"
    },
    {
        "link": "https://learn.microsoft.com/th-th/sql/relational-databases/tables/create-foreign-key-relationships?view=azuresqldb-current",
        "document": "Applies to: SQL Server 2016 (13.x) and later versions Azure SQL Database Azure SQL Managed Instance SQL database in Microsoft Fabric\n\nThis article describes how to create foreign key relationships in SQL Server by using SQL Server Management Studio or Transact-SQL. You create a relationship between two tables when you want to associate rows of one table with rows of another.\n\nCreating a new table with a foreign key requires CREATE TABLE permission in the database, and ALTER SCHEMA permission on the schema in which the table is being created.\n\nCreating a foreign key in an existing table requires ALTER TABLE permission on the table.\n• None A foreign key constraint doesn't have to be linked only to a primary key constraint in another table. Foreign keys can also be defined to reference the columns of a constraint in another table.\n• None When a value other than is entered into the column of a constraint, the value must exist in the referenced column. Otherwise, a foreign key violation error message is returned. To make sure that all values of a composite foreign key constraint are verified, specify on all the participating columns.\n• None constraints can reference only tables within the same database on the same server. Cross-database referential integrity must be implemented through triggers. For more information, see CREATE TRIGGER (Transact-SQL).\n• None constraints can reference another column in the same table, and is referred to as a self-reference.\n• None A constraint specified at the column level can list only one reference column. This column must have the same data type as the column on which the constraint is defined.\n• None A constraint specified at the table level must have the same number of reference columns as the number of columns in the constraint column list. The data type of each reference column must also be the same as the corresponding column in the column list.\n• None The Database Engine doesn't have a predefined limit on the number of constraints a table can contain that reference other tables. The Database Engine also doesn't limit the number of constraints owned by other tables that reference a specific table. However, the actual number of constraints used is limited by the hardware configuration, and by the design of the database and application. A table can reference a maximum of 253 other tables and columns as foreign keys (outgoing references). SQL Server 2016 (13.x) and later versions increase the limit for the number of other tables and columns that can reference columns in a single table (incoming references), from 253 to 10,000. (Requires at least 130 compatibility level.) The increase has the following restrictions:\n• None Greater than 253 foreign key references are supported for and DML operations. operations aren't supported.\n• None A table with a foreign key reference to itself is still limited to 253 foreign key references.\n• None Greater than 253 foreign key references aren't currently available for columnstore indexes, or memory-optimized tables.\n• None If a foreign key is defined on a CLR user-defined type column, the implementation of the type must support binary ordering. For more information, see CLR User-Defined Types.\n• None A column of type varchar(max) can participate in a constraint only if the primary key it references is also defined as type varchar(max).\n• None In Object Explorer, right-click the table that will be on the foreign-key side of the relationship and select Design. The table opens in Create and update database tables.\n• None From the Table Designer menu, select Relationships. (See the Table Designer menu in the header, or, right-click in the empty space of the table definition, then select Relationships....)\n• The relationship appears in the Selected Relationship list with a system-provided name in the format , where the first tablename is the name of the foreign key table, and the second tablename is the name of the primary key table. This is just a default and common naming convention for the (Name) field of the foreign key object.\n• None Select the relationship in the Selected Relationship list.\n• None Select Tables and Columns Specification in the grid to the right and select the ellipses (...) to the right of the property.\n• None In the Tables and Columns dialog box, in the Primary Key dropdown list, choose the table that will be on the primary-key side of the relationship.\n• None In the grid beneath the dialog box, choose the columns contributing to the table's primary key. In the adjacent grid cell to the right of each column, choose the corresponding foreign-key column of the foreign-key table. Table Designer suggests a name for the relationship. To change this name, edit the contents of the Relationship Name text box.\n• None Choose OK to create the relationship.\n• None Close the table designer window and Save your changes for the foreign key relationship change to take effect.\n\nThe following example creates a table and defines a foreign key constraint on the column that references the column in the table in the database. The and clauses are used to ensure that changes made to table are automatically propagated to the table.\n\nThe following example creates a foreign key on the column and references the column in the table in the database."
    }
]