[
    {
        "link": "https://numpy.org/doc/2.2/reference/routines.array-manipulation.html",
        "document": "Return a new array with sub-arrays along an axis deleted.\n\nInsert values along the given axis before the given indices.\n\nAppend values to the end of an array.\n\nReturn a new array with the specified shape.\n\nRemove values along a dimension which are zero along all other.\n\nFind the unique elements of an array."
    },
    {
        "link": "https://numpy.org/doc/2.1/reference/routines.array-manipulation.html",
        "document": "Return a new array with sub-arrays along an axis deleted.\n\nInsert values along the given axis before the given indices.\n\nAppend values to the end of an array.\n\nReturn a new array with the specified shape.\n\nTrim the leading and/or trailing zeros from a 1-D array or sequence.\n\nFind the unique elements of an array."
    },
    {
        "link": "https://scipy-lectures.org/intro/numpy/operations.html",
        "document": "These operations are of course much faster than if you did them in pure python: 10000 loops, best of 3: 24.3 us per loop 1000 loops, best of 3: 861 us per loop\n• Try simple arithmetic elementwise operations: add even elements with odd elements\n• Time them against their pure python counterparts using . File , line , in : operands could not be broadcast together with shapes (4) (2) Broadcasting? We’ll return to that later. The transpose returns a view of the original array: The sub-module implements basic linear algebra, such as solving linear systems, singular value decomposition, etc. However, it is not guaranteed to be compiled using efficient routines, and thus we recommend the use of , as detailed in section Linear algebra operations: scipy.linalg\n• Look at the help for . When might this be useful?\n• Look at the help for and .\n\nSum by rows and by columns: — works the same way (and take ) Can be used for array comparisons: … and many more (best to learn as you go).\n• Given there is a , what other function might you expect to see?\n• What is the difference between and ? Let us consider a simple 1D random walk process: at each time step a walker jumps right or left with equal probability. We are interested in finding the typical distance from the origin of a random walker after left or right jumps? We are going to simulate many “walkers” to find this law, and we are going to do so using array computing tricks: we are going to create a 2D array with the “stories” (each walker has a story) in one direction, and the time in the other: # time during which we follow the walker We randomly choose all the steps 1 or -1 of the walk: # +1 because the high value is exclusive # Verification: all steps are 1 or -1 We build the walks by summing steps along the time: We get the mean in the axis of the stories: We find a well-known result in physics: the RMS distance grows as the square root of the time!\n• None Basic operations on arrays (addition, etc.) are elementwise\n• None This works on arrays of the same size. Nevertheless, It’s also possible to do operations on arrays of different sizes if NumPy can transform these arrays so that they all have the same size: this conversion is called broadcasting. The image below gives an example of broadcasting: We have already used broadcasting without knowing it!: # we assign an array of dimension 0 to an array of dimension 1 Broadcasting seems a bit magical, but it is actually quite natural to use it when we want to solve a problem whose output data is an array with more dimensions than input data. Let’s construct an array of distances (in miles) between cities of Route 66: Chicago, Springfield, Saint-Louis, Tulsa, Oklahoma City, Amarillo, Santa Fe, Albuquerque, Flagstaff and Los Angeles. A lot of grid-based or network-based problems can also use broadcasting. For instance, if we want to compute the distance from the origin of points on a 5x5 grid, we can do Remark : the function allows to directly create vectors x and y of the previous example, with two “significant dimensions”: So, is very useful as soon as we have to handle computations on a grid. On the other hand, directly provides matrices full of indices for cases where we can’t (or don’t want to) benefit from broadcasting: Broadcasting: discussion of broadcasting in the Advanced NumPy chapter.\n\nTo understand this you need to learn more about the memory layout of a numpy array. Indexing with the object allows us to add an axis to an array (you have seen this already above in the broadcasting section): Size of an array can be changed with : However, it must not be referred to somewhere else: File , line , in : cannot resize an array that has been referenced or is referencing another array in this way. Use the resize function\n• Look at the docstring for , especially the notes section which has some more information about copies and views.\n• Use as an alternative to . What is the difference? (Hint: check which one returns a view and which a copy)\n\nWhat do you need to know to get started?\n• None Know how to create arrays : , , , .\n• None Know the shape of the array with , then use slicing to obtain different views of the array: , etc. Adjust the shape of the array using or flatten it with .\n• None Obtain a subset of the elements of an array and/or modify their values with masks\n• None Know miscellaneous operations on arrays, such as finding the mean or max ( , ). No need to retain everything, but have the reflex to search in the documentation (online docs, , )!!\n• None For advanced use: master the indexing with arrays of integers, as well as broadcasting. Know more NumPy functions to handle various array operations. If you want to do a first quick pass through the Scipy lectures to learn the ecosystem, you can directly skip to the next chapter: Matplotlib: plotting. The remainder of this chapter is not necessary to follow the rest of the intro part. But be sure to come back and finish this chapter, as well as to do some more exercices."
    },
    {
        "link": "https://geeksforgeeks.org/numpy-tutorial",
        "document": "NumPy is a powerful library for numerical computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays. NumPy’s array objects are more memory-efficient and perform better than Python lists, which is essential for tasks in scientific computing, data analysis, and machine learning. This NumPy tutorial will cover core features, and all concept from basic to advanced divided in 10 sections.\n\nThe numpy array also called ndarray is a grid of values, all of the same types. They can be one-dimensional (like a list), two-dimensional (like a matrix) or multi-dimensional (like a table with rows and columns).\n\nNumPy arrays are created using the function, which converts lists, tuples, or other sequences into a NumPy array. You can create different types of arrays, such as 1D arrays from a simple list of elements, 2D arrays from nested lists representing rows and columns, and multi-dimensional arrays by further nesting lists.\n\nTo understand all ways for creating Array refer to: Numpy array creation page. For understanding techniques in-depth ( explained in numpy creation page ) refer to following resources:\n\nNote: These numPy array creation methods form the foundation for efficiently working with arrays in data science, machine learning, and scientific computing.\n\nNumPy arrays offer four essential types of operations that allow efficient data manipulation by performing element-wise computations, mathematical functions, string processing, and logical comparisons.\n\nWhile indexing is used to access specific elements in an array based on their positions or conditions, slicing is used to extract a subset of elements from an array using the syntax . It works for both 1D and multi-dimensional arrays, making it easy to select specific rows, columns, or ranges of data.\n\nShape of an array can be defined as the number of elements in each dimension. It can be accessed using the attribute, which returns a tuple representing the dimensions of the array. In this section, we will explore how to change the shape of a NumPy array. This includes reshaping, flattening, and modifying the structure of arrays to suit specific tasks.\n\nSorting in NumPy refers to arranging the elements of an array in a specific order, either ascending or descending. The function is used for this purpose. By default, it sorts elements in ascending order, and descending order can be achieved using slicing techniques like .\n\nSearching in NumPy involves finding specific values or conditions within an array. In this section, we’ll explore different techniques for searching within NumPy arrays searching for Specific Values using and that returns the indices of all non-zero elements.\n\nCombining arrays involves merging smaller arrays into a single larger one. arrays can be combined vertically, horizontally, or along any specific axis. It include Techniques like concatenation and stacking ( horizontal , vertical ).\n\nSplitting arrays is the process of dividing a larger array into smaller, manageable sub-arrays. The division can occur along rows, columns, or other axes.\n\nAggregation refers to summarizing data within an array by applying mathematical operations like summing, finding the average, or determining the maximum/minimum values.\n\nIn NumPy, a matrix is represented as a 2D array. Matrix operations are a fundamental part of linear algebra. Matrix Addition, Subtraction, and Multiplication are fundamental for manipulating matrices. To work with matrices, NumPy provides simple tools. For example, flips the matrix by turning rows into columns and columns into rows. If you want to change the shape of a matrix, like turning a single row into multiple rows, you use . To simplify a matrix and turn it into a single list of values, you can use . Finally, is used for multiplying two matrices.\n\nEigenvalues and eigenvectors are fundamental concepts in linear algebra. NumPy provides a robust module to perform various linear algebra operations efficiently.\n\nNumPy provides a powerful module, , for generating random data efficiently enables users to create random numbers, samples, and arrays for a variety of distributions.\n\nThe module allows for generating random integers, random floats between 0 and 1 and random samples from normal, uniform, and other statistical distributions. For example: if you want to generate 5 random numbers from the normal distribution using NumPy. Click on the link for solution.\n\nWelcome to the advanced section of NumPy! If you’ve already learned the basics of arrays, you’re ready to explore some powerful tools that make working with data faster and easier\n\nRefer to Practice Exercises, Questions, and Solutions for hands-on-numpy problems"
    },
    {
        "link": "https://medium.com/@debopamdeycse19/numpy-array-manipulation-97690e2bb1e9",
        "document": "Hello friends, In previous articles we talked about numpy basics, and how to install Numpy on your computer then we discussed how to create numpy arrays, slicing, indexing, and much more. In today's article, we will discuss different array manipulation techniques, element-wise operations, broadcasting, and more methods in Numpy.\n\nYou may easily reshape, concatenate, and split arrays with NumPy’s extensive array manipulation features.\n• Reshaping Arrays: Using the reshape() function you can modify the array’s shape. Make sure that the new form has the same number of components as the old array. Let's see one example:\n\nThe output will be:\n\n2. Concatenating Arrays: You can concatenate (combine) two or more arrays using functions like numpy.concatenate(). Ensure that the dimensions along the specified axis match. Let's see one example:\n\nThe output will be:\n\nTo learn more about concatenation Click here\n\n3. Splitting Numpy Arrays: You can split an array into multiple smaller arrays using functions like numpy.split() or numpy.hsplit() for horizontal splitting. Let’s see one example:\n\nThe output will be:\n\nTo learn more about splitting Check the docs.\n\nOne of the key features of NumPy is its ability to perform element-wise operations on arrays. These operations apply mathematical operations like addition, subtraction, multiplication, and division to each element of an array.\n\nThe output will be:\n\nYou can explore different Numpy’s methods like insert, delete, append, resize, and more.\n\nNumPy introduces the concept of broadcasting, which allows operations on arrays of different shapes without the need for explicit loops. Broadcasting is especially useful when performing element-wise operations on arrays with mismatched dimensions. Let's see one example:\n\nThe output will be:\n\nThe output will be:\n\nTo learn more about broadcasting visit the documentation.\n\nNumerous aggregation functions are available in NumPy for reducing data in arrays. You can use these functions to calculate statistics like sum, mean, median, and more. Let's see one example:\n\nThe output will be:\n\nTo learn more about aggregation functions see docs.\n\nWe showed how NumPy makes it easy to do math with numbers in arrays. NumPy can add, subtract, multiply, or divide these numbers, and it’s good at it!\n\nWe were able to conduct operations on arrays of various shapes without costly loops thanks to the concept of broadcasting, which opened up new possibilities. As a result, NumPy is a reliable tool for managing a variety of datasets. Finally, we looked at aggregation functions like sum, mean, and median that help with data reduction and statistical analysis.\n\n“If you learn something new from this article, please show your support by giving it a clap. Your appreciation motivates me to create more articles for you. Thank you for your encouragement!”"
    },
    {
        "link": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html",
        "document": "Convolve in1 and in2 with output size determined by mode, and boundary conditions determined by boundary and fillvalue.\n\nSecond input. Should have the same number of dimensions as in1. A string indicating the size of the output: The output is the full discrete linear convolution of the inputs. (Default) The output consists only of those elements that do not rely on the zero-padding. In ‘valid’ mode, either in1 or in2 must be at least as large as the other in every dimension. The output is the same size as in1, centered with respect to the ‘full’ output. Value to fill pad input arrays with. Default is 0. A 2-dimensional array containing a subset of the discrete linear convolution of in1 with in2.\n\nCompute the gradient of an image by 2D convolution with a complex Scharr operator. (Horizontal operator is real, vertical is imaginary.) Use symmetric boundary condition to avoid creating edges at the image boundaries."
    },
    {
        "link": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve.html",
        "document": "Convolve in1 and in2, with the output size determined by the mode argument.\n\nSecond input. Should have the same number of dimensions as in1. A string indicating the size of the output: The output is the full discrete linear convolution of the inputs. (Default) The output consists only of those elements that do not rely on the zero-padding. In ‘valid’ mode, either in1 or in2 must be at least as large as the other in every dimension. The output is the same size as in1, centered with respect to the ‘full’ output. A string indicating which method to use to calculate the convolution. The convolution is determined directly from sums, the definition of convolution. The Fourier Transform is used to perform the convolution by calling . Automatically chooses direct or Fourier method based on an estimate of which is faster (default). See Notes for more detail. An N-dimensional array containing a subset of the discrete linear convolution of in1 with in2. Use of the FFT convolution on input containing NAN or INF will lead to the entire output being NAN or INF. Use method=’direct’ when your input contains NAN or INF values.\n\nperforms polynomial multiplication (same operation, but also accepts poly1d objects) Always uses the FFT method. Uses the overlap-add method to do convolution, which is generally faster when the input arrays are large and significantly different in size.\n\nBy default, and use , which calls to choose the fastest method using pre-computed values ( can also measure real-world timing with a keyword argument). Because relies on floating point numbers, there are certain constraints that may force (more detail in docstring)."
    },
    {
        "link": "https://stackoverflow.com/questions/16121269/2d-convolution-in-python-similar-to-matlabs-conv2",
        "document": "There are a number of different ways to do it with , but 2D convolution isn't directly included in . (It's also easy to implement with an fft using only numpy, if you need to avoid a scipy dependency.)\n\n, , , and will all handle a 2D convolution (the last three are N-d) in different ways.\n\ndoes the convolution in the fft domain (where it's a simple multiplication). This is much faster in many cases, but can lead to very small differences in edge effects than the discrete case, and your data will be coerced into floating point with this particular implementation. Additionally, there's unnecessary memory usage when convolving a small array with a much larger array. All in all, fft-based methods can be dramatically faster, but there are some common use cases where is not an ideal solution.\n\n, , and all use a discrete convolution implemented in C, however, they implement it in different ways.\n\nkeeps the same data type, and gives you control over the location of the output to minimize memory usage. If you're convolving 's (e.g. image data), it's often the best option. The output will always be the same shape as the first input array, which makes sense for images, but perhaps not for more general convolution. gives you a lot of control over how edge effects are handled through the kwarg (which functions completely differently than 's kwarg).\n\nAvoid if you're working with 2d arrays. It works for the N-d case, but it's suboptimal for 2d arrays, and exists to do the exact same thing a bit more efficiently. The convolution functions in give you control over the output shape using the kwarg. (By default, they'll behave just like matlab's .) This is useful for general mathematical convolution, but less useful for image processing. However, is generally slower than .\n\nThere are a lot of different options partly due to duplication in the different submodules of and partly because there are different ways to implement a convolution that have different performance tradeoffs.\n\nIf you can give a bit more detail about your use case, we can recommend a better solution. If you're convolving two arrays of roughly the same size, and they're already floats, is an excellent choice. Otherwise, may beat it."
    },
    {
        "link": "https://stackoverflow.com/questions/2448015/2d-convolution-using-python-and-numpy",
        "document": "It might not be the most optimized solution either, but it is approximately ten times faster than the one proposed by @omotto and it only uses basic numpy function (as reshape, expand_dims, tile...) and no 'for' loops:\n\nI tried to add a lot of comments to explain the method but the global idea is to reshape the 3D input image to a 5D one of shape (output_image_height, kernel_height, output_image_width, kernel_width, output_image_channel) and then to apply the kernel directly using the basic array multiplication. Of course, this methods is then using more memory (during the execution the size of the image is thus multiply by kernel_height*kernel_width) but it is faster.\n\nTo do this reshape step, I 'over-used' the indexing methods of numpy arrays, especially, the possibility of giving a numpy array as indices into a numpy array.\n\nThis methods could also be used to re-code the 2D convolution product in Pytorch or Tensorflow using the base math functions but I have no doubt in saying that it will be slower than the existing nn.conv2d operator...\n\nI really enjoyed coding this method by only using the numpy basic tools."
    },
    {
        "link": "https://blog.rtwilson.com/convolution-in-python-which-function-to-use",
        "document": "Convolution in python – which function to use?\n\nSlightly boringly, this very similar to my last post – but it’s also something useful that you may want to know, and that I’ll probably forget if I don’t write it down somewhere.\n\nBasically, scipy.ndimage.filters.convolve is about twice as fast as scipy.signal.convolve2d.\n\nI run convolutions a lot on satellite images, and Landsat images are around 8000 x 8000 pixels. Using a random 8000 x 8000 pixel image, with a 3 x 3 kernel (a size I often use), I find that takes 2.9 seconds, whereas takes 1.1 seconds – a useful speedup, particularly if you’re running a loop of various convolutions. The same sort of speed-up can be seen with larger kernel sizes, for example, a 9 x 9 kernel takes 14.7 seconds and 6.8 seconds – an even more useful speed-up.\n\nWhen I switched my code from to , I had to do a bit of playing around to get the various options set to ensure that I got the same results. For reference, the following two lines of code are equivalent:\n\nInterestingly these functions are both within scipy, so there must be a reason for them both to exist. Can anyone enlighten me? My naive view is that they could be merged – or if not, then a note could be added to the documentation saying that is far faster!\n\nIf you found this post useful, please consider buying me a coffee.\n\nThis post originally appeared on Robin's Blog."
    },
    {
        "link": "https://pyimagesearch.com/2016/07/25/convolutions-with-opencv-and-python",
        "document": "I’m going to start today’s blog post by asking a series of questions which will then be addressed later in the tutorial:\n• What do they do?\n• Why do we use them?\n• How do we apply them?\n• And what role do convolutions play in deep learning?\n\nThe word “convolution” sounds like a fancy, complicated term — but it’s really not. In fact, if you’ve ever worked with computer vision, image processing, or OpenCV before, you’ve already applied convolutions, whether you realize it or not!\n\nHave you opened Photoshop or GIMP to sharpen an image? You guessed it — convolution.\n\nConvolutions are one of the most critical, fundamental building-blocks in computer vision and image processing. But the term itself tends to scare people off — in fact, on the the surface, the word even appears to have a negative connotation.\n\nTrust me, convolutions are anything but scary. They’re actually quite easy to understand.\n\nIn reality, an (image) convolution is simply an element-wise multiplication of two matrices followed by a sum.\n\nSeriously. That’s it. You just learned what convolution is:\n• Take two matrices (which both have the same dimensions).\n• Multiply them, element-by-element (i.e., not the dot-product, just a simple multiplication).\n\nTo understand more about convolutions, why we use them, how to apply them, and the overall role they play in deep learning + image classification, be sure to keep reading this post.\n\nThink of it this way — an image is just a multi-dimensional matrix. Our image has a width (# of columns) and a height (# of rows), just like a matrix.\n\nBut unlike the traditional matrices you may have worked with back in grade school, images also have a depth to them — the number of channels in the image. For a standard RGB image, we have a depth of 3 — one channel for each of the Red, Green, and Blue channels, respectively.\n\nGiven this knowledge, we can think of an image as a big matrix and kernel or convolutional matrix as a tiny matrix that is used for blurring, sharpening, edge detection, and other image processing functions.\n\nEssentially, this tiny kernel sits on top of the big image and slides from left-to-right and top-to-bottom, applying a mathematical operation (i.e., a convolution) at each (x, y)-coordinate of the original image.\n\nIt’s normal to hand-define kernels to obtain various image processing functions. In fact, you might already be familiar with blurring (average smoothing, Gaussian smoothing, median smoothing, etc.), edge detection (Laplacian, Sobel, Scharr, Prewitt, etc.), and sharpening — all of these operations are forms of hand-defined kernels that are specifically designed to perform a particular function.\n\nSo that raises the question, is there a way to automatically learn these types of filters? And even use these filters for image classification and object detection?\n\nYou bet there is.\n\nBut before we get there, we need to understand kernels and convolutions a bit more.\n\nAgain, let’s think of an image as a big matrix and a kernel as tiny matrix (at least in respect to the original “big matrix” image):\n\nAs the figure above demonstrates, we are sliding the kernel from left-to-right and top-to-bottom along the original image.\n\nAt each (x, y)-coordinate of the original image, we stop and examine the neighborhood of pixels located at the center of the image kernel. We then take this neighborhood of pixels, convolve them with the kernel, and obtain a single output value. This output value is then stored in the output image at the same (x, y)-coordinates as the center of the kernel.\n\nIf this sounds confusing, no worries, we’ll be reviewing an example in the “Understanding Image Convolutions” section later in this blog post.\n\nBut before we dive into an example, let’s first take a look at what a kernel looks like:\n\nAbove we have defined a square 3 x 3 kernel (any guesses on what this kernel is used for?)\n\nKernels can be an arbitrary size of M x N pixels, provided that both M and N are odd integers.\n\nNote: Most kernels you’ll typically see are actually square N x N matrices.\n\nWe use an odd kernel size to ensure there is a valid integer (x, y)-coordinate at the center of the image:\n\nOn the left, we have a 3 x 3 matrix. The center of the matrix is obviously located at x=1, y=1 where the top-left corner of the matrix is used as the origin and our coordinates are zero-indexed.\n\nBut on the right, we have a 2 x 2 matrix. The center of this matrix would be located at x=0.5, y=0.5. But as we know, without applying interpolation, there is no such thing as pixel location (0.5, 0.5) — our pixel coordinates must be integers! This reasoning is exactly why we use odd kernel sizes — to always ensure there is a valid (x, y)-coordinate at the center of the kernel.\n\nNow that we have discussed the basics of kernels, let’s talk about a mathematical term called convolution.\n• A kernel matrix that we are going to apply to the input image.\n• An output image to store the output of the input image convolved with the kernel.\n\nConvolution itself is actually very easy. All we need to do is:\n• Select an (x, y)-coordinate from the original image.\n• Place the center of the kernel at this (x, y)-coordinate.\n• Take the element-wise multiplication of the input image region and the kernel, then sum up the values of these multiplication operations into a single value. The sum of these multiplications is called the kernel output.\n• Use the same (x, y)-coordinates from Step #1, but this time, store the kernel output in the same (x, y)-location as the output image.\n\nBelow you can find an example of convolving (denoted mathematically as the “*” operator) a 3 x 3 region of an image with a 3 x 3 kernel used for blurring:\n\nAfter applying this convolution, we would set the pixel located at the coordinate (i, j) of the output image O to O_i,j = 126.\n\nThat’s all there is to it!\n\nConvolution is simply the sum of element-wise matrix multiplication between the kernel and neighborhood that the kernel covers of the input image.\n\nThat was fun discussing kernels and convolutions — but now let’s move on to looking at some actual code to ensure you understand how kernels and convolutions are implemented. This source code will also help you understand how to apply convolutions to images.\n\nOpen up a new file, name it , and let’s get to work:\n\nWe start on Lines 2-5 by importing our required Python packages. You should already have NumPy and OpenCV installed on your system, but you might not have scikit-image installed. To install scikit-image, just use :\n\nNext, we can start defining our custom method:\n\nThe function requires two parameters: the (grayscale) that we want to convolve with the .\n\nGiven both our and (which we presume to be NumPy arrays), we then determine the spatial dimensions (i.e., width and height) of each (Lines 10 and 11).\n\nBefore we continue, it’s important to understand that the process of “sliding” a convolutional matrix across an image, applying the convolution, and then storing the output will actually decrease the spatial dimensions of our output image.\n\nWhy is this?\n\nRecall that we “center” our computation around the center (x, y)-coordinate of the input image that the kernel is currently positioned over. This implies there is no such thing as “center” pixels for pixels that fall along the border of the image. The decrease in spatial dimension is simply a side effect of applying convolutions to images. Sometimes this effect is desirable and other times its not, it simply depends on your application.\n\nHowever, in most cases, we want our output image to have the same dimensions as our input image. To ensure this, we apply padding (Lines 16-19). Here we are simply replicating the pixels along the border of the image, such that the output image will match the dimensions of the input image.\n\nOther padding methods exist, including zero padding (filling the borders with zeros — very common when building Convolutional Neural Networks) and wrap around (where the border pixels are determined by examining the opposite end of the image). In most cases, you’ll see either replicate or zero padding.\n\nWe are now ready to apply the actual convolution to our image:\n\nLines 24 and 25 loop over our , “sliding” the kernel from left-to-right and top-to-bottom 1 pixel at a time.\n\nLine 29 extracts the Region of Interest (ROI) from the using NumPy array slicing. The will be centered around the current (x, y)-coordinates of the . The will also have the same size as our , which is critical for the next step.\n\nConvolution is performed on Line 34 by taking the element-wise multiplication between the and , followed by summing the entries in the matrix.\n\nThe output value is then stored in the array at the same (x, y)-coordinates (relative to the input image).\n\nWe can now finish up our method:\n\nWhen working with images, we typically deal with pixel values falling in the range [0, 255]. However, when applying convolutions, we can easily obtain values that fall outside this range.\n\nIn order to bring our image back into the range [0, 255], we apply the function of scikit-image (Line 41). We also convert our image back to an unsigned 8-bit integer data type on Line 42 (previously, the image was a floating point type in order to handle pixel values outside the range [0, 255]).\n\nFinally, the image is returned to the calling function on Line 45.\n\nNow that we’ve defined our function, let’s move on to the driver portion of the script. This section of our program will handle parsing command line arguments, defining a series of kernels we are going to apply to our image, and then displaying the output results:\n\nLines 48-51 handle parsing our command line arguments. We only need a single argument here, , which is the path to our input path.\n\nWe then move on to Lines 54 and 55 which define a 7 x 7 kernel and a 21 x 21 kernel used to blur/smooth an image. The larger the kernel is, the more the image will be blurred. Examining this kernel, you can see that the output of applying the kernel to an ROI will simply be the average of the input region.\n\nWe define a sharpening kernel on Lines 58-61, used to enhance line structures and other details of an image. Explaining each of these kernels in detail is outside the scope of this tutorial, so if you’re interested in learning more about kernel construction, I would suggest starting here and then playing around with the excellent kernel visualization tool on Setosa.io.\n\nLines 65-68 define a Laplacian operator that can be used as a form of edge detection.\n\nNote: The Laplacian is also very useful for detecting blur in images.\n\nFinally, we’ll define two Sobel filters on Lines 71-80. The first (Lines 71-74) is used to detect vertical changes in the gradient of the image. Similarly, Lines 77-80 constructs a filter used to detect horizontal changes in the gradient.\n\nGiven all these kernels, we lump them together into a set of tuples called a “kernel bank”:\n\nFinally, we are ready to apply our to our image:\n\nLines 95 and 96 load our image from disk and convert it to grayscale. Convolution operators can certainly be applied to RGB (or other multi-channel images), but for the sake of simplicity in this blog post, we’ll only apply our filters to grayscale images).\n\nWe start looping over our set of kernels in the on Line 99 and then apply the current to the image on Line 104 by calling our custom method which we defined earlier.\n\nAs a sanity check, we also call which also applies our to the image. The function is a much more optimized version of our function. The main reason I included the implementation of in this blog post is to give you a better understanding of how convolutions work under the hood.\n\nFinally, Lines 108-112 display the output images to our screen.\n\nExample Convolutions with OpenCV and Python\n\nToday’s example image comes from a photo I took a few weeks ago at my favorite bar in South Norwalk, CT — Cask Republic. In this image you’ll see a glass of my favorite beer (Smuttynose Findest Kind IPA) along with three 3D-printed Pokemon from the (unfortunately, now closed) Industrial Chimp shop:\n\nTo run our script, just issue the following command:\n\nYou’ll then see the results of applying our kernel to the input image:\n\nOn the left, we have our original image. Then in the center we have the results from the function. And on the right, the results from . As the results demonstrate, our output matches , indicating that our function is working properly. Furthermore, our original image now appears “blurred” and “smoothed”, thanks to the smoothing kernel.\n\nComparing Figure 7 and Figure 8, notice how as the size of the averaging kernel increases, the amount of blur in the output image increases as well.\n\nWe can also sharpen our image:\n\nAnd find horizontal edges using Sobel as well:\n\nThe Role of Convolutions in Deep Learning\n\nAs you’ve gathered through this blog post, we must manually hand-define each of our kernels for applying various operations such as smoothing, sharpening, and edge detection.\n\nThat’s all fine and good, but what if there was a way to learn these filters instead? Is it possible to define a machine learning algorithm that can look at images and eventually learn these types of operators?\n\nIn fact, there is — these types of algorithms are a sub-type of Neural Networks called Convolutional Neural Networks (CNNs). By applying convolutional filters, nonlinear activation functions, pooling, and backpropagation, CNNs are able to learn filters that can detect edges and blob-like structures in lower-level layers of the network — and then use the edges and structures as building blocks, eventually detecting higher-level objects (i.e., faces, cats, dogs, cups, etc.) in the deeper layers of the network.\n\nExactly how do CNNs do this?\n\nI’ll show you — but it will have to wait for another few blog posts until we cover enough basics.\n\nIn today’s blog post, we discussed image kernels and convolutions. If we think of an image as a big matrix, then an image kernel is just a tiny matrix that sits on top of the image.\n\nThis kernel then slides from left-to-right and top-to-bottom, computing the sum of element-wise multiplications between the input image and the kernel along the way — we call this value the kernel output. The kernel output is then stored in an output image at the same (x, y)-coordinates as the input image (after accounting for any padding to ensure the output image has the same dimensions as the input).\n\nGiven our newfound knowledge of convolutions, we defined an OpenCV and Python function to apply a series of kernels to an image. These operators allowed us to blur an image, sharpen it, and detect edges.\n\nFinally, we briefly discussed the roles kernels/convolutions play in deep learning, specifically Convolutional Neural Networks, and how these filters can be learned automatically instead of needing to manually define them first.\n\nIn next week’s blog post, I’ll be showing you how to train your first Convolutional Neural Network from scratch using Python — be sure to signup for the PyImageSearch Newsletter using the form below to be notified when the blog post goes live!"
    },
    {
        "link": "https://medium.com/data-science/tensorflow-for-computer-vision-how-to-implement-convolutions-from-scratch-in-python-609158c24f82",
        "document": "TensorFlow for Computer Vision — How to Implement Convolutions From Scratch in Python\n\nConvolutional networks are fun. You saw last week how they improve model performance when compared to vanilla artificial neural networks. But what a convolution actually does to an image? That’s what you’ll learn today.\n\nAfter reading, you’ll know how to write your convolution function from scratch with Numpy. You’ll apply filters such as blurring, sharpening, and outlining to images, and you’ll also learn the role of padding in convolutional layers.\n\nIt’s a lot of work, and we’re doing everything from scratch. Let’s dive in.\n\nDon’t feel like reading? Watch my video instead:"
    },
    {
        "link": "https://medium.com/analytics-vidhya/image-processing-using-opencv-cnn-and-keras-backed-by-tensor-flow-c9adf22bb271",
        "document": "There are various applications of Image processing in computer vision.\n\nImage processing involves manipulating digital images in order to extract additional information. We have seen a lot of evolutions in Computer hardware in the past decade resulting in faster processors and GPUs. That enabled us to solve new and emerging problems using Image processing.\n\nIts applications range from medicine to entertainment, passing by geological processing and remote sensing. Multimedia systems, one of the pillars of the modern information society, rely heavily on digital image processing.\n\nIn this article, we will try to solve a simpler problem using Image processing.\n\nWe will try to solve a classical classification problem. Let’s assume we are dealing with the garment industry. After each production unit, we need to validate if the units are ready to sell, which in turn involves identifying any defects in the produced clothes.\n\nLet's try to identify two types of defects in clothes:\n\nAfter each production, the idea is to pass the images of the produced clothes to a system which will categorize if it contains one of the mentioned defects.\n\nA digital image is a 2-D matrix of pixels of different values.\n\nAll images consist of pixels which are the raw building blocks of images. Images are made of pixels in a grid. A 640 x 480 image has 640 columns (the width) and 480 rows (the height). There are 640 * 480 = 307200 pixels in an image with those dimensions.\n• Output in which either you can alter an image or make some analysis out of it.\n\nWe are going to use the library for all the image pre-processing tasks. reads data from a contiguous memory location. For the sole purpose of that, we are going to use format for reading and writing the image data.\n\nWe will briefly touch upon all the needed tools/ libraries for better understanding.\n\nThe format can be thought of as a file system contained and described within one single file. Think about the files and folders stored on your computer. However in an file, what we call “directories” or “ folders” on our computers, is called and what we call files on our computer are called .\n\nFor our use-case, we will store all the images in the HDF5 format organizing them into different folders based on the type and category that the image belongs to.\n\nIt is an open-source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in commercial products.\n\nWe will use library for resizing the images and creating feature vectors out of it, that can be achieved by converting the image data to arrays.\n\nWe will use one of the extensions of Deep Neural Nets named (Convolutional Neural Network) for training the model.\n\nOne of the important aspects of solving any problem using Machine learning is to extract features out of the entity set. In the case of Image processing, the feature set is essentially each pixel which the image is constructed of.\n\nThe feature set depends upon the resolution and size of the image.\n\nThe number of pixels in One Megabyte depends on the color mode of the picture.\n• In an 8-bit (256 colors) picture, there are 1048576, or 1024 X 1024 pixels in one megabyte.\n• 48-bit picture, one megabyte has only 174960 (486 X 360) pixels.\n\nFor the classification problem, we are going to identify the boundaries/edges within images to classify them in one of the mentioned categories. So basically we are going to solve an Edge detection problem using CNN.\n\nConvolutional layers are the major building blocks used in convolutional neural networks.\n\nConvolution is the simple application of a filter to an input that results in an activation. Repeated application of the same filter to an input results in a map of activations called a feature map, indicating the locations and strength of a detected feature in input, such as an image.\n\nThe innovation of convolutional neural networks is the ability to automatically learn a large number of filters in parallel specific to a training dataset under the constraints of a specific predictive modeling problem, such as image classification. The result is highly specific features that can be detected anywhere on input images.\n\nTo understand we need to understand how convolutions work. Let's take an image represented as a 5x5 matrix of value, with each cell representing a single pixel. Then you can take a 3x3 matrix and slide a 3x3 window around the image. At each position, the 3x3 matrix visit on the image we matrix multiply it with the values at the current image position.\n\nIn a nutshell, convolution works as following.\n\nThe window that moves is called Kernel.\n\nThe distance that the window moves each time is called stride.\n\nThe goal of a convolutional layer is filtering. As we move over an image we effectively check for patterns in that section of the image. This works because of filters, stacks of weights represented as a vector, which are multiplied by the values output by the convolution.\n\nA typical architecture of CNN involves the following components.\n\nPooling works similar to convolution, the difference is the function that is applied to the kernel and the image window isn’t linear.\n\nThe most common pooling functions are Max pooling and Average pooling. Max pooling takes the max value from the window, while average pooling takes the average of all the values in the window.\n\nRELU is an activation function, that squash the values into a range, typically [0,1] or [-1,1].\n\nSoftmax is a probabilistic function that allows us to express our inputs as a discrete probability distribution.\n\nThe implementation will constitute the following steps:\n• Label the data and store it in an HDF5 file format.\n\nWe will use Google image search for finding the images we are looking for. Let's write a function which will collect the links of the search results.\n\nSearch the images in Google image search, then run the following script in the javascript console in your browser. This will store the links of all the images in a text file named .\n\nExecute the following python script to save all the images into a local drive, whose links are collected in . We will use python’s module to store the images in a directory.\n\nRun the script with the following CMD line options. .\n\nOnce we collected the training data, the next step is to pre-process it.\n\nLabel the data and store it in an HDF5 file format\n• Labeling the data - Assuming images of the same category are stored in the same folder. We can have two folders say and . Label the images in and .\n• Compress the data, shuffle it and store it into a batch file using and .\n• Compute the training mean, subtract it from each image, and create one-hot encoding\n\nThe following script will execute the steps 1 to 3. As a result, will create an file from the training data.\n\nSo far we have generated the training data and brought it to the format which can be feed to a training model.\n\nFinally, let's train the data using for generating the model.\n\nRefer to the Github link for complete implementation.\n\nI hope you found this article helpful. Thanks for taking out time to read it.\n\nHappy Coding !!"
    },
    {
        "link": "https://stackoverflow.com/questions/61861533/are-these-advisable-best-practices-for-convolutional-neural-networks",
        "document": "1) If input set has small details, have a smaller filter size. -> Yes, you can use the most 3x3 common filter or if need be then 2x2. But, still you will have to go through a lot of prototyping. An added fact to this, the number of filters is directly proportional to the position of layers. So, in first layer, have 16, then next layers 32 and so on.\n\n2) Qty is dependent on how many small details. -> You don't know the number of small details. Neural networks are effectively a black-box, there is no rigorous mathematical way to define neural networks till now.\n\n3) Always have a maxpool layer for showing the output layers the most important parts of the image. -> By that logic, you would just put MaxPool at the end of the network. Usually, have a MaxPool, after 2-3 Conv layers. The point of MaxPool is to highlight the important features but also at the same time to reduce parameter space. Using it sparingly defeats it purpose.\n\n4) If accuracy between the last few epochs is very small, and training time is long, consider increasing learning rate and/or decreasing epochs. -> Accuracy usually slows increases towards the end of training. Even in learning rate schedules, you decrease the learning rate w.r.t to increasing epochs. Don't expect big jumps in accuracy towards the end of training. Try to find a good learning rate and train till satisfaction w.r.t validation set.\n\n5) If dropout is to be used, use it between two dense layers, preferably right before the output layer. -> The point of dropout is to give a regularization effect. You can use dropout in Conv layers also. Use dropout to reduce overfitting, the place doesn't matter. The effectiveness of using dropout in Dense layers should be more as they are towards the end of network holding the weights that result into your prediction but dropout should be used throughout the network if possible. Or you can use regularizer on layer weights."
    },
    {
        "link": "https://analyticsvidhya.com/blog/2021/08/beginners-guide-to-convolutional-neural-network-with-implementation-in-python",
        "document": "Beginners Guide to Convolutional Neural Network with Implementation in Python\n\nWe have learned about the Artificial Neural network and its application in the last few articles. This blog will be all about another Deep Learning model which is the Convolutional Neural Network. As always, this will be a beginner’s guide written in a way that newcomers to the data science field can easily understand the concept. So, keep reading! 🙂\n\nThis article was published as a part of the Data Science Blogathon\n\nConvolutional Neural Network is a Deep Learning algorithm specially designed for working with Images and videos. It takes images as inputs, extracts and learns the features of the image, and classifies them based on the learned features.\n\nThis algorithm is inspired by the working of a part of the human brain which is the Visual Cortex. The visual Cortex is a part of the human brain which is responsible for processing visual information from the outside world. It consists of various layers, with each layer serving a specific function by extracting information from the image or visual. Ultimately, all the information gathered from each layer combines to interpret or classify the image or visual.\n\nSimilarly, CNN utilizes various filters, with each filter extracting specific information from the image, such as edges and different shapes (vertical, horizontal, round). These extracted features combine to help identify the image.\n\nNow, the question here can be: Why can’t we use Artificial Neural Networks for the same purpose? This is because there are some disadvantages with ANN:\n• It is too much computation for an ANN model to train large-size images and different types of image channels.\n• The next disadvantage is that it is unable to capture all the information from an image whereas a CNN model can capture the spatial dependencies of the image.\n• Another reason is that ANN is sensitive to the location of the object in the image i.e if the location or place of the same object changes, it will not be able to classify properly.\n\nThe CNN model works in two steps: feature extraction and Classification\n\nFeature extraction is a phase where various filters and layers apply to the images to extract information and features. Once this process is complete, the extracted data moves to the next phase, classification, where it is classified based on the target variable of the problem.\n\nA typical CNN model looks like this:\n\nLet’s learn about each layer in detail.\n\nAs the name says, it’s our input image and can be Grayscale or RGB. Every image is made up of pixels that range from 0 to 255. We need to normalize them i.e convert the range between 0 to 1 before passing it to the model.\n\nBelow is the example of an input image of size 4*4 and has 3 channels i.e RGB and pixel values.\n\nThe convolution layer applies the filter to the input image to extract or detect its features. The filter processes the image multiple times, creating a feature map that aids in classifying the input image. Let’s understand this with the help of an example. For simplicity, we will take a 2D input image with normalized pixels.\n\nIn the above figure, we have an input image of size 6*6 and applied a filter of 3*3 on it to detect some features. In this example, we have applied only one filter but in practice, many such filters are applied to extract information from the image.\n\nThe result of applying the filter to the image is that we get a Feature Map of 4*4 which has some information about the input image. Many such feature maps are generated in practical applications.\n\nLet’s get into some maths behind getting the feature map in the above image.\n\nAs presented in the above figure, in the first step, the filter applies to the green highlighted part of the image. The pixel values of the image multiply with the filter values (as shown in the figure using lines) and then sum up to produce the final value.\n\nIn the next step, shift the filter by one column, as illustrated in the figure below. This movement to the next column or row is known as stride. In this example, we take a stride of 1, meaning we shift by one column.\n\nSimilarly, the filter passes over the entire image and we get our final Feature Map. Once we get the feature map, an activation function is applied to it for introducing nonlinearity.\n\nA point to note here is that the Feature map we get is smaller than the size of our image. As we increase the value of stride the size of the feature map decreases.\n\nThe pooling layer follows the convolutional layer and reduces the dimensions of the feature map, helping to preserve important information and features of the input image while also reducing computation time.\n\nThe most common types of Pooling are Max Pooling and Average Pooling. The below figure shows how Max Pooling works. Using the Feature map which we got from the above example to apply Pooling. Here we are using a Pooling layer of size 2*2 with a stride of 2.\n\nThe process takes the maximum value from each highlighted area, resulting in a new version of the input image with a size of 2×2. After applying pooling, the dimension of the feature map reduces.\n\nTill now we have performed the Feature Extraction steps, now comes the Classification part. The Fully connected layer (as we have in ANN) is used for classifying the input image into a label. This layer connects the information extracted from the previous steps (i.e Convolution layer and Pooling layers) to the output layer and eventually classifies the input into the desired label.\n\nThe complete process of a CNN model can be seen in the below image.\n\nHow to Implement CNN in Python?\n\nThis blog covers some important elements of CNN, but many topics remain, such as padding, data augmentation, and more details on stride. Since deep learning is a vast and ever-evolving field, I will discuss these topics in future blogs. I hope you found this article helpful and worth your time investing on.\n\nIn the next few blogs, you can expect a detailed implementation of CNN with explanations and concepts like Data augmentation and Hyperparameter tuning.\n\nI am Deepanshi Dhingra currently working as a Data Science Researcher, and possess knowledge of Analytics, Exploratory Data Analysis, Machine Learning, and Deep Learning. Feel free to content with me on LinkedIn for any feedback and suggestions.\n\nThe media shown in this article does not belong to Analytics Vidhya, and the author uses it at their discretion."
    }
]