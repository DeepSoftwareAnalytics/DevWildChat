[
    {
        "link": "https://geeksforgeeks.org/linear-regression-python-implementation",
        "document": "Linear regression is a statistical method that is used to predict a continuous dependent variable i.e target variable based on one or more independent variables. This technique assumes a linear relationship between the dependent and independent variables which means the dependent variable changes proportionally with changes in the independent variables.\n\nIn this article we will understand types of linear regression and its implementation in the Python programming language. Linear regression is a statistical method of modeling relationships between a dependent variable with a given set of independent variables.\n\nWe will discuss three types of linear regression:\n• Polynomial linear regression: This involves predicting a dependent variable based on a polynomial relationship between independent and dependent variables.\n\nSimple linear regression is an approach for predicting a response using a single feature. It is one of the most basic and simple machine learning models. In linear regression we assume that the two variables i.e. dependent and independent variables are linearly related. Hence we try to find a linear function that predicts the value (y) with reference to independent variable(x). Let us consider a dataset where we have a value of response y for every feature x:\n\nfor n observations (in the above example, n=10). A scatter plot of the above dataset looks like this:-\n\nNow, the task is to find a line that fits best in the above scatter plot so that we can predict the response for any new feature values. (i.e a value of x not present in a dataset) This line is called a regression line. The equation of the regression line is represented as:\n• None b_0 and b_1 are regression coefficients and represent the y-intercept slope of the regression line respectively.\n\nTo create our model we must “learn” or estimate the values of regression coefficients b_0 and b_1. And once we’ve estimated these coefficients, we can use the model to predict responses!\n\nIn this article we are going to use the principle of Least Squares.\n\nHere, e_i is a residual error in ith observation. So, our aim is to minimize the total residual error. We define the squared error or cost function, J as:\n\nAnd our task is to find the value of b and b for which J(b , b ) is minimum! Without going into the mathematical details, we present the result here:\n\nWhere SS is the sum of cross-deviations of y and x:\n\nAnd SS is the sum of squared deviations of x:\n\nWe can use the Python language to learn the coefficient of linear regression models. For plotting the input data and best-fitted line we will use the matplotlib library. It is one of the most used Python libraries for plotting graphs. Here is the example of simpe Linear regression using Python.\n\nThis function , takes the input data (independent variable) and (dependent variable) and estimates the coefficients of the linear regression line using the least squares method.\n• Calculating Means: compute the mean values of\n• Calculating Cross-Deviation and Deviation about x: calculate the sum of squared deviations between and the sum of squared deviations of about its mean, respectively.\n• Calculating Regression Coefficients: ) of the regression line using the least squares method.\n• Returning Coefficients: The function returns the estimated coefficients as a tuple\n\nThis function , takes the input data (independent variable), (dependent variable) and the estimated coefficients to plot the regression line and the data points.\n• Plotting Scatter Plot: plots the original data points as a scatter plot with red markers.\n• Plotting Regression Line: plots the regression line using the predicted values and the independent variable\n• Adding Labels: and the y-axis as\n\nThe provided code implements simple linear regression analysis by defining a function that performs the following steps:\n• Coefficient Estimation: function to determine the coefficients of the linear regression line using the provided data.\n\nMultiple linear regression attempts to model the relationship between two or more features and a response by fitting a linear equation to the observed data. It is a extension of simple linear regression. Consider a dataset with p features(or independent variables) and one response(or dependent variable). \n\nAlso, the dataset contains n rows/observations.\n\nX (feature matrix) = a matrix of size n X p where x denotes the values of the jth feature for ith observation.\n\ny (response vector) = a vector of size n where y_{i} denotes the value of response for ith observation.\n\n\n\nThe regression line for p features is represented as:\n\nwhere h(x_i) is predicted response value for ith observation and b_0, b_1, …, b_p are the regression coefficients. Also, we can write:\n\nwhere e_i represents a residual error in ith observation. We can generalize our linear model a little bit more by representing feature matrix X as:\n\nSo now, the linear model can be expressed in terms of matrices as:\n\nNow, we determine an estimate of b i.e. b’ using the Least Squares method. As already explained, the Least Squares method tends to determine b’ for which total residual error is minimized.\n\nWe present the result directly here:\n\nwhere ‘ represents the transpose of the matrix while -1 represents the matrix inverse. Knowing the least square estimates, b’, the multiple linear regression model can now be estimated as:\n\nwhere y’ is the estimated response vector.\n\nFor multiple linear regression using Python, we will use the Boston house pricing dataset.\n\nThe code downloads the Boston Housing dataset from the provided URL and reads it into a Pandas DataFrame ( )\n\nThis extracts the input variables (X) and target variable (y) from the DataFrame. The input variables are selected from every other row to match the target variable, which is available every other row.\n\nHere it divides the data into training and testing sets using the function from scikit-learn. The parameter specifies that 40% of the data should be used for testing.\n\nThis initializes a LinearRegression object ( ) and trains the model using the training data ( , )\n\nEvaluates the model’s performance by printing the regression coefficients and calculating the variance score, which measures the proportion of explained variance. A score of 1 indicates perfect prediction.\n\nPlotting and analyzing the residual errors, which represent the difference between the predicted values and the actual values.\n\nIn the above example, we determine the accuracy score using Explained Variance Score. We define:\n\nwhere y’ is the estimated target output, y is the corresponding (correct) target output, and Var is Variance, the square of the standard deviation. The best possible score is 1.0, lower values are worse.\n\nPolynomial Regression is a form of linear regression in which the relationship between the independent variable x and dependent variable y is modeled as an nth-degree polynomial. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y | x).\n\nThe choice of degree for polynomial regression is a trade-off between bias and variance. Bias is the tendency of a model to consistently predict the same value, regardless of the true value of the dependent variable. Variance is the tendency of a model to make different predictions for the same data point, depending on the specific training data used.\n\nA higher-degree polynomial can reduce bias but can also increase variance, leading to overfitting. Conversely, a lower-degree polynomial can reduce variance but can also increase bias.\n\nThere are a number of methods for choosing a degree for polynomial regression, such as cross-validation and using information criteria such as Akaike information criterion (AIC) or Bayesian information criterion (BIC).\n\nHere we will import all the necessary libraries for data analysis and machine learning tasks and then loads the ‘Position_Salaries.csv‘ dataset using Pandas. It then prepares the data for modeling by handling missing values and encoding categorical data. Finally, it splits the data into training and testing sets and standardizes the numerical features using StandardScaler.\n\nThe code creates a linear regression model and fits it to the provided data, establishing a linear relationship between the independent and dependent variables.\n\nThe code performs quadratic and cubic regression by generating polynomial features from the original data and fitting linear regression models to these features. This enables modeling nonlinear relationships between the independent and dependent variables.\n\nThe code creates a scatter plot of the data point, It effectively visualizes the linear relationship between position level and salary.\n\nThe code creates a scatter plot of the data points, overlays the predicted quadratic and cubic regression lines. It effectively visualizes the nonlinear relationship between position level and salary and compares the fits of quadratic and cubic regression models.\n\nThe code effectively visualizes the relationship between position level and salary using cubic regression and generates a continuous prediction line for a broader range of position levels.\n\nHow to use linear regression to make predictions?\n\n3. How to perform linear regression in Python?\n\nWhat are some applications of linear regression?\n\n5. How linear regression is implemented in sklearn?"
    },
    {
        "link": "https://medium.com/codex/stock-predication-using-regression-algorithm-in-python-fb8b426453b9",
        "document": "For this exercise, I will use the Yfinance library for scrapping information right away from Yahoo Finance website, Yahoo Finance is a great website that gives a quick glimpse of listed equities, funds, and investment data for the company you are looking for. Also, I will test and predict certain parameters and stock prices by implementing regression model analysis on the data.\n\nFirstly, we will the required libraries for this exercise to be executed successfully. The main library to call and pay attention to here is yfinance. This library will enable us to extract and call different data from the Yahoo website\n\nIn this section, We will use certain methods to call data related to company description, stock prices, close prices, volumes, and corporate actions associated with the stock.\n\n{'zip': '201804',\n\n 'sector': 'Consumer Cyclical',\n\n 'fullTimeEmployees': 7763,\n\n 'longBusinessSummary': 'NIO Inc. designs, develops, manufactures, and sells smart electric vehicles in Mainland China, Hong Kong, the United States, the United Kingdom, and Germany. The company offers five, six, and seven-seater electric SUVs, as well as smart electric sedans. It is also involved in the provision of energy and service packages to its users; marketing, design, and technology development activities; manufacture of e-powertrains, battery packs, and components; and sales and after sales management activities. In addition, the company offers power solutions, including Power Home, a home charging solution; Power Swap, a battery swapping service; Public Charger, a public fast charging solution; Power Mobile, a mobile charging service through charging vans; Power Map, an application that provides access to a network of public chargers and their real-time information; and One Click for Power valet service, where it offers vehicle pick up, charging, and return services. Further, it provides repair, maintenance, and bodywork services through its NIO service centers and authorized third-party service centers; statutory and third-party liability insurance, and vehicle damage insurance through third-party insurers; courtesy car services; and roadside assistance, as well as data packages; and auto financing services. Additionally, the company offers NIO Certified, an used vehicle inspection, evaluation, acquisition, and sales service. NIO Inc. has a strategic collaboration with Mobileye N.V. for the development of automated and autonomous vehicles for consumer markets. The company was formerly known as NextEV Inc. and changed its name to NIO Inc. in July 2017. NIO Inc. was founded in 2014 and is headquartered in Shanghai, China.',\n\n 'city': 'Shanghai',\n\n 'phone': '86 21 6908 2018',\n\n 'country': 'China',\n\n 'companyOfficers': [],\n\n 'website': 'http://www.nio.com',\n\n 'maxAge': 1,\n\n 'address1': 'Building 20',\n\n 'industry': 'Auto Manufacturers',\n\n 'address2': 'No. 56 AnTuo Road Anting Town Jiading District',\n\n 'previousClose': 38.12,\n\n 'regularMarketOpen': 37.96,\n\n 'twoHundredDayAverage': 43.683704,\n\n 'trailingAnnualDividendYield': None,\n\n 'payoutRatio': 0,\n\n 'volume24Hr': None,\n\n 'regularMarketDayHigh': 38,\n\n 'navPrice': None,\n\n 'averageDailyVolume10Day': 70446940,\n\n 'totalAssets': None,\n\n 'regularMarketPreviousClose': 38.12,\n\n 'fiftyDayAverage': 41.888824,\n\n 'trailingAnnualDividendRate': None,\n\n 'open': 37.96,\n\n 'toCurrency': None,\n\n 'averageVolume10days': 70446940,\n\n 'expireDate': None,\n\n 'yield': None,\n\n 'algorithm': None,\n\n 'dividendRate': None,\n\n 'exDividendDate': None,\n\n 'beta': 2.613181,\n\n 'circulatingSupply': None,\n\n 'startDate': None,\n\n 'regularMarketDayLow': 36.76,\n\n 'priceHint': 2,\n\n 'currency': 'USD',\n\n 'regularMarketVolume': 52839807,\n\n 'lastMarket': None,\n\n 'maxSupply': None,\n\n 'openInterest': None,\n\n 'marketCap': 60854632448,\n\n 'volumeAllCurrencies': None,\n\n 'strikePrice': None,\n\n 'averageVolume': 97445091,\n\n 'priceToSalesTrailing12Months': 3.743073,\n\n 'dayLow': 36.76,\n\n 'ask': 37.43,\n\n 'ytdReturn': None,\n\n 'askSize': 3000,\n\n 'volume': 52839807,\n\n 'fiftyTwoWeekHigh': 66.99,\n\n 'forwardPE': -371.4,\n\n 'fromCurrency': None,\n\n 'fiveYearAvgDividendYield': None,\n\n 'fiftyTwoWeekLow': 2.88,\n\n 'bid': 37.21,\n\n 'tradeable': False,\n\n 'dividendYield': None,\n\n 'bidSize': 1400,\n\n 'dayHigh': 38,\n\n 'exchange': 'NYQ',\n\n 'shortName': 'NIO Inc.',\n\n 'longName': 'NIO Inc.',\n\n 'exchangeTimezoneName': 'America/New_York',\n\n 'exchangeTimezoneShortName': 'EDT',\n\n 'isEsgPopulated': False,\n\n 'gmtOffSetMilliseconds': '-14400000',\n\n 'quoteType': 'EQUITY',\n\n 'symbol': 'NIO',\n\n 'messageBoardId': 'finmb_311626862',\n\n 'market': 'us_market',\n\n 'annualHoldingsTurnover': None,\n\n 'enterpriseToRevenue': 2.107,\n\n 'beta3Year': None,\n\n 'profitMargins': -0.34511003,\n\n 'enterpriseToEbitda': -9.618,\n\n '52WeekChange': 11.664452,\n\n 'morningStarRiskRating': None,\n\n 'forwardEps': -0.1,\n\n 'revenueQuarterlyGrowth': None,\n\n 'sharesOutstanding': 1638520064,\n\n 'fundInceptionDate': None,\n\n 'annualReportExpenseRatio': None,\n\n 'bookValue': 17.315,\n\n 'sharesShort': 65406021,\n\n 'sharesPercentSharesOut': 0.0399,\n\n 'fundFamily': None,\n\n 'lastFiscalYearEnd': 1609372800,\n\n 'heldPercentInstitutions': 0.36421,\n\n 'netIncomeToCommon': -5610789888,\n\n 'trailingEps': -0.724,\n\n 'lastDividendValue': None,\n\n 'SandP52WeekChange': 0.45070732,\n\n 'priceToBook': 2.1449609,\n\n 'heldPercentInsiders': 0.00533,\n\n 'nextFiscalYearEnd': 1672444800,\n\n 'mostRecentQuarter': 1609372800,\n\n 'shortRatio': 0.53,\n\n 'sharesShortPreviousMonthDate': 1614297600,\n\n 'floatShares': 1327183283,\n\n 'enterpriseValue': 34251128832,\n\n 'threeYearAverageReturn': None,\n\n 'lastSplitDate': None,\n\n 'lastSplitFactor': None,\n\n 'legalType': None,\n\n 'lastDividendDate': None,\n\n 'morningStarOverallRating': None,\n\n 'earningsQuarterlyGrowth': None,\n\n 'dateShortInterest': 1617148800,\n\n 'pegRatio': 4298.73,\n\n 'lastCapGain': None,\n\n 'shortPercentOfFloat': None,\n\n 'sharesShortPriorMonth': 48084071,\n\n 'impliedSharesOutstanding': None,\n\n 'category': None,\n\n 'fiveYearAverageReturn': None,\n\n 'regularMarketPrice': 37.14,\n\n 'logo_url': 'https://logo.clearbit.com/nio.com'}\n\nThe data from Yahoo Finance is straightforward and reflects real-time data of the stock market, therefore cleaning and processing the exported data will not be a difficult task.\n\nFor this data, I’ve split the data into training and test datasets with a test size of 15% of the total dataset. Afterward, we can simply check if the data was split successfully by using the shape() method.\n\nBefore we get to the technical part of implementing the regression model to the dataset, let’s talk a bit about the regression algorithm. Basically, Regression is a set of techniques for estimating relationships. For example in real life, we can relate the force for stretching a spring and the distance that the spring stretches (the likes in Hooke’s law), or explain how many transistors the semiconductor industry can pack into a circuit over time (Moore’s law).\n\nObviously there are many regression algorithms in Machine learning ,such as Linear regression , Logistic regression and Lasso regression algorithms :\n\nLinear regression is a supervised learning algorithm , where it predict a depended variable or a target based on the given independent variables, so basically you find the relationship between a dependent variable and independent variables.\n\nA logistic regression algorithm is another supervised algorithm that is used to describe data and to explain the relationship between one dependent binary variable and one or more independent variables. The output of using this algorithm is either 0 or 1 .\n\nIn our case we will use linear regression algorithm to predict these stock prices\n\nThe equation for linear regression can be written as follows:\n\nWhere, x1, x2,….xn represents the independent variables while the coefficients θ1, θ2, θn represent the weights.\n\nHere we will compute the coefficient of determination denoted by R², which takes values between 0 and 1, the higher the value R² the more successful the linear regression is at explaining the variation of Y values, in our case the Y values represent the close stock prices of the subjected company. The below is the math behind The coefficient of determination R²\n\nThe coefficient of determination R² for our data is at 0.98 which’s 98%, which means that our model is a linear model that explains the variation of all Y values.\n\nAs we can see below, the predicted list of data points from open, high, low, and vol are not sorted based on time or date, at this point It’s not important to sort these data point, as we will plot is based on their associated dates using scatter plot() method.\n\nThe below table displays a summary statistics values of actual values vs predicted values of the dataset\n\nMAE and RMSE are the most common statistical metrics used to measure continuous variables or in our case the accuracy of our regression models.\n\nThe math behind both Models might be confusing or a bit mouthful to absorb its meaning, but think about it in this easy way, We have actual stock close prices and predicted stock prices computed from the same actual stock prices we talked about, now we need to calculate the error or the difference between them to see how accurate these prediction compared to the actual values at hand.\n\nMAE measures the average magnitude of the errors in a set of predictions, without considering their direction.\n\nRMSE is a quadratic scoring rule that also measures the average magnitude of the error.\n\nMSE Mean squared error (MSE) measures the average of the squares of the errors — that is, the average squared difference between the estimated values and the actual value. MSE is a risk function, corresponding to the expected value of the squared error loss.\n\nAll mentioned metrics above can range from 0 to ∞ and are indifferent to the direction of errors. They are negatively-oriented scores, which means the lower values they present the better. Remember that RMSE will always be larger in value than MSE, Also it can penalize more error-related data so RMSE can be a better measure than MSE.\n\nIn our case our evaluation results are mentioned as following :\n\nAll of our metric results are showing values less than 1, from an interpretation standpoint, I think MAE is a better metric measurement for linear problems than RMSE, as RMSE does not describe average error alone and has other implications that are more difficult to tease out and understand. Also, RMSE gives much more importance to large errors, so models will try to minimize these as much as possible.\n\nThe stock market has been always the hottest topic when it comes to time series forecasting or trying to feel where the market is going overall. It’s impossible to find “to go to” formula to predict the direction of the stock market, because of constant volatility of the market, the uncertainty of moving variables that could impact the stock market volatility from associated risk to political instability and Macroeconomic factors, well the list could go on.\n\nTo have better visibility on where the market is going, relying on regression models and predicting certain values based on past performance is not good enough. The following points should complement a full-fledged regression model report.\n\nFundamental analysis is a method to analyze and predict the company’s intrinsic value based on historical and current performance data, these data are in form of financial statements and balance sheet information. Hence, information can be analyzed to compute the company’s current multiples such as P/E, P/B, liquidity ratios, debt ratios, Return ratios, Margins, etc. This information can give you a solid conviction on the direction of the company and help you make critical decisions either to consider investing in the company or not.\n\nTechnical analysis is the method of using statistical methods and trends based on historical data, for example, daily total volume or value of a traded stock, and evaluate historical patterns to predict future stock price movement.\n\nBasically, Sentiment Analysis is the use of high-end Natural language processing to determine whether the given textual data is positive, negative, or neutral. You might conduct this analysis in paragraphs, a large set of writing textual data, reviews from your customer, research thesis, scientific papers, etc. In our case, you might use this method to analyze a Twitter account for the subject company or review from its Facebook account and so on."
    },
    {
        "link": "https://spotintelligence.com/2024/10/10/linear-regression-in-machine-learning-made-simple-how-to-python-tutorial",
        "document": "What is Linear Regression in Machine Learning?\n\nLinear regression is one of the fundamental techniques in machine learning and statistics used to understand the relationship between one or more independent variables (also known as features or predictors) and a dependent variable (the outcome we want to predict). By establishing a linear relationship, linear regression helps us make predictions and infer patterns from data.\n\nLinear regression is a supervised learning algorithm that models the relationship between variables by fitting a linear equation to the observed data. The goal is to find the best-fitting line (or hyperplane in higher dimensions) that can be used to predict outcomes for new data points.\n\nThe relationship between the independent variable(s) and the dependent variable can be expressed mathematically as:\n• y represents the dependent variable (the value we want to predict).\n• x denotes the independent variable (the feature used for prediction).\n• m is the slope of the line, which indicates the change in y for a one-unit change in x.\n• b is the y-intercept, which represents the value of y when x is zero.\n\nIn the case of multiple linear regression, where there are numerous independent variables, the equation expands to:\n• b1,b2,…,bn​ are the coefficients corresponding to each independent variable x1,x2,…,xn​.\n• Involves one independent variable and one dependent variable.\n• The model attempts to establish a straight-line relationship between the two variables.\n\nExample: Predicting a person’s weight based on their height.\n• Involves two or more independent variables that predict a single dependent variable.\n• It allows us to account for the influence of various factors on the outcome.\n\nExample: Predicting housing prices based on size, number of bedrooms, location, and age of the house.\n\nLinear regression is a powerful and intuitive tool for modelling relationships in data. Its simplicity and interpretability make it popular for beginners and experienced data scientists. The following sections will delve deeper into how linear regression works, its implementation, advantages, limitations, and real-world applications. Understanding these concepts will provide a solid foundation for leveraging linear regression in machine learning projects.\n\nLinear regression is based on a few fundamental principles allowing it to effectively model relationships between variables. This section will cover the concepts of the line of best fit, loss function, gradient descent, and the assumptions underlying linear regression.\n\nThe line of best fit, or regression line, is the core of linear regression. It represents the predicted relationship between the independent variable(s) and the dependent variable. Linear regression aims to find the line that minimises the discrepancies between the predicted and observed values in the dataset.\n\nTo visualise this, consider a scatter plot of data points, where each point represents an observation with its corresponding independent and dependent variable values. The regression line is fitted through the data points to minimise the vertical distances (errors) between the points and the line itself. The following formula for the distance of each point from the line can mathematically represent this:\n\nwhere y is the actual value, and y^​ is the predicted value from the regression line.\n\nTo evaluate the performance of a linear regression model, we need a way to quantify how well the line fits the data. The most commonly used loss function for linear regression is the Mean Squared Error (MSE), which measures the average of the squares of the errors:\n• n is the number of observations,\n• Actual_i​ is the actual value,\n• Predicted_​i​ is the predicted value for the ith observation.\n\nMinimising the MSE helps us find the optimal parameters (coefficients) for our regression equation that best fits the data.\n\nOnce we define the loss function, the next step is to optimise the model parameters. This is where gradient descent comes into play. Gradient descent is an iterative optimisation algorithm that minimises the loss function by updating the parameters (slope and intercept) in the direction that reduces the error.\n\nThe update rule for gradient descent can be expressed as follows:\n• α is the learning rate, a hyperparameter that determines the size of the steps taken towards the minimum.\n\nThe model converges toward the optimal coefficients that minimise the loss function by repeatedly applying this update rule.\n\nFor linear regression to be effective, certain assumptions must hold. These include:\n• Linearity: The relationship between the independent variable(s) and the dependent variable should be linear. This means that changes in the predictor(s) result in proportional changes in the response variable.\n• Homoscedasticity: The variance of the errors (residuals) should be constant across all levels of the independent variable(s). If the spread of errors varies, it can lead to inefficiencies in estimates.\n• No Multicollinearity: The independent variables should not be highly correlated in multiple linear regression. High multicollinearity can lead to instability in coefficient estimates.\n• Independence of Errors: The residuals (errors) should be independent. This means that the error associated with one observation does not depend on the errors of others.\n• Normally Distributed Errors: While not strictly necessary for prediction, normally distributed errors are essential for conducting hypothesis tests on the coefficients.\n\nUnderstanding how linear regression works—through the line of best fit, loss function, gradient descent, and underlying assumptions—is crucial for effectively applying this technique to real-world problems. With a solid grasp of these concepts, you will be well-prepared to implement linear regression in various data-driven scenarios, which we will explore in the following sections.\n\nImplementing linear regression involves steps that transform raw data into predictive models. This section will outline the critical steps for building a linear regression model, from data collection to evaluation, along with a practical coding example to illustrate the process.\n• Data Collection: The first step in any machine learning project is to gather relevant data. The quality and quantity of data are crucial for building an effective model. Data can be collected from various sources, including public datasets, APIs, or by conducting surveys. Standard datasets for linear regression include housing prices, sales data, and stock prices.\n• Data Preprocessing: Data preprocessing is essential to prepare the dataset for analysis. Key tasks in this step include:\n• Handling Missing Values: Missing data can bias model predictions. Techniques for addressing missing values include imputation (replacing missing values with the mean, median, or mode) or removing records with missing data.\n• Feature Scaling: For linear regression, it is often helpful to scale features to ensure that they contribute equally to the model’s performance. Common methods include normalisation (scaling to a range of 0 to 1) and standardisation (scaling with a mean of 0 and a standard deviation of 1).\n• Splitting Data: To assess the model’s performance, the dataset is typically divided into two parts: a training set and a testing set. A common split is 80% for training and 20% for testing. The training set fits the model, while the testing set evaluates its performance on unseen data.\n• Model Training: We can train the linear regression model with the preprocessed data. This step involves fitting the model to the training data by estimating the coefficients that minimise the loss function (Mean Squared Error).\n• Model Evaluation: After training the model, evaluating its performance on the testing set is essential. Common metrics used to assess the model include:\n• Mean Absolute Error (MAE): The average absolute difference between the predicted and actual values.\n• Mean Squared Error (MSE): As previously mentioned, the average of the squared differences between predicted and actual values.\n• R-squared (R²): A statistical measure that indicates how well the independent variables explain the variability of the dependent variable. An R² value of 1 indicates perfect prediction, while a value of 0 suggests no predictive power.\n\nHow To Implement Linear Regression In Python\n\nHere is a simple example of implementing linear regression using Python with the Scikit-learn library. This example will predict housing prices based on the size of the house.\n\nImplementing linear regression involves several critical steps, from data collection to model evaluation. You can successfully build and evaluate a linear regression model by following these steps and utilising the provided coding example. Understanding this process is essential for leveraging linear regression effectively in various applications, setting the stage for deeper exploration into more complex models and techniques in the subsequent sections.\n\nAdvantages and Limitations of Linear Regression in Machine Learning\n\nLinear regression is a powerful and widely used statistical method, particularly in machine learning. While it offers numerous benefits, it also has some limitations that practitioners should consider when deciding whether to use it for a given problem. This section will outline the key advantages and limitations of linear regression.\n• Simplicity and Interpretability: One of the most significant advantages of linear regression is its simplicity. The model is straightforward to understand and implement, making it an excellent choice for beginners. The model’s coefficients can be directly interpreted, providing insights into the relationship between the independent and dependent variables.\n• Computational Efficiency: Linear regression is computationally efficient compared to more complex algorithms, making it suitable for large datasets. The training process generally requires less time and fewer resources, allowing for faster model development and deployment.\n• Good Performance with Linearly Separable Data: Linear regression works well when the relationship between the features and the target variable is linear. If the underlying data distribution aligns closely with this assumption, linear regression can yield accurate predictions.\n• Foundation for More Complex Models: Linear regression is a fundamental building block for more advanced machine learning techniques. Understanding linear regression lays the groundwork for grasping concepts like polynomial regression, regularisation techniques (Ridge and Lasso), and various generalised linear models.\n• Wide Applicability: Linear regression can be applied to various domains, including finance (e.g., stock price predictions), healthcare (e.g., disease progression modelling), and marketing (e.g., customer behaviour analysis). Its versatility makes it a valuable tool in a data scientist’s toolkit.\n• Sensitivity to Outliers: Linear regressOutliers Since they can significantly affect linear regressions’ minimisation of squared errors, a few extreme values can disproportionately influence the fitted line, leading to misleading predictions and interpretations.\n• Assumption of Linearity: Linear regression assumes a linear relationship between the independent and dependent variables. When the relationship is non-linear, the model may not capture the underlying patterns in the data, resulting in poor predictions.\n• Multicollinearity Issues: In multiple linear regression, if independent variables are highly correlated (multicollinearity), it can lead to inflated standard errors of the coefficients. This makes it challenging to determine the individual effect of each predictor on the dependent variable.\n• Limited to Predicting a Single Continuous Outcome: Linear regression is primarily designed for predicting a single continuous outcome variable. While it can handle multiple predictors, it does not naturally extend to problems involving classification or more complex types of predictions without modifications.\n• Assumption of Homoscedasticity: Linear regression assumes that the variance of the residuals is constant across all levels of the independent variable(s). If this assumption is violated (heteroscedasticity), it can affect the reliability of statistical tests and predictions.\n\nWhile linear regression has numerous advantages, including simplicity, interpretability, and computational efficiency, it also has limitations that can impact its effectiveness. Understanding these strengths and weaknesses is crucial for data scientists and practitioners to make informed decisions about when to use linear regression and when to consider alternative modelling approaches. In the following sections, we will explore real-world applications of linear regression and demonstrate how to leverage its strengths in practice.\n\nLinear regression is a versatile statistical technique widely used across various industries and fields. Its ability to model relationships between variables makes it suitable for numerous practical applications. This section will explore several real-world use cases where linear regression has proven effective.\n\nOne of the most common applications of linear regression is in the real estate industry, where it is used to predict housing prices based on various features such as size, number of bedrooms, location, and age of the property. Real estate agents, buyers, and sellers can make informed decisions about pricing and investments by analysing historical sales data. For example, a linear regression model might reveal that each additional square foot increases the house price by a specific dollar amount, helping stakeholders understand market dynamics.\n\nBusinesses often use linear regression to forecast future sales based on historical sales data and other relevant factors such as marketing expenditures, seasonal trends, and economic indicators. By modelling the relationship between these variables, companies can better plan their inventory, budget for marketing campaigns, and set sales targets. For instance, a retail store might analyse past sales data with promotional spending to predict future sales during the holiday season.\n\nIn healthcare, linear regression is used to analyse the relationships between various medical conditions and patient characteristics. Researchers might study how different factors—such as age, weight, and lifestyle—affect the progression of diseases like diabetes or hypertension. For example, a linear regression model could help predict a patient’s blood pressure based on their body mass index (BMI) and other lifestyle factors, allowing healthcare providers to identify at-risk patients and develop tailored treatment plans.\n\nIn finance, linear regression is commonly used to model relationships between financial metrics, such as predicting stock prices based on historical performance, market trends, and macroeconomic indicators. For instance, an analyst might use linear regression to assess the relationship between a company’s earnings and stock price, helping investors make informed decisions. Additionally, regression analysis can aid in risk assessment, portfolio management, and capital budgeting.\n\nMarketers frequently apply linear regression to evaluate the effectiveness of various advertising channels and strategies. Marketers can identify the most effective channels by analysing data on customer acquisition costs, conversion rates, and sales generated from different campaigns and allocate resources accordingly. For example, a company may use linear regression to analyse the relationship between social media ad spending and the number of new customers acquired, allowing for optimised marketing budgets and strategies.\n\nIn environmental science, linear regression is used to study the relationships between environmental variables and their impacts. Researchers might investigate how temperature, precipitation, and pollution affect biodiversity or ecosystem health. For example, a study might use linear regression to predict the impact of climate change on species distribution, helping policymakers make data-driven decisions about conservation efforts.\n\nLinear regression is a powerful tool with real-world applications across various fields, from predicting housing prices to analysing healthcare outcomes. Its simplicity, interpretability, and effectiveness in modelling relationships between variables make it an essential technique in data analysis. Understanding these applications enables practitioners to leverage linear regression effectively in their work, ultimately leading to better decision-making and improved outcomes in diverse domains. In the final section, we will summarise the key points discussed and guide the next steps for those looking to deepen their understanding of linear regression and its applications.\n\nIn conclusion, linear regression is a fundamental and widely used technique in machine learning and statistics that provides valuable insights into the relationships between variables. This blog post has explored various aspects of linear regression, including its definition, underlying principles, implementation steps, advantages and limitations, and real-world applications.\n\nLinear regression stands out for its simplicity, interpretability, and computational efficiency, making it an excellent choice for beginners and seasoned data scientists. Its versatility allows it to be applied across diverse fields such as real estate, finance, healthcare, marketing, and environmental science, enabling stakeholders to make data-driven decisions and predict outcomes effectively.\n\nHowever, while linear regression offers many advantages, its limitations include its sensitivity to outliers and assumptions of linearity and homoscedasticity. Understanding these constraints is essential for practitioners to ensure that linear regression is the right tool.\n\nAs you embark on your journey with linear regression, consider the various applications discussed and experiment with real-world datasets to solidify your understanding. Explore more complex models and techniques to expand your toolkit further. By mastering linear regression, you will lay a strong foundation for tackling more advanced statistical and machine-learning challenges in the future.\n\nWhether you are analysing housing prices, forecasting sales, or studying environmental impacts, the skills you gain from linear regression will be invaluable in navigating the data-driven landscape. Embrace the opportunities that lie ahead, and let the power of linear regression guide your analytical endeavours!"
    },
    {
        "link": "https://interactivebrokers.com/campus/ibkr-quant-news/linear-regression-on-market-data-implemented-from-scratch-in-python-and-r-part-i",
        "document": ""
    },
    {
        "link": "https://realpython.com/linear-regression-in-python",
        "document": "Linear regression is a foundational statistical tool for modeling the relationship between a dependent variable and one or more independent variables. It’s widely used in data science and machine learning to predict outcomes and understand relationships between variables. In Python, implementing linear regression can be straightforward with the help of third-party libraries such as scikit-learn and statsmodels.\n\nBy the end of this tutorial, you’ll understand that:\n• Linear regression is a statistical method for modeling the relationship between a dependent variable and one or more independent variables by fitting a linear equation.\n• Implementing linear regression in Python involves using libraries like scikit-learn and statsmodels to fit models and make predictions.\n• The formula for linear regression is 𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽ᵣ𝑥ᵣ + 𝜀, representing the linear relationship between variables.\n• Simple linear regression involves one independent variable, whereas multiple linear regression involves two or more.\n• The scikit-learn library provides a convenient and efficient interface for performing linear regression in Python.\n\nTo implement linear regression in Python, you typically follow a five-step process: import necessary packages, provide and transform data, create and fit a regression model, evaluate the results, and make predictions. This approach allows you to perform both simple and multiple linear regressions, as well as polynomial regression, using Python’s robust ecosystem of scientific libraries.\n\nRegression analysis is one of the most important fields in statistics and machine learning. There are many regression methods available. Linear regression is one of them. Regression searches for relationships among variables. For example, you can observe several employees of some company and try to understand how their salaries depend on their features, such as experience, education level, role, city of employment, and so on. This is a regression problem where data related to each employee represents one observation. The presumption is that the experience, education, role, and city are the independent features, while the salary depends on them. Similarly, you can try to establish the mathematical dependence of housing prices on area, number of bedrooms, distance to the city center, and so on. Generally, in regression analysis, you consider some phenomenon of interest and have a number of observations. Each observation has two or more features. Following the assumption that at least one of the features depends on the others, you try to establish a relation among them. In other words, you need to find a function that maps some features or variables to others sufficiently well. The dependent features are called the dependent variables, outputs, or responses. The independent features are called the independent variables, inputs, regressors, or predictors. Regression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, or brand. It’s a common practice to denote the outputs with 𝑦 and the inputs with 𝑥. If there are two or more independent variables, then they can be represented as the vector 𝐱 = (𝑥₁, …, 𝑥ᵣ), where 𝑟 is the number of inputs. When Do You Need Regression? Typically, you need regression to answer whether and how some phenomenon influences the other or how several variables are related. For example, you can use it to determine if and to what extent experience or gender impacts salaries. Regression is also useful when you want to forecast a response using a new set of predictors. For example, you could try to predict electricity consumption of a household for the next hour given the outdoor temperature, time of day, and number of residents in that household. Regression is used in many different fields, including economics, computer science, and the social sciences. Its importance rises every day with the availability of large amounts of data and increased awareness of the practical value of data.\n\nLinear regression is probably one of the most important and widely used regression techniques. It’s among the simplest regression methods. One of its main advantages is the ease of interpreting results. When implementing linear regression of some dependent variable 𝑦 on the set of independent variables 𝐱 = (𝑥₁, …, 𝑥ᵣ), where 𝑟 is the number of predictors, you assume a linear relationship between 𝑦 and 𝐱: 𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽ᵣ𝑥ᵣ + 𝜀. This equation is the regression equation. 𝛽₀, 𝛽₁, …, 𝛽ᵣ are the regression coefficients, and 𝜀 is the random error. Linear regression calculates the estimators of the regression coefficients or simply the predicted weights, denoted with 𝑏₀, 𝑏₁, …, 𝑏ᵣ. These estimators define the estimated regression function 𝑓(𝐱) = 𝑏₀ + 𝑏₁𝑥₁ + ⋯ + 𝑏ᵣ𝑥ᵣ. This function should capture the dependencies between the inputs and output sufficiently well. The estimated or predicted response, 𝑓(𝐱ᵢ), for each observation 𝑖 = 1, …, 𝑛, should be as close as possible to the corresponding actual response 𝑦ᵢ. The differences 𝑦ᵢ - 𝑓(𝐱ᵢ) for all observations 𝑖 = 1, …, 𝑛, are called the residuals. Regression is about determining the best predicted weights—that is, the weights corresponding to the smallest residuals. To get the best weights, you usually minimize the sum of squared residuals (SSR) for all observations 𝑖 = 1, …, 𝑛: SSR = Σᵢ(𝑦ᵢ - 𝑓(𝐱ᵢ))². This approach is called the method of ordinary least squares. The variation of actual responses 𝑦ᵢ, 𝑖 = 1, …, 𝑛, occurs partly due to the dependence on the predictors 𝐱ᵢ. However, there’s also an additional inherent variance of the output. The coefficient of determination, denoted as 𝑅², tells you which amount of variation in 𝑦 can be explained by the dependence on 𝐱, using the particular regression model. A larger 𝑅² indicates a better fit and means that the model can better explain the variation of the output with different inputs. The value 𝑅² = 1 corresponds to SSR = 0. That’s the perfect fit, since the values of predicted and actual responses fit completely to each other. Simple or single-variate linear regression is the simplest case of linear regression, as it has a single independent variable, 𝐱 = 𝑥. When implementing simple linear regression, you typically start with a given set of input-output (𝑥-𝑦) pairs. These pairs are your observations, shown as green circles in the figure. For example, the leftmost observation has the input 𝑥 = 5 and the actual output, or response, 𝑦 = 5. The next one has 𝑥 = 15 and 𝑦 = 20, and so on. The estimated regression function, represented by the black line, has the equation 𝑓(𝑥) = 𝑏₀ + 𝑏₁𝑥. Your goal is to calculate the optimal values of the predicted weights 𝑏₀ and 𝑏₁ that minimize SSR and determine the estimated regression function. The value of 𝑏₀, also called the intercept, shows the point where the estimated regression line crosses the 𝑦 axis. It’s the value of the estimated response 𝑓(𝑥) for 𝑥 = 0. The value of 𝑏₁ determines the slope of the estimated regression line. The predicted responses, shown as red squares, are the points on the regression line that correspond to the input values. For example, for the input 𝑥 = 5, the predicted response is 𝑓(5) = 8.33, which the leftmost red square represents. The vertical dashed gray lines represent the residuals, which can be calculated as 𝑦ᵢ - 𝑓(𝐱ᵢ) = 𝑦ᵢ - 𝑏₀ - 𝑏₁𝑥ᵢ for 𝑖 = 1, …, 𝑛. They’re the distances between the green circles and red squares. When you implement linear regression, you’re actually trying to minimize these distances and make the red squares as close to the predefined green circles as possible. Multiple or multivariate linear regression is a case of linear regression with two or more independent variables. If there are just two independent variables, then the estimated regression function is 𝑓(𝑥₁, 𝑥₂) = 𝑏₀ + 𝑏₁𝑥₁ + 𝑏₂𝑥₂. It represents a regression plane in a three-dimensional space. The goal of regression is to determine the values of the weights 𝑏₀, 𝑏₁, and 𝑏₂ such that this plane is as close as possible to the actual responses, while yielding the minimal SSR. The case of more than two independent variables is similar, but more general. The estimated regression function is 𝑓(𝑥₁, …, 𝑥ᵣ) = 𝑏₀ + 𝑏₁𝑥₁ + ⋯ +𝑏ᵣ𝑥ᵣ, and there are 𝑟 + 1 weights to be determined when the number of inputs is 𝑟. You can regard polynomial regression as a generalized case of linear regression. You assume the polynomial dependence between the output and inputs and, consequently, the polynomial estimated regression function. In other words, in addition to linear terms like 𝑏₁𝑥₁, your regression function 𝑓 can include nonlinear terms such as 𝑏₂𝑥₁², 𝑏₃𝑥₁³, or even 𝑏₄𝑥₁𝑥₂, 𝑏₅𝑥₁²𝑥₂. The simplest example of polynomial regression has a single independent variable, and the estimated regression function is a polynomial of degree two: 𝑓(𝑥) = 𝑏₀ + 𝑏₁𝑥 + 𝑏₂𝑥². Now, remember that you want to calculate 𝑏₀, 𝑏₁, and 𝑏₂ to minimize SSR. These are your unknowns! Keeping this in mind, compare the previous regression function with the function 𝑓(𝑥₁, 𝑥₂) = 𝑏₀ + 𝑏₁𝑥₁ + 𝑏₂𝑥₂, used for linear regression. They look very similar and are both linear functions of the unknowns 𝑏₀, 𝑏₁, and 𝑏₂. This is why you can solve the polynomial regression problem as a linear problem with the term 𝑥² regarded as an input variable. In the case of two variables and the polynomial of degree two, the regression function has this form: 𝑓(𝑥₁, 𝑥₂) = 𝑏₀ + 𝑏₁𝑥₁ + 𝑏₂𝑥₂ + 𝑏₃𝑥₁² + 𝑏₄𝑥₁𝑥₂ + 𝑏₅𝑥₂². The procedure for solving the problem is identical to the previous case. You apply linear regression for five inputs: 𝑥₁, 𝑥₂, 𝑥₁², 𝑥₁𝑥₂, and 𝑥₂². As the result of regression, you get the values of six weights that minimize SSR: 𝑏₀, 𝑏₁, 𝑏₂, 𝑏₃, 𝑏₄, and 𝑏₅. Of course, there are more general problems, but this should be enough to illustrate the point. One very important question that might arise when you’re implementing polynomial regression is related to the choice of the optimal degree of the polynomial regression function. There’s no straightforward rule for doing this. It depends on the case. You should, however, be aware of two problems that might follow the choice of the degree: underfitting and overfitting. Underfitting occurs when a model can’t accurately capture the dependencies among data, usually as a consequence of its own simplicity. It often yields a low 𝑅² with known data and bad generalization capabilities when applied with new data. Overfitting happens when a model learns both data dependencies and random fluctuations. In other words, a model learns the existing data too well. Complex models, which have many features or terms, are often prone to overfitting. When applied to known data, such models usually yield high 𝑅². However, they often don’t generalize well and have significantly lower 𝑅² when used with new data. The next figure illustrates the underfitted, well-fitted, and overfitted models: Example of underfitted, well-fitted and overfitted models The top-left plot shows a linear regression line that has a low 𝑅². It might also be important that a straight line can’t take into account the fact that the actual response increases as 𝑥 moves away from twenty-five and toward zero. This is likely an example of underfitting. The top-right plot illustrates polynomial regression with the degree equal to two. In this instance, this might be the optimal degree for modeling this data. The model has a value of 𝑅² that’s satisfactory in many cases and shows trends nicely. The bottom-left plot presents polynomial regression with the degree equal to three. The value of 𝑅² is higher than in the preceding cases. This model behaves better with known data than the previous ones. However, it shows some signs of overfitting, especially for the input values close to sixy, where the line starts decreasing, although the actual data doesn’t show that. Finally, on the bottom-right plot, you can see the perfect fit: six points and the polynomial line of the degree five (or higher) yield 𝑅² = 1. Each actual response equals its corresponding prediction. In some situations, this might be exactly what you’re looking for. In many cases, however, this is an overfitted model. It’s likely to have poor behavior with unseen data, especially with the inputs larger than fifty. For example, it assumes, without any evidence, that there’s a significant drop in responses for 𝑥 greater than fifty and that 𝑦 reaches zero for 𝑥 near sixty. Such behavior is the consequence of excessive effort to learn and fit the existing data. There are a lot of resources where you can find more information about regression in general and linear regression in particular. The regression analysis page on Wikipedia, Wikipedia’s linear regression entry, and Khan Academy’s linear regression article are good starting points.\n\nYou’ll start with the simplest case, which is simple linear regression. There are five basic steps when you’re implementing linear regression:\n• Import the packages and classes that you need.\n• Provide data to work with, and eventually do appropriate transformations.\n• Create a regression model and fit it with existing data.\n• Check the results of model fitting to know whether the model is satisfactory. These steps are more or less general for most of the regression approaches and implementations. Throughout the rest of the tutorial, you’ll learn how to do these steps for several different scenarios. The first step is to import the package and the class from : Now, you have all the functionalities that you need to implement linear regression. The fundamental data type of NumPy is the array type called . The rest of this tutorial uses the term array to refer to instances of the type . You’ll use the class to perform linear and polynomial regression and make predictions accordingly. The second step is defining data to work with. The inputs (regressors, 𝑥) and output (response, 𝑦) should be arrays or similar objects. This is the simplest way of providing data for regression: Now, you have two arrays: the input, , and the output, . You should call on because this array must be two-dimensional, or more precisely, it must have one column and as many rows as necessary. That’s exactly what the argument of specifies. This is how and look now: As you can see, has two dimensions, and is , while has a single dimension, and is . The next step is to create a linear regression model and fit it using the existing data. Create an instance of the class , which will represent the regression model: This statement creates the variable as an instance of . You can provide several optional parameters to :\n• is a Boolean that, if , decides to calculate the intercept 𝑏₀ or, if , considers it equal to zero. It defaults to .\n• is a Boolean that, if , decides to normalize the input variables. It defaults to , in which case it doesn’t normalize the input variables.\n• is a Boolean that decides whether to copy ( ) or overwrite the input variables ( ). It’s by default.\n• is either an integer or . It represents the number of jobs used in parallel computation. It defaults to , which usually means one job. means to use all available processors. Your as defined above uses the default values of all parameters. It’s time to start using the model. First, you need to call on : With , you calculate the optimal values of the weights 𝑏₀ and 𝑏₁, using the existing input and output, and , as the arguments. In other words, fits the model. It returns , which is the variable itself. That’s why you can replace the last two statements with this one: This statement does the same thing as the previous two. It’s just shorter. Once you have your model fitted, you can get the results to check whether the model works satisfactorily and to interpret it. You can obtain the coefficient of determination, 𝑅², with called on : When you’re applying , the arguments are also the predictor and response , and the return value is 𝑅². The attributes of are , which represents the coefficient 𝑏₀, and , which represents 𝑏₁: The code above illustrates how to get 𝑏₀ and 𝑏₁. You can notice that is a scalar, while is an array. Note: In scikit-learn, by convention, a trailing underscore indicates that an attribute is estimated. In this example, and are estimated values. The value of 𝑏₀ is approximately 5.63. This illustrates that your model predicts the response 5.63 when 𝑥 is zero. The value 𝑏₁ = 0.54 means that the predicted response rises by 0.54 when 𝑥 is increased by one. You’ll notice that you can provide as a two-dimensional array as well. In this case, you’ll get a similar result. This is how it might look: As you can see, this example is very similar to the previous one, but in this case, is a one-dimensional array with the single element 𝑏₀, and is a two-dimensional array with the single element 𝑏₁. Once you have a satisfactory model, then you can use it for predictions with either existing or new data. To obtain the predicted response, use : When applying , you pass the regressor as the argument and get the corresponding predicted response. This is a nearly identical way to predict the response: In this case, you multiply each element of with and add to the product. The output here differs from the previous example only in dimensions. The predicted response is now a two-dimensional array, while in the previous case, it had one dimension. If you reduce the number of dimensions of to one, then these two approaches will yield the same result. You can do this by replacing with , , or when multiplying it with . In practice, regression models are often applied for forecasts. This means that you can use fitted models to calculate the outputs based on new inputs: Here is applied to the new regressor and yields the response . This example conveniently uses from to generate an array with the elements from 0, inclusive, up to but excluding 5—that is, , , , , and . You can find more information about on the official documentation page.\n\nYou can implement multiple linear regression following the same steps as you would for simple regression. The main difference is that your array will now have two or more columns. Steps 1 and 2: Import packages and classes, and provide data First, you import and and provide known inputs and output: That’s a simple way to define the input and output . You can print and to see how they look now: In multiple linear regression, is a two-dimensional array with at least two columns, while is usually a one-dimensional array. This is a simple example of multiple linear regression, and has exactly two columns. The next step is to create the regression model as an instance of and fit it with : The result of this statement is the variable referring to the object of type . It represents the regression model fitted with existing data. You can obtain the properties of the model the same way as in the case of simple linear regression: You obtain the value of 𝑅² using and the values of the estimators of regression coefficients with and . Again, holds the bias 𝑏₀, while now is an array containing 𝑏₁ and 𝑏₂. In this example, the intercept is approximately 5.52, and this is the value of the predicted response when 𝑥₁ = 𝑥₂ = 0. An increase of 𝑥₁ by 1 yields a rise of the predicted response by 0.45. Similarly, when 𝑥₂ grows by 1, the response rises by 0.26. Predictions also work the same way as in the case of simple linear regression: The predicted response is obtained with , which is equivalent to the following: You can predict the output values by multiplying each column of the input with the appropriate weight, summing the results, and adding the intercept to the sum. You can apply this model to new data as well:\n\nImplementing polynomial regression with scikit-learn is very similar to linear regression. There’s only one extra step: you need to transform the array of inputs to include nonlinear terms such as 𝑥². In addition to and , you should also import the class from : The import is now done, and you have everything you need to work with. This step defines the input and output and is the same as in the case of linear regression: Now you have the input and output in a suitable format. Keep in mind that you need the input to be a two-dimensional array. That’s why is used. This is the new step that you need to implement for polynomial regression! As you learned earlier, you need to include 𝑥²—and perhaps other terms—as additional features when implementing polynomial regression. For that reason, you should transform the input array to contain any additional columns with the values of 𝑥², and eventually more features. It’s possible to transform the input array in several ways, like using from . But the class is very convenient for this purpose. Go ahead and create an instance of this class: The variable refers to an instance of that you can use to transform the input . You can provide several optional parameters to :\n• is an integer ( by default) that represents the degree of the polynomial regression function.\n• is a Boolean ( by default) that decides whether to include only interaction features ( ) or all features ( ).\n• is a Boolean ( by default) that decides whether to include the bias, or intercept, column of values ( ) or not ( ). This example uses the default values of all parameters except . You’ll sometimes want to experiment with the degree of the function, and it can be beneficial for readability to provide this argument anyway. Before applying , you need to fit it with : Once is fitted, then it’s ready to create a new, modified input array. You apply to do that: That’s the transformation of the input array with . It takes the input array as the argument and returns the modified array. You can also use to replace the three previous statements with only one: With , you’re fitting and transforming the input array in one statement. This method also takes the input array and effectively does the same thing as and called in that order. It also returns the modified array. This is how the new input array looks: The modified input array contains two columns: one with the original inputs and the other with their squares. You can find more information about on the official documentation page. This step is also the same as in the case of linear regression. You create and fit the model: The regression model is now created and fitted. It’s ready for application. You should keep in mind that the first argument of is the modified input array and not the original . You can obtain the properties of the model the same way as in the case of linear regression: Again, returns 𝑅². Its first argument is also the modified input , not . The values of the weights are associated to and . Here, represents 𝑏₀, while references the array that contains 𝑏₁ and 𝑏₂. You can obtain a very similar result with different transformation and regression arguments: If you call with the default parameter , or if you just omit it, then you’ll obtain the new input array with the additional leftmost column containing only values. This column corresponds to the intercept. This is how the modified input array looks in this case: The first column of contains ones, the second has the values of , while the third holds the squares of . The intercept is already included with the leftmost column of ones, and you don’t need to include it again when creating the instance of . Thus, you can provide . This is how the next statement looks: The variable again corresponds to the new input array . Therefore, should be passed as the first argument instead of . This approach yields the following results, which are similar to the previous case: You see that now is zero, but actually contains 𝑏₀ as its first element. Everything else is the same. If you want to get the predicted response, just use , but remember that the argument should be the modified input instead of the old : As you can see, the prediction works almost the same way as in the case of linear regression. It just requires the modified input instead of the original. You can apply an identical procedure if you have several input variables. You’ll have an input array with more than one column, but everything else will be the same. Here’s an example: This regression example yields the following results and predictions: In this case, there are six regression coefficients, including the intercept, as shown in the estimated regression function 𝑓(𝑥₁, 𝑥₂) = 𝑏₀ + 𝑏₁𝑥₁ + 𝑏₂𝑥₂ + 𝑏₃𝑥₁² + 𝑏₄𝑥₁𝑥₂ + 𝑏₅𝑥₂². You can also notice that polynomial regression yielded a higher coefficient of determination than multiple linear regression for the same problem. At first, you could think that obtaining such a large 𝑅² is an excellent result. It might be. However, in real-world situations, having a complex model and 𝑅² very close to one might also be a sign of overfitting. To check the performance of a model, you should test it with new data—that is, with observations not used to fit, or train, the model. To learn how to split your dataset into the training and test subsets, check out Split Your Dataset With scikit-learn’s train_test_split().\n\nYou can implement linear regression in Python by using the package statsmodels as well. Typically, this is desirable when you need more detailed results. The procedure is similar to that of scikit-learn. First you need to do some imports. In addition to , you need to import : Now you have the packages that you need. You can provide the inputs and outputs the same way as you did when you were using scikit-learn: The input and output arrays are created, but the job isn’t done yet. You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept 𝑏₀. It doesn’t take 𝑏₀ into account by default. This is just one function call: That’s how you add the column of ones to with . It takes the input array as an argument and returns a new array with the column of ones inserted at the beginning. This is how and look now: You can see that the modified has three columns: the first column of ones, corresponding to 𝑏₀ and replacing the intercept, as well as two columns of the original features. The regression model based on ordinary least squares is an instance of the class . This is how you can obtain one: You should be careful here! Notice that the first argument is the output, followed by the input. This is the opposite order of the corresponding scikit-learn functions. There are several more optional parameters. To find more information about this class, you can visit the official documentation page. Once your model is created, then you can apply on it: By calling , you obtain the variable , which is an instance of the class . This object holds a lot of information about the regression model. The variable refers to the object that contains detailed information about the results of linear regression. Explaining these results is far beyond the scope of this tutorial, but you’ll learn here how to extract them. You can call to get the table with the results of linear regression: [1] Standard Errors assume that the covariance matrix of the errors is This table is very comprehensive. You can find many statistical values associated with linear regression, including 𝑅², 𝑏₀, 𝑏₁, and 𝑏₂. In this particular case, you might obtain a warning saying . This is due to the small number of observations provided in the example. You can extract any of the values from the table above. Here’s an example: That’s how you obtain some of the results of linear regression:\n• represents adjusted 𝑅²—that is, 𝑅² corrected according to the number of input features.\n• refers the array with 𝑏₀, 𝑏₁, and 𝑏₂. You can also notice that these results are identical to those obtained with scikit-learn for the same problem. To find more information about the results of linear regression, please visit the official documentation page. You can obtain the predicted response on the input values used for creating the model using or with the input array as the argument: This is the predicted response for known inputs. If you want predictions with new regressors, you can also apply with new data as the argument: You can notice that the predicted results are the same as those obtained with scikit-learn for the same problem."
    },
    {
        "link": "https://kellton.com/kellton-tech-blog/data-visualization-best-practices-every-app-developer-should-know",
        "document": "Data is plain clutter if you can’t unlock its real value.\n\nThis is where visualization comes in. At its simplest, it’s a process that depicts information graphically, mainly charts, graphs, and maps, making it easier for you to figure out trends and patterns, both ongoing and upcoming.\n\nWhy do you need it? Because it makes intricate details easier to understand, saving you time and brainpower. These insights get you fired up to shake up market strategies and lead with confidence.\n\nInstead of getting lost in endless rows of numbers, you can quickly grasp what’s important and make smarter decisions faster. You can see patterns, trends, and outliers at a glance — and take actions that fit well with their requirements, driving the results you expect. Plus, it’s way easier to share insights with your team when they’re looking at an interactive graph instead of a boring table.\n\nIn short, data visualization helps you unlock the hidden value in your data and communicate it effectively. The practice thrives on three “Is”: interactivity, intuitiveness, and informativeness. Let’s understand them up-close.\n\na. Interactivity\n\nEnables users to interact with the data via zooming in, filtering, and viewing different aspects of data to gain a deeper understanding.\n\nb. Intuitiveness\n\nMakes data easier to understand and interpret, even for those with no technical background and can’t understand an iota of complex data.\n\nc. Informativeness\n\nOrganizes information in a clear manner, emphasizing patterns, trends, and anomalies that are important in data analysis.\n\nNot only does it help organize and condense large amounts of information, but also makes it efficient for users to work with and pay attention to data. It makes data interesting and appealing to the eye.\n• How does it play out in the sphere of web and mobile apps?\n• How does it contribute to smartening up the modern-day SDLC?\n• What data visualization best practices ensure you get the most out of your data?\n\nImplementing data visualization in the context of web and mobile application development can improve user experience by a tenfold.\n\nData visualization converts stale, raw data to a form that’s not only consumable but also visually appealing. Consider your fitness application that displays key metrics in vibgyor-ish bars and charts. These visualizations will be far more interesting and interpretive than just a number, right?\n\nOn the front of user experience, data visualization can give users information in a snapshot. For example, financial apps use graphs to show spending habits, helping users quickly see where their money goes. This kind of instant clarity is a huge plus.\n\nVisualizing data can improve decision-making significantly. When information is presented visually, trends, patterns, and anomalies pop up instantly. For instance, a sales application dashboard with bar charts and line graphs can aid managers in identifying products that are popular and are not. This quick understanding can lead to faster, wiser decisions.\n\nEngaging visuals catch the attention of users from the get-go. When people can tap, swipe, and zoom into data, user engagement touches a new high. Take a weather application for example, where you can swipe across the screen to see how the temperature is fluctuating over the week.\n\nVisual data is highly accessible and a blessing for people who might struggle with numbers. A pie chart displaying budget percentages is more understandable than a detailed spreadsheet, especially on a mobile display.\n\nAll in all, data visualization makes an application easy on the eyes and user-friendly. It dramatically improves user engagement and experience. By integrating clear and compelling visuals, it gives your business a unique opportunity to turn web and mobile apps into powerful tools for insight and action.\n\nData visualization best practices: 10 simple yet effective approaches for impactful outcomes\n\nData visualization helps us make sense of large amounts of information, but it's crucial to follow some best practices to create clear and accurate visuals. Here are 10 best practices for data visualization to keep in mind while designing your next application.\n\nWant to ace your data visualizations? Keep them simple!\n\nDon’t cook up clutter. Steer away from distractions. Ditch using too many elements and excessive details. Instead, make your data visualizations simple, clean, and predictable. Stick to what’s essential for conveying an insight.\n\nBy adhering to simplicity, you can make it easier for users to quickly wrap their head around the main insights without getting overwhelmed.\n\n2. Pick the visual component that strikes the right chord\n\nThere are many types of charts and graphs you can use to display your data. Choosing the right one is key to effectively conveying your information. Here are some common types:\n• Bar Chart: Ideal for comparing different categories, like the time spent on a phone or downloads of paid versus free apps.\n• Line Chart: Best for showing trends over time with continuous data, such as stock prices or website traffic.\n• Pie Chart: Good for displaying percentages when you have fewer than seven categories, helping you see what portion each category represents out of 100%.\n\nOther charts include column charts and area charts. Always pick the one that best suits the data you’re presenting.\n\nDesigning for mobile means considering smaller screens and touch interactions.\n\nEnsure your visualizations are responsive, meaning they look good and are functional on any device. Simplify interactions so they’re easy to use with a finger instead of a mouse. Keep text large enough to read without zooming and ensure buttons and interactive elements are not too small to tap.\n\nMobile users expect quick, intuitive access to information, so prioritize usability and clarity.\n\nData visualizations should look clean and modest. Not gaudy.\n\nDial down the rainbow and use 3-4 colors to demonstrate data and trends within it. Keep a clean, cohesive look to ensure maximum user engagement.\n\nPick a specific color palette to represent the same type of data across your visuals. Use another color palette to highlight key data points with contrasting colors to draw attention. This approach not only makes your visualizations aesthetically pleasing but also helps users easily differentiate between different data sets and understand the information at a glance.\n\nRemember, a hint of color goes a long way in turning up positive outcomes. Don’t overdo it.\n\n5. Keeping high contrast is your secret sauce to success\n\nA contrasting color combination, such as having dark text on a light background or having light text on a dark background, is usually a win-win choice.\n\nThe dark color against the light background prevents rapid fatigue and makes the text easy to read for people with low vision, especially for those who use screens with low resolution and poor lighting conditions.\n\nThis practice benefits users using your app when it is highly sunny or dark. Build accessibility in, too; high contrast makes content more accessible to read and is helpful for users with vision impairment.\n\nThe entire idea behind data visualizations is helping users understand and make sense out of complex information easier and faster.\n\nAnd so, unreadable data visualizations defeat the whole purpose.\n\nVisualizations that are legible, comprehensible, and clearly laid out make for a winning recipe. Avoid small fonts, and use text components that should be readable to the eyes.\n\nDon’t use calligraphic or other font styles that may look artistic but are difficult to read. Ensure that text size is big enough to be read across devices. Also, do not place your text elements in the way of critical data, but ensure they are strategic enough to give the required context.\n\nIt’s recommended to add tooltips that appear with the descriptions if users hover over the point or tap on it. This interactivity enables you to add more descriptions without the screen becoming congested with information.\n\nUsers can navigate through the data independently, focusing on the certain points of interest they find more relevant. It makes your visualizations much better and informative than just the raw data without being cumbersome at the same time.\n\nStop looking at data beyond its numeric value. Tell a tale out of it.\n\nArrange data to provide users with key points initially. Emphasize recent changes, peculiarities, and differences. Annotations and callouts are the best practices to apply to the content to highlight important metrics.\n\nRemember, it’s easier for users to understand concepts when they are presented in a logical format and when backed by data.\n\nThe wise word out there says: add labels, titles, and legends to inform the users what numbers depict and demonstrate. Include the use of units of measurement and time wherever necessary. Otherwise, people can easily misinterpret statistics or overlook its meaning altogether.\n\nWhen all context is shared with users, there remains no room for misinterpretation and misunderstanding, enabling them to make better decisions - faster.\n\nUser testing is an invaluable step in the design process.\n\nAvoid self-bias. Gather feedback to understand how users interact with your application visualizations. . This can unveil the usability problems, misconceptions or potential opportunities that might be unknown to you beforehand.\n\nDo usability tests, collect results, and base your decisions on the outcomes acquired. This ensures that your final product is user-friendly and effectively communicates the intended insights.\n\nDevs that comply with these practices can set the groundwork for effective data visualization. But are there any tools that can help? Let’s find out!\n\nFor a business to nail data visualizations, it need two sorts of tools:\n\nAnalytical database platforms do the heavy lifting job of streaming and processing data, whereas viz tools turn this data into snazzy visuals - concise and consumable.\n\nLet’s explore the top 5 tools in each of these categories as we dwell further on data visualization best practices.\n• What it is: Fully managed data warehouse service by AWS.\n• Why it rocks: Handles petabyte-scale data. Integrates seamlessly with other AWS services. Fast query performance with columnar storage.\n• Gotchas: Can get expensive at scale. Initial setup and optimization can be complex.\n• What it is: Serverless, highly scalable, and cost-effective multi-cloud data warehouse by Google.\n• Why it rocks: Handles huge datasets with ease. Super fast query execution. Simple pricing model based on storage and query processing.\n• Gotchas: Costs can add up with frequent queries. Learning curve for SQL syntax and Google Cloud integration.\n• What it is: High-performance real-time analytics database.\n• Why it rocks: Designed for low-latency query performance. Handles streaming and batch data equally well. Scales horizontally with ease.\n• Gotchas: Complex to set up and manage. Requires tuning for optimal performance.\n• What it is: Open-source columnar database management system.\n• Why it rocks: Blazing fast for analytical queries. Handles billions of rows per second. Supports real-time data ingestion and querying.\n• Gotchas: Can be resource-intensive. Less mature ecosystem compared to some competitors.\n• What it is: Time-series database based on PostgreSQL.\n• Why it rocks: Combines SQL with time-series data efficiency. Handles high data ingestion rates. Leverages PostgreSQL's reliability and features.\n• Gotchas: Requires understanding of PostgreSQL for advanced use. Performance tuning can be necessary for very large datasets.\n• What it is: Lightweight, open-source library for simple charts.\n• Why it rocks: Super easy to use, perfect for beginners. It's responsive, works great on mobile. Tons of customization options.\n• Gotchas: Limited to basic charts, not for heavy-duty data viz.\n• What it is: Powerful JavaScript library for complex, interactive data visualizations.\n• Why it rocks: Insanely flexible, lets you create custom visuals. It's all about SVG, CSS, and HTML, making it super versatile.\n• What it is: Online tool for creating responsive, interactive charts and maps.\n• Why it rocks: Simple interface, no coding required. Produces visually appealing graphics optimized for web and mobile. Great for storytelling with data.\n• Gotchas: Limited customization compared to some other tools. Free version has some feature limitations.\n• What it is: Leading data visualization and analytics platform.\n• Why it rocks: Powerful drag-and-drop interface for creating complex visualizations. Supports a wide range of data sources. Great for exploring and sharing insights.\n• Gotchas: Can be expensive for full features. Steeper learning curve for advanced functionality.\n• What it is: Open-source web application for creating and sharing documents with live code, equations, visualizations, and narrative text.\n• Why it rocks: Integrates code (Python, R, etc.) with visualizations and explanations in one document. Perfect for data exploration, analysis, and sharing reproducible research.\n• Gotchas: Requires some programming knowledge. Sharing notebooks with non-technical users can be tricky.\n\nIn a world drowning in data, visualization is the lifeline that makes businesses interpret the complex insights in it and take informed decisions.\n\nIt helps turn raw piles of data into something you can easily understand and act on.\n\nWith a proper visualization architecture in place, you can do away with the need of endless scrolling through a data quagmire. Instead, you can throw that data into a cool chart or an interactive graph and immediately make sense out of it.\n\nBut is data visualization as easy as it seems? Not at all!\n\nExtracting hidden value in piles of data requires much more than just an architecture. It requires you to get your head around data visualization best practices to cover all the right bases and turn in insights on the fly.\n\nThe article above lays down the best practices for data visualization that you must leverage to keep your data a key business differentiator."
    },
    {
        "link": "https://linkedin.com/advice/0/what-best-data-visualization-platforms-analyzing-o8bif",
        "document": ""
    },
    {
        "link": "https://transcenda.com/insights/data-visualization-ui-best-practices-and-winning-approaches",
        "document": "How can you increase the value of your apps for users? One powerful and potentially underrated tactic involves improving your approach to data visualization as part of the user interface.\n\nWhat is data visualization? It's the practice of showing information through imagery instead of text, and there's a scientific reason to prioritize it in product design: 90% of information processed by the brain is visual and up to 75% of users judge credibility by aesthetics.\n\nPoorly designed visual-focused user interfaces may fail to live up to their potential. That's why it's important to learn how to display data effectively and internalize data visualization best practices.\n\nWhat are the best practices for data visualization UI?\n\nWhen considering how to optimize the performance of your app's data visualization UI, it pays to focus on a few specific guiding principles. If you let these ideas guide your decision-making process, they'll keep your efforts on the right track.\n• Display data accurately and clearly: Data should always be displayed with clarity and integrity, avoiding any distortions that could lead to confusion or misinterpretation. Even the best-looking and most sophisticated UI will fail to live up to its potential if it doesn't present information accurately.\n• Provide adequate context so users can understand data: The purpose of data visualization UI is to present clear insights to users and help them answer questions. They may want to know about performance, growth of a variable over time, the status of an account or asset or one of a near-unlimited number of other facts or figures. This means your app should include all the tools and contextual cues necessary to help those users interpret the figures they're presented with.\n• Scale the data for visibility on various devices: Modern data visualization design principles demand UIs that are highly scalable and flexible, with the ability to adapt automatically to different types and sizes of screens. Users' needs regarding the scale of charts, icons and more will change across the variety of devices available to them today.\n• Collaborate with developers early in the process: Designers shouldn't isolate themselves when working on data visualization UI. Speaking with developers as early as possible about matters such as data source access and visual rendering will help the design and development processes go more smoothly. Otherwise, designers could end up dealing with a backend that doesn't allow them to easily present the data they've determined users need most.\n\nMore specific and granular suggestions that can help you master the art of data visualization UI design include:\n• Use consistent visual language: When icons, graph axes and colors always mean the same thing, it's quick and simple for users to navigate your UI visually.\n• Make labels, legends and colors easily readable: Clear labeling, at any screen size and orientation, is also key to the overall visual clarity of your application.\n• Present the right chart for the job: Your choice of bar graphs, pie charts, line graphs and other visualization types contributes to the story you're telling with data.\n• Avoid unnecessary items: A cluttered UI can be a major problem for data visualization — if any features don't serve a purpose, they can and should be removed.\n• Make sure every pixel is meaningful: Every piece of on-screen real estate in a data visualization should contribute to a clear and easy-to-follow visual logic.\n• Embrace accessibility and inclusive design: Embracing inclusivity and highly accessible design, for example, by choosing visual cues that colorblind users can interpret, helps you reach the widest possible audience.\n• Enable interactions and drill-down into complex data: If you're presenting complex numbers that demand in-depth study, you can enable zooming in on specific features rather than over-complicating the primary UI.\n• Choose a refresh rate that makes sense for users: When data refreshes too quickly, it can be challenging for users to put it to use, while if it rarely refreshes, it could end up being out of date. Finding the right rate for your particular data set is a must.\n• Stay in touch with developers about UI/backend interactions: As a designer, you should be in constant contact with developers as your plans and intentions evolve over time.\n\nCreating an application with a high-quality data visualization UI means confronting questions at every step of the design process. Every choice you make, on both technical and aesthetic levels, can contribute to the success of the finished product.\n\nWhat's the best way to present data to your audience?\n\nThere is no one generic audience for every digital product. Your specific application will have a very specific target group of users. One way to determine whether the UI design process is on the right track is to ask, \"Will my target audience be able to take value from this chart?\"\n\nThe role of a data visualization UI is to tell a story with data. This means — instead of presenting raw figures with no context, the app will help users come to conclusions and make choices based on the information in the chart.\n\nThere are a few key questions that can help you center yourself during the design process, verifying that your chosen ways to present data visually are telling the correct story to the right audience. These include:\n• Who is going to be reading this data visualization, in terms of role and experience?\n• Based on their knowledge of this type of data, what expectations will users have?\n• What kinds of problems are users hoping to solve with the insights they're getting from this chart?\n• How can the visual design features included in this graph help users understand it and take useful knowledge from it?\n• What value could come from applying the concept of user stories?\n\nIncorporating user stories into the design process of data visualization UIs is essential for ensuring that the final product truly resonates with the target audience. User stories are brief, simple descriptions of a feature told from the perspective of the person seeking the new capability, usually a user or customer of the system.\n\nThis approach is incredibly useful in data visualization because it keeps the focus on the user's needs and goals. By framing requirements in the context of user stories, designers can more effectively choose visual elements, layouts and features that not only look aesthetically pleasing but also serve practical, user-centered purposes. User stories help in creating a UI that is not just visually engaging but also functional and intuitive for the end user, making the data both accessible and actionable.\n\nEvery piece of the design process feeds into the goal of telling a clear, useful data story to a specific audience segment. Rather than asking if a visual cue looks good in general, it's more helpful to ask whether it helps people understand what they're looking at.\n\nWhat are some common data visualization mistakes to avoid?\n\nEffective data visualization UI design doesn't just mean following best practices — it also involves avoiding common pitfalls. Some design problems are particularly dangerous because they might not become apparent immediately. The general through-line between data visualization mistakes is that they lead to products that don't serve the users' best interests.\n• Using the wrong type of graph for the data: In some cases, a specific type of graph can make the importance of data unclear or, even worse, give a deceptive impression of what the figures mean.‍\n• Colors that are hard to parse: The use of colors that are too similar or clash visually can be an accessibility problem. In extreme cases, the incorrect use of color can make data harder to interpret for all audiences.\n\n‍‍\n• Design too minimal to tell a story: While it's good to strip out unnecessary information from a UI, going too far causes problems. A lack of key context or an overly minimal presentation can confuse users.\n\n‍‍‍\n• A lack of interaction: When data is detailed enough to demand interaction, transformation or drilling down, it's important to support these interactive data visualization features and make them intuitive. Otherwise, audiences can fail to fully explore the data set.\n\nSome problems aren't universal but are rather tied to the target user profile. Creating an interface that's too complex for a general audience or too simplistic for a specialized audience can make an application less useful for its intended purpose.\n\nHow has data visualization UI evolved, and what's next?\n\nTechnology's constant evolution has affected every aspect of design best practices, and the concept of what makes a good data visualization has shifted over time. Adapting UI design efforts to keep up with the march of new tech innovations is simply an intelligent approach.\n\nSo, what has this meant in recent years?\n• More depth of colors for data visualization: Modern screens can show richer colors than previous generations of devices could offer, opening possibilities for greater clarity in charts.\n• Advanced rendering capabilities: More powerful processors allow designers to use more compelling visualization styles.\n• Drilldown options: Along with visually compelling primary graphs, today's devices also allow deeper explorations of large data sets.\n\nDesigners are also grappling with philosophical questions about the best ways to present data. Can one graph tell the story you're focusing on, or is it better to combine multiple visualizations? This could mean:\n• Showing less with more: Adding extra graphs to a screen, each contributing to a clear and minimal overall story.\n• Showing more with less: Answering in-depth questions through a clean, minimalistic presentation that shows only what's necessary,\n\nIn the years to come, recent trends, including the rise of generative AI, could have an impact on data visualization UI design. For instance, AI can help pick the right chart for a question or act as a companion when choosing what data to present. In the tech-saturated present and future, it can be valuable for you as a designer to pick up some development know-how alongside your design skills.\n\nHow can you master data visualization UI in your next project?\n\nWorking with expert consultants can help you with the data visualization elements of your next project. This assistance can take a variety of forms, including:\n\nTranscenda's experts have worked with companies in all those capacities and more. Examples of these successful engagements include our collaborations with Savant, Medidata and Centric Software. In each of these cases, there was a need for an effective data visualization UI to address a very specific issue.\n\n‍Contact Transcenda to delve deeper into the world of data visualization UI design."
    },
    {
        "link": "https://sciencedirect.com/science/article/pii/S2405656118301421",
        "document": ""
    },
    {
        "link": "https://researchgate.net/publication/231178325_User_Interface_Development_and_Data_Visualization_for_Building_Monitoring_Systems",
        "document": ""
    }
]