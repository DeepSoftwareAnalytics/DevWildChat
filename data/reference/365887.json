[
    {
        "link": "https://stackoverflow.com/questions/44399422/read-file-from-resources-folder-in-spring-boot",
        "document": "I'm using Spring Boot and . I'm trying to read a file called from the folder. I've tried a few different ways but I can't get it to work. This is my code. This is the location of the file. And here I can see the file in the folder. But when I run the code I get the following error. jsonSchemaValidator error: java.io.FileNotFoundException: /home/user/Dev/Java/Java%20Programs/SystemRoutines/target/classes/jsonschema.json (No such file or directory) What is it I'm doing wrong in my code?"
    },
    {
        "link": "https://docs.spring.io/spring-integration/reference/file/reading.html",
        "document": "A can be used to consume files from the filesystem. This is an implementation of that creates messages from a file system directory. The following example shows how to configure a : To prevent creating messages for certain files, you can supply a . By default, we use the following filters: The ensures that hidden files are not processed. Note that the exact definition of hidden is system-dependent. For example, on UNIX-based systems, a file beginning with a period character is considered to be hidden. Microsoft Windows, on the other hand, has a dedicated file attribute to indicate hidden files. Version 4.2 introduced the . In prior versions, hidden files were included. With the default configuration, the is triggered first, followed by the . The ensures files are picked up only once from the directory. The stores its state in memory. If you wish the state to survive a system restart, you can use the . This filter stores the accepted file names in a implementation (see Metadata Store). This filter matches on the filename and modified time. Since version 4.0, this filter requires a . When used with a shared data store (such as with the ), it lets filter keys be shared across multiple application instances or across a network file share being used by multiple servers. Since version 4.1.5, this filter has a new property ( ), which causes it to flush the metadata store on every update (if the store implements ). The persistent file list filters now have a boolean property . Setting this property to , also sets , which means that the recursive operation on the outbound gateways ( and ) will now always traverse the full directory tree each time. This is to solve a problem where changes deep in the directory tree were not detected. In addition, causes the full path to files to be used as the metadata store keys; this solves a problem where the filter did not work properly if a file with the same name appears multiple times in different directories. IMPORTANT: This means that existing keys in a persistent metadata store will not be found for files beneath the top level directory. For this reason, the property is by default; this may change in a future release. The following example configures a with a filter: A common problem with reading files is that a file may be detected before it is ready (that is, some other process may still be writing the file). The default does not prevent this. In most cases, this can be prevented if the file-writing process renames each file as soon as it is ready for reading. A or filter that accepts only files that are ready (perhaps based on a known suffix), composed with the default , allows for this situation. The enables the composition, as the following example shows: If it is not possible to create the file with a temporary name and rename to the final name, Spring Integration provides another alternative. Version 4.2 added the . This filter can be configured with an property so that only files older than this value are passed by the filter. The age defaults to 60 seconds, but you should choose an age that is large enough to avoid picking up a file early (due to, say, network glitches). The following example shows how to configure a : Starting with version 4.3.7, a (an extension of ) has been introduced to allow scenarios when subsequent filters should only see the result of the previous filter. (With the , all filters see all the files, but it passes only files that have passed all filters). An example of where the new behavior is required is a combination of and , when we do not wish to accept the file until some amount of time has elapsed. With the , since the sees all the files on the first pass, it does not pass it later when the other filter does. The approach is useful when a pattern filter is combined with a custom filter that looks for a secondary file to indicate that file transfer is complete. The pattern filter might only pass the primary file (such as ) but the “done” filter needs to see whether (for example) is present. Say we have files , , and . The pattern filter passes only and , while the “done” filter sees all three files and passes only . The final result of the composite filter is that only is released. With the , if any filter in the chain returns an empty list, the remaining filters are not invoked. Version 5.0 introduced an to execute SpEL expression against a file as a context evaluation root object. For this purpose, all the XML components for file handling (local and remote), along with an existing attribute, have been supplied with the option, as the following example shows: Version 5.0.5 introduced the implementations that have an interest in rejected files. For this purpose, such a filter implementation should be supplied with a callback through . In the framework, this functionality is used from the , in combination with . Unlike the regular , the provides files for processing according to the events on the target file system. At the moment of polling an internal queue with those files, the may discard them because they are too young relative to its configured . Therefore, we lose the file for future possible considerations. The discard callback hook lets us retain the file in the internal queue so that it is available to be checked against the in subsequent polls. The also implements a and populates a discard callback to all its delegates. Since matches the files against all delegates, the may be called several times for the same file. Starting with version 5.1, the doesn’t check a directory for existence and doesn’t create it until its is called (typically via wrapping ). Previously, there was no simple way to prevent an operation system permissions error when referencing the directory, for example from tests, or when permissions are applied later.\n\nThe does not produce messages for files from the directory immediately. It uses an internal queue for 'eligible files' returned by the . The option is used to ensure that the internal queue is refreshed with the latest input directory content on each poll. By default ( ), the empties its queue before scanning the directory again. This default behavior is particularly useful to reduce scans of large numbers of files in a directory. However, in cases where custom ordering is required, it is important to consider the effects of setting this flag to . The order in which files are processed may not be as expected. By default, files in the queue are processed in their natural ( ) order. New files added by a scan, even when the queue already has files, are inserted in the appropriate position to maintain that natural order. To customize the order, the can accept a as a constructor argument. It is used by the internal ( ) to reorder its content according to the business requirements. Therefore, to process files in a specific order, you should provide a comparator to the rather than ordering the list produced by a custom . Version 5.0 introduced to perform file tree visiting. The implementation is based on the functionality. The root directory ( ) argument is excluded from the result. All other sub-directories inclusions and exclusions are based on the target implementation. For example, the filters out directories by default. See and its implementations for more information. Starting with version 5.5, the of the Java DSL has a convenient option to use a in the target instead of the default one.\n\nThe configuration for file reading can be simplified by using the file-specific namespace. To do so, use the following template: Within this namespace, you can reduce the and wrap it in an inbound Channel Adapter, as follows: The first channel adapter example relies on the default implementations: Therefore, you can also leave off the and attributes, as they are by default. Spring Integration 4.2 introduced the attribute. In prior versions, hidden files were included. The second channel adapter example uses a custom filter, the third uses the attribute to add an based filter, and the fourth uses the attribute to add a regular expression pattern-based filter to the . The and attributes are each mutually exclusive with the regular reference attribute. However, you can use the attribute to reference an instance of that combines any number of filters, including one or more pattern-based filters to fit your particular needs. When multiple processes read from the same directory, you may want to lock files to prevent them from being picked up concurrently. To do so, you can use a . There is a -based implementation available, but it is also possible to implement your own locking scheme. The locker can be injected as follows: You can configure a custom locker as follows: When a file inbound adapter is configured with a locker, it takes responsibility for acquiring a lock before the file is allowed to be received. It does not assume the responsibility to unlock the file. If you have processed the file and keep the locks hanging around, you have a memory leak. If this is a problem, you should call yourself at the appropriate time. When filtering and locking files is not enough, you might need to control the way files are listed entirely. To implement this type of requirement, you can use an implementation of . This scanner lets you determine exactly what files are listed in each poll. This is also the interface that Spring Integration uses internally to wire instances and to the . You can inject a custom into the on the attribute, as the following example shows: Doing so gives you full freedom to choose the ordering, listing, and locking strategies. It is also important to understand that filters (including , , , and others) and instances are actually used by the . Any of these attributes set on the adapter are subsequently injected into the internal . For the case of an external , all filter and locker attributes are prohibited on the . They must be specified (if required) on that custom . In other words, if you inject a into the , you should supply and on that , not on the . By default, the uses an and an . To prevent their use, you can configure your own filter (such as ) or even set it to .\n\nThe relies on file-system events when new files are added to the directory. During initialization, the directory is registered to generate events. The initial file list is also built during initialization. While walking the directory tree, any subdirectories encountered are also registered to generate events. On the first poll, the initial file list from walking the directory is returned. On subsequent polls, files from new creation events are returned. If a new subdirectory is added, its creation event is used to walk the new subtree to find existing files and register any new subdirectories found. There is an issue with when its internal events is not drained by the program as quickly as the directory modification events occur. If the queue size is exceeded, a is emitted to indicate that some file system events may be lost. In this case, the root directory is re-scanned completely. To avoid duplicates, consider using an appropriate (such as the ) or removing files when processing is complete. The can be enabled through the option, which is mutually exclusive with the option. An internal instance is populated for the provided . In addition, now the polling logic can track the and . If you need to track the modification of existing files as well as new files, you should implement the events logic in the . Otherwise, the files from those events are treated the same way. The implementations pick up the events. Consequently, their files are provided for the operation. When this event is enabled, filters such as the have the file removed. As a result, if a file with the same name appears, it passes the filter and is sent as a message. For this purpose, the property ( ) has been introduced. ( is a public inner enumeration in .) With such an option, we can use one downstream flow logic for new files and use some other logic for modified files. The following example shows how to configure different logic for create and modify events in the same directory: It is worth mentioning that the event is involved in the rename operation of sub-directory of the watched directory. More specifically, event, which is related to the previous directory name, precedes event which notifies about the new (renamed) directory. On some operating systems (like Windows), the event has to be registered to deal with that situation. Otherwise, renaming watched sub-directory in the File Explorer could result in the new files not being detected in that sub-directory. Starting with version 6.1, the exposes two new -related options:\n• - an argument for the API;\n• - a to test if a directory in the scanned tree should be walked and registered with the and the configured watch event kinds.\n\nAnother popular use case is to get 'lines' from the end (or tail) of a file, capturing new lines when they are added. Two implementations are provided. The first, , uses the native command (on operating systems that have one). This is generally the most efficient implementation on those platforms. For operating systems that do not have a command, the second implementation, , uses the Apache class. In both cases, file system events, such as files being unavailable and other events, are published as instances by using the normal Spring event publishing mechanism. Examples of such events include the following: [message=tail: cannot open '/tmp/somefile' for reading: No such file or directory, file=/tmp/somefile] [message=tail: '/tmp/somefile' has become accessible, file=/tmp/somefile] [message=tail: '/tmp/somefile' has become inaccessible: No such file or directory, file=/tmp/somefile] [message=tail: '/tmp/somefile' has appeared; following end of new file, file=/tmp/somefile] The sequence of events shown in the preceding example might occur, for example, when a file is rotated. Starting with version 5.0, a is emitted when there is no data in the file during . The following example shows what such an event looks like: Not all platforms that support a command provide these status messages. Messages emitted from these endpoints have the following headers: In versions prior to version 5.0, the header contained a string representation of the file’s absolute path. You can now obtain that string representation by calling on the original file header. The following example creates a native adapter with the default options ('-F -n 0', meaning to follow the file name from the current end). The following example creates a native adapter with '-F -n +0' options (meaning follow the file name, emitting all existing lines). If the command fails (on some platforms, a missing file causes the to fail, even with specified), the command is retried every 10 seconds. By default, native adapters capture from standard output and send the content as messages. They also capture from standard error to raise events. Starting with version 4.3.6, you can discard the standard error events by setting the to , as the following example shows: In the following example, is set to , meaning that, if no lines are written for five seconds, is triggered every five seconds: This can be useful when you need to stop the adapter. The following example creates an Apache adapter that examines the file for new lines every two seconds and checks for existence of a missing file every ten seconds: The file is tailed from the beginning ( ) instead of the end (which is the default). The file is reopened for each chunk (the default is to keep the file open). Specifying the , or attributes forces the use of the Apache adapter and makes the attribute unavailable."
    },
    {
        "link": "https://stackoverflow.com/questions/36407575/how-to-get-files-from-resources-folder-spring-framework",
        "document": "I had the same problem trying to load some XML files into my test classes. If you use Spring, as one can suggest from your question, the easiest way is to use org.springframework.core.io.Resource - the one Raphael Roth already mentioned.\n\nThe code is really straight forward. Just declare a field of the type org.springframework.core.io.Resource and annotate it with org.springframework.beans.factory.annotation.Value - like that:\n\nTo obtain the needed InputStream, just call\n\nand you should be okay :)\n\nBut forgive me, I have to ask one thing: Why do you want to implement a XML parser with the help of Spring? There are plenty build in :) E.g. for web services there are very good solutions that marshall your XMLs into Java Objects and back..."
    },
    {
        "link": "https://baeldung.com/spring-classpath-file-access",
        "document": "In this tutorial, we’ll demonstrate various ways to access and load the contents of a file that’s on the classpath using Spring.\n\nThe Resource interface helps in abstracting access to low-level resources. In fact, it supports the handling of all kinds of file resources in a uniform manner.\n\nLet’s start by looking at various methods to obtain a Resource instance.\n\nFor accessing a resource from the classpath, we can simply use ClassPathResource:\n\nBy default, ClassPathResource removes some boilerplate by selecting between the thread’s context classloader and the default system classloader.\n\nHowever, we can also indicate the classloader to use either directly:\n\nOr indirectly through a specified class:\n\nNote that from Resource, we can easily jump to Java standard representations like InputStream or File.\n\nAnother thing to note here is that the above method works only for absolute paths. If we want to specify a relative path, we can pass a second class argument. The path will be relative to this class:\n\nThe file path above is relative to the Example class.\n\nWe can also inject a Resource with @Value:\n\n@Value supports other prefixes too, like file: and url:.\n\nIf we want to lazily load our resource, we can use ResourceLoader:\n\nThen we retrieve our resource with getResource:\n\nNote too that ResourceLoader is implemented by all concrete ApplicationContexts, which means that we can also simply depend on ApplicationContext if that suits our situation better:\n\nAs a caveat, there is another way to retrieve resources in Spring, but the ResourceUtils Javadoc is clear that the class is mainly for internal use.\n\nIf we see usages of ResourceUtils in our code:\n\nWe should carefully consider the rationale, as it’s probably better to use one of the standard approaches above.\n\nOnce we have a Resource, it’s easy for us to read the contents. As we have already discussed, we can easily obtain a File or an InputStream reference from the Resource.\n\nLet’s imagine we have the following file, data/employees.dat, on the classpath:\n\nNow we can read its contents by calling getFile:\n\nAlthough, it should be noted that this approach expects the resource to be present in the filesystem and not within a jar file.\n\nLet’s say though, that our resource is inside a jar.\n\nThen we can instead read a Resource as an InputStream:\n\nIn this brief article, we’ve examined a few ways to access and read a resource from the classpath using Spring. This includes eager and lazy loading, and on the filesystem or in a jar."
    },
    {
        "link": "https://medium.com/@arthurraposodev/test-694cf70bd013",
        "document": "A common task when developing applications is reading from input files and writing to output files. Most of the time, we will reference files contained in our application resources, as they are more easily accessible and generally fall in our domain. Hoping to address this pain point, the Spring Framework and Java offer libraries to make this task a bit easier on the developer. In this article, we will quickly go over the main alternatives.\n\nThe resources folder holds a special place for a java developer as it is part of the classpath. Any files stored here are packaged into the applications JAR file and can be accessed while the application is running. This makes it an ideal location for storing configuration files, data files or any other files required by your application.\n\nThere are approaches to read a file in a Spring Boot application, each with its own advantages and disadvantages. We will discuss the following methods:\n\nThis method uses the Resource interface and will lazily load the resource file for use.\n\nThis method uses Spring’s @Value annotation while passing a classpath.\n\nIt is important to note that since @Value is populated after bean instantiation, this will not work if used inside a constructor or while Spring is still setting up the context.\n\nIn both cases, it is possible to read from the Resource interface to a string using a method like the following:\n\nis a part of the Spring framework that provides another way to access files within the classpath, like those in the directory. It is particularly useful when dealing with resources in a more direct way.\n\nWhen it’s better: This method is ideal when you need a simple and straightforward approach to access classpath resources without autowiring additional components like . It is also handy when dealing with resource files in a context where dependency injection is not easily available, such as a during bean instantiation.\n\nMany are familiar with Java’s traditional IO package, but there is also the newer Java NIO (New I/O) library. This library provides a different approach to handle file operations, offering a non-blocking, more modern and scalable solution. It’s part of the standard Java library and does not depend on Spring-specific classes.\n\nNote: When using Java NIO, you need to handle URISyntaxException along with IOException, as the method can throw this exception.\n\nThe choice of method largely depends on the specific requirements of your application. If you are working within the Spring ecosystem and require a quick and simple solution, , or might be your best bet. On the other hand, if you are dealing with more complex file operations or require a solution that's not tightly coupled with Spring, Java NIO offers a robust alternative."
    },
    {
        "link": "https://mybatis.org/spring/sqlsession.html",
        "document": "In MyBatis you use the to create an . Once you have a session, you use it to execute your mapped statements, commit or rollback connections and finally, when it is no longer needed, you close the session. With MyBatis-Spring you don't need to use directly because your beans can be injected with a thread safe that automatically commits, rollbacks and closes the session based on Spring's transaction configuration.\n\nis the heart of MyBatis-Spring. It implements and is meant to be a drop-in replacement for any existing use of in your code. is thread safe and can be shared by multiple DAOs or mappers.\n\nWhen calling SQL methods, including any method from Mappers returned by , will ensure that the used is the one associated with the current Spring transaction. In addition, it manages the session life-cycle, including closing, committing or rolling back the session as necessary. It will also translate MyBatis exceptions into Spring s.\n\nshould always be used instead of default MyBatis implementation because the template can participate in Spring transactions and is thread safe for use by multiple injected mapper classes. Switching between the two classes in the same application can cause data integrity issues.\n\nA can be constructed using an as a constructor argument.\n\nThis bean can now be injected directly in your DAO beans. You need a property in your bean like the following\n\nAnd inject the as follows\n\nhas also a constructor that takes an as an argument. This allows you to construct, for example, a batch by using the following in Spring's configuration file:\n\nNow all your statements will be batched so the following could be coded in a DAO\n\nNote that this configuration style only needs to be used if the desired execution method differs from the default set for the .\n\nThe caveat to this form is that there cannot be an existing transaction running with a different ExecutorType when this method is called. Either ensure that calls to s with different executor types run in a separate transaction (e.g. with ) or completely outside of a transaction."
    },
    {
        "link": "https://stackoverflow.com/questions/55655413/how-to-execute-several-sql-statements-in-one-session-with-mybatis-string",
        "document": "I am setting up an application with mybatis-spring, which should execute several sql-statements (mostly selects) and print the result to the console.\n\nMy applicationContext.xml looks like this:\n\nI have noticed that whenever I execute a sql-statement the session is created just for this statement and closes directly after execution.\n\nIs there a way to execute multiple sql-statements in just one session, which closes itself not until all methods/statements are executed?\n\nThanks and Greetings."
    },
    {
        "link": "https://stackoverflow.com/questions/7174225/mybatis-executing-multiple-sql-statements-in-one-go-is-that-possible",
        "document": "Yes, most databases allow this. Usually you have to delimit your SQL statements with something. In PostGRES and MySQL it's a semicolon (;). In Microsoft SQL server you should use the keyword GO. [ May 2013 Update: As of SQL Server 2012, you can and should use semicolons to delimit your statements. After SQL Server 2012 (ie. the next version and beyond) these will be mandatory. Using GO is now the deprecated way to do things in SQL2012 and beyond). ]\n\nBetter databases (ie. not MySQL) will also support transactions with BEGIN TRAN / COMMIT TRAN / ROLLBACK TRAN. Using transactions you can actually batch all the statements into one atomic operation, where if part of it failed, all three would be rolled back. See http://www.sqlteam.com/article/introduction-to-transactions for some more information about those.\n\nMost likely all you need is the semicolons between your SQL statements though!"
    },
    {
        "link": "https://mybatis.org/spring-boot-starter/mybatis-spring-boot-autoconfigure",
        "document": "The MyBatis-Spring-Boot-Starter help you build quickly MyBatis applications on top of the Spring Boot. By using this module you will achieve:\n• Reduce the boilerplate to almost zero\n\nTo use the MyBatis-Spring-Boot-Starter module, you just need to include the file and its dependencies( , and etc …) in the classpath. If you are using Maven just add the following dependency to your : If using gradle add this to your :\n\nAs you may already know, to use MyBatis with Spring you need at least an and at least one mapper interface.\n• Will create and register an instance of a passing that as an input using the\n• Will create and register an instance of a got out of the\n• Auto-scan your mappers, link them to the and register them to Spring context so they can be injected into your beans Suppose we have the following mapper: You just need to create a normal Spring boot application and let the mapper be injected like follows(available on Spring 4.3+): This is all you have to do. You application can now be run as a normal Spring Boot application.\n\nThe MyBatis-Spring-Boot-Starter will search, by default, for mappers marked with the annotation. You may want to specify a custom annotation or a marker interface for scanning. If so, you must use the annotation. See more about it in the MyBatis-Spring reference page. The MyBatis-Spring-Boot-Starter will not start the scanning process if it finds at least one in the Spring's context so if you want to stop the scanning at all you should register your mappers explicitly with methods.\n\nAs any other Spring Boot application a MyBatis-Spring-Boot-Application configuration parameters are stored inside the (or ). MyBatis uses the prefix for its properties. Indicates whether perform presence check of the MyBatis xml config file. Packages to search for type aliases. (Package delimiters are “ ”) The super class for filtering type alias. If this not specifies, the MyBatis deal as type alias all classes that searched from . Packages to search for type handlers. (Package delimiters are “ ”) The default scripting language driver class. This feature requires to use together with mybatis-spring 2.0.2+. Externalized properties for MyBatis configuration. Specified properties can be used as placeholder on MyBatis config file and Mapper file. For detail see the MyBatis reference page. Whether enable lazy initialization of mapper bean. Set to enable lazy initialization. This feature requires to use together with mybatis-spring 2.0.2+. Default scope for mapper bean that scanned by auto-configure. This feature requires to use together with mybatis-spring 2.0.6+. Set whether inject a or bean (If you want to back to the behavior of 2.2.1 or before, specify ). If you use together with spring-native, should be set (default). Property keys for bean provided by MyBatis Core. About available nested properties see the MyBatis reference page. : This property cannot be used at the same time with the . Property keys for bean provided by MyBatis Thymeleaf. About available nested properties see the MyBatis Thymeleaf reference page. Properties keys for bean provided by MyBatis FreeMarker. About available nested properties see the MyBatis FreeMarker reference page. This feature requires to use together with mybatis-freemarker 1.2.0+. Properties keys for bean provided by MyBatis Velocity. About available nested properties see the MyBatis Velocity reference page. This feature requires to use together with mybatis-velocity 2.1.0+.\n\nThe MyBatis-Spring-Boot-Starter provides the as an implementation class of . The is used for searching classes (e.g. target class of type alias, type handler class) from an application (or application server). If you run a Spring Boot application using the executable jar, you need to use the . The auto-configuration feature provided by the MyBatis-Spring-Boot-Starter used it automatically, but it does not use automatically by a manual configuration (e.g. when uses multiple ). How to use the on manual configuration: @Configuration public class MyBatisConfig { @Bean public SqlSessionFactory masterSqlSessionFactory() throws Exception { SqlSessionFactoryBean factoryBean = new SqlSessionFactoryBean(); factoryBean.setDataSource(masterDataSource()); factoryBean.setVfs(SpringBootVFS.class); // Sets the SpringBootVFS class into SqlSessionFactoryBean // ... return factoryBean.getObject(); } }"
    },
    {
        "link": "https://baeldung.com/spring-mybatis",
        "document": "MyBatis is one of the most commonly used open-source frameworks for implementing SQL databases access in Java applications.\n\nIn this quick tutorial, we’ll present how to integrate MyBatis with Spring and Spring Boot.\n\nFor those not yet familiar with this framework, be sure to check out our article on working with MyBatis.\n\nLet’s start by defining simple POJO that we’ll use throughout our article:\n\nNext, let’s create a data.sql file, which simply inserts one record into our articles table:\n\nBoth SQL files must be included in the classpath.\n\nTo start using MyBatis, we have to include two main dependencies — MyBatis and MyBatis-Spring:\n\nApart from that, we’ll need basic Spring dependencies:\n\nIn our examples, we’ll use the H2 embedded database to simplify the setup and EmbeddedDatabaseBuilder class from the spring-jdbc module for configuration:\n\nSpring simplifies the configuration for MyBatis. The only required elements are javax.sql.Datasource, org.apache.ibatis.session.SqlSessionFactory, and at least one mapper.\n\nWe also applied a @MapperScan annotation from MyBatis-Spring that scans defined packages and automatically picks up interfaces using any of the mapper annotations, such as @Select or @Delete.\n\nUsing @MapperScan also ensures that every provided mapper is automatically registered as a Bean and can be later used with the @Autowired annotation.\n\nWe can now create a simple ArticleMapper interface:\n\nIn the above example, we’ve used MyBatis to retrieve the only record we inserted previously in our data.sql file.\n\nAs previously described, to use MyBatis with Spring, we need Datasource, SqlSessionFactory, and at least one mapper.\n\nLet’s create the required bean definitions in the beans.xml configuration file:\n\nIn this example, we also used the custom XML schema provided by spring-jdbc to configure our H2 datasource.\n\nTo test this configuration, we can reuse the previously implemented test class. However, we have to adjust the context configuration, which we can do by applying the annotation:\n\nSpring Boot provides mechanisms that simplify the configuration of MyBatis with Spring even more.\n\nFirst, let’s add the mybatis-spring-boot-starter dependency to our pom.xml:\n\nBy default, if we use an auto-configuration feature, Spring Boot detects the H2 dependency from our classpath and configures both Datasource and SqlSessionFactory for us. In addition, it also executes both schema.sql and data.sql on startup.\n\nIf we don’t use an embedded database, we can use configuration via an application.yml or application.properties file or define a Datasource bean pointing to our database.\n\nThe only thing we have left to do is to define a mapper interface, in the same manner as before, and annotate it with the @Mapper annotation from MyBatis. As a result, Spring Boot scans our project, looking for that annotation, and registers our mappers as beans.\n\nAfter that, we can test our configuration using the previously defined test class by applying annotations from spring-boot-starter-test:\n\nIn this article, we explored multiple ways of configuring MyBatis with Spring.\n\nWe looked at examples of using annotation-based and XML configuration and showed the auto-configuration features of MyBatis with Spring Boot."
    },
    {
        "link": "https://forum.xwiki.org/t/java-11-best-practices/10328",
        "document": "We have adopted java 11 since the beginning of the 14.x cycle. On thing that haven’t been discussed is the best practices regarding the use of new features introduced in java 9, 10, and 11.\n\nMost of the changes are at the JVM level, or are part of the Java API.\n\nI think we can agree that new Java API can be used at will, except for code that have high chances to be back-ported to older branches.\n\nOne syntactic change has also been introduced in Java 10, LocalVariable Type-Inference (a.k.a., the keyword), and that what I’d like to discuss.\n\nIn summary, using the keyword let the compiler decide the actual static type of a variable from the context.\n\n Note that this can only be used for local variables and, since java 11, for lambas parameters (and not for methods parameters or fields declaration for instance).\n\nFor the pros and cons, I’d like to quote OpenJDK Style guidelines:\n\nThere is a certain amount of controversy over this feature. Some welcome the concision it enables; others fear that it deprives readers of important type information, impairing readability. And both groups are right. It can make code more readable by eliminating redundant information, and it can also make code less readable by eliding useful information. Another group worries that it will be overused, resulting in more bad Java code being written. This is also true, but it’s also likely to result in more good Java code being written. Like all features, it must be used with judgment. There’s no blanket rule for when it should and shouldn’t be used.\n\nSo the questions is, do you agree to allow the use of the keyword on 14.x+.\n\n If we go for yes, I suggest to do so only while following the style guidelines of openjdk."
    },
    {
        "link": "https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/ArrayList.html",
        "document": "Resizable-array implementation of theinterface. Implements all optional list operations, and permits all elements, including. In addition to implementing theinterface, this class provides methods to manipulate the size of the array that is used internally to store the list. (This class is roughly equivalent to, except that it is unsynchronized.)\n\nThe , , , , , and operations run in constant time. The operation runs in amortized constant time, that is, adding n elements requires O(n) time. All of the other operations run in linear time (roughly speaking). The constant factor is low compared to that for the implementation.\n\nEach instance has a capacity. The capacity is the size of the array used to store the elements in the list. It is always at least as large as the list size. As elements are added to an ArrayList, its capacity grows automatically. The details of the growth policy are not specified beyond the fact that adding an element has constant amortized time cost.\n\nAn application can increase the capacity of an instance before adding a large number of elements using the operation. This may reduce the amount of incremental reallocation.\n\nNote that this implementation is not synchronized. If multiple threads access an instance concurrently, and at least one of the threads modifies the list structurally, it must be synchronized externally. (A structural modification is any operation that adds or deletes one or more elements, or explicitly resizes the backing array; merely setting the value of an element is not a structural modification.) This is typically accomplished by synchronizing on some object that naturally encapsulates the list. If no such object exists, the list should be \"wrapped\" using the method. This is best done at creation time, to prevent accidental unsynchronized access to the list:\n\nThe iterators returned by this class's and methods are fail-fast: if the list is structurally modified at any time after the iterator is created, in any way except through the iterator's own or methods, the iterator will throw a . Thus, in the face of concurrent modification, the iterator fails quickly and cleanly, rather than risking arbitrary, non-deterministic behavior at an undetermined time in the future.\n\nNote that the fail-fast behavior of an iterator cannot be guaranteed as it is, generally speaking, impossible to make any hard guarantees in the presence of unsynchronized concurrent modification. Fail-fast iterators throw on a best-effort basis. Therefore, it would be wrong to write a program that depended on this exception for its correctness: the fail-fast behavior of iterators should be used only to detect bugs.\n\nThis class is a member of the Java Collections Framework."
    },
    {
        "link": "https://stackoverflow.com/questions/29726891/best-practice-to-declare-arraylist-or-collection-implementation-classes",
        "document": "The first two use raw types. Doing that means your list isn't type-safe at all. The compiler will let you store Integers inside even if your intention is to have a list of strings. The compiler will emit a warning, that you should not ignore.\n\nThe third one is right. It tells the compiler that your intention is to use a List of strings, and that the specific implementation you chose is ArrayList. If you change your mind later and want to use a LinkedList, this line of code is the only one you need to change.\n\nThe fourth one tells the compiler that your program doest't just need a List. It needs this List to be an ArrayList. This is OK if your code indeed needs to call methods that are specific to ArrayList, and are not present in the List interface. But in 99.9% of the cases, that's not the case and you should prefer the third one.\n\nThe two last ones declare a variable and initialize it to null instead of creating a list. That is a design smell. You'll have to make sure everywhere, before using the list, that it's not null. It's much safer to initialize it with a valid list right away."
    },
    {
        "link": "https://reddit.com/r/java/comments/p1jz1z/is_using_arraylist_good_practice_for_a_regular",
        "document": "Is using ArrayList good practice for a regular software engineering job?\n\nEverytime I do LeetCode or codewars problems that contain array manipulations they never use ArrayList. How often do software engineers use ArrayList? Is it useful? Is it practical?\n\nArchived post. New comments cannot be posted and votes cannot be cast."
    },
    {
        "link": "https://baeldung.com/java-arraylist",
        "document": "In this tutorial, we’ll look at the ArrayList class from the Java Collections Framework. We’ll discuss its properties, common use cases, and advantages and disadvantages.\n\nArrayList resides within Java Core Libraries; therefore, we don’t need additional libraries. To use it, we add its import statement:\n\nThe List represents an ordered sequence of values where a value can occur more than once.\n\nArrayList is a List implementation built atop an array that can dynamically grow and shrink as we add/remove elements. We can easily access an element by its index starting from zero. This implementation has the following properties:\n• Searching takes O(n) time for an unsorted array and O(log n) for a sorted one\n\nAt the outset, ArrayList<E> is a generic class; therefore, we can parameterize it with any type we want. The compiler ensures that we can’t use a non-compatible type. For example, we can’t put Integer values inside a collection of Strings. Furthermore, we don’t need to cast elements when retrieving them from a collection.\n\nWe should use the generic interface List<E> as a variable type as a best practice because it decouples it from any specific implementation.\n\nWe can create an empty ArrayList instance using the no-arg constructor:\n\nWe can specify the initial length of an underlying array to avoid unnecessary resizing while adding new items using the constructor that accepts initial capacity:\n\nWe can create a new ArrayList instance using elements of a Collection instance for populating the underlying array:\n\nWe can add an element either at the end or at a specific index position:\n\nWe can also add a collection or a batch of elements:\n\nWe have two types of iterators available: Iterator and ListIterator.\n\nWe use an Iterator to traverse the list in one direction only and a ListIterator to traverse it in both directions.\n\nLet’s use the ListIterator as an example:\n\nWe can also search, add, or remove elements using iterators.\n\nWe may use the indexOf() or the lastIndexOf() method to find an element. They both accept an object and return an int value:\n\nIf we want to find all elements satisfying a predicate, we may filter collection using Java 8 Stream API using a Predicate:\n\nIt is also possible to use a for loop or an iterator:\n\nTo search a sorted array we may use a binary search algorithm, which works faster than linear search:\n\nNotice that if an element isn’t found then -1 is returned.\n\nTo remove an element, we find its index and then remove it using the remove() method. We can also use an overloaded version of this method, which accepts an object, searches for it, and removes its first occurrence:\n\nLet’s remember that when working with boxed types such as Integer, to remove a particular element, we should first box int value or otherwise, an element is removed by its index.\n\nWe can use an iterator to remove several items:\n\nWe may use the Stream API for removing several items, but we won’t show it here.\n\nWe can add, get, and remove the first or the last element in an ArrayList using a sequenced collection. Sequenced collections are introduced in Java 21 with the new java.util.SequencedCollection<E> interface. A sequenced collection has a well-defined encounter order for its elements. Accordingly, the elements have a linear arrangement: first element, second element, …, and last element. With java.util.Collection<E> as the root interface in the collection hierarchy, the java.util.SequencedCollection<E> extends it to provide a sequential arrangement for a collection’s elements.\n\nThe java.util.SequencedCollection<E> interface provides several methods for adding/getting/removing an element that’s either first or last in the sequence:\n\nLet’s discuss with examples how to perform the add/get/remove operations on an ArrayList that’s a sequenced collection.\n\n7.1. Getting First or Last Element\n\nTo get the first element, we call the instance method getFirst(). To demonstrate, let’s create an ArrayList:\n\nWe can use a JUnit 5 test with an assertEquals assertion to verify the first element returned:\n\nThe JUnit assertion test should pass. Similarly, to get the last element, we call the instance method getLast(). To demonstrate, let’s create an ArrayList as before. Again, we can use a JUnit 5 test to verify the last element returned:\n\nThe JUnit assertion test should pass because the getLast() returns the last element.\n\n7.2. Adding First or Last Element\n\nTo add a new first element in an existing collection, we call the instance method addFirst(E e). To demonstrate, let’s create an ArrayList, and add a new first element:\n\nWe can use a JUnit 5 test with an assertEquals assertion to verify the first element added:\n\nSimilarly, to add the last element to an existing collection, we call the instance method addLast(E e). Again, let’s create an ArrayList and use a JUnit 5 test to verify the last element added:\n\nThe JUnit assertion test should pass because the getLast() returns the last element added with addLast(E e).\n\n7.3. Removing First or Last Element\n\nTo remove the first element in an existing collection, we call the instance method removeFirst(). To demonstrate, let’s create an ArrayList and remove the first element. We can use a JUnit 5 test with an assertEquals assertion to verify the first element is removed:\n\nSimilarly, to remove the last element in an existing collection, we call the instance method removeLast(). Let’s create an ArrayList and use a JUnit 5 test to verify the last element is removed:\n\nThe JUnit assertion test should pass because the removeLast() returns the removed last element.\n\nIn this quick article, we had a look at the ArrayList in Java.\n\nWe showed how to create an ArrayList instance, and how to add, find, or remove elements using different approaches. Furthermore, we showed how to add, get, and remove the first, or the last element using sequenced collections introduced in Java 21."
    }
]