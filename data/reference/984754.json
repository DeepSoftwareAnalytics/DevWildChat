[
    {
        "link": "https://veeevek.medium.com/max-pooling-in-cnns-dcd1eec655a5",
        "document": "Max Pooling is used in Convolutional Neural Networks which is used to downsample an image to acquire spatial invariance. It is used after a convolutional layer and before the fully connected layer.\n\nIn convolutional neural networks, you look at an image through a smaller window and move that window to the right and down. That way you can find features in that window, for example a horizontal line or a vertical line or a curve, or edge, or shapes etc. Wherever you find those features, you put that in the feature maps. These combination of features are then used to identify different objects in images during testing or in production.\n\nWhy do we need Max Pooling ?\n\nA problem with feature maps output of a convolution layer is that they are dependent on the location of different features in an image i.e. the layer records the precise position of features in the input/image. So when you give this kind of a model to recognize an object in an image where the object is differently positioned, the model will fail to recognize this object.\n\nYour CNN model should be able to identify all the above images as Cheetah even though your image would be tilted, angled different, rotated, horizontally suqashed, or posing differently.\n\nSo when we downsample an image using max pooling, we negate this dependency of features on locations so that the model can easily identify different images without any positional hinderance. A lower resolution image is created which still contains important structural elements without the fine details( which may not be relevant like for example positions of features of an object.) This lower resolution of an image also helps in creating a faster model and predicting the results in less time.\n\nThis property makes the network capable of detecting the object in the image without being confused by the differences in the image’s textures, the distances from where they are shot, their angles, or otherwise. In order to do that, the network needs to acquire a property that is known as “spatial invariance.”\n\nIt allows the CNN to detect features/objects even if it does not look exactly like the images in it’s training period.\n\nWe select a matrix of filter which is applied to the feature map, so that the resultant output is smaller than the input. For example, a pooling filter of 2*2 (4 pixels) when applied to a feature map of 4*4 (16 pixels) will result in an output pooled feature map of 2*2 (4 pixels).\n\nYou see in the above output calculation, we consider the highest value for every block(2*2) of pooling filter size matrix from the feature map matrix. This step helps in removing any unnecessary features. When you take the highest values from that block of matrix while applying max pool, it accounts for only important features in the output.\n\nStride is a parameter of the neural network’s filter that modifies the amount of movement over the image or video.\n\nCheck the Tensorflow official docs to understand the parameters in the code.\n\nTwo common operations of pooling are :\n\nMax Pooling & Average Pooling. In Max Pooling we calculate the maximum value for each patch of the feature map. In Average pooling, we calculate the average value for each patch in a feature map."
    },
    {
        "link": "https://deeplizard.com/learn/video/ZjM_XQa5s6s",
        "document": "Hey, what's going on everyone? In this post, we're going to discuss what max pooling is in a convolutional neural network. Without further ado, let's get started. We're going to start out by explaining what max pooling is, and we'll show how it's calculated by looking at some examples. We'll then discuss the motivation for why max pooling is used, and we'll see how we can add max pooling to a convolutional neural network in code using Keras. We're going to be building on some of the ideas that we discussed in our post on CNNs, so if you haven't seen that yet, go ahead and check it out, and then come back to read this post once you've finished up there.\n\nMax pooling is a type of operation that is typically added to CNNs following individual convolutional layers. When added to a model, max pooling reduces the dimensionality of images by reducing the number of pixels in the output from the previous convolutional layer. Let's go ahead and check out a couple of examples to see what exactly max pooling is doing operation-wise, and then we'll come back to discuss why we may want to use max pooling. Example using a sample from the MNIST dataset We've seen in our post on CNNs that each convolutional layer has some number of filters that we define with a specified dimension and that these filters convolve our image input channels. When a filter convolves a given input, it then gives us an output. This output is a matrix of pixels with the values that were computed during the convolutions that occurred on our image. We call these output channels. We're going to be using the same image of a seven that we used in our previous post on CNNs. Recall, we have a matrix of the pixel values from an image of a from the MNIST data set. We used a filter to produce the output channel below: As mentioned earlier, max pooling is added after a convolutional layer. This is the output from the convolution operation and is the input to the max pooling operation. After the max pooling operation, we have the following output channel: Max pooling works like this. We define some region as a corresponding filter for the max pooling operation. We're going to use in this example. We define a stride, which determines how many pixels we want our filter to move as it slides across the image. Stride determines how many units the filter slides. On the convolutional output, and we take the first region and calculate the max value from each value in the block. This value is stored in the output channel, which makes up the full output from this max pooling operation. We move over by the number of pixels that we defined our stride size to be. We're using here, so we just slide over by , then do the same thing. We calculate the max value in the next block, store it in the output, and then, go on our way sliding over by again. Once we reach the edge over on the far right, we then move down by (because that's our stride size), and then we do the same exact thing of calculating the max value for the blocks in this row. We can think of these blocks as pools of numbers, and since we're taking the max value from each pool, we can see where the name max pooling came from. This process is carried out for the entire image, and when we're finished, we get the new representation of the image, the output channel. In this example, our convolution operation output is in size. After performing max pooling, we can see the dimension of this image was reduced by a factor of and is now . Just to make sure we fully understand this operation, we're going to quickly look at a scaled down example that may be more simple to visualize. Suppose we have the following: We have some sample input of size , and we're assuming that we have a filter size with a stride of to do max pooling on this input channel. Our first region is in orange, and we can see the max value of this region is , and so we store that over in the output channel. Next, we slide over by pixels, and we see the max value in the green region is . As a result, we store the value over in the output channel. Since we've reached the edge, we now move back over to the far left, and go down by pixels. Here, the max value in the blue region is , and we store that here in our output channel. Finally, we move to the right by , and see the max value of the yellow region is . We store this value in our output channel. This completes the process of max pooling on this sample input channel, and the resulting output channel is this block. As a result, we can see that our input dimensions were again reduced by a factor of two. Alright, we know what max pooling is and how it works, so let's discuss why would we want to add this to our network?\n\nThere are a couple of reasons why adding max pooling to our network may be helpful. Since max pooling is reducing the resolution of the given output of a convolutional layer, the network will be looking at larger areas of the image at a time going forward, which reduces the amount of parameters in the network and consequently reduces computational load. Additionally, max pooling may also help to reduce overfitting. The intuition for why max pooling works is that, for a particular image, our network will be looking to extract some particular features. Maybe, it's trying to identify numbers from the MNIST dataset, and so it's looking for edges, and curves, and circles, and such. From the output of the convolutional layer, we can think of the higher valued pixels as being the ones that are the most activated. With max pooling, as we're going over each region from the convolutional output, we're able to pick out the most activated pixels and preserve these high values going forward while discarding the lower valued pixels that are not as activated. Just to mention quickly before going forward, there are other types of pooling that follow the exact same process we've just gone through, except for that it does some other operation on the regions rather than finding the max value. For example, average pooling is another type of pooling, and that's where you take the average value from each region rather than the max. Currently max pooling is used vastly more than average pooling, but I did just want to mention that point. Alright, now let's jump over to Keras and see how this is done in code.\n\nHere, we have a completely arbitrary CNN.\n\nIt has an input layer that accepts input of dimensions, then a dense layer followed by a convolutional layer followed by a max pooling layer, and then one more convolutional layer, which is finally followed by an output layer.\n\nFollowing the first convolutional layer, we specify max pooling. Since the convolutional layers are here, We're using the layer from Keras, but Keras also has and max pooling layers as well.\n\nThe first parameter we're specifying is the . This is the size of what we were calling a filter before, and in our example, we used a filter.\n\nThe next parameter is . Again, in our earlier examples, we used as well, so that's what we've specified here. The last parameter that we have specified is the parameter. If you're unsure what padding or zero-padding is in regards to CNNs, be sure to check out the earlier post that explains the concept.\n\nRecall from that post, we discussed how valid padding means to use no padding, that's what we've specified here, and actually I don't think it's a common practice at all to use padding on max pooling layers.\n\nBut while we're on the subject of padding, I wanted to point something else out, which is that for the two convolutional layers, we've specified same padding so that the input is padded such that the output of the convolutional layers will be the same size as the input.\n\nIf we go ahead and look at a summary of our model, we can see that the dimensions from the output of our first layer are , which matches the original input size. The dimensions of the output from our first convolutional layer maintain the same values because we're using same padding on that layer.\n\nOnce we go down to the max pooling layer, we see the value of the dimensions has been cut in half to become . This is because, as we saw with our earlier examples, a filter of size along with a stride of for our max pooling layer will reduce the dimensions of our input by a factor of two, so that's exactly what we see here.\n\nLastly, this max pooling layer is followed by one last convolutional layer that is using same padding, so we can see that the output shape for this last layer maintains the dimensions from the previous max pooling layer."
    },
    {
        "link": "https://geeksforgeeks.org/cnn-introduction-to-pooling-layer",
        "document": "Pooling layer is used in CNNs to reduce the spatial dimensions (width and height) of the input feature maps while retaining the most important information. It involves sliding a two-dimensional filter over each channel of a feature map and summarizing the features within the region covered by the filter.\n\nFor a feature map with dimensions [Tex]n_h \\times n_w \\times n_c[/Tex], the dimensions of the output after a pooling layer are:\n• [Tex]n_c[/Tex] → number of channels in the feature map\n\nA typical CNN model architecture consists of multiple convolution and pooling layers stacked together.\n• Dimensionality Reduction : Pooling layers reduce the spatial size of the feature maps, which decreases the number of parameters and computations in the network. This makes the model faster and more efficient.\n• Translation Invariance : Pooling helps the network become invariant to small translations or distortions in the input image. For example, even if an object in an image is slightly shifted, the pooled output will remain relatively unchanged.\n• Overfitting Prevention : By reducing the spatial dimensions, pooling layers help prevent overfitting by providing a form of regularization.\n• Feature Hierarchy : Pooling layers help build a hierarchical representation of features, where lower layers capture fine details and higher layers capture more abstract and global features.\n\nMax pooling selects the maximum element from the region of the feature map covered by the filter. Thus, the output after max-pooling layer would be a feature map containing the most prominent features of the previous feature map.\n\nMax pooling layer preserves the most important features (edges, textures, etc.) and provides better performance in most cases.\n\nAverage pooling computes the average of the elements present in the region of feature map covered by the filter. Thus, while max pooling gives the most prominent feature in a particular patch of the feature map, average pooling gives the average of features present in a patch.\n\nAverage pooling provides a more generalized representation of the input. It is useful in the cases where preserving the overall context is important.\n\nGlobal pooling reduces each channel in the feature map to a single value, producing a [Tex]1 \\times 1 \\times n_c[/Tex] output. This is equivalent to applying a filter of size [Tex]n_h × n_w[/Tex].\n\nThere are two types of global pooling:\n• Global Max Pooling : Takes the maximum value across the entire feature map.\n• Global Average Pooling : Computes the average of all values in the feature map.\n• Define a Pooling Window (Filter) : The size of the pooling window (e.g., 2×2) is chosen, along with a stride (the step size by which the window moves). A common choice is a 2×2 window with a stride of 2, which reduces the feature map size by half.\n• Slide the Window Over the Input : The pooling operation is applied to each region of the input feature map covered by the window.\n• Apply the Pooling Operation : Depending on the type of pooling (max, average, etc.), the operation extracts the required value from each window.\n• Output the Downsampled Feature Map : The result is a smaller feature map that retains the most important information.\n\nKey Factors to Consider for Optimizing Pooling Layer\n• Pooling Window Size : The size of the pooling window affects the degree of downsampling. A larger window results in more aggressive downsampling but may lose important details.\n• Stride : The stride determines how much the pooling window moves at each step. A larger stride results in greater dimensionality reduction.\n• Padding : In some cases, padding is used to ensure that the pooling operation covers the entire input feature map.\n• Dimensionality reduction: Pooling layer helps in reducing the spatial dimensions of the feature maps. This reduces the computational cost and also helps in avoiding overfitting by reducing the number of parameters in the model.\n• Translation invariance: Pooling layers are useful in achieving translation invariance in the feature maps. This means that the position of an object in the image does not affect the classification result, as the same features are detected regardless of the position of the object.\n• Feature selection: Pooling layers help in selecting the most important features from the input, as max pooling selects the most salient features and average pooling provides a balanced representation.\n• Information Loss : Pooling reduces spatial resolution, which can lead to a loss of important fine details.\n• Hyperparameter Tuning : The choice of pooling size and stride affects performance and requires careful tuning."
    },
    {
        "link": "https://cs231n.github.io/convolutional-networks",
        "document": "Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply.\n\nSo what changes? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.\n\nRecall: Regular Neural Nets. As we saw in the previous chapter, Neural Networks receive an input (a single vector), and transform it through a series of hidden layers. Each hidden layer is made up of a set of neurons, where each neuron is fully connected to all neurons in the previous layer, and where neurons in a single layer function completely independently and do not share any connections. The last fully-connected layer is called the “output layer” and in classification settings it represents the class scores.\n\nRegular Neural Nets don’t scale well to full images. In CIFAR-10, images are only of size 32x32x3 (32 wide, 32 high, 3 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 32*32*3 = 3072 weights. This amount still seems manageable, but clearly this fully-connected structure does not scale to larger images. For example, an image of more respectable size, e.g. 200x200x3, would lead to neurons that have 200*200*3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.\n\n3D volumes of neurons. Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth. (Note that the word depth here refers to the third dimension of an activation volume, not to the depth of a full Neural Network, which can refer to the total number of layers in a network.) For example, the input images in CIFAR-10 are an input volume of activations, and the volume has dimensions 32x32x3 (width, height, depth respectively). As we will soon see, the neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner. Moreover, the final output layer would for CIFAR-10 have dimensions 1x1x10, because by the end of the ConvNet architecture we will reduce the full image into a single vector of class scores, arranged along the depth dimension. Here is a visualization:\n\nAs we described above, a simple ConvNet is a sequence of layers, and every layer of a ConvNet transforms one volume of activations to another through a differentiable function. We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer (exactly as seen in regular Neural Networks). We will stack these layers to form a full ConvNet architecture.\n\nExample Architecture: Overview. We will go into more details below, but a simple ConvNet for CIFAR-10 classification could have the architecture [INPUT - CONV - RELU - POOL - FC]. In more detail:\n• INPUT [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.\n• CONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.\n• RELU layer will apply an elementwise activation function, such as the \\(max(0,x)\\) thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).\n• POOL layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].\n• FC (i.e. fully-connected) layer will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.\n\nIn this way, ConvNets transform the original image layer by layer from the original pixel values to the final class scores. Note that some layers contain parameters and other don’t. In particular, the CONV/FC layers perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons). On the other hand, the RELU/POOL layers will implement a fixed function. The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image.\n• A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores)\n• There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)\n• Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function\n• Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don’t)\n• Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn’t)\n\nWe now describe the individual layers and the details of their hyperparameters and their connectivities.\n\nThe Conv layer is the core building block of a Convolutional Network that does most of the computational heavy lifting.\n\nOverview and intuition without brain stuff. Let’s first discuss what the CONV layer computes without brain/neuron analogies. The CONV layer’s parameters consist of a set of learnable filters. Every filter is small spatially (along width and height), but extends through the full depth of the input volume. For example, a typical filter on a first layer of a ConvNet might have size 5x5x3 (i.e. 5 pixels width and height, and 3 because images have depth 3, the color channels). During the forward pass, we slide (more precisely, convolve) each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position. As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position. Intuitively, the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the first layer, or eventually entire honeycomb or wheel-like patterns on higher layers of the network. Now, we will have an entire set of filters in each CONV layer (e.g. 12 filters), and each of them will produce a separate 2-dimensional activation map. We will stack these activation maps along the depth dimension and produce the output volume.\n\nThe brain view. If you’re a fan of the brain/neuron analogies, every entry in the 3D output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially (since these numbers all result from applying the same filter).\n\nWe now discuss the details of the neuron connectivities, their arrangement in space, and their parameter sharing scheme.\n\nLocal Connectivity. When dealing with high-dimensional inputs such as images, as we saw above it is impractical to connect neurons to all neurons in the previous volume. Instead, we will connect each neuron to only a local region of the input volume. The spatial extent of this connectivity is a hyperparameter called the receptive field of the neuron (equivalently this is the filter size). The extent of the connectivity along the depth axis is always equal to the depth of the input volume. It is important to emphasize again this asymmetry in how we treat the spatial dimensions (width and height) and the depth dimension: The connections are local in 2D space (along width and height), but always full along the entire depth of the input volume.\n\nExample 1. For example, suppose that the input volume has size [32x32x3], (e.g. an RGB CIFAR-10 image). If the receptive field (or the filter size) is 5x5, then each neuron in the Conv Layer will have weights to a [5x5x3] region in the input volume, for a total of 5*5*3 = 75 weights (and +1 bias parameter). Notice that the extent of the connectivity along the depth axis must be 3, since this is the depth of the input volume.\n\nExample 2. Suppose an input volume had size [16x16x20]. Then using an example receptive field size of 3x3, every neuron in the Conv Layer would now have a total of 3*3*20 = 180 connections to the input volume. Notice that, again, the connectivity is local in 2D space (e.g. 3x3), but full along the input depth (20).\n\nSpatial arrangement. We have explained the connectivity of each neuron in the Conv Layer to the input volume, but we haven’t yet discussed how many neurons there are in the output volume or how they are arranged. Three hyperparameters control the size of the output volume: the depth, stride and zero-padding. We discuss these next:\n• First, the depth of the output volume is a hyperparameter: it corresponds to the number of filters we would like to use, each learning to look for something different in the input. For example, if the first Convolutional Layer takes as input the raw image, then different neurons along the depth dimension may activate in presence of various oriented edges, or blobs of color. We will refer to a set of neurons that are all looking at the same region of the input as a depth column (some people also prefer the term fibre).\n• Second, we must specify the stride with which we slide the filter. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filters jump 2 pixels at a time as we slide them around. This will produce smaller output volumes spatially.\n• As we will soon see, sometimes it will be convenient to pad the input volume with zeros around the border. The size of this zero-padding is a hyperparameter. The nice feature of zero padding is that it will allow us to control the spatial size of the output volumes (most commonly as we’ll see soon we will use it to exactly preserve the spatial size of the input volume so the input and output width and height are the same).\n\nWe can compute the spatial size of the output volume as a function of the input volume size (\\(W\\)), the receptive field size of the Conv Layer neurons (\\(F\\)), the stride with which they are applied (\\(S\\)), and the amount of zero padding used (\\(P\\)) on the border. You can convince yourself that the correct formula for calculating how many neurons “fit” is given by \\((W - F + 2P)/S + 1\\). For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output. Lets also see one more graphical example:\n\nUse of zero-padding. In the example above on left, note that the input dimension was 5 and the output dimension was equal: also 5. This worked out so because our receptive fields were 3 and we used zero padding of 1. If there was no zero-padding used, then the output volume would have had spatial dimension of only 3, because that is how many neurons would have “fit” across the original input. In general, setting zero padding to be \\(P = (F - 1)/2\\) when the stride is \\(S = 1\\) ensures that the input volume and output volume will have the same size spatially. It is very common to use zero-padding in this way and we will discuss the full reasons when we talk more about ConvNet architectures.\n\nConstraints on strides. Note again that the spatial arrangement hyperparameters have mutual constraints. For example, when the input has size \\(W = 10\\), no zero-padding is used \\(P = 0\\), and the filter size is \\(F = 3\\), then it would be impossible to use stride \\(S = 2\\), since \\((W - F + 2P)/S + 1 = (10 - 3 + 0) / 2 + 1 = 4.5\\), i.e. not an integer, indicating that the neurons don’t “fit” neatly and symmetrically across the input. Therefore, this setting of the hyperparameters is considered to be invalid, and a ConvNet library could throw an exception or zero pad the rest to make it fit, or crop the input to make it fit, or something. As we will see in the ConvNet architectures section, sizing the ConvNets appropriately so that all the dimensions “work out” can be a real headache, which the use of zero-padding and some design guidelines will significantly alleviate.\n\nReal-world example. The Krizhevsky et al. architecture that won the ImageNet challenge in 2012 accepted images of size [227x227x3]. On the first Convolutional Layer, it used neurons with receptive field size \\(F = 11\\), stride \\(S = 4\\) and no zero padding \\(P = 0\\). Since (227 - 11)/4 + 1 = 55, and since the Conv layer had a depth of \\(K = 96\\), the Conv layer output volume had size [55x55x96]. Each of the 55*55*96 neurons in this volume was connected to a region of size [11x11x3] in the input volume. Moreover, all 96 neurons in each depth column are connected to the same [11x11x3] region of the input, but of course with different weights. As a fun aside, if you read the actual paper it claims that the input images were 224x224, which is surely incorrect because (224 - 11)/4 + 1 is quite clearly not an integer. This has confused many people in the history of ConvNets and little is known about what happened. My own best guess is that Alex used zero-padding of 3 extra pixels that he does not mention in the paper.\n\nParameter Sharing. Parameter sharing scheme is used in Convolutional Layers to control the number of parameters. Using the real-world example above, we see that there are 55*55*96 = 290,400 neurons in the first Conv Layer, and each has 11*11*3 = 363 weights and 1 bias. Together, this adds up to 290400 * 364 = 105,705,600 parameters on the first layer of the ConvNet alone. Clearly, this number is very high.\n\nIt turns out that we can dramatically reduce the number of parameters by making one reasonable assumption: That if one feature is useful to compute at some spatial position (x,y), then it should also be useful to compute at a different position (x2,y2). In other words, denoting a single 2-dimensional slice of depth as a depth slice (e.g. a volume of size [55x55x96] has 96 depth slices, each of size [55x55]), we are going to constrain the neurons in each depth slice to use the same weights and bias. With this parameter sharing scheme, the first Conv Layer in our example would now have only 96 unique set of weights (one for each depth slice), for a total of 96*11*11*3 = 34,848 unique weights, or 34,944 parameters (+96 biases). Alternatively, all 55*55 neurons in each depth slice will now be using the same parameters. In practice during backpropagation, every neuron in the volume will compute the gradient for its weights, but these gradients will be added up across each depth slice and only update a single set of weights per slice.\n\nNotice that if all neurons in a single depth slice are using the same weight vector, then the forward pass of the CONV layer can in each depth slice be computed as a convolution of the neuron’s weights with the input volume (Hence the name: Convolutional Layer). This is why it is common to refer to the sets of weights as a filter (or a kernel), that is convolved with the input.\n\nNote that sometimes the parameter sharing assumption may not make sense. This is especially the case when the input images to a ConvNet have some specific centered structure, where we should expect, for example, that completely different features should be learned on one side of the image than another. One practical example is when the input are faces that have been centered in the image. You might expect that different eye-specific or hair-specific features could (and should) be learned in different spatial locations. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a Locally-Connected Layer.\n\nNumpy examples. To make the discussion above more concrete, lets express the same ideas but in code and with a specific example. Suppose that the input volume is a numpy array . Then:\n• A depth column (or a fibre) at position would be the activations .\n• A depth slice, or equivalently an activation map at depth would be the activations .\n\nConv Layer Example. Suppose that the input volume has shape . Suppose further that we use no zero padding (\\(P = 0\\)), that the filter size is \\(F = 5\\), and that the stride is \\(S = 2\\). The output volume would therefore have spatial size (11-5)/2+1 = 4, giving a volume with width and height of 4. The activation map in the output volume (call it ), would then look as follows (only some of the elements are computed in this example):\n\nRemember that in numpy, the operation above denotes elementwise multiplication between the arrays. Notice also that the weight vector is the weight vector of that neuron and is the bias. Here, is assumed to be of shape , since the filter size is 5 and the depth of the input volume is 4. Notice that at each point, we are computing the dot product as seen before in ordinary neural networks. Also, we see that we are using the same weight and bias (due to parameter sharing), and where the dimensions along the width are increasing in steps of 2 (i.e. the stride). To construct a second activation map in the output volume, we would have:\n\nwhere we see that we are indexing into the second depth dimension in (at index 1) because we are computing the second activation map, and that a different set of parameters ( ) is now used. In the example above, we are for brevity leaving out some of the other operations the Conv Layer would perform to fill the other parts of the output array . Additionally, recall that these activation maps are often followed elementwise through an activation function such as ReLU, but this is not shown here.\n• Requires four hyperparameters:\n• the amount of zero padding \\(P\\).\n• Produces a volume of size \\(W_2 \\times H_2 \\times D_2\\) where:\n• \\(H_2 = (H_1 - F + 2P)/S + 1\\) (i.e. width and height are computed equally by symmetry)\n• With parameter sharing, it introduces \\(F \\cdot F \\cdot D_1\\) weights per filter, for a total of \\((F \\cdot F \\cdot D_1) \\cdot K\\) weights and \\(K\\) biases.\n• In the output volume, the \\(d\\)-th depth slice (of size \\(W_2 \\times H_2\\)) is the result of performing a valid convolution of the \\(d\\)-th filter over the input volume with a stride of \\(S\\), and then offset by \\(d\\)-th bias.\n\nA common setting of the hyperparameters is \\(F = 3, S = 1, P = 1\\). However, there are common conventions and rules of thumb that motivate these hyperparameters. See the ConvNet architectures section below.\n\nConvolution Demo. Below is a running demo of a CONV layer. Since 3D volumes are hard to visualize, all the volumes (the input volume (in blue), the weight volumes (in red), the output volume (in green)) are visualized with each depth slice stacked in rows. The input volume is of size \\(W_1 = 5, H_1 = 5, D_1 = 3\\), and the CONV layer parameters are \\(K = 2, F = 3, S = 2, P = 1\\). That is, we have two filters of size \\(3 \\times 3\\), and they are applied with a stride of 2. Therefore, the output volume size has spatial size (5 - 3 + 2)/2 + 1 = 3. Moreover, notice that a padding of \\(P = 1\\) is applied to the input volume, making the outer border of the input volume zero. The visualization below iterates over the output activations (green), and shows that each element is computed by elementwise multiplying the highlighted input (blue) with the filter (red), summing it up, and then offsetting the result by the bias.\n\nImplementation as Matrix Multiplication. Note that the convolution operation essentially performs dot products between the filters and local regions of the input. A common implementation pattern of the CONV layer is to take advantage of this fact and formulate the forward pass of a convolutional layer as one big matrix multiply as follows:\n• The local regions in the input image are stretched out into columns in an operation commonly called im2col. For example, if the input is [227x227x3] and it is to be convolved with 11x11x3 filters at stride 4, then we would take [11x11x3] blocks of pixels in the input and stretch each block into a column vector of size 11*11*3 = 363. Iterating this process in the input at stride of 4 gives (227-11)/4+1 = 55 locations along both width and height, leading to an output matrix of im2col of size [363 x 3025], where every column is a stretched out receptive field and there are 55*55 = 3025 of them in total. Note that since the receptive fields overlap, every number in the input volume may be duplicated in multiple distinct columns.\n• The weights of the CONV layer are similarly stretched out into rows. For example, if there are 96 filters of size [11x11x3] this would give a matrix of size [96 x 363].\n• The result of a convolution is now equivalent to performing one large matrix multiply , which evaluates the dot product between every filter and every receptive field location. In our example, the output of this operation would be [96 x 3025], giving the output of the dot product of each filter at each location.\n• The result must finally be reshaped back to its proper output dimension [55x55x96].\n\nThis approach has the downside that it can use a lot of memory, since some values in the input volume are replicated multiple times in . However, the benefit is that there are many very efficient implementations of Matrix Multiplication that we can take advantage of (for example, in the commonly used BLAS API). Moreover, the same im2col idea can be reused to perform the pooling operation, which we discuss next.\n\nBackpropagation. The backward pass for a convolution operation (for both the data and the weights) is also a convolution (but with spatially-flipped filters). This is easy to derive in the 1-dimensional case with a toy example (not expanded on for now).\n\n1x1 convolution. As an aside, several papers use 1x1 convolutions, as first investigated by Network in Network. Some people are at first confused to see 1x1 convolutions especially when they come from signal processing background. Normally signals are 2-dimensional so 1x1 convolutions do not make sense (it’s just pointwise scaling). However, in ConvNets this is not the case because one must remember that we operate over 3-dimensional volumes, and that the filters always extend through the full depth of the input volume. For example, if the input is [32x32x3] then doing 1x1 convolutions would effectively be doing 3-dimensional dot products (since the input depth is 3 channels).\n\nDilated convolutions. A recent development (e.g. see paper by Fisher Yu and Vladlen Koltun) is to introduce one more hyperparameter to the CONV layer called the dilation. So far we’ve only discussed CONV filters that are contiguous. However, it’s possible to have filters that have spaces between each cell, called dilation. As an example, in one dimension a filter of size 3 would compute over input the following: . This is dilation of 0. For dilation 1 the filter would instead compute ; In other words there is a gap of 1 between the applications. This can be very useful in some settings to use in conjunction with 0-dilated filters because it allows you to merge spatial information across the inputs much more agressively with fewer layers. For example, if you stack two 3x3 CONV layers on top of each other then you can convince yourself that the neurons on the 2nd layer are a function of a 5x5 patch of the input (we would say that the effective receptive field of these neurons is 5x5). If we use dilated convolutions then this effective receptive field would grow much quicker.\n\nIt is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations. Every MAX operation would in this case be taking a max over 4 numbers (little 2x2 region in some depth slice). The depth dimension remains unchanged. More generally, the pooling layer:\n• Introduces zero parameters since it computes a fixed function of the input\n• For Pooling layers, it is not common to pad the input using zero-padding.\n\nIt is worth noting that there are only two commonly seen variations of the max pooling layer found in practice: A pooling layer with \\(F = 3, S = 2\\) (also called overlapping pooling), and more commonly \\(F = 2, S = 2\\). Pooling sizes with larger receptive fields are too destructive.\n\nGeneral pooling. In addition to max pooling, the pooling units can also perform other functions, such as average pooling or even L2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to the max pooling operation, which has been shown to work better in practice.\n\nBackpropagation. Recall from the backpropagation chapter that the backward pass for a max(x, y) operation has a simple interpretation as only routing the gradient to the input that had the highest value in the forward pass. Hence, during the forward pass of a pooling layer it is common to keep track of the index of the max activation (sometimes also called the switches) so that gradient routing is efficient during backpropagation.\n\nGetting rid of pooling. Many people dislike the pooling operation and think that we can get away without it. For example, Striving for Simplicity: The All Convolutional Net proposes to discard the pooling layer in favor of architecture that only consists of repeated CONV layers. To reduce the size of the representation they suggest using larger stride in CONV layer once in a while. Discarding pooling layers has also been found to be important in training good generative models, such as variational autoencoders (VAEs) or generative adversarial networks (GANs). It seems likely that future architectures will feature very few to no pooling layers.\n\nMany types of normalization layers have been proposed for use in ConvNet architectures, sometimes with the intentions of implementing inhibition schemes observed in the biological brain. However, these layers have since fallen out of favor because in practice their contribution has been shown to be minimal, if any. For various types of normalizations, see the discussion in Alex Krizhevsky’s cuda-convnet library API.\n\nNeurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. Their activations can hence be computed with a matrix multiplication followed by a bias offset. See the Neural Network section of the notes for more information.\n\nIt is worth noting that the only difference between FC and CONV layers is that the neurons in the CONV layer are connected only to a local region in the input, and that many of the neurons in a CONV volume share parameters. However, the neurons in both layers still compute dot products, so their functional form is identical. Therefore, it turns out that it’s possible to convert between FC and CONV layers:\n• For any CONV layer there is an FC layer that implements the same forward function. The weight matrix would be a large matrix that is mostly zero except for at certain blocks (due to local connectivity) where the weights in many of the blocks are equal (due to parameter sharing).\n• Conversely, any FC layer can be converted to a CONV layer. For example, an FC layer with \\(K = 4096\\) that is looking at some input volume of size \\(7 \\times 7 \\times 512\\) can be equivalently expressed as a CONV layer with \\(F = 7, P = 0, S = 1, K = 4096\\). In other words, we are setting the filter size to be exactly the size of the input volume, and hence the output will simply be \\(1 \\times 1 \\times 4096\\) since only a single depth column “fits” across the input volume, giving identical result as the initial FC layer.\n\nFC->CONV conversion. Of these two conversions, the ability to convert an FC layer to a CONV layer is particularly useful in practice. Consider a ConvNet architecture that takes a 224x224x3 image, and then uses a series of CONV layers and POOL layers to reduce the image to an activations volume of size 7x7x512 (in an AlexNet architecture that we’ll see later, this is done by use of 5 pooling layers that downsample the input spatially by a factor of two each time, making the final spatial size 224/2/2/2/2/2 = 7). From there, an AlexNet uses two FC layers of size 4096 and finally the last FC layers with 1000 neurons that compute the class scores. We can convert each of these three FC layers to CONV layers as described above:\n• Replace the first FC layer that looks at [7x7x512] volume with a CONV layer that uses filter size \\(F = 7\\), giving output volume [1x1x4096].\n• Replace the second FC layer with a CONV layer that uses filter size \\(F = 1\\), giving output volume [1x1x4096]\n• Replace the last FC layer similarly, with \\(F=1\\), giving final output [1x1x1000]\n\nEach of these conversions could in practice involve manipulating (e.g. reshaping) the weight matrix \\(W\\) in each FC layer into CONV layer filters. It turns out that this conversion allows us to “slide” the original ConvNet very efficiently across many spatial positions in a larger image, in a single forward pass.\n\nFor example, if 224x224 image gives a volume of size [7x7x512] - i.e. a reduction by 32, then forwarding an image of size 384x384 through the converted architecture would give the equivalent volume in size [12x12x512], since 384/32 = 12. Following through with the next 3 CONV layers that we just converted from FC layers would now give the final volume of size [6x6x1000], since (12 - 7)/1 + 1 = 6. Note that instead of a single vector of class scores of size [1x1x1000], we’re now getting an entire 6x6 array of class scores across the 384x384 image.\n\nNaturally, forwarding the converted ConvNet a single time is much more efficient than iterating the original ConvNet over all those 36 locations, since the 36 evaluations share computation. This trick is often used in practice to get better performance, where for example, it is common to resize an image to make it bigger, use a converted ConvNet to evaluate the class scores at many spatial positions and then average the class scores.\n\nLastly, what if we wanted to efficiently apply the original ConvNet over the image but at a stride smaller than 32 pixels? We could achieve this with multiple forward passes. For example, note that if we wanted to use a stride of 16 pixels we could do so by combining the volumes received by forwarding the converted ConvNet twice: First over the original image and second over the image but with the image shifted spatially by 16 pixels along both width and height.\n• An IPython Notebook on Net Surgery shows how to perform the conversion in practice, in code (using Caffe)\n\nWe have seen that Convolutional Networks are commonly made up of only three layer types: CONV, POOL (we assume Max pool unless stated otherwise) and FC (short for fully-connected). We will also explicitly write the RELU activation function as a layer, which applies elementwise non-linearity. In this section we discuss how these are commonly stacked together to form entire ConvNets.\n\nThe most common form of a ConvNet architecture stacks a few CONV-RELU layers, follows them with POOL layers, and repeats this pattern until the image has been merged spatially to a small size. At some point, it is common to transition to fully-connected layers. The last fully-connected layer holds the output, such as the class scores. In other words, the most common ConvNet architecture follows the pattern:\n\nwhere the indicates repetition, and the indicates an optional pooling layer. Moreover, (and usually ), , (and usually ). For example, here are some common ConvNet architectures you may see that follow this pattern:\n• . Here we see that there is a single CONV layer between every POOL layer.\n• Here we see two CONV layers stacked before every POOL layer. This is generally a good idea for larger and deeper networks, because multiple stacked CONV layers can develop more complex features of the input volume before the destructive pooling operation.\n\nPrefer a stack of small filter CONV to one large receptive field CONV layer. Suppose that you stack three 3x3 CONV layers on top of each other (with non-linearities in between, of course). In this arrangement, each neuron on the first CONV layer has a 3x3 view of the input volume. A neuron on the second CONV layer has a 3x3 view of the first CONV layer, and hence by extension a 5x5 view of the input volume. Similarly, a neuron on the third CONV layer has a 3x3 view of the 2nd CONV layer, and hence a 7x7 view of the input volume. Suppose that instead of these three layers of 3x3 CONV, we only wanted to use a single CONV layer with 7x7 receptive fields. These neurons would have a receptive field size of the input volume that is identical in spatial extent (7x7), but with several disadvantages. First, the neurons would be computing a linear function over the input, while the three stacks of CONV layers contain non-linearities that make their features more expressive. Second, if we suppose that all the volumes have \\(C\\) channels, then it can be seen that the single 7x7 CONV layer would contain \\(C \\times (7 \\times 7 \\times C) = 49 C^2\\) parameters, while the three 3x3 CONV layers would only contain \\(3 \\times (C \\times (3 \\times 3 \\times C)) = 27 C^2\\) parameters. Intuitively, stacking CONV layers with tiny filters as opposed to having one CONV layer with big filters allows us to express more powerful features of the input, and with fewer parameters. As a practical disadvantage, we might need more memory to hold all the intermediate CONV layer results if we plan to do backpropagation.\n\nRecent departures. It should be noted that the conventional paradigm of a linear list of layers has recently been challenged, in Google’s Inception architectures and also in current (state of the art) Residual Networks from Microsoft Research Asia. Both of these (see details below in case studies section) feature more intricate and different connectivity structures.\n\nIn practice: use whatever works best on ImageNet. If you’re feeling a bit of a fatigue in thinking about the architectural decisions, you’ll be pleased to know that in 90% or more of applications you should not have to worry about these. I like to summarize this point as “don’t be a hero”: Instead of rolling your own architecture for a problem, you should look at whatever architecture currently works best on ImageNet, download a pretrained model and finetune it on your data. You should rarely ever have to train a ConvNet from scratch or design one from scratch. I also made this point at the Deep Learning school.\n\nUntil now we’ve omitted mentions of common hyperparameters used in each of the layers in a ConvNet. We will first state the common rules of thumb for sizing the architectures and then follow the rules with a discussion of the notation:\n\nThe input layer (that contains the image) should be divisible by 2 many times. Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and 512.\n\nThe conv layers should be using small filters (e.g. 3x3 or at most 5x5), using a stride of \\(S = 1\\), and crucially, padding the input volume with zeros in such way that the conv layer does not alter the spatial dimensions of the input. That is, when \\(F = 3\\), then using \\(P = 1\\) will retain the original size of the input. When \\(F = 5\\), \\(P = 2\\). For a general \\(F\\), it can be seen that \\(P = (F - 1) / 2\\) preserves the input size. If you must use bigger filter sizes (such as 7x7 or so), it is only common to see this on the very first conv layer that is looking at the input image.\n\nThe pool layers are in charge of downsampling the spatial dimensions of the input. The most common setting is to use max-pooling with 2x2 receptive fields (i.e. \\(F = 2\\)), and with a stride of 2 (i.e. \\(S = 2\\)). Note that this discards exactly 75% of the activations in an input volume (due to downsampling by 2 in both width and height). Another slightly less common setting is to use 3x3 receptive fields with a stride of 2, but this makes “fitting” more complicated (e.g., a 32x32x3 layer would require zero padding to be used with a max-pooling layer with 3x3 receptive field and stride 2). It is very uncommon to see receptive field sizes for max pooling that are larger than 3 because the pooling is then too lossy and aggressive. This usually leads to worse performance.\n\nReducing sizing headaches. The scheme presented above is pleasing because all the CONV layers preserve the spatial size of their input, while the POOL layers alone are in charge of down-sampling the volumes spatially. In an alternative scheme where we use strides greater than 1 or don’t zero-pad the input in CONV layers, we would have to very carefully keep track of the input volumes throughout the CNN architecture and make sure that all strides and filters “work out”, and that the ConvNet architecture is nicely and symmetrically wired.\n\nWhy use stride of 1 in CONV? Smaller strides work better in practice. Additionally, as already mentioned stride 1 allows us to leave all spatial down-sampling to the POOL layers, with the CONV layers only transforming the input volume depth-wise.\n\nWhy use padding? In addition to the aforementioned benefit of keeping the spatial sizes constant after CONV, doing this actually improves performance. If the CONV layers were to not zero-pad the inputs and only perform valid convolutions, then the size of the volumes would reduce by a small amount after each CONV, and the information at the borders would be “washed away” too quickly.\n\nCompromising based on memory constraints. In some cases (especially early in the ConvNet architectures), the amount of memory can build up very quickly with the rules of thumb presented above. For example, filtering a 224x224x3 image with three 3x3 CONV layers with 64 filters each and padding 1 would create three activation volumes of size [224x224x64]. This amounts to a total of about 10 million activations, or 72MB of memory (per image, for both activations and gradients). Since GPUs are often bottlenecked by memory, it may be necessary to compromise. In practice, people prefer to make the compromise at only the first CONV layer of the network. For example, one compromise might be to use a first CONV layer with filter sizes of 7x7 and stride of 2 (as seen in a ZF net). As another example, an AlexNet uses filter sizes of 11x11 and stride of 4.\n\nThere are several architectures in the field of Convolutional Networks that have a name. The most common are:\n• LeNet. The first successful applications of Convolutional Networks were developed by Yann LeCun in 1990’s. Of these, the best known is the LeNet architecture that was used to read zip codes, digits, etc.\n• AlexNet. The first work that popularized Convolutional Networks in Computer Vision was the AlexNet, developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. The AlexNet was submitted to the ImageNet ILSVRC challenge in 2012 and significantly outperformed the second runner-up (top 5 error of 16% compared to runner-up with 26% error). The Network had a very similar architecture to LeNet, but was deeper, bigger, and featured Convolutional Layers stacked on top of each other (previously it was common to only have a single CONV layer always immediately followed by a POOL layer).\n• ZF Net. The ILSVRC 2013 winner was a Convolutional Network from Matthew Zeiler and Rob Fergus. It became known as the ZFNet (short for Zeiler & Fergus Net). It was an improvement on AlexNet by tweaking the architecture hyperparameters, in particular by expanding the size of the middle convolutional layers and making the stride and filter size on the first layer smaller.\n• GoogLeNet. The ILSVRC 2014 winner was a Convolutional Network from Szegedy et al. from Google. Its main contribution was the development of an Inception Module that dramatically reduced the number of parameters in the network (4M, compared to AlexNet with 60M). Additionally, this paper uses Average Pooling instead of Fully Connected layers at the top of the ConvNet, eliminating a large amount of parameters that do not seem to matter much. There are also several followup versions to the GoogLeNet, most recently Inception-v4.\n• VGGNet. The runner-up in ILSVRC 2014 was the network from Karen Simonyan and Andrew Zisserman that became known as the VGGNet. Its main contribution was in showing that the depth of the network is a critical component for good performance. Their final best network contains 16 CONV/FC layers and, appealingly, features an extremely homogeneous architecture that only performs 3x3 convolutions and 2x2 pooling from the beginning to the end. Their pretrained model is available for plug and play use in Caffe. A downside of the VGGNet is that it is more expensive to evaluate and uses a lot more memory and parameters (140M). Most of these parameters are in the first fully connected layer, and it was since found that these FC layers can be removed with no performance downgrade, significantly reducing the number of necessary parameters.\n• ResNet. Residual Network developed by Kaiming He et al. was the winner of ILSVRC 2015. It features special skip connections and a heavy use of batch normalization. The architecture is also missing fully connected layers at the end of the network. The reader is also referred to Kaiming’s presentation (video, slides), and some recent experiments that reproduce these networks in Torch. ResNets are currently by far state of the art Convolutional Neural Network models and are the default choice for using ConvNets in practice (as of May 10, 2016). In particular, also see more recent developments that tweak the original architecture from Kaiming He et al. Identity Mappings in Deep Residual Networks (published March 2016).\n\nVGGNet in detail. Lets break down the VGGNet in more detail as a case study. The whole VGGNet is composed of CONV layers that perform 3x3 convolutions with stride 1 and pad 1, and of POOL layers that perform 2x2 max pooling with stride 2 (and no padding). We can write out the size of the representation at each step of the processing and keep track of both the representation size and the total number of weights:\n\nAs is common with Convolutional Networks, notice that most of the memory (and also compute time) is used in the early CONV layers, and that most of the parameters are in the last FC layers. In this particular case, the first FC layer contains 100M weights, out of a total of 140M.\n\nThe largest bottleneck to be aware of when constructing ConvNet architectures is the memory bottleneck. Many modern GPUs have a limit of 3/4/6GB memory, with the best GPUs having about 12GB of memory. There are three major sources of memory to keep track of:\n• From the intermediate volume sizes: These are the raw number of activations at every layer of the ConvNet, and also their gradients (of equal size). Usually, most of the activations are on the earlier layers of a ConvNet (i.e. first Conv Layers). These are kept around because they are needed for backpropagation, but a clever implementation that runs a ConvNet only at test time could in principle reduce this by a huge amount, by only storing the current activations at any layer and discarding the previous activations on layers below.\n• From the parameter sizes: These are the numbers that hold the network parameters, their gradients during backpropagation, and commonly also a step cache if the optimization is using momentum, Adagrad, or RMSProp. Therefore, the memory to store the parameter vector alone must usually be multiplied by a factor of at least 3 or so.\n• Every ConvNet implementation has to maintain miscellaneous memory, such as the image data batches, perhaps their augmented versions, etc.\n\nOnce you have a rough estimate of the total number of values (for activations, gradients, and misc), the number should be converted to size in GB. Take the number of values, multiply by 4 to get the raw number of bytes (since every floating point is 4 bytes, or maybe by 8 for double precision), and then divide by 1024 multiple times to get the amount of memory in KB, MB, and finally GB. If your network doesn’t fit, a common heuristic to “make it fit” is to decrease the batch size, since most of the memory is usually consumed by the activations.\n• ConvNetJS CIFAR-10 demo allows you to play with ConvNet architectures and see the results and computations in real time, in the browser.\n• Caffe, one of the popular ConvNet libraries.\n• State of the art ResNets in Torch7"
    },
    {
        "link": "https://medium.com/thedeephub/convolutional-neural-networks-a-comprehensive-guide-5cc0b5eae175",
        "document": "Convolutional Neural Networks, commonly referred to as CNNs are a specialized type of neural network designed to process and classify images. If you are new to this field you might be thinking how is it possible to classify an image? Digital images are essentially grids of tiny units called pixels. Each pixel represents the smallest unit of an image and holds information about the color and intensity at that particular point. Typically, each pixel is composed of three values corresponding to the red, green, and blue (RGB) color channels. These values determine the color and intensity of that pixel. You can use the following tool to understand better how the RGB vector is formed: In contrast, in a grayscale image, each pixel carries a single value that represents the intensity of light at that point. Usually ranging from black (0) to white (255).\n\nTo understand how a CNN functions let´s recap some of the basic concepts about Neural Networks. (If you are reading this post I am assuming that you are familiar with basic neural networks. If that´s not the case I strongly recommend you to read this article). 1.- Neurons: The most basic unit in a neural network. They are composed of a sum of linear functions and a non-linear function known as the activation function is applied to them. 2.- Input layer: Each neuron in the input layer corresponds to one of the input features. For instance, in an image classification task where the input is a 28 x 28-pixel image, the input layer would have 784 neurons (one for each pixel). 3.- Hidden Layer: The layers between the input and the output layer. Each neuron in this layer is summed by the result of the neurons in the previous layers and multiplied by a non-linear function. 4.- Output Layer: The number of neurons in the output layer corresponds to the number of output classes (In case we are facing a regression problem the output layer will only have one neuron). For example, in a classification task with digits from 0 to 9, the output layer would have 10 neurons. Once a prediction is made, a loss is calculated and the network enters a self-improvement iterative process through which the weights are adjusted with backpropagation to reduce this error.\n\nNow we are ready to understand convolutional neural networks! The first question we should ask ourselves:\n• What makes a CNN different from a basic neural network? They are the fundamental building blocks of CNNs. These layers perform a critical mathematical operation known as convolution. This process entails the application of specialized filters known as kernels, that traverse through the input image to learn complex visual patterns. Kernels\n\nThey are essentially small matrices of numbers. These filters move across the image performing element-wise multiplication with the part of the image they cover, extracting features such as edges, textures, and shapes. In the figure above, visualize the input as an image transformed into pixels. We multiply each term of the image by a 3 × 3 matrix (this shape can vary) and pass it into an output matrix. There are various methods to decide the digits inside the kernel. This will depend on the effect you want to achieve such as detecting edges, blurring, sharpening… But what are we doing exactly? Let´s take a deeper look at it. Convolution Operation \n\nThe convolution operation involves multiplying the kernel values by the original pixel values of the image and then summing up the results. This is a basic example with a 2 × 2 kernel: We start in the left corner of the input: Then we slice one pixel to the right and perform the same operation: After we completed the first row we move one pixel down and start again from the left: Finally, we again slice one pixel to the right: The output matrix of this process is known as the Feature map.\n\nAs I explained before, digital images are often composed of three channels (RGB) which are represented in three different matrices. For an RGB image, there are typically separate kernels for each color channel because different features might be more visible or relevant in one channel compared to the others. The ‘depth’ of a layer refers to the number of kernels it contains. Each filter produces a separate feature map, and the collection of these feature maps forms the complete output of the layer. The output normally has multiple channels, where each channel is a feature map corresponding to a particular kernel. In the case of RGB, we typically use one channel for each of the 3 matrices, but we can add as many as we want. For example, let´s say that you have a gray-scale image of a cat, you could create a channel specialized in detecting the ears and another in the mouth. This image illustrates the concept quite well, think of each layer in the convolution as a feature map with a different kernel (don´t worry about the pooling part for now, we`ll break it down in a minute). ☣️ BE CAREFUL with misunderstanding the channels in the convolution layer with the color channels in the image. That was a representative example to understand the concept but you can add as many channels as you want. Each channel will detect a different feature in the image based on the values you assign to its kernel.\n\nPadding refers to the addition of extra pixels around the edge of the input image. When you focus on the pixels in the image’s edges, you’ll notice that we traverse them fewer times compared to those positioned in the center. The purpose of padding is to adjust the spatial size of the output of a convolutional operation and to preserve spatial information at the borders. Let´s see another example with the CNN explainer\n• Padding = 0 (focus on the edges and count how many times the kernel is passing through them) Now we are passing more times through the pixels in the edges and getting more information about them. In which cases do you want to apply padding? Mainly when the edges of the image contain useful information that you want to capture. You can increase the padding up to the kernel size you are using. And how does it affect the output field? Padding increases the size of the output feature map. If you increase the padding while keeping the kernel size and stride constant, the convolution operation has more “room” to take place, resulting in a larger output. The output size of a convolutional layer can be calculated using the following formula:\n• “2 × Padding” accounts for padding applied to both the left and right sides (or top and bottom sides) of the input.\n• “+ 1” accounts for the initial position of the filter, which starts at the beginning of the padded input. ☣️ This is a visual explanation of Padding but at a practical level, it doesn´t have to be always the same on all sides of the image. The padding dimensions can be asymmetric or even have a custom padding design.\n\nIf you have reached this point now you can officially say that you know how Convolutional Layers work! Nevertheless, this is not the end of the journey… There is a common misconception among beginners that Conv. layers are Convolutional Neural Networks. Well, convolutional layers are an essential component, but as its name indicates, they are a LAYER inside CNNs. We have comprehended the most important part of CNNs, but there are still two other special types of layers that we have to understand: Before explaining how these layers work it´s crucial to have this clear: Although Convolutional Layers can decrease the output size, their principal objective is not DIMENSIONALITY REDUCTION. The main objective of Convolutional Layers is FEATURE EXTRACTION. In fact, in most cases we are not reducing the dimensions of our data because we are creating new channels that weren´t there before, so even if our feature map dimensions are smaller, we have more of them. Take a look at this example, here we might be reducing a bit our feature map in each Convolutional Layer but we are creating much more channels. What about the subsampling layers? Those are pooling layers and its main objective is indeed dimensionality reduction! Imagine you have a large image and want to make it smaller but keep all the important features like edges and colors. The pooling layer operates independently on every depth slice of the input. It resizes it spatially, using the Max or Average of the values in a window slid over the input data. In this example, we have reduced the feature map from (4 × 4) to (2 × 2). What is the difference between pooling and the convolution operation? In pooling, we are not applying any kernel to the input data, we are just simplifying the information with a math operation (Max or Avg). What about the channels, pooling also reduces the number of channels? You must understand this: Pooling layers DO NOT REDUCE THE NUMBER OF CHANNELS. Each pooling operation IS APPLIED INDEPENDENTLY TO EACH CHANNEL of the input data. Let´s see another example, channels can be a bit complex to visualize at first and I want to ensure that you understand correctly how they work. This is a good representation, here you can see how each pooling layer is reducing the dimensions of the spatial space but it's not reducing the number of channels. The number of channels is not reduced until the end of the architecture. With Convolutional and Pooling layers we CAN´T reduce the number of channels, just add more to the existing ones. So why and how do we combine all these channels? After convolutional and pooling layers have extracted relevant features from the input image we have to turn this high-dimensional feature map into a format suitable for feeding into fully connected layers. Here´s where flattening layers come into action!\n\nAs you may know activation functions are indispensable, otherwise, we would be creating a very large linear model. As in simple neural networks, we also need these non-linear terms in ConvNets. However, not all the layers we have seen have an activation function. Let's use an image as a reference to visualize this. Now you should understand the representation without any problem! Just one little thing… The first two pooling layers are not shown in this diagram, this is another way of visualizing CNNs, it doesn´t mean that they are not there, just imagine a filter between each layer that makes them smaller. In the feature extraction part, the activations will be in the convolutional layers. The process is quite straightforward, after each convolution operation you multiply the result by an activation function. The pooling and flattening layers DON´T have an activation function. As we explained before the main function of pooling layers is dimensionality reduction and the main purpose of flattening layers is restructuring the data into a 1D vector. We don´t need to include non-linearities for doing that. Nevertheless, we do need activation functions for extracting complex features (we won´t be able to capture relevant characteristics of an image with only a linear function). In the classification part, all the fully connected layers and the output layer will have an activation function, as in simple neural nets. Here we also need an activation function because we are using the features extracted to make a classification or a prediction, and the algorithm has to learn complex interactions (as a simple neural network would do)."
    },
    {
        "link": "https://tensorflow.org/tutorials/images/cnn",
        "document": "This tutorial demonstrates training a simple Convolutional Neural Network (CNN) to classify CIFAR images. Because this tutorial uses the Keras Sequential API, creating and training your model will take just a few lines of code.\n\nThe CIFAR10 dataset contains 60,000 color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them.\n\nTo verify that the dataset looks correct, let's plot the first 25 images from the training set and display the class name below each image:\n\nThe 6 lines of code below define the convolutional base using a common pattern: a stack of Conv2D and MaxPooling2D layers.\n\nAs input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size. If you are new to these dimensions, color_channels refers to (R,G,B). In this example, you will configure your CNN to process inputs of shape (32, 32, 3), which is the format of CIFAR images. You can do this by passing the argument to your first layer.\n\nLet's display the architecture of your model so far:\n\nAbove, you can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64). Typically, as the width and height shrink, you can afford (computationally) to add more output channels in each Conv2D layer.\n\nTo complete the model, you will feed the last output tensor from the convolutional base (of shape (4, 4, 64)) into one or more Dense layers to perform classification. Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor. First, you will flatten (or unroll) the 3D output to 1D, then add one or more Dense layers on top. CIFAR has 10 output classes, so you use a final Dense layer with 10 outputs.\n\nHere's the complete architecture of your model:\n\nThe network summary shows that (4, 4, 64) outputs were flattened into vectors of shape (1024) before going through two Dense layers.\n\nYour simple CNN has achieved a test accuracy of over 70%. Not bad for a few lines of code! For another CNN style, check out the TensorFlow 2 quickstart for experts example that uses the Keras subclassing API and ."
    },
    {
        "link": "https://medium.com/intelligentmachines/convolutional-neural-network-and-regularization-techniques-with-tensorflow-and-keras-5a09e6e65dc7",
        "document": "Here is my notebook to help you follow along: Rock, paper and scissor classifier\n\nLet’s first import all the libraries and packages that we are going to be using\n\nYou need a function to plot the loss and accuracy of the model that was trained so that you can observe the change graphically.\n\nKeras preprocessing has a class called ImageDataGenerator. It generates batches of tensor image data with real-time data augmentation. With this, you can add any form of data augmentation technique you want and you can specify the validation split. Here I used horizontal flip, vertical flip, height shift, and rescaling for augmentation. The reason why you might use flips is because you can take the picture of your hand from any angle. So by using flipping you are going to help your model generalize on better on different scenarios. If it were to say, train on horizontal images of hands it might not predict hands in a vertical orientation.\n\nNow that we are done with preprocessing let’s define our models we are going to create 4 models one with one without regularization and 3 models with the regularization technique mentioned above. Then we are going to observe the loss and accuracy of our model.\n\nWe are going to use a model without regularization first. The model has 4 conv-pool layers and 2 dense layers.\n\nIn the code, you can see Conv2D, MaxPooling2D, Flatten, and Dense. Let me explain what those functions do.\n• Conv2D: Conv2D performs 2-dimensional convolution on your images. It takes an input which is a tensor (matrix with more than 2 dimensions) and gives convoluted tensor as output. It takes the following parameters: the number of filters, filter dimension, regularized, and if this is the first layer then the shape of the input, activation function.\n• MaxPooling2D: Performs pooling on 2D data. As parameters, it only needs the shape of the pooling filter. It reduces the shape of the Matrix\n• Dense: It simply describes a layer of neurons. You need to provide the activation function\n• Flatten: Flattens the data. The output of conv layer is 2D or more. So flatten converts them into a unidimensional vector\n\nHere is the summary of the model\n\nFrom the summary of the model, you want to identify which layers have the most parameters. I am going to apply regularization to the three layers with the highest parameters. From the snippet above we can see that dense, conv2d_3 and conv2d_2 have the largest parameters. So I am going to apply regularization to those layers\n\nWe use model.compile() to tell what kind of loss, optimizer we will be using and what metric we want to observe. Once you have compiled your model initiate your model’s training with model.fit()\n\nThe history variable stores information about your trained model. Once you initiate your training with model.fit() you will observe that the TensorFlow Keras API will train and validate for some steps giving out your training loss and accuracy as well as validation loss and accuracy\n\nThen call your show_history function to observe the loss graph and accuracy graph for both training and validation of each. For the model with no regularization, the graph looks something like this.\n\nNow that you know the process of how we are going to code the model and display the architecture let’s get down to the theory and code of the regularized models\n\nFor L1 regularization you have to add the following value to the cost function L(x,y) of your model.\n\nHere theta is your parameter that you multiply with your input to get a prediction. In L1 regularization you take the modulus of all parameters and sum them up. Lambda is set before initiating the training\n\nHowever, one downside of L1 regularization is that you will end up with parameters with sparse values. The values of the parameters will be close to zeros.\n\nYou can add the L1 regularizer in the layers such as conv_2d by specifying the kernel_regularizer\n\nThis is the code snippet of a model with L1 regularizer.\n\nAs you can see we have added the tf.keras.regularizer() inside the Conv2d, dense layer’s kernel_regularizer, and set lambda to 0.01 . To train and compile the model use the same code as before\n\nAfter training for 20 epochs(loops) we observe a very weird graph of accuracy.\n\nYou can see that loss is very high and the accuracy of the model is very low. There are a few possible causes of these values.\n• We have a dataset with only 2188 images which is very low. So our model didn’t have enough data for training.\n• L1 regularization makes the parameters theta sparse which means that the values are mostly zeros\n\n3. We have added too many regularizers which could result in the model ‘underfitting’ the data\n\nThere are 3 ways to improve the performance of the model\n\nIn L2 regularization we take the sum of all the parameters squared and add it with the square difference of the actual output and predictions. Same as L1 if you increase the value of lambda, the value of the parameters will decrease as L2 will penalize the parameters. The difference is that the weights will not be sparse and we will get much better accuracy values compared to L1.\n\ntf.keras.regularizers.l2() denotes the L2 regularizers. After 20 epochs the graphs look like this. Train using the same step as before\n\nSince the parameters were not sparse the model did not underfit and we have low loss and high accuracy exactly what we want\n\nHow exactly do L1 and L2 penalize?\n\nThis has to do with backpropagation. Backpropagation is another topic in itself but the basic intuition is simple\n\nAt every step your model makes predictions. It finds the difference between the actual output and the prediction. If the difference is not the minimum then the model must make another prediction that is much closer to the output. To make different predictions the model must have different parameters that are better than the previous parameters. The parameters are updated by taking the gradient of the loss function at each step.\n\nThis is the way the regularizers penalize the parameters of the model.\n\nDropout is another regularization technique that is used widely in models. What it does is it drops some random nodes in your layers in the neural network.\n\nThe model architecture is like this:\n\ntf.keras.layers.Dropout(0.2) drops the input layers at a probability of 0.2.\n\nAfter training and visualizing with the above code the graph looks like this:\n\nThe losses are very low and the accuracy of the dropout model is high. The parameters are not sparse. So if you are planning to use regularization on your models go for dropout models\n\nUsually, dropout is placed on the fully connected layers or dense layers only because they are the ones with the greater number of parameters and thus they’re likely to excessively co-adapting themselves causing overfitting.\n\nImproving the performance of the model\n\nThere are few ways to improve the performance of the models\n• Use fewer conv layers. I have used 4 conv layers you can opt for two\n• Use regularization on the layer with the highest parameters only\n\nCongratulations on making this far. I hope by reading this article you can understand regularization better."
    },
    {
        "link": "https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks",
        "document": "Convolutional layers in a convolutional neural network summarize the presence of features in an input image.\n\nA problem with the output feature maps is that they are sensitive to the location of the features in the input. One approach to address this sensitivity is to down sample the feature maps. This has the effect of making the resulting down sampled feature maps more robust to changes in the position of the feature in the image, referred to by the technical phrase “local translation invariance.”\n\nPooling layers provide an approach to down sampling feature maps by summarizing the presence of features in patches of the feature map. Two common pooling methods are average pooling and max pooling that summarize the average presence of a feature and the most activated presence of a feature respectively.\n\nIn this tutorial, you will discover how the pooling operation works and how to implement it in convolutional neural networks.\n\nAfter completing this tutorial, you will know:\n• Pooling is required to down sample the detection of features in feature maps.\n• How to calculate and implement average and maximum pooling in a convolutional neural network.\n• How to use global pooling in a convolutional neural network.\n\nKick-start your project with my new book Deep Learning for Computer Vision, including step-by-step tutorials and the Python source code files for all examples.\n\nThis tutorial is divided into five parts; they are:\n\nConvolutional layers in a convolutional neural network systematically apply learned filters to input images in order to create feature maps that summarize the presence of those features in the input.\n\nConvolutional layers prove very effective, and stacking convolutional layers in deep models allows layers close to the input to learn low-level features (e.g. lines) and layers deeper in the model to learn high-order or more abstract features, like shapes or specific objects.\n\nA limitation of the feature map output of convolutional layers is that they record the precise position of features in the input. This means that small movements in the position of the feature in the input image will result in a different feature map. This can happen with re-cropping, rotation, shifting, and other minor changes to the input image.\n\nA common approach to addressing this problem from signal processing is called down sampling. This is where a lower resolution version of an input signal is created that still contains the large or important structural elements, without the fine detail that may not be as useful to the task.\n\nDown sampling can be achieved with convolutional layers by changing the stride of the convolution across the image. A more robust and common approach is to use a pooling layer.\n\nA pooling layer is a new layer added after the convolutional layer. Specifically, after a nonlinearity (e.g. ReLU) has been applied to the feature maps output by a convolutional layer; for example the layers in a model may look as follows:\n\nThe addition of a pooling layer after the convolutional layer is a common pattern used for ordering layers within a convolutional neural network that may be repeated one or more times in a given model.\n\nThe pooling layer operates upon each feature map separately to create a new set of the same number of pooled feature maps.\n\nPooling involves selecting a pooling operation, much like a filter to be applied to feature maps. The size of the pooling operation or filter is smaller than the size of the feature map; specifically, it is almost always 2×2 pixels applied with a stride of 2 pixels.\n\nThis means that the pooling layer will always reduce the size of each feature map by a factor of 2, e.g. each dimension is halved, reducing the number of pixels or values in each feature map to one quarter the size. For example, a pooling layer applied to a feature map of 6×6 (36 pixels) will result in an output pooled feature map of 3×3 (9 pixels).\n\nThe pooling operation is specified, rather than learned. Two common functions used in the pooling operation are:\n• Average Pooling: Calculate the average value for each patch on the feature map.\n• Maximum Pooling (or Max Pooling): Calculate the maximum value for each patch of the feature map.\n\nThe result of using a pooling layer and creating down sampled or pooled feature maps is a summarized version of the features detected in the input. They are useful as small changes in the location of the feature in the input detected by the convolutional layer will result in a pooled feature map with the feature in the same location. This capability added by pooling is called the model’s invariance to local translation.\n\nNow that we are familiar with the need and benefit of pooling layers, let’s look at some specific examples.\n\nBefore we look at some examples of pooling layers and their effects, let’s develop a small example of an input image and convolutional layer to which we can later add and evaluate pooling layers.\n\nIn this example, we define a single input image or sample that has one channel and is an 8 pixel by 8 pixel square with all 0 values and a two-pixel wide vertical line in the center.\n\nNext, we can define a model that expects input samples to have the shape (8, 8, 1) and has a single hidden convolutional layer with a single filter with the shape of 3 pixels by 3 pixels.\n\nA rectified linear activation function, or ReLU for short, is then applied to each value in the feature map. This is a simple and effective nonlinearity, that in this case will not change the values in the feature map, but is present because we will later add subsequent pooling layers and pooling is added after the nonlinearity applied to the feature maps, e.g. a best practice.\n\nThe filter is initialized with random weights as part of the initialization of the model.\n\nInstead, we will hard code our own 3×3 filter that will detect vertical lines. That is the filter will strongly activate when it detects a vertical line and weakly activate when it does not. We expect that by applying this filter across the input image that the output feature map will show that the vertical line was detected.\n\nNext, we can apply the filter to our input image by calling the predict() function on the model.\n\nThe result is a four-dimensional output with one batch, a given number of rows and columns, and one filter, or [batch, rows, columns, filters]. We can print the activations in the single feature map to confirm that the line was detected.\n\nTying all of this together, the complete example is listed below.\n\nRunning the example first summarizes the structure of the model.\n\nOf note is that the single hidden convolutional layer will take the 8×8 pixel input image and will produce a feature map with the dimensions of 6×6.\n\nWe can also see that the layer has 10 parameters: that is nine weights for the filter (3×3) and one weight for the bias.\n\nWe can see from reviewing the numbers in the 6×6 matrix that indeed the manually specified filter detected the vertical line in the middle of our input image.\n\nWe can now look at some common approaches to pooling and how they impact the output feature maps.\n\nOn two-dimensional feature maps, pooling is typically applied in 2×2 patches of the feature map with a stride of (2,2).\n\nAverage pooling involves calculating the average for each patch of the feature map. This means that each 2×2 square of the feature map is down sampled to the average value in the square.\n\nFor example, the output of the line detector convolutional filter in the previous section was a 6×6 feature map. We can look at applying the average pooling operation to the first line of that feature map manually.\n\nThe first line for pooling (first two rows and six columns) of the output feature map were as follows:\n\nThe first pooling operation is applied as follows:\n\nGiven the stride of two, the operation is moved along two columns to the left and the average is calculated:\n\nAgain, the operation is moved along two columns to the left and the average is calculated:\n\nThat’s it for the first line of pooling operations. The result is the first line of the average pooling operation:\n\nGiven the (2,2) stride, the operation would then be moved down two rows and back to the first column and the process continued.\n\nBecause the downsampling operation halves each dimension, we will expect the output of pooling applied to the 6×6 feature map to be a new 3×3 feature map. Given the horizontal symmetry of the feature map input, we would expect each row to have the same average pooling values. Therefore, we would expect the resulting average pooling of the detected line feature map from the previous section to look as follows:\n\nWe can confirm this by updating the example from the previous section to use average pooling.\n\nThis can be achieved in Keras by using the AveragePooling2D layer. The default pool_size (e.g. like the kernel size or filter size) of the layer is (2,2) and the default strides is None, which in this case means using the pool_size as the strides, which will be (2,2).\n\nThe complete example with average pooling is listed below.\n\nRunning the example first summarizes the model.\n\nWe can see from the model summary that the input to the pooling layer will be a single feature map with the shape (6,6) and that the output of the average pooling layer will be a single feature map with each dimension halved, with the shape (3,3).\n\nApplying the average pooling results in a new feature map that still detects the line, although in a down sampled manner, exactly as we expected from calculating the operation manually.\n\nAverage pooling works well, although it is more common to use max pooling.\n\nMaximum pooling, or max pooling, is a pooling operation that calculates the maximum, or largest, value in each patch of each feature map.\n\nThe results are down sampled or pooled feature maps that highlight the most present feature in the patch, not the average presence of the feature in the case of average pooling. This has been found to work better in practice than average pooling for computer vision tasks like image classification.\n\nWe can make the max pooling operation concrete by again applying it to the output feature map of the line detector convolutional operation and manually calculate the first row of the pooled feature map.\n\nThe first line for pooling (first two rows and six columns) of the output feature map were as follows:\n\nThe first max pooling operation is applied as follows:\n\nGiven the stride of two, the operation is moved along two columns to the left and the max is calculated:\n\nAgain, the operation is moved along two columns to the left and the max is calculated:\n\nThat’s it for the first line of pooling operations.\n\nThe result is the first line of the max pooling operation:\n\nAgain, given the horizontal symmetry of the feature map provided for pooling, we would expect the pooled feature map to look as follows:\n\nIt just so happens that the chosen line detector image and feature map produce the same output when downsampled with average pooling and maximum pooling.\n\nThe maximum pooling operation can be added to the worked example by adding the MaxPooling2D layer provided by the Keras API.\n\nThe complete example of vertical line detection with max pooling is listed below.\n\nRunning the example first summarizes the model.\n\nWe can see, as we might expect by now, that the output of the max pooling layer will be a single feature map with each dimension halved, with the shape (3,3).\n\nApplying the max pooling results in a new feature map that still detects the line, although in a down sampled manner.\n\nThere is another type of pooling that is sometimes used called global pooling.\n\nInstead of down sampling patches of the input feature map, global pooling down samples the entire feature map to a single value. This would be the same as setting the pool_size to the size of the input feature map.\n\nGlobal pooling can be used in a model to aggressively summarize the presence of a feature in an image. It is also sometimes used in models as an alternative to using a fully connected layer to transition from feature maps to an output prediction for the model.\n\nBoth global average pooling and global max pooling are supported by Keras via the GlobalAveragePooling2D and GlobalMaxPooling2D classes respectively.\n\nFor example, we can add global max pooling to the convolutional model used for vertical line detection.\n\nThe outcome will be a single value that will summarize the strongest activation or presence of the vertical line in the input image.\n\nThe complete code listing is provided below.\n\nRunning the example first summarizes the model\n\nWe can see that, as expected, the output of the global pooling layer is a single value that summarizes the presence of the feature in the single feature map.\n\nNext, the output of the model is printed showing the effect of global max pooling on the feature map, printing the single largest activation.\n\nThis section provides more resources on the topic if you are looking to go deeper.\n• Crash Course in Convolutional Neural Networks for Machine Learning\n\nIn this tutorial, you discovered how the pooling operation works and how to implement it in convolutional neural networks.\n• Pooling is required to down sample the detection of features in feature maps.\n• How to calculate and implement average and maximum pooling in a convolutional neural network.\n• How to use global pooling in a convolutional neural network.\n\nDo you have any questions?\n\n Ask your questions in the comments below and I will do my best to answer."
    },
    {
        "link": "https://victorzhou.com/blog/keras-cnn-tutorial",
        "document": "Keras is a simple-to-use but powerful deep learning library for Python. In this post, we’ll build a simple Convolutional Neural Network (CNN) and train it to solve a real problem with Keras.\n\nThis post is intended for complete beginners to Keras but does assume a basic background knowledge of CNNs. My introduction to Convolutional Neural Networks covers everything you need to know (and more) for this post - read that first if necessary.\n\nHere we go!\n\nWe’re going to tackle a classic introductory Computer Vision problem: MNIST handwritten digit classification. It’s simple: given an image, classify it as a digit.\n\nEach image in the MNIST dataset is 28x28 and contains a centered, grayscale digit. Our CNN will take an image and output one of 10 possible classes (one for each digit).\n\nI’m assuming you already have a basic Python installation (you probably do). Let’s first download some packages we’ll need:\n\nYou should now be able to import these packages and poke around the MNIST dataset:\n\nBefore we begin, we’ll normalize the image pixel values from [0, 255] to [-0.5, 0.5] to make our network easier to train (using smaller, centered values usually leads to better results). We’ll also reshape each image from to because Keras requires the third dimension.\n\nEvery Keras model is either built using the Sequential class, which represents a linear stack of layers, or the functional Model class, which is more customizeable. We’ll be using the simpler model, since our CNN will be a linear stack of layers.\n\nThe constructor takes an array of Keras Layers. We’ll use 3 types of layers for our CNN: Convolutional, Max Pooling, and Softmax.\n• , , and are self-explanatory variables that set the hyperparameters for our CNN.\n• The first layer in any model must specify the , so we do so on . Once this input shape is specified, Keras will automatically infer the shapes of inputs for later layers.\n• The output Softmax layer has 10 nodes, one for each class.\n\nBefore we can begin training, we need to configure the training process. We decide 3 key factors during the compilation step:\n• The optimizer. We’ll stick with a pretty good default: the Adam gradient-based optimizer. Keras has many other optimizers you can look into as well.\n• The loss function. Since we’re using a Softmax output layer, we’ll use the Cross-Entropy loss. Keras distinguishes between (2 classes) and (>2 classes), so we’ll use the latter. See all Keras losses.\n• A list of metrics. Since this is a classification problem, we’ll just have Keras report on the accuracy metric.\n\nHere’s what that compilation looks like:\n\nTraining a model in Keras literally consists only of calling and specifying some parameters. There are a lot of possible parameters, but we’ll only supply these:\n• The training data (images and labels), commonly known as X and Y, respectively.\n• The number of epochs (iterations over the entire dataset) to train for.\n• The validation data (or test data), which is used during training to periodically measure the network’s performance against data it hasn’t seen before.\n\nThere’s one thing we have to be careful about: Keras expects the training targets to be 10-dimensional vectors, since there are 10 nodes in our Softmax output layer. Right now, our and arrays contain single integers representing the class for each image:\n\nConveniently, Keras has a utility method that fixes this exact issue: to_categorical. It turns our array of class integers into an array of one-hot vectors instead. For example, would become (it’s zero-indexed).\n\nHere’s what that looks like:\n\nWe can now put everything together to train our network:\n\nRunning that code on the full MNIST dataset gives us results like this:\n\nWe achieve 97.4% test accuracy with this simple CNN!\n\nNow that we have a working, trained model, let’s put it to use. The first thing we’ll do is save it to disk so we can load it back up anytime:\n\nWe can now reload the trained model whenever we want by rebuilding it and loading in the saved weights:\n\nUsing the trained model to make predictions is easy: we pass an array of inputs to and it returns an array of outputs. Keep in mind that the output of our network is 10 probabilities (because of softmax), so we’ll use np.argmax() to turn those into actual digits.\n\nThere’s much more we can do to experiment with and improve our network - in this official Keras MNIST CNN example, they achieve 99 test accuracy after 15 epochs. Some examples of modifications you could make to our CNN include:\n\nWhat happens if we add or remove Convolutional layers? How does that affect training and/or the model’s final performance?\n\nWhat if we tried adding Dropout layers, which are commonly used to prevent overfitting?\n\nWhat if we add fully-connected layers between the Convolutional outputs and the final Softmax layer? This is something commonly done in CNNs used for Computer Vision.\n\nWhat if we play with the Conv2D parameters? For example:\n\nYou’ve implemented your first CNN with Keras! We achieved a test accuracy of 97.4% with our simple initial network. I’ll include the full source code again below for your reference.\n\nFurther reading you might be interested in include:\n• My Keras for Beginners series, which has more Keras guides.\n• The official getting started with Keras guide.\n• My post on deriving backpropagation for training CNNs.\n\nThanks for reading! The full source code is below."
    },
    {
        "link": "https://medium.com/towards-data-science/beginners-guide-to-building-convolutional-neural-networks-using-tensorflow-s-keras-api-in-python-6e8035e28238",
        "document": "Beginner’s guide to building Convolutional Neural Networks using TensorFlow’s Keras API in Python\n\nWelcome to Part 2 of the Neural Network series! In Part 1, we worked our way through an Artificial Neural Network (ANNs) using the Keras API. We talked about Sequential network architecture, activation functions, hidden layers, neurons, etc. and finally wrapped it all up in an end-to-end example that predicted whether loan application would be approved or rejected.\n\nIn this tutorial, we will be learning how to create a Convolutional Neural Network (CNN) using the Keras API. To make it more intuitive, I will be explaining what each layer of this network does and provide tips and tricks to ease your deep learning journey. Our aim in this tutorial is to build a basic CNN that can classify images of chest Xrays and establish if it is normal or has pneumonia. Given the Covid-19 pandemic, I think this would make for an interesting project even for your data science interviews!\n\nWhen should I use a Convolutional Neural Network instead of an Artificial Neural Network?\n\nFor instance, if you recall the loan application dataset from Part 1, it had two columns (or features), namely and , and if I were to swap the two columns (before feeding it to my network) it would make no difference whatsoever to my dataset. Hence, ANNs are preferred for such datasets. On the contrary, if I were to swap the columns (which are essentially pixel arrays) in my image, I am surely going to mess up my actual image. Hence, using ANNs is a big no-no and you must use CNNs.\n\nLet’s dive right into the coding...\n\nWe begin by installing Keras onto our machine. As I discussed in Part 1, Keras is integrated within TensorFlow, so all you have to do is in your terminal (for Mac OS) to access Keras in your Jupyter notebook. To check the version of Tensorflow, use .\n\nWe will be working with the Chest X-Ray Images Kaggle dataset. Once you download and unzip it, you will see the chest_xray folder that contains images organized neatly into train, valid, and test subfolders. Each of these subfolders contains two sub-sub folders, one for chest Xrays and another for Xrays indicating .\n\nNote: Make sure the chest_xray folder is sitting in the same directory as your Python Jupyter notebook.\n\nSetting the paths to the train, validate, and test folders:\n\nWe need to put the data into a format that the model expects. So we must put our images into a format of a Keras . In short, we are going to create batches from train, test, and valid directories using the function in the class. And it is precisely these batches of data that will be passed to the sequential model using the function. (Previously, we were passing NumPy arrays to the fit function when building ANNs in Part 1, but now we need to pass the batches to the fit function).\n\nAs part of the preprocessing (before the batches are created), we will be applying . This will process our images in the same manner that images get processed when they are passed to the vgg16 model (the CNN model that won the ImageNet competition in 2014).\n\nThe specifies what height and width we want the images to be resized to. This is important since we might have images of different sizes in our dataset.\n\nThe is set to 10 and the choice of 10 is arbitrary. Another common batch size value used in 32 but ideally, the most optimal value should be found by hyperparameter tuning (we will learn how to do so in the next tutorial).\n\nWe also specify only for the test set because later on when we test our model, we want to have access to unshuffled labels to plot our confusion matrix. By default, shuffle is set to True.\n\nNote: If you see when you run the code above, chances are you are pointing to the wrong directory! Fix that and it should work fine!\n\nUsing the plotting helper function from TensorFlow’s documentation.\n\nNewt, we will use the Python function to grab a single batch of images and corresponding labels from the trainset. Since we set , we will see we have 10 images in and 10 corresponding labels in .\n\nNote: expects inputs in the range of or so if the input array does not have this range, it will clip input. Thus, we explicitly cast the images to format when displaying."
    }
]