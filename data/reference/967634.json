[
    {
        "link": "https://tensorflow.org/api_docs/python/tf/keras/Model",
        "document": ""
    },
    {
        "link": "https://tensorflow.org/guide/keras/training_with_built_in_methods",
        "document": ""
    },
    {
        "link": "https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras",
        "document": "Once you choose and fit a final deep learning model in Keras, you can use it to make predictions on new data instances.\n\nThere is some confusion amongst beginners about how exactly to do this. I often see questions such as:\n\nIn this tutorial, you will discover exactly how you can make classification and regression predictions with a finalized deep learning model with the Keras Python library.\n\nAfter completing this tutorial, you will know:\n• How to finalize a model in order to make it ready for making predictions.\n• How to make class and probability predictions for classification problems in Keras.\n• How to make regression predictions in in Keras.\n\nKick-start your project with my new book Deep Learning With Python, including step-by-step tutorials and the Python source code files for all examples.\n• Updated Jan/2020: Updated for changes in scikit-learn v0.22 API.\n\nThis tutorial is divided into 3 parts; they are:\n\nBefore you can make predictions, you must train a final model.\n\nYou may have trained models using k-fold cross validation or train/test splits of your data. This was done in order to give you an estimate of the skill of the model on out of sample data, e.g. new data.\n\nThese models have served their purpose and can now be discarded.\n\nYou now must train a final model on all of your available data. You can learn more about how to train a final model here:\n\nClassification problems are those where the model learns a mapping between input features and an output feature that is a label, such as “spam” and “not spam“.\n\nBelow is an example of a finalized neural network model in Keras developed for a simple two-class (binary) classification problem.\n\nIf developing a neural network model in Keras is new to you, see this Keras tutorial.\n\nAfter finalizing, you may want to save the model to file, e.g. via the Keras API. Once saved, you can load the model any time and use it to make predictions. For an example of this, see the post:\n\nFor simplicity, we will skip this step for the examples in this tutorial.\n\nThere are two types of classification predictions we may wish to make with our finalized model; they are class predictions and probability predictions.\n\nA class prediction is given the finalized model and one or more data instances, predict the class for the data instances.\n\nWe do not know the outcome classes for the new data. That is why we need the model in the first place.\n\nWe can predict the class for new data instances using our finalized classification model in Keras using the predict_classes() function. Note that this function is only available on Sequential models, not those models developed using the functional API.\n\nFor example, we have one or more data instances in an array called Xnew. This can be passed to the predict_classes() function on our model in order to predict the class values for each instance in the array.\n\nLet’s make this concrete with an example:\n\nRunning the example predicts the class for the three new data instances, then prints the data and the predictions together.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nNote that when you prepared your data, you will have mapped the class values from your domain (such as strings) to integer values. You may have used a LabelEncoder.\n\nThis LabelEncoder can be used to convert the integers back into string values via the inverse_transform() function.\n\nFor this reason, you may want to save (pickle) the LabelEncoder used to encode your y values when fitting your final model.\n\nAnother type of prediction you may wish to make is the probability of the data instance belonging to each class.\n\nThis is called a probability prediction where, given a new instance, the model returns the probability for each outcome class as a value between 0 and 1.\n\nYou can make these types of predictions in Keras by calling the predict_proba() function; for example:\n\nIn the case of a two-class (binary) classification problem, the sigmoid activation function is often used in the output layer. The predicted probability is taken as the likelihood of the observation belonging to class 1, or inverted (1 – probability) to give the probability for class 0.\n\nIn the case of a multi-class classification problem, the softmax activation function is often used on the output layer and the likelihood of the observation for each class is returned as a vector.\n\nThe example below makes a probability prediction for each example in the Xnew array of data instance.\n\nRunning the instance makes the probability predictions and then prints the input data instance and the probability of each instance belonging to class 1.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRegression is a supervised learning problem where given input examples, the model learns a mapping to suitable output quantities, such as “0.1” and “0.2”, etc.\n\nBelow is an example of a finalized Keras model for regression.\n\nWe can predict quantities with the finalized regression model by calling the predict() function on the finalized model.\n\nThe predict() function takes an array of one or more data instances.\n\nThe example below demonstrates how to make regression predictions on multiple data instances with an unknown expected outcome.\n\nRunning the example makes multiple predictions, then prints the inputs and predictions side by side for review.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example makes a single prediction and prints the data instance and prediction for review.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nThis section provides more resources on the topic if you are looking to go deeper.\n• Develop Your First Neural Network in Python With Keras Step-By-Step\n• The 5 Step Life-Cycle for Long Short-Term Memory Models in Keras\n• How to Make Predictions with Long Short-Term Memory Models in Keras\n\nIn this tutorial, you discovered how you can make classification and regression predictions with a finalized deep learning model with the Keras Python library.\n• How to finalize a model in order to make it ready for making predictions.\n• How to make class and probability predictions for classification problems in Keras.\n• How to make regression predictions in in Keras.\n\nDo you have any questions?\n\n Ask your questions in the comments below and I will do my best to answer."
    },
    {
        "link": "https://github.com/keras-team/tf-keras/issues/141",
        "document": "I'm using the deepface library with tensorflow 2.11.1. For one of the detection methods, i.e. MTCNN, and for the face attribute analysis, it prints the progress bar like the below:\n\n 1/1 [==============================] - 0s 15ms/step\n\n 1/1 [==============================] - 0s 15ms/step\n\n 1/1 [==============================] - 0s 14ms/step\n\n 1/1 [==============================] - 0s 14ms/step\n\n 1/1 [==============================] - 0s 15ms/step\n\n 1/1 [==============================] - 0s 14ms/step\n\n 2/2 [==============================] - 0s 2ms/step\n\n 1/1 [==============================] - 0s 14ms/step\n\n 1/1 [==============================] - 0s 15ms/step\n\nUnfortunately, because of this printing, my code is running for too long.\n\nI tried to find the corresponding code in the deepface code and here are the lines:\n\n emotion_predictions = models[\"emotion\"].predict(img_gray, verbose=0)[0, :]\n\n age_predictions = models[\"age\"].predict(img_content, verbose=0)[0, :]\n\n gender_predictions = models[\"gender\"].predict(img_content, verbose=0)[0, :]\n\n race_predictions = models[\"race\"].predict(img_content, verbose=0)[0, :]\n\n embedding = model.predict(img, verbose=0)[0].tolist()\n\nIf there is something wrong with the tensorflow version and I change it to a different version, I don't know what will happen to my other libraries and if they can still be run."
    },
    {
        "link": "https://tensorflow.org/decision_forests/api_docs/python/tfdf/keras/RandomForestModel",
        "document": ""
    },
    {
        "link": "https://tensorflow.org/tutorials/images/classification",
        "document": ""
    },
    {
        "link": "https://medium.com/@DIYCoding/a-beginners-guide-to-image-classification-with-keras-and-tensorflow-58aa3758aac9",
        "document": ""
    },
    {
        "link": "https://tensorflow.org/tutorials/keras/classification",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nThis guide trains a neural network model to classify images of clothing, like sneakers and shirts. It's okay if you don't understand all the details; this is a fast-paced overview of a complete TensorFlow program with the details explained as you go.\n\nThis guide uses tf.keras, a high-level API to build and train models in TensorFlow.\n\nThis guide uses the Fashion MNIST dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n\nFashion MNIST is intended as a drop-in replacement for the classic MNIST dataset—often used as the \"Hello, World\" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc.) in a format identical to that of the articles of clothing you'll use here.\n\nThis guide uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code.\n\nHere, 60,000 images are used to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow. Import and load the Fashion MNIST data directly from TensorFlow:\n• The and arrays are the training set—the data the model uses to learn.\n• The model is tested against the test set, the , and arrays.\n\nThe images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The labels are an array of integers, ranging from 0 to 9. These correspond to the class of clothing the image represents:\n\nEach image is mapped to a single label. Since the class names are not included with the dataset, store them here to use later when plotting the images:\n\nLet's explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, with each image represented as 28 x 28 pixels:\n\nLikewise, there are 60,000 labels in the training set:\n\nEach label is an integer between 0 and 9:\n\nThere are 10,000 images in the test set. Again, each image is represented as 28 x 28 pixels:\n\nAnd the test set contains 10,000 images labels:\n\nThe data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:\n\nScale these values to a range of 0 to 1 before feeding them to the neural network model. To do so, divide the values by 255. It's important that the training set and the testing set be preprocessed in the same way:\n\nTo verify that the data is in the correct format and that you're ready to build and train the network, let's display the first 25 images from the training set and display the class name below each image.\n\nBuilding the neural network requires configuring the layers of the model, then compiling the model.\n\nThe basic building block of a neural network is the layer. Layers extract representations from the data fed into them. Hopefully, these representations are meaningful for the problem at hand.\n\nMost of deep learning consists of chaining together simple layers. Most layers, such as , have parameters that are learned during training.\n\nThe first layer in this network, , transforms the format of the images from a two-dimensional array (of 28 by 28 pixels) to a one-dimensional array (of 28 * 28 = 784 pixels). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\n\nAfter the pixels are flattened, the network consists of a sequence of two layers. These are densely connected, or fully connected, neural layers. The first layer has 128 nodes (or neurons). The second (and last) layer returns a logits array with length of 10. Each node contains a score that indicates the current image belongs to one of the 10 classes.\n\nBefore the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n• Optimizer —This is how the model is updated based on the data it sees and its loss function.\n• Loss function —This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n• Metrics —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified.\n\nTraining the neural network model requires the following steps:\n• Feed the training data to the model. In this example, the training data is in the and arrays.\n• The model learns to associate images and labels.\n• You ask the model to make predictions about a test set—in this example, the array.\n• Verify that the predictions match the labels from the array.\n\nTo start training, call the method—so called because it \"fits\" the model to the training data:\n\nAs the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.91 (or 91%) on the training data.\n\nNext, compare how the model performs on the test dataset:\n\nIt turns out that the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy represents overfitting. Overfitting happens when a machine learning model performs worse on new, previously unseen inputs than it does on the training data. An overfitted model \"memorizes\" the noise and details in the training dataset to a point where it negatively impacts the performance of the model on the new data. For more information, see the following:\n\nWith the model trained, you can use it to make predictions about some images. Attach a softmax layer to convert the model's linear outputs—logits—to probabilities, which should be easier to interpret.\n\nHere, the model has predicted the label for each image in the testing set. Let's take a look at the first prediction:\n\nA prediction is an array of 10 numbers. They represent the model's \"confidence\" that the image corresponds to each of the 10 different articles of clothing. You can see which label has the highest confidence value:\n\nSo, the model is most confident that this image is an ankle boot, or . Examining the test label shows that this classification is correct:\n\nDefine functions to graph the full set of 10 class predictions.\n\nWith the model trained, you can use it to make predictions about some images.\n\nLet's look at the 0th image, predictions, and prediction array. Correct prediction labels are blue and incorrect prediction labels are red. The number gives the percentage (out of 100) for the predicted label.\n\nLet's plot several images with their predictions. Note that the model can be wrong even when very confident.\n\nFinally, use the trained model to make a prediction about a single image.\n\nmodels are optimized to make predictions on a batch, or collection, of examples at once. Accordingly, even though you're using a single image, you need to add it to a list:\n\nNow predict the correct label for this image:\n\nreturns a list of lists—one list for each image in the batch of data. Grab the predictions for our (only) image in the batch:\n\nAnd the model predicts a label as expected.\n\nTo learn more about building models with Keras, see the Keras guides.\n\n# Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the \"Software\"), # to deal in the Software without restriction, including without limitation # the rights to use, copy, modify, merge, publish, distribute, sublicense, # and/or sell copies of the Software, and to permit persons to whom the # Software is furnished to do so, subject to the following conditions: # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL # THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING # FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER"
    },
    {
        "link": "https://analyticsvidhya.com/blog/2024/04/preprocessing-layers-in-tensorflow-keras",
        "document": "Explore the power of TensorFlow Keras preprocessing layers! This article will show you the tools that TensorFlow Keras gives you to get your data ready for neural networks quickly and easily. Keras’s flexible preprocessing layers are extremely handy when working with text, numbers, or images. We’ll examine the importance of these layers and how they simplify the process of preparing data, including encoding, normalization, resizing, and augmentation.\n• Understanding the role and significance of TF-Keras preprocessing layers in data preparation for neural networks.\n• Exploring various preprocessing layers for text and image data.\n• Learning how to apply different preprocessing techniques such as normalization, encoding, resizing, and augmentation.\n• Gaining proficiency in utilizing TF-Keras preprocessing layers to streamline the data preprocessing pipeline.\n• Finally learn to preprocess diverse types of data in a simple manner for improved model performance in neural network applications.\n\nThe TensorFlow-Keras preprocessing layers API allows developers to construct input processing pipelines that seamlessly integrate with Keras models. These pipelines are adaptable for use both within Keras workflows and as standalone preprocessing routines in other frameworks. They can be effortlessly combined with Keras models, ensuring efficient and unified data handling. Additionally, these preprocessing pipelines can be saved and exported as part of a Keras SavedModel, facilitating easy deployment and sharing of models.\n\nWhat is the Need of TF-Keras?\n\nPrior to the data being fed into the neural network model, it plays a crucial role in the data preparation pipeline. You may construct end-to-end model pipelines that incorporate phases for both data preparation and model training using Keras preprocessing layers. By combining the entire workflow into a single Keras model, this feature simplifies the development process and promotes reproducibility.\n\nWe have two approaches to use these preprocessing layers. Let us explore them.\n\nIncorporating preprocessing layers directly into the model architecture. This involves integrating preprocessing steps as part of the model’s computational graph, ensuring that data transformations occur synchronously with the rest of the model execution. This approach leverages the computational power of devices, such as GPUs, enabling efficient preprocessing alongside model training. Particularly advantageous for operations like normalization, image preprocessing, and data augmentation, this method maximizes the benefits of GPU acceleration.\n\nApplying preprocessing to the input data pipeline, here the preprocessing is conducted on the CPU asynchronously, with the preprocessed data buffered before being fed into the model. By utilizing techniques such as dataset mapping and prefetching, preprocessing can occur efficiently in parallel with model training, optimizing overall performance. This can be used for TextVectorization.\n\nImage preprocessing layers, such as tf.keras.layers.Resizing, tf.keras.layers.Rescaling, and tf.keras.layers.CenterCrop, prepare image inputs by resizing, rescaling, and cropping them to standardized dimensions and ranges.\n\nImage data augmentation layers, like tf.keras.layers.RandomCrop, tf.keras.layers.RandomFlip, tf.keras.layers.RandomTranslation, tf.keras.layers.RandomRotation, tf.keras.layers.RandomZoom, and tf.keras.layers.RandomContrast, introduce random transformations to augment the training data, enhancing the model’s robustness and generalization.\n\nLet us use these layers on the emergency classification dataset from kaggle to learn how they can be implemented (note that here label 1 means presence of an emergency vehicle).\n• Notice that we didn’t need to know about what preprocessing we needed to perform and we directly fed the test data to the model.\n• In this scenario, we apply preprocessing techniques like resizing, rescaling, cropping, and augmentation to image data using various layers from TensorFlow’s Keras API. These techniques help prepare the images for model training by standardizing their sizes and introducing variations for improved generalization. Training the model on the preprocessed images enables it to learn and make predictions based on the features extracted from the images.\n• By incorporating these preprocessing layers directly into the neural network model, the entire preprocessing becomes part of the model architecture\n• Moreover, by encapsulating the preprocessing steps within the model, the model becomes more portable and reusable. It allows for easy deployment and inference on new data without the need to manually preprocess the data externally.\n\nFor text preprocessing we use tf.keras.layers.TextVectorization, this turns the text into an encoded representation that can be easily fed to an Embedding layer or a Dense layer.\n\nLet me demonstrate the use of the TextVectorizer using Tweets dataset from kaggle:\n\nThe TextVectorization layer exposes itself to the training data using the adapt() method because these are non-trainable layers, and their state must be set before the model training. This allows the layer to analyze the training data and configure its internal state accordingly. Once the object is instantiated, it can be reused on the test data later on.\n\n“tf.data.AUTOTUNE” dynamically adjusts the data processing operations in TensorFlow to maximize CPU utilization. Applying prefetching to the pipeline enables the system to automatically tune the number of elements to prefetch, optimizing performance during training and validation.\n\nComparison of TextVectorizer with another module Tokenizer\n\nLet’s compare TextVectorizer with another module Tokenizer from tf.keras.preprocessing.text to convert text to numerical values:\n\nAt the first glance we can see that the dimensions from both of them are different, let’s look at the differences in detail:\n• TextVectorization: Outputs a tensor with integer values, representing the indices of tokens in the vocabulary. The output_sequence_length parameter determines the shape of the output tensor, padding or truncating the sequences to a fixed length.\n• texts_to_matrix: Outputs a matrix where each row corresponds to a text sample, and each column corresponds to a unique word in the vocabulary. The values in the matrix represent word counts, determined by the mode parameter.\n• TextVectorization: The output_sequence_length parameter determines the shape of the output tensor, resulting in fixed-length sequences.\n• texts_to_matrix: The number of text samples and the size of the vocabulary determine the shape of the output matrix.\n• TextVectorization: Provides more flexibility in terms of preprocessing options, such as tokenization, lowercasing, and padding/truncating sequences.\n• texts_to_matrix: Provides options for different matrix modes (‘binary’, ‘count’, ‘tfidf’, ‘freq’) but doesn’t offer as much control over preprocessing steps.\n• tf.keras.layers.Normalization: It performs feature-wise normalization of the input.\n\nThese layers can easily be implemented in the following way:\n\nThe Normalization layers make each feature to have a mean close to 0 and a standard deviation close to 1, which is a characteristic of standardized data.\n\nIt is worth noting that we can set the mean and standard deviation of the resultant features to our preferences by utilizing the normalization layer’s hyperparameters.\n\nComing to the outputs of the latter code, the discretization layer creates equi-width bins. In the first row, the first feature -1.5 belongs to bin 0, the second feature 1.0 belongs to bin 2, the third feature 3.4 belongs to bin 3, and the fourth feature 0.5 belongs to bin 2.\n• tf.keras.layers.CategoryEncoding transforms integer categorical features into dense representations like one-hot, multi-hot, or count.\n• tf.keras.layers.Hashing executes categorical feature hashing, commonly referred to as the “hashing trick”.\n• tf.keras.layers.IntegerLookup converts integer categorical values into an encoded representation compatible with Embedding or Dense layers.\n• tf.keras.layers.StringLookup converts string categorical values into an encoded representation compatible with Embedding or Dense layers.\n\nThe elements in the matrix are float values representing the one-hot encoding of each category.\n\nFor example, the first row [0. 0. 0. 1.] corresponds to the category 3 (as indexing starts from 0), indicating that the original data item was 3.\n\nEach element represents the hash value assigned to the corresponding item.\n\nFor example, the first row [1] indicates that the hashing algorithm assigned the first item to the value 1.\n\nSimilarly, the second row [0] indicates that the hashing algorithm assigned the second item to the value 0.\n\nThere are multiple applications of TF-Keras. Let us look into few of the most important ones:\n\nBy integrating preprocessing layers into the model itself, it becomes easier to export an inference-only end-to-end model. This ensures that all the necessary preprocessing steps are encapsulated within the model, making it portable.\n\nUsers of the model don’t need to worry about the details of how each feature is preprocessed, encoded, or normalized. Whether it’s raw images or structured data, the inference model can handle them seamlessly without requiring users to understand the preprocessing pipelines.\n\nEase of Exporting to Other Runtimes\n\nExporting models to other runtimes, such as TensorFlow.js, becomes more straightforward when the model includes preprocessing layers within it. There’s no need to reimplement the preprocessing pipeline in the target language or framework.\n\nWith preprocessing layers integrated into the model, the inference model can directly process raw data. This is advantageous as it simplifies the deployment process and eliminates the need for users to preprocess data separately before feeding it into the model.\n\nPreprocessing layers are compatible with the tf.distribute API, enabling training across multiple machines or workers. For optimal performance, place these layers inside a tf.distribute.Strategy.scope().\n\nThe text can be encoded using different schemes such as multi-hot encoding or TF-IDF weighting. These preprocessing steps can be included within the model, simplifying the deployment process.\n• While working with very large vocabularies in lookup layers (e.g., TextVectorization, StringLookup) may impact performance. For such cases, it’s recommended to pre-compute the vocabulary and store it in a file rather than using adapt().\n• The TensorFlow team is slated to fix known issues with using lookup layers on TPUs or with ParameterServerStrategy in TensorFlow 2.7.\n\nThe TensorFlow Keras preprocessing layers API empowers developers to create Keras-native input processing pipelines. It facilitates building end-to-end models that handle raw data, perform feature normalization, and apply categorical feature encoding or hashing. You can integrate these preprocessing layers, adaptable to training data, directly into Keras models or employ them independently. Whether processed within the model or as part of the dataset, these functionalities enhance model portability and mitigate training/serving discrepancies, offering flexibility and efficiency in model deployment across diverse environments."
    },
    {
        "link": "https://keras.io/examples/vision/image_classification_from_scratch",
        "document": "Author: fchollet\n\n Date created: 2020/04/27\n\n Last modified: 2023/11/09\n\n Description: Training an image classifier from scratch on the Kaggle Cats vs Dogs dataset.\n\nⓘ This example uses Keras 3\n\nThis example shows how to do image classification from scratch, starting from JPEG image files on disk, without leveraging pre-trained weights or a pre-made Keras Application model. We demonstrate the workflow on the Kaggle Cats vs Dogs binary classification dataset.\n\nWe use the utility to generate the datasets, and we use Keras image preprocessing layers for image standardization and data augmentation.\n\nLoad the data: the Cats vs Dogs dataset\n\nFirst, let's download the 786M ZIP archive of the raw data:\n\nNow we have a folder which contain two subfolders, and . Each subfolder contains image files for each category.\n\nWhen working with lots of real-world image data, corrupted images are a common occurence. Let's filter out badly-encoded images that do not feature the string \"JFIF\" in their header.\n\nHere are the first 9 images in the training dataset.\n\nWhen you don't have a large image dataset, it's a good practice to artificially introduce sample diversity by applying random yet realistic transformations to the training images, such as random horizontal flipping or small random rotations. This helps expose the model to different aspects of the training data while slowing down overfitting.\n\nLet's visualize what the augmented samples look like, by applying repeatedly to the first few images in the dataset:\n\nOur image are already in a standard size (180x180), as they are being yielded as contiguous batches by our dataset. However, their RGB channel values are in the range. This is not ideal for a neural network; in general you should seek to make your input values small. Here, we will standardize values to be in the by using a layer at the start of our model.\n\nTwo options to preprocess the data\n\nThere are two ways you could be using the preprocessor:\n\nOption 1: Make it part of the model, like this:\n\nWith this option, your data augmentation will happen on device, synchronously with the rest of the model execution, meaning that it will benefit from GPU acceleration.\n\nNote that data augmentation is inactive at test time, so the input samples will only be augmented during , not when calling or .\n\nIf you're training on GPU, this may be a good option.\n\nOption 2: apply it to the dataset, so as to obtain a dataset that yields batches of augmented images, like this:\n\nWith this option, your data augmentation will happen on CPU, asynchronously, and will be buffered before going into the model.\n\nIf you're training on CPU, this is the better option, since it makes data augmentation asynchronous and non-blocking.\n\nIn our case, we'll go with the second option. If you're not sure which one to pick, this second option (asynchronous preprocessing) is always a solid choice.\n\nLet's apply data augmentation to our training dataset, and let's make sure to use buffered prefetching so we can yield data from disk without having I/O becoming blocking:\n\nWe'll build a small version of the Xception network. We haven't particularly tried to optimize the architecture; if you want to do a systematic search for the best model configuration, consider using KerasTuner.\n• We start the model with the preprocessor, followed by a layer.\n• We include a layer before the final classification layer.\n\nWe get to >90% validation accuracy after training for 25 epochs on the full dataset (in practice, you can train for 50+ epochs before validation performance starts degrading).\n\nNote that data augmentation and dropout are inactive at inference time."
    }
]