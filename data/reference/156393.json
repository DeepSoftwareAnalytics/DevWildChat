[
    {
        "link": "https://en.cppreference.com/w/cpp/thread",
        "document": "C++ includes built-in support for threads, atomic operations, mutual exclusion, condition variables, and futures.\n\nThreads enable programs to execute across several processor cores.\n\nThe components stop source, stop token, and stop callback can be used to asynchronously request that an operation stops execution in a timely manner, typically because the result is no longer required. Such a request is called a stop request.\n\nThese components specify the semantics of shared access to a stop state. Any object modeling any of these components that refer to the same stop state is an associated stop source, stop token, or stop callback, respectively.\n• to cooperatively cancel the execution such as for ,\n\nIn fact, they do not even need to be used to \"stop\" anything, but can instead be used for a thread-safe one-time function(s) invocation trigger, for example.\n\nThese components are provided for fine-grained atomic operations allowing for lockless concurrent programming. Each atomic operation is indivisible with regards to any other atomic operation that involves the same object. Atomic objects are free of data races.\n\nNeither the macro, nor any of the non-macro global namespace declarations are provided by any C++ standard library header other than .\n\nMutual exclusion algorithms prevent multiple threads from simultaneously accessing shared resources. This prevents data races and provides support for synchronization between threads.\n\nA condition variable is a synchronization primitive that allows multiple threads to communicate with each other. It allows some number of threads to wait (possibly with a timeout) for notification from another thread that they may proceed. A condition variable is always associated with a mutex.\n\nA semaphore is a lightweight synchronization primitive used to constrain concurrent access to a shared resource. When either would suffice, a semaphore can be more efficient than a condition variable.\n\nLatches and barriers are thread coordination mechanisms that allow any number of threads to block until an expected number of threads arrive. A latch cannot be reused, while a barrier can be used repeatedly.\n\nThe standard library provides facilities to obtain values that are returned and to catch exceptions that are thrown by asynchronous tasks (i.e. functions launched in separate threads). These values are communicated in a shared state, in which the asynchronous task may write its return value or store an exception, and which may be examined, waited for, and otherwise manipulated by other threads that hold instances of std::future or std::shared_future that reference that shared state.\n\nSafe-reclamation techniques are most frequently used to straightforwardly resolve access-deletion races."
    },
    {
        "link": "https://learn.microsoft.com/en-us/archive/msdn-magazine/2012/march/c-new-standard-concurrency-features-in-visual-c-11",
        "document": "C++ - New Standard Concurrency Features in Visual C++ 11\n\nThe latest C++ iteration, known as C++11 and approved by the International Organization for Standardization (ISO) in the past year, formalizes a new set of libraries and a few reserved words to deal with concurrency. Many developers have used concurrency in C++ before, but always through a third-party library—often directly exposing OS APIs.\n\nHerb Sutter announced in December 2004 that the “free performance lunch” was over in the sense that CPU manufacturers were prevented from shipping faster CPUs by physical power consumption and increasing heat reasons. This led to the current, mainstream multicore era, a new reality to which C++—the standard one—has just made an important leap to adapt.\n\nThe rest of this article is organized in two main sections and smaller subsections. The first main section, starting with Parallel Execution, covers technologies that allow applications to run inde­pendent or semi-independent activities in parallel. The second main section, starting with Syncing up Concurrent Execution, explores mechanisms for synchronizing the way these activities handle data, thus avoiding race conditions.\n\nThis article is based on features included in the upcoming version of Visual C++ (for now, called Visual C++ 11). A few of them are already available in the current version, Visual C++ 2010. Although not a guide to model parallel algorithms, nor an exhaustive documentation about all the available options, this article is a solid introduction to new C++11 concurrency features.\n\nWhen you model processes and design algorithms over data, there’s a natural tendency to specify them in a sequence of steps. As long as performance is within acceptable bounds, this is the most recommendable schema because it’s typically easier to understand—a requirement for maintainable code bases.\n\nWhen performance becomes a worrisome factor, a classic initial attempt to overcome the situation is to optimize the sequential algorithm in order to reduce the consumed CPU cycles. This can be done until you come to a point where no further optimizations are available—or they’re hard to achieve. Then the time to split the sequential series of steps into activities of simultaneous occurrence has come.\n\nIn the first section you’ll learn about the following:\n• Asynchronous tasks: those smaller portions of the original algorithm only linked by the data they produce or consume.\n• Threads: units of execution administrated by the runtime environment. They relate to tasks in the sense that tasks are run on threads.\n• Thread internals: thread-bound variables, exceptions propagated from threads and so on.\n\nIn the companion code to this article, you’ll find a project called Sequential Case, as shown in Figure 1.\n\nThe main function asks the user for some data and then submits that data to three functions: calculateA, calculateB and calculateC. The results are later combined to produce some output information for the user.\n\nThe calculating functions in the companion material are coded in a way such that a random delay between one and three seconds is introduced in each. Considering that these steps are executed sequentially, this leads to an overall execution time—once the input data is entered—of nine seconds in the worst-case scenario. You can try this code out by pressing F5 and running the sample.\n\nSo I need to revise the execution sequence and find steps to be performed concurrently. As these functions are independent, I can execute them in parallel by using the async function:\n\nI’ve introduced two concepts here: async and future, both defined in the <future> header and the std namespace. The first one receives a function, a lambda or a function object (functor) and returns a future. You can understand the concept of a future as the placeholder for an eventual result. Which result? The one returned by the function called asynchronously.\n\nAt some point, I’ll need the results of these parallel-running functions. Calling the get method on each future blocks the execution until the value is available.\n\nYou can test and compare the revised code with the sequential case by running the AsyncTasks project in the companion sample. The worst-case delay of this modification is about three seconds versus nine seconds for the sequential version.\n\nThis is a lightweight programming model that releases the developer from the duty of creating threads. However, you can specify threading policies, but I won’t cover those here.\n\nThe asynchronous task model presented in the previous section might suffice in some given scenarios, but if you need a deeper handling and control of the execution of threads, C++11 comes with the thread class, declared in the <thread> header and located in the std namespace.\n\nDespite being a more complex programming model, threads offer better methods for synchronization and coordination, allowing them to yield execution to another thread and wait for a determined amount of time or until another thread is finished before continuing.\n\nIn the following example (available in the Threads project of the companion code), I have a lambda function, which, given an integer argument, prints its multiples of less than 100,000 to the console:\n\nAs you’ll see in later examples, the fact that I passed a lambda to the thread is circumstantial; a function or functor would’ve sufficed as well.\n\nIn the main function I run this function in two threads with different parameters. Take a look at my result (which could vary between different runs due to timings):\n\nI might implement the example about asynchronous tasks in the previous section with threads. For this, I need to introduce the concept of a promise. A promise can be understood as a sink through which a result will be dropped when available. Where will that result come out once dropped? Each promise has an associated future.\n\nThe code shown in Figure 2, available in the Promises project of the sample code, associates three threads (instead of tasks) with promises and makes each thread call a calculate function. Compare these details with the lighter AsyncTasks version.\n\nIn C++ you can define global variables whose scope is bound to the entire application, including threads. But relative to threads, now there’s a way to define these global variables such that every thread keeps its own copy. This concept is known as thread local storage and it’s declared as follows:\n\nIf the declaration is done in the scope of a function, the visibility of the variable will be narrowed to that function but each thread will keep maintaining its own static copy. That is to say, values of the variable per thread are being kept between function invocations.\n\nAlthough thread_local isn’t available in Visual C++ 11, it can be simulated with a non-standard Microsoft extension:\n\nWhat would happen if an exception were thrown inside a thread? There will be cases in which the exception can be caught and handled in the call stack inside the thread. But if the thread doesn’t deal with the exception, you need a way to transport the exception to the initiator thread. C++11 introduces such mechanisms.\n\nIn Figure 3, available in the companion code in the project ThreadInternals, there’s a function sum_until_element_with_threshold, which traverses a vector until it finds a specific element, summing all the elements along the way. If the sum exceeds a threshold, an exception is thrown.\n\nIf that happens, the exception is captured via current_exception into an exception_ptr.\n\nThe main function triggers a thread on sum_until_element_with_threshold, while calling that same function with a different parameter. When both invocations have finished (the one in the main thread and the one in the thread triggered from it), their respective exception_ptrs will be analyzed:\n\nIf any of these exception_ptrs come initialized—a sign that some exception happened—their exceptions are triggered back with rethrow_exception:\n\nThis is the result of our execution, as the sum in the second thread exceeded its threshold:\n\nIt would be desirable if all applications could be split into a 100 percent-independent set of asynchronous tasks. In practice this is almost never possible, as there are at least dependencies on the data that all parties concurrently handle. This section introduces new C++11 technologies to avoid race conditions.\n• Mutexes and locks: elements that enable us to define thread-safe critical regions.\n• Condition variables: a way to freeze threads from execution until some criteria is satisfied.\n\nThe <atomic> header introduces a series of primitive types—atomic_char, atomic_int and so on—implemented in terms of interlocking operations. Thus, these types are equivalent to their homonyms without the atomic_ prefix but with the difference that all their assignment operators (==, ++, --, +=, *= and so on) are protected from race conditions. So it won’t happen that in the midst of an assignment to these data types, another thread irrupts and changes values before we’re done.\n\nIn the following example there are two parallel threads (one being the main) looking for different elements within the same vector:\n\nWhen each element is found, a message from within the thread is printed, telling the position in the vector (or iteration) where the element was found:\n\nThere’s also a common variable, total_iterations, which is updated with the compounded number of iterations applied by both threads. Thus, total_iterations must be atomic to prevent both threads from updating it at the same time. In the preceding example, even if you didn’t need to print the partial number of iterations in find_element, you’d still accumulate iterations in that local variable instead of total_iterations, to avoid contention over the atomic variable.\n\nYou’ll find the preceding sample in the Atomics project in the companion code download. I ran it, getting the following:\n\nThe previous section depicted a particular case of mutual exclusion for writing access on primitive types. The <mutex> header defines a series of lockable classes to define critical regions. That way, you can define a mutex to establish a critical region throughout a series of functions or methods, in the sense that only one thread at a time will be able to access any member in this series by successfully locking its mutex.\n\nA thread attempting to lock a mutex can either stay blocked until the mutex is available or just fail in the attempt. In the middle of these two extremes, the alternative timed_mutex class can stay blocked for a small interval of time before failing. Allowing lock attempts to desist helps prevent deadlocks.\n\nA locked mutex must be explicitly unlocked for others to lock it. Failing to do so could lead to an undetermined application behavior—which could be error-prone, similar to forgetting to release dynamic memory. Forgetting to release a lock is actually much worse, because it might mean that the application can’t function properly anymore if other code keeps waiting on that lock. Fortunately, C++11 also comes with locking classes. A lock acts on a mutex, but its destructor makes sure to release it if locked.\n\nThe following code (available in the Mutex project in the code download) defines a critical region around a mutex mx:\n\nThis mutex is used to guarantee that two functions, funcA and funcB, can run in parallel without coming together in the critical region.\n\nThe function funcA will wait, if necessary, in order to come to the critical region. In order to make it do so, you just need the simplest locking mechanism—lock_guard:\n\nThe way it’s defined, funcA should access the critical region three times. The function funcB, instead, will attempt to lock, but if the mutex is by then already locked, funcB will just wait for a second before again attempting to get access to the critical region. The mechanism it uses is unique_lock with the policy try_to_lock_t, as shown in Figure 4.\n\nThe way it’s defined, funcB will try up to five times to enter the critical region. Figure 5 shows the result of the execution. Out of the five attempts, funcB could only come to the critical region twice.\n\nThe header <condition_variable> comes with the last facility covered in this article, fundamental for those cases when coordination between threads is tied to events.\n\nIn the following example, available in project CondVar in the code download, a producer function pushes elements in a queue:\n\nThe standard queue isn’t thread-safe, so you must make sure that nobody else is using it (that is, the consumer isn’t popping any element) when queuing.\n\nThe consumer function attempts to fetch elements from the queue when available, or it just waits for a while on the condition variable before attempting again; after two consecutive failed attempts, the consumer ends (see Figure 6).\n\nThe consumer is to be awoken via notify_all by the producer every time a new element is available. That way, the producer avoids having the consumer sleep for the entire interval if elements are ready.\n\nFigure 7shows the result of my run.\n\nTo recap, this article has shown a conceptual panorama of mechanisms introduced in C++11 to allow parallel execution in an era where multicore environments are mainstream.\n\nAsynchronous tasks enable a lightweight programming model to parallelize execution. The outcomes of each task can be retrieved through an associated future.\n\nThreads offer more granularity than tasks—although they’re heavier—together with mechanisms for keeping separated copies of static variables and transporting exceptions between threads.\n\nAs parallel threads act on common data, C++11 provides resources to avoid race conditions. Atomic types enable a trusted way to ensure that data is modified by one thread at a time.\n\nMutexes help us define critical regions throughout the code—regions to which threads are prevented access simultaneously. Locks wrap mutexes, tying the unlocking of the latter to the lifecycle of the former.\n\nFinally, condition variables grant more efficiency to thread synchronization, as some threads can wait for events notified by other threads.\n\nThis article hasn’t covered all the many ways to configure and use each of these features, but the reader now has a holistic vision of them and is ready to dig deeper.\n\nDiego Dagum is a software developer with more than 20 years of experience. He’s currently a Visual C++ community program manager with Microsoft.\n\nThanks to the following technical experts for reviewing this article: David Cravey, Alon Fliess, Fabio Galuppo and Marc Gregoire"
    },
    {
        "link": "https://stackoverflow.com/questions/19759983/what-does-c11-consider-to-be-a-thread",
        "document": "A thread of execution (also known as a thread) is a single flow of control within a program, including the initial invocation of a specific top-level function, and recursively including every function invocation subsequently executed by the thread. [ Note: When one thread creates another, the initial call to the top-level function of the new thread is executed by the new thread, not by the creating thread. — end note ]\n\nThe italicized terms indicate that this is definitive. You could argue that this definition is mathematically deficient, because each function invocation defines a new thread, but that's just obviously wrong. They mean maximal single flow of control, otherwise the non-normative note would cancel the effect of the normative \"recursively including\" text.\n\nFrom the standpoint of the core language, it is merely incidental that causes such a thing to exist.\n\nWhat if I use a library that provides user-space threads - does each of those get its own copies of objects (I don't really see how that could be implemented)?\n\nThere is no way to write such a library without kernel calls. In all likelihood all threads in your process are already represented a high-level abstraction such as pthreads, just to satisfy the kernel. The C++ standard library is likely written against the native threading library to \"just work\" without additional glue.\n\nFor example, objects are initialized at first access rather than when each new thread starts, so the compiler just has to insert a query based on to access and perhaps initialize. Initialization would register a destructor with the facility.\n\nWhat is implementation-defined here is whether the pre-existing native library is compatible with C++. Supposing they provide that, and it's something customers would tend to want, all other threading libraries built atop it will be automatically compatible barring some other conflict."
    },
    {
        "link": "https://geeksforgeeks.org/multithreading-in-cpp",
        "document": "Multithreading is a feature that allows concurrent execution of two or more parts of a program for maximum utilization of the CPU. Each part of such a program is called a thread. So, threads are lightweight processes within a process.\n\nMultithreading support was introduced in C++11. Prior to C++11, we had to use POSIX threads or <pthreads> library. While this library did the job, the lack of any standard language-provided feature set caused serious portability issues. C++ 11 did away with all that and gave us std::thread. The thread classes and related functions are defined in the <thread> header file.\n\nthread is the thread class that represents a single thread in C++. To start a thread we simply need to create a new thread object and pass the executing code to be called (i.e, a callable object) into the constructor of the object. Once the object is created a new thread is launched which will execute the code specified in callable. A callable can be any of the five:\n\nAfter defining the callable, we pass it to the constructor.\n\nMultithreading allows you to execute multiple threads simultaneously, improving performance.\n\nA function pointer can be a callable object to pass to the std::thread constructor for initializing a thread. The following code snippet demonstrates how it is done.\n\nstd::thread object can also be launched using a lambda expression as a callable. The following code snippet demonstrates how this is done:\n\nFunction Objects or Functors can also be used for launching a thread in C++. The following code snippet demonstrates how it is done:\n\nWe can also launch the thread using the non-static member function of a class. The following snippet demonstrates how to do it.\n\nWe can also launch the threads using static member functions.\n\nOnce a thread has started we may need to wait for the thread to finish before we can take some action. For instance, if we allocate the task of initializing the GUI of an application to a thread, we need to wait for the thread to finish to ensure that the GUI has loaded properly.\n\nTo wait for a thread, use the std::thread::join() function. This function makes the current thread wait until the thread identified by *this has finished executing.\n\nFor instance, to block the main thread until thread t1 has finished we would do:\n\nA C++ program is given below. It launches three threads from the main function. Each thread is called using one of the callable objects specified above."
    },
    {
        "link": "https://stackoverflow.com/questions/15752659/thread-pooling-in-c11",
        "document": "Instead of creating and joining threads each iteration, I'd prefer to send tasks to my worker threads each iteration and only create them once.\n\nI have code that looks like this:\n\nHow do I get a pool of threads to send tasks to , without creating and deleting them over and over again? This means persistent threads to resynchronize without joining.\n\nThis is adapted from my answer to another very similar post. class ThreadPool { public: void Start(); void QueueJob(const std::function<void()>& job); void Stop(); bool busy(); private: void ThreadLoop(); bool should_terminate = false; // Tells threads to stop looking for jobs std::mutex queue_mutex; // Prevents data races to the job queue std::condition_variable mutex_condition; // Allows threads to wait on new jobs or termination std::vector<std::thread> threads; std::queue<std::function<void()>> jobs; }; For an efficient threadpool implementation, once threads are created according to , it's better not to create new ones or destroy old ones (by joining). There will be a performance penalty, and it might even make your application go slower than the serial version. Thus, we keep a pool of threads that can be used at any time (if they aren't already running a job). Each thread should be running its own infinite loop, constantly waiting for new tasks to grab and run. void ThreadPool::Start() { const uint32_t num_threads = std::thread::hardware_concurrency(); // Max # of threads the system supports for (uint32_t ii = 0; ii < num_threads; ++ii) { threads.emplace_back(std::thread(&ThreadPool::ThreadLoop,this)) } } The infinite loop function. This is a loop waiting for the task queue to open up. void ThreadPool::ThreadLoop() { while (true) { std::function<void()> job; { std::unique_lock<std::mutex> lock(queue_mutex); mutex_condition.wait(lock, [this] { return !jobs.empty() || should_terminate; }); if (should_terminate) { return; } job = jobs.front(); jobs.pop(); } job(); } } Add a new job to the pool; use a lock so that there isn't a data race. The busy() function can be used in a while loop, such that the main thread can wait the threadpool to complete all the tasks before calling the threadpool destructor. Once you integrate these ingredients, you have your own dynamic threading pool. These threads always run, waiting for job to do. I apologize if there are some syntax errors, I typed this code and and I have a bad memory. Sorry that I cannot provide you the complete thread pool code; that would violate my job integrity.\n• The anonymous code blocks are used so that when they are exited, the variables created within them go out of scope, unlocking the mutex.\n• will not terminate any currently running jobs, it just waits for them to finish via .\n\nA pool of threads means that all your threads are running, all the time – in other words, the thread function never returns. To give the threads something meaningful to do, you have to design a system of inter-thread communication, both for the purpose of telling the thread that there's something to do, as well as for communicating the actual work data. Typically this will involve some kind of concurrent data structure, and each thread would presumably sleep on some kind of condition variable, which would be notified when there's work to do. Upon receiving the notification, one or several of the threads wake up, recover a task from the concurrent data structure, process it, and store the result in an analogous fashion. The thread would then go on to check whether there's even more work to do, and if not go back to sleep. The upshot is that you have to design all this yourself, since there isn't a natural notion of \"work\" that's universally applicable. It's quite a bit of work, and there are some subtle issues you have to get right. (You can program in Go if you like a system which takes care of thread management for you behind the scenes.)\n\nA threadpool is at core a set of threads all bound to a function working as an event loop. These threads will endlessly wait for a task to be executed, or their own termination. The threadpool job is to provide an interface to submit jobs, define (and perhaps modify) the policy of running these jobs (scheduling rules, thread instantiation, size of the pool), and monitor the status of the threads and related resources. So for a versatile pool, one must start by defining what a task is, how it is launched, interrupted, what is the result (see the notion of promise and future for that question), what sort of events the threads will have to respond to, how they will handle them, how these events shall be discriminated from the ones handled by the tasks. This can become quite complicated as you can see, and impose restrictions on how the threads will work, as the solution becomes more and more involved. The current tooling for handling events is fairly barebones(*): primitives like mutexes, condition variables, and a few abstractions on top of that (locks, barriers). But in some cases, these abstrations may turn out to be unfit (see this related question), and one must revert to using the primitives. Other problems have to be managed too: How would these play out in your setting? This answer to a similar question points to an existing implementation meant for boost and the stl. I offered a very crude implementation of a threadpool for another question, which doesn't address many problems outlined above. You might want to build up on it. You might also want to have a look of existing frameworks in other languages, to find inspiration. (*) I don't see that as a problem, quite to the contrary. I think it's the very spirit of C++ inherited from C.\n\nEdit: This now requires C++17 and concepts. (As of 9/12/16, only g++ 6.0+ is sufficient.) The template deduction is a lot more accurate because of it, though, so it's worth the effort of getting a newer compiler. I've not yet found a function that requires explicit template arguments. It also now takes any appropriate callable object (and is still statically typesafe!!!). It also now includes an optional green threading priority thread pool using the same API. This class is POSIX only, though. It uses the API for userspace task switching. I created a simple library for this. An example of usage is given below. (I'm answering this because it was one of the things I found before I decided it was necessary to write it myself.) bool is_prime(int n){ // Determine if n is prime. } int main(){ thread_pool pool(8); // 8 threads list<future<bool>> results; for(int n = 2;n < 10000;n++){ // Submit a job to the pool. results.emplace_back(pool.async(is_prime, n)); } int n = 2; for(auto i = results.begin();i != results.end();i++, n++){ // i is an iterator pointing to a future representing the result of is_prime(n) cout << n << \" \"; bool prime = i->get(); // Wait for the task is_prime(n) to finish and get the result. if(prime) cout << \"is prime\"; else cout << \"is not prime\"; cout << endl; } } You can pass any function with any (or void) return value and any (or no) arguments and it will return a corresponding . To get the result (or just wait until a task has completed) you call on the future."
    },
    {
        "link": "https://stackoverflow.com/questions/76184202/c-thread-synchronization-using-mutex-and-condition-variable",
        "document": "Ping-Pong with mutex and two condition variables\n\nThis is the canonical ping-pong using a mutex and condition variables. Note that 1) you need two condition variables to make ping-pong work and 2) you have to be careful about placing the output statements in a block where the lock is still held. Your code is close.\n\nThis should result in the following output.\n\nDepending on your platform, it may be more performant (and a little simpler to grok) to use an atomic flag instead of condition variables. This produces the same output as above."
    },
    {
        "link": "https://geeksforgeeks.org/cpp-multithreading-condition-variables",
        "document": "In C++, the condition variable is a synchronization primitive that is used to notify the other threads in a multithreading environment that the shared resource is free to access. It is defined as the std::condition_variable class inside the <condition_variable> header file.\n\nPrerequisite: C++ Multithreading, Mutex in C++.\n\nNeed for Condition Variable in C++\n\nCondition variable is especially needed in cases where one thread has to wait for another thread execution to continue the work. For example, the producer-consumer relationship, sender-receiver relationship, etc.\n\nIn these cases, the condition variable makes the thread wait till it is notified by the other thread. It is used with mutex locks to block access to the shared resource when one thread is working on it.\n\nThe syntax to declare a condition variable is simple:\n\nAfter that, we use the associated method for different operations.\n\nThe std::condition_variable methods contain some member methods to provide the basic functionalities. Some of these are:\n\nExample: Program to Illustrate the Use of Condition Variable\n\nIn this program, the consumer thread uses the condition variable cv to wait until data_ready is set to true while the producer thread sleeps for two seconds to mimic data generation.\n\nErrors Associated with C++ Condition Variable\n\nThe condition variable is prone to the following errors:\n• Spurious Wakeup: Spurious wakeup refers to the condition when the consumer/receiver thread finishes its work before it is notified by the producer/sender. In the above example, we have used the variable data_ready precisely to cope with this error.\n• Lost Wakeup: Lost wakeup refers to the condition when the sender sends the notification but there is no receiver in the wait for the notification yet.\n\nThe following are the major advantages of using condition variables in our C++ program:\n• None The condition variable provide a way to signal the thread of a particular condition.\n• None The sleeping thread in condition variable is different from the waiting threads consuming less resources.\n\nIn conclusion, condition variables are an effective tool for assuring safe access to shared data, lowering contention, and establishing effective thread synchronisation in multi-threaded C++ programmes. They are commonly used with the mutex to provide an efficient synchronization technique."
    },
    {
        "link": "https://stackoverflow.com/questions/20982270/sync-is-unreliable-using-stdatomic-and-stdcondition-variable",
        "document": "In a distributed job system written in C++11 I have implemented a fence (i.e. a thread outside the worker thread pool may ask to block until all currently scheduled jobs are done) using the following structure:\n\nThe code implementing the fence looks like this:\n\nThis works very well if threads enter the fence over a period of time. However, if they try to do it almost simultaneously, it seems to sometimes happen that between the atomic decrementation (1) and starting the wait on the conditional var (3), the thread yields CPU time and another thread decrements the counter to zero (1) and fires the cond. var (2). This results in the previous thread waiting forever in (3), because it starts waiting on it after it has already been notified.\n\nA hack to make the thing workable is to put a 10 ms sleep just before (2), but that's unacceptable for obvious reasons.\n\nAny suggestions on how to fix this in a performant way?"
    },
    {
        "link": "https://en.cppreference.com/w/cpp/thread/condition_variable",
        "document": "is a synchronization primitive used with a std::mutex to block one or more threads until another thread both modifies a shared variable (the condition) and notifies the .\n\nThe thread that intends to modify the shared variable must:\n• Modify the shared variable while the lock is owned.\n• Call or on the (can be done after releasing the lock).\n\nEven if the shared variable is atomic, it must be modified while owning the mutex to correctly publish the modification to the waiting thread.\n\nAny thread that intends to wait on a must:\n• Acquire a on the mutex used to protect the shared variable.\n• Do one of the following:\n\nworks only with std::unique_lock<std::mutex>, which allows for maximal efficiency on some platforms. std::condition_variable_any provides a condition variable that works with any BasicLockable object, such as std::shared_lock.\n\nCondition variables permit concurrent invocation of the wait, wait_for, wait_until, notify_one and notify_all member functions.\n\nThe class is a StandardLayoutType. It is not CopyConstructible, MoveConstructible, CopyAssignable, or MoveAssignable.\n\nis used in combination with a std::mutex to facilitate inter-thread communication. m std cv data ready processed worker_thread lk m cv. lk, ready // after the wait, we own the lock data processed // manual unlocking is done before notifying, to avoid waking up // the waiting thread only to block again (see notify_one for details) lk. cv. main worker worker_thread data lk m ready cv. lk m cv. lk, processed data worker. main() signals data ready for processing Worker thread is processing data Worker thread signals data processing completed Back in main(), data = Example data after processing"
    },
    {
        "link": "https://modernescpp.com/index.php/synchronization-with-atomics-in-c-20",
        "document": "Sender/receiver workflows are pretty common for threads. In such a workflow, the receiver is waiting for the sender’s notification before it continues to work. There are various ways to implement these workflows. With C++11, you can use condition variables or promise/future pairs; with C++20, you can use atomics.\n\nThere are various ways to synchronize threads. Each way has its pros and cons. Consequently, I want to compare them. I assume you don’t know the details of condition variables or promises and futures. Therefore, I will give a short refresher.\n\nA condition variable can fulfill the role of a sender or a receiver. As a sender, it can notify one or more receivers.\n\nThe program has two child threads: and . They get their payload and in lines (1) and (2). The function notifies that it is done with the preparation of the work: . While holding the lock, the thread is waiting for its notification: . The waiting thread always performs the same steps. When it is waked up, it checks the predicate while holding the lock ( ). If the predicate does not hold, it puts itself back to sleep. If the predicate holds, it continues with its work. In the concrete workflow, the sending thread puts the initial values into the (3), which the receiving thread completes (4).\n\nCondition variables have many inherent issues. For example, the receiver could be awakened without notification or could lose the notification. The first issue is spurious wakeup, and the second is lost wakeup. The predicate protects against both flaws. The notification would be lost when the sender sends its notification before the receiver is in the wait state and does not use a predicate.\n\nConsequently, the receiver waits for something that never happens. This is a deadlock. When you study the program’s output, you see that each second run would cause a deadlock if I would not use a predicate. Of course, it is possible to use condition variables without a predicate.\n\nIf you want to know the sender/receiver workflow details and the traps of condition variables, read my previous post, “C++ Core Guidelines: Be Aware of the Traps of Condition Variables“.\n\nWhen you only need a one-time notification, such as in the previous program, promises and futures are a better choice than condition variables. Promises and futures cannot be victims of spurious or lost wakeups.\n\nA promise can send a value, an exception, or a notification to its associated future. Let me use a promise and a future to refactor the previous workflow. Here is the same workflow using a promise/future pair.\n\nWhen you study the workflow, you recognize that the synchronization is reduced to its essential parts: (1) and There is neither a need to use locks or mutexes nor is there a need to use a predicate to protect against spurious or lost wakeups. I skip the screenshot to this run because it is essentially the same, such as in the case of the previous run with condition variables.\n\nOne downside to using promises and futures is that they can only be used once. Here are my previous posts on promises and futures, often called tasks.\n\nIf you want to communicate more than once, use condition variables or atomics.\n\nstd::atomic_flag in C++11 has a simple interface. Its member function clear lets you set its value to false, with test_and_set to true. In case you use test_and_set you get the old value back. enables it to initialize the to . std::atomic_flag has two exciting properties.\n\nThe remaining more powerful atomics can provide their functionality by using a mutex. That is according to the C++ standard. So these atomics have a member function is_lock_free .On the popular platforms, I always get the answer . But you should be aware of that. Here are more details on the capabilities of C++11.\n\nNow, I jump directly from C++11 to C++20. With C++20, support new member functions: ), , and . The member functions or notify one or all of the waiting atomic flags. needs a boolean . The call blocks until the subsequent notification or spurious wakeup. It checks then if the value is equal to and unblocks if not. The value serves as a kind of predicate.\n\nAdditionally, to C++11, default construction of a sets it in its state, and you can ask for the value of the via . With this knowledge, it’s pretty easy to refactor previous programs using a .\n\nThe thread preparing the work (1) sets the to and sends the notification. The thread completing the work waits for the notification. It is only unblocked if is equal to .\n\nHere are a few runs of the program with the Microsoft Compiler.\n\nI’m not sure if I would use a future/promise pair or a for such a simple thread synchronization workflow. Both are thread-safe by design and require no protection mechanism so far. Promise and promise are easier to use but is probably faster. I’m sure I would not use a condition variable if possible.\n\nWhen you create a more complicated thread synchronization workflow, such as a ping/pong game, a promise/future pair is no option. You have to use condition variables or atomics for multiple synchronizations. In my next post, I will implement a ping/pong game using condition variables and a and measure their performance.\n\nI take a short Christmas break and publish the following post on the 11.th of January. To learn more about C++20, read my new book at Leanpub to C++20."
    }
]