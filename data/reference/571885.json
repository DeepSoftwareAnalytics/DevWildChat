[
    {
        "link": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html",
        "document": "Minimization of scalar function of one or more variables.\n\nThe objective function to be minimized: where is a 1-D array with shape (n,) and is a tuple of the fixed parameters needed to completely specify the function. Suppose the callable has signature , where and are required positional and keyword arguments. Rather than passing as the callable, wrap it to accept only ; e.g., pass as the callable, where (tuple) and (dict) have been gathered before invoking this function. Initial guess. Array of real elements of size (n,), where is the number of independent variables. Extra arguments passed to the objective function and its derivatives (fun, jac and hess functions). Type of solver. Should be one of\n• None custom - a callable object, see below for description. If not given, chosen to be one of , , , depending on whether or not the problem has constraints or bounds. Method for computing the gradient vector. Only for CG, BFGS, Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov, trust-exact and trust-constr. If it is a callable, it should be a function that returns the gradient vector: where is an array with shape (n,) and is a tuple with the fixed parameters. If jac is a Boolean and is True, fun is assumed to return a tuple containing the objective function and the gradient. Methods ‘Newton-CG’, ‘trust-ncg’, ‘dogleg’, ‘trust-exact’, and ‘trust-krylov’ require that either a callable be supplied, or that fun return the objective and gradient. If None or False, the gradient will be estimated using 2-point finite difference estimation with an absolute step size. Alternatively, the keywords {‘2-point’, ‘3-point’, ‘cs’} can be used to select a finite difference scheme for numerical estimation of the gradient with a relative step size. These finite difference schemes obey any specified bounds. Method for computing the Hessian matrix. Only for Newton-CG, dogleg, trust-ncg, trust-krylov, trust-exact and trust-constr. If it is callable, it should return the Hessian matrix: where is a (n,) ndarray and is a tuple with the fixed parameters. The keywords {‘2-point’, ‘3-point’, ‘cs’} can also be used to select a finite difference scheme for numerical estimation of the hessian. Alternatively, objects implementing the interface can be used to approximate the Hessian. Available quasi-Newton methods implementing this interface are: Not all of the options are available for each of the methods; for availability refer to the notes. Hessian of objective function times an arbitrary vector p. Only for Newton-CG, trust-ncg, trust-krylov, trust-constr. Only one of hessp or hess needs to be given. If hess is provided, then hessp will be ignored. hessp must compute the Hessian times an arbitrary vector: where is a (n,) ndarray, is an arbitrary vector with dimension (n,) and is a tuple with the fixed parameters. Bounds on variables for Nelder-Mead, L-BFGS-B, TNC, SLSQP, Powell, trust-constr, COBYLA, and COBYQA methods. There are two ways to specify the bounds:\n• None Sequence of pairs for each element in x. None is used to specify no bound. Constraints definition. Only for COBYLA, COBYQA, SLSQP and trust-constr. Constraints for ‘trust-constr’ and ‘cobyqa’ are defined as a single object or a list of objects specifying constraints to the optimization problem. Available constraints are: Constraints for COBYLA, SLSQP are defined as a list of dictionaries. Each dictionary with fields: The Jacobian of fun (only for SLSQP). Extra arguments to be passed to the function and Jacobian. Equality constraint means that the constraint function result is to be zero whereas inequality means that it is to be non-negative. Note that COBYLA only supports inequality constraints. Tolerance for termination. When tol is specified, the selected minimization algorithm sets some relevant solver-specific tolerance(s) equal to tol. For detailed control, use solver-specific options. A dictionary of solver options. All methods except TNC accept the following generic options: Maximum number of iterations to perform. Depending on the method each iteration may use several function evaluations. For TNC use maxfun instead of maxiter. All methods except TNC, SLSQP, and COBYLA support a callable with the signature: where is a keyword parameter containing an with attributes and , the present values of the parameter vector and objective function. Note that the name of the parameter must be for the callback to be passed an . These methods will also terminate if the callback raises . All methods except trust-constr (also) support a signature like: where is the current parameter vector. Introspection is used to determine which of the signatures above to invoke. The optimization result represented as a object. Important attributes are: the solution array, a Boolean flag indicating if the optimizer exited successfully and which describes the cause of the termination. See for a description of other attributes.\n\nThis section describes the available solvers that can be selected by the ‘method’ parameter. The default method is BFGS.\n\nMethod CG uses a nonlinear conjugate gradient algorithm by Polak and Ribiere, a variant of the Fletcher-Reeves method described in [5] pp.120-122. Only the first derivatives are used.\n\nMethod BFGS uses the quasi-Newton method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5] pp. 136. It uses the first derivatives only. BFGS has proven good performance even for non-smooth optimizations. This method also returns an approximation of the Hessian inverse, stored as hess_inv in the OptimizeResult object.\n\nMethod Newton-CG uses a Newton-CG algorithm [5] pp. 168 (also known as the truncated Newton method). It uses a CG method to the compute the search direction. See also TNC method for a box-constrained minimization with a similar algorithm. Suitable for large-scale problems.\n\nMethod dogleg uses the dog-leg trust-region algorithm [5] for unconstrained minimization. This algorithm requires the gradient and Hessian; furthermore the Hessian is required to be positive definite.\n\nMethod trust-ncg uses the Newton conjugate gradient trust-region algorithm [5] for unconstrained minimization. This algorithm requires the gradient and either the Hessian or a function that computes the product of the Hessian with a given vector. Suitable for large-scale problems.\n\nMethod trust-krylov uses the Newton GLTR trust-region algorithm [14], [15] for unconstrained minimization. This algorithm requires the gradient and either the Hessian or a function that computes the product of the Hessian with a given vector. Suitable for large-scale problems. On indefinite problems it requires usually less iterations than the trust-ncg method and is recommended for medium and large-scale problems.\n\nMethod trust-exact is a trust-region method for unconstrained minimization in which quadratic subproblems are solved almost exactly [13]. This algorithm requires the gradient and the Hessian (which is not required to be positive definite). It is, in many situations, the Newton method to converge in fewer iterations and the most recommended for small and medium-size problems.\n\nMethod Nelder-Mead uses the Simplex algorithm [1], [2]. This algorithm is robust in many applications. However, if numerical computation of derivative can be trusted, other algorithms using the first and/or second derivatives information might be preferred for their better performance in general.\n\nMethod L-BFGS-B uses the L-BFGS-B algorithm [6], [7] for bound constrained minimization.\n\nMethod Powell is a modification of Powell’s method [3], [4] which is a conjugate direction method. It performs sequential one-dimensional minimizations along each vector of the directions set (direc field in options and info), which is updated at each iteration of the main minimization loop. The function need not be differentiable, and no derivatives are taken. If bounds are not provided, then an unbounded line search will be used. If bounds are provided and the initial guess is within the bounds, then every function evaluation throughout the minimization procedure will be within the bounds. If bounds are provided, the initial guess is outside the bounds, and direc is full rank (default has full rank), then some function evaluations during the first iteration may be outside the bounds, but every function evaluation after the first iteration will be within the bounds. If direc is not full rank, then some parameters may not be optimized and the solution is not guaranteed to be within the bounds.\n\nMethod TNC uses a truncated Newton algorithm [5], [8] to minimize a function with variables subject to bounds. This algorithm uses gradient information; it is also called Newton Conjugate-Gradient. It differs from the Newton-CG method described above as it wraps a C implementation and allows each variable to be given upper and lower bounds.\n\nMethod COBYLA uses the Constrained Optimization BY Linear Approximation (COBYLA) method [9], [10], [11]. The algorithm is based on linear approximations to the objective function and each constraint. The method wraps a FORTRAN implementation of the algorithm. The constraints functions ‘fun’ may return either a single number or an array or list of numbers.\n\nMethod COBYQA uses the Constrained Optimization BY Quadratic Approximations (COBYQA) method [18]. The algorithm is a derivative-free trust-region SQP method based on quadratic approximations to the objective function and each nonlinear constraint. The bounds are treated as unrelaxable constraints, in the sense that the algorithm always respects them throughout the optimization process.\n\nMethod SLSQP uses Sequential Least SQuares Programming to minimize a function of several variables with any combination of bounds, equality and inequality constraints. The method wraps the SLSQP Optimization subroutine originally implemented by Dieter Kraft [12]. Note that the wrapper handles infinite values in bounds by converting them into large floating values.\n\nMethod trust-constr is a trust-region algorithm for constrained optimization. It switches between two implementations depending on the problem definition. It is the most versatile constrained minimization algorithm implemented in SciPy and the most appropriate for large-scale problems. For equality constrained problems it is an implementation of Byrd-Omojokun Trust-Region SQP method described in [17] and in [5], p. 549. When inequality constraints are imposed as well, it switches to the trust-region interior point method described in [16]. This interior point algorithm, in turn, solves inequality constraints by introducing slack variables and solving a sequence of equality-constrained barrier problems for progressively smaller values of the barrier parameter. The previously described equality constrained SQP method is used to solve the subproblems with increasing levels of accuracy as the iterate gets closer to a solution.\n\nFor Method trust-constr the gradient and the Hessian may be approximated using three finite-difference schemes: {‘2-point’, ‘3-point’, ‘cs’}. The scheme ‘cs’ is, potentially, the most accurate but it requires the function to correctly handle complex inputs and to be differentiable in the complex plane. The scheme ‘3-point’ is more accurate than ‘2-point’ but requires twice as many operations. If the gradient is estimated via finite-differences the Hessian must be estimated using one of the quasi-Newton strategies.\n\nIt may be useful to pass a custom minimization method, for example when using a frontend to this method such as or a different library. You can simply pass a callable as the parameter.\n\nThe callable is called as where corresponds to any other parameters passed to (such as callback, hess, etc.), except the options dict, which has its contents also passed as method parameters pair by pair. Also, if jac has been passed as a bool type, jac and fun are mangled so that fun returns just the function values and jac is converted to a function returning the Jacobian. The method shall return an object.\n\nThe provided method callable must be able to accept (and possibly ignore) arbitrary parameters; the set of parameters accepted by may expand in future versions and then these parameters will be passed to the method. You can find an example in the scipy.optimize tutorial.\n\nLet us consider the problem of minimizing the Rosenbrock function. This function (and its respective derivatives) is implemented in (resp. , ) in the . A simple application of the Nelder-Mead method is: Now using the BFGS algorithm, using the first derivative and a few options: Next, consider a minimization problem with several constraints (namely Example 16.4 from [5]). The objective function is: There are three constraints defined as: And variables must be positive, hence the following bounds: The optimization problem is solved using the SLSQP method as: It should converge to the theoretical solution (1.4 ,1.7)."
    },
    {
        "link": "https://docs.scipy.org/doc/scipy/tutorial/optimize.html",
        "document": "The package provides several commonly used optimization algorithms. A detailed listing is available: (can also be found by ).\n\nThe function provides a common interface to unconstrained and constrained minimization algorithms for multivariate scalar functions in . To demonstrate the minimization function, consider the problem of minimizing the Rosenbrock function of \\(N\\) variables: The minimum value of this function is 0 which is achieved when \\(x_{i}=1.\\) Note that the Rosenbrock function and its derivatives are included in . The implementations shown in the following sections provide examples of how to define an objective function as well as its jacobian and hessian functions. Objective functions in expect a numpy array as their first parameter which is to be optimized and must return a float value. The exact calling signature must be where represents a numpy array and a tuple of additional arguments supplied to the objective function. In the example below, the routine is used with the Nelder-Mead simplex algorithm (selected through the parameter): The simplex algorithm is probably the simplest way to minimize a fairly well-behaved function. It requires only function evaluations and is a good choice for simple minimization problems. However, because it does not use any gradient evaluations, it may take longer to find the minimum. Another optimization algorithm that needs only function calls to find the minimum is Powell’s method available by setting in . To demonstrate how to supply additional arguments to an objective function, let us minimize the Rosenbrock function with an additional scaling factor a and an offset b: Again using the routine this can be solved by the following code block for the example parameters and . As an alternative to using the parameter of , simply wrap the objective function in a new function that accepts only . This approach is also useful when it is necessary to pass additional parameters to the objective function as keyword arguments. Another alternative is to use . In order to converge more quickly to the solution, this routine uses the gradient of the objective function. If the gradient is not given by the user, then it is estimated using first-differences. The Broyden-Fletcher-Goldfarb-Shanno (BFGS) method typically requires fewer function calls than the simplex algorithm even when the gradient must be estimated. To demonstrate this algorithm, the Rosenbrock function is again used. The gradient of the Rosenbrock function is the vector: This expression is valid for the interior derivatives. Special cases are A Python function which computes this gradient is constructed by the code-segment: This gradient information is specified in the function through the parameter as illustrated below. It is common for the objective function and its gradient to share parts of the calculation. For instance, consider the following problem. # let's keep track of how many times it runs Here, is called 12 times: six times in the objective function and six times from the gradient. One way of reducing redundant calculations is to create a single function that returns both the objective function and the gradient. When we call minimize, we specify to indicate that the provided function returns both the objective function and its gradient. While convenient, not all functions support this feature, and moreover, it is only for sharing calculations between the function and its gradient, whereas in some problems we will want to share calculations with the Hessian (second derivative of the objective function) and constraints. A more general approach is to memoize the expensive parts of the calculation. In simple situations, this can be accomplished with the wrapper. Newton-Conjugate Gradient algorithm is a modified Newton’s method and uses a conjugate gradient algorithm to (approximately) invert the local Hessian [NW]. Newton’s method is based on fitting the function locally to a quadratic form: where \\(\\mathbf{H}\\left(\\mathbf{x}_{0}\\right)\\) is a matrix of second-derivatives (the Hessian). If the Hessian is positive definite then the local minimum of this function can be found by setting the gradient of the quadratic form to zero, resulting in The inverse of the Hessian is evaluated using the conjugate-gradient method. An example of employing this method to minimizing the Rosenbrock function is given below. To take full advantage of the Newton-CG method, a function which computes the Hessian must be provided. The Hessian matrix itself does not need to be constructed, only a vector which is the product of the Hessian with an arbitrary vector needs to be available to the minimization routine. As a result, the user can provide either a function to compute the Hessian matrix, or a function to compute the product of the Hessian with an arbitrary vector. The Hessian of the Rosenbrock function is if \\(i,j\\in\\left[1,N-2\\right]\\) with \\(i,j\\in\\left[0,N-1\\right]\\) defining the \\(N\\times N\\) matrix. Other non-zero entries of the matrix are For example, the Hessian when \\(N=5\\) is The code which computes this Hessian along with the code to minimize the function using Newton-CG method is shown in the following example: For larger minimization problems, storing the entire Hessian matrix can consume considerable time and memory. The Newton-CG algorithm only needs the product of the Hessian times an arbitrary vector. As a result, the user can supply code to compute this product rather than the full Hessian by giving a function which take the minimization vector as the first argument and the arbitrary vector as the second argument (along with extra arguments passed to the function to be minimized). If possible, using Newton-CG with the Hessian product option is probably the fastest way to minimize the function. In this case, the product of the Rosenbrock Hessian with an arbitrary vector is not difficult to compute. If \\(\\mathbf{p}\\) is the arbitrary vector, then \\(\\mathbf{H}\\left(\\mathbf{x}\\right)\\mathbf{p}\\) has elements: Code which makes use of this Hessian product to minimize the Rosenbrock function using follows: According to [NW] p. 170 the algorithm can be inefficient when the Hessian is ill-conditioned because of the poor quality search directions provided by the method in those situations. The method , according to the authors, deals more effectively with this problematic situation and will be described next. The method is a line search method: it finds a direction of search minimizing a quadratic approximation of the function and then uses a line search algorithm to find the (nearly) optimal step size in that direction. An alternative approach is to, first, fix the step size limit \\(\\Delta\\) and then find the optimal step \\(\\mathbf{p}\\) inside the given trust-radius by solving the following quadratic subproblem: The solution is then updated \\(\\mathbf{x}_{k+1} = \\mathbf{x}_{k} + \\mathbf{p}\\) and the trust-radius \\(\\Delta\\) is adjusted according to the degree of agreement of the quadratic model with the real function. This family of methods is known as trust-region methods. The algorithm is a trust-region method that uses a conjugate gradient algorithm to solve the trust-region subproblem [NW]. Similar to the method, the method is a method suitable for large-scale problems as it uses the hessian only as linear operator by means of matrix-vector products. It solves the quadratic subproblem more accurately than the method. This method wraps the [TRLIB] implementation of the [GLTR] method solving exactly a trust-region subproblem restricted to a truncated Krylov subspace. For indefinite problems it is usually better to use this method as it reduces the number of nonlinear iterations at the expense of few more matrix-vector products per subproblem solve in comparison to the method. F. Lenders, C. Kirches, A. Potschka: “trlib: A vector-free implementation of the GLTR method for iterative solution of the trust region problem”, arXiv:1611.04718 N. Gould, S. Lucidi, M. Roma, P. Toint: “Solving the Trust-Region Subproblem using the Lanczos Method”, SIAM J. Optim., 9(2), 504–525, (1999). DOI:10.1137/S1052623497322735 All methods , and are suitable for dealing with large-scale problems (problems with thousands of variables). That is because the conjugate gradient algorithm approximately solve the trust-region subproblem (or invert the Hessian) by iterations without the explicit Hessian factorization. Since only the product of the Hessian with an arbitrary vector is needed, the algorithm is specially suited for dealing with sparse Hessians, allowing low storage requirements and significant time savings for those sparse problems. For medium-size problems, for which the storage and factorization cost of the Hessian are not critical, it is possible to obtain a solution within fewer iteration by solving the trust-region subproblems almost exactly. To achieve that, a certain nonlinear equations is solved iteratively for each quadratic subproblem [CGT]. This solution requires usually 3 or 4 Cholesky factorizations of the Hessian matrix. As the result, the method converges in fewer number of iterations and takes fewer evaluations of the objective function than the other implemented trust-region methods. The Hessian product option is not supported by this algorithm. An example using the Rosenbrock function follows: The function provides several algorithms for constrained minimization, namely , , , and . They require the constraints to be defined using slightly different structures. The methods and require the constraints to be defined as a sequence of objects and . Methods and , on the other hand, require constraints to be defined as a sequence of dictionaries, with keys , and . As an example let us consider the constrained minimization of the Rosenbrock function: This optimization problem has the unique solution \\([x_0, x_1] = [0.4149,~ 0.1701]\\), for which only the first and fourth constraints are active. The trust-region constrained method deals with constrained minimization problems of the form: When \\(c^l_j = c^u_j\\) the method reads the \\(j\\)-th constraint as an equality constraint and deals with it accordingly. Besides that, one-sided constraint can be specified by setting the upper or lower bound to with the appropriate sign. The implementation is based on [EQSQP] for equality-constraint problems and on [TRIP] for problems with inequality constraints. Both are trust-region type algorithms suitable for large-scale problems. The bound constraints \\(0 \\leq x_0 \\leq 1\\) and \\(-0.5 \\leq x_1 \\leq 2.0\\) are defined using a object. The constraints \\(x_0 + 2 x_1 \\leq 1\\) and \\(2 x_0 + x_1 = 1\\) can be written in the linear constraint standard format: and linear combination of the Hessians: Alternatively, it is also possible to define the Hessian \\(H(x, v)\\) as a sparse matrix, When the evaluation of the Hessian \\(H(x, v)\\) is difficult to implement or computationally infeasible, one may use . Currently available strategies are and . Alternatively, the Hessian may be approximated using finite differences. The Jacobian of the constraints can be approximated by finite differences as well. In this case, however, the Hessian cannot be computed with finite differences and needs to be provided by the user or defined using . Solving the Optimization Problem The optimization problem is solved using: When needed, the objective function Hessian can be defined using a object, or a Hessian-vector product through the parameter . Alternatively, the first and second derivatives of the objective function can be approximated. For instance, the Hessian can be approximated with quasi-Newton approximation and the gradient with finite differences. Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999. An interior point algorithm for large-scale nonlinear programming. SIAM Journal on Optimization 9.4: 877-900. Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the implementation of an algorithm for large-scale equality constrained optimization. SIAM Journal on Optimization 8.3: 682-706. The SLSQP method deals with constrained minimization problems of the form: \\begin{eqnarray*} \\min_x & f(x) \\\\ \\text{subject to: } & c_j(x) = 0 , &j \\in \\mathcal{E}\\\\ & c_j(x) \\geq 0 , &j \\in \\mathcal{I}\\\\ & \\text{lb}_i \\leq x_i \\leq \\text{ub}_i , &i = 1,...,N. \\end{eqnarray*} Where \\(\\mathcal{E}\\) or \\(\\mathcal{I}\\) are sets of indices containing equality and inequality constraints. Both linear and nonlinear constraints are defined as dictionaries with keys , and . And the optimization problem is solved with: Most of the options available for the method are not available for . Find a solver that meets your requirements using the table below. If there are multiple candidates, try several and see which ones best meet your needs (e.g. execution time, objective function value).\n\nGlobal optimization aims to find the global minimum of a function within given bounds, in the presence of potentially many local minima. Typically, global minimizers efficiently search the parameter space, while using a local minimizer (e.g., ) under the hood. SciPy contains a number of good global optimizers. Here, we’ll use those on the same objective function, namely the (aptly named) function: This function looks like an egg carton: We now use the global optimizers to obtain the minimum and the function value at the minimum. We’ll store the results in a dictionary so we can compare different optimization results later. All optimizers return an , which in addition to the solution contains information on the number of function evaluations, whether the optimization was successful, and more. For brevity, we won’t show the full output of the other optimizers: has a second method, which returns all local minima rather than only what it thinks is the global minimum: We’ll now plot all found minima on a heatmap of the function: # SHGO produces multiple minima, plot them all (with a smaller marker size) Find a solver that meets your requirements using the table below. If there are multiple candidates, try several and see which ones best meet your needs (e.g. execution time, objective function value).\n\nHere \\(f_i(\\mathbf{x})\\) are smooth functions from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}\\), we refer to them as residuals. The purpose of a scalar-valued function \\(\\rho(\\cdot)\\) is to reduce the influence of outlier residuals and contribute to robustness of the solution, we refer to it as a loss function. A linear loss function gives a standard least-squares problem. Additionally, constraints in a form of lower and upper bounds on some of \\(x_j\\) are allowed. All methods specific to least-squares minimization utilize a \\(m \\times n\\) matrix of partial derivatives called Jacobian and defined as \\(J_{ij} = \\partial f_i / \\partial x_j\\). It is highly recommended to compute this matrix analytically and pass it to , otherwise, it will be estimated by finite differences, which takes a lot of additional time and can be very inaccurate in hard cases. Function can be used for fitting a function \\(\\varphi(t; \\mathbf{x})\\) to empirical data \\(\\{(t_i, y_i), i = 0, \\ldots, m-1\\}\\). To do this, one should simply precompute residuals as \\(f_i(\\mathbf{x}) = w_i (\\varphi(t_i; \\mathbf{x}) - y_i)\\), where \\(w_i\\) are weights assigned to each observation. Here we consider an enzymatic reaction . There are 11 residuals defined as where \\(y_i\\) are measurement values and \\(u_i\\) are values of the independent variable. The unknown vector of parameters is \\(\\mathbf{x} = (x_0, x_1, x_2, x_3)^T\\). As was said previously, it is recommended to compute Jacobian matrix in a closed form: We are going to use the “hard” starting point defined in . To find a physically meaningful solution, avoid potential division by zero and assure convergence to the global minimum we impose constraints \\(0 \\leq x_j \\leq 100, j = 0, 1, 2, 3\\). The code below implements least-squares estimation of \\(\\mathbf{x}\\) and finally plots the original data and the fitted model function: Three interactive examples below illustrate usage of in greater detail.\n• None Large-scale bundle adjustment in scipy demonstrates large-scale capabilities of and how to efficiently compute finite difference approximation of sparse Jacobian.\n• None Robust nonlinear regression in scipy shows how to handle outliers with a robust loss function in a nonlinear regression.\n• None Solving a discrete boundary-value problem in scipy examines how to solve a large system of equations and use bounds to achieve desired properties of the solution. For the details about mathematical algorithms behind the implementation refer to documentation of .\n\nOften only the minimum of an univariate function (i.e., a function that takes a scalar as input) is needed. In these circumstances, other optimization techniques have been developed that can work faster. These are accessible from the function, which proposes several algorithms. There are, actually, two methods that can be used to minimize an univariate function: and , but is included only for academic purposes and should rarely be used. These can be respectively selected through the method parameter in . The method uses Brent’s algorithm for locating a minimum. Optimally, a bracket (the parameter) should be given which contains the minimum desired. A bracket is a triple \\(\\left( a, b, c \\right)\\) such that \\(f \\left( a \\right) > f \\left( b \\right) < f \\left( c \\right)\\) and \\(a < b < c\\) . If this is not given, then alternatively two starting points can be chosen and a bracket will be found from these points using a simple marching algorithm. If these two starting points are not provided, 0 and 1 will be used (this may not be the right choice for your function and result in an unexpected minimum being returned). Very often, there are constraints that can be placed on the solution space before minimization occurs. The bounded method in is an example of a constrained minimization procedure that provides a rudimentary interval constraint for scalar functions. The interval constraint allows the minimization to occur only between two fixed endpoints, specified using the mandatory bounds parameter. For example, to find the minimum of \\(J_{1}\\left( x \\right)\\) near \\(x=5\\) , can be called using the interval \\(\\left[ 4, 7 \\right]\\) as a constraint. The result is \\(x_{\\textrm{min}}=5.3314\\) :\n\nIf one has a single-variable equation, there are multiple different root finding algorithms that can be tried. Most of these algorithms require the endpoints of an interval in which a root is expected (because the function changes signs). In general, is the best choice, but the other methods may be useful in certain circumstances or for academic purposes. When a bracket is not available, but one or more derivatives are available, then (or , ) may be applicable. This is especially the case if the function is defined on a subset of the complex plane, and the bracketing methods cannot be used. A problem closely related to finding the zeros of a function is the problem of finding a fixed point of a function. A fixed point of a function is the point at which evaluation of the function returns the point: \\(g\\left(x\\right)=x.\\) Clearly, the fixed point of \\(g\\) is the root of \\(f\\left(x\\right)=g\\left(x\\right)-x.\\) Equivalently, the root of \\(f\\) is the fixed point of \\(g\\left(x\\right)=f\\left(x\\right)+x.\\) The routine provides a simple iterative method using Aitkens sequence acceleration to estimate the fixed point of \\(g\\) given a starting point. Finding a root of a set of non-linear equations can be achieved using the function. Several methods are available, amongst which (the default) and , which, respectively, use the hybrid method of Powell and the Levenberg-Marquardt method from MINPACK. The following example considers the single-variable transcendental equation a root of which can be found as follows: Consider now a set of non-linear equations We define the objective function so that it also returns the Jacobian and indicate this by setting the parameter to . Also, the Levenberg-Marquardt solver is used here. Methods and in cannot deal with a very large number of variables (N), as they need to calculate and invert a dense N x N Jacobian matrix on every Newton step. This becomes rather inefficient when N grows. Consider, for instance, the following problem: we need to solve the following integrodifferential equation on the square \\([0,1]\\times[0,1]\\): with the boundary condition \\(P(x,1) = 1\\) on the upper edge and \\(P=0\\) elsewhere on the boundary of the square. This can be done by approximating the continuous function P by its values on a grid, \\(P_{n,m}\\approx{}P(n h, m h)\\), with a small grid spacing h. The derivatives and integrals can then be approximated; for instance \\(\\partial_x^2 P(x,y)\\approx{}(P(x+h,y) - 2 P(x,y) + P(x-h,y))/h^2\\). The problem is then equivalent to finding the root of some function , where is a vector of length \\(N_x N_y\\). Now, because \\(N_x N_y\\) can be large, methods or in will take a long time to solve this problem. The solution can, however, be found using one of the large-scale solvers, for example , , or . These use what is known as the inexact Newton method, which instead of computing the Jacobian matrix exactly, forms an approximation for it. The problem we have can now be solved as follows: When looking for the zero of the functions \\(f_i({\\bf x}) = 0\\), i = 1, 2, …, N, the solver spends most of the time inverting the Jacobian matrix, If you have an approximation for the inverse matrix \\(M\\approx{}J^{-1}\\), you can use it for preconditioning the linear-inversion problem. The idea is that instead of solving \\(J{\\bf s}={\\bf y}\\) one solves \\(MJ{\\bf s}=M{\\bf y}\\): since matrix \\(MJ\\) is “closer” to the identity matrix than \\(J\\) is, the equation should be easier for the Krylov method to deal with. The matrix M can be passed to with method as an option . It can be a (sparse) matrix or a instance. For the problem in the previous section, we note that the function to solve consists of two parts: the first one is the application of the Laplace operator, \\([\\partial_x^2 + \\partial_y^2] P\\), and the second is the integral. We can actually easily compute the Jacobian corresponding to the Laplace operator part: we know that in 1-D so that the whole 2-D operator is represented by The matrix \\(J_2\\) of the Jacobian corresponding to the integral is more difficult to calculate, and since all of it entries are nonzero, it will be difficult to invert. \\(J_1\\) on the other hand is a relatively simple matrix, and can be inverted by (or the inverse can be approximated by ). So we are content to take \\(M\\approx{}J_1^{-1}\\) and hope for the best. In the example below, we use the preconditioner \\(M=J_1^{-1}\\). # Now we have the matrix `J_1`. We need to find its inverse `M` -- # however, since an approximate inverse is enough, we can use # This returns an object with a method .solve() that evaluates # the corresponding matrix-vector product. We need to wrap it into # a LinearOperator before it can be passed to the Krylov methods: and then with preconditioning: Using a preconditioner reduced the number of evaluations of the function by a factor of 4. For problems where the residual is expensive to compute, good preconditioning can be crucial — it can even decide whether the problem is solvable in practice or not. Preconditioning is an art, science, and industry. Here, we were lucky in making a simple choice that worked reasonably well, but there is a lot more depth to this topic than is shown here.\n\nThe function can minimize a linear objective function subject to linear equality and inequality constraints. This kind of problem is well known as linear programming. Linear programming solves problems of the following form: where \\(x\\) is a vector of decision variables; \\(c\\), \\(b_{ub}\\), \\(b_{eq}\\), \\(l\\), and \\(u\\) are vectors; and \\(A_{ub}\\) and \\(A_{eq}\\) are matrices. In this tutorial, we will try to solve a typical linear programming problem using . Consider the following simple linear programming problem: We need some mathematical manipulations to convert the target problem to the form accepted by . First of all, let’s consider the objective function. We want to maximize the objective function, but can only accept a minimization problem. This is easily remedied by converting the maximize \\(29x_1 + 45x_2\\) to minimizing \\(-29x_1 -45x_2\\). Also, \\(x_3, x_4\\) are not shown in the objective function. That means the weights corresponding with \\(x_3, x_4\\) are zero. So, the objective function can be converted to: If we define the vector of decision variables \\(x = [x_1, x_2, x_3, x_4]^T\\), the objective weights vector \\(c\\) of in this problem should be Next, let’s consider the two inequality constraints. The first one is a “less than” inequality, so it is already in the form accepted by . The second one is a “greater than” inequality, so we need to multiply both sides by \\(-1\\) to convert it to a “less than” inequality. Explicitly showing zero coefficients, we have: These equations can be converted to matrix form: Next, let’s consider the two equality constraints. Showing zero weights explicitly, these are: These equations can be converted to matrix form: Lastly, let’s consider the separate inequality constraints on individual decision variables, which are known as “box constraints” or “simple bounds”. These constraints can be applied using the bounds argument of . As noted in the documentation, the default value of bounds is , meaning that the lower bound on each decision variable is 0, and the upper bound on each decision variable is infinity: all the decision variables are non-negative. Our bounds are different, so we will need to specify the lower and upper bound on each decision variable as a tuple and group these tuples into a list. Finally, we can solve the transformed problem using . # +/- np.inf can be used instead of None The problem is infeasible. (HiGHS Status 8: model_status is Infeasible; primal_status is None) The result states that our problem is infeasible, meaning that there is no solution vector that satisfies all the constraints. That doesn’t necessarily mean we did anything wrong; some problems truly are infeasible. Suppose, however, that we were to decide that our bound constraint on \\(x_1\\) was too tight and that it could be loosened to \\(0 \\leq x_1 \\leq 6\\). After adjusting our code to reflect the change and executing it again: The result shows the optimization was successful. We can check the objective value ( ) is same as \\(c^Tx\\): We can also check that all constraints are satisfied within reasonable tolerances: # this is equivalent to result.slack # this is equivalent to result.con\n\nThe knapsack problem is a well known combinatorial optimization problem. Given a set of items, each with a size and a value, the problem is to choose the items that maximize the total value under the condition that the total size is below a certain threshold.\n• None \\(x_i\\) be a boolean variable that indicates whether item \\(i\\) is included in the knapsack,\n• None \\(n\\) be the total number of items,\n• None \\(v_i\\) be the value of item \\(i\\),\n• None \\(s_i\\) be the size of item \\(i\\), and\n• None \\(C\\) be the capacity of the knapsack. \\[\\text{subject to} \\sum_i^n s_{i} x_{i} \\leq C, x_{i} \\in {0, 1}\\] Although the objective function and inequality constraints are linear in the decision variables \\(x_i\\), this differs from a typical linear programming problem in that the decision variables can only assume integer values. Specifically, our decision variables can only be \\(0\\) or \\(1\\), so this is known as a binary integer linear program (BILP). Such a problem falls within the larger class of mixed integer linear programs (MILPs), which we we can solve with . In our example, there are 8 items to choose from, and the size and value of each is specified as follows. We need to constrain our eight decision variables to be binary. We do so by adding a : constraint to ensure that they lie between \\(0\\) and \\(1\\), and we apply “integrality” constraints to ensure that they are either \\(0\\) or \\(1\\). The knapsack capacity constraint is specified using . If we are following the usual rules of linear algebra, the input should be a two-dimensional matrix, and the lower and upper bounds and should be one-dimensional vectors, but is forgiving as long as the inputs can be broadcast to consistent shapes. Using the variables defined above, we can solve the knapsack problem using . Note that minimizes the objective function, but we want to maximize the total value, so we set c to be negative of the values. This means that we should select the items 1, 2, 4, 5, 6 to optimize the total value under the size constraint. Note that this is different from we would have obtained had we solved the linear programming relaxation (without integrality constraints) and attempted to round the decision variables. If we were to round this solution up to , our knapsack would be over the capacity constraint, whereas if we were to round down to , we would have a sub-optimal solution. For more MILP tutorials, see the Jupyter notebooks on SciPy Cookbooks:"
    },
    {
        "link": "https://github.com/pv/scipy-work/blob/master/scipy/optimize/_minimize.py",
        "document": ""
    },
    {
        "link": "https://people.duke.edu/~ccc14/sta-663-2017/14C_Optimization_In_Python.html",
        "document": "Using optimization routines from and ¶\n\nWe will assume that our optimization problem is to minimize some univariate or multivariate function \\(f(x)\\). This is without loss of generality, since to find the maximum, we can simply minimize \\(-f(x)\\). We will also assume that we are dealing with multivariate or real-valued smooth functions - non-smooth or discrete functions (e.g. integer-valued) are outside the scope of this course. To find the minimum of a function, we first need to be able to express the function as a mathematical expresssion. For example, in least squares regression, the function that we are optimizing is of the form \\(y_i - f(x_i, \\theta)\\) for some parameter(s) \\(\\theta\\). To choose an appropirate optimization algorithm, we should at least answer these two questions if possible:\n• Are there any constraints that the solution must meet? Finally, we need to realize that optimization methods are nearly always designed to find local optima. For convex problems, there is only one minimum and so this is not a problem. However, if there are multiple local minima, often heuristics such as multiple random starts must be adopted to find a “good” enough solution. Convex functions are very nice because they have a single global minimum, and there are very efficient algorithms for solving large convex systems. Intuitively, a function is convex if every chord joining two points on the function lies above the function. More formally, a function is convex if for some \\(t\\) between 0 and 1 - this is shown in the figure below. Checking if a function is convex using the Hessian¶ The formal definition is only useful for checking if a function is convex if you can find a counter-example. More practically, a twice differentiable function is convex if its Hessian is positive semi-definite, and strictly convex if the Hessian is positive definite. For example, suppose we want to minimize the scalar-valued function Since the matrix is symmetric and all eigenvalues are positive, the Hessian is positive defintie and the function is convex. The following rules may be useful to determine if more complex functions are convex:\n• The intersection of convex functions is convex\n• If the functions and are convex and and then the function is convex.\n• If the function is convex and the function is nondecreasing and convex then the function defined by is convex. Many more technical details about convexity and convex optimization can be found in this book. Are there any constraints that the solution must meet?¶ In general, optimization without constraints is easier to perform than optimization in the presence of constraints. The solutions may be very different in the presence or absence of constraints, so it is important to know if there are any constraints. We will see some examples of two general strategies: - convert a problem with constraints into one without constraints or - use an algorithm that can optimize with constraints.\n\nOne of the most convenient libraries to use is , since it is already part of the Anaconda installation and it has a fairly intuitive interface. The <http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar>`__ function will find the minimum, and can also be told to search within given bounds. By default, it uses the Brent algorithm, which combines a bracketing strategy with a parabolic approximation. # note how additional function arguments are passed in We can try multiple random starts to find the global minimum¶ See documentation for the <http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.basinhopping.html>`__ algorithm, which also works with multivariate scalar optimization. Note that this is heuristic and not guaranteed to find a global minimum. We will next move on to optimization of multivariate scalar functions, where the scalar may (say) be the norm of a vector. Minimizing a multivariable set of equations \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) is not well-defined, but we will later see how to solve the closely related problem of finding roots or fixed points of such a set of equations. We will use the Rosenbrock “banana” function to illustrate unconstrained multivariate optimization. In 2D, this is The function has a global minimum at (1,1) and the standard expression takes \\(a = 1\\) and \\(b = 100\\). With these values for \\(a\\) and \\(b\\), the problem is ill-conditioned. As we shall see, one of the factors affecting the ease of optimization is the condition number of the curvature (Hessian). When the condition number is high, the gradient may not point in the direction of the minimum, and simple gradient descent methods may be inefficient since they may be forced to take many sharp turns. As pointed out in the previous lecture, the condition number is basically the ratio of largest to smallest eigenvalue of the Hessian¶ Why is the condition number so large?¶ # Note: the global minimum is at (1,1) in a tiny contour island Zooming in to the global minimum at (1,1)¶\n\nThe gradient (or Jacobian) at a point indicates the direction of steepest ascent. Since we are looking for a minimum, one obvious possibility is to take a step in the opposite direction to the gradient. We weight the size of the step by a factor \\(\\alpha\\) known in the machine learning literature as the learning rate. If \\(\\alpha\\) is small, the algorithm will eventually converge towards a local minimum, but it may take long time. If \\(\\alpha\\) is large, the algorithm may converge faster, but it may also overshoot and never find the minimum. Gradient descent is also known as a first order method because it requires calculation of the first derivative at each iteration. Some algorithms also determine the appropriate value of \\(\\alpha\\) at each stage by using a line search, i.e., As suggested above, the problem is that the gradient may not point towards the global minimum especially when the condition number is large, and we are forced to use a small \\(\\alpha\\) for convergence. Let’s warm up by minimizing a trivial function \\(f(x, y) = x^2 + y^2\\) to illustrate the basic idea of gradient descent. Usually, when we optimize, we are not just finding the minimum, but also want to know the parameters that give us the minimum. As a simple example, suppose we want to find parameters that minimize the least squares difference between a linear model and some data. Suppose we have some data \\((0,1), (1,2), (2,3), (3,3.5), (4,6), (5,9), (6,8)\\) and want to find a line \\(y = \\beta_0 +\\beta_1 x\\) that is the best least squares fit. One way to do this is to solve \\(X^TX\\hat{\\beta} = X^Ty\\), but we want to show how this can be formulated as a gradient descent problem. We want to find \\(\\beta = (\\beta_0, \\beta_1)\\) that minimize the squared differences We calculate the gradient with respect to \\(\\beta\\) as \"\"\"Gradient of objective function with respect to parameters b.\"\"\" Gradient descent to minimize the Rosen function using ¶ Because gradient descent is unreliable in practice, it is not part of the optimize suite of functions, but we will write a custom function below to illustrate how to use gradient descent while maintaining the interface. Warning One of the most common causes of failure of optimization is because the gradient or Hessian function is specified incorrectly. You can check for this using which compares the analytical gradient with one calculated using finite differences. # the next 2 lines are gradient descent When we are looking for a minimum, we are looking for the roots of the derivative \\(f'(x)\\), so Newton’s method can also be seen as a Taylor series approximation At the function minimum, the derivative is 0, so and letting \\(\\Delta x = \\frac{h}{2}\\), we get that the Newton step is The multivariate analog replaces \\(f'\\) with the Jacobian and \\(f''\\) with the Hessian, so the Newton step is Second order methods solve for \\(H^{-1}\\) and so require calculation of the Hessian (either provided or approximated using finite differences). For efficiency reasons, the Hessian is not directly inverted, but solved for using a variety of methods such as conjugate gradient. An example of a second order method in the package is . As calculating the Hessian is computationally expensive, sometimes first order methods that only use the first derivatives are preferred. Quasi-Newton methods use functions of the first derivatives to approximate the inverse Hessian. A well know example of the Quasi-Newoton class of algorithjms is BFGS, named after the initials of the creators. As usual, the first derivatives can either be provided via the argument or approximated by finite difference methods. Finally, there are some optimization algorithms not based on the Newton method, but on other heuristic search strategies that do not require any derivatives, only function evaluations. One well-known example is the Nelder-Mead simplex algorithm. Recall why Lagrange multipliers are useful for constrained optimization - a stationary point must be where the constraint surface \\(g\\) touches a level set of the function \\(f\\) (since the value of \\(f\\) does not change on a level set). At that point, \\(f\\) and \\(g\\) are parallel, and hence their gradients are also parallel (since the gradient is normal to the level set). So we want to solve Numerical example of using Lagrange multipliers¶ Maximize \\(f (x, y, z) = xy + yz\\) subject to the constraints \\(x + 2y = 6\\) and \\(x − 3z = 0\\). Now set partial derivatives to zero and solve the following set of equations which is a linear equation in \\(x, y, z, \\lambda, \\mu\\) Another example of constrained optimization¶ Many real-world optimization problems have constraints - for example, a set of parameters may have to sum to 1.0 (equality constraint), or some parameters may have to be non-negative (inequality constraint). Sometimes, the constraints can be incorporated into the function to be minimized, for example, the non-negativity constraint \\(p \\gt 0\\) can be removed by substituting \\(p = e^q\\) and optimizing for \\(q\\). Using such workarounds, it may be possible to convert a constrained optimization problem into an unconstrained one, and use the methods discussed above to solve the problem. Alternatively, we can use optimization methods that allow the specification of constraints directly in the problem statement as shown in this section. Internally, constraint violation penalties, barriers and Lagrange multipliers are some of the methods used used to handle these constraints. We use the example provided in the Scipy tutorial to illustrate how to set constraints. To set constraints, we pass in a dictionary with keys , and . Note that the inequality constraint assumes a \\(C_j x \\ge 0\\) form. As usual, the is optional and will be numerically estimated if not provided. Sometimes, we simply want to use non-linear least squares to fit a function to data, perhaps to estimate parameters for a mechanistic or phenomenological model. The function uses the quasi-Newton Levenberg-Marquadt algorithm to perform such fits. Behind the scenes, is just a wrapper around the function that does nonlinear least squares fitting. \"\"\"The four paramter logistic function is often used to fit dose-response relationships.\"\"\" This is a specialized application of , in which the curve to be fitted is defined implicitly by an ordinary differential equation and we want to use observed data to estimate the parameters \\(k\\) and the initial value \\(x_0\\). Of course this can be explicitly solved but the same approach can be used to find multiple parameters for \\(n\\)-dimensional systems of ODEs. A more elaborate example for fitting a system of ODEs to model the zombie apocalypse Solution to the ODE x'(t) = f(t,x,k) with initial condition x(0) = x0 # Some random data genererated from closed form solution plus Gaussian noise Another example of fitting a system of ODEs using the package¶ You may have to install the <http://cars9.uchicago.edu/software/python/lmfit/index.html>`__ package using and restart your kernel. The algorithm is another wrapper around but allows for richer model specification and more diagnostics. Requirement already satisfied (use --upgrade to upgrade): lmfit in /opt/conda/lib/python3.5/site-packages Requirement already satisfied (use --upgrade to upgrade): scipy in /opt/conda/lib/python3.5/site-packages (from lmfit) Requirement already satisfied (use --upgrade to upgrade): numpy in /opt/conda/lib/python3.5/site-packages (from lmfit) [33mYou are using pip version 8.1.2, however version 9.0.1 is available. You should consider upgrading via the 'pip install --upgrade pip' command.[0m Solution to the ODE x'(t) = f(t,x,k) with initial condition x(0) = x0 To show the many different applications of optimization, here is an example using optimization to change the layout of nodes of a graph. We use a physical analogy - nodes are connected by springs, and the springs resist deformation from their natural length \\(l_{ij}\\). Some nodes are pinned to their initial locations while others are free to move. Because the initial configuration of nodes does not have springs at their natural length, there is tension resulting in a high potential energy \\(U\\), given by the physics formula shown below. Optimization finds the configuration of lowest potential energy given that some nodes are fixed (set up as boundary constraints on the positions of the nodes). Note that the ordination algorithm Multi-Dimensional Scaling (MDS) works on a very similar idea - take a high dimensional data set in \\(\\mathbb{R}^n\\), and project down to a lower dimension (\\(\\mathbb{R}^k\\)) such that the sum of distances \\(d_n(x_i, x_j) - d_k(x_i, x_j)\\), where \\(d_n\\) and \\(d_k\\) are some measure of distance between two points \\(x_i\\) and \\(x_j\\) in \\(n\\) and \\(d\\) dimension respectively, is minimized. MDS is often used in exploratory analysis of high-dimensional data to get some intuitive understanding of its “structure”.\n• P0 is the initial location of nodes\n• P is the minimal energy location of nodes given constraints\n• A is a connectivity matrix - there is a spring between and if\n• is the resting length of the spring connecting and\n• In addition, there are a number of nodes whose positions are pinned. # fix the position of the first few nodes just to show constraints\n\nWhen we solve standard statistical problems, an optimization procedure similar to the ones discussed here is performed. For example, consider multivariate logistic regression - typically, a Newton-like algorithm known as iteratively reweighted least squares (IRLS) is used to find the maximum likelihood estimate for the generalized linear model family. However, using one of the multivariate scalar minimization methods shown above will also work, for example, the BFGS minimization algorithm. The take home message is that there is nothing magic going on when Python or R fits a statistical model using a formula - all that is happening is that the objective function is set to be the negative of the log likelihood, and the minimum found using some first or second order optimization algorithm. Suppose we have a binary outcome measure \\(Y \\in {0,1}\\) that is conditinal on some input variable (vector) \\(x \\in (-\\infty, +\\infty)\\). Let the conditioanl probability be \\(p(x) = P(Y=y | X=x)\\). Given some data, one simple probability model is \\(p(x) = \\beta_0 + x\\cdot\\beta\\) - i.e. linear regression. This doesn’t really work for the obvious reason that \\(p(x)\\) must be between 0 and 1 as \\(x\\) ranges across the real line. One simple way to fix this is to use the transformation \\(g(x) = \\frac{p(x)}{1 - p(x)} = \\beta_0 + x.\\beta\\). Solving for \\(p\\), we get As you all know very well, this is logistic regression. Suppose we have \\(n\\) data points \\((x_i, y_i)\\) where \\(x_i\\) is a vector of features and \\(y_i\\) is an observed class (0 or 1). For each event, we either have “success” (\\(y = 1\\)) or “failure” (\\(Y = 0\\)), so the likelihood looks like the product of Bernoulli random variables. According to the logistic model, the probability of success is \\(p(x_i)\\) if \\(y_i = 1\\) and \\(1-p(x_i)\\) if \\(y_i = 0\\). So the likelihood is and the log-likelihood is Using the standard ‘trick’, if we augment the matrix \\(X\\) with a column of 1s, we can write \\(\\beta_0 + x_i\\cdot\\beta\\) as just \\(X\\beta\\). # We will ignore the rank categorical value This is very similar to what you would do in R, only using Python’s package. The GLM solver uses a special variant of Newton’s method known as iteratively reweighted least squares (IRLS), which will be further desribed in the lecture on multivarite and constrained optimizaiton. Call: glm(formula = admit ~ gre + gpa, family = \"binomial\", data = df) Deviance Residuals: Min 1Q Median 3Q Max -1.2730 -0.8988 -0.7206 1.3013 2.0620 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -4.949378 1.075093 -4.604 4.15e-06 * gre 0.002691 0.001057 2.544 0.0109 * gpa 0.754687 0.319586 2.361 0.0182 * --- Signif. codes: 0 ‘*’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 499.98 on 399 degrees of freedom Residual deviance: 480.34 on 397 degrees of freedom AIC: 486.34 Number of Fisher Scoring iterations: 4 This is to show that there is no magic going on - you can write the function to minimize directly from the log-likelihood equation and run a minimizer. It will be more accurate if you also provide the derivative (+/- the Hessian for second order methods), but using just the function and numerical approximations to the derivative will also work. As usual, this is for illustration so you understand what is going on - when there is a library function available, you should probably use that instead. There are also many optimization routines in the package, as you already know from the previous lectures. Many machine learning problems essentially boil down to the minimization of some appropriate loss function."
    },
    {
        "link": "https://stackoverflow.com/questions/31289023/scipy-optimize-a-function-with-argument-dependent-constraints",
        "document": "I am trying to use negative of to maximize a function . is a of guess variables.\n\nI am trying to put some bounds on each . And also a constraint on each such that ( being the other argument to the subject function ).\n\nMy problem is how do I define this constraint as an argument to the maximize function.\n\nI could not find any function in the library so we're using the negative of with documentation over here.\n\nPlease consider asking for clarifications if the question is not clear enough."
    },
    {
        "link": "https://numpy.org/doc/2.1/reference/generated/numpy.linalg.norm.html",
        "document": "This function is able to return one of eight different matrix norms, or one of an infinite number of vector norms (described below), depending on the value of the parameter.\n\nInput array. If axis is None, x must be 1-D or 2-D, unless ord is None. If both axis and ord are None, the 2-norm of will be returned. Order of the norm (see table under ). inf means numpy’s object. The default is None. If axis is an integer, it specifies the axis of x along which to compute the vector norms. If axis is a 2-tuple, it specifies the axes that hold 2-D matrices, and the matrix norms of these matrices are computed. If axis is None then either a vector norm (when x is 1-D) or a matrix norm (when x is 2-D) is returned. The default is None. If this is set to True, the axes which are normed over are left in the result as dimensions with size one. With this option the result will broadcast correctly against the original x. Norm of the matrix or vector(s).\n\nFor values of , the result is, strictly speaking, not a mathematical ‘norm’, but it may still be useful for various numerical purposes.\n\nThe following norms can be calculated:\n\nThe Frobenius norm is given by [1]:\n\nThe nuclear norm is the sum of the singular values.\n\nBoth the Frobenius and nuclear norm orders are only defined for matrices and raise a ValueError when .\n\nUsing the axis argument to compute vector norms:\n\nUsing the axis argument to compute matrix norms:"
    },
    {
        "link": "https://analyticsvidhya.com/blog/2024/01/exploring-the-power-of-norms-with-numpy-linalg",
        "document": "Exploring the Power of Norms with NumPy Linalg\n\nLinear algebra, a foundational concept in mathematics, is a powerful tool with applications spanning various domains, including data science, machine learning, and computer graphics. At the core of linear algebra lies the concept of norms—mathematical functions that quantify the “size” or “magnitude” of vectors and matrices. This article explores the significance of norms in linear algebra and delves into the efficient calculation of vector and matrix norms using NumPy’s linalg.norm function. Our journey will be guided by the robust capabilities of NumPy, a widely-used library for numerical computing in Python, and its dedicated tool, NumPy Linalg Norm.\n\nNorms, fundamental mathematical functions, are indispensable in linear algebra for quantifying the “size” or “magnitude” of vectors and matrices. To grasp these concepts more concretely, let’s explore a simple example.\n\nIn the context of vectors, norms provide a quantitative measure of their length or magnitude. For instance, consider the vector v=[3,4]. The Euclidean norm (L norm) of v is calculated as:\n\nThis calculation illustrates how the Euclidean norm captures the “length” of the vector by summing the squares of its elements and taking the square root.\n\nWhen dealing with matrices, norms extend the concept of “size.” For a 2×2 matrix A= [[1, 2], [3, 4]], the Frobenius norm is computed as:\n\nThis example illustrates how the Frobenius norm measures the “size” of a matrix by considering the sum of squared elements.\n\nThese examples demonstrate that Norms provide a versatile means to quantify the essential characteristics of vectors and matrices, forming the basis for various linear algebra operations.\n\nNorms play a crucial role in linear algebra for several reasons. Firstly, norms provide a way to compare and contrast the “size” or “magnitude” of vectors and matrices. This comparison is often essential in various applications, such as determining the similarity between vectors or measuring the error in a numerical approximation.\n\nSecondly, norms enable us to define convergence and continuity in vector and matrix spaces. By quantifying the “size” of vectors and matrices, we can establish criteria for convergence and continuity, which are fundamental concepts in mathematical analysis.\n\nLastly, norms are used extensively in optimization problems. Many optimization algorithms rely on norms to measure the objective function’s gradient or define the problem’s constraints. By leveraging norms, we can efficiently solve optimization problems in various domains.\n\nNumPy, a popular library for numerical computing in Python, provides a comprehensive set of functions for linear algebra operations. One of the essential functions in NumPy’s linear algebra module, linalg, is the norm function. The linalg.norm function allows us to calculate vector and matrix norms efficiently.\n\nDifferent Types of Norms in NumPy\n\nNumPy supports various norms, each with its characteristics and applications. Let’s explore some of the most commonly used norms in NumPy:\n\nThe Euclidean norm, or the L2 norm, is perhaps the most well-known norm. It measures a vector’s “length” or “magnitude” using the square root of the sum of squared elements. The Euclidean norm is defined as:\n\nThe Manhattan norm, the L1 norm, calculates a vector’s “length” or “magnitude” by summing its elements’ absolute values. The Manhattan norm is defined as:\n\nThe maximum norm, or the L-infinity norm, determines a vector’s “length” or “magnitude” by taking its elements’ maximum absolute value. The maximum norm is defined as:\n\nThe Frobenius norm is a matrix norm that measures a matrix’s “size” or “magnitude”. It is defined as the square root of the sum of squared elements of the matrix. The Frobenius norm is defined as:\n\nIn addition to the abovementioned norms, NumPy supports other norms, such as the Lp norm, which generalizes the Euclidean and Manhattan norms. The Lp norm is defined as:\n\nNumPy’s linalg.norm function provides a convenient way to calculate vector norms. Let’s explore the syntax and parameters of the linalg.norm function and see some examples of vector norm calculations.\n\nThe syntax of the linalg.norm function is as follows:\n\nThe parameters of the linalg.norm function are as follows:\n• ord: The order of the norm to be calculated. If not specified, the default is the Euclidean norm (ord=2).\n• axis: The axis along which the norm is calculated. If not specified, the norm is calculated over the entire array.\n\nYou can use the numpy.linalg.norm function to calculate different types of norms for vectors and matrices:\n\nLet’s consider a few examples to illustrate how to calculate vector norms using NumPy’s linalg.norm function.\n\nIn this example, we calculate the Euclidean norm of a 2-dimensional vector [3, 4]. The Euclidean norm is calculated as the square root of the sum of squared elements, which in this case is 5.0.\n\nIn this example, we calculate the Manhattan norm of the same 2-dimensional vector [3, 4]. The Manhattan norm is calculated as the sum of absolute values, which in this case is 7.0.\n\nIn this example, we calculate the maximum norm of the same 2-dimensional vector [3, 4]. The maximum norm is calculated as the maximum absolute value, which in this case is 4.0.\n\nNumPy’s linalg.norm function can also efficiently calculate matrix norms. Let’s explore the syntax and parameters of the linalg.norm function for matrix norm calculations and see some examples.\n\nThe syntax of the linalg.norm function for matrix norm calculations is as follows:\n\nThe parameters of the linalg.norm function for matrix norm calculations are the same as those for vector norm calculations.\n\nLet’s consider a few examples to illustrate how to calculate matrix norms using NumPy’s linalg.norm function.\n\nIn this example, we calculate the Frobenius norm of a 2×2 matrix [[1, 2], [3, 4]]. The Frobenius norm is calculated as the square root of the sum of squared elements, which in this case is approximately 5.477.\n\nIn this example, we calculate the maximum norm of the same 2×2 matrix [[1, 2], [3, 4]]. The maximum norm is calculated as the maximum absolute value, which in this case is 7.0.\n\nChoosing the right norm for a specific task is crucial, as different norms capture different aspects of vectors and matrices. Let’s explore the importance of choosing the right norm and discuss some applications of different norms in data science.\n\nChoosing the Right Norm for the Task\n\nWhen choosing a norm, it is essential to consider the properties of the norm and the specific requirements of the task at hand. For example, the Euclidean norm is often suitable for measuring distances or determining similarity between vectors. On the other hand, the Manhattan norm is useful when dealing with sparse data or when the “length” or “magnitude” of individual elements is more important than their relative positions.\n\nApplications of Different Norms in Data Science\n\nDifferent norms find applications in various data science tasks. For instance, the Euclidean norm is commonly used in clustering algorithms, such as k-means, to measure the distance between data points. The Manhattan norm is often employed in feature selection or regularization techniques where sparsity is desired. The maximum norm is useful in robust statistics, where outliers must be identified and handled appropriately. The Frobenius norm is frequently used in matrix factorization and low-rank approximation problems.\n\nEfficient computation of norms becomes paramount, especially when dealing with large-scale data. NumPy’s linalg.norm function incorporates highly optimized algorithms to ensure swift calculations. Let’s explore some performance considerations and optimization techniques that can significantly enhance the efficiency of norm calculations.\n\nEfficient Computation of Norms: NumPy’s linalg.norm leverages optimized algorithms that make the most of underlying hardware capabilities, such as vectorization and parallelization. This ensures that norm calculations are executed with optimal efficiency.\n\nVectorization and Broadcasting for Improved Performance: NumPy’s vectorization and broadcasting capabilities play a crucial role in boosting the performance of norm calculations. By applying operations to entire arrays instead of individual elements, NumPy takes advantage of optimized low-level routines, reducing the overhead of Python loops. While these techniques enhance performance, it’s essential to be mindful of potential trade-offs regarding memory usage.\n\nHandling Large-scale Data with NumPy Linalg Norm: Memory consumption becomes critical when working with large-scale data. The linalg.norm function in NumPy supports the axis parameter, allowing users to calculate norms along specific axes of multi-dimensional arrays. By carefully specifying the appropriate axis, unnecessary memory allocations can be avoided, contributing to efficient norm calculations.\n\nBalancing the need for speed with considerations like memory usage is crucial when optimizing norm calculations. NumPy’s inherent efficiency combined with these optimization techniques ensures that norm computations are fast and mindful of resource constraints. This makes them suitable for a wide range of applications, including those involving extensive datasets.\n\nWhile calculating norms, it is essential to be aware of common mistakes and pitfalls that can lead to incorrect results. Let’s discuss some of these mistakes and how to avoid them.\n\nOne common mistake is misinterpreting the results of norm calculations. Norms provide a measure of “size” or “magnitude” and should not be confused with other concepts, such as distances or angles. It is crucial to understand the properties and limitations of the chosen norm to interpret the results correctly.\n\nAnother common mistake is using incorrect parameters when calculating norms. For example, specifying the wrong order (ord) or axis can lead to incorrect results. It is essential to consult the documentation and understand the parameters’ meanings and effects on the norm calculations.\n\nNorm calculations can be challenging when dealing with singular matrices or zero vectors. Singular matrices have a zero determinant and can lead to undefined or infinite norms. Similarly, zero vectors can result in zero norms. Handling these special cases appropriately is crucial to avoid errors or incorrect results.\n\nBest Practices for Working with NumPy Linalg Norm\n\nTo ensure accurate and efficient norm calculations, it is essential to follow best practices when working with NumPy’s linalg.norm function. Let’s discuss some of these best practices.\n\nWriting clean and readable code is crucial for maintaining code quality and facilitating collaboration. When calculating norms, it is essential to use meaningful variable names, provide comments where necessary, and follow consistent coding conventions. This practice improves code readability and makes it easier to understand and maintain.\n\nTesting and validating norm calculations are essential to ensure the correctness of the implemented algorithms. By comparing the results with known values or using analytical solutions, we can verify the accuracy of the norm calculations. Additionally, unit tests can be written to cover different scenarios and edge cases, ensuring the robustness of the code.\n\nNumPy provides comprehensive documentation covering its functions’ usage and behavior, including linalg.norm. It is essential to consult the documentation to understand the available options, parameters, and their effects. The NumPy community is also active and supportive, providing forums and resources for seeking help and sharing knowledge.\n\nNorms are fundamental concepts in linear algebra that allow us to measure the “size” or “magnitude” of vectors and matrices. NumPy’s linalg.norm function provides a powerful tool for efficiently calculating vector and matrix norms. By understanding the different types of norms, their applications, and the optimization techniques available in NumPy, we can leverage norms effectively in various data science and mathematical tasks. Following best practices and avoiding common mistakes ensure accurate and efficient norm calculations.\n\nUnlock the future of technology with our Certified AI & ML BlackBelt Plus Program! Power ahead in your AI & ML career with exclusive benefits:\n\nDon’t just learn, thrive in the world of AI & ML. Enroll now and take the next step toward shaping the future!"
    },
    {
        "link": "https://numpy.org/devdocs/reference/generated/numpy.linalg.norm.html",
        "document": "This function is able to return one of eight different matrix norms, or one of an infinite number of vector norms (described below), depending on the value of the parameter.\n\nInput array. If axis is None, x must be 1-D or 2-D, unless ord is None. If both axis and ord are None, the 2-norm of will be returned. Order of the norm (see table under for what values are supported for matrices and vectors respectively). inf means numpy’s object. The default is None. If axis is an integer, it specifies the axis of x along which to compute the vector norms. If axis is a 2-tuple, it specifies the axes that hold 2-D matrices, and the matrix norms of these matrices are computed. If axis is None then either a vector norm (when x is 1-D) or a matrix norm (when x is 2-D) is returned. The default is None. If this is set to True, the axes which are normed over are left in the result as dimensions with size one. With this option the result will broadcast correctly against the original x. Norm of the matrix or vector(s).\n\nFor values of , the result is, strictly speaking, not a mathematical ‘norm’, but it may still be useful for various numerical purposes.\n\nThe following norms can be calculated:\n\nThe Frobenius norm is given by [1]:\n\nThe nuclear norm is the sum of the singular values.\n\nBoth the Frobenius and nuclear norm orders are only defined for matrices and raise a ValueError when ."
    },
    {
        "link": "https://stackoverflow.com/questions/16763420/trying-to-understand-numpys-linalg-norm-method",
        "document": "The documentation is clear on the matter. You are passing None for the ord parameter to linalg.norm() so you get the Frobenius norm.\n\nThe code appears to be normalising the input, by dividing by the norm. Then it seems makes a poor attempt to scale to have 8 bit color values. But the code scales to the range 0 to 256 instead of 0 to 255.\n\nHowever, the first step seems pointless to me. The code could simply read:\n\nBut perhaps it should be 255 instead of 256.\n\nSince we've got not context here, I'm reluctant to state that the code is wrong. Only you are in a position to decide that because only you know the context."
    },
    {
        "link": "https://stackoverflow.com/questions/50849789/what-does-the-numpy-linalg-norm-function",
        "document": "I am not a mathematician but here is my layman's explanation of “norm”:\n\nA vector describes the location of a point in space relative to the origin. Here’s an example in 2D space for the point [3 2]:\n\nThe norm is the distance from the origin to the point. In the 2D case it’s easy to visualize the point as the diametrically opposed point of a right triangle and see that the norm is the same thing as the hypotenuse.\n\nHowever, In higher dimensions it’s no longer a shape we describe in average-person language, but the distance from the origin to the point is still called the norm. Here's an example in 3D space:\n\nI don’t know why the norm is used in K-means clustering. You stated that it was part of determing the distance between the old and new centroid in each step. Not sure why one would use the norm for this since you can get the distance between two points in any dimensionality* using an extension of the from used in 2D algebra:\n\nYou just add a term for each addtional dimension, for example here is a 3D version:\n\n*where the dimensions are positive integers"
    }
]