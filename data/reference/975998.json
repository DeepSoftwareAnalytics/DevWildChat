[
    {
        "link": "https://gnu.org/s/gawk/manual/gawk.html",
        "document": ""
    },
    {
        "link": "https://geeksforgeeks.org/awk-command-unixlinux-examples",
        "document": "Awk is a scripting language used for manipulating data and generating reports. The awk command programming language requires no compiling and allows the user to use variables, numeric functions, string functions, and logical operators.\n\nAwk is a utility that enables a programmer to write tiny but effective programs in the form of statements that define text patterns that are to be searched for in each line of a document and the action that is to be taken when a match is found within a line. Awk is mostly used for pattern scanning and processing. It searches one or more files to see if they contain lines that matches with the specified patterns and then perform the associated actions.\n\nAwk is abbreviated from the names of the developers – Aho, Weinberger, and Kernighan.\n\nWHAT CAN WE DO WITH AWK?\n\n1. AWK Operations: \n\n(a) Scans a file line by line \n\n(b) Splits each input line into fields \n\n(c) Compares input line/fields to pattern \n\n(d) Performs action(s) on matched lines\n\nConsider the following text file as the input file for all cases below:\n\n1. Default behavior of Awk: By default Awk prints every line of data from the specified file.\n\nIn the above example, no pattern is given. So the actions are applicable to all the lines. Action print without any argument prints the whole line by default, so it prints all the lines of the file without failure.\n\n2. Print the lines which match the given pattern.\n\nIn the above example, the awk command prints all the line which matches with the ‘manager’.\n\n3. Splitting a Line Into Fields : For each record i.e line, the awk command splits the record delimited by whitespace character by default and stores it in the $n variables. If the line has 4 words, it will be stored in $1, $2, $3 and $4 respectively. Also, $0 represents the whole line.\n\nIn the above example, $1 and $4 represents Name and Salary fields respectively.\n\nAwk’s built-in variables include the field variables—$1, $2, $3, and so on ($0 is the entire line) — that break a line of text into individual words or pieces called fields.\n• NR: NR command keeps a current count of the number of input records. Remember that records are usually lines. Awk command performs the pattern/action statements once for each record in a file.\n• NF: NF command keeps a count of the number of fields within the current input record.\n• FS: FS command contains the field separator character which is used to divide fields on the input line. The default is “white space”, meaning space and tab characters. FS can be reassigned to another character (typically in BEGIN) to change the field separator.\n• RS: RS command stores the current record separator character. Since, by default, an input line is the input record, the default record separator character is a newline.\n• OFS: OFS command stores the output field separator, which separates the fields when Awk prints them. The default is a blank space. Whenever print has several parameters separated with commas, it will print the value of OFS in between each parameter.\n• ORS: ORS command stores the output record separator, which separates the output lines when Awk prints them. The default is a newline character. print automatically outputs the contents of ORS at the end of whatever it is given to print.\n\nIn the above example, the awk command with NR prints all the lines along with the line number.\n\nUse of NF built-in variables (Display Last Field)\n\nIn the above example $1 represents Name and $NF represents Salary. We can get the Salary using $NF , where $NF represents last field.\n\nAnother use of NR built-in variables (Display Line From 3 to 6)\n\nFor the given text file:\n\n1) To print the first item along with the row number(NR) separated with ” – “ from each line in geeksforgeeks.txt:\n\n2) To return the second column/item from geeksforgeeks.txt:\n\nThe question should be:- To return the second column/item from geeksforgeeks.txt:\n\n3) To print any non empty line if present\n\nhere NF should be 0 not less than and the user have to print the line number also:\n\n4) To find the length of the longest line present in the file:\n\n5) To count the lines in a file:\n\n6) Printing lines with more than 10 characters:\n\n7) To find/check for any string in any specific column:\n\n8) To print the squares of first numbers from 1 to n say 6:\n\nWhat is awk command in Linux?\n\nWhat does awk ‘{ print $1 }’ do?\n\nWhat is the full form of awk?\n\nWhat is the main purpose of awk?\n\nWhat is the basic syntax of awk?"
    },
    {
        "link": "https://math.utah.edu/docs/info/gawk_9.html",
        "document": ""
    },
    {
        "link": "https://docs.rockylinux.org/books/sed_awk_grep/4_awk_command",
        "document": "In 1977, a programming language-level tool for processing text, named' awk', was born at Bell Labs. The name comes from the first letters of the last names of three famous people:\n\nSimilar to shell (bash, csh, zsh, and ksh), has derivatives with the development of history:\n• (new awk): It was born in 1985 and is an updated and enhanced version of . It was widely used with Unix System V Release 3.1 (1987). The old version of is called (old awk).\n• (GNU awk): It was written by Paul Rubin in 1986. The GNU Project was born in 1984.\n• : was written in 1996 by Mike Brennan, the interpreter of the awk programming language.\n\nIn the GNU/Linux operating system, the usual refers to . However, some distributions, such as Ubuntu or Debian, use as their default .\n\nIn the Rocky Linux 8.8, refers to .\n\nFor information not covered, see the gawk manual.\n\nAlthough is a tool for processing text, it has some programming language features:\n\nThe working principle of : Similar to relational databases, it supports processing fields (columns) and records (rows). By default, treats each line of a file as a record and places these records in memory for line-by-line processing, with a portion of each line treated as a field in the record. By default, delimiters to separate different fields use spaces and tabs, while numbers represent different fields in the row record. To reference multiple fields, separate them with commas or tabs.\n\nA simple example that is easy to understand：\n\nThe usage of is -\n\npattern: Find specific content in the text action: Action instruction { }: Group some instructions according to specific patterns\n\nis powerful and involves a lot of knowledge, so some of the content will be explained later.\n\nBefore formally learning , beginners need to understand the command .\n\n：format and print data. Its usage is -\n\nFORMAT：Used to control the content of the output. The following common interpretation sequences are supported：\n• %Ns - The output string. The N represents the number of strings, for example:\n• %Ni - Output integers. The N represents the number of integers of the output, for example:\n• %m.nf - Output Floating Point Number. The m represents the total number of digits output, and the n represents the number of digits after the decimal point. For example:\n\nARGUMENT: If it is a file, you need to do some preprocessing to output correctly.\n\nNo command exists in RockyLinux OS. You can only use in , and its difference from is that it automatically adds a newline at the end of each line. For example:\n• None You can also use words as delimiters. Parentheses indicate this is an overall delimiter, and \"|\" means or.\n• None Assign the value of user-defined variables in bash to awk's variables.\n• None Later, we will introduce what these variables mean. To review them now, jump to variables.\n• None Shell > df -hT awk --profile start line Filesystem Type Size Used Avail Use% Mounted on devtmpfs devtmpfs .8G .8G % /dev tmpfs tmpfs .8G .8G % /dev/shm tmpfs tmpfs .8G .9M .8G % /run tmpfs tmpfs .8G .8G % /sys/fs/cgroup /dev/nvme0n1p2 ext4 47G .7G 42G % / /dev/nvme0n1p1 xfs 1014M 181M 834M % /boot tmpfs tmpfs 363M 363M % /run/user/0 end line Shell > cat /root/awkprof.out BEGIN print print END print Shell > vim /root/awkprof.out BEGIN print print END print Shell > df -hT awk -f /root/awkprof.out start line Filesystem Type Size Used Avail Use% Mounted on devtmpfs devtmpfs .8G .8G % /dev tmpfs tmpfs .8G .8G % /dev/shm tmpfs tmpfs .8G .9M .8G % /run tmpfs tmpfs .8G .8G % /sys/fs/cgroup /dev/nvme0n1p2 ext4 47G .7G 42G % / /dev/nvme0n1p1 xfs 1014M 181M 834M % /boot tmpfs tmpfs 363M 363M % /run/user/0 end line\n• None Locates consecutive lines by string and prints them Start range: stop matching when the first match is encountered. End range: stop matching when the first match is encountered.\n• None You can also use the -v option to assign values to variables. The default output delimiter is a space when using commas to reference multiple fields. You can, however, specify the output delimiter separately.\n• None By default, uses newline characters to distinguish each line record\n• None Count the number of fields per line in the current text Exclude the last two fields\n• None Print the total number of lines in the file content Print the second field on line 200 Shell > cat /etc/services awk # Note that it is presently the policy of IANA to assign a single well-known # port number for both TCP and UDP; hence, most entries here have two entries # even if the protocol doesn't support UDP operations. # Updated from RFC 1700, ``Assigned Numbers'' (October 1994). Not all ports\n• None This variable is mainly used to determine the file the program is working on.\n• None You can reference operating systems or user-defined variables in programs.\n• None This variable is useful if you want to use regular expressions in and ignore case.\n• None Why? Read the first line: Because \"i\" is not assigned a value, so \"i=!i\" indicates TRUE. Read the second line: At this point, \"i=!i\" indicates FALSE. And so on, the final printed line is an odd number. As you can see, sometimes you can ignore the syntax for the \"action\" part, which by default is equivalent to \"{print $0}\".\n• None It can also be used in the \"pattern\":\n• None You can use the bash command in the awk program, for example: Please pay attention! You must use double quotes to include the command.\n• None Here, we cover basic examples of regular expressions. You can use regular expressions on row records. If the file has a large amount of text, regular expressions can also be used for fields, which will help improve processing efficiency. The usage example is as follows:\n• None Example of a single branch use of an if statement: The condition is determined as a regular expression:\n• None Traverse and print out the fields of all row records.\n• None Traverse and print out the fields of all row records. Print the fields for each row of records in reverse order. Print each line of records in the opposite direction.\n• None The comparison between the two is as follows:\n• None You can specify a return value in the range of [0,255]\n\narray: A collection of data with the same data type arranged in a certain order. Each data in an array is called an element.\n\nLike most programming languages, also supports arrays, which are divided into indexed arrays (with numbers as subscripts) and associative arrays (with strings as subscripts).\n\nhas a lot of functions, and the functions related to arrays are:\n• None length(Array_Name) - Get the length of the array.\n• None Get the length of the array: Store all GNU/Linux users in an array: The numeric subscript of an array can be a positive integer, a negative integer, a string, or 0, so the numeric subscript of an array has no concept of an initial value. This is not the same as arrays in .\n• None Delete an element from an array\n• None You can use the for statement, which is suitable for cases where the array subscript is unknown: If the subscript of an array is regular, you can use this form of the for statement:\n• None Use \"++\" as the subscript of the array\n• None Use a field as the subscript of an array\n• None Count the number of occurrences of the same field Count the number of occurrences of the same IPv4 address. Basic idea:\n• First use the command to filter out all IPv4 addresses\n• Then hand it over to the program for processing Count the number of occurrences of words regardless of case. Basic idea:\n• Split all fields into multiple rows of records\n• Then hand it over to the program for processing You can first filter specific row records and then perform statistics, such as:\n• None Print lines based on the number of occurrences of a specific field\n• None The program does not support multi-dimensional arrays, but support for multi-dimensional arrays is achievable through simulation. By default, \"\\034\" is the delimiter for the subscript of a multidimensional array. Please note the following differences when using multidimensional arrays: Count the number of times the field appears:\n• None As you can see, the int function only works for numbers, and when encountering a string, converts it to 0. When encountering a string starting with a number, truncates it.\n• None The example of using the rand function is as follows: The example of using the srand function is as follows: Generate an integer within the range of (0,100):\n• None\n• Numbers have higher priority than strings and are arranged in ascending order. If you are using the asorti function, the example is as follows:\n• If a negative number is encountered, the first digit from the left will be compared. If it is the same, the second digit will be compared, and so on\n• If a positive number is encountered, it will be arranged in ascending order\n• None Just like the command, you can also use the \"&\" symbol to reference already matched strings. Shell > vim /tmp/tmp-file1.txt A .168.1.1 HTTP B .168.1.2 HTTP B .168.1.2 MYSQL C .168.1.1 MYSQL C .168.1.1 MQ D .168.1.4 NGINX # Add a line of text before the second line Shell > cat /tmp/tmp-file1.txt awk A .168.1.1 HTTP add a line B .168.1.2 HTTP B .168.1.2 MYSQL C .168.1.1 MYSQL C .168.1.1 MQ D .168.1.4 NGINX # Add a string after the IP address in the second line Shell > cat /tmp/tmp-file1.txt awk A .168.1.1 HTTP B .168.1.2 STRING HTTP B .168.1.2 MYSQL C .168.1.1 MYSQL C .168.1.1 MQ D .168.1.4 NGINX\n• None # The length of the output field Shell > tail -n /etc/services awk # The length of the output array Shell > cat /etc/passwd awk -F\n• None Shell > head -n /etc/passwd root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin adm:x:3:4:adm:/var/adm:/sbin/nologin lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin # I need this part of the content - \"emon:/sbin:/sbin/nologin\" Shell > head -n /etc/passwd awk emon:/sbin:/sbin/nologin Shell > tail -n /etc/services axio-disc /udp pmwebapi /tcp cloudcheck-ping /udp cloudcheck /tcp spremotetablet /tcp # I need this part of the content - \"tablet\" Shell > tail -n /etc/services awk tablet\n• None Functions that deal with time and date What is a UNIX timestamp? According to the development history of GNU/Linux, UNIX V1 was born in 1971, and the book \"UNIX Programmer's Manual\" was published on November 3 of the same year, which defines 1970-01-01 as the reference date of the start of UNIX. The conversion between a timestamp and a natural date time in days: The conversion between a timestamp and a natural date time in seconds: The conversion between natural date time and UNIX timestamp in program:\n• None Using the functions we learned earlier and the \"&\" symbol, we can:\n• None Add each line of the b file to the end of each line of the C file: Shell > cat /tmp/b.txt b1 b2 b3 b4 b5 b6 Shell > cat /tmp/c.txt A .168.1.1 HTTP B .168.1.2 HTTP B .168.1.2 MYSQL C .168.1.1 MYSQL C .168.1.1 MQ D .168.1.4 NGINX Shell > awk /tmp/c.txt A .168.1.1 HTTP b1 B .168.1.2 HTTP b2 B .168.1.2 MYSQL b3 C .168.1.1 MYSQL b4 C .168.1.1 MQ b5 D .168.1.4 NGINX b6 Replace the specified field of the c file with the content line of the b file:\n• None Earlier, we introduced the break statement and the continue statement, the former used to terminate the loop, and the latter used to jump out of the current loop. See here. For next, when the conditions are met, it will stop the input recording that meets the conditions and continue with subsequent actions. \"next\" cannot be used in \"BEGIN{}\" and \"END{}\".\n• None You can use this function to call commands in the Shell, such as: Please note to add double quotes when using the system function. If not added, the program will consider it a variable of the program. What if the Shell command itself contains double quotes? Using escape characters - \"\\\", such as:\n• None Write the output of the program to a file \">\" indicates writing to the file as an overlay. If you want to write to the file as an append, please use \">>\". Reminder again, you should use double quotation marks to include the file path.\n\nIf you have specialized programming language skills, is relatively easy to learn. However, for most sysadmins with weak programming language skills (including the author), can be very complicated to learn. For information not covered, please refer to here.\n\nThank you again for reading."
    },
    {
        "link": "https://gnu.org/software/gawk/manual/html_node/Pattern-Action-Summary.html",
        "document": ""
    },
    {
        "link": "https://medium.com/itversity/advanced-text-processing-with-awk-command-5eecfa479623",
        "document": "Hey! Ever found yourself needing to extract and manipulate text data efficiently while working on a DevOps task? Whether it’s parsing log files, extracting configuration values, or generating reports, mastering the command can significantly streamline your workflow. Let's dive into the power of , its basic usage, and some advanced techniques to elevate your text processing skills.\n\nis a versatile and powerful text processing language used for pattern scanning and processing. It is incredibly useful for extracting information from text files, transforming data, and performing complex text manipulations.\n\nImagine as a highly skilled chef who can take raw ingredients (text data), apply precise cuts and transformations, and present a perfectly prepared dish (processed output).\n\nWhile tools like and are excellent for specific text operations, combines the capabilities of searching, extracting, and transforming data, making it a more comprehensive solution for complex text processing tasks.\n\nThis command prints the first column of each line from .\n• : Built-in variable representing the number of fields in the current record.\n• and : Special patterns for actions to be performed before and after processing.\n\nCombine with to set the field separator only once:\n\nThis command prints the first and third columns where the third column’s value is greater than 50.\n\nThis command calculates the sum of the third column and prints the total at the end.\n• Use with for more complex filtering:\n\nThis command prints the filename and content, handling multiple files.\n\nThis command converts the second column to uppercase and uses a comma as the output field separator.\n• Use with shell loops for batch processing:\n\nA DevOps engineer uses to automate the extraction of error logs from multiple Kubernetes pod logs, summarizing the errors by type and frequency.\n\nStep Three: Count Each Type of Log Message\n• Speed: is faster for complex text manipulations compared to and .\n\nWith an increasing focus on automation and data-driven operations, will remain a crucial tool for DevOps engineers. Expect more integration with CI/CD pipelines and enhanced features for handling large datasets.\n• Consistency: Using ensures consistent text processing in scripts and automation.\n\nis an essential tool for any DevOps professional. Understanding its basic and advanced usage can significantly enhance your workflow, making complex text processing tasks simpler and more efficient."
    },
    {
        "link": "https://digitalocean.com/community/tutorials/how-to-use-the-awk-language-to-manipulate-text-in-linux",
        "document": "Linux utilities often follow the Unix philosophy of design. Tools are encouraged to be small, use plain text files for input and output, and operate in a modular manner. Because of this legacy, we have great text processing functionality with tools like sed and .\n\nis both a programming language and text processor that you can use to manipulate text data in very useful ways. In this guide, you’ll explore how to use the command line tool and how to use it to process text.\n\nThe command is included by default in all modern Linux systems, so you do not need to install it to begin using it.\n\nis most useful when handling text files that are formatted in a predictable way. For instance, it is excellent at parsing and manipulating tabular data. It operates on a line-by-line basis and iterates through the entire file.\n\nBy default, it uses whitespace (spaces, tabs, etc.) to separate fields. Luckily, many configuration files on your Linux system use this format.\n\nThe basic format of an command is:\n\nYou can omit either the search portion or the action portion from any command. By default, the action taken if the “action” portion is not given is “print”. This simply prints all lines that match.\n\nIf the search portion is not given, performs the action listed on each line.\n\nIf both are given, uses the search portion to decide if the current line reflects the pattern, and then performs the actions on matches.\n\nIn its simplest form, you can use like to print all lines of a text file out to the screen.\n\nCreate a file which lists the favorite foods of a group of friends:\n\nNow use the command to print the file to the screen:\n\nYou’ll see the file printed to the screen:\n\nThis isn’t very useful. Let’s try out ’s search filtering capabilities by searching through the file for the text “sand”:\n\nAs you can see, now only prints the lines that have the characters “sand” in them.\n\nUsing regular expressions, you can target specific parts of the text. To display only the line that starts with the letters “sand”, use the regular expression :\n\nThis time, only one line is displayed:\n\nSimilarly, you can use the action section to specify which pieces of information you want to print. For instance, to print only the first column, use the following command:\n\nYou can reference every column (as delimited by whitespace) by variables associated with their column number. For example, the The first column is , the second is , and you can reference the entire line with .\n\nThe command uses some internal variables to assign certain pieces of information as it processes a file.\n\nThe internal variables that uses are:\n• FNR: References the number of the current record relative to the current input file. For instance, if you have two input files, this would tell you the record number of each file instead of as a total.\n• FS: The current field separator used to denote each field in a record. By default, this is set to whitespace.\n• NF: The number of fields in the current record.\n• NR: The number of the current record.\n• OFS: The field separator for the outputted data. By default, this is set to whitespace.\n• ORS: The record separator for the outputted data. By default, this is a newline character.\n• RS: The record separator used to distinguish separate records in the input file. By default, this is a newline character.\n\nYou can change the values of these variables at will to match the needs of your files. Usually you do this during the initialization phase of your processing.\n\nThis brings us to another important concept. The syntax is slightly more complex than what you’ve used so far There are also optional and blocks that can contain commands to execute before and after the file processing, respectively.\n\nThis makes our expanded syntax look something like this:\n\nThe and keywords are specific sets of conditions, just like the search parameters. They match before and after the document has been processed.\n\nThis means that you can change some of the internal variables in the section. For instance, the file is delimited with colons ( ) instead of whitespace.\n\nTo print out the first column of this file, execute the following command:\n\nYou can use the and blocks to print information about the fields you are printing. Use the following command to transform the data from the file into a table, nicely spaced with tabs using :\n\nAs you can see, you can format things quite nicely by taking advantage of some of ’s features.\n\nEach of the expanded sections are optional. In fact, the main action section itself is optional if another section is defined. For example, you can do things like this:\n\nAnd you’ll see this output:\n\nNow let’s look at how to look for text within fields of the output.\n\nIn one of the previous examples, you printed the line in the file that began with “sand”. This was easy because you were looking for the beginning of the entire line.\n\nWhat if you wanted to find out if a search pattern matched at the beginning of a field instead?\n\nCreate a new version of the file which adds an item number in front of each person’s food:\n\nIf you want to find all foods from this file that begin with “sa”, you might begin by trying something like this:\n\nThis shows all lines that contain “sa”:\n\nHere, you are matching any instance of “sa” in the word. This ends up including things like “wasabi” which has the pattern in the middle, or “sandy” which is not in the column you want. In this case you’re only interested in words beginning with “sa” in the second column.\n\nYou can tell to only match at the beginning of the second column by using this command:\n\nAs you can see, this allows us to only search at the beginning of the second column for a match.\n\nThe part specifies that should only pay attention to the second column.\n\nYou can just as easily search for things that do not match by including the “!” character before the tilde (~). This command will return all lines that do not have a food that starts with “sa”:\n\nIf you decide later on that you are only interested in lines that don’t start with “sa” and the item number is less than 5, you could use a compound expression like this:\n\nThis introduces a few new concepts. The first is the ability to add additional requirements for the line to match by using the operator. Using this, you can combine an arbitrary number of conditions for the line to match. In this case, you’re using this operator to add a check that the value of the first column is less than 5.\n\nYou can use to process files, but you can also work with the output of other programs.\n\nYou can use the command to parse the output of other programs rather than specifying a filename. For example, you can use to parse out the IPv4 address from the command.\n\nThe command displays the IP address, broadcast address, and other information about all the network interfaces on your machine. To display the information for the interface called , use this command:\n\nYou’ll see the following results:\n\nYou can use to target the line and then print out just the IP address:\n\nThe flag tells to delimit by forward slashes or spaces using the regular expression . This splits the line into separate fields. The IP address is in the third field because the spaces at the start of the line also count as a field, since you delimited by spaces as well as slashes. Note that treated consecutive spaces as a single space in this case.\n\nYou’ll find many places where you can use to search or parse the output of other commands.\n\nBy now, you should have a basic understanding of how you can use the command to manipulate, format, and selectively print text files and streams of text. Awk is a much larger topic though, and is actually an entire programming language complete with variable assignment, control structures, built-in functions, and more. You can use it within your own scripts to format text in a reliable way.\n\nTo learn more about , you can read the free public-domain book by its creators which goes into much more detail."
    },
    {
        "link": "https://stackoverflow.com/questions/36291088/using-awk-to-place-each-word-in-a-text-file-on-a-new-line",
        "document": "I'm trying to use AWK to place every word within a text document on a new line. I don't really know how to use AWK but I've found some commands online which should solve my problem. I've tried the following commands:\n\nHowever, both of these commands have the same effect, which is that all spaces are removed.\n\nFor clarity, lets say that input.txt contains the text:\n\nI'm using Cygwin on Windows 7 to use these commands. Is there something I'm missing within the commands?"
    },
    {
        "link": "https://oreilly.com/library/view/effective-awk-programming/9781491904930/ch04.html",
        "document": "In the typical program, reads all input either from the standard input (by default, this is the keyboard, but often it is a pipe from another command) or from files whose names you specify on the command line. If you specify input files, reads them in order, processing all the data from one before going on to the next. The name of the current input file can be found in the predefined variable (see Predefined Variables).\n\nThe input is read in units called records, and is processed by the rules of your program one record at a time. By default, each record is one line. Each record is automatically split into chunks called fields. This makes it more convenient for programs to work on the parts of a record.\n\nOn rare occasions, you may need to use the command. The command is valuable both because it can do explicit input from any number of files, and because the files used with it do not have to be named on the command line (see Explicit Input with getline).\n\nHow Input Is Split into Records divides the input for your program into records and fields. It keeps track of the number of records that have been read so far from the current input file. This value is stored in a predefined variable called , which is reset to zero every time a new file is started. Another predefined variable, , records the total number of input records read so far from all datafiles. It starts at zero, but is never automatically reset to zero. Records are separated by a character called the record separator. By default, the record separator is the newline character. This is why records are, by default, single lines. To use a different character for the record separator, simply assign that character to the predefined variable . Like any other variable, the value of can be changed in the program with the assignment operator, ‘ ’ (see Assignment Expressions). The new record-separator character should be enclosed in quotation marks, which indicate a string constant. Often, the right time to do this is at the beginning of execution, before any input is processed, so that the very first record is read with the proper separator. To do this, use the special pattern (see The BEGIN and END Special Patterns). For example: changes the value of to ‘ ’, before reading any input. The new value is a string whose first character is the letter “u”; as a result, records are separated by the letter “u”. Then the input file is read, and the second rule in the program (the action with no pattern) prints each record. Because each statement adds a newline at the end of its output, this program copies the input with each ‘ ’ changed to a newline. Here are the results of running the program on : Note that the entry for the name ‘ ’ is not split. In the original datafile (see Datafiles for the Examples), the line looks like this: It contains no ‘ ’, so there is no reason to split the record, unlike the others, which each have one or more occurrences of the ‘ ’. In fact, this record is treated as part of the previous record; the newline separating them in the output is the original newline in the datafile, not the one added by when it printed the record! Another way to change the record separator is on the command line, using the variable-assignment feature (see Other Command-Line Arguments): This sets to ‘ ’ before processing . Using an alphabetic character such as ‘ ’ for the record separator is highly likely to produce strange results. Using an unusual character such as ‘ ’ is more likely to produce correct behavior in the majority of cases, but there are no guarantees. The moral is: Know Your Data. When using regular characters as the record separator, there is one unusual case that occurs when is being fully POSIX-compliant (see Command-Line Options). Then, the following (extreme) pipeline prints a surprising ‘ ’: There is one field, consisting of a newline. The value of the built-in variable is the number of fields in the current record. (In the normal case, treats the newline as whitespace, printing ‘ ’ as the result. Most other versions of also act this way.) Reaching the end of an input file terminates the current input record, even if the last character in the file is not the character in . (d.c.) The empty string (a string without any characters) has a special meaning as the value of . It means that records are separated by one or more blank lines and nothing else. See Multiple-Line Records for more details. If you change the value of in the middle of an run, the new value is used to delimit subsequent records, but the record currently being processed, as well as records already processed, are not affected. After the end of the record has been determined, sets the variable to the text in the input that matched . When using , the value of is not limited to a one-character string. It can be any regular expression (see Chapter 3). (c.e.) In general, each record ends at the next string that matches the regular expression; the next record starts at the end of the matching string. This general rule is actually at work in the usual case, where contains just a newline: a record ends at the beginning of the next matching string (the next newline in the input), and the following record starts just after the end of this string (at the first character of the following line). The newline, because it matches , is not part of either record. When is a single character, contains the same single character. However, when is a regular expression, contains the actual input text that matched the regular expression. If the input file ends without any text matching , sets to the null string. The following example illustrates both of these features. It sets equal to a regular expression that matches either a newline or a series of one or more uppercase letters with optional leading and/or trailing whitespace: $ > > Record = record 1 and RT = [ AAAA ] Record = record 2 and RT = [ BBBB ] Record = record 3 and RT = [ ] The square brackets delineate the contents of , letting you see the leading and trailing whitespace. The final value of is a newline. See A Simple Stream Editor for a more useful example of as a regexp and . If you set to a regular expression that allows optional trailing text, such as ‘ ’, it is possible, due to implementation constraints, that may match the leading part of the regular expression, but not the trailing part, particularly if the input text that could match the trailing part is fairly long. attempts to avoid this problem, but currently, there’s no guarantee that this will never happen. Remember that in , the ‘ ’ and ‘ ’ anchor metacharacters match the beginning and end of a string, and not the beginning and end of a line. As a result, something like ‘ ’ can only match at the beginning of a file. This is because views the input file as one long string that happens to contain newline characters. It is thus best to avoid anchor metacharacters in the value of . The use of as a regular expression and the variable are extensions; they are not available in compatibility mode (see Command-Line Options). In compatibility mode, only the first character of the value of determines the end of the record. There are times when you might want to treat an entire datafile as a single record. The only way to make this happen is to give a value that you know doesn’t occur in the input file. This is hard to do in a general way, such that a program always works for arbitrary input files. You might think that for text files, the NUL character, which consists of a character with all bits equal to zero, is a good value to use for in this case: BEGIN { RS = \"\\0\" } # whole file becomes one record? in fact accepts this, and uses the NUL character for the record separator. This works for certain special files, such as on GNU/Linux systems, where the NUL character is in fact the record separator. However, this usage is not portable to most other implementations. Almost all other implementations[ ] store strings internally as C-style strings. C strings use the NUL character as the string terminator. In effect, this means that ‘ ’ is the same as ‘ ’. (d.c.) It happens that recent versions of can use the NUL character as a record separator. However, this is a special case: does not allow embedded NUL characters in strings. (This may change in a future version of .) See Reading a Whole File at Once for an interesting way to read whole files. If you are using , see Reading an Entire File for another option.\n\nWhen reads an input record, the record is automatically parsed or separated by the utility into chunks called fields. By default, fields are separated by whitespace, like words in a line. Whitespace in means any string of one or more spaces, TABs, or newlines;[ ] other characters that are considered whitespace by other languages (such as formfeed, vertical tab, etc.) are not considered whitespace by . The purpose of fields is to make it more convenient for you to refer to these pieces of the record. You don’t have to use them—you can operate on the whole record if you want—but fields are what make simple programs so powerful. You use a dollar sign (‘ ’) to refer to a field in an program, followed by the number of the field you want. Thus, refers to the first field, to the second, and so on. (Unlike in the Unix shells, the field numbers are not limited to single digits. is the 127th field in the record.) For example, suppose the following is a line of input: This seems like a pretty nice example. Here the first field, or , is ‘ ’, the second field, or , is ‘ ’, and so on. Note that the last field, , is ‘ ’. Because there is no space between the ‘ ’ and the ‘ ’, the period is considered part of the seventh field. is a predefined variable whose value is the number of fields in the current record. automatically updates the value of each time it reads a record. No matter how many fields there are, the last field in a record can be represented by . So, is the same as , which is ‘ ’. If you try to reference a field beyond the last one (such as when the record has only seven fields), you get the empty string. (If used in a numeric operation, you get zero.) The use of , which looks like a reference to the “zeroth” field, is a special case: it represents the whole input record. Use it when you are not interested in specific fields. Here are some more examples: This example prints each record in the file whose first field contains the string ‘ ’. By contrast, the following example looks for ‘ ’ in the entire record and prints the first and last fields for each matching input record:\n\nThe contents of a field, as seen by , can be changed within an program; this changes what perceives as the current input record. (The actual input is untouched; never modifies the input file.) Consider the following example and its output: The program first saves the original value of field three in the variable . The ‘ ’ sign represents subtraction, so this program reassigns field three, , as the original value of field three minus ten: ‘ ’. (See Arithmetic Operators.) Then it prints the original and new values for field three. (Someone in the warehouse made a consistent mistake while inventorying the red boxes.) For this to work, the text in must make sense as a number; the string of characters must be converted to a number for the computer to do arithmetic on it. The number resulting from the subtraction is converted back to a string of characters that then becomes field three. See Conversion of Strings and Numbers. When the value of a field is changed (as perceived by ), the text of the input record is recalculated to contain the new field where the old one was. In other words, changes to reflect the altered field. Thus, this program prints a copy of the input file, with 10 subtracted from the second field of each line: It is also possible to assign contents to fields that are out of range. For example: We’ve just created , whose value is the sum of fields , , , and . The ‘ ’ sign represents addition. For the file , represents the total number of parcels shipped for a particular month. Creating a new field changes ’s internal copy of the current input record, which is the value of . Thus, if you do ‘ ’ after adding a field, the record printed includes the new field, with the appropriate number of field separators between it and the previously existing fields. This recomputation affects and is affected by (the number of fields; see Examining Fields). For example, the value of is set to the number of the highest field you create. The exact format of is also affected by a feature that has not been discussed yet: the output field separator, , used to separate the fields (see Output Separators). Note, however, that merely referencing an out-of-range field does not change the value of either or . Referencing an out-of-range field only produces an empty string. For example: if ($(NF+1) != \"\") print \"can't happen\" else print \"everything is normal\" should print ‘ ’, because is certain to be out of range. (See The if-else Statement for more information about ’s statements. See Variable Typing and Comparison Expressions for more information about the ‘ ’ operator.) It is important to note that making an assignment to an existing field changes the value of but does not change the value of , even when you assign the empty string to a field. For example: The field is still there; it just has an empty value, delimited by the two colons between ‘ ’ and ‘ ’. This example shows what happens if you create a new field: The intervening field, , is created with an empty value (indicated by the second pair of adjacent colons), and is updated with the value six. Decrementing throws away the values of the fields after the new value of and recomputes . (d.c.) Here is an example: Some versions of don’t rebuild when is decremented. Finally, there are times when it is convenient to force to rebuild the entire record, using the current values of the fields and . To do this, use the seemingly innocuous assignment: $1 = $1 # force record to be reconstituted print $0 # or whatever else with $0 This forces to rebuild the record. It does help to add a comment, as we’ve shown here. There is a flip side to the relationship between and the fields. Any assignment to causes the record to be reparsed into fields using the current value of . This also applies to any built-in function that updates , such as and (see String-Manipulation Functions). It is important to remember that is the full record, exactly as it was read from the input. This includes any leading or trailing whitespace, and the exact whitespace (or other characters) that separates the fields. It is a common error to try to change the field separators in a record simply by setting and , and then expecting a plain ‘ ’ or ‘ ’ to print the modified record. But this does not work, because nothing was done to change the record itself. Instead, you must force the record to be rebuilt, typically with a statement such as ‘ ’, as described earlier.\n\nSpecifying How Fields Are Separated The field separator, which is either a single character or a regular expression, controls the way splits an input record into fields. scans the input record for character sequences that match the separator; the fields themselves are the text between the matches. In the examples that follow, we use the bullet symbol (•) to represent spaces in the output. If the field separator is ‘ ’, then the following line: is split into three fields: ‘ ’, ‘ ’, and ‘ ’. Note the leading spaces in the values of the second and third fields. The field separator is represented by the predefined variable . Shell programmers take note: does not use the name that is used by the POSIX-compliant shells (such as the Unix Bourne shell, , or Bash). The value of can be changed in the program with the assignment operator, ‘ ’ (see Assignment Expressions). Often, the right time to do this is at the beginning of execution before any input has been processed, so that the very first record is read with the proper separator. To do this, use the special pattern (see The BEGIN and END Special Patterns). For example, here we set the value of to the string : this program extracts and prints the string ‘ ’. Sometimes the input data contains separator characters that don’t separate fields the way you thought they would. For instance, the person’s name in the example we just used might have a title or suffix attached, such as: The same program would extract ‘ ’ instead of ‘ ’. If you were expecting the program to print the address, you would be surprised. The moral is to choose your data layout and separator characters carefully to prevent such problems. (If the data is not in a form that is easy to process, perhaps you can massage it first with a separate program.) Fields are normally separated by whitespace sequences (spaces, TABs, and newlines), not by single spaces. Two spaces in a row do not delimit an empty field. The default value of the field separator is a string containing a single space, . If interpreted this value in the usual way, each space character would separate fields, so two spaces in a row would make an empty field between them. The reason this does not happen is that a single space as the value of is a special case—it is taken to specify the default manner of delimiting fields. If is any other single character, such as , then each occurrence of that character separates two fields. Two consecutive occurrences delimit an empty field. If the character occurs at the beginning or the end of the line, that too delimits an empty field. The space character is the only single character that does not follow these rules. The previous subsection discussed the use of single characters or simple strings as the value of . More generally, the value of may be a string containing any regular expression. In this case, each match in the record for the regular expression separates fields. For example, the assignment: makes every area of an input line that consists of a comma followed by a space and a TAB into a field separator. For a less trivial example of a regular expression, try using single spaces to separate fields the way single commas are used. can be set to (left bracket, space, right bracket). This regular expression matches a single space and nothing else (see Chapter 3). There is an important difference between the two cases of ‘ ’ (a single space) and ‘ ’ (a regular expression matching one or more spaces, TABs, or newlines). For both values of , fields are separated by runs (multiple adjacent occurrences) of spaces, TABs, and/or newlines. However, when the value of is , first strips leading and trailing whitespace from the record and then decides where the fields are. For example, the following pipeline prints ‘ ’: However, this pipeline prints ‘ ’ (note the extra spaces around each letter): In this case, the first field is null, or empty. The stripping of leading and trailing whitespace also comes into play whenever is recomputed. For instance, study this pipeline: The first statement prints the record as it was read, with leading whitespace intact. The assignment to rebuilds by concatenating through together, separated by the value of (which is a space by default). Because the leading whitespace was ignored when finding , it is not part of the new . Finally, the last statement prints the new . There is an additional subtlety to be aware of when using regular expressions for field splitting. It is not well specified in the POSIX standard, or anywhere else, what ‘ ’ means when splitting fields. Does the ‘ ’ match only at the beginning of the entire record? Or is each field separator a new string? It turns out that different versions answer this question differently, and you should not rely on any specific behavior in your programs. (d.c.) As a point of information, BWK allows ‘ ’ to match only at the beginning of the record. also works this way. For example: There are times when you may want to examine each character of a record separately. This can be done in by simply assigning the null string ( ) to . (c.e.) In this case, each individual character in the record becomes a separate field. For example: $ > > > > Field 1 is a Field 2 is Field 3 is b Traditionally, the behavior of equal to was not defined. In this case, most versions of Unix simply treat the entire record as only having one field. (d.c.) In compatibility mode (see Command-Line Options), if is the null string, then also behaves this way. can be set on the command line. Use the option to do so. For example: sets to the ‘ ’ character. Notice that the option uses an uppercase ‘ ’ instead of a lowercase ‘ ’. The latter option ( ) specifies a file containing an program. The value used for the argument to is processed in exactly the same way as assignments to the predefined variable . Any special characters in the field separator must be escaped appropriately. For example, to use a ‘ ’ as the field separator on the command line, you would have to type: Because ‘ ’ is used for quoting in the shell, sees ‘ ’. Then processes the ‘ ’ for escape characters (see Escape Sequences), finally yielding a single ‘ ’ to use for the field separator. As a special case, in compatibility mode (see Command-Line Options), if the argument to is ‘ ’, then is set to the TAB character. If you type ‘ ’ at the shell, without any quotes, the ‘ ’ gets deleted, so figures that you really want your fields to be separated with TABs and not ‘ ’s. Use ‘ ’ or ‘ ’ on the command line if you really do want to separate your fields with ‘ ’s. Use ‘ ’ when not in compatibility mode to specify that TABs separate fields. As an example, let’s use an program file called that contains the pattern and the action ‘ ’: Let’s also set to be the ‘ ’ character and run the program on the file . The following a university, and the first three digits of their phone numbers: Note the third line of output. The third line in the original file looked like this: The ‘ ’ as part of the person’s name was used as the field separator, instead of the ‘ ’ in the phone number that was originally intended. This demonstrates why you have to be careful in choosing your field and record separators. Perhaps the most common use of a single character as the field separator occurs when processing the Unix system password file. On many Unix systems, each user has a separate entry in the system password file, with one line per user. The information in these lines is separated by colons. The first field is the user’s login name and the second is the user’s encrypted or shadow password. (A shadow password is indicated by the presence of a single ‘ ’ in the second field.) A password file entry might look like this: The following program searches the system password file and prints the entries for users whose full name is not indicated: Occasionally, it’s useful to treat the whole input line as a single field. This can be done easily and portably simply by setting to (a newline):[ ] When you do this, is the same as . Changing FS Does Not Affect the Fields According to the POSIX standard, is supposed to behave as if each record is split into fields at the time it is read. In particular, this means that if you change the value of after a record is read, the values of the fields (i.e., how they were split) should reflect the old value of , not the new one. However, many older implementations of do not work this way. Instead, they defer splitting the fields until a field is actually referenced. The fields are split using the current value of ! (d.c.) This behavior can be difficult to diagnose. The following example illustrates the difference between the two methods: on an incorrect implementation of , while prints the full first line of the file, something like: It is important to remember that when you assign a string constant as the value of , it undergoes normal string processing. For example, with Unix and , the assignment ‘ ’ assigns the character string to (the backslash is stripped). This creates a regexp meaning “fields are separated by occurrences of any two characters.” If instead you want fields to be separated by a literal period followed by any single character, use ‘ ’. The following list summarizes how fields are split, based on the value of (‘ ’ means “is equal to”): Fields are separated by runs of whitespace. Leading and trailing whitespace are ignored. This is the default. Fields are separated by each occurrence of the character. Multiple successive occurrences delimit empty fields, as do leading and trailing occurrences. The character can even be a regexp metacharacter; it does not need to be escaped. Fields are separated by occurrences of characters that match . Leading and trailing matches of delimit empty fields. Each individual character in the record becomes a separate field. (This is a common extension; it is not specified by the POSIX standard.) The variable (see Built-in Variables That Control awk) affects field splitting only when the value of is a regexp. It has no effect when is a single character, even if that character is a letter. Thus, in the following code: The output is ‘ ’. If you really want to split fields on an alphabetic character while ignoring case, use a regexp that will do it for you (e.g., ‘ ’). In this case, will take effect.\n\nThis section discusses an advanced feature of . If you are a novice user, you might want to skip it on the first reading. provides a facility for dealing with fixed-width fields with no distinctive field separator. For example, data of this nature arises in the input for old Fortran programs where numbers are run together, or in the output of programs that did not anticipate the use of their output as input for other programs. An example of the latter is a table where all the columns are lined up by the use of a variable number of spaces and empty fields are just spaces. Clearly, ’s normal field splitting based on does not work well in this case. Although a portable program can use a series of calls on (see String-Manipulation Functions), this is awkward and inefficient for a large number of fields. The splitting of an input record into fixed-width fields is specified by assigning a string containing space-separated numbers to the built-in variable . Each number specifies the width of the field, including columns between fields. If you want to ignore the columns between fields, you can specify the width as a separate field that is subsequently ignored. It is a fatal error to supply a field width that is not a positive number. The following data is the output of the Unix utility. It is useful to illustrate the use of : The following program takes this input, converts the idle time to number of seconds, and prints out the first two fields and the calculated idle time: BEGIN { FIELDWIDTHS = \"9 6 10 6 7 7 35\" } NR > 2 { idle = $4 sub(/^ +/, \"\", idle) # strip leading spaces if (idle == \"\") idle = 0 if (idle ~ /:/) { split(idle, t, \":\") idle = t[1] * 60 + t[2] } if (idle ~ /days/) idle *= 24 * 60 * 60 print $1, $2, idle } The preceding program uses a number of features that haven’t been introduced yet. Running the program on the data produces the following results: Another (possibly more practical) example of fixed-width input data is the input from a deck of balloting cards. In some parts of the United States, voters mark their choices by punching holes in computer cards. These cards are then processed to count the votes for any particular candidate or on any particular issue. Because a voter may choose not to vote on some issue, any column on the card may be empty. An program for processing such data could use the feature to simplify reading the data. (Of course, getting to run on a system with card readers is another story!) Assigning a value to causes to use for field splitting again. Use ‘ ’ to make this happen, without having to know the current value of . In order to tell which kind of field splitting is in effect, use (see Built-in Variables That Convey Information). The value is if regular field splitting is being used, or is if fixed-width field splitting is being used: if (PROCINFO[\"FS\"] == \"FS\") … else if (PROCINFO[\"FS\"] == \"FIELDWIDTHS\") … else … This information is useful when writing a function that needs to temporarily change or , read some records, and then restore the original settings (see Reading the User Database for an example of such a function).\n\nThis section discusses an advanced feature of . If you are a novice user, you might want to skip it on the first reading. Normally, when using , defines the fields as the parts of the record that occur in between each field separator. In other words, defines what a field is not, instead of what a field is. However, there are times when you really want to define the fields by what they are, and not by what they are not. The most notorious such case is so-called comma-separated values (CSV) data. Many spreadsheet programs, for example, can export their data into text files, where each record is terminated with a newline, and fields are separated by commas. If commas only separated the data, there wouldn’t be an issue. The problem comes when one of the fields contains an embedded comma. In such cases, most programs embed the field in double quotes.[ ] So, we might have data like this: The variable offers a solution for cases like this. The value of should be a string that provides a regular expression. This regular expression describes the contents of each field. In the case of CSV data as presented here, each field is either “anything that is not a comma,” or “a double quote, anything that is not a double quote, and a closing double quote.” If written as a regular expression constant (see Chapter 3), we would have . Writing this as a string requires us to escape the double quotes, leading to: Putting this to use, here is a simple program to parse the data: When run, we get the following: Note the embedded comma in the value of . A straightforward improvement when processing CSV data of this sort would be to remove the quotes when they occur, with something like this: if (substr($i, 1, 1) == \"\\\"\") { len = length($i) $i = substr($i, 2, len - 2) # Get text within the two quotes } As with , the variable (see Built-in Variables That Control awk) affects field splitting with . Assigning a value to overrides field splitting with and with . Similar to , the value of will be if content-based field splitting is being used. Some programs export CSV data that contains embedded newlines between the double quotes. provides no way to deal with this. Even though a formal specification for CSV data exists, there isn’t much more to be done; the mechanism provides an elegant solution for the majority of cases, and the developers are satisfied with that. As written, the regexp used for requires that each field contain at least one character. A straightforward modification (changing the first ‘ ’ to ‘ ’) allows fields to be empty: Finally, the function makes the same functionality available for splitting regular strings (see String-Manipulation Functions). To recap, provides three independent methods to split input records into fields. The mechanism used is based on which of the three variables— , , or —was last assigned to.\n\nIn some databases, a single line cannot conveniently hold all the information in one entry. In such cases, you can use multiline records. The first step in doing this is to choose your data format. One technique is to use an unusual character or string to separate records. For example, you could use the formfeed character (written ‘ ’ in , as in C) to separate them, making each record a page of the file. To do this, just set the variable to (a string containing the formfeed character). Any other character could equally well be used, as long as it won’t be part of the data in a record. Another technique is to have blank lines separate records. By a special dispensation, an empty string as the value of indicates that records are separated by one or more blank lines. When is set to the empty string, each record always ends at the first blank line encountered. The next record doesn’t start until the first nonblank line that follows. No matter how many blank lines appear in a row, they all act as one record separator. (Blank lines must be completely empty; lines that contain only whitespace do not count.) You can achieve the same effect as ‘ ’ by assigning the string to . This regexp matches the newline at the end of the record and one or more blank lines after the record. In addition, a regular expression always matches the longest possible sequence when there is a choice (see How Much Text Matches?). So, the next record doesn’t start until the first nonblank line that follows—no matter how many blank lines appear in a row, they are considered one record separator. However, there is an important difference between ‘ ’ and ‘ ’. In the first case, leading newlines in the input datafile are ignored, and if a file ends without extra blank lines after the last record, the final newline is removed from the record. In the second case, this special processing is not done. (d.c.) Now that the input is separated into records, the second step is to separate the fields in the records. One way to do this is to divide each of the lines into fields in the normal manner. This happens by default as the result of a special feature. When is set to the empty string and is set to a single character, the newline character always acts as a field separator. This is in addition to whatever field separations result from .[ ] The original motivation for this special exception was probably to provide useful behavior in the default case (i.e., is equal to ). This feature can be a problem if you really don’t want the newline character to separate fields, because there is no way to prevent it. However, you can work around this by using the function to break up the record manually (see String-Manipulation Functions). If you have a single-character field separator, you can work around the special feature in a different way, by making into a regexp for that single character. For example, if the field separator is a percent character, instead of ‘ ’, use ‘ ’. Another way to separate fields is to put each field on a separate line: to do this, just set the variable to the string . (This single-character separator matches a single newline.) A practical example of a datafile organized this way might be a mailing list, where blank lines separate the entries. Consider a mailing list in a file named , which looks like this: A simple program to process this file is as follows: # addrs.awk --- simple mailing list program # Records are separated by blank lines. # Each line is one field. BEGIN { RS = \"\" ; FS = \"\n\n\" } { print \"Name is:\", $1 print \"Address is:\", $2 print \"City and State are:\", $3 print \"\" } Running the program produces the following output: $ Name is: Jane Doe Address is: 123 Main Street City and State are: Anywhere, SE 12345-6789 Name is: John Smith Address is: 456 Tree-lined Avenue City and State are: Smallville, MW 98765-4321 … See Printing Mailing Labels for a more realistic program dealing with address lists. The following list summarizes how records are split, based on the value of : Records are separated by the newline character (‘ ’). In effect, every line in the datafile is a separate record, including blank lines. This is the default. Records are separated by each occurrence of the character. Multiple successive occurrences delimit empty records. Records are separated by runs of blank lines. When is a single character, then the newline character always serves as a field separator, in addition to whatever value may have. Leading and trailing newlines in a file are ignored. Records are separated by occurrences of characters that match . Leading and trailing matches of delimit empty records. (This is a extension; it is not specified by the POSIX standard.) If not in compatibility mode (see Command-Line Options), sets to the input text that matched the value specified by . But if the input file ended without any text that matches , then sets to the null string.\n\nSo far we have been getting our input data from ’s main input stream—either the standard input (usually your keyboard, sometimes the output from another program) or the files specified on the command line. The language has a special built-in command called that can be used to read input under your explicit control. The command is used in several different ways and should not be used by beginners. The examples that follow the explanation of the command include material that has not been covered yet. Therefore, come back and study the command after you have reviewed the rest of Parts I and II and have a good knowledge of how works. The command returns 1 if it finds a record and 0 if it encounters the end of the file. If there is some error in getting a record, such as a file that cannot be opened, then returns −1. In this case, sets the variable to a string describing the error that occurred. In the following examples, stands for a string value that represents a shell command. When is specified (see Command-Line Options), reading lines from files, pipes, and coprocesses is disabled. Using getline with No Arguments The command can be used without arguments to read input from the current input file. All it does in this case is read the next input record and split it up into fields. This is useful if you’ve finished processing the current record, but want to do some special processing on the next record right now. For example: # Remove text between /* and */, inclusive { if ((i = index($0, \"/*\")) != 0) { out = substr($0, 1, i - 1) # leading part of the string rest = substr($0, i + 2) # ... */ ... j = index(rest, \"*/\") # is */ in trailing part? if (j > 0) { rest = substr(rest, j + 2) # remove comment } else { while (j == 0) { # get more text if (getline <= 0) { print(\"unexpected EOF or error:\", ERRNO) > \"/dev/stderr\" exit } # build up the line using string concatenation rest = rest $0 j = index(rest, \"*/\") # is */ in trailing part? if (j != 0) { rest = substr(rest, j + 2) break } } } # build up the output line using string concatenation $0 = out rest } print $0 } This program deletes C-style comments (‘ ’) from the input. It uses a number of features we haven’t covered yet, including string concatenation (see String Concatenation) and the and built-in functions (see String-Manipulation Functions). By replacing the ‘ ’ with other statements, you could perform more complicated processing on the decommented input, such as searching for matches of a regular expression. (This program has a subtle problem—it does not work if one comment ends and another begins on the same line.) This form of the command sets , , , , and the value of . The new value of is used to test the patterns of any subsequent rules. The original value of that triggered the rule that executed is lost. By contrast, the statement reads a new record but immediately begins processing it normally, starting with the first rule in the program. See The next Statement. You can use ‘ ’ to read the next record from ’s input into the variable . No other processing is done. For example, suppose the next line is a comment or a special string, and you want to read it without triggering any rules. This form of allows you to read that line and store it in a variable so that the main read-a-line-and-check-each-rule loop of never sees it. The following example swaps every two lines of input: It takes the following list: The command used in this way sets only the variables , , and (and, of course, ). The record is not split into fields, so the values of the fields (including ) and the value of do not change. Use ‘ ’ to read the next record from . Here, is a string-valued expression that specifies the filename. ‘ ’ is called a redirection because it directs input to come from a different place. For example, the following program reads its input record from the file when it encounters a first field with a value equal to 10 in the current input file: Because the main input stream is not used, the values of and are not changed. However, the record it reads is split into fields in the normal manner, so the values of and the other fields are changed, resulting in a new value of . is also set. According to POSIX, ‘ ’ is ambiguous if contains unparenthesized operators other than ‘ ’; for example, ‘ ’ is ambiguous because the concatenation operator (not discussed yet; see String Concatenation) is not parenthesized. You should write it as ‘ ’ if you want your program to be portable to all implementations. Using getline into a Variable from a File Use ‘ ’ to read input from the file , and put it in the variable . As earlier, is a string-valued expression that specifies the file from which to read. In this version of , none of the predefined variables are changed and the record is not split into fields. The only variable changed is .[ ] For example, the following program copies all the input files to the output, except for records that say ‘ ’. Such a record is replaced by the contents of the file : { if (NF == 2 && $1 == \"@include\") { while ((getline line < $2) > 0) print line close($2) } else print } Note here how the name of the extra input file is not built into the program; it is taken directly from the data, specifically from the second field on the line. The function is called to ensure that if two identical lines appear in the input, the entire specified file is included twice. See Closing Input and Output Redirections. One deficiency of this program is that it does not process nested statements (i.e., statements in included files) the way a true macro preprocessor would. See An Easy Way to Use Library Functions for a program that does handle nested statements. Omniscience has much to recommend it. Failing that, attention to details would be useful. The output of a command can also be piped into , using ‘ ’. In this case, the string is run as a shell command and its output is piped into to be used as input. This form of reads one record at a time from the pipe. For example, the following program copies its input to its output, except for lines that begin with ‘ ’, which are replaced by the output produced by running the rest of the line as a shell command: { if ($1 == \"@execute\") { tmp = substr($0, 10) # Remove \"@execute\" while ((tmp | getline) > 0) print close(tmp) } else print } The function is called to ensure that if two identical ‘ ’ lines appear in the input, the command is run for each one. Given the input: Notice that this program ran the command and printed the result. (If you try this program yourself, you will of course get different results, depending upon who is logged in on your system.) This variation of splits the record into fields, sets the value of , and recomputes the value of . The values of and are not changed. is set. According to POSIX, ‘ ’ is ambiguous if contains unparenthesized operators other than ‘ ’—for example, ‘ ’ is ambiguous because the concatenation operator is not parenthesized. You should write it as ‘ ’ if you want your program to be portable to all implementations. Unfortunately, has not been consistent in its treatment of a construct like ‘ ’. Most versions, including the current version, treat it at as ‘ ’. (This is also how BWK behaves.) Some versions instead treat it as ‘ ’. (This is how behaves.) In short, always use explicit parentheses, and then you won’t have to worry. Using getline into a Variable from a Pipe When you use ‘ ’, the output of is sent through a pipe to and into the variable . For example, the following program reads the current date and time into the variable , using the utility, and then prints it: In this version of , none of the predefined variables are changed and the record is not split into fields. However, is set. Reading input into from a pipe is a one-way operation. The command that is started with ‘ ’ only sends data to your program. On occasion, you might want to send data to another program for processing and then read the results back. allows you to start a coprocess, with which two-way communications are possible. This is done with the ‘ ’ operator. Typically, you write data to the coprocess first and then read the results back, as shown in the following: which sends a query to and then reads the results. The values of and are not changed, because the main input stream is not used. However, the record is split into fields in the normal manner, thus changing the values of , of the other fields, and of and . Coprocesses are an advanced feature. They are discussed here only because this is the section on . See Two-Way Communications with Another Process, where coprocesses are discussed in more detail. Using getline into a Variable from a Coprocess When you use ‘ ’, the output from the coprocess is sent through a two-way pipe to and into the variable . In this version of , none of the predefined variables are changed and the record is not split into fields. The only variable changed is . However, is set. Here are some miscellaneous points about that you should bear in mind:\n• None When changes the value of and , does not automatically jump to the start of the program and start testing the new record against every pattern. However, the new record is tested against any subsequent rules.\n• None Some very old implementations limit the number of pipelines that an program may have open to just one. In , there is no such limit. You can open as many pipelines (and coprocesses) as the underlying operating system permits.\n• None An interesting side effect occurs if you use without a redirection inside a rule. Because an unredirected reads from the command-line datafiles, the first command causes to set the value of . Normally, does not have a value inside rules, because you have not yet started to process the command-line datafiles. (d.c.) (See The BEGIN and END Special Patterns; also see Built-in Variables That Convey Information.)\n• None Using with (‘ ’) is likely to be a source of confusion. opens a separate input stream from the current input file. However, by not using a variable, and are still updated. If you’re doing this, it’s probably by accident, and you should reconsider what it is you’re trying to accomplish.\n• None The next section presents a table summarizing the variants and which variables they can affect. It is worth noting that those variants that do not use redirection can cause to be updated if they cause to start reading a new input file.\n• None If the variable being assigned is an expression with side effects, different versions of behave differently upon encountering end-of-file. Some versions don’t evaluate the expression; many versions (including ) do. Here is an example, courtesy of Duncan Moore: Here, the side effect is the ‘ ’. Is incremented if end-of-file is encountered before the element in is assigned? treats like a function call, and evaluates the expression ‘ ’ before attempting to read from . However, some versions of only evaluate the expression once they know that there is a string value to be assigned. Table 4-1 summarizes the eight variants of , listing which predefined variables are set by each one, and whether the variant is standard or a extension. Note: for each variant, sets the predefined variable. Table 4-1. getline variants and what they set\n\nThis section describes a feature that is specific to . You may specify a timeout in milliseconds for reading input from the keyboard, a pipe, or two-way communication, including TCP/IP sockets. This can be done on a per-input, per-command, or per-connection basis, by setting a special element in the array (see Built-in Variables That Convey Information): When set, this causes to time out and return failure if no data is available to read within the specified timeout period. For example, a TCP client can decide to give up on receiving any response from the server after a certain amount of time: Service = \"/inet/tcp/0/localhost/daytime\" PROCINFO[Service, \"READ_TIMEOUT\"] = 100 if ((Service |& getline) > 0) print $0 else if (ERRNO != \"\") print ERRNO Here is how to read interactively from the user[ ] without waiting for more than five seconds: terminates the read operation if input does not arrive after waiting for the timeout period, returns failure, and sets to an appropriate string value. A negative or zero value for the timeout is the same as specifying no timeout at all. A timeout can also be set for reading from the keyboard in the implicit loop that reads input records and matches them against patterns, like so: In this case, failure to respond within five seconds results in the following error message: The timeout can be set or changed at any time, and will take effect on the next attempt to read from the input device. In the following example, we start with a timeout value of one second, and progressively reduce it by one-tenth of a second until we wait indefinitely for the input to arrive: You should not assume that the read operation will block exactly after the tenth record has been printed. It is possible that will read and buffer more than one record’s worth of data the first time. Because of this, changing the value of timeout like the preceding example is not very useful. If the element is not present and the environment variable exists, uses its value to initialize the timeout value. The exclusive use of the environment variable to specify timeout has the disadvantage of not being able to control it on a per-command or per-connection basis. considers a timeout event to be an error even though the attempt to read from the underlying device may succeed in a later attempt. This is a limitation, and it also means that you cannot use this to multiplex input from two or more sources. Assigning a timeout value prevents read operations from blocking indefinitely. But bear in mind that there are other ways can stall waiting for an input device to be ready. A network client can sometimes take a long time to establish a connection before it can start reading any data, or the attempt to open a FIFO special file for reading can block indefinitely until some other process opens it for writing."
    },
    {
        "link": "https://stackoverflow.com/questions/1393489/using-awk-to-process-a-file-where-each-record-has-different-fixed-width-fields",
        "document": "I have some data files from a legacy system that I would like to process using Awk. Each file consists of a list of records. There are several different record types and each record type has a different set of fixed-width fields (there is no field separator character). The first two characters of the record indicate the type, from this you then know which fields should follow. A file might look something like this:\n\nUsing Gawk I can set the FIELDWIDTHS, but that applies to the whole file (unless I am missing some way of setting this on a record-by-record basis), or I can set FS to \"\" and process the file one character at a time, but that's a bit cumbersome.\n\nIs there a good way to extract the fields from such a file using Awk?\n\nEdit: Yes, I could use Perl (or something else). I'm still keen to know whether there is a sensible way of doing it with Awk though."
    }
]