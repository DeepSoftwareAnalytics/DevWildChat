[
    {
        "link": "https://geeksforgeeks.org/linux-system-call-in-detail",
        "document": "A system call is a procedure that provides the interface between a process and the operating system. It is the way by which a computer program requests a service from the kernel of the operating system.\n\nIn Linux, making a system call involves transferring control from unprivileged user mode to privileged kernel mode; the details of this transfer vary from architecture to architecture. The libraries take care of collecting the system-call arguments and, if necessary, arranging those arguments in the special form necessary to make the system call.\n\nSystem calls are divided into 5 categories mainly :\n\nThis system calls perform the task of process creation, process termination, etc.\n\nThe Linux System calls under this are fork() , exit() , exec().\n• fork()\n• A new process is created by the fork() system call.\n• A new process may be created with fork() without a new program being run-the new sub-process simply continues to execute exactly the same program that the first (parent) process was running.\n• It is one of the most widely used system calls under process management.\n• exit()\n• The exit() system call is used by a program to terminate its execution.\n• The operating system reclaims resources that were used by the process after the exit() system call.\n• exec()\n• A new program will start executing after a call to exec()\n• Running a new program does not require that a new process be created first: any process may call exec() at any time. The currently running program is immediately terminated, and the new program starts executing in the context of the existing process.\n\nFile management system calls handle file manipulation jobs like creating a file, reading, and writing, etc. The Linux System calls under this are open(), read(), write(), close().\n• open():\n• It is the system call to open a file.\n• This system call just opens the file, to perform operations such as read and write, we need to execute different system call to perform the operations.\n• read():\n• This system call opens the file in reading mode\n• We can not edit the files with this system call.\n• Multiple processes can execute the read() system call on the same file simultaneously.\n• write():\n• This system call opens the file in writing mode\n• We can edit the files with this system call.\n• Multiple processes can not execute the write() system call on the same file simultaneously.\n\nDevice management does the job of device manipulation like reading from device buffers, writing into device buffers, etc. The Linux System calls under this is ioctl().\n• ioctl():\n• ioctl() is referred to as Input and Output Control.\n• ioctl is a system call for device-specific input/output operations and other operations which cannot be expressed by regular system calls.\n\nIt handles information and its transfer between the OS and the user program. In addition, OS keeps the information about all its processes and system calls are used to access this information. The System calls under this are getpid(), alarm(), sleep().\n• getpid():\n• getpid stands for Get the Process ID.\n• The getpid() function shall return the process ID of the calling process.\n• The getpid() function shall always be successful and no return value is reserved to indicate an error.\n• alarm():\n• This system call sets an alarm clock for the delivery of a signal that when it has to be reached.\n• It arranges for a signal to be delivered to the calling process.\n• sleep():\n• This System call suspends the execution of the currently running process for some interval of time\n• Meanwhile, during this interval, another process is given chance to execute\n\nThese types of system calls are specially used for inter-process communications.\n\nTwo models are used for inter-process communication\n• Message Passing(processes exchange messages with one another)\n\nThe system calls under this are pipe() , shmget() ,mmap().\n• pipe():\n• The pipe() system call is used to communicate between different Linux processes.\n• It is mainly used for inter-process communication.\n• The pipe() system function is used to open file descriptors.\n• shmget():\n• It is mainly used for Shared memory communication.\n• This system call is used to access the shared memory and access the messages in order to communicate with the process.\n• mmap():\n• This function call is used to map or unmap files or devices into memory.\n• The mmap() system call is responsible for mapping the content of the file to the virtual memory space of the process.\n\n\n\nThese are the various system calls involved in LINUX operating system."
    },
    {
        "link": "https://blog.packagecloud.io/the-definitive-guide-to-linux-system-calls",
        "document": "This blog post explains how Linux programs call functions in the Linux kernel. It will outline several different methods of making systems calls, how to handcraft your own assembly to make system calls (examples included), kernel entry points into system calls, kernel exit points from system calls, glibc wrappers, bugs, and much, much more.\n\nHere is a summary of the topics this blog post will cover,\n• Calling system calls with assembly is a bad idea - A word of caution about calling system calls with assembly\n• \n• None Using legacy system calls with your own assembly\n• \n• \n• None Using system calls with your own assembly\n• \n• None Using system calls with your own assembly\n\nWhen you run a program which calls , , , (and many others) you are making a system call.\n\nSystem calls are how a program enters the kernel to perform some task. Programs use system calls to perform a variety of operations such as: creating processes, doing network and file IO, and much more.\n\nYou can find a list of system calls by checking the man page for syscalls(2).\n\nThere are several different ways for user programs to make system calls and the low-level instructions for making a system call vary among CPU architectures.\n\nAs an application developer, you don’t typically need to think about how exactly a system call is made. You simply include the appropriate header file and make the call as if it were a normal function.\n\nprovides wrapper code which abstracts you away from the underlying code which arranges the arguments you’ve passed and enters the kernel.\n\nBefore we can dive into the details of how system calls are made, we’ll need to define some terms and examine some core ideas that will appear later.\n\nThis blog post makes the following assumptions that:\n• You are using a 32-bit or 64-bit Intel or AMD CPU. The discussion about the methods may be useful for people using other systems, but the code samples below contain CPU-specific code.\n• You are interested in the Linux kernel, version 3.13.0. Other kernel versions will be similar, but the exact line numbers, organization of code, and file paths will vary. Links to the 3.13.0 kernel source tree on GitHub are provided.\n• You are interested in or derived libc implementations (e.g., ).\n\nx86-64 in this blog post will refer to 64bit Intel and AMD CPUs that are based on the x86 architecture.\n\nUser programs (like your editor, terminal, ssh daemon, etc) need to interact with the Linux kernel so that the kernel can perform a set of operations on behalf of your user programs that they can’t perform themselves.\n\nFor example, if a user program needs to do some sort of IO ( , , , etc) or modify its address space ( , , etc) it must trigger the kernel to run to complete those actions on its behalf.\n\nWhat prevents user programs from performing these actions themselves?\n\nIt turns out that the x86-64 CPUs have a concept called privilege levels. Privilege levels are a complex topic suitable for their own blog post. For the purposes of this post, we can (greatly) simplify the concept of privilege levels by saying:\n• Privilege levels are a means of access control. The current privilege level determines which CPU instructions and IO may be performed.\n• The kernel runs at the most privileged level, called “Ring 0”. User programs run at a lesser level, typically “Ring 3”.\n\nIn order for a user program to perform some privileged operation, it must cause a privilege level change (from “Ring 3” to “Ring 0”) so that the kernel can execute.\n\nThere are several ways to cause a privilege level change and trigger the kernel to perform some action.\n\nLet’s start with a common way to cause the kernel to execute: interrupts.\n\nYou can think of an interrupt as an event that is generated (or “raised”) by hardware or software.\n\nA hardware interrupt is raised by a hardware device to notify the kernel that a particular event has occurred. A common example of this type of interrupt is an interrupt generated when a NIC receives a packet.\n\nA software interrupt is raised by executing a piece of code. On x86-64 systems, a software interrupt can be raised by executing the instruction.\n\nInterrupts usually have numbers assigned to them. Some of these interrupt numbers have a special meaning.\n\nYou can imagine an array that lives in memory on the CPU. Each entry in this array maps to an interrupt number. Each entry contains the address of a function that the CPU will begin executing when that interrupt is received along with some options, like what privilege level the interrupt handler function should be executed in.\n\nHere’s a photo from the Intel CPU manual showing the layout of an entry in this array:\n\nIf you look closely at the diagram, you can see a 2-bit field labeled DPL (Descriptor Privilege Level). The value in this field determines the minimum privilege level the CPU will be in when the handler function is executed.\n\nThis is how the CPU knows which address it should execute when a particular type of event is received and what privilege level the handler for that event should execute in.\n\nIn practice, there are lots of different ways to deal with interrupts on x86-64 systems. If you are interested in learning more read about the 8259 Programmable Interrupt Controller, Advanced Interrupt Controllers, and IO Advanced Interrupt Controllers.\n\nThere are other complexities involved with dealing with both hardware and software interrupts, such as interrupt number collisions and remapping.\n\nWe don’t need to concern ourselves with these details for this discussion about system calls.\n\nModel Specific Registers (also known as MSRs) are control registers that have a specific purpose to control certain features of the CPU. The CPU documentation lists the addresses of each of the MSRs.\n\nYou can use the CPU instructions to to read and write MSRs, respectively.\n\nThere are also command line tools which allow you to read and write MSRs, but doing this is not recommended as changing these values (especially while a system is running) is dangerous unless you are really careful.\n\nIf you don’t mind potentially destabilizing your system or irreversibly corrupting your data, you can read and write MSRs by installing and loading the kernel module:\n\nSome of the system call methods we’ll see later make use of MSRs, as we’ll see soon.\n\nIt’s not a great idea to call system calls by writing your own assembly code.\n\nOne big reason for this is that some system calls have additional code that runs in glibc before or after the system call runs.\n\nIn the examples below, we’ll be using the system call. It turns out that you can register functions to run when is called by a program by using .\n\nThose functions are called from glibc, not the kernel. So, if you write your own assembly to call as we show below, your registered handler functions won’t be executed since you are bypassing glibc.\n\nNevertheless, manually making system calls with assembly is a good learning experience.\n\nUsing our prerequisite knowledge we know two things:\n• We know that we can trigger the kernel to execute by generating a software interrupt.\n• We can generate a software interrupt with the assembly instruction.\n\nCombining these two concepts leads us to the legacy system call interface on Linux.\n\nThe Linux kernel sets aside a specific software interrupt number that can be used by user space programs to enter the kernel and execute a system call.\n\nThe Linux kernel registers an interrupt handler named for the interrupt number: 128 (0x80). Let’s take a look at the code that actually does this.\n\nFrom the function in the kernel 3.13.0 source in :\n\nWhere is a defined as in .\n\nBut, if the kernel reserves a single software interrupt that userland programs can raise to trigger the kernel, how does the kernel know which of the many system calls it should execute?\n\nThe userland program is expected to put the system call number in the register. The arguments for the syscall itself are to be placed in the remaining general purpose registers.\n\nOne place this is documented is in a comment in :\n\nNow that we know how to make a system call and where the arguments should live, let’s try to make one by writing some inline assembly.\n\nUsing legacy system calls with your own assembly\n\nTo make a legacy system call, you can write a small bit of inline assembly. While this is interesting from a learning perspective, I encourage readers to never make system calls by crafting their own assembly.\n\nIn this example, we’ll try calling the system call, which takes a single argument: the exit status.\n\nFirst, we need to find the system call number for . The Linux kernel includes a file which lists each system call in a table. This file is processed by various scripts at build time to generate header files which can be used by user programs.\n\nLet’s look at the table found in :\n\nThe syscall is number . According to the interface described above, we just need to move the syscall number into the register and the first argument (the exit status) into .\n\nHere’s a piece of C code with some inline assembly that does this. Let’s set the exit status to “42”:\n\nNext, compile, execute, and check the exit status:\n\nSuccess! We called the system call using the legacy system call method by raising a software interrupt.\n\nSo now that we’ve seen how to trigger a system call from a userland program, let’s see how the kernel uses the system call number to execute the system call code.\n\nRecall from the previous section that the kernel registered a syscall handler function called .\n\nThis function is implemented in assembly in and we can see several things happening in this function, the most important of which is the call to the actual syscall itself:\n\nis a macro which rearranges the legacy arguments so that they may be properly understood by the current system call layer.\n\nThe identifier refers to a table which is defined in . Note the line toward the end of the code:\n\nRecall earlier we saw the syscall table defined in .\n\nThere are a few scripts which run at compile time which take this table and generate the file from it. The generated header file is comprised of valid C code, which is simply inserted with the shown above to fill in with function addresses indexed by system call number.\n\nAnd this is how you enter the kernel via a legacy system call.\n\nWe’ve seen how to enter the kernel with a software interrupt, but how does the kernel return back to the user program and drop the privilege level after it has finished running?\n\nIf we turn to the (warning: large PDF) Intel Software Developer’s Manual we can find a helpful diagram that illustrates how the program stack will be arranged when a privilege level change occurs.\n\nWhen execution is transferred to the kernel function via the execution of a software interrupt from a user program, a privilege level change occurs. The result is that the stack when is entered will look like the diagram above.\n\nThis means that the return address and the CPU flags which encode the privilege level (and other stuff), and more are all saved on the program stack before executes.\n\nSo, in order to resume execution the kernel just needs to copy these values from the program stack back into the registers where they belong and execution will resume back in userland.\n\nOK, so how do you do that?\n\nThere’s a few ways to do that, but one of the easiest ways is to the use the instruction.\n\nThe Intel instruction set manual explains that the instruction pops the return address and saved register values from the stack in the order they were prepared:\n\nFinding this code in the Linux kernel is a bit difficult as it is hidden beneath several macros and there is extensive care taken to deal with things like signals and ptrace system call exit tracking.\n\nEventually all the macros in the assembly stubs in the kernel reveal the which returns from a system call back to a user program.\n\nWhere is defined in as .\n\nAnd now you know how legacy system calls work.\n\nThe legacy method seems pretty reasonable, but there are newer ways to trigger a system call which don’t involve a software interrupt and are much faster than using a software interrupt.\n\nEach of the two faster methods is comprised of two instructions. One to enter the kernel and one to leave. Both methods are described in the Intel CPU documentation as “Fast System Call”.\n\nUnfortunately, Intel and AMD implementations have some disagreement on which method is valid when a CPU is in 32bit or 64bit mode.\n\nIn order to maximize compatibility across both Intel and AMD CPUs:\n• On 32bit systems use: and .\n• On 64bit systems use: and .\n\nUsing to make a system call is more complicated than using the legacy interrupt method and involves more coordination between the user program (via ) and the kernel.\n\nLet’s take it one step at a time and sort out the details. First, let’s see what the documentation in the Intel Instruction Set Reference (warning very large PDF) says about the and how to use it.\n\nIn other words: in order for the kernel to receive incoming system calls with , the kernel must set 3 Model Specific Registers (MSRs). The most interesting MSR in our case is (which has the address 0x176). This MSR is where the kernel should specify the address of the function that will execute when a instruction is executed by a user program.\n\nWe can find the code in the Linux kernel which writes to the MSR in :\n\nWhere is defined as a .\n\nMuch like the legacy software interrupt syscalls, there is a defined convention for making system calls with .\n\nOne place this is documented is in a comment in :\n\nRecall that the legacy system call method includes a mechanism for returning back to the userland program which was interrupted: the instruction.\n\nCapturing the logic needed to make work properly is complicated because unlike software interrupts, does not store the return address.\n\nHow, exactly, the kernel does this and other bookkeeping prior to executing a instruction can change over time (and it has changed, as you will see in the Bugs section below).\n\nIn order to protect against future changes, user programs are intended to use a function called which is implemented in the kernel, but mapped into each user process when the process is started.\n\nThis is a bit odd; it’s code that comes with the kernel, but runs in userland.\n\nIt turns out that is part of something called a virtual Dynamic Shared Object (vDSO) which exists to allow programs to execute kernel code in userland.\n\nWe’ll examine what the vDSO is, what it does, and how it works in depth later.\n\nFor now, let’s examine the internals.\n\nThe function that encapulates the calling convention can be found in :\n\nis part of a Dynamic Shared Object (also known as a shared library) how does a user program locate the address of that function at runtime?\n\nThe address of the function is written into an ELF auxilliary vector where a user program or library (typically ) can find it and use it.\n\nThere are a few methods for searching ELF auxilliary vectors:\n• By using with the argument.\n• By iterating to the end of the environment variables and parsing them from memory.\n\nOption 1 is the simplest option, but does not exist on prior to 2.16. The example code shown below illustrates option 2.\n\nAs we can see in the code above, does some bookkeeping before executing .\n\nSo, all we need to do to manually enter the kernel with is:\n• Search the ELF auxilliary vectors for where the address of is written.\n• Put the system call number and arguments into the registers as we would normally for legacy system calls\n\nYou should absolutely never write your own wrapper function as the convention the kernel uses to enter and leave system calls with can change and your code will break.\n\nYou should always start a system call by calling through .\n\nSo, lets do that.\n\nUsing system calls with your own assembly\n\nKeeping with our legacy system call example from earlier, we’ll call with an exit status of .\n\nThe syscall is number . According to the interface described above, we just need to move the syscall number into the register and the first argument (the exit status) into .\n\nNext, compile, execute, and check the exit status:\n\nSuccess! We called the system call using the legacy sysenter method without raising a software interrupt.\n\nSo now that we’ve seen how to trigger a system call from a userland program with via , let’s see how the kernel uses the system call number to execute the system call code.\n\nRecall from the previous section that the kernel registered a syscall handler function called .\n\nThis function is implemented in assembly in . Let’s take a look at where the value in the eax register is used to execute the system call:\n\nThis is identical code as we saw in the legacy system call mode: a table named which is indexed into with the system call number.\n\nAfter all the needed bookkeeping is done both the legacy system call model and the system call model use the same mechanism and system call table for dispatching system calls.\n\nRefer to the entry point section to learn where the is defined and how it is constructed.\n\nAnd this is how you enter the kernel via a system call.\n\nThe kernel can use the instruction to resume execution back to the user program.\n\nUsing this instruction is not as straight forward as using . The caller is expected to put the address to return to into the register, and to put the pointer to the program stack to use in the register.\n\nThis means that your software must compute the address where execution should be resumed, preserve that value, and restore it prior to calling .\n\nWe can find the code which does this in: :\n\nis a macro which is defined in which contains the instruction.\n\nAnd now you know how 32-bit fast system calls work.\n\nNext up on our journey are 64-bit fast system calls. These system calls use the instructions and to enter and return from a system call, respectively.\n\nThe documentation in the Intel Instruction Set Reference (very large PDF) explains how the instruction works:\n\nIn other words: for the kernel to receive incoming system calls, it must register the address of the code that will execute when a system call occurs by writing its address to the MSR.\n\nWe can find that code in the kernel in :\n\nWhere is defined as in .\n\nMuch like the legacy software interrupt syscalls, there is a defined convention for making system calls with .\n\nThe userland program is expected to put the system call number to be in the register. The arguments to the syscall are expected to be placed in a subset of the general purpose registers.\n\nThis is documented in the x86-64 ABI in section A.2.1:\n\nThis is also documented in a comment in .\n\nNow that we know how to make a system call and where the arguments should live, let’s try to make one by writing some inline assembly.\n\nUsing system calls with your own assembly\n\nBuilding on the previous example, let’s build a small C program with inline assembly which executes the exit system call passing the exit status of 42.\n\nFirst, we need to find the system call number for . In this case we need to read the table found in :\n\nThe syscall is number . According to the interface described above, we just need to move into the register and the first argument (the exit status) into .\n\nHere’s a piece of C code with some inline assembly that does this. Like the previous example, this example is more wordy than necessary in the interest of clarity:\n\nNext, compile, execute, and check the exit status:\n\nSuccess! We called the system call using the system call method. We avoided raising a software interrupt and (if we were timing a micro-benchmark) it executes much faster.\n\nNow we’ve seen how to trigger a system call from a userland program, let’s see how the kernel uses the system call number to execute the system call code.\n\nRecall from the previous section we saw the address of a function named get written to the MSR.\n\nLet’s take a look at the code for this function and see how it uses to actually hand off execution to the system call, from :\n\nMuch like the legacy system call method, is a table defined in a C file that uses to pull in C code generated by a script.\n\nFrom , note the at the bottom:\n\nEarlier we saw the syscall table defined in . Exactly like the legacy interrupt mode, a script runs at kernel compile time and generates the file from the table in .\n\nThe code above simply includes the generated C code producing an array of function pointers indexed by system call number.\n\nAnd this is how you enter the kernel via a system call.\n\nThe kernel can use the instruction to resume execution back to where execution left off when the user program used .\n\nis simpler than because the address to where execution should be resume is copied into the register when is used.\n\nAs long as you preserve that value somewhere and restore it to before calling , execution will resume where it left off before the call to .\n\nThis is convenient because requires that you compute this address yourself in addition to clobbering an additional register.\n\nWe can find the code which does this in :\n\nis a macro which is defined in which contains the instruction.\n\nAnd now you know how 64-bit fast system calls work.\n\nGreat, we’ve seen how to call system calls manually by crafting assembly for a few different system call methods.\n\nUsually, you don’t need to write your own assembly. Wrapper functions are provided by glibc that handle all of the assembly code for you.\n\nThere are some system calls, however, for which no glibc wrapper exists. One example of a system call like this is , the fast userspace locking system call.\n\nBut, wait, why does no system call wrapper exist for ?\n\nis intended only to be called by libraries, not application code, and thus in order to call you must do it by:\n• Generating assembly stubs for every platform you want to support\n• Using the wrapper provided by glibc\n\nIf you find yourself in the situation of needing to call a system call for which no wrapper exists, you should definitely choose option 2: use the function from glibc.\n\nLet’s use from glibc to call with exit status of :\n\nNext, compile, execute, and check the exit status:\n\nSuccess! We called the system call using the wrapper from glibc.\n\nLet’s take a look at the wrapper function we used in the previous example to see how it works in glibc.\n\nEarlier we showed an excerpt from the x86_64 ABI document that describes both userland and kernel calling conventions.\n\nThis assembly stub is cool because it shows both calling conventions. The arguments passed into this function follow the userland calling convention, but are then moved to a different set of registers to obey the kernel calling convention prior to entering the kernel with .\n\nThis is how the glibc syscall wrapper works when you use it to call system calls that do not come with a wrapper by default.\n\nWe’ve now covered all the methods of making a system call by entering the kernel and shown how you can make those calls manually (or semi-manually) to transition the system from userland to the kernel.\n\nWhat if programs could call certain system calls without entering the kernel at all?\n\nThat’s precisely why the Linux virtual Dynamic Shared Object (vDSO) exists. The Linux vDSO is a set of code that is part of the kernel, but is mapped into the address space of a user program to be run in userland.\n\nThe idea is that some system calls can be used without entering the kernel. One such call is: .\n\nPrograms calling the system call do not actually enter the kernel. They instead make a simple function call to a piece of code that was provided by the kernel, but is run in userland.\n\nNo software interrupt is raised, no complicated or bookkeeping is required. is just a normal function call.\n\nYou can see the vDSO listed as the first entry when you use :\n\nLet’s see how the vDSO is setup in the kernel.\n\nYou can find the vDSO source in . There are a few assembly and C source files along with a linker script.\n\nThe linker script is a cool thing to take a look at.\n\nLinker scripts are pretty useful, but not particularly very well known. This linker script arranges the symbols that are going to be exported in the vDSO.\n\nWe can see that vDSO exports 4 different functions, each with two names. You can find the source for these functions in the C files in this directory.\n\nFor example, the source for found in :\n\nThis is defining to be a weak alias for .\n\nThe function in the same file contains the actual source which will be executed in user land when a user program calls the system call.\n\nDue to address space layout randomization the vDSO will be loaded at a random address when a program is started.\n\nHow can user programs find the vDSO if its loaded at a random address?\n\nIf you recall earlier when examining the system call method we saw that user programs should call instead of writing their own assembly code themselves.\n\nThis function is part of the vDSO, as well.\n\nThe sample code provided located by searching the ELF auxilliary headers to find a header with type which contained the address of .\n\nSimilarly, to locate the vDSO, a user program can search for an ELF auxilliary header of type . It will contain the address of the start of the ELF header for the vDSO that was generated by a linker script.\n\nIn both cases, the kernel writes the address in to the ELF header when the program is loaded. That’s how the correct addresses always end up in and .\n\nOnce that header is located, user programs can parse the ELF object (perhaps using libelf) and call the functions in the ELF object as needed.\n\nThis is nice because this means that the vDSO can take advantage of some useful ELF features like symbol versioning.\n\nAn example of parsing and calling functions in the vDSO is provided in the kernel documentation in .\n\nMost of the time, people access the vDSO without knowing it because abstracts this away from them by using the interface described in the previous section.\n\nWhen a program is loaded, the dynamic linker and loader loads the DSOs that the program depends on, including the vDSO.\n\nstores some data about the location of the vDSO when it parses the ELF headers of the program that is being loaded. It also includes short stub functions that will search the vDSO for a symbol name prior to making an actual system call.\n\nFor example, the function in , from :\n\nThis code in searches the vDSO for the function and returns the address. This is wrapped up nicely with an indirect function.\n\nThat’s how programs calling pass through and hit the vDSO all without switching into kernel mode, incurring a privilege level change, or raising a software interrupt.\n\nAnd, that concludes the showcase of every single system call method available on Linux for 32-bit and 64-bit Intel and AMD CPUs.\n\nWhile we’re talking about system calls ;) it makes sense to briefly mention how deals with system calls.\n\nFor many system calls, simply needs a wrapper function where it moves arguments into the proper registers and then executes the or instructions, or calls .\n\nIt does this by using a series of tables defined in text files that are processed with scripts and output C code.\n\nFor example, the file describes some common system calls:\n\nTo learn more about each column, check the comments in the script which processes this file: .\n\nMore complex system calls, like which invokes handlers have actual implementations in C or assembly code and will not be found in a templated text file like this.\n\nFuture blog posts will explore the implementation in and the linux kernel for interesting system calls.\n\nIt would be unfortunate not to take this opportunity to mention two fabulous bugs related to system calls in Linux.\n\nSo, let’s take a look!\n\nThis security exploit allows local users to gain root access.\n\nThe cause is a small bug in the assembly code which allows user programs to make legacy system calls on x86-64 systems.\n\nThe exploit code is pretty clever: it generates a region of memory with at a particular address and uses an integer overflow to cause this code:\n\nto hand execution off to an arbitrary address which runs as kernel code and can escalate the running process to root.\n\nRemember the part about not hardcoding the ABI in your application code?\n\nUnfortunately, the android-x86 folks made this mistake. The kernel ABI changed and suddenly android-x86 stopped working.\n\nThe kernel folks ended up restoring the old ABI to avoid breaking the Android devices in the wild with stale hardcoded sequences.\n\nHere’s the fix that was added to the Linux kernel. You can find a link to the offending commit in the android source in the commit message.\n\nRemember: never write your own assembly code. If you have to implement it directly for some reason, use a piece of code like the example above and go through at the very least.\n\nThe system call infrastructure in the Linux kernel is incredibly complex. There are many different methods for making system calls each with their own advantages and disadvantages.\n\nCalling system calls by crafting your own assembly is generally a bad idea as the ABI may break underneath you. Your kernel and libc implementation will (probably) choose the fastest method for making system calls on your system.\n\nIf you can’t use the provided wrappers (or if one doesn’t exist), you should at the very least use the wrapper function, or try to go through the vDSO provided .\n\nStay tuned for future blog posts investigating individual system calls and their implementations."
    },
    {
        "link": "https://tutorialspoint.com/unix_system_calls/execve.htm",
        "document": "execve() executes the program pointed to by filename. filename must be either a binary executable, or a script starting with a line of the form \"#! interpreter [arg]\". In the latter case, the interpreter must be a valid pathname for an executable which is not itself a script, which will be invoked as interpreter [arg] filename.\n\nargv is an array of argument strings passed to the new program. envp is an array of strings, conventionally of the form key=value, which are passed as environment to the new program. Both argv and envp must be terminated by a null pointer. The argument vector and environment can be accessed by the called programs main function, when it is defined as int main(int argc, char *argv[], char *envp[]).\n\nexecve() does not return on success, and the text, data, bss, and stack of the calling process are overwritten by that of the program loaded. The program invoked inherits the calling processs PID, and any open file descriptors that are not set to close-on-exec. Signals pending on the calling process are cleared. Any signals set to be caught by the calling process are reset to their default behaviour. The SIGCHLD signal (when set to SIG_IGN) may or may not be reset to SIG_DFL.\n\nIf the current program is being ptraced, a SIGTRAP is sent to it after a successful execve().\n\nIf the set-user-ID bit is set on the program file pointed to by filename, and the calling process is not being ptraced, then the effective user ID of the calling process is changed to that of the owner of the program file. i Similarly, when the set-group-ID bit of the program file is set the effective group ID of the calling process is set to the group of the program file.\n\nThe effective user ID of the process is copied to the saved set-user-ID; similarly, the effective group ID is copied to the saved set-group-ID. This copying takes place after any effective ID changes that occur because of the set-user-ID and set-group-ID permission bits.\n\nIf the executable is an a.out dynamically-linked binary executable containing shared-library stubs, the Linux dynamic linker ld.so(8) is called at the start of execution to bring needed shared libraries into memory and link the executable with them.\n\nIf the executable is a dynamically-linked ELF executable, the interpreter named in the PT_INTERP segment is used to load the needed shared libraries. This interpreter is typically /lib/ld-linux.so.1 for binaries linked with the Linux libc version 5, or /lib/ld-linux.so.2 for binaries linked with the GNU libc version 2.\n\nOn success, execve() does not return, on error -1 is returned, and errno is set appropriately.\n\nSVr4, 4.3BSD, POSIX.1-2001. POSIX.1-2001 does not document the #! behavior but is otherwise compatible.\n\nSUID and SGID processes can not be ptrace()d. Linux ignores the SUID and SGID bits on scripts.\n\nThe result of mounting a filesystem nosuid vary between Linux kernel versions: some will refuse execution of SUID/SGID executables when this would give the user powers she did not have already (and return EPERM), some will just ignore the SUID/SGID bits and exec() successfully.\n\nA maximum line length of 127 characters is allowed for the first line in a #! executable shell script.\n\nWith Unix V6 the argument list of an exec() call was ended by 0, while the argument list of main was ended by -1. Thus, this argument list was not directly usable in a further exec() call. Since Unix V7 both are NULL."
    },
    {
        "link": "https://learn.redhat.com/t5/General/Beyond-the-Veil-Exploring-System-Calls/td-p/45910",
        "document": "Have you ever wondered what happens behind the scenes when you run : \" ls newdir/ \" in your terminal ?\n\nEvery action, from opening a file to launching an application, involves a series of . In this post, we'll explore the powerful strace tool and delve into the world of system calls, revealing the intricate workings of your Linux system.\n\nstrace utility displays information about system calls made by a process. This command helps identify the kernel function on which a program spends its time.\n\nA is essentially a request from a program to the operating system's kernel for a specific service. Lets run our command and there will be lot of actions behind the veil until we get the result :\n\n\n\nLets break down the calls one by one :\n• None when you execute a program, a running process needs to call fork() to create a new process by duplicating itself. This duplicated process, a \"child\" process, then calls execve () (which is the first syscall you see listed by strace) : it replaces the current process with the /bin/ls executable.\n• None the process asking where its heap memory ends\n• None (\"/etc/ld.so.preload\", R_OK): Checks if the process has read access to the file ( which is used to load shared libraries ).\n• (AT_FDCWD, \"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) file for reading in read-only mode and closes it when the process exits.\n• (3, {st_mode=S_IFREG|0644, st_size=27749,...... Gets information about the file descriptor\n• (NULL, 27749, PROT_READ, MAP_PRIVATE, 3, 0) Maps the contents of the file descriptor\n\n(0x7f38af670920, 24) Sets the robust list for the process, which is used for recovery in case of a crash.\n\nSets the signal handler for the real-time signal SIGRTMIN.\n\nSets the stack size limit for the process.\n• None Gets information about the\n\nstatfs(\"/sys/fs/selinux\") system call is essential for understanding and managing SELinux security policies and configurations\n\ngets terminal attributes for the standard input\n\nGets the window size for the standard input\n\n(1, \"fileA fileB\n\n\", 13) Writes the string \"fileA fileB\n\n\" to the standard output\n\nand then we see this final result in the screen :\n\nThe next time you casually type ls, remember the intricate symphony of system calls that make it happen. You're welcome!!"
    },
    {
        "link": "https://wenboshen.github.io/posts/2016-09-15-kernel-execve",
        "document": "\n• System call execve\n• How the user space arguments pass to the program?\n• How the entry point (not main) in elf is reached?\n\ndo_execve is called in three places, one for , one for init process in and another one in .\n\nSyscall execve usually is used after syscall fork() to execute a different binary for child process, the typical scenario is\n\nIn C library, execl, execlp, execle, execv, execvp, execvpe are all supported by the , the call chain is -> -> . The main body of is:\n\nwill invoke to find the corresponding handler for each binary format. will search a global list called . The will add a linux_binfmt struct into the list. The linux_binfmt contains three important function pointers, , , and .\n\nTake the elf format as an example, will call register_binfmt to put elf_format into the formats list.\n\n-> , which will in turn call fmt-> load_binary, which is .The critical code snippet of load_elf_binay is\n• When does the child give up mm struct copied from parent?\n\n-> frees the parent’s mm, the details code logic is in -> -> .\n• When the new forked process get its name?\n\n–> –> So when the process gets forked, its name is not set yet, when it gets to run the execve, its name will be updated.\n\nHow the user space arguments pass to the program?\n\n-> -> ` __bprm_mm_init`, in which it will initialize a vma, and the vma has only one page which is stack.\n\n–> will find and link a page frame with bprm through , and then will kmap the arg_page and copy all the user space arguments into that page. This page will be set as stack in -> . In , saves the stack top, and it is passed to and saved to , which is loaded to stack pointer register after returning to user space.\n\nHow the entry point (not main) in elf is reached?\n• For syscall SyS_execve, the call chain is SyS_execve -> do_execve -> do_execve_common -> exec_binprm -> search_binary_handler -> load_elf_binary -> start_thread. In start_thread, the pc in pt_regs will be set to the entry point in elf.\n• The execution path will return to ret_fast_syscall, as in . ret_fast_syscall will reload pt_regs and return to the user space. Now the pc points to the entry point in elf, the execution path goes to elf.\n\ninit process begins as a kernel thread, -> -> The kernel_init function pointer is passed to kernel_thread as the first parameter.\n\nDetails of how kernel thread is forked can be found in Kernel thread vs user thread. Here we want to focus on how init process becomes a user thread.\n\nHere we can see that -> -> , which is the same with a regular execve syscall, the argument is binary.\n\nuser mode helper allows kernel to run a user space binary, one example is in kernel/reboot.c, calls user mode helper.\n\nuser mode helper code is mainly in kernel/kmod.c. -> -> which will create a kernel_thread and pass as the function parameter, same to the init process. call_usermodehelper_exec_async will call do_execve, same to kernel_init function. The do_execve will load the file in argv[0], which is reboot_cmd .\n\nHere, as the reboot_cmd is global read/write string, it give the attacker chance to overwrite it to any string path. For example, as show in New Reliable Android Kernel Root Exploitation Techniques, the attacker can use arbitrary kernel memory read to write it to be a binary file in /data, which is controlled by the attacker. Then the attacker can trigger the kernel thread (uid is root) to load a binary which will give him the root shell."
    },
    {
        "link": "https://linux-kernel-labs.github.io/refs/heads/master/lectures/syscalls.html",
        "document": "At a high level system calls are \"services\" offered by the kernel to user applications and they resemble library APIs in that they are described as a function call with a name, parameters, and return value.\n\nHowever, on a closer look, we can see that system calls are actually not function calls, but specific assembly instructions (architecture and kernel specific) that do the following:\n• setup information to identify the system call and its parameters\n• retrieve the result of the system call\n\nIn Linux, system calls are identified by numbers and the parameters for system calls are machine word sized (32 or 64 bit). There can be a maximum of 6 system call parameters. Both the system call number and the parameters are stored in certain registers.\n\nFor example, on 32bit x86 architecture, the system call identifier is stored in the EAX register, while parameters in registers EBX, ECX, EDX, ESI, EDI, EBP.\n\nSystem libraries (e.g. libc) offers functions that implement the actual system calls in order to make it easier for applications to use them.\n\nWhen a user to kernel mode transition occurs, the execution flow is interrupted and it is transferred to a kernel entry point. This is similar to how interrupts and exceptions are handled (in fact on some architectures this transition happens as a result of an exception).\n\nThe system call entry point will save registers (which contains values from user space, including system call number and system call parameters) on stack and then it will continue with executing the system call dispatcher.\n\nThe purpose of the system call dispatcher is to verify the system call number and run the kernel function associated with the system call.\n\nTo demonstrate the system call flow we are going to use the virtual machine setup, attach gdb to a running kernel, add a breakpoint to the dup2 system call and inspect the state.\n\nIn summary, this is what happens during a system call:\n• The application is setting up the system call number and parameters and it issues a trap instruction\n• The execution mode switches from user to kernel; the CPU switches to a kernel stack; the user stack and the return address to user space is saved on the kernel stack\n• The kernel entry point saves registers on the kernel stack\n• The system call dispatcher identifies the system call function and runs it\n• The user space registers are restored and execution is switched back to user (e.g. calling IRET)\n\nHandling system call parameters is tricky. Since these values are setup by user space, the kernel can not assume correctness and must always verify them thoroughly. Pointers have a few important special cases that must be checked:\n• Never allow pointers to kernel-space Since system calls are executed in kernel mode, they have access to kernel space and if pointers are not properly checked user applications might get read or write access to kernel space. For example, let's consider the case where such a check is not made for the read or write system calls. If the user passes a kernel-space pointer to a write system call then it can get access to kernel data by later reading the file. If it passes a kernel-space pointer to a read system call then it can corrupt kernel memory. Likewise, if a pointer passed by the application is invalid (e.g. unmapped, read-only for cases where it is used for writing), it could \"crash\" the kernel. Two approaches could be used:\n• Check the pointer against the user address space before using it, or\n• Avoid checking the pointer and rely on the MMU to detect when the pointer is invalid and use the page fault handler to determine that the pointer was invalid Although it sounds tempting, the second approach is not that easy to implement. The page fault handler uses the fault address (the address that was accessed), the faulting address (the address of the instruction that did the access) and information from the user address space to determine the cause:\n• Copy on write, demand paging, swapping: both the fault and faulting addresses are in user space; the fault address is valid (checked against the user address space)\n• Invalid pointer used in system call: the faulting address is in kernel space; the fault address is in user space and it is invalid\n• Kernel bug (kernel accesses invalid pointer): same as above But in the last two cases we don't have enough information to determine the cause of the fault. In order to solve this issue, Linux uses special APIs (e.g ) to accesses user space that are specially crafted:\n• The exact instructions that access user space are recorded in a table (exception table)\n• When a page fault occurs the faulting address is checked against this table Although the fault handling case may be more costly overall depending on the address space vs exception table size, and it is more complex, it is optimized for the common case and that is why it is preferred and used in Linux."
    },
    {
        "link": "https://kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt",
        "document": ""
    },
    {
        "link": "https://man7.org/linux/man-pages/man2/execve.2.html",
        "document": "Pages that refer to this page: pmcd(1), setpriv(1), strace(1), access(2), alarm(2), arch_prctl(2), brk(2), chdir(2), chmod(2), chroot(2), clone(2), close(2), eventfd(2), execveat(2), _exit(2), fanotify_mark(2), fcntl(2), flock(2), fork(2), getgroups(2), getitimer(2), getpriority(2), getrlimit(2), get_robust_list(2), getrusage(2), ioctl(2), ioctl_console(2), ioperm(2), iopl(2), KEYCTL_SET_REQKEY_KEYRING(2const), madvise(2), memfd_create(2), memfd_secret(2), mlock(2), mount(2), open(2), perf_event_open(2), personality(2), PR_CAPBSET_READ(2const), PR_GET_NO_NEW_PRIVS(2const), PR_GET_SPECULATION_CTRL(2const), PR_MPX_ENABLE_MANAGEMENT(2const), PR_PAC_RESET_KEYS(2const), PR_SET_CHILD_SUBREAPER(2const), PR_SET_DUMPABLE(2const), PR_SET_IO_FLUSHER(2const), PR_SET_KEEPCAPS(2const), PR_SET_NO_NEW_PRIVS(2const), PR_SET_PDEATHSIG(2const), PR_SET_SPECULATION_CTRL(2const), PR_SET_SYSCALL_USER_DISPATCH(2const), PR_SET_TAGGED_ADDR_CTRL(2const), PR_SET_THP_DISABLE(2const), PR_SET_TIMERSLACK(2const), PR_SVE_GET_VL(2const), PR_SVE_SET_VL(2const), ptrace(2), sched_setaffinity(2), seccomp(2), semop(2), set_mempolicy(2), setpgid(2), setresuid(2), setreuid(2), setsid(2), setuid(2), shmop(2), sigaction(2), sigaltstack(2), signalfd(2), sigpending(2), sigprocmask(2), syscalls(2), timer_create(2), timerfd_create(2), umask(2), vfork(2), cap_get_file(3), cap_iab(3), cap_launch(3), catopen(3), exec(3), exit(3), fexecve(3), getauxval(3), getexeccon(3), getfscreatecon(3), getkeycreatecon(3), getsockcreatecon(3), libexpect(3), mq_close(3), posix_spawn(3), pthread_atfork(3), pthread_kill_other_threads_np(3), pthread_mutexattr_setrobust(3), sd_bus_creds_get_pid(3), sem_close(3), sigvec(3), system(3), auditd-plugins(5), core(5), elf(5), proc_pid_attr(5), proc_pid_cmdline(5), proc_pid_environ(5), proc_sys_kernel(5), systemd.exec(5), systemd-system.conf(5), capabilities(7), cgroups(7), credentials(7), environ(7), inode(7), inotify(7), persistent-keyring(7), process-keyring(7), pthreads(7), sched(7), session-keyring(7), signal(7), signal-safety(7), thread-keyring(7), user-keyring(7), user_namespaces(7), user-session-keyring(7), vdso(7), pam_selinux(8)"
    },
    {
        "link": "https://docs.kernel.org/process/adding-syscalls.html",
        "document": "This document describes what’s involved in adding a new system call to the Linux kernel, over and above the normal submission advice in Documentation/process/submitting-patches.rst.\n\nA new system call forms part of the API of the kernel, and has to be supported indefinitely. As such, it’s a very good idea to explicitly discuss the interface on the kernel mailing list, and it’s important to plan for future extensions of the interface. For simpler system calls that only take a couple of arguments, the preferred way to allow for future extensibility is to include a flags argument to the system call. To make sure that userspace programs can safely use flags between kernel versions, check whether the flags value holds any unknown flags, and reject the system call (with ) if it does: For more sophisticated system calls that involve a larger number of arguments, it’s preferred to encapsulate the majority of the arguments into a structure that is passed in by pointer. Such a structure can cope with future extension by including a size argument in the structure: As long as any subsequently added field, say , is designed so that a zero value gives the previous behaviour, then this allows both directions of version mismatch:\n• None To cope with a later userspace program calling an older kernel, the kernel code should check that any memory beyond the size of the structure that it expects is zero (effectively checking that ).\n• None To cope with an older userspace program calling a newer kernel, the kernel code can zero-extend a smaller instance of the structure (effectively setting ). See perf_event_open(2) and the function (in ) for an example of this approach.\n\nIf your new system call allows userspace to refer to a kernel object, it should use a file descriptor as the handle for that object -- don’t invent a new type of userspace object handle when the kernel already has mechanisms and well-defined semantics for using file descriptors. If your new xyzzy(2) system call does return a new file descriptor, then the flags argument should include a value that is equivalent to setting on the new FD. This makes it possible for userspace to close the timing window between and calling , where an unexpected and in another thread could leak a descriptor to the exec’ed program. (However, resist the temptation to re-use the actual value of the constant, as it is architecture-specific and is part of a numbering space of flags that is fairly full.) If your system call returns a new file descriptor, you should also consider what it means to use the poll(2) family of system calls on that file descriptor. Making a file descriptor ready for reading or writing is the normal way for the kernel to indicate to userspace that an event has occurred on the corresponding kernel object. If your new xyzzy(2) system call involves a filename argument: you should also consider whether an xyzzyat(2) version is more appropriate: This allows more flexibility for how userspace specifies the file in question; in particular it allows userspace to request the functionality for an already-opened file descriptor using the flag, effectively giving an fxyzzy(3) operation for free: - xyzzyat(AT_FDCWD, path, ..., 0) is equivalent to xyzzy(path,...) - xyzzyat(fd, \"\", ..., AT_EMPTY_PATH) is equivalent to fxyzzy(fd, ...) If your new xyzzy(2) system call involves a parameter describing an offset within a file, make its type so that 64-bit offsets can be supported even on 32-bit architectures. If your new xyzzy(2) system call involves privileged functionality, it needs to be governed by the appropriate Linux capability bit (checked with a call to ), as described in the capabilities(7) man page. Choose an existing capability bit that governs related functionality, but try to avoid combining lots of only vaguely related functions together under the same bit, as this goes against capabilities’ purpose of splitting the power of root. In particular, avoid adding new uses of the already overly-general capability. If your new xyzzy(2) system call manipulates a process other than the calling process, it should be restricted (using a call to ) so that only a calling process with the same permissions as the target process, or with the necessary capabilities, can manipulate the target process. Finally, be aware that some non-x86 architectures have an easier time if system call parameters that are explicitly 64-bit fall on odd-numbered arguments (i.e. parameter 1, 3, 5), to allow use of contiguous pairs of 32-bit registers. (This concern does not apply if the arguments are part of a structure that’s passed in by pointer.)\n\nThe main entry point for your new xyzzy(2) system call will be called , but you add this entry point with the appropriate macro rather than explicitly. The ‘n’ indicates the number of arguments to the system call, and the macro takes the system call name followed by the (type, name) pairs for the parameters as arguments. Using this macro allows metadata about the new system call to be made available for other tools. The new entry point also needs a corresponding function prototype, in , marked as asmlinkage to match the way that system calls are invoked: Some architectures (e.g. x86) have their own architecture-specific syscall tables, but several other architectures share a generic syscall table. Add your new system call to the generic list by adding an entry to the list in : Also update the __NR_syscalls count to reflect the additional system call, and note that if multiple new system calls are added in the same merge window, your new syscall number may get adjusted to resolve conflicts. The file provides a fallback stub implementation of each system call, returning . Add your new system call here too: Your new kernel functionality, and the system call that controls it, should normally be optional, so add a option (typically to ) for it. As usual for new options:\n• None Include a description of the new functionality and system call controlled by the option.\n• None Make the option depend on EXPERT if it should be hidden from normal users.\n• None Make any new source files implementing the function dependent on the CONFIG option in the Makefile (e.g. ).\n• None Double check that the kernel still builds with the new CONFIG option turned off. To summarize, you need a commit that includes:\n• None option for the new function, normally in\n\nFor most system calls the same 64-bit implementation can be invoked even when the userspace program is itself 32-bit; even if the system call’s parameters include an explicit pointer, this is handled transparently. However, there are a couple of situations where a compatibility layer is needed to cope with size differences between 32-bit and 64-bit. The first is if the 64-bit kernel also supports 32-bit userspace programs, and so needs to parse areas of ( ) memory that could hold either 32-bit or 64-bit values. In particular, this is needed whenever a system call argument is: The second situation that requires a compatibility layer is if one of the system call’s arguments has a type that is explicitly 64-bit even on a 32-bit architecture, for example or . In this case, a value that arrives at a 64-bit kernel from a 32-bit application will be split into two 32-bit values, which then need to be re-assembled in the compatibility layer. The compatibility version of the system call is called , and is added with the macro, analogously to SYSCALL_DEFINEn. This version of the implementation runs as part of a 64-bit kernel, but expects to receive 32-bit parameter values and does whatever is needed to deal with them. (Typically, the version converts the values to 64-bit versions and either calls on to the version, or both of them call a common inner implementation function.) The compat entry point also needs a corresponding function prototype, in , marked as asmlinkage to match the way that system calls are invoked: If the system call involves a structure that is laid out differently on 32-bit and 64-bit systems, say , then the include/linux/compat.h header file should also include a compat version of the structure ( ) where each variable-size field has the appropriate type that corresponds to the type in . The routine can then use this structure to parse the arguments from a 32-bit invocation. For example, if there are fields: in struct xyzzy_args, then struct compat_xyzzy_args would have: The generic system call list also needs adjusting to allow for the compat version; the entry in should use rather than :\n• None instance of not in\n\nTo wire up the x86 architecture of a system call with a compatibility version, the entries in the syscall tables need to be adjusted. First, the entry in gets an extra column to indicate that a 32-bit userspace program running on a 64-bit kernel should hit the compat entry point: Second, you need to figure out what should happen for the x32 ABI version of the new system call. There’s a choice here: the layout of the arguments should either match the 64-bit version or the 32-bit version. If there’s a pointer-to-a-pointer involved, the decision is easy: x32 is ILP32, so the layout should match the 32-bit version, and the entry in is split so that x32 programs hit the compatibility wrapper: If no pointers are involved, then it is preferable to re-use the 64-bit system call for the x32 ABI (and consequently the entry in arch/x86/entry/syscalls/syscall_64.tbl is unchanged). In either case, you should check that the types involved in your argument layout do indeed map exactly from x32 (-mx32) to either the 32-bit (-m32) or 64-bit (-m64) equivalents.\n\nFor most system calls, once the system call is complete the user program continues exactly where it left off -- at the next instruction, with the stack the same and most of the registers the same as before the system call, and with the same virtual memory space. However, a few system calls do things differently. They might return to a different location ( ) or change the memory space ( / / ) or even architecture ( / ) of the program. To allow for this, the kernel implementation of the system call may need to save and restore additional registers to the kernel stack, allowing complete control of where and how execution continues after the system call. This is arch-specific, but typically involves defining assembly entry points that save/restore additional registers and invoke the real system call entry point. For x86_64, this is implemented as a entry point in , and the entry in the syscall table ( ) is adjusted to match: The equivalent for 32-bit programs running on a 64-bit kernel is normally called and implemented in , with the corresponding syscall table adjustment in : If the system call needs a compatibility layer (as in the previous section) then the version needs to call on to the version of the system call rather than the native 64-bit version. Also, if the x32 ABI implementation is not common with the x86_64 version, then its syscall table will also need to invoke a stub that calls on to the version. For completeness, it’s also nice to set up a mapping so that user-mode Linux still works -- its syscall table will reference stub_xyzzy, but the UML build doesn’t include implementation (because UML simulates registers etc). Fixing this is as simple as adding a #define to :\n\nDo not call System Calls in the Kernel¶ System calls are, as stated above, interaction points between userspace and the kernel. Therefore, system call functions such as or should only be called from userspace via the syscall table, but not from elsewhere in the kernel. If the syscall functionality is useful to be used within the kernel, needs to be shared between an old and a new syscall, or needs to be shared between a syscall and its compatibility variant, it should be implemented by means of a “helper” function (such as ). This kernel function may then be called within the syscall stub ( ), the compatibility syscall stub ( ), and/or other kernel code. At least on 64-bit x86, it will be a hard requirement from v4.17 onwards to not call system call functions in the kernel. It uses a different calling convention for system calls where is decoded on-the-fly in a syscall wrapper which then hands processing over to the actual syscall function. This means that only those parameters which are actually needed for a specific syscall are passed on during syscall entry, instead of filling in six CPU registers with random user space content all the time (which may cause serious trouble down the call chain). Moreover, rules on how data may be accessed may differ between kernel data and user data. This is another reason why calling is generally a bad idea. Exceptions to this rule are only allowed in architecture-specific overrides, architecture-specific compatibility wrappers, or other code in arch/."
    },
    {
        "link": "https://linux-kernel-labs.github.io/refs/pull/183/merge/lectures/syscalls.html",
        "document": "At a high level system calls are “services” offered by the kernel to user applications and they resemble library APIs in that they are described as a function call with a name, parameters and return value.\n\nHowever, on a closer look, we can see that system calls are actually not function calls, but specific assembly instructions (architecture and kernel specific) that do the following:\n• setup information to identify the system call and its parameters\n• retrieve the result of the system call\n\nIn Linux, system calls are identified by numbers and the parameters for system calls are machine word sized (32 or 64 bit). There can be a maximum of 6 system call parameters. Both the system call number and the parameters are stored in certain registers.\n\nFor example, on 32bit x86 architecture, the system call identifier is stored in the EAX register, while parameters in registers EBX, ECX, EDX, ESI, EDI, EBP.\n\nSystem libraries (e.g. libc) offers functions that implement the actual system calls in order to make it easier for applications to use them.\n\nWhen a user to kernel mode transition occurs, the execution flow is interrupted and it is transfered to a kernel entry point. This is similar with how interrupts and exception are handled (in fact on some architectures this transition happens as a result of an exception).\n\nThe system call entry point will save registers (which contains values from user space, including system call number and system call parameters) on stack and then it will continue with executing the system call dispatcher.\n\nThe purpose of the system call dispatcher is to verify the system call number and run the kernel function associated with the system call.\n\nTo demonstrate the system call flow we are going to use the virtual machine setup, attach gdb to a running kernel, add a breakpoint to the dup2 system call and inspect the state.\n\nIn summary, this is what happens during a system call:\n• The application is setting up the system call number and parameters and it issues a trap instruction\n• The execution mode switches from user to kernel; the CPU switches to a kernel stack; the user stack and the return address to user space is saved on the kernel stack\n• The kernel entry point saves registers on the kernel stack\n• The system call dispatcher identifies the system call function and runs it\n• The user space registers are restored and execution is switched back to user (e.g. calling IRET)\n\nHandling system call parameters is tricky. Since these values are setup by user space, the kernel can not assume correctness and must always verify them throughly. Pointers have a few important special cases that must be checked:\n• Never allow pointers to kernel-space Since system calls are executed in kernel mode, they have access to kernel space and if pointers are not properly checked user applications might get read or write access to kernel space. For example, lets consider the case where such a check is not made for the read or write system calls. If the user passes a kernel-space pointer to a write system call then it can get access to kernel data by later reading the file. If it passes a kernel-space pointer to a read system call then it can corrupt kernel memory. Likewise, if a pointer passed by the application is invalid (e.g. unmaped, read-only for cases where its need to be written), it could “crash” the kernel. There two approaches that could be used:\n• Check the pointer against the user address space before using it, or\n• Avoid checking the pointer and rely on the MMU to detect when the pointer is invalid and use the page fault handler to determine that the pointer was invalid Although it sounds tempting, the second approach is not that easy to implement. The page fault handler uses the fault address (the address that was accessed), the faulting address (the address of the instruction that did the access) and information from the user address space to determine the cause:\n• Copy on write, demand paging, swapping: both the fault and faulting addresses are in user space; the fault address is valid (checked against the user address space)\n• Invalid pointer used in system call: the faulting address is in kernel space; the fault address is in user space and it is invalid\n• Kernel bug (kernel accesses invalid pointer): same as above But in the last two cases we don’t have enough information to determine the cause of the fault. In order to solve this issue Linux uses special APIs (e.g ) to accesses user space that are specially crafted:\n• The exact instructions that access user space are recorded in a table (exception table)\n• When a page fault occurs the faulting address is checked against this table Although the fault handling case may be more costly overall depending on the address space vs exception table size, and it is more complex, it is optimized for the common case and that is why it is preferred and used in Linux."
    }
]