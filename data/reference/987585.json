[
    {
        "link": "https://geeksforgeeks.org/how-to-decide-number-of-filters-in-cnn",
        "document": "How to Decide Number of Filters in CNN?\n\nAnswer: The number of filters in a CNN is often determined empirically through experimentation, balancing model complexity and performance on the validation set.\n\nDeciding the number of filters in a Convolutional Neural Network (CNN) involves a combination of domain knowledge, experimentation, and understanding of the architecture's requirements. Here's a detailed breakdown of the process:\n• Understand the Data and Task\n• None The complexity and diversity of the dataset play a significant role in determining the number of filters. A dataset with intricate patterns or diverse features might require more filters to capture these variations effectively.\n• None The nature of the task also matters. For example, tasks involving fine-grained distinctions might benefit from more filters to extract subtle features, while simpler tasks might require fewer filters.\n• Start Conservatively\n• None It's often prudent to start with a conservative number of filters, especially if computational resources are limited. Beginning with a smaller number allows for faster experimentation and model training iterations.\n• None For the initial layers of the network, where low-level features like edges and textures are extracted, fewer filters are usually sufficient.\n• Experiment with Different Architectures\n• None Experiment with different CNN architectures and observe their performance on a validation set.\n• None Common architectures like VGG, ResNet, and Inception provide guidelines on the number of filters used in each layer. You can start with these architectures as baselines and then adjust the number of filters based on your dataset and task requirements.\n• Consider Model Capacity and Overfitting\n• None Increasing the number of filters adds model capacity, allowing it to learn more complex representations. However, it also increases the risk of overfitting, especially if the dataset is small.\n• None Monitor the model's performance on both the training and validation sets. If the training accuracy is significantly higher than the validation accuracy, it might indicate overfitting. In such cases, reducing the number of filters can help generalize better.\n• Regularization Techniques\n• None Regularization techniques like dropout, batch normalization, and weight decay can help mitigate overfitting caused by a large number of filters. Incorporating these techniques allows you to use a higher number of filters without compromising generalization performance.\n• Use Transfer Learning\n• None Leveraging pre-trained models through transfer learning can provide insights into the number of filters suitable for your task. You can fine-tune these pre-trained models on your dataset and observe the performance with different filter configurations.\n• Grid Search or Random Search\n• None If computational resources permit, you can perform a grid search or random search over a range of filter sizes to find the optimal configuration. This approach systematically explores the hyperparameter space and helps identify the best-performing model.\n• Iterative Refinement\n• None CNN model development is often an iterative process. Continuously refine the architecture and hyperparameters based on feedback from validation performance until satisfactory results are achieved.\n\nBy following these steps, you can systematically determine the appropriate number of filters for your CNN architecture, tailored to your specific dataset and task requirements.\n\nIn conclusion, deciding the number of filters in a Convolutional Neural Network (CNN) involves a nuanced approach that balances model complexity, task requirements, and dataset characteristics. By starting conservatively, experimenting with different architectures, and considering factors like model capacity, regularization, and transfer learning, researchers and practitioners can systematically determine the optimal number of filters for their specific CNN design, ensuring effective feature extraction and model performance."
    },
    {
        "link": "https://reddit.com/r/MachineLearning/comments/3l5qu7/rules_of_thumb_for_cnn_architectures",
        "document": "I'm currently trying to get a CNN up and running but so far haven't been able to get results quite as promising as I was expecting. This may just be down to a difficult dataset, but I'm fairly inexperienced so I was wondering if there were any rules of thumb to get reasonable initial parameters/hyperparameters.\n\nSpecifically I'm not sure how many filters I should be trying to generate for a given input, how much dropout is too much, reasonable sizes for convolution windows, etc. I suspect that most of these will depend heavily on the size of the input volume, and currently my first layer takes in 48x48 grayscale images."
    },
    {
        "link": "https://quora.com/What-is-the-ideal-number-of-filters-to-use-in-a-convolutional-neural-network-CNN-for-classification-problems-with-high-dimensionality-or-datasets-having-less-number-of-samples",
        "document": "Something went wrong. Wait a moment and try again."
    },
    {
        "link": "https://linkedin.com/advice/0/what-some-best-practices-designing-convolutional",
        "document": ""
    },
    {
        "link": "https://massedcompute.com/faq-answers?question=What+are+the+optimal+number+of+filters+in+a+convolutional+neural+network+for+image+classification+tasks%3F",
        "document": "DISCLAIMER: This is for large language model education purpose only. All content displayed below is AI generate content. Some content may not be accurate. Please review our Terms & Conditions and our Privacy Policy for subscription policies."
    },
    {
        "link": "https://medium.com/@hassaanidrees7/neural-architecture-search-nas-automating-the-design-of-efficient-ai-models-df7aec39d60a",
        "document": "How NAS is Transforming the Way AI Models are Designed and Optimized Building deep learning models traditionally requires human expertise to manually design and optimize architectures — a process that is often time-consuming and complex. Neural Architecture Search (NAS) aims to automate this process by using algorithms to search for the optimal neural network architecture. NAS is revolutionizing how machine learning models are developed, enabling the discovery of high-performing models that are faster, smaller, and more accurate than those designed manually. In this blog post, we’ll explore what NAS is, how it works, and its impact on the future of AI.\n\nNeural Architecture Search (NAS) is a technique in machine learning that automates the process of designing neural network architectures. Instead of manually specifying the structure of a neural network (e.g., the number of layers, types of operations, or connections), NAS algorithms search through a predefined space of potential architectures to find the one that best fits a given task. NAS can be seen as an extension of hyperparameter optimization, but instead of tuning parameters like learning rate or batch size, NAS focuses on discovering the network’s architecture. This not only speeds up the model development process but can also lead to more innovative and optimized architectures.\n\nNAS involves a search space, a search strategy, and an evaluation strategy. Each of these components plays a crucial role in finding the optimal neural network architecture. The search space defines the set of possible architectures that NAS can explore. It includes various architectural choices such as: The size of the search space can be enormous, and defining it effectively is key to the success of NAS. The search strategy determines how the NAS algorithm explores the search space to find the best architecture. Common search strategies include:\n• Random Search: Randomly samples architectures from the search space and evaluates them.\n• Reinforcement Learning (RL): Uses an RL agent (e.g., a controller) to generate architectures based on rewards from past searches.\n• Evolutionary Algorithms (EA): Uses evolutionary principles such as mutation and crossover to evolve a population of architectures over time.\n• Gradient-Based Search: Allows the architecture to be optimized using gradient descent by relaxing the search space into a differentiable form (e.g., DARTS). Once a candidate architecture is selected, the evaluation strategy measures its performance. This usually involves training the architecture on a given dataset and evaluating it based on accuracy, latency, or model size. Since training each candidate from scratch is computationally expensive, strategies like weight-sharing (reusing weights across architectures) are often employed to reduce the cost of evaluation.\n\nThere are several different approaches to Neural Architecture Search, each with its strengths and trade-offs. In Reinforcement Learning-based NAS, a controller (often an RNN) is trained to generate architectures. The controller proposes an architecture, and after the architecture is trained and evaluated, the result (e.g., accuracy) is used as a reward signal to improve the controller. NASNet is a well-known RL-based NAS approach developed by Google. It uses an RNN controller to predict neural network architectures, which are then trained and evaluated. The reward signal from the evaluation is fed back to the controller, allowing it to generate better architectures in subsequent iterations.\n• Cell Search: NASNet searches for a small, repeatable “cell” that can be stacked multiple times to form a larger network. This reduces the search space and computation time. In Evolutionary Algorithm (EA)-based NAS, a population of neural networks is evolved over time. New architectures are generated through mutation (random changes) and crossover (combining parts of different architectures). The best-performing architectures are selected for the next generation. AmoebaNet is an evolutionary algorithm-based NAS method that evolves architectures using mutation and selection. It explores the search space by creating small changes to existing architectures and selecting the top performers for further evolution.\n• High Accuracy: AmoebaNet has achieved state-of-the-art performance on image classification tasks, rivaling architectures designed through manual effort.\n• Evolutionary Process: It leverages evolutionary algorithms to explore the vast architecture space without requiring hand-engineering. Gradient-based NAS uses continuous relaxation of the architecture search space to make it differentiable, allowing the architecture to be optimized using gradient descent. One popular example is DARTS (Differentiable Architecture Search). DARTS formulates the NAS problem as a continuous optimization task by relaxing discrete architectural choices into weighted sums. This allows the architecture search to be optimized using standard gradient-based optimization techniques, making it more efficient than RL or EA approaches.\n• Efficiency: DARTS is more efficient than traditional NAS approaches, as it does not require training a separate model for each architecture sampled.\n• Scalability: DARTS can scale to larger tasks due to its efficient optimization process.\n\nNAS automates the entire process of designing neural networks, eliminating the need for manual trial and error by human experts. This significantly speeds up the development cycle, enabling faster experimentation and deployment of AI models. NAS can discover architectures that outperform human-designed models. For example, architectures like NASNet and AmoebaNet have achieved state-of-the-art performance in tasks such as image classification and object detection. NAS allows for the optimization of multiple objectives simultaneously, such as accuracy, latency, and model size. This is particularly useful for designing efficient models for edge devices, where resources are limited.\n\nWhile NAS offers tremendous potential, it also comes with its own set of challenges: Searching for optimal architectures can be computationally expensive, especially when training each candidate from scratch. Many early NAS methods required thousands of GPU hours to find a single architecture. Defining an effective search space is crucial. A search space that is too large can make the search inefficient, while a search space that is too small may miss out on high-performing architectures. Architectures found using NAS for one task (e.g., image classification) may not always transfer well to other tasks (e.g., object detection or segmentation). This limits the broader applicability of NAS results.\n\nTo make NAS more practical and accessible, several techniques have been developed to reduce the computational cost: Weight Sharing reduces the computational cost by reusing the same set of weights across different architectures. Instead of training each architecture from scratch, NAS algorithms share weights between similar architectures during the search process. ENAS uses weight sharing to significantly reduce the time required for NAS. By reusing weights across different architectures, ENAS reduces the computational cost of architecture search by several orders of magnitude. In proxy tasks, NAS is performed on smaller versions of the problem (e.g., using a smaller dataset or fewer layers), allowing architectures to be tested more quickly. Once an optimal architecture is found for the proxy task, it can be scaled up for the full problem. In One-Shot NAS, all architectures are trained simultaneously in a single “supernet,” and NAS is performed by sampling sub-networks from the supernet. This reduces the need to train each candidate architecture separately.\n\nNeural Architecture Search (NAS) is transforming the field of AI by automating the design of efficient and high-performing neural networks. From reinforcement learning-based approaches like NASNet to gradient-based methods like DARTS, NAS is making it easier and faster to discover innovative architectures for a wide range of tasks. While challenges like computational cost and transferability remain, ongoing research and advancements like weight sharing and one-shot NAS are making NAS more practical and scalable. As NAS continues to evolve, it will play a crucial role in the future of AI development, enabling more efficient, powerful, and accessible machine learning models."
    },
    {
        "link": "https://mdpi.com/2227-7390/12/19/3032",
        "document": "Over the last years, deep neural networks (DNNs) have become the state-of-the-art technique in several challenging tasks, such as speech recognition [ 1 ], natural language processing [ 2 ], and computer vision [ 3 ]. In particular, convolutional neural networks (CNNs) have achieved remarkable success across a broad range of computer vision challenges, such as image classification [ 3 ], object detection in images [ 4 ], object detection in video [ 5 ], semantic segmentation [ 6 ], video restoration [ 7 ], and medical diagnosis [ 8 ]. The astonishing results of CNNs are associated with the huge number of annotated data and important advances in hardware. Unfortunately, CNNs usually have an immense number of parameters, incurring in high storage requirements, and significant computational and energetic costs [ 9 ]. This imposes severe restrictions on the deployment of these models on devices with limited resources, such as edge devices or mobile devices. Thus, some methods to develop smaller-size models or simplify the complexity of the available models are necessary [ 10 ]. On the other hand, these large models requiring large amounts of memory and computation time have an evident environmental impact. In 2019, researchers from the University of Massachusetts discovered that training various deep learning models, such as those involving neural architecture search, could release over 284,019 kg of carbon dioxide. This amount is roughly equivalent to five times the total lifetime emissions of the typical American vehicle, including the car’s production [ 11 12 ]. Concretely, to classify a single image, the VGG-16 model [ 13 ] needs over 30 billion floating-point operations (FLOPs) and has 138 million parameters, which demand more than 500 MB of storage space [ 14 ]. In consequence, reducing the model’s storage requirements and computational cost becomes critical for resource-limited devices, specially in IoT applications, embedded systems, autonomous agents, mobile devices, and edge devices [ 15 ]. Actually, the concerns relative to such a high level of energy consumption—among other resources—have lead to the surge and development of a new line of research seeking a more sustainable future for Artificial Intelligence development and deployment, named Green Artificial Intelligence [ 16 ]. Nonetheless, Ref. [ 17 ] remarks that a usual DNN property is their considerable redundancy in parameterization, which leads to the idea of reducing this redundancy by compressing the networks. However, a severe problem along with compressing the models is the loss of accuracy. In order to avoid it, there are some ways of designing efficient DNNs and generating effective solutions. For instance, one approach involves using memetic algorithms to discover an effective architecture for a given task, while another method entails employing alternative optimization algorithms to fine-tune the connection weights of a pre-defined architecture. Kernel Quantization is also used for efficient network compression [ 18 ]. Alternatively, pruning is a widely utilized technique for simplifying neural networks. Since the introduction of Optimal Brain Damage (OBD) and Optimal Brain Surgery (OBS) in 1990, pruning methods have been thoroughly researched for model compression. Over the years, numerous other approaches have been developed to create more efficient and effective neural networks of various types, including dense, convolutional, and recurrent networks [ 12 ]. The primary objective of pruning algorithms is to derive a subnetwork with significantly fewer parameters while maintaining the same level of accuracy. ), called the percentile of significance, which represents the proportion of filters which will be transferred to the new model based on their importance. Only the -th percentile of filters with higher values after applying the OCNNA process will remain. The proposed OCNNA can directly be applied to trained CNNs, avoiding the training process from scratch. In this paper, we propose a novel CNN optimization and construction technique called Optimizing Convolutional Neural Network Architectures (OCNNA) based on pruning which requires minimal tuning. OCNNA has been designed to assess the importance of convolutional layers. Since this measure is computed for every convolutional layer and unit from the model input to the output, it essentially reflects the importance of every single convolutional filter and its contribution to the information flow through the network [ 19 ]. Moreover, our method is able to sort convolutional filters within a layer by importance; as a consequence, it can be seen as a feature importance computation tool which can generate more explainable neural networks. OCNNA is easy to apply, having only one parameter (), called the percentile of significance, which represents the proportion of filters which will be transferred to the new model based on their importance. Only the-th percentile of filters with higher values after applying the OCNNA process will remain. The proposed OCNNA can directly be applied to trained CNNs, avoiding the training process from scratch. We thoroughly evaluated the optimization efficacy of the OCNNA method compared with the state-of-the-art pruning techniques. Our experiments on the CIFAR-10, CIFAR-100 [ 20 ], and Imagenet [ 21 ] datasets and popular CNN architectures, such as ResNet-50, VGG-16, DenseNet40, and MobileNet, show that our algorithm leads to better performance in terms of the accuracy in prediction and the reduction in the number of parameters, following a cornerstone metric for Green AI [ 22 ] and efficient Machine Learning. Our main contributions can be summarized as follows:\n• None A Green AI simplification method for CNNs called OCNNA is proposed. OCNNA measures the importance of each convolutional layer and unit from a trained model by combining well-known data science and statistical techniques, such as PCA, Frobenius norm, and Coefficient of Variation. After that, it builds a simplified model which can be even more precise than the original one and more efficient in terms of computational costs and memory footprint.\n• None Experiments on three benchmark datasets (CIFAR-10, CIFAR-100, and Imagenet) demonstrate that our simplification technique yields highly competitive results in terms of efficiency metrics (reduction in the number of parameters) and prediction accuracy when applied to the most widely used CNN models, such as VGG16, ResNet50, DenseNet40, and MobileNetV1. The paper is organized as follows: Section 2 presents an overview of the latest methods for developing efficient deep neural networks. Our proposal is described in Section 3 . In Section 4 , our methodology is experimentally analyzed, and the results are analyzed and discussed in Section 5 . Finally, Section 6 highlights the conclusions.\n\nThe purpose of this section is to provide a brief overview of the main approaches in model compression: neuroevolution, neural architecture search, quantization, and pruning. Our research mainly focuses on convolutional neural network pruning. Neuroevolution can be applied to several tasks related to efficient neural network design, such as learning neural networks (NN) building blocks, hyperparameters, or architectures. In 2002, NeuroEvolution of Augmenting Topologies (NEAT) was presented in [ 23 ], showing the effectiveness of a Genetic Algorithm (GA) in evolving NN topologies and strengthening the analogy with biological evolution. More recently, ref. [ 24 ] drew inspiration from NEAT, evolving deep neural networks by beginning with a simple neural network and gradually increasing its complexity through mutations. In [ 25 ], a more accurate approach, which consists in stacking the same layer module to make a deep neural network, like Inception, DenseNet, and ResNet, can be found [ 26 ]. Recently, neural architecture search (NAS), whose main goal is to automatically design the best DNN architecture, has achieved great importance in model design. On the one hand, NAS algorithms can be divided into two categories: (1) microstructure search focuses on identifying the best operation for each layer; (2) macrostructure search aims to find the optimal number of channels or filters for each layer, or the ideal model depth [ 27 ]. Additionally, NAS algorithms can be categorized into three groups based on the optimizer used: reinforcement learning-based NAS algorithms, gradient-based NAS algorithms, and evolutionary NAS (ENAS) algorithms. In this sense, NSGA-II has been recently used for NAS creating NSGA-Net [ 28 ]. In [ 29 ], NATS-BEnch is proposed, consisting in a unified benchmark for topology and size aggregating more than 10 state-of-the-art NAS algorithms. Quantization refers to the process of approximating a continuous signal by using a set of discrete symbols or integer values [ 30 ]. In other words, it reduces computations by reducing the precision of the data type. Advanced quantization techniques, such as asymmetric quantization [ 31 ] or calibration-based quantization, have been presented to improve accuracy. In [ 30 ], we find a complete quantization guide and recommendations. Knowledge distillation, which was first defined by [ 32 ] and generalized in [ 33 ], is the process of transferring knowledge from one neural network to a different one. In [ 34 ], a student–teacher framework is presented, introducing different scenarios, such as distillation based on the number of teachers (one teacher vs. multiple teachers), distillation based on data format (data-free, with a few samples, or cross-modal distillation), or online and teacher-free distillation. Network pruning is one of the most effective and prevalent approaches to model compression. Pruning techniques can be classified by various aspects: structured and unstructured pruning, depending on whether the pruned network is symmetric or not [ 30 35 ]; neuron, weight, or filter pruning, depending on the network’s element which is pruned; or static and dynamic pruning [ 30 ]. While static pruning removes elements offline from the network after training and before inference, dynamic pruning determines at runtime which elements will not participate in further activity [ 30 ]. Most researchers focus on how to find unimportant filters. Magnitude-based methods [ 36 ] use the magnitude of the weights in feature maps from certain layers as a measure of importance, pruning those with lower magnitudes. Others measure the importance of a filter through their reconstruction loss (Thinet) [ 37 ] or Taylor expansion [ 38 39 ]. In [ 40 ], Average Percentage of Zeros (APoZ) is used to assess the proportion of zero activations in a neuron following ReLU mapping, thus allowing for the pruning of redundant neurons. HRank [ 41 ] understands filter pruning as an optimization problem, using the feature maps as the function which measures the importance of a filter part of the CNN. Inspired by HRank, FPWT [ 42 ] introduces a new method which transforms the feature map in the spatial domain into the frequency domain by using Wavelet Transform. In [ 43 ], a new CNN compression technique is presented based on the filter-level pruning of redundant weights according to entropy importance criteria (FPEI) with different versions depending on the learning task and the NN. SFP [ 44 ] and FPGM, based on filter pruning via geometric median [ 45 ], use soft filter pruning; PScratch [ 46 ] proposes to prune from scratch, before training the model. In [ 47 ], a criterion for CNN pruning inspired by NN interpretability is proposed: the most relevant units are identified based on their relevance scores, which are derived from explainable AI (XAI) concepts. Ref. [ 48 ] introduces a data-driven CNN architecture determination algorithm called AutoCNN which consists of three training stages (spatial filter, classification layers, and hyperparameters). AutoCNN uses statistical parameters to decide whether to add new layers, prune redundant filters, or add new fully connected layers pruning low-information units. An iterative pruning method based on deep reinforcement learning (DRL) called Neon, formed by a DRL agent which controls the trade-off between performance and the efficiency of the pruned network, is introduced in [ 49 ]. For each hidden layer, Neon extracts the architecture-based and the layer-based feature maps which represent an observation. Then, the aforementioned hidden layer is compressed and fine-tuned. After that, a reward is calculated and used to update the deep reinforcement learning agent’s weights. This process is repeated several times for the whole neural network. In [ 50 ], a multi-objective evolution strategy algorithm called DeepPruningES is proposed. Its final output consists of three neural networks with different trade-offs called knee (with the best trade-off between training error and the number of FLOPs), boundary-heavy (with the smallest training error), and boundary-light solutions (with the smallest number of FLOPs). In [ 27 ], a customized correlation-based filter-level pruning method for deep CNN compression called COP, which removes redundant filters through their correlations, is presented. In [ 51 ], SCWC is introduced, a shared channel weight convolution method to decrease the number of multiplications in CNNs by leveraging the distributive property, made possible through structured channel parameter sharing. In [ 52 ], a new method called CHWP for identifying the most redundant filters is proposed, taking into account the size of filters, the difference between them, and the role of Batch Normalization layers. In [ 53 ], a training method for CNN compression is proposed. It integrates matrix factorization and regularization techniques, based on Singular Value Decomposition. Nonetheless, the method has been evaluated only on ResNet-18, ResNet-20, and ResNet-32. -norm and redundancy. Unlike other methods, ResPrune does not completely omit the filters identified as irrelevant; instead, it restores them to their original values by using stochastic techniques. In [ In [ 54 ], a CNN pruning method called MOP-FMS is introduced, in which the pruning task is modeled as a bi-objective optimization problem based on feature map selection. The two objectives of the method are accuracy and FLOPs, which are achieved by using an ad hoc evolutionary optimization algorithm designed to perform the pruning. Ref. [ 55 ] presents a CNN pruning method, which obtains a simplified network by clustering its filters. After that, it searches the optimal compact network structure by applying a social group optimization algorithm. In [ 56 ], an evolutionary method called Bi-CNN-Pruning is proposed to prune filters and channels with the objective of preserving the performance of the original model according to different pruning criteria, such as weight magnitude or activation statistics, among others. Concretely, it maintains the important channels in an ordered way, i.e., useful filters are selected first and then the channels. ResPrune is proposed in [ 57 ]. It is a selection filter method which uses two criteria:-norm and redundancy. Unlike other methods, ResPrune does not completely omit the filters identified as irrelevant; instead, it restores them to their original values by using stochastic techniques. In [ 58 ], a pruning framework called MGPF is introduced. MGPF generates sparse models of different granularity without fine-tuning the remaining connections after pruning. CIE [ 59 ] is a cross-layer importance evaluation technique used for neural network pruning, which assesses the significance of convolutional layers based on the model’s prediction accuracy. This evaluation is highly efficient in terms of time, as the process is performed only once for a given model. As demonstrated in this section, there are numerous approaches and techniques for simplifying CNNs. Over the years, increasingly sophisticated solutions have been proposed; whether through neuroevolution, neural architecture search, quantization, knowledge distillation, or pruning, the goal remains the same: to achieve smaller models without sacrificing prediction accuracy. However, designing an efficient, versatile, and effective method is not easy. In the case of neuroevolution and neural architecture search, competitive techniques can be achieved in terms of accuracy, but the computational cost of generating them can be prohibitive. For quantization and knowledge distillation, conceptual challenges may arise, while pruning algorithms are typically designed for specific models, unable to tackle the simplification of networks of different natures or assess their performance on various benchmarks. To address all these challenges, we developed OCNNA, an efficient technique that will be evaluated on the de facto benchmarks for image classification and will simplify the most paradigmatic and general convolutional networks. It can be applied in virtually any industry or academic use case involving CNNs and image classification.\n\nIn this paper, we address the challenge of finding an optimized topology for a convolutional neural network. Thus, we introduce the Optimizing Convolutional Neural Network Architectures (OCNNA) method, a new convolutional neural network construction method based on pruning. In this section, we provide a detailed description of the method. First of all, we introduce the notation used in our proposal. Let be a neural network with convolutional layers. Let and be the convolutional filter and the output of the -th layer. The subscript represents the filter index, where indicates the total number of output filters in the corresponding layer. In consequence, pruning the -th filter in layer implies removing the corresponding . as the matrix result of computing PCA on the -th filter’s output of the -th layer. Principal Component Analysis (PCA) [ 60 ] is a data analysis tool applied to identify the most meaningful basis to re-express, revealing a hidden structure, a given dataset. We defineas the matrix result of computing PCA on the-th filter’s output of the-th layer. matrix defined as The Frobenius norm [ 61 ] is a norm of anmatrixdefined as : where is the conjugate transpose [ as the Frobenius norm of the above PCA calculation (on the -th filter’s output of the -th layer). It is also equal to the square root of the matrix trace ofwhereis the conjugate transpose [ 62 ]. Let us defineas the Frobenius norm of the above PCA calculation (on the-th filter’s output of the-th layer). is a data distribution, with its standard deviation and its mean, CV is calculated as The Coefficient of Variation (CV) is the relationship between mean and standard deviation [ 63 ]. Ifis a data distribution, withits standard deviation andits mean, CV is calculated as It is attractive as a statistical tool because it permits the comparison of variables free from scale effects (dimensionless). We call if , for a given . . Only the -th percentile of filters with higher values in terms of importance will remain. All notations can be found in Finally, we define the percentile of significance. Only the-th percentile of filters with higher values in terms of importance will remain. All notations can be found in Table 1 -th percentile ( for layer in model.Layers do if layer is Convolutional then Add new layer with new_layers_index filters from model Since the main components of a CNN are the convolutional filters, OCNNA is designed to identify the most important ones, thus creating a new model in which the less significant convolutional units are not included. This way, OCNNA generates a more efficient model in terms of the number of parameters with minimum precision loss—as we will see in the next section. Additionally, OCNNA allows for the ordering of the convolutional filters by importance, providing a feature importance assessment method and, as a result, helping to better understand these models. Our method employs three important techniques to identify the significant filters: Principal Components Analysis (PCA), for selecting the most important features based on their hidden structure; the Frobenius norm, to summarize the PCA output information; and the Coefficient of Variation (CV), to measure the variability of Frobenius norm outputs. Figure 1 shows, as an example, the simplification process of a VGG-16 convolutional filter. As can be seen, the algorithm takes the filters from each layer ( Figure 1 , 1); applies PCA, Frobenius norm, and Coefficient of Variation ( Figure 1 , 2); and ranks the filters by importance, selecting only their top-th percentile ( Figure 1 , 3). More details can be found in Algorithm 1. , a dataset, used exclusively for measuring the importance of filters, is evaluated by generating output . is formed by filters. In consequence, is the -th filter’s output from the -th layer. By using , OCNNA helps us to measure the importance of the filters in the -th layer. Concretely, we adopt a three-stage process. First, for each filter and image contained in , we apply PCA retaining the of variance. with a matrix which contains the most meaningful features generated by the -th filter of the -th convolutional layer. Nonetheless, the information embedded in is too large. In consequence, we apply the Frobenius norm to summarize this information: obtaining a vector , in which each component is the result of the process described above applied to each image from . Finally, we calculate the CV: Given convolutional layer, adataset, used exclusively for measuring the importance of filters, is evaluated by generating outputis formed byfilters. In consequence,is the-th filter’s output from the-th layer. By using, OCNNA helps us to measure the importance of thefilters in the-th layer. Concretely, we adopt a three-stage process. First, for each filter and image contained in, we apply PCA retaining theof variance.witha matrix which contains the most meaningful features generated by the-th filter of the-th convolutional layer. Nonetheless, the information embedded inis too large. In consequence, we apply the Frobenius norm to summarize this information:obtaining a vector, in which each component is the result of the process described above applied to each image from. Finally, we calculate the CV: is a number which summarizes the -th filter significance within the -th convolutional layer by measuring the variability of the process PCA and Frobenius norm for each image in . In other words, OCNNA gives a low score of importance to a filter if for a subset of images, it generates an output whose hidden structures (PCA, 95% variance), after being summarized (Frobenius norm), have little variability (CV). , and generating a holistic vision summarizing the filter significance into a single number ( -th component represents the -th filter’s importance. Finally, using parameter , or percentile of significance, we extract the -th percentile of filters in terms of significance, completing the simplification process. To sum up, OCNNA is able to extract insights and measure the importance of each filter of a convolutional layer, starting from hundreds of arrays which constitute the output from a dataset, called, and generating a holistic vision summarizing the filter significance into a single number ( Figure 2 ). As a result, OCNNA transforms the output of a convolutional layer into an array in which the-th component represents the-th filter’s importance. Finally, using parameter, or percentile of significance, we extract the-th percentile of filters in terms of significance, completing the simplification process. The larger , the more strict the filter selection. In consequence, fewer filters will be selected and the new model will be simpler (a smaller number of parameters compared with the original one). As mentioned above, OCNNA can measure the importance of a convolutional filter by extracting hidden insights from a multidimensional array and express it through a number. This process implies heavy computational costs. In this sense, OCNNA is designed to maximize its performance in terms of the time required to complete the simplification process by proposing a parallel computing paradigm. In other words, this entails counting the number of CPUs available and distributing the tasks associated to each filter (prediction, PCA, Frobenius norm, and CV) of the convolutional layer among them, carrying out the calculations simultaneously and, as a result, speeding-up the results. This parallelism is absolutely transparent to the user. Once all the operations are finished, a synchronization process between the different subtasks is accomplished, mapping every result (the significance of the filter) in the correct component of the vector, which represents the importance of each filter in the convolutional layer. It was implemented in Python 3.9, and Tensorflow 2.9 was used as the machine learning framework.\n\nTo assess the performance of OCNNA, we designed a thorough empirical procedure that included different well-known datasets (CIFAR-10, CIFAR-100, and Imagenet) and architectures (ResNet-50, VGG-16, DenseNet-40, and MobileNet) which represent core benchmarks extensively referenced in the literature. Moreover, OCNNA was compared with 20 state-of-the-art CNN simplification techniques, obtaining successful results. This section is structured as follows: The architectures and datasets, metrics, compared state-of-the-art approaches, and training process settings are explained in order to assure the experiments’ reproducibility. Finally, the results and analysis for the CIFAR and Imagenet datasets are presented, comparing them with the other state-of-the-art techniques. M parameters and 16 layers. The input is an RGB image with a size of , which is passed through a stack of receptive field convolutional layers, with padding fixed to 1 pixel. Five max-pooling layers (pixel window of size and stride set to 2), which follow some of the convolutional layers, are included as spatial pooling. The last stack of convolutional layers is followed by three dense layers of 4096, 4096, and 1000 channels. We thoroughly evaluated our CNN building and optimizing scheme. To obtain comparable results with other state-of-the-art approaches, we selected four popular CNN architectures: VGG-16 [ 13 ], ResNet-50 [ 64 ], DenseNet-40 [ 65 ], and MobileNet [ 66 ]. VGG-16 is a convolutional neural network formed byM parameters and 16 layers. The input is an RGB image with a size of, which is passed through a stack ofreceptive field convolutional layers, with padding fixed to 1 pixel. Five max-pooling layers (pixel window of sizeand stride set to 2), which follow some of the convolutional layers, are included as spatial pooling. The last stack of convolutional layers is followed by three dense layers of 4096, 4096, and 1000 channels. ResNet-50, drawing inspiration from the VGG networks, incorporates the idea of residual learning to simplify training by reconfiguring the layers to learn residual functions relative to their inputs, rather than learning functions without references. In practice, ResNet includes shortcut connections and has lower complexity than VGG-16, given the fact that it is formed by M parameters. DenseNet (1 M parameters and 40 layers) connects each convolutional unit as if it were a feed-forward neural network, reducing the number of parameters and diminishing problems such as the vanishing gradient. Finally, MobileNet is a light-weight neural network designed for mobile and embedded vision apps. It has M parameters and 55 layers. (each class consists of 6000 images, with a total of 50,000 images for training and 10,000 images for testing). CIFAR-100 contains the same number of images as CIFAR-10 but 100 classes (600 images each). On the other hand, Imagenet is a dataset formed by 1431,167 annotated images ( ) and 1000 object classes. As we have mentioned, OCNNA requires a dataset to measure convolutional filter importance. In the case of CIFAR-10 and CIFAR-100, we selected of the training images, in other words, 5000 images identically distributed by class. After completing the optimization process, we evaluated the new model’s performance by using the test images. For the Imagenet dataset, we used as the “imagenet_v2/topimages” subset and as test set “imagenet_v2/matched-frequency”. Both of them have 10,000 images sampled after a decade of progress on the original ImageNet dataset, making the new test data independent of existing models and guaranteeing that the accuracy scores are not affected by adaptive overfitting [ To demonstrate the versatility of our approach, we evaluated it by using a core set of widely used benchmark datasets for image classification: CIFAR-10 [ 20 ], CIFAR-100 [ 20 ], and ImageNet [ 21 ]. The CIFAR-10 dataset is an image classification problem with 10 classes formed by 60,000 images with size(each class consists of 6000 images, with a total of 50,000 images for training and 10,000 images for testing). CIFAR-100 contains the same number of images as CIFAR-10 but 100 classes (600 images each). On the other hand, Imagenet is a dataset formed by 1431,167 annotated images () and 1000 object classes. As we have mentioned, OCNNA requires adataset to measure convolutional filter importance. In the case of CIFAR-10 and CIFAR-100, we selectedof the training images, in other words, 5000 images identically distributed by class. After completing the optimization process, we evaluated the new model’s performance by using the test images. For the Imagenet dataset, we used asthe “imagenet_v2/topimages” subset and as test set “imagenet_v2/matched-frequency”. Both of them have 10,000 images sampled after a decade of progress on the original ImageNet dataset, making the new test data independent of existing models and guaranteeing that the accuracy scores are not affected by adaptive overfitting [ 67 ]. These datasets can be found in [ 68 ]. where and represent the number of parameters of the original model and the optimized one, respectively. In any case, we adjusted the metrics (and their presentation) to match those used in the literature, ensuring a fair and precise comparison between OCNNA and other approaches. We measured the prediction performance of the optimized models with accuracy (ACC). In addition, we registered the number of parameters to assess complexity and efficiency in terms of memory requirements and runtime as it is defined for the Green AI paradigm (a lower number of parameters may lead to increased efficiency). We also recorded the remaining parameters ratio (RPR) compared with the original model for compression, as previously performed for other state-of-the-art approaches. A higher parameter reduction ration means a smaller model size and, as a result, a less complex model. The definition of RPR iswhereandrepresent the number of parameters of the original model and the optimized one, respectively. In any case, we adjusted the metrics (and their presentation) to match those used in the literature, ensuring a fair and precise comparison between OCNNA and other approaches. , the momentum to , the learning rate to , and the batch size to 64. All images were augmented by horizontal and vertical flipping, zoom with range between and , rotation range of 180, and fill mode as reflect; in other words, pixels outside the boundaries of the input image were filled according to the following mode [ For the VGG-16, ResNet-50, MobileNet, and DenseNet models training on CIFAR-10 or CIFAR-100, we set the weight decay to, the momentum to, the learning rate to, and the batch size to 64. All images were augmented by horizontal and vertical flipping, zoom with range betweenand, rotation range of 180, and fill mode as reflect; in other words, pixels outside the boundaries of the input image were filled according to the following mode [ 69 ]: We did not train any model on Imagenet due to the existence of publicly available pre-trained models. Table 3, The results on the CIFAR datasets for different architectures are shown in Table 2 Table 4 and Table 5 . The Dataset column shows the learning task; the Architecture column shows the neural network used in the learning task; the Base (%) column refers to the accuracy originally obtained with the aforementioned architecture for the dataset given; Acc (%) is the accuracy obtained after applying the pruning algorithm; RPR means the remaining parameters ratio, where the lower, the better; Acc. Drop is the accuracy loss after pruning, where the smaller, the better. As we have said, we used three benchmark architectures, ResNet-50, VGG-16, and DenseNet-40. In the case of ResNet-50, OCNNA generates a compressed model where just over of the parameters remain ( for CIFAR-10 and for CIFAR-100), while the accuracy loss is very small. As a result, OCNNA is an effective method for compressing CNNs; actually, it achieves the best values for both metrics among all the considered methods. Given the fact that VGG-16 is a very complex network, it might present greater redundancy than the other architectures. In fact, we were able to reduce the model, pruning of the parameters for CIFAR-10 and for CIFAR-100 without any accuracy loss for CIFAR-10 (actually, an improvement of ) and keeping it nearly unchanged for CIFAR-100 ( ). OCCNA achieved the best scores for both metrics in CIFAR-10 and CIFAR-100, with the exception of RPR, where it ranked third. For DenseNet-40, OCNNA again achieved the best results in both metrics and both datasets. In addition, it improved test accuracy for CIFAR-10 ( ) and for CIFAR-100 ( ). Finally, we can observe the same behavior for MobileNet: OCNNA ranked first in both RPR and ACC for both datasets but only in size reduction for CIFAR-10. While not compressing the most for MobileNet built for CIFAR-10, its simplified model improves test accuracy ( ). Overall, OCNNA is the winner according to the results for the CIFAR-10 and CIFAR-100 datasets. The performance of OCNNA and state-of-the-art methods for VGG-16, ResNet-50, and MobileNet on the Imagenet dataset are presented in Table 6 Table 7 and Table 8 . We begin by describing the results obtained for the VGG-16 architecture ( Table 6 ). In this case, the best performing algorithm for compression was MGPF, at the cost of a suboptimal accuracy. OCNNA ranked second in RPR. and ) are the only methods which improve the accuracy (negative accuracy drop) but with an RPR above for SCWC (no data available for ResPrune). For ResNet-50 ( Table 7 ), more extensive experimental results can be found in the literature. To the best of our knowledge, ResPrune and SCWC (and) are the only methods which improve the accuracy (negative accuracy drop) but with an RPR abovefor SCWC (no data available for ResPrune). ), still smaller than OCNNA’s ( ), but the accuracy drop is quite higher ( vs. for OCNNA). In [ ). OCNNA was not as effective as FPWT in terms of RPR, but the accuracy drop for OCNNA was more than four times smaller compared with FPWT ( for OCNNA vs. for FPWT). FPEI-R7 with DR obtains a notable RPR (), still smaller than OCNNA’s (), but the accuracy drop is quite higher (vs.for OCNNA). In [ 42 ] (FPWT), we found the best approach to compressing ResNet-50 (). OCNNA was not as effective as FPWT in terms of RPR, but the accuracy drop for OCNNA was more than four times smaller compared with FPWT (for OCNNA vs.for FPWT). In real-world applications, it is essential to balance performance and compression rates based on varying computational demands, energy consumption constraints [ 39 ], and accuracy requisites. For MobileNet ( Table 8 ), OCNNA outperformed the different approaches of the COP method and the direct simplification of the original model. As a final, overall conclusion, we can assert that OCNNA can obtain simplified CNNs with remarkable complexity reduction while retaining the accuracy or even improving it in some cases.\n\nDeep neural networks have emerged as the leading technique for tackling various challenging tasks in AI. In particular, CNNs have achieved an extraordinary success in a wide range of computer vision problems. However, these models come with high energy demands and are challenging to design efficiently. In this paper, we introduce OCNNA, a novel CNN optimization and construction method based on pruning designed to assess the importance of convolutional layers, ordering the filters (features) by importance. Our approach enables the efficient end-to-end training and compression of CNNs. It is easy to apply and depends on a single parameter , called percentile of significance, which represents the proportion of filters which will be transferred to the new model based on their importance. Only the -th percentile of filters with higher values after applying the OCNNA process (PCA for feature selection, Frobenius norm for summary, and CV for measuring variability) will form part of the new optimized model. OCNNA was evaluated through a comprehensive and detailed experimentation including the best known datasets (CIFAR-10, CIFAR-100, and Imagenet) and CNN architectures (VGG-16, ResNet-50, DenseNet-40, and MobileNet). The experimental results, based on the comparison with 20 state-of-the-art CNN simplification techniques and obtaining successful results, confirm that more efficient CNN models, following typical Green AI metrics, can be obtained with small accuracy losses. As a result, OCNNA is a competitive CNN construction method based on pruning which could ease the deployment of AI models onto edge devices (e.g., IoT devices) or other resource-limited devices."
    },
    {
        "link": "https://medium.com/@heyamit10/a-practical-guide-to-neural-architecture-search-nas-166bb6137edc",
        "document": "When we talk about designing neural networks, you’re probably aware that it’s often a combination of skill, intuition, and countless hours of trial and error. But what if I told you there’s a more efficient way to approach this? Enter Neural Architecture Search (NAS) — a game-changer in deep learning. At its core, NAS automates the design of neural networks, reducing the need for manual architecture tweaking. Imagine a world where you don’t have to manually decide how many layers to use or what type of convolutions to stack — NAS can handle that for you. But why is this so critical? You see, traditional manual design often leads to suboptimal architectures, even for seasoned data scientists. The sheer complexity of modern networks means there are billions of possible configurations. Choosing the best one? That’s a needle-in-a-haystack problem. NAS solves this by searching through these configurations, often uncovering architectures that outperform human-designed ones. Key benefits? Simple: speed, efficiency, and often better performance. You don’t have to spend weeks fine-tuning your network. NAS can automatically find architectures that are not only effective but also tailored to your specific dataset. Real-world applications? Oh, plenty! For instance, Google’s AutoML framework uses NAS to design state-of-the-art models for image classification and language processing. Another prime example is autonomous driving, where NAS is used to design lightweight models that run in real-time on edge devices. So, NAS isn’t just theory — it’s powering some of the most advanced technologies we interact with every day.\n\nNow that you’re motivated to ditch manual architecture design, let’s dive into how NAS actually works. NAS is not a one-size-fits-all solution, and there are different strategies and search spaces that tailor to various types of problems. Think of the search space as the universe of possible architectures NAS can explore. You can have a fixed search space, which might seem limiting but is efficient when dealing with known architectures, like CNNs. Here, NAS explores specific options within predefined parameters. You give it boundaries, and it plays within them. On the other hand, flexible or evolving search spaces let NAS truly flex its muscles. Instead of sticking to predefined structures, NAS can evolve architectures by modifying them, adding layers, or even reshaping connections dynamically. But there’s a trade-off: the more flexible your search space, the greater the computational cost. For instance, in image classification, you might fix your search space to only convolutional layers and pooling operations, but in more advanced settings like object detection, you might want to leave room for more sophisticated layers (e.g., attention mechanisms). You see, designing an efficient search space is about balancing flexibility with practicality. If you open it up too much, the search can become computationally overwhelming. Now let’s talk about how NAS searches through that space. There are several strategies, and each has its pros and cons depending on your task:\n• Reinforcement Learning-based NAS: In this method, NAS treats architecture search like a game — it learns to choose better architectures over time based on rewards (typically, model accuracy). One famous implementation is Google’s AutoML, where RL agents design neural networks for tasks like image classification. The trade-off? It’s slow and computationally heavy. But if you’re working on a problem where accuracy is paramount, RL-based NAS can give you top-tier results.\n• Evolutionary Algorithms: Just like organisms evolve to better adapt to their environment, architectures in NAS can “evolve.” In evolutionary NAS, you start with a population of architectures and let them mutate over time, picking the fittest models. This method shines when you have a large search space but need efficiency. Evolutionary algorithms are often used in scenarios where you can’t afford to run a slow search, like real-time processing or edge computing.\n• Bayesian Optimization: Here’s where things get interesting. Bayesian optimization doesn’t blindly search the space. It models the performance of architectures as a probability distribution, allowing NAS to focus on the most promising architectures. This is great when you don’t have the computational budget to explore a massive search space. You get quicker results, though it may not always find the absolute best architecture. Think of this as a way to balance exploration with exploitation.\n• Gradient-based NAS: Recently, gradient-based methods have gained attention, particularly with Differentiable Architecture Search (DARTS). Instead of treating NAS as a discrete search problem, DARTS relaxes the search space to be continuous, so that you can apply gradient descent to optimize the architecture. This method is fast and efficient, making it ideal for large-scale tasks where traditional methods would be too slow. But beware: it requires careful tuning and is prone to overfitting if not handled properly. Let’s take a deeper dive into DARTS, one of the most exciting innovations in NAS. Imagine being able to use the same techniques that you use to train weights (i.e., gradient descent) to optimize the architecture itself. DARTS turns NAS into a differentiable problem, which allows it to search over architectures continuously and apply gradients to determine which parts of the architecture to favor. You might be thinking, “This sounds too good to be true!” In many ways, it is. DARTS enables NAS to run faster than traditional methods, but it’s not without its challenges. The continuous nature of DARTS can sometimes lead to architectures that perform well during the search but poorly during final evaluation. So, while it’s a powerful tool, it requires expertise in hyperparameter tuning and regularization techniques to prevent overfitting.\n\nWhen it comes to Neural Architecture Search (NAS), theory is great, but as experienced data scientists, we know the real magic happens when you apply these concepts to a real-world problem. So, how do we get started with building a practical NAS project? Before jumping into code, you need to clearly define the problem you’re solving. Whether it’s image classification, object detection, or even something more niche like natural language translation, the goal should be well-defined because it directly impacts how you structure your search space. You’re not just optimizing any random neural network architecture — you’re searching for an architecture that can solve your specific problem in the most efficient way possible. Let’s consider a problem like image classification. For this example, we’ll aim to classify images using the well-known CIFAR-10 dataset. Now, you might be thinking, “CIFAR-10? Isn’t that a pretty basic dataset?” It is, but here’s the deal: CIFAR-10 offers the perfect mix of simplicity and complexity to demonstrate NAS, making it great for prototyping before moving to larger datasets like ImageNet. By selecting a relatively simple dataset initially, you can validate your NAS approach quickly and efficiently before scaling it to more demanding tasks. Plus, it allows you to fine-tune your search space and search strategies without burning through too much computational power. Dataset Selection: Choosing the Right Data for NAS Here’s a key point when you’re picking a dataset for NAS: It’s not just about complexity; it’s about practicality. Sure, you could throw ImageNet at your NAS framework and let it grind through the search process, but that’s going to be costly and time-consuming. Instead, you want a dataset that strikes a balance between enough complexity to challenge your NAS but also manageable enough to produce quick iterations. In our case, we’re using CIFAR-10, a dataset with 60,000 32×32 color images in 10 different classes. It’s well-documented, and importantly, it’s diverse enough to test the capabilities of NAS without overwhelming your compute resources. Once you’ve validated your NAS approach on CIFAR-10, you can then easily scale to larger datasets. Let’s set up CIFAR-10 for our NAS project. As you know, data preparation is a critical step, and for image classification, data augmentation helps your NAS find more robust architectures. import torch\n\nimport torchvision\n\nfrom torchvision import datasets, transforms\n\n\n\n# Data Transformations for augmentation\n\ntransform = transforms.Compose([\n\n transforms.RandomHorizontalFlip(), # Augmentation to generalize better\n\n transforms.RandomCrop(32, padding=4), # Cropping for spatial variability\n\n transforms.ToTensor(), # Converting images to tensors\n\n transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizing pixel values\n\n])\n\n\n\n# CIFAR-10 Dataset\n\ntrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n\ntest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\n\n\n# Data Loaders for batching\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False) As you can see, I’ve used random horizontal flips and random crops — two simple but effective augmentation techniques. These small tweaks can make a massive difference when NAS evaluates architectures, as they encourage generalization rather than overfitting to specific features of the dataset.\n\nThis is where things get really interesting. Now that we have our data ready, let’s shift our focus to building a NAS framework from scratch. Trust me, once you get the core framework in place, you’ll have a solid foundation to tackle almost any neural architecture search problem. Let’s talk about search space. If NAS is the engine, then the search space is the fuel. This is where you define the building blocks that NAS will explore. Essentially, you’re defining the components of your network, and NAS will mix and match them in different ways to find the optimal configuration. For our image classification problem, a typical search space might consist of convolutional layers, pooling layers, activation functions, and normalization techniques. You want to strike a balance here — give NAS enough flexibility to explore meaningful architectures but don’t make the search space so large that it becomes computationally prohibitive. Here’s a pro tip: If you’re working with large datasets, it’s often useful to restrict the search space early on to avoid wasting time exploring architectures that are unlikely to work well. You can gradually expand the search space as you refine your approach. How to Design an Efficient Search Space You might be wondering, “How do I design a search space that’s both efficient and effective?” The key is to break down the architecture into parametric components that NAS can tune. For example, instead of hardcoding a fixed number of filters in a convolutional layer, let NAS decide whether 16, 32, or 64 filters work best. Instead of locking down a specific activation function, let NAS choose between ReLU and LeakyReLU. This flexibility allows NAS to optimize architectures for your specific dataset. Let’s get hands-on. Below is an example of how to define a search space for a simple convolutional network: import random\n\n\n\nclass SearchSpace():\n\n def __init__(self):\n\n self.conv_layers = [16, 32, 64] # Number of filters in Conv layers\n\n self.kernel_sizes = [3, 5, 7] # Various kernel sizes to test\n\n self.activations = [torch.nn.ReLU, torch.nn.LeakyReLU] # Different activation functions\n\n \n\n def sample(self):\n\n # Randomly sample from the defined search space\n\n conv_layer = random.choice(self.conv_layers)\n\n kernel_size = random.choice(self.kernel_sizes)\n\n activation = random.choice(self.activations)\n\n return conv_layer, kernel_size, activation What’s happening here? The class encapsulates the different components of your network (convolutional layers, kernel sizes, activations). When you call , NAS will randomly select from these options, allowing it to explore various architectures.\n• Real-world problems demand real-world solutions. Don’t settle for abstract problems — pick a dataset and challenge that matter.\n• Dataset selection is key. Choose one that balances complexity with practicality. Start with something manageable like CIFAR-10, then scale up as your NAS framework improves.\n• Search space is your secret weapon. A well-designed search space can mean the difference between a day’s search and a week’s. Keep it flexible but not too broad. By the end of this section, you should have a clear idea of how to set up your NAS framework and prepare your dataset. In the next section, we’ll dive deeper into search strategies and how to actually implement NAS on this framework.\n\nSo, you’ve defined your problem, prepared your dataset, and designed a search space. But here’s the real kicker: how do you actually search through this space to find the best architecture? This is where search strategies come into play. The choice of search strategy can make or break your NAS project, determining whether you find an optimal architecture quickly or waste days of compute power chasing subpar results. Let’s walk through a few search strategies, each with its own strengths and limitations, and I’ll show you how to implement them step by step. You might be thinking, “Random? Really?” But yes, sometimes, random search works surprisingly well. In fact, it’s often used as a baseline in NAS experiments because it’s fast and easy to implement. Here’s how it works: You randomly sample architectures from your search space, evaluate their performance, and keep track of the best one. No fancy optimization, no learning, just pure exploration. While it may seem crude, random search can often find architectures that are good enough, especially when the search space isn’t too large. However, there’s a catch. If you’re working with a large search space — say, trying to tune dozens of hyperparameters — random search can become inefficient. You’re essentially shooting in the dark with the hope of hitting a target. When the search space grows, you’re likely to miss more often than you hit.\n• Not scalable: As the search space grows, random search becomes computationally prohibitive.\n• No learning: Random search doesn’t “learn” from past attempts. Every sample is independent, which means it can’t build on previous successes. Let’s look at how you can quickly implement random search in Python for your NAS project: import random\n\n\n\ndef random_search(search_space, iterations=10):\n\n best_arch = None\n\n best_acc = 0.0\n\n for _ in range(iterations):\n\n # Sample a random architecture from the search space\n\n arch = search_space.sample()\n\n \n\n # Evaluate the architecture (this function is problem-specific)\n\n acc = evaluate_architecture(arch)\n\n \n\n # Track the best architecture\n\n if acc > best_acc:\n\n best_arch = arch\n\n best_acc = acc\n\n \n\n return best_arch, best_acc Here’s the deal: Random search is a brute-force method. In this example, picks a random architecture, and assesses how well it performs. We run the loop for a predefined number of iterations, and at the end, we return the architecture with the best accuracy. Simple, but sometimes surprisingly effective!\n\nNow, let’s level up. Imagine if instead of sampling randomly, your NAS could learn which architectures are performing well and adapt its search strategy over time. This is where reinforcement learning (RL)-based search comes into play. In RL-based NAS, the search process is modeled as a sequential decision-making problem. You can think of it like training an agent to explore the search space intelligently, based on the feedback (rewards) it receives after evaluating architectures. Typically, the architecture is treated as the agent’s “action,” and the reward is the model’s performance (e.g., accuracy, latency). Over time, the agent learns to favor actions (architectures) that yield higher rewards, improving its search efficiency. A popular RL algorithm for NAS is Proximal Policy Optimization (PPO), which balances exploration and exploitation. Other options like REINFORCE are also used, but PPO tends to be more stable and efficient. Below is a simplified snippet of how you might integrate reinforcement learning into NAS: class RLEnvironment():\n\n def __init__(self, search_space):\n\n self.search_space = search_space\n\n self.state = None # In RL, the state can be the current architecture's parameters\n\n\n\n def step(self, action):\n\n # Action corresponds to selecting an architecture\n\n architecture = self.search_space.sample(action)\n\n \n\n # Reward is the performance of the architecture (accuracy, etc.)\n\n reward = evaluate_architecture(architecture)\n\n \n\n return reward\n\n\n\n# The actual RL implementation would involve training an agent using PPO or REINFORCE\n\n# This part would include policy updates, rewards accumulation, and exploration-exploitation balance In this example, we’ve set up an where the agent samples architectures based on its action space (in this case, the NAS search space). The agent receives a reward—usually, the model’s accuracy—after evaluating the architecture. Over time, it learns which architectures yield better rewards and refines its search. The full RL-based NAS would involve training an agent to optimize this process. For instance, you’d implement PPO to allow the agent to adjust its actions based on the rewards it receives, using a policy-gradient approach. This is more advanced, but for high-performance tasks, RL can significantly outperform random search.\n\nYou’ve probably heard of DARTS (Differentiable Architecture Search) — it’s one of the most innovative approaches to NAS. Unlike traditional methods, where you sample discrete architectures, DARTS turns NAS into a continuous optimization problem. How? Instead of searching over discrete architectures, DARTS treats the search space as a continuous one, allowing you to apply gradient descent (yes, the same technique you use to optimize neural network weights) to the architecture parameters. This drastically reduces the computational burden because you can now optimize architectures in a differentiable manner, much like training a standard neural network. Here’s how DARTS works: You define a “super-network” that contains all possible architectures in your search space. During training, instead of selecting a specific architecture, DARTS learns a set of weights that represent a softmax distribution over all possible operations. The final architecture is derived by selecting the operations with the highest weights. In DARTS, the search space is continuously relaxed, and the optimization of architectures is performed using standard backpropagation. Let’s see how you can set up a differentiable search space: import torch\n\n\n\nclass DifferentiableSearchSpace(torch.nn.Module):\n\n def __init__(self):\n\n super().__init__()\n\n # List of candidate operations (e.g., Conv layers with different kernel sizes)\n\n self.ops = torch.nn.ModuleList([\n\n torch.nn.Conv2d(16, 32, kernel_size=3),\n\n torch.nn.Conv2d(16, 32, kernel_size=5),\n\n torch.nn.Conv2d(16, 32, kernel_size=7)\n\n ])\n\n \n\n # Weights to represent how important each operation is\n\n self.weights = torch.nn.Parameter(torch.randn(len(self.ops)))\n\n\n\n def forward(self, x):\n\n # Weighted sum of operations, where weights are learned via gradient descent\n\n out = sum(w * op(x) for w, op in zip(self.weights, self.ops))\n\n return out In this setup, we define a simple differentiable search space with a few convolutional operations. The key here is the weights parameter, which DARTS uses to learn which operations are most important. These weights are updated through gradient descent, allowing the architecture search to be fully differentiable. During the search process, you’ll optimize both the network’s weights (like in any standard neural network) and the architecture weights ( in the example). After training, the final architecture is derived by selecting the operation with the highest weight for each layer.\n• Random search is your go-to for quick and dirty exploration. It’s not the most sophisticated method, but it can be surprisingly effective for small search spaces.\n• Reinforcement learning brings intelligence into the search process. By treating NAS as a sequential decision-making problem, RL agents can learn to favor architectures that perform well and adapt over time.\n• DARTS is the cutting edge of NAS. By transforming NAS into a continuous optimization problem, DARTS allows you to apply gradient-based methods to the architecture itself, significantly speeding up the search process. Each strategy has its strengths, so the one you choose will depend on your specific project’s requirements — whether that’s speed, scalability, or absolute performance.\n\nSo, you’ve got your NAS framework running, and it’s churning out potential architectures. But here’s the big question: How do you know which architecture is the best? It all comes down to how you evaluate the models. The evaluation metrics you choose will shape the architectures that NAS favors, and let me tell you — this isn’t just about accuracy. Now, you might be thinking, “Isn’t accuracy enough to evaluate an architecture?” Well, yes and no. Accuracy is certainly important, but when you’re designing models for real-world deployment, you have to consider much more than just how well it performs on a test set. Here’s the deal: in practical scenarios, factors like latency, FLOPs (floating-point operations), and even energy consumption can be critical, especially if you’re deploying models on edge devices or need real-time performance. What good is a model that’s 99% accurate if it’s too slow or resource-intensive to use? Let’s break down some of the key performance metrics that you should keep an eye on:\n• Accuracy: This is still your go-to metric for most tasks, especially when classification is involved. It gives you a sense of how well the architecture generalizes to unseen data.\n• Latency: This is the time it takes for the model to make a prediction. If you’re working in fields like autonomous driving or real-time analytics, latency could be the make-or-break factor.\n• FLOPs: This measures the computational complexity of the model — how many operations are needed to perform inference. Lower FLOPs mean faster inference and lower energy consumption, which is crucial for edge computing.\n• Energy consumption: Particularly relevant for mobile or embedded systems, this metric tells you how power-hungry your architecture is. Efficiency is key when deploying on devices with limited battery life. By combining these metrics, you can holistically evaluate an architecture to ensure it meets both performance and efficiency requirements. Let’s look at how you could implement a basic evaluation function that takes these factors into account. While I’m keeping it simple for this example, in a real-world project, you’d want to track all the key metrics we just discussed. def evaluate_architecture(architecture):\n\n # Step 1: Build the model based on the architecture\n\n model = build_model(architecture)\n\n \n\n # Step 2: Train and evaluate the model (e.g., accuracy, latency)\n\n accuracy = train_and_evaluate(model)\n\n \n\n # (Optional) You could add additional metrics like latency or FLOPs here\n\n latency = measure_latency(model)\n\n flops = calculate_flops(model)\n\n \n\n # Step 3: Return the evaluation results\n\n return {\n\n 'accuracy': accuracy,\n\n 'latency': latency,\n\n 'flops': flops\n\n } In this function, you build the model based on the architecture sampled by NAS, then train it to gather metrics like accuracy. If you’re working in real-time applications, you could also call helper functions to calculate latency and FLOPs, giving you a more complete picture of the architecture’s performance.\n\nBy now, you might be thinking, “All of this sounds great, but NAS is computationally expensive!” And you’re absolutely right. NAS can be a heavy process, especially when you’re exploring large search spaces. So how do we tackle this? Luckily, there are several efficient NAS techniques that can help reduce the computational burden without sacrificing performance. Let’s start with some practical techniques that you can implement to speed up your NAS process. One of the biggest computational challenges in NAS is training each architecture from scratch. Imagine if you had to train hundreds or thousands of models individually — it would take forever! Weight sharing solves this by allowing different architectures to share weights during training, reducing the time spent on redundant computations. In weight-sharing approaches like ENAS (Efficient NAS), the architectures are treated as subgraphs of a larger, “super-network.” When NAS searches for an optimal architecture, it reuses weights from this shared pool, drastically reducing training time. Another effective technique is early stopping. When training a neural network, it’s often clear after a few epochs whether it’s heading in the right direction or not. If you’re training an architecture and notice that it’s not improving after a certain number of epochs, why continue? Early stopping is a great way to cut down on wasted computation. By setting a patience parameter (i.e., how many epochs you’re willing to wait for improvement), you can stop training early if the model’s performance plateaus. Let me show you how you can implement this. Here’s a Python function that trains a model with early stopping: def train_with_early_stopping(model, patience=5):\n\n best_accuracy = 0.0\n\n stop_counter = 0\n\n for epoch in range(epochs):\n\n accuracy = train_and_evaluate(model)\n\n \n\n if accuracy > best_accuracy:\n\n best_accuracy = accuracy\n\n stop_counter = 0 # Reset patience counter if improvement is seen\n\n else:\n\n stop_counter += 1\n\n \n\n # Stop training if no improvement for `patience` epochs\n\n if stop_counter >= patience:\n\n print(\"Stopping early due to no improvement.\")\n\n break\n\n \n\n return best_accuracy In this function, the loop evaluates the architecture and stops training if there’s no improvement in accuracy after epochs. This saves you from unnecessarily training poor-performing architectures for too long. If your search space is too large, a full-blown search from the start can be overwhelming. Instead, you can use progressive search, where you start with a smaller search space and gradually expand it as you gather more information about what works. In practice, you might begin by searching architectures with fewer layers or a limited set of operations. Once you’ve identified promising configurations, you can gradually expand the search to more complex architectures. This is a great way to minimize computational waste and focus your search on high-potential areas. Here’s a strategy you might not expect: Transfer Learning. Instead of training every architecture from scratch, you can leverage pre-trained models to initialize the weights of your NAS models. This allows you to cut down the time spent on training significantly, especially when dealing with large datasets like ImageNet. You might be wondering, “How does this work in the context of NAS?” It’s simple. During the search process, instead of randomly initializing the weights for every architecture, you can load weights from a pre-trained model and fine-tune from there. This gives NAS a head start, especially in tasks like image classification or object detection. Sometimes, you don’t need full-scale training to decide whether an architecture is worth exploring further. This is where multi-fidelity NAS comes in. The idea is to evaluate architectures using low-fidelity approximations — such as training on downsampled datasets, training for fewer epochs, or using smaller versions of the architecture. If an architecture shows promise under these conditions, you can then scale it up and evaluate it properly. By starting with low-fidelity evaluations, you reduce the computational cost of evaluating architectures early in the search process. Once you’ve narrowed down the best candidates, you can devote more resources to fully evaluating them.\n• Evaluating architectures in NAS goes beyond accuracy. Consider latency, FLOPs, and energy consumption to ensure that your model is not only accurate but also efficient for real-world deployment.\n• To reduce computational challenges in NAS, you can leverage techniques like weight sharing, early stopping, and multi-fidelity NAS. These strategies can significantly cut down search time while maintaining robust performance.\n• Transfer Learning can give you a head start, reducing training time by using pre-trained models as a foundation. By implementing these evaluation and efficiency strategies, you’ll be able to optimize your NAS framework to search faster and more intelligently. Now, you’re ready to dive deeper into these techniques and fine-tune your approach!\n\nAlright, now that we’ve covered the mechanics of NAS, let’s take things up a notch. Here’s the thing: even with a well-designed NAS process, your models aren’t going to reach their full potential if you ignore hyperparameter tuning. Why? Because hyperparameters, like learning rates or batch sizes, have a significant impact on model performance. So, how do you integrate hyperparameter optimization with NAS to get the best of both worlds? This might surprise you: NAS and hyperparameter optimization are not the same thing. NAS focuses on optimizing the architecture itself, while hyperparameter optimization focuses on tuning the parameters that control how that architecture learns. Both are crucial for achieving optimal model performance. You might be wondering: “If NAS is already searching through architectures, why not simultaneously optimize hyperparameters?” Exactly. That’s where the magic happens. By combining NAS with techniques like grid search, random search, or Bayesian optimization, you can jointly optimize both the architecture and its hyperparameters — maximizing model performance. Grid search is the most straightforward way to optimize hyperparameters. You create a grid of possible hyperparameter values and systematically try every combination. The downside? It’s computationally expensive and scales poorly as the number of hyperparameters increases. Imagine running a NAS search where you also systematically try different learning rates, batch sizes, and regularization parameters. While it’s exhaustive, grid search can easily become overkill for large search spaces. You’re better off using it when you have a small set of hyperparameters to tune. If grid search feels like overkill, random search might be your friend. Instead of trying every combination, random search samples a fixed number of hyperparameter combinations. Surprisingly, random search often finds optimal or near-optimal solutions without needing to try every possibility, saving you time and compute resources. For instance, if you have a large architecture search space and you’re tuning 5–6 hyperparameters, random search can help explore enough of the space to find great solutions without exhausting your computational budget. Here’s where things get really interesting. Bayesian optimization doesn’t just randomly sample the search space — it intelligently models it based on past performance. In other words, it learns from previous hyperparameter evaluations and predicts which combinations are most likely to improve performance. Think of it like this: while random search is throwing darts at a board blindfolded, Bayesian optimization is like throwing darts with precision, adjusting based on where previous darts landed. It’s faster, more efficient, and often better at finding optimal configurations. Let’s get practical. Below is a simple example of how you can integrate hyperparameter optimization with NAS using grid search. You can easily extend this to random search or Bayesian optimization frameworks like Optuna or Scikit-Optimize. from sklearn.model_selection import ParameterGrid\n\n\n\ndef hyperparameter_search(search_space, param_grid):\n\n best_model = None\n\n best_score = 0\n\n \n\n # Loop over all combinations of hyperparameters in the grid\n\n for params in ParameterGrid(param_grid):\n\n # Sample an architecture from the search space based on hyperparameters\n\n model = search_space.sample(params)\n\n \n\n # Evaluate the architecture using the current set of hyperparameters\n\n score = evaluate_architecture(model)\n\n \n\n # Track the best performing model and its score\n\n if score > best_score:\n\n best_model = model\n\n best_score = score\n\n \n\n return best_model, best_score\n\n\n\n# Example usage\n\nparam_grid = {\n\n 'learning_rate': [0.001, 0.01, 0.1],\n\n 'batch_size': [32, 64, 128]\n\n}\n\n\n\nbest_model, best_score = hyperparameter_search(search_space, param_grid) In this example, we define a grid of possible hyperparameters (learning rates and batch sizes) and use to loop through each combination. For each combination, NAS samples an architecture, and we evaluate it. At the end, we return the model with the best performance.\n\nLet’s face it: while building NAS from scratch is an excellent learning exercise, it’s not always practical for real-world projects. Luckily, there are several powerful open-source NAS frameworks that can save you time, effort, and computational resources. So, let’s talk about some of the best tools available today. If you’re looking for a no-fuss, beginner-friendly NAS tool, look no further than Auto-Keras. It’s built on top of Keras (surprise, surprise) and provides an easy-to-use API for performing NAS without having to get into the weeds of search space design or search strategies.\n• Simple API that integrates seamlessly with Keras and TensorFlow.\n• May not be suitable for highly complex architectures. You might want to use Auto-Keras when you’re short on time and need a solution that just works out of the box. However, for larger, more complex tasks, you may want to look at more flexible frameworks. If you’re ready to take NAS to the next level, Microsoft’s NNI might be your tool of choice. NNI is a highly customizable NAS framework that allows you to design complex search spaces and incorporate various search strategies like random search, Bayesian optimization, and reinforcement learning.\n• Extremely flexible, allowing you to define custom search spaces and strategies.\n• Integrates with popular ML frameworks like PyTorch and TensorFlow.\n• Can scale from local machines to cloud clusters.\n• Requires more setup and customization, which may slow down initial experiments. NNI is ideal when you’re working on a project that demands fine-tuned control over both the search space and the search strategy. It’s especially useful when deploying NAS on large-scale, distributed environments. When we’re talking about state-of-the-art NAS at scale, Google’s AutoML comes to mind. AutoML is designed for enterprises and large-scale projects where you need the highest level of accuracy and efficiency. Powered by Google’s infrastructure, it provides advanced NAS capabilities for tasks like image classification, object detection, and natural language processing.\n• Extremely accurate, as seen in several high-profile AutoML competitions.\n• Integrated with the Google Cloud Platform for seamless deployment.\n• Expensive, especially for small teams or individual researchers.\n• Limited transparency into the inner workings of the NAS process. AutoML is great for organizations that need the absolute best performance and are willing to pay for it. It’s not just a tool for research — it’s used in real-world applications like autonomous driving and healthcare diagnostics. Pros and Cons of Pre-Built Tools vs. Custom NAS So, which approach is best for you? Should you go with a pre-built NAS tool or design your own custom framework from scratch?\n• Pre-built tools like Auto-Keras, NNI, or AutoML are fantastic when you need to get results quickly or if you don’t have the resources to build a custom NAS framework.\n• Pros: Fast to set up, easy to use, and often come with built-in optimizations.\n• Cons: Limited flexibility and control over the search process. Pre-built tools might not work for highly specific or niche problems.\n• Building NAS from scratch gives you total control over every aspect of the search, allowing you to tailor the framework to your exact needs.\n• Pros: Maximum flexibility and customizability. Perfect for research projects where cutting-edge methods or novel architectures are required.\n• Cons: Takes significantly more time and effort to develop and optimize."
    },
    {
        "link": "https://sciencedirect.com/science/article/pii/S0950705124006002",
        "document": ""
    },
    {
        "link": "https://nature.com/articles/s41598-021-98978-7",
        "document": "The design of neural architecture to address the challenge of detecting abnormalities in histopathology images can leverage the gains made in the field of neural architecture search (NAS). The NAS model consists of a search space, search strategy and evaluation strategy. The approach supports the automation of deep learning (DL) based networks such as convolutional neural networks (CNN). Automating the process of CNN architecture engineering using this approach allows for finding the best performing network for learning classification problems in specific domains and datasets. However, the engineering process of NAS is often limited by the potential solutions in search space and the search strategy. This problem often narrows the possibility of obtaining best performing networks for challenging tasks such as the classification of breast cancer in digital histopathological samples. This study proposes a NAS model with a novel search space initialization algorithm and a new search strategy. We designed a block-based stochastic categorical-to-binary (BSCB) algorithm for generating potential CNN solutions into the search space. Also, we applied and investigated the performance of a new bioinspired optimization algorithm, namely the Ebola optimization search algorithm (EOSA), for the search strategy. The evaluation strategy was achieved through computation of loss function, architectural latency and accuracy. The results obtained using images from the BACH and BreakHis databases showed that our approach obtained best performing architectures with the top-5 of the architectures yielding a significant detection rate. The top-1 CNN architecture demonstrated a state-of-the-art performance of base on classification accuracy. The NAS strategy applied in this study and the resulting candidate architecture provides researchers with the most appropriate or suitable network configuration for using digital histopathology.\n\nDeep learning (DL) models represent a family of machine learning algorithms that assign the task of feature extraction and classification to the machine, thereby eliminating semi-autonomous feature extraction. Although the application of the feature extracted may not only be applied to image classification tasks, the DL models have achieved an impressive performance in image classification1. Nevertheless, most of the outstanding performances recorded by DL models were largely dependent on handcrafted neural networks requiring some human expertise and domain-specific knowledge. This limited the possibility of designing best-performing networks for application to new domains because amateurs would have to rely on pre-trained DL models, an approach referred to as transfer learning. In addition to that challenge, a significant effort is required to manually design deep neural network architecture as it is a laborious task, often limiting the exploration of network search spaces. The reliance on human expertise in achieving state-of-the-art architectures resulting from this manual approach is due to the use of manual backbone architectures or micro building blocks2. A new research field, namely neural architecture search (NAS), aimed at using reinforcement learning (RL) or optimization algorithms to automate the design of DL architecture, has been proposed3. The NAS technique allows for the design of high-performing models by using search strategy based on RL or optimization algorithms to search and design neural architectures iteratively. Initial candidate solutions (neural architectures) are generated based on a constrained formal definition of a search space allowing the search strategy to apply an evaluation function in realigning the networks during iteration. A search space, search strategy, and evaluation function are the three components of a NAS model that allows for automating neural architecture engineering. Studies have shown that DL architectures, convolutional neural networks (CNN) specifically engineered using these approaches, have outperformed the handcrafted architectures applied to some problems4,5,6. The NAS methods allow for obtaining the best performing CNN design suitable for a classification or learning problem for which the model is trained. There is an overlap between the application of optimization algorithms in the tuning of hyperparameters and the use of optimization algorithms in automating neural architecture design. The former, referred to as hyperparameter optimization, aims to tune the hyperparameter of the already designed neural network. In contrast, the latter describes NAS, which embodies search space, search strategy and evaluation operations. In building search space for NAS, sequential layer-wise, cell-based, hierarchical structure, and memory-bank representations have been applied in literature7. These representations generated a population of networks that NAS-based search (optimization) algorithms sample to obtain the best performing network. The most frequently used algorithms for the search process in NAS are random search (RS), reinforcement learning (RL), evolutionary algorithms (EA), progressive decision process (PDP), and gradient descent (GD). The EA approach supports neural network topologies' evolvement using an algorithm such as the genetic algorithm (GA). The EA approach mirrors the novel Ebola virus disease propagation optimization model proposed in this study. To reward the outcome of the search algorithm, evaluation strategies are employed as feedback to the search algorithm for improving its task of outputting high-performance candidate architecture. Strategies such as the full training of all candidate solutions (networks) from scratch; training with a smaller dataset with fewer iterations (proxy evaluation); weight sharing among semblance networks; and one-shot architecture with sharing of weight parameters8 have been widely used for evaluating the performance of potential solutions. On the other hand, digital histopathology images are digitized images curated from the examination of biopsy samples on a microscopic slide for detecting cancer growth, a process known as histopathology. These digital histopathology images present a difficult deep learning problem compared to digital mammograms images9. The latter category of images often captures a case-based representation using an image sample, while in the former, a patient case is represented by large sets of images resulting from different observations of biopsy situations. Additionally, a list of subtle signs of malignancy is required to be checked to rule out benign cases in histopathology images. For instance, detecting the presence of disruption of basement membranes, marked cellular atypia, metastasize, and mitosis is an important indicator of breast cancer in histopathology images. Moreover, pathologists are expected to apply their years of experience to observe these images, classifying them as normal tissue, benign tissue, in situ carcinoma, and invasive carcinoma. Classification of these tissues often presents a complex task owning to background structures and heterogeneity in such images10. However, a gold standard for detecting breast cancer is the use of histopathology images above mammography images11. Ensuring the use of this standard will help improve the detection of breast cancer which accounts for about 32% of all cancer cases12. Finding an optimal neural architecture for this learning problem often proves difficult and daunting even for those with expertise in neural network design. The application of the NAS approach in designing the best performing neural architecture for this task remains promising. However, finding candidate neural architecture to address the learning process that uses histopathology images and generates an efficient potential search space is extremely challenging. It is argued that the efficiency of a search space determines the quality of neural architectures that a NAS model can output13. Also, the limitations often placed on the size of this search space have mostly inhibited the upper bound of the optimal neural architectures14. This echoes the concern of Garg et al. 2 which noted that current NAS methods depend heavily on manual efforts in the search space design and are still being prototyped after the approach used in building models before the advent of the NAS2. In addition, manually tweaking network configuration and hyperparameters is time-consuming and challenging. Applying an optimization algorithm to fine-tune hyperparameters alone might not be sufficient, hence the need to improve the search strategy of NAS by designing a complete CNN architecture using the enhanced NAS model. Besides these issues of search space and search strategy and the viability of NAS notwithstanding, we found no study investigating the use of NAS models to the task of breast cancer histopathology dataset except those using a manual approach15,16,17,18,19,20,21,22,23,24,25,26. Considering the great benefit the neural network architecture holds in detecting and staging breast cancer using histopathology images27, we seek to address the research question: Is it possible to generate new state-of-the-art CNN architecture using NAS model, driven by biology-based optimization strategy, for solving classification problem on histopathology images? This study proposes a new NAS model to generate candidate CNN architecture for detecting breast cancer using histopathology images to address the aforementioned problems. We designed a novel block-based stochastic categorical-to-binary (BSCB) algorithm for generating and encoding CNN architectures in the search space. Also, we investigated the performance of our recently proposed optimization algorithm28, namely, the Ebola optimization search algorithm (EOSA) for the search strategy as compared to other existing metaheuristic optimization approaches. The study's novelty involves designing a new NAS model, the BSCB algorithm, and the enhancement of EOSA to support the formalization of solutions as CNN architectures. Secondly, this paper represents the first study to have applied the NAS model to the complex problem of classifying digital histopathology images for the detection of breast cancer. Moreover, the study aims to obtain the best performing CNN architecture to improve classification accuracy and reduce breast cancer false-positive rates in digital histopathology images. The main contributions of this study are elaborated as follows:\n• Propose a new block-based stochastic categorical-to-binary (BSCB) algorithm for generating initial solutions and an encoding scheme for formalizing the solutions as neural networks.\n• Propose the application of a novel bio-inspired metaheuristic algorithm (EOSA) to perform the task of adaptive search strategy for best performing neural architectures.\n• Implement efficient image preprocessing methods using a normalization algorithm before applying the images for training.\n• Evaluate the proposed approach by performing a wide range of extensive experiments and comparisons with existing state-of-the-art CNN architectures and other relevant recent studies dealing with the detection of breast cancer using histopathology images. The remaining sections of the paper are organized as follows: Sect. 2 presents an overview of the Ebola Optimization Search Algorithm (EOSA) and related studies in NAS; Sect. 3 presents the methodology applied in this study; Sect. 4 presents the parameter configuration and datasets for the experimentation; Sect. 5 presents the results obtained and discussion on findings; and Sect. 6 presents the conclusion on the relevance of the study.\n\nOverview of EOSA and review of related studies This section presents an overview of the optimization algorithm proposed for the NAS search strategy phase as applied in this study. The mathematical model and a summary of the Ebola optimization search algorithm (EOSA) procedure are provided in this section to conceptualise its initialization, exploitation, and exploration mechanisms. In addition, we present a review of studies focused on automation of neural network architecture design, emphasising those approaches aimed at image classification. Oyelade and Ezugwu proposed a novel nature-inspired metaheuristic algorithm called Ebola Optimization Search Algorithm (EOSA) which is based on the propagation model of Ebola virus disease28. The formalization of the EOSA algorithm is achieved in the following procedure:\n• Initialize all vector and scalar quantities which are individuals and parameters. Individuals in the sets: Susceptible (S), Infected (I), Recovered (R), Dead (D), Vaccinated (V), Hospitalized (H), and Quarantine (Q) with their initial values.\n• Set the index case as the global best and current best, and compute the fitness value of the index case.\n• While the number of iterations is not exhausted and there exists at least an infected individual, then\n• For each susceptible individual, generate and update their position based on their displacement. Note that the further an infected case is displaced, the more the infection number, so short displacement describes exploitation, otherwise exploration.\n• Compute the number of individuals to be added to H, D, R, B, V, and Q using their respective rates based on the size of I\n• Select the current best from I and compare it with the global best.\n• If the condition for termination is not satisfied, go back to step 6.\n• Return global best solution and all solutions. The mathematical model of the procedure above follows: update of Susceptible (S), Infected (I), Hospitalized (H), Exposed (E), Vaccinated (V), Recovered (R), Funeral (F), Quarantine (Q), and Dead (D) is governed by a system of ordinary differential equations derived based on those in29,30. Differential calculus is a branch of calculus which in turn is a branch in mathematics. The former deals with the rate of change of one quantity with respect to another, while the latter deals with finding different properties of integrals and derivatives. In our case, the application of differential calculus intends to obtain the rates of change of quantities S, I, H, R, V D, and Q with respect to time t. Hence, the Eqs. (1, 2, 3, 4, 5, 6, and 7) for S, I, H, R, V D, and Q respectively as follows: Table 1 presents the definition of the parameters and symbols used in the EOSA model design for the proposed bio-inspired neural architecture search. Table 1 A description of notation and coefficients used in Eqs. (1)–(7). In Table 1, we summarise the notations or coefficients used in Eqs. (1)–(7). This study proposes to adapt the EOSA algorithm to searching neural network architectures for improved classification tasks. Meanwhile, we review related studies to investigate the approach adopted for the optimization stage of the search strategy of recent NAS models. The following sub-section reveals our findings from this review. Neural architecture search (NAS) models consist of search space, search strategy and evaluation strategy. Several studies in the literature have demonstrated variant techniques for formulating each of the components of NAS. In this section, we present the review in chronological order to understand research trends in the field. Cortes et al.31 proposed a NAS framework, namely AdaNet, which applied adaptive structural learning technique for the search strategy. The learning strategy utilized a data-dependent generalization method which successfully learnt both the structure of the network and its weights automatically to yield an optimal network structure. In a related study, Negrinho and Gordon32 applied modular language techniques for the design of the search space for their NAS framework. The technique allows for populating the complex search spaces with representations of CNN architectures and their hyperparameters. Experimentation was done using three search algorithms, namely random search, Monte Carlo tree search (MCTS), and sequential model-based optimization (SMBO) over the search space. Garg et al.2 proposed an approach called ReNAS, which represents the search space for architectures as a direct acyclic graph (DAG), which consists of basic operations. The resulting graph was then mapped to a neural network to search for candidate solutions using a differentiable architecture search approach. The results obtained in the study showed that although the approach outperformed handcrafted architectures, itcould not, however, achieve superiority over state-of-the-art NAS methods but was competitive in performance. Wang et al.33 addressed the shortcomings of using only the Hyperband algorithm for searching optimal neural architecture. As a result, the Hyperband algorithm and Bayesian optimization technique were hybridized to design the search strategy of NAS. The hybridization aims to build a memory to recall previous configurations when sampling the next trial configuration in searching for the optimal CNN configuration. In another study focused on improving the NAS search strategy, Huang et al.34 proposed using a greedy technique to enhance the neural architecture search. The resulting GNAS framework was applied to address the problem of finding optimal CNN architecture for extracting features from images by exploiting an efficient greedy search approach. The greedy technique achieves its search strategy by splitting a bigger neural architecture into smaller versions optimized in a stepwise manner. The GNAS automatically discovered optimal tree-like CNN architecture for learning and extraction of multi-attribute. Using another approach for search strategy, Weng et al.35 applied differential architecture (DARTs) search method to design CNN architecture. The resulting search strategy was built into a convolutional neural search architecture (CNAS) framework. The proposed DART-based search strategy finds architectures from a search space utilizing both shuffle operation and squeeze-and-excitation. We, however, found their approach to be sub-optimal when compared with the use of evolutionary-based optimization techniques, as seen in the works of Erivaldo et al.36 and Liu et al.37. The two studies approached the design of search strategy of their NAS framework using particle swarm optimization (PSO) and Genetic Algorithm (GA)-based, respectively. The optimization mechanism of PSO was employed for a direct encoding strategy. In contrast, the optimization task of GA was supported by an experience-based greedy exploration strategy and transfer learning techniques. Considering the cardinal role of the search strategy in the NAS model, we reviewed recent studies to observe the approaches applied. For instance, Krishna et al.38 proposed two techniques, namely reinforcement learning strategy and attention-based mechanism with simplified transformer block method for the search strategy of NAS and improving hyperparameter candidate neural architecture, respectively. The study uses a two-stream attention-based mechanism to model hyper-parameter dependencies and a simplified transformer block. In contrast, the second method aimed to remove layer normalization, the former models the policy network for searching the space. The authors reported that the performance of their method surpasses most methods in NAS-Bench-101 benchmarked models. A similarity to the works of Erivaldo et al.36 and Liu et al.37 is seen in the study by Calisto and Lai-Yuen39. This study applied evolutionary algorithms to search for neural networks in the search space to discover high-performing and efficient neural architecture. The optimization algorithm is rewarded if it can discover architectures with improved classification accuracy and reduced hyperparameters size. The resulting architecture which comprises a self-adaptive 2D and 3D ensemble of FCNs used for 3D medical image segmentation was code-named AdaEn-Net. In a similar approach, Chen and Li40 proposed using an evolutionary algorithm for the search strategy of a search space. The search space is composed of a major super-network whose weight is shared with sub-network architectures in obtaining an optimal candidate network. This optimal architecture is derived from a collection of most performing or excellent architectures by examining the commonalities of these architectures40. Wang et al.41 proposed what is referred to as DC-NAS, which was derived from a divide-and-conquer (DC) approach to the NAS problem. The study applied the DC method to obtain the best performing sub-graphs of potential complete network architecture. Meanwhile, sub-graphs are first clustered based on their similarity so that best performing sub-graphs in a cluster are merged with other optimal sub-graphs in related clusters to form a new architecture. The resulting optimal sub-graphs combine to form a new neural architecture that is effective and efficient. On the issue of NAS search space, we found the work of Cassimon et al.42 which uses a cell-based representation approach for search space very interesting. The method adapted reinforcement learning to optimise cells for detecting the two types of networks, namely Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN). The study considered a network optimal if it could successfully predict spoken words and classify RGB images on an embedded device. The study's main contribution is the proposal of an efficient neural architecture search (ENAS) fitted for embedded devices and with improving performance network architecture. In a similar approach, though using a different NAS-based method, Fan et al.43 jointly searched for operations like LSTM and CNN from a search space using gradient-based neural architecture optimization (NAO) technique. The search space combined two heterogeneous spaces with network operations space and dropout space. The former consists of basic network operations, while the latter space consists of dropout rates. The resulting networks are those whose architectures and hyperparameters are well optimized for neural machine translation (NMT). An improvement to the work of Cortes et al.31 is reported in the study of Dai et al.44. The authors employed a NAS framework driven by the use of the AdaNet technique. The focus of the improvement is achieving a better search space and search strategies for obtaining the optimal structure of the CNN architecture and optimising the weights of the CNN architecture. AdaNet utilizes a simple linear model representing the search space and then gradually augments the network with more neurons and additional layers until an optimal network architecture is obtained. Each step in building the resulting architecture requires that a Gradient-decent-based optimization method using Momentum be applied. The study's outcome is a CNN architecture used for three classes (3-hinge gyral vs 2-hinge gyral vs sulcal) classification in f-MRI signals classification problem. In a different approach, Gheshlaghi et al.45 proposed a NAS model by applying the binary gate method to search strategy through stacking of cells upon cells of sub-networks using primitive operations. These cells consist of Down-Sampling Cell (DownSC) and Up-Sampling Cell (UpSC) whose designs are automated into the NAS process. The resulting optimal neural network architecture is expected to outperform handcrafted architecture, which is purposed for the same task of retinal layer segmentation in Optical Coherence Tomography (OCT) scans. Chen et al.46 proposed a single-stage NAS framework named you only search once (YOSO) for automating the process of finding the optimal deep neural network (DNN), which are used for co-designing of software/hardware. The need for co-design of software and hardware by the resulting DNN further swelled the volume of the search space with hyperparameters of the DNN and hardware design parameters. The study applied reinforcement learning with LSTM for the search strategy. The resulting NAS framework applied a multi-objective reward system aimed at maximizing accuracy, power, and QoS. Meanwhile, several DNNs are generated from basic operations to formulate the search space. An interesting aspect of the study is the use of an auxiliary HyperNet that voids the training of candidate DNNs before applying resulting weights for evaluating their performances in terms of accuracy. In another study, Guo et al.47, proposed a variant of NAS capable of generating neural architectures using an inference model. The neural architecture generating (NAG) model learns from a Pareto frontier, which guides optimal architectures based on the given budget for the target system on which the resulting architecture is expected to be used. On the other hand, Zhang et al.48 addressed the problem of the non-convexity of NAS through the use of an adaptive, scalable neural architecture search method (AS-NAS). The scalability of AS-NAS was achieved through a search strategy that combined a simple reinforcement learning, namely: reinforced I-Ching divination evolutionary algorithm (IDEA) and variable-architecture encoding strategy. In a similar approach to Krishna et al.38 and Weng et al.35, though an improvement on the approach, He et al.49 proposed a special kind of NAS model called attention-guided differentiable architecture search (A-DARTS), which adopts a mechanism for reducing the sensitivity of initialization of searched space. Also, Xu et al.50 improved the efficiency and stability of searched networks using the Partially-Connected DARTS (PC-DARTS) approach. The PC-DARTS improves the search strategy by randomly selecting a small subset of channels for partial channel connection to overcome over-fitting the search networks. Several studies have proposed new variants of the NAS model. For instance, Ru et al.51, applied the technique of Bayesian optimization (BO) to the design of the NAS model to obtain a new model known as interpretable neural architecture search (INAS). The proposed INAS uses graph-like search spaces while combining the Weisfeiler-Lehman graph kernel with a Gaussian process surrogate with BO for the search strategy. Fu et al.52, addressed the problem of incremental learning in the classification of images through the approach of neural architecture search for incremental learning (NASIL). This was done by using reinforcement learning, parameter-sharing mechanism, and Long Short-Term Memory (LSTM). Also, Lin et al.53 added novelty to the approach of NAS by improving the evaluation strategy, which replaces an accuracy predictor with zero-shot in the ranking of searched architectures. The resulting value from the zero-shot operation is maximized using an inference budgets model called Zen-NAS. On the other hand, Liu et al.54, applied an evolutionary method to optimise weight-sharing parameters when searching for optimal neural architectures. This search strategy, called continuous particle swarm optimization (CPSO-Net), computes the gradient of networks resulting from shared parameters of candidate operations to obtain candidate architecture. Lastly, Liang et al.55, applied a variant of NAS to generate optimal feature pyramid networks (FPNs). The resulting One-Shot Path Aggregation Network Architecture Search (OPANAS) approach uses a one-short strategy for searching for candidate FPNs that are drawn from DAG-based FPNs search space. The review presented in this section, and summarized in Table 2, demonstrates that different methods have been applied to improve the components of the NAS model. The components which have received more research attention are the search space encoding strategy and the search strategy. Our findings revealed that most of the studies had applied reinforcement learning techniques, evolutionary and use of metaheuristic algorithms. We discovered that the most promising approach is seen in studies that used evolutionary or computational biology methods for search strategy. Hence, this study aims to improve the NAS search strategy by using EOSA, a bio-inspired optimization algorithm, to generate optimal neural architecture in classifying histopathological images to detect breast cancer. In addition, a novel search space encoding algorithm is proposed to allow for good coverage of the potential CNN architectures. The following section details the search space and search strategy proposed in this paper.\n\nThis section is focused on the design methods of the three (3) components of the neural architecture search (NAS). First, we present our proposed NAS model which demonstrates the interoperability of the 3 components. Secondly, the design of a novel search space encoding algorithm that defines a population of initial CNN solutions is discussed. Thirdly, the neural search strategy which is based on the Ebola optimization search algorithm (EOSA) method, is presented. Fourthly, we demonstrate how the multi-objective evaluation strategy is computed and how its results are passed back to the search strategy for refinement purposes. This sub-section gives a high-level overview of the proposed NAS model, which shows basic operations for the search space, the search strategy, and the performance evaluation strategy. The overall aim of the model is to guide the selection process of best performance arbitrary CNN architecture, from the search space, for solving the classification problem. The proposed NAS model is presented in Fig. 1 and shows the three major components of NAS. In addition, a mechanism for evaluating the best performing architecture resulting from the search strategy is also provided. The following is a brief discussion of each component:\n• Search space The proposed model shows how an encoding scheme is used to generate n potential initial solutions, representing CNN architectures. The proposed encoding scheme aims to create a pool of potential configurations of basic operations and hyperparameters of n CNN architectures that are capable of yielding the best performance.\n• Search strategy A bioinspired EOSA-based search strategy iteratively optimizes each CNN solution from a pool of potential solutions located in the search space. For each iteration, the configuration of each CNN solution is improved towards learning the classification problem. This is attained in conjunction with a mechanism for evaluating performance based on high accuracy, reduced loss value, and low latency.\n• Evaluation strategy We designed a mechanism for measuring and estimating the performance of CNN models resulting from the optimization operations on the search space. This allows for passing a kind of reward to the search algorithm to support the process of finding a candidate CNN solution. To minimize the computational cost of the evaluation, we trained the CNN models for few epochs and computed the result of the objective functions, namely classification accuracy, loss function, and CNN architectural latency.\n• Evaluation of top-performing CNN models After an exhaustive optimization process, the top-5 performing CNN models are chosen from the n solutions. These top-5 are subjected to further comparative analysis to measure and discover their capability in solving the classification problem—the problem of detecting the presence of breast cancer in histopathological images. Here, the top-5 CNN architectures are subjected to full training using the complete datasets. The proposed EOSA-NAS model consisting of four components: the search space, EOSA-NAS search strategy, evaluation strategy and the breast cancer detection module using the top-5 and top-1 CNN architectures. The following sections present detail on the design and applicability of each component in the NAS model illustrated by Fig. 1. The quality of a search space determines the performance of both the initial and candidate CNN solutions in NAS models. Also, the encoding scheme applied to a search space directly impacts the complexity of the neural search strategy. Hence, it is necessary to carefully choose the technique to represent a search space in a NAS model. In this section, we present the design of a novel search space encoding initialization and encoding scheme. The design is based on a block-based approach. Firstly, the proposed encoding strategy is designed to generate potential initial CNN solutions exhaustively. Secondly, the design also models each CNN solution in such a manner as to allow the evaluation of the multi-objective functions inexpensively. Thirdly, the scheme provides scalable and easy navigation within the search space using the search space algorithm. We propose a block-based stochastic categorical-to-binary (BSCB) encoding scheme that maps each unique parameter label to an integer value when constructing the search space. The categorical feature or parameter is first converted into a numeric value using an ordinal encoder. This strategy allows for digitizing each convolutional operation and hyperparameters of the CNN solution, which then allows for the efficient representation in the solution space. Once the categorical transformation is achieved, we binarize the resulting integer values. Each binarized value is then bounded within its lower and upper bounds to ensure that it represents a valid CNN architecture. The encoded parameters are then used for building a multi-block-based schematic representation of a CNN model. The resulting blocks are stacked in an ordinal fashion based on the traditional approach of designing CNN architectures. A well-stacked group of blocks represents a potential CNN solution generated into the search space. The implication of this is that CNN architectures are designed on the fly with no prior handcrafted configurations. To achieve the encoding scheme described in the previous paragraph, we provide a list of potential parameters from which blocks are encoded. These parameters, listed in Table 3, represent the convolutional operations and hyperparameters of a vanilla CNN model. The listing allows for a wider range of combinations of values for each parameter. This outcome is a pool of rich potential initial CNN solutions for use by the search strategy. Table 3 Categorization of parameters based on the block encoding scheme for representation of the hyperparameters of convolutional neural network. The list of parameters that constitutes the search space includes the batch size of samples used for input, learning rate, optimization algorithm, the number of convolutional layers, number of kernels, kernel size, the activation function of each convolutional layer, the pooling type and size, the number of dense layers, the connectivity pattern, the activation function, weight regularization techniques, and the dropout for each dense layer. To generate potential CNN solutions into the search space, the following describes how the proposed block-based encoding scheme utilizes these parameters as defined in Table 3. First, we note that when required for generating an arbitrary CNN solution, each parameter is derived using Eq. (8). Moreover, an arbitrary CNN solution combines a number of these parameters to build its blocks: where P represents the ith parameter in the cth category and the jth parameter in the list of parameters (P) to be passed to the encoding algorithm. Note that for each parameter, the ub and lb represent the upper and lower bounds, respectively. A corresponding value for each parameter is computed by generating a random number, multiplying by the difference of ub and lb, and then adding it to the lower bound. Once these values for all parameters are obtained, we proceed to the block encoding for generating CNN solutions. In Eq. (9), we show the complete encoding of a CNN solution where each block is composed by some P . This potential auto-generated CNN solution consists of blocks of different structures arranged in an ordinal pattern to reflect the traditional architecture of CNN. Furthermore, to form the search space for the neural search algorithm, several of this CNN are generated and represented as seen in Eq. (10), where CNN represents a collection of CNN solutions in the search space A predefined number of blocks to be generated for each category are defined in Table 3. The algorithm iterates over each category and digitalizes its parameter, and computes the corresponding value of the binarized parameter mapped to it so that each category translates to a block. Note that the structures of an arbitrary block in solution might not have the same parameter value as another in the same solution. This representation allows for a radial coverage of the potential solution space to allow for an effective and efficient search space. The binarized parameter and its corresponding category are denoted by vector v such that \\({v}_{b}\\in {\\{0, 1\\}}^{n}\\) as detailed in Eqs. (11) or (12). A general structural representation of a CNN architecture using the encoding scheme is shown in Fig. 2. While blocks 1, 2, … n-1, n form the basic structural representation for each potential CNN solution, we note that each solution could represent a more complex structure than Fig. 2. Algorithm 1 presents a pseudocode of the technique for generating all CNN solutions into the search space. A generic representation of an encoded CNN architecture based on the parameters covered by the search space. The algorithm generates n solutions for the search space by using the block-based encoding scheme. This is done by first carrying out category-based parameter extraction so that extracted parameters are then digitized. Meanwhile, the equivalent value for each parameter is computed before mapping them in a corresponding parameter-category association. Finally, blocks are formed from such mappings, which are then translated and chained into potential CNN solutions. In the next section, the application of the neural search algorithm to the search space is described in detail. The search strategy proposed in this study is based on the Ebola optimization search algorithm (EOSA). This allows for widening the search operation in the direction of both exploration and exploitation. The outcome of the search process often yields the best performing CNN architecture for the detection and classification of breast cancer using histopathological images. The resulting CNN search algorithm is henceforth referred to as the EOSA-NAS algorithm. The EOSA-NAS algorithm explores the search space to obtain a candidate CNN architecture suitable for addressing the classification problem. The approach ensures that irrelevant candidate architectures are lined behind the potential architectures. The search algorithm first initializes the compartments to empty sets: Susceptible (S), Exposed (E), Infected (I), Hospitalized (H), Recovered (R), Vaccinated (V), and Quarantine (Q). Thereafter, a variable is created to keep track of the top performing architectures of each iteration. The set S contains all potential solutions (CNN architectures) in the search space, ranked according to their performance based on evaluation strategy so that the most performing architectures are at the head of the queue. The CNN architecture or solution at index 0 is assigned to the exposed E set and eventually to the I set. The position of each solution is updated using (13). where ρ represents the scale factor of displacement such that individuals \\({mI}_{i}^{t}\\) and \\({mI}_{i}^{t+1}\\) represents the updated or current position and previous position at time t and t + 1, respectively. M(I) is the movement rate made by individual solutions, as shown in Eqs. (14) and (15). The search strategy is able to search within the neighborhood threshold (exploitation) using the short distance movement, srate. Also, the algorithm can search beyond the neighborhood threshold (exploration) using the long distance movement, lrate. Both srate and lrate are regulated by neighborhood parameters. For instance, if the computed neighborhood parameter is above 0.5, it is assumed the infected individual (solution) has moved beyond the neighborhood, hence the exploration phase. Otherwise, it is assumed it remains within the neighborhood, hence the exploitation phase. With this mechanism, candidate solutions or CNN architectures evolve and are placed in the I set for use in the next operation. The next operation mutates the configuration of the solutions or CNN architectures for improved performance. This mutation or optimization process is guided by the need for solutions to learn the classification problem. Every infection operation weakens the immunity of the individual (CNN architecture). The configurations of any CNN architecture in I are represented in Eq. (16); solutions (CNN architectures) which have recovered (R) have their immunity strengthened as shown in Eq. (17), and those dead individuals (D) are replaced by new solutions; individuals or solutions which were not infected are maintained in S. where NA stands for neural architecture, which is the same as solutions, cfactor is the rate of change of the structure as determined by neighborhood value, and l is a sample drawn from a uniform distribution in the range of [−1,1]. The resulting value from the evaluation of Eqs. (16) and (17) affects the operations defined by each parameter in all blocks, as shown in Fig. 2. The procedure described by the mathematical model above is summarized in Algorithm 2. The use of back-arrow notation ( ←) represents storage or assignment statement, while the combined use of back-arrow and plus notations (+ ←) represents cumulative storage of values in a variable. This algorithm outlines the call to the initialization of the search space, the iteration through a given epoch for the evolvement of improved CNN architecture or solutions, and the application of the multi-objective function in obtaining the best solution. The last line returns a list of solutions representing CNN architecture with the top best at the head of the queue. The search strategy ensures that all potential architectures are evaluated based on three (3) objective functions that yield a one-value metric. The following section details this evaluation strategy. The selection of the current best, at any time t, is computed on the set of infected individuals at that time t, whereas selection of the global best is based on the best performing CNN solution at the end of the training process. Performance is measured using classification accuracy on CNN training and validation, the latency of the CNN architecture, and the loss function (categorical or sparse cross-entropy). This multi-objective approach is motivated by findings from the literature that justify the need to consider factors such as model size, latency, computational time and fast response time56. We motivate the need for a multi-criteria evaluation strategy considering that a single-objective focused on over-classification accuracy will be inaccurate in obtaining the best performing CNN architecture. In Eqs. (18), (19) and (20) are definitions of the metrics applied for the multi-criteria evaluation strategy. Performance comparison for the similarity between CNN architectures is achieved using Eq. (21). where NA represents any arbitrary neural network, and the function Similarity(NA , NA ) allows for comparing two neural networks in a search space. Algorithm 3 demonstrates the procedure for the evaluation of the multi-objective criteria as described previously. The expected output of the algorithm is a single value that is passed to the search strategy for improving the configuration of the CNN to achieve optimal performance. It iterates over all the CNN models and randomly generates a batch of image samples from the dataset for training the model. Once the training is completed, the training time, accuracy and loss function is computed and evaluated as a value. The configuration of Algorithm 2 to seamlessly use Algorithm 1 and 3 is presented in the next section, which is focused on the experimentation of the proposed approach.\n\nThis study aimed to obtain the most optimal neural architecture by applying a novel search space and search strategies for solving the classification problem defined in Introduction section. Therefore, the experiment carried out was two-fold: firstly, we experimented with the proposed search space and search strategies to demonstrate the effectiveness of the methods. Secondly, the top-performing CNN architecture obtained from the first experiment was then applied to detect abnormalities in digital histopathology images confirming the presence of breast cancer. This section therefore presents a detailed outline of configurations, parameter values, and characteristics of the histopathological datasets applied in the experimentation. The configuration for generating potential solutions into the search space is presented in this subsection. This configuration is necessary to guide Algorithm 1 to boost the possibility of generating potential solutions (neural networks) for maximizing classification accuracy and minimizing loss in digital histopathology images for the detection of breast cancer. This configuration provides the encoding scheme proposed in this study with a wide range of parameters to generate and encode the possible network topologies to make the search efficient and effective. The general hyperparameters (GH) block consists of four parameters and is summarized in GH = {G , G , G , G } so that G , G , G , and G are computed using \\({G}_{b}={2}^{n}-1\\), \\({G}_{\\alpha }=rand(1|5)\\cdot {10}^{-n}\\), \\({G}_{o}=O\\left[n\\right],\\) and \\({G}_{e}=5\\) respectively. Where n = 0, 1, 2 for batch size (G ), is represented as random mode = 0, batch mode = 1, mini-batch mode = 3; learning rates (G ) is computed by generating random number between 1 and 5 resulting in α = {1 × 10 − 5, 5 × 10 − 5, 1 × 10 − 4, 5 × 10 − 4, 1 × 10 − 3, 5 × 10 − 3, 1 × 10 − 2, 5 × 10 − 2, 1 × 10 − 1, 5 × 10 − 1};n = 1,2,3,4,5 for G ; n = 0,1,2,3,4,5,6,7 for G , and O = {0 = > \"SGD\", 1 = > \"Adam\", 2 = > \"RMSprop\", 3 = > \"Adagrad\", 4 = > \"Nestrov\", 5 = > \"Adadelta\", 6 = > \"Adamax\", 7 = > \"Momentum\", 8 = > \" Nestrov Accelerated Gradient\"}. The range of values derivable for the input-Zeropadding block, represented as IZ = { Z }, to determine if input will be zero-padded or not, is shown computed using \\({I}_{z}=rand(0|1)\\). Convolutional block (CB) hyperparameters are denoted as follows: CB = {C , C , C , C , C , C , C , C } and so that C , C , C , C , C , C , C , and C are computed using \\({C}_{L}=2n-1\\), \\({C}_{C}=3-n\\), \\({C}_{AF}=AF\\left[n\\right], {C}_{K}={2}^{n}\\), \\({C}_{F}=2+\\left(n-1\\right)\\), \\({C}_{PS}=2+n\\), \\({C}_{PT}=n\\), and \\({C}_{R}=n\\) respectively. Where n = 1,2,3,4 for C , to determine the number of blocks of convolutional layers an arbitrary neural network may possess; n = 0, 1, 2 for C , to compute convolutional layers in a block; n = 0, 1, 2 for C , and indexes AF = {0 = > \"ReLU\", 1 = > \"LeakyReLU\", 2 = > “Parametric ReLU”}; n = 3, 4, 5, 6, 7, 8, 9, 10 for C ; n = 0, 2, 4, 6, 8, 10 for C ; n = 0, 1, 2, for C ; n = 0 = > Max pooling, 1 = > Average pooling C , n = 0 = > L1, 1 = > L2 and 2 = > L1L2 regularizations for C ; meanwhile, our configuration allows for the use of padding as same and stride = 1 in convolutional layers. Fully-connected block (FCB) parameters are denoted by FCB = {F , F , F , F } and are computed as follows: F , F , F , and F using \\({F}_{L}=1+n\\) \\({F}_{AF}=FAF(n)\\) \\({F}_{D}=\\frac{1}{n}\\), and \\({F}_{R}=n\\) respectively, Where n = 0, 1 for computing the number of F flatten operations; n = 0, 1 for obtaining F which is further defined by indexing: FAF = {0 = > \"softmax\", 1 = > \"sigmoid\"}; n = 1.0, 1.1…0.2.0, are used in computing F ; n = 0 = > L1, 1 = > L2 and 2 = > L1L2 regularizations for F . The Loss function block denoted by LF has only one element: {LF } where loss function for the search space can be drawn from the {categorical cross-entropy, sparse-cross-entropy} when n = 0 and 1, respectively. The summary presented in Table 4 identifies the collection of possible values derivable for the search space in configuring potential CNN architectures. The EOSA algorithm is also configured and experimented with the parameters listed in Table 5. The following sub-section presents the configuration of the environment for the experiment. Table 4 A summary of formula for computing values for hyperparameters and the corresponding search space using the proposed encoding scheme. Table 5 Notations and description for variables and parameters used for experimenting with EOSA optimization algorithm. Exhaustive experimentation done for evaluating the proposed EOSA, described in Algorithm 1, was carried out in a workstation environment with the following configurations: Intel (R) Core i5-7500 CPU 3.40 GHz, 3.41 GHz; RAM of 16 GB; and 64-bit Windows 10 OS for each configuration of the system on the network. Similarly, those for the neural architecture search and for convolutional and classification processes were carried out in the same computational environment. This study is focused on applying the experimentation of the proposed NAS model on digital histopathological images. We allowed every candidate CNN architecture to be evaluated using these images for performance evaluation. We chose the publicly available benchmark datasets, namely BACH57 and BreakHis58,59. The motive for choosing these datasets was to provide sufficient data for the experimentation and allow for the reproducibility of the proposed approach. The experiments were staged in two (2): generating and searching for best performing networks, and the second experiment for full training of top-5 networks. As a result, we rigorously applied the datasets to the top-performing CNN architecture resulting from the stage 1 experiment. The image samples obtained from the BACH and BreakHis datasets were further resized to sizes 224 × 224 to allow for input into the neural architectures and the top-performing neural network architectures. This resizing became necessary because the original image size from BACH was 2048 × 1536 pixels and consisted of 400 Hematoxylin and eosin (HE) stained images, while the BreakHis dataset contained a total of 9,109 (actually 7,909 samples after removal of tissue samples) microscopic images with an image size of 700 × 460 pixels. The classes of images obtained from BACH are normal, benign, in situ carcinoma or invasive carcinoma, while those of BreakHis are categorized as benign or malignant. The benign and malignant samples of BreakHis are further categorized into adenosis (A), fibroadenoma (F), phyllodes tumor (PT), and tubular adenona (TA) as benign; and carcinoma (DC), lobular carcinoma (LC), mucinous carcinoma (MC) and papillary carcinoma (PC) as malignant. Figures 3 and 4 show some samples drawn from BACH and BreakHis datasets respectively. Sample images from the BACH datasets showing (a) normal (b) benign (c) in situ carcinoma and (d) invasive carcinoma cases. Sample images from the BreakHis datasets showing (a) adenosis, (b) ductal carcinoma, (c) mucinous carcinoma, and (d) papillary carcinoma malignant cases. Each column shows the magnification of samples for (a)–(d) in 40X, 100X, 200X, and 400X accordingly. The H&E stain the nuclei with a dark purple (Hematoxylin) and the cytoplasm with a light pink (Eosin). According to their classes, the breakdown for the BACH image samples are: 100 samples of normal, 100 samples of benign, 100 samples of in situ carcinoma, and 100 samples of invasive carcinoma. Similarly, the BreakHis datasets image samples contain 2,480 benign and 5,429 malignant samples. Both the BACH and BreakHis datasets image samples are 3-channelled RGB. Also, we discovered that the magnification for the BACH dataset is 200 × , and those of BreakHis were presented at 40X, 100X, 200X, and 400Xmagnifications. We, however, preprocessed the images to allow for resizing and elimination of potential errors arising from the stain on the raw inputs. We applied the basic operations of reduction of background noise and image enhancement. Furthermore, we applied image normalization operations from Reinhard60 and Macenku61 to normalize our histopathology images.\n\nIn this section, the result of the experimentation is presented, and the findings are discussed. Two categories of results are considered: performance of the EOSA algorithm as compared with four similar metaheuristic algorithms and the performance of the NAS model in obtaining best performing CNN architecture. The EOSA experiment was carried out using 25 benchmark optimization functions listed in Table 6. These same functions were applied to artificial bee colony (ABC), whale optimization algorithm (WOA), particle swarm optimization (PSO), and genetic algorithm (GA) metaheuristic algorithms. Each of these optimization algorithms was executed for 500 epochs and 20 runs for stability. The result of the experimentation is listed in Table 7. Table 6 Standard and CEC benchmark functions used for the experimentation in evaluating the performances of EOSA, ABC, WOA, PSO and GA. Table 7 Comparison of best, worst, mean, median and standard deviation (stdev) values for EOSA, ABC, WOA, PSO, and GA metaheuristic algorithms using the classical benchmark and IEEE CEC functions over 500 epochs and 100 population size. Table 7 shows that EOSA had the lowest values for the best solutions using the F1–F12 and F14–F15 compared with the best solutions for ABC, WOA, PSO and GA. Although PSO maintained a lead only in F13 compared to EOSA, ABC, WOA, and GA, the performance margin was small compared with EOSA, and EOSA showed superiority in fourteen (14) of fifteen (15) functions evaluated. Also, EOSA yielded a significant performance compared with ABC, WOA, PSO and GA based on the values of worst solutions for F1–F15. Table 8 shows that the EOSA performed well in the solutions obtained for the constrained IEEE CEC-2017 benchmark functions compared to other competing algorithms. The EOSA had obtained a total of eight (8) best results out of the nine (9) functions. Table 8 Comparison of best, worst, mean, median and standard deviation (stdev) values for EOSA, ABC, WOA, PSO, and GA metaheuristic algorithms using the constrained IEEE CEC-2017 benchmark functions over 500 epochs and 100 population size. Figures 5 and 6 illustrate the convergence of EOSA on F1–F15 and convergence of EOSA compared with ABC, WOA, PSO and GA on F1–F15, respectively. The plots in Fig. 5 confirm that the convergence of EOSA is impressive though the significance of its convergence has been overshadowed in Fig. 6 due to variation of values. Also, we observed the convergence of each solution for EOSA, ABC, WOA, PSO, and GA using a scatter plot. The outcome, as shown in Fig. 7, aligns with the graphs in Figs. 5 and 6. The results show that the EOSA algorithm is a candidate optimization algorithm capable of sufficiently learning the problem of automating the design of CNN architectures for the search strategy of a NAS model. Furthermore, the results guarantee that EOSA can compete with state-of-the-art optimization algorithms. Convergent curves of EOSA optimization algorithm on F1, F2, F3, F4, F5, F6, F7, F8, F9, F10, F11, F12, F13, F14 and F15 standard benchmark functions. Comparison of convergence curves of the performance of EOSA, ABC, WOA, PSO, and GA optimization algorithms on all standard benchmark functions applied in this study. Comparison of convergence curves of the performance of EOSA, ABC, WOA, PSO, and GA optimization algorithms on all standard benchmark functions applied in this study. Now that the performance of EOSA as a metaheuristic algorithm was confirmed to be suitable for optimizing the search strategy of a NAS model, we proceeded to experiment using it in the NAS model experimentation. The result of this experiment is presented and discussed in the next section. The initial solutions (CNN architectures) generated into the search space were optimized using the EOSA algorithm during the search strategy stage of the NAS model. The optimization in EOSA was executed for 500 epochs, and the configuration of each solution was reevaluated using the evaluation strategy of our NAS model. The optimized CNN architectures were logged for each iteration, while the final configurations for all the CNN architectures were examined and used for the result presented in this section. Table 9 presents the configurations of the top-5 CNN architectures, and their network topologies are shown in Fig. 8. Table 9 Comparison of parameters for the best five (5) initial neural network configurations (solutions) generated for the search space. Neural network architectures of the Top-5 generated network architectures generated for the search space. In Table 9, a detailed definition of each of the top five (5) architectures is outlined. Similarly, a graphical illustration of the architectures is shown in Fig. 8. We found that the Top-1 architecture represents a minimal utilization of convolutional and pooling operations while the Top-5 architecture has more of these operations. For instance, the Top-1 has two convolutional blocks with a single convo operation in a block and three convo operations in the second block. In contrast, the Top-5 has 6 convolutional blocks with mostly three convo operations combined with either max or average pooling operations. Another interesting outcome of the resulting top five architectures is that we found a structural similarity between the Top-1 compared with Top-3 and another variation of structural similarity between the Top-2 and Top-4 architectures. However, in Table 9, we observed that whereas these similarities exist in the structural view of the architectures, there are some significant variations in their detailed implementations. For instance, we found that the three convolutional blocks of the Top-2 architecture have the Max-Avg-Avg pooling operations and the Top-4 Max-Avg-Max pooling operations. In addition, the second convolutional block of the Top-2 architecture allows the use of the L1 (weight decay) network regularizer, whereas that of Top-4 uses none. The reverse of this arrangement is seen in the third convolutional block. The result in Table 10 shows that the Top-1 architecture achieved a good performance during the 250 iterations as its accuracy for best, mean and median are 0.655, 0.415, and 0.417, respectively. This is a distance from the Top-5, which maintained the values of 0.551, 0.313, and 0.332 for best, mean and median, respectively. We found a similar trend as shown in the results of Top-2, Top-3, and Top-4 performing architectures. The interpretation of these variations informs us that the Top-1 architecture learned the classification problem very well compared to the remaining four (4) architectures. Using a radar chart, we plotted the performance of the top five (5) network architectures using the resulting values of their best, mean, median, worst, and standard deviation. Radar charts provide a good way for visualizing comparisons of data of related attributes or variables which are displayed along their axis. Table 10 Performance comparison for training the five (5) best performing CNN architectures from EOSA-NAS algorithm using mean, median, accuracy and standard deviation for accuracy, and loss, computation time values for the 250 epochs of EOSA. In Fig. 9, we see that the overall difference in visual representation is apparent by the size and shape of the polygons’ pointing. The polygons point to the best axis more closely because the top5 architectures have their highest accuracy within this variable. The nearness of the polygons’ closeness to the axis is followed by those of mean and median variables, confirming the distribution of accuracies for the top-5 architectures within those two variables. Lastly, we see that the pointing of the polygons of the worst and standard deviation variables is far from their axes. These distributions of accuracies across the five variables demonstrate the discrepancies which exist in the performance of the top5 architectures. Clearly, the Top-1 architecture has the highest and best performance followed by the Top-2, then the Top-3, Top-4 and Top-5. A radar plot showing the performance comparison of the top-5 best performing network architectures from EOSA-NAS algorithm based on mean, median, worst, and best accuracy values. Complete training of the top one (1) best performing architectures listed in Table 11 and illustrated in Fig. 10 showed that only the Top-1 and Top-2 demonstrated significant results. The two previous architectures overshadowed the outcome of those of Top-3 and Top-4. As a result, the Top-1 and Top-2 architectures were further evaluated beyond the 500 epochs of training. We found the Top-1 architecture converging well and learning the problem with impressive accuracy from the 60th epoch to the 100th epoch. Meanwhile, that of Top-2 architecture only began to show this stability later. This implies that the Top-1 architecture remains the best architecture that has learnt the classification problem well. Table 11 Performance comparison for prediction of the four (4) best performing CNN architectures of the EOSA-NAS algorithm using AUC, precision, recall, sensitivity, specificity, accuracy and loss after full train for 60, 70 and 100 epochs. Plot of the accuracy and loss values for the training of the Top-1, 2, and 3 architectures respectively which were optimized using the EOSA-NAS model, showing their performances after sixty (60) training epoch. To fully evaluate and investigate the performance of the top five architectures, we experimented again with these architectures on larger datasets and allowed for training using a longer epoch. In Table 11, we see the performance of each of the architectures in terms of F1-score, precision, recall, sensitivity, specificity, accuracy and Kappa values after the full train. We applied the distributions of these variables to plot the boxplot of their corresponding values and found that an interesting distribution was seen for values in each distribution. Also, the result obtained from the Table showed that the architecture corresponding to the CNN model in Fig. 11 outputs the optimal performance with an accuracy of 0.1. This then reflects the most acceptable CNN configuration required to learn the problem of classification of digital histopathology images using deep learning. Neural network architecture of the Top-1 architecture optimized using EOSA-NAS model, which represents the overall best performing architecture after hundred (100) training epoch. Also, plotting the graph of the training phase of the Top-1 CNN model, we found that the loss function graph in Fig. 12 showed that the problem was learnt well as we see the loss values for those of training and validation overlapping as the training progressed. Similarly, the accuracy plot in the same figure demonstrates the evidence that the resulting CNN model is a candidate solution for consideration in future research on the application of deep learning to the classification of abnormalities in digital histopathology images. Plot of the accuracy and loss values for the training of the Top-1 architecture optimized using EOSA-NAS model, which represents the overall best performing architecture after hundred (100) training epoch. The result shown in Table 12 shows that most efforts in designing CNN models for histopathology image classification have all been approached using manual methods. Although the studies listed in the Table demonstrate some significant performance, the outcome of our experimentation confirms that automating the process is more beneficial. While the works of Zheng et al. 19 and Kandel and Castelli 25 compete with our method, we note that our method outperforms them. The graph in Fig. 13 shows a pictorial representation of the performance of all similar studies when compared with the outcome of this study. Table 12 Comparison of NAS-based CNN design with state-of-the-art canonical CNN design approach for detection and classification of breast cancer using histopathology images. Comparison of the CNN architecture designed using EOSA-NAS model with state-of-the-art CNN architectures applied to the detection of breast cancer in histopathology images. This study is focused on investigating the outcome of applying a NAS-based approach to the automation for the design of CNN architectures in the classification of breast histopathology images. The study aimed to address the difficulty in learning the problem associated with the domain. The outcome of the experimentation performed using EOSA-NAS based model for generating and optimising CNN architecture has proven very effective. This is based on the results obtained which have shown that applying the NAS approach to finding the best network configuration in detecting abnormalities in histopathology yields better performance. The accuracy obtained confirms that the application of the EOSA metaheuristic algorithm contributed to the overall performance of the NAS model. Meanwhile, this study has also shown that the proposed optimization algorithm, EOSA, competes well with similar state-of-the-art algorithms while showing superiority in the case of GA. The EOSA metaheuristic algorithm was experimented with using fifteen (15) standard benchmark functions to demonstrate its viability and usefulness for solving optimization problems as in NAS model. Therefore, this study's finding confirms that automatic design for the CNN model in the classification task of histopathology images is more accurate than the manually designed models. Secondly, we showed that using the EOSA metaheuristic algorithm in a NAS-based model in optimizing purpose is also very positive. The approach in this study is in contrast to the widely adopted method for designing CNN architectures in learning the problem of detection of abnormalities in histopathology samples. Therefore, the proposed method offers a new order for the design of CNN architectures for this class of problem for the domain mentioned."
    }
]