[
    {
        "link": "https://geeksforgeeks.org/sql-create-table",
        "document": "The SQL CREATE TABLE statement is a foundational command used to define and structure a new table in a database. By specifying the columns, data types, and constraints such as PRIMARY KEY, NOT NULL, and CHECK, helps you design the database schema.\n\nIn this article, we’ll learn the syntax, best practices, and practical examples of using the CREATE TABLE statement in SQL. We’ll also cover how to create tables from existing data and troubleshoot common errors.\n\nTo create a new table in the database, use the SQL CREATE TABLE statement. A table’s structure, including column names, data types, and constraints like NOT NULL, PRIMARY KEY, and CHECK, are defined when it is created in SQL.\n\nThe CREATE TABLE command is a crucial tool for database administration because of these limitations, which aid in ensuring data integrity. To create a table in SQL, use this CREATE TABLE syntax:\n• table_name : The name you assign to the new table.\n• column1, column2, … : The names of the columns in the table.\n• datatype(size): Defines the data type and size of each column.\n\nLet’s look at examples of CREATE TABLE command in SQL and see how to create table in SQL.\n\nIn this example, we will create a new table and insert data into it. Let us create a table to store data of Customers, so the table name is Customer, Columns are Name, Country, age, phone, and so on.\n\nAfter creating the table, you can use INSERT INTO command to add data to it. Here’s how to add some sample records into the Customer table:\n\nWe can also create a new table based on the structure (and optionally the data) of an existing table. The following query creates a new table called SubTable that contains CustomerID and CustomerName from the existing Customer table.\n\nNote: We can use * instead of column name to copy whole table to another table.\n• CREATE TABLE statement is used to create new table in a database.\n• None It defines the structure of table including name and datatype of columns.\n• None command can be used to display the structure of the created table\n• None We can also add constraint to table like NOT NULL, UNIQUE\n• None If you try to create a table that already exists, MySQL will throw an error. To avoid this, you can use the\n\nThe SQL CREATE TABLE statement is essential for setting up tables and defining data structures within a database. Knowing how to create a table in SQL with the CREATE TABLE SQL syntax helps you establish reliable tables with appropriate constraints. Whether you’re creating an Employee table in SQL with primary keys or duplicating tables with CREATE TABLE AS SELECT, mastering this command is vital for managing data effectively.\n\nIn this guide, we covered SQL CREATE TABLE syntax with various examples, from basic to advanced usage. By applying these practices, you can confidently build tables in SQL for effective data organization and retrieval.\n\nHow do you create a table in SQL?\n\nHow to create a query in SQL?\n\nHow to create a duplicate table in SQL Server?"
    },
    {
        "link": "https://support.microsoft.com/en-us/office/database-design-basics-eb2159cf-1e30-401a-8084-bd4f9c9ca1f5",
        "document": "To find and organize the information required, start with your existing information. For example, you might record purchase orders in a ledger or keep customer information on paper forms in a file cabinet. Gather those documents and list each type of information shown (for example, each box that you fill in on a form). If you don't have any existing forms, imagine instead that you have to design a form to record the customer information. What information would you put on the form? What fill-in boxes would you create? Identify and list each of these items. For example, suppose you currently keep the customer list on index cards. Examining these cards might show that each card holds a customers name, address, city, state, postal code and telephone number. Each of these items represents a potential column in a table. As you prepare this list, don’t worry about getting it perfect at first. Instead, list each item that comes to mind. If someone else will be using the database, ask for their ideas, too. You can fine-tune the list later. Next, consider the types of reports or mailings you might want to produce from the database. For instance, you might want a product sales report to show sales by region, or an inventory summary report that shows product inventory levels. You might also want to generate form letters to send to customers that announces a sale event or offers a premium. Design the report in your mind, and imagine what it would look like. What information would you place on the report? List each item. Do the same for the form letter and for any other report you anticipate creating. Giving thought to the reports and mailings you might want to create helps you identify items you will need in your database. For example, suppose you give customers the opportunity to opt in to (or out of) periodic e-mail updates, and you want to print a listing of those who have opted in. To record that information, you add a “Send e-mail” column to the customer table. For each customer, you can set the field to Yes or No. The requirement to send e-mail messages to customers suggests another item to record. Once you know that a customer wants to receive e-mail messages, you will also need to know the e-mail address to which to send them. Therefore you need to record an e-mail address for each customer. It makes good sense to construct a prototype of each report or output listing and consider what items you will need to produce the report. For instance, when you examine a form letter, a few things might come to mind. If you want to include a proper salutation — for example, the \"Mr.\", \"Mrs.\" or \"Ms.\" string that starts a greeting, you will have to create a salutation item. Also, you might typically start a letter with “Dear Mr. Smith”, rather than “Dear. Mr. Sylvester Smith”. This suggests you would typically want to store the last name separate from the first name. A key point to remember is that you should break each piece of information into its smallest useful parts. In the case of a name, to make the last name readily available, you will break the name into two parts — First Name and Last Name. To sort a report by last name, for example, it helps to have the customer's last name stored separately. In general, if you want to sort, search, calculate, or report based on an item of information, you should put that item in its own field. Think about the questions you might want the database to answer. For instance, how many sales of your featured product did you close last month? Where do your best customers live? Who is the supplier for your best-selling product? Anticipating these questions helps you zero in on additional items to record. After gathering this information, you are ready for the next step.\n\nTo divide the information into tables, choose the major entities, or subjects. For example, after finding and organizing information for a product sales database, the preliminary list might look like this: The major entities shown here are the products, the suppliers, the customers, and the orders. Therefore, it makes sense to start out with these four tables: one for facts about products, one for facts about suppliers, one for facts about customers, and one for facts about orders. Although this doesn’t complete the list, it is a good starting point. You can continue to refine this list until you have a design that works well. When you first review the preliminary list of items, you might be tempted to place them all in a single table, instead of the four shown in the preceding illustration. You will learn here why that is a bad idea. Consider for a moment, the table shown here: In this case, each row contains information about both the product and its supplier. Because you can have many products from the same supplier, the supplier name and address information has to be repeated many times. This wastes disk space. Recording the supplier information only once in a separate Suppliers table, and then linking that table to the Products table, is a much better solution. A second problem with this design comes about when you need to modify information about the supplier. For example, suppose you need to change a supplier's address. Because it appears in many places, you might accidentally change the address in one place but forget to change it in the others. Recording the supplier’s address in only one place solves the problem. When you design your database, always try to record each fact just once. If you find yourself repeating the same information in more than one place, such as the address for a particular supplier, place that information in a separate table. Finally, suppose there is only one product supplied by Coho Winery, and you want to delete the product, but retain the supplier name and address information. How would you delete the product record without also losing the supplier information? You can't. Because each record contains facts about a product, as well as facts about a supplier, you cannot delete one without deleting the other. To keep these facts separate, you must split the one table into two: one table for product information, and another table for supplier information. Deleting a product record should delete only the facts about the product, not the facts about the supplier. Once you have chosen the subject that is represented by a table, columns in that table should store facts only about the subject. For instance, the product table should store facts only about products. Because the supplier address is a fact about the supplier, and not a fact about the product, it belongs in the supplier table.\n\nTo determine the columns in a table, decide what information you need to track about the subject recorded in the table. For example, for the Customers table, Name, Address, City-State-Zip, Send e-mail, Salutation and E-mail address comprise a good starting list of columns. Each record in the table contains the same set of columns, so you can store Name, Address, City-State-Zip, Send e-mail, Salutation and E-mail address information for each record. For example, the address column contains customers’ addresses. Each record contains data about one customer, and the address field contains the address for that customer. Once you have determined the initial set of columns for each table, you can further refine the columns. For example, it makes sense to store the customer name as two separate columns: first name and last name, so that you can sort, search, and index on just those columns. Similarly, the address actually consists of five separate components, address, city, state, postal code, and country/region, and it also makes sense to store them in separate columns. If you want to perform a search, filter or sort operation by state, for example, you need the state information stored in a separate column. You should also consider whether the database will hold information that is of domestic origin only, or international, as well. For instance, if you plan to store international addresses, it is better to have a Region column instead of State, because such a column can accommodate both domestic states and the regions of other countries/regions. Similarly, Postal Code makes more sense than Zip Code if you are going to store international addresses. The following list shows a few tips for determining your columns.\n• In most cases, you should not store the result of calculations in tables. Instead, you can have Access perform the calculations when you want to see the result. For example, suppose there is a Products On Order report that displays the subtotal of units on order for each category of product in the database. However, there is no Units On Order subtotal column in any table. Instead, the Products table includes a Units On Order column that stores the units on order for each product. Using that data, Access calculates the subtotal each time you print the report. The subtotal itself should not be stored in a table.\n• You may be tempted to have a single field for full names, or for product names along with product descriptions. If you combine more than one kind of information in a field, it is difficult to retrieve individual facts later. Try to break down information into logical parts; for example, create separate fields for first and last name, or for product name, category, and description. Once you have refined the data columns in each table, you are ready to choose each table's primary key.\n\nEach table should include a column or set of columns that uniquely identifies each row stored in the table. This is often a unique identification number, such as an employee ID number or a serial number. In database terminology, this information is called the primary key of the table. Access uses primary key fields to quickly associate data from multiple tables and bring the data together for you. If you already have a unique identifier for a table, such as a product number that uniquely identifies each product in your catalog, you can use that identifier as the table’s primary key — but only if the values in this column will always be different for each record. You cannot have duplicate values in a primary key. For example, don’t use people’s names as a primary key, because names are not unique. You could easily have two people with the same name in the same table. A primary key must always have a value. If a column's value can become unassigned or unknown (a missing value) at some point, it can't be used as a component in a primary key. You should always choose a primary key whose value will not change. In a database that uses more than one table, a table’s primary key can be used as a reference in other tables. If the primary key changes, the change must also be applied everywhere the key is referenced. Using a primary key that will not change reduces the chance that the primary key might become out of sync with other tables that reference it. Often, an arbitrary unique number is used as the primary key. For example, you might assign each order a unique order number. The order number's only purpose is to identify an order. Once assigned, it never changes. If you don’t have in mind a column or set of columns that might make a good primary key, consider using a column that has the AutoNumber data type. When you use the AutoNumber data type, Access automatically assigns a value for you. Such an identifier is factless; it contains no factual information describing the row that it represents. Factless identifiers are ideal for use as a primary key because they do not change. A primary key that contains facts about a row — a telephone number or a customer name, for example — is more likely to change, because the factual information itself might change. 1. A column set to the AutoNumber data type often makes a good primary key. No two product IDs are the same. In some cases, you may want to use two or more fields that, together, provide the primary key of a table. For example, an Order Details table that stores line items for orders would use two columns in its primary key: Order ID and Product ID. When a primary key employs more than one column, it is also called a composite key. For the product sales database, you can create an AutoNumber column for each of the tables to serve as primary key: ProductID for the Products table, OrderID for the Orders table, CustomerID for the Customers table, and SupplierID for the Suppliers table.\n\nNow that you have divided your information into tables, you need a way to bring the information together again in meaningful ways. For example, the following form includes information from several tables. 1. Information in this form comes from the Customers table... Access is a relational database management system. In a relational database, you divide your information into separate, subject-based tables. You then use table relationships to bring the information together as needed. Consider this example: the Suppliers and Products tables in the product orders database. A supplier can supply any number of products. It follows that for any supplier represented in the Suppliers table, there can be many products represented in the Products table. The relationship between the Suppliers table and the Products table is, therefore, a one-to-many relationship. To represent a one-to-many relationship in your database design, take the primary key on the \"one\" side of the relationship and add it as an additional column or columns to the table on the \"many\" side of the relationship. In this case, for example, you add the Supplier ID column from the Suppliers table to the Products table. Access can then use the supplier ID number in the Products table to locate the correct supplier for each product. The Supplier ID column in the Products table is called a foreign key. A foreign key is another table’s primary key. The Supplier ID column in the Products table is a foreign key because it is also the primary key in the Suppliers table. You provide the basis for joining related tables by establishing pairings of primary keys and foreign keys. If you are not sure which tables should share a common column, identifying a one-to-many relationship ensures that the two tables involved will, indeed, require a shared column. Consider the relationship between the Products table and Orders table. A single order can include more than one product. On the other hand, a single product can appear on many orders. Therefore, for each record in the Orders table, there can be many records in the Products table. And for each record in the Products table, there can be many records in the Orders table. This type of relationship is called a many-to-many relationship because for any product, there can be many orders; and for any order, there can be many products. Note that to detect many-to-many relationships between your tables, it is important that you consider both sides of the relationship. The subjects of the two tables — orders and products — have a many-to-many relationship. This presents a problem. To understand the problem, imagine what would happen if you tried to create the relationship between the two tables by adding the Product ID field to the Orders table. To have more than one product per order, you need more than one record in the Orders table per order. You would be repeating order information for each row that relates to a single order — resulting in an inefficient design that could lead to inaccurate data. You run into the same problem if you put the Order ID field in the Products table — you would have more than one record in the Products table for each product. How do you solve this problem? The answer is to create a third table, often called a junction table, that breaks down the many-to-many relationship into two one-to-many relationships. You insert the primary key from each of the two tables into the third table. As a result, the third table records each occurrence or instance of the relationship. Each record in the Order Details table represents one line item on an order. The Order Details table’s primary key consists of two fields — the foreign keys from the Orders and the Products tables. Using the Order ID field alone doesn’t work as the primary key for this table, because one order can have many line items. The Order ID is repeated for each line item on an order, so the field doesn’t contain unique values. Using the Product ID field alone doesn’t work either, because one product can appear on many different orders. But together, the two fields always produce a unique value for each record. In the product sales database, the Orders table and the Products table are not related to each other directly. Instead, they are related indirectly through the Order Details table. The many-to-many relationship between orders and products is represented in the database by using two one-to-many relationships:\n• The Orders table and Order Details table have a one-to-many relationship. Each order can have more than one line item, but each line item is connected to only one order.\n• The Products table and Order Details table have a one-to-many relationship. Each product can have many line items associated with it, but each line item refers to only one product. From the Order Details table, you can determine all of the products on a particular order. You can also determine all of the orders for a particular product. After incorporating the Order Details table, the list of tables and fields might look something like this: Another type of relationship is the one-to-one relationship. For instance, suppose you need to record some special supplementary product information that you will need rarely or that only applies to a few products. Because you don't need the information often, and because storing the information in the Products table would result in empty space for every product to which it doesn’t apply, you place it in a separate table. Like the Products table, you use the ProductID as the primary key. The relationship between this supplemental table and the Product table is a one-to-one relationship. For each record in the Product table, there exists a single matching record in the supplemental table. When you do identify such a relationship, both tables must share a common field. When you detect the need for a one-to-one relationship in your database, consider whether you can put the information from the two tables together in one table. If you don’t want to do that for some reason, perhaps because it would result in a lot of empty space, the following list shows how you would represent the relationship in your design:\n• If the two tables have the same subject, you can probably set up the relationship by using the same primary key in both tables.\n• If the two tables have different subjects with different primary keys, choose one of the tables (either one) and insert its primary key in the other table as a foreign key. Determining the relationships between tables helps you ensure that you have the right tables and columns. When a one-to-one or one-to-many relationship exists, the tables involved need to share a common column or columns. When a many-to-many relationship exists, a third table is needed to represent the relationship.\n\nOnce you have the tables, fields, and relationships you need, you should create and populate your tables with sample data and try working with the information: creating queries, adding new records, and so on. Doing this helps highlight potential problems — for example, you might need to add a column that you forgot to insert during your design phase, or you may have a table that you should split into two tables to remove duplication. See if you can use the database to get the answers you want. Create rough drafts of your forms and reports and see if they show the data you expect. Look for unnecessary duplication of data and, when you find any, alter your design to eliminate it. As you try out your initial database, you will probably discover room for improvement. Here are a few things to check for:\n• Did you forget any columns? If so, does the information belong in the existing tables? If it is information about something else, you may need to create another table. Create a column for every information item you need to track. If the information can’t be calculated from other columns, it is likely that you will need a new column for it.\n• Are any columns unnecessary because they can be calculated from existing fields? If an information item can be calculated from other existing columns — a discounted price calculated from the retail price, for example — it is usually better to do just that, and avoid creating new column.\n• Are you repeatedly entering duplicate information in one of your tables? If so, you probably need to divide the table into two tables that have a one-to-many relationship.\n• Do you have tables with many fields, a limited number of records, and many empty fields in individual records? If so, think about redesigning the table so it has fewer fields and more records.\n• Has each information item been broken into its smallest useful parts? If you need to report, sort, search, or calculate on an item of information, put that item in its own column.\n• Does each column contain a fact about the table's subject? If a column does not contain information about the table's subject, it belongs in a different table.\n• Are all relationships between tables represented, either by common fields or by a third table? One-to-one and one-to- many relationships require common columns. Many-to-many relationships require a third table. Suppose that each product in the product sales database falls under a general category, such as beverages, condiments, or seafood. The Products table could include a field that shows the category of each product. Suppose that after examining and refining the design of the database, you decide to store a description of the category along with its name. If you add a Category Description field to the Products table, you have to repeat each category description for each product that falls under the category — this is not a good solution. A better solution is to make Categories a new subject for the database to track, with its own table and its own primary key. You can then add the primary key from the Categories table to the Products table as a foreign key. The Categories and Products tables have a one-to-many relationship: a category can include more than one product, but a product can belong to only one category. When you review your table structures, be on the lookout for repeating groups. For example, consider a table containing the following columns: Here, each product is a repeating group of columns that differs from the others only by adding a number to the end of the column name. When you see columns numbered this way, you should revisit your design. Such a design has several flaws. For starters, it forces you to place an upper limit on the number of products. As soon as you exceed that limit, you must add a new group of columns to the table structure, which is a major administrative task. Another problem is that those suppliers that have fewer than the maximum number of products will waste some space, since the additional columns will be blank. The most serious flaw with such a design is that it makes many tasks difficult to perform, such as sorting or indexing the table by product ID or name. Whenever you see repeating groups review the design closely with an eye on splitting the table in two. In the above example it is better to use two tables, one for suppliers and one for products, linked by supplier ID.\n\nYou can apply the data normalization rules (sometimes just called normalization rules) as the next step in your design. You use these rules to see if your tables are structured correctly. The process of applying the rules to your database design is called normalizing the database, or just normalization. Normalization is most useful after you have represented all of the information items and have arrived at a preliminary design. The idea is to help you ensure that you have divided your information items into the appropriate tables. What normalization cannot do is ensure that you have all the correct data items to begin with. You apply the rules in succession, at each step ensuring that your design arrives at one of what is known as the \"normal forms.\" Five normal forms are widely accepted — the first normal form through the fifth normal form. This article expands on the first three, because they are all that is required for the majority of database designs. First normal form states that at every row and column intersection in the table there, exists a single value, and never a list of values. For example, you cannot have a field named Price in which you place more than one Price. If you think of each intersection of rows and columns as a cell, each cell can hold only one value. Second normal form requires that each non-key column be fully dependent on the entire primary key, not on just part of the key. This rule applies when you have a primary key that consists of more than one column. For example, suppose you have a table containing the following columns, where Order ID and Product ID form the primary key: This design violates second normal form, because Product Name is dependent on Product ID, but not on Order ID, so it is not dependent on the entire primary key. You must remove Product Name from the table. It belongs in a different table (Products). Third normal form requires that not only every non-key column be dependent on the entire primary key, but that non-key columns be independent of each other. Another way of saying this is that each non-key column must be dependent on the primary key and nothing but the primary key. For example, suppose you have a table containing the following columns: Assume that Discount depends on the suggested retail price (SRP). This table violates third normal form because a non-key column, Discount, depends on another non-key column, SRP. Column independence means that you should be able to change any non-key column without affecting any other column. If you change a value in the SRP field, the Discount would change accordingly, thus violating that rule. In this case Discount should be moved to another table that is keyed on SRP."
    },
    {
        "link": "https://w3schools.com/sql/sql_create_table.asp",
        "document": "The statement is used to create a new table in a database.\n\nThe column parameters specify the names of the columns of the table.\n\nThe datatype parameter specifies the type of data the column can hold (e.g. varchar, integer, date, etc.).\n\nTip: For an overview of the available data types, go to our complete Data Types Reference.\n\nThe following example creates a table called \"Persons\" that contains five columns: PersonID, LastName, FirstName, Address, and City:\n\nThe PersonID column is of type int and will hold an integer.\n\nThe LastName, FirstName, Address, and City columns are of type varchar and will hold characters, and the maximum length for these fields is 255 characters.\n\nThe empty \"Persons\" table will now look like this:\n\nTip: The empty \"Persons\" table can now be filled with data with the SQL INSERT INTO statement.\n\nA copy of an existing table can also be created using .\n\nThe new table gets the same column definitions. All columns or specific columns can be selected.\n\nIf you create a new table using an existing table, the new table will be filled with the existing values from the old table.\n\nThe following SQL creates a new table called \"TestTable\" (which is a copy of the \"Customers\" table):"
    },
    {
        "link": "https://sisense.com/blog/better-sql-schema",
        "document": "There are a lot of decisions to make when creating new tables and data warehouses. Some that seem inconsequential at the time end up causing you and your users pain for the life of the database. We’ve worked with thousands…\n\nThere are a lot of decisions to make when creating new tables and data warehouses. Some that seem inconsequential at the time end up causing you and your users pain for the life of the database.\n\nWe’ve worked with thousands of people and their databases and, after countless hours of reading and writing queries, we’ve seen almost everything. Here are our top 10 rules for creating pain-free schemas.\n\n1. Only use lowercase letters, numbers, and underscores\n\nDon’t use dots, spaces, or dashes in database, schema, table, or column names. Dots are for identifying objects, usually in the database.schema.table.column pattern.\n\nHaving dots in names of objects will cause confusion. Likewise, using spaces in object names will force you to add a bunch of otherwise unnecessary quotes to your query:\n\nQueries are harder to write if you use capital letters in table or column names. If everything is lowercase, no one has to remember if the users table is Users or users.\n\nAnd when you eventually change databases or replicate your tables into a warehouse, you won’t need to remember which database is case-sensitive, as only some are.\n\nIf the users table needs a foreign key to the packages table, name the key package_id. Avoid short and cryptic names like pkg_fk; others won’t know what that means. Descriptive names make it easier for others to understand the schema, which is vital to maintaining efficiency as the team grows.\n\nDon’t use ambiguous names for polymorphic data. If you find yourself creating columns with an item_type or item_value pattern, you’re likely better off using more columns with specific names like photo_count, view_count, transaction_price.\n\nThis way, the contents of a column are always known from the schema, and are not dependent on other values in the row.\n\nDon’t prefix column names with the name of the containing table. It’s generally unhelpful to have the users table contain columns like user_birthday, user_created_at, user_name.\n\nAvoid using reserved keywords like column, tag, and user as column names. You’ll have to use extra quotes in your queries and forgetting to do so will get you very confusing error messages. The database can wildly misunderstand the query if a keyword shows up where a column name should be.\n\nIf the table name is made of up of multiple words, use underscores to separate the words. It’s much easier to read package_deliveries than packagedeliveries.\n\nAnd whenever possible, use one word instead of two: deliveries is even easier to read.\n\nDon’t prefix tables to imply a schema. If you need the table grouped into a scope, put those tables into a schema. Having tables with names like store_items, store_transactions, store_coupons, like prefixed column names, is generally not worth the extra typing.\n\nWe recommend using pluralized names for tables (e.g. packages), and pluralizing both words in the name of a join table (e.g. packages_users). Singular table names are more likely to accidentally collide with reserved keywords and are generally less readable in queries.\n\nEven if you’re using UUIDs or it doesn’t make sense (e.g. for join tables), add the standard id column with an auto-incrementing integer sequence. This kind of key makes certain analyses much easier, like selecting only the first row of a group.\n\nAnd if an import job ever duplicates data, this key will be a life-saver because you’ll be able to delete specific rows:\n\nAvoid multi-column primary keys. They can be difficult to reason about when trying to write efficient queries, and very difficult to change. Use an integer primary key, a multi-column unique constraint, and several single-column indexes instead.\n\nThere are many styles for naming primary and foreign keys. Our recommendation, and the most popular, is to have a primary key called id for any table foo, and have all foreign keys be named foo_id.\n\nAnother popular style uses globally unique key names, where the foo table has a primary key called foo_id and all foreign keys are also called foo_id. This can get confusing or have name collisions if you use abbreviations (e.g. uid for the users table), so don’t abbreviate.\n\nWhatever style you choose, stick to it. Don’t use uid in some places and user_id or users_fk in others.\n\nAnd be careful with foreign keys that don’t obviously match up to a table. A column named owner_id might be a foreign key to the users table, or it might not. Name the column user_id or, if necessary, owner_user_id.\n\nDon’t store Unix timestamps or strings as dates: convert them to datetimes instead. While SQL’s date math functions aren’t the greatest, doing it yourself on timestamps is even harder. Using SQL date functions requires every query to involve a conversion from the timestamp to a datetime:\n\nDon’t store the year, month, and day in separate columns. This will make every time series query much harder to write, and will prevent most novice SQL users from being able to use the date information in this table.\n\nUsing a timezone other than UTC will cause endless problems. Great tools (including Sisense for Cloud Data Teams) have all the functionality you need you convert the data from UTC to your current timezone. In Sisense, it’s as easy as adding :pst to convert to from UTC to Pacific Time:\n\nThe database’s time zone should be UTC, and all datetime columns should be types that strip time zones (e.g. timestamp without time zone).\n\nIf your database’s time zone is not UTC, or you have a mix of UTC and non-UTC datetimes in your database, time series analysis will be a lot harder.\n\n8. Have one source of truth\n\nThere should only ever be one source of truth for a piece of data. Views and rollups should be labeled as such. This way consumers of that data will know there is a difference between the data they are using and the raw truth.\n\nLeaving legacy columns around like user_id, user_id_old, user_id_v2 can become an endless source of confusion. Be sure to drop abandoned tables and unused columns during regular maintenance.\n\nYou don’t want to have super-wide tables. If there’s more than a few dozen columns and some of them are named sequentially (e.g. answer1, answer2, answer3), you’re going to have a bad time later.\n\nPivot the table into a schema that doesn’t have duplicated columns – this schema shape will be a lot easier to query. For example, getting the number of completed answers for a survey:\n\nFor analysis queries, extracting data from JSON columns can greatly degrade a query’s performance. While there are a lot of great reasons to use JSON columns in production, there aren’t for analysis. Aggressively schematize JSON columns into the simpler data types to make analysis a lot easier and faster.\n\nDates, zip codes, and countries don’t need their own tables with foreign key lookups. If you do that, every query ends up with a handful of the same joins. It creates a lot of duplicated SQL and a lot of extra work for the database.\n\nTables are for first class objects that have a lot of their own data. Everything else can be additional columns on a more important object.\n\nArmed with these rules, your next table or warehouse will be easier to query for both you and new team members as you expand."
    },
    {
        "link": "https://stackoverflow.com/questions/38860345/using-sql-what-is-the-best-practice-for-designing-tables-that-behave-like-inter",
        "document": "I'm trying to figure out what the best practices are for a situation I've encountered a few times over the past year.\n\nTo illustrate I've designed a fake database comprised of users, movies, and movies watched by users.\n\nIn this example, we want to be able to capture meta-data about any time a user watches a movie, like their location or the device they used, but the types of meta-data we may want to collect in the future may grow. The goal is to design a database that can aggregate many types of meta data for one movie watching experience.\n\nSo, I made a table called MovieWatchedMetaData to act like a bridge between different tables that contain meta-data. The premise is that this table will link the primary key from a meta data table like LocationWatched to a particular movie watching occasion.\n\nI'm unsure about this approach and if it will be detrimental in the long run. Is this a poor practice? Is there a better, more appropriate way to accomplish this? How would you organize these tables?"
    },
    {
        "link": "https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-transact-sql?view=sql-server-ver16",
        "document": "Creates a new table in the database.\n\nSimple CREATE TABLE syntax (common if not using options):\n\nThe name of the database in which the table is created. database_name must specify the name of an existing database. If not specified, database_name defaults to the current database. The login for the current connection must be associated with an existing user ID in the database specified by database_name, and that user ID must have CREATE TABLE permissions.\n\nThe name of the schema to which the new table belongs.\n\nThe name of the new table. Table names must follow the rules for identifiers. table_name can be a maximum of 128 characters, except for local temporary table names (names prefixed with a single number sign ( )) that can't exceed 116 characters.\n\nApplies to: SQL Server 2012 (11.x) and later.\n\nCreates the new table as a FileTable. You don't specify columns because a FileTable has a fixed schema. For more information, see FileTables.\n\nAn expression that defines the value of a computed column. A computed column is a virtual column that isn't physically stored in the table, unless the column is marked PERSISTED. The column is computed from an expression that uses other columns in the same table. For example, a computed column can have the definition: . The expression can be a noncomputed column name, constant, function, variable, and any combination of these connected by one or more operators. The expression can't be a subquery or contain alias data types.\n\nComputed columns can be used in select lists, WHERE clauses, ORDER BY clauses, or any other locations in which regular expressions can be used, with the following exceptions:\n• None Computed columns must be marked PERSISTED to participate in a FOREIGN KEY or CHECK constraint.\n• None A computed column can be used as a key column in an index or as part of any PRIMARY KEY or UNIQUE constraint, if the computed column value is defined by a deterministic expression and the data type of the result is allowed in index columns. For example, if the table has integer columns and , the computed column might be indexed, but computed column can't be indexed because the value might change in subsequent invocations.\n• None A computed column can't be the target of an INSERT or UPDATE statement.\n\nBased on the expressions that are used, the nullability of computed columns is determined automatically by the Database Engine. The result of most expressions is considered nullable even if only nonnullable columns are present, because possible underflows or overflows also produce NULL results. Use the function with the AllowsNull property to investigate the nullability of any computed column in a table. An expression that is nullable can be turned into a nonnullable one by specifying with the check_expression constant, where the constant is a nonnull value substituted for any NULL result. REFERENCES permission on the type is required for computed columns based on common language runtime (CLR) user-defined type expressions.\n\nSpecifies that the SQL Server Database Engine will physically store the computed values in the table, and update the values when any other columns on which the computed column depends are updated. Marking a computed column as lets you create an index on a computed column that is deterministic, but not precise. For more information, see Indexes on Computed Columns. Any computed columns that are used as partitioning columns of a partitioned table must be explicitly marked . computed_column_expression must be deterministic when is specified.\n\nSpecifies the partition scheme or filegroup on which the table is stored. If partition_scheme is specified, the table is to be a partitioned table whose partitions are stored on a set of one or more filegroups specified in partition_scheme. If filegroup is specified, the table is stored in the named filegroup. The filegroup must exist within the database. If is specified, or if ON isn't specified at all, the table is stored on the default filegroup. The storage mechanism of a table as specified in CREATE TABLE can't be subsequently altered.\n\nON { partition_scheme | filegroup | \"default\" } can also be specified in a PRIMARY KEY or UNIQUE constraint. These constraints create indexes. If filegroup is specified, the index is stored in the named filegroup. If is specified, or if ON isn't specified at all, the index is stored in the same filegroup as the table. If the PRIMARY KEY or UNIQUE constraint creates a clustered index, the data pages for the table are stored in the same filegroup as the index. If is specified or the constraint otherwise creates a clustered index, and a partition_scheme is specified that differs from the partition_scheme or filegroup of the table definition, or vice-versa, only the constraint definition will be honored, and the other will be ignored.\n\nIndicates that the text, ntext, image, xml, varchar(max), nvarchar(max), varbinary(max), and CLR user-defined type columns (including geometry and geography) are stored on the specified filegroup.\n\nisn't allowed if there are no large value columns in the table. can't be specified if partition_scheme is specified. If is specified, or if isn't specified at all, the large value columns are stored in the default filegroup. The storage of any large value column data specified in can't be subsequently altered.\n\nApplies to: SQL Server 2008 R2 (10.50.x) and later. Azure SQL Database and Azure SQL Managed Instance do not support .\n\nIf the table contains FILESTREAM data and the table is partitioned, the FILESTREAM_ON clause must be included, and must specify a partition scheme of FILESTREAM filegroups. This partition scheme must use the same partition function and partition columns as the partition scheme for the table; otherwise, an error is raised.\n\nIf the table isn't partitioned, the FILESTREAM column can't be partitioned. FILESTREAM data for the table must be stored in a single filegroup. This filegroup is specified in the FILESTREAM_ON clause.\n\nIf the table isn't partitioned and the clause isn't specified, the FILESTREAM filegroup that has the property set is used. If there is no FILESTREAM filegroup, an error is raised.\n\nAs with ON and , the value set by using for can't be changed, except in the following cases:\n• A CREATE INDEX statement converts a heap into a clustered index. In this case, a different FILESTREAM filegroup, partition scheme, or NULL can be specified.\n• A DROP INDEX statement converts a clustered index into a heap. In this case, a different FILESTREAM filegroup, partition scheme, or can be specified.\n\nThe filegroup in the clause, or each FILESTREAM filegroup that is named in the partition scheme, must have one file defined for the filegroup. This file must be defined by using a CREATE DATABASE or ALTER DATABASE statement; otherwise, an error is raised.\n\nSpecifies the data type of the column, and the schema to which it belongs. For disk-based tables, use one of the following data types:\n• An alias type based on a SQL Server system data type. Alias data types are created with the statement before they can be used in a table definition. The NULL or NOT NULL assignment for an alias data type can be overridden during the statement. However, the length specification can't be changed; the length for an alias data type can't be specified in a statement.\n• A CLR user-defined type. CLR user-defined types are created with the statement before they can be used in a table definition. To create a column on CLR user-defined type, REFERENCES permission is required on the type.\n\nIf type_schema_name isn't specified, the SQL Server Database Engine references type_name in the following order:\n• The default schema of the current user in the current database.\n• The schema in the current database.\n\nFor memory-optimized tables, see Supported Data Types for In-Memory OLTP for a list of supported system types.\n• The precision for the specified data type. For more information about valid precision values, see Precision, Scale, and Length.\n• The scale for the specified data type. For more information about valid scale values, see Precision, Scale, and Length.\n• Applies only to the varchar, nvarchar, and varbinary data types for storing 2^31 bytes of character and binary data, and 2^30 bytes of Unicode data.\n\nSpecifies that each instance of the xml data type in column_name can contain multiple top-level elements. CONTENT applies only to the xml data type and can be specified only if xml_schema_collection is also specified. If not specified, CONTENT is the default behavior.\n\nSpecifies that each instance of the xml data type in column_name can contain only one top-level element. DOCUMENT applies only to the xml data type and can be specified only if xml_schema_collection is also specified.\n\nApplies only to the xml data type for associating an XML schema collection with the type. Before typing an xml column to a schema, the schema must first be created in the database by using CREATE XML SCHEMA COLLECTION.\n\nSpecifies the value provided for the column when a value isn't explicitly supplied during an insert. DEFAULT definitions can be applied to any columns except those defined as timestamp, or those with the property. If a default value is specified for a user-defined type column, the type should support an implicit conversion from constant_expression to the user-defined type. DEFAULT definitions are removed when the table is dropped. Only a constant value, such as a character string; a scalar function (either a system, user-defined, or CLR function); or NULL can be used as a default. To maintain compatibility with earlier versions of SQL Server, a constraint name can be assigned to a DEFAULT.\n• A constant, NULL, or a system function that is used as the default value for the column.\n• A constant, NULL, or a system function that is supported in used as the default value for the column. Must be supported in natively compiled stored procedures. For more information about built-in functions in natively compiled stored procedures, see Supported Features for Natively Compiled T-SQL Modules.\n\nIndicates that the new column is an identity column. When a new row is added to the table, the Database Engine provides a unique, incremental value for the column. Identity columns are typically used with PRIMARY KEY constraints to serve as the unique row identifier for the table. The property can be assigned to tinyint, smallint, int, bigint, decimal(p, 0), or numeric(p, 0) columns. Only one identity column can be created per table. Bound defaults and DEFAULT constraints can't be used with an identity column. Both the seed and increment or neither must be specified. If neither is specified, the default is (1,1).\n• The value used for the first row loaded into the table.\n• The incremental value added to the identity value of the previous row loaded.\n\nIn the statement, the clause can be specified for the IDENTITY property, FOREIGN KEY constraints, and CHECK constraints. If this clause is specified for the property, values aren't incremented in identity columns when replication agents perform inserts. If this clause is specified for a constraint, the constraint isn't enforced when replication agents perform insert, update, or delete operations.\n\nGENERATED ALWAYS AS { ROW | TRANSACTION_ID | SEQUENCE_NUMBER } { START | END } [ HIDDEN ] [ NOT NULL ]\n\nApplies to: SQL Server 2016 (13.x) and later, Azure SQL Database, and Azure SQL Managed Instance.\n\nSpecifies a column used by the system to automatically record information about row versions in the table and its history table (if the table is system versioned and has a history table). Use this argument with the parameter to create system-versioned tables: temporal or ledger tables. For more information, see updateable ledger tables and temporal tables.\n\nIf you attempt to specify a column that doesn't meet the above data type or nullability requirements, the system will throw an error. If you don't explicitly specify nullability, the system will define the column as or per the above requirements.\n\nYou can mark one or both period columns with flag to implicitly hide these columns such that doesn't return a value for those columns. By default, period columns aren't hidden. In order to be used, hidden columns must be explicitly included in all queries that directly reference the temporal table. To change the attribute for an existing period column, must be dropped and recreated with a different hidden flag.\n\nApplies to: SQL Server 2014 (12.x) and later, and Azure SQL Database.\n\nSpecifies to create an index on the table. This can be a clustered index, or a nonclustered index. The index will contain the columns listed, and will sort the data in either ascending or descending order.\n\nApplies to: SQL Server 2014 (12.x) and later, and Azure SQL Database.\n\nSpecifies to store the entire table in columnar format with a clustered columnstore index. This always includes all columns in the table. The data isn't sorted in alphabetical or numeric order since the rows are organized to gain columnstore compression benefits.\n\nYou can specify an order for the data in a clustered columnstore index starting with SQL Server 2022 (16.x), in Azure SQL Database, in Azure SQL Managed Instance with the Always-up-to-date update policy, and in Azure Synapse Analytics. For more information, see Performance tuning with ordered columnstore indexes.\n\nApplies to: SQL Server 2014 (12.x) and later versions, Azure SQL Database, and Azure SQL Managed Instance.\n\nSpecifies to create a nonclustered columnstore index on the table. The underlying table can be a rowstore heap or clustered index, or it can be a clustered columnstore index. In all cases, creating a nonclustered columnstore index on a table stores a second copy of the data for the columns in the index.\n\nThe nonclustered columnstore index is stored and managed as a clustered columnstore index. It is called a nonclustered columnstore index to because the columns can be limited and it exists as a secondary index on a table.\n\nYou can specify an order for the data in a nonclustered columnstore index in Azure SQL Database and in Azure SQL Managed Instance with the Always-up-to-date update policy. For more information, see Performance tuning with ordered columnstore indexes.\n\nSpecifies the partition scheme that defines the filegroups onto which the partitions of a partitioned index will be mapped. The partition scheme must exist within the database by executing either CREATE PARTITION SCHEME or ALTER PARTITION SCHEME. column_name specifies the column against which a partitioned index will be partitioned. This column must match the data type, length, and precision of the argument of the partition function that partition_scheme_name is using. column_name isn't restricted to the columns in the index definition. Any column in the base table can be specified, except when partitioning a UNIQUE index, column_name must be chosen from among those used as the unique key. This restriction allows the Database Engine to verify uniqueness of key values within a single partition only.\n\nIf partition_scheme_name or filegroup isn't specified and the table is partitioned, the index is placed in the same partition scheme, using the same partitioning column, as the underlying table.\n\nFor more information about partitioning indexes, Partitioned Tables and Indexes.\n\nCreates the specified index on the specified filegroup. If no location is specified and the table or view isn't partitioned, the index uses the same filegroup as the underlying table or view. The filegroup must already exist.\n\nCreates the specified index on the default filegroup.\n\nApplies to: SQL Server 2008 R2 (10.50.x) and later.\n\nSpecifies the placement of FILESTREAM data for the table when a clustered index is created. The FILESTREAM_ON clause allows FILESTREAM data to be moved to a different FILESTREAM filegroup or partition scheme.\n\nfilestream_filegroup_name is the name of a FILESTREAM filegroup. The filegroup must have one file defined for the filegroup by using a CREATE DATABASE or ALTER DATABASE statement; otherwise, an error is raised.\n\nIf the table is partitioned, the clause must be included, and must specify a partition scheme of FILESTREAM filegroups that uses the same partition function and partition columns as the partition scheme for the table. Otherwise, an error is raised.\n\nIf the table isn't partitioned, the FILESTREAM column can't be partitioned. FILESTREAM data for the table must be stored in a single filegroup that is specified in the clause.\n\ncan be specified in a statement if a clustered index is being created and the table doesn't contain a FILESTREAM column.\n\nFor more information, see FILESTREAM.\n\nIndicates that the new column is a row GUID column. Only one uniqueidentifier column per table can be designated as the ROWGUIDCOL column. Applying the ROWGUIDCOL property enables the column to be referenced using . The ROWGUIDCOL property can be assigned only to a uniqueidentifier column. User-defined data type columns can't be designated with ROWGUIDCOL.\n\nThe ROWGUIDCOL property doesn't enforce uniqueness of the values stored in the column. ROWGUIDCOL also doesn't automatically generate values for new rows inserted into the table. To generate unique values for each column, either use the NEWID or NEWSEQUENTIALID function on INSERT statements or use these functions as the default for the column.\n\nSpecifies encrypting columns by using the Always Encrypted feature.\n• Specifies the column encryption key. For more information, see CREATE COLUMN ENCRYPTION KEY.\n• Deterministic encryption uses a method that always generates the same encrypted value for any given plain text value. Using deterministic encryption allows searching using equality comparison, grouping, and joining tables using equality joins based on encrypted values, but can also allow unauthorized users to guess information about encrypted values by examining patterns in the encrypted column. Joining two tables on columns encrypted deterministically is only possible if both columns are encrypted using the same column encryption key. Deterministic encryption must use a column collation with a binary2 sort order for character columns. Randomized encryption uses a method that encrypts data in a less predictable manner. Randomized encryption is more secure, but it prevents any computations and indexing on encrypted columns, unless your SQL Server instance supports Always Encrypted with secure enclaves. See Always Encrypted with secure enclaves for details. If you are using Always Encrypted (without secure enclaves), use deterministic encryption for columns that will be searched with parameters or grouping parameters, for example a government ID number. Use randomized encryption, for data such as a credit card number, which isn't grouped with other records or used to join tables, and which isn't searched for because you use other columns (such as a transaction number) to find the row that contains the encrypted column of interest. If you are using Always Encrypted with secure enclaves, randomized encryption is a recommended encryption type. Columns must be of a qualifying data type.\n• Applies to: SQL Server 2016 (13.x) and later. For more information including feature constraints, see Always Encrypted.\n\nIndicates that the column is a sparse column. The storage of sparse columns is optimized for null values. Sparse columns can't be designated as NOT NULL. For additional restrictions and more information about sparse columns, see Use Sparse Columns.\n\nApplies to: SQL Server 2016 (13.x) and later.\n\nSpecifies a dynamic data mask. mask_function is the name of the masking function with the appropriate parameters. Four functions are available:\n\nApplies to: SQL Server 2008 R2 (10.50.x) and later.\n\nValid only for varbinary(max) columns. Specifies FILESTREAM storage for the varbinary(max) BLOB data.\n\nThe table must also have a column of the uniqueidentifier data type that has the ROWGUIDCOL attribute. This column must not allow null values and must have either a UNIQUE or PRIMARY KEY single-column constraint. The GUID value for the column must be supplied either by an application when inserting data, or by a DEFAULT constraint that uses the NEWID () function.\n\nThe ROWGUIDCOL column can't be dropped and the related constraints can't be changed while there is a FILESTREAM column defined for the table. The ROWGUIDCOL column can be dropped only after the last FILESTREAM column is dropped.\n\nWhen the FILESTREAM storage attribute is specified for a column, all values for that column are stored in a FILESTREAM data container on the file system.\n\nSpecifies the collation for the column. Collation name can be either a Windows collation name or a SQL collation name. collation_name is applicable only for columns of the char, varchar, text, nchar, nvarchar, and ntext data types. If not specified, the column is assigned either the collation of the user-defined data type, if the column is of a user-defined data type, or the default collation of the database.\n\nFor more information about the Windows and SQL collation names, see Windows Collation Name and SQL Collation Name.\n\nFor more information, see COLLATE.\n\nAn optional keyword that indicates the start of the definition of a PRIMARY KEY, NOT NULL, UNIQUE, FOREIGN KEY, or CHECK constraint.\n• The name of a constraint. Constraint names must be unique within the schema to which the table belongs.\n• Determine whether null values are allowed in the column. NULL isn't strictly a constraint but can be specified just like NOT NULL. NOT NULL can be specified for computed columns only if PERSISTED is also specified.\n• A constraint that enforces entity integrity for a specified column or columns through a unique index. Only one PRIMARY KEY constraint can be created per table.\n• A constraint that provides entity integrity for a specified column or columns through a unique index. A table can have multiple UNIQUE constraints.\n• Indicates that a clustered or a nonclustered index is created for the PRIMARY KEY or UNIQUE constraint. PRIMARY KEY constraints default to CLUSTERED, and UNIQUE constraints default to NONCLUSTERED. In a statement, CLUSTERED can be specified for only one constraint. If CLUSTERED is specified for a UNIQUE constraint and a PRIMARY KEY constraint is also specified, the PRIMARY KEY defaults to NONCLUSTERED.\n• A constraint that provides referential integrity for the data in the column or columns. FOREIGN KEY constraints require that each value in the column exists in the corresponding referenced column or columns in the referenced table. FOREIGN KEY constraints can reference only columns that are PRIMARY KEY or UNIQUE constraints in the referenced table or columns referenced in a UNIQUE INDEX on the referenced table. Foreign keys on computed columns must also be marked PERSISTED.\n• The name of the table referenced by the FOREIGN KEY constraint, and the schema to which it belongs.\n• A column, or list of columns, from the table referenced by the FOREIGN KEY constraint.\n• Specifies what action happens to rows in the table created, if those rows have a referential relationship and the referenced row is deleted from the parent table. The default is NO ACTION.\n• The Database Engine raises an error and the delete action on the row in the parent table is rolled back.\n• Corresponding rows are deleted from the referencing table if that row is deleted from the parent table.\n• All the values that make up the foreign key are set to NULL if the corresponding row in the parent table is deleted. For this constraint to execute, the foreign key columns must be nullable.\n• All the values that make up the foreign key are set to their default values when the corresponding row in the parent table is deleted. For this constraint to execute, all foreign key columns must have default definitions. If a column is nullable, and there is no explicit default value set, NULL becomes the implicit default value of the column. Don't specify if the table will be included in a merge publication that uses logical records. For more information about logical records, see Group Changes to Related Rows with Logical Records. can't be defined if an trigger already exists on the table. For example, in the database, the table has a referential relationship with the table. The foreign key references the primary key. If a statement is executed on a row in the table, and an action is specified for , the Database Engine checks for one or more dependent rows in the table. If any exist, the dependent rows in the table are deleted, and also the row referenced in the table. Conversely, if is specified, the Database Engine raises an error and rolls back the delete action on the row if there is at least one row in the table that references it.\n• Specifies what action happens to rows in the table altered when those rows have a referential relationship and the referenced row is updated in the parent table. The default is NO ACTION.\n• The Database Engine raises an error, and the update action on the row in the parent table is rolled back.\n• Corresponding rows are updated in the referencing table when that row is updated in the parent table.\n• All the values that make up the foreign key are set to NULL when the corresponding row in the parent table is updated. For this constraint to execute, the foreign key columns must be nullable.\n• All the values that make up the foreign key are set to their default values when the corresponding row in the parent table is updated. For this constraint to execute, all foreign key columns must have default definitions. If a column is nullable, and there is no explicit default value set, NULL becomes the implicit default value of the column. Don't specify if the table will be included in a merge publication that uses logical records. For more information about logical records, see Group Changes to Related Rows with Logical Records. , , or can't be defined if an trigger already exists on the table that is being altered. For example, in the database, the table has a referential relationship with the table: foreign key references the primary key. If an UPDATE statement is executed on a row in the table, and an ON UPDATE CASCADE action is specified for , the Database Engine checks for one or more dependent rows in the table. If any exist, the dependent rows in the table are updated, and also the row referenced in the table. Conversely, if NO ACTION is specified, the Database Engine raises an error and rolls back the update action on the row if there is at least one row in the table that references it.\n• A constraint that enforces domain integrity by limiting the possible values that can be entered into a column or columns. CHECK constraints on computed columns must also be marked PERSISTED.\n• A logical expression that returns TRUE or FALSE. Alias data types can't be part of the expression.\n• A column or list of columns, in parentheses, used in table constraints to indicate the columns used in the constraint definition.\n• Specifies the order in which the column or columns participating in table constraints are sorted. The default is ASC.\n• The name of the partition scheme that defines the filegroups onto which the partitions of a partitioned table will be mapped. The partition scheme must exist within the database.\n• Specifies the column against which a partitioned table will be partitioned. The column must match that specified in the partition function that partition_scheme_name is using in terms of data type, length, and precision. A computed column that participates in a partition function must be explicitly marked PERSISTED. We recommend that you specify NOT NULL on the partitioning column of partitioned tables, and also nonpartitioned tables that are sources or targets of ALTER TABLE...SWITCH operations. Doing this makes sure that any CHECK constraints on partitioning columns do not have to check for null values.\n• Specifies how full the Database Engine should make each index page that is used to store the index data. User-specified fillfactor values can be from 1 through 100. If a value isn't specified, the default is 0. Fill factor values 0 and 100 are the same in all respects. Documenting WITH FILLFACTOR = fillfactor as the only index option that applies to PRIMARY KEY or UNIQUE constraints is maintained for backward compatibility, but will not be documented in this manner in future releases.\n\nThe name of the column set. A column set is an untyped XML representation that combines all of the sparse columns of a table into a structured output. For more information about column sets, see Use Column Sets.\n\nApplies to: SQL Server 2016 (13.x) and later, and Azure SQL Database.\n\nSpecifies the names of the columns that the system will use to record the period for which a record is valid. Use this argument with the and arguments to create a temporal table. For more information, see Temporal Tables.\n\nApplies to: SQL Server 2016 (13.x) and later, and Azure SQL Database.\n\nFor a memory-optimized, delay specifies the minimum number of minutes a row must remain in the table, unchanged, before it is eligible for compression into the columnstore index. SQL Server selects specific rows to compress according to their last update time. For example, if rows are changing frequently during a two-hour period of time, you could set to ensure updates are completed before SQL Server compresses the row.\n\nFor a disk-based table, delay specifies the minimum number of minutes a delta rowgroup in the CLOSED state must remain in the delta rowgroup before SQL Server can compress it into the compressed rowgroup. Since disk-based tables don't track insert and update times on individual rows, SQL Server applies the delay to delta rowgroups in the CLOSED state.\n\nFor recommendations on when to use , see Get started with Columnstore for real time operational analytics\n\nSpecifies one or more table options.\n\nSpecifies the data compression option for the specified table, partition number, or range of partitions. The options are as follows:\n• Table or specified partitions are compressed by using row compression.\n• Table or specified partitions are compressed by using page compression.\n• Applies to: SQL Server 2016 (13.x) and later, and Azure SQL Database. Applies only to columnstore indexes, including both nonclustered columnstore and clustered columnstore indexes. COLUMNSTORE specifies to compress with the most performant columnstore compression. This is the typical choice.\n• Applies to: SQL Server 2016 (13.x) and later, and Azure SQL Database. Applies only to columnstore indexes, including both nonclustered columnstore and clustered columnstore indexes. COLUMNSTORE_ARCHIVE will further compress the table or partition to a smaller size. This can be used for archival, or for other situations that require a smaller storage size and can afford more time for storage and retrieval.\n\nFor more information, see Data Compression.\n\nApplies to: SQL Server 2022 (16.x) and later versions, Azure SQL Database, and Azure SQL Managed Instance.\n\nSpecifies the XML compression option for any xml data type columns in the table. The options are as follows:\n• Columns using the xml data type are compressed.\n\nSpecifies the partitions to which the or settings apply. If the table isn't partitioned, the argument will generate an error. If the clause isn't provided, the option will apply to all partitions of a partitioned table.\n\npartition_number_expression can be specified in the following ways:\n• Provide the partition number of a partition, for example:\n• Provide the partition numbers for several individual partitions separated by commas, for example:\n• Provide both ranges and individual partitions, for example:\n\ncan be specified as partition numbers separated by the word TO, for example: .\n\nTo set different types of data compression for different partitions, specify the option more than once, for example:\n\nYou can also specify the option more than once, for example:\n\nSpecifies one or more index options. For a complete description of these options, see CREATE INDEX.\n\nWhen ON, the percentage of free space specified by FILLFACTOR is applied to the intermediate level pages of the index. When OFF or a FILLFACTOR value it not specified, the intermediate level pages are filled to near capacity leaving enough space for at least one row of the maximum size the index can have, considering the set of keys on the intermediate pages. The default is OFF.\n\nSpecifies a percentage that indicates how full the Database Engine should make the leaf level of each index page during index creation or alteration. fillfactor must be an integer value from 1 to 100. The default is 0. Fill factor values 0 and 100 are the same in all respects.\n\nSpecifies the error response when an insert operation attempts to insert duplicate key values into a unique index. The IGNORE_DUP_KEY option applies only to insert operations after the index is created or rebuilt. The option has no effect when executing CREATE INDEX, ALTER INDEX, or UPDATE. The default is OFF.\n• A warning message will occur when duplicate key values are inserted into a unique index. Only the rows violating the uniqueness constraint will fail.\n• An error message will occur when duplicate key values are inserted into a unique index. The entire INSERT operation will be rolled back.\n\ncan't be set to ON for indexes created on a view, non-unique indexes, XML indexes, spatial indexes, and filtered indexes.\n\nIn backward compatible syntax, is equivalent to .\n\nWhen ON, out-of-date index statistics aren't automatically recomputed. When OFF, automatic statistics updating are enabled. The default is OFF.\n\nWhen ON, row locks are allowed when you access the index. The Database Engine determines when row locks are used. When OFF, row locks aren't used. The default is ON.\n\nWhen ON, page locks are allowed when you access the index. The Database Engine determines when page locks are used. When OFF, page locks aren't used. The default is ON.\n\nApplies to: SQL Server 2019 (15.x) and later, Azure SQL Database, and Azure SQL Managed Instance.\n\nSpecifies whether or not to optimize for last-page insert contention. The default is OFF. See the Sequential Keys section of the CREATE INDEX page for more information.\n\nApplies to: SQL Server 2012 (11.x) and later.\n\nSpecifies the windows-compatible FileTable directory name. This name should be unique among all the FileTable directory names in the database. Uniqueness comparison is case-insensitive, regardless of collation settings. If this value isn't specified, the name of the FileTable is used.\n\nApplies to: SQL Server 2012 (11.x) and later. Azure SQL Database and Azure SQL Managed Instance do not support .\n\nSpecifies the name of the collation to be applied to the column in the FileTable. The collation must be case-insensitive to comply with Windows operating system file naming semantics. If this value isn't specified, the database default collation is used. If the database default collation is case-sensitive, an error is raised, and the CREATE TABLE operation fails.\n• The name of a case-insensitive collation.\n• Specifies that the default collation for the database should be used. This collation must be case-insensitive.\n\nApplies to: SQL Server 2012 (11.x) and later. Azure SQL Database and Azure SQL Managed Instance do not support .\n\nSpecifies the name to be used for the primary key constraint that is automatically created on the FileTable. If this value isn't specified, the system generates a name for the constraint.\n\nApplies to: SQL Server 2012 (11.x) and later. Azure SQL Database and Azure SQL Managed Instance do not support .\n\nSpecifies the name to be used for the unique constraint that is automatically created on the stream_id column in the FileTable. If this value isn't specified, the system generates a name for the constraint.\n\nApplies to: SQL Server 2012 (11.x) and later. Azure SQL Database and Azure SQL Managed Instance do not support .\n\nSpecifies the name to be used for the unique constraint that is automatically created on the parent_path_locator and name columns in the FileTable. If this value isn't specified, the system generates a name for the constraint.\n\nSYSTEM_VERSIONING = ON [ ( HISTORY_TABLE = schema_name.history_table_name [ , DATA_CONSISTENCY_CHECK = { ON | OFF } ] ) ]\n\nApplies to: SQL Server 2016 (13.x) and later, Azure SQL Database, and Azure SQL Managed Instance.\n\nEnables system versioning of the table if the data type, nullability constraint, and primary key constraint requirements are met. The system will record the history of each record in the system-versioned table in a separate history table. If the argument isn't used, the name of this history table will be . If the name of a history table is specified during history table creation, you must specify the schema and table name.\n\nIf the history table doesn't exist, the system generates a new history table matching the schema of the current table in the same filegroup as the current table, creating a link between the two tables and enables the system to record the history of each record in the current table in the history table. By default, the history table is compressed.\n\nIf the argument is used to create a link to and use an existing history table, the link is created between the current table and the specified table. If current table is partitioned, the history table is created on default file group because partitioning configuration isn't replicated automatically from the current table to the history table. When creating a link to an existing history table, you can choose to perform a data consistency check. This data consistency check ensures that existing records don't overlap. Performing the data consistency check is the default.\n\nUse this argument with the and arguments to enable system versioning on a table. For more information, see Temporal Tables. Use this argument with the argument to create an updatable ledger table. Using existing history tables with ledger tables isn't allowed.\n\nApplies to: SQL Server 2016 (13.x) and later.\n\nCreates the new table with Stretch Database enabled or disabled. For more info, see Stretch Database.\n\nWhen you enable Stretch for a table by specifying , you can optionally specify to begin migrating data immediately, or to postpone data migration. The default value is . For more info about enabling Stretch for a table, see Enable Stretch Database for a table.\n\nPrerequisites. Before you enable Stretch for a table, you have to enable Stretch on the server and on the database. For more info, see Enable Stretch Database for a database.\n\nPermissions. Enabling Stretch for a database or a table requires db_owner permissions. Enabling Stretch for a table also requires ALTER permissions on the table.\n\nApplies to: SQL Server 2016 (13.x) and later.\n\nOptionally specifies a filter predicate to select rows to migrate from a table that contains both historical and current data. The predicate must call a deterministic inline table-valued function. For more info, see Enable Stretch Database for a table and Select rows to migrate by using a filter function.\n\nIf you don't specify a filter predicate, the entire table is migrated.\n\nWhen you specify a filter predicate, you also have to specify MIGRATION_STATE.\n\nApplies to: SQL Server 2016 (13.x) and later, Azure SQL Database, and Azure SQL Managed Instance.\n• None Specify to migrate data from SQL Server to Azure SQL Database.\n• None Specify to copy the remote data for the table from Azure SQL Database back to SQL Server and to disable Stretch for the table. For more info, see Disable Stretch Database and bring back remote data. This operation incurs data transfer costs, and it can't be canceled.\n• None Specify to pause or postpone data migration. For more info, see Pause and resume data migration -Stretch Database.\n\nEnables retention policy based cleanup of old or aged data from tables within a database. For more information, see Enable and Disable Data Retention. The following parameters must be specified for data retention to be enabled.\n• Specifies the column that should be used to determine if the rows in the table are obsolete or not. The following data types are allowed for the filter column.\n• Specifies the retention period policy for the table. The retention period is specified as a combination of a positive integer value and the date part unit.\n\nApplies to: SQL Server 2014 (12.x) and later, Azure SQL Database, and Azure SQL Managed Instance. Azure SQL Managed Instance does not support memory optimized tables in General Purpose tier.\n\nThe value ON indicates that the table is memory optimized. Memory-optimized tables are part of the In-Memory OLTP feature, which is used to optimize the performance of transaction processing. To get started with In-Memory OLTP see Quickstart 1: In-Memory OLTP Technologies for Faster Transact-SQL Performance. For more in-depth information about memory-optimized tables, see Memory-Optimized Tables.\n\nThe default value OFF indicates that the table is disk-based.\n\nApplies to: SQL Server 2014 (12.x) and later, Azure SQL Database, and Azure SQL Managed Instance.\n\nThe value of indicates that the table is durable, meaning that changes are persisted on disk and survive restart or failover. SCHEMA_AND_DATA is the default value.\n\nThe value of indicates that the table is non-durable. The table schema is persisted but any data updates aren't persisted upon a restart or failover of the database. is only allowed with .\n\nApplies to: SQL Server 2014 (12.x) and later, Azure SQL Database, and Azure SQL Managed Instance.\n\nIndicates the number of buckets that should be created in the hash index. The maximum value for BUCKET_COUNT in hash indexes is 1,073,741,824. For more information about bucket counts, see Indexes for Memory-Optimized Tables.\n\nApplies to: SQL Server 2014 (12.x) and later, Azure SQL Database, and Azure SQL Managed Instance.\n\nColumn and table indexes can be specified as part of the CREATE TABLE statement. For details about adding and removing indexes on memory-optimized tables, see Altering Memory-Optimized Tables\n• Applies to: SQL Server 2014 (12.x) and later, Azure SQL Database, and Azure SQL Managed Instance. Indicates that a HASH index is created. Hash indexes are supported only on memory-optimized tables.\n\nIndicates whether the table being created is a ledger table (ON) or not (OFF). The default is OFF. If the option is specified, the system creates an append-only ledger table allowing only inserting new rows. Otherwise, the system creates an updatable ledger table. An updatable ledger table also requires the argument. An updatable ledger table must also be a system-versioned table. However, an updatable ledger table doesn't have to be a temporal table (it doesn't require the parameter). If the history table is specified with and , it must not reference an existing table.\n\nA ledger database (a database created with the option) only allows the creation of ledger tables. Attempts to create a table with will raise an error. Each new table by default is created as an updatable ledger table, even if you don't specify , and will be created with default values for all other parameters.\n\nAn updatable ledger table must contain four columns, exactly one column defined with each of the following arguments:\n\nAn append-only ledger table must contain exactly one column defined with each of the following arguments:\n\nIf any of the required generated always columns isn't defined in the statement and the statement includes , the system will automatically attempt to add the column using an applicable column definition from the below list. If there is a name conflict with an already defined column, the system will raise an error.\n\nThe <ledger_view_option> specifies the schema and the name of the ledger view the system automatically creates and links to the table. If the option isn't specified, the system generates the ledger view name by appending to the name of the table being created ( ). If a view with the specified or generated name exists, the system will raise an error. If the table is an updatable ledger table, the ledger view is created as a union on the table and its history table.\n\nEach row in the ledger view represents either the creation or deletion of a row version in the ledger table. The ledger view contains all columns of the ledger table, except the generated always columns listed above. The ledger view also contains the following additional columns:\n\nTransactions that include creating ledger table are captured in sys.database_ledger_transactions.\n\nSpecifies the name of the ledger view and the names of additional columns the system adds to the ledger view.\n\nSpecifies whether the ledger table being created is append-only or updatable. The default is .\n\nSpecifies one or more ledger view options. Each of the ledger view option specifies a name of a column, the system will add to the view, in addition to the columns defined in the ledger table.\n\nSpecifies the name of the column storing the ID of the transaction that created or deleted a row version. The default column name is .\n\nSpecifies the name of the columns storing the sequence number of a row-level operation within the transaction on the table. The default column name is .\n\nSpecifies the name of the columns storing the operation type ID. The default column name is ledger_operation_type.\n\nSpecifies the name of the columns storing the operation type description. The default column name is .\n\nFor information about the number of allowed tables, columns, constraints and indexes, see Maximum Capacity Specifications for SQL Server.\n\nSpace is generally allocated to tables and indexes in increments of one extent at a time. When the option of is set to TRUE, or always prior to SQL Server 2016 (13.x), when a table or index is created, it is allocated pages from mixed extents until it has enough pages to fill a uniform extent. After it has enough pages to fill a uniform extent, another extent is allocated every time the currently allocated extents become full. For a report about the amount of space allocated and used by a table, execute .\n\nThe Database Engine doesn't enforce an order in which DEFAULT, IDENTITY, ROWGUIDCOL, or column constraints are specified in a column definition.\n\nWhen a table is created, the QUOTED IDENTIFIER option is always stored as ON in the metadata for the table, even if the option is set to OFF when the table is created.\n\nIn SQL database in Microsoft Fabric, some table features can be created but will not be mirrored into the Fabric OneLake. For more information, see Limitations of Fabric SQL database mirroring.\n\nYou can create local and global temporary tables. Local temporary tables are visible only in the current session, and global temporary tables are visible to all sessions. Temporary tables can't be partitioned.\n\nPrefix local temporary table names with single number sign ( ), and prefix global temporary table names with a double number sign ( ).\n\nTransact-SQL statements reference the temporary table by using the value specified for table_name in the statement, for example:\n\nIf more than one temporary table is created inside a single stored procedure or batch, they must have different names.\n\nIf you include a schema_name when you create or access a temporary table, it is ignored. All temporary tables are created in the schema.\n\nIf a local temporary table is created in a stored procedure or a SQL module that can be executed at the same time by several sessions, the Database Engine must be able to distinguish the tables created by the different sessions. The Database Engine does this by internally appending a unique suffix to each local temporary table name. The full name of a temporary table as stored in the table in is made up of the table name specified in the statement and the system-generated unique suffix. To allow for the suffix, table_name specified for a local temporary name can't exceed 116 characters.\n\nTemporary tables are automatically dropped when they go out of scope, unless explicitly dropped earlier by using :\n• A local temporary table created in a stored procedure is dropped automatically when the stored procedure is finished. The table can be referenced by any nested stored procedures executed by the stored procedure that created the table. The table can't be referenced by the process that called the stored procedure that created the table.\n• All other local temporary tables are dropped automatically at the end of the current session.\n• If the database-scoped configuration is set to ON (default), then global temporary tables are automatically dropped when the session that created the table ends and all other tasks have stopped referencing them. The association between a task and a table is maintained only for the life of a single Transact-SQL statement. This means that a global temporary table is dropped at the completion of the last Transact-SQL statement that was actively referencing the table when the creating session ended.\n• If the database-scoped configuration is set to OFF, then global temporary tables are only dropped using , or when the Database Engine instance restarts. For more information, see GLOBAL_TEMPORARY_TABLE_AUTO_DROP.\n\nA local temporary table created within a stored procedure or trigger can have the same name as a temporary table that was created before the stored procedure or trigger is called. However, if a query references a temporary table and two temporary tables with the same name exist at that time, it isn't defined which table the query is resolved against. Nested stored procedures can also create temporary tables with the same name as a temporary table that was created by the calling stored procedure. However, for modifications to resolve to the table that was created in the nested procedure, the table must have the same structure, with the same column names, as the table created in the calling procedure. This is shown in the following example.\n\nWhen you create local or global temporary tables, the syntax supports constraint definitions except for constraints. If a constraint is specified in a temporary table, the statement returns a warning message that states the constraint was skipped. The table is still created without the constraint. Temporary tables can't be referenced in constraints.\n\nIf a temporary table is created with a named constraint and the temporary table is created within the scope of a user-defined transaction, only one user at a time can execute the statement that creates the temp table. For example, if a stored procedure creates a temporary table with a named primary key constraint, the stored procedure can't be executed simultaneously by multiple users.\n\nGlobal temporary tables in SQL Server (table names prefixed with ) are stored in and shared among all user sessions across the entire SQL Server instance.\n\nAzure SQL Database supports global temporary tables that are also stored in but are scoped to the database level. This means that global temporary tables are shared among all user sessions within the same database. User sessions from other databases can't access global temporary tables. Otherwise, global temporary tables for Azure SQL Database follow the same syntax and semantics that SQL Server uses.\n\nSimilarly, global temporary stored procedures are also scoped to the database level in Azure SQL Database.\n\nLocal temporary tables (table names prefixed with ) are also supported for Azure SQL Database and follow the same syntax and semantics that SQL Server uses. For more information, see Temporary tables.\n\nAny user can create and access temporary objects.\n\nBefore creating a partitioned table by using CREATE TABLE, you must first create a partition function to specify how the table becomes partitioned. A partition function is created by using CREATE PARTITION FUNCTION. Second, you must create a partition scheme to specify the filegroups that will hold the partitions indicated by the partition function. A partition scheme is created by using CREATE PARTITION SCHEME. Placement of PRIMARY KEY or UNIQUE constraints to separate filegroups can't be specified for partitioned tables. For more information, see Partitioned Tables and Indexes.\n• None A table can contain only one PRIMARY KEY constraint.\n• None The index generated by a PRIMARY KEY constraint can't cause the number of indexes on the table to exceed 999 nonclustered indexes and 1 clustered index.\n• None If CLUSTERED or NONCLUSTERED isn't specified for a PRIMARY KEY constraint, CLUSTERED is used if there are no clustered indexes specified for UNIQUE constraints.\n• None All columns defined within a PRIMARY KEY constraint must be defined as NOT NULL. If nullability isn't specified, all columns participating in a PRIMARY KEY constraint have their nullability set to NOT NULL. For memory-optimized tables, the nullable key column is allowed.\n• None If a primary key is defined on a CLR user-defined type column, the implementation of the type must support binary ordering. For more information, see CLR User-Defined Types.\n• If CLUSTERED or NONCLUSTERED isn't specified for a UNIQUE constraint, NONCLUSTERED is used by default.\n• Each UNIQUE constraint generates an index. The number of UNIQUE constraints can't cause the number of indexes on the table to exceed 999 nonclustered indexes and 1 clustered index.\n• If a unique constraint is defined on a CLR user-defined type column, the implementation of the type must support binary or operator-based ordering. For more information, see CLR User-Defined Types.\n• None When a value other than NULL is entered into the column of a FOREIGN KEY constraint, the value must exist in the referenced column; otherwise, a foreign key violation error message is returned.\n• None FOREIGN KEY constraints are applied to the preceding column, unless source columns are specified.\n• None FOREIGN KEY constraints can reference only tables within the same database on the same server. Cross-database referential integrity must be implemented through triggers. For more information, see CREATE TRIGGER.\n• None FOREIGN KEY constraints can reference another column in the same table. This is referred to as a self-reference.\n• None The REFERENCES clause of a column-level FOREIGN KEY constraint can list only one reference column. This column must have the same data type as the column on which the constraint is defined.\n• None The REFERENCES clause of a table-level FOREIGN KEY constraint must have the same number of reference columns as the number of columns in the constraint column list. The data type of each reference column must also be the same as the corresponding column in the column list. The reference columns must be specified in the same order that was used when specifying the columns of the primary key or unique constraint on the referenced table.\n• None CASCADE, SET NULL or SET DEFAULT can't be specified if a column of type timestamp is part of either the foreign key or the referenced key.\n• None CASCADE, SET NULL, SET DEFAULT and NO ACTION can be combined on tables that have referential relationships with each other. If the Database Engine encounters NO ACTION, it stops and rolls back related CASCADE, SET NULL and SET DEFAULT actions. When a DELETE statement causes a combination of CASCADE, SET NULL, SET DEFAULT and NO ACTION actions, all the CASCADE, SET NULL and SET DEFAULT actions are applied before the Database Engine checks for any NO ACTION.\n• None The Database Engine doesn't have a predefined limit on either the number of FOREIGN KEY constraints a table can contain that reference other tables, or the number of FOREIGN KEY constraints that are owned by other tables that reference a specific table. Nevertheless, the actual number of FOREIGN KEY constraints that can be used is limited by the hardware configuration and by the design of the database and application. We recommend that a table contain no more than 253 FOREIGN KEY constraints, and that it be referenced by no more than 253 FOREIGN KEY constraints. The effective limit for you might be more or less depending on the application and hardware. Consider the cost of enforcing FOREIGN KEY constraints when you design your database and applications.\n• None FOREIGN KEY constraints can reference only columns in PRIMARY KEY or UNIQUE constraints in the referenced table or in a UNIQUE INDEX on the referenced table.\n• None If a foreign key is defined on a CLR user-defined type column, the implementation of the type must support binary ordering. For more information, see CLR User-Defined Types.\n• None Columns participating in a foreign key relationship must be defined with the same length and scale.\n• None A column can have only one DEFAULT definition.\n• None A DEFAULT definition can contain constant values, functions, SQL standard niladic functions, or . The following table shows the niladic functions and the values they return for the default during an INSERT statement. Name of user performing an insert. Name of user performing an insert. Name of user performing an insert. Name of user performing an insert.\n• None constant_expression in a DEFAULT definition can't refer to another column in the table, or to other tables, views, or stored procedures.\n• None DEFAULT definitions can't be created on columns with a timestamp data type or columns with an IDENTITY property.\n• None DEFAULT definitions can't be created for columns with alias data types if the alias data type is bound to a default object.\n• None A column can have any number of CHECK constraints, and the condition can include multiple logical expressions combined with AND and OR. Multiple CHECK constraints for a column are validated in the order they are created.\n• None The search condition must evaluate to a Boolean expression and can't reference another table.\n• None A column-level CHECK constraint can reference only the constrained column, and a table-level CHECK constraint can reference only columns in the same table. CHECK CONSTRAINTS and rules serve the same function of validating the data during INSERT and UPDATE statements.\n• None When a rule and one or more CHECK constraints exist for a column or columns, all restrictions are evaluated.\n• None CHECK constraints can't be defined on text, ntext, or image columns.\n• An index created for a constraint can't be dropped by using ; the constraint must be dropped by using . An index created for and used by a constraint can be rebuilt by using . For more information, see Reorganize and Rebuild Indexes.\n• Constraint names must follow the rules for identifiers, except that the name can't start with a number sign (#). If constraint_name isn't supplied, a system-generated name is assigned to the constraint. The constraint name appears in any error message about constraint violations.\n• When a constraint is violated in an , , or statement, the statement is ended. However, when is set to OFF, the transaction, if the statement is part of an explicit transaction, continues to be processed. When is set to ON, the whole transaction is rolled back. You can also use the statement with the transaction definition by checking the system function.\n• When and , row-, page-, and table-level locks are allowed when you access the index. The Database Engine chooses the appropriate lock and can escalate the lock from a row or page lock to a table lock. When and , only a table-level lock is allowed when you access the index.\n• If a table has FOREIGN KEY or CHECK CONSTRAINTS and triggers, the constraint conditions are evaluated before the trigger is executed.\n\nFor a report on a table and its columns, use or . To rename a table, use . For a report on the views and stored procedures that depend on a table, use sys.dm_sql_referenced_entities and sys.dm_sql_referencing_entities.\n\nThe nullability of a column determines whether that column can allow a null value ( ) as the data in that column. isn't zero or blank: means no entry was made or an explicit was supplied, and it typically implies that the value is either unknown or not applicable.\n\nWhen you use or to create or alter a table, database and session settings influence and possibly override the nullability of the data type that is used in a column definition. We recommend that you always explicitly define a column as NULL or NOT NULL for noncomputed columns or, if you use a user-defined data type, that you allow the column to use the default nullability of the data type. Sparse columns must always allow NULL.\n\nWhen column nullability isn't explicitly specified, column nullability follows the rules shown in the following table.\n\nWhen neither of the ANSI_NULL_DFLT options is set for the session and the database is set to the default (ANSI_NULL_DEFAULT is OFF), the default of NOT NULL is assigned.\n\nIf the column is a computed column, its nullability is always automatically determined by the Database Engine. To find out the nullability of this type of column, use the function with the AllowsNull property.\n\nSystem tables can't be enabled for compression. When you are creating a table, data compression is set to NONE, unless specified otherwise. If you specify a list of partitions or a partition that is out of range, an error will be generated. For a more information about data compression, see Data Compression.\n\nTo evaluate how changing the compression state will affect a table, an index, or a partition, use the sp_estimate_data_compression_savings stored procedure.\n\nRequires permission in the database and permission on the schema in which the table is being created.\n\nIf any columns in the statement are defined to be of a user-defined type, permission on the user-defined type is required.\n\nIf any columns in the statement are defined to be of a CLR user-defined type, either ownership of the type or permission on it is required.\n\nIf any columns in the statement have an XML schema collection associated with them, either ownership of the XML schema collection or permission on it is required.\n\nAny user can create temporary tables in .\n\nIf the statement creates a ledger table, permission is required.\n\nThe following example shows the column definition for a PRIMARY KEY constraint with a clustered index on the column of the table. Because a constraint name isn't specified, the system supplies the constraint name.\n\nA FOREIGN KEY constraint is used to reference another table. Foreign keys can be single-column keys or multicolumn keys. This following example shows a single-column FOREIGN KEY constraint on the table that references the table. Only the REFERENCES clause is required for a single-column FOREIGN KEY constraint.\n\nYou can also explicitly use the FOREIGN KEY clause and restate the column attribute. The column name doesn't have to be the same in both tables.\n\nMulticolumn key constraints are created as table constraints. In the database, the table includes a multicolumn PRIMARY KEY. The following example shows how to reference this key from another table; an explicit constraint name is optional.\n\nUNIQUE constraints are used to enforce uniqueness on nonprimary key columns. The following example enforces a restriction that the column of the table must be unique.\n\nDefaults supply a value (with the INSERT and UPDATE statements) when no value is supplied. For example, the database could include a lookup table listing the different jobs employees can fill in the company. Under a column that describes each job, a character string default could supply a description when an actual description isn't entered explicitly.\n\nIn addition to constants, DEFAULT definitions can include functions. Use the following example to get the current date for an entry.\n\nA niladic-function scan can also improve data integrity. To keep track of the user that inserted a row, use the niladic-function for USER. Don't enclose the niladic-functions with parentheses.\n\nThe following example shows a restriction made to values that are entered into the column of the table. The constraint is unnamed.\n\nThis example shows a named constraint with a pattern restriction on the character data entered into a column of a table.\n\nThis example specifies that the values must be within a specific list or follow a specified pattern.\n\nThe following example shows the complete table definitions with all constraint definitions for table created in the database. To run the sample, the table schema is changed to .\n\nG. Create a table with an xml column typed to an XML schema collection\n\nThe following example creates a table with an column that is typed to XML schema collection . The keyword specifies that each instance of the data type in column_name can contain only one top-level element.\n\nThe following example creates a partition function to partition a table or index into four partitions. Then, the example creates a partition scheme that specifies the filegroups in which to hold each of the four partitions. Finally, the example creates a table that uses the partition scheme. This example assumes the filegroups already exist in the database.\n\nBased on the values of column of , the partitions are assigned in the following ways.\n\nI. Use the UNIQUEIDENTIFIER data type in a column\n\nThe following example creates a table with a column. The example uses a PRIMARY KEY constraint to protect the table against users inserting duplicated values, and it uses the function in the constraint to provide values for new rows. The ROWGUIDCOL property is applied to the column so that it can be referenced using the $ROWGUID keyword.\n\nJ. Use an expression for a computed column\n\nThe following example shows the use of an expression ( ) for calculating the computed column.\n\nThe following example creates a table with one column defined as user-defined type , assuming that the type's assembly, and the type itself, have already been created in the current database. A second column is defined based on , and uses method of type(class) to compute a value for the column.\n\nL. Use the USER_NAME function for a computed column\n\nThe following example uses the function in the column.\n\nThe following example creates a table that has a column . If a table has one or more columns, the table must have one column.\n\nThe following example creates a table that uses row compression.\n\nApplies to: SQL Server 2022 (16.x) and later versions, Azure SQL Database, and Azure SQL Managed Instance.\n\nThe following example creates a table that uses XML compression.\n\nP. Create a table that has sparse columns and a column set\n\nThe following examples show to how to create a table that has a sparse column, and a table that has two sparse columns and a column set. The examples use the basic syntax. For more complex examples, see Use Sparse Columns and Use Column Sets.\n\nThis example creates a table that has a sparse column.\n\nThis example creates a table that has two sparse columns and a column set named .\n\nApplies to: SQL Server 2016 (13.x) and later, and Azure SQL Database.\n\nThe following examples show how to create a temporal table linked to a new history table, and how to create a temporal table linked to an existing history table. The temporal table must have a primary key defined to be enabled for the table to be enabled for system versioning. For examples showing how to add or remove system versioning on an existing table, see System Versioning in Examples. For use cases, see Temporal Tables.\n\nThis example creates a new temporal table linked to a new history table.\n\nThis example creates a new temporal table linked to an existing history table.\n\nApplies to: SQL Server 2016 (13.x) and later, and Azure SQL Database.\n\nThe following example shows how to create a system-versioned memory-optimized temporal table linked to a new disk-based history table.\n\nThis example creates a new temporal table linked to a new history table.\n\nThis example creates a new temporal table linked to an existing history table.\n\nThe following example creates a table with two encrypted columns. For more information, see Always Encrypted.\n\nThe following shows how to use NONCLUSTERED inline for disk-based tables:\n\nCreates a table with an anonymously named compound primary key. This is useful to avoid run-time conflicts where two session-scoped temp tables, each in a separate session, use the same name for a constraint.\n\nIf you explicitly name the constraint, the second session will generate an error such as:\n\nThe problem arises from the fact that while the temp table name is unique, the constraint names aren't.\n\nSession A creates a global temp table ##test in Azure SQL Database testdb1 and adds one row\n\nObtain global temp table name for a given object ID 1253579504 in (2)\n\nSession B connects to Azure SQL Database testdb1 and can access table ##test created by session A\n\nSession C connects to another database in Azure SQL Database testdb2 and wants to access ##test created in testdb1. This select fails due to the database scope for the global temp tables\n\nWhich generates the following error:\n\nThe following example creates a table with data retention enabled and a retention period of one week. This example applies to Azure SQL Edge only.\n\nThe following example creates an updatable ledger table that isn't a temporal table with an anonymous history table (the system will generate the name of the history table) and the generated ledger view name. As the names of the required generated always columns and the additional columns in the ledger view aren't specified, the columns will have the default names.\n\nThe following example creates a table that is both a temporal table and an updatable ledger table, with an anonymous history table (with a name generated by the system), the generated ledger view name and the default names of the generated always columns and the additional ledger view columns.\n\nThe following example creates a table that is both a temporal table and an updatable ledger table with the explicitly named history table, the user-specified name of the ledger view, and the user-specified names of generated always columns and additional columns in the ledger view.\n\nThe following example creates an append-only ledger table with the generated names of the ledger view and the columns in the ledger view.\n\nThe following example creates a ledger database in Azure SQL Database and an updatable ledger table using the default settings. Creating an updatable ledger table in a ledger database doesn't require using WITH (SYSTEM_VERSIONING = ON, LEDGER = ON); ."
    },
    {
        "link": "https://w3schools.com/sql/sql_constraints.asp",
        "document": "W3Schools offers a wide range of services and products for beginners and professionals, helping millions of people everyday to learn and master new skills."
    },
    {
        "link": "https://postgresql.org/docs/current/sql-createtable.html",
        "document": "The optional clause specifies a list of tables from which the new table automatically inherits all columns. Parent tables can be plain tables or foreign tables. Use of creates a persistent relationship between the new child table and its parent table(s). Schema modifications to the parent(s) normally propagate to children as well, and by default the data of the child table is included in scans of the parent(s). If the same column name exists in more than one parent table, an error is reported unless the data types of the columns match in each of the parent tables. If there is no conflict, then the duplicate columns are merged to form a single column in the new table. If the column name list of the new table contains a column name that is also inherited, the data type must likewise match the inherited column(s), and the column definitions are merged into one. If the new table explicitly specifies a default value for the column, this default overrides any defaults from inherited declarations of the column. Otherwise, any parents that specify default values for the column must all specify the same default, or an error will be reported. constraints are merged in essentially the same way as columns: if multiple parent tables and/or the new table definition contain identically-named constraints, these constraints must all have the same check expression, or an error will be reported. Constraints having the same name and expression will be merged into one copy. A constraint marked in a parent will not be considered. Notice that an unnamed constraint in the new table will never be merged, since a unique name will always be chosen for it. Column settings are also copied from parent tables. If a column in the parent table is an identity column, that property is not inherited. A column in the child table can be declared identity column if desired.\n\nCreates the table as a partition of the specified parent table. The table can be created either as a partition for specific values using or as a default partition using . Any indexes, constraints and user-defined row-level triggers that exist in the parent table are cloned on the new partition. The must correspond to the partitioning method and partition key of the parent table, and must not overlap with any existing partition of that parent. The form with is used for list partitioning, the form with and is used for range partitioning, and the form with is used for hash partitioning. is any variable-free expression (subqueries, window functions, aggregate functions, and set-returning functions are not allowed). Its data type must match the data type of the corresponding partition key column. The expression is evaluated once at table creation time, so it can even contain volatile expressions such as . When creating a list partition, can be specified to signify that the partition allows the partition key column to be null. However, there cannot be more than one such list partition for a given parent table. cannot be specified for range partitions. When creating a range partition, the lower bound specified with is an inclusive bound, whereas the upper bound specified with is an exclusive bound. That is, the values specified in the list are valid values of the corresponding partition key columns for this partition, whereas those in the list are not. Note that this statement must be understood according to the rules of row-wise comparison (Section 9.25.5). For example, given , a partition bound allows with any , with any non-null , and with any . The special values and may be used when creating a range partition to indicate that there is no lower or upper bound on the column's value. For example, a partition defined using allows any values less than 10, and a partition defined using allows any values greater than or equal to 10. When creating a range partition involving more than one column, it can also make sense to use as part of the lower bound, and as part of the upper bound. For example, a partition defined using allows any rows where the first partition key column is greater than 0 and less than or equal to 10. Similarly, a partition defined using allows any rows where the first partition key column starts with \"a\". Note that if or is used for one column of a partitioning bound, the same value must be used for all subsequent columns. For example, is not a valid bound; you should write . Also note that some element types, such as , have a notion of \"infinity\", which is just another value that can be stored. This is different from and , which are not real values that can be stored, but rather they are ways of saying that the value is unbounded. can be thought of as being greater than any other value, including \"infinity\" and as being less than any other value, including \"minus infinity\". Thus the range is not an empty range; it allows precisely one value to be stored — \"infinity\". If is specified, the table will be created as the default partition of the parent table. This option is not available for hash-partitioned tables. A partition key value not fitting into any other partition of the given parent will be routed to the default partition. When a table has an existing partition and a new partition is added to it, the default partition must be scanned to verify that it does not contain any rows which properly belong in the new partition. If the default partition contains a large number of rows, this may be slow. The scan will be skipped if the default partition is a foreign table or if it has a constraint which proves that it cannot contain rows which should be placed in the new partition. When creating a hash partition, a modulus and remainder must be specified. The modulus must be a positive integer, and the remainder must be a non-negative integer less than the modulus. Typically, when initially setting up a hash-partitioned table, you should choose a modulus equal to the number of partitions and assign every table the same modulus and a different remainder (see examples, below). However, it is not required that every partition have the same modulus, only that every modulus which occurs among the partitions of a hash-partitioned table is a factor of the next larger modulus. This allows the number of partitions to be increased incrementally without needing to move all the data at once. For example, suppose you have a hash-partitioned table with 8 partitions, each of which has modulus 8, but find it necessary to increase the number of partitions to 16. You can detach one of the modulus-8 partitions, create two new modulus-16 partitions covering the same portion of the key space (one with a remainder equal to the remainder of the detached partition, and the other with a remainder equal to that value plus 8), and repopulate them with data. You can then repeat this -- perhaps at a later time -- for each modulus-8 partition until none remain. While this may still involve a large amount of data movement at each step, it is still better than having to create a whole new table and move all the data at once. A partition must have the same column names and types as the partitioned table to which it belongs. Modifications to the column names or types of a partitioned table will automatically propagate to all partitions. constraints will be inherited automatically by every partition, but an individual partition may specify additional constraints; additional constraints with the same name and condition as in the parent will be merged with the parent constraint. Defaults may be specified separately for each partition. But note that a partition's default value is not applied when inserting a tuple through a partitioned table. Rows inserted into a partitioned table will be automatically routed to the correct partition. If no suitable partition exists, an error will occur. Operations such as which normally affect a table and all of its inheritance children will cascade to all partitions, but may also be performed on an individual partition. Note that creating a partition using requires taking an lock on the parent partitioned table. Likewise, dropping a partition with requires taking an lock on the parent table. It is possible to use to perform these operations with a weaker lock, thus reducing interference with concurrent operations on the partitioned table.\n\nThe clause specifies a table from which the new table automatically copies all column names, their data types, and their not-null constraints. Unlike , the new table and original table are completely decoupled after creation is complete. Changes to the original table will not be applied to the new table, and it is not possible to include data of the new table in scans of the original table. Also unlike , columns and constraints copied by are not merged with similarly named columns and constraints. If the same name is specified explicitly or in another clause, an error is signaled. The optional clauses specify which additional properties of the original table to copy. Specifying copies the property, specifying omits the property. is the default. If multiple specifications are made for the same kind of object, the last one is used. The available options are: Comments for the copied columns, constraints, and indexes will be copied. The default behavior is to exclude comments, resulting in the copied columns and constraints in the new table having no comments. Compression method of the columns will be copied. The default behavior is to exclude compression methods, resulting in columns having the default compression method. constraints will be copied. No distinction is made between column constraints and table constraints. Not-null constraints are always copied to the new table. Default expressions for the copied column definitions will be copied. Otherwise, default expressions are not copied, resulting in the copied columns in the new table having null defaults. Note that copying defaults that call database-modification functions, such as , may create a functional linkage between the original and new tables. Any generation expressions of copied column definitions will be copied. By default, new columns will be regular base columns. Any identity specifications of copied column definitions will be copied. A new sequence is created for each identity column of the new table, separate from the sequences associated with the old table. Indexes, , , and constraints on the original table will be created on the new table. Names for the new indexes and constraints are chosen according to the default rules, regardless of how the originals were named. (This behavior avoids possible duplicate-name failures for the new indexes.) Extended statistics are copied to the new table. settings for the copied column definitions will be copied. The default behavior is to exclude settings, resulting in the copied columns in the new table having type-specific default settings. For more on settings, see Section 65.2. is an abbreviated form selecting all the available individual options. (It could be useful to write individual clauses after to select all but some specific options.) The clause can also be used to copy column definitions from views, foreign tables, or composite types. Inapplicable options (e.g., from a view) are ignored.\n\nThe constraint specifies that a group of one or more columns of a table can contain only unique values. The behavior of a unique table constraint is the same as that of a unique column constraint, with the additional capability to span multiple columns. The constraint therefore enforces that any two rows must differ in at least one of these columns. For the purpose of a unique constraint, null values are not considered equal, unless is specified. Each unique constraint should name a set of columns that is different from the set of columns named by any other unique or primary key constraint defined for the table. (Otherwise, redundant unique constraints will be discarded.) When establishing a unique constraint for a multi-level partition hierarchy, all the columns in the partition key of the target partitioned table, as well as those of all its descendant partitioned tables, must be included in the constraint definition. Adding a unique constraint will automatically create a unique btree index on the column or group of columns used in the constraint. The created index has the same name as the unique constraint. The optional clause adds to that index one or more columns that are simply “payload”: uniqueness is not enforced on them, and the index cannot be searched on the basis of those columns. However they can be retrieved by an index-only scan. Note that although the constraint is not enforced on included columns, it still depends on them. Consequently, some operations on such columns (e.g., ) can cause cascaded constraint and index deletion.\n\nThe clause defines an exclusion constraint, which guarantees that if any two rows are compared on the specified column(s) or expression(s) using the specified operator(s), not all of these comparisons will return . If all of the specified operators test for equality, this is equivalent to a constraint, although an ordinary unique constraint will be faster. However, exclusion constraints can specify constraints that are more general than simple equality. For example, you can specify a constraint that no two rows in the table contain overlapping circles (see Section 8.8) by using the operator. The operator(s) are required to be commutative. Exclusion constraints are implemented using an index that has the same name as the constraint, so each specified operator must be associated with an appropriate operator class (see Section 11.10) for the index access method . Each defines a column of the index, so it can optionally specify a collation, an operator class, operator class parameters, and/or ordering options; these are described fully under CREATE INDEX. The access method must support (see Chapter 62); at present this means cannot be used. Although it's allowed, there is little point in using B-tree or hash indexes with an exclusion constraint, because this does nothing that an ordinary unique constraint doesn't do better. So in practice the access method will always be or . The allows you to specify an exclusion constraint on a subset of the table; internally this creates a partial index. Note that parentheses are required around the predicate.\n\nThese clauses specify a foreign key constraint, which requires that a group of one or more columns of the new table must only contain values that match values in the referenced column(s) of some row of the referenced table. If the list is omitted, the primary key of the is used. Otherwise, the list must refer to the columns of a non-deferrable unique or primary key constraint or be the columns of a non-partial unique index. The user must have permission on the referenced table (either the whole table, or the specific referenced columns). The addition of a foreign key constraint requires a lock on the referenced table. Note that foreign key constraints cannot be defined between temporary tables and permanent tables. A value inserted into the referencing column(s) is matched against the values of the referenced table and referenced columns using the given match type. There are three match types: , , and (which is the default). will not allow one column of a multicolumn foreign key to be null unless all foreign key columns are null; if they are all null, the row is not required to have a match in the referenced table. allows any of the foreign key columns to be null; if any of them are null, the row is not required to have a match in the referenced table. is not yet implemented. (Of course, constraints can be applied to the referencing column(s) to prevent these cases from arising.) In addition, when the data in the referenced columns is changed, certain actions are performed on the data in this table's columns. The clause specifies the action to perform when a referenced row in the referenced table is being deleted. Likewise, the clause specifies the action to perform when a referenced column in the referenced table is being updated to a new value. If the row is updated, but the referenced column is not actually changed, no action is done. Referential actions other than the check cannot be deferred, even if the constraint is declared deferrable. There are the following possible actions for each clause: Produce an error indicating that the deletion or update would create a foreign key constraint violation. If the constraint is deferred, this error will be produced at constraint check time if there still exist any referencing rows. This is the default action. Produce an error indicating that the deletion or update would create a foreign key constraint violation. This is the same as except that the check is not deferrable. Delete any rows referencing the deleted row, or update the values of the referencing column(s) to the new values of the referenced columns, respectively. Set all of the referencing columns, or a specified subset of the referencing columns, to null. A subset of columns can only be specified for actions. Set all of the referencing columns, or a specified subset of the referencing columns, to their default values. A subset of columns can only be specified for actions. (There must be a row in the referenced table matching the default values, if they are not null, or the operation will fail.) If the referenced column(s) are changed frequently, it might be wise to add an index to the referencing column(s) so that referential actions associated with the foreign key constraint can be performed more efficiently."
    },
    {
        "link": "https://w3schools.com/sql/sql_datatypes.asp",
        "document": "The data type of a column defines what value the column can hold: integer, character, money, date and time, binary, and so on.\n\nEach column in a database table is required to have a name and a data type.\n\nAn SQL developer must decide what type of data that will be stored inside each column when creating a table. The data type is a guideline for SQL to understand what type of data is expected inside of each column, and it also identifies how SQL will interact with the stored data.\n\nIn MySQL there are three main data types: string, numeric, and date and time.\n\nNote: All the numeric data types may have an extra option: UNSIGNED or ZEROFILL. If you add the UNSIGNED option, MySQL disallows negative values for the column. If you add the ZEROFILL option, MySQL automatically also adds the UNSIGNED attribute to the column.\n\nInteger that can be 0, 1, or NULL Allows whole numbers from 0 to 255 Allows whole numbers between -32,768 and 32,767 Allows whole numbers between -2,147,483,648 and 2,147,483,647 Allows whole numbers between -9,223,372,036,854,775,808 and 9,223,372,036,854,775,807 Fixed precision and scale numbers. Allows numbers from -10^38 +1 to 10^38 –1. The p parameter indicates the maximum total number of digits that can be stored (both to the left and to the right of the decimal point). p must be a value from 1 to 38. Default is 18. The s parameter indicates the maximum number of digits stored to the right of the decimal point. s must be a value from 0 to p. Default value is 0 Fixed precision and scale numbers. Allows numbers from -10^38 +1 to 10^38 –1. The p parameter indicates the maximum total number of digits that can be stored (both to the left and to the right of the decimal point). p must be a value from 1 to 38. Default is 18. The s parameter indicates the maximum number of digits stored to the right of the decimal point. s must be a value from 0 to p. Default value is 0 Floating precision number data from -1.79E + 308 to 1.79E + 308. The n parameter indicates whether the field should hold 4 or 8 bytes. float(24) holds a 4-byte field and float(53) holds an 8-byte field. Default value of n is 53."
    },
    {
        "link": "https://dev.mysql.com/doc/en/create-table.html",
        "document": "creates a table with the given name. You must have the privilege for the table.\n\nBy default, tables are created in the default database, using the storage engine. An error occurs if the table exists, if there is no default database, or if the database does not exist.\n\nMySQL has no limit on the number of tables. The underlying file system may have a limit on the number of files that represent tables. Individual storage engines may impose engine-specific constraints. permits up to 4 billion tables.\n\nFor information about the physical representation of a table, see Section 15.1.20.1, “Files Created by CREATE TABLE”.\n\nThere are several aspects to the statement, described under the following topics in this section:\n\nYou can use the keyword when creating a table. A table is visible only within the current session, and is dropped automatically when the session is closed. For more information, see Section 15.1.20.2, “CREATE TEMPORARY TABLE Statement”.\n\nThere is a hard limit of 4096 columns per table, but the effective maximum may be less for a given table and depends on the factors discussed in Section 10.4.7, “Limits on Table Column Count and Row Size”.\n\nSeveral keywords apply to creation of indexes, foreign keys, and constraints. For general background in addition to the following descriptions, see Section 15.1.15, “CREATE INDEX Statement”, Section 15.1.20.5, “FOREIGN KEY Constraints”, and Section 15.1.20.6, “CHECK Constraints”.\n\nTable options are used to optimize the behavior of the table. In most cases, you do not have to specify any of them. These options apply to all storage engines unless otherwise indicated. Options that do not apply to a given storage engine may be accepted and remembered as part of the table definition. Such options then apply if you later use to convert the table to use a different storage engine.\n\ncan be used to control partitioning of the table created with .\n\nNot all options shown in the syntax for at the beginning of this section are available for all partitioning types. Please see the listings for the following individual types for information specific to each type, and see Chapter 26, Partitioning, for more complete information about the workings of and uses for partitioning in MySQL, as well as additional examples of table creation and other statements relating to MySQL partitioning.\n\nPartitions can be modified, merged, added to tables, and dropped from tables. For basic information about the MySQL statements to accomplish these tasks, see Section 15.1.9, “ALTER TABLE Statement”. For more detailed descriptions and examples, see Section 26.3, “Partition Management”.\n• None If used, a clause begins with . This clause contains the function that is used to determine the partition; the function returns an integer value ranging from 1 to , where is the number of partitions. (The maximum number of user-defined partitions which a table may contain is 1024; the number of subpartitions—discussed later in this section—is included in this maximum.) The expression ( ) used in a clause cannot refer to any columns not in the table being created; such references are specifically not permitted and cause the statement to fail with an error. (Bug #29444)\n• None Hashes one or more columns to create a key for placing and locating rows. is an expression using one or more table columns. This can be any valid MySQL expression (including MySQL functions) that yields a single integer value. For example, these are both valid statements using : You may not use either or clauses with . uses the remainder of divided by the number of partitions (that is, the modulus). For examples and additional information, see Section 26.2.4, “HASH Partitioning”. The keyword entails a somewhat different algorithm. In this case, the number of the partition in which a row is stored is calculated as the result of one or more logical operations. For discussion and examples of linear hashing, see Section 26.2.4.1, “LINEAR HASH Partitioning”.\n• None This is similar to , except that MySQL supplies the hashing function so as to guarantee an even data distribution. The argument is simply a list of 1 or more table columns (maximum: 16). This example shows a simple table partitioned by key, with 4 partitions: For tables that are partitioned by key, you can employ linear partitioning by using the keyword. This has the same effect as with tables that are partitioned by . That is, the partition number is found using the operator rather than the modulus (see Section 26.2.4.1, “LINEAR HASH Partitioning”, and Section 26.2.5, “KEY Partitioning”, for details). This example uses linear partitioning by key to distribute data between 5 partitions: The option is supported with . causes the server to use the same key-hashing functions as MySQL 5.1; means that the server employs the key-hashing functions implemented and used by default for new partitioned tables in MySQL 5.5 and later. (Partitioned tables created with the key-hashing functions employed in MySQL 5.5 and later cannot be used by a MySQL 5.1 server.) Not specifying the option has the same effect as using . This option is intended for use chiefly when upgrading or downgrading partitioned tables between MySQL 5.1 and later MySQL versions, or for creating tables partitioned by or on a MySQL 5.5 or later server which can be used on a MySQL 5.1 server. For more information, see Section 15.1.9.1, “ALTER TABLE Partition Operations”. is shown when necessary in the output of using versioned comments in the same manner as mysqldump. is always omitted from output, even if this option was specified when creating the original table. You may not use either or clauses with .\n• None In this case, shows a range of values using a set of operators. When using range partitioning, you must define at least one partition using . You cannot use with range partitioning. For tables partitioned by , must be used with either an integer literal value or an expression that evaluates to a single integer value. In MySQL 8.4, you can overcome this limitation in a table that is defined using , as described later in this section. Suppose that you have a table that you wish to partition on a column containing year values, according to the following scheme. A table implementing such a partitioning scheme can be realized by the statement shown here: CREATE TABLE t1 ( year_col INT, some_data INT ) PARTITION BY RANGE (year_col) ( PARTITION p0 VALUES LESS THAN (1991), PARTITION p1 VALUES LESS THAN (1995), PARTITION p2 VALUES LESS THAN (1999), PARTITION p3 VALUES LESS THAN (2002), PARTITION p4 VALUES LESS THAN (2006), PARTITION p5 VALUES LESS THAN MAXVALUE ); statements work in a consecutive fashion. works to specify “leftover” values that are greater than the maximum value otherwise specified. clauses work sequentially in a manner similar to that of the portions of a block (as found in many programming languages such as C, Java, and PHP). That is, the clauses must be arranged in such a way that the upper limit specified in each successive is greater than that of the previous one, with the one referencing coming last of all in the list.\n• None This variant on facilitates partition pruning for queries using range conditions on multiple columns (that is, having conditions such as or WHERE a = 1 AND b = 10 AND c < 10 ). It enables you to specify value ranges in multiple columns by using a list of columns in the clause and a set of column values in each partition definition clause. (In the simplest case, this set consists of a single column.) The maximum number of columns that can be referenced in the and is 16. The used in the clause may contain only names of columns; each column in the list must be one of the following MySQL data types: the integer types; the string types; and time or date column types. Columns using , , , , , or spatial data types are not permitted; columns that use floating-point number types are also not permitted. You also may not use functions or arithmetic expressions in the clause. The clause used in a partition definition must specify a literal value for each column that appears in the clause; that is, the list of values used for each clause must contain the same number of values as there are columns listed in the clause. An attempt to use more or fewer values in a clause than there are in the clause causes the statement to fail with the error Inconsistency in usage of column lists for partitioning.... You cannot use for any value appearing in . It is possible to use more than once for a given column other than the first, as shown in this example: CREATE TABLE rc ( a INT NOT NULL, b INT NOT NULL ) PARTITION BY RANGE COLUMNS(a,b) ( PARTITION p0 VALUES LESS THAN (10,5), PARTITION p1 VALUES LESS THAN (20,10), PARTITION p2 VALUES LESS THAN (50,MAXVALUE), PARTITION p3 VALUES LESS THAN (65,MAXVALUE), PARTITION p4 VALUES LESS THAN (MAXVALUE,MAXVALUE) ); Each value used in a value list must match the type of the corresponding column exactly; no conversion is made. For example, you cannot use the string for a value that matches a column that uses an integer type (you must use the numeral instead), nor can you use the numeral for a value that matches a column that uses a string type (in such a case, you must use a quoted string: ). For more information, see Section 26.2.1, “RANGE Partitioning”, and Section 26.4, “Partition Pruning”.\n• None This is useful when assigning partitions based on a table column with a restricted set of possible values, such as a state or country code. In such a case, all rows pertaining to a certain state or country can be assigned to a single partition, or a partition can be reserved for a certain set of states or countries. It is similar to , except that only may be used to specify permissible values for each partition. is used with a list of values to be matched. For instance, you could create a partitioning scheme such as the following: CREATE TABLE client_firms ( id INT, name VARCHAR(35) ) PARTITION BY LIST (id) ( PARTITION r0 VALUES IN (1, 5, 9, 13, 17, 21), PARTITION r1 VALUES IN (2, 6, 10, 14, 18, 22), PARTITION r2 VALUES IN (3, 7, 11, 15, 19, 23), PARTITION r3 VALUES IN (4, 8, 12, 16, 20, 24) ); When using list partitioning, you must define at least one partition using . You cannot use with . For tables partitioned by , the value list used with must consist of integer values only. In MySQL 8.4, you can overcome this limitation using partitioning by , which is described later in this section.\n• None This variant on facilitates partition pruning for queries using comparison conditions on multiple columns (that is, having conditions such as or WHERE a = 1 AND b = 10 AND c = 5 ). It enables you to specify values in multiple columns by using a list of columns in the clause and a set of column values in each partition definition clause. The rules governing regarding data types for the column list used in and the value list used in are the same as those for the column list used in and the value list used in , respectively, except that in the clause, is not permitted, and you may use . There is one important difference between the list of values used for with as opposed to when it is used with . When used with , each element in the clause must be a set of column values; the number of values in each set must be the same as the number of columns used in the clause, and the data types of these values must match those of the columns (and occur in the same order). In the simplest case, the set consists of a single column. The maximum number of columns that can be used in the and in the elements making up the is 16. The table defined by the following statement provides an example of a table using partitioning: CREATE TABLE lc ( a INT NULL, b INT NULL ) PARTITION BY LIST COLUMNS(a,b) ( PARTITION p0 VALUES IN( (0,0), (NULL,NULL) ), PARTITION p1 VALUES IN( (0,1), (0,2), (0,3), (1,1), (1,2) ), PARTITION p2 VALUES IN( (1,0), (2,0), (2,1), (3,0), (3,1) ), PARTITION p3 VALUES IN( (1,3), (2,2), (2,3), (3,2), (3,3) ) );\n• None The number of partitions may optionally be specified with a clause, where is the number of partitions. If both this clause and any clauses are used, must be equal to the total number of any partitions that are declared using clauses. Whether or not you use a clause in creating a table that is partitioned by or , you must still include at least one clause in the table definition (see below).\n• None A partition may optionally be divided into a number of subpartitions. This can be indicated by using the optional clause. Subpartitioning may be done by or . Either of these may be . These work in the same way as previously described for the equivalent partitioning types. (It is not possible to subpartition by or .) The number of subpartitions can be indicated using the keyword followed by an integer value.\n• None Rigorous checking of the value used in or clauses is applied and this value must adhere to the following rules:\n• None The value must be a positive, nonzero integer.\n• None The value must be an integer literal, and cannot not be an expression. For example, is not permitted, even though evaluates to . (Bug #15890)\n• None Each partition may be individually defined using a clause. The individual parts making up this clause are as follows:\n• None Specifies a logical name for the partition.\n• None For range partitioning, each partition must include a clause; for list partitioning, you must specify a clause for each partition. This is used to determine which rows are to be stored in this partition. See the discussions of partitioning types in Chapter 26, Partitioning, for syntax examples.\n• None MySQL accepts a option for both and . Currently, the only way in which this option can be used is to set all partitions or all subpartitions to the same storage engine, and an attempt to set different storage engines for partitions or subpartitions in the same table raises the error ERROR 1469 (HY000): The mix of handlers in the partitions is not permitted in this version of MySQL.\n• None An optional clause may be used to specify a string that describes the partition. Example: COMMENT = 'Data for the years previous to 1999' The maximum length for a partition comment is 1024 characters.\n• None and may be used to indicate the directory where, respectively, the data and indexes for this partition are to be stored. Both the and the must be absolute system path names. The directory specified in a clause must be known to . For more information, see Using the DATA DIRECTORY Clause. You must have the privilege to use the or partition option. CREATE TABLE th (id INT, name VARCHAR(30), adate DATE) PARTITION BY LIST(YEAR(adate)) ( PARTITION p1999 VALUES IN (1995, 1999, 2003) DATA DIRECTORY = '/var/appdata/95/data' INDEX DIRECTORY = '/var/appdata/95/idx', PARTITION p2000 VALUES IN (1996, 2000, 2004) DATA DIRECTORY = '/var/appdata/96/data' INDEX DIRECTORY = '/var/appdata/96/idx', PARTITION p2001 VALUES IN (1997, 2001, 2005) DATA DIRECTORY = '/var/appdata/97/data' INDEX DIRECTORY = '/var/appdata/97/idx', PARTITION p2002 VALUES IN (1998, 2002, 2006) DATA DIRECTORY = '/var/appdata/98/data' INDEX DIRECTORY = '/var/appdata/98/idx' ); and behave in the same way as in the statement's clause as used for tables. One data directory and one index directory may be specified per partition. If left unspecified, the data and indexes are stored by default in the table's database directory. The and options are ignored for creating partitioned tables if is in effect.\n• None May be used to specify, respectively, the maximum and minimum number of rows to be stored in the partition. The values for and must be positive integers. As with the table-level options with the same names, these act only as “suggestions” to the server and are not hard limits.\n• None May be used to designate an file-per-table tablespace for the partition by specifying . All partitions must belong to the same storage engine. Placing table partitions in shared tablespaces is not supported. Shared tablespaces include the system tablespace and general tablespaces.\n• None The partition definition may optionally contain one or more clauses. Each of these consists at a minimum of the , where is an identifier for the subpartition. Except for the replacement of the keyword with , the syntax for a subpartition definition is identical to that for a partition definition. Subpartitioning must be done by or , and can be done only on or partitions. See Section 26.2.6, “Subpartitioning”.\n\nPartitioning by generated columns is permitted. For example:\n\nPartitioning sees a generated column as a regular column, which enables workarounds for limitations on functions that are not permitted for partitioning (see Section 26.6.3, “Partitioning Limitations Relating to Functions”). The preceding example demonstrates this technique: cannot be used directly in the clause, but a generated column defined using is permitted."
    }
]