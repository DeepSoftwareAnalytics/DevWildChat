[
    {
        "link": "https://pkg.go.dev/net/http",
        "document": "Get, Head, Post, and PostForm make HTTP (or HTTPS) requests: The caller must close the response body when finished with it: For control over HTTP client headers, redirect policy, and other settings, create a Client: For control over proxies, TLS configuration, keep-alives, compression, and other settings, create a Transport: Clients and Transports are safe for concurrent use by multiple goroutines and for efficiency should only be created once and re-used. ListenAndServe starts an HTTP server with a given address and handler. The handler is usually nil, which means to use DefaultServeMux. Handle and HandleFunc add handlers to DefaultServeMux: More control over the server's behavior is available by creating a custom Server: Starting with Go 1.6, the http package has transparent support for the HTTP/2 protocol when using HTTPS. Programs that must disable HTTP/2 can do so by setting [Transport.TLSNextProto] (for clients) or [Server.TLSNextProto] (for servers) to a non-nil, empty map. Alternatively, the following GODEBUG settings are currently supported: GODEBUG=http2client=0 # disable HTTP/2 client support GODEBUG=http2server=0 # disable HTTP/2 server support GODEBUG=http2debug=1 # enable verbose HTTP/2 debug logs GODEBUG=http2debug=2 # ... even more verbose, with frame dumps Please report any issues before disabling HTTP/2 support: https://golang.org/s/http2bug The http package's Transport and Server both automatically enable HTTP/2 support for simple configurations. To enable HTTP/2 for more complex configurations, to use lower-level HTTP/2 features, or to use a newer version of Go's http2 package, import \"golang.org/x/net/http2\" directly and use its ConfigureTransport and/or ConfigureServer functions. Manually configuring HTTP/2 via the golang.org/x/net/http2 package takes precedence over the net/http package's built-in HTTP/2 support.\n\nCanonicalHeaderKey returns the canonical format of the header key s. The canonicalization converts the first letter and any letter following a hyphen to upper case; the rest are converted to lowercase. For example, the canonical key for \"accept-encoding\" is \"Accept-Encoding\". If s contains a space or invalid header field bytes, it is returned without modifications. DetectContentType implements the algorithm described at https://mimesniff.spec.whatwg.org/ to determine the Content-Type of the given data. It considers at most the first 512 bytes of data. DetectContentType always returns a valid MIME type: if it cannot determine a more specific one, it returns \"application/octet-stream\". Error replies to the request with the specified error message and HTTP code. It does not otherwise end the request; the caller should ensure no further writes are done to w. The error message should be plain text. Error deletes the Content-Length header, sets Content-Type to “text/plain; charset=utf-8”, and sets X-Content-Type-Options to “nosniff”. This configures the header properly for the error message, in case the caller had set it up expecting a successful output. Handle registers the handler for the given pattern in DefaultServeMux. The documentation for ServeMux explains how patterns are matched. HandleFunc registers the handler function for the given pattern in DefaultServeMux. The documentation for ServeMux explains how patterns are matched. package main import ( \"io\" \"log\" \"net/http\" ) func main() { h1 := func(w http.ResponseWriter, _ *http.Request) { io.WriteString(w, \"Hello from a HandleFunc #1!\n\n\") } h2 := func(w http.ResponseWriter, _ *http.Request) { io.WriteString(w, \"Hello from a HandleFunc #2!\n\n\") } http.HandleFunc(\"/\", h1) http.HandleFunc(\"/endpoint\", h2) log.Fatal(http.ListenAndServe(\":8080\", nil)) } ListenAndServe listens on the TCP network address addr and then calls Serve with handler to handle requests on incoming connections. Accepted connections are configured to enable TCP keep-alives. The handler is typically nil, in which case DefaultServeMux is used. package main import ( \"io\" \"log\" \"net/http\" ) func main() { // Hello world, the web server helloHandler := func(w http.ResponseWriter, req *http.Request) { io.WriteString(w, \"Hello, world!\n\n\") } http.HandleFunc(\"/hello\", helloHandler) log.Fatal(http.ListenAndServe(\":8080\", nil)) } ListenAndServeTLS acts identically to ListenAndServe, except that it expects HTTPS connections. Additionally, files containing a certificate and matching private key for the server must be provided. If the certificate is signed by a certificate authority, the certFile should be the concatenation of the server's certificate, any intermediates, and the CA's certificate. package main import ( \"io\" \"log\" \"net/http\" ) func main() { http.HandleFunc(\"/\", func(w http.ResponseWriter, req *http.Request) { io.WriteString(w, \"Hello, TLS!\n\n\") }) // One can use generate_cert.go in crypto/tls to generate cert.pem and key.pem. log.Printf(\"About to listen on 8443. Go to https://127.0.0.1:8443/\") err := http.ListenAndServeTLS(\":8443\", \"cert.pem\", \"key.pem\", nil) log.Fatal(err) } MaxBytesReader is similar to io.LimitReader but is intended for limiting the size of incoming request bodies. In contrast to io.LimitReader, MaxBytesReader's result is a ReadCloser, returns a non-nil error of type *MaxBytesError for a Read beyond the limit, and closes the underlying reader when its Close method is called. MaxBytesReader prevents clients from accidentally or maliciously sending a large request and wasting server resources. If possible, it tells the ResponseWriter to close the connection after the limit has been reached. NotFound replies to the request with an HTTP 404 not found error. ParseHTTPVersion parses an HTTP version string according to RFC 7230, section 2.6. \"HTTP/1.0\" returns (1, 0, true). Note that strings without a minor version, such as \"HTTP/2\", are not valid. ParseTime parses a time header (such as the Date: header), trying each of the three formats allowed by HTTP/1.1: TimeFormat, time.RFC850, and time.ANSIC. ProxyFromEnvironment returns the URL of the proxy to use for a given request, as indicated by the environment variables HTTP_PROXY, HTTPS_PROXY and NO_PROXY (or the lowercase versions thereof). Requests use the proxy from the environment variable matching their scheme, unless excluded by NO_PROXY. The environment values may be either a complete URL or a \"host[:port]\", in which case the \"http\" scheme is assumed. An error is returned if the value is a different form. A nil URL and nil error are returned if no proxy is defined in the environment, or a proxy should not be used for the given request, as defined by NO_PROXY. As a special case, if req.URL.Host is \"localhost\" (with or without a port number), then a nil URL and nil error will be returned. ProxyURL returns a proxy function (for use in a Transport) that always returns the same URL. Redirect replies to the request with a redirect to url, which may be a path relative to the request path. The provided code should be in the 3xx range and is usually StatusMovedPermanently, StatusFound or StatusSeeOther. If the Content-Type header has not been set, Redirect sets it to \"text/html; charset=utf-8\" and writes a small HTML body. Setting the Content-Type header to any value, including nil, disables that behavior. Serve accepts incoming HTTP connections on the listener l, creating a new service goroutine for each. The service goroutines read requests and then call handler to reply to them. The handler is typically nil, in which case DefaultServeMux is used. HTTP/2 support is only enabled if the Listener returns *tls.Conn connections and they were configured with \"h2\" in the TLS Config.NextProtos. ServeContent replies to the request using the content in the provided ReadSeeker. The main benefit of ServeContent over io.Copy is that it handles Range requests properly, sets the MIME type, and handles If-Match, If-Unmodified-Since, If-None-Match, If-Modified-Since, and If-Range requests. If the response's Content-Type header is not set, ServeContent first tries to deduce the type from name's file extension and, if that fails, falls back to reading the first block of the content and passing it to DetectContentType. The name is otherwise unused; in particular it can be empty and is never sent in the response. If modtime is not the zero time or Unix epoch, ServeContent includes it in a Last-Modified header in the response. If the request includes an If-Modified-Since header, ServeContent uses modtime to decide whether the content needs to be sent at all. The content's Seek method must work: ServeContent uses a seek to the end of the content to determine its size. Note that *os.File implements the io.ReadSeeker interface. If the caller has set w's ETag header formatted per RFC 7232, section 2.3, ServeContent uses it to handle requests using If-Match, If-None-Match, or If-Range. If an error occurs when serving the request (for example, when handling an invalid range request), ServeContent responds with an error message. By default, ServeContent strips the Cache-Control, Content-Encoding, ETag, and Last-Modified headers from error responses. The GODEBUG setting httpservecontentkeepheaders=1 causes ServeContent to preserve these headers. ServeFile replies to the request with the contents of the named file or directory. If the provided file or directory name is a relative path, it is interpreted relative to the current directory and may ascend to parent directories. If the provided name is constructed from user input, it should be sanitized before calling ServeFile. As a precaution, ServeFile will reject requests where r.URL.Path contains a \"..\" path element; this protects against callers who might unsafely use filepath.Join on r.URL.Path without sanitizing it and then use that filepath.Join result as the name argument. As another special case, ServeFile redirects any request where r.URL.Path ends in \"/index.html\" to the same path, without the final \"index.html\". To avoid such redirects either modify the path or use ServeContent. Outside of those two special cases, ServeFile does not use r.URL.Path for selecting the file or directory to serve; only the file or directory provided in the name argument is used. ServeFileFS replies to the request with the contents of the named file or directory from the file system fsys. The files provided by fsys must implement io.Seeker. If the provided name is constructed from user input, it should be sanitized before calling ServeFileFS. As a precaution, ServeFileFS will reject requests where r.URL.Path contains a \"..\" path element; this protects against callers who might unsafely use filepath.Join on r.URL.Path without sanitizing it and then use that filepath.Join result as the name argument. As another special case, ServeFileFS redirects any request where r.URL.Path ends in \"/index.html\" to the same path, without the final \"index.html\". To avoid such redirects either modify the path or use ServeContent. Outside of those two special cases, ServeFileFS does not use r.URL.Path for selecting the file or directory to serve; only the file or directory provided in the name argument is used. ServeTLS accepts incoming HTTPS connections on the listener l, creating a new service goroutine for each. The service goroutines read requests and then call handler to reply to them. The handler is typically nil, in which case DefaultServeMux is used. Additionally, files containing a certificate and matching private key for the server must be provided. If the certificate is signed by a certificate authority, the certFile should be the concatenation of the server's certificate, any intermediates, and the CA's certificate. SetCookie adds a Set-Cookie header to the provided ResponseWriter's headers. The provided cookie must have a valid Name. Invalid cookies may be silently dropped. StatusText returns a text for the HTTP status code. It returns the empty string if the code is unknown."
    },
    {
        "link": "https://tutorialedge.net/golang/go-file-upload-tutorial",
        "document": "Hi everybody! In this tutorial, we are going to be building a really simple file-upload HTTP server that allows you to upload your files to the server running your Go application.\n\nThere are countless different reasons why you would want to do this, you could be uploading CSV reports for further processing within your complex financial system, or you could be creating a cool image manipulation app that allows you to modify various aspects of any photos you want to upload.\n\nThankfully, the task of dealing with image uploading in Go is fairly simple and, coupled with the new API introduced in Go version 1.11, we can come up with a really elegant system fairly quickly!\n\nWe’ll start off by creating a really simple HTTP server using the package. This will feature just the one solitary endpoint which will be our endpoint.\n\nIf we want to run this, we can do so by running and if we haven’t made any mistakes, we should see our server starting up successfully.\n\nOk, so now that we have a base to build on top of, let’s set about implementing our upload endpoint to handle file uploads.\n\n// FormFile returns the first file for the given key `myFile` // it also returns the FileHeader so we can get the Filename, // the Header and the size of the file // Create a temporary file within our temp-images directory that follows // read all of the contents of our uploaded file into a // write this byte array to our temporary file // return that we have successfully uploaded our file!\n\nAwesome, we can try running this and seeing if everything else works by again calling within our terminal.\n\nWe’ll need a really simple HTML frontend that will act as our portal for uploading our files. We won’t bother with any of the more complex aspects such as authentication and user management, we’ll just create a really simple element that will allow us to select a file from our local machine and hit the API endpoint we have defined above!\n\nAwesome, we can now test that what we have done works and it successfully uploads our files!\n\nTry opening up this file within your browser and try uploading a file to our running web server.\n\nYou should see that a new file has been generated in the directory that follows the convention .\n\nHopefully you found this tutorial useful and entertaining! If you did, or you spotted anything wrong with the tutorial, then please feel free to let me know through the suggestion section below!"
    },
    {
        "link": "https://medium.com/@edwardpie/processing-form-request-data-in-golang-2dff4c2441be",
        "document": "A request in HTTP is more than just an instruction to the server to respond with information for the resource uniquely identified by the request’s url. Aside the url, a request object has headers and optionally, a body among many other things.\n\nThe headers are key-value pairs of information exchanged between the client and the server in either directions of the HTTP request-response cycles. Both the client and the server are able to send meta-information packaged as key-value pair entries into headers. One of the most common headers in HTTP is the “Content-Type” header which indicates to the receiving end (can be either client or server) how to process or parse the request/response body. Enough has been said about headers but hey, that’s not our focus of today. Let’s talk about request body.\n\nAnd, if you love my posts on Golang, then do well to subscribe to my Go Rapidly course on Youtube to get notified of the latest videos I post every other day.\n\nAs indicated in the text above, HTTP requests can optionally send data to the server in many forms. JSON, Url Form Encoding, Raw Stream Of Bytes are the common encoding formats used to send request body data to backend servers. See my story on handling json request body in golang if that’s what you’re interested in. HTTP clients and servers largely depend on the “Content-Type” header to determine how to parse request/response body. Common values for the “Content-Type” header are application/json, application/atom+xml, application/pdf, application/octet-stream, application/x-www-form-urlencoded for stating that request/response body should be processed as JSON, XML, PDF, Raw Stream, or Form Data respectively. Modern RESTFul APIs favor JSON over XML encoding for both request body and response body because it is computationally easier to parse, and aesthetically easier for humans to read and understand JSON. However, in traditional monolithic web applications where data exchange between client and server isn’t done through APIs, HTML form is the goto technique for information exchange. To this effect, HTML forms are very important in modern web development and being able to parse request body encoded as application/x-www-form-urlencoded is a fundamental feature required of any self-respected web framework. One of the reasons why I love Golang is the fact that I can create complete web applications without relying on third-party libraries or frameworks. Frameworks are built to improve developer productivity by abstracting away mundane and repetitive tasks so that developers can focus on implementing core business solutions. As much as frameworks can boost your productivity, they can also get in your way so choose them wisely. Thank goodness, frameworks aren’t needed much in Golang. Have I talked too much? May be yes! but no worries, we’ll code soon. Stay with me!\n\nYes, you read this paragraph’s title right! Request data can be sent either through the url (usually known as query parameters) or the request body. We will discuss how to choose between the two before we end but believe me, 99.99% of forms in your web applications will be url-encoded and sent in the request body instead of the request url for obvious reasons. Data sent in the url is seen by users so sensitive information such as user account credentials, access tokens etc should not be sent in this manner. When in doubt, I suggest you choose url-encoded body.\n\nHave you been waiting for this section? I bet you have! Ok, let’s get awesome!\n\nParsing both url and body request data in Golang is very easy but also very tricky if things aren’t done in the right order. Lemme give you the secret recipe here:\n• Always set content type to application/x-www-form-urlencoded if-and-only-if you want to process request body as form-encoded data. This is very important. Adhere to it, thank me later. However, note that once your HTML form’s method is set to “POST” or “PUT”, this will be done for you automatically.\n• Always invoke ParseForm or ParseMultipartForm on the request object in your handler before attempting to read url or body data from the request object. Note : call ParseMultipartForm if your form supports file upload else, ParseForm will do just fine.\n\nAccording to the golang documentation on the net/http package, the following are true :\n\nFor all requests, ParseForm parses the raw query from the URL and updates r.Form. For POST, PUT, and PATCH requests, it also parses the request body as a form and puts the results into both r.PostForm and r.Form. Request body parameters take precedence over URL query string values in r.Form. For other HTTP methods, or when the Content-Type is not application/x-www-form-urlencoded, the request Body is not read, and r.PostForm is initialized to a non-nil, empty value. If the request Body’s size has not already been limited by MaxBytesReader, the size is capped at 10MB.\n\nWhen in doubt, always consult golang’s splendid documentation. This is also a reason why I’ll choose golang anytime.\n\nTalk Is Cheap, Show Me The Code\n\nFinally! Finally!! Finally!!! We’re here to get our hands dirty with code. In this sections, I will show you how to process request data sent in the url and also the body."
    },
    {
        "link": "https://stackoverflow.com/questions/40684307/how-can-i-receive-an-uploaded-file-using-a-golang-net-http-server",
        "document": "I'm playing around with Mux and . Lately, I'm trying to get a simple server with one endpoint to accept a file upload.\n\nHere's the code I've got so far:\n\nI think I've narrowed the issue down to actually retrieving the body from the request inside . When I run this cURL command:\n\nI get an empty response (as expected; I'm not printing to the ), but I just get a new (empty) line printed at the prompt where I'm running the server, instead of the request body.\n\nI'm sending the file as multipart (AFAIK, implied by using rather than in cURL), and cURL's verbose output is showing 502 bytes sent:\n\nWhat's the proper way to receive files uploaded as multipart form data using a server in Go?"
    },
    {
        "link": "https://blog.logrocket.com/making-http-requests-in-go",
        "document": "Editor’s note: This article was last updated by Nwani Victory on 15 July 2024 to include information on managing HTTP headers and cookies, handling concurrency with HTTP requests, and using third-party libraries for HTTP requests.\n\nHTTP, or hypertext transfer protocol, is a communication protocol that ensures the transfer of data between a client and a server. A perfect instance of an HTTP client-server interaction is when you open your browser and type in a URL. Your browser acts as a client and fetches resources from a server, which it then displays.\n\nIn web development, cases where we need to fetch resources are very common. You might be making a weather application and need to fetch the weather data from an API. In such a case, using your browser as a client would no longer be possible from within your application. So you have to set up an HTTP client within your application to handle the making of these requests.\n\nMost programming languages have various structures in place for setting up HTTP clients to make requests. In the following sections, we will take a hands-on approach to exploring how you can make HTTP requests in Golang, or Go, as I will refer to the language for the rest of the article.\n\nTo follow this article you will need:\n\nThe first request we will be making is a GET request. The HTTP GET method is used to request data from a specified source or server. The GET method is mostly used when data needs to be fetched.\n\nFor the sake of clarity, it is important to note that the HTTP methods, as seen in this article, are always capitalized.\n\nFor our example, we will be fetching some example JSON data from https://jsonplaceholder.typicode.com/posts using the GET method.\n\nThe first step in making an HTTP request with Go is to import the package from the standard library. This package provides us with all the utilities we need to make HTTP requests with ease. We can import the package and other packages we will need by adding the following lines of code to a file that we create:\n\nThe package we imported has a function used for making GET requests. The function takes in a URL and returns a response of type pointer to a struct and an error. When the error is , the response returned will contain a response body and vice versa:\n\nTo make the request, we invoke the function, passing in a URL string (https://jsonplaceholder.typicode.com/posts) as seen above. The values returned from the invocation of this function are stored in two variables typically called and .\n\nAlthough the variable contains our response, if we print it out we would get a load of incoherent data, which includes the header and properties of the request made. To get the response we are interested in, we have to access the property on the response struct and read it before finally printing it out to the terminal. We can read the response body using the function.\n\nSimilar to the function, the function returns a body and an error. It is important to note that the response should be closed after we are done reading from it to prevent memory leaks.\n\nThe defer keyword, which executes at the end of the function, is used to close the response body. We can then print out the value of the response to the terminal. As good programmers, it is important to handle possible errors, so we use an if statement to check for any errors and log the error if it exists:\n\nAt this point, we are all set and can execute the file containing our code. If everything went well, you will notice that some JSON data similar to the image below gets printed to the terminal:\n\nCongratulations, you have just made your first HTTP request with Go. Now that we have seen how we can fetch resources from a server using the HTTP GET method, we will look at how to post resources to a server next.\n\nThe HTTP POST method is used to make requests that usually contain a body. It is used to send data to a server; the data sent is usually used to create or update resources.\n\nA clear instance where a POST request is used is when a user tries to create a social media account and they are required to provide their data (name, email, and password). This data is then parsed and sent as a POST request to a server, which then creates and saves the user. Just like with the GET method seen above, Go’s package also provides functionality for making POST requests through the function. The function takes three parameters:\n• The URL address of the server\n• The content type of the body as a string\n• The request body that is to be sent using the POST method of type\n\nThe function returns a response and an error. For us to invoke the function, we have to convert our request body to the accepted type. For this example, we will make a post request to https://postman-echo.com/post and pass in JSON data containing a name and an email.\n\nTo get started, we convert our JSON data to a type that implements the interface the function expects. This is a two-part step:\n• The first step is to encode our JSON data so it can return data in byte format. To do this, we use the Marshal function that Go’s JSON package provides\n• Next, we convert the encoded JSON data to a type implemented by the interface. We simply use the function for this, passing in the encoded JSON data as an argument. The function returns a value of type buffer, which we can then pass onto the function:\n\nNow that we have all the arguments the function requires, we can invoke it, passing in https://postman-echo.com/post as the URL string, application/JSON as the content type, and the request body returned by the function as the body.\n\nThe values returned by the function are then assigned to and representing the response and error, respectively. After handling the error, we read and print in the response body as we did for the function in the previous section. At this point, your file should look like this:\n\nWhen the file is executed, if everything works well, we should have the response printed out. Amazing, right? We just made a post request with Go using the package, which provides functionality that makes HTTP requests easier. In the next section, we will work on a project, to help us see HTTP requests being used in a real-life scenario.\n\nHeaders are crucial in network requests, containing additional data about the resource being fetched and the request’s originator. For most API services, the request rate limit and authentication details are specified in the request headers.\n\nGo’s HTTP client provides developers with the option to add headers to their network requests to pass additional information such as authentication tokens or keys, TTL values, and location.\n\nTo specify a request header, you will use the function from the package. The function allows you to modify a request’s cookies and headers, and specify an optional request body.\n\nThe following code block shows a modification of the initial POST request to specify a header with with the value . Notice that the function was replaced with the function:\n\nExecuting this request, you will find the API value in the JSON output as highlighted in the following image:\n\nIn this section, we will build a cryptocurrency price checker CLI tool! This exercise aims to show you a real-life use case for HTTP requests. The tool we are building will check the price of whatever cryptocurrency the user selects in the specified fiat currency.\n\nWe will use the crypto market cap and pricing data provided by Nomics to get the price of the cryptocurrencies in real time! To get started, create the needed files and folders to match the tree structure below:\n• The crypto-client file will house the code that fetches the cryptocurrency data from the API\n• The crypto-model file houses a couple of utility functions necessary for our application\n• The main file is the central engine of the application — it will merge all the parts of the application to make it functional\n\nIn the crypto-model file, we create a struct that models the data received from the API. This struct includes only the specific data we need/intend to work with. Next, we create a function called , which is a receiver that belongs to the struct we created up above.\n\nThe purpose of the function is to format the data received from the API to plain text, which is easier to read than JSON (which we receive from the server). We use the function to format the data:\n\nNow that the file is ready, we can move on to the file, which is the most relevant to us. In the file, we create a function that takes in the name of the cryptocurrency and fiat currency as parameters.\n\nIn the function, we create a variable called . The variable is a concatenation of the URL string provided by the Nomics API and the various variables that will be passed into our application.\n\nRemember how our application takes in the name of the desired cryptocurrency and the preferred fiat currency? These are the variables that are then used to build our URL string. Our URL string will look like this:\n\nAfter setting up the URL, we can then use the function we saw above to make a request. The function returns the response and we handle the error elegantly. To get the data we want, in the format we want, we have to decode it! To do so, we use the function that takes in the response body and a decode function, which takes in a variable of type , which we created in the file.\n\nLastly, we invoke the function on the decoded data to enable us to get our result in plaintext:\n\nFrom what we have above, the application is coming together nicely. However, if you try to run the file above, you will encounter a couple of errors. This is because we are not invoking the function and so the value of the fiat and crypto parameters are not provided. We will put all the various parts of our application together in the file we created. Because our application is a command-line tool, users will have to pass in data through the terminal. We will handle that using Go’s flag package.\n\nIn the main function, we create two variables: and . These variables both invoke the function, passing in:\n• \n• The name of the commands as the first argument\n• The fallback values as the second\n• The information on how to use the command as the third argument\n\nNext, we invoke the function we defined in the file and pass in the and variables. We can then print the result of the call to :\n\nAt this point, we are good to go. If we run the command , we will get an output similar to the image below:\n\nThis shows our application is working fine, which is pretty awesome! We have an application that fetches data from a remote server using the HTTP protocol.\n\nFaster and more efficient task processing through concurrency is one of Go’s key features. Developers leverage Golang’s concurrent design for applications such as microservices, real-time monitors, and chat apps that make multiple requests simultaneously.\n\nGo offers various concurrency mechanisms for developers to use based on their needs. These include goroutines and s, channels, Mutexes, Worker Pools, and more. It is important to know these mechanisms in-depth as each has its own advantages and disadvantages.\n\nFor example, while goroutines are the simplest concurrent mechanism to use, they are ideal for quick scripts with minimal complexity and little need for synchronization. On the other hand, worker pools are efficient for large tasks with better ways to manage resources, but they are complex to implement.\n\nThe following code block demonstrates the launching of multiple requests with goroutines to make them run concurrently:\n\nThe code above iterates over an array containing numerical IDs of posts and launches the function as a Goroutine over each iteration using the keyword. The code has minimal error handling and uses a to wait for all the goroutines to finish before continuing the main function.\n\nIn addition to the package, there are other community-managed libraries within the Go ecosystem for making network requests. Third-party libraries such as Resty, Sling, and Gentleman add new features and improve the developer experience of writing code for network requests. A practical example from third-party libraries is the feature to automatically marshal and unmarshal the data within request bodies.\n\nLet’s look more closely at the Resty, Sling, and Gentleman libraries and check out code samples for how to make requests with them:\n\nThe Sling library simplifies HTTP requests in Go by using a syntax with a preference for function chaining and other features to avoid code duplication. Sling allows setting a base URL and header values without creating a factory function.\n\nWith four lines of code, the following block demonstrates a network request in Go. The code block makes a GET request to the endpoint of the Postman Echo service and checks for any error from the network request:\n\nResty is a feature-rich HTTP client library for making network requests in Go. Its supported features increase development speed as developers do not need to reimplement them. Some highly used features include support to marshal and unmarshal data in JSON and XML formats, retry mechanisms, request redirect policies, and quick mocks for unit testing.\n\nThe following code demonstrates a familiar request with the Resty library but this time with a retry logic in use. The retry count after a failed request is set to with an interval between each retry attempt:\n\nGentleman is another highly extensible but complex request library for Go. At the core, it is lightweight, with features for making basic HTTP requests. To use additional features such as auto retries, file upload, redirects, logging, and debugging, you need to install its supported plugins, introducing new overheads to your application.\n\nThe following code block shows a GET request being made to the endpoint of the Postman Echo service. The code also sets the request headers to contain an header with a demo bearer token:\n\nIn this article, we discussed how to make HTTP requests in Go, and we built a CLI tool for checking the prices of cryptocurrencies. I highly recommend checking out the source code and documentation of the package to explore the other amazing functionalities it provides."
    },
    {
        "link": "https://mongodb.com/docs/drivers/go/current/usage-examples",
        "document": "How to Use the Usage Examples\n\nA full Go program that you can run in your own environment\n\nUsage examples provide convenient starting points for popular MongoDB operations. Each example provides the following:\n\nHow to Use the Usage Examples\n\nThese examples use the sample datasets provided by Atlas. You can load them into your database on the free tier of MongoDB Atlas by following the Get Started with Atlas Guide or you can import the sample dataset into a local MongoDB instance .\n\nOnce you import the dataset, you can copy and paste a usage example into your development environment of choice. You can follow the Quick Start to learn more about getting started with the MongoDB Go Driver.\n\nFollow the \"Connect to your Cluster\" step to find the connection string to define your environment variable to run the usage examples. If your instance uses SCRAM authentication, you can replace with your username, with your password, and with the URL or IP address of your instance. To learn more about connecting to your MongoDB instance, see Connection Guide .\n\nTo connect the example to your MongoDB instance, you must define an environment variable by using your connection string. You can use GoDotEnv to define your environment variable. Add the following application configuration in your file at the root of your project, replacing the placeholders with the values for your deployment's connection string. To learn more, see the GoDotEnv documentation ."
    },
    {
        "link": "https://pkg.go.dev/gopkg.in/mgo.v2",
        "document": "For more details, see the documentation for the types and methods.\n\nNew sessions are typically created by calling session.Copy on the initial session obtained at dial time. These new sessions will share the same cluster information and connection pool, and may be easily handed into other methods and functions for organizing logic. Every session created must have its Close method called at the end of its life time, so its resources may be put back in the pool or collected, depending on the case.\n\nThis will establish one or more connections with the cluster of servers defined by the url parameter. From then on, the cluster may be queried with multiple consistency rules (see SetMode) and documents retrieved with statements such as:\n\nUsage of the driver revolves around the concept of sessions. To get started, obtain a session using the Dial function:\n\nTHIS DRIVER IS UNMAINTAINED! See here for details:\n\nSpecify the *log.Logger object where log messages should be sent to.\n\nEnable the delivery of debug messages to the logger. Only meaningful if a logger is also set.\n\nIsDup returns whether err informs of a duplicate key error because a primary key index or a secondary unique index already has an entry with the given value.\n\nBulk returns a value to prepare the execution of a bulk operation. Count returns the total number of documents in the collection. Create explicitly creates the c collection with details of info. MongoDB creates collections automatically on use, so this method is only necessary when creating collection with non-default characteristics, such as capped collections. DropCollection removes the entire collection including all of its documents. DropIndex drops the index with the provided key from the c collection. See EnsureIndex for details on the accepted key variants. DropIndexName removes the index with the provided index name. EnsureIndex ensures an index with the given key exists, creating it with the provided parameters if necessary. EnsureIndex does not modify a previously existent index with a matching key. The old index must be dropped first instead. Once EnsureIndex returns successfully, following requests for the same index will not contact the server unless Collection.DropIndex is used to drop the same index, or Session.ResetIndexCache is called. The Key value determines which fields compose the index. The index ordering will be ascending by default. To obtain an index with a descending order, the field name should be prefixed by a dash (e.g. []string{\"-time\"}). It can also be optionally prefixed by an index kind, as in \"$text:summary\" or \"$2d:-point\". The key string format is: If the Unique field is true, the index must necessarily contain only a single document per Key. With DropDups set to true, documents with the same key as a previously indexed one will be dropped rather than an error returned. If Background is true, other connections will be allowed to proceed using the collection without the index while it's being built. Note that the session executing EnsureIndex will be blocked for as long as it takes for the index to be built. If Sparse is true, only documents containing the provided Key fields will be included in the index. When using a sparse index for sorting, only indexed documents will be returned. If ExpireAfter is non-zero, the server will periodically scan the collection and remove documents containing an indexed time.Time field with a value older than ExpireAfter. See the documentation for details: Other kinds of indexes are also supported through that API. Here is an example: The example above requests the creation of a \"2d\" index for the \"loc\" field. The 2D index bounds may be changed using the Min and Max attributes of the Index value. The default bound setting of (-180, 180) is suitable for latitude/longitude pairs. The Bits parameter sets the precision of the 2D geohash values. If not provided, 26 bits are used, which is roughly equivalent to 1 foot of precision for the default (-180, 180) index bounds. EnsureIndexKey ensures an index with the given key exists, creating it if necessary. See the EnsureIndex method for more details. Find prepares a query using the provided document. The document may be a map or a struct value capable of being marshalled with bson. The map may be a generic one using interface{} for its key and/or values, such as bson.M, or it may be a properly typed map. Providing nil as the document is equivalent to providing an empty document such as bson.M{}. Further details of the query may be tweaked using the resulting Query value, and then executed to retrieve results using methods such as One, For, Iter, or Tail. In case the resulting document includes a field named $err or errmsg, which are standard ways for MongoDB to return query errors, the returned err will be set to a *QueryError value including the Err message and the Code. In those cases, the result argument is still unmarshalled into with the received document so that any other custom values may be obtained if desired. See the Find method for more details. Indexes returns a list of all indexes for the collection. For example, this snippet would drop all available indexes: indexes, err := collection.Indexes() if err != nil { return err } for _, index := range indexes { err = collection.DropIndex(index.Key...) if err != nil { return err } } See the EnsureIndex method for more details on indexes. Insert inserts one or more documents in the respective collection. In case the session is in safe mode (see the SetSafe method) and an error happens while inserting the provided documents, the returned error will be of type *LastError. NewIter returns a newly created iterator with the provided parameters. Using this method is not recommended unless the desired functionality is not yet exposed via a more convenient interface (Find, Pipe, etc). The optional session parameter associates the lifetime of the returned iterator to an arbitrary session. If nil, the iterator will be bound to c's session. Documents in firstBatch will be individually provided by the returned iterator before documents from cursorId are made available. If cursorId is zero, only the documents in firstBatch are provided. If err is not nil, the iterator's Err method will report it after exhausting documents in firstBatch. NewIter must be called right after the cursor id is obtained, and must not be called on a collection in Eventual mode, because the cursor id is associated with the specific server that returned it. The provided session parameter may be in any mode or state, though. Pipe prepares a pipeline to aggregate. The pipeline document must be a slice built in terms of the aggregation framework language. Remove finds a single document matching the provided selector document and removes it from the database. If the session is in safe mode (see SetSafe) a ErrNotFound error is returned if a document isn't found, or a value of type *LastError when some other error is detected. RemoveAll finds all documents matching the provided selector document and removes them from the database. In case the session is in safe mode (see the SetSafe method) and an error happens when attempting the change, the returned error will be of type *LastError. See the Remove method for more details. Repair returns an iterator that goes over all recovered documents in the collection, in a best-effort manner. This is most useful when there are damaged data files. Multiple copies of the same document may be returned by the iterator. Repair is supported in MongoDB 2.7.8 and later. Update finds a single document matching the provided selector document and modifies it according to the update document. If the session is in safe mode (see SetSafe) a ErrNotFound error is returned if a document isn't found, or a value of type *LastError when some other error is detected. UpdateAll finds all documents matching the provided selector document and modifies them according to the update document. If the session is in safe mode (see SetSafe) details of the executed operation are returned in info or an error of type *LastError when some problem is detected. It is not an error for the update to not be applied on any documents because the selector doesn't match. See the Update method for more details. Upsert finds a single document matching the provided selector document and modifies it according to the update document. If no document matching the selector is found, the update document is applied to the selector document and the result is inserted in the collection. If the session is in safe mode (see SetSafe) details of the executed operation are returned in info, or an error of type *LastError when some problem is detected. See the Upsert method for more details. With returns a copy of c that uses session s.\n\nCreate creates a new file with the provided name in the GridFS. If the file name already exists, a new version will be inserted with an up-to-date uploadDate that will cause it to be atomically visible to the Open and OpenId methods. If the file name is not important, an empty name may be provided and the file Id used instead. It's important to Close files whether they are being written to or read from, and to check the err result to ensure the operation completed successfully. The io.Writer interface is implemented by *GridFile and may be used to help on the file creation. For example: Find runs query on GridFS's files collection and returns the resulting Query. Open returns the most recently uploaded file with the provided name, for reading. If the file isn't found, err will be set to mgo.ErrNotFound. It's important to Close files whether they are being written to or read from, and to check the err result to ensure the operation completed successfully. The following example will print the first 8192 bytes from the file: The io.Reader interface is implemented by *GridFile and may be used to deal with it. As an example, the following snippet will dump the whole file into the standard output: OpenId returns the file with the provided id, for reading. If the file isn't found, err will be set to mgo.ErrNotFound. It's important to Close files whether they are being written to or read from, and to check the err result to ensure the operation completed successfully. The following example will print the first 8192 bytes from the file: The io.Reader interface is implemented by *GridFile and may be used to deal with it. As an example, the following snippet will dump the whole file into the standard output: OpenNext opens the next file from iter for reading, sets *file to it, and returns true on the success case. If no more documents are available on iter or an error occurred, *file is set to nil and the result is false. Errors will be available via iter.Err(). The iter parameter must be an iterator on the GridFS files collection. Using the GridFS.Find method is an easy way to obtain such an iterator, but any iterator on the collection will work. If the provided *file is non-nil, OpenNext will close it before attempting to iterate to the next element. This means that in a loop one only has to worry about closing files when breaking out of the loop early (break, return, or panic). Remove deletes all files with the provided name from the GridFS. RemoveId deletes the file with the provided id from the GridFS.\n\nAbort cancels an in-progress write, preventing the file from being automically created and ensuring previously written chunks are removed when the file is closed. It is a runtime error to call Abort when the file was not opened for writing. Close flushes any pending changes in case the file is being written to, waits for any background operations to finish, and closes the file. It's important to Close files whether they are being written to or read from, and to check the err result to ensure the operation completed successfully. ContentType returns the optional file content type. An empty string will be returned in case it is unset. GetMeta unmarshals the optional \"metadata\" field associated with the file into the result parameter. The meaning of keys under that field is user-defined. For example: Name returns the optional file name. An empty string will be returned in case it is unset. Read reads into b the next available data from the file and returns the number of bytes written and an error in case something wrong happened. At the end of the file, n will be zero and err will be set to io.EOF. The parameters and behavior of this function turn the file into an io.Reader. Seek sets the offset for the next Read or Write on file to offset, interpreted according to whence: 0 means relative to the origin of the file, 1 means relative to the current offset, and 2 means relative to the end. It returns the new offset and an error, if any. SetChunkSize sets size of saved chunks. Once the file is written to, it will be split in blocks of that size and each block saved into an independent chunk document. The default chunk size is 255kb. It is a runtime error to call this function once the file has started being written to. ContentType changes the optional file content type. An empty string may be used to unset it. It is a runtime error to call this function when the file is not open for writing. It is a runtime error to call this function once the file has started being written to, or when the file is not open for writing. SetMeta changes the optional \"metadata\" field associated with the file. The meaning of keys under that field is user-defined. For example: It is a runtime error to call this function when the file is not open for writing. SetName changes the optional file name. An empty string may be used to unset it. It is a runtime error to call this function when the file is not open for writing. It is a runtime error to call this function when the file is not open for writing. Write writes the provided data to the file and returns the number of bytes written and an error in case something wrong happened. The file will internally cache the data so that all but the last chunk sent to the database have the size defined by SetChunkSize. This also means that errors may be deferred until a future call to Write or Close. The parameters and behavior of this function turn the file into an io.Writer.\n\nApply runs the findAndModify MongoDB command, which allows updating, upserting or removing a document matching a query and atomically returning either the old version (the default) or the new version of the document (when ReturnNew is true). If no objects are found Apply returns ErrNotFound. The Sort and Select query methods affect the result of Apply. In case multiple documents match the query, Sort enables selecting which document to act upon by ordering it first. Select enables retrieving only a selection of fields of the new or old document. This simple example increments a counter and prints its new value: This method depends on MongoDB >= 2.0 to work properly. The default batch size is defined by the database itself. As of this writing, MongoDB will use an initial size of min(100 docs, 4MB) on the first batch, and 4MB on remaining ones. Comment adds a comment to the query to identify it in the database profiler output. Count returns the total number of documents in the result set. Distinct unmarshals into result the list of distinct values for the given key. Explain returns a number of details about how the MongoDB server would execute the requested query, such as the number of objects examined, the number of times the read lock was yielded to allow writes to go in, and so on. The For method is obsolete and will be removed in a future release. See Iter as an elegant replacement. Hint will include an explicit \"hint\" in the query to force the server to use a specified index, potentially improving performance in some situations. The provided parameters are the fields that compose the key of the index to be used. For details on how the indexKey may be built, see the EnsureIndex method. Iter executes the query and returns an iterator capable of going over all the results. Results will be returned in batches of configurable size (see the Batch method) and more documents will be requested when a configurable number of documents is iterated over (see the Prefetch method). Limit restricts the maximum number of documents retrieved to n, and also changes the batch size to the same value. Once n documents have been returned by Next, the following call will return ErrNotFound. LogReplay enables an option that optimizes queries that are typically made on the MongoDB oplog for replaying it. This is an internal implementation aspect and most likely uninteresting for other uses. It has seen at least one use case, though, so it's exposed via the API. MapReduce executes a map/reduce job for documents covered by the query. That kind of job is suitable for very flexible bulk aggregation of data performed at the server side via Javascript functions. Results from the job may be returned as a result of the query itself through the result parameter in case they'll certainly fit in memory and in a single document. If there's the possibility that the amount of data might be too large, results must be stored back in an alternative collection or even a separate database, by setting the Out field of the provided MapReduce job. In that case, provide nil as the result parameter. These are some of the ways to set Out: nil Inline results into the result parameter. bson.M{\"replace\": \"mycollection\"} The output will be inserted into a collection which replaces any existing collection with the same name. bson.M{\"merge\": \"mycollection\"} This option will merge new data into the old output collection. In other words, if the same key exists in both the result set and the old collection, the new key will overwrite the old one. bson.M{\"reduce\": \"mycollection\"} If documents exist for a given key in the result set and in the old collection, then a reduce operation (using the specified reduce function) will be performed on the two values and the result will be written to the output collection. If a finalize function was provided, this will be run after the reduce as well. bson.M{...., \"db\": \"mydb\"} Any of the above options can have the \"db\" key included for doing the respective action in a separate database. The following is a trivial example which will count the number of occurrences of a field named n on each document in a collection, and will return results inline: job := &mgo.MapReduce{ Map: \"function() { emit(this.n, 1) }\", Reduce: \"function(key, values) { return Array.sum(values) }\", } var result []struct { Id int \"_id\"; Value int } _, err := collection.Find(nil).MapReduce(job, &result) if err != nil { return err } for _, item := range result { fmt.Println(item.Value) } This function is compatible with MongoDB 1.7.4+. One executes the query and unmarshals the first obtained document into the result argument. The result must be a struct or map value capable of being unmarshalled into by gobson. This function blocks until either a result is available or an error happens. For example: In case the resulting document includes a field named $err or errmsg, which are standard ways for MongoDB to return query errors, the returned err will be set to a *QueryError value including the Err message and the Code. In those cases, the result argument is still unmarshalled into with the received document so that any other custom values may be obtained if desired. Prefetch sets the point at which the next batch of results will be requested. When there are p*batch_size remaining documents cached in an Iter, the next batch will be requested in background. For instance, when using this: and there are only 50 documents cached in the Iter to be processed, the next batch of 200 will be requested. It's possible to change this setting on a per-session basis as well, using the SetPrefetch method of Session. The default prefetch value is 0.25. Select enables selecting which fields should be retrieved for the results found. For example, the following query would only retrieve the name field: SetMaxScan constrains the query to stop after scanning the specified number of documents. This modifier is generally used to prevent potentially long running queries from disrupting performance by scanning through too much data. SetMaxTime constrains the query to stop after running for the specified time. When the time limit is reached MongoDB automatically cancels the query. This can be used to efficiently prevent and identify unexpectedly slow queries. A few important notes about the mechanism enforcing this limit:\n• None Requests can block behind locking operations on the server, and that blocking time is not accounted for. In other words, the timer starts ticking only after the actual start of the query when it initially acquires the appropriate lock;\n• None Operations are interrupted only at interrupt points where an operation can be safely aborted – the total execution time may exceed the specified value;\n• None The limit can be applied to both CRUD operations and commands, but not all commands are interruptible;\n• None While iterating over results, computing follow up batches is included in the total time and the iteration continues until the alloted time is over, but network roundtrips are not taken into account for the limit.\n• None This limit does not override the inactive cursor timeout for idle cursors (default is 10 min). This mechanism was introduced in MongoDB 2.6. Skip skips over the n initial documents from the query results. Note that this only makes sense with capped collections where documents are naturally ordered by insertion time, or with sorted results. Snapshot will force the performed query to make use of an available index on the _id field to prevent the same document from being returned more than once in a single iteration. This might happen without this setting in situations when the document changes in size and thus has to be moved while the iteration is running. Because snapshot mode traverses the _id index, it may not be used with sorting or explicit hints. It also cannot use any other index for the query. Even with snapshot mode, items inserted or deleted during the query may or may not be returned; that is, this mode is not a true point-in-time snapshot. The same effect of Snapshot may be obtained by using any unique index on field(s) that will not be modified (best to use Hint explicitly too). A non-unique index (such as creation time) may be made unique by appending _id to the index when creating it. Sort asks the database to order returned documents according to the provided field names. A field name may be prefixed by - (minus) for it to be sorted in reverse order. Tail returns a tailable iterator. Unlike a normal iterator, a tailable iterator may wait for new values to be inserted in the collection once the end of the current result set is reached, A tailable iterator may only be used with capped collections. The timeout parameter indicates how long Next will block waiting for a result before timing out. If set to -1, Next will not timeout, and will continue waiting for a result for as long as the cursor is valid and the session is not closed. If set to 0, Next times out as soon as it reaches the end of the result set. Otherwise, Next will wait for at least the given number of seconds for a new document to be available before timing out. On timeouts, Next will unblock and return false, and the Timeout method will return true if called. In these cases, Next may still be called again on the same iterator to check if a new value is available at the current cursor position, and again it will block according to the specified timeoutSecs. If the cursor becomes invalid, though, both Next and Timeout will return false and the query must be restarted. The following example demonstrates timeout handling and query restarting: iter := collection.Find(nil).Sort(\"$natural\").Tail(5 * time.Second) for { for iter.Next(&result) { fmt.Println(result.Id) lastId = result.Id } if iter.Err() != nil { return iter.Close() } if iter.Timeout() { continue } query := collection.Find(bson.M{\"_id\": bson.M{\"$gt\": lastId}}) iter = query.Sort(\"$natural\").Tail(5 * time.Second) } iter.Close()\n\nAll Session methods are concurrency-safe and may be called from multiple goroutines. In all session modes but Eventual, using the session from multiple goroutines will cause them to share the same underlying socket. See the documentation on Session.SetMode for more details. Dial establishes a new session to the cluster identified by the given seed server(s). The session will enable communication with all of the servers in the cluster, so the seed servers are used only to find out about the cluster topology. Dial will timeout after 10 seconds if a server isn't reached. The returned session will timeout operations after one minute by default if servers aren't available. To customize the timeout, see DialWithTimeout, SetSyncTimeout, and SetSocketTimeout. This method is generally called just once for a given cluster. Further sessions to the same cluster are then established using the New or Copy methods on the obtained session. This will make them share the underlying cluster, and manage the pool of connections appropriately. Once the session is not useful anymore, Close must be called to release the resources appropriately. The seed servers must be provided in the following format: For example, it may be as simple as: If the port number is not provided for a server, it defaults to 27017. The username and password provided in the URL will be used to authenticate into the database named after the slash at the end of the host names, or into the \"admin\" database if none is provided. The authentication information will persist in sessions obtained through the New method as well. The following connection options are supported after the question mark: connect=direct Disables the automatic replica set server discovery logic, and forces the use of servers provided only (even if secondaries). Note that to talk to a secondary the consistency requirements must be relaxed to Monotonic or Eventual via SetMode. connect=replicaSet Discover replica sets automatically. Default connection behavior. replicaSet=<setname> If specified will prevent the obtained session from communicating with any server which is not part of a replica set with the given name. The default is to communicate with any server specified or discovered via the servers contacted. authSource=<db> Informs the database used to establish credentials and privileges with a MongoDB server. Defaults to the database name provided via the URL path, and \"admin\" if that's unset. authMechanism=<mechanism> Defines the protocol for credential negotiation. Defaults to \"MONGODB-CR\", which is the default username/password challenge-response mechanism. gssapiServiceName=<name> Defines the service name to use when authenticating with the GSSAPI mechanism. Defaults to \"mongodb\". maxPoolSize=<limit> Defines the per-server socket pool limit. Defaults to 4096. See Session.SetPoolLimit for details. DialWithInfo establishes a new session to the cluster identified by info. DialWithTimeout works like Dial, but uses timeout as the amount of time to wait for a server to respond when first connecting and also on follow up operations in the session. If timeout is zero, the call may block forever waiting for a connection to be made. See SetSyncTimeout for customizing the timeout for the session. BuildInfo retrieves the version and other details about the running MongoDB server. Clone works just like Copy, but also reuses the same socket as the original session, in case it had already reserved one due to its consistency guarantees. This behavior ensures that writes performed in the old session are necessarily observed when using the new session, as long as it was a strong or monotonic session. That said, it also means that long operations may cause other goroutines using the original session to wait. Close terminates the session. It's a runtime error to use a session after it has been closed. Copy works just like New, but preserves the exact authentication information from the original session. DB returns a value representing the named database. If name is empty, the database name provided in the dialed URL is used instead. If that is also empty, \"test\" is used as a fallback in a way equivalent to the mongo shell. Creating this value is a very lightweight operation, and involves no network communication. DatabaseNames returns the names of non-empty databases present in the cluster. EnsureSafe compares the provided safety parameters with the ones currently in use by the session and picks the most conservative choice for each setting.\n• safe.WMode is always used if set.\n• safe.W is used if larger than the current W and WMode is empty.\n• safe.FSync is always used if true.\n• safe.J is used if FSync is false.\n• safe.WTimeout is used if set and smaller than the current WTimeout. For example, the following statement will ensure the session is at least checking for errors, without enforcing further constraints. If a more conservative SetSafe or EnsureSafe call was previously done, the following call will be ignored. See also the SetSafe method for details on what each option means. FindRef returns a query that looks for the document in the provided reference. For a DBRef to be resolved correctly at the session level it must necessarily have the optional DB field defined. See also the DBRef type and the FindRef method on Database. Fsync flushes in-memory writes to disk on the server the session is established with. If async is true, the call returns immediately, otherwise it returns after the flush has been made. FsyncLock locks all writes in the specific server the session is established with and returns. Any writes attempted to the server after it is successfully locked will block until FsyncUnlock is called for the same server. This method works on secondaries as well, preventing the oplog from being flushed while the server is locked, but since only the server connected to is locked, for locking specific secondaries it may be necessary to establish a connection directly to the secondary (see Dial's connect=direct option). As an important caveat, note that once a write is attempted and blocks, follow up reads will block as well due to the way the lock is internally implemented in the server. More details at: FsyncLock is often used for performing consistent backups of the database files on disk. FsyncUnlock releases the server for writes. See FsyncLock for details. LiveServers returns a list of server addresses which are currently known to be alive. Login authenticates with MongoDB using the provided credential. The authentication is valid for the whole session and will stay valid until Logout is explicitly called for the same database, or the session is closed. LogoutAll removes all established authentication credentials for the session. Mode returns the current consistency mode for the session. New creates a new session with the same parameters as the original session, including consistency, batch size, prefetching, safety mode, etc. The returned session will use sockets from the pool, so there's a chance that writes just performed in another session may not yet be visible. Login information from the original session will not be copied over into the new session unless it was provided through the initial URL for the Dial function. See the Copy and Clone methods. Ping runs a trivial ping command just to get in touch with the server. Refresh puts back any reserved sockets in use and restarts the consistency guarantees according to the current consistency setting for the session. ResetIndexCache() clears the cache of previously ensured indexes. Following requests to EnsureIndex will contact the server. Run issues the provided command on the \"admin\" database and and unmarshals its result in the respective argument. The cmd argument may be either a string with the command name itself, in which case an empty document of the form bson.M{cmd: 1} will be used, or it may be a full command document. Note that MongoDB considers the first marshalled key as the command name, so when providing a command with options, it's important to use an ordering-preserving document, such as a struct value or an instance of bson.D. For instance: For commands on arbitrary databases, see the Run method in the Database type. Safe returns the current safety mode for the session. SelectServers restricts communication to servers configured with the given tags. For example, the following statement restricts servers used for reading operations to those with both tag \"disk\" set to \"ssd\" and tag \"rack\" set to 1: Multiple sets of tags may be provided, in which case the used server must match all tags within any one set. If a connection was previously assigned to the session due to the current session mode (see Session.SetMode), the tag selection will only be enforced after the session is refreshed. SetBatch sets the default batch size used when fetching documents from the database. It's possible to change this setting on a per-query basis as well, using the Query.Batch method. The default batch size is defined by the database itself. As of this writing, MongoDB will use an initial size of min(100 docs, 4MB) on the first batch, and 4MB on remaining ones. SetBypassValidation sets whether the server should bypass the registered validation expressions executed when documents are inserted or modified, in the interest of preserving invariants in the collection being modified. The default is to not bypass, and thus to perform the validation expressions registered for modified collections. SetCursorTimeout changes the standard timeout period that the server enforces on created cursors. The only supported value right now is 0, which disables the timeout. The standard server timeout is 10 minutes. SetMode changes the consistency mode for the session. In the Strong consistency mode reads and writes will always be made to the primary server using a unique connection so that reads and writes are fully consistent, ordered, and observing the most up-to-date data. This offers the least benefits in terms of distributing load, but the most guarantees. See also Monotonic and Eventual. In the Monotonic consistency mode reads may not be entirely up-to-date, but they will always see the history of changes moving forward, the data read will be consistent across sequential queries in the same session, and modifications made within the session will be observed in following queries (read-your-writes). In practice, the Monotonic mode is obtained by performing initial reads on a unique connection to an arbitrary secondary, if one is available, and once the first write happens, the session connection is switched over to the primary server. This manages to distribute some of the reading load with secondaries, while maintaining some useful guarantees. In the Eventual consistency mode reads will be made to any secondary in the cluster, if one is available, and sequential reads will not necessarily be made with the same connection. This means that data may be observed out of order. Writes will of course be issued to the primary, but independent writes in the same Eventual session may also be made with independent connections, so there are also no guarantees in terms of write ordering (no read-your-writes guarantees either). The Eventual mode is the fastest and most resource-friendly, but is also the one offering the least guarantees about ordering of the data read and written. If refresh is true, in addition to ensuring the session is in the given consistency mode, the consistency guarantees will also be reset (e.g. a Monotonic session will be allowed to read from secondaries again). This is equivalent to calling the Refresh function. Shifting between Monotonic and Strong modes will keep a previously reserved connection for the session unless refresh is true or the connection is unsuitable (to a secondary server in a Strong session). SetPoolLimit sets the maximum number of sockets in use in a single server before this session will block waiting for a socket to be available. The default limit is 4096. This limit must be set to cover more than any expected workload of the application. It is a bad practice and an unsupported use case to use the database driver to define the concurrency limit of an application. Prevent such concurrency \"at the door\" instead, by properly restricting the amount of used resources and number of goroutines before they are created. SetPrefetch sets the default point at which the next batch of results will be requested. When there are p*batch_size remaining documents cached in an Iter, the next batch will be requested in background. For instance, when using this: and there are only 50 documents cached in the Iter to be processed, the next batch of 200 will be requested. It's possible to change this setting on a per-query basis as well, using the Prefetch method of Query. The default prefetch value is 0.25. If the safe parameter is nil, the session is put in unsafe mode, and writes become fire-and-forget, without error checking. The unsafe mode is faster since operations won't hold on waiting for a confirmation. If the safe parameter is not nil, any changing query (insert, update, ...) will be followed by a getLastError command with the specified parameters, to ensure the request was correctly processed. The default is &Safe{}, meaning check for errors and use the default behavior for all fields. The safe.W parameter determines how many servers should confirm a write before the operation is considered successful. If set to 0 or 1, the command will return as soon as the primary is done with the request. If safe.WTimeout is greater than zero, it determines how many milliseconds to wait for the safe.W servers to respond before returning an error. Starting with MongoDB 2.0.0 the safe.WMode parameter can be used instead of W to request for richer semantics. If set to \"majority\" the server will wait for a majority of members from the replica set to respond before returning. Custom modes may also be defined within the server to create very detailed placement schemas. See the data awareness documentation in the links below for more details (note that MongoDB internally reuses the \"w\" field name for WMode). If safe.J is true, servers will block until write operations have been committed to the journal. Cannot be used in combination with FSync. Prior to MongoDB 2.6 this option was ignored if the server was running without journaling. Starting with MongoDB 2.6 write operations will fail with an exception if this option is used when the server is running without journaling. If safe.FSync is true and the server is running without journaling, blocks until the server has synced all data files to disk. If the server is running with journaling, this acts the same as the J option, blocking until write operations have been committed to the journal. Cannot be used in combination with J. Since MongoDB 2.0.0, the safe.J option can also be used instead of FSync to force the server to wait for a group commit in case journaling is enabled. The option has no effect if the server has journaling disabled. For example, the following statement will make the session check for errors, without imposing further constraints: The following statement will force the server to wait for a majority of members of a replica set to return (MongoDB 2.0+ only): The following statement, on the other hand, ensures that at least two servers have flushed the change to disk before confirming the success of operations: The following statement, on the other hand, disables the verification of errors entirely: See also the EnsureSafe method. SetSocketTimeout sets the amount of time to wait for a non-responding socket to the database before it is forcefully closed. SetSyncTimeout sets the amount of time an operation with this session will wait before returning an error in case a connection to a usable server can't be established. Set it to zero to wait forever. The default value is 7 seconds."
    },
    {
        "link": "https://mongodb.com/docs/drivers/go/current/fundamentals/crud/write-operations/insert",
        "document": "In this guide, you can learn how to insert documents into a MongoDB collection. Before you can find, update, and delete documents in MongoDB, you must insert those documents. You can insert one document by using the method, or insert multiple documents by using either the or method. The following sections focus on and . To learn how to use the method, see the Bulk Operations guide.\n\nIn MongoDB, each document must contain a unique field. The two options for managing this field are:\n• None Managing this field yourself, ensuring that each value you use is unique.\n• None Letting the driver automatically generate unique values. The driver generates unique values for documents that you do not explicitly specify an . Unless you provide strong guarantees for uniqueness, MongoDB recommends you let the driver automatically generate values. Duplicate values violate unique index constraints, which causes the driver to return a . To learn more about the field, see the Server Manual Entry on Unique Indexes . To learn more about document structure and rules, see the Server Manual Entry on Documents .\n\nUse the method to insert a single document into a collection. Upon successful insertion, the method returns an instance that contains the of the new document. This example uses the following struct as a model for documents in the collection: The following example creates and inserts a document into the collection using the method: You can modify the behavior of by constructing and passing an optional struct. The available options to set with are: , allows the write to opt-out of If, allows the write to opt-out of Construct an as follows:\n\nUse the method to insert multiple documents into a collection. Upon successful insertion, the method returns an instance that contains the fields of the inserted documents. The following example creates and inserts multiple documents into the collection using the method: After running the preceding code, your output resembles the following: You can modify the behavior of by constructing and passing an optional struct. The available options to set with are: , allows the write to opt-out of If, allows the write to opt-out of , the driver sends documents to the server in the order provided. If an error occurs, the driver and server end all remaining insert operations. To learn more, see If, the driver sends documents to the server in the order provided. If an error occurs, the driver and server end all remaining insert operations. To learn more, see Behavior Construct an as follows: Assume you want to insert the following documents: \"Where the Wild Things Are\" If you attempt to insert these documents with default , a occurs at the third document because of the repeated value, but the documents before the error-producing document still get inserted into your collection. You can get an acknowledgement of successful document insertion even if a BulkWriteException occurs: Book{ID: , Title: \"Where the Wild Things Are\" }, fmt.Printf( \"A bulk write error occurred, but %v documents were still inserted. , (result.InsertedIDs)) A bulk write error occurred, but 2 documents were still inserted. After running the preceding code, your collection contains the following documents: \"Where the Wild Things Are\""
    },
    {
        "link": "https://stackoverflow.com/questions/54876209/find-all-documents-in-a-collection-with-mongo-go-driver",
        "document": "I checked out the answer here but this uses the old and unmaintained mgo. How can I find all documents in a collection using the mongo-go-driver?\n\nI tried passing a filter, but this does not return any documents and instead returns . I also checked the documentation but did not see any mention of returning all documents. Here is what I've tried with aforementioned result.\n\nAlso, I'm about confused about why I need to specify when I've called the function on the collection (or do I not need to specify?)."
    },
    {
        "link": "https://docs.objectrocket.com/mongodb_go_examples.html",
        "document": "The Go MongoDB driver isn’t an officially supported driver at the moment, and as such is maintained by the community. It’s called mgo.\n\nmgo at the time of this writing, , supports the following versions of MongoDB:\n\nHere are the current versions of the driver supports:\n\nWhen connecting using the MongoDB URI, we highly recommend avoiding usernames or passwords with an @ symbol inside. This can break the URI parsing and cause failures when trying to connect. The below examples are connecting via SSL, which doesn’t work with our Replica Set instances. Please adjust accordingly.\n\nIf you need more help with , here are some links to more documentation: As always, if you have any questions, please don’t hesitate to reach out to our support team!"
    }
]