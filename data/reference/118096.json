[
    {
        "link": "https://geeksforgeeks.org/python-sys-setrecursionlimit-method",
        "document": "This sys module provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It provides information about constants, functions and methods of python interpreter. It can be used for manipulating Python runtime environment.\n\nmethod is used to set the maximum depth of the Python interpreter stack to the required limit. This limit prevents any program from getting into infinite recursion, Otherwise infinite recursion will lead to overflow of the C stack and crash the Python.\n\nNote: The highest possible limit is platform-dependent. This should be done with care because too-high limit can lead to crash.\n\nParameter:\n\n limit: It the value of integer type that denotes the new limit of python interpreter stack. Return Value: This method returns nothing."
    },
    {
        "link": "https://docs.python.org/3/library/sys.html",
        "document": "This module provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available. Unless explicitly noted otherwise, all variables are read-only."
    },
    {
        "link": "https://note.nkmk.me/en/python-sys-recursionlimit",
        "document": "In Python, the and functions are used to get and set the recursion limit, which is the maximum recursion depth allowed by the interpreter.\n\nThe number of recursions is also limited by the stack size. In some environments, the maximum stack size can be modified using the module. In my case, this was successful on Ubuntu but not on Windows or macOS.\n\nThe sample code below was tested on Ubuntu.\n\nYou can get the current value of the recursion limit with .\n\nThe default limit is in this example, but it may vary depending on the environment.\n\nConsider a simple recursive function that takes a positive integer as its argument, where the number of recursions equals .\n\nIt is important to note that the value returned by represents the maximum depth of the Python interpreter stack rather than the absolute maximum number of recursions. As a result, a may be raised even if the actual number of recursions is slightly below this limit.\n\nYou can set the recursion limit with .\n\nSetting the limit too high or too low may cause errors.\n\nAdditionally, as explained next, the stack size also constrains the maximum recursive limit.\n\nEven after increasing the recursion limit, executing a large number of recursions can lead to a segmentation fault.\n\nIn Python, you can change the maximum stack size with the module. Note that the module is specific to Unix-based systems and is unavailable on Windows.\n\nYou can get the limits of the specified resource as with . , which represents the maximum size of the call stack of the current process, is specified in the following example.\n\nIn this example, the soft limit is (8388608 B = 8192 KB = 8 MB), and the hard limit is (unlimited).\n\nYou can change the limit of the resource with . The soft limit is set to (unlimited) in the following example. Alternatively, the constant can be used to represent an unlimited limit.\n\nSetting the soft limit to enables deeper recursion previously limited by segmentation faults.\n\nIn this example, the soft limit is set to for a simple experiment, but in practice, it is safer to set an appropriate limit.\n\nOn macOS, attempting to set an unlimited soft limit results in a . Running the script with did not help. It might be a system-imposed limitation."
    },
    {
        "link": "https://stackoverflow.com/questions/61026446/python-3-use-of-sys-setrecursionlimit",
        "document": "However, for d>3, I get the following error:\n\nSo, I have tried to use:\n\nand tried to run the following code (both from IDLE and command prompt):\n\nAt this point, no error arises, but output is blank. More precisely, running from IDLE I simply get:\n\nSo, it seems to me that the previous RecursionError has vanished, but, still, the code does not execute as I would expect, giving me essentially nothing as an output.\n\nWhat's wrong with this? Also increasing the recursion limit up to 10^9 I get nothing as well. Any suggestion on how to solve the problem?"
    },
    {
        "link": "https://geeksforgeeks.org/python-sys-module",
        "document": "The sys module in Python provides various functions and variables that are used to manipulate different parts of the Python runtime environment. It allows operating on the interpreter as it provides access to the variables and functions that interact strongly with the interpreter. Let’s consider the below example.\n\nIn this example, sys.version is used which returns a string containing the version of Python Interpreter with some additional information. This shows how the sys module interacts with the interpreter. Let us dive into the article to get more information about the sys module.\n\nThe sys modules provide variables for better control over input or output. We can even redirect the input and output to other devices. This can be done using three variables –\n\nstdin: It can be used to get input from the command line directly. It is used for standard input. It internally calls the input() method. It, also, automatically adds ‘\n\n’ after each sentence.\n\nThis code reads lines from the standard input until the user enters ‘q’. For each line, it prints “Input : ” followed by the line. Finally, it prints “Exit”.\n\nstdout: A built-in file object that is analogous to the interpreter’s standard output stream in Python. stdout is used to display output directly to the screen console. Output can be of any form, it can be output from a print statement, an expression statement, and even a prompt direct for input. By default, streams are in text mode. In fact, wherever a print function is called within the code, it is first written to sys.stdout and then finally on to the screen.\n\nThis code will print the string “Geeks” to the standard output. The object represents the standard output stream, and the method writes the specified string to the stream.\n\nstderr: Whenever an exception occurs in Python it is written to sys.stderr.\n\nThis code will print the string “Hello World” to the standard error stream. The object represents the standard error stream, and the function writes the specified strings to the stream.\n\nCommand-line arguments are those which are passed during the calling of the program along with the calling statement. To achieve this using the sys module, the sys module provides a variable called sys.argv. It’s main purpose are:\n• None It is a list of command-line arguments.\n• None len(sys.argv) provides the number of command-line arguments.\n• None sys.argv[0] is the name of the current Python script.\n\nExample: Consider a program for adding numbers and the numbers are passed along with the calling statement.\n\nThis code calculates the sum of the command-line arguments passed to the Python script. It imports the module to access the command-line arguments and then iterates over the arguments, converting each one to an integer and adding it to a running total. Finally, it prints the total sum of the arguments.\n\nsys.exit([arg]) can be used to exit the program. The optional argument arg can be an integer giving the exit or another type of object. If it is an integer, zero is considered “successful termination”.\n\nNote: A string can also be passed to the sys.exit() method.\n\nThis code checks if the age is less than 18. If it is, it exits the program with a message “Age less than 18”. Otherwise, it prints the message “Age is not less than 18”. The function takes an optional message as an argument, which is displayed when the program exits.\n\nsys.path is a built-in variable within the sys module that returns the list of directories that the interpreter will search for the required module.\n\nWhen a module is imported within a Python file, the interpreter first searches for the specified module among its built-in modules. If not found it looks through the list of directories defined by sys.path.\n\nNote: sys.path is an ordinary list and can be manipulated.\n\nExample 1: Listing out all the paths\n\nThis code will print the system paths that Python uses to search for modules. The list contains the directories that Python will search for modules when it imports them.\n\nExample 2: Truncating the value of sys.path\n\nThis code will print an error message because the module is not in the list. The list is a list of directories that Python will search for modules when it imports them. By setting the list to an empty list, the code effectively disables Python’s ability to find any modules.\n\nsys.modules return the name of the Python modules that the current shell has imported.\n\nThis code will print a dictionary of all the modules that have been imported by the current Python interpreter. The dictionary keys are the module names, and the dictionary values are the module objects.\n\nsys.getrefcount() method is used to get the reference count for any given object. This value is used by Python as when this value becomes 0, the memory for that particular value is deleted.\n\nThis code prints the reference count of the object . The reference count of an object is the number of times it is referenced by other objects. An object is garbage collected when its reference count reaches 0, meaning that it is no longer referenced by any other objects"
    },
    {
        "link": "https://reddit.com/r/Python/comments/sekrzq/how_to_optimize_python_code",
        "document": "I'm interested in learning what optimization techniques you know for python code. I know its a general statement, but I'm interested in really pushing execution to the maximum.\n\nI use the following -\n• I use builtins when possible\n• I use set lookups wherever possible\n\nEdit: I am using a profiler, and benchmarks. I'm working on a library - an ASGI Api framework. The code is async. Its not darascience. Its neither compatible with pypy, nor with numba.."
    },
    {
        "link": "https://softformance.com/blog/how-to-speed-up-python-code",
        "document": "We at SoftFormance adore Python. Actually, it is our favorite programming language, as both startups and established enterprises can benefit from Python software development. With its remarkable diversity of libraries and frameworks, Python provides excellent solutions for mobile and web application development.\n\nOver 10+ years in the market, the SoftFormance team has developed and released more than 100 applications using Python and our favorite framework, Django.\n\nWe certainly know how to take the maximum out of Python, and we are eager to share our expertise in Python code optimization with you.\n\nKeep reading if you want a development team that knows how to speed up Python code and save your time, money, and effort.\n\nTo begin with, let’s outline some Python essentials.\n\nOne of the main distinctions and selling points of Python is that it is an interpreted language. Python code can be executed directly, eliminating the need for pre-compilation into machine code. This significantly boosts the speed of Python development.\n\nHowever, Python code needs to undergo interpretation each time it is executed on the processor. To prevent the necessity of recompiling the code with each run, Python employs compiled .pyc files. These files enable the storage of bytecode produced from the original Python code, facilitating caching. Later on, we will dwell on some useful tips on Python code compiling.\n\nFinally, Python code is dynamically typed. This means that you aren’t required to specify a data type every time you create a variable. This dynamic approach significantly boosts the speed of coding with Python, but it can also negatively impact Python’s performance if not managed properly.\n\nFortunately, this article will give you useful tips on how to speed up Python code and the overall performance of this programming language.\n\nWith Python code optimization tips mentioned in this article, you will have great ideas on how to:\n• Boost the performance of apps built with Python\n\nAs a team that has an impressive track record of successful Python application development projects, we certainly have some advice to offer to Python developers.\n\nMany tips on this list are rather simple yet often overlooked. And, most probably, you will find, at least, some useful ideas here.\n\nHere’s a more general suggestion that will help you embrace the complete potential of Python.\n\nFrom our experience, there’s no better Python framework than Django.\n\nIt is fast, efficient, popular, and rich with Python development tools.\n\nAs a result, writing Python code with Django may become a clear highway to success.\n\nBut, surely, there are more specific ideas on optimizing Python code to come.\n\n2. Use PyPy Instead of CPython\n\nPyPy is an implementation of Python that uses just-in-time compilation instead of ahead-of-time compilation, peculiar to this language.\n\nAs a result, PyPy allows our developers to speed up code execution.\n\nSometimes, code execution with PyPy can be seven times faster than with CPython.\n\n3. Use NumPy Arrays Instead of Lists\n\nThe NumPy library has a great implementation in scientific computing. When dealing with substantial data and mathematical operations, NumPy arrays can significantly outpace common Python lists.\n\nNumPy arrays are tailored for numerical tasks, enhancing efficiency with sizable datasets and consuming less memory than lists. This, in turn, means improved performance.\n\nThe “timeit” module is a special feature that allows you to control Python, improve performance, and track its efficiency much better.\n\nIt allows the developer to measure how long it takes to execute a piece of code.\n\nAs a result, there appears a great space for testing the efficiency of different coding approaches.\n\nGenerator expressions offer a memory-efficient approach to crafting lists by generating values on-the-fly instead of storing the entire list at once.\n\nUnlike list comprehensions, generator expressions rely on parentheses, yielding a generator object rather than a list, which helps users enhance code performance while minimizing memory consumption.\n\nMultiprocessing allows you to partition your code into multiple processes.\n\nAs a result, you can harness the additional processing capability offered by multicore processors, thereby enhancing your code’s performance.\n\nMind that your technical team may need to show a lot of skill in order to handle multiprocessing properly.\n\nThe Python profiling feature is a perfect way for you to track memory usage, measure the number of function calls, and analyze the time needed for the execution of those calls.\n\nVarious continuous profilers provided by the vibrant community of Python developers can come in handy.\n\nOr you may aim for a more custom profiler, which will allow you to ensure an always-on approach.\n\nLoops are very common in coding, and Python provides inherent mechanisms to facilitate them. The point is that such loops often slow down Python programs.\n\nFortunately, code mapping is here to optimize time utilization and accelerate the execution of such loops.\n\nCode maps are native structure elements that simplify intricate code, making it more shareable and comprehensible. The more efficient and consolidated the code, the better your Python code speed up.\n\nWhile writing Python code, the developers should review it regularly. The point is to remove unnecessary code parts and save memory.\n\nThere are multiple ways for removing dead code. These include multiprocessing, using content managers, and relying on preload managers.\n\nDon’t forget to monitor the performance of your Python apps because this allows you to properly evaluate the efficiency of your work.\n\nAPM tools, such as New Relic, will come in handy. They benchmark a program, identify performance bottlenecks, and provide optimization solutions to these issues.\n\nPeephole optimization is a Python coding technique that boosts code performance during the compilation.\n\nIts main tasks are pre-computing constant expressions and employing membership tests.\n\nFor example, a developer can improve code readability by writing “a = 606024” to represent the number of seconds in a day. However, the language interpreter automatically calculates this and replaces repetitive instances, which, in turn, boosts software performance.\n\nIf you are using Peephole optimization, Python precomputes constant expressions like 606024 and replaces them with the result, such as 86400. This allows you to avoid performance decrease.\n\nPython string objects are sequences of Unicode characters, referred to as “text” sequences in the documentation.\n\nIf different character sizes are appended to a string, the overall size and weight of the string grow exponentially. In addition, Python allocates additional information for storing these strings. As a result, too much space is consumed.\n\nThat’s when string interning comes into action. It is based on caching of specific strings in memory upon creation.\n\nAs a result, only a single instance of each unique string remains active at any point. This, in turn, means more efficient memory allocation.\n\ncProfile offers functionality for advanced profiling, which is part of the Python package since Python 2.5.\n\nYou can connect it to the Python code in the following ways:\n• Encapsulate a function within its “run” method to measure its performance.;\n• Run the command line script, activate cProfile as an argument, and use Python’s “-m” option.\n\n14. Use Generators and Keys for Sorting\n\nUsing generators is one more way to optimize memory consumption.\n\nThese generators can yield items one by one instead of yielding them all at once. When you are sorting items in a list afterwards, we advise you to employ keys and the default <sort()> method.\n\nThe developers can employ this technique to sort both lists and strings based on a chosen index specified within the key argument.\n\nPython offers a wide array of built-in operators and libraries. We don’t know all of them, but we do know for sure that there are thousands in existence.\n\nUse these built-ins whenever feasible to make your code more efficient. As long as such built-in operators are pre-compiled, they bring you truly swift performance.\n\nThe “C” counterparts of certain Python libraries can bring you the same or even more advanced functionality than the original versions of these libraries. Unsurprisingly, their usage can help you with Python performance optimization.\n\nFor example, you may consider substituting Pickle with cPickle to observe the performance disparity.\n\nThe developers can also boost Python coding efficiency with the above-mentioned PyPy and <Cython>.\n\nBoth these solutions serve as means to optimize a static compiler.\n\nGlobal variables have their benefits, but they can also bring unexpected side effects, such as excessively complex code structures.\n\nPython performance may drop when you are accessing external variables. So, we advise you to minimize their usage or avoid applying such variables at all.\n\nIf there is no way to avoid using globals, consider the following suggestions:\n• Apply the ‘global’ keyword to explicitly declare an external variable.\n• Before applying globals within loops, generate local copies to boost efficiency.\n\nPython offers list, tuple, set, and dictionary as the built-in data structures.\n\nAnd most developers rely on a list of all cases.\n\nHowever, if you want a truly good performance, check how different data structures fit different cases. And, as a result of your research, choose data structures depending on your needs\n\nThis approach optimizes and speeds up the code execution.\n\nThe key point is to reject variables like this:\n\nAnd to assign variables like this:\n\nIn Python, you have the option to concatenate strings using the + operator. For example:\n\nWe recommend you try the join () method because it is faster.\n\nIn this case, your optimization code in Python will look the following way:\n\nInstead of using the “while True” construct in your code, rely on the “while 1” construct. Unless quite simple, this modification can reduce your code’s runtime.\n\nThe point is that “while 1” offers a more straightforward depiction of an infinite loop condition. This can enhance performance compared to “while True” construct, which is slightly more abstract.\n\nC/C++ outperforms Python in terms of speed. And there are numerous packages and modules developed in C/C++ that can be integrated into your Python programs.\n\nNotably, Numpy, Scipy, and Pandas are three prominent examples, known for their effectiveness in handling large datasets.\n\nTake a look at an example. What you see is a code to list all the numbers between 1 and 1000 that is the multiplier of 3:\n\nWith list comprehension, this code will look as follows:\n\nThe main benefit of list comprehension is that it can help you improve Python performance.\n\nDo not write a function that already exists in the library manually.\n\nLibrary functions prove to be very efficient. Actually, replicating their level of efficiency in your own code can be quite challenging.\n\nAnd, surely, their use helps you save a lot of time.\n\n25. Do Not Use .dot Operation\n\nTry to avoid dot operations, as it may be time-consuming. Take a look at the example below:\n\nYou may write the same operation in the following style:\n\nThe point is that the function with a .(dot) first calls __getattribute()__ or __getattr()__, which then uses a dictionary operation. This means that your operation takes more time. We recommend you use the module import function for optimizing Python code for speed.\n\nSo, these 25 tips will help you or your developers optimize the Python code.\n\nAs a result, you will develop apps that show excellent performance a few times faster.\n\nBut it is always better to rely on specialists who have already mastered these tips long before and know how to improve Python performance.\n\nIf you have issues with performance of your Python/Django app, SoftFormance, a perfect Python development team, is ready to help you.\n\nWe will thoroughly analyze your code and tune it up with the best Python code optimization techniques.\n\nContact us to get your Python apps running faster and more efficiently than ever!"
    },
    {
        "link": "https://stackify.com/how-to-optimize-python-code",
        "document": "Currently, Python is the most used programming language for different projects around the world. According to statistics, 44.1% of programmers choose Python coding language for application/web development. However, that does not mean that Python developers are exempt from creating messy and inefficient code that can cost you and your clients time and money This is where Python code optimization comes in.\n\nLet’s start by defining code optimization, so that you get the basic idea and understand why it’s needed. Sometimes it’s not enough to create code that just executes the task. Large and inefficient code can slow down the application, lead to financial losses for the customer, or require more time for further improvements and debugging.\n\nPython code optimization is a way to make your program perform any task more efficiently and quickly with fewer lines of code, less memory, or other resources involved, while producing the right results.\n\nIt’s crucial when it comes to processing a large number of operations or data while performing a task. Thus, replacing and optimizing some inefficient code blocks and features can work wonders:\n• Speed up the performance of the application;\n• Save a lot of computational power, and so on.\n\nWhy do you need optimization?\n\nThere are certain scenarios where Python can lead to performance issues. Alternately, some situations may demand additional code manipulation to optimize your Python code due to the nature of operation being performed. Let’s explore some common performance issues in Python which will help us understand why we need optimization in the first place?\n\nA lot of times you’ll need to write code that does intensive calculations, complex mathematical operations and data manipulation which can be CPU intensive. In such cases, using Python’s interpreter will give you a slower execution time as compared to compiled languages.\n\nWhen dealing with long-running processes or large datasets, Python’s inefficient memory management can be inefficient can lead to excessive memory consumption. As a result, your code can execute slower than usual and may also cause a crash at times.\n\nDealing with operations such as reading and writing to files, making network requests or interacting with the database, can slow down your Python code.\n\nSometimes, due to either a lack of knowledge or experience, Python developers use inappropriate data structures which can cause performance issues. For instance, using lists when sets or dictionaries would be more efficient can cause slower lookups and inserts.\n\nThe choice of algorithms and data processing techniques can significantly impact performance. Using algorithms with higher time complexity (e.g., nested loops) when more efficient alternatives exist can lead to slow execution.\n\nPython strings are immutable, and operations like string concatenation in loops can result in significant performance overhead due to the creation of new string objects.\n\nOverusing global variables can slow down your code since Python has to look up these variables in a global namespace, which is slower than accessing local variables.\n\nPython’s Global Interpreter Lock (GIL) restricts the execution of multiple threads in CPython, the reference implementation. This can limit the parallelism of multi-threaded applications, making them slower than expected.\n\nPython provides many built-in functions and libraries for common tasks. Failing to leverage these can lead to slower code due to reinventing the wheel.\n\nUsing external libraries or packages that are not optimized for performance can impact your code’s efficiency. It’s crucial to choose well-maintained and optimized libraries when possible.\n\nPython developers need to be able to use code optimization techniques instead of basic programming to ensure applications run smoothly and quickly. Below we have listed 6 tips on how to optimize Python code to make it clean and efficient.\n\nTo better understand the Peephole optimization technique, let’s start with how the Python code is executed. Initially the code is written to a standard file, then you can run the command “python -m compileall <filename>”and get the same file in *.pyc format which is the result of the optimization.\n\n<Peephole> is a code optimization technique in Python that is done at compile time to improve your code performance. With the Peephole optimization technique, code is optimized behind the scenes and is done either by pre-calculating constant expressions or by using membership tests. For example, you can write something like the number of seconds in a day as a = 60*60*24 to make the code more readable, but the language interpreter will immediately perform the calculation and use it instead of repeating the above statement over and over again and thereby reducing the software performance.\n\nThe result of the Peephole optimization technique is that Python pre-calculates constant expressions 60*60*24, replacing them with 86400. So even if you write 60*60*24 all the time, it won’t decrease performance.\n\nUsing this technique, you can replace a section of the program or a segment of instruction without significant changes in output.\n\nApplying the optimization technique, you can:\n• Turn mutable constructs into immutable ones. It can be done by using one of the 3 tuples:\n• <__code__.co_consts> that references all the constants.\n• Verify the membership of the element by treating the instructions as a constant cost operation irrespective of the size of the set.\n• Turn both the set and list into Constants.\n\nPay attention to the fact that this transformation can only be performed by Python for literals. Thus, the optimization will not happen if any of the sets or lists used are not literals.\n\nLet’s take a look at some examples:\n• “Hello, world!” * 5 is a constant expression less than 4096 long. So it gets evaluated by the compiler as “Hello, world!” 5 consecutive times\n• [1, 2] * 7 is a list (mutable object) so it’s not evaluated.\n• (10, 20, 30) * 3 is a sequence of length 9 which is less than 256 (for tuples), so it’s stored as (10, 20, 30, 10, 20, 30, 10, 20, 30).\n\nString objects in Python are sequences of Unicode characters, so they are called “text” sequences in the documentation. When characters of different sizes are added to a string, its total size and weight increase, but not only by the size of the added character. Python also allocates extra information to store strings, which causes them to take up too much space. To increase efficiency, an optimization method called string interning is used.\n\nThe idea behind string interning is to cache certain strings in memory as they are created. This means that only one instance of a particular string is active at any given time, and no new memory is needed to refer to it.\n\nString interning has a lot in common with shared objects. When a string is interned, it is treated as a shared object because an instance of that string object is globally shared by all programs running in a given Python session.\n\nAs the most common implementation of the Python programming language, CPython loads shared objects into memory every time a Python interactive session is initialized.\n\nThis is why string interning allows Python to run efficiently, both in terms of saving time and memory.\n\nPython tends to store only those strings that are most likely to be reused, namely identifier strings:\n\nPrinciples according to which a string should be interned:\n• Only a string loaded at compile time as a constant string will be interned, and conversely, a string constructed at runtime will not be interned.\n• A string will not be interned if it is the product of constant folding and is more than 20 characters long, because it is hardly an identifier.\n• Python will only intern a string and create a hash for it if it declares a string with a name that includes only combinations of letters/numbers/black characters and begins with either a letter or an underscore character.\n\nThus, all strings that are read from a file or received through the network are not part of the about-interning. However, just offload such a task to the intern() function to start interning such strings and processing them.\n\nBy profiling your code, you can identify areas of improvement in your code for further optimization. There are two main ways to do this.\n\nUse stop-watch profiling with this module. <timeit> records the time needed for task execution by a certain code segment and measures the time elapsed in milliseconds.\n\nThis is advanced profiling, which is part of the Python package since Python 2.5. There are several ways to connect it to the Python code:\n• Wrap a function inside its run method and thus measure the performance;\n• Run the whole script from the command line while activating cProfile as an argument, using Python’s “-m” option.\n\nKnowing the key elements of the cProfile report, you can find bottlenecks in your code.\n\nHere are the elements to consider:\n• <tottime> — the aggregate time spent in the given function and which has the greatest value;\n• <percall> — the quotient of <tottime> divided by <ncalls>;\n• <cumtime> — another parameter of high importance for all the projects that represents cumulative time in executing functions, its subfunctions;\n• <percall> — the quotient of <cumtime> divided by primitive calls;\n\n4. Use Generators and Keys For Sorting\n\nThis is a way to optimize memory by using such a great tool as generators. Their peculiarity is that they don’t return all items (iterators) at once, but can return only one at a time. It’s better to use keys and the default <sort()> method while sorting items in a list. Thus, for instance, you can sort the list and strings according to the index selected as part of the key argument.\n\nWhat this might look like:\n\nThere are thousands of built-in operators and libraries available in Python. It’s better to use the built-ins wherever possible to make your code more efficient. It’s possible due to the fact that all the built-ins are pre-compiled and, thus, pretty fast.\n\nThe “C” equivalent of some Python libraries gives you the same features as the original library but with faster performance. So, try to use cPickle instead of Pickle, for example, to see the difference. The PyPy package and <Cython> are a way to optimize a static compiler to make the process even faster.\n\nGlobals can have non-obvious and hidden side effects resulting in Spaghetti code. What’s more, Python is slow at accessing external variables. Herewith, it’s better to avoid using them, or at least limit their usage. If they are a necessity, here are a few recommendations:\n• Use the global keyword to declare an external variable;\n• Make a local copy before using them inside loops.\n\nTo optimize method lookup in Python, you can store a reference to a method or function in a local variable rather than repeatedly accessing it through attribute lookup. This can be particularly useful in loops or frequently called functions. Let’s explore this with an example:\n\nBy doing this, you reduce the overhead of attribute lookup on each iteration, potentially improving performance.\n\nString concatenation can be slow, especially when performed in a loop. In such cases, using a list to accumulate parts of the string and then joining them at the end can be much more efficient. Here’s an example:\n\nMinimizing the number of statements or optimizing their structure can also improve code performance. Use early returns when possible to avoid unnecessary condition checking.\n\nIn the optimized code, the function avoids the need to check the condition twice and simplifies the code flow.\n\nIt’s critical to create a robust and scalable application that performs tasks rapidly and smoothly. However, it’s impossible to develop such an application by using only basic coding techniques. That’s why you need to optimize the Python code. At the same time, using the optimization techniques described in the article, you can not only create a clean code, but also improve the app performance and save a lot of time and money.\n\nYou can also use Stackify’s Retrace performance monitoring and code profiling tool for your Python application. Its Python agent collects performance data and sends it to Retrace. You can also set custom markers or decorators to pinpoint specific areas for monitoring in your code. After running your code, Retrace analyzes data in the Retrace dashboard and finds the bottlenecks for you. Using the insights obtained from Retrace you can optimize your code through restructuring or algorithm improvements.\n\n\n\nBogdan Ivanov is a CTO at DDI development. He is a professional with an advanced degree in Cybersecurity, and 7 years of experience in building a cybersecurity strategy for all the company’s projects. He has a deep understanding of network security, compliance, and operational security."
    },
    {
        "link": "https://theserverside.com/tip/Tips-to-improve-Python-performance",
        "document": ""
    },
    {
        "link": "https://quora.com/How-can-I-optimize-my-Python-code-for-faster-execution-and-efficiency",
        "document": "Something went wrong. Wait a moment and try again."
    },
    {
        "link": "https://docs.python.org/3/library/multiprocessing.html",
        "document": "is a package that supports spawning processes using an API similar to the module. The package offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads. Due to this, the module allows the programmer to fully leverage multiple processors on a given machine. It runs on both POSIX and Windows. The module also introduces APIs which do not have analogs in the module. A prime example of this is the object which offers a convenient means of parallelizing the execution of a function across multiple input values, distributing the input data across processes (data parallelism). The following example demonstrates the common practice of defining such functions in a module so that child processes can successfully import that module. This basic example of data parallelism using , offers a higher level interface to push tasks to a background process without blocking execution of the calling process. Compared to using the interface directly, the API more readily allows the submission of work to the underlying process pool to be separated from waiting for the results. In , processes are spawned by creating a object and then calling its method. follows the API of . A trivial example of a multiprocess program is To show the individual process IDs involved, here is an expanded example: For an explanation of why the part is necessary, see Programming guidelines. Depending on the platform, supports three ways to start a process. These start methods are The parent process starts a fresh Python interpreter process. The child process will only inherit those resources necessary to run the process object’s method. In particular, unnecessary file descriptors and handles from the parent process will not be inherited. Starting a process using this method is rather slow compared to using fork or forkserver. Available on POSIX and Windows platforms. The default on Windows and macOS. The parent process uses to fork the Python interpreter. The child process, when it begins, is effectively identical to the parent process. All resources of the parent are inherited by the child process. Note that safely forking a multithreaded process is problematic. Available on POSIX systems. Currently the default on POSIX except macOS. The default start method will change away from fork in Python 3.14. Code that requires fork should explicitly specify that via or . Changed in version 3.12: If Python is able to detect that your process has multiple threads, the function that this start method calls internally will raise a . Use a different start method. See the documentation for further explanation. When the program starts and selects the forkserver start method, a server process is spawned. From then on, whenever a new process is needed, the parent process connects to the server and requests that it fork a new process. The fork server process is single threaded unless system libraries or preloaded imports spawn threads as a side-effect so it is generally safe for it to use . No unnecessary resources are inherited. Available on POSIX platforms which support passing file descriptors over Unix pipes such as Linux. Changed in version 3.4: spawn added on all POSIX platforms, and forkserver added for some POSIX platforms. Child processes no longer inherit all of the parents inheritable handles on Windows. Changed in version 3.8: On macOS, the spawn start method is now the default. The fork start method should be considered unsafe as it can lead to crashes of the subprocess as macOS system libraries may start threads. See bpo-33725. On POSIX using the spawn or forkserver start methods will also start a resource tracker process which tracks the unlinked named system resources (such as named semaphores or objects) created by processes of the program. When all processes have exited the resource tracker unlinks any remaining tracked object. Usually there should be none, but if a process was killed by a signal there may be some “leaked” resources. (Neither leaked semaphores nor shared memory segments will be automatically unlinked until the next reboot. This is problematic for both objects because the system allows only a limited number of named semaphores, and shared memory segments occupy some space in the main memory.) To select a start method you use the in the clause of the main module. For example: should not be used more than once in the program. Alternatively, you can use to obtain a context object. Context objects have the same API as the multiprocessing module, and allow one to use multiple start methods in the same program. Note that objects related to one context may not be compatible with processes for a different context. In particular, locks created using the fork context cannot be passed to processes started using the spawn or forkserver start methods. A library which wants to use a particular start method should probably use to avoid interfering with the choice of the library user. The and start methods generally cannot be used with “frozen” executables (i.e., binaries produced by packages like PyInstaller and cx_Freeze) on POSIX systems. The start method may work if code does not use threads. supports two types of communication channel between processes: The class is a near clone of . For example: Queues are thread and process safe. Any object put into a queue will be serialized. The function returns a pair of connection objects connected by a pipe which by default is duplex (two-way). For example: The two connection objects returned by represent the two ends of the pipe. Each connection object has and methods (among others). Note that data in a pipe may become corrupted if two processes (or threads) try to read from or write to the same end of the pipe at the same time. Of course there is no risk of corruption from processes using different ends of the pipe at the same time. The method serializes the object and re-creates the object. contains equivalents of all the synchronization primitives from . For instance one can use a lock to ensure that only one process prints to standard output at a time: Without using the lock output from the different processes is liable to get all mixed up. As mentioned above, when doing concurrent programming it is usually best to avoid using shared state as far as possible. This is particularly true when using multiple processes. However, if you really do need to use some shared data then provides a couple of ways of doing so. Data can be stored in a shared memory map using or . For example, the following code The and arguments used when creating and are typecodes of the kind used by the module: indicates a double precision float and indicates a signed integer. These shared objects will be process and thread-safe. For more flexibility in using shared memory one can use the module which supports the creation of arbitrary ctypes objects allocated from shared memory. A manager object returned by controls a server process which holds Python objects and allows other processes to manipulate them using proxies. A manager returned by will support types , , , , , , , , , , , and . For example, Server process managers are more flexible than using shared memory objects because they can be made to support arbitrary object types. Also, a single manager can be shared by processes on different computers over a network. They are, however, slower than using shared memory. The class represents a pool of worker processes. It has methods which allows tasks to be offloaded to the worker processes in a few different ways. # runs in *only* one process # runs in *only* one process # prints the PID of that process # launching multiple evaluations asynchronously *may* use more processes \"We lacked patience and got a multiprocessing.TimeoutError\" \"For the moment, the pool remains available for more work\" # exiting the 'with'-block has stopped the pool \"Now the pool is closed and no longer available\" Note that the methods of a pool should only ever be used by the process which created it. Functionality within this package requires that the module be importable by the children. This is covered in Programming guidelines however it is worth pointing out here. This means that some examples, such as the examples will not work in the interactive interpreter. For example: : Can't get attribute 'f' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)> AttributeError: Can't get attribute 'f' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)> AttributeError: Can't get attribute 'f' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n\nThere are certain guidelines and idioms which should be adhered to when using . The following applies to all start methods. As far as possible one should try to avoid shifting large amounts of data between processes. It is probably best to stick to using queues or pipes for communication between processes rather than using the lower level synchronization primitives. Ensure that the arguments to the methods of proxies are picklable. Do not use a proxy object from more than one thread unless you protect it with a lock. On POSIX when a process finishes but has not been joined it becomes a zombie. There should never be very many because each time a new process starts (or is called) all completed processes which have not yet been joined will be joined. Also calling a finished process’s will join the process. Even so it is probably good practice to explicitly join all the processes that you start. Better to inherit than pickle/unpickle When using the spawn or forkserver start methods many types from need to be picklable so that child processes can use them. However, one should generally avoid sending shared objects to other processes using pipes or queues. Instead you should arrange the program so that a process which needs access to a shared resource created elsewhere can inherit it from an ancestor process. Using the method to stop a process is liable to cause any shared resources (such as locks, semaphores, pipes and queues) currently being used by the process to become broken or unavailable to other processes. Therefore it is probably best to only consider using on processes which never use any shared resources. Bear in mind that a process that has put items in a queue will wait before terminating until all the buffered items are fed by the “feeder” thread to the underlying pipe. (The child process can call the method of the queue to avoid this behaviour.) This means that whenever you use a queue you need to make sure that all items which have been put on the queue will eventually be removed before the process is joined. Otherwise you cannot be sure that processes which have put items on the queue will terminate. Remember also that non-daemonic processes will be joined automatically. An example which will deadlock is the following: A fix here would be to swap the last two lines (or simply remove the line). On POSIX using the fork start method, a child process can make use of a shared resource created in a parent process using a global resource. However, it is better to pass the object as an argument to the constructor for the child process. Apart from making the code (potentially) compatible with Windows and the other start methods this also ensures that as long as the child process is still alive the object will not be garbage collected in the parent process. This might be important if some resource is freed when the object is garbage collected in the parent process. Beware of replacing with a “file like object” in the method — this resulted in issues with processes-in-processes. This has been changed to: Which solves the fundamental issue of processes colliding with each other resulting in a bad file descriptor error, but introduces a potential danger to applications which replace with a “file-like object” with output buffering. This danger is that if multiple processes call on this file-like object, it could result in the same data being flushed to the object multiple times, resulting in corruption. If you write a file-like object and implement your own caching, you can make it fork-safe by storing the pid whenever you append to the cache, and discarding the cache when the pid changes. For example: For more information, see bpo-5155, bpo-5313 and bpo-5331 There are a few extra restrictions which don’t apply to the fork start method. Ensure that all arguments to are picklable. Also, if you subclass then make sure that instances will be picklable when the method is called. Bear in mind that if code run in a child process tries to access a global variable, then the value it sees (if any) may not be the same as the value in the parent process at the time that was called. However, global variables which are just module level constants cause no problems. Make sure that the main module can be safely imported by a new Python interpreter without causing unintended side effects (such as starting a new process). For example, using the spawn or forkserver start method running the following module would fail with a : Instead one should protect the “entry point” of the program by using as follows: This allows the newly spawned Python interpreter to safely import the module and then run the module’s function. Similar restrictions apply if a pool or manager is created in the main module."
    },
    {
        "link": "https://docs.python.org/3/library/threading.html",
        "document": "This module constructs higher-level threading interfaces on top of the lower level module.\n\noffers a higher level interface to push tasks to a background thread without blocking execution of the calling thread, while still being able to retrieve their results when needed. provides a thread-safe interface for exchanging data between running threads. offers an alternative approach to achieving task level concurrency without requiring the use of multiple operating system threads.\n\nThis module defines the following functions:\n\nThis module also defines the following constant:\n\nThis module defines a number of classes, which are detailed in the sections below.\n\nThe design of this module is loosely based on Java’s threading model. However, where Java makes locks and condition variables basic behavior of every object, they are separate objects in Python. Python’s class supports a subset of the behavior of Java’s Thread class; currently, there are no priorities, no thread groups, and threads cannot be destroyed, stopped, suspended, resumed, or interrupted. The static methods of Java’s Thread class, when implemented, are mapped to module-level functions.\n\nAll of the methods described below are executed atomically.\n\nThe class represents an activity that is run in a separate thread of control. There are two ways to specify the activity: by passing a callable object to the constructor, or by overriding the method in a subclass. No other methods (except for the constructor) should be overridden in a subclass. In other words, only override the and methods of this class. Once a thread object is created, its activity must be started by calling the thread’s method. This invokes the method in a separate thread of control. Once the thread’s activity is started, the thread is considered ‘alive’. It stops being alive when its method terminates – either normally, or by raising an unhandled exception. The method tests whether the thread is alive. Other threads can call a thread’s method. This blocks the calling thread until the thread whose method is called is terminated. A thread has a name. The name can be passed to the constructor, and read or changed through the attribute. If the method raises an exception, is called to handle it. By default, ignores silently . A thread can be flagged as a “daemon thread”. The significance of this flag is that the entire Python program exits when only daemon threads are left. The initial value is inherited from the creating thread. The flag can be set through the property or the daemon constructor argument. Daemon threads are abruptly stopped at shutdown. Their resources (such as open files, database transactions, etc.) may not be released properly. If you want your threads to stop gracefully, make them non-daemonic and use a suitable signalling mechanism such as an . There is a “main thread” object; this corresponds to the initial thread of control in the Python program. It is not a daemon thread. There is the possibility that “dummy thread objects” are created. These are thread objects corresponding to “alien threads”, which are threads of control started outside the threading module, such as directly from C code. Dummy thread objects have limited functionality; they are always considered alive and daemonic, and cannot be joined. They are never deleted, since it is impossible to detect the termination of alien threads. This constructor should always be called with keyword arguments. Arguments are: group should be ; reserved for future extension when a class is implemented. target is the callable object to be invoked by the method. Defaults to , meaning nothing is called. name is the thread name. By default, a unique name is constructed of the form “Thread-N” where N is a small decimal number, or “Thread-N (target)” where “target” is if the target argument is specified. args is a list or tuple of arguments for the target invocation. Defaults to . kwargs is a dictionary of keyword arguments for the target invocation. Defaults to . If not , daemon explicitly sets whether the thread is daemonic. If (the default), the daemonic property is inherited from the current thread. If the subclass overrides the constructor, it must make sure to invoke the base class constructor ( ) before doing anything else to the thread. Changed in version 3.10: Use the target name if name argument is omitted. It must be called at most once per thread object. It arranges for the object’s method to be invoked in a separate thread of control. This method will raise a if called more than once on the same thread object. You may override this method in a subclass. The standard method invokes the callable object passed to the object’s constructor as the target argument, if any, with positional and keyword arguments taken from the args and kwargs arguments, respectively. Using list or tuple as the args argument which passed to the could achieve the same effect. Wait until the thread terminates. This blocks the calling thread until the thread whose method is called terminates – either normally or through an unhandled exception – or until the optional timeout occurs. When the timeout argument is present and not , it should be a floating-point number specifying a timeout for the operation in seconds (or fractions thereof). As always returns , you must call after to decide whether a timeout happened – if the thread is still alive, the call timed out. When the timeout argument is not present or , the operation will block until the thread terminates. A thread can be joined many times. raises a if an attempt is made to join the current thread as that would cause a deadlock. It is also an error to a thread before it has been started and attempts to do so raise the same exception. A string used for identification purposes only. It has no semantics. Multiple threads may be given the same name. The initial name is set by the constructor. Deprecated getter/setter API for ; use it directly as a property instead. The ‘thread identifier’ of this thread or if the thread has not been started. This is a nonzero integer. See the function. Thread identifiers may be recycled when a thread exits and another thread is created. The identifier is available even after the thread has exited. The Thread ID ( ) of this thread, as assigned by the OS (kernel). This is a non-negative integer, or if the thread has not been started. See the function. This value may be used to uniquely identify this particular thread system-wide (until the thread terminates, after which the value may be recycled by the OS). Similar to Process IDs, Thread IDs are only valid (guaranteed unique system-wide) from the time the thread is created until the thread has been terminated. Return whether the thread is alive. This method returns just before the method starts until just after the method terminates. The module function returns a list of all alive threads. A boolean value indicating whether this thread is a daemon thread ( ) or not ( ). This must be set before is called, otherwise is raised. Its initial value is inherited from the creating thread; the main thread is not a daemon thread and therefore all threads created in the main thread default to = . The entire Python program exits when no alive non-daemon threads are left. Deprecated getter/setter API for ; use it directly as a property instead.\n\nA primitive lock is a synchronization primitive that is not owned by a particular thread when locked. In Python, it is currently the lowest level synchronization primitive available, implemented directly by the extension module. A primitive lock is in one of two states, “locked” or “unlocked”. It is created in the unlocked state. It has two basic methods, and . When the state is unlocked, changes the state to locked and returns immediately. When the state is locked, blocks until a call to in another thread changes it to unlocked, then the call resets it to locked and returns. The method should only be called in the locked state; it changes the state to unlocked and returns immediately. If an attempt is made to release an unlocked lock, a will be raised. When more than one thread is blocked in waiting for the state to turn to unlocked, only one thread proceeds when a call resets the state to unlocked; which one of the waiting threads proceeds is not defined, and may vary across implementations. The class implementing primitive lock objects. Once a thread has acquired a lock, subsequent attempts to acquire it block, until it is released; any thread may release it. Changed in version 3.13: is now a class. In earlier Pythons, was a factory function which returned an instance of the underlying private lock type. When invoked with the blocking argument set to (the default), block until the lock is unlocked, then set it to locked and return . When invoked with the blocking argument set to , do not block. If a call with blocking set to would block, return immediately; otherwise, set the lock to locked and return . When invoked with the floating-point timeout argument set to a positive value, block for at most the number of seconds specified by timeout and as long as the lock cannot be acquired. A timeout argument of specifies an unbounded wait. It is forbidden to specify a timeout when blocking is . The return value is if the lock is acquired successfully, if not (for example if the timeout expired). Changed in version 3.2: The timeout parameter is new. Changed in version 3.2: Lock acquisition can now be interrupted by signals on POSIX if the underlying threading implementation supports it. Release a lock. This can be called from any thread, not only the thread which has acquired the lock. When the lock is locked, reset it to unlocked, and return. If any other threads are blocked waiting for the lock to become unlocked, allow exactly one of them to proceed. When invoked on an unlocked lock, a is raised. There is no return value. Return if the lock is acquired.\n\nA reentrant lock is a synchronization primitive that may be acquired multiple times by the same thread. Internally, it uses the concepts of “owning thread” and “recursion level” in addition to the locked/unlocked state used by primitive locks. In the locked state, some thread owns the lock; in the unlocked state, no thread owns it. Threads call a lock’s method to lock it, and its method to unlock it. Reentrant locks support the context management protocol, so it is recommended to use instead of manually calling and to handle acquiring and releasing the lock for a block of code. RLock’s / call pairs may be nested, unlike Lock’s / . Only the final (the of the outermost pair) resets the lock to an unlocked state and allows another thread blocked in to proceed. / must be used in pairs: each acquire must have a release in the thread that has acquired the lock. Failing to call release as many times the lock has been acquired can lead to deadlock. This class implements reentrant lock objects. A reentrant lock must be released by the thread that acquired it. Once a thread has acquired a reentrant lock, the same thread may acquire it again without blocking; the thread must release it once for each time it has acquired it. Note that is actually a factory function which returns an instance of the most efficient version of the concrete RLock class that is supported by the platform. Recommended over manual and calls whenever practical. When invoked with the blocking argument set to (the default):\n• None If no thread owns the lock, acquire the lock and return immediately.\n• None If another thread owns the lock, block until we are able to acquire lock, or timeout, if set to a positive float value.\n• None If the same thread owns the lock, acquire the lock again, and return immediately. This is the difference between and ; handles this case the same as the previous, blocking until the lock can be acquired. When invoked with the blocking argument set to :\n• None If no thread owns the lock, acquire the lock and return immediately.\n• None If another thread owns the lock, return immediately.\n• None If the same thread owns the lock, acquire the lock again and return immediately. In all cases, if the thread was able to acquire the lock, return . If the thread was unable to acquire the lock (i.e. if not blocking or the timeout was reached) return . If called multiple times, failing to call as many times may lead to deadlock. Consider using as a context manager rather than calling acquire/release directly. Changed in version 3.2: The timeout parameter is new. Release a lock, decrementing the recursion level. If after the decrement it is zero, reset the lock to unlocked (not owned by any thread), and if any other threads are blocked waiting for the lock to become unlocked, allow exactly one of them to proceed. If after the decrement the recursion level is still nonzero, the lock remains locked and owned by the calling thread. Only call this method when the calling thread owns the lock. A is raised if this method is called when the lock is not acquired. There is no return value.\n\nA condition variable is always associated with some kind of lock; this can be passed in or one will be created by default. Passing one in is useful when several condition variables must share the same lock. The lock is part of the condition object: you don’t have to track it separately. A condition variable obeys the context management protocol: using the statement acquires the associated lock for the duration of the enclosed block. The and methods also call the corresponding methods of the associated lock. Other methods must be called with the associated lock held. The method releases the lock, and then blocks until another thread awakens it by calling or . Once awakened, re-acquires the lock and returns. It is also possible to specify a timeout. The method wakes up one of the threads waiting for the condition variable, if any are waiting. The method wakes up all threads waiting for the condition variable. Note: the and methods don’t release the lock; this means that the thread or threads awakened will not return from their call immediately, but only when the thread that called or finally relinquishes ownership of the lock. The typical programming style using condition variables uses the lock to synchronize access to some shared state; threads that are interested in a particular change of state call repeatedly until they see the desired state, while threads that modify the state call or when they change the state in such a way that it could possibly be a desired state for one of the waiters. For example, the following code is a generic producer-consumer situation with unlimited buffer capacity: The loop checking for the application’s condition is necessary because can return after an arbitrary long time, and the condition which prompted the call may no longer hold true. This is inherent to multi-threaded programming. The method can be used to automate the condition checking, and eases the computation of timeouts: To choose between and , consider whether one state change can be interesting for only one or several waiting threads. E.g. in a typical producer-consumer situation, adding one item to the buffer only needs to wake up one consumer thread. This class implements condition variable objects. A condition variable allows one or more threads to wait until they are notified by another thread. If the lock argument is given and not , it must be a or object, and it is used as the underlying lock. Otherwise, a new object is created and used as the underlying lock. Changed in version 3.3: changed from a factory function to a class. Acquire the underlying lock. This method calls the corresponding method on the underlying lock; the return value is whatever that method returns. Release the underlying lock. This method calls the corresponding method on the underlying lock; there is no return value. Wait until notified or until a timeout occurs. If the calling thread has not acquired the lock when this method is called, a is raised. This method releases the underlying lock, and then blocks until it is awakened by a or call for the same condition variable in another thread, or until the optional timeout occurs. Once awakened or timed out, it re-acquires the lock and returns. When the timeout argument is present and not , it should be a floating-point number specifying a timeout for the operation in seconds (or fractions thereof). When the underlying lock is an , it is not released using its method, since this may not actually unlock the lock when it was acquired multiple times recursively. Instead, an internal interface of the class is used, which really unlocks it even when it has been recursively acquired several times. Another internal interface is then used to restore the recursion level when the lock is reacquired. The return value is unless a given timeout expired, in which case it is . Changed in version 3.2: Previously, the method always returned . Wait until a condition evaluates to true. predicate should be a callable which result will be interpreted as a boolean value. A timeout may be provided giving the maximum time to wait. This utility method may call repeatedly until the predicate is satisfied, or until a timeout occurs. The return value is the last return value of the predicate and will evaluate to if the method timed out. Ignoring the timeout feature, calling this method is roughly equivalent to writing: Therefore, the same rules apply as with : The lock must be held when called and is re-acquired on return. The predicate is evaluated with the lock held. By default, wake up one thread waiting on this condition, if any. If the calling thread has not acquired the lock when this method is called, a is raised. This method wakes up at most n of the threads waiting for the condition variable; it is a no-op if no threads are waiting. The current implementation wakes up exactly n threads, if at least n threads are waiting. However, it’s not safe to rely on this behavior. A future, optimized implementation may occasionally wake up more than n threads. Note: an awakened thread does not actually return from its call until it can reacquire the lock. Since does not release the lock, its caller should. Wake up all threads waiting on this condition. This method acts like , but wakes up all waiting threads instead of one. If the calling thread has not acquired the lock when this method is called, a is raised. The method is a deprecated alias for this method.\n\nThis class provides a simple synchronization primitive for use by a fixed number of threads that need to wait for each other. Each of the threads tries to pass the barrier by calling the method and will block until all of the threads have made their calls. At this point, the threads are released simultaneously. The barrier can be reused any number of times for the same number of threads. As an example, here is a simple way to synchronize a client and server thread: Create a barrier object for parties number of threads. An action, when provided, is a callable to be called by one of the threads when they are released. timeout is the default timeout value if none is specified for the method. Pass the barrier. When all the threads party to the barrier have called this function, they are all released simultaneously. If a timeout is provided, it is used in preference to any that was supplied to the class constructor. The return value is an integer in the range 0 to parties – 1, different for each thread. This can be used to select a thread to do some special housekeeping, e.g.: # Only one thread needs to print this If an action was provided to the constructor, one of the threads will have called it prior to being released. Should this call raise an error, the barrier is put into the broken state. If the call times out, the barrier is put into the broken state. This method may raise a exception if the barrier is broken or reset while a thread is waiting. Return the barrier to the default, empty state. Any threads waiting on it will receive the exception. Note that using this function may require some external synchronization if there are other threads whose state is unknown. If a barrier is broken it may be better to just leave it and create a new one. Put the barrier into a broken state. This causes any active or future calls to to fail with the . Use this for example if one of the threads needs to abort, to avoid deadlocking the application. It may be preferable to simply create the barrier with a sensible timeout value to automatically guard against one of the threads going awry. The number of threads required to pass the barrier. The number of threads currently waiting in the barrier. A boolean that is if the barrier is in the broken state. This exception, a subclass of , is raised when the object is reset or broken."
    },
    {
        "link": "https://uwpce-pythoncert.github.io/PythonCertDevel/modules/ThreadingMultiprocessing.html",
        "document": "If your problem can be solved sequentially, consider the costs and benefits before going parallel.\n\nHow to actually DO threading and multiprocessing:\n\nThe mechanics: how do you use threads and/or processes¶\n\nPython provides the and modules to facility concurrency.\n\nThey have similar APIs – so you can use them in similar ways.\n\nStarting threads is relatively simple, but there are many potential issues.\n\nWe already talked about shared data, this can lead to a “race condition”.\n\nConsider the following code in: We can do better than this Break down the problem into parallelizable chunks, then add the results together:\n\nIn the last example we saw threads competing for access to stdout. Worse, if competing threads try to update the same value, we might get an unexpected race condition Race conditions occur when multiple statements need to execute atomically, but get interrupted midway\n\nSynchronization and Critical Sections are used to control race conditions But they introduce other potential problems… “A deadlock is a situation in which two or more competing actions are each waiting for the other to finish, and thus neither ever does.” When two trains approach each other at a crossing, both shall come to a full stop and neither shall start up again until the other has gone Two people meet in a narrow corridor, and each tries to be polite by moving aside to let the other pass, but they end up swaying from side to side without making any progress because they both repeatedly move the same way at the same time.\n\nLock objects allow threads to control access to a resource until they’re done with it This is known as mutual exclusion, often called “mutex”. A Lock has two states: locked and unlocked If multiple threads have access to the same Lock, they can police themselves by calling its and methods If a Lock is locked, .acquire will block until it becomes unlocked These threads will wait in line politely for access to the statements in f()\n\nLike an , but in reverse A Semaphore is given an initial counter value, defaulting to 1 Each call to decrements the counter, increments it If is called on a Semaphore with a counter of 0, it will block until the Semaphore counter is greater than 0. Useful for controlling the maximum number of threads allowed to access a resource simultaneously\n\nWe need a thread safe way of storing results from multiple threads of execution. That is provided by the Queue module. Queues allow multiple producers and multiple consumers to exchange data safely Size of the queue is managed with the maxsize kwarg It will block consumers if empty and block producers if full If maxsize is less than or equal to zero, the queue size is infinite\n\nTry running the code in It has a couple of tunable parameters: # the start of the integration # the end point of the integration # the number of steps to use in the integration # the number of threads to use What happens when you change the thread count? What thread count gives the maximum speed?"
    },
    {
        "link": "https://medium.com/@arjunprakash027/threading-vs-multiprocessing-in-python-a-comprehensive-guide-cae3ce0ca6c1",
        "document": "Concurrent programming plays a vital role in optimizing performance and improving efficiency in modern applications. Python offers two primary approaches for achieving concurrency: threading and multiprocessing. In this blog, we will delve into the world of threading and multiprocessing, exploring their differences, advantages, and best use cases.\n\nTo provide a practical understanding, we will demonstrate these concepts using a comprehensive example. We will design a image downloading task and measure its performance using both threading and multiprocessing. By examining the performance metrics, we can gain insights into when to leverage threading and multiprocessing for optimal results.\n\nConcurrency refers to the ability of an application to handle multiple tasks concurrently. Rather than executing tasks in a strictly sequential manner, concurrency allows for overlapping and interleaving of tasks. By utilizing context switching, the application can switch rapidly between different tasks, giving the impression of parallel execution.\n\nConcurrency is achieved using a single processing unit, such as a CPU. Unlike true parallel processing, where multiple tasks are executed simultaneously on separate cores, concurrency maximizes the utilization of a single core by rapidly switching between tasks. This approach significantly reduces the overall execution time of tasks.\n\nWhen employing concurrency, tasks are divided into smaller chunks or threads. These threads are not processed in parallel, but rather scheduled and switched between rapidly. The operating system or programming language manages the scheduling and context switching, allowing each task to make progress in a time-sliced manner.\n\nParallelism, on the other hand, involves true simultaneous execution of tasks across multiple processing units. Unlike concurrency, which utilizes smaller subtasks, parallel processing divides the tasks into independent units that can be executed in parallel. By leveraging multiple processing units, parallelism significantly increases the speed and throughput of an application.\n\nIn parallelism, the tasks are executed simultaneously, enabling the overlapping of CPU-intensive tasks and I/O tasks across different processes. This allows for maximum utilization of resources and efficient task execution. In concurrency, the overlapping occurs between the I/O tasks of one process and the CPU tasks of another process. While concurrency provides the illusion of simultaneous execution, it is not true parallelism.\n• Concurrency Use Case: Web Scraping Web scraping involves fetching data from multiple web pages simultaneously. By employing concurrency, you can make concurrent requests to different web pages using threads or asynchronous programming libraries like asyncio or gevent. This allows for efficient utilization of I/O resources and faster data retrieval.\n• Parallelism Use Case: Data Processing When dealing with large datasets, parallelism can significantly enhance data processing speed. For instance, if you need to perform complex calculations on each data point independently, you can distribute the workload across multiple CPU cores using multiprocessing. Each core can handle a portion of the data, leading to faster overall processing time.\n• Synchronization Overhead: Coordinating shared resources and managing thread safety introduces additional complexity and overhead. Ensuring proper synchronization between threads can be challenging and may lead to issues like deadlocks or race conditions.\n• Debugging Complexity: Debugging concurrent programs can be more challenging due to non-deterministic behavior caused by thread interleaving. Identifying and fixing issues related to thread synchronization or data sharing can be time-consuming.\n• Communication Overhead: Coordinating communication and data sharing between parallel processes can introduce additional complexity and overhead. Explicit mechanisms like interprocess communication (IPC) or shared memory need to be implemented, which can impact performance.\n• Load Imbalance: In parallel computing, load balancing becomes crucial to ensure that tasks are evenly distributed across processes. Load imbalance can lead to underutilization of some processes and inefficient resource utilization\n\nWe learnt about concurrency in previous section, Threading in Python allows for concurrent execution of tasks by utilizing multiple threads within a single process. Threads share the same memory space and can switch rapidly between tasks, giving the illusion of parallel execution. However, due to the Global Interpreter Lock (GIL) in Python, threading is more suitable for I/O-bound tasks, where threads can wait for I/O operations without blocking the entire process. This makes threading well-suited for achieving concurrency in Python applications.\n\nHow to use threading in python:\n\nDefine a function that must be executed using threads\n\nthreading.Thread() is used to create a thread taking our desired function as arguement\n\nthread.start() starts the thread and thread.join() stops the thread\n\nMultiprocessing in Python enables true parallelism by utilizing multiple processes that can run on separate CPU cores. Each process has its own memory space, allowing for independent execution of tasks. Multiprocessing is ideal for CPU-bound tasks, where the workload can be divided and executed in parallel across multiple cores. Unlike threading, multiprocessing can fully utilize multiple CPU cores and achieve significant speed improvements for parallel tasks.\n\nHow to use multiprocessing in python:\n\ndefine the function that must be executed parallely\n\nstart the process using multiprocessing.Process() method, it takes function name as arguement\n\nprocess.start() starts the process and process.join() stops the process\n\nIn this section, we will compare the download time of images using normal process, threading process, and multiprocessing process in Python. To achieve this, we will utilize the module for threading, the module for multiprocessing, and the module to benchmark the execution time.\n\nThe goal is to demonstrate the performance differences between the different approaches. By measuring the time it takes to download multiple images using each method, we can observe the impact of concurrency and parallelism on the overall execution time.\n\nThe provided code snippet begins by importing necessary modules ( , , , and )\n\ncreate a list that stores URLs for downloading images from the 'picsum.photos' service.\n\nThe function is defined to handle the downloading of images. It takes an parameter and uses the library to retrieve the image from the URL. If the download is successful (status code 200), the image is saved to the 'images' directory with a filename derived from the URL. If there is an error downloading the image, an error message is printed.\n\nThe function is defined to download images using a normal execution approach, where each image is downloaded sequentially. The function utilizes a loop to iterate over each image URL in the list and calls the function to download the image.\n\nThe execution time is measured using the module. The function from is used to record the start and end times of the execution. The difference between these two times represents the total execution time. The calculated execution time is then printed as \"Normal Execution Time\".\n\nRemember to have the function defined and available for use when executing the function.\n\nit takes about 265 seconds to download 400 images using normal method\n\nThe function is defined to download images using threading for concurrent execution. It creates multiple threads, where each thread is responsible for downloading an image from the list of image URLs.\n\nThe function begins by measuring the start time using . It initializes an empty list called to store the created threads.\n\nNext, a loop iterates over each image URL in the list. For each URL, a new thread is created using the class from the module. The parameter is set to the function, and the parameter is used to pass the image URL as an argument to the function. The created thread is then appended to the list.\n\nAfter creating all the threads, another loop starts each thread using the method.\n\nTo ensure that the main program waits for all the threads to finish their execution, another loop with is used.\n\nFinally, the end time is measured using , and the difference between the start and end times represents the total execution time. The calculated execution time is then printed as \"Threading Execution Time\".\n\nRemember to have the function defined and available for use when executing the function.\n\nit takes only 4.2 seconds to download using threads\n\nThe function utilizes Python's multiprocessing module to achieve parallel execution of image downloads. It creates multiple processes, each responsible for downloading an image from the provided list of image URLs. By leveraging multiprocessing, the function enables concurrent execution of the download tasks, improving overall performance.\n\nThe function starts by creating processes for each image URL using the Process class. These processes are stored in a list called . It then starts each process using the method and waits for them to finish using the method. The execution time is measured using and printed as \"Multiprocessing Execution Time.\"\n\nNote that the function needs to be defined and available for use when running the function.\n\nit takes 3.4 seconds to complete the download in multiprocess mode.\n\nUse cases for threading in Python:\n• Concurrent I/O Operations: Threading is useful when performing I/O-bound tasks, such as reading from or writing to files, making API requests, or interacting with databases. By using threads, you can overlap I/O operations and improve overall efficiency.\n• GUI Applications: Threading is commonly employed in graphical user interface (GUI) applications to keep the user interface responsive while performing background tasks. For example, you can use threads to update the user interface, handle user input, and perform computations simultaneously.\n• Network Operations: Threading is beneficial for network-related tasks, such as handling multiple client connections or implementing network servers. By assigning a separate thread to each connection or request, you can handle concurrent network operations efficiently.\n\nUse cases for multiprocessing in Python:\n• CPU-Intensive Tasks: Multiprocessing is ideal for computationally intensive tasks that consume a significant amount of CPU resources. By utilizing multiple processes, you can distribute the workload across multiple cores or CPUs, thereby improving performance.\n• Data Processing and Analysis: When dealing with large datasets or performing complex data processing tasks, multiprocessing can significantly accelerate the processing time. By dividing the data or computations into smaller chunks and processing them in parallel, you can achieve faster data processing and analysis.\n• Scientific Computing and Simulations: Multiprocessing is commonly used in scientific computing and simulations, such as numerical computations, simulations, and mathematical modeling. By leveraging multiple processes, you can exploit the parallelism inherent in these tasks and reduce the execution time.\n\nIn conclusion, understanding the concepts of threading and multiprocessing in Python is essential for developing efficient and concurrent applications. Threading allows for concurrent execution of tasks within a single process, making it suitable for I/O-bound operations and GUI applications. On the other hand, multiprocessing enables parallel execution of tasks across multiple processes, making it beneficial for CPU-bound tasks and data-intensive operations.\n\nBoth threading and multiprocessing have their advantages and considerations. Threading offers lightweight concurrency but can face limitations due to the Global Interpreter Lock (GIL). Multiprocessing, although requiring more system resources, provides true parallelism and is well-suited for CPU-intensive tasks.\n\nWhen choosing between threading and multiprocessing, it is crucial to assess the nature of the task, system requirements, and potential synchronization challenges. By understanding the strengths and drawbacks of each approach, developers can leverage threading and multiprocessing effectively to achieve optimal performance and responsiveness in their Python applications."
    },
    {
        "link": "https://stackoverflow.com/questions/3044580/multiprocessing-vs-threading-python",
        "document": "I am trying to understand the advantages of multiprocessing over threading . I know that multiprocessing gets around the Global Interpreter Lock, but what other advantages are there, and can threading not do the same thing?\n\nThreading's job is to enable applications to be responsive. Suppose you have a database connection and you need to respond to user input. Without threading, if the database connection is busy the application will not be able to respond to the user. By splitting off the database connection into a separate thread you can make the application more responsive. Also because both threads are in the same process, they can access the same data structures - good performance, plus a flexible software design. Note that due to the GIL the app isn't actually doing two things at once, but what we've done is put the resource lock on the database into a separate thread so that CPU time can be switched between it and the user interaction. CPU time gets rationed out between the threads. Multiprocessing is for times when you really do want more than one thing to be done at any given time. Suppose your application needs to connect to 6 databases and perform a complex matrix transformation on each dataset. Putting each job in a separate thread might help a little because when one connection is idle another one could get some CPU time, but the processing would not be done in parallel because the GIL means that you're only ever using the resources of one CPU. By putting each job in a Multiprocessing process, each can run on its own CPU and run at full efficiency.\n\nAs mentioned in the question, Multiprocessing in Python is the only real way to achieve true parallelism. Multithreading cannot achieve this because the GIL prevents threads from running in parallel. As a consequence, threading may not always be useful in Python, and in fact, may even result in worse performance depending on what you are trying to achieve. For example, if you are performing a CPU-bound task such as decompressing gzip files or 3D-rendering (anything CPU intensive) then threading may actually hinder your performance rather than help. In such a case, you would want to use Multiprocessing as only this method actually runs in parallel and will help distribute the weight of the task at hand. There could be some overhead to this since Multiprocessing involves copying the memory of a script into each subprocess which may cause issues for larger-sized applications. However, Multithreading becomes useful when your task is IO-bound. For example, if most of your task involves waiting on API-calls, you would use Multithreading because why not start up another request in another thread while you wait, rather than have your CPU sit idly by.\n• Multithreading is concurrent and is used for IO-bound tasks\n• Multiprocessing achieves true parallelism and is used for CPU-bound tasks\n\nOther answers have focused more on the multithreading vs multiprocessing aspect, but in python Global Interpreter Lock (GIL) has to be taken into account. When more number (say k) of threads are created, generally they will not increase the performance by k times, as it will still be running as a single threaded application. GIL is a global lock which locks everything out and allows only single thread execution utilizing only a single core. The performance does increase in places where C extensions like numpy, Network, I/O are being used, where a lot of background work is done and GIL is released. \n\n So when threading is used, there is only a single operating system level thread while python creates pseudo-threads which are completely managed by threading itself but are essentially running as a single process. Preemption takes place between these pseudo threads. If the CPU runs at maximum capacity, you may want to switch to multiprocessing.\n\n Now in case of self-contained instances of execution, you can instead opt for pool. But in case of overlapping data, where you may want processes communicating you should use .\n• Creation of a process is time-consuming and resource intensive.\n• Multiprocessing can be symmetric or asymmetric.\n• The multiprocessing library in Python uses separate memory space, multiple CPU cores, bypasses GIL limitations in CPython, child processes are killable (ex. function calls in program) and is much easier to use.\n• Some caveats of the module are a larger memory footprint and IPC’s a little more complicated with more overhead.\n• Creation of a thread is economical in both sense time and resource.\n• The multithreading library is lightweight, shares memory, responsible for responsive UI and is used well for I/O bound applications.\n• The module isn’t killable and is subject to the GIL.\n• Multiple threads live in the same process in the same space, each thread will do a specific task, have its own code, own stack memory, instruction pointer, and share heap memory.\n• If a thread has a memory leak it can damage the other threads and parent process. Example of Multi-threading and Multiprocessing using Python Python 3 has the facility of Launching parallel tasks. This makes our work easier. It has for thread pooling and Process pooling. The following gives an insight: import concurrent.futures import urllib.request URLS = ['http://www.foxnews.com/', 'http://www.cnn.com/', 'http://europe.wsj.com/', 'http://www.bbc.co.uk/', 'http://some-made-up-domain.com/'] # Retrieve a single page and report the URL and contents def load_url(url, timeout): with urllib.request.urlopen(url, timeout=timeout) as conn: return conn.read() # We can use a with statement to ensure threads are cleaned up promptly with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor: # Start the load operations and mark each future with its URL future_to_url = {executor.submit(load_url, url, 60): url for url in URLS} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: data = future.result() except Exception as exc: print('%r generated an exception: %s' % (url, exc)) else: print('%r page is %d bytes' % (url, len(data))) import concurrent.futures import math PRIMES = [ 112272535095293, 112582705942171, 112272535095293, 115280095190773, 115797848077099, 1099726899285419] def is_prime(n): if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True def main(): with concurrent.futures.ProcessPoolExecutor() as executor: for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)): print('%d is prime: %s' % (number, prime)) if __name__ == '__main__': main()\n\nThreads share the same memory space to guarantee that two threads don't share the same memory location so special precautions must be taken the CPython interpreter handles this using a mechanism called , or the Global Interpreter Lock what is GIL(Just I want to Clarify GIL it's repeated above)? In CPython, the global interpreter lock, or GIL, is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecodes at once. This lock is necessary mainly because CPython's memory management is not thread-safe. For the main question, we can compare using Use Cases, How? 1-Use Cases for Threading: in case of GUI programs threading can be used to make the application responsive For example, in a text editing program, one thread can take care of recording the user inputs, another can be responsible for displaying the text, a third can do spell-checking, and so on. Here, the program has to wait for user interaction. which is the biggest bottleneck. Another use case for threading is programs that are IO bound or network bound, such as web-scrapers. 2-Use Cases for Multiprocessing: Multiprocessing outshines threading in cases where the program is CPU intensive and doesn’t have to do any IO or user interaction. For More Details visit this link and link or you need in-depth knowledge for threading visit here for Multiprocessing visit here"
    }
]