[
    {
        "link": "https://scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/opengl-perspective-projection-matrix.html",
        "document": "In all OpenGL books and references, the perspective projection matrix used in OpenGL is defined as:\n\nWhat similarities does this matrix have with the matrix we studied in the previous chapter? It is important to remember that matrices in OpenGL are defined using a column-major order, as opposed to row-major order. In the lesson on Geometry, we explained that to transition from one order to the other, one can simply transpose the matrix. If we transpose the above matrix, we get:\n\nThis is the matrix we would use on Scratchapixel, as we use row vectors. However, in OpenGL, you would use the first matrix, as OpenGL uses column vectors by default, though this can be changed in OpenGL 4.x and modern real-time 3D graphics APIs such as Vulkan. Pay attention to the element in red (third row and fourth column). When we multiply a homogeneous point with this matrix, the point's \\(w\\) coordinate is multiplied by this element, and the value of \\(w\\) ends up being the projected point's \\(z\\) coordinate:\n\nOur mathematical expressions and equations are accurate, reflecting the correct formulas for the perspective projection matrix as used in OpenGL and its transformation upon transposition.\n\nIn summary, we understand that the matrix is correctly set up for the z-divide. Let's now examine how points are projected in OpenGL (Vulkan, Meta, Direct3D or WebGL). The principle remains the same as discussed in the previous chapter. A line is drawn from the camera's origin to the point \\(P\\) that we want to project, and the intersection of this line with the image plane determines the position of the projected point \\(P_s\\). While the setup mirrors that shown in figure 1 from the previous chapter, it's important to note that in OpenGL, the image plane is situated on the near clipping plane, as opposed to being precisely one unit away from the camera's origin.\n\nThe technique of using similar triangles, as employed in chapter 1, is applicable here as well. The triangles \\(\\Delta ABC\\) and \\(\\Delta DEF\\) are similar. Thus, we can express:\n\nBy substituting \\(AB\\) with \\(n\\) (the near clipping plane), \\(DE\\) with \\(P_z\\) (the z-coordinate of \\(P\\)), and \\(EF\\) with \\(P_y\\) (the y-coordinate of \\(P\\)), we can rewrite this equation as (equation 1):\n\nAs observed, the only difference from the equation in the previous chapter is the inclusion of \\(n\\) in the numerator. However, the principle of division by \\(P_z\\) remains unchanged (noting that since the camera is oriented along the negative z-axis, \\(P_z\\) is negative: \\(P_z < 0\\)). To maintain the y-coordinate of the projected point as positive, given that \\(P_y\\) is positive, we negate \\(P_z\\). Following the same logic, we derive the x-coordinate of the projected point with the following equation (equation 2):\n\nHaving determined the values for \\(P_s{}_x\\) and \\(P_s{}_y\\), we now need to elucidate how they correlate with the OpenGL perspective matrix. The purpose of a projection matrix is to remap the values projected onto the image plane to a unit cube (defined by minimum and maximum extents of \\((-1,-1,-1)\\) and \\((1,1,1)\\), respectively). Once the point \\(P\\) is projected onto the image plane, \\(P_s\\) is considered visible if its \\(x\\) and \\(y\\) coordinates fall within the range \\([left, right]\\) for \\(x\\) and \\([bottom, top]\\) for \\(y\\), as depicted in Figure 2. While we have previously discussed in the lesson 3D Viewing: the Pinhole Camera Model how the \\(left\\), \\(right\\), \\(bottom\\), and \\(top\\) coordinates are calculated, we will revisit this explanation in this chapter. These screen coordinates set the limits or boundaries on the image plane for visible points (all points contained in the viewing frustum and projected onto the image plane). Assuming \\(P_s{}_x\\) is visible, it can be expressed as:\n\nwhere \\(l\\) and \\(r\\) represent the left and right coordinates, respectively. Our objective is to remap \\(P_s{}_x\\) so that its final value resides within the range \\([-1,1]\\) (the dimensions of the unit cube along the \\(x\\)-axis). Reiterating the equations introduced in the previous lesson, let's start by subtracting \\(l\\) from all terms to rewrite the equation as:\n\nNormalizing the term on the right by dividing all terms of this formula by \\(r-l\\) yields:\n\nMultiplying all terms by 2 gives:\n\nSubtracting 1 from all terms results in:\n\nThis remaps the central term to the range \\([-1,1]\\), which was our goal, though the terms can be further rearranged:\n\nThese two terms are quite similar to the first two terms of the first row in the OpenGL perspective projection matrix. We are getting closer. If we replace \\(Ps_x\\) from the previous equation with equation 2, we get:\n\nWe can easily encode this equation in matrix form. If we replace the first and third coefficients of the matrix's first row with the first and second term of this formula, here is what we get:\n\nRemember, the OpenGL matrix uses column-major ordering, therefore, we will have to place the multiplication sign to the right of the matrix and the point coordinates in a column vector:\n\nAnd since \\(Ps_x\\) will be divided at the end of the process by \\(-P_z\\) when we convert \\(Ps\\) from homogeneous to Cartesian coordinates, we get:\n\nThis is the first coordinate of the projected point \\(Ps\\) computed using the OpenGL perspective matrix. The derivation is quite lengthy, and we will skip it for \\(Ps_y\\). However, if you follow the steps we used for \\(Ps_x\\), doing it yourself shouldn't be a problem. You just need to replace \\(l\\) and \\(r\\) with \\(b\\) and \\(t\\), and you end up with the following formula:\n\nWe can achieve this result with point-matrix multiplication if we replace the second and third coefficients of the matrix's second row with the first and second terms of this equation:\n\nComputing \\(Ps_y\\) using this matrix gives:\n\nand after the division by \\(-P_z\\):\n\nOur matrix works again. All that's left to do to complete it is find a way to remap the z-coordinate of the projected points to the range [-1,1]. We know that the x- and y-coordinates of \\(P\\) don't contribute to the calculation of the projected point's z-coordinate. Thus, the first and second coefficients of the matrix's third row, which would be multiplied by \\(P\\)'s x- and y-coordinates, are necessarily zero (in green). We are left with two coefficients, \\(A\\) and \\(B\\), in the matrix which are unknown (in red).\n\nIf we write the equation to compute \\(Ps_z\\) using this matrix, we get (remember that \\(Ps_z\\) is also divided by \\(Ps_w\\) when the point is converted from homogeneous to Cartesian coordinates, and that \\(P_w = 1\\)):\n\nWe need to find the values of A and B. Fortunately, we know that when \\(P_z\\) is on the near clipping plane, \\(Ps_z\\) needs to be remapped to -1, and when \\(P_z\\) is on the far clipping plane, \\(Ps_z\\) needs to be remapped to 1. Therefore, we need to replace \\(Ps_z\\) with \\(n\\) and \\(f\\) in the equation to get two new equations (note that the z-coordinate of all the points projected on the image plane is negative, but \\(n\\) and \\(f\\) are positive, therefore we will use \\(-n\\) and \\(-f\\) instead):\n\nAnd substitute B in equation 2 with this equation:\n\nNow that we have a solution for A, finding B is straightforward. We just replace A in equation 1 to find B:\n\nWe can replace the solutions we found for A and B in our matrix, and we finally get:\n\nwhich is the OpenGL perspective projection matrix.\n\nShould Z be in Range \\([-1,1]\\) or \\([0,1]\\)? This Sadly Matters\n\nWhether Z should be in the range \\([-1,1]\\) or \\([0,1]\\) is a valid question. So far, we've chosen to remap \\(z\\) to the range \\([-1,1]\\) and have provided the equations for that. Most perspective matrix code remaps Z to that range. However, many (if not most) real-time graphics APIs, such as OpenGL and Vulkan, expect the depth range to be within \\([0,1]\\). Technically, these graphics APIs allow you to define this range more or less as you like when you create your graphics pipeline. In Vulkan, you set and in the structure (which itself has a parameter where you set these depth bounds). In OpenGL, these values are set through .\n\nNow, you can achieve this in two ways. You can modify the original perspective projection matrix so that the Z-coordinate directly remaps to \\([0,1]\\).\n\nFrom which we can derive:\n\nAnother solution simply consists of using the original matrix and letting the real-time graphics API remap these values for you. This is possible in OpenGL because when points are transformed from NDC space to window space (or raster space, if you prefer), the \\(z\\) is remapped as follows:\n\nDon't worry too much about \\(x_w\\) and \\(y_w\\) here. We are only interested in \\(z_w\\) (by the way, \\(w\\) stands for window space here). In OpenGL, at this particular stage, the variables \\(n\\) and \\(f\\) are set to the and values that we spoke about earlier. So if those are 0 and 1 respectively, this is similar to \\(\\frac{1}{2} z_d + \\frac{1}{2}\\), which effectively remaps points in the range \\([-1,1]\\) to the range \\([0,1]\\).\n\nThings get a little more complicated in APIs such as Vulkan. XX\n\nThe remapping of the z-coordinate uniquely prioritizes points closer to the camera with greater numerical precision compared to points further away. As discussed in the previous chapter, this characteristic can lead to issues where the lack of numerical precision results in adjacent samples receiving identical depth values after projection onto the screen, despite having distinct z-coordinates in world space. This phenomenon, known as z-fighting, poses a challenge. Although the problem cannot be entirely eliminated—given the inherent limitations of precision in single-precision floating-point numbers—it can be mitigated by carefully adjusting the near and far clipping planes to align as closely as possible with the nearest and furthest objects visible in the scene. This rationale underlines the importance of precise clipping plane adjustments.\n\nThe Field of View and Image Aspect Ratio\n\nYou may have noticed that, so far, we haven't made any reference to the camera's field of view (FOV) and image aspect ratio. However, as mentioned in the previous chapter and the lesson on cameras (in the basic section), changing the FOV alters the extent of the scene viewed through the camera. Thus, the field of view and the image aspect ratio are somehow related to the projection process. We deliberately ignored this detail until now to stay focused on the OpenGL perspective projection matrix, which doesn't directly depend on the camera's field of view, but it does so indirectly. The construction of the matrix relies on six parameters: the left, right, bottom, and top coordinates, as well as the near and far clipping planes. The user provides the values for the near and far clipping planes, but how about the left, right, bottom, and top coordinates? What are these, where do they come from, and how do we calculate them? Observing Figures 2 and 5, you can see that these coordinates correspond to the lower-left and upper-right corners of the frustum front face, where the image of the 3D scene is projected.\n\nTo compute the top coordinate, we look at the right-angled triangle ABC. The angle subtended by AB and AC is half the FOV, and the adjacent side of the triangle is the value for the near-clipping plane. Using trigonometry, we can express this as:\n\nAnd since the bottom half of the camera is symmetrical to the upper half, we can state that:\n\nIn Figure 5, two scenarios are considered: the image can either be square or rectangular. For a square camera, it's straightforward: the left and bottom coordinates are the same, the right and top coordinates are also the same, and mirroring the bottom-left coordinates around the x- and y-axis gives the top-right coordinates. Therefore, if we compute the top coordinates, we can easily set the other three:\n\nFor a non-square camera, as shown in the right inside of figure 5, computing the coordinates becomes slightly more complicated. The bottom and top coordinates remain the same, but the left and right coordinates are scaled by the aspect ratio, defined as the image width over the image height. The general formulas for computing the left, right, and bottom coordinates are:\n\nThus, the camera's field of view and image aspect ratio are crucial in calculating the left, right, bottom, and top coordinates, which in turn are used in constructing the perspective projection matrix. This is how they indirectly influence how much of the scene is visible through the camera.\n\nTo test the OpenGL perspective projection matrix, we will reuse the code from the previous chapter. In the old fixed-function rendering pipeline, two functions, (part of the GLU library) and , were utilized to set the screen coordinates and the projection matrix. These functions are deprecated (since OpenGL 3.1) in the new programmable rendering pipeline, but we use them in this lesson to demonstrate their implementation based on what we have learned in this chapter. You can still emulate them in your CPU program if desired.\n\nSetting up the perspective projection matrix in OpenGL was achieved through a call to . This function accepted six arguments:\n\nThe implementation of this function is shown below (line 20). The function was used to set the screen coordinates, taking as arguments the angle of view, the image aspect ratio (image width over image height), and the clipping planes.\n\nIn OpenGL, the angle of view is defined as the vertical angle (hence the 'y' in the variable name). On Scratchapixel, we use the horizontal angle of view. An implementation of this function is provided below (line 8). The rest of the code remains unchanged. We first compute the screen coordinates, then the projection matrix. Next, we iterate over all the vertices of the teapot geometry, transform them from object/world space to camera space, and finally project them onto the screen using the perspective projection matrix. Remember, the matrix remaps the projected point to NDC space. Thus, as in the previous version of the code, visible points fall within the range [-1,1] in height and [-imageAspectRatio, imageAspectRatio] (or [-1,1] if the image is square) in width.\n\nWe noted in the first chapter that even if matrices are constructed differently (and appear different), they should always yield the same result: a point in 3D space should be projected to the same pixel location on the image. Comparing the results of projecting the teapot's vertices using the first matrix with those using the same camera settings (same field of view, image aspect ratio, near and far clipping planes) and the OpenGL perspective projection matrix produces identical images (see image below).\n\nThe source code of this program is available on Scratchapixel's GitHub repo."
    },
    {
        "link": "https://stackoverflow.com/questions/30376001/calculating-the-perspective-projection-matrix-according-to-the-view-plane",
        "document": "I think you created a bit of confusion by putting this question under the \"opengl\" tag. The problem is that in computer graphics, the term projection is not understood in a strictly mathematical sense.\n\nIn maths, a projection is defined (and the following is not the exact mathematical definiton, but just my own paraphrasing) as something which doesn't further change the results when applied twice. Think about it. When you project a point in 3d space to a 2d plane (which is still in that 3d space), each point's projection will end up on that plane. But points which already are on this plane aren't moving at all any more, so you can apply this as many times as you want without changing the outcome any further.\n\nThe classic \"projection\" matrices in computer graphics don't do this. They transfrom the space in a way that a general frustum is mapped to a cube (or cuboid). For that, you basically need all the parameters to describe the frustum, which typically is aspect ratio, field of view angle, and distances to near and far plane, as well as the projection direction and the center point (the latter two are typically implicitely defined by convention). For the general case, there are also the horizontal and vertical asymmetries components (think of it like \"lens shift\" with projectors). And all of that is what the typical projection matrix in computer graphics represents.\n\nTo construct such a matrix from the paramters you have given is not really possible, because you are lacking lots of parameters. Also - and I think this is kind of revealing - you have given a view plane. But the projection matrices discussed so far do not define a view plane - any plane parallel to the near or far plane and in front of the camera can be imagined as the viewing plane (behind the camere would also work, but the image would be mirrored), if you should need one. But in the strict sense, it would only be a \"view plane\" if all of the projected points would also end up on that plane - which the computer graphics perspective matrix explicitely does'nt do. It instead keeps their 3d distance information - which also means that the operation is invertible, while a classical mathematical projection typically isn't.\n\nFrom all of that, I simply guess that what you are looking for is a perspective projection from 3D space onto a 2D plane, as opposed to a perspective transformation used for computer graphics. And all parameters you need for that are just the view point and a plane. Note that this is exactly what you have givent: The projection center shall be the origin and and define the plane.\n\nSuch a projection can also be expressed in terms of a 4x4 homogenous matrix. There is one thing that is not defined in your question: the orientation of the normal. I'm assuming standard maths convention again and assume that the view plane is defined as . From using in that equation, we can get . So the projection matrix is just\n\nThere are a few properties of this matrix. There is a zero column, so it is not invertible. Also note that for every point you apply this to, the result (after division by resulting , of course) is just the same no matter what is - so every point on a line between the origin and will result in the same projected point - which is what a perspective projection is supposed to do. And finally note that , so for every point fulfilling the above plane equation, will result. In combination with the identity transform for the other dimensions, which just means that such a point is unchanged."
    },
    {
        "link": "https://scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/building-basic-perspective-projection-matrix.html",
        "document": "A note of caution. The matrix introduced in this section is distinct from the projection matrices utilized in APIs like OpenGL, Direct3D, Vulkan, Metal or WebGL, yet it effectively achieves the same outcome. From the lesson 3D Viewing: the Pinhole Camera Model, we learned to determine screen coordinates (left, right, top, and bottom) using the camera's near clipping plane and angle-of-view, based on the specifications of a physically based camera model. These coordinates were then used to ascertain if the projected points fell within the visible image frame. In the lesson Rasterization: a Practical Implementation, we explored remapping the projected points' coordinates to NDC (Normalized Device Coordinates) space, using the screen coordinates to avoid direct comparison with the projected point coordinates by standardizing them within the [-1,1] range.\n\nIn this discussion, our approach deviates slightly. We start by presuming screen coordinates of (-1,1) for both left and right, and similarly for bottom and top (assuming a square screen), aligning with the desired range for coordinate comparison. Instead of scaling the projected point coordinates by the angle-of-view to map them to NDC space, we'll adjust the projected point coordinates directly to account for the camera's field-of-view. Both methodologies ultimately yield the same result.\n\nReferencing our Geometry lesson, the equation for multiplying a point by a matrix is as follows:\n\nRecall, the projection of point P onto the image plane, denoted as P', is obtained by dividing P's x- and y-coordinates by the inverse of P's z-coordinate:\n\nNow, how can we achieve the computation of P' through a point-matrix multiplication process?\n\nFirst, the coordinates \\(x'\\), \\(y'\\), and \\(z'\\) (of point \\(P'\\)) in the equation above need to be set as \\(x\\), \\(y\\), and \\(-z\\) respectively, where \\(x\\), \\(y\\), and \\(z\\) are the coordinates of point \\(P\\) that we want to project. Setting \\(z'\\) to \\(-z\\) instead of just \\(z\\) is crucial because, in the transformation from world space to camera space, all points defined in the camera coordinate system and located in front of the camera have a negative \\(z\\)-value. This is due to cameras typically pointing down the negative \\(z\\)-axis (Figure 1). Consequently, we assign \\(z\\) to \\(z'\\) but invert its sign so that \\(z'\\) becomes positive:\n\nSuppose, during the point-matrix multiplication process, we could divide \\(x'\\), \\(y'\\), and \\(z'\\) by \\(-z\\), resulting in:\n\nThese equations compute the projected coordinates of point \\(P'\\) (without worrying about \\(z'\\) for the moment). The next question is whether it's possible to achieve this result through point-matrix multiplication and what the corresponding matrix would look like. Initially, we establish the coordinates \\(x'\\), \\(y'\\), and \\(z'\\) as \\(x\\), \\(y\\), and \\(-z\\) respectively, which can be achieved using an identity matrix with slight modification:\n\nThe multiplication involves a point with homogeneous coordinates, where the fourth coordinate, \\(w\\), is set to 1. To convert back to Cartesian coordinates, \\(x'\\), \\(y'\\), and \\(z'\\) need to be divided by \\(w'\\):\n\nIf \\(w'\\) equaled \\(-z\\), dividing \\(x'\\), \\(y'\\), and \\(z'\\) by \\(-z\\) would yield the desired result:\n\nCan the perspective projection matrix be adjusted to ensure the point-matrix multiplication sets \\(w'\\) to \\(-z\\)? Observing the equation for \\(w'\\) reveals:\n\nWith the point \\(P\\) \\(w\\)-coordinate equal to 1, this simplifies to:\n\nThe goal is for \\(w'\\) to equal \\(-z\\), achievable by setting the matrix coefficient \\(m_{23}\\) to \\(-1\\) and the others to 0, leading to:\n\nThus, for \\(w'\\) to equal \\(-z\\), the matrix coefficients \\(m_{03}\\), \\(m_{13}\\), \\(\\color{red}{m_{23}}\\), and \\(m_{33}\\) must be 0, 0, -1, and 0 respectively. Here is the updated perspective projection matrix:\n\nWhen utilizing the specialized projection matrix in point-matrix multiplication, the resultant expressions for \\(x'\\), \\(y'\\), \\(z'\\), and \\(w'\\) are:\n\nSubsequent division of all coordinates by \\(w'\\) converts the point's homogeneous coordinates back to Cartesian coordinates:\n\nThis process effectively achieves the desired outcome, aligning with the perspective projection principles. The next steps involve adjusting the projection matrix further to accommodate additional aspects of a realistic camera model, specifically:\n\n1. Remapping \\(z'\\) within the range \\([0,1]\\): This adjustment is crucial for depth calculations in a 3D scene, ensuring that objects are correctly rendered with respect to their distance from the camera. It utilizes the camera's near (\\(z_{near}\\)) and far (\\(z_{far}\\)) clipping planes to linearly interpolate \\(z'\\) values within this range. The transformation is generally achieved through additional matrix operations that scale and translate \\(z'\\) values accordingly.\n\n2. Incorporating the Camera's Angle of View: The field of view (FOV) parameter influences how much of the scene is visible through the camera, mimicking the effect of a pinhole camera model. This aspect is accounted for by adjusting the projection matrix to alter the scale of the projected points, thereby affecting the perceived FOV. The matrix adjustments typically involve scaling factors derived from the FOV, ensuring that the scene's spatial dimensions are correctly projected onto the 2D view space.\n\nThe normalization of the z-coordinate, scaling it between 0 and 1, is a critical step in the perspective projection process, as it ensures that objects are correctly rendered according to their distance from the camera. This scaling uses the camera's near and far clipping planes. The aim is to adjust the z-coordinate so that when a point lies on the near clipping plane, its transformed z-coordinate (\\(z'\\)) equals 0, and when it lies on the far clipping plane, \\(z'\\) equals 1. This is achieved by setting specific coefficients in the perspective projection matrix:\n\nTo fulfill the conditions for \\(z'\\) to be 0 at the near clipping plane and 1 at the far clipping plane, the coefficients for \\(m_{22}\\) and \\(m_{23}\\) are set as follows:\n\nrespectively, where \\(n\\) is the distance to the near clipping plane and \\(f\\) is the distance to the far clipping plane. These adjustments ensure that \\(z'\\) is correctly normalized across the specified range.\n\nWhen evaluating \\(z'\\) at the near and far clipping planes, with \\(m_{20}\\) and \\(m_{21}\\) set to 0, the results validate the effectiveness of these coefficients:\n• At the near clipping plane (\\(z = n\\)):\n• At the far clipping plane (\\(z = f\\)):\n\nThis demonstrates how the chosen coefficients correctly map \\(z\\) values at the near and far clipping planes to \\(z'\\) values of 0 and 1, respectively.\n\nThe resulting perspective projection matrix, incorporating these adjustments for \\(z'\\) normalization, is:\n\nThis matrix now not only projects points from 3D space to 2D space but also remaps their z-coordinates to a normalized range of [0,1], ensuring depth values are correctly interpreted for rendering.\n\nTo get a basic perspective projection matrix working, we need to account for the angle of view or field-of-view (FOV) of the camera. Changing the focal length of a zoom lens on a real camera alters the extent of the scene we see. We aim for our CG camera to function similarly.\n\nThe projection window size is [-1:1] in each dimension, meaning a projected point is visible if its x- and y-coordinates are within this range. Points outside this range are invisible and not drawn.\n\nNote that in our system, the screen window's maximum and minimum values remain unchanged, always in the range [-1:1], assuming the screen is square. When points' coordinates are within the range [-1,1], we say they are defined in NDC space.\n\nRemember from chapter 1, the goal of the perspective projection matrix is to project a point onto the screen and remap their coordinates to the range [-1,1] (or to NDC space).\n\nThe distance to the screen window from the eye position remains the same (equal to 1). However, when the FOV changes, the screen window should become larger or smaller accordingly (see figures 2 and 5). How do we reconcile this contradiction? Since we want the screen window size to remain fixed, we instead adjust the projected coordinates by scaling them up or down, testing them against the fixed borders of the screen window. Let's explore a few examples.\n\nImagine a point whose projected x-y coordinates are (1.2, 1.3). These coordinates are outside the range [-1:1], making the point invisible. If we scale them down by multiplying them by 0.7, the new, scaled coordinates become (0.84, 0.91), making the point visible as both coordinates are now within the range [-1:1]. This action is analogous to zooming out, which means decreasing the focal length on a zoom lens or increasing the FOV. Conversely, to achieve the opposite effect, multiply by a value greater than 1. For instance, a point with projected coordinates (-0.5, 0.3) scaled by 2.1 results in new coordinates (-1.05, 0.63). The y-coordinate remains within the range [-1:1], but the x-coordinate is now lower than -1, rendering the point invisible. This effect is akin to zooming in.\n\nTo scale the projected coordinates, we use the camera's field of view. The FOV intuitively controls how much of the scene is visible. For more information, see the lesson 3D Viewing: the Pinhole Camera Model.\n\nThe value of the field-of-view (FOV) is not utilized directly in calculations; instead, the tangent of the angle is used. In computer graphics literature, the FOV can be defined either as the entire angle or half of the angle subtended by the viewing cone. It is more intuitive to perceive the FOV as the angular extent of the visible scene rather than half of this angle, as illustrated in figures 3 and 5. However, to derive a value for scaling the projected coordinates, the FOV angle needs to be halved. This is why the FOV is sometimes expressed as the half-angle.\n\nThe rationale behind dividing the angle by two lies in focusing on the right triangle within the viewing cone. The angle between the hypotenuse and the adjacent side of the triangle, or the FOV half-angle, dictates the length of the triangle's opposite side. Altering this angle allows for the scaling of the image window's border. Utilizing the tangent of this angle scales our projected coordinates. Notably, at an FOV half-angle of 45 degrees (total FOV of 90 degrees), the tangent of the angle equals 1, leaving the coordinates unchanged when multiplied by this value. For FOV values less than 90 degrees, the tangent of the half-angle yields values smaller than 1, and for values greater than 90 degrees, it results in values greater than 1.\n\nHowever, the required effect is the inverse. Zooming in, which corresponds to a decrease in FOV, necessitates multiplying the projected point coordinates by a value greater than 1. Conversely, zooming out, which increases the FOV, requires multiplication by a value less than 1. Therefore, the reciprocal of the tangent, or one over the tangent of the FOV half-angle, is employed.\n\nThe final equation to compute the scaling factor for the projected point coordinates is as follows:\n\nWith this scaling factor, the complete version of our basic perspective projection matrix is:\n\nThis matrix accounts for the FOV by scaling the x and y coordinates of projected points, thus effectively incorporating the angle of view into the perspective projection process.\n\nAre There Different Ways of Building this Matrix?\n\nYes and no. Some renderers may implement the perspective projection matrix differently. For instance, OpenGL utilizes a function called to create perspective projection matrices. This function accepts arguments for the left, right, bottom, and top coordinates, in addition to the near and far clipping planes. Unlike our approach, OpenGL projects points onto the near-clipping plane, rather than onto a plane one unit away from the camera position. The matrix configuration might also appear slightly different.\n\nIt's important to pay attention to the convention used for vectors and matrices. The projected point can be represented as either a row or column vector. Additionally, it's crucial to verify whether the renderer employs a left- or right-handed coordinate system, as this could alter the sign of the matrix coefficients.\n\nDespite these variations, the fundamental concept of the perspective projection matrix remains consistent across all renderers. They invariably divide the x- and y-coordinates of a point by its z-coordinate. Ultimately, all matrices are designed to project the same points to the same pixel coordinates, regardless of the specific conventions or the matrix being used.\n\nWe will explore the construction of the OpenGL matrix in greater detail in the next chapter.\n\nTo validate our basic perspective projection matrix, we developed a simple program to project the vertices of a polygonal object (Newell's teapot) onto the image plane using the projection matrix crafted in this chapter. The program is straightforward. It employs a function to construct the perspective projection matrix based on the camera's near and far clipping planes and the camera field-of-view, specified in degrees. The teapot's vertices are stored in an array. Each point undergoes projection onto the image plane via straightforward point-matrix multiplication, initially transforming points from world or object space to camera space. The function facilitates the multiplication of a point by a matrix, highlighting the creation of the fourth component, \\(w\\), and dividing the resultant point's coordinates by \\(w\\) if \\(w\\) differs from 1—this stage marks the perspective or \\(z\\) divide. A point is deemed visible only if its projected \\(x\\) and \\(y\\) coordinates fall within the \\([-1:1]\\) interval, irrespective of the image's aspect ratio. Points outside this range are considered beyond the camera's screen boundaries. Visible points are then mapped to raster space, i.e., pixel coordinates, through a remapping from \\([-1:1]\\) to \\([0:1]\\), followed by scaling to the image size and rounding to the nearest integer to align with pixel coordinates.\n\nTesting the program with the teapot rendered in a commercial renderer using identical camera settings resulted in matching images, as expected. The teapot geometry and program files are detailed in the Source Code chapter at the end of this lesson.\n\nIn the upcoming chapter, we'll delve into the construction of the perspective projection matrix as utilized in OpenGL. While the foundational principles remain consistent, OpenGL diverges by projecting points onto the near clipping plane and remapping the projected point coordinates to Normalized Device Coordinates (NDC) space. This remapping leverages screen coordinates derived from the camera's near clipping plane and angle-of-view, culminating in a distinct matrix configuration.\n\nFollowing our exploration of the perspective projection matrix in OpenGL, we'll transition to discussing the orthographic projection matrix. Unlike its perspective counterpart, the orthographic projection matrix offers a different view of three-dimensional scenes by projecting objects onto the viewing plane without the depth distortion inherent in perspective projection. This makes it particularly useful for technical and engineering drawings where maintaining the true dimensions and proportions of objects is crucial."
    },
    {
        "link": "https://stackoverflow.com/questions/25351528/opengl-set-perspective-camera-distance-based-on-bounding-box-on-an-ellipsoidal-m",
        "document": "I have a perspective camera that i am using to look at a 3D scene consisting of the earth model. The earth model is ellipsoidal. I want to an overview on this model based on a certain bounding box on the model i.e. basically i want to do a simple 2D overview on this 3D model. I wanted to calculate the camera distance for this.\n\nI am actually doing this in open scene graph so i can get an osg::BoundingBox for the desired scene/area.\n\nI am using a very simple method to calculate the distance.\n\nThis calculation is all in world space but somehow it is not giving me the right distance. Am i calculating this wrong?"
    },
    {
        "link": "https://reddit.com/r/opengl/comments/17agy0n/question_about_near_plane_and_projection_plane",
        "document": "I have encountered some sources that said that a near plane is the same as a projection plane. This confuses me because I think they are distinct concepts. A projection plane is a plane where 3D points are projected onto. Its height and distance from the center of projection determine the FOV. On the other hand, a near plane and a far plane define the size of the view frustum. Only the objects between the near and far planes will be rendered. The distance of the near plane from the center of projection has no effect on the FOV. I have seen some online sources that use these two terms interchangeably.\n\nHave I understood correctly? If not, could you please correct me?\n\nDo the projection plane and the near plane refer to the same thing?"
    },
    {
        "link": "https://scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/opengl-perspective-projection-matrix.html",
        "document": "In all OpenGL books and references, the perspective projection matrix used in OpenGL is defined as:\n\nWhat similarities does this matrix have with the matrix we studied in the previous chapter? It is important to remember that matrices in OpenGL are defined using a column-major order, as opposed to row-major order. In the lesson on Geometry, we explained that to transition from one order to the other, one can simply transpose the matrix. If we transpose the above matrix, we get:\n\nThis is the matrix we would use on Scratchapixel, as we use row vectors. However, in OpenGL, you would use the first matrix, as OpenGL uses column vectors by default, though this can be changed in OpenGL 4.x and modern real-time 3D graphics APIs such as Vulkan. Pay attention to the element in red (third row and fourth column). When we multiply a homogeneous point with this matrix, the point's \\(w\\) coordinate is multiplied by this element, and the value of \\(w\\) ends up being the projected point's \\(z\\) coordinate:\n\nOur mathematical expressions and equations are accurate, reflecting the correct formulas for the perspective projection matrix as used in OpenGL and its transformation upon transposition.\n\nIn summary, we understand that the matrix is correctly set up for the z-divide. Let's now examine how points are projected in OpenGL (Vulkan, Meta, Direct3D or WebGL). The principle remains the same as discussed in the previous chapter. A line is drawn from the camera's origin to the point \\(P\\) that we want to project, and the intersection of this line with the image plane determines the position of the projected point \\(P_s\\). While the setup mirrors that shown in figure 1 from the previous chapter, it's important to note that in OpenGL, the image plane is situated on the near clipping plane, as opposed to being precisely one unit away from the camera's origin.\n\nThe technique of using similar triangles, as employed in chapter 1, is applicable here as well. The triangles \\(\\Delta ABC\\) and \\(\\Delta DEF\\) are similar. Thus, we can express:\n\nBy substituting \\(AB\\) with \\(n\\) (the near clipping plane), \\(DE\\) with \\(P_z\\) (the z-coordinate of \\(P\\)), and \\(EF\\) with \\(P_y\\) (the y-coordinate of \\(P\\)), we can rewrite this equation as (equation 1):\n\nAs observed, the only difference from the equation in the previous chapter is the inclusion of \\(n\\) in the numerator. However, the principle of division by \\(P_z\\) remains unchanged (noting that since the camera is oriented along the negative z-axis, \\(P_z\\) is negative: \\(P_z < 0\\)). To maintain the y-coordinate of the projected point as positive, given that \\(P_y\\) is positive, we negate \\(P_z\\). Following the same logic, we derive the x-coordinate of the projected point with the following equation (equation 2):\n\nHaving determined the values for \\(P_s{}_x\\) and \\(P_s{}_y\\), we now need to elucidate how they correlate with the OpenGL perspective matrix. The purpose of a projection matrix is to remap the values projected onto the image plane to a unit cube (defined by minimum and maximum extents of \\((-1,-1,-1)\\) and \\((1,1,1)\\), respectively). Once the point \\(P\\) is projected onto the image plane, \\(P_s\\) is considered visible if its \\(x\\) and \\(y\\) coordinates fall within the range \\([left, right]\\) for \\(x\\) and \\([bottom, top]\\) for \\(y\\), as depicted in Figure 2. While we have previously discussed in the lesson 3D Viewing: the Pinhole Camera Model how the \\(left\\), \\(right\\), \\(bottom\\), and \\(top\\) coordinates are calculated, we will revisit this explanation in this chapter. These screen coordinates set the limits or boundaries on the image plane for visible points (all points contained in the viewing frustum and projected onto the image plane). Assuming \\(P_s{}_x\\) is visible, it can be expressed as:\n\nwhere \\(l\\) and \\(r\\) represent the left and right coordinates, respectively. Our objective is to remap \\(P_s{}_x\\) so that its final value resides within the range \\([-1,1]\\) (the dimensions of the unit cube along the \\(x\\)-axis). Reiterating the equations introduced in the previous lesson, let's start by subtracting \\(l\\) from all terms to rewrite the equation as:\n\nNormalizing the term on the right by dividing all terms of this formula by \\(r-l\\) yields:\n\nMultiplying all terms by 2 gives:\n\nSubtracting 1 from all terms results in:\n\nThis remaps the central term to the range \\([-1,1]\\), which was our goal, though the terms can be further rearranged:\n\nThese two terms are quite similar to the first two terms of the first row in the OpenGL perspective projection matrix. We are getting closer. If we replace \\(Ps_x\\) from the previous equation with equation 2, we get:\n\nWe can easily encode this equation in matrix form. If we replace the first and third coefficients of the matrix's first row with the first and second term of this formula, here is what we get:\n\nRemember, the OpenGL matrix uses column-major ordering, therefore, we will have to place the multiplication sign to the right of the matrix and the point coordinates in a column vector:\n\nAnd since \\(Ps_x\\) will be divided at the end of the process by \\(-P_z\\) when we convert \\(Ps\\) from homogeneous to Cartesian coordinates, we get:\n\nThis is the first coordinate of the projected point \\(Ps\\) computed using the OpenGL perspective matrix. The derivation is quite lengthy, and we will skip it for \\(Ps_y\\). However, if you follow the steps we used for \\(Ps_x\\), doing it yourself shouldn't be a problem. You just need to replace \\(l\\) and \\(r\\) with \\(b\\) and \\(t\\), and you end up with the following formula:\n\nWe can achieve this result with point-matrix multiplication if we replace the second and third coefficients of the matrix's second row with the first and second terms of this equation:\n\nComputing \\(Ps_y\\) using this matrix gives:\n\nand after the division by \\(-P_z\\):\n\nOur matrix works again. All that's left to do to complete it is find a way to remap the z-coordinate of the projected points to the range [-1,1]. We know that the x- and y-coordinates of \\(P\\) don't contribute to the calculation of the projected point's z-coordinate. Thus, the first and second coefficients of the matrix's third row, which would be multiplied by \\(P\\)'s x- and y-coordinates, are necessarily zero (in green). We are left with two coefficients, \\(A\\) and \\(B\\), in the matrix which are unknown (in red).\n\nIf we write the equation to compute \\(Ps_z\\) using this matrix, we get (remember that \\(Ps_z\\) is also divided by \\(Ps_w\\) when the point is converted from homogeneous to Cartesian coordinates, and that \\(P_w = 1\\)):\n\nWe need to find the values of A and B. Fortunately, we know that when \\(P_z\\) is on the near clipping plane, \\(Ps_z\\) needs to be remapped to -1, and when \\(P_z\\) is on the far clipping plane, \\(Ps_z\\) needs to be remapped to 1. Therefore, we need to replace \\(Ps_z\\) with \\(n\\) and \\(f\\) in the equation to get two new equations (note that the z-coordinate of all the points projected on the image plane is negative, but \\(n\\) and \\(f\\) are positive, therefore we will use \\(-n\\) and \\(-f\\) instead):\n\nAnd substitute B in equation 2 with this equation:\n\nNow that we have a solution for A, finding B is straightforward. We just replace A in equation 1 to find B:\n\nWe can replace the solutions we found for A and B in our matrix, and we finally get:\n\nwhich is the OpenGL perspective projection matrix.\n\nShould Z be in Range \\([-1,1]\\) or \\([0,1]\\)? This Sadly Matters\n\nWhether Z should be in the range \\([-1,1]\\) or \\([0,1]\\) is a valid question. So far, we've chosen to remap \\(z\\) to the range \\([-1,1]\\) and have provided the equations for that. Most perspective matrix code remaps Z to that range. However, many (if not most) real-time graphics APIs, such as OpenGL and Vulkan, expect the depth range to be within \\([0,1]\\). Technically, these graphics APIs allow you to define this range more or less as you like when you create your graphics pipeline. In Vulkan, you set and in the structure (which itself has a parameter where you set these depth bounds). In OpenGL, these values are set through .\n\nNow, you can achieve this in two ways. You can modify the original perspective projection matrix so that the Z-coordinate directly remaps to \\([0,1]\\).\n\nFrom which we can derive:\n\nAnother solution simply consists of using the original matrix and letting the real-time graphics API remap these values for you. This is possible in OpenGL because when points are transformed from NDC space to window space (or raster space, if you prefer), the \\(z\\) is remapped as follows:\n\nDon't worry too much about \\(x_w\\) and \\(y_w\\) here. We are only interested in \\(z_w\\) (by the way, \\(w\\) stands for window space here). In OpenGL, at this particular stage, the variables \\(n\\) and \\(f\\) are set to the and values that we spoke about earlier. So if those are 0 and 1 respectively, this is similar to \\(\\frac{1}{2} z_d + \\frac{1}{2}\\), which effectively remaps points in the range \\([-1,1]\\) to the range \\([0,1]\\).\n\nThings get a little more complicated in APIs such as Vulkan. XX\n\nThe remapping of the z-coordinate uniquely prioritizes points closer to the camera with greater numerical precision compared to points further away. As discussed in the previous chapter, this characteristic can lead to issues where the lack of numerical precision results in adjacent samples receiving identical depth values after projection onto the screen, despite having distinct z-coordinates in world space. This phenomenon, known as z-fighting, poses a challenge. Although the problem cannot be entirely eliminated—given the inherent limitations of precision in single-precision floating-point numbers—it can be mitigated by carefully adjusting the near and far clipping planes to align as closely as possible with the nearest and furthest objects visible in the scene. This rationale underlines the importance of precise clipping plane adjustments.\n\nThe Field of View and Image Aspect Ratio\n\nYou may have noticed that, so far, we haven't made any reference to the camera's field of view (FOV) and image aspect ratio. However, as mentioned in the previous chapter and the lesson on cameras (in the basic section), changing the FOV alters the extent of the scene viewed through the camera. Thus, the field of view and the image aspect ratio are somehow related to the projection process. We deliberately ignored this detail until now to stay focused on the OpenGL perspective projection matrix, which doesn't directly depend on the camera's field of view, but it does so indirectly. The construction of the matrix relies on six parameters: the left, right, bottom, and top coordinates, as well as the near and far clipping planes. The user provides the values for the near and far clipping planes, but how about the left, right, bottom, and top coordinates? What are these, where do they come from, and how do we calculate them? Observing Figures 2 and 5, you can see that these coordinates correspond to the lower-left and upper-right corners of the frustum front face, where the image of the 3D scene is projected.\n\nTo compute the top coordinate, we look at the right-angled triangle ABC. The angle subtended by AB and AC is half the FOV, and the adjacent side of the triangle is the value for the near-clipping plane. Using trigonometry, we can express this as:\n\nAnd since the bottom half of the camera is symmetrical to the upper half, we can state that:\n\nIn Figure 5, two scenarios are considered: the image can either be square or rectangular. For a square camera, it's straightforward: the left and bottom coordinates are the same, the right and top coordinates are also the same, and mirroring the bottom-left coordinates around the x- and y-axis gives the top-right coordinates. Therefore, if we compute the top coordinates, we can easily set the other three:\n\nFor a non-square camera, as shown in the right inside of figure 5, computing the coordinates becomes slightly more complicated. The bottom and top coordinates remain the same, but the left and right coordinates are scaled by the aspect ratio, defined as the image width over the image height. The general formulas for computing the left, right, and bottom coordinates are:\n\nThus, the camera's field of view and image aspect ratio are crucial in calculating the left, right, bottom, and top coordinates, which in turn are used in constructing the perspective projection matrix. This is how they indirectly influence how much of the scene is visible through the camera.\n\nTo test the OpenGL perspective projection matrix, we will reuse the code from the previous chapter. In the old fixed-function rendering pipeline, two functions, (part of the GLU library) and , were utilized to set the screen coordinates and the projection matrix. These functions are deprecated (since OpenGL 3.1) in the new programmable rendering pipeline, but we use them in this lesson to demonstrate their implementation based on what we have learned in this chapter. You can still emulate them in your CPU program if desired.\n\nSetting up the perspective projection matrix in OpenGL was achieved through a call to . This function accepted six arguments:\n\nThe implementation of this function is shown below (line 20). The function was used to set the screen coordinates, taking as arguments the angle of view, the image aspect ratio (image width over image height), and the clipping planes.\n\nIn OpenGL, the angle of view is defined as the vertical angle (hence the 'y' in the variable name). On Scratchapixel, we use the horizontal angle of view. An implementation of this function is provided below (line 8). The rest of the code remains unchanged. We first compute the screen coordinates, then the projection matrix. Next, we iterate over all the vertices of the teapot geometry, transform them from object/world space to camera space, and finally project them onto the screen using the perspective projection matrix. Remember, the matrix remaps the projected point to NDC space. Thus, as in the previous version of the code, visible points fall within the range [-1,1] in height and [-imageAspectRatio, imageAspectRatio] (or [-1,1] if the image is square) in width.\n\nWe noted in the first chapter that even if matrices are constructed differently (and appear different), they should always yield the same result: a point in 3D space should be projected to the same pixel location on the image. Comparing the results of projecting the teapot's vertices using the first matrix with those using the same camera settings (same field of view, image aspect ratio, near and far clipping planes) and the OpenGL perspective projection matrix produces identical images (see image below).\n\nThe source code of this program is available on Scratchapixel's GitHub repo."
    },
    {
        "link": "https://khronos.org/opengl/wiki/Viewing_and_Transformations",
        "document": "How does the camera work in OpenGL?\n\nAs far as OpenGL is concerned, there is no camera. More specifically, the camera is always located at the eye space coordinate (0.0, 0.0, 0.0). To give the appearance of moving the camera, your OpenGL application must move the scene with the inverse of the camera transformation by placing it on the MODELVIEW matrix. This is commonly referred to as the viewing transformation.\n\nIn practice this is mathematically equivalent to a camera transformation but more efficient because model transformations and camera transformations are concatenated to a single matrix. As a result though, certain operations must be performed when the camera and only the camera is on the MODELVIEW matrix. For example to position a light source in world space it most be positioned while the viewing transformation and only the viewing transformation is applied to the MODELVIEW matrix.\n\nHow can I move my eye, or camera, in my scene?\n\nOpenGL doesn't provide an interface to do this using a camera model. However, the GLU library provides the gluLookAt() function, which takes an eye position, a position to look at, and an up vector, all in object space coordinates. This function computes the inverse camera transform according to its parameters and multiplies it onto the current matrix stack.\n\nWhere should my camera go, the ModelView or Projection matrix?\n\nThe GL_PROJECTION matrix should contain only the projection transformation calls it needs to transform eye space coordinates into clip coordinates.\n\nThe GL_MODELVIEW matrix, as its name implies, should contain modeling and viewing transformations, which transform object space coordinates into eye space coordinates. Remember to place the camera transformations on the GL_MODELVIEW matrix and never on the GL_PROJECTION matrix.\n\nThink of the projection matrix as describing the attributes of your camera, such as field of view, focal length, fish eye lens, etc. Think of the ModelView matrix as where you stand with the camera and the direction you point it.\n\nThe game dev FAQ has good information on these two matrices.\n\nRead Steve Baker's article on projection abuse. This article is highly recommended and well-written. It's helped several new OpenGL programmers.\n\nA simple method for zooming is to use a uniform scale on the ModelView matrix. However, this often results in clipping by the zNear and zFar clipping planes if the model is scaled too large.\n\nA better method is to restrict the width and height of the view volume in the Projection matrix.\n\nFor example, your program might maintain a zoom factor based on user input, which is a floating-point number. When set to a value of 1.0, no zooming takes place. Larger values result in greater zooming or a more restricted field of view, while smaller values cause the opposite to occur. Code to create this effect might look like:\n\nInstead of gluPerspective(), your application might use glFrustum(). This gets tricky, because the left, right, bottom, and top parameters, along with the zNear plane distance, also affect the field of view. Assuming you desire to keep a constant zNear plane distance (a reasonable assumption), glFrustum() code might look like this:\n\nGiven the current ModelView matrix, how can I determine the object-space location of the camera?\n\nThe \"camera\" or viewpoint is at (0., 0., 0.) in eye space. When you turn this into a vector [0 0 0 1] and multiply it by the inverse of the ModelView matrix, the resulting vector is the object-space location of the camera.\n\nOpenGL doesn't let you inquire (through a glGet* routine) the inverse of the ModelView matrix. You'll need to compute the inverse with your own code.\n\nHow do I make the camera \"orbit\" around a point in my scene?\n\nYou can simulate an orbit by translating/rotating the scene/object and leaving your camera in the same place. For example, to orbit an object placed somewhere on the Y axis, while continuously looking at the origin, you might do this:\n\nIf you insist on physically orbiting the camera position, you'll need to transform the current camera position vector before using it in your viewing transformations.\n\nIn either event, I recommend you investigate gluLookAt() (if you aren't using this routine already).\n\nHow can I automatically calculate a view that displays my entire model? (I know the bounding sphere and up vector.)\n\nThe following is from a posting by Dave Shreiner on setting up a basic viewing system:\n\nFirst, compute a bounding sphere for all objects in your scene. This should provide you with two bits of information: the center of the sphere (let ( c.x, c.y, c.z ) be that point) and its diameter (call it \"diam\").\n\nNext, choose a value for the zNear clipping plane. General guidelines are to choose something larger than, but close to 1.0. So, let's say you set\n\nStructure your matrix calls in this order (for an Orthographic projection):\n\nThis approach should center your objects in the middle of the window and stretch them to fit (i.e., its assuming that you're using a window with aspect ratio = 1.0). If your window isn't square, compute left, right, bottom, and top, as above, and put in the following logic before the call to glOrtho():\n\nThe above code should position the objects in your scene appropriately. If you intend to manipulate (i.e. rotate, etc.), you need to add a viewing transform to it.\n\nA typical viewing transform will go on the ModelView matrix and might look like this:\n\nThis is usually caused by incorrect transformations.\n\nAssuming you are using gluPerspective() on the Projection matrix stack with zNear and zFar as the third and fourth parameters, you need to set gluLookAt on the ModelView matrix stack, and pass parameters so your geometry falls between zNear and zFar.\n\nIt's usually best to experiment with a simple piece of code when you're trying to understand viewing transformations. Let's say you are trying to look at a unit sphere centered on the origin. You'll want to set up your transformations as follows:\n\nIt's important to note how the Projection and ModelView transforms work together.\n\nIn this example, the Projection transform sets up a 50.0-degree field of view with trigonometric functions to add a zoom factor sub-parameter, with an aspect ratio and zoom factor of 1.0. The zNear clipping plane is 3.0 units in front of the eye, and the zFar clipping plane is 7.0 units in front of the eye. This leaves a Z volume distance of 4.0 units, ample room for a unit sphere.\n\nThe ModelView transform sets the eye position at (0.0, 0.0, 5.0), and the look-at point is the origin in the center of our unit sphere. Note that the eye position is 5.0 units away from the look at point. This is important, because a distance of 5.0 units in front of the eye is in the middle of the Z volume that the Projection transform defines. If the gluLookAt() call had placed the eye at (0.0, 0.0, 1.0), it would produce a distance of 1.0 to the origin. This isn't long enough to include the sphere in the view volume, and it would be clipped by the zNear clipping plane.\n\nSimilarly, if you place the eye at (0.0, 0.0, 10.0), the distance of 10.0 to the look at point will result in the unit sphere being 10.0 units away from the eye and far behind the zFar clipping plane placed at 7.0 units.\n\nIf this has confused you, read up on transformations in the OpenGL red book or OpenGL Specification. After you understand object coordinate space, eye coordinate space, and clip coordinate space, the above should become clear. Also, experiment with small test programs. If you're having trouble getting the correct transforms in your main application project, it can be educational to write a small piece of code that tries to reproduce the problem with simpler geometry.\n\nHow do I get a specified point (XYZ) to appear at the center of the scene?\n\ngluLookAt() is the easiest way to do this. Simply set the X, Y, and Z values of your point as the fourth, fifth, and sixth parameters to gluLookAt().\n\nI put my gluLookAt() call on my Projection matrix and now fog, lighting, and texture mapping don't work correctly. What happened?\n\nOften this is caused by placing the transformation on the wrong matrix, look at question 3 for an explanation of this problem. However occasionally, and in particular for lighting, this can be caused because the lights were positioned when the wrong matrix is on the MODELVIEW matrix stack. When a light is positioned in eye space, i.e. relative to the eye, it should be positioned when an identity matrix is on the MODELVIEW stack. When a light is positioned in the world it should be positioned when the viewing matrix is on the MODELVIEW stack. When the light is positioned relative to an object under transformation it should be positioned when that object's model matrix has been multiplied with the viewing matrix on the MODELVIEW stack, remembering that it will have to be positioned before anything lit by it is rendered. If any light moves relative to the eye between frames it must be repositioned each frame using the appropriate matrix.\n\nStereo viewing is accomplished by presenting a different image to the left and right eyes of the viewer. These images must be appropriate for the viewers relationship to the display they are looking at, much moreso than a mono 3D image. In addition the method used is tied closely to the display technology being used. Some graphics systems and display devices support stereo viewing in hardware and support features like left and right framebuffers in addition to the front and back buffers of conventional double buffered systems. Other systems support stereo correctly when two viewports are created in specific screen regions and specific video mode is used to send these to the screen. In conjunction with these modes a viewer often wears glasses either shuttered or polarized to select the displayed image appropriate to each eye. However even without these graphics features a developer can generate stereo views using features like color filtering where colored filters select an image based on red or blue filters and draw left and right eye images to red and blue framebuffer components for example, or even more simply just have multiple systems or graphics cards (or even a single card) generate two entirely separate video signals, for which a separate left and right eye image is drawn. The video is then sent to the appropriate eye either using a display employing polarizing filters or a head mounted display or some other custom display operating on similar principals.\n\nFrom an OpenGL perspective, the requirements of stereo rendering are to use the appropriate setup to render to left and right eyes (be it color masks, separate contexts or different viewports) and then match the geometry of the OpenGL projection to the relationship of the viewer's left and right eyes with the display. The final OpenGL requirement is that the position of the eyes in the 'virtual' world must be given a pupil separation on the modelview stack, this separation would of course be a translation in eye space, but could be calculated in other equivalent ways.\n\nI can't get transformations to work. Where can I learn more about matrices?\n\nA thorough explanation of basic matrix math and linear algebra is beyond the scope of this FAQ. These concepts are taught in high school math classes in the United States.\n\nIf you understand the basics, but just get confused (a common problem even for the experienced!), read through Steve Baker's review of matrix concepts and his article on Euler angles.\n\nFor programming purposes, OpenGL matrices are 16-value arrays with base vectors laid out contiguously in memory. The translation components occupy the 13th, 14th, and 15th elements of the 16-element matrix.\n\nColumn-major versus row-major is purely a notational convention. Note that post-multiplying with column-major matrices produces the same result as pre-multiplying with row-major matrices. The OpenGL Specification and the OpenGL Reference Manual both use column-major notation. You can use any notation, as long as it's clearly stated.\n\nSadly, the use of column-major format in the spec and blue book has resulted in endless confusion in the OpenGL programming community. Column-major notation suggests that matrices are not laid out in memory as a programmer would expect.\n\nA summary of Usenet postings on the subject can be found here.\n\nThe short answer: Anything you want them to be.\n\nDepending on the contents of your geometry database, it may be convenient for your application to treat one OpenGL coordinate unit as being equal to one millimeter or one parsec or anything in between (or larger or smaller).\n\nOpenGL also lets you specify your geometry with coordinates of differing values. For example, you may find it convenient to model an airplane's controls in centimeters, its fuselage in meters, and a world to fly around in kilometers. OpenGL's ModelView matrix can then scale these different coordinate systems into the same eye coordinate space.\n\nIt's the application's responsibility to ensure that the Projection and ModelView matrices are constructed to provide an image that keeps the viewer at an appropriate distance, with an appropriate field of view, and keeps the zNear and zFar clipping planes at an appropriate range. An application that displays molecules in micron scale, for example, would probably not want to place the viewer at a distance of 10 feet with a 60 degree field of view.\n\nHow are coordinates transformed? What are the different coordinate spaces?\n\nObject Coordinates are transformed by the ModelView matrix to produce Eye Coordinates.\n\nEye Coordinates are transformed by the Projection matrix to produce Clip Coordinates.\n\nClip Coordinate X, Y, and Z are divided by Clip Coordinate W to produce Normalized Device Coordinates.\n\nNormalized Device Coordinates are scaled and translated by the viewport parameters to produce Window Coordinates.\n\nObject coordinates are the raw coordinates you submit to OpenGL with a call to glVertex*() or glVertexPointer(). They represent the coordinates of your object or other geometry you want to render.\n\nMany programmers use a World Coordinate system. Objects are often modeled in one coordinate system, then scaled, translated, and rotated into the world you're constructing. World Coordinates result from transforming Object Coordinates by the modelling transforms stored in the ModelView matrix. However, OpenGL has no concept of World Coordinates. World Coordinates are purely an application construct.\n\nEye Coordinates result from transforming Object Coordinates by the ModelView matrix. The ModelView matrix contains both modelling and viewing transformations that place the viewer at the origin with the view direction aligned with the negative Z axis.\n\nClip Coordinates result from transforming Eye Coordinates by the Projection matrix. Clip Coordinate space ranges from -Wc to Wc in all three axes, where Wc is the Clip Coordinate W value. OpenGL clips all coordinates outside this range.\n\nPerspective division performed on the Clip Coordinates produces Normalized Device Coordinates, ranging from -1 to 1 in all three axes.\n\nWindow Coordinates result from scaling and translating Normalized Device Coordinates by the viewport. The parameters to glViewport() and glDepthRange() control this transformation. With the viewport, you can map the Normalized Device Coordinate cube to any location in your window and depth buffer.\n\nFor more information, see the OpenGL Specification, Figure 2.6.\n\nHow do I transform only one object in my scene or give each object its own transform?\n\nOpenGL provides matrix stacks specifically for this purpose. In this case, use the ModelView matrix stack.\n\nA typical OpenGL application first sets the matrix mode with a call to glMatrixMode(GL_MODELVIEW) and loads a viewing transform, perhaps with a call to gluLookAt(). More information is available on gluLookAt().\n\nThen the code renders each object in the scene with its own transformation by wrapping the rendering with calls to glPushMatrix() and glPopMatrix(). For example:\n\nThe above code renders a cylinder rotated 90 degrees around the X-axis. The ModelView matrix is restored to its previous value after the glPopMatrix() call. Similar call sequences can render subsequent objects in the scene.\n\nHow do I draw 2D controls over my 3D rendering?\n\nThe basic strategy is to set up a 2D projection for drawing controls. You can do this either on top of your 3D rendering or in overlay planes. If you do so on top of a 3D rendering, you'll need to redraw the controls at the end of every frame (immediately before swapping buffers). If you draw into the overlay planes, you only need to redraw the controls if you're updating them.\n\nTo set up a 2D projection, you need to change the Projection matrix. Normally, it's convenient to set up the projection so one world coordinate unit is equal to one screen pixel, as follows:\n\ngluOrtho2D() sets up a Z range of -1 to 1, so you need to use one of the glVertex2*() functions to ensure your geometry isn't clipped by the zNear or zFar clipping planes.\n\nNormally, the ModelView matrix is set to the identity when drawing 2D controls, though you may find it convenient to do otherwise (for example, you can draw repeated controls with interleaved translation matrices).\n\nIf exact pixelization is required, you might want to put a small translation in the ModelView matrix, as shown below:\n\nIf you're drawing on top of a 3D-depth buffered image, you'll need to somehow disable depth testing while drawing your 2D geometry. You can do this by calling glDisable(GL_DEPTH_TEST) or glDepthFunc (GL_ALWAYS). Depending on your application, you might also simply clear the depth buffer before starting the 2D rendering. Finally, drawing all 2D geometry with a minimum Z coordinate is also a solution.\n\nAfter the 2D projection is established as above, you can render normal OpenGL primitives to the screen, specifying their coordinates with XY pixel addresses (using OpenGL-centric screen coordinates, with (0,0) in the lower left).\n\nHow do I bypass OpenGL matrix transformations and send 2D coordinates directly for rasterization?\n\nThere isn't a mode switch to disable OpenGL matrix transformations. However, if you set either or both matrices to the identity with a glLoadIdentity() call, typical OpenGL implementations are intelligent enough to know that an identity transformation is a no-op and will act accordingly.\n\nMore detailed information on using OpenGL as a rasterization-only API is in the OpenGL Game Developer’s FAQ.\n\nWhat are the pros and cons of using absolute versus relative coordinates?\n\nSome OpenGL applications may need to render the same object in multiple locations in a single scene. OpenGL lets you do this two ways:\n• Use “absolute coordinates\". Maintain multiple copies of each object, each with its own unique set of vertices. You don't need to change the ModelView matrix to render the object at the desired location.\n• Use “relative coordinates\". Keep only one copy of the object, and render it multiple times by pushing the ModelView matrix stack, setting the desired transform, sending the geometry, and popping the stack. Repeat these steps for each object.\n\nIn general, frequent changes to state, such as to the ModelView matrix, can negatively impact your application’s performance. OpenGL can process your geometry faster if you don't wrap each individual primitive in a lot of changes to the ModelView matrix.\n\nHowever, sometimes you need to weigh this against the memory savings of replicating geometry. Let's say you define a doorknob with high approximation, such as 200 or 300 triangles, and you're modeling a house with 50 doors in it, all of which have the same doorknob. It's probably preferable to use a single doorknob display list, with multiple unique transform matrices, rather than use absolute coordinates with 10-15K triangles in memory.\n\nAs with many computing issues, it's a trade-off between processing time and memory that you'll need to make on a case-by-case basis.\n\nHow can I draw more than one view of the same scene?\n\nYou can draw two views into the same window by using the glViewport() call. Set glViewport() to the area that you want the first view, set your scene’s view, and render. Then set glViewport() to the area for the second view, again set your scene’s view, and render.\n\nYou need to be aware that some operations don't pay attention to the glViewport, such as SwapBuffers and glClear(). SwapBuffers always swaps the entire window. However, you can restrain glClear() to a rectangular window by using the scissor rectangle.\n\nYour application might only allow different views in separate windows. If so, you need to perform a MakeCurrent operation between the two renderings. If the two windows share a context, you need to change the scene’s view as described above. This might not be necessary if your application uses separate contexts for each window.\n\nHow do I transform my objects around a fixed coordinate system rather than the object's local coordinate system?\n\nIf you rotate an object around its Y-axis, you'll find that the X- and Z-axes rotate with the object. A subsequent rotation around one of these axes rotates around the newly transformed axis and not the original axis. It's often desirable to perform transformations in a fixed coordinate system rather than the object’s local coordinate system.\n\nThe OpenGL Game Developer’s FAQ contains information on using quaternions to store rotations, which may be useful in solving this problem.\n\nThe root cause of the problem is that OpenGL matrix operations postmultiply onto the matrix stack, thus causing transformations to occur in object space. To affect screen space transformations, you need to premultiply. OpenGL doesn't provide a mode switch for the order of matrix multiplication, so you need to premultiply by hand. An application might implement this by retrieving the current matrix after each frame. The application multiplies new transformations for the next frame on top of an identity matrix and multiplies the accumulated current transformations (from the last frame) onto those transformations using glMultMatrix().\n\nYou need to be aware that retrieving the ModelView matrix once per frame might have a detrimental impact on your application’s performance. However, you need to benchmark this operation, because the performance will vary from one implementation to the next.\n\nWhat are the pros and cons of using glFrustum() versus gluPerspective()? Why would I want to use one over the other?\n\nglFrustum() and gluPerspective() both produce perspective projection matrices that you can use to transform from eye coordinate space to clip coordinate space. The primary difference between the two is that glFrustum() is more general and allows off-axis projections, while gluPerspective() only produces symmetrical (on-axis) projections. Indeed, you can use glFrustum() to implement gluPerspective(). However, aside from the layering of function calls that is a natural part of the GLU interface, there is no performance advantage to using matrices generated by glFrustum() over gluPerspective().\n\nSince glFrustum() is more general than gluPerspective(), you can use it in cases when gluPerspective() can't be used. Some examples include projection shadows, tiled renderings, and stereo views.\n\nTiled rendering uses multiple off-axis projections to render different sections of a scene. The results are assembled into one large image array to produce the final image. This is often necessary when the desired dimensions of the final rendering exceed the OpenGL implementation's maximum viewport size.\n\nIn a stereo view, two renderings of the same scene are done with the view location slightly shifted. Since the view axis is right between the “eyes”, each view must use a slightly off-axis projection to either side to achieve correct visual results.\n\nHow can I make a call to glFrustum() that matches my call to gluPerspective()?\n\nThe field of view (fov) of your glFrustum() call is:\n\nSince bottom == -top for the symmetrical projection that gluPerspective() produces, then:\n\nNote: fov must be in radians for the above formula to work with the C math library. If you have computer your fov in degrees (as in the call to gluPerspective()), then calculate top as follows:\n\nThe left and right parameters are simply functions of the top, bottom, and aspect:\n\nThe OpenGL Reference Manual (where do I get this?) shows the matrices produced by both functions.\n\nThis question usually means, \"How do I draw a quad that fills the entire OpenGL viewport?\" There are many ways to do this.\n\nThe most straightforward method is to set the desired color, set both the Projection and ModelView matrices to the identity, and call glRectf() or draw an equivalent GL_QUADS primitive. Your rectangle or quad's Z value should be in the range of –1.0 to 1.0, with –1.0 mapping to the zNear clipping plane, and 1.0 to the zFar clipping plane.\n\nAs an example, here's how to draw a full-screen quad at the zNear clipping plane:\n\nYour application might want the quad to have a maximum Z value, in which case 1 should be used for the Z value instead of -1.\n\nWhen painting a full-screen quad, it might be useful to mask off some buffers so that only specified buffers are touched. For example, you might mask off the color buffer and set the depth function to GL_ALWAYS, so only the depth buffer is painted. Also, you can set masks to allow the stencil buffer to be set or any combination of buffers.\n\nHow can I find the screen coordinates for a given object-space coordinate?\n\nYou can use the GLU library gluProject() utility routine if you only need to find it for a few vertices. For a large number of coordinates, it can be more efficient to use the Feedback mechanism.\n\nTo use gluProject(), you'll need to provide the ModelView matrix, projection matrix, viewport, and input object space coordinates. Screen space coordinates are returned for X, Y, and Z, with Z being normalized (0 <= Z <= 1).\n\nHow can I find the object-space coordinates for a pixel on the screen?\n\nThe GLU library provides the gluUnProject() function for this purpose.\n\nYou'll need to read the depth buffer to obtain the input screen coordinate Z value at the X,Y location of interest. This can be coded as follows:\n\nNote that x and y are OpenGL-centric with (0,0) in the lower-left corner.\n\nYou'll need to provide the screen space X, Y, and Z values as input to gluUnProject() with the ModelView matrix, Projection matrix, and viewport that were current at the time the specific pixel of interest was rendered.\n\nHow do I find the coordinates of a vertex transformed only by the ModelView matrix?\n\nIt's often useful to obtain the eye coordinate space value of a vertex (i.e., the object space vertex transformed by the ModelView matrix). You can obtain this by retrieving the current ModelView matrix and performing simple vector / matrix multiplication.\n\nHow do I get the active MODELVIEW or PROJECTION matrices?\n\nHow do I calculate the object-space distance from the viewer to a given point?\n\nTransform the point into eye-coordinate space by multiplying it by the ModelView matrix. Then simply calculate its distance from the origin. (If this doesn't work, you may have incorrectly placed the view transform on the Projection matrix stack.)\n\nAs with any OpenGL call, you must have a context current with a window or drawable in order for glGet*() function calls to work.\n\nHow do I keep my aspect ratio correct after a window resize?\n\nIt depends on how you are setting your projection matrix. In any case, you'll need to know the new dimensions (width and height) of your window. How to obtain these depends on which platform you're using. In GLUT, for example, the dimensions are passed as parameters to the reshape function callback.\n\nThe following assumes you're maintaining a viewport that's the same size as your window. If you are not, substitute viewportWidth and viewportHeight for windowWidth and windowHeight.\n\nIf you're using gluPerspective() to set your Projection matrix, the second parameter controls the aspect ratio in which can be implemented into the first parameter using trigonometric math to re-calculate the FOV changing the aspect direction to extend vertically instead of horizontal as it follows:\n\nFor best results, you might want to use conditional aspect to switch direction if it goes below a specified ratio:\n\nWhen your program catches a window resize, you'll need to change your Projection matrix as follows:\n\nIf you're using glFrustum(), the aspect ratio varies with the width of the view volume to the height of the view volume. You might maintain a 1:1 aspect ratio with the following window resize code:\n\nglOrtho() and gluOrtho2D() are similar to glFrustum().\n\nOpenGL doesn't have a mode switch to change from right- to left-handed coordinates. However, you can easily obtain a left-handed coordinate system by multiplying a negative Z scale onto the ModelView matrix. For example:\n\nHow can I transform an object so that it points at or follows another object or point in my scene?\n\nYou need to construct a matrix that transforms from your object's local coordinate system into a coordinate system that faces in the desired direction. See this example code to see how this type of matrix is created.\n\nIf you merely want to render an object so that it always faces the viewer, you might consider simply rendering it in eye-coordinate space with the ModelView matrix set to the identity.\n\nHow can I transform an object with a given yaw, pitch, and roll?\n\nThe upper left 3x3 portion of a transformation matrix is composed of the new X, Y, and Z axes of the post-transformation coordinate space.\n\nIf the new transform is a roll, compute new local Y and X axes by rotating them \"roll\" degrees around the local Z axis. Do similar calculations if the transform is a pitch or yaw. Then simply construct your transformation matrix by inserting the new local X, Y, and Z axes into the upper left 3x3 portion of an identity matrix. This matrix can be passed as a parameter to glMultMatrix().\n\nFurther rotations should be computed around the new local axes. This will inevitably require rotation about an arbitrary axis, which can be confusing to inexperienced 3D programmers. This is a basic concept in linear algebra.\n\nMany programmers apply all three transformations -- yaw, pitch, and roll -- at once as successive glRotate() calls about the X, Y, and Z axes. This has the disadvantage of creating gimbal lock, in which the result depends on the order of glRotate() calls.\n\nRender your scene twice, once as it is reflected in the mirror, then once from the normal (non-reflected) view. Example code demonstrates this technique.\n\nFor axis-aligned mirrors, such as a mirror on the YZ plane, the reflected scene can be rendered with a simple scale and translate. Scale by -1.0 in the axis corresponding to the mirror's normal, and translate by twice the mirror's distance from the origin. Rendering the scene with these transforms in place will yield the scene reflected in the mirror. Use the matrix stack to restore the view transform to its previous value.\n\nNext, clear the depth buffer with a call to glClear(GL_DEPTH_BUFFER_BIT). Then render the mirror. For a perfectly reflecting mirror, render into the depth buffer only. Real mirrors are not perfect reflectors, as they absorb some light. To create this effect, use blending to render a black mirror with an alpha of 0.05. glBlendFunc(GL_SRC_ALPHA,GL_ONE_MINUS_SRC_ALPHA) is a good blending function for this purpose.\n\nFinally, render the non-reflected scene. Since the entire reflected scene exists in the color buffer, and not just the portion of the reflected scene in the mirror, you will need to touch all pixels to overwrite areas of the reflected scene that should not be visible.\n\nHow can I do my own perspective scaling?\n\nOpenGL multiplies your coordinates by the ModelView matrix, then by the Projection matrix to get clip coordinates. It then performs the perspective divide to obtain normalized device coordinates. It's the perspective division step that creates a perspective rendering, with geometry in the distance appearing smaller than the geometry in the foreground. The perspective division stage is accomplished by dividing your XYZ clipping coordinate values by the clipping coordinate W value, such as:\n\nTo do your own perspective correction, you need to obtain the clipping coordinate W value. The feedback buffer provides homogenous coordinates with XYZ in device coordinates and W in clip coordinates. You might also glGetFloatv(GL_CURRENT_RASTER_POSITION,…) and the W value will again be in clipping coordinates, while XYZ are in device coordinates."
    },
    {
        "link": "https://scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/building-basic-perspective-projection-matrix.html",
        "document": "A note of caution. The matrix introduced in this section is distinct from the projection matrices utilized in APIs like OpenGL, Direct3D, Vulkan, Metal or WebGL, yet it effectively achieves the same outcome. From the lesson 3D Viewing: the Pinhole Camera Model, we learned to determine screen coordinates (left, right, top, and bottom) using the camera's near clipping plane and angle-of-view, based on the specifications of a physically based camera model. These coordinates were then used to ascertain if the projected points fell within the visible image frame. In the lesson Rasterization: a Practical Implementation, we explored remapping the projected points' coordinates to NDC (Normalized Device Coordinates) space, using the screen coordinates to avoid direct comparison with the projected point coordinates by standardizing them within the [-1,1] range.\n\nIn this discussion, our approach deviates slightly. We start by presuming screen coordinates of (-1,1) for both left and right, and similarly for bottom and top (assuming a square screen), aligning with the desired range for coordinate comparison. Instead of scaling the projected point coordinates by the angle-of-view to map them to NDC space, we'll adjust the projected point coordinates directly to account for the camera's field-of-view. Both methodologies ultimately yield the same result.\n\nReferencing our Geometry lesson, the equation for multiplying a point by a matrix is as follows:\n\nRecall, the projection of point P onto the image plane, denoted as P', is obtained by dividing P's x- and y-coordinates by the inverse of P's z-coordinate:\n\nNow, how can we achieve the computation of P' through a point-matrix multiplication process?\n\nFirst, the coordinates \\(x'\\), \\(y'\\), and \\(z'\\) (of point \\(P'\\)) in the equation above need to be set as \\(x\\), \\(y\\), and \\(-z\\) respectively, where \\(x\\), \\(y\\), and \\(z\\) are the coordinates of point \\(P\\) that we want to project. Setting \\(z'\\) to \\(-z\\) instead of just \\(z\\) is crucial because, in the transformation from world space to camera space, all points defined in the camera coordinate system and located in front of the camera have a negative \\(z\\)-value. This is due to cameras typically pointing down the negative \\(z\\)-axis (Figure 1). Consequently, we assign \\(z\\) to \\(z'\\) but invert its sign so that \\(z'\\) becomes positive:\n\nSuppose, during the point-matrix multiplication process, we could divide \\(x'\\), \\(y'\\), and \\(z'\\) by \\(-z\\), resulting in:\n\nThese equations compute the projected coordinates of point \\(P'\\) (without worrying about \\(z'\\) for the moment). The next question is whether it's possible to achieve this result through point-matrix multiplication and what the corresponding matrix would look like. Initially, we establish the coordinates \\(x'\\), \\(y'\\), and \\(z'\\) as \\(x\\), \\(y\\), and \\(-z\\) respectively, which can be achieved using an identity matrix with slight modification:\n\nThe multiplication involves a point with homogeneous coordinates, where the fourth coordinate, \\(w\\), is set to 1. To convert back to Cartesian coordinates, \\(x'\\), \\(y'\\), and \\(z'\\) need to be divided by \\(w'\\):\n\nIf \\(w'\\) equaled \\(-z\\), dividing \\(x'\\), \\(y'\\), and \\(z'\\) by \\(-z\\) would yield the desired result:\n\nCan the perspective projection matrix be adjusted to ensure the point-matrix multiplication sets \\(w'\\) to \\(-z\\)? Observing the equation for \\(w'\\) reveals:\n\nWith the point \\(P\\) \\(w\\)-coordinate equal to 1, this simplifies to:\n\nThe goal is for \\(w'\\) to equal \\(-z\\), achievable by setting the matrix coefficient \\(m_{23}\\) to \\(-1\\) and the others to 0, leading to:\n\nThus, for \\(w'\\) to equal \\(-z\\), the matrix coefficients \\(m_{03}\\), \\(m_{13}\\), \\(\\color{red}{m_{23}}\\), and \\(m_{33}\\) must be 0, 0, -1, and 0 respectively. Here is the updated perspective projection matrix:\n\nWhen utilizing the specialized projection matrix in point-matrix multiplication, the resultant expressions for \\(x'\\), \\(y'\\), \\(z'\\), and \\(w'\\) are:\n\nSubsequent division of all coordinates by \\(w'\\) converts the point's homogeneous coordinates back to Cartesian coordinates:\n\nThis process effectively achieves the desired outcome, aligning with the perspective projection principles. The next steps involve adjusting the projection matrix further to accommodate additional aspects of a realistic camera model, specifically:\n\n1. Remapping \\(z'\\) within the range \\([0,1]\\): This adjustment is crucial for depth calculations in a 3D scene, ensuring that objects are correctly rendered with respect to their distance from the camera. It utilizes the camera's near (\\(z_{near}\\)) and far (\\(z_{far}\\)) clipping planes to linearly interpolate \\(z'\\) values within this range. The transformation is generally achieved through additional matrix operations that scale and translate \\(z'\\) values accordingly.\n\n2. Incorporating the Camera's Angle of View: The field of view (FOV) parameter influences how much of the scene is visible through the camera, mimicking the effect of a pinhole camera model. This aspect is accounted for by adjusting the projection matrix to alter the scale of the projected points, thereby affecting the perceived FOV. The matrix adjustments typically involve scaling factors derived from the FOV, ensuring that the scene's spatial dimensions are correctly projected onto the 2D view space.\n\nThe normalization of the z-coordinate, scaling it between 0 and 1, is a critical step in the perspective projection process, as it ensures that objects are correctly rendered according to their distance from the camera. This scaling uses the camera's near and far clipping planes. The aim is to adjust the z-coordinate so that when a point lies on the near clipping plane, its transformed z-coordinate (\\(z'\\)) equals 0, and when it lies on the far clipping plane, \\(z'\\) equals 1. This is achieved by setting specific coefficients in the perspective projection matrix:\n\nTo fulfill the conditions for \\(z'\\) to be 0 at the near clipping plane and 1 at the far clipping plane, the coefficients for \\(m_{22}\\) and \\(m_{23}\\) are set as follows:\n\nrespectively, where \\(n\\) is the distance to the near clipping plane and \\(f\\) is the distance to the far clipping plane. These adjustments ensure that \\(z'\\) is correctly normalized across the specified range.\n\nWhen evaluating \\(z'\\) at the near and far clipping planes, with \\(m_{20}\\) and \\(m_{21}\\) set to 0, the results validate the effectiveness of these coefficients:\n• At the near clipping plane (\\(z = n\\)):\n• At the far clipping plane (\\(z = f\\)):\n\nThis demonstrates how the chosen coefficients correctly map \\(z\\) values at the near and far clipping planes to \\(z'\\) values of 0 and 1, respectively.\n\nThe resulting perspective projection matrix, incorporating these adjustments for \\(z'\\) normalization, is:\n\nThis matrix now not only projects points from 3D space to 2D space but also remaps their z-coordinates to a normalized range of [0,1], ensuring depth values are correctly interpreted for rendering.\n\nTo get a basic perspective projection matrix working, we need to account for the angle of view or field-of-view (FOV) of the camera. Changing the focal length of a zoom lens on a real camera alters the extent of the scene we see. We aim for our CG camera to function similarly.\n\nThe projection window size is [-1:1] in each dimension, meaning a projected point is visible if its x- and y-coordinates are within this range. Points outside this range are invisible and not drawn.\n\nNote that in our system, the screen window's maximum and minimum values remain unchanged, always in the range [-1:1], assuming the screen is square. When points' coordinates are within the range [-1,1], we say they are defined in NDC space.\n\nRemember from chapter 1, the goal of the perspective projection matrix is to project a point onto the screen and remap their coordinates to the range [-1,1] (or to NDC space).\n\nThe distance to the screen window from the eye position remains the same (equal to 1). However, when the FOV changes, the screen window should become larger or smaller accordingly (see figures 2 and 5). How do we reconcile this contradiction? Since we want the screen window size to remain fixed, we instead adjust the projected coordinates by scaling them up or down, testing them against the fixed borders of the screen window. Let's explore a few examples.\n\nImagine a point whose projected x-y coordinates are (1.2, 1.3). These coordinates are outside the range [-1:1], making the point invisible. If we scale them down by multiplying them by 0.7, the new, scaled coordinates become (0.84, 0.91), making the point visible as both coordinates are now within the range [-1:1]. This action is analogous to zooming out, which means decreasing the focal length on a zoom lens or increasing the FOV. Conversely, to achieve the opposite effect, multiply by a value greater than 1. For instance, a point with projected coordinates (-0.5, 0.3) scaled by 2.1 results in new coordinates (-1.05, 0.63). The y-coordinate remains within the range [-1:1], but the x-coordinate is now lower than -1, rendering the point invisible. This effect is akin to zooming in.\n\nTo scale the projected coordinates, we use the camera's field of view. The FOV intuitively controls how much of the scene is visible. For more information, see the lesson 3D Viewing: the Pinhole Camera Model.\n\nThe value of the field-of-view (FOV) is not utilized directly in calculations; instead, the tangent of the angle is used. In computer graphics literature, the FOV can be defined either as the entire angle or half of the angle subtended by the viewing cone. It is more intuitive to perceive the FOV as the angular extent of the visible scene rather than half of this angle, as illustrated in figures 3 and 5. However, to derive a value for scaling the projected coordinates, the FOV angle needs to be halved. This is why the FOV is sometimes expressed as the half-angle.\n\nThe rationale behind dividing the angle by two lies in focusing on the right triangle within the viewing cone. The angle between the hypotenuse and the adjacent side of the triangle, or the FOV half-angle, dictates the length of the triangle's opposite side. Altering this angle allows for the scaling of the image window's border. Utilizing the tangent of this angle scales our projected coordinates. Notably, at an FOV half-angle of 45 degrees (total FOV of 90 degrees), the tangent of the angle equals 1, leaving the coordinates unchanged when multiplied by this value. For FOV values less than 90 degrees, the tangent of the half-angle yields values smaller than 1, and for values greater than 90 degrees, it results in values greater than 1.\n\nHowever, the required effect is the inverse. Zooming in, which corresponds to a decrease in FOV, necessitates multiplying the projected point coordinates by a value greater than 1. Conversely, zooming out, which increases the FOV, requires multiplication by a value less than 1. Therefore, the reciprocal of the tangent, or one over the tangent of the FOV half-angle, is employed.\n\nThe final equation to compute the scaling factor for the projected point coordinates is as follows:\n\nWith this scaling factor, the complete version of our basic perspective projection matrix is:\n\nThis matrix accounts for the FOV by scaling the x and y coordinates of projected points, thus effectively incorporating the angle of view into the perspective projection process.\n\nAre There Different Ways of Building this Matrix?\n\nYes and no. Some renderers may implement the perspective projection matrix differently. For instance, OpenGL utilizes a function called to create perspective projection matrices. This function accepts arguments for the left, right, bottom, and top coordinates, in addition to the near and far clipping planes. Unlike our approach, OpenGL projects points onto the near-clipping plane, rather than onto a plane one unit away from the camera position. The matrix configuration might also appear slightly different.\n\nIt's important to pay attention to the convention used for vectors and matrices. The projected point can be represented as either a row or column vector. Additionally, it's crucial to verify whether the renderer employs a left- or right-handed coordinate system, as this could alter the sign of the matrix coefficients.\n\nDespite these variations, the fundamental concept of the perspective projection matrix remains consistent across all renderers. They invariably divide the x- and y-coordinates of a point by its z-coordinate. Ultimately, all matrices are designed to project the same points to the same pixel coordinates, regardless of the specific conventions or the matrix being used.\n\nWe will explore the construction of the OpenGL matrix in greater detail in the next chapter.\n\nTo validate our basic perspective projection matrix, we developed a simple program to project the vertices of a polygonal object (Newell's teapot) onto the image plane using the projection matrix crafted in this chapter. The program is straightforward. It employs a function to construct the perspective projection matrix based on the camera's near and far clipping planes and the camera field-of-view, specified in degrees. The teapot's vertices are stored in an array. Each point undergoes projection onto the image plane via straightforward point-matrix multiplication, initially transforming points from world or object space to camera space. The function facilitates the multiplication of a point by a matrix, highlighting the creation of the fourth component, \\(w\\), and dividing the resultant point's coordinates by \\(w\\) if \\(w\\) differs from 1—this stage marks the perspective or \\(z\\) divide. A point is deemed visible only if its projected \\(x\\) and \\(y\\) coordinates fall within the \\([-1:1]\\) interval, irrespective of the image's aspect ratio. Points outside this range are considered beyond the camera's screen boundaries. Visible points are then mapped to raster space, i.e., pixel coordinates, through a remapping from \\([-1:1]\\) to \\([0:1]\\), followed by scaling to the image size and rounding to the nearest integer to align with pixel coordinates.\n\nTesting the program with the teapot rendered in a commercial renderer using identical camera settings resulted in matching images, as expected. The teapot geometry and program files are detailed in the Source Code chapter at the end of this lesson.\n\nIn the upcoming chapter, we'll delve into the construction of the perspective projection matrix as utilized in OpenGL. While the foundational principles remain consistent, OpenGL diverges by projecting points onto the near clipping plane and remapping the projected point coordinates to Normalized Device Coordinates (NDC) space. This remapping leverages screen coordinates derived from the camera's near clipping plane and angle-of-view, culminating in a distinct matrix configuration.\n\nFollowing our exploration of the perspective projection matrix in OpenGL, we'll transition to discussing the orthographic projection matrix. Unlike its perspective counterpart, the orthographic projection matrix offers a different view of three-dimensional scenes by projecting objects onto the viewing plane without the depth distortion inherent in perspective projection. This makes it particularly useful for technical and engineering drawings where maintaining the true dimensions and proportions of objects is crucial."
    },
    {
        "link": "https://learnopengl.com/Getting-started/Camera",
        "document": "In the previous chapter we discussed the view matrix and how we can use the view matrix to move around the scene (we moved backwards a little). OpenGL by itself is not familiar with the concept of a camera, but we can try to simulate one by moving all objects in the scene in the reverse direction, giving the illusion that we are moving.\n\nIn this chapter we'll discuss how we can set up a camera in OpenGL. We will discuss a fly style camera that allows you to freely move around in a 3D scene. We'll also discuss keyboard and mouse input and finish with a custom camera class.\n\nWhen we're talking about camera/view space we're talking about all the vertex coordinates as seen from the camera's perspective as the origin of the scene: the view matrix transforms all the world coordinates into view coordinates that are relative to the camera's position and direction. To define a camera we need its position in world space, the direction it's looking at, a vector pointing to the right and a vector pointing upwards from the camera. A careful reader may notice that we're actually going to create a coordinate system with 3 perpendicular unit axes with the camera's position as the origin.\n\nGetting the camera position is easy. The camera position is a vector in world space that points to the camera's position. We set the camera at the same position we've set the camera in the previous chapter:\n\nThe next vector required is the camera's direction e.g. at what direction it is pointing at. For now we let the camera point to the origin of our scene: . Remember that if we subtract two vectors from each other we get a vector that's the difference of these two vectors? Subtracting the camera position vector from the scene's origin vector thus results in the direction vector we want. For the view matrix's coordinate system we want its z-axis to be positive and because by convention (in OpenGL) the camera points towards the negative z-axis we want to negate the direction vector. If we switch the subtraction order around we now get a vector pointing towards the camera's positive z-axis:\n\nThe next vector that we need is a right vector that represents the positive x-axis of the camera space. To get the right vector we use a little trick by first specifying an up vector that points upwards (in world space). Then we do a cross product on the up vector and the direction vector from step 2. Since the result of a cross product is a vector perpendicular to both vectors, we will get a vector that points in the positive x-axis's direction (if we would switch the cross product order we'd get a vector that points in the negative x-axis):\n\nNow that we have both the x-axis vector and the z-axis vector, retrieving the vector that points to the camera's positive y-axis is relatively easy: we take the cross product of the right and direction vector:\n\nWith the help of the cross product and a few tricks we were able to create all the vectors that form the view/camera space. For the more mathematically inclined readers, this process is known as the Gram-Schmidt process in linear algebra. Using these camera vectors we can now create a matrix that proves very useful for creating a camera.\n\nA great thing about matrices is that if you define a coordinate space using 3 perpendicular (or non-linear) axes you can create a matrix with those 3 axes plus a translation vector and you can transform any vector to that coordinate space by multiplying it with this matrix. This is exactly what the LookAt matrix does and now that we have 3 perpendicular axes and a position vector to define the camera space we can create our own LookAt matrix: \\[LookAt = \\begin{bmatrix} \\color{red}{R_x} & \\color{red}{R_y} & \\color{red}{R_z} & 0 \\\\ \\color{green}{U_x} & \\color{green}{U_y} & \\color{green}{U_z} & 0 \\\\ \\color{blue}{D_x} & \\color{blue}{D_y} & \\color{blue}{D_z} & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} * \\begin{bmatrix} 1 & 0 & 0 & -\\color{purple}{P_x} \\\\ 0 & 1 & 0 & -\\color{purple}{P_y} \\\\ 0 & 0 & 1 & -\\color{purple}{P_z} \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\] Where \\(\\color{red}R\\) is the right vector, \\(\\color{green}U\\) is the up vector, \\(\\color{blue}D\\) is the direction vector and \\(\\color{purple}P\\) is the camera's position vector. Note that the rotation (left matrix) and translation (right matrix) parts are inverted (transposed and negated respectively) since we want to rotate and translate the world in the opposite direction of where we want the camera to move. Using this LookAt matrix as our view matrix effectively transforms all the world coordinates to the view space we just defined. The LookAt matrix then does exactly what it says: it creates a view matrix that looks at a given target.\n\nLuckily for us, GLM already does all this work for us. We only have to specify a camera position, a target position and a vector that represents the up vector in world space (the up vector we used for calculating the right vector). GLM then creates the LookAt matrix that we can use as our view matrix:\n\nThe function requires a position, target and up vector respectively. This example creates a view matrix that is the same as the one we created in the previous chapter.\n\nBefore delving into user input, let's get a little funky first by rotating the camera around our scene. We keep the target of the scene at . We use a little bit of trigonometry to create an and coordinate each frame that represents a point on a circle and we'll use these for our camera position. By re-calculating the and coordinate over time we're traversing all the points in a circle and thus the camera rotates around the scene. We enlarge this circle by a pre-defined and create a new view matrix each frame using GLFW's function:\n\nIf you run this code you should get something like this:\n\nWith this little snippet of code the camera now circles around the scene over time. Feel free to experiment with the radius and position/direction parameters to get the feel of how this LookAt matrix works. Also, check the source code if you're stuck.\n\nSwinging the camera around a scene is fun, but it's more fun to do all the movement ourselves! First we need to set up a camera system, so it is useful to define some camera variables at the top of our program:\n\nThe function now becomes:\n\nFirst we set the camera position to the previously defined . The direction is the current position + the direction vector we just defined. This ensures that however we move, the camera keeps looking at the target direction. Let's play a bit with these variables by updating the vector when we press some keys.\n\nWe already defined a function to manage GLFW's keyboard input so let's add a few extra key commands:\n\nWhenever we press one of the keys, the camera's position is updated accordingly. If we want to move forward or backwards we add or subtract the direction vector from the position vector scaled by some speed value. If we want to move sideways we do a cross product to create a right vector and we move along the right vector accordingly. This creates the familiar effect when using the camera.\n\nBy now, you should already be able to move the camera somewhat, albeit at a speed that's system-specific so you may need to adjust .\n\nCurrently we used a constant value for movement speed when walking around. In theory this seems fine, but in practice people's machines have different processing powers and the result of that is that some people are able to render much more frames than others each second. Whenever a user renders more frames than another user he also calls more often. The result is that some people move really fast and some really slow depending on their setup. When shipping your application you want to make sure it runs the same on all kinds of hardware.\n\nGraphics applications and games usually keep track of a variable that stores the time it took to render the last frame. We then multiply all velocities with this value. The result is that when we have a large in a frame, meaning that the last frame took longer than average, the velocity for that frame will also be a bit higher to balance it all out. When using this approach it does not matter if you have a very fast or slow pc, the velocity of the camera will be balanced out accordingly so each user will have the same experience.\n\nTo calculate the value we keep track of 2 global variables:\n\nWithin each frame we then calculate the new value for later use:\n\nNow that we have we can take it into account when calculating the velocities:\n\nSince we're using the camera will now move at a constant speed of units per second. Together with the previous section we should now have a much smoother and more consistent camera system for moving around the scene:\n\nAnd now we have a camera that walks and looks equally fast on any system. Again, check the source code if you're stuck. We'll see the value frequently return with anything movement related.\n\nOnly using the keyboard keys to move around isn't that interesting. Especially since we can't turn around making the movement rather restricted. That's where the mouse comes in!\n\nTo look around the scene we have to change the vector based on the input of the mouse. However, changing the direction vector based on mouse rotations is a little complicated and requires some trigonometry. If you do not understand the trigonometry, don't worry, you can just skip to the code sections and paste them in your code; you can always come back later if you want to know more.\n\nEuler angles are 3 values that can represent any rotation in 3D, defined by Leonhard Euler somewhere in the 1700s. There are 3 Euler angles: pitch, yaw and roll. The following image gives them a visual meaning:\n\nThe is the angle that depicts how much we're looking up or down as seen in the first image. The second image shows the value which represents the magnitude we're looking to the left or to the right. The represents how much we roll as mostly used in space-flight cameras. Each of the Euler angles are represented by a single value and with the combination of all 3 of them we can calculate any rotation vector in 3D.\n\nFor our camera system we only care about the yaw and pitch values so we won't discuss the roll value here. Given a pitch and a yaw value we can convert them into a 3D vector that represents a new direction vector. The process of converting yaw and pitch values to a direction vector requires a bit of trigonometry. and we start with a basic case:\n\nLet's start with a bit of a refresher and check the general right triangle case (with one side at a 90 degree angle):\n\nIf we define the hypotenuse to be of length we know from trigonometry (soh cah toa) that the adjacant side's length is \\(\\cos \\ \\color{red}x/\\color{purple}h = \\cos \\ \\color{red}x/\\color{purple}1 = \\cos\\ \\color{red}x\\) and that the opposing side's length is \\(\\sin \\ \\color{green}y/\\color{purple}h = \\sin \\ \\color{green}y/\\color{purple}1 = \\sin\\ \\color{green}y\\). This gives us some general formulas for retrieving the length in both the and sides on right triangles, depending on the given angle. Let's use this to calculate the components of the direction vector.\n\nLet's imagine this same triangle, but now looking at it from a top perspective with the adjacent and opposite sides being parallel to the scene's x and z axis (as if looking down the y-axis).\n\nIf we visualize the yaw angle to be the counter-clockwise angle starting from the side we can see that the length of the side relates to . And similarly how the length of the side relates to .\n\nIf we take this knowledge and a given value we can use it to create a camera direction vector:\n\nThis solves how we can get a 3D direction vector from a yaw value, but pitch needs to be included as well. Let's now look at the axis side as if we're sitting on the plane:\n\nSimilarly, from this triangle we can see that the direction's y component equals so let's fill that in:\n\nHowever, from the pitch triangle we can also see the sides are influenced by so we need to make sure this is also part of the direction vector. With this included we get the final direction vector as translated from yaw and pitch Euler angles:\n\nThis gives us a formula to convert yaw and pitch values to a 3-dimensional direction vector that we can use for looking around.\n\nWe've set up the scene world so everything's positioned in the direction of the negative z-axis. However, if we look at the and yaw triangle we see that a \\(\\theta\\) of results in the camera's vector to point towards the positive x-axis. To make sure the camera points towards the negative z-axis by default we can give the a default value of a 90 degree clockwise rotation. Positive degrees rotate counter-clockwise so we set the default value to:\n\nYou've probably wondered by now: how do we set and modify these yaw and pitch values?\n\nThe yaw and pitch values are obtained from mouse (or controller/joystick) movement where horizontal mouse-movement affects the yaw and vertical mouse-movement affects the pitch. The idea is to store the last frame's mouse positions and calculate in the current frame how much the mouse values changed. The higher the horizontal or vertical difference, the more we update the pitch or yaw value and thus the more the camera should move.\n\nFirst we will tell GLFW that it should hide the cursor and it. Capturing a cursor means that, once the application has focus, the mouse cursor stays within the center of the window (unless the application loses focus or quits). We can do this with one simple configuration call:\n\nAfter this call, wherever we move the mouse it won't be visible and it should not leave the window. This is perfect for an FPS camera system.\n\nTo calculate the pitch and yaw values we need to tell GLFW to listen to mouse-movement events. We do this by creating a callback function with the following prototype:\n\nHere and represent the current mouse positions. As soon as we register the callback function with GLFW each time the mouse moves, the function is called:\n\nWhen handling mouse input for a fly style camera there are several steps we have to take before we're able to fully calculate the camera's direction vector:\n• Calculate the mouse's offset since the last frame.\n• Add the offset values to the camera's yaw and pitch values.\n• Add some constraints to the minimum/maximum pitch values.\n\nThe first step is to calculate the offset of the mouse since last frame. We first have to store the last mouse positions in the application, which we initialize to be in the center of the screen (screen size is by ) initially:\n\nThen in the mouse's callback function we calculate the offset movement between the last and current frame:\n\nNote that we multiply the offset values by a value. If we omit this multiplication the mouse movement would be way too strong; fiddle around with the sensitivity value to your liking.\n\nNext we add the offset values to the globally declared and values:\n\nIn the third step we'd like to add some constraints to the camera so users won't be able to make weird camera movements (also causes a LookAt flip once direction vector is parallel to the world up direction). The pitch needs to be constrained in such a way that users won't be able to look higher than degrees (at degrees we get the LookAt flip) and also not below degrees. This ensures the user will be able to look up to the sky or below to his feet but not further. The constraints work by replacing the Euler value with its constraint value whenever it breaches the constraint:\n\nNote that we set no constraint on the yaw value since we don't want to constrain the user in horizontal rotation. However, it's just as easy to add a constraint to the yaw as well if you feel like it.\n\nThe fourth and last step is to calculate the actual direction vector using the formula from the previous section:\n\nThis computed direction vector then contains all the rotations calculated from the mouse's movement. Since the vector is already included in glm's function we're set to go.\n\nIf you'd now run the code you'll notice the camera makes a large sudden jump whenever the window first receives focus of your mouse cursor. The cause for this sudden jump is that as soon as your cursor enters the window the mouse callback function is called with an and position equal to the location your mouse entered the screen from. This is often a position that is significantly far away from the center of the screen, resulting in large offsets and thus a large movement jump. We can circumvent this issue by defining a global variable to check if this is the first time we receive mouse input. If it is the first time, we update the initial mouse positions to the new and values. The resulting mouse movements will then use the newly entered mouse's position coordinates to calculate the offsets:\n\nThe final code then becomes:\n\nThere we go! Give it a spin and you'll see that we can now freely move through our 3D scene!\n\nAs a little extra to the camera system we'll also implement a zooming interface. In the previous chapter we said the Field of view or fov largely defines how much we can see of the scene. When the field of view becomes smaller, the scene's projected space gets smaller. This smaller space is projected over the same NDC, giving the illusion of zooming in. To zoom in, we're going to use the mouse's scroll wheel. Similar to mouse movement and keyboard input we have a callback function for mouse scrolling:\n\nWhen scrolling, the value tells us the amount we scrolled vertically. When the function is called we change the content of the globally declared variable. Since is the default fov value we want to constrain the zoom level between and .\n\nWe now have to upload the perspective projection matrix to the GPU each frame, but this time with the variable as its field of view:\n\nAnd lastly don't forget to register the scroll callback function:\n\nAnd there you have it. We implemented a simple camera system that allows for free movement in a 3D environment.\n\nFeel free to experiment a little and if you're stuck compare your code with the source code.\n\nIn the upcoming chapters we'll always use a camera to easily look around the scenes and see the results from all angles. However, since the camera code can take up a significant amount of space on each chapter we'll abstract its details a little and create our own camera object that does most of the work for us with some neat little extras. Unlike the Shader chapter we won't walk you through creating the camera class, but provide you with the (fully commented) source code if you want to know the inner workings.\n\nLike the object, we define the camera class entirely in a single header file. You can find the camera class here; you should be able to understand the code after this chapter. It is advised to at least check the class out once as an example on how you could create your own camera system.\n\nThe updated version of the source code using the new camera object can be found here.\n• See if you can transform the camera class in such a way that it becomes a true fps camera where you cannot fly; you can only look around while staying on the plane: solution.\n• Try to create your own LookAt function where you manually create a view matrix as discussed at the start of this chapter. Replace glm's LookAt function with your own implementation and see if it still acts the same: solution."
    },
    {
        "link": "https://songho.ca/opengl/gl_projectionmatrix.html",
        "document": "Updates: The MathML version is available here.\n\nA computer monitor is a 2D surface. A 3D scene rendered by OpenGL must be projected onto the computer screen as a 2D image. GL_PROJECTION matrix is used for this projection transformation. First, it transforms all vertex data from the eye coordinates to the clip coordinates. Then, these clip coordinates are also transformed to the normalized device coordinates (NDC) by dividing with w component of the clip coordinates.\n\nTherefore, we have to keep in mind that both clipping (frustum culling) and NDC transformations are integrated into GL_PROJECTION matrix. The following sections describe how to build the projection matrix from 6 parameters; left, right, bottom, top, near and far boundary values.\n\nNote that the frustum culling (clipping) is performed in the clip coordinates, just before dividing by w . The clip coordinates, x , y and z are tested by comparing with w . If any clip coordinate is less than -w , or greater than w , then the vertex will be discarded. \n\n\n\nThen, OpenGL will reconstruct the edges of the polygon where clipping occurs.\n\nIn perspective projection, a 3D point in a truncated pyramid frustum (eye coordinates) is mapped to a cube (NDC); the range of x-coordinate from [l, r] to [-1, 1], the y-coordinate from [b, t] to [-1, 1] and the z-coordinate from [-n, -f] to [-1, 1].\n\nNote that the eye coordinates are defined in the right-handed coordinate system, but NDC uses the left-handed coordinate system. That is, the camera at the origin is looking along -Z axis in eye space, but it is looking along +Z axis in NDC. Since glFrustum() accepts only positive values of near and far distances, we need to negate them during the construction of GL_PROJECTION matrix.\n\nIn OpenGL, a 3D point in eye space is projected onto the near plane (projection plane). The following diagrams show how a point (x , y , z ) in eye space is projected to (x , y , z ) on the near plane.\n\nFrom the top view of the frustum, the x-coordinate of eye space, x is mapped to x , which is calculated by using the ratio of similar triangles; \n\n\n\nFrom the side view of the frustum, y is also calculated in a similar way; \n\n\n\nNote that both x and y depend on z ; they are inversely propotional to -z . In other words, they are both divided by -z . It is a very first clue to construct GL_PROJECTION matrix. After the eye coordinates are transformed by multiplying GL_PROJECTION matrix, the clip coordinates are still a homogeneous coordinates. It finally becomes the normalized device coordinates (NDC) by divided by the w-component of the clip coordinates. (See more details on OpenGL Transformation.) \n\n ,\n\nTherefore, we can set the w-component of the clip coordinates as -z . And, the 4th of GL_PROJECTION matrix becomes (0, 0, -1, 0). \n\n\n\nNext, we map x and y to x and y of NDC with linear relationship; [l, r] ⇒ [-1, 1] and [b, t] ⇒ [-1, 1].\n\nThen, we substitute x and y into the above equations.\n\nNote that we make both terms of each equation divisible by -z for perspective division (x /w , y /w ). And we set w to -z earlier, and the terms inside parentheses become x and y of the clip coordiantes.\n\nFrom these equations, we can find the 1st and 2nd rows of GL_PROJECTION matrix. \n\n\n\nNow, we only have the 3rd row of GL_PROJECTION matrix to solve. Finding z is a little different from others because z in eye space is always projected to -n on the near plane. But we need unique z value for the clipping and depth test. Plus, we should be able to unproject (inverse transform) it. Since we know z does not depend on x or y value, we borrow w-component to find the relationship between z and z . Therefore, we can specify the 3rd row of GL_PROJECTION matrix like this. \n\n\n\nIn eye space, w equals to 1. Therefore, the equation becomes; \n\n\n\nTo find the coefficients, A and B, we use the (z , z ) relation; (-n, -1) and (-f, 1), and put them into the above equation. \n\n\n\nTo solve the equations for A and B, rewrite eq.(1) for B; \n\n\n\nSubstitute eq.(1') to B in eq.(2), then solve for A; \n\n\n\nWe found A and B. Therefore, the relation between z and z becomes; \n\n\n\nFinally, we found all entries of GL_PROJECTION matrix. The complete projection matrix is;\n\nThis projection matrix is for a general frustum. If the viewing volume is symmetric, which is and , then it can be simplified as; \n\n\n\nBefore we move on, please take a look at the relation between z and z , eq.(3) once again. You notice it is a rational function and is non-linear relationship between z and z . It means there is very high precision at the near plane, but very little precision at the far plane. If the range [-n, -f] is getting larger, it causes a depth precision problem (z-fighting); a small change of z around the far plane does not affect on z value. The distance between n and f should be short as possible to minimize the depth buffer precision problem.\n\nThe perspective projection matrix can be simplified by setting the far plane to ∞ in the third row of the perspective matrix. \n\n\n\nTherefore, both general and symmetric perspective projection matices with infinite far plane become;\n\nNote that the infinite projection matrix still suffers from the depth precision error .\n\nIt is hard to determine 4 parameters (left, right, top, and bottom) properly with a given near and far planes for the perspective projection on a specific window dimension. You can easily derive these 4 parameters from the vertical/horizontal field of view angle and the aspect ratio, width/height. However these conversion are limited for a symmetric perspective projection matrix.\n\nConstructing GL_PROJECTION matrix for orthographic projection is much simpler than perspective mode.\n\nAll x , y and z components in eye space are linearly mapped to NDC. We just need to scale a rectangular volume to a cube, then move it to the origin. Let's find out the elements of GL_PROJECTION using linear relationship.\n\nSince w-component is not necessary for orthographic projection, the 4th row of GL_PROJECTION matrix remains as (0, 0, 0, 1). Therefore, the complete GL_PROJECTION matrix for orthographic projection is;\n\nIt can be further simplified if the viewing volume is symmetrical, and ."
    }
]