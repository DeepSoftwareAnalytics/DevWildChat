[
    {
        "link": "https://en.wikipedia.org/wiki/High_Efficiency_Video_Coding",
        "document": "High Efficiency Video Coding (HEVC), also known as H.265 and MPEG-H Part 2, is a video compression standard designed as part of the MPEG-H project as a successor to the widely used Advanced Video Coding (AVC, H.264, or MPEG-4 Part 10). In comparison to AVC, HEVC offers from 25% to 50% better data compression at the same level of video quality, or substantially improved video quality at the same bit rate. It supports resolutions up to 8192×4320, including 8K UHD, and unlike the primarily 8-bit AVC, HEVC's higher fidelity Main 10 profile has been incorporated into nearly all supporting hardware.\n\nWhile AVC uses the integer discrete cosine transform (DCT) with 4×4 and 8×8 block sizes, HEVC uses both integer DCT and discrete sine transform (DST) with varied block sizes between 4×4 and 32×32. The High Efficiency Image Format (HEIF) is based on HEVC.[2]\n\nIn most ways, HEVC is an extension of the concepts in H.264/MPEG-4 AVC. Both work by comparing different parts of a frame of video to find areas that are redundant, both within a single frame and between consecutive frames. These redundant areas are then replaced with a short description instead of the original pixels. The primary changes for HEVC include the expansion of the pattern comparison and difference-coding areas from 16×16 pixel to sizes up to 64×64, improved variable-block-size segmentation, improved \"intra\" prediction within the same picture, improved motion vector prediction and motion region merging, improved motion compensation filtering, and an additional filtering step called sample-adaptive offset filtering. Effective use of these improvements requires much more signal processing capability for compressing the video but has less impact on the amount of computation needed for decompression.\n\nHEVC was standardized by the Joint Collaborative Team on Video Coding (JCT-VC), a collaboration between the ISO/IEC MPEG and ITU-T Study Group 16 VCEG. The ISO/IEC group refers to it as MPEG-H Part 2 and the ITU-T as H.265. The first version of the HEVC standard was ratified in January 2013 and published in June 2013. The second version, with multiview extensions (MV-HEVC), range extensions (RExt), and scalability extensions (SHVC), was completed and approved in 2014 and published in early 2015. Extensions for 3D video (3D-HEVC) were completed in early 2015, and extensions for screen content coding (SCC) were completed in early 2016 and published in early 2017, covering video containing rendered graphics, text, or animation as well as (or instead of) camera-captured video scenes. In October 2017, the standard was recognized by a Primetime Emmy Engineering Award as having had a material effect on the technology of television.[3][4][5][6][7]\n\nHEVC contains technologies covered by patents owned by the organizations that participated in the JCT-VC. Implementing a device or software application that uses HEVC may require a license from HEVC patent holders. The ISO/IEC and ITU require companies that belong to their organizations to offer their patents on reasonable and non-discriminatory licensing (RAND) terms. Patent licenses can be obtained directly from each patent holder, or through patent licensing bodies, such as MPEG LA, Access Advance, and Velos Media.\n\nThe combined licensing fees currently offered by all of the patent licensing bodies are higher than for AVC. The licensing fees are one of the main reasons HEVC adoption has been low on the web and is why some of the largest tech companies (Amazon, AMD, Apple, ARM, Cisco, Google, Intel, Microsoft, Mozilla, Netflix, Nvidia, and more) have joined the Alliance for Open Media,[8] which finalized royalty-free alternative video coding format AV1 on March 28, 2018.[9]\n\nThe HEVC format was jointly developed by more than a dozen organisations across the world. The majority of active patent contributions towards the development of the HEVC format came from five organizations: Samsung Electronics (4,249 patents), General Electric (1,127 patents),[10] M&K Holdings (907 patents), NTT (878 patents), and JVC Kenwood (628 patents).[11] Other patent holders include Fujitsu, Apple, Canon, Columbia University, KAIST, Kwangwoon University, MIT, Sungkyunkwan University, Funai, Hikvision, KBS, KT and NEC.[12]\n\nIn 2004, the ITU-T Video Coding Experts Group (VCEG) began a major study of technology advances that could enable the creation of a new video compression standard (or substantial compression-oriented enhancements of the H.264/MPEG-4 AVC standard). In October 2004, various techniques for potential enhancement of the H.264/MPEG-4 AVC standard were surveyed. In January 2005, at the next meeting of VCEG, VCEG began designating certain topics as \"Key Technical Areas\" (KTA) for further investigation. A software codebase called the KTA codebase was established for evaluating such proposals.[14] The KTA software was based on the Joint Model (JM) reference software that was developed by the MPEG & VCEG Joint Video Team for H.264/MPEG-4 AVC. Additional proposed technologies were integrated into the KTA software and tested in experiment evaluations over the next four years.[15][16][17]\n\nTwo approaches for standardizing enhanced compression technology were considered: either creating a new standard or creating extensions of H.264/MPEG-4 AVC. The project had tentative names H.265 and H.NGVC (Next-generation Video Coding), and was a major part of the work of VCEG until it evolved into the HEVC joint project with MPEG in 2010.[18][19][20]\n\nThe preliminary requirements for NGVC were the capability to have a bit rate reduction of 50% at the same subjective image quality compared with the H.264/MPEG-4 AVC High profile, and computational complexity ranging from 1/2 to 3 times that of the High profile.[20] NGVC would be able to provide 25% bit rate reduction along with 50% reduction in complexity at the same perceived video quality as the High profile, or to provide greater bit rate reduction with somewhat higher complexity.[20][21]\n\nThe ISO/IEC Moving Picture Experts Group (MPEG) started a similar project in 2007, tentatively named High-performance Video Coding.[22][23] An agreement of getting a bit rate reduction of 50% had been decided as the goal of the project by July 2007.[22] Early evaluations were performed with modifications of the KTA reference software encoder developed by VCEG. By July 2009, experimental results showed average bit reduction of around 20% compared with AVC High Profile; these results prompted MPEG to initiate its standardization effort in collaboration with VCEG.[23]\n\nMPEG and VCEG established a Joint Collaborative Team on Video Coding (JCT-VC) to develop the HEVC standard.[25][26]\n\nA formal joint Call for Proposals on video compression technology was issued in January 2010 by VCEG and MPEG, and proposals were evaluated at the first meeting of the MPEG & VCEG Joint Collaborative Team on Video Coding (JCT-VC), which took place in April 2010. A total of 27 full proposals were submitted.[18][27] Evaluations showed that some proposals could reach the same visual quality as AVC at only half the bit rate in many of the test cases, at the cost of 2–10× increase in computational complexity, and some proposals achieved good subjective quality and bit rate results with lower computational complexity than the reference AVC High profile encodings. At that meeting, the name High Efficiency Video Coding (HEVC) was adopted for the joint project.[18] Starting at that meeting, the JCT-VC integrated features of some of the best proposals into a single software codebase and a \"Test Model under Consideration\", and performed further experiments to evaluate various proposed features.[28] The first working draft specification of HEVC was produced at the third JCT-VC meeting in October 2010. Many changes in the coding tools and configuration of HEVC were made in later JCT-VC meetings.\n\nOn January 25, 2013, the ITU announced that HEVC had received first stage approval (consent) in the ITU-T Alternative Approval Process (AAP).[29][30][31] On the same day, MPEG announced that HEVC had been promoted to Final Draft International Standard (FDIS) status in the MPEG standardization process.[32][33]\n\nOn April 13, 2013, HEVC/H.265 was approved as an ITU-T standard.[34][35][36] The standard was formally published by the ITU-T on June 7, 2013, and by the ISO/IEC on November 25, 2013.[17]\n\nOn July 11, 2014, MPEG announced that the 2nd edition of HEVC will contain three recently completed extensions which are the multiview extensions (MV-HEVC), the range extensions (RExt), and the scalability extensions (SHVC).[37]\n\nOn October 29, 2014, HEVC/H.265 version 2 was approved as an ITU-T standard.[38][39][40] It was then formally published on January 12, 2015.\n\nOn April 29, 2015, HEVC/H.265 version 3 was approved as an ITU-T standard.[41][42][43]\n\nOn June 3, 2016, HEVC/H.265 version 4 was consented in the ITU-T and was not approved during a vote in October 2016.[44][45]\n\nOn December 22, 2016, HEVC/H.265 version 4 was approved as an ITU-T standard.[46][47]\n\nOn September 29, 2014, MPEG LA announced their HEVC license which covers the essential patents from 23 companies.[48] The first 100,000 \"devices\" (which includes software implementations) are royalty free, and after that the fee is $0.20 per device up to an annual cap of $25 million.[49] This is significantly more expensive than the fees on AVC, which were $0.10 per device, with the same 100,000 waiver, and an annual cap of $6.5 million. MPEG LA does not charge any fee on the content itself, something they had attempted when initially licensing AVC, but subsequently dropped when content producers refused to pay it.[50] The license has been expanded to include the profiles in version 2 of the HEVC standard.[51]\n\nWhen the MPEG LA terms were announced, commenters noted that a number of prominent patent holders were not part of the group. Among these were AT&T, Microsoft, Nokia, and Motorola. Speculation at the time was that these companies would form their own licensing pool to compete with or add to the MPEG LA pool. Such a group was formally announced on March 26, 2015, as HEVC Advance.[52] The terms, covering 500 essential patents, were announced on July 22, 2015, with rates that depend on the country of sale, type of device, HEVC profile, HEVC extensions, and HEVC optional features. Unlike the MPEG LA terms, HEVC Advance reintroduced license fees on content encoded with HEVC, through a revenue sharing fee.[53]\n\nThe initial HEVC Advance license had a maximum royalty rate of US$2.60 per device for Region 1 countries and a content royalty rate of 0.5% of the revenue generated from HEVC video services. Region 1 countries in the HEVC Advance license include the United States, Canada, European Union, Japan, South Korea, Australia, New Zealand, and others. Region 2 countries are countries not listed in the Region 1 country list. The HEVC Advance license had a maximum royalty rate of US$1.30 per device for Region 2 countries. Unlike MPEG LA, there was no annual cap. On top of this, HEVC Advance also charged a royalty rate of 0.5% of the revenue generated from video services encoding content in HEVC.[53]\n\nWhen they were announced, there was considerable backlash from industry observers about the \"unreasonable and greedy\" fees on devices, which were about seven times that of the MPEG LA's fees. Added together, a device would require licenses costing $2.80, twenty-eight times as expensive as AVC, as well as license fees on the content. This led to calls for \"content owners [to] band together and agree not to license from HEVC Advance\".[54] Others argued the rates might cause companies to switch to competing standards such as Daala and VP9.[55]\n\nOn December 18, 2015, HEVC Advance announced changes in the royalty rates. The changes include a reduction in the maximum royalty rate for Region 1 countries to US$2.03 per device, the creation of annual royalty caps, and a waiving of royalties on content that is free to end users. The annual royalty caps for a company is US$40 million for devices, US$5 million for content, and US$2 million for optional features.[56]\n\nOn February 3, 2016, Technicolor SA announced that they had withdrawn from the HEVC Advance patent pool[57] and would be directly licensing their HEVC patents.[58] HEVC Advance previously listed 12 patents from Technicolor.[59] Technicolor announced that they had rejoined on October 22, 2019.[60]\n\nOn November 22, 2016, HEVC Advance announced a major initiative, revising their policy to allow software implementations of HEVC to be distributed directly to consumer mobile devices and personal computers royalty free, without requiring a patent license.[61]\n\nOn March 31, 2017, Velos Media announced their HEVC license which covers the essential patents from Ericsson, Panasonic, Qualcomm Incorporated, Sharp, and Sony.[62]\n\nAs of April 2019, the MPEG LA HEVC patent list is 164 pages long.[63][64]\n\nThe following organizations currently hold the most active patents in the HEVC patent pools listed by MPEG LA and HEVC Advance:\n\nVersions of the HEVC/H.265 standard using the ITU-T approval dates.\n• Version 1: (April 13, 2013) First approved version of the HEVC/H.265 standard containing Main, Main10, and Main Still Picture profiles. 34 35 36\n• Version 2: (October 29, 2014) Second approved version of the HEVC/H.265 standard which adds 21 range extensions profiles, two scalable extensions profiles, and one multi-view extensions profile. 38 39 40\n• Version 3: (April 29, 2015) Third approved version of the HEVC/H.265 standard which adds the 3D Main profile. 41 42 43\n• Version 4: (December 22, 2016) Fourth approved version of the HEVC/H.265 standard which adds seven screen content coding extensions profiles, three high throughput extensions profiles, and four scalable extensions profiles. 65 46 47\n• Version 5: (February 13, 2018) Fifth approved version of the HEVC/H.265 standard which adds additional SEI messages that include omnidirectional video SEI messages, a Monochrome 10 profile, a Main 10 Still Picture profile, and corrections to various minor defects in the prior content of the Specification. 66 67\n• Version 6: (June 29, 2019) Sixth approved version of the HEVC/H.265 standard which adds additional SEI messages that include SEI manifest and SEI prefix messages, and corrections to various minor defects in the prior content of the Specification. 66 68\n• Version 7: (November 29, 2019) Seventh approved version of the HEVC/H.265 standard which adds additional SEI messages for fisheye video information and annotated regions, and also includes corrections to various minor defects in the prior content of the Specification. 66 69\n• Version 10: on 29 July, 2024 Version 10 was approved, it is the latest version. 72\n\nOn February 29, 2012, at the 2012 Mobile World Congress, Qualcomm demonstrated a HEVC decoder running on an Android tablet, with a Qualcomm Snapdragon S4 dual-core processor running at 1.5 GHz, showing H.264/MPEG-4 AVC and HEVC versions of the same video content playing side by side. In this demonstration, HEVC reportedly showed almost a 50% bit rate reduction compared with H.264/MPEG-4 AVC.[73]\n\nOn February 11, 2013, researchers from MIT demonstrated the world's first published HEVC ASIC decoder at the International Solid-State Circuits Conference (ISSCC) 2013.[74] Their chip was capable of decoding a 3840×2160p at 30 fps video stream in real time, consuming under 0.1 W of power.[75][76]\n\nOn April 3, 2013, Ateme announced the availability of the first open source implementation of a HEVC software player based on the OpenHEVC decoder and GPAC video player which are both licensed under LGPL. The OpenHEVC decoder supports the Main profile of HEVC and can decode 1080p at 30 fps video using a single core CPU.[77] A live transcoder that supports HEVC and used in combination with the GPAC video player was shown at the ATEME booth at the NAB Show in April 2013.[77][78]\n\nOn July 23, 2013, MulticoreWare announced, and made the source code available for the x265 HEVC Encoder Library under the GPL v2 license.[79][80]\n\nOn August 8, 2013, Nippon Telegraph and Telephone announced the release of their HEVC-1000 SDK software encoder which supports the Main 10 profile, resolutions up to 7680×4320, and frame rates up to 120 fps.[81]\n\nOn November 14, 2013, DivX developers released information on HEVC decoding performance using an Intel i7 CPU at 3.5 GHz with 4 cores and 8 threads.[82] The DivX 10.1 Beta decoder was capable of 210.9 fps at 720p, 101.5 fps at 1080p, and 29.6 fps at 4K.[82]\n\nOn December 18, 2013, ViXS Systems announced shipments of their XCode (not to be confused with Apple's Xcode IDE for MacOS) 6400 SoC which was the first SoC to support the Main 10 profile of HEVC.[83]\n\nOn April 5, 2014, at the NAB show, eBrisk Video, Inc. and Altera Corporation demonstrated an FPGA-accelerated HEVC Main10 encoder that encoded 4Kp60/10-bit video in real-time, using a dual-Xeon E5-2697-v2 platform.[84][85]\n\nOn August 13, 2014, Ittiam Systems announced availability of its third generation H.265/HEVC codec with 4:2:2 12-bit support.[86]\n\nOn September 5, 2014, the Blu-ray Disc Association announced that the 4K Blu-ray Disc specification would support HEVC-encoded 4K video at 60 fps, the Rec. 2020 color space, high dynamic range (PQ and HLG), and 10-bit color depth.[87][88] 4K Blu-ray Discs have a data rate of at least 50 Mbit/s and disc capacity up to 100 GB.[87][88] 4K Blu-ray Discs and players became available for purchase in 2015 or 2016.[87][88]\n\nOn September 9, 2014, Apple announced the iPhone 6 and iPhone 6 Plus which support HEVC/H.265 for FaceTime over cellular.[89]\n\nOn September 18, 2014, Nvidia released the GeForce GTX 980 (GM204) and GTX 970 (GM204), which includes Nvidia NVENC, the world's first HEVC hardware encoder in a discrete graphics card.[90]\n\nOn October 31, 2014, Microsoft confirmed that Windows 10 will support HEVC out of the box, according to a statement from Gabriel Aul, the leader of Microsoft Operating Systems Group's Data and Fundamentals Team.[91][92] Windows 10 Technical Preview Build 9860 added platform level support for HEVC and Matroska.[93][94]\n\nOn November 3, 2014, Android Lollipop was released with out of the box support for HEVC using Ittiam Systems' software.[95]\n\nOn January 5, 2015, ViXS Systems announced the XCode 6800 which is the first SoC to support the Main 12 profile of HEVC.[96]\n\nOn January 5, 2015, Nvidia officially announced the Tegra X1 SoC with full fixed-function HEVC hardware decoding.[97][98]\n\nOn January 22, 2015, Nvidia released the GeForce GTX 960 (GM206), which includes the world's first full fixed function HEVC Main/Main10 hardware decoder in a discrete graphics card.[99]\n\nOn February 23, 2015, Advanced Micro Devices (AMD) announced that their UVD ASIC to be found in the Carrizo APUs would be the first x86 based CPUs to have a HEVC hardware decoder.[100]\n\nOn February 27, 2015, VLC media player version 2.2.0 was released with robust support of HEVC playback. The corresponding versions on Android and iOS are also able to play HEVC.\n\nOn March 31, 2015, VITEC announced the MGW Ace which was the first 100% hardware-based portable HEVC encoder that provides mobile HEVC encoding.[101]\n\nOn August 5, 2015, Intel launched Skylake products with full fixed function Main/8-bit decoding/encoding and hybrid/partial Main10/10-bit decoding.\n\nOn September 9, 2015 Apple announced the Apple A9 chip, first used in the iPhone 6S, its first processor with a hardware HEVC decoder supporting Main 8 and 10. This feature would not be unlocked until the release of iOS 11 in 2017.[102]\n\nOn April 11, 2016, full HEVC (H.265) support was announced in the newest MythTV version (0.28).[103]\n\nOn September 7, 2016 Apple announced the Apple A10 chip, first used in the iPhone 7, which included a hardware HEVC encoder supporting Main 8 and 10. This feature would not be unlocked until the release of iOS 11 in 2017.[102]\n\nOn October 25, 2016, Nvidia released the GeForce GTX 1050Ti (GP107) and GeForce GTX 1050 (GP107), which includes full fixed function HEVC Main10/Main12 hardware encoder.\n\nOn June 5, 2017, Apple announced HEVC H.265 support in macOS High Sierra, iOS 11, tvOS,[105] HTTP Live Streaming[106] and Safari.[107][108]\n\nOn June 25, 2017, Microsoft released a free HEVC app extension for Windows 10, enabling some Windows 10 devices with HEVC decoding hardware to play video using the HEVC format inside any app.[109]\n\nOn September 19, 2017, Apple released iOS 11 and tvOS 11 with HEVC encoding & decoding support.[110][105]\n\nOn September 28, 2017, GoPro released the Hero6 Black action camera, with 4K60P HEVC video encoding.[111]\n\nOn October 17, 2017, Microsoft removed HEVC decoding support from Windows 10 with the Version 1709 Fall Creators Update, making HEVC available instead as a separate, paid download from the Microsoft Store.[112]\n\nOn November 2, 2017, Nvidia released the GeForce GTX 1070 Ti (GP104), which includes full fixed function HEVC Main10/Main12 hardware decoder.\n\nOn September 20, 2018, Nvidia released the GeForce RTX 2080 (TU104), which includes full fixed function HEVC Main 4:4:4 12 hardware decoder.\n\nOn October 25, 2022, Chrome released version 107, which starts supporting HEVC hardware decoding for all platforms \"out of the box\", if the hardware is supported.\n\nHEVC is implemented in these web browsers:\n• Edge (since version 77 from July 2017, supported on Windows 10 1709+ for devices with supported hardware when HEVC video extensions is installed, since version 107 from October 2022, supported on macOS 11+, Android 5.0+) 115\n• Chrome (since version 107 from October 2022, supported on macOS 11+, Android 5.0+, supported on Windows 7+, ChromeOS, and Linux for devices with supported hardware) 116\n• Opera (since version 94 from December 2022, supported on the same platforms as Chrome)\n\nIn June 2023, an estimated 88.31% of browsers in use on desktop and mobile systems were able to play HEVC videos in HTML5 webpages, based on data from Can I Use.[117]\n\nMost video coding standards are designed primarily to achieve the highest coding efficiency. Coding efficiency is the ability to encode video at the lowest possible bit rate while maintaining a certain level of video quality. There are two standard ways to measure the coding efficiency of a video coding standard, which are to use an objective metric, such as peak signal-to-noise ratio (PSNR), or to use subjective assessment of video quality. Subjective assessment of video quality is considered to be the most important way to measure a video coding standard since humans perceive video quality subjectively.\n\nHEVC benefits from the use of larger coding tree unit (CTU) sizes. This has been shown in PSNR tests with a HM-8.0 HEVC encoder where it was forced to use progressively smaller CTU sizes. For all test sequences, when compared with a 64×64 CTU size, it was shown that the HEVC bit rate increased by 2.2% when forced to use a 32×32 CTU size, and increased by 11.0% when forced to use a 16×16 CTU size. In the Class A test sequences, where the resolution of the video was 2560×1600, when compared with a 64×64 CTU size, it was shown that the HEVC bit rate increased by 5.7% when forced to use a 32×32 CTU size, and increased by 28.2% when forced to use a 16×16 CTU size. The tests showed that large CTU sizes increase coding efficiency while also reducing decoding time.\n\nThe HEVC Main Profile (MP) has been compared in coding efficiency to H.264/MPEG-4 AVC High Profile (HP), MPEG-4 Advanced Simple Profile (ASP), H.263 High Latency Profile (HLP), and H.262/MPEG-2 Main Profile (MP). The video encoding was done for entertainment applications and twelve different bitrates were made for the nine video test sequences with a HM-8.0 HEVC encoder being used. Of the nine video test sequences, five were at HD resolution, while four were at WVGA (800×480) resolution. The bit rate reductions for HEVC were determined based on PSNR with HEVC having a bit rate reduction of 35.4% compared with H.264/MPEG-4 AVC HP, 63.7% compared with MPEG-4 ASP, 65.1% compared with H.263 HLP, and 70.8% compared with H.262/MPEG-2 MP.\n\nHEVC MP has also been compared with H.264/MPEG-4 AVC HP for subjective video quality. The video encoding was done for entertainment applications and four different bitrates were made for nine video test sequences with a HM-5.0 HEVC encoder being used. The subjective assessment was done at an earlier date than the PSNR comparison and so it used an earlier version of the HEVC encoder that had slightly lower performance. The bit rate reductions were determined based on subjective assessment using mean opinion score values. The overall subjective bitrate reduction for HEVC MP compared with H.264/MPEG-4 AVC HP was 49.3%.\n\nÉcole Polytechnique Fédérale de Lausanne (EPFL) did a study to evaluate the subjective video quality of HEVC at resolutions higher than HDTV. The study was done with three videos with resolutions of 3840×1744 at 24 fps, 3840×2048 at 30 fps, and 3840×2160 at 30 fps. The five second video sequences showed people on a street, traffic, and a scene from the open source computer animated movie Sintel. The video sequences were encoded at five different bitrates using the HM-6.1.1 HEVC encoder and the JM-18.3 H.264/MPEG-4 AVC encoder. The subjective bit rate reductions were determined based on subjective assessment using mean opinion score values. The study compared HEVC MP with H.264/MPEG-4 AVC HP and showed that, for HEVC MP, the average bitrate reduction based on PSNR was 44.4%, while the average bitrate reduction based on subjective video quality was 66.5%.[124][125]\n\nIn a HEVC performance comparison released in April 2013, the HEVC MP and Main 10 Profile (M10P) were compared with H.264/MPEG-4 AVC HP and High 10 Profile (H10P) using 3840×2160 video sequences. The video sequences were encoded using the HM-10.0 HEVC encoder and the JM-18.4 H.264/MPEG-4 AVC encoder. The average bit rate reduction based on PSNR was 45% for inter frame video.\n\nIn a video encoder comparison released in December 2013, the HM-10.0 HEVC encoder was compared with the x264 encoder (version r2334) and the VP9 encoder (version v1.2.0-3088-ga81bd12). The comparison used the Bjøntegaard-Delta bit-rate (BD-BR) measurement method, in which negative values tell how much lower the bit rate is reduced, and positive values tell how much the bit rate is increased for the same PSNR. In the comparison, the HM-10.0 HEVC encoder had the highest coding efficiency and, on average, to get the same objective quality, the x264 encoder needed to increase the bit rate by 66.4%, while the VP9 encoder needed to increase the bit rate by 79.4%.[126]\n\nIn a subjective video performance comparison released in May 2014, the JCT-VC compared the HEVC Main profile to the H.264/MPEG-4 AVC High profile. The comparison used mean opinion score values and was conducted by the BBC and the University of the West of Scotland. The video sequences were encoded using the HM-12.1 HEVC encoder and the JM-18.5 H.264/MPEG-4 AVC encoder. The comparison used a range of resolutions and the average bit rate reduction for HEVC was 59%. The average bit rate reduction for HEVC was 52% for 480p, 56% for 720p, 62% for 1080p, and 64% for 4K UHD.[127]\n\nIn a subjective video codec comparison released in August 2014 by the EPFL, the HM-15.0 HEVC encoder was compared with the VP9 1.2.0–5183 encoder and the JM-18.8 H.264/MPEG-4 AVC encoder. Four 4K resolutions sequences were encoded at five different bit rates with the encoders set to use an intra period of one second. In the comparison, the HM-15.0 HEVC encoder had the highest coding efficiency and, on average, for the same subjective quality the bit rate could be reduced by 49.4% compared with the VP9 1.2.0–5183 encoder, and it could be reduced by 52.6% compared with the JM-18.8 H.264/MPEG-4 AVC encoder.[128][129][130]\n\nIn August, 2016, Netflix published the results of a large-scale study comparing the leading open-source HEVC encoder, x265, with the leading open-source AVC encoder, x264 and the reference VP9 encoder, libvpx.[131] Using their advanced Video Multimethod Assessment Fusion (VMAF) video quality measurement tool, Netflix found that x265 delivered identical quality at bit rates ranging from 35.4% to 53.3% lower than x264, and from 17.8% to 21.8% lower than VP9.[132]\n\nHEVC was designed to substantially improve coding efficiency compared with H.264/MPEG-4 AVC HP, i.e. to reduce bitrate requirements by half with comparable image quality, at the expense of increased computational complexity. HEVC was designed with the goal of allowing video content to have a data compression ratio of up to 1000:1.[133] Depending on the application requirements, HEVC encoders can trade off computational complexity, compression rate, robustness to errors, and encoding delay time. Two of the key features where HEVC was improved compared with H.264/MPEG-4 AVC was support for higher resolution video and improved parallel processing methods.\n\nHEVC is targeted at next-generation HDTV displays and content capture systems which feature progressive scanned frame rates and display resolutions from QVGA (320×240) to 4320p (7680×4320), as well as improved picture quality in terms of noise level, color spaces, and dynamic range.[21][134][135][136]\n\nThe HEVC video coding layer uses the same \"hybrid\" approach used in all modern video standards, starting from H.261, in that it uses inter-/intra-picture prediction and 2D transform coding. A HEVC encoder first proceeds by splitting a picture into block shaped regions for the first picture, or the first picture of a random access point, which uses intra-picture prediction. Intra-picture prediction is when the prediction of the blocks in the picture is based only on the information in that picture. For all other pictures, inter-picture prediction is used, in which prediction information is used from other pictures. After the prediction methods are finished and the picture goes through the loop filters, the final picture representation is stored in the decoded picture buffer. Pictures stored in the decoded picture buffer can be used for the prediction of other pictures.\n\nHEVC was designed with the idea that progressive scan video would be used and no coding tools were added specifically for interlaced video. Interlace specific coding tools, such as MBAFF and PAFF, are not supported in HEVC.[137] HEVC instead sends metadata that tells how the interlaced video was sent. Interlaced video may be sent either by coding each frame as a separate picture or by coding each field as a separate picture. For interlaced video HEVC can change between frame coding and field coding using Sequence Adaptive Frame Field (SAFF), which allows the coding mode to be changed for each video sequence.[138] This allows interlaced video to be sent with HEVC without needing special interlaced decoding processes to be added to HEVC decoders.\n\nThe HEVC standard supports color spaces such as generic film (colour filters using Illuminant C), NTSC, PAL, Rec. 601 (SMPTE 170M), Rec. 709, Rec. 2020, Rec. 2100, SMPTE 240M, sRGB, sYCC, xvYCC, XYZ, and externally specified color spaces such as Dolby Vision or HDR Vivid. HEVC supports color encoding representations such as RGB, YCbCr and ICtCp, and YCoCg.\n\nHEVC replaces 16×16 pixel macroblocks, which were used with previous standards, with coding tree units (CTUs) which can use larger block structures of up to 64×64 samples and can better sub-partition the picture into variable sized structures.[139] HEVC initially divides the picture into CTUs which can be 64×64, 32×32, or 16×16 with a larger pixel block size usually increasing the coding efficiency.\n\nHEVC specifies four transform units (TUs) sizes of 4×4, 8×8, 16×16, and 32×32 to code the prediction residual. A CTB may be recursively partitioned into 4 or more TUs. TUs use integer basis functions based on the discrete cosine transform (DCT).[2] In addition, 4×4 luma transform blocks that belong to an intra coded region are transformed using an integer transform that is derived from discrete sine transform (DST). This provides a 1% bit rate reduction but was restricted to 4×4 luma transform blocks due to marginal benefits for the other transform cases. Chroma uses the same TU sizes as luma so there is no 2×2 transform for chroma.\n• Tiles allow for the picture to be divided into a grid of rectangular regions that can independently be decoded/encoded. The main purpose of tiles is to allow for parallel processing. Tiles can be independently decoded and can even allow for random access to specific regions of a picture in a video stream.\n• Wavefront parallel processing (WPP) is when a slice is divided into rows of CTUs in which the first row is decoded normally but each additional row requires that decisions be made in the previous row. WPP has the entropy encoder use information from the preceding row of CTUs and allows for a method of parallel processing that may allow for better compression than tiles.\n• Tiles and WPP are allowed, but are optional. If tiles are present, they must be at least 64 pixels high and 256 pixels wide with a level specific limit on the number of tiles allowed.\n• Slices can, for the most part, be decoded independently from each other with the main purpose of tiles being the re-synchronization in case of data loss in the video stream. Slices can be defined as self-contained in that prediction is not made across slice boundaries. When in-loop filtering is done on a picture though, information across slice boundaries may be required. Slices are CTUs decoded in the order of the raster scan, and different coding types can be used for slices such as I types, P types, or B types.\n• Dependent slices can allow for data related to tiles or WPP to be accessed more quickly by the system than if the entire slice had to be decoded. The main purpose of dependent slices is to allow for low-delay video encoding due to its lower latency.\n\nHEVC uses a context-adaptive binary arithmetic coding (CABAC) algorithm that is fundamentally similar to CABAC in H.264/MPEG-4 AVC. CABAC is the only entropy encoder method that is allowed in HEVC while there are two entropy encoder methods allowed by H.264/MPEG-4 AVC. CABAC and the entropy coding of transform coefficients in HEVC were designed for a higher throughput than H.264/MPEG-4 AVC,[140] while maintaining higher compression efficiency for larger transform block sizes relative to simple extensions.[141] For instance, the number of context coded bins have been reduced by 8× and the CABAC bypass-mode has been improved in terms of its design to increase throughput.[140][142] Another improvement with HEVC is that the dependencies between the coded data has been changed to further increase throughput.[140] Context modeling in HEVC has also been improved so that CABAC can better select a context that increases efficiency when compared with H.264/MPEG-4 AVC.\n\nHEVC specifies 33 directional modes for intra prediction compared with the 8 directional modes for intra prediction specified by H.264/MPEG-4 AVC. HEVC also specifies DC intra prediction and planar prediction modes. The DC intra prediction mode generates a mean value by averaging reference samples and can be used for flat surfaces. The planar prediction mode in HEVC supports all block sizes defined in HEVC while the planar prediction mode in H.264/MPEG-4 AVC is limited to a block size of 16×16 pixels. The intra prediction modes use data from neighboring prediction blocks that have been previously decoded from within the same picture.\n\nFor the interpolation of fractional luma sample positions HEVC uses separable application of one-dimensional half-sample interpolation with an 8-tap filter or quarter-sample interpolation with a 7-tap filter while, in comparison, H.264/MPEG-4 AVC uses a two-stage process that first derives values at half-sample positions using separable one-dimensional 6-tap interpolation followed by integer rounding and then applies linear interpolation between values at nearby half-sample positions to generate values at quarter-sample positions. HEVC has improved precision due to the longer interpolation filter and the elimination of the intermediate rounding error. For 4:2:0 video, the chroma samples are interpolated with separable one-dimensional 4-tap filtering to generate eighth-sample precision, while in comparison H.264/MPEG-4 AVC uses only a 2-tap bilinear filter (also with eighth-sample precision).\n\nAs in H.264/MPEG-4 AVC, weighted prediction in HEVC can be used either with uni-prediction (in which a single prediction value is used) or bi-prediction (in which the prediction values from two prediction blocks are combined).\n\nHEVC defines a signed 16-bit range for both horizontal and vertical motion vectors (MVs).[143][144][145] This was added to HEVC at the July 2012 HEVC meeting with the mvLX variables.[143][144][145] HEVC horizontal/vertical MVs have a range of −32768 to 32767 which given the quarter pixel precision used by HEVC allows for a MV range of −8192 to 8191.75 luma samples.[143][144][145] This compares to H.264/MPEG-4 AVC which allows for a horizontal MV range of −2048 to 2047.75 luma samples and a vertical MV range of −512 to 511.75 luma samples.[144]\n\nHEVC allows for two MV modes which are Advanced Motion Vector Prediction (AMVP) and merge mode. AMVP uses data from the reference picture and can also use data from adjacent prediction blocks. The merge mode allows for the MVs to be inherited from neighboring prediction blocks. Merge mode in HEVC is similar to \"skipped\" and \"direct\" motion inference modes in H.264/MPEG-4 AVC but with two improvements. The first improvement is that HEVC uses index information to select one of several available candidates. The second improvement is that HEVC uses information from the reference picture list and reference picture index.\n\nHEVC specifies two loop filters that are applied sequentially, with the deblocking filter (DBF) applied first and the sample adaptive offset (SAO) filter applied afterwards. Both loop filters are applied in the inter-picture prediction loop, i.e. the filtered image is stored in the decoded picture buffer (DPB) as a reference for inter-picture prediction.\n\nThe DBF is similar to the one used by H.264/MPEG-4 AVC but with a simpler design and better support for parallel processing. In HEVC the DBF only applies to a 8×8 sample grid while with H.264/MPEG-4 AVC the DBF applies to a 4×4 sample grid. DBF uses a 8×8 sample grid since it causes no noticeable degradation and significantly improves parallel processing because the DBF no longer causes cascading interactions with other operations. Another change is that HEVC only allows for three DBF strengths of 0 to 2. HEVC also requires that the DBF first apply horizontal filtering for vertical edges to the picture and only after that does it apply vertical filtering for horizontal edges to the picture. This allows for multiple parallel threads to be used for the DBF.\n\nThe SAO filter is applied after the DBF and is designed to allow for better reconstruction of the original signal amplitudes by applying offsets stored in a lookup table in the bitstream.[146] Per CTB the SAO filter can be disabled or applied in one of two modes: edge offset mode or band offset mode.[146] The edge offset mode operates by comparing the value of a sample to two of its eight neighbors using one of four directional gradient patterns.[146] Based on a comparison with these two neighbors, the sample is classified into one of five categories: minimum, maximum, an edge with the sample having the lower value, an edge with the sample having the higher value, or monotonic.[146] For each of the first four categories an offset is applied.[146] The band offset mode applies an offset based on the amplitude of a single sample.[146] A sample is categorized by its amplitude into one of 32 bands (histogram bins).[146] Offsets are specified for four consecutive of the 32 bands, because in flat areas which are prone to banding artifacts, sample amplitudes tend to be clustered in a small range.[146] The SAO filter was designed to increase picture quality, reduce banding artifacts, and reduce ringing artifacts.[146]\n\nRange extensions in MPEG are additional profiles, levels, and techniques that support needs beyond consumer video playback:\n• Intra profiles for when file size is much less important than random-access decoding speed.\n• Still Picture profiles, forming the basis of High Efficiency Image File Format, without any limit on the picture size or complexity (level 8.5). Unlike all other levels, no minimum decoder capacity is required, only a best-effort with reasonable fallback.\n\nWithin these new profiles came enhanced coding features, many of which support efficient screen encoding or high-speed processing:\n• Cross-component prediction, allowing the imperfect YCbCr color decorrelation to let the luma (or G) match set the predicted chroma (or R/B) matches, which results in up to 7% gain for YCbCr 4:4:4 and up to 26% for RGB video. Particularly useful for screen coding. 147 148\n• Intra smoothing control, allowing the encoder to turn smoothing on or off per-block, instead of per-frame.\n• Modifications of transform skip:\n• Residual DPCM (RDPCM), allowing more-optimal coding of residual data if possible, vs the typical zig-zag.\n• Block size flexibility, supporting block sizes up to 32×32 (versus only 4×4 transform skip support in version 1).\n• Color remapping: mapping one color space to another. 149\n• Knee function: hints for converting between dynamic ranges, particularly from HDR to SDR.\n\nAdditional coding tool options have been added in the March 2016 draft of the screen content coding (SCC) extensions:[150]\n\nThe ITU-T version of the standard that added the SCC extensions (approved in December 2016 and published in March 2017) added support for the hybrid log–gamma (HLG) transfer function and the ICtCp color matrix.[65] This allows the fourth version of HEVC to support both of the HDR transfer functions defined in Rec. 2100.[65]\n\nThe fourth version of HEVC adds several supplemental enhancement information (SEI) messages which include:\n• Alternative transfer characteristics information SEI message, provides information on the preferred transfer function to use. 150 The primary use case for this would be to deliver HLG video in a way that would be backward compatible with legacy devices. 151\n• Ambient viewing environment SEI message, provides information on the ambient light of the viewing environment that was used to author the video. 150 152\n\nVersion 1 of the HEVC standard defines three profiles: Main, Main 10, and Main Still Picture. Version 2 of HEVC adds 21 range extensions profiles, two scalable extensions profiles, and one multi-view profile. HEVC also contains provisions for additional profiles. Extensions that were added to HEVC include increased bit depth, 4:2:2/4:4:4 chroma sampling, Multiview Video Coding (MVC), and Scalable Video Coding (SVC).[153] The HEVC range extensions, HEVC scalable extensions, and HEVC multi-view extensions were completed in July 2014.[154][155][156] In July 2014 a draft of the second version of HEVC was released.[154] Screen content coding (SCC) extensions were under development for screen content video, which contains text and graphics, with an expected final draft release date of 2015.[157][158]\n\nA profile is a defined set of coding tools that can be used to create a bitstream that conforms to that profile. An encoder for a profile may choose which coding tools to use as long as it generates a conforming bitstream while a decoder for a profile must support all coding tools that can be used in that profile.\n\nThe Main profile allows for a bit depth of 8 bits per sample with 4:2:0 chroma sampling, which is the most common type of video used with consumer devices.[155]\n\nThe Main 10 ( ) profile was added at the October 2012 HEVC meeting based on proposal JCTVC-K0109 which proposed that a 10-bit profile be added to HEVC for consumer applications. The proposal said this was to allow for improved video quality and to support the Rec. 2020 color space that has become widely used in UHDTV systems and to be able to deliver higher dynamic range and color fidelity avoiding the banding artifacts. A variety of companies supported the proposal which included Ateme, BBC, BSkyB, Cisco, DirecTV, Ericsson, Motorola Mobility, NGCodec, NHK, RAI, ST, SVT, Thomson Video Networks, Technicolor, and ViXS Systems.[159] The Main 10 profile allows for a bit depth of 8 to 10 bits per sample with 4:2:0 chroma sampling. HEVC decoders that conform to the Main 10 profile must be capable of decoding bitstreams made with the following profiles: Main and Main 10. A higher bit depth allows for a greater number of colors. 8 bits per sample allows for 256 shades per primary color (a total of 16.78 million colors) while 10 bits per sample allows for 1024 shades per primary color (a total of 1.07 billion colors). A higher bit depth allows for a smoother transition of color which resolves the problem known as color banding.[160][161]\n\nThe Main 10 profile allows for improved video quality since it can support video with a higher bit depth than what is supported by the Main profile.[159] Additionally, in the Main 10 profile 8-bit video can be coded with a higher bit depth of 10 bits, which allows improved coding efficiency compared to the Main profile.[162][163][164]\n\nEricsson said the Main 10 profile would bring the benefits of 10 bits per sample video to consumer TV. They also said that for higher resolutions there is no bit rate penalty for encoding video at 10 bits per sample.[160] Imagination Technologies said that 10-bit per sample video would allow for larger color spaces and is required for the Rec. 2020 color space that will be used by UHDTV. They also said the Rec. 2020 color space would drive the widespread adoption of 10-bit-per-sample video.[161][165]\n\nIn a PSNR based performance comparison released in April 2013 the Main 10 profile was compared to the Main profile using a set of 3840×2160 10-bit video sequences. The 10-bit video sequences were converted to 8 bits for the Main profile and remained at 10 bits for the Main 10 profile. The reference PSNR was based on the original 10-bit video sequences. In the performance comparison the Main 10 profile provided a 5% bit rate reduction for inter frame video coding compared to the Main profile. The performance comparison states that for the tested video sequences the Main 10 profile outperformed the Main profile.[166]\n\nThe Main Still Picture ( ) profile allows for a single still picture to be encoded with the same constraints as the Main profile. As a subset of the Main profile the Main Still Picture profile allows for a bit depth of 8 bits per sample with 4:2:0 chroma sampling.[155] An objective performance comparison was done in April 2012 in which HEVC reduced the average bit rate for images by 56% compared to JPEG.[168] A PSNR based performance comparison for still image compression was done in May 2012 using the HEVC HM 6.0 encoder and the reference software encoders for the other standards. For still images HEVC reduced the average bit rate by 15.8% compared to H.264/MPEG-4 AVC, 22.6% compared to JPEG 2000, 30.0% compared to JPEG XR, 31.0% compared to WebP, and 43.0% compared to JPEG.[169]\n\nA performance comparison for still image compression was done in January 2013 using the HEVC HM 8.0rc2 encoder, Kakadu version 6.0 for JPEG 2000, and IJG version 6b for JPEG. The performance comparison used PSNR for the objective assessment and mean opinion score (MOS) values for the subjective assessment. The subjective assessment used the same test methodology and images as those used by the JPEG committee when it evaluated JPEG XR. For 4:2:0 chroma sampled images the average bit rate reduction for HEVC compared to JPEG 2000 was 20.26% for PSNR and 30.96% for MOS while compared to JPEG it was 61.63% for PSNR and 43.10% for MOS.[167]\n\nA PSNR based HEVC performance comparison for still image compression was done in April 2013 by Nokia. HEVC has a larger performance improvement for higher resolution images than lower resolution images and a larger performance improvement for lower bit rates than higher bit rates. For lossy compression to get the same PSNR as HEVC took on average 1.4× more bits with JPEG 2000, 1.6× more bits with JPEG-XR, and 2.3× more bits with JPEG.[170]\n\nA compression efficiency study of HEVC, JPEG, JPEG XR, and WebP was done in October 2013 by Mozilla. The study showed that HEVC was significantly better at compression than the other image formats that were tested. Four different methods for comparing image quality were used in the study which were Y-SSIM, RGB-SSIM, IW-SSIM, and PSNR-HVS-M.[171][172]\n\nVersion 2 of HEVC adds 21 range extensions profiles, two scalable extensions profiles, and one multi-view profile: Monochrome, Monochrome 12, Monochrome 16, Main 12, Main 4:2:2 10, Main 4:2:2 12, Main 4:4:4, Main 4:4:4 10, Main 4:4:4 12, Monochrome 12 Intra, Monochrome 16 Intra, Main 12 Intra, Main 4:2:2 10 Intra, Main 4:2:2 12 Intra, Main 4:4:4 Intra, Main 4:4:4 10 Intra, Main 4:4:4 12 Intra, Main 4:4:4 16 Intra, Main 4:4:4 Still Picture, Main 4:4:4 16 Still Picture, High Throughput 4:4:4 16 Intra, Scalable Main, Scalable Main 10, and Multiview Main.[173] All of the inter frame range extensions profiles have an Intra profile.\n\nVersion 3 of HEVC added one 3D profile: 3D Main. The February 2016 draft of the screen content coding extensions added seven screen content coding extensions profiles, three high throughput extensions profiles, and four scalable extensions profiles: Screen-Extended Main, Screen-Extended Main 10, Screen-Extended Main 4:4:4, Screen-Extended Main 4:4:4 10, Screen-Extended High Throughput 4:4:4, Screen-Extended High Throughput 4:4:4 10, Screen-Extended High Throughput 4:4:4 14, High Throughput 4:4:4, High Throughput 4:4:4 10, High Throughput 4:4:4 14, Scalable Monochrome, Scalable Monochrome 12, Scalable Monochrome 16, and Scalable Main 4:4:4.[150]\n\nThe HEVC standard defines two tiers, Main and High, and thirteen levels. A level is a set of constraints for a bitstream. For levels below level 4 only the Main tier is allowed. The Main tier is a lower tier than the High tier. The tiers were made to deal with applications that differ in terms of their maximum bit rate. The Main tier was designed for most applications while the High tier was designed for very demanding applications. A decoder that conforms to a given tier/level is required to be capable of decoding all bitstreams that are encoded for that tier/level and for all lower tiers/levels.\n\nPreviously decoded pictures are stored in a decoded picture buffer (DPB), and are used by HEVC encoders to form predictions for subsequent pictures. The maximum number of pictures that can be stored in the DPB, called the DPB capacity, is 6 (including the current picture) for all HEVC levels when operating at the maximum picture size supported by the level. The DPB capacity (in units of pictures) increases from 6 to 8, 12, or 16 as the picture size decreases from the maximum picture size supported by the level. The encoder selects which specific pictures are retained in the DPB on a picture-by-picture basis, so the encoder has the flexibility to determine for itself the best way to use the DPB capacity when encoding the video content.\n\nMPEG has published an amendment which added HEVC support to the MPEG transport stream used by ATSC, DVB, and Blu-ray Disc; MPEG decided not to update the MPEG program stream used by DVD-Video.[175][176] MPEG has also added HEVC support to the ISO base media file format.[177][178] HEVC is also supported by the MPEG media transport standard.[175][179] Support for HEVC was added to Matroska starting with the release of MKVToolNix v6.8.0 after a patch from DivX was merged.[180][181] A draft document has been submitted to the Internet Engineering Task Force which describes a method to add HEVC support to the Real-time Transport Protocol.[182]\n\nUsing HEVC's intra frame encoding, a still-image coded format called Better Portable Graphics (BPG) has been proposed by the programmer Fabrice Bellard.[183] It is essentially a wrapper for images coded using the HEVC Main 4:4:4 16 Still Picture profile with up to 14 bits per sample, although it uses an abbreviated header syntax and adds explicit support for Exif, ICC profiles, and XMP metadata.[183][184]\n\nLicense terms and fees for HEVC patents, compared with its main competitors:\n\nAs with its predecessor AVC, software distributors that implement HEVC in products must pay a price per distributed copy.[i] While this licensing model is manageable for paid software, it is an obstacle to most free and open-source software, which is meant to be freely distributable. In the opinion of MulticoreWare, the developer of x265, enabling royalty-free software encoders and decoders is in the interest of accelerating HEVC adoption.[191][195][196] HEVC Advance made an exception that specifically waives the royalties on software-only implementations (both decoders and encoders) when not bundled with hardware.[197] However, the exempted software is not free from the licensing obligations of other patent holders (e.g. members of the MPEG LA pool).\n\nWhile the obstacle to free software is no concern in for example TV broadcast networks, this problem, combined with the prospect of future collective lock-in to the format, makes several organizations like Mozilla (see OpenH264) and the Free Software Foundation Europe[198] wary of royalty-bearing formats for internet use. Competing formats intended for internet use (VP9 and AV1) are intended to steer clear of these concerns by being royalty free (provided there are no third-party claims of patent rights).\n\n^i : Regardless of how the software is licensed from the software authors (see software licensing), if what it does is patented, its use remains bound by the patent holders' rights unless the use of the patents has been authorized by a license.\n\nIn October 2015, MPEG and VCEG formed Joint Video Exploration Team (JVET)[199] to evaluate available compression technologies and study the requirements for a next-generation video compression standard. The new algorithm should have 30–50% better compression rate for the same perceptual quality, with support for lossless and subjectively lossless compression. It should also support YCbCr 4:4:4, 4:2:2 and 4:2:0 with 10 to 16 bits per component, BT.2100 wide color gamut and high dynamic range (HDR) of more than 16 stops (with peak brightness of 1,000, 4,000 and 10,000 nits), auxiliary channels (for depth, transparency, etc.), variable and fractional frame rates from 0 to 120 Hz, scalable video coding for temporal (frame rate), spatial (resolution), SNR, color gamut and dynamic range differences, stereo/multiview coding, panoramic formats, and still picture coding. Encoding complexity of 10 times that of HEVC is expected. JVET issued a final \"Call for Proposals\" in October 2017, with the first working draft of the Versatile Video Coding (VVC) standard released in April 2018.[200][201] The VVC standard was finalized on July 6, 2020.[202]\n• UHDTV – digital television formats with resolutions of 4K / 2160p (3840×2160) and 8K / 4320p (7680×4320)\n• Rec. 2100 – ITU-R Recommendation for HDTV and UHDTV with high dynamic range\n• Image file formats based on HEVC\n• Better Portable Graphics – a file format for images based on HEVC\n• High Efficiency Image File Format – a file format for images and image sequences based on HEVC\n• List of multimedia (audio/video) codecs\n• AV1 – an open format developed by the Alliance for Open Media as a successor to VP9 and a competitor to HEVC\n• VP9 – an open format developed by Google as a competitor to HEVC\n• Daala – an open format that is being developed by Mozilla Foundation and Xiph.Org Foundation as a competitor to HEVC\n• Dirac (video compression format) – an open format that is being developed by the BBC Research & Development as a competitor to HEVC\n• Thor (video codec) – an open format that is being developed by Cisco as a competitor to HEVC\n• None G. J. Sullivan; J.-R. Ohm; W.-J. Han; T. Wiegand (December 2012). \"Overview of the High Efficiency Video Coding (HEVC) Standard\". IEEE Transactions on Circuits and Systems for Video Technology. 22 (12). IEEE: 1668. doi: .\n• None J.-R. Ohm; G. J. Sullivan; H. Schwarz; T. K. Tan; T. Wiegand (December 2012). \"Comparison of the Coding Efficiency of Video Coding Standards – Including High Efficiency Video Coding (HEVC)\" . IEEE Transactions on Circuits and Systems for Video Technology. 22 (12). IEEE .\n• None Vivienne Sze; Madhukar Budagavi; G. J. Sullivan (2014). \"High Efficiency Video Coding (HEVC): Algorithms and Architectures\". Integrated Circuit and Systems. Integrated Circuits and Systems. Springer. doi:10.1007/978-3-319-06895-4. ISBN .\n• None Gerhard Tech; Ying Chen; Karsten Müller; Jens-Rainer Ohm; Anthony Vetro; Ye-Kui Wang (January 2016). \"Overview of the Multiview and 3D Extensions of High Efficiency Video Coding\" . IEEE Transactions on Circuits and Systems for Video Technology. 26 (1). IEEE: 49. doi:10.1109/TCSVT.2015.2477935. S2CID 750942."
    },
    {
        "link": "https://deeprender.ai/blog/motion-compensation-and-prediction-h265",
        "document": "tl;dr We outline inter-frame compression techniques in the H.26x family of coding formats. We emphasize the motion compensation and motion prediction modules, with an emphasis on the H.265 (HEVC) format.\n\nTraditional video compression achieves remarkable compression rates by exploiting temporal correlations between successive video frames. In this blog post, we will provide a high-level overview of how exactly this is done. We will focus on the H.26x series of video coding formats, and will pay particular attention to H.265 (or HEVC), standardized in 2013.\n\nAll video compression algorithms (both traditional and AI-based) have used motion compensation in some form or another. The idea is quite simple: suppose you have a series of video frames, and you know the motion vectors which transform each successive frame to the next. Rather than encoding each frame individually, encode instead only the relative difference between each frame.\n\n\n\nIn particular, it is the relative difference between the prior frame warped by the known motion vector, and the current frame, which is encoded. To illustrate: suppose we have a ground-truth frame $$x_t$$ at time $$t$$, and the reconstructed frame $$\\hat x_{t-1}$$ at time $$t-1$$. Suppose also we have somehow deduced the motion vector $$v$$ which accounts for (most of) the changes between these two frames. Then, it is the residual difference\n\n\n\nbetween these frames which is actually encoded. Here $$\\mathscr F(\\cdot\\;, v)$$ is the function which transforms an image under a motion vector $$v$$. This is variously called motion compensation (in video compression), a flow (math, physics and elsewhere), and warping (in computer vision). In traditional compression, this residual is chopped into blocks, quantized and encoded via the DCT transform; in AI-based compression the residual is encoded via a neural-network with side-information.\n\nAt decode time, the reconstructed frame $$\\hat x_t$$ is recovered simply as\n\n\n\nwhere $$\\hat r_t$$ is the decoded residual. This process is then repeated for each successive frame.\n\nWhy encode the residual rather than each individual frame? Simply because the residuals are typically very sparse, and therefore extremely easy to entropy encode. For example, suppose we have the following example of a moving ball:\n\nWe know the previous decoded frame, the motion vector, and the ground-truth current frame. Therefore, we compensate the prior frame using the motion vector, and calculate their difference:\n\nIn this contrived example, the difference is all zeros (black), and hence is extremely easy to entropy encode.\n\nUp until this point, we have assumed that the motion vector is given to us. Of course, this is almost never the case. Instead, we must use some sort of motion estimation algorithm to calculate the motion vectors.\n\n\n\nIn traditional compression, including the H.26x formats, a block-based approach is used. Each image is cut up into macroblocks of $$N \\times N$$ pixels (eg, $$16 \\times 16$$ or $$8 \\times 8 $$). Then, for each block in the current frame, a search algorithm is used to find the closest matching block in the previous frame. The relative change between the current block and the closest matching block in the previous frame exactly defines the motion vector. The final result of this procedure is illustrated in the following figure. Note that the matched block on the previous frame does not necessarily lie on the fixed macroblock grid.\n\nFigure 1. A search is performed over all blocks of the preceding image to determine the best matching prior block. The best matching block determines the motion vector.\n\nThere are countless motion estimation algorithms (also known as optical flow estimation), and a discussion of these algorithms is outside the scope of this blog post. Interestingly, the H.26x formats do not specify a motion estimation algorithm; they let each codec implementation choose which motion estimation algorithm to use.\n\nThe final result is that each macroblock in the current frame is given a motion vector, which is then used to encode the frame residual (1).\n\nHowever, there is no free lunch. The motion vectors $$v$$ must themselves be encoded and decoded, for how else can the reconstruction $$\\hat x_t$$ be recovered from the motion compensated prior frame? Note that at decode, for Equation (2) to be executed, we actually need $$v$$!\n\nIn practice, it is too expensive to code the motion vectors themselves. Indeed, naively encoding motion vectors can incur more cost than straight-up encoding standalone images! So, much like image blocks, we instead code the residuals between the motion vectors and predictions of the motion vectors. These predictions are typically denoted as $$MV$$. The exact details of how motion vector predictions are generated depend on the video format. However, all prediction methods rely on previously decoded motion vectors, much like block intra predictions in traditional image compression. We will detail per-format specifics in the following sections.\n\nHaving calculated the motion predictions, only then can the motion vector residual be computed. The residual between the ground-truth motion vector $$v$$, and the prediction $$MV$$ is known appropriately as the Motion Vector Difference ($$MVD$$):\n\n\n\nThe $$MVD$$ is quantized, and sent in the bitstream via a custom entropy encoding module (who's details are rather technical and will not be covered here).\n\nAs a side remark, we note that it is the reconstructed motion vector $$\\hat v = MV + \\widehat{MVD}$$ which is used to actually compute the motion compensation residuals in Equation (1), and for image recovery in Equation (2). We emphasize ground truth motion vector $$v$$ is not used, for it is not available at decode time.\n\nIn summary, at decode time, the entire reconstruction process proceeds as follows.\n• $$\\widehat{MVD}$$ is recovered from the bitstream\n• Motion vector predictions $$MV$$ are generated using previously decode information (more on this below)\n• Motion compensated residuals $$\\hat r_t$$ are recovered from the bistream\n• The actual image is recovered via $$\\hat x_t = \\mathscr F(\\hat x_{t-1}, \\hat v) + \\hat r_t$$\n\nTo illustrate motion prediction in practice, we now turn to two older (simpler and easy-to-understand) formats, H.261 (standardized in 1988!) and H.263 (standardized in 1996).\n\nH.261 was the first truly practical video compression format. It uses a uniform macroblock grid structure for each of the channels. The macroblocks in each frame naturally have a correspondence to prior frames, since they remain unchanged.\n\nFor motion prediction, H.261 takes the simplest approach. Frames are decoded in their temporal order. H.261 exploits the fact that not only are there strong temporal correlations between frames, but also between motion vectors. Therefore, H.261 defines the motion vector prediction of the current block to simply be the motion vector of the previous block $$MV=\\hat v_{t-1}$$. Therefore, $$MVD$$ in this case is just the difference between successive motion vectors of the same macroblock:\n\n\n\nWe emphasize H.261 motion predictions rely on temporal correlations between the motion vectors. Refer to Figure 2.\n\nFigure 2. Illustration of the blocks (blue for temporal and red for spatial) used for motion vector prediction between different coding methods. For H.261 (left), only the same block in the previous frame (MV0) is considered, whereas for H.263 (right), previously decoded spatially neighbouring blocks (MV0, MV1, MV2) are considered.\n\nH.263 also uses a uniform macroblock grid structure to partition the input image. One of the key differences, however, between H.261 and H.263 is that H.263 forgoes the use of previous temporal motion vectors, and instead takes advantage of the spatial correlations between neighbouring blocks. This is very similar in spirit to intra-predictions in traditional image codecs. More precisely, with Figure 2 in mind, the $$MV$$ assigned to the current block is given by the component-wise medians of the $$MV$$s of the neighbouring blocks. In other words:\n\nIn the case where a neighbouring block falls out of the image domain, the motion vector for that block is set to $$(0,0)$$. Using the neighbouring blocks allows for better spatial correlation in the prediction motion vectors, making the resulting motion vector differences easier to encode.\n\nFurthermore, for better accuracy of the flow, H.263 also allows for non-integer directions. That is, the motion-compensated reference macroblock can lie off the pixel-grid (refer to Figure 1). Motion compensation is then achieved via interpolation onto the pixel-grid.\n\nWe now turn to H.265, also referred to High Efficient Video Coding (HEVC), standardized in 2013. H.265 has the distinction of being the most recent H.26x format with some adoption (accounting for ~10% of all video streams in 2020 [4]). Though it is not as predominate as its older sibling H.264 (AVC), it is more modern, with good 4k support.\n\nH.265 follows the same development trend as older formats in the H.26x series: it builds on prior existing components, incrementally changing various components of the pipeline. Among the changes introduced, the most significant for inter-frames are: variable macroblock sizes, Advanced Motion Vector Prediction (AVMP), and block merging. Each of these changes will be discussed in this section.\n\nFigure 3. Diagram of quadtree used to partition an image into variable size blocks. Left: Visualization of underlying tree structure. Right: The resulting partition.\n\nOne of the major contributions of HEVC is its shift away from a structured grid of macroblocks of a fixed size (as in H.261 and H.263), to an irregularly structured grid of macroblocks of varying sizes.\n\nEach frame is divided into $$64 \\times 64$$ blocks, called Coding Tree Units (CTUs). Then, depending on the complexity of the frame, the frame is recursively broken down into smaller blocks, called Coding Units (CUs). This is done by halving the resolution at each step using a quadtree (refer to Figure 3). The smallest allowable block size is $$4 \\times 4$$. The actual decision on whether or not to further partition a particular branch (CU) in a tree is based on optimization of rate and distortion. This results in a semantically meaningful quadtree partition, and much improved rate-distortion curves, but comes at the cost of a higher computational overhead relative to a uniform macroblock grid.\n\nFigure 4. Illustration of block candidates used for motion prediction in HEVC. Red blocks are spatially neighbouring blocks and blue blocks are temporally neighbouring blocks. Note that in this case, the grid of macroblocks is unstructured due to the varying block sizes.\n\nAdvanced Motion Vector Prediction (AVMP) is the method used by HEVC to generate motion vector predictions, which are used to compute motion vector differences (as described earlier). AMVP generates MVs by considering candidates of motion vectors at a time, among which a select few will be chosen. For a given block, the candidates are stored in a list as indices. When an appropriate block has been chosen, its index is sent in the bitstream. During decoding, the decoder will generate the same list of candidates and will have access to the index pointing to the block that was chosen by the encoder, thereby knowing exactly which MV to select.\n\nAs opposed to H.261 and H.263, block candidates are drawn from both spatially and temporally neighbouring blocks. Referring to Figure 4, spatial candidates $$\\{MVa0, MVa1, MVa2, MVb0, MVb1\\}$$ are the adjacent blocks found on top and to the left of the current block. Since we cannot look to the right or bottom of the current block (as these will not have been decoded yet), HEVC also considers blocks from the previously decoded frame ($${MVt0, MVt1}$$). Moreover, one of these temporal blocks is a block that is co-located with the current block ($$MVt1$$). The choice of these blocks as candidates was made from extensive ablations.\n\nThe number of blocks chosen from the candidates varies, but in general is up to two spatial blocks (one from $$\\{MVa0, MVa1, MVa2\\}$$ and one from $$\\{MVb0, MVb1\\}$$) and one temporal block. Among these blocks, a final block is chosen. The final block is chosen to be the best block minimizing the MVD residual, and this choice is transmitted in the bitstream. However, before transmitting, the MVs are scaled proportional to their temporal distance from the current frame [2]. The temporal distance is computed based on the difference between the temporal index of the current block and the temporal index of the chosen block. The reason this is necessary is because some neighbouring blocks may use an MV from an even earlier frame. To take this into account, HEVC scales the MVs by weighting them according to this distance.\n\nThe motion vector difference is computed the same way as in H.261 and H.263, and it is this value which is quantized and encoded.\n\nFigure 5. Depiction of excessive refinement from the quadtree partitioning, and the resulting simplification. A frame of a swinging pendulum (left), whose motion is indicated by a white arrow, is partitioned with a quadtree (center). In regions where the same or no motion occurs (along and away from the arm, respectively), there is redundant partitioning. Block merging results in a simplified partition (right), that consolidates redundant information between neighbouring blocks. Source: High Efficiency Video Coding (page 121, Figure 5.6) [2].\n\nThe added robustness of having variable sized macroblocks does not come for free. One unfavourable outcome is that the resulting tree may have regions that are too refined. In other words, there may be regions in which neighbouring blocks may share motion information, thereby making it pointless to separate them with a border. Consider Figure 5, for example, in which a frame of a moving pendulum, naively partitioned with a quadtree, may needlessly refine regions that contain no motion (i.e. motion vectors that are $$(0,0)$$). One can eliminate redundancies by merging some blocks together. This does not just apply to adjacent regions that have zero motion; movement along the pendulum arm, for neighbouring blocks, is roughly the same as well.\n\nHEVC addresses the above concern by introducing a block merging module into the pipeline to eliminate and reduce redundancies in coding. Adjacent blocks with identical, or nearly identical, motion vectors are \"merged\" into one large unit, who all share the same underlying motion vector. This merging procedure has the effect of massively reducing the amount of information needed to encode motion vectors.\n\nThe block merging module is similar to the process used by AVMP. It operates by considering several candidates for block merging at a time. These candidates are identical to those used for AVMP (shown in Figure 4). As in AVMP, only a select few are chosen from the available candidates. In this case, it is up to four of the spatial blocks, and one of the temporal blocks. Blocks are considered in a sequential order, gathering data that is not repeated among other blocks. If the current block's motion vector is very similar to a prior candidate block, the two are merged, and share the same underlying motion vector.\n\nIn addition to merging, H.265 has a skip feature, which is a flag to indicate that the motion vector of the current block has not changed from the previous frame. This is particularly important for static regions of the image.\n\nHere we briefly remark on motion compensation in the latest of the H.26x formats, standardized in 2020. H.266 builds atop H.265 by considering affine block transformations. All previous H.26x formats performed motion estimation by translating macroblocks -- the translation being given by the motion vector. H.266 extends this concept by allowing for affine transformations of each macroblock. In other words, macroblocks may be translated, resized, rotated, or sheared. This introduces a very heavy computational cost at encode time (the motion estimation algorithm now has a very large search space), and necessitates encoding additional affine information into the bitstream (all details of the affine transform, not just the motion vector, must be encoded). H.266 devotes much effort into overcoming these problems introduced by this added complexity.\n\nIn this blog post we outlined two key techniques of traditional video compression, motion compensation and motion prediction. We illustrated these techniques via the H.26x family of compression formats, with an emphasis on H.265. With each successive format, we see an increase in complexity of the motion compensation and motion prediction algorithms.\n\nVideo Coding Standards - from H.261 to MPEG1,2,4,7 - to H.265 MPEG-H\n\n[1] Zhang, Yongfei, et al. ‘Recent Advances on HEVC Inter-Frame Coding: From Optimization to Implementation and Beyond’. IEEE Transactions on Circuits and Systems for Video Technology, vol. 30, no. 11, Nov. 2020, pp. 4321–39. DOI.org (Crossref), https://doi.org/10.1109/TCSVT.2019.2954474.\n\n[3] Li, Ze-Nian, and Mark S. Drew. Fundamentals of Multimedia. Second edition, Springer, 2014.\n\n[4] Traci Ruether, 'Video Codecs and Encoding: Everything You Should Know', 2021. URL https://www.wowza.com/blog/video-codecs-encoding, accessed 2022-03-18."
    },
    {
        "link": "https://vcodex.com/hevc-an-introduction-to-high-efficiency-coding",
        "document": "High Efficiency Video Coding (HEVC) is a new standard for video compression that has the potential to deliver better performance than earlier standards such as H.264/AVC.\n\nSource video, consisting of a sequence of video frames, is encoded or compressed by an HEVC video encoder to create a compressed video bitstream. The compressed bitstream is stored or transmitted. A video decoder decompresses the bitstream to create a sequence of decoded frames.\n\nHEVC has the same basic structure as previous standards such as MPEG-2 Video and H.264/AVC. However, HEVC contains many incremental improvements such as:\n• None More flexible partitioning, from large to small partition sizes\n• None More sophisticated prediction and signaling of modes and motion vectors\n\nThe result is a video coding standard that can enable better compression, at the cost of potentially increased processing power.\n• None An international standard for video compression. Developed by a working group of ISO/IEC MPEG (Moving Picture Experts Group) and ITU-T VCEG (Video Coding Experts Group), HEVC is an international standard, jointly published as ISO/IEC 23008-2 and ITU-T Recommendation H.265. HEVC is published as a document (the standard itself) together with a reference software implementation (the test model, HM).\n• None A format for compressed video. The HEVC standard specifies a format for compressed or encoded video sequences, together with a method for decoding this format. An HEVC-compatible video sequence should (a) meet the specification of the compressed video format and (b) be correctly decode-able using the method described in the standard. HEVC video sequences can be stored in media files, streamed over the internet, transmitted by broadcast, etc.\n• None A set of tools or methods for video compression. HEVC specifies a number of methods or tools that may be used by a video compression encoder. It’s up to the designer of the encoder which tools are actually used, and how they are applied\n• None Better video compression. Depending on how the tools are used, HEVC has the potential to offer significantly higher compression than earlier standards such as H.264 / AVC. Achieving the best possible compression is likely to require significant computational resources.\n\n3. Why do we need it?\n\nHEVC aims to provide a step change improvement in video compression compared with earlier standards. HEVC’s predecessor, the H.264/AVC standard, was first published in 2003. Since then, digital video has become increasingly ubiquitous. High Definition is now the norm for many devices and applications. HEVC was developed to address the following trends:\n• None Widespread use of digital video, at increasingly high resolutions, which puts a significant strain on network capacity.\n• None Increasing use of video resolutions beyond HD, which will increase the burden on networks and storage even further.\n• None Continuing improvements in processing capacity. In 2013, a mobile handset or tablet is likely to have more computing power than a desktop computer from 2003.\n\nWith these issues in mind, a new video compression standard that makes use of higher computational capacities to enable more efficient handling of high resolution video is an attractive proposition. With HEVC, it should be possible to store or transmit video more efficiently than with earlier technologies such as H.264. This means:\n• None At the same picture size and quality, an HEVC video sequence should occupy less storage or transmission capacity than the equiv lent H.264 video sequence.\n• None At the same storage or transmission bandwidth, the quality and/or resolution of an HEVC video sequence should be higher than the corresponding H.264 video sequence.\n\nHEVC is based on the same general structure as previous standards. Source video, consisting of a sequence of video frames, is encoded or compressed by a video encoder to create a compressed video bitstream. The compressed bitstream is stored or transmitted. A video decoder decompresses the bitstream to create a sequence of decoded frames.\n\nThe steps carried out by a video encoder (Figure 2) include:\n• None Predicting each unit using inter or intra prediction, and subtracting the prediction from the unit\n• None Transforming and quantizing the residual (the difference between the original picture unit and the prediction)\n• None Entropy decoding and extracting the elements of the coded sequence\n• None Predicting each unit and adding the prediction to the output of the inverse transform\n\nThe HEVC standard defines (ii) the syntax or format of a compressed video sequence and (ii) a method of decoding a compressed sequence. The actual design of the encoder is not standardised.\n\nHEVC supports highly flexible partitioning of a video sequence. Each frame of the sequence is split up into rectangular or square regions (Units or Blocks), each of which is predicted from previously coded data. After prediction, any residual information is transformed and entropy encoded.\n\nEach coded video frame, or picture, is partitioned into Tiles and/or Slices, which are further partitioned into Coding Tree Units (CTUs). The CTU is the basic unit of coding, analogous to the Macroblock in earlier standards, and can be up to 64x64 pixels in siz .\n\nA Coding Tree Unit can be subdivided into square regions known s Coding Units (CUs) using a quadtree structure (Figure 3). Each CU is predicted using Inter or Intra prediction and transformed using one or more Transform Units (see below).\n\nFigure 4 shows a video frame partitioned into slices, with one slice highlighted in blue. The highlighted slice contains six 64x64 CTUs.\n\nFigure 5 shows a close-up of the CTU highlighted in Figure 4. The 64x64 CTU is split into four 32x32 regions, with the top-left 32x32 CU highlighted. In the other four quarters, the 32x32 region is split further, to 16x16 or 8x8 CUs.\n\nFrames of video are coded using Intra or Inter prediction. Figure 6 shows a sequence of coded video frames or coded pictures. The first picture (0) is coded using Intra prediction only, using spatial prediction from other regions of the same picture. Subsequent pictures are predicted from one, two or more reference pictures, using Inte and/or Intra prediction for each Prediction Unit (PU). The prediction sources for each picture are indicated by arrows.\n\nEach Coding Unit (CU) is partitioned into one or more Prediction Units (PUs), each of which is predicted using Intra or Inter prediction.\n\nIntra prediction: Each PU is predicted from neighbouring image data in the same picture, using DC prediction (an average value for the PU), planar prediction (fitting a plane surface to the PU) or directional prediction (extrapolating from neighbouring data).\n\nInter prediction: Each PU is predicted from image data in one or two reference pictures (before or after the current picture in display order), using motion compensated prediction. Motion vectors have up to quarter-sample resolution (luma component).\n\nFigure 7 shows two examples of Prediction Units. The CTU in the centre of the Figure is predicted using a single 64x64 PU. All the samples in this PU are predicted using the same motion compensated inter prediction from one or two reference frames. Shown on the right is an 8x16 PU, which is part of the prediction structure for a 32x32 CU.\n\nVcodex is led by Professor Iain Richardson, an internationally known expert on the MPEG and H.264 video compression standards. Based in Delft, The Netherlands, he frequently travels to the US and Europe.\n\nIain Richardson is an internationally recognised expert on video compression and digital video communications. He is the author of four other books about video coding which include two widely-cited books on the H.264 Advanced Video Coding standard. For over thirty years, he has carried out research in the field of video compression and video communications, as a Professor at the Robert Gordon University in Aberdeen, Scotland and as an independent consultant with his own company, Vcodex. He advises companies on video compression technology and is sought after as an expert witness in litigation cases involving video coding."
    },
    {
        "link": "https://x265.readthedocs.io/_/downloads/en/release_3.2/epub",
        "document": ""
    },
    {
        "link": "https://loc.gov/preservation/digital/formats/fdd/fdd000530.shtml",
        "document": "Terminology The terminology used in the HEVC specification and descriptions of the encoding is complex and sometimes extends or modifies terms found in glossaries such as the Glossary of Video Terms and Acronyms from Tektronix, the FADGI Glossary, or the Wikipedia entry for Video compression picture types. The notes below relate to terms for which the compilers of this resource did not find definitions in these sources that covered HEVC usage.\n• Quadtree (often hyphenated). A mathematical term for a tree in which a parent node can be split into four child nodes, each of which may become parent node for another split into four child nodes. For an example of a quadtree as used in HEVC, see Block Structures and Parallelism Features in HEVC: Figure 3.\n• CTU (Coding Tree Unit). In HEVC a picture is divided into CTUs, which are square blocks except possibly for the bottom row and right-hand column. A CTU may be further divided into 4 child CTUs.\n• Tiles and Slices. HEVC has two additional types of partition that may be used for a picture. They consist of sequences of CTUs. A tile is a rectangular region of CTUs within a rectangular grid of tiles that form a picture; tiles are treated like independent pictures. A slice is a sequence of CTUs that can be decoded independently from other slices of the same picture. A slice can either be an entire picture or a region of a picture. One of the main purposes of slices is resynchronization in the event of data losses. In contrast to common usage, a slice in HEVC is not limited to a single row of blocks in a rectangular grid. See subclause 6.3 in the HEVC specification or Block Structures and Parallelism Features in HEVC: Figure 6.\n• I-slices (intra slices) are independently compressed HEVC slices. An I-slice is decoded using intra prediction only. An extension of the more common term, I-frame.\n• P-slices (predictive slices) employ inter-picture coding, based on differences from previous pictures in the video source. An extension of the more common term, P-frame.\n• B-slices (bi-predictive slices) are bidirectionally predicted slices using differences between the current picture and both preceding and following pictures to derive coded content for the current slice. An extension of the more common term, B-frame.\n• Intra coding uses various spatial prediction modes to exploit spatial statistical dependencies in the source signal for a single picture or tile.\n• Inter coding uses motion vectors for block-based prediction to exploit temporal statistical dependencies between different pictures. In HEVC, inter prediction can apply to block and slice substructures, not only to entire pictures or tiles.\n• SEI: supplemental enhancement information. An SEI message contains information (often referred to as the message payload) that is not necessary simply to decode the samples of coded pictures, but is a standardized, structured message to a decoder that can assist in processes related to decoding, display, or other purposes. Some advanced features, such as 3-D, will not function as intended in an application that does not recognize the corresponding SEI messages.\n• Levels consist of a defined set of constraints on the values that may be taken by HEVC syntax elements and variables or the value of a transform coefficient prior to scaling. A common \"general\" set of levels is defined for all HEVC profiles. Hence most aspects of the definition of each level are common across different profiles. Within a coded video sequence, profiles, levels, and tiers are identified in very terse fashion in parameter sets and not accessible to simple tools. See File type signifiers above.\n• Tiers are specified categories of level constraints imposed on values of the syntax elements in the bitstream, where the level constraints are nested within a tier and a decoder conforming to a certain tier and level would be capable of decoding all bitstreams that conform to the same tier or the lower tier of that level or any level below it.\n• CPB (coded picture buffer) is a buffer containing decoding units in decoding order specified in the hypothetical reference decoder in Annex C of the HEVC spec. The size of CPB in content coding generated by an encoder is an important factor for decoders. Levels specify a maximum CPB size in bits.\n• Parameter sets in HEVC are similar to the parameter sets in H.264/AVC, and share the same basic design goals—namely bit rate efficiency, error resiliency, and supporting systems layer interfaces. There is a hierarchy of parameter sets in HEVC, including the Sequence Parameter Set (SPS) and Picture Parameter Set (PPS) which are similar to their counterparts in AVC. HEVC introduced a new type of parameter set called the Video Parameter Set (VPS). The parameter sets are stored in special network abstraction layer (NAL) units, with specific NAL unit types encoded as binary integers at the beginning of the unit. See Table 7-1 in the HEVC specification or Table I in Overview of the High Efficiency Video Coding (HEVC) Standard for a mapping of the integer codes to human-readable names. For compression efficiency, these parameter sets are very tersely coded, primarily using 1-bit flags and binary integers. Note that if an HEVC coded video sequence or still image is embedded in an ISO_BMFF container as specified in ISO/IEC 14496-15, the VPS and SPS parameter sets will be found in an HEVC configuration item property in a box of type hvcC.\n• SPS (Sequence Parameter Set). Contains parameters that apply to an entire coded video sequence, and do not change from picture to picture within a coded video sequence. The SPS has codes for characteristics such as bit-depth, chroma format, picture width and picture height. Has NAL unit type of 33.\n• VPS (Video Parameter Set). A new parameter set defined in HEVC, which applies to all of the layers of a bitstream. A layer may contain multiple temporal sub-layers. Has NAL unit type of 32 and is stored before the SPS.\n• PPS (Picture Parameter Set). There is one PPS per picture. Among the details stored in this parameter set are: number of tile rows; number of tile columns; flags related to whether particular extensions, e.g., for 3D or screen content coding(SCC) are used; and other parameters needed for decoding the picture. Has NAL unit type of 34. The article titled Overview of the High Efficiency Video Coding (HEVC) Standard, published in 2012 before the HEVC specification had been approved as a standard, discussed the antecedent standards. The summary that follows is adapted from that discussion. Some important video coding standards have evolved in a series of well-known ITU-T and ISO/IEC standards. The ITU-T produced H.261 and H.263; ISO/IEC produced MPEG-1 and MPEG-4 Visual; and the two organizations jointly produced the H.262/MPEG-2 Video and H.264/MPEG-4 Advanced Video Coding (AVC) standards. Throughout this evolution, efforts have been made to maximize compression capability, while considering the computational resources that were practical for use in products at the time of anticipated deployment of each standard. The two standards that were jointly produced had been widely deployed. The standard preceding HEVC, H.264/MPEG-4 AVC, was developed in the period between 1999 and 2003, and extended in several important ways from 2003–2009. By 2012, AVC had been widely used for many applications, including broadcast of high definition (HD) TV signals over satellite, cable, and terrestrial transmission systems, video content acquisition and editing systems, camcorders, security applications, Internet and mobile network video, Blu-ray Discs, and real-time conversational applications such as video chat, video conferencing, and telepresence systems. By 2012, AVC had largely displaced the older H.262/MPEG-2 Video standard within its more limited application domains. Key drivers behind the development of HEVC were: the growing popularity of HD video, and the emergence of yet larger formats (e.g., 4k×2k or 8k×4k resolution); and the networking issues caused by the growing volume of video applications targeting mobile devices and the transmission needs for video-on-demand services. HEVC was designed with a focus on two key issues: increased video resolution and increased use of parallel processing architectures, while still addressing applications that had been supported by AVC. The first version of the HEVC (H.265) specification was published in April 2013. Editions of the standard are listed below: Two more recent video encodings have been developed, namely Versatile Video Coding (VVC) and Essential Video Coding (EVC). VVC, also known as ITU-T H.266, aims at improved compression over HEVC, yet higher spatial resolution, and full support for immersive applications. The first edition was approved in 2020 as ITU-T H.266 (08/2020). See also VVC Overview from Fraunhofer HHI. The objective of EVC is to be a royalty-free standard.\n• Published as ISO/IEC 23008-2 and ITU-T H.265. Text for all editions is available via http://handle.itu.int/11.1002/1000/14107 or https://www.itu.int/rec/T-REC-H.265. Initially published in 2013. As of November 2020, the most recent edition is Edition 7, published November 2019.\n• ISO/IEC 23008-2:2020 Information technology — High efficiency coding and media delivery in heterogeneous environments — Part 2: High efficiency video coding (https://www.iso.org/standard/75484.html). ISO catalog record and preview.\n• Reference software for HEVC, produced by JCT-VC. Known as HM (for Hevc test Model). As of November 2020, the software has explicit support for profiles in versions 1 and 2 of the HEVC specification. Commercial encoders may well achieve higher efficiency and support profiles added in later versions of the specification.\n• ISO/IEC 23008-5:2017 Information technology — High efficiency coding and media delivery in heterogeneous environments — Part 5: Reference software for high efficiency video coding (https://www.iso.org/standard/72216.html). A two-page document with software as an electronic supplement. Relates to ITU-T H.265 V1 (04/2013)\n• Repository for source code and software manual for HM HEVC reference software (https://vcgit.hhi.fraunhofer.de/jct-vc/HM). Supports versions 1 (2013) and 2 (2014) of HEVC. Code modules for profiles added later appear to be maintained separately.\n• HM Software Manual for version 16.22 (July 2020) (https://vcgit.hhi.fraunhofer.de/jct-vc/HM/-/blob/12251f5cf170aef649a6e15b8f65cfddce84a99e/doc/software-manual.pdf). Latest version as of November 2020.\n• Resources from the MPEG website and participants in the working group that developed the initial HEVC specification and its extensions.\n• High Efficiency Video Coding | on MPEG website (https://mpeg.chiariglione.org/standards/mpeg-h/high-efficiency-video-coding). As of June 2020, this MPEG website is no longer actively maintained by its former editor, but there is a substantial body of useful documentation, both from the initial development phase and relating to later extensions.\n• Persistent identifier for ITU-T H.265 (04/2013) (http://handle.itu.int/11.1002/1000/11885). Record for first edition of ITU-T H.265. Lists other editions.\n• Overview of the High Efficiency Video Coding (HEVC) Standard (2012) | by Gary J. Sullivan, et al. (http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf). Published in IEEE Transactions on Circuits and Systems for Video Technology, Vol. 22, No. 12, December 2012\n• Intra Coding of the HEVC Standard (2012) | by Jani Lainema, et al. (http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.352.3008). Published in IEEE Transactions on Circuits and Systems for Video Technology. Vol. 22, No. 12. DOI: 10.1109/TCSVT.2012.2221525\n• Two HEVC encoder methods for block artifact reduction (2013) | by Norkin, et al. (https://www.semanticscholar.org/paper/Two-HEVC-encoder-methods-for-block-artifact-Norkin-Andersson/7990dbd5ae60db91d9e312f16ea59c91afef9ef5). Proceedings of the IEEE international conference on visual communications and image processing (VCIP) 2013\n• Block Structures and Parallelism Features in HEVC (2014) | by Heiko Schwarz, et al. (https://www.researchgate.net/publication/300315241_Block_Structures_and_Parallelism_Features_in_HEVC). Published as Chapter 3 in High Efficiency Video Coding (HEVC): Algorithms and Architectures, edited by Vivienne Sze, et al. ISBN 978-3-319-06895-4.\n• Overview of the Range Extensions for the HEVC Standard: Tools, Profiles, and Performance (2015) | by David Flynn, et al. (https://www.researchgate.net/publication/281791165_Overview_of_the_Range_Extensions_for_the_HEVC_Standard_Tools_Profiles_and_Performance). Published in IEEE Transactions on Circuits and Systems for Video Technology, Vol. 26, No. 1. DOI: 10.1109/TCSVT.2015.2478707\n• Overview of the Multiview and 3D Extensions of High Efficiency Video Coding (2015) | by Gerhard Tech, et al. (https://doi.org/10.1109/TCSVT.2015.2477935). Published in IEEE Transactions on Circuits and Systems for Video Technology, Vol. 26, No. 1. DOI: 10.1109/TCSVT.2015.24779351):1-1\n• Overview of SHVC: Scalable Extensions of the High Efficiency Video Coding (HEVC) Standard (2015) | by Jill Bouce, et al. (https://www.researchgate.net/publication/282477513_Overview_of_SHVC_Scalable_Extensions_of_the_High_Efficiency_Video_Coding_HEVC_Standard). Published in IEEE Transactions on Circuits and Systems for Video Technology, Vol. 26, No. 1. DOI: 10.1109/TCSVT.2015.2461951\n• SHVC, the Scalable Extensions of HEVC, and Its Applications (2016) | by Yan Ye, et al. (https://www.zte.com.cn/global/about/magazine/zte-communications/2016/1/en_214/448971.html). Discusses potential adoption in Advanced Television Standardization Committee (ATSC) and 3GPP standards.\n• Overview of Screen Content Video Coding: Technologies, Standards, and Beyond (2016) | by Wen-Hsiao Peng, et al. (https://www.researchgate.net/publication/311443280_Overview_of_Screen_Content_Video_Coding_Technologies_Standards_and_Beyond). Published in: IEEE Journal on Emerging and Selected Topics in Circuits and Systems, Vol. 6, No 4, Dec. 2016\n• Omnidirectional 360° Video Coding Technology in Responses to the Joint Call for Proposals on Video Compression With Capability Beyond HEVC (2019) | by Yan Ye, et al. (https://ieeexplore.ieee.org/document/8902161). Published in: IEEE Transactions on Circuits and Systems for Video Technology, Vol. 30, No: 5, May 2020)\n• Towards Bandwidth Efficient Adaptive Streaming of Omnidirectional Video over HTTP (2017) | by Mario Graf, et al. (https://www.semanticscholar.org/paper/Towards-Bandwidth-Efficient-Adaptive-Streaming-of-Graf-Timmerer/cbc3dd6bbf8bb4d14b56d37a04d155e5dda7014c). Describes usage of tiles as specified in HEVC.\n• Multi-viewpoint and Overlays in the MPEG OMAF Standard (2019) | by Igor D.D. Curcio, et al. (https://www.itu.int/en/journal/2020/001/Pages/03.aspx). The OMAF (Omnidirectional MediA Format) standard (ISO/IEC 23090-2) has profiles based on using HEVC Main 10 profile for encoding pictures or tiles.\n• High Efficiency Video Coding (HEVC) Test Model 16 (HM 16) Encoder Description Update 13 (January 2020) (https://mpeg.chiariglione.org/standards/mpeg-h/high-efficiency-video-coding/high-efficiency-video-coding-hevc-test-model-16-hm-2). Describes the encoding process as implemented in the HM-16.21 reference software\n• Fraunhofer HHI | High Efficiency Video Coding (HEVC) (http://hevc.info/). Includes good resources relating to Multiview, 3D, and Scalability extensions of HEVC. Appears to be alias for https://hevc.hhi.fraunhofer.de/\n• Apple held several sessions on HEVC at its 2017 developers conference, WWDC 2017. The videos listed below are excellent resources on the special features of HEVC encoding and the High Efficiency Image File Format (HEIF) file as implemented by Apple. Note that each video is supported by a transcript with convenient built-in links to the corresponding spot in the video.\n• Working with HEIF and HEVC (2017) | Video (60 mins) from Apple Developers Conference WWDC 2017 (https://developer.apple.com/videos/play/wwdc2017/511). Details for developers on API modifications in existing toolkits. HEIF section begins at 20 mins.\n• Advances in HTTP Live Streaming (2017) | Video (60 mins) from Apple Developers Conference WWDC 2017 (https://developer.apple.com/videos/play/wwdc2017/504). HEVC section begins at 2 1/2 minutes.\n• In 2014 and 2015, books were published about HEVC to serve as companions to the formal text specification and reference software. Published by Springer, these are available online by subscription or purchase.\n• High Efficiency Video Coding (HEVC): Algorithms and Architectures | Edited by Vivienne Sze, et al. ISBN 978-3-319-06895-4 (https://www.springer.com/us/book/9783319068947). Relates to the initial version of the HEVC specification. Separately authored chapters on different technical aspects of HEVC. Chapter 9 compares compression performance of HEVC with AVC using reference software.\n• High Efficiency Video Coding (HEVC): Coding Tools and Specification (2015) | by Mathias Wien (https://www.springer.com/us/book/9783662442753). Includes history and technical background on video coding and comparisons with AVC in all the chapters that relate to aspects of video coding (such as temporal and spatial predictive coding, color representation, etc.). Introduces new range extension profiles, that were to be included in version 2 of the specification.\n• Comparisons of HEVC encoding with other compression schemes. Note that many of the articles listed above have sections including compression efficiency comparisons.\n• Comparison of the Coding Efficiency of Video Coding Standards—Including High Efficiency Video Coding (HEVC) (2012) | by Jens-Rainer Ohm, et al. (http://publica.fraunhofer.de/documents/N-234468.html). Comparison of HEVC with several earlier video coding standards, including AVC.\n• Lossy Compression of Multispectral Satellite Images with Application to Crop Thematic Mapping: A HEVC Comparative Study (May 2020) (https://www.researchgate.net/publication/341454495_Lossy_Compression_of_Multispectral_Satellite_Images_with_Application_to_Crop_Thematic_Mapping_A_HEVC_Comparative_Study).\n• Evaluation of High Efficiency Video Coding (HEVC) for 3GPP services (Release 16) (https://www.3gpp.org/ftp/Specs/archive/26_series/26.906/26906-g00.zip). Includes objective and subjective assessments. Comparison is to AVC for video and JPEG for still images.\n• Resources related to other standards based on HEVC\n• ATSC 3.0 A/341:2019 -- Video – HEVC (https://muygs2x2vhb2pjk6g160f1s8-wpengine.netdna-ssl.com/wp-content/uploads/2021/04/A341-2019-Video-HEVC.pdf). Approved in 2020. The constraints and specifications applicable to HEVC encoded ATSC 3.0 video bit streams are listed in Sections 6.1 through 6.4.\n• 3GPP supports High Efficiency Video Coding (https://www.3gpp.org/news-events/1621-hevc). 3GPP Release 12 specifications (2015-03-13 -- SA#67) added support for HEVC in several services\n• DICOM PS3.5 - Data Structures and Encoding (current version) (http://dicom.nema.org/medical/dicom/current/output/html/part05.html). In November 2020, specified two variants of HEVC encoding.\n• Miscellaneous resources related to adoption of HEVC for video encoding\n• How to Install Free HEVC Codecs on Windows 10 (for H.265 Video) (https://www.howtogeek.com/680690/how-to-install-free-hevc-codecs-on-windows-10-for-h.265-video/). The free codec is apparently no longer available as of Fall 2020. The author recommends installing the VLC application which comes with an HEVC codec.\n• MainConcept encoder for HEVC/H.265 (https://www.mainconcept.com/hevc). Available as a plugin for widely used FFmpeg software."
    },
    {
        "link": "https://stackoverflow.com/questions/404615/what-are-some-best-practices-for-reducing-memory-usage-in-c",
        "document": "1) Before you start the project, build in a way of measuring how much memory you're using, preferably on a per-component basis. That way, each time you make a change you can see its effects on memory use. You can't optimise what you can't measure.\n\n2) If the project is already mature and hits the memory limits, (or is ported to a device with less memory), find out what you're using the memory for already.\n\nMy experience has been that almost all the significant optimisation when fixing an over-size application comes from a small number of changes: reduce cache sizes, strip out some textures (of course this is a functional change requiring stakeholder agreement, i.e. meetings, so may not be efficient in terms of your time), resample audio, reduce the upfront size of custom-allocated heaps, find ways to free resources that are only used temporarily and reload them when required again. Occasionally you'll find some structure which is 64 bytes that could be reduced to 16, or whatever, but that's rarely the lowest-hanging fruit. If you know what the biggest lists and arrays in the app are, though, then you know which structs to look at first.\n\nOh yes: find and fix memory leaks. Any memory you can regain without sacrificing performance is a great start.\n\nI've spent a lot of time in the past worrying about code size. Main considerations (aside from: make sure you measure it at build time so that you can see it change), are:\n\n1) Find out what code is referenced, and by what. If you discover that an entire XML library is being linked into your app just to parse a two-element config file, consider changing the config file format and/or writing your own trivial parser. If you can, use either source or binary analysis to draw a big dependency graph, and look for large components with only a small number of users: it may be possible to snip these out with only minor code rewrites. Be prepared to play diplomat: if two different components in your app use XML, and you want to cut it, then that's two people you have to convince of the benefits of hand-rolling something that's currently a trusted, off-the-shelf library.\n\n2) Mess around with the compiler options. Consult your platform-specific documentation. For instance, you may want to reduce the default acceptable code size increase due to inlining, and on GCC at least you can tell the compiler only to apply optimisations which don't typically increase code size.\n\n3) Take advantage of libraries already on the target platform(s) where possible, even if it means writing an adaptor layer. In the XML example above, you may find that on your target platform there's an XML library in memory at all times anyway, because the OS uses it, in which case link to it dynamically.\n\n4) As someone else mentioned, thumb mode can help on ARM. If you only use it for the code which is not performance critical, and leave the critical routines in ARM, then you won't notice the difference.\n\nFinally, there may be clever tricks you can play if you have sufficient control over the device. The UI only allows one application to run at a time? Unload all the drivers and services your app doesn't need. Screen is double buffered, but your app is synched to the refresh cycle? You may be able to reclaim an entire screen buffer."
    },
    {
        "link": "https://quora.com/I-want-to-work-on-data-compression-algorithms-using-the-C-language-Where-should-I-begin",
        "document": "Something went wrong. Wait a moment and try again."
    },
    {
        "link": "https://visionular.ai/advanced-optimization-techniques-in-video-compression",
        "document": "In today’s world, videos are more than just a medium; they are an omnipresent form of communication, education, entertainment, and business. From social media reels to high-definition films, the digital era is replete with video data. However, this immense volume of visual data poses significant challenges related to storage, bandwidth, and playback quality. Enter video compression – the art and science of reducing the size of digital video files, without compromising the overall viewing quality. And, this is done to reduce storage, delivery costs, and improve playback video quality. Video compression’s importance stems from the pressing need for efficient storage and swift transmission, particularly in a time where streaming reigns supreme. At the heart of video compression lies a series of complex algorithms and operations. These algorithms are tasked with the job of recognizing and eliminating redundant data, preserving only the essential components that contribute most significantly to the video’s quality. It is a balancing act between conserving storage and retaining quality, requiring an intricate understanding of video data and human perception.\n\nThe Need for Optimization in Software-based Compression\n\nVideo compression can be bifurcated into two primary categories: hardware and software-based compression. While both have their merits, they differ significantly in methodology and application. Hardware video compression is typically achieved using dedicated equipment or chips designed exclusively for this task. It is faster due to its dedicated nature but can be less flexible when adapting to newer algorithms or standards. In contrast, software video compression capitalizes on general-purpose computer hardware and specialized software. These software-driven approaches provide flexibility, scalability, and adaptability to emerging standards. Moreover, with software, updates can be rolled out seamlessly, ensuring the encoding techniques remain at the forefront. In this blog, we look at the challenges to high-speed software-based video compression and provide ideas on how video can be compressed faster on modern general-purpose hardware. These ideas stem from research and development done by Visionular’s engineers and we hope this helps the video community at large.\n\nIn software development, the quality and efficiency of code can be critical for both a business’s success and a customer’s satisfaction. The methodology with which a software is constructed can significantly impact its performance, scalability, and maintainability, especially in a complex and deep-tech field such as video compression. Efficiency in Video Compression Algorithms: Clean code allows for better readability and understanding of the algorithms used. When video compression algorithms, which are complex mathematical entities, are neatly articulated in code, it enhances the efficiency of the encoding and decoding process. Modular Encapsulation: video compression tasks can be broken down into modular components, each focusing on specific tasks, such as motion estimation, quantization, or entropy coding. This modularity ensures that components can be updated, replaced, re-used, or improved without affecting the entire system. E.g., creating the final bitstream (such as the NAL units, SPS, PPS, etc.) is independent of whether the encoder is running in 8 or 10-bit configurations. Or, for example, using C++ templates can minimize code required for carrying out motion estimation, while supporting 8bit/10bit modes, different color spaces such as YUV420, YUV422, YUV422-10bit, etc. Scalability: As video resolutions grow and new formats emerge, a clean and object-oriented codebase can be more easily scaled to accommodate these changes. A well-structured system can be expanded to handle 4K, 8K, or even more data-intensive formats without a complete overhaul. Performance Optimization: In video compression, performance is paramount. Clean, well-organized code is more likely to be efficient, reducing the computational resources required for encoding and decoding. In conclusion, for engineers delving into video compression, adopting clean coding isn’t just a best practice; it is a prerequisite for creating efficient, scalable, and maintainable compression systems. The resource-intensive demands of video data make these practices not just beneficial but essential for success.\n\n“Coupling” in the context of software design refers to the degree of interdependence between different modules or components of a system. Advocating for coupling might seem counterintuitive given the emphasis on modular and independent design in modern software engineering (i.e., decoupled architectures). However, in the realm of video compression, strategic coupling becomes not just relevant but critically important, and here’s why:\n• None : Modern compression techniques frequently employ predictive coding, in which future frames are forecasted based on preceding ones. SAD, or Sum of Absolute Differences, is a common method employed in motion estimation to find the difference between blocks in consecutive video frames. Given how motion estimation works, there are many overlapping computations when analyzing adjacent blocks. Here’s how tight coupling helps in making motion estimation efficient – a. When calculating the SAD for a block, we are comparing it to a reference block in a previous frame. Now, when moving to an adjacent block, many of the comparisons made for the previous block remain relevant. b. Instead of recalculating the entire SAD for the new block, we can reuse a significant portion of the calculations from the previous block and only compute the differences for the new pixels.c. By reusing these calculations, the process becomes much more efficient, reducing the computational load and speeding up the motion estimation process. This practice not only conserves processing resources but also hastens the video compression process without compromising accuracy.\n• None : Coupling ensures that relevant data, such as motion vectors, macroblock information, or quantization parameters, flow smoothly between related components of the compression system. This smooth data flow is vital to maintain the speed and efficiency of the compression process.\n• None : Video compression, being memory-intensive, benefits from coupling by reducing unnecessary data duplication and moving entire video frames to and from the disk to the system RAM. By strategically sharing data between related processes, the algorithm can minimize its memory footprint, leading to faster operations and reduced computational resources.\n• None : Coupled systems in video compression can adapt in real-time. For instance, if a particular encoding strategy results in a higher-than-expected bitrate for a segment, a coupled system might dynamically adjust the quantization or prediction strategy for subsequent segments, optimizing quality and compression ratio. To put it succinctly, while decoupling promotes modularity and independent functionality, strategic coupling in video compression taps into the interconnected nature of video data, ensuring efficiency and quality.\n\nHarnessing the Power of Advanced Instruction Sets in Video Compression\n\nLet’s first dive into the significance of instruction sets like SSE (Streaming SIMD Extensions) and AVX2 (Advanced Vector Extensions 2) for x86/x64 architectures. SIMD stands for Single Instruction, Multiple Data. As the name suggests, this design philosophy enables a single instruction to process multiple data elements concurrently. Imagine a scenario where a video encoder needs to calculate the difference between corresponding pixels in two video frames, a common operation in motion estimation. Without SIMD, the CPU would iterate through each pixel pair sequentially. However, with SSE or AVX2, the CPU can process multiple pixel pairs simultaneously, drastically speeding up the computation. The sheer volume of data involved in video compression makes these parallel operations not just beneficial but essential. Encoding a mere second of high-definition video can involve processing millions of pixels. Thus, the parallel processing capabilities of SSE and AVX2 prove invaluable.\n\nWhile SSE and AVX2 are mainstays in desktop and server environments, the mobile world, dominated by ARM architectures, requires its own set of optimizations. This is where the NEON instruction set comes into play. NEON, like its counterparts in the X86/X64 realm, allows for efficient parallel processing of data. Given the power constraints and performance needs of mobile devices, optimizing video compression is even more critical. Videos on mobile platforms need to be efficiently compressed for playback, transmission (like in video calls), or even editing on the go. Let’s dissect some of the operations NEON optimizes:\n• None This is about predicting the pixel values within a single frame based on neighboring pixel values. NEON can process these predictions for multiple pixels or blocks simultaneously, making the intra-frame compression faster.\n• None In inter-frame compression, predicting the content of one frame based on another is crucial. This involves estimating motion vectors, which denote how certain blocks of pixels move between frames. NEON can expedite these calculations by handling multiple vectors concurrently.\n• None This involves representing data using fewer bits based on its frequency. In video compression, this step is vital to shrink the video size further after pixel-based optimizations. With NEON, the encoder can process larger chunks of data in tandem, making entropy coding faster and more efficient. Advanced instruction sets, whether SSE, AVX2, or NEON, enable encoders to tackle the vast amounts of data inherent to videos with efficiency and speed.\n\nVideo compression demands the highest standards of precision and efficiency. Through the integration of clean coding principles, systems coupling, and the leverage of advanced instruction sets such as (SSE, AVX2, NEON), software optimization elevates the process to its peak performance. Thread pool optimization and priority scheduling further ensure that these strategies are executed seamlessly. Every decision, from code structuring to thread management, directly impacts the encoding quality and speed. Thus, a dedicated focus on software optimization not only ensures technical excellence but also delivers the best possible video output."
    },
    {
        "link": "https://stackoverflow.com/questions/9990903/c-video-compression-library-that-supports-many-different-compression-algorithm",
        "document": "As satuon already answered, FFmpeg is the go-to solution for all things multimedia. However, I just wanted to suggest an easier path for you than trying to hook your program up to its libraries. It would probably be far easier for you to generate a sequence of raw RGB images within your program, dump each out to disc (perhaps using a ridiculously simple format like PPM), and then use FFmpeg from the command like to compress them into a proper movie.\n\nThis workflow might cut down on your prototyping and development time.\n\nAs for the specific video codec you will want to use, you have a plethora of options available to you. One of the most important considerations will be: Who needs to be able to play your video and what software will they have available?"
    },
    {
        "link": "https://runtimerec.com/how-to-implement-efficient-data-compression-in-firmware-for-embedded-devices",
        "document": "Implementing efficient data compression in firmware for embedded devices is crucial for optimizing memory usage, reducing transmission times, and enhancing overall performance. With the rise of IoT and data-intensive applications, the ability to effectively compress data is increasingly valuable in constrained environments. In this article, we’ll explore the basics of data compression, the challenges specific to embedded systems, and practical methods to implement efficient data compression in firmware.\n\nData compression involves reducing the amount of data needed to represent information, effectively optimizing storage and transmission. In embedded devices, this is especially important due to limited memory and processing power. Compression can enhance data throughput, conserve storage, and even lower energy consumption by minimizing the data that needs to be processed or transmitted.\n\nEmbedded devices often operate with constrained resources, where memory (both volatile and non-volatile), processing power, and bandwidth are limited. By compressing data:\n• Memory and storage efficiency: Storing more information in a limited space.\n• Energy savings: Reducing data for transmission or processing leads to lower power consumption, crucial in battery-operated devices.\n• Lossless Compression: This method allows the original data to be perfectly reconstructed from the compressed data. It’s essential for applications where data integrity is critical, such as sensor readings and logs. Common algorithms include Huffman coding, Lempel-Ziv-Welch (LZW), and run-length encoding.\n• Lossy Compression: This approach sacrifices some data accuracy for higher compression rates, commonly used in image and audio data where slight inaccuracies aren’t perceptible. Examples include JPEG and MP3.\n\nIn embedded systems, lossless compression is often preferred due to the need for exact data recovery.\n\nWhen designing firmware with data compression, several unique constraints of embedded systems come into play:\n• Limited Processing Power: Compression algorithms can be computationally intensive, which is a challenge on low-power CPUs.\n• Memory Constraints: Both RAM and storage are limited, so memory-intensive algorithms might not be feasible.\n• Real-Time Requirements: Many embedded systems operate in real-time; therefore, compression and decompression must occur without affecting system responsiveness.\n• Power Consumption: Compression and decompression consume CPU cycles, impacting battery life, so power-efficient solutions are necessary.\n\nSelecting an appropriate algorithm and balancing compression effectiveness with processing overhead is essential.\n\nWhen choosing a compression algorithm, consider factors such as compression ratio, speed, and memory footprint. Here are some options suitable for embedded systems:\n\nRLE is a simple lossless algorithm that compresses data by reducing consecutive repeated values. It’s ideal for data with many repeating elements, such as sensor readings in constant environments.\n• Drawbacks: Not effective on data with little repetition.\n\nHuffman coding is a lossless algorithm that assigns variable-length codes to symbols based on their frequency. Frequently occurring symbols use shorter codes, leading to a smaller data footprint.\n• Drawbacks: Requires a pre-built dictionary, which can be memory-intensive.\n\nLZW builds a dictionary of patterns encountered in data, replacing these patterns with shorter codes. It’s used in formats like GIF and TIFF and is effective for general-purpose compression.\n\nDPCM encodes the difference between consecutive data points rather than the absolute values, reducing data size. It’s effective for data with minimal variation between successive values, like temperature or pressure readings.\n\nAdaptive Huffman coding dynamically adjusts to data patterns, creating an evolving code tree. It’s particularly useful for real-time applications where data characteristics change over time.\n\nImplementing compression algorithms in firmware requires careful consideration of embedded constraints, and each step of the implementation should align with resource availability and application needs.\n\nChoose an algorithm that best fits your data patterns and device capabilities. For example, if sensor data is being compressed and exhibits repetitive values, RLE or DPCM might be optimal. Avoid complex algorithms that exceed memory or processing constraints.\n\nUse bitwise operations and fixed-point arithmetic where possible to reduce computational overhead. Efficient coding practices, such as using in-line functions and avoiding recursion, can significantly reduce memory usage and improve performance.\n\nMemory is a limited resource in embedded systems, so managing buffers for compressed and decompressed data is critical.\n• Static Buffer Allocation: Allocate fixed-size buffers to prevent memory fragmentation, which is a risk in dynamic allocation.\n• Circular Buffers: For streaming data, circular buffers allow continuous read/write operations, which is essential for real-time processing.\n\nConsider the trade-off between compression speed and ratio. For real-time applications, prioritize faster algorithms (like RLE), even if they offer lower compression ratios. For non-time-sensitive data, algorithms like LZW or Huffman coding can be used to maximize compression at the cost of speed.\n• Duty Cycling: Schedule compression tasks during low-power states or when the CPU is underutilized.\n• Selective Compression: Compress only essential data to conserve power, such as reducing the frequency of transmissions for data that doesn’t change often.\n• Hardware Acceleration: Some microcontrollers have hardware-based compression, like CRC modules, that can offload work from the CPU, saving energy.\n\nTo illustrate these principles, let’s walk through an example of implementing RLE for sensor data compression in C:\n\n// Count consecutive bytes with the same value\n\n// Store the value and count\n\n// Repeat value ‘count’ times in the output\n• Compression: The compressRLE function iterates through the input array, counting consecutive occurrences of each value. It then stores each unique value followed by its count in the output buffer.\n• Decompression: The decompressRLE function reads the compressed data, repeating each value by the specified count.\n\nThis simple implementation demonstrates how RLE compression can be applied to an embedded system with minimal resource overhead.\n\nEnsure that compressed data fits within allocated buffers and test for edge cases, such as empty or highly variable data. Optimize by removing redundant code and using direct memory access (DMA) for data transfers if your microcontroller supports it.\n• Wearable Health Monitors: Data compression helps manage the vast amount of data generated by sensors in health monitors. These devices use differential encoding to compress continuous data streams like heart rate and body temperature.\n• Industrial IoT Sensors: Embedded devices in industrial applications often operate in remote areas, where bandwidth is limited. Data is compressed before transmission to reduce cellular data usage, with algorithms like Huffman coding to preserve critical information.\n• Environmental Monitoring Systems: Sensor networks that monitor environmental parameters (temperature, humidity, etc.) compress data on-device to prolong battery life, especially for solar-powered nodes in remote locations.\n\nFinal Thoughts: Best Practices for Efficient Data Compression in Firmware\n\nEffective data compression in embedded systems is a balancing act between algorithm complexity, memory usage, processing time, and power consumption. Here are some best practices to ensure efficient compression:\n• Use Contextual Compression: Choose algorithms based on data patterns. For example, if data is mostly zeros (as in some IoT sensors), use RLE or differential encoding.\n• Optimize for Platform Constraints: Understand your microcontroller’s capabilities and limitations, tailoring compression to available resources.\n• Implement Error Handling: Ensure data integrity by adding error-checking routines, especially if data is being decompressed after transmission.\n• Use Compression Libraries: If your firmware stack allows it, lightweight compression libraries (like miniz for LZ-based compression) can save time on implementation while maintaining efficiency.\n• Test for Power Consumption: Profile and test the impact of your compression routines on battery life and optimize accordingly.\n\nBy implementing efficient data compression strategies, embedded engineers can enhance system performance, reduce resource usage, and improve device longevity. Whether working with low-powered IoT nodes, medical devices, or industrial sensors, mastering data compression techniques is a valuable skill that benefits a wide range of applications."
    }
]