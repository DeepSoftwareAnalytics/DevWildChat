[
    {
        "link": "https://arrow.apache.org/docs/python/pandas.html",
        "document": "To interface with pandas, PyArrow provides various conversion routines to consume pandas structures and convert back to them.\n\nTo follow examples in this document, make sure to run:\n\nThe equivalent to a pandas DataFrame in Arrow is a Table. Both consist of a set of named columns of equal length. While pandas only supports flat columns, the Table also provides nested columns, thus it can represent more data than a DataFrame, so a full conversion is not always possible. Conversion from a Table to a DataFrame is done by calling . The inverse is then achieved by using . By default tries to preserve and restore the data as accurately as possible. See the section below for more about this, and how to disable this logic.\n\nMethods like have a option which defines how to preserve (store) or not to preserve (to not store) the data in the member of the corresponding pandas object. This data is tracked using schema-level metadata in the internal object. The default of is , which behaves as follows:\n• None is stored as metadata-only, not requiring any extra storage.\n• None Other index types are stored as one or more physical data columns in the resulting To not store the index at all pass . Since storing a can cause issues in some limited scenarios (such as storing multiple DataFrame objects in a Parquet file), to force all index data to be serialized in the resulting table, pass .\n\nWith the current design of pandas and Arrow, it is not possible to convert all column types unmodified. One of the main issues here is that pandas has no support for nullable columns of arbitrary type. Also is currently fixed to nanosecond resolution. On the other side, Arrow might be still missing support for some types. Pandas categorical columns are converted to Arrow dictionary arrays, a special array type optimized to handle repeated and limited number of possible values. We can inspect the of the created table and see the same categories of the Pandas DataFrame. The builtin objects inside Pandas data structures will be converted to an Arrow and respectively. When converting to pandas, arrays of objects are returned:\n\nIn Arrow all data types are nullable, meaning they support storing missing values. In pandas, however, not all data types have support for missing data. Most notably, the default integer data types do not, and will get casted to float when missing values are introduced. Therefore, when an Arrow array or table gets converted to pandas, integer columns will become float when missing values are present: Pandas has experimental nullable data types (https://pandas.pydata.org/docs/user_guide/integer_na.html). Arrows supports round trip conversion for those: This roundtrip conversion works because metadata about the original pandas DataFrame gets stored in the Arrow table. However, if you have Arrow data (or e.g. a Parquet file) not originating from a pandas DataFrame with nullable data types, the default conversion to pandas will not use those nullable dtypes. The method has a keyword that can be used to override the default data type used for the resulting pandas DataFrame. This way, you can instruct Arrow to create a pandas DataFrame using nullable dtypes. The keyword expects a function that will return the pandas data type to use given a pyarrow data type. By using the method, we can create such a function using a dictionary. If you want to use all currently supported nullable dtypes by pandas, this dictionary becomes: When using the pandas API for reading Parquet files ( ), this can also be achieved by passing :\n\nWhen converting from Arrow data structures to pandas objects using various methods, one must occasionally be mindful of issues related to performance and memory usage. Since pandas’s internal data representation is generally different from the Arrow columnar format, zero copy conversions (where no memory allocation or computation is required) are only possible in certain limited cases. In the worst case scenario, calling will result in two versions of the data in memory, one for Arrow and one for pandas, yielding approximately twice the memory footprint. We have implement some mitigations for this case, particularly when creating large objects, that we describe below. Zero copy conversions from or to NumPy arrays or pandas Series are possible in certain narrow cases:\n• None The Arrow data is stored in an integer (signed or unsigned through ) or floating point type ( through ). This includes many numeric types as well as timestamps.\n• None The Arrow data has no null values (since these are represented using bitmaps which are not supported by pandas).\n• None For , the data consists of a single chunk, i.e. . Multiple chunks will always require a copy because of pandas’s contiguousness requirement. In these scenarios, or will be zero copy. In all other scenarios, a copy will be required. As of this writing, pandas applies a data management strategy called “consolidation” to collect like-typed DataFrame columns in two-dimensional NumPy arrays, referred to internally as “blocks”. We have gone to great effort to construct the precise “consolidated” blocks so that pandas will not perform any further allocation or copies after we hand off the data to . The obvious downside of this consolidation strategy is that it forces a “memory doubling”. To try to limit the potential effects of “memory doubling” during , we provide a couple of options:\n• None , when enabled produces one internal DataFrame “block” for each column, skipping the “consolidation” step. Note that many pandas operations will trigger consolidation anyway, but the peak memory use may be less than the worst case scenario of a full memory doubling. As a result of this option, we are able to do zero copy conversions of columns in the same cases where we can do zero copy with and .\n• None , this destroys the internal Arrow memory buffers in each column object as they are converted to the pandas-compatible representation, potentially releasing memory to the operating system as soon as a column is converted. Note that this renders the calling object unsafe for further use, and any further methods called will cause your Python process to crash. # not necessary, but a good practice will yield significantly lower memory usage in some scenarios. Without these options, will always double memory. Note that is not guaranteed to save memory. Since the conversion happens column by column, memory is also freed column by column. But if multiple columns share an underlying buffer, then no memory will be freed until all of those columns are converted. In particular, due to implementation details, data that comes from IPC or Flight is prone to this, as memory will be laid out as follows: In this case, no memory can be freed until the entire table is converted, even with ."
    },
    {
        "link": "https://arrow.apache.org/docs/python/index.html",
        "document": "This is the documentation of the Python API of Apache Arrow.\n\nApache Arrow is a universal columnar format and multi-language toolbox for fast data interchange and in-memory analytics. It contains a set of technologies that enable data systems to efficiently store, process, and move data.\n\nSee the parent documentation for additional details on the Arrow Project itself, on the Arrow format and the other language bindings.\n\nThe Arrow Python bindings (also named “PyArrow”) have first-class integration with NumPy, pandas, and built-in Python objects. They are based on the C++ implementation of Arrow.\n\nHere we will detail the usage of the Python API for Arrow and the leaf libraries that add additional functionality such as reading Apache Parquet files into Arrow structures."
    },
    {
        "link": "https://pandas.pydata.org/docs/user_guide/pyarrow.html",
        "document": "pandas can utilize PyArrow to extend functionality and improve the performance of various APIs. This includes:\n• None Facilitate interoperability with other dataframe libraries based on the Apache Arrow specification (e.g. polars, cuDF)\n\nTo use this functionality, please ensure you have installed the minimum supported PyArrow version.\n\nA , , or the columns of a can be directly backed by a which is similar to a NumPy array. To construct these from the main pandas data structures, you can pass in a string of the type followed by , e.g. into the parameter The string alias maps to which is not equivalent to specifying . Generally, operations on the data will behave similarly except can return NumPy-backed nullable types while will return . For PyArrow types that accept parameters, you can pass in a PyArrow type with those parameters into to use in the parameter. If you already have an or , you can pass it into to construct the associated , or object. To retrieve a pyarrow from a or , you can call the pyarrow array constructor on the or . To convert a to a , you can call the method with .\n\nPyArrow data structure integration is implemented through pandas’ interface; therefore, supported functionality exists where this interface is integrated within the pandas API. Additionally, this functionality is accelerated with PyArrow compute functions where available. This includes: The following are just some examples of operations that are accelerated by native PyArrow compute functions.\n\nPyArrow also provides IO reading functionality that has been integrated into several pandas IO readers. The following functions provide an keyword that can dispatch to PyArrow to accelerate reading from an IO source. By default, these functions and all other IO reader functions return NumPy-backed data. These readers can return PyArrow-backed data by specifying the parameter . A reader does not need to set to necessarily return PyArrow-backed data. Several non-IO reader functions can also use the argument to return PyArrow-backed data including:"
    },
    {
        "link": "https://wesm.github.io/arrow-site-test/python/pandas.html",
        "document": "To interface with pandas, PyArrow provides various conversion routines to consume pandas structures and convert back to them.\n\nWhile pandas uses NumPy as a backend, it has enough peculiarities (such as a different type system, and support for null values) that this is a separate topic from NumPy Integration.\n\nTo follow examples in this document, make sure to run:\n\nThe equivalent to a pandas DataFrame in Arrow is a Table. Both consist of a set of named columns of equal length. While pandas only supports flat columns, the Table also provides nested columns, thus it can represent more data than a DataFrame, so a full conversion is not always possible. Conversion from a Table to a DataFrame is done by calling . The inverse is then achieved by using .\n\nIn Arrow, the most similar structure to a pandas Series is an Array. It is a vector that contains data of the same type as linear memory. You can convert a pandas Series to an Arrow Array using . As Arrow Arrays are always nullable, you can supply an optional mask using the parameter to mark all null-entries.\n\nWith the current design of pandas and Arrow, it is not possible to convert all column types unmodified. One of the main issues here is that pandas has no support for nullable columns of arbitrary type. Also is currently fixed to nanosecond resolution. On the other side, Arrow might be still missing support for some types."
    },
    {
        "link": "https://medium.com/towards-data-science/a-gentle-introduction-to-apache-arrow-with-apache-spark-and-pandas-bb19ffe0ddae",
        "document": "This time I am going to try to explain how can we use Apache Arrow in conjunction with Apache Spark and Python. First, let me share some basic concepts about this open source project.\n\nIn simple words, It facilitates communication between many components, for example, reading a parquet file with Python (pandas) and transforming to a Spark dataframe, Falcon Data Visualization or Cassandra without worrying about conversion.\n\nA good question is to ask how does the data look like in memory? Well, Apache Arrow takes advantage of a columnar buffer to reduce IO and accelerate analytical processing performance."
    },
    {
        "link": "https://flink.apache.org/2020/08/04/pyflink-the-integration-of-pandas-into-pyflink",
        "document": "PyFlink: The integration of Pandas into PyFlink\n\nPython has evolved into one of the most important programming languages for many fields of data processing. So big has been Python’s popularity, that it has pretty much become the default data processing language for data scientists. On top of that, there is a plethora of Python-based data processing tools such as NumPy, Pandas, and Scikit-learn that have gained additional popularity due to their flexibility or powerful functionalities.\n\nIn an effort to meet the user needs and demands, the Flink community hopes to leverage and make better use of these tools. Along this direction, the Flink community put some great effort in integrating Pandas into PyFlink with the latest Flink version 1.11. Some of the added features include support for Pandas UDF and the conversion between Pandas DataFrame and Table. Pandas UDF not only greatly improve the execution performance of Python UDF, but also make it more convenient for users to leverage libraries such as Pandas and NumPy in Python UDF. Additionally, providing support for the conversion between Pandas DataFrame and Table enables users to switch processing engines seamlessly without the need for an intermediate connector. In the remainder of this article, we will introduce how these functionalities work and how to use them with a step-by-step example.\n\nCurrently, only Scalar Pandas UDFs are supported in PyFlink.\n\nUsing scalar Python UDF was already possible in Flink 1.10 as described in a previous article on the Flink blog. Scalar Python UDFs work based on three primary steps:\n• the Java operator serializes one input row to bytes and sends them to the Python worker;\n• the Python worker deserializes the input row and evaluates the Python UDF with it;\n• the resulting row is serialized and sent back to the Java operator\n\nWhile providing support for Python UDFs in PyFlink greatly improved the user experience, it had some drawbacks, namely resulting in:\n• Difficulty when leveraging popular Python libraries used by data scientists — such as Pandas or NumPy — that provide high-performance data structure and functions.\n\nThe introduction of Pandas UDF is used to address these drawbacks. For Pandas UDF, a batch of rows is transferred between the JVM and PVM in a columnar format (Arrow memory format). The batch of rows will be converted into a collection of Pandas Series and will be transferred to the Pandas UDF to then leverage popular Python libraries (such as Pandas, or NumPy) for the Python UDF implementation.\n\nThe performance of vectorized UDFs is usually much higher when compared to the normal Python UDF, as the serialization/deserialization overhead is minimized by falling back to Apache Arrow, while handling as input/output allows us to take full advantage of the Pandas and NumPy libraries, making it a popular solution to parallelize Machine Learning and other large-scale, distributed data science workloads (e.g. feature engineering, distributed model application).\n\nPandas DataFrame is the de-facto standard for working with tabular data in the Python community while PyFlink Table is Flink’s representation of the tabular data in Python. Enabling the conversion between PyFlink Table and Pandas DataFrame allows switching between PyFlink and Pandas seamlessly when processing data in Python. Users can process data by utilizing one execution engine and switch to a different one effortlessly. For example, in case users already have a Pandas DataFrame at hand and want to perform some expensive transformation, they can easily convert it to a PyFlink Table and leverage the power of the Flink engine. On the other hand, users can also convert a PyFlink Table to a Pandas DataFrame and perform the same transformation with the rich functionalities provided by the Pandas ecosystem.\n\nUsing Python in Apache Flink requires installing PyFlink, which is available on PyPI and can be easily installed using . Before installing PyFlink, check the working version of Python running in your system using:\n\nPlease note that Python 3.5 or higher is required to install and run PyFlink\n\nPandas UDFs take as the input and return a of the same length as the output. Pandas UDFs can be used at the exact same place where non-Pandas functions are currently being utilized. To mark a UDF as a Pandas UDF, you only need to add an extra parameter udf_type=“pandas” in the udf decorator:\n\nThe Pandas UDF above uses the Pandas function to interpolate the missing temperature data for each equipment id. This is a common IoT scenario whereby each equipment/device reports it’s id and temperature to be analyzed, but the temperature field may be null due to various reasons. With the function, you can register and use it in the same way as the normal Python UDF. Below is a complete example of how to use the Pandas UDF in PyFlink.\n• Firstly, you need to prepare the input data in the “/tmp/input” file. For example,\n• Next, you can run this example on the command line,\n\nThe command builds and runs the Python Table API program in a local mini-cluster. You can also submit the Python Table API program to a remote cluster using different command lines, see more details here.\n• Finally, you can see the execution result on the command line. As you can see, all the temperature data with an empty value has been interpolated:\n\nYou can use the method to create a PyFlink Table from a Pandas DataFrame or use the method to convert a PyFlink Table to a Pandas DataFrame.\n\nIn this article, we introduce the integration of Pandas in Flink 1.11, including Pandas UDF and the conversion between Table and Pandas. In fact, in the latest Apache Flink release, there are many excellent features added to PyFlink, such as support of User-defined Table functions and User-defined Metrics for Python UDFs. What’s more, from Flink 1.11, you can build PyFlink with Cython support and “Cythonize” your Python UDFs to substantially improve code execution speed (up to 30x faster, compared to Python UDFs in Flink 1.10).\n\nFuture work by the community will focus on adding more features and bringing additional optimizations with follow up releases. Such optimizations and additions include a Python DataStream API and more integration with the Python ecosystem, such as support for distributed Pandas in Flink. Stay tuned for more information and updates with the upcoming releases!"
    },
    {
        "link": "https://reddit.com/r/dataengineering/comments/1gx33ka/apache_arrow_usecase_example",
        "document": "I am trying to get my hands on into Apache Arrow.\n\nThe main advantage that I come across in using Apache Arrow is that (or that's how it is popularised): Arrow's table can be shared with other processes running the same machine without copying the data residing in the RAM. The other process may use the same language/ different language than that of the process holding the Arrow table.\n\nI am trying to find an example of this so that I get a better idea; but somehow not able to land on a good one.(Lets say: host process in running in Python and the other process trying to access the Arrow table is running in Java)\n\nBesides, I have a general question: Operating system generally does not allow processes accessing each other address space. How Arrow overcomes this?\n\nwould it be possible to redirect me to a suitable place which has these sorts of examples."
    },
    {
        "link": "https://stackoverflow.com/questions/75091180/whats-the-best-practice-for-swap-apache-arrow-data-between-different-processes",
        "document": "The easiest way to do this is to send the file across a socket, preferably with something like Flight. That would be my recommendation until you prove that performance is insufficient.\n\nSince these are on the same machine you can potentially use something like memory mapped files. You first open a memory mapped file (you'll need to know the size and I'm not sure exactly how to do this but you can easily find the size of the buffers and you can just make a conservative guess for how much you'll need for metadata) and then write to that file on the producer. Make sure to write data using the Arrow format with no compression. This will involve one memory copy from user space to kernel space. You would then send the filename to the consumers over a socket.\n\nThen, on the consumers, you can open the file as a memory mapped file and work with the data (presumably in a read-only fashion since there are multiple consumers). This read will be zero-copy as the file should still be in the kernel's cache. If you are in a situation where the kernel is swapping however then this is likely to backfire."
    },
    {
        "link": "https://medium.com/data-engineering-with-dremio/getting-started-with-data-analytics-using-pyarrow-in-python-ac7a100bc569",
        "document": "\n• Apache Iceberg Crash Course: What is a Data Lakehouse and a Table Format?\n\nIn this guide, we will explore data analytics using PyArrow, a powerful library designed for efficient in-memory data processing with columnar storage. We will work within a pre-configured environment using the Python Data Science Notebook Docker Image. This environment includes all the essential libraries for data manipulation, machine learning, and database connectivity, making it an ideal setup for performing analytics with PyArrow.\n\nTo get started, you can pull and run the Docker container by following these steps:\n\nAccess Jupyter Notebook: Open your browser and navigate to http://localhost:8888 to access the notebook interface.\n\nThis setup provides a user-friendly experience with Jupyter Notebook running on port 8888, where you can easily write and execute Python code for data analysis.\n\nApache Arrow is an open-source framework optimized for in-memory data processing with a columnar format. PyArrow, the Python implementation of Arrow, enables faster, more efficient data access and manipulation compared to traditional column-based libraries like Pandas. Here are some key benefits of using PyArrow:\n• Faster Data Processing: PyArrow uses a columnar memory layout that accelerates access to large datasets.\n• Lower Memory Usage: Thanks to Arrow’s efficient memory format, you can handle larger datasets with less memory.\n• Interoperability: PyArrow integrates smoothly with other systems and languages, making it a versatile tool for multi-language environments.\n• Better Support for Large Datasets: PyArrow is designed to handle big data tasks, making it ideal for workloads that Pandas struggles with.\n\nPyArrow provides a set of data structures that are specifically optimized for in-memory analytics and manipulation. In this section, we will explore the key objects in PyArrow and their purposes.\n• A in PyArrow is a collection of columnar data, optimized for efficient processing and memory usage.\n• It can be thought of as similar to a DataFrame in Pandas but designed to work seamlessly with Arrow’s columnar format.\n• Tables can be partitioned and processed in parallel, which improves performance with large datasets.\n\nA RecordBatch is a collection of rows with a defined schema. It allows for efficient in-memory processing of data in batches.\n\nIt’s useful when you need to process data in chunks, enabling better memory management.\n\nAn Array in PyArrow is a fundamental data structure representing a one-dimensional, homogeneous sequence of values.\n\nArrays can be of various types, including integers, floats, strings, and more. PyArrow provides specialized arrays for different types of data.\n\nA Schema defines the structure of data in a Table or RecordBatch. It consists of the names and data types of each column.\n\nSchemas ensure that all data being processed follows a consistent format.\n\nA ChunkedArray is a sequence of Array objects that have been split into smaller chunks. This allows for parallel processing on chunks of data, improving efficiency when working with larger datasets.\n\nThese core objects are essential for working with PyArrow and enable efficient data processing in memory. By utilizing PyArrow’s columnar format and its efficient handling of large datasets, you can perform complex data manipulations with ease. As we continue, you’ll see how these objects interact to make reading, writing, and analyzing data faster and more memory-efficient.\n\nParquet is a columnar storage file format that is widely used in big data analytics. Its efficient compression and encoding make it ideal for storing large datasets. In this section, we will explore how to use PyArrow to read from and write to Parquet files.\n• Efficient Storage: Parquet’s columnar format allows for efficient compression, reducing the storage size of large datasets.\n• Faster Querying: By storing data in columns, Parquet files allow analytical queries to scan only the relevant columns, reducing I/O and improving performance.\n• Interoperability: Parquet is a widely supported format that can be read and written by many different systems, making it ideal for data exchange.\n\nUsing PyArrow, you can easily read a Parquet file into memory as a PyArrow . This table can then be used for further data processing or manipulation.\n\nIn this example, the function reads the Parquet file and returns a object. This table can now be used for in-memory operations such as filtering, joining, or aggregating data.\n\nTo store data as Parquet, you can write a PyArrow Table back to disk in Parquet format. PyArrow provides methods for this purpose, allowing you to save your data efficiently.\n\nIn this example, a PyArrow table is created and saved to disk as a Parquet file using the function.\n\nOne of the key advantages of Parquet is its ability to handle large datasets efficiently. When reading a Parquet file, you can load only specific columns into memory, which is especially useful when working with large datasets.\n\nThis code demonstrates how to read only the relevant columns, reducing the memory footprint when loading the dataset.\n\nBy using PyArrow to read and write Parquet files, you gain access to a highly efficient, compressed, and columnar data format that works well for large datasets. PyArrow simplifies working with Parquet by providing easy-to-use functions for loading and saving data, while also supporting advanced operations like selective column reads to optimize performance.\n\nPyArrow not only provides efficient tools for reading and writing Parquet files but also enables you to perform basic data analytics operations like filtering, joining, and aggregating data in memory. These operations can be performed directly on PyArrow objects, offering a significant performance boost when dealing with large datasets.\n\nPyArrow allows you to filter rows based on conditions, similar to how you would with Pandas. This operation is highly efficient due to the columnar nature of PyArrow’s data structures.\n\nIn this example, we use PyArrow’s compute module to filter the data. The pc.greater() function returns a boolean mask, and the filter() method applies this mask to the table, returning only rows where ‘column1’ is greater than 2.\n\nJust like in SQL or Pandas, PyArrow allows you to join two tables based on a common column. This operation is particularly useful when combining datasets.\n\nHere, we use PyArrow’s join method to perform an inner join on two tables, combining them based on the common column ‘key’. The result is a new table with data from both tables.\n\nAggregation operations like summing, counting, and averaging are essential for data analytics. PyArrow provides efficient methods to perform these operations on large datasets.\n\nIn this example, we use the function to calculate the sum of a column. Similarly, you can apply other aggregation functions like , , or .\n\nPyArrow allows you to chain operations together, such as filtering the data first and then applying aggregation.\n\nIn this case, we first filter the data and then apply the aggregation function on the filtered subset. This combination of operations enables more complex analyses with just a few lines of code.\n\nPyArrow’s powerful analytical capabilities make it a great choice for performing data operations on large datasets. By leveraging its efficient in-memory structures, you can filter, join, and aggregate data in a way that is both fast and memory-efficient. Whether you are working with small or large datasets, PyArrow provides the tools to handle your data analytics tasks with ease.\n\nSection 4: Working with JSON, CSV, and Feather Files using PyArrow\n\nIn addition to Parquet, PyArrow supports a wide variety of file formats, including JSON, CSV, and Feather. These formats are commonly used for data storage and interchange, and PyArrow makes it easy to read from and write to them efficiently.\n\nJSON (JavaScript Object Notation) is a lightweight data-interchange format that is widely used for data transfer. While it may not be as efficient as columnar formats like Parquet, JSON is still commonly used, especially for web data.\n\nPyArrow allows you to read JSON data and convert it into a PyArrow for further processing.\n\nPyArrow can also write Table data back into JSON format, making it convenient for exchanging data in systems where JSON is the preferred format.\n\nCSV (Comma-Separated Values) is one of the most common file formats for structured data, particularly in data science and analytics. PyArrow makes it easy to work with CSV files by converting them to Table objects.\n\nPyArrow’s CSV reader allows for fast parsing of large CSV files, which can then be converted into tables for in-memory analytics.\n\nYou can also write PyArrow tables back to CSV format, which is helpful for data sharing and reporting.\n\nFeather is a binary columnar file format that provides better performance compared to CSV and JSON, while maintaining interoperability between Python and R. PyArrow natively supports Feather, allowing for efficient storage and fast reads.\n\nFeather files are ideal for fast I/O operations and work seamlessly with PyArrow.\n\nPyArrow can write Table objects to Feather format, offering a balance between ease of use and performance, particularly for in-memory data sharing.\n\nPyArrow’s support for various file formats — such as JSON, CSV, and Feather — makes it a versatile tool for data analytics. Whether you’re working with structured CSVs, exchanging JSON data, or aiming for faster performance with Feather files, PyArrow simplifies the process of reading and writing these formats. This flexibility allows you to handle a wide range of data tasks, from data ingestion to efficient storage and retrieval.\n\nApache Arrow Flight is a high-performance data transport layer built on top of Apache Arrow. It provides an efficient way to transfer large datasets between systems. One of its key benefits is the ability to perform fast, scalable data transfers using gRPC for remote procedure calls. In this section, we will explore how to use Apache Arrow Flight with PyArrow with an example of connecting to Dremio, a popular data platform that supports Arrow Flight for query execution.\n\nBelow is an example of how to connect to Dremio using PyArrow Flight, execute a query, and retrieve the results.\n\nThe location variable holds the address of the Dremio server that supports Apache Arrow Flight. Here, we use gRPC over TLS for a secure connection to Dremio Cloud.\n\nThe token is retrieved from an environment variable using . This token is required for authenticating requests to Dremio’s Arrow Flight server.\n\nThe headers include an authorization field with the bearer token, which is required for Dremio to authenticate the request. We use the to attach this header to our request later.\n\nThis is the SQL query we will execute on Dremio. You can replace “table1” with any table or a more complex SQL query as needed.\n\nThe is the main object used to interact with the Arrow Flight server. It is initialized with the location of the server, allowing us to send requests and receive results.\n\nHere, FlightCallOptions is used to attach the headers (including our authentication token) to the requests made by the FlightClient.\n\nThe function sends the query to Dremio and returns information about the query’s execution, such as where the results are located. The method is used to wrap the SQL query into a format understood by the Flight server.\n\nThe function fetches the results of the query from the server. It takes in a ticket, which points to the data location, and the options to pass authentication headers.\n\nFinally, the function is called to read all of the results into memory, and displays the data.\n• High Performance: Arrow Flight is optimized for fast, high-volume data transfers, making it ideal for large datasets.\n• gRPC Communication: The use of gRPC allows for more efficient, low-latency communication between systems.\n• Cross-Language Support: Arrow Flight works across multiple programming languages, providing flexibility in how data is accessed and processed.\n\nApache Arrow Flight with PyArrow offers an efficient and powerful way to transport data between systems, especially in high-performance environments. Using the example above, you can easily connect to Dremio, execute queries, and retrieve data in a highly optimized fashion. The combination of Arrow’s in-memory data structures and Flight’s fast data transport capabilities makes it an excellent tool for scalable, real-time data analytics.\n\nIn this blog, we explored the powerful capabilities of PyArrow for data analytics and efficient data handling. We began by setting up a practice environment using a Python Data Science Notebook Docker Image, which provides a comprehensive suite of pre-installed libraries for data manipulation and analysis.\n\nWe discussed the core benefits of PyArrow over traditional libraries like Pandas, focusing on its performance advantages, particularly for large datasets. PyArrow’s columnar memory layout and efficient in-memory processing make it a go-to tool for high-performance analytics.\n\nThroughout the blog, we covered key PyArrow objects like , , , , and , explaining how they work together to enable efficient data processing. We also demonstrated how to read and write Parquet, JSON, CSV, and Feather files, showcasing PyArrow's versatility across various file formats commonly used in data science.\n\nAdditionally, we delved into essential data operations like filtering, joining, and aggregating data using PyArrow. These operations allow users to handle large datasets efficiently while performing complex analyses with minimal memory usage.\n\nLastly, we introduced Apache Arrow Flight as a high-performance transport layer for data transfer. We provided a detailed example of how to connect to Dremio, execute SQL queries, and retrieve results using Arrow Flight, highlighting its benefits for scalable, real-time data access.\n\nWith these tools and techniques, you are equipped to perform efficient data analytics using PyArrow, whether you’re working with local files or connecting to powerful cloud-based platforms like Dremio. By leveraging PyArrow’s capabilities, you can handle big data tasks with speed and precision, making it an indispensable tool for modern\n• Apache Iceberg Crash Course: What is a Data Lakehouse and a Table Format?"
    },
    {
        "link": "https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i",
        "document": "At the time of writing this post, we are in the process of releasing pandas 2.0. The project has a large number of users, and it's used in production quite widely by personal and corporate users. This large use based forces us to be conservative and make us avoid most big changes that would break existing pandas code, or would change what users already know about pandas. So, most changes to pandas, while they are important, they are quite subtle. Most of our changes are bug fixes, code improvements and clean up, performance improvements, keep up to date with our dependencies, small changes that make the API more consistent, etc.\n\nA recent change that may seem subtle and it's easy to not be noticed, but it's actually very important is the new Apache Arrow backend for pandas data. To understand this change, let's quickly summarize how pandas works. The general idea is that before being able to do anything in pandas it is needed to load into memory the data of interest (using methods like , , , etc). When loading data into memory it's required to decide how this data will be stored in memory. For simple data like integers of floats this is in general not so complicated, as how to represent a single item is mostly standard, and we just need arrays of the number of elements in our data. But for other types (such as strings, dates and times, categories, etc.) some decisions need to be made. Python is able to represent mostly anything, but Python data structures (lists, dictionaries, tuples, etc) are very slow and can't be used. So the data representation is not Python and is not standard, and an implementation needs to happen via Python extensions, usually implemented in C (also in C++, Rust and others). For many years, the main extension to represent arrays and perform operations on them in a fast way has been NumPy. And this is what pandas was initially built on.\n\nWhile NumPy has been good enough to make pandas the popular library it is, it was never built as a backend for dataframe libraries, and it has some important limitations. A couple of examples are the poor support for strings and the lack of missing values. To get a more detailed idea of this topic, you can read Wes McKinney's article Apache Arrow and the 10 Things I Hate About pandas.\n\nFor some years now, while still relying heavely on NumPy, pandas has been slowly decoupling from it. A couple of important milestones were the addition of an API to implement Extension Arrays for pandas in 2018. Extension arrays allow pandas to work with custom data types. Not long after they were added to pandas, all existing complex types in pandas (e.g. categorical or datetime with timezone types) were moved to this interface. Another important milestone was the implementation of a string data type based on Arrow that started in 2020.\n\nAfter these additions, in pandas 1.5 and 2.0 we are adding Apache Arrow support for all data types. Which is the topic of the rest of this article.\n\nBy default pandas will keep using the original types. As mentioned earlier, one of our top priorities is not breaking existing code or APIs. See these pandas 2.0 examples:\n\nNothing really changed. But we can change the to use Arrow:\n\nIn the pandas 2.0 release candidates there was a option to let pandas know we want Arrow backed types by default. The option was confusing since not all operations support generating Arrow backed data yet, and it was removed. For I/O operators that support creating Arrow-backed data, there is a parameter:\n\nNote that the engine is somehow independent of the backend. We can use PyArrow function (engine) to read CSV files while using columns with a NumPy data type (backend), and the other way round.\n\nThere are several advantages that Arrow provides, even for simple types. We are going to list some of them in the next sections.\n\nRepresenting missing values is a complex topic. In Python it's not, since everything is wrapped as a Python object, it's possible to mix different types in lists, and you can simply use the value for any missing data. But when performance is important, data types are represented in the CPU representation, and can't be mixed with other types. For example, an unsigned integer of 8 bits, will represent 0 as 00000000 and 255 as 11111111, and our whole list of values needs to use that. Any bit representation corresponds to a number in the range 0 to 255, and it's not possible to represent a missing value unless using a sentinel value. For example, using 255 as the missing value in your code, and only allowing values from 0 to 254 for your type. But this is complex and tricky, and surely not ideal.\n\nFor the case of floating point numbers, the internal (CPU) representation is more complex, and there are actually some sentinel values already defined in the IEEE 754 standard, which CPUs implement, and are able to deal with efficiently. For this reason, historically, the approach of pandas to missing values has been to convert numbers to floating point if they were not already, and use as the missing value. Converting integer values to floating point notation to support missing value is again not ideal, and has side effects.\n\nMore recently, after extension arrays were added to pandas, it was possible for pandas to add its own data types for missing values. These are implemented by using two arrays instead of one. The main array represents the data in the same way as if there were no missing values. But there is an additional boolean array that indicates which values from the main array are present and need to be considered, and which are not and must be ignored.\n\nThe Apache Arrow in-memory data representation includes an equivalent representation as part of its specification. By using Arrow, pandas is able to deal with missing values without having to implement its own version for each data type.\n\nThere are many operations that can be done with dataframes. And each case would require its own analysis, but in general we can assume that the Arrow implementation is able to perform operations faster. Some examples, using a dataframe with 2.5 million rows in my laptop:\n\nWe can see how Arrow seems to be consistenly faster. And in the case of dealing with strings, the difference is huge, since NumPy is not really designed to work with strings (even if it can support them).\n\nIn the same way a CSV is a file format that different projects understand (pandas, R, Excel, etc), Arrow is also a program independent format. It may be less obvious, since it's an in-memory format, not a file format. But even if it doesn't have a file extension associated, you can think of it as data that can be accessed by different consumers that implement its open specification.\n\nThis has two main implications. It's (relatively) easy and standard to share the data among different programs. And it can be done in a extremely fast and memory efficient way, since two programs can literally share the same data (the same memory, without having to make a copy for each program).\n\nThis may not sound useful or intuitive, since there isn't an easy way (that I know) to use simultaneously for example pandas and R with the same loaded data. And maybe it's not common to want to do do anyway. But there are some examples where interoperability can be helpful, and we will see one of them.\n\nI need to build a pipeline to load some data from my company data warehouse, transform it, compute some analytics, and then export an automatically generated long report with the analytics. The infrastructure in my company is not very modern, and my data is available as SAS files. I want to generate my reports with a professional and scientific looking style, so I decide to use LATEX for the output.\n\nGiven the problem, pandas seems like a reasonable choice of tool for the job. It can import data from SAS files, and it can build LATEX tables. Another choice could be Polars, which is similar to pandas. Not as stable or mature yet, but it's faster and more memory efficient than pandas. This is among other things because it provides a query optimizer that can make the pipeline run faster by analyzing all operations together before executing them. Unfortunately, Polars is not able to load data from SAS or export LATEX tables as easily as pandas (at least that I know).\n\nBesides just ignore Polars and use pandas, another option could be:\n• Load the data from SAS into a pandas dataframe\n• Export the Polars dataframe into a second parquet file\n• Export the data to the final LATEX file\n\nThis would somehow solve our problem, but given that we're using Polars to speed up things, writing and reading from disk is going to be slowing down my pipeline significantly.\n\nIn the old pandas world, we would appreciate if pandas could export the NumPy arrays containing the dataframe data to a memory format that Polars could understand. Or if Polars could understand NumPy directly. Both happen to be true actually. But things are even better with the new pandas. Because Polars internal data representation is actually Apache Arrow, and now it's also one of the possible internal representations for pandas dataframes.\n\nIt may seem that by using Apache Arrow I can just share a Python Arrow structure among both projects and that's it. And it really is for final users. But things are actually more complex. Both pandas and Polars are Python wrappers around other libraries, so the data to share is not really a Python structure. In the case of pandas the Arrow backed data is actually a PyArrow (C++) structure. While the Python dataframe in Polars is an wrapper around a Rust data structure, an Arrow2 Rust structure to be specific (there is another Arrow implementation in Rust, the official Arrow).\n\nSo, when sharing data between pandas and Polars, what we are really doing is to convert a PyArrow object into an Arrow2 object (or the other way round). And the good news is that there is not much to convert, since internally both libraries implement the same data representation specification, the Apache Arrow specification.\n\nThe change is not immediate, and some metadata with the pointer to where the data is stored in memory and information about the schema needs to be generated from one library and sent to the other. But the data itself (which could potentially be gigabytes or terabytes of data) stays the same, and doesn't need to be copied or converted. This allows sharing the data to happen extremely fast even when the data is huge.\n\nSo, our program could look something like this:\n\nAt the time of writing this post has been updated just few days ago to support the and allow the Arrow data in Polars to be shared directly to pandas (without converting it to NumPy arrays. And has a bug that causes the data being copied, but it should be fixed soon. The above code will unnecessarily make a copy right now, but it should not making a copy after the release of pandas 2.0 and the next release of Polars.\n\nOne last advantage of using Apache Arrow as the container for pandas data is the support for more and better data types compared to NumPy. As a numerical computing tool, NumPy provides great support for integer and float values. It also has support for boolean values backed by 8 bits per value, and datetime values backed by 64 bits. But not much more than that.\n\nArrow types are broader and better when used outside of a numerical tool like NumPy. It has better support for dates and time, including types for date-only or time-only data, different precision (e.g. seconds, milliseconds, etc.), different sizes (32 bits, 63 bits, etc.). The boolean type in Arrow uses a single bit per value, consuming one eighth of memory. It also supports other types, like decimals, or binary data, as well as complex types (for example a column where each value is a list). There is a table in the pandas documentation mapping Arrow to NumPy types.\n\nArrow also defines a type to encode categories, so you can for example have a column with the values \"red\", \"green\" and \"blue\", and instead of having to store the strings for each element in the possibly huge column, an index is stored for each value (e.g. 0, 1 and 2), and then a small lookup table to know that 0 is \"red\", 1 is \"green\" and 2 is \"blue\". All this transparent to users, who will only notice of this data representation because of the memory saving. As a note, pandas has its own implementation of an equivalent categorical type backed by NumPy arrays.\n\nSee this example:\n\nAs you can see by the attributes, pandas will be storing this information in formats you may have not seen before. pandas has supported all the data in the example above since the early days, but the internal data type used was . If you are not familiar with the type, it is just a pointer to a Python object, and operations with the data type would probably be as slow as if you implemented your code directly using Python structures (quite slow). In comparison, the Arrow types are stored in memory with performance in mind, and operations with the data should be reasonably fast.\n\nOne question you probably have is, what operations can I do with Arrow types? And the answer depends on the exact data type. For example, Arrow strings are well supported, and have been available in pandas for few versions now. The data type is quite new, but operations like will work.\n\nWhat about the list of strings type used for ? There is nothing much supported with them right now. If you try the example above, you will see that not even printing pandas data with the data type is supported. This may sound disappointing but I have some good news. First, support for some Arrow types has just been introduced, and it'll be improved over time. And second, nothing prevents you from writing your own operations with any language that has an Apache Arrow implementation. And as we'll see in the part II of this article, it is not difficult.\n\nIn this article we've seen how Apache Arrow and its integration into pandas is helping pandas be simpler, run faster, play better with related libraries, and efficiently represent more types of data.\n\nIn part II we will show how to implement pandas extensions by using Arrow. These extensions can run extremely fast, and not only work for pandas, but also to work for any other library using Apache Arrow internally (such as Polars or Vaex)."
    }
]