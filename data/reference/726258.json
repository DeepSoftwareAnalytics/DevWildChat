[
    {
        "link": "https://pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn",
        "document": "In this tutorial, you will receive a gentle introduction to training your first Convolutional Neural Network (CNN) using the PyTorch deep learning library. This network will be able to recognize handwritten Hiragana characters.\n\nIn training a Convolutional Neural Network (CNN), a dataset provides the necessary variety of inputs and labels that the network needs to learn from. This ensures the model can generalize well when encountering unseen data.\n\nRoboflow has free tools for each stage of the computer vision pipeline that will streamline your workflows and supercharge your productivity.\n\nSign up or Log in to your Roboflow account to access state of the art dataset libaries and revolutionize your computer vision pipeline.\n\nYou can start by choosing your own datasets or using our PyimageSearch’s assorted library of useful datasets.\n\nBring data in any of 40+ formats to Roboflow, train using any state-of-the-art model architectures, deploy across multiple platforms (API, NVIDIA, browser, iOS, etc), and connect to applications or 3rd party tools.\n\nToday’s tutorial is part three in our five part series on PyTorch fundamentals:\n• Intro to PyTorch: Training your first neural network using PyTorch\n\nLast week you learned how to train a very basic feedforward neural network using the PyTorch library. That tutorial focused on simple numerical data.\n\nToday, we will take the next step and learn how to train a CNN to recognize handwritten Hiragana characters using the Kuzushiji-MNIST (KMNIST) dataset.\n\nAs you’ll see, training a CNN on an image dataset isn’t all that different from training a basic multi-layer perceptron (MLP) on numerical data. We still need to:\n• Loop over our epochs and batches\n• Properly zero our gradient, perform backpropagation, and update our model parameters\n\nFurthermore, this post will also give you some experience with PyTorch’s implementation which makes it super easy to work with datasets — becoming proficient with PyTorch’s is a critical skill you’ll want to develop as a deep learning practitioner (and it’s a topic that I’ve dedicated an entire course to inside PyImageSearch University).\n\nTo learn how to train your first CNN with PyTorch, just keep reading.\n\nCNNs are a type of deep learning algorithm that can analyze and extract features from images, making them highly effective for image classification and object detection tasks. In this tutorial, we will go through the steps of implementing a CNN in PyTorch\n\nThroughout the remainder of this tutorial, you will learn how to train your first CNN using the PyTorch framework.\n\nWe’ll start by configuring our development environment to install both and , followed by reviewing our project directory structure.\n\nI’ll then show you the KMNIST dataset (a drop-in replacement for the MNIST digits dataset) that contains Hiragana characters. Later in this tutorial, you’ll learn how to train a CNN to recognize each of the Hiragana characters in the KMNIST dataset.\n\nWe’ll then implement three Python scripts with PyTorch, including our CNN architecture, training script, and a final script used to make predictions on input images.\n\nBy the end of this tutorial, you’ll be comfortable with the steps required to train a CNN with PyTorch.\n\nUpdates:\n\nThis blog post was last updated in January 2023 with additional explanations of CNNs and PyTorch background information.\n\nTo follow this guide, you need to have PyTorch, OpenCV, and scikit-learn installed on your system.\n\nLuckily, all three are extremely easy to install using pip:\n\nIf you need help configuring your development environment for PyTorch, I highly recommend that you read the PyTorch documentation — PyTorch’s documentation is comprehensive and will have you up and running quickly.\n\nAnd if you need help installing OpenCV, be sure to refer to my pip install OpenCV tutorial.\n\nAll that said, are you:\n• Wanting to skip the hassle of fighting with the command line, package managers, and virtual environments?\n• Ready to run the code right now on your Windows, macOS, or Linux system?\n\nGain access to Jupyter Notebooks for this tutorial and other PyImageSearch guides that are pre-configured to run on Google Colab’s ecosystem right in your web browser! No installation required.\n\nAnd best of all, these Jupyter Notebooks will run on Windows, macOS, and Linux!\n\nThe dataset we are using today is the Kuzushiji-MNIST dataset, or KMNIST, for short. This dataset is meant to be a drop-in replacement for the standard MNIST digits recognition dataset.\n\nThe KMNIST dataset consists of 70,000 images and their corresponding labels (60,000 for training and 10,000 for testing).\n\nThere are a total of 10 classes (meaning 10 Hiragana characters) in the KMNIST dataset, each equally distributed and represented. Our goal is to train a CNN that can accurately classify each of these 10 characters.\n\nAnd lucky for us, the KMNIST dataset is built into PyTorch, making it super easy for us to work with!\n\nBefore we start implementing any PyTorch code, let’s first review our project directory structure.\n\nStart by accessing the “Downloads” section of this tutorial to retrieve the source code and pre-trained model.\n\nYou’ll then be presented with the following directory structure:\n\nWe have three Python scripts to review today:\n• : Our PyTorch implementation of the famous LeNet architecture\n• : Trains LeNet on the KMNIST dataset using PyTorch, then serializes the trained model to disk (i.e., )\n• : Loads our trained model from disk, makes predictions on testing images, and displays the results on our screen\n\nThe directory will be populated with (a plot of our training/validation loss and accuracy) and (our trained model file) once we run .\n\nWith our project directory structure reviewed, we can move on to implementing our CNN with PyTorch.\n\nThe Convolutional Neural Network (CNN) we are implementing here with PyTorch is the seminal LeNet architecture, first proposed by one of the grandfathers of deep learning, Yann LeCunn.\n\nBy today’s standards, LeNet is a very shallow neural network, consisting of the following layers:\n\nAs you’ll see, we’ll be able to implement LeNet with PyTorch in only 60 lines of code (including comments).\n\nThe best way to learn about CNNs with PyTorch is to implement one, so with that said, open the file in the module, and let’s get to work:\n\nLines 2-8 import our required packages. Let’s break each of them down:\n• : Rather than using the PyTorch class to implement LeNet, we’ll instead subclass the object so you can see how PyTorch implements neural networks using classes\n• : Applies 2D max-pooling to reduce the spatial dimensions of the input volume\n• : Used when building our softmax classifier to return the predicted probabilities of each class\n• : Flattens the output of a multi-dimensional volume (e.g., a CONV or POOL layer) such that we can apply fully connected layers to it\n\nWith our imports taken care of, we can implement our class using PyTorch:\n\nLine 10 defines the class. Notice how we are subclassing the object — by building our model as a class we can easily:\n• Implement custom functions to generate subnetworks/components (used very often when implementing more complex networks, such as ResNet, Inception, etc.)\n\nBest of all, when defined correctly, PyTorch can automatically apply its autograd module to perform automatic differentiation — backpropagation is taken care of for us by virtue of the PyTorch library!\n\nThe constructor to accepts two variables:\n• : The number of channels in the input images ( for grayscale or for RGB)\n• : Total number of unique class labels in our dataset\n\nLine 13 calls the parent constructor (i.e., ) which performs a number of PyTorch-specific operations.\n\nFrom there, we start defining the actual LeNet architecture.\n\nLines 16-19 initialize our first set of layers. Our first CONV layer learns a total of 20 filters, each of which are 5×5. A ReLU activation function is then applied, followed by a 2×2 max-pooling layer with a 2×2 stride to reduce the spatial dimensions of our input image.\n\nWe then have a second set of layers on Lines 22-25. We increase the number of filters learned in the CONV layer to 50, but maintain the 5×5 kernel size. Again, a ReLU activation is applied, followed by max-pooling.\n\nNext comes our first and only set of fully connected layers (Lines 28 and 29). We define the number of inputs to the layer ( ) along with our desired number of output nodes ( ). A ReLU activation follows the FC layer.\n\nFinally, we apply our softmax classifier (Lines 32 and 33). The number of is set to , which is the output dimensionality from the previous layer. We then apply such that we can obtain predicted probabilities during evaluation.\n\nIt’s important to understand that at this point all we have done is initialized variables. These variables are essentially placeholders. PyTorch has absolutely no idea what the network architecture is, just that some variables exist inside the class definition.\n\nTo build the network architecture itself (i.e., what layer is input to some other layer), we need to override the method of the class.\n• It connects layers/subnetworks together from variables defined in the constructor (i.e., ) of the class\n• It defines the network architecture itself\n• It allows the forward pass of the model to be performed, resulting in our output predictions\n• And, thanks to PyTorch’s autograd module, it allows us to perform automatic differentiation and update our model weights\n\nThe method accepts a single parameter, , which is the batch of input data to the network.\n\nWe then connect our , , and layers together to form the first layer of the network (Lines 38-40).\n\nA similar operation is performed on Lines 44-46, this time building the second set of layers.\n\nAt this point, the variable is a multi-dimensional tensor; however, in order to create our fully connected layers, we need to “flatten” this tensor into what essentially amounts to a 1D list of values — the function on Line 50 takes care of this operation for us.\n\nFrom there, we connect the and layers to the network architecture (Lines 51 and 52), followed by attaching the final and (Lines 56 and 57).\n\nThe of the network is then returned to the calling function.\n\nAgain, I want to reiterate the importance of initializing variables in the constructor versus building the network itself in the function:\n• The constructor to your only initializes your layer types. PyTorch keeps track of these variables, but it has no idea how the layers connect to each other.\n• For PyTorch to understand the network architecture you’re building, you define the function.\n• Inside the function you take the variables initialized in your constructor and connect them.\n• PyTorch can then make predictions using your network and perform automatic backpropagation, thanks to the autograd module\n\nCongrats on implementing your first CNN with PyTorch!\n\nWith our CNN architecture implemented, we can move on to creating our training script with PyTorch.\n\nOpen the file in your project directory structure, and let’s get to work:\n\nLines 2 and 3 import and set the appropriate background engine.\n\nFrom there, we import a number of notable packages:\n• : Our PyTorch implementation of the LeNet CNN from the previous section\n• : Used to display a detailed classification report on our testing set\n• : Constructs a random training/testing split from an input set of data\n• : PyTorch’s awesome data loading utility that allows us to effortlessly build data pipelines to train our CNN\n• : A preprocessing function that converts input data into a PyTorch tensor for us automatically\n• : The Kuzushiji-MNIST dataset loader built into the PyTorch library\n• : The optimizer we’ll use to train our neural network\n\nWe have two command line arguments that need parsing:\n• : The path to our output serialized model after training (we save this model to disk so we can use it to make predictions in our script)\n• : The path to our output training history plot\n\nMoving on, we now have some important initializations to take care of:\n\nLines 29-31 set our initial learning rate, batch size, and number of epochs to train for, while Lines 34 and 35 define our training and validation split size (75% of training, 25% for validation).\n\nLine 38 then determines our (i.e., whether we’ll be using our CPU or GPU).\n\nLines 42-45 load the KMNIST dataset using PyTorch’s build in class.\n\nFor our , we set while our is loaded with . These Booleans come in handy when working with datasets built into the PyTorch library.\n\nThe flag indicates that PyTorch will automatically download and cache the KMNIST dataset to disk for us if we had not previously downloaded it.\n\nAlso take note of the parameter — here we can apply a number of data transformations (outside the scope of this tutorial but will be covered soon). The only transform we need is to convert the NumPy array loaded by PyTorch into a tensor data type.\n\nWith our training and testing set loaded, we drive our training and validation set on Lines 49-53. Using PyTorch’s function, we can easily split our data.\n\nWe now have three sets of data:\n\nThe next step is to create a for each one:\n\nBuilding the objects is accomplished on Lines 56-59. We set only for our since our validation and testing sets do not require shuffling.\n\nWe also derive the number of training steps and validation steps per epoch (Lines 62 and 63).\n\nAt this point our data is ready for training; however, we don’t have a model to train yet!\n\nLines 67-69 initialize our . Since the KMNIST dataset is grayscale, we set . We can easily set the number of by calling of our .\n\nWe also call to move the to either our CPU or GPU.\n\nLines 72 and 73 initialize our optimizer and loss function. We’ll use the Adam optimizer for training and the negative log-likelihood for our loss function.\n\nWhen we combine the class with in our model definition, we arrive at categorical cross-entropy loss (which is the equivalent to training a model with an output layer and an loss). Basically, PyTorch allows you to implement categorical cross-entropy in two separate ways.\n\nGet used to seeing both methods as some deep learning practitioners (almost arbitrarily) prefer one over the other.\n\nWe then initialize , our training history dictionary (Lines 76-81). After every epoch we’ll update this dictionary with our training loss, training accuracy, testing loss, and testing accuracy for the given epoch.\n\nFinally, we start a timer to measure how long training takes (Line 85).\n\nAt this point, all of our initializations are complete, so it’s time to train our model.\n\nNote: Be sure you’ve read the previous tutorial in this series, Intro to PyTorch: Training your first neural network using PyTorch, as we’ll be building on concepts learned in that guide.\n\nBelow follows our training loop:\n\nOn Line 88, we loop over our desired number of epochs.\n\nWe then proceed to:\n• Initialize our training loss and validation loss for the current epoch\n• Initialize our number of correct training and validation predictions for the current epoch\n\nLine 102 shows the benefit of using PyTorch’s class — all we have to do is start a loop over the object. PyTorch automatically yields a batch of training data. Under the hood, the is also shuffling our training data (and if we were doing any additional preprocessing or data augmentation, it would happen here as well).\n\nFor each batch of data (Line 104) we perform a forward pass, obtain our predictions, and compute the loss (Lines 107 and 108).\n\nNext comes the all important step of:\n• Updating the weights of our model\n\nSeriously, don’t forget this step! Failure to do those three steps in that exact order will lead to erroneous training results. Whenever you write a training loop with PyTorch, I highly recommend you insert those three lines of code before you do anything else so that you are reminded to ensure they are in the proper place.\n\nWe wrap up the code block by updating our and bookkeeping variables.\n\nAt this point, we’ve looped over all batches of data in our training set for the current epoch — now we can evaluate our model on the validation set:\n\nWhen evaluating a PyTorch model on a validation or testing set, you need to first:\n• Use the context to turn off gradient tracking and computation\n\nFrom there, you loop over all validation (Line 128), move the data to the correct (Line 130), and use the data to make predictions (Line 133) and compute your loss (Line 134).\n\nYou can then derive your total number of correct predictions (Lines 137 and 138).\n\nWe round out our training loop by computing a number of statistics:\n\nLines 141 and 142 compute our average training and validation loss. Lines 146 and 146 do the same thing, but for our training and validation accuracy.\n\nWe then take these values and update our training history dictionary (Lines 149-152).\n\nFinally, we display the training loss, training accuracy, validation loss, and validation accuracy on our terminal (Lines 149-152).\n\nNow that training is complete, we need to evaluate our model on the testing set (previously we’ve only used the training and validation sets):\n\nLines 162-164 stop our training timer and show how long training took.\n\nWe then set up another context and put our model in mode (Lines 170 and 172).\n• Sending the current batch of data to the appropriate device (Line 180)\n• Making predictions on the current batch of data (Line 183)\n• Updating our list with the top predictions from the model (Line 184)\n\nThe last step we’ll do here is plot our training and validation history, followed by serializing our model weights to disk:\n\nWe then call to save our PyTorch model weights to disk so that we can load them from disk and make predictions from a separate Python script.\n\nAs a whole, reviewing this script shows you how much more control PyTorch gives you over the training loop — this is both a good and a bad thing:\n• It’s good if you want full control over the training loop and need to implement custom procedures\n• It’s bad when your training loop is simple and a Keras/TensorFlow equivalent to would suffice\n\nAs I mentioned in part one of this series, What is PyTorch, neither PyTorch nor Keras/TensorFlow is better than the other, there are just different caveats and use cases for each library.\n\nWe are now ready to train our CNN using PyTorch.\n\nBe sure to access the “Downloads” section of this tutorial to retrieve the source code to this guide.\n\nFrom there, you can train your PyTorch CNN by executing the following command:\n\nTraining our CNN took ≈160 seconds on my CPU. Using my GPU training time drops to ≈82 seconds.\n\nAt the end of the final epoch we have obtained 99.67% training accuracy and 98.23% validation accuracy.\n\nWhen we evaluate on our testing set we reach ≈95% accuracy, which is quite good given the complexity of the Hiragana characters and the simplicity of our shallow network architecture (using a deeper network such as a VGG-inspired model or ResNet-like would allow us to obtain even higher accuracy, but those models are more complex for an introduction to CNNs with PyTorch).\n\nFurthermore, as Figure 4 shows, our training history plot is smooth, demonstrating there is little/no overfitting happening.\n\nBefore moving to the next section, take a look at your directory:\n\nNote the file — this is our trained PyTorch model saved to disk. We will load this model from disk and use it to make predictions in the following section.\n\nThe final script we are reviewing here will show you how to make predictions with a PyTorch model that has been saved to disk.\n\nOpen the file in your project directory structure, and we’ll get started:\n\nLines 2-13 import our required Python packages. We set the NumPy random seed at the top of the script for better reproducibility across machines.\n• : Used to load our KMNIST testing data\n• : The Kuzushiji-MNIST dataset loader built into the PyTorch library\n• : Our OpenCV bindings which we’ll use for basic drawing and displaying output images on our screen\n\nNext comes our command line arguments:\n\nWe only need a single argument here, , the path to our trained PyTorch model saved to disk. Presumably, this switch will point to .\n\nLine 22 determines if we will be performing inference on our CPU or GPU.\n\nWe then load the testing data from the KMNIST dataset on Lines 26 and 27. We randomly sample a total of images from this dataset on Lines 28 and 29 using the class (which creates a smaller “view” of the full testing data).\n\nA is created to pass our subset of testing data through the model on Line 32.\n\nWe then load our serialized PyTorch model from disk on Line 35, passing it to the appropriate .\n\nFinally, the is placed into evaluation mode (Line 36).\n\nLet’s now make predictions on a sample of our testing set:\n\nLine 39 turns off gradient tracking, while Line 41 loops over all images in our subset of the test set.\n\nFor each image, we:\n• Grab the current image and turn it into a NumPy array (so we can draw on it later with OpenCV)\n• Sends the to the appropriate\n• Uses our trained LeNet model to make predictions on the current\n• Extracts the class label with the top predicted probability\n\nAll that’s left is a bit of visualization:\n\nEach image in the KMNIST dataset is a single channel grayscale image; however, we want to use OpenCV’s function to draw the predicted class label and ground-truth label on the .\n\nTo draw RGB colors on a grayscale image, we first need to create an RGB representation of the grayscale image by stacking the grayscale image depth-wise a total of three times (Line 58).\n\nAdditionally, we resize the so that we can more easily see it on our screen (by default, KMNIST images are only 28×28 pixels, which can be hard to see, especially on a high resolution monitor).\n\nFrom there, we determine the text and draw the label on the output image.\n\nWe wrap up the script by displaying the output on our screen.\n\nWe are now ready to make predictions using our trained PyTorch model!\n\nBe sure to access the “Downloads” section of this tutorial to retrieve the source code and pre-trained PyTorch model.\n\nFrom there, you can execute the script:\n\nAs our output demonstrates, we have been able to successfully recognize each of the Hiragana characters using our PyTorch model.\n\nIn this tutorial, you learned how to train your first Convolutional Neural Network (CNN) using the PyTorch deep learning library.\n\nYou also learned how to:\n• Load it from disk in a separate Python script\n• Use the PyTorch model to make predictions on images\n\nThis sequence of saving a model after training, and then loading it and using the model to make predictions, is a process you should become comfortable with — you’ll be doing it often as a PyTorch deep learning practitioner.\n\nSpeaking of loading saved PyTorch models from disk, next week you will learn how to use pre-trained PyTorch to recognize 1,000 image classes that you often encounter in everyday life. These models can save you a bunch of time and hassle — they are highly accurate and don’t require you to manually train them.\n\nTo download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), simply enter your email address in the form below!"
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html",
        "document": "Click here to download the full example code\n\nIn this tutorial, you will learn how to train a convolutional neural network for image classification using transfer learning. You can read more about the transfer learning at cs231n notes\n\nThese two major transfer learning scenarios look as follows:\n• None Finetuning the ConvNet: Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual.\n• None ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained.\n\nNow, let’s write a general function to train a model. Here, we will illustrate: In the following, parameter is an LR scheduler object from . # Each epoch has a training and validation phase # track history if only in train # backward + optimize only if in training phase Generic function to display predictions for a few images\n\nHere, we need to freeze all the network except the final layer. We need to set to freeze the parameters so that the gradients are not computed in . You can read more about this in the documentation here. # Parameters of newly constructed modules have requires_grad=True by default # Observe that only parameters of final layer are being optimized as # Decay LR by a factor of 0.1 every 7 epochs On CPU this will take about half the time compared to previous scenario. This is expected as gradients don’t need to be computed for most of the network. However, forward does need to be computed."
    },
    {
        "link": "https://datacamp.com/tutorial/pytorch-cnn-tutorial",
        "document": "Learn the fundamentals of neural networks and how to build deep learning models using Keras 2.0 in Python."
    },
    {
        "link": "https://medium.com/@myringoleMLGOD/simple-convolutional-neural-network-cnn-for-dummies-in-pytorch-a-step-by-step-guide-6f4109f6df80",
        "document": "In this blog, we’ll walk through building and training a simple Convolutional Neural Network (CNN) using PyTorch. We’ll use the MNIST dataset, a collection of handwritten digits, to train our model. This guide assumes you have some basic knowledge of Python and neural networks, but no prior experience with PyTorch is required.\n\nFirst, we need to import the necessary libraries. PyTorch is the main library we’ll use for building and training the neural network. We’ll also use for handling the dataset and transformations.\n\nWe’ll create a simple CNN with two convolutional layers followed by a fully connected layer. CNNs are particularly well-suited for image data because they automatically capture spatial hierarchies in images, such as edges, textures, and more complex patterns as we go deeper into the network.\n\nConvolutional layers are the building blocks of CNNs. They consist of several key components:\n• Filters are small matrices that slide over the input image and perform element-wise multiplications followed by summation. Each filter is designed to detect a specific feature in the input image.\n• For example, a filter might detect horizontal edges, vertical edges, or more complex textures.\n• The output of applying a filter to the input image is called a feature map or activation map. If you have multiple filters, you will get multiple feature maps.\n• Stride is the step size with which the filter moves across the input image.\n• A stride of 1 means the filter moves one pixel at a time, both horizontally and vertically.\n• A larger stride reduces the size of the feature map because the filter skips more pixels. For instance, a stride of 2 means the filter moves two pixels at a time, effectively down-sampling the feature map.\n• Padding involves adding extra pixels around the input image’s border. These extra pixels are typically set to zero (zero-padding).\n• Padding ensures that the filter fits properly over the image, especially at the edges. Without padding, the feature map’s size reduces after each convolution operation.\n• For example, if you have a 5x5 input image and a 3x3 filter with no padding, the resulting feature map will be 3x3. With padding of 1, the feature map remains the same size as the input.\n• A feature map is the output of a convolutional layer after applying filters to the input image.\n• Each feature map corresponds to a different filter and captures different features from the input.\n• Stacking multiple feature maps together forms a multi-channel output that serves as the input for the next layer.\n\nPooling layers reduce the spatial dimensions of the feature maps, which helps in making the network computationally efficient and reducing overfitting. There are two main types of pooling:\n• Max pooling takes the maximum value from each patch of the feature map.\n• For example, in a 2x2 max pooling operation, the maximum value from each 2x2 block of the feature map is taken to create a new, smaller feature map.\n• This operation reduces the size of the feature map by half, both horizontally and vertically, but retains the most prominent features.\n• Average pooling takes the average value from each patch of the feature map.\n• Similar to max pooling, but instead of the maximum value, it takes the average value from each block.\n• This can be useful in different contexts, though max pooling is more common in practice.\n\nHere’s how we define our CNN:\n\nclass CNN(nn.Module):\n\n def __init__(self, in_channels, num_classes=10):\n\n \"\"\"\n\n Define the layers of the convolutional neural network.\n\n\n\n Parameters:\n\n in_channels: int\n\n The number of channels in the input image. For MNIST, this is 1 (grayscale images).\n\n num_classes: int\n\n The number of classes we want to predict, in our case 10 (digits 0 to 9).\n\n \"\"\"\n\n super(CNN, self).__init__()\n\n\n\n # First convolutional layer: 1 input channel, 8 output channels, 3x3 kernel, stride 1, padding 1\n\n self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, stride=1, padding=1)\n\n # Max pooling layer: 2x2 window, stride 2\n\n self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n # Second convolutional layer: 8 input channels, 16 output channels, 3x3 kernel, stride 1, padding 1\n\n self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n\n # Fully connected layer: 16*7*7 input features (after two 2x2 poolings), 10 output features (num_classes)\n\n self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n\n\n\n def forward(self, x):\n\n \"\"\"\n\n Define the forward pass of the neural network.\n\n\n\n Parameters:\n\n x: torch.Tensor\n\n The input tensor.\n\n\n\n Returns:\n\n torch.Tensor\n\n The output tensor after passing through the network.\n\n \"\"\"\n\n x = F.relu(self.conv1(x)) # Apply first convolution and ReLU activation\n\n x = self.pool(x) # Apply max pooling\n\n x = F.relu(self.conv2(x)) # Apply second convolution and ReLU activation\n\n x = self.pool(x) # Apply max pooling\n\n x = x.reshape(x.shape[0], -1) # Flatten the tensor\n\n x = self.fc1(x) # Apply fully connected layer\n\n return x\n\nPyTorch can run on both CPUs and GPUs. We’ll set up the device to use a GPU if available; otherwise, we’ll use a CPU.\n\nHyperparameters are configuration settings used to tune how the model is trained.\n\nWe’ll use the module to download and load the MNIST dataset. We'll also use to handle batching and shuffling.\n\nWe instantiate our neural network and move it to the device (GPU or CPU).\n\nWe’ll use cross-entropy loss for classification and the Adam optimizer to update the model’s weights.\n\nWe’ll loop through the dataset multiple times (epochs) and update the model’s weights based on the loss.\n\nWe’ll define a function to check the accuracy of our model on both the training and test datasets.\n\ndef check_accuracy(loader, model):\n\n \"\"\"\n\n Checks the accuracy of the model on the given dataset loader.\n\n\n\n Parameters:\n\n loader: DataLoader\n\n The DataLoader for the dataset to check accuracy on.\n\n model: nn.Module\n\n The neural network model.\n\n \"\"\"\n\n if loader.dataset.train:\n\n print(\"Checking accuracy on training data\")\n\n else:\n\n print(\"Checking accuracy on test data\")\n\n\n\n num_correct = 0\n\n num_samples = 0\n\n model.eval() # Set the model to evaluation mode\n\n\n\n with torch.no_grad(): # Disable gradient calculation\n\n for x, y in loader:\n\n x = x.to(device)\n\n y = y.to(device)\n\n\n\n # Forward pass: compute the model output\n\n scores = model(x)\n\n _, predictions = scores.max(1) # Get the index of the max log-probability\n\n num_correct += (predictions == y).sum() # Count correct predictions\n\n num_samples += predictions.size(0) # Count total samples\n\n\n\n # Calculate accuracy\n\n accuracy = float(num_correct) / float(num_samples) * 100\n\n print(f\"Got {num_correct}/{num_samples} with accuracy {accuracy:.2f}%\")\n\n \n\n model.train() # Set the model back to training mode\n\n\n\n# Final accuracy check on training and test sets\n\ncheck_accuracy(train_loader, model)\n\ncheck_accuracy(test_loader, model)\n\nCongratulations! You’ve built, trained, and evaluated a simple convolutional neural network (CNN) using PyTorch. This guide covered the essentials, from defining the model architecture to loading data, training the model, and evaluating its performance. CNNs are powerful tools for image recognition tasks, and PyTorch provides a flexible and powerful framework for developing them. Happy coding!"
    },
    {
        "link": "https://pytorch.org/tutorials",
        "document": ""
    },
    {
        "link": "https://pytorch.org/docs/stable/nn.html",
        "document": "These are the basic building blocks for graphs:\n\nApplies a 1D convolution over an input signal composed of several input planes. Applies a 2D convolution over an input signal composed of several input planes. Applies a 3D convolution over an input signal composed of several input planes. Applies a 1D transposed convolution operator over an input image composed of several input planes. Applies a 2D transposed convolution operator over an input image composed of several input planes. Applies a 3D transposed convolution operator over an input image composed of several input planes. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. Combines an array of sliding local blocks into a large containing tensor.\n\nApplies a 1D max pooling over an input signal composed of several input planes. Applies a 2D max pooling over an input signal composed of several input planes. Applies a 3D max pooling over an input signal composed of several input planes. Applies a 1D average pooling over an input signal composed of several input planes. Applies a 2D average pooling over an input signal composed of several input planes. Applies a 3D average pooling over an input signal composed of several input planes. Applies a 2D fractional max pooling over an input signal composed of several input planes. Applies a 3D fractional max pooling over an input signal composed of several input planes. Applies a 1D power-average pooling over an input signal composed of several input planes. Applies a 2D power-average pooling over an input signal composed of several input planes. Applies a 3D power-average pooling over an input signal composed of several input planes. Applies a 1D adaptive max pooling over an input signal composed of several input planes. Applies a 2D adaptive max pooling over an input signal composed of several input planes. Applies a 3D adaptive max pooling over an input signal composed of several input planes. Applies a 1D adaptive average pooling over an input signal composed of several input planes. Applies a 2D adaptive average pooling over an input signal composed of several input planes. Applies a 3D adaptive average pooling over an input signal composed of several input planes.\n\nPads the input tensor using the reflection of the input boundary. Pads the input tensor using the reflection of the input boundary. Pads the input tensor using the reflection of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with a constant value. Pads the input tensor boundaries with a constant value. Pads the input tensor boundaries with a constant value. Pads the input tensor using circular padding of the input boundary. Pads the input tensor using circular padding of the input boundary. Pads the input tensor using circular padding of the input boundary.\n\nCreates a criterion that measures the mean absolute error (MAE) between each element in the input x and target y. Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input x and target y. This criterion computes the cross entropy loss between input logits and target. Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities: This loss combines a layer and the in one single class. Creates a criterion that measures the loss given inputs x1, x2, two 1D mini-batch or 0D , and a label 1D mini-batch or 0D y (containing 1 or -1). Measures the loss given an input tensor x and a labels tensor y (containing 1 or -1). Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input x (a 2D mini-batch ) and output y (which is a 2D of target class indices). Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. Creates a criterion that optimizes a two-class classification logistic loss between input tensor x and target tensor y (containing 1 or -1). Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input x and target y of size (N,C). Creates a criterion that measures the loss given input tensors x1​, x2​ and a label y with values 1 or -1. Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input x (a 2D mini-batch ) and output y (which is a 1D tensor of target class indices, 0≤y≤x.size(1)−1): Creates a criterion that measures the triplet loss given an input tensors x1, x2, x3 and a margin with a value greater than 0. Creates a criterion that measures the triplet loss given input tensors a, p, and n (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\"distance function\") used to compute the relationship between the anchor and positive example (\"positive distance\") and the anchor and negative example (\"negative distance\").\n\nClip the gradient norm of an iterable of parameters. Clip the gradient norm of an iterable of parameters. Clip the gradients of an iterable of parameters at specified value. Compute the norm of an iterable of tensors. Scale the gradients of an iterable of parameters given a pre-calculated total norm and desired max norm. Utility functions to flatten and unflatten Module parameters to and from a single vector. Flatten an iterable of parameters into a single vector. Copy slices of a vector into an iterable of parameters. Fuse a convolutional module and a BatchNorm module into a single, new convolutional module. Fuse convolutional module parameters and BatchNorm module parameters into new convolutional module parameters. Fuse a linear module and a BatchNorm module into a single, new linear module. Fuse linear module parameters and BatchNorm module parameters into new linear module parameters. Convert of to The conversion recursively applies to nested , including . Utility functions to apply and remove weight normalization from Module parameters. Apply weight normalization to a parameter in the given module. Apply spectral normalization to a parameter in the given module. Given a module class object and args / kwargs, instantiate the module without initializing parameters / buffers. Abstract base class for creation of new pruning techniques. Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. Prune (currently unpruned) units in a tensor at random. Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. Prune entire (currently unpruned) channels in a tensor at random. Prune entire (currently unpruned) channels in a tensor based on their L -norm. Prune tensor by removing units with the lowest L1-norm. Prune tensor by removing random channels along the specified dimension. Prune tensor by removing channels with the lowest L -norm along the specified dimension. Globally prunes tensors corresponding to all parameters in by applying the specified . Prune tensor corresponding to parameter called in by applying the pre-computed mask in . Remove the pruning reparameterization from a module and the pruning method from the forward hook. Check if a module is pruned by looking for pruning pre-hooks. Parametrizations implemented using the new parametrization functionality in . Apply an orthogonal or unitary parametrization to a matrix or a batch of matrices. Apply weight normalization to a parameter in the given module. Apply spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules. Note that these functions can be used to parametrize a given Parameter or Buffer given a specific function that maps from an input space to the parametrized space. They are not parameterizations that would transform an object into a parameter. See the Parametrizations tutorial for more information on how to implement your own parametrizations. Remove the parametrizations on a tensor in a module. Context manager that enables the caching system within parametrizations registered with . A sequential container that holds and manages the original parameters or buffers of a parametrized . Utility functions to call a given Module in a stateless manner. Perform a functional call on the module by replacing the module parameters and buffers with the provided ones. Holds the data and list of of a packed sequence."
    },
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html",
        "document": "Applies a 2D convolution over an input signal composed of several input planes.\n\nIn the simplest case, the output value of the layer with input size (N,Cin​,H,W) and output (N,Cout​,Hout​,Wout​) can be precisely described as:\n\nwhere ⋆ is the valid 2D cross-correlation operator, N is a batch size, C denotes a number of channels, H is a height of input planes in pixels, and W is width in pixels.\n\nOn certain ROCm devices, when using float16 inputs this module will use different precision for backward.\n• None controls the stride for the cross-correlation, a single number or a tuple.\n• None controls the amount of padding applied to the input. It can be either a string {‘valid’, ‘same’} or an int / a tuple of ints giving the amount of implicit padding applied on both sides.\n• None controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this link has a nice visualization of what does.\n• None controls the connections between inputs and outputs. and must both be divisible by . For example,\n• None At groups=1, all inputs are convolved to all outputs.\n• None At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.\n• None At groups= , each input channel is convolved with its own set of filters (of size in_channelsout_channels​).\n\nThe parameters , , , can either be:\n\nWhen and , where is a positive integer, this operation is also known as a “depthwise convolution”. In other words, for an input of size (N,Cin​,Lin​), a depthwise convolution with a depthwise multiplier can be performed with the arguments (Cin​=Cin​,Cout​=Cin​×K,...,groups=Cin​).\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting . See Reproducibility for more information."
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html",
        "document": "Click here to download the full example code\n\nNeural networks can be constructed using the package.\n\nNow that you had a glimpse of , depends on to define models and differentiate them. An contains layers, and a method that returns the .\n\nFor example, look at this network that classifies digit images:\n\nIt is a simple feed-forward network. It takes the input, feeds it through several layers one after the other, and then finally gives the output.\n\nA typical training procedure for a neural network is as follows:\n• None Define the neural network that has some learnable parameters (or weights)\n• None Compute the loss (how far is the output from being correct)\n• None Update the weights of the network, typically using a simple update rule:\n\nA loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target. There are several different loss functions under the nn package . A simple loss is: which computes the mean-squared error between the output and the target. # make it the same shape as output Now, if you follow in the backward direction, using its attribute, you will see a graph of computations that looks like this: So, when we call , the whole graph is differentiated w.r.t. the neural net parameters, and all Tensors in the graph that have will have their Tensor accumulated with the gradient. For illustration, let us follow a few steps backward: <MseLossBackward0 object at 0x7fa1139a9d80> <AddmmBackward0 object at 0x7fa1139a8b50> <AccumulateGrad object at 0x7fa1139ab2b0>\n\nTo backpropagate the error all we have to do is to . You need to clear the existing gradients though, else gradients will be accumulated to existing gradients. Now we shall call , and have a look at conv1’s bias gradients before and after the backward. # zeroes the gradient buffers of all parameters conv1.bias.grad before backward None conv1.bias.grad after backward tensor([ 0.0081, -0.0080, -0.0039, 0.0150, 0.0003, -0.0105]) Now, we have seen how to use loss functions. The neural network package contains various modules and loss functions that form the building blocks of deep neural networks. A full list with documentation is here. The only thing left to learn is:\n• None Updating the weights of the network"
    },
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html",
        "document": "Applies a 2D max pooling over an input signal composed of several input planes.\n\nIn the simplest case, the output value of the layer with input size (N,C,H,W), output (N,C,Hout​,Wout​) and (kH,kW) can be precisely described as:\n\nIf is non-zero, then the input is implicitly padded with negative infinity on both sides for number of points. controls the spacing between the kernel points. It is harder to describe, but this link has a nice visualization of what does.\n\nWhen ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.\n\nThe parameters , , , can either be:"
    },
    {
        "link": "https://pytorch.org/docs/stable/nn.functional.html",
        "document": "Applies a 1D average pooling over an input signal composed of several input planes. Applies a 1D max pooling over an input signal composed of several input planes. Applies a 2D max pooling over an input signal composed of several input planes. Applies a 3D max pooling over an input signal composed of several input planes. Apply a 1D power-average pooling over an input signal composed of several input planes. Apply a 2D power-average pooling over an input signal composed of several input planes. Apply a 3D power-average pooling over an input signal composed of several input planes. Applies a 1D adaptive max pooling over an input signal composed of several input planes. Applies a 2D adaptive max pooling over an input signal composed of several input planes. Applies a 3D adaptive max pooling over an input signal composed of several input planes. Applies a 1D adaptive average pooling over an input signal composed of several input planes. Apply a 2D adaptive average pooling over an input signal composed of several input planes. Apply a 3D adaptive average pooling over an input signal composed of several input planes. Applies 2D fractional max pooling over an input signal composed of several input planes. Applies 3D fractional max pooling over an input signal composed of several input planes.\n\nApply a threshold to each element of the input Tensor. Applies element-wise the function PReLU(x)=max(0,x)+weight∗min(0,x) where weight is a learnable parameter. When the approximate argument is 'none', it applies element-wise the function GELU(x)=x∗Φ(x) Sample from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally discretize. Apply Batch Normalization for each channel across a batch of data. Apply Group Normalization for last certain number of dimensions. Apply Instance Normalization independently for each channel in every data sample within a batch. Apply Layer Normalization for last certain number of dimensions. Perform Lp​ normalization of inputs over specified dimension.\n\nDuring training, randomly zeroes some elements of the input tensor with probability . Randomly zero out entire channels (a channel is a 1D feature map). Randomly zero out entire channels (a channel is a 2D feature map). Randomly zero out entire channels (a channel is a 3D feature map).\n\nGenerate a simple lookup table that looks up embeddings in a fixed dictionary and size. Compute sums, means or maxes of of embeddings. Takes LongTensor with index values of shape and returns a tensor of shape that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.\n\nMeasure Binary Cross Entropy between the target and input probabilities. Compute the cross entropy loss between input logits and target. Function that takes the mean element-wise absolute value difference. Measures the element-wise mean squared error, with optional weighting. Compute the triplet loss between given input tensors and a margin greater than 0. Compute the triplet margin loss for input tensors using a custom distance function.\n\nRearranges elements in a tensor of shape (∗,C×r2,H,W) to a tensor of shape (∗,C,H×r,W×r), where r is the . Reverses the operation by rearranging elements in a tensor of shape (∗,C,H×r,W×r) to a tensor of shape (∗,C×r2,H,W), where r is the . Generate 2D or 3D flow field (sampling grid), given a batch of affine matrices ."
    }
]