[
    {
        "link": "https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html",
        "document": "In the constructor we instantiate four parameters and assign them as In the forward function we accept a Tensor of input data and we must return a Tensor of output data. We can use Modules defined in the constructor as well as arbitrary operators on Tensors. Just like any class in Python, you can also define custom method on PyTorch modules # Construct our model by instantiating the class defined above # Construct our loss function and an Optimizer. The call to model.parameters() # in the SGD constructor will contain the learnable parameters (defined # with torch.nn.Parameter) which are members of the model. # Forward pass: Compute predicted y by passing x to the model # Zero gradients, perform a backward pass, and update the weights."
    },
    {
        "link": "https://pytorch.org/docs/stable/notes/modules.html",
        "document": "PyTorch uses modules to represent neural networks. Modules are:\n• None Building blocks of stateful computation. PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for easy construction of elaborate, multi-layer neural networks.\n• None Tightly integrated with PyTorch’s autograd system. Modules make it simple to specify learnable parameters for PyTorch’s Optimizers to update.\n• None Easy to work with and transform. Modules are straightforward to save and restore, transfer between CPU / GPU / TPU devices, prune, quantize, and more.\n\nThis note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch, many topics in this note are elaborated on in other notes or tutorials, and links to many of those documents are provided here as well.\n\nTo get started, let’s look at a simpler, custom version of PyTorch’s module. This module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules:\n• None It inherits from the base Module class. All modules should subclass for composability with other modules.\n• None It defines some “state” that is used in computation. Here, the state consists of randomly-initialized and tensors that define the affine transformation. Because each of these is defined as a , they are registered for the module and will automatically be tracked and returned from calls to . Parameters can be considered the “learnable” aspects of the module’s computation (more on this later). Note that modules are not required to have state, and can also be stateless.\n• None It defines a forward() function that performs the computation. For this affine transformation module, the input is matrix-multiplied with the parameter (using the short-hand notation) and added to the parameter to produce the output. More generally, the implementation for a module can perform arbitrary computation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be constructed and called: Note that the module itself is callable, and that calling it invokes its function. This name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module. The “forward pass” is responsible for applying the computation represented by the module to the given input(s) (as shown in the above snippet). The “backward pass” computes gradients of module outputs with respect to its inputs, which can be used for “training” parameters through gradient descent methods. PyTorch’s autograd system automatically takes care of this backward pass computation, so it is not required to manually implement a function for each module. The process of training module parameters through successive forward / backward passes is covered in detail in Neural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call to or , where the latter includes each parameter’s name: In general, the parameters registered by a module are aspects of the module’s computation that should be “learned”. A later section of this note shows how to update these parameters using one of PyTorch’s Optimizers. Before we get to that, however, let’s first examine how modules can be composed with one another.\n\nModules can contain other modules, making them useful building blocks for developing more elaborate functionality. The simplest way to do this is using the module. It allows us to chain together multiple modules: Note that automatically feeds the output of the first module as input into the , and the output of that as input into the second module. As shown, it is limited to in-order chaining of modules with a single input and output. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives full flexibility on how submodules are used for a module’s computation. For example, here’s a simple neural network implemented as a custom module: This module is composed of two “children” or “submodules” ( and ) that define the layers of the neural network and are utilized for computation within the module’s method. Immediate children of a module can be iterated through via a call to or : To go deeper than just the immediate children, and recursively iterate through a module and its child modules: Sometimes, it’s necessary for a module to dynamically define submodules. The and modules are useful here; they register submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules. This means that calls to and will recursively include child parameters, allowing for convenient optimization of all parameters within the network: It’s also easy to move all parameters to a different device or change their precision using : More generally, an arbitrary function can be applied to a module and its submodules recursively by using the function. For example, to apply custom initialization to parameters of a module and its submodules: # Note that no_grad() is used here to avoid tracking this computation in the autograd graph. # Apply the function recursively on the module and its submodules. These examples show how elaborate neural networks can be formed through module composition and conveniently manipulated. To allow for quick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of performant modules within the namespace that perform common neural network operations like pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out:\n\nOnce a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch’s Optimizers from : # Create the network (from previous section) and optimizer # to output the constant zero function # After training, switch the module to eval mode to do inference, compute performance metrics, etc. # (see discussion below for a description of training and evaluation modes) In this simplified example, the network learns to simply output zero, as any non-zero output is “penalized” according to its absolute value by employing as a loss function. While this is not a very interesting task, the key parts of training are present:\n• None An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network’s parameters are associated with it.\n• None\n• None calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network’s parameters have changed. In particular, examining the value of ‘s parameter shows that its values are now much closer to 0 (as may be expected): Note that the above process is done entirely while the network module is in “training mode”. Modules default to training mode and can be switched between training and evaluation modes using and . They can behave differently depending on which mode they are in. For example, the module maintains a running mean and variance during training that are not updated when the module is in evaluation mode. In general, modules should be in training mode during training and only switched to evaluation mode for inference or evaluation. Below is an example of a custom module that behaves differently between the two modes: Training neural networks can often be tricky. For more information, check out:\n\nIn the previous section, we demonstrated training a module’s “parameters”, or learnable aspects of computation. Now, if we want to save the trained model to disk, we can do so by saving its (i.e. “state dictionary”): # Load the module later on A module’s contains state that affects its computation. This includes, but is not limited to, the module’s parameters. For some modules, it may be useful to have state beyond parameters that affects module computation but is not learnable. For such cases, PyTorch provides the concept of “buffers”, both “persistent” and “non-persistent”. Following is an overview of the various types of state a module can have:\n• None Parameters: learnable aspects of computation; contained within the\n• \n• None Persistent buffers: contained within the (i.e. serialized when saving & loading)\n• None Non-persistent buffers: not contained within the (i.e. left out of serialization) As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want the current value of the running mean to be considered part of the module’s so that it will be restored when loading a serialized form of the module, but we don’t want it to be learnable. This snippet shows how to use to accomplish this: Now, the current value of the running mean is considered part of the module’s and will be properly restored when loading the module from disk: # Serialized form will contain the 'mean' tensor As mentioned previously, buffers can be left out of the module’s by marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied with : # Moves all module parameters and buffers to the specified device / dtype Buffers of a module can be iterated over using or . The following class demonstrates the various ways of registering parameters and buffers within a module: # Setting a nn.Parameter as an attribute of the module automatically registers the tensor # as a parameter of the module. # Reserves the \"param3\" attribute as a parameter, preventing it from being set to anything # except a parameter. \"None\" entries like this will not be present in the module's state_dict. # Registers a persistent buffer (one that appears in the module's state_dict). # Registers a non-persistent buffer (one that does not appear in the module's state_dict). # Reserves the \"buffer3\" attribute as a buffer, preventing it from being set to anything # except a buffer. \"None\" entries like this will not be present in the module's state_dict. # Adding a submodule registers its parameters as parameters of the module. # Note that non-persistent buffer \"buffer2\" and reserved attributes \"param3\" and \"buffer3\" do # not appear in the state_dict. For more information, check out:\n\nBy default, parameters and floating-point buffers for modules provided by are initialized during module instantiation as 32-bit floating point values on the CPU using an initialization scheme determined to perform well historically for the module type. For certain use cases, it may be desired to initialize with a different dtype, device (e.g. GPU), or initialization technique. Note that the device and dtype options demonstrated above also apply to any floating-point buffers registered for the module: While module writers can use any device or dtype to initialize parameters in their custom modules, good practice is to use and by default as well. Optionally, you can provide full flexibility in these areas for your custom module by conforming to the convention demonstrated above that all modules follow:\n• None Provide a constructor kwarg that applies to any parameters / buffers registered by the module.\n• None Provide a constructor kwarg that applies to any parameters / floating-point buffers registered by the module.\n• None Only use initialization functions (i.e. functions from ) on parameters and buffers within the module’s constructor. Note that this is only required to use ; see this page for an explanation. For more information, check out:\n\nIn Neural Network Training with Modules, we demonstrated the training process for a module, which iteratively performs forward and backward passes, updating module parameters each iteration. For more control over this process, PyTorch provides “hooks” that can perform arbitrary computation during a forward or backward pass, even modifying how the pass is done if desired. Some useful examples for this functionality include debugging, visualizing activations, examining gradients in-depth, etc. Hooks can be added to modules you haven’t written yourself, meaning this functionality can be applied to third-party or PyTorch-provided modules. PyTorch provides two types of hooks for modules:\n• None Forward hooks are called during the forward pass. They can be installed for a given module with and . These hooks will be called respectively just before the forward function is called and just after it is called. Alternatively, these hooks can be installed globally for all modules with the analogous and functions.\n• None Backward hooks are called during the backward pass. They can be installed with and . These hooks will be called when the backward for this Module has been computed. will allow the user to access the gradients for outputs while will allow the user to access the gradients both the inputs and outputs. Alternatively, they can be installed globally for all modules with and . All hooks allow the user to return an updated value that will be used throughout the remaining computation. Thus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or modify some inputs/outputs without having to change the module’s function. Below is an example demonstrating usage of forward and backward hooks: # Allows for examination and modification of the input before the forward pass. # Note that inputs are always wrapped in a tuple. # Allows for examination of inputs / outputs and modification of the outputs # after the forward pass. Note that inputs are always wrapped in a tuple while outputs # Allows for examination of grad_inputs / grad_outputs and modification of # grad_inputs used in the rest of the backwards pass. Note that grad_inputs and # grad_outputs are always wrapped in tuples. # Run input through module before and after adding hooks. # Note that the modified input results in a different output. # Remove hooks; note that the output here matches the output before adding hooks."
    },
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html",
        "document": "Your models should also subclass this class.\n\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes:\n\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call , etc.\n\nThe hook will be called every time before is invoked. If is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won’t be passed to the hooks and only to the . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature: If is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature:\n• None hook (Callable) – The user defined hook to be registered.\n• None prepend (bool) – If true, the provided will be fired before all existing hooks on this . Otherwise, the provided will be fired after all existing hooks on this . Note that global hooks registered with will fire before all hooks registered by this method. Default:\n• None with_kwargs (bool) – If true, the will be passed the kwargs given to the forward function. Default: a handle that can be used to remove the added hook by calling\n\nThe hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature: The and are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of in subsequent computations. will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in and will be for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module’s forward function. Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.\n• None hook (Callable) – The user-defined hook to be registered.\n• None prepend (bool) – If true, the provided will be fired before all existing hooks on this . Otherwise, the provided will be fired after all existing hooks on this . Note that global hooks registered with will fire before all hooks registered by this method. a handle that can be used to remove the added hook by calling\n\nThe hook will be called every time the gradients for the module are computed. The hook should have the following signature: The is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of in subsequent computations. Entries in will be for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module’s forward function. Modifying inputs inplace is not allowed when using backward hooks and will raise an error.\n• None hook (Callable) – The user-defined hook to be registered.\n• None prepend (bool) – If true, the provided will be fired before all existing hooks on this . Otherwise, the provided will be fired after all existing hooks on this . Note that global hooks registered with will fire before all hooks registered by this method. a handle that can be used to remove the added hook by calling"
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html",
        "document": "Neural networks comprise of layers/modules that perform operations on data. The torch.nn namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the nn.Module. A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.\n\nIn the following sections, we’ll build a neural network to classify images in the FashionMNIST dataset.\n\nWe want to be able to train our model on an accelerator such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU.\n\nMany layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. Subclassing automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s or methods. In this example, we iterate over each parameter, and print its size and a preview of its values."
    },
    {
        "link": "https://geeksforgeeks.org/create-model-using-custom-module-in-pytorch",
        "document": "A custom module in PyTorch is a user-defined module that is built using the PyTorch library’s built-in neural network module, torch.nn.Module. It’s a way of creating new modules by combining and extending the functionality provided by existing PyTorch modules.\n\nThe torch.nn.Module class provides a convenient way to create custom modules because it includes some key features that are important for building neural networks, such as the ability to keep track of learnable parameters and the ability to perform automatic differentiation (for computing gradients during training).\n\nBy creating a new class that inherits from torch.nn.Module, and defining an __init__ method to initialize the module’s parameters, and forward method that perform the computation, we can create our own custom module. These custom modules can be used just like any of the built-in PyTorch modules, such as torch.nn.Module or torch.nn.Conv2d, and can be included in a larger model architecture.\n\nCreating a custom module can be useful in many situations. For example, we might create a custom module to implement a novel layer or activation function that is not included in PyTorch’s built-in modules. Or we could create a custom module that represents a more complex model, such as a sequence-to-sequence model, composed of multiple layers and other modules.\n\nWhen creating a custom data model using a custom module in PyTorch, we will need to define a subclass of the torch.nn.Module class and define the __init__() and forward() methods.\n• __init__(): The __init__ method is used to initialize the module’s parameters. This method is called when the module is created, and it allows we to set up any internal state that the module needs. For example, we might use this method to initialize the weights of a neural network or to create other modules that the module needs in order to function.\n• forward(): The forward method is used to perform the computation that the module represents. This method takes in one or more input tensors, performs computations on them, and returns the output tensors. It is a forward pass of the module.\n\nOnce defined the custom module, we can create an instance of the module and use it to train a model by defining the loss function and optimizer, and then iterating through the training data to perform the forward and backward passes and optimize the model parameters.\n\nBefore going forward with creating a custom module in Pytorch, we have to install the torch library using the following command:\n\nHere is a step-by-step example of creating a custom module in PyTorch and training it on a dataset from torchvision.datasets:\n\nIn this step, we define a custom module called MyModule by creating a new class that inherits from the nn.Module base class. In the __init__ method, define the architecture of the model by creating the necessary layers. Here, we create two linear layers, one with num_inputs and hidden_size, and the other one with hidden_size and num_outputs.\n\nDefining the forward pass: Here we define the forward pass of the model within the class by implementing the forward method. In this example, the input is passed through the first linear layer, then a relu activation function is applied to it, and then it is passed through the second linear layer.\n\nStep 4: Define the transformations for the dataset\n\nIn this step, we load the MNIST dataset with pytorch dataset from torchvision.datasets. This will download the datasets and save them in the data folder. Here we use transform to transform the dataset into pytorch tensor.\n\nThis will convert the dataset into batch size of 64.\n\nOnce we have trained our custom model on a dataset, we can use it to make predictions and evaluate its performance using a classification report. A classification report is a summary of the performance of a classification model and includes several metrics such as precision, recall, f1-score, and support."
    },
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html",
        "document": "Apply a multi-layer long short-term memory (LSTM) RNN to an input sequence. For each element in the input sequence, each layer computes the following function:\n\n\\begin{array}{ll} \\\\ i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\ f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\ g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\ o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\ c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\ h_t = o_t \\odot \\tanh(c_t) \\\\ \\end{array}\n\nwhere ht​ is the hidden state at time , ct​ is the cell state at time , xt​ is the input at time , ht−1​ is the hidden state of the layer at time or the initial hidden state at time , and it​, ft​, gt​, ot​ are the input, forget, cell, and output gates, respectively. σ is the sigmoid function, and ⊙ is the Hadamard product.\n\nIn a multilayer LSTM, the input xt(l)​ of the l -th layer ( l≥2) is the hidden state ht(l−1)​ of the previous layer multiplied by dropout δt(l−1)​ where each δt(l−1)​ is a Bernoulli random variable which is 0 with probability .\n\nIf is specified, LSTM with projections will be used. This changes the LSTM cell in the following way. First, the dimension of ht​ will be changed from to (dimensions of Whi​ will be changed accordingly). Second, the output hidden state of each layer will be multiplied by a learnable projection matrix: ht​=Whr​ht​. Note that as a consequence of this, the output of LSTM network will be of different shape as well. See Inputs/Outputs sections below for exact dimensions of all variables. You can find more details in https://arxiv.org/abs/1402.1128.\n• None input: tensor of shape (L,Hin​) for unbatched input, (L,N,Hin​) when or (N,L,Hin​) when containing the features of the input sequence. The input can also be a packed variable length sequence. See or for details.\n• None h_0: tensor of shape (D∗num_layers,Hout​) for unbatched input or (D∗num_layers,N,Hout​) containing the initial hidden state for each element in the input sequence. Defaults to zeros if (h_0, c_0) is not provided.\n• None c_0: tensor of shape (D∗num_layers,Hcell​) for unbatched input or (D∗num_layers,N,Hcell​) containing the initial cell state for each element in the input sequence. Defaults to zeros if (h_0, c_0) is not provided. \\begin{aligned} N ={} & \\text{batch size} \\\\ L ={} & \\text{sequence length} \\\\ D ={} & 2 \\text{ if bidirectional=True otherwise } 1 \\\\ H_{in} ={} & \\text{input\\_size} \\\\ H_{cell} ={} & \\text{hidden\\_size} \\\\ H_{out} ={} & \\text{proj\\_size if } \\text{proj\\_size}>0 \\text{ otherwise hidden\\_size} \\\\ \\end{aligned}\n• None output: tensor of shape (L,D∗Hout​) for unbatched input, (L,N,D∗Hout​) when or (N,L,D∗Hout​) when containing the output features from the last layer of the LSTM, for each . If a has been given as the input, the output will also be a packed sequence. When , will contain a concatenation of the forward and reverse hidden states at each time step in the sequence.\n• None h_n: tensor of shape (D∗num_layers,Hout​) for unbatched input or (D∗num_layers,N,Hout​) containing the final hidden state for each element in the sequence. When , will contain a concatenation of the final forward and reverse hidden states, respectively.\n• None c_n: tensor of shape (D∗num_layers,Hcell​) for unbatched input or (D∗num_layers,N,Hcell​) containing the final cell state for each element in the sequence. When , will contain a concatenation of the final forward and reverse cell states, respectively.\n\nThere are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA. You can enforce deterministic behavior by setting the following environment variables: On CUDA 10.1, set environment variable . This may affect performance. On CUDA 10.2 or later, set environment variable (note the leading colon symbol) or . See the cuDNN 8 Release Notes for more information."
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html",
        "document": "Click here to download the full example code\n\nAt this point, we have seen various feed-forward networks. That is, there is no state maintained by the network at all. This might not be the behavior we want. Sequence models are central to NLP: they are models where there is some sort of dependence through time between your inputs. The classical example of a sequence model is the Hidden Markov Model for part-of-speech tagging. Another example is the conditional random field.\n\nA recurrent neural network is a network that maintains some kind of state. For example, its output could be used as part of the next input, so that information can propagate along as the network passes over the sequence. In the case of an LSTM, for each element in the sequence, there is a corresponding hidden state \\(h_t\\), which in principle can contain information from arbitrary points earlier in the sequence. We can use the hidden state to predict words in a language model, part-of-speech tags, and a myriad of other things.\n\nBefore getting to the example, note a few things. Pytorch’s LSTM expects all of its inputs to be 3D tensors. The semantics of the axes of these tensors is important. The first axis is the sequence itself, the second indexes instances in the mini-batch, and the third indexes elements of the input. We haven’t discussed mini-batching, so let’s just ignore that and assume we will always have just 1 dimension on the second axis. If we want to run the sequence model over the sentence “The cow jumped”, our input should look like Except remember there is an additional 2nd dimension with size 1. In addition, you could go through the sequence one at a time, in which case the 1st axis will have size 1 also. # Step through the sequence one element at a time. # after each step, hidden contains the hidden state. # alternatively, we can do the entire sequence all at once. # the first value returned by LSTM is all of the hidden states throughout # the sequence. the second is just the most recent hidden state # (compare the last slice of \"out\" with \"hidden\" below, they are the same) # The reason for this is that: # \"out\" will give you access to all hidden states in the sequence # \"hidden\" will allow you to continue the sequence and backpropagate, # by passing it as an argument to the lstm at a later time\n\nIn the example above, each word had an embedding, which served as the inputs to our sequence model. Let’s augment the word embeddings with a representation derived from the characters of the word. We expect that this should help significantly, since character-level information like affixes have a large bearing on part-of-speech. For example, words with the affix -ly are almost always tagged as adverbs in English. To do this, let \\(c_w\\) be the character-level representation of word \\(w\\). Let \\(x_w\\) be the word embedding as before. Then the input to our sequence model is the concatenation of \\(x_w\\) and \\(c_w\\). So if \\(x_w\\) has dimension 5, and \\(c_w\\) dimension 3, then our LSTM should accept an input of dimension 8. To get the character level representation, do an LSTM over the characters of a word, and let \\(c_w\\) be the final hidden state of this LSTM. Hints:\n• None There are going to be two LSTM’s in your new model. The original one that outputs POS tag scores, and the new one that outputs a character-level representation of each word.\n• None To do a sequence model over characters, you will have to embed characters. The character embeddings will be the input to the character LSTM."
    },
    {
        "link": "https://medium.com/towards-data-science/pytorch-lstms-for-time-series-data-cd16190929d7",
        "document": "You might have noticed that, despite the frequency with which we encounter sequential data in the real world, there isn’t a huge amount of content online showing how to build simple LSTMs from the ground up using the Pytorch functional API. Even the LSTM example on Pytorch’s official documentation only applies it to a natural language problem, which can be disorienting when trying to get these recurrent models working on time series data. In this article, we’ll set a solid foundation for constructing an end-to-end LSTM, from tensor input and output shapes to the LSTM itself.\n\nThis article is structured with the goal of being able to implement any univariate time-series LSTM. We begin by examining the shortcomings of traditional neural networks for these tasks, and why an LSTM’s input is differently shaped to simple neural nets. We’ll then intuitively describe the mechanics that allow an LSTM to “remember.” With this approximate understanding, we can implement a Pytorch LSTM using a traditional model class structure inheriting from , and write a forward method for it. We use this to see if we can get the LSTM to learn a simple sine wave. Finally, we attempt to write code to generalise how we might initialise an LSTM based on the problem at hand, and test it on our previous examples.\n\nLet’s suppose we have the following time-series data. Rather than using complicated recurrent models, we’re going to treat the time series as a simple input-output function: the input is the time, and the output is the value of whatever dependent variable we’re measuring. This is essentially just simplifying a univariate time series.\n\nLet’s suppose that we’re trying to model the number of minutes Klay Thompson will play in his return from injury. Steve Kerr, the coach of the Golden State Warriors, doesn’t want Klay to come back and immediately play heavy minutes. Instead, he will start Klay with a few minutes per game, and ramp up the amount of time he’s allowed to play as the season goes on. We’re going to be Klay Thompson’s physio, and we need to predict how many minutes per game Klay will be playing in order to determine how much strapping to put on his knee.\n\nThus, the number of games since returning from injury (representing the input time step) is the independent variable, and Klay Thompson’s number of minutes in the game is the dependent variable. Suppose we observe Klay for 11 games, recording his minutes per game in each outing to get the following data.\n\nHere, we’ve generated the minutes per game as a linear relationship with the number of games since returning. We’re going to use 9 samples for our training set, and 2 samples for validation.\n\nWe know that the relationship between game number and minutes is linear. However, we’re still going to use a non-linear activation function, because that’s the whole point of a neural network. (Otherwise, this would just turn into linear regression: the composition of linear operations is just a linear operation.) As per usual, we use to build our model with one hidden layer, with 13 hidden neurons.\n\nWe now need to write a training loop, as we always do when using gradient descent and backpropagation to force a network to learn. To remind you, each training step has several key tasks:\n• Compute the forward pass through the network by applying the model to the training examples.\n• Calculate the loss based on the defined loss function, which compares the model output to the actual training labels.\n• Backpropagate the derivative of the loss with respect to the model parameters through the network. This is done with call on the loss, after setting the current parameter gradients to zero with .\n• Update the model parameters by subtracting the gradient times the learning rate. This is done with our optimiser, using .\n\nNow, all we need to do is instantiate the required objects, including our model, our optimiser, our loss function and the number of epochs we’re going to train for.\n\nAs we can see, the model is likely overfitting significantly (which could be solved with many techniques, such as regularisation, or lowering the number of model parameters, or enforcing a linear model form). The training loss is essentially zero. Due to the inherent random variation in our dependent variable, the minutes played taper off into a flat curve towards the last few games, leading the model to believes that the relationship more resembles a log rather than a straight line.\n\nAlthough it wasn’t very successful, this initial neural network is a proof-of-concept that we can just develop sequential models out of nothing more than inputting all the time steps together. However, without more information about the past, and without the ability to store and recall this information, model performance on sequential data will be extremely limited.\n\nThe simplest neural networks make the assumption that the relationship between the input and output is independent of previous output states. It assumes that the function shape can be learnt from the input alone. In cases such as sequential data, this assumption is not true. The function value at any one particular time step can be thought of as directly influenced by the function value at past time steps. There is a temporal dependency between such values. Long-short term memory networks, or LSTMs, are a form of recurrent neural network that are excellent at learning such temporal dependencies.\n\nThe key to LSTMs is the cell state, which allows information to flow from one cell to another. This represents the LSTM’s memory, which can be updated, altered or forgotten over time. The components of the LSTM that do this updating are called gates, which regulate the information contained by the cell. Gates can be viewed as combinations of neural network layers and pointwise operations.\n\nIf you don’t already know how LSTMs work, the maths is straightforward and the fundamental LSTM equations are available in the Pytorch docs. There are many great resources online, such as this one. As a quick refresher, here are the four main steps each LSTM cell undertakes:\n• Decide what information to remove from the cell state that is no longer relevant. This is controlled by a neural network layer (with a sigmoid activation function) called the forget gate. We feed the output of the previous cell into the forget gate, which in turn outputs a number between 0 and 1 determining how much or little to forget.\n• Update the cell state with new information. An NN layer called the input gate takes the concatenation of the previous cell’s output and the current input and decides what to update. A tanh layer takes the same concatenation and creates a vector of new candidate values that could be added to the state.\n• Update the old cell state to create a new cell state. We multiply the old state by the value determined in Step 1, forgetting the things we decided to forget earlier. Then we add the new candidate values we found in Step 2. These constitute the new cell state, scaled by how much we decided to update each state value. This is finished for this cell; we can pass this directly to the next cell in the model.\n• Generate the model output based on the previous output and the current input. First, we take our updated cell state and pass it through an NN layer. We then find the output of the output/input vector passed through the sigmoid layer, and then pointwise compose it with the modified cell state. This allows the cell full control over composing the cell state and the current cell inputs, which gives us an appropriate output.\n\nNote that we give the output twice in the diagram above. One of these outputs is to be stored as a model prediction, for plotting etc. The other is passed to the next LSTM cell, much as the updated cell state is passed to the next LSTM cell.\n\nOur problem is to see if an LSTM can “learn” a sine wave. This is actually a relatively famous (read: infamous) example in the Pytorch community. It’s the only example on Pytorch’s Examples Github repository of an LSTM for a time-series problem. However, the example is old, and most people find that the code either doesn’t compile for them, or won’t converge to any sensible output. (A quick Google search gives a litany of Stack Overflow issues and questions just on this example.) Here, we’re going to break down and alter their code step by step.\n\nWe begin by generating a sample of 100 different sine waves, each with the same frequency and amplitude but beginning at slightly different points on the x-axis.\n\nLet’s walk through the code above. N is the number of samples; that is, we are generating 100 different sine waves. Many people intuitively trip up at this point. Since we are used to training a neural network on individual data points, such as the simple Klay Thompson example from above, it is tempting to think of N here as the number of points at which we measure the sine function. This is wrong; we are generating N different sine waves, each with a multitude of points. The LSTM network learns by examining not one sine wave, but many.\n\nNext, we instantiate an empty array x. Think of this array as a sample of points along the x-axis. The array has 100 rows (representing the 100 different sine waves), and each row is 1000 elements long (representing L, or the granularity of the sine wave i.e. the number of distinct sampled points in each wave). We then fill x by sampling the first 1000 integers points and then adding a random integer in a certain range governed by T, where is just syntax to add the integer along rows. Note that we must reshape this second random integer to shape (N, 1) in order for Numpy to be able to broadcast it to each row of x.\n\nFinally, we simply apply the Numpy sine function to x, and let broadcasting apply the function to each sample in each row, creating one sine wave per row. We cast it to type . We can pick any individual sine wave and plot it using Matplotlib. Let’s pick the first sampled sine wave at index 0.\n\nTo build the LSTM model, we actually only have one module being called for the LSTM cell specifically. First, we’ll present the entire model class (inheriting from , as always), and then walk through it piece by piece.\n\nThe key step in the initialisation is the declaration of a Pytorch . You can find the documentation here. The cell has three main parameters:\n• : the number of expected features in the input x.\n• : the number of features in the hidden state h.\n• : this defaults to true, and in general we leave it that way.\n\nKeep in mind that the parameters of the LSTM cell are different from the inputs. The parameters here largely govern the shape of the expected inputs, so that Pytorch can set up the appropriate structure. The inputs are the actual training examples or prediction examples we feed into the cell.\n\nWe define two LSTM layers using two LSTM cells. Much like a convolutional neural network, the key to setting up input and hidden sizes lies in the way the two layers connect to each other. For the first LSTM cell, we pass in an input of size 1. Recall why this is so: in an LSTM, we don’t need to pass in a sliced array of inputs. We don’t need a sliding window over the data, as the memory and forget gates take care of the cell state for us. We don’t need to specifically hand feed the model with old data each time, because of the model’s ability to recall this information. This is what makes LSTMs so special.\n\nWe then give this first LSTM cell a hidden size governed by the variable when we declare our class, . This number is rather arbitrary; here, we pick 64. As mentioned above, this becomes an output of sorts which we pass to the next LSTM cell, much like in a CNN: the output size of the last step becomes the input size of the next step. In this cell, we thus have an input of size , and also a hidden layer of size . We then pass this output of size to a linear layer, which itself outputs a scalar of size one. We are outputting a scalar, because we are simply trying to predict the function value y at that particular time step.\n\nIn the forward method, once the individual layers of the LSTM have been instantiated with the correct sizes, we can begin to focus on the actual inputs moving through the network. An LSTM cell takes the following inputs:\n• : a tensor of inputs of shape , where we declared in the creation of the LSTM cell.\n• : a tensor containing the initial hidden state for each element in the batch, of shape\n• : a tensor containing the initial cell state for each element in the batch, of shape .\n\nTo link the two LSTM cells (and the second LSTM cell with the linear, fully-connected layer), we also need to know what an LSTM cell actually outputs: a tensor of shape .\n• : a tensor containing the next hidden state for each element in the batch, of shape\n• : a tensor containing the next cell state for each element in the batch, of shape .\n\nHere, our batch size is 100, which is given by the first dimension of our input; hence, we take . Since we know the shapes of the hidden and cell states are both , we can instantiate a tensor of zeros of this size, and do so for both of our LSTM cells.\n\nThe next step is arguably the most difficult. We must feed in an appropriately shaped tensor. Here, that would be a tensor of m points, where m is our training size on each sequence. However, in the Pytorch method (documentation here), if the parameter is not passed in, it will simply split each tensor into chunks of size 1. We want to split this along each individual batch, so our dimension will be the rows, which is equivalent to dimension 1.\n\nIt’s always a good idea to check the output shape when we’re vectorising an array in this way. Suppose we choose three sine curves for the test set, and use the rest for training. We can check what our training input will look like in our split method:\n\nSo, for each sample, we’re passing in an array of 97 inputs, with an extra dimension to represent that it comes from a batch. (Pytorch usually operates in this way. Even if we’re passing in a single image to the world’s simplest CNN, Pytorch expects a batch of images, and so we have to use ) We then output a new hidden and cell state. As we know from above, the hidden state output is used as input to the next LSTM cell. The hidden state output from the second cell is then passed to the linear layer.\n\nGreat — we’ve completed our model predictions based on the actual points we have data for. But the whole point of an LSTM is to predict the future shape of the curve, based on past outputs. So, in the next stage of the forward pass, we’re going to predict the next time steps. Recall that in the previous loop, we calculated the output to append to our array by passing the second LSTM output through a linear layer. This variable is still in operation — we can access it and pass it to our model again. This is good news, as we can predict the next time step in the future, one time step after the last point we have data for. The model takes its prediction for this final data point as input, and predicts the next data point.\n\nWe then do this again, with the prediction now being fed as input to the model. In total, we do this number of times, to produce a curve of length , in addition to the 1000 predictions we’ve already made on the 1000 points we actually have data for.\n\nThe last thing we do is concatenate the array of scalar tensors representing our outputs, before returning them. That’s it! We’ve built an LSTM which takes in a certain number of inputs, and, one by one, predicts a certain number of time steps into the future.\n\nDefining a training loop in Pytorch is quite homogeneous across a variety of common applications. However, in our case, we can’t really gain an intuitive understanding of how the model is converging by examining the loss. Yes, a low loss is good, but there’s been plenty of times when I’ve gone to look at the model outputs after achieving a low loss and seen absolute garbage predictions. This is usually due to a mistake in my plotting code, or even more likely a mistake in my model declaration. Thus, the most useful tool we can apply to model assessment and debugging is plotting the model predictions at each training step to see if they improve.\n\nOur first step is to figure out the shape of our inputs and our targets. We know that our data has the shape . That is, 100 different sine curves of 1000 points each. Next, we want to figure out what our train-test split is. We’ll save 3 curves for the test set, and so indexing along the first dimension of we can use the last 97 curves for the training set.\n\nNow comes time to think about our model input. One at a time, we want to input the last time step and get a new time step prediction out. To do this, we input the first 999 samples from each sine wave, because inputting the last 1000 would lead to predicting the 1001st time step, which we can’t validate because we don’t have data on it. Similarly, for the training target, we use the first 97 sine waves, and start at the 2nd sample in each wave and use the last 999 samples from each wave; this is because we need a previous time step to actually input to the model — we can’t input . Hence, the starting index for the target in the second dimension (representing the samples in each wave) is 1. This gives us two arrays of shape .\n\nThe test input and test target follow very similar reasoning, except this time, we index only the first three sine waves along the first dimension. Everything else is exactly the same, as we would expect: apart from the batch input size (97 vs 3) we need to have the same input and outputs for train and test sets.\n\nWe now need to instantiate the main components of our training loop: the model itself, the loss function, and the optimiser. The model is simply an instance of our class, and the loss function we will use for what amounts to a regression problem is . The only thing different to normal here is our optimiser. Instead of Adam, we will use what is called a limited-memory BFGS algorithm, which essentially boils down to estimating an inverse of the Hessian matrix as a guide through the variable space. You don’t need to worry about the specifics, but you do need to worry about the difference between and other optimisers. We’ll cover that in the training loop below.\n\nFinally, we get around to constructing the training loop. Fair warning, as much as I’ll try to make this look like a typical Pytorch training loop, there will be some differences. These are mainly in the function we have to pass to the optimiser, , which represents the typical forward and backward pass through the network. We update the weights with by passing in this function. According to Pytorch, the function is a callable that reevaluates the model (forward pass), and returns the loss. So this is exactly what we do.\n\nThe training loop starts out much as other garden-variety training loops do. However, notice that the typical steps of forward and backwards pass are captured in the function closure. This is just an idiosyncrasy of how the optimiser function is designed in Pytorch. We return the loss in , and then pass this function to the optimiser during . And that’s pretty much it for the training step.\n\nNext, we want to plot some predictions, so we can sanity-check our results as we go. To do this, we need to take the test input, and pass it through the model. This is where our parameter we included in the model itself is going to come in handy. Recall that passing in some non-negative integer to the forward pass through the model will give us predictions after the last output from the actual samples. This allows us to see if the model generalises into future time steps. We then detach this output from the current computational graph and store it as a numpy array.\n\nFinally, we write some simple code to plot the model’s predictions on the test set at each epoch. There are only three test sine curves, so we only need to call our function three times (we’ll draw each curve in a different colour). The plotted lines indicate future predictions, and the solid lines indicate predictions in the current range of the data.\n\nThe predictions clearly improve over time, as well as the loss going down. Our model works: by the 8th epoch, the model has learnt the sine wave. However, if you keep training the model, you might see the predictions start to do something funny. This is because, at each time step, the LSTM relies on outputs from the previous time step. If the prediction changes slightly for the 1001st prediction, this will perturb the predictions all the way up to prediction 2000, resulting in a nonsensical curve. There are many ways to counter this, but they are beyond the scope of this article. The best strategy right now would be to watch the plots to see if this error accumulation starts happening. Then, you can either go back to an earlier epoch, or train past it and see what happens.\n\nIf you’re having trouble getting your LSTM to converge, here’s a few things you can try:\n• Lower the number of model parameters (maybe even down to 15) by changing the size of the hidden layer. This reduces the model search space.\n• Try downsampling from the first LSTM cell to the second by reducing the passed to the second cell. You could also do this from the second LSTM cell to the linear fully-connected layer.\n• Add batchnorm regularisation, which limits the size of the weights by placing penalties on larger weight values, giving the loss a smoother topography.\n• Add dropout, which zeros out a random fraction of neuronal outputs across the whole model at each epoch. This generates slightly different models each time, meaning the model is forced to rely on individual neurons less.\n\nIf you implement the last two strategies, remember to call to instantiate the regularisation during training, and turn off the regularisation during prediction and evaluation using .\n\nThis whole exercise is pointless if we still can’t apply an LSTM to other shapes of input. Let’s generate some new data, except this time, we’ll randomly generate the number of curves and the samples in each curve. We won’t know what the actual values of these parameters are, and so this is a perfect way to see if we can construct an LSTM based on the relationships between input and output shapes.\n\nWe could then change the following input and output shapes by determining the percentage of samples in each curve we’d like to use for the training set.\n\nThe input and output shapes thus become:\n\nYou can verify that this works by running these inputs and targets through the LSTM (hint: make sure you instantiate a variable for future based on the length of the input).\n\nLet’s see if we can apply this to the original Klay Thompson example. We need to generate more than one set of minutes if we’re going to feed it to our LSTM. That is, we’re going to generate 100 different hypothetical sets of minutes that Klay Thompson played in 100 different hypothetical worlds. We’ll feed 95 of these in for training, and plot three of the remaining five to see how our model is learning.\n\nAfter using the code above to reshape the inputs and outputs based on L and N, we run the model and achieve the following:\n\nThis gives us the following images (we only show the first and last):\n\nVery interesting! Initially, the LSTM also thinks the curve is logarithmic. Whilst it figures out that the curve is linear on the first 11 games after a bit of training, it insists on providing a logarithmic curve for future games. What is so fascinating about that is that the LSTM is right — Klay can’t keep linearly increasing his game time, as a basketball game only goes for 48 minutes, and most processes such as this are logarithmic anyway. Obviously, there’s no way that the LSTM could know this, but regardless, it’s interesting to see how the model ends up interpreting our toy data. A future task could be to play around with the hyperparameters of the LSTM to see if it is possible to make it learn a linear function for future time steps as well. Additionally, I like to create a Python class to store all these functions in one spot. Then, you can create an object with the data, and you can write functions which read the shape of the data, and feed it to the appropriate LSTM constructors.\n\nIn summary, creating an LSTM for univariate time series data in Pytorch doesn’t need to be overly complicated. However, the lack of available resources online (particularly resources that don’t focus on natural language forms of sequential data) make it difficult to learn how to construct such recurrent models. Hopefully, this article provided guidance on setting up your inputs and targets, writing a Pytorch class for the LSTM forward method, defining a training loop with the quirks of our new optimiser, and debugging using visual tools such as plotting.\n\nIf you would like to learn more about the maths behind the LSTM cell, I highly recommend this article which sets out the fundamental equations of LSTMs beautifully (I have no connection to the author). I also recommend attempting to adapt the above code to multivariate time-series. All the core ideas are the same — you just need to think about how you might expand the dimensionality of the input."
    },
    {
        "link": "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial",
        "document": "Click here to download the full example code\n\nThis tutorials is part of a three-part series:\n• None NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\n\nWe will be building and training a basic character-level Recurrent Neural Network (RNN) to classify words. This tutorial, along with two other Natural Language Processing (NLP) “from scratch” tutorials NLP From Scratch: Generating Names with a Character-Level RNN and NLP From Scratch: Translation with a Sequence to Sequence Network and Attention, show how to preprocess data to model NLP. In particular, these tutorials show how preprocessing to model NLP works at a low level.\n\nA character-level RNN reads words as a series of characters - outputting a prediction and “hidden state” at each step, feeding its previous hidden state into each next step. We take the final prediction to be the output, i.e. which class the word belongs to.\n\nSpecifically, we’ll train on a few thousand surnames from 18 languages of origin, and predict which language a name is from based on the spelling.\n\nDownload the data from here and extract it to the current directory. Included in the directory are 18 text files named as . Each file contains a bunch of names, one name per line, mostly romanized (but we still need to convert from Unicode to ASCII). The first step is to define and clean our data. Initially, we need to convert Unicode to plain ASCII to limit the RNN input layers. This is accomplished by converting Unicode strings to ASCII and allowing only a small set of allowed characters. # We can use \"_\" to represent an out-of-vocabulary character, that is, any character we are not handling in our model # Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427 Here’s an example of converting a unicode alphabet name to plain ASCII. This simplifies the input layer\n\nNow that we have all the names organized, we need to turn them into Tensors to make any use of them. To represent a single letter, we use a “one-hot vector” of size . A one-hot vector is filled with 0s except for a 1 at index of the current letter, e.g. . To make a word we join a bunch of those into a 2D matrix . That extra 1 dimension is because PyTorch assumes everything is in batches - we’re just using a batch size of 1 here. # return our out-of-vocabulary character if we encounter a letter unknown to our model # or an array of one-hot letter vectors Here are some examples of how to use for a single and multiple character string. #notice that the first position in the tensor = 1 The letter 'a' becomes tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], device='cuda:0') The name 'Ahn' becomes tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], device='cuda:0') Congratulations, you have built the foundational tensor objects for this learning task! You can use a similar approach for other RNN tasks with text. Next, we need to combine all our examples into a dataset so we can train, test and validate our models. For this, we will use the Dataset and DataLoader classes to hold our dataset. Each Dataset needs to implement three functions: , , and . #for provenance of the dataset #for provenance of the dataset #read all the ``.txt`` files in the specified directory #Cache the tensor representation of the labels Here we can load our example data into the Using the dataset object allows us to easily split the data into train and test sets. Here we create a 80/20 split but the has more useful utilities. Here we specify a generator since we need to use the same device as PyTorch defaults to above. Now we have a basic dataset containing 20074 examples where each example is a pairing of label and name. We have also split the dataset into training and testing so we can validate the model that we build.\n\nBefore autograd, creating a recurrent neural network in Torch involved cloning the parameters of a layer over several timesteps. The layers held hidden state and gradients which are now entirely handled by the graph itself. This means you can implement a RNN in a very “pure” way, as regular feed-forward layers. This CharRNN class implements an RNN with three components. First, we use the nn.RNN implementation. Next, we define a layer that maps the RNN hidden layers to our output. And finally, we apply a function. Using leads to a significant improvement in performance, such as cuDNN-accelerated kernels, versus implementing each layer as a . It also simplifies the implementation in . We can then create an RNN with 58 input nodes, 128 hidden nodes, and 18 outputs: After that we can pass our Tensor to the RNN to obtain a predicted output. Subsequently, we use a helper function, , to derive a text label for the class. #this is equivalent to ``output = rnn.forward(input)``\n\nNow all it takes to train this network is show it a bunch of examples, have it make guesses, and tell it if it’s wrong. We do this by defining a function which trains the model on a given dataset using minibatches. RNNs RNNs are trained similarly to other networks; therefore, for completeness, we include a batched training method here. The loop ( ) computes the losses for each of the items in the batch before adjusting the weights. This operation is repeated until the number of epochs is reached. Learn on a batch of training_data for a specified number of iterations and reporting thresholds # Keep track of losses for plotting # we cannot use dataloaders because each of our names is a different length #for each example in this batch We can now train a dataset with minibatches for a specified number of epochs. The number of epochs for this example is reduced to speed up the build. You can get better results with different parameters. training on data set with n = 17063 5 (19%): average batch loss = 0.8898524227412606 10 (37%): average batch loss = 0.689940884885173 15 (56%): average batch loss = 0.5758164712895265 20 (74%): average batch loss = 0.4935523093884359 25 (93%): average batch loss = 0.43487304784680736 training took 679.4782810211182s Plotting the historical loss from shows the network learning:\n\nTo see how well the network performs on different categories, we will create a confusion matrix, indicating for every actual language (rows) which language the network guesses (columns). To calculate the confusion matrix a bunch of samples are run through the network with , which is the same as minus the backprop. # do not record the gradients during eval phase # Normalize by dividing every row by its sum #numpy uses cpu here so we need to use a cpu version You can pick out bright spots off the main axis that show which languages it guesses incorrectly, e.g. Chinese for Korean, and Spanish for Italian. It seems to do very well with Greek, and very poorly with English (perhaps because of overlap with other languages)."
    },
    {
        "link": "https://python-engineer.com/posts/pytorch-rnn-lstm-gru",
        "document": "Implement a Recurrent Neural Net (RNN) in PyTorch! Learn how we can use the nn.RNN module and work with an input sequence.\n\nImplement a Recurrent Neural Net (RNN) in PyTorch! Learn how we can use the nn.RNN module and work with an input sequence. I also show you how easily we can switch to a gated recurrent unit (GRU) or long short-term memory (LSTM) RNN."
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html",
        "document": "Click here to download the full example code\n\nWord embeddings are dense vectors of real numbers, one per word in your vocabulary. In NLP, it is almost always the case that your features are words! But how should you represent a word in a computer? You could store its ascii character representation, but that only tells you what the word is, it doesn’t say much about what it means (you might be able to derive its part of speech from its affixes, or properties from its capitalization, but not much). Even more, in what sense could you combine these representations? We often want dense outputs from our neural networks, where the inputs are \\(|V|\\) dimensional, where \\(V\\) is our vocabulary, but often the outputs are only a few dimensional (if we are only predicting a handful of labels, for instance). How do we get from a massive dimensional space to a smaller dimensional space?\n\nHow about instead of ascii representations, we use a one-hot encoding? That is, we represent the word \\(w\\) by\n\nwhere the 1 is in a location unique to \\(w\\). Any other word will have a 1 in some other location, and a 0 everywhere else.\n\nThere is an enormous drawback to this representation, besides just how huge it is. It basically treats all words as independent entities with no relation to each other. What we really want is some notion of similarity between words. Why? Let’s see an example.\n\nSuppose we are building a language model. Suppose we have seen the sentences\n• None The mathematician ran to the store.\n• None The physicist ran to the store.\n\nin our training data. Now suppose we get a new sentence never before seen in our training data:\n\nOur language model might do OK on this sentence, but wouldn’t it be much better if we could use the following two facts:\n• None We have seen mathematician and physicist in the same role in a sentence. Somehow they have a semantic relation.\n• None We have seen mathematician in the same role in this new unseen sentence as we are now seeing physicist.\n\nand then infer that physicist is actually a good fit in the new unseen sentence? This is what we mean by a notion of similarity: we mean semantic similarity, not simply having similar orthographic representations. It is a technique to combat the sparsity of linguistic data, by connecting the dots between what we have seen and what we haven’t. This example of course relies on a fundamental linguistic assumption: that words appearing in similar contexts are related to each other semantically. This is called the distributional hypothesis.\n\nHow can we solve this problem? That is, how could we actually encode semantic similarity in words? Maybe we think up some semantic attributes. For example, we see that both mathematicians and physicists can run, so maybe we give these words a high score for the “is able to run” semantic attribute. Think of some other attributes, and imagine what you might score some common words on those attributes. If each attribute is a dimension, then we might give each word a vector, like this: Then we can get a measure of similarity between these words by doing: Although it is more common to normalize by the lengths: Where \\(\\phi\\) is the angle between the two vectors. That way, extremely similar words (words whose embeddings point in the same direction) will have similarity 1. Extremely dissimilar words should have similarity -1. You can think of the sparse one-hot vectors from the beginning of this section as a special case of these new vectors we have defined, where each word basically has similarity 0, and we gave each word some unique semantic attribute. These new vectors are dense, which is to say their entries are (typically) non-zero. But these new vectors are a big pain: you could think of thousands of different semantic attributes that might be relevant to determining similarity, and how on earth would you set the values of the different attributes? Central to the idea of deep learning is that the neural network learns representations of the features, rather than requiring the programmer to design them herself. So why not just let the word embeddings be parameters in our model, and then be updated during training? This is exactly what we will do. We will have some latent semantic attributes that the network can, in principle, learn. Note that the word embeddings will probably not be interpretable. That is, although with our hand-crafted vectors above we can see that mathematicians and physicists are similar in that they both like coffee, if we allow a neural network to learn the embeddings and see that both mathematicians and physicists have a large value in the second dimension, it is not clear what that means. They are similar in some latent semantic dimension, but this probably has no interpretation to us. In summary, word embeddings are a representation of the *semantics* of a word, efficiently encoding semantic information that might be relevant to the task at hand. You can embed other things too: part of speech tags, parse trees, anything! The idea of feature embeddings is central to the field.\n\nBefore we get to a worked example and an exercise, a few quick notes about how to use embeddings in Pytorch and in deep learning programming in general. Similar to how we defined a unique index for each word when making one-hot vectors, we also need to define an index for each word when using embeddings. These will be keys into a lookup table. That is, embeddings are stored as a \\(|V| \\times D\\) matrix, where \\(D\\) is the dimensionality of the embeddings, such that the word assigned index \\(i\\) has its embedding stored in the \\(i\\)’th row of the matrix. In all of my code, the mapping from words to indices is a dictionary named word_to_ix. The module that allows you to use embeddings is torch.nn.Embedding, which takes two arguments: the vocabulary size, and the dimensionality of the embeddings. To index into this table, you must use torch.LongTensor (since the indices are integers, not floats).\n\nThe Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep learning. It is a model that tries to predict words given the context of a few words before and a few words after the target word. This is distinct from language modeling, since CBOW is not sequential and does not have to be probabilistic. Typically, CBOW is used to quickly train word embeddings, and these embeddings are used to initialize the embeddings of some more complicated model. Usually, this is referred to as pretraining embeddings. It almost always helps performance a couple of percent. The CBOW model is as follows. Given a target word \\(w_i\\) and an \\(N\\) context window on each side, \\(w_{i-1}, \\dots, w_{i-N}\\) and \\(w_{i+1}, \\dots, w_{i+N}\\), referring to all context words collectively as \\(C\\), CBOW tries to minimize \\[-\\log p(w_i | C) = -\\log \\text{Softmax}\\left(A(\\sum_{w \\in C} q_w) + b\\right) \\] where \\(q_w\\) is the embedding for word \\(w\\). Implement this model in Pytorch by filling in the class below. Some tips:\n• None Think about which parameters you need to define.\n• None Make sure you know what shape each operation expects. Use .view() if you need to reshape. # 2 words to the left, 2 to the right \"\"\"We are about to study the idea of a computational process. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules we conjure the spirits of the computer with our spells.\"\"\" # By deriving a set from `raw_text`, we deduplicate the array # Create your model and train. Here are some functions to help you make # the data ready for use by your module. [(['are', 'We', 'to', 'study'], 'about'), (['about', 'are', 'study', 'the'], 'to'), (['to', 'about', 'the', 'idea'], 'study'), (['study', 'to', 'idea', 'of'], 'the'), (['the', 'study', 'of', 'a'], 'idea')] tensor([16, 22, 21, 23])"
    },
    {
        "link": "https://discuss.pytorch.org/t/how-does-nn-embedding-work/88518",
        "document": "I am new in the NLP field am I have some question about . I have already seen this post, but I’m still confusing with how generate the vector representation. From the official website and the answer in this post. I concluded:\n• It’s only a lookup table, given the index, it will return the corresponding vector.\n• The vector representation indicated the weighted matrix is initialized as random values and will be updated by backpropagation.\n• If I have 1000 words, using to make 30 dimension vectors of each word. Will generate one-hot vector of each word and create a hidden layer of 30 neuron like word2vec? If so, is it CBOW or Skip-Gram model? What’s the difference between and\n• Now, I am researching about the Visual-Question-Answering tasks. Any suggestion about using pretrain vector representation, e.g. word2vec, in question vocabulary or not using it?\n\nAn Embedding layer is essentially just a Linear layer. So you could define a your layer as , and represent each word as a one-hot vector, e.g., (the length of the vector is 1,000). As you can see, any word is a unique vector of size 1,000 with a 1 in a unique position, compared to all other words. Now giving such a vector with (cf. example vector above) to the Linear layer gives you simply the 2nd row of that layer. just simplifies this. Instead of giving it a big one-hot vector, you just give it an index. This index basically is the same as the position of the single 1 in the one-hot vector. Confusing about the dimension of Seq2Seq model\n\nIt seems you want to implement the CBOW setup of Word2Vec. You can easily find PyTorch implementations for that. For example, I found this implementation in 10 seconds :). This example uses so the of the method is a list of word indexes (the implementation doesn’t seem to use batches). But yes, instead of you could use . The only change needed would be that not has to be a list of one-hot vectors. But I wouldn’t bother, keeps things simpler.\n\nDoes PyTorch treat backpropagation of (1-hot input to linear layer) the same as (index selection of embedding)? I’m guessing that PyTorch will calculate the gradient for all entries of the linear layer and all but one will be zero given the 1 hot input. (ie: lots of computation for a large linear layer). Or does Pytorch optimize this out? Will Pytorch do the same for embedding or will PyTorch initialize and backpropagate only to the index embedding? Assuming limited GPU memory and large CPU memory. Do both share the same minimal (> zero) amount of data that can be sent to GPU?\n\nTransformers most often have as input the addition of something and a position embedding.\n\n For example, position 1 to 128 represented as .\n\n I never see to project a float position to embedding. Nor do I see the sparce flag set for the embedding. If they (Linear and Embedding) are essentially the same, I would assume some people would choose the linear projection (cleaner in my mind when the embedding is for position). In non AI, non backprogration, a lookup can be implemented much more efficiently than multiplying an array by a mask. Especially so if the table is large. I believe BERT usage of transformer use very large embedding (52K) to represent words in addition to embeddings for word position. Scavenged the GitHub repo for PyTorch and found Embedding.cpp in the call path of nn.Embedding. No idea of how this code does its magic, but embedding_dense_backward_cpu has a bunch of if statements before adding grad_weights while Linear.cpp does a multiplication. So I’m guessing embedding is much faster in backpropagation over linear especially when large embedding are used. If small embeddings, then essentially the same. Hoping someone who understands the PyTorch implementation to say for sure.\n\nI’m just trying to make the connection between nn.Embedding and nn.Linear. I think I understand what an embedding is: A representation of the input in a different vector space. Would you mind clarifying this point: Now giving such a vector with (cf. example vector above) to the Linear layer gives you simply the 2nd row of that layer. I think I’ve made embeddings before by training an LSTM autoencoder to reconstruct the input sequence from the final hidden state. I thought the embedding would then be the hidden state of the encoder. I’ve only ever done time-series related work with LSTMs, but am trying to learn their NLP applications and can’t find a word written on NLP that doesn’t include the use of an layer.\n\nWhen you time series analysis, your input is presumably already numerical, so the notion of an embedding is not an issues there. In NLP you typically deal with words, i.e., non-numerical input. So you have to encode your sentences, paragraphs, documents, etc. somehow for the model to “understand” them. The naive approach would be to One-Hot encoding where each word is a vector of the size of your vocabulary with only a single 1 at the index of the corresponding word. For example if the index of word “hello” has the index 42 in your vocabulary, the One-Hot encoded word vector for “hello” would look like: with the whole vector being long, and the 1 is at index 42. The problem is that for many NLP applications, your vocabulary can be very large, e.g., way beyond 100,000 words (but let’s stick with 100k). Now, if your input document has, say 100 words, then your input for your model is a . This is annoyingly large…and unnecessarily so, as we see in a bit… Your first layer getting this input is typically an Linear layer of, say, shape to reduce the dimensionality. If you do the calculations, you will notice that does nothing more than selecting the rows in E that correspond to the indices of the 1’s in M (because M is One-Hot encoded). If this is unclear, I recommend doing this with a toy example on paper. Knowing this, there’s no need for the One-Hot encoded matrix M anymore, we only need the indices of the words. is more or less just a linear layer to facilitate the but without the need of the large matrix M. This is why you don’t see any example in NLP without an explicit embedding layer (an exception might be character-based models where you vocabulary is small, e.g., < 100 characters)."
    },
    {
        "link": "https://geeksforgeeks.org/word-embedding-in-pytorch",
        "document": "Word Embedding is a powerful concept that helps in solving most of the natural language processing problems. As the machine doesn’t understand raw text, we need to transform that into numerical data to perform various operations. The most basic approach is to assign words/ letters a vector that is unique to them but this approach is not very useful as the words with similar meanings will get completely different vectors. Another more useful approach is training a model that can generate vectors of words. This is better than the previous approach because it will group similar words together and generate similar vectors for them. It also captures the overall meaning/ context of the words and sentences which is better than random assignment of vectors.\n\nEmbeddings are real-valued dense vectors (multi-dimensional arrays) that carry the meaning of the words. They can capture the context of the word/sentence in a document, semantic similarity, relation with other words/sentences, etc. A popular example of how they extract the contexts from the words is if you remove a man from the king and add a woman, it will output a vector similar to a queen. Also, similar words are close to each other in the embedding space. Many pre-trained models are available such as Word2Vec, GloVe, Bert, etc.\n\nAs defined in the official Pytorch Documentation, an Embedding layer is – “A simple lookup table that stores embeddings of a fixed dictionary and size.” So basically at the low level, the Embedding layer is just a lookup table that maps an index value to a weight matrix of some dimension. This weight matrix is further optimised during training (updated during backpropagation to reduce the loss) to produce more useful vectors.\n\nNow let’s look into the working of embedding in Pytorch. When an embedding layer is created, an embedding matrix is initialised with random vectors having dimensions of (num_embeddings, embedding_dim). This is basically our lookup table where our words are mapped to indexes.\n\nGiven an input word or token, represented by its index in the vocabulary, you pass this index to the embedding layer which then looks up the corresponding row in the embedding matrix. The embedding vector is then extracted from the row as output which is of the dimension embedding_dim.\n\nDuring training, the embedding vectors are updated through backpropagation to minimize the loss. This means the vectors are adjusted to better represent the semantics and relationships between words.\n\nEmbedding layer takes minimum of two arguments – num_embeddings and embedding_dim. There are various other optional parameters also such as padding_idx, max_norm, etc. Refer to the official docs for this. Now the first required parameter is num_embeddings which means what is the dictionary size. For example, if you have a vocabulary of 5000 words, then the value that will go into the first parameter will be 5000. The second required parameter is embedding_dim which means the size of each embedding vector(as all the learned vectors will have a fixed size).\n\nThere are two major techniques in embeddings known as Continuous Bag of Words (CBOW) and Skip gram. Let’s learn about them a little below-\n\nContinuous Bag of Words (CBOW)– BOW predicts a target word based on the surrounding context words. This means that, for it to predict the focus word (the word we are interested in), it checks the surrounding words around it. The contextual representation of surrounding words for the focus word, helps in clearly predicting the word. It takes a pre-defined fixed window size into account and tries to predict the target word.\n\nExample- Suppose we have a sentence – “I eat pizza every day”. So, if we have a context window of 2, the input will be [“I”, “pizza”] and the target will be “eat”.\n\nIn the above diagram, W(t-2) and W(t-1) represent the words before our focus word, i.e., [‘I’, ‘eat’]. And W(t+1), W(t+2) represent the words after focus word, i.e., [‘every’, ‘day’]. These four words are used to predict the focus word, that is, “pizza”. So when given the task to get embedding for each word in the vocabulary by following Continuous Bag of Words, the context_window is taken as 2, the (context_words, focus word) pairs would be:\n\nSkip-gram– Skip gram technique is similar to the continuous bag of words but the main difference is instead of predicting the target word from the context, this takes the target word as input and tries to predict the context (which is a set of words). Example- If we take the above sentence, the input will be “eat” and the target will be [“I”, “pizza”].\n\nHere, the context words for the focus word is predicted. As seen from the diagram above, for each input word the neighbouring context words will be predicted. To get multiple outputs from a single input, a SoftMax activation is used to assign probability for the context words. We will consider the same context window of 1 and see how the (context word, focus words) look like:\n\nBefore we dive into the modelling building, lets first understand how the Embedding layer syntax works in Pytorch. It is give as:\n• num_embeddings (int) is the size of the vocabulary\n• embedding_dim (int ) talks about size of each embedding vector\n• padding_idx(int, optional) is used to treat a specific index as padding token\n• max_norm (float, optional) limits the normalisation to a specific value, thus, avoiding exploding gradient.\n• norm_type (float, optional) specifies the type of normalisation (L1 or L2)\n• scale_grad_by_freq(bool, optional) scales the embeddings based on the frequency of the words in the vocab.\n• sparse (bool, optional) decides whether to use sparse gradient updates for getting embedding gradients, when it is set to True\n\nNow let’s look at some code to see how this works.\n\nThe above code basically created a lookup table named embedding which has 5 rows and 40 columns. Each row will represents a single word embedding initialized randomly between -1 and 1.\n\nYou can also see the shape of the vector printed at the end. It’s the same as we defined in our parameter. These vectors later gets optimised during training to make more meaningful vectors. Now let’s look at an example that’s present in the docs.\n\nFirstly, we’ll import all the required libraries. For this, we’ll be needing torch and the nn module from torch. Then we’ll set the manual seed to 1 to control the randomness.\n\nThen, we’ll create a dictionary which has the numerical mappings of the words and initialise the embedding layer by using nn.Embedding of shape (2,5).\n\nThen the next step is to convert the numerical mapping of the word (which we want to create embedding for) into tensor with the dtype long. We use LongTensors as they can be used to represent labels/categories. We can access the embeddings of the word “geeks” as shown below using the embeds.\n\nAs these are randomly initialised vectors so they are not of much significance. They can be optimised through training but that will require a lot of effort. To show case how other padding affect an embedding, let’s take a look at another example.\n\nWe will take a dummy sentence and using Counter, create a dictionary of words with keys as the words and their frequency as the value.\n\nAfter creating the dictionary and assigning a value for our embedding dimension, we are ready to initialise the embedding and see its output, but this time we are going to introduce padding index at position 4. What this will do is, pad the input at the specific index and assign a value of zero there.\n\nAs you can see, the embedding layer provided for the sentences contains padded values at index 3.\n\nOther than this, nn.Embedding layer is a key component in transformer architecture. In transformers, it is used to convert input tokens into continuous representations. In conclusion, the nn.Embedding layer is a fundamental component in many NLP models. Understanding this layer and how it works is an important step in building natural language processing models with pytorch effectively."
    },
    {
        "link": "https://medium.com/towards-data-science/the-secret-to-improved-nlp-an-in-depth-look-at-the-nn-embedding-layer-in-pytorch-6e901e193e16",
        "document": "The Secret to Improved NLP: An In-Depth Look at the nn.Embedding Layer in PyTorch\n\nYou might have seen the famous PyTorch nn.Embedding() layer in multiple neural network architectures that involves natural language processing (NLP). This is one of the simplest and most important layers when it comes to designing advanced NLP architectures. Let me explain what it is, in simple terms.\n\nAfter spending some time looking into its C++ source code, here is what I found. The nn.Embedding layer is a simple lookup table that maps an index value to a weight matrix of a certain dimension. This simple operation is the foundation of many advanced NLP architectures, allowing for the processing of discrete input symbols in a continuous space. During the training the parameters of the nn.Embedding layer in a neural network are adjusted in order to optimize the performance of the model. Specifically, the embedding matrix is updated via backpropagation to minimize the loss function. This can be thought of as learning a mapping from discrete input tokens (such as words) to continuous embedding vectors in a high-dimensional space, where the vectors are optimised to represent the meaning or context of the input tokens in relation to the task the…"
    },
    {
        "link": "https://medium.com/@bao.character/how-to-use-pytorchs-nn-embedding-a-comprehensive-guide-with-examples-da00ea42e952",
        "document": "In the world of natural language processing (NLP) and many other machine learning tasks, working with categorical data is a common challenge. Often, we need to convert categorical data, such as words in a vocabulary, into a format that can be processed by neural networks. This is where embeddings come into play, and PyTorch provides a powerful tool for this through the function. In this article, we'll delve into what is, why it's useful, and how to use it with clear examples.\n\nis a PyTorch layer that maps indices from a fixed vocabulary to dense vectors of fixed size, known as embeddings. This mapping is done through an embedding matrix, which is a learnable parameter of the model. The primary purpose of embeddings is to convert categorical data into continuous vectors that neural networks can process.\n• Dimensionality Reduction: Instead of representing words as one-hot encoded vectors, which can be extremely high-dimensional and sparse, embeddings provide a dense, low-dimensional representation.\n• Learnable Representations: The embedding vectors are learned during the training process, enabling the model to capture…"
    }
]