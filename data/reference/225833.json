[
    {
        "link": "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras",
        "document": "Unlike regression predictive modeling, time series also adds the complexity of a sequence dependence among the input variables.\n\nA powerful type of neural network designed to handle sequence dependence is called a recurrent neural network. The Long Short-Term Memory network or LSTM network is a type of recurrent neural network used in deep learning because very large architectures can be successfully trained.\n\nIn this post, you will discover how to develop LSTM networks in Python using the Keras deep learning library to address a demonstration time-series prediction problem.\n\nAfter completing this tutorial, you will know how to implement and develop LSTM networks for your own time series prediction problems and other more general sequence problems. You will know:\n• How to develop LSTM networks for regression, window, and time-step-based framing of time series prediction problems\n• How to develop and make predictions using LSTM networks that maintain state (memory) across very long sequences\n\nIn this tutorial, we will develop a number of LSTMs for a standard time series prediction problem. The problem and the chosen configuration for the LSTM networks are for demonstration purposes only; they are not optimized.\n\nThese examples will show exactly how you can develop your own differently structured LSTM networks for time series predictive modeling problems.\n\nKick-start your project with my new book Deep Learning for Time Series Forecasting, including step-by-step tutorials and the Python source code files for all examples.\n• Update Oct/2016: There was an error in how RMSE was calculated in each example. Reported RMSEs were just plain wrong. Now, RMSE is calculated directly from predictions, and both RMSE and graphs of predictions are in the units of the original dataset. Models were evaluated using Keras 1.1.0, TensorFlow 0.10.0, and scikit-learn v0.18. Thanks to all those that pointed out the issue and to Philip O’Brien for helping to point out the fix.\n• Update Mar/2017: Updated example for Keras 2.0.2, TensorFlow 1.0.1 and Theano 0.9.0\n• Update Apr/2017: For a more complete and better-explained tutorial of LSTMs for time series forecasting, see the post Time Series Forecasting with the Long Short-Term Memory Network in Python\n\nThe example in this post is quite dated. You can view some better examples using LSTMs on time series with:\n\nThe problem you will look at in this post is the International Airline Passengers prediction problem.\n\nThis is a problem where, given a year and a month, the task is to predict the number of international airline passengers in units of 1,000. The data ranges from January 1949 to December 1960, or 12 years, with 144 observations.\n\nBelow is a sample of the first few lines of the file.\n\nYou can load this dataset easily using the Pandas library. You are not interested in the date, given that each observation is separated by the same interval of one month. Therefore, when you load the dataset, you can exclude the first column.\n\nOnce loaded, you can easily plot the whole dataset. The code to load and plot the dataset is listed below.\n\nYou can see an upward trend in the dataset over time.\n\nYou can also see some periodicity in the dataset that probably corresponds to the Northern Hemisphere vacation period.\n\nLet’s keep things simple and work with the data as-is.\n\nNormally, it is a good idea to investigate various data preparation techniques to rescale the data and make it stationary.\n\nThe Long Short-Term Memory network, or LSTM network, is a recurrent neural network trained using Backpropagation Through Time that overcomes the vanishing gradient problem.\n\nAs such, it can be used to create large recurrent networks that, in turn, can be used to address difficult sequence problems in machine learning and achieve state-of-the-art results.\n\nInstead of neurons, LSTM networks have memory blocks connected through layers.\n\nA block has components that make it smarter than a classical neuron and a memory for recent sequences. A block contains gates that manage the block’s state and output. A block operates upon an input sequence, and each gate within a block uses the sigmoid activation units to control whether it is triggered or not, making the change of state and addition of information flowing through the block conditional.\n\nThere are three types of gates within a unit:\n• Forget Gate: conditionally decides what information to throw away from the block\n• Input Gate: conditionally decides which values from the input to update the memory state\n• Output Gate: conditionally decides what to output based on input and the memory of the block\n\nEach unit is like a mini-state machine where the gates of the units have weights that are learned during the training procedure.\n\nYou can see how you may achieve sophisticated learning and memory from a layer of LSTMs, and it is not hard to imagine how higher-order abstractions may be layered with multiple such layers.\n\nYou can phrase the problem as a regression problem.\n\nThat is, given the number of passengers (in units of thousands) this month, what is the number of passengers next month?\n\nYou can write a simple function to convert the single column of data into a two-column dataset: the first column containing this month’s (t) passenger count and the second column containing next month’s (t+1) passenger count to be predicted.\n\nBefore you start, let’s first import all the functions and classes you will use. This assumes a working SciPy environment with the Keras deep learning library installed.\n\nBefore you do anything, it is a good idea to fix the random number seed to ensure your results are reproducible.\n\nYou can also use the code from the previous section to load the dataset as a Pandas dataframe. You can then extract the NumPy array from the dataframe and convert the integer values to floating point values, which are more suitable for modeling with a neural network.\n\nLSTMs are sensitive to the scale of the input data, specifically when the sigmoid (default) or tanh activation functions are used. It can be a good practice to rescale the data to the range of 0-to-1, also called normalizing. You can easily normalize the dataset using the MinMaxScaler preprocessing class from the scikit-learn library.\n\nAfter you model the data and estimate the skill of your model on the training dataset, you need to get an idea of the skill of the model on new unseen data. For a normal classification or regression problem, you would do this using cross validation.\n\nWith time series data, the sequence of values is important. A simple method that you can use is to split the ordered dataset into train and test datasets. The code below calculates the index of the split point and separates the data into the training datasets, with 67% of the observations used to train the model, leaving the remaining 33% for testing the model.\n\nNow, you can define a function to create a new dataset, as described above.\n\nThe function takes two arguments: the dataset, which is a NumPy array you want to convert into a dataset, and the look_back, which is the number of previous time steps to use as input variables to predict the next time period—in this case, defaulted to 1.\n\nThis default will create a dataset where X is the number of passengers at a given time (t), and Y is the number of passengers at the next time (t + 1).\n\nIt can be configured by constructing a differently shaped dataset in the next section.\n\nLet’s take a look at the effect of this function on the first rows of the dataset (shown in the unnormalized form for clarity).\n\nIf you compare these first five rows to the original dataset sample listed in the previous section, you can see the X=t and Y=t+1 pattern in the numbers.\n\nLet’s use this function to prepare the train and test datasets for modeling.\n\nThe LSTM network expects the input data (X) to be provided with a specific array structure in the form of [samples, time steps, features].\n\nCurrently, the data is in the form of [samples, features], and you are framing the problem as one time step for each sample. You can transform the prepared train and test input data into the expected structure using numpy.reshape() as follows:\n\nYou are now ready to design and fit your LSTM network for this problem.\n\nThe network has a visible layer with 1 input, a hidden layer with 4 LSTM blocks or neurons, and an output layer that makes a single value prediction. The default sigmoid activation function is used for the LSTM blocks. The network is trained for 100 epochs, and a batch size of 1 is used.\n\nOnce the model is fit, you can estimate the performance of the model on the train and test datasets. This will give you a point of comparison for new models.\n\nNote that you will invert the predictions before calculating error scores to ensure that performance is reported in the same units as the original data (thousands of passengers per month).\n\nFinally, you can generate predictions using the model for both the train and test dataset to get a visual indication of the skill of the model.\n\nBecause of how the dataset was prepared, you must shift the predictions so that they align on the x-axis with the original dataset. Once prepared, the data is plotted, showing the original dataset in blue, the predictions for the training dataset in green, and the predictions on the unseen test dataset in red.\n\nYou can see that the model did an excellent job of fitting both the training and the test datasets.\n\nFor completeness, below is the entire code example.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example produces the following output.\n\nYou can see that the model has an average error of about 23 passengers (in thousands) on the training dataset and about 49 passengers (in thousands) on the test dataset. Not that bad.\n\nLSTM for Regression Using the Window Method\n\nYou can also phrase the problem so that multiple, recent time steps can be used to make the prediction for the next time step.\n\nThis is called a window, and the size of the window is a parameter that can be tuned for each problem.\n\nFor example, given the current time (t) to predict the value at the next time in the sequence (t+1), you can use the current time (t), as well as the two prior times (t-1 and t-2) as input variables.\n\nWhen phrased as a regression problem, the input variables are t-2, t-1, and t, and the output variable is t+1.\n\nThe create_dataset() function created in the previous section allows you to create this formulation of the time series problem by increasing the look_back argument from 1 to 3.\n\nA sample of the dataset with this formulation is as follows:\n\nYou can re-run the example in the previous section with the larger window size. The whole code listing with just the window size change is listed below for completeness.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example provides the following output:\n\nYou can see that the error was increased slightly compared to that of the previous section. The window size and the network architecture were not tuned: This is just a demonstration of how to frame a prediction problem.\n\nYou may have noticed that the data preparation for the LSTM network includes time steps.\n\nSome sequence problems may have a varied number of time steps per sample. For example, you may have measurements of a physical machine leading up to the point of failure or a point of surge. Each incident would be a sample of observations that lead up to the event, which would be the time steps, and the variables observed would be the features.\n\nTime steps provide another way to phrase your time series problem. Like above in the window example, you can take prior time steps in your time series as inputs to predict the output at the next time step.\n\nInstead of phrasing the past observations as separate input features, you can use them as time steps of the one input feature, which is indeed a more accurate framing of the problem.\n\nYou can do this using the same data representation as in the previous window-based example, except when you reshape the data, you set the columns to be the time steps dimension and change the features dimension back to 1. For example:\n\nThe entire code listing is provided below for completeness.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example provides the following output:\n\nYou can see that the results are slightly better than the previous example, although the structure of the input data makes a lot more sense.\n\nThe LSTM network has memory capable of remembering across long sequences.\n\nNormally, the state within the network is reset after each training batch when fitting the model, as well as each call to model.predict() or model.evaluate().\n\nYou can gain finer control over when the internal state of the LSTM network is cleared in Keras by making the LSTM layer “stateful.” This means it can build a state over the entire training sequence and even maintain that state if needed to make predictions.\n\nIt requires that the training data not be shuffled when fitting the network. It also requires explicit resetting of the network state after each exposure to the training data (epoch) by calls to model.reset_states(). This means that you must create your own outer loop of epochs and within each epoch call model.fit() and model.reset_states(). For example:\n\nFinally, when the LSTM layer is constructed, the stateful parameter must be set to True. Instead of specifying the input dimensions, you must hard code the number of samples in a batch, the number of time steps in a sample, and the number of features in a time step by setting the batch_input_shape parameter. For example:\n\nThis same batch size must then be used later when evaluating the model and making predictions. For example:\n\nYou can adapt the previous time step example to use a stateful LSTM. The full code listing is provided below.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example provides the following output:\n\nYou do see that results are better than some, worse than others. The model may need more modules and may need to be trained for more epochs to internalize the structure of the problem.\n\nFinally, let’s take a look at one of the big benefits of LSTMs: the fact that they can be successfully trained when stacked into deep network architectures.\n\nLSTM networks can be stacked in Keras in the same way that other layer types can be stacked. One addition to the configuration that is required is that an LSTM layer prior to each subsequent LSTM layer must return the sequence. This can be done by setting the return_sequences parameter on the layer to True.\n\nYou can extend the stateful LSTM in the previous section to have two layers, as follows:\n\nThe entire code listing is provided below for completeness.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example produces the following output.\n\nThe predictions on the test dataset are again worse. This is more evidence to suggest the need for additional training epochs.\n\nIn this post, you discovered how to develop LSTM recurrent neural networks for time series prediction in Python with the Keras deep learning network.\n• How to create an LSTM for a regression and a window formulation of the time series problem\n• How to create an LSTM with a time step formulation of the time series problem\n• How to create an LSTM with state and stacked LSTMs with state to learn long sequences\n\nDo you have any questions about LSTMs for time series prediction or about this post?\n\n Ask your questions in the comments below, and I will do my best to answer.\n\nThe example in this post is quite dated. See these better examples available for using LSTMs on time series:"
    },
    {
        "link": "https://kaggle.com/code/hassanamin/time-series-analysis-using-lstm-keras",
        "document": ""
    },
    {
        "link": "https://medium.com/@iqra1804/time-series-forecasting-using-lstm-an-introduction-with-code-explanations-c5c2e8ca137d",
        "document": "Time series forecasting plays an important role in stock market forecasting, finance, and weather prediction. Long Short-Term Memory (LSTM) is a popular deep learning model that has proven to be effective in capturing temporal dependencies and making accurate predictions. In this blog post, we will explore the basics of time series forecasting using LSTM neural network. We will also go through some code explanations to help you get started with implementing your own models.\n\nThe very first step in time series forecasting is understanding the problem. Time series data consists of a sequence of observations collected at regular intervals over time. It is essential to preprocess and analyze the data before building a forecasting model. Some important steps include:\n\na. Data Visualization: Plot the time series data to understand its patterns, trends, and seasonality.\n\nb. Data Preprocessing: Handle missing values, remove outliers, and normalize the data if necessary.\n\nc. Train-Test Split: Divide the data into training and testing sets. The test set should contain data from a future time period to evaluate the model’s performance accurately.\n\n2. LSTM for Time Series Forecasting:\n\nLong Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) designed to overcome the vanishing gradient problem and capture long-term dependencies. LSTM cells have memory units that can store and retrieve information over extended periods. Here’s an overview of the LSTM architecture:\n\nb. LSTM Layer: The LSTM layer consists of memory cells and gates that control the flow of information. It processes the sequential data and captures the temporal dependencies.\n\nc. Output Layer: The output layer produces the predicted values.\n\n3. Implementing LSTM Time Series Forecasting in Python:\n\nLet’s dive into the code and see how to implement LSTM for time series forecasting using the Keras library in Python.\n\n4. Interpretation and Evaluation:\n\nIn the code snippet above, we load and preprocess the time series data, split it into training and testing sets, and create input-output sequences. We then build an LSTM model using the Sequential API from Keras, compile it with an appropriate optimizer and loss function, and train it on the training data. Finally, we make predictions on the test set and evaluate the model’s performance using the mean squared error (MSE).\n\n5. Conclusion:\n\nTime series forecasting using LSTM is a powerful technique for predicting future values based on historical patterns. In this blog post, we introduced the fundamentals of time series forecasting and walked through a step-by-step implementation of an LSTM model using Python and Keras. By following these code explanations, you can start building your own time series forecasting models and gain valuable insights from your data.\n• Jason Brownlee. (2018). How to Get Started with Deep Learning for Time Series Forecasting (7-Day Mini-Course). Machine Learning Mastery."
    },
    {
        "link": "https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras",
        "document": "Neural networks like Long Short-Term Memory (LSTM) recurrent neural networks are able to almost seamlessly model problems with multiple input variables.\n\nThis is a great benefit in time series forecasting, where classical linear methods can be difficult to adapt to multivariate or multiple input forecasting problems.\n\nIn this tutorial, you will discover how you can develop an LSTM model for multivariate time series forecasting with the Keras deep learning library.\n\nAfter completing this tutorial, you will know:\n• How to transform a raw dataset into something we can use for time series forecasting.\n• How to prepare data and fit an LSTM for a multivariate time series forecasting problem.\n• How to make a forecast and rescale the result back into the original units.\n\nKick-start your project with my new book Deep Learning for Time Series Forecasting, including step-by-step tutorials and the Python source code files for all examples.\n• Update Aug/2017: Fixed a bug where yhat was compared to obs at the previous time step when calculating the final RMSE. Thanks, Songbin Xu and David Righart.\n• Update Oct/2017: Added a new example showing how to train on multiple prior time steps due to popular demand.\n\nThis tutorial is divided into 4 parts; they are:\n\nThis tutorial assumes you have a Python SciPy environment installed. I recommend that youuse Python 3 with this tutorial.\n\nYou must have Keras (2.0 or higher) installed with either the TensorFlow or Theano backend, Ideally Keras 2.3 and TensorFlow 2.2, or higher.\n\nThe tutorial also assumes you have scikit-learn, Pandas, NumPy and Matplotlib installed.\n\nIf you need help with your environment, see this post:\n• How to Setup a Python Environment for Machine Learning\n\nIn this tutorial, we are going to use the Air Quality dataset.\n\nThis is a dataset that reports on the weather and the level of pollution each hour for five years at the US embassy in Beijing, China.\n\nThe data includes the date-time, the pollution called PM2.5 concentration, and the weather information including dew point, temperature, pressure, wind direction, wind speed and the cumulative number of hours of snow and rain. The complete feature list in the raw data is as follows:\n• year: year of data in this row\n• month: month of data in this row\n• day: day of data in this row\n• hour: hour of data in this row\n\nWe can use this data and frame a forecasting problem where, given the weather conditions and pollution for prior hours, we forecast the pollution at the next hour.\n\nThis dataset can be used to frame other forecasting problems.\n\n Do you have good ideas? Let me know in the comments below.\n\nYou can download the dataset from the UCI Machine Learning Repository.\n\nUpdate, I have mirrored the dataset here because UCI has become unreliable:\n\nDownload the dataset and place it in your current working directory with the filename “raw.csv“.\n\nThe data is not ready to use. We must prepare it first.\n\nBelow are the first few rows of the raw dataset.\n\nThe first step is to consolidate the date-time information into a single date-time so that we can use it as an index in Pandas.\n\nA quick check reveals NA values for pm2.5 for the first 24 hours. We will, therefore, need to remove the first row of data. There are also a few scattered “NA” values later in the dataset; we can mark them with 0 values for now.\n\nThe script below loads the raw dataset and parses the date-time information as the Pandas DataFrame index. The “No” column is dropped and then clearer names are specified for each column. Finally, the NA values are replaced with “0” values and the first 24 hours are removed.\n\nThe “No” column is dropped and then clearer names are specified for each column. Finally, the NA values are replaced with “0” values and the first 24 hours are removed.\n\nRunning the example prints the first 5 rows of the transformed dataset and saves the dataset to “pollution.csv“.\n\nNow that we have the data in an easy-to-use form, we can create a quick plot of each series and see what we have.\n\nThe code below loads the new “pollution.csv” file and plots each series as a separate subplot, except wind speed dir, which is categorical.\n\nRunning the example creates a plot with 7 subplots showing the 5 years of data for each variable.\n\nIn this section, we will fit an LSTM to the problem.\n\nThe first step is to prepare the pollution dataset for the LSTM.\n\nThis involves framing the dataset as a supervised learning problem and normalizing the input variables.\n\nWe will frame the supervised learning problem as predicting the pollution at the current hour (t) given the pollution measurement and weather conditions at the prior time step.\n\nThis formulation is straightforward and just for this demonstration. Some alternate formulations you could explore include:\n• Predict the pollution for the next hour based on the weather conditions and pollution over the last 24 hours.\n• Predict the pollution for the next hour as above and given the “expected” weather conditions for the next hour.\n\nWe can transform the dataset using the series_to_supervised() function developed in the blog post:\n• How to Convert a Time Series to a Supervised Learning Problem in Python\n\nFirst, the “pollution.csv” dataset is loaded. The wind direction feature is label encoded (integer encoded). This could further be one-hot encoded in the future if you are interested in exploring it.\n\nNext, all features are normalized, then the dataset is transformed into a supervised learning problem. The weather variables for the hour to be predicted (t) are then removed.\n\nThe complete code listing is provided below.\n\nRunning the example prints the first 5 rows of the transformed dataset. We can see the 8 input variables (input series) and the 1 output variable (pollution level at the current hour).\n\nThis data preparation is simple and there is more we could explore. Some ideas you could look at include:\n• Making all series stationary with differencing and seasonal adjustment.\n• Providing more than 1 hour of input time steps.\n\nThis last point is perhaps the most important given the use of Backpropagation through time by LSTMs when learning sequence prediction problems.\n\nIn this section, we will fit an LSTM on the multivariate input data.\n\nFirst, we must split the prepared dataset into train and test sets. To speed up the training of the model for this demonstration, we will only fit the model on the first year of data, then evaluate it on the remaining 4 years of data. If you have time, consider exploring the inverted version of this test harness.\n\nThe example below splits the dataset into train and test sets, then splits the train and test sets into input and output variables. Finally, the inputs (X) are reshaped into the 3D format expected by LSTMs, namely [samples, timesteps, features].\n\nRunning this example prints the shape of the train and test input and output sets with about 9K hours of data for training and about 35K hours for testing.\n\nNow we can define and fit our LSTM model.\n\nWe will define the LSTM with 50 neurons in the first hidden layer and 1 neuron in the output layer for predicting pollution. The input shape will be 1 time step with 8 features.\n\nWe will use the Mean Absolute Error (MAE) loss function and the efficient Adam version of stochastic gradient descent.\n\nThe model will be fit for 50 training epochs with a batch size of 72. Remember that the internal state of the LSTM in Keras is reset at the end of each batch, so an internal state that is a function of a number of days may be helpful (try testing this).\n\nFinally, we keep track of both the training and test loss during training by setting the validation_data argument in the fit() function. At the end of the run both the training and test loss are plotted.\n\nAfter the model is fit, we can forecast for the entire test dataset.\n\nWe combine the forecast with the test dataset and invert the scaling. We also invert scaling on the test dataset with the expected pollution numbers.\n\nWith forecasts and actual values in their original scale, we can then calculate an error score for the model. In this case, we calculate the Root Mean Squared Error (RMSE) that gives error in the same units as the variable itself.\n\nThe complete example is listed below.\n\nNOTE: This example assumes you have prepared the data correctly, e.g. converted the downloaded “raw.csv” to the prepared “pollution.csv“. See the first part of this tutorial.\n\nRunning the example first creates a plot showing the train and test loss during training.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nInterestingly, we can see that test loss drops below training loss. The model may be overfitting the training data. Measuring and plotting RMSE during training may shed more light on this.\n\nThe Train and test loss are printed at the end of each training epoch. At the end of the run, the final RMSE of the model on the test dataset is printed.\n\nWe can see that the model achieves a respectable RMSE of 26.496, which is lower than an RMSE of 30 found with a persistence model.\n\nThis model is not tuned. Can you do better?\n\n Let me know your problem framing, model configuration, and RMSE in the comments below.\n\nThere have been many requests for advice on how to adapt the above example to train the model on multiple previous time steps.\n\nI had tried this and a myriad of other configurations when writing the original post and decided not to include them because they did not lift model skill.\n\nNevertheless, I have included this example below as reference template that you could adapt for your own problems.\n\nThe changes needed to train the model on multiple previous time steps are quite minimal, as follows:\n\nFirst, you must frame the problem suitably when calling series_to_supervised(). We will use 3 hours of data as input. Also note, we no longer explictly drop the columns from all of the other fields at ob(t).\n\nNext, we need to be more careful in specifying the column for input and output.\n\nWe have 3 * 8 + 8 columns in our framed dataset. We will take 3 * 8 or 24 columns as input for the obs of all features across the previous 3 hours. We will take just the pollution variable as output at the following hour, as follows:\n\nNext, we can reshape our input data correctly to reflect the time steps and features.\n\nFitting the model is the same.\n\nThe only other small change is in how to evaluate the model. Specifically, in how we reconstruct the rows with 8 columns suitable for reversing the scaling operation to get the y and yhat back into the original scale so that we can calculate the RMSE.\n\nThe gist of the change is that we concatenate the y or yhat column with the last 7 features of the test dataset in order to inverse the scaling, as follows:\n\nWe can tie all of these modifications to the above example together. The complete example of multvariate time series forecasting with multiple lag inputs is listed below:\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nThe model is fit as before in a minute or two.\n\nA plot of train and test loss over the epochs is plotted.\n\nFinally, the Test RMSE is printed, not really showing any advantage in skill, at least on this problem.\n\nI would add that the LSTM does not appear to be suitable for autoregression type problems and that you may be better off exploring an MLP with a large window.\n\nI hope this example helps you with your own time series forecasting experiments.\n\nThis section provides more resources on the topic if you are looking go deeper.\n• The 5 Step Life-Cycle for Long Short-Term Memory Models in Keras\n• Time Series Forecasting with the Long Short-Term Memory Network in Python\n\nIn this tutorial, you discovered how to fit an LSTM to a multivariate time series forecasting problem.\n• How to transform a raw dataset into something we can use for time series forecasting.\n• How to prepare data and fit an LSTM for a multivariate time series forecasting problem.\n• How to make a forecast and rescale the result back into the original units.\n\nDo you have any questions?\n\n Ask your questions in the comments below and I will do my best to answer."
    },
    {
        "link": "https://stackoverflow.com/questions/56952239/time-series-prediction-of-seasonal-data-using-keras-lstm",
        "document": "Trying to predict the hot water consumption profile of a household using LSTM with Python's Keras library. Watched some tutorials and did a Udemy course, did not find one that helped too much (recommendations appreciated). Since it's just a 1-time problem I don't really want to read a tone of books about this, which is why I was hoping I could count on some assistance by the experts on SO. The task:\n\nThe input is a ~1,5-year long consumption profile with 1-minute resolution. I put this profile into a csv and named it \"labels.csv\". A second csv, called \"features.csv\" contains, like the name suggests, the most important features: the minute of the day, the hour of day, the day of the week. The idea is that, usually consumption ocurrs between 6am-8am and 6pm-8pm during weekdays, and a little later on the weekend. Other influencing factors like vacation days, month of the year etc. were disregarded. The output should be the consumption profile of the next week, i.e. 10080 rows.\n\nFirst, I import relevant models and upload the csv files.\n\nThen I devide it up into training and testing sets:\n\nNow I add the layers (I still do not know how to decide how many layers I should take and how large they should be, but that I can find out via try and error.):\n\nCompilig the model as such:\n\nThe execution of the last line produces the error:\n\nSo I don't even get to ...\n\nIf anyone can maybe point out what I did wrong, point me to a suitable (newbie comprehensible) tutorial or point me to a similar project which I can recycle - I would really appreciate it :)\n\nI added the project to my GitHub\n\nEdit: I also do get lots of deprecation warnings, even though\n\nreturns that everything is up to date ..."
    },
    {
        "link": "https://pypi.org/project/yfinance",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://algotrading101.com/learn/yfinance-guide",
        "document": "\n• Why should I use the yfinance library?\n• Why shouldn’t I use the yfinance library?\n• What are some of the alternatives to the yfinance library?\n• How do I get started with the yfinance library?\n• How do I download historical data using the yfinance library?\n• How do I download fundamental data using the yfinance library?\n• Fundamentals data with multiple tickers at once\n• How do I download trading data using the yfinance library?\n• How do I download options data using the yfinance library?\n• How do I get Expiration dates?\n• How do I get Calls Data?\n• How do I get Puts Data?\n\nyfinance is a popular open source library developed by Ran Aroussi as a means to access the financial data available on Yahoo Finance.\n\nYahoo Finance offers an excellent range of market data on stocks, bonds, currencies and cryptocurrencies. It also offers market news, reports and analysis and additionally options and fundamentals data- setting it apart from some of it’s competitors.\n\nYahoo Finance used to have their own official API, but this was decommissioned on May 15th 2017, following wide-spread misuse of data.\n\nThese days a range of unofficial APIs and libraries exist to access the same data, including of course yfinance.\n\nNote you might know of yfinance under it’s old name- fix-yahoo-finance, since it was re-named on May 26th 2019 at the same time that it went over a large overhaul to fix some usability issues.\n\nTo ensure backwards compatibility, fix-yahoo-finance now imports and uses yfinance anyway, but Ran Aroussi still recommends to install and use yfinance directly.\n\nIn this article we will focus mainly on the yfinance library, but we discuss the overall range of options and other alternative providers in more depth in our parent article, Yahoo Finance API – A Complete Guide.\n\nYes, yfinance is completely open source and free. You can find the documentation here.\n\nWhy should I use the yfinance library?\n• Quick and easy to set yourself up\n\nAs we have just mentioned yfinance is completely open source and free. There are other ways to access the Yahoo Finance data, some free and some paid, and there are certain benefits to some of the options that require paying, like being ensured a degree of maintenance to the solution, but everybody loves free!\n\nInstallation couldn’t be quicker or easier. yfinance has just 4 dependencies, all of which come with Anaconda anyway, and installs fully in a single line of code. No account creation required, or signing up for and using API keys!\n\nIts simple. yfinance is highly Pythonic in it’s design and incredibly streamlined. It’s as easy as creating a ticker object for a particular ticker/list of tickers and then just calling all the methods on this object. Like this:\n\nDon’t worry, we’ll break down that code further in a bit!\n\nFurthermore, the documentation is concise- fitting on a single page, and the method names are very self explanatory.\n\nHigh granularity of data. One cool feature of yfinance is that you can get highly refined data, all the way down to 5 minute, 3 minute and even 1 minute data! The full range of intervals available are:\n\nHowever it is important to note that the 1m data is only retrievable for the last 7 days, and anything intraday (interval <1d) only for the last 60 days.\n\nyfinance also handily returns data directly in padas dataframes or series. This is on contrast to some other options to access Yahoo Finance’s data where you will get lengthy JSONs you need parse for the specific information you want, and will have to manually convert to data-frames yourself.\n\nWhy shouldn’t I use the yfinance library?\n• Can get yourself rate limited/blacklisted\n\nLacks specialised features. Despite the fact you can use it to get a good range of core data, including options and fundamentals data, yfinance doesn’t provide a method to scrape any of the news reports/analysis that are available on Yahoo Finance.\n\nThis obviously isn’t ideal if you want to build model that relies in part on sentiment analysis, so if you want that sort of data, you might want to check out RapidAPI (which will talk about more shortly) that does offer such data.\n\nAlso, other market data alternatives often include interesting extras. For example Alpha Vantage provides modules that calculate various technical analysis indicators for you- obviously an enormous effort save if you want to build an algorithm utilising any of them! yfinance just provides the basics.\n\nSome methods are fragile. yfinance mainly makes API calls to Yahoo Finance to gather it’s data, but it does occasionally employ HTML scraping and pandas tables scraping to unofficially gather the information off the Yahoo Finance website for some of it’s methods. As such, the functionality of some of it’s methods is at the mercy of Yahoo not changing the layout or design of some of their pages. In fact, yfinance is widely known to already have a few issues.\n\nAs a quick aside, data scraping works by simply downloading the HTML code of a web page, and searching through all the HTML tags to find the specific elements of a page you want.\n\nFor instance below is the Yahoo Finance Apple (‘AAPL’) historical data page:\n\nIf the method to get the historical data HTML scraped, it would be searching the various div, class and tr tags etc. for various IDs to pick out the data that should be returned.\n\nFor instance the class ID “Py(10px) Pstart(10px)” refers to the historical prices populating the table. If in this case Yahoo Finance was to change the class ID pointing to this value, the method might return completely incorrect data, or even nothing at all. Again, this sort of vulnerability doesn’t apply to all of yfinance’s methods- most of them do in fact make direct API calls- but it does affect a few.\n\nIt’s an unofficial solution. Again, because yfinance is simply the result of one man’s hard work and not in any way affiliated with Yahoo Finance, there’s no guarantee if it breaks it will be maintained.\n\nAs we already mentioned it did have a big update to fix issues on May 26th 2019 on the same day it was renamed, but that’s no guarantee problems will be fixed in the future. Are you sure you want to build a trading algorithm on-top of data that might one day suddenly and without warning be wrong? There are already a few known issues with yfinance, which we will highlight later on in this article.\n\nYou can get yourself rate limited/blacklisted. Again because yfinance scrapes data for a few of it’s functions, you sometimes run the risk of getting rate limited or blacklisted for too many scraping attempts.\n\nThis is a risk that’s always present when trying to scrape websites, but when you’re building applications trading real money on-top of infrastructure that might be making a lot of data requests, the risk:reward changes.\n\nOverall yfinance an incredibly beginner friendly option. You’ll be able to dive right in and test out ideas without wasting time puzzling over complex documentation whilst still having access to a good range of data!\n\nThat said, the risk of getting faulty data or being blocked from getting any data at all when employing algorithms trading real money is absolutely unacceptable.\n\nWe think yfinance is great for prototyping, or if you are beginner, or just want to download a bunch of historic data.\n\nBut if you want complete confidence that a serious trading system is going to function with total reliability, we’d absolutely recommend going with a official and alternative market data provider- preferably one claiming to provide low latency data directly from exchanges.\n\nWhat are some of the alternatives to the yfinance library?\n\nOf the two alternatives to yfinance we will consider, RapidAPI is the most distinct.\n\nFirstly, whilst it does still have a limited usage free tier, you will have to pay for anything over 500 requests per month:\n\nSecondly, its not quite as simple as yfinance to get started with. You will have to sign up for an account to get your own access API keys.\n\nThat said, a big plus of RapidAPI is that you can use it with 15 different languages, if for some reason Python isn’t your thing:\n\nIt also offers more range of data than our other options, specifically the option to download market news and analysis which is fantastically useful if you want to add a degree of sentiment analysis in your model!\n\nMaking snap trading decisions based on machine scanning of news far faster than a human ever could can be one way (if slightly uncertain) to gain a trading edge.\n\nThat said RapidAPI does have a few drawbacks.\n\nAs you can see requests have an average latency of 1660ms which isn’t terrible, but alternative data providers such as polygon.io offer anything from 200ms down to 1ms delays- quite the difference.\n\nMore concerning is the fact requests only have a 98% success rate. Having 1 in 50 data requests fail could be a big deal if you have a system trading real money, especially if you are making a lower frequency of calls. Definitely something to consider.\n\nResults returned can also be in quite lengthy and nested JSONs, making the data a bit trickier to get ready for use than when using yfinance:\n\nThat said a further plus of RapidAPI is that it offers a huge range of APIs for other purposes, so familiarising yourself with how to use the their API for Yahoo Finance data might carry over into easily using another of their APIs for a different project in the future.\n\nIn summary, RapidAPI offers a very limited free tier, but perhaps by using a solution where some people are paying, it is more likely that any scraping issues from Yahoo Finance structure changes are resolved more quickly.\n\nIts also fiddlier to use and harder get started with, but does provide a bigger range of data than our other two options.\n\nyahoo_fin is an open source and free library similar to yfinance.\n\nYou can find the documentation here.\n\nIt offers a similar range of data to yfinance, but notably has a few functions that generate all the tickers for certain markets for you:\n\nwhich is a useful feature yfinance lacks.\n\nWe actually focus on the yahoo_fin library in the example sections of our parent article, Yahoo Finance API – A Complete Guide, so we won’t talk about it anymore here.\n\nHow do I get started with the yfinance library?\n\nGetting started with the yfinance library is super easy.\n\nIt has the following dependencies:\n\nThese all come as standard in an installation with Anaconda, but are really easy to install manually if for some reason you don’t have them.\n\nAfter that its as easy as:\n\nThe layout itself is also really simple, there are just three modules:\n\nAlmost all the methods are in the Tickers module.\n\nThe download module is for rapidly downloading the historical data of multiple tickers at once.\n\nAnd pandas_datareader is for back compatibility with legacy code, which we will ignore as irrelevant since if you’re reading this you are probably a new user of the library!\n\nHow do I download historical data using the yfinance library?\n\nFirstly, lets import yfinance as yf and create ourselves a ticker object for a particular ticker (stock):\n\nRemember we now use this aapl ticker object for almost everything- calling various methods on it.\n\nTo get the historical data we want to use the history() method, which is the most “complicated” method in the yfinance library.\n\nIt takes the following parameters as input:\n• period: data period to download (either use period parameter or use start and end) Valid periods are:\n• interval: data interval (1m data is only for available for last 7 days, and data interval <1d for the last 60 days) Valid intervals are:\n• start: If not using period – in the format (yyyy-mm-dd) or datetime.\n• end: If not using period – in the format (yyyy-mm-dd) or datetime.\n• prepost: Include Pre and Post regular market data in results? (Default is )- no need usually to change this from False\n• auto_adjust: Adjust all OHLC (Open/High/Low/Close prices) automatically? (Default is )- just leave this always as true and don’t worry about it\n\nThat might look a little complex but mainly you will just be changing the period (or start and end) and interval parameters.\n\nSo as an example, to get 1minute historical data for Apple between 02/06/2020 and 07/06/2020 (British format) we just use the ticker object we created and run:\n\nIt’s as simple as that!\n\nTo download the historical data for multiple tickers at once you can use the download module.\n\nIt takes mostly the same arguments as the history() method on a ticker object, but additionally:\n• group_by: group by column or ticker (‘column’/’ticker’, default is ‘column’)\n• proxy: proxy URL if you want to use a proxy server for downloading the data (optional, default is None)\n\nFor example to get the data for Amazon, Apple and Google all at once we can run:\n\nNote that the default with no interval specified is daily data.\n\nThen, if we want to group by ticker instead of Open/High/Low/Close we can do:\n\nHow do I download fundamental data using the yfinance library?\n\nYou can get the price to earnings ratio with the Ticker.info() method.\n\nTicker.info() returns a dictionary with a wide range of information about a ticker, including such things as a summary description, employee count, marketcap, volume, P/E ratios, dividends etc.- we recommend taking a look at it yourself as it takes a lot of space to show, but in short if you can’t find the information you’re looking for with the other methods, try the info() method!\n\nTo get specifically the price to earnings ratio search the dictionary for ‘forwardPE’:\n\nYou can get the yearly dividend % also by using info():\n\nAnd if you want a breakdown of each dividend payout as it occurred and on what date, you can use Ticker.dividends():\n\nFundamentals data with multiple tickers at once\n\nWe might also want to grab fundamentals (or other) data for a bunch of tickers at once.\n\nLets have a go at doing that and then try comparing our tickers by a particular attribute!\n\nTo do this we can start by creating a list of the tickers we want to get data for, and an empty dictionary to store all the data.\n\nWe will need to use the pandas library to manipulate the data frames:\n\nWe then loop through the list of the tickers, in each case adding to our dictionary a key, value pair where the key is the ticker and the value the dataframe returned by the info() method for that ticker:\n\nWe then combine this dictionary of dataframes into a single dataframe:\n\nAnd then delete the unnecessary “level_1” column and clean up the column names:\n\nGreat, so we now know how to get any data we want for multiple tickers at once into the same dataframe!\n\nBut how do we easily compare by a particular attribute?\n\nIt’s quite easy actually, lets try for one of the attributes in info()– the fullTimeEmployees count:\n\nSo now we have a dataframe of just the employee counts- one entry per ticker- and we can now order by the ‘Recent’ column:\n\nBoom! Obviously not that required with only 5 tickers in our list, but a fantastically easy and powerful way to quickly compare by a particular attribute if we had the ticker list of an entire market!\n\nYou can easily use this exact same method to compare any attribute you want!\n\nHow do I download trading data using the yfinance library?\n\nYou can find the data for all three of Market Cap, Volume and Highs and Lows from the info() method.\n\nTo get the market cap, use:\n\nTo find the current volume do:\n\nIf you want the average volume over the last 24 hours do:\n\nAnd finally if you want the average volume over the last 10 days:\n\nRemember, you can find the highs and lows for any time interval:\n\nwithin a desired period by using the history() method and adjusting the interval.\n\nFor example, to get the weekly highs and lows for all the historical data that exists, use:\n\nJust filter the dataframe with:\n\nAnd so forth to get the individual columns.\n\nAlternatively, you can use info() to get the following useful high/low information:\n\nHow do I download options data using the yfinance library?\n\nBriefly, options are contracts giving a trader the right, but not the obligation, to buy (call) or sell (put) the underlying asset they represent at a specific price on or before a certain date.\n\nTo download options data we can use the option_chain() method. It takes the parameter as input:\n• date: (YYYY-MM-DD), expiry date. If None return all options data.\n\nAnd has the opt.calls and opt.puts methods.\n\nTo get the various expiry dates for options for a particular ticker it’s as easy as:\n\nHow do I get Calls Data?\n\nTo get the calls data, we can do:\n\nHow do I get Puts Data?\n\nTo get puts data, we do:\n\nFinally, opts by itself returns a ticker object containing both the calls and puts data together, if that’s useful to you!\n\nAs we highlighted near the beginning of this article, yfinance is an unofficial scraping solution to gather data from Yahoo Finance, so is subject to breaking if Yahoo Finance changes any of its layout.\n\nUnfortunately this already seems to have happened in part, with the following problems discovered when writing this guide:\n• Tickers, the multiple tickers object for interacting with multiple tickers at once, doesn’t seem to work. We have provided a more manual workaround for this in the Fundamentals data with multiple tickers at once section.\n• The financials, quarterly_financials, balance_sheet, quarterly_balance_sheet, cashflow, quarterly_cashflow, earnings, quarterly_earnings Ticker methods do not work and return empty dataframes.\n\nThis is a big problem as in many cases there is no alternative way to the data in some of these methods from other methods in yfinance.\n\nIf you are building something that requires any of this data, for example balance sheets and income and cashflow statements and still want free access to the Yahoo Finance data, check out the yahoo_fin library in the examples section of our guide https://algotrading101.com/learn/yahoo-finance-api/ which has working methods to get all of this data!\n\nSo clearly as we have just demonstrated, yfinance is NOT a safe bet to build critical infrastructure on.\n\nIf you want to build algorithms trading real money, we absolutely recommend you use an official data source/API, preferably one connected directly to exchange data and with low latency. Something like Polygon.io or IEX might suit you better.\n\nIf you absolutely HAVE to use the Yahoo Finance data specifically, we recommend at least paying for an unofficial API like RapidAPI, where you stand a good bet there is an active team of developers constantly maintaining the API. Remember RapidAPI does still have a limited usage free tier!\n\nThat said, yfinance can be good to use to build test applications as a beginner, as the sections of it that do work are fantastically easy to get started with and use.\n\nA particular forte of yfinance is that the threads parameter of yf.download does allow very rapid downloading of historical for multiple tickers when set to True!\n\nYou can find the code used in this article here."
    },
    {
        "link": "https://github.com/ranaroussi/yfinance",
        "document": "yfinance offers a Pythonic way to fetch financial & market data from Yahoo!Ⓡ finance.\n\nYahoo!, Y!Finance, and Yahoo! finance are registered trademarks of Yahoo, Inc. yfinance is not affiliated, endorsed, or vetted by Yahoo, Inc. It's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes. You should refer to Yahoo!'s terms of use (here, here, and here) **for details on your rights to use the actual data downloaded. Remember - the Yahoo! finance API is intended for personal use only.**\n\nThe list of changes can be found in the Changelog\n\nrelies on the community to investigate bugs, review code, and contribute code. Developer guide: #1084\n\nyfinance is distributed under the Apache Software License. See the LICENSE.txt file in the release for details.\n\nAGAIN - yfinance is not affiliated, endorsed, or vetted by Yahoo, Inc. It's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes. You should refer to Yahoo!'s terms of use (here, here, and here) for details on your rights to use the actual data downloaded.\n\nPlease drop me a note with any feedback you have."
    },
    {
        "link": "https://ranaroussi.github.io/yfinance",
        "document": "Showing a small sample of yfinance API, the full API is much bigger and covered in API Reference."
    },
    {
        "link": "http://help.yahoo.com/kb/SLN2311.html",
        "document": "Downloading historical data is available with a Yahoo Finance Gold subscription. Learn more about Yahoo Finance subscription plans.\n\nYou can view historical price, dividend, and split data for most quotes in Yahoo Finance to forecast the future of a company or gain market insight. If you have a Gold subscription, you may download historical data as a CSV file to be used offline, which you can open with Excel or a similar program. If the data requested is beyond the range of historical prices available through Yahoo Finance, all available data within the range is displayed. Historical prices usually don't go back earlier than 1970.\n• Enter a company name or stock symbol into the Quote Lookup field.\n• Tap a quote in the search results to view it.\n• To use the data offline in a CSV file, tap Download. Once you're able to see the downloaded file, the options to save it will vary by device. In many cases, you can email yourself the file. Important - This feature isn't available for all instruments due to data licensing restrictions, in which case the Download option is not present."
    },
    {
        "link": "https://eodhd.com/financial-academy/backtesting-strategies-examples/a-step-by-step-guide-to-implementing-the-supertrend-indicator-in-python",
        "document": "I’ve backtested numerous trading strategies using a wide variety of technical indicators. Among these, a few indicators have delivered exceptional results, and I consider them to be “premium” indicators. In today’s analysis, we’ll focus on one such trend-following indicator that has earned its place in this exclusive group due to its accuracy and efficiency—the SuperTrend indicator.\n\nWe’ll begin by understanding what the SuperTrend indicator is and how its formula works, including its key parameters like volatility and settings for intraday trading. Afterward, we’ll demonstrate how to build the indicator from scratch using Python, step-by-step, and integrate it into a simple trading strategy. Next, we’ll backtest the strategy on Tesla’s stock and analyze its performance against the SPY ETF (an ETF designed to mimic the movements of the S&P 500 market index).\n\nFinally, we’ll discuss the best indicators to use with SuperTrend and how to optimize the indicator for maximum accuracy rate in various market conditions. Let’s dive into the article and explore how you can harness the power of the SuperTrend indicator in your trading strategies.\n\nBefore moving on to discovering the SuperTrend indicator, it is essential to understand the parameters and role of the Average True Range (ATR), as it is crucial to the SuperTrend formula used in technical analysis and charting.\n\nThe Average True Range is a technical indicator that measures how much an asset moves on average, often used in intraday trading strategies. It is a lagging indicator, meaning it relies on historical data to calculate the current value but does not predict future price movements. However, this is not considered a drawback, as ATR is widely regarded as one of the best indicators to track volatility. Its ability to assess market volatility is why it pairs well with the SuperTrend indicator for strategies focused on dynamic market conditions.\n\nAdditionally, the ATR is a non-directional indicator, meaning its movement is not directly tied to the market’s trend. This makes it valuable for understanding overall volatility, regardless of whether the market is trending up or down. To calculate ATR, you must follow two key steps:\n• Calculate True Range (TR): A True Range of an asset is calculated by taking the greatest values of three price differences which are: market high minus marker low, market high minus previous market close, previous market close minus market low. It can be represented as follows:\n• Calculate ATR: The calculation for the Average True Range is simple. We just have to take a smoothed average of the previously calculated True Range values for a specified number of periods. The smoothed average is not just any SMA or EMA but an own type of smoothed average created by Wilder Wiles himself but there aren’t any restrictions in using other MAs too. In this article, we will be using the Exponential Moving Average (EMA) to calculate ATR rather than the custom moving average created by the founder of the indicator just to make things simple. The calculation of ATR with a traditional setting of 14 as the number of periods can be represented as follows:\n\nWhile using ATR as an indicator for trading purposes, traders must ensure that they are more cautious than ever, as the indicator is very lagging. Now that we have an understanding of what the Average True Range is all about. Let’s now dive into the main concept of this article, the SuperTrend Indicator.\n\nAs the name suggests, the SuperTrend indicator tracks the direction of a trending market. This indicator is well-known for its precision in spotting efficient buy and sell signals for trades, making it a reliable tool for intraday trading as well. While the calculation of the SuperTrend might seem complex at first, I’ll break it down into simpler steps to help you understand it better.\n\nThere are two main parameters involved in the calculation of this indicator: the lookback period and the multiplier. The lookback period represents the number of data points considered, while the multiplier is used to adjust the volatilitycomponent, specifically by multiplying the ATR (Average True Range). The traditional setting for the SuperTrend indicator is a 10-period lookback and a multiplier of 3. Using these settings, let’s proceed to the calculation of the SuperTrend formula.\n\nThe first step is to determine the 10-day ATR, which we discussed earlier. Next, we calculate the basic upper and lower bands. To do this, we first find the High/Low average (HLA), calculated by adding the high and low values of the stock and dividing by 2. Using the HLA, the upper band is determined by multiplying the 10-day ATR by the multiplier (3 in this case) and then adding it to the HLA value. For the lower band, the same method is applied, but instead of adding, we subtract the ATR product from the HLA. These two bands form the basis of the SuperTrend chart used in market analysis.\n\nThe calculation of these two bands can be mathematically represented as follows:\n\nThen comes the calculation of the final upper and lower bands, which are the core components involved in the calculation of the SuperTrend indicator. There is no formula for the calculation of the final bands, but instead, conditions are passed and the values will be appended to the bands concerning which condition gets satisfied. The condition for the current final upper band goes as follows:\n• If the current basic upper band is lesser than the previous final upper band, or the previous closing price of the stock is greater than the previous final upper band, then the current final upper band’s value is the current basic upper band.\n• If the condition fails to get satisfied, then the current final upper band’s value is the previous final upper band. The condition of the final upper band can be represented as follows:\n\nThe condition for the current lower band goes as follows:\n• If the current basic lower band is greater than the previous final lower band or the previous closing price of the stock is lesser than the previous final lower band, then the current final lower band’s value is the current basic lower band.\n• If this condition of the current final lower band fails to get satisfied, then the current final lower band is the previous final lower band. The condition can be represented as follows:\n\nNow we have all the essential components to determine the values of the SuperTrend indicator. The same method that we used conditions to calculate the final bands’ values applies to the calculation of the SuperTrend indicator too. While there is only one condition for determining the final bands’ values, there are four different conditions for the SuperTrend indicator. The conditions for the current SuperTrend value goes as follows:\n• If the previous SuperTrend indicator value is equal to the previous final upper band and the current closing price of the stock is lesser than the current final upper band, then the current SuperTrend indicator value is the current final upper band.\n• If the previous SuperTrend indicator value is equal to the previous final upper band and the current closing price of the stock is greater than the current final upper band, then the current SuperTrend indicator value is the current final lower band.\n• If the previous SuperTrend indicator value is equal to the previous final lower band and the current closing price of the stock is greater than the current final lower band, then the current SuperTrend indicator value is the current final lower band.\n• If the previous SuperTrend indicator value is equal to the previous final lower band and the current closing price of the stock is lesser than the current final lower band, then the current SuperTrend indicator value is the current final upper band.\n\nWhen putting all these conditions together, the collective number of conditions can be represented as follows:\n\nThat’s the whole process of calculating the SuperTrend indicator values. To build a stronger understanding of the indicator and how it works, let’s explore a chart where the closing price of a stock is plotted along with the SuperTrend indicator’s readings.\n\nIn the above chart, the blue line represents the closing price of the Tesla stock and the line with both red and green color represents the readings of the SuperTrend indicator. The line of the SuperTrend indicator turns green if the readings of the indicator are below the closing price and turns red if it’s above the closing price. As I said before, the SuperTrend indicator is a trend-following indicator and this can be observed in the chart that the indicator directly reveals the current trend of the market more accurately.\n\nTraders use the color changes or trend changes observed in the SuperTrend indicator line to mark buy and sell signals for their trades. To be more elaborate, traders go long (buy the stock) if the indicator’s line crosses from above to below the closing price line, and similarly, they go short (sell the stock) if the indicator’s line crosses from below to above the closing price line. This SuperTrend strategy is called the crossover strategy. This strategy can be represented as follows:\n\nThis is the strategy we are going to implement in this article too. Many other strategies can also be implemented based on the SuperTrend indicator but just to make things simple to understand, we are going with the crossover strategy. This concludes our theory part on the SuperTrend indicator. Now, let’s move on to the coding part where we are first going to build the indicator from scratch, build the crossover strategy which we just discussed, then, compare our strategy’s performance with the SPY ETF’s returns in Python. Let’s do some coding! Before moving on, a note on disclaimer: This article’s sole purpose is to educate people and must be considered as an information piece but not as investment advice.\n\nThe coding part is classified into various steps as follows:\n\nWe will be following the order mentioned in the above list and buckle up your seat belts to follow every upcoming coding part.\n\nImporting the required packages into the Python environment is a non-skippable step. The primary packages are going to be eodhd for extracting historical stock data, Pandas for data formatting and manipulations, NumPy to work with arrays and for complex functions, and Matplotlib for plotting purposes. The secondary packages are going to be Math for mathematical functions and Termcolor for font customization (optional).\n\nWith the required packages imported into Python, we can proceed to fetch historical data for Tesla using EODHD’s Python library. Also, if you haven’t installed any of the imported packages, make sure to do so using the command in your terminal.\n\nIt is essential to register the EODHD API key with the package in order to use its functions. If you don’t have an EODHD API key, firstly, head over to their website, then, finish the registration process to create an EODHD account, and finally, navigate to the ‘Settings’ page where you could find your secret EODHD API key. It is important to ensure that this secret API key is not revealed to anyone. You can activate the API key by following this code:\n\nThe code is pretty simple. In the first line, we are storing the secret EODHD API key into the and then in the second line, we are using the class provided by the package to activate the API key and stored the response in the variable.\n\nNote that you need to replace with your secret EODHD API key. Apart from directly storing the API key with text, there are other ways for better security such as utilizing environmental variables, and so on.\n\nBefore heading into the extraction part, it is first essential to have some background about historical or end-of-day data. In a nutshell, historical data consists of information accumulated over a period of time. It helps in identifying patterns and trends in the data. It also assists in studying market behavior. Now, you can easily extract the historical data of any tradeable assets using the package by following this code:\n\nIn the above code, we are using the function provided by the package to extract the split-adjusted historical stock data of Tesla. The function consists of the following parameters:\n• the parameter where the symbol of the stock we are interested in extracting the data should be mentioned\n• the refers to the time interval between each data point (one-day interval in our case).\n• the and parameters which indicate the starting and ending date of the data respectively. The format of the input should be “YYYY-MM-DD”\n• the parameter which is an optional parameter that can be used to order the dataframe either in ascending ( ) or descending ( ). It is ordered based on the dates.\n\nAfter extracting the historical data, we are performing some data-wrangling processes to clean and format the data. The final dataframe looks like this:\n\nIn this step, we are going to calculate the values of the SuperTrend indicator by following the methods we discussed before.\n\nCode Explanation: We are first defining a function named ‘get_supertrend’ which takes a stock’s high (‘high’), low (‘low’), close (‘close’), the lookback period (‘lookback), and the multiplier (‘multiplier’) as parameters. The code inside the function can be divided into six parts: ATR calculation, HLA and basic bands calculation, final upper band calculation, final lower band calculation, SuperTrend indicator calculation, and determining the uptrend and downtrend of the indicator.\n\nATR calculation: To determine the readings of the Average True Range, we are first calculating the three differences and stored them into their respective variables. Then we are combining all three differences into one dataframe using the ‘concat’ function and took the maximum values out of the three collective differences to determine the True Range. Then, using the ‘ewm’ and ‘mean’ function, we are taking the Exponential Moving Average of True Range for a specified number of periods to get the ATR values. Many prefer using SMA for ATR calculation while determining the SuperTrend, but I used EMA for more accuracy.\n\nHLA and Basic Bands calculation: Before calculating the basic upper and lower bands, we need to first determine the High Low Average. To calculate the values of HLA, we are first finding the total of the high and low values of a stock, then dividing the total by 2. Using the HLA values which are stored into the ‘hl_avg’ variable, we are determining both the upper and lower bands by following the formula we discussed and stored the values into the ‘upper_band’ and ‘lower_band’ respectively.\n\nFinal Upper Band calculation: Before directly moving into calculating the final upper band values, we are first creating a dataframe named ‘final_bands’ to store both the final upper and lower bands. Then just for the sake to fill the dataframe with values, we are subtracting the upper band by itself to store zeros matching the length of the upper band series so that it can be used for iterations. This step of creating the dataframe is optional but highly recommended since it reduces future works of data processing and all similar stuff. After creating the dataframe, we are passing a for-loop to create the conditions we discussed before for determining the values of the final upper band.\n\nFinal Lower Band calculation: The code structure for determining the final lower band is most similar to the final upper band calculation but only the conditions change. But now, let’s dive into exploring what’s happening inside the for-loop we are passing to get the values of the final lower band. First, we are passing a for-loop that iterates over the length of the ‘final_bands’ dataframe we created before. Inside the for-loop, we are first defining an if-statement that appends the final lower band’s value as 0 if the current iteration value is zero. This if-statement’s purpose is to fill the first value of the final lower band as zero. After the if-statement, we are defining nested else-statement which appends the final lower band’s value as the basic lower band value if the condition we discussed before gets satisfied, or else, it appends the previous final lower band value.\n\nSuperTrend calculation: We are first creating a dataframe named ‘supertrend’ to store the values of the SuperTrend indicator. Like how we did before just to fill the dataframe with zeros to match the length of the basic bands series, we are doing the same here too to match the final bands series. Then comes the for-loop to determine the values of the SuperTrend indicator which appends the values concerning which condition gets satisfied out of the four. After the for-loop, we are doing some data processing to modify a bit and clean the dataframe.\n\nUptrend/Downtrend determination: This step is optional as I did only to make the visualization of the indicator a bit easy but you can try doing this too. The main aim of this step is to classify the SuperTrend indicator’s periods into two categories: the SuperTrend being below the closing price (uptrend), the SuperTrend being above the closing price (downtrend). With that being said, let’s dive into the methods involved. We are first defining two empty lists named ‘up’ and ‘down’ in which the values of the uptrend and the downtrend will be appended respectively. We are also reducing the length of the closing price data to match that of the SuperTrend data, only then, the iteration would be possible. Next, we are passing a for-loop to iterate over the length of the SuperTrend data to determine and append both uptrend and downtrend values into their respective variables. The values will be appended concerning which condition gets satisfied that are defined inside the for-loop. If either condition gets satisfied, the values of both uptrend and downtrend will be appended as ‘NaN’ (not defined). Then, we are converting the lists we created to store the uptrend and downtrend into Pandas series as it will be more convenient to work with.\n\nAfter calculating all these values, we are finally returning the SuperTrend indicator values, and both the uptrend and downtrend readings. Then, we are calling the created function to store the SuperTrend indicator values of Tesla along with the uptrend and downtrend readings with 10 as the lookback period and 3 as the multiplier. From the output being shown, it is observable that whenever the closing price is greater than the SuperTrend indicator, the downtrend readings (‘st_dt’) represent ‘NaN’. Likewise, whenever the closing price is lesser than the SuperTrend indicator, the uptrend readings (‘st_upt’) represent ‘NaN’.\n\nIn this step, we are going to implement the discussed SuperTrend indicator crossover trading strategy in python.\n\nCode Explanation: First, we are defining a function named ‘implement_st_strategy’ which takes the stock prices (‘prices’), and the values of the SuperTrend indicator (‘st’) as parameters.\n\nInside the function, we are creating three empty lists (buy_price, sell_price, and st_signal) in which the values will be appended while creating the trading strategy.\n\nAfter that, we are implementing the trading strategy through a for-loop. Inside the for-loop, we are passing certain conditions, and if the conditions are satisfied, the respective values will be appended to the empty lists. If the condition to buy the stock gets satisfied, the buying price will be appended to the ‘buy_price’ list, and the signal value will be appended as 1 representing to buy the stock. Similarly, if the condition to sell the stock gets satisfied, the selling price will be appended to the ‘sell_price’ list, and the signal value will be appended as -1 representing to sell the stock.\n\nFinally, we are returning the lists appended with values. Then, we are calling the created function and stored the values into their respective variables. The list doesn’t make any sense unless we plot the values. So, let’s plot the values of the created trading lists.\n\nIn this step, we are going to plot the created trading lists to make sense out of them.\n\nCode Explanation: We are plotting the readings of the SuperTrend indicator along with the buy and sell signals generated by the trading strategy. We can observe that whenever the SuperTrend indicator line crosses from above to below the closing price line, a green-colored buy signal is plotted in the chart. Similarly, whenever the SuperTrend indicator line crosses from below to above the closing price line, a red-colored sell signal is plotted in the chart.\n\nIn this step, we are going to create a list that indicates 1 if we hold the stock or 0 if we don’t own or hold the stock.\n\nCode Explanation: First, we are creating an empty list named ‘position’. We are passing two for-loops, one is to generate values for the ‘position’ list to just match the length of the ‘signal’ list. The other for-loop is the one we are using to generate actual position values. Inside the second for-loop, we are iterating over the values of the ‘signal’ list, and the values of the ‘position’ list get appended concerning which condition gets satisfied. The value of the position remains 1 if we hold the stock or remains 0 if we sold or don’t own the stock. Finally, we are doing some data manipulations to combine all the created lists into one dataframe.\n\nFrom the output being shown, we can see that in the first two rows our position in the stock has remained 1 (since there isn’t any change in the SuperTrend indicator signal) but our position suddenly turned to -1 as we sold the stock when the SuperTrend indicator trading signal represents a sell signal (-1). Our position will remain 0 until some changes in the trading signal occur. Now it’s time to implement some backtesting process!\n\nBefore moving on, it is essential to know what backtesting is. Backtesting is the process of seeing how well our trading strategy has performed on the given stock data. In our case, we are going to implement a backtesting process for our SuperTrend indicator trading strategy over the Tesla stock data.\n\nCode Explanation: First, we are calculating the returns of the Tesla stock using the ‘diff’ function provided by the NumPy package and we have stored it as a dataframe into the ‘tsla_ret’ variable. Next, we are passing a for-loop to iterate over the values of the ‘tsla_ret’ variable to calculate the returns we gained from our SuperTrend indicator trading strategy, and these returns values are appended to the ‘st_strategy_ret’ list. Next, we are converting the ‘st_strategy_ret’ list into a dataframe and stored it into the ‘st_strategy_ret_df’ variable.\n\nNext comes the backtesting process. We are going to backtest our strategy by investing a hundred thousand USD into our trading strategy. So first, we are storing the amount of investment into the ‘investment_value’ variable. After that, we are calculating the number of Tesla stocks we can buy using the investment amount. You can notice that I’ve used the ‘floor’ function provided by the Math package because, while dividing the investment amount by the closing price of Tesla stock, it spits out an output with decimal numbers. The number of stocks should be an integer but not a decimal number. Using the ‘floor’ function, we can cut out the decimals. Remember that the ‘floor’ function is way more complex than the ‘round’ function. Then, we are passing a for-loop to find the investment returns followed by some data manipulation tasks.\n\nFinally, we are printing the total return we got by investing a hundred thousand into our trading strategy and it is revealed that we have made an approximate profit of two hundred and thirty-two thousand USD in one year. That’s wonderful! Now, let’s compare our returns with SPY ETF (an ETF designed to track the S&P 500 stock market index) returns.\n\nThis step is optional but it is highly recommended as we can get an idea of how well our trading strategy performs against a benchmark (SPY ETF). In this step, we are going to extract the data of the SPY ETF using the ‘get_historical_data’ function we created and compare the returns we get from the SPY ETF with our SuperTrend indicator trading strategy returns on Tesla.\n\nCode Explanation: The code used in this step is almost similar to the one used in the previous backtesting step but, instead of investing in Tesla, we are investing in SPY ETF by not implementing any trading strategies. From the output, we can see that our SuperTrend indicator trading strategy has outperformed the SPY ETF by 191%. That’s great!\n\nAfter a long process of crushing both theory and coding parts, we have successfully learned what the SuperTrend indicator is all about, its calculation, and how to build a simple crossover trading strategy based on it in python. One important thing we managed to accomplish in this article is understanding the complex math behind the indicator and this will help in increasing your understanding of the indicator significantly.\n\nAs the SuperTrend indicator has just started gaining momentum, there are a lot more spaces for improvement. One such important space is strategy optimization. I talk about this in almost every article of mine as it should be considered of paramount importance.\n\nFor those who don’t know what strategy optimization is, it is the process of tuning the trading algorithm to perform at its best. You can tune the SuperTrend indicator by experimenting with different settings. In this article, we built the indicator with the traditional setting of 14 as the lookback period and 3 as the multiplier but, you can try changing the values and run backtests for each and every change to acknowledge its performance. By doing this will help you reach the optimal setting for the indicator that can outperform the market itself.\n\nWe didn’t consider doing it in this article, as the sole purpose is not on building an optimistic trading strategy ,but on building a strong intuition on what the SuperTrend indicator is all about. But, it is highly recommended to get your hands dirty on experimenting with the indicator and introduce yourselves to a whole new level of possibilities. With that being said, you’ve reached the end of the article. Hope you learned something useful from this article. Happy programming!"
    },
    {
        "link": "https://blog.stackademic.com/how-to-boost-your-returns-using-the-supertrend-strategy-with-a-python-based-implementation-cbe31a9894a1",
        "document": "How to Implement The Supertrend Trading Strategy in Python for a Potential Good Return\n\nIt is a wonderful morning and you’re sitting there browsing through your TradingView chart configuration looking for some hot signals, only to be confronted with uncertain trends. You seem to want to bend space-time — but, it feels like you’re just trying to fold the fabric of reality with your mind. It might be frustrating when you want to make a move but the market isn’t your regular friend at the grocery store — it just isn't cooperating! Trust me, we’ve all been there — it sucks. However, you cannot force the market to behave in your favor. Timing is pretty much everything, you only can wait until the stars align, I guess. We simply have to wait for the perfect moment. That’s when the Supertrend indicator comes in handy! It’s a tool that kind of takes an analysis of the specified target markets and watches out for trends and reversals. All you need is to make good investment decisions, of course, some can blow out on you, but it’s a game of consistency — you need more wins than losses. You need to know when to enter or exit the market.\n\nWhy would you want to use Python for this implementation?\n\nIf you have worked with Python before, you know that it is a very versatile and comprehensive programming language with great libraries, specially designed for data analysis and visualization. Pandas is one of such libraries that provides efficient ways to import, clean, and preprocess market data, making it ready for analysis. It is also superb at handling large datasets, an essential aspect when dealing with market data.\n\nTalk of Numpy another useful library for financial analysis. It is very powerful in providing an extensive collection of mathematical functions and tools that come in handy with statistical calculations. These functions speed up computations over large datasets, such as when dealing with market data, thereby providing deeper insights and analysis in a short amount of time. This, compared to manually looking through numerous charts, can be quite time-consuming and may not lead to accurate analysis.\n\nI bet you don’t want to trade with bias and emotional sentiments that may lead to bad investment decisions. This is why you may want to consider an algorithmic trading strategy such as this one that can potentially lead to higher returns.\n\nHow does the Supertrend indicator work?\n\nLet's picture this, a river sometimes flows smoothly and other times behaves so rough and may break the river banks, this is pretty much similar to the markets — prices can move smoothly or swing wildly and spike through certain unexpected ranges. So the supertrend indicator takes a look at these swings, which is known as volatility. It does this by measuring the Average True Range (ATR); which is basically the trading range of an asset within a specified period. The supertrend then uses the ATR to generate the upper band and the lower band — which are more like boundaries within which the asset prices range within that period. If the the price breaks the upper boundary that signals an uptrend and a break through the lower band signals a downward trend. This is a straightforward concept — right?\n\nFirst and foremost, we calculate the Average True Range(ATR). This is the measure of volatility. It is calculated over a specified period of time.\n\nYou want to start by getting the True Range(TR) for each data point. A data point can be collected per day, 4-hour timeframe, hourly, or by minute, etc. It all depends on the timeframe setup.\n\nWe then calculate the ATR for the past specified period, say we could do a period of 14. So this means that the ATR is calculated for every 14 data points( take these as rows in the dataframe) — which is pretty much the average of the True Ranges(TR) within that period.\n\nHaving understood the core concepts of the supertrend strategy, we now have to get hands-on and execute it with Python.\n\nInstall the latest Python 3 distribution if you don't have it already. You can check the version of Python on your machine by running “python -v” in your terminal.\n\nInstall all the required Python libraries such as Pandas, Numpy, Matplotlib, mplfinance, and ccxt.\n\nImport the required dependencies for the project. The package ccxt is the data provider that connects to several crypto exchanges — in this case we will use it to connect to Binance. The other libraries Pandas and Numpy are for statistical computations while mplfinance and matplotlib are for creating plots to visualize the data.\n\nThe function “fetch_asset_data” is getting the historical price data needed for backtesting. It takes in 4 arguments— the ticker symbol, start date, time period (like daily, hourly etc.), and the exchange the trading pair is on.\n\nFirst it changes the start date into a number that the exchange API understands. Then it connects to that exchange and requests for all the open, high, low, close prices (OHLC) from that start date until now for the ticker symbol.\n\nIt then puts all that price data into a neat dataframe and drops the last row of data. Why? Because that last row might have live prices data that are changing right now. And we can’t use future prices when we’re backtesting what happened in the past! So it drops that last row to give us only historical data before today. Then it returns the clean dataframe table to us.\n\nSo this “supertrend” function takes the dataframe we got from the last function. It also needs one other thing — the “atr_multiplier” number.\n\nIt calculates the Average True Range or ATR. That tells us how much the price has been moving up and down each day or any period configured, so we know how “volatile” or jumpy the market is. It uses the ATR and the multiplier number to calculate the basic upper band and basic lower band each row.\n\nNow it starts looking at each row, except the very first one. For the upper band — if the basic upper band is lower than the final upper band from the row before, OR if the close price is above the final upper band from the row before, it adds the basic upper band value to the final upper band list.\n\nIt does the same thing for the lower band, just checking if the basic lower band is higher than the final bottom band from the row before, OR if the close price is below the final lower before.\n\nAfter checking every row, it drops the columns we don’t need anymore and returns the dataframe. These final bands (df[‘upperband’] and df[‘lowerband’]) and the closing price will tell us the signals for when to buy or sell.\n\nThe function “generate_signals” starts by making a list called “signals” and setting the first signal to 0. Then it looks through the whole dataframe row by row to find buy and sell signals.\n\nA buy signal happens when the closing price for that row goes above the upper band value. A sell signal happens when the closing price drops below the lower band value.\n\nIf the current signal is the same as the previous one, we keep the position the same as the last row.\n\nEach time it looks at a new row, it updates the signals list with the new signal for that row. Then it adds the whole signals list as a new column in the dataframe called “df[‘signals’]”.\n\nIt removes the look ahead bias to simulate taking positions in a similar way you would in a live trading environment after you have seen the signal. That way we don’t get signals using information from the future, thus the backtested results are more realistic and reliable. The functions then returns the “df ”— which will be used to create positions.\n\nThe function starts off by getting rid of some numbers in the upper and lower bands that we don’t need to see. Remember how those upper and lower bands were calculated before? Well, sometimes the bands would keep showing the closing price values even when the signal didn’t say to buy or sell there. That could get confusing later on when we want to just look at the bands where we actually have an open position (trend). So those first two lines in the function are just cleaning things up for us. It’s putting “NaN” in the spots of the upper band where the signal isn’t “1”, and in the lower band where the signal isn’t “-1”. Especially when we want to plot the bands — you don't want to plot the entire lines, only the spots where the trend is active. So you know — “NaN” is a special number that means “no value”. This will make more sense as we move on.\n\nWe go ahead and initialize two empty lists — one for our buy prices and one for our sell prices. The function then loops through the dataframe looking at each row, except the very first one.\n\nSo if the signal is a “1” (meaning buy) and it’s different than the signal before, that’s a reversal so we make a buy. It adds the close price for that row to the buy list. In a similar fashion, if the signal is a “-1” (sell) and different than before, that’s a sell. It adds the close price to the sell list. If nothing changed, it just leaves those spots empty(“NaN”) in the lists.\n\nAfter checking every row, it adds the final buy and sell lists as new columns to the dataframe. Now we have a clear record of exactly when we would have bought and sold based on the signals.\n\nNow we want to visualize the data, the function “plot_data”, makes lines on the chart to show the lower and upper bands we calculated before. The lower band will be green and the upper will be red.\n\nIt’s also making markers to show exactly when we bought and sold — little up arrows for buys and down arrows for sells. These will be a little big and colorful so they kind of stand out. Then it groups all those lines and markers together so we can add them to the chart. Then, it goes ahead to add shaded areas between the close price and the bands. This will make it really clear to visualize the up and down trends.\n\nIt plots the candlesticks for the ohlv (open, high, low, close), sets the title and legends, and then shows the plot. Now we will easily see how the strategy would have played out over time.\n\nWe need to check on some metrics to see how well our trading strategy does. That is what this function “strategy_performance” does. It figures out first of all what our money would be worth if we just held onto it — that’s the benchmark.\n\nNow it sets up some empty lists and variables to track things like the balance, profits and losses (P&L), and how much money we invest during each position. It then looks at each row in our strategy dataframe, one by one. For each signal, it calculates the P&L by seeing how much the asset price changed between when we bought and sold, and then updates our balance, investment amount, and adds the new numbers to the lists. It also tracks other metrics like the highest and lowest balance we ever had, overall profit, maximum drawdown etc. These, it prints out. Note that this functions also compounds the returns, i.e the investment is the available balance from even all the closed positions.\n\nWe plot the strategy and the benchmark returns to compare how well the strategy would have performed.\n\nSet up the variables under the “if __name__ == ‘__main__’:” constructor, and then get to the terminal point, it to the directory containing file and then run the script.\n\nMy file was supertrend.py so, you run it by running “python supertrend.py” in your terminal.\n\nHere is the resulting dataframe:\n\nAnd these were the calculated performance metrics, Note that past performance does not guarantee future results. So use this strategy with caution. And of course, nothing in this article is investment advice. It's all for educational purposes.\n\nAs you can see for a period of 3-months, with an investment of $100 and no leverage, BTC/USDT would have made a 40% return on investment. Feel free to calculate other metrics like Sortino ratio, alpha e.t.c\n\nAnd this was the strategy plot (it is a little zoomed out so some data points may be cut off)\n\nAnd lastly here is the resulting plot for the performance curve:\n\nParameter Senstivity: The supertrend indicator can be senstive to its parameters such as the period, and the atr volatility multiplier. Different trading pairs can move around a lot in a given period, but be pretty quiet on others. So what worked well for one pair might not work as good for a different one. You could tweak the volatility parameters to suit the trading pair.\n\nVolatility Differences: The SuperTrend indicator relies heavily on the ATR to capture trends, but if a trading pair is so volatile or experiences frequent sharp price fluctuations, it may produce false signals or fail to capture meaningful trends. Therefore the supertrend indicator may not be suitable for highly volatile assets.\n\nTo wrap up, the super trend strategy is a powerful tool that can assist investors in making some well-informed decisions regarding their trading activities. It assists in identifying the current trends and reversals in the market, enabling traders to execute timely entry and exit signals. When implemented properly, this strategy can result in significantly higher returns, making it an invaluable resource.\n\nHere is the full code on my Github\n\nThank you for reading until the end. Before you go:\n• Please consider clapping and following the writer! 👏\n• Visit our other platforms: In Plain English | CoFeed | Venture | Cubed"
    },
    {
        "link": "https://insightbig.com/post/step-by-step-implementation-of-the-supertrend-indicator-in-python",
        "document": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import requests from math import floor from termcolor import colored as cl plt.style.use('fivethirtyeight') plt.rcParams['figure.figsize'] = (20,10) def get_supertrend(high, low, close, lookback, multiplier): # ATR tr1 = pd.DataFrame(high - low) tr2 = pd.DataFrame(abs(high - close.shift(1))) tr3 = pd.DataFrame(abs(low - close.shift(1))) frames = [tr1, tr2, tr3] tr = pd.concat(frames, axis = 1, join = 'inner').max(axis = 1) atr = tr.ewm(lookback).mean() # H/L AVG AND BASIC UPPER & LOWER BAND hl_avg = (high + low) / 2 upper_band = (hl_avg + multiplier * atr).dropna() lower_band = (hl_avg - multiplier * atr).dropna() # FINAL UPPER BAND final_bands = pd.DataFrame(columns = ['upper', 'lower']) final_bands.iloc[:,0] = [x for x in upper_band - upper_band] final_bands.iloc[:,1] = final_bands.iloc[:,0] for i in range(len(final_bands)): if i == 0: final_bands.iloc[i,0] = 0 else: if (upper_band[i] < final_bands.iloc[i-1,0]) | (close[i-1] > final_bands.iloc[i-1,0]): final_bands.iloc[i,0] = upper_band[i] else: final_bands.iloc[i,0] = final_bands.iloc[i-1,0] # FINAL LOWER BAND for i in range(len(final_bands)): if i == 0: final_bands.iloc[i, 1] = 0 else: if (lower_band[i] > final_bands.iloc[i-1,1]) | (close[i-1] < final_bands.iloc[i-1,1]): final_bands.iloc[i,1] = lower_band[i] else: final_bands.iloc[i,1] = final_bands.iloc[i-1,1] # SUPERTREND supertrend = pd.DataFrame(columns = [f'supertrend_{lookback}']) supertrend.iloc[:,0] = [x for x in final_bands['upper'] - final_bands['upper']] for i in range(len(supertrend)): if i == 0: supertrend.iloc[i, 0] = 0 elif supertrend.iloc[i-1, 0] == final_bands.iloc[i-1, 0] and close[i] < final_bands.iloc[i, 0]: supertrend.iloc[i, 0] = final_bands.iloc[i, 0] elif supertrend.iloc[i-1, 0] == final_bands.iloc[i-1, 0] and close[i] > final_bands.iloc[i, 0]: supertrend.iloc[i, 0] = final_bands.iloc[i, 1] elif supertrend.iloc[i-1, 0] == final_bands.iloc[i-1, 1] and close[i] > final_bands.iloc[i, 1]: supertrend.iloc[i, 0] = final_bands.iloc[i, 1] elif supertrend.iloc[i-1, 0] == final_bands.iloc[i-1, 1] and close[i] < final_bands.iloc[i, 1]: supertrend.iloc[i, 0] = final_bands.iloc[i, 0] supertrend = supertrend.set_index(upper_band.index) supertrend = supertrend.dropna()[1:] # ST UPTREND/DOWNTREND upt = [] dt = [] close = close.iloc[len(close) - len(supertrend):] for i in range(len(supertrend)): if close[i] > supertrend.iloc[i, 0]: upt.append(supertrend.iloc[i, 0]) dt.append(np.nan) elif close[i] < supertrend.iloc[i, 0]: upt.append(np.nan) dt.append(supertrend.iloc[i, 0]) else: upt.append(np.nan) dt.append(np.nan) st, upt, dt = pd.Series(supertrend.iloc[:, 0]), pd.Series(upt), pd.Series(dt) upt.index, dt.index = supertrend.index, supertrend.index return st, upt, dt tsla['st'], tsla['s_upt'], tsla['st_dt'] = get_supertrend(tsla['high'], tsla['low'], tsla['close'], 10, 3) tsla = tsla[1:] print(tsla.head()) def implement_st_strategy(prices, st): buy_price = [] sell_price = [] st_signal = [] signal = 0 for i in range(len(st)): if st[i-1] > prices[i-1] and st[i] < prices[i]: if signal != 1: buy_price.append(prices[i]) sell_price.append(np.nan) signal = 1 st_signal.append(signal) else: buy_price.append(np.nan) sell_price.append(np.nan) st_signal.append(0) elif st[i-1] < prices[i-1] and st[i] > prices[i]: if signal != -1: buy_price.append(np.nan) sell_price.append(prices[i]) signal = -1 st_signal.append(signal) else: buy_price.append(np.nan) sell_price.append(np.nan) st_signal.append(0) else: buy_price.append(np.nan) sell_price.append(np.nan) st_signal.append(0) return buy_price, sell_price, st_signal buy_price, sell_price, st_signal = implement_st_strategy(tsla['close'], tsla['st']) position = [] for i in range(len(st_signal)): if st_signal[i] > 1: position.append(0) else: position.append(1) for i in range(len(tsla['close'])): if st_signal[i] == 1: position[i] = 1 elif st_signal[i] == -1: position[i] = 0 else: position[i] = position[i-1] close_price = tsla['close'] st = tsla['st'] st_signal = pd.DataFrame(st_signal).rename(columns = {0:'st_signal'}).set_index(tsla.index) position = pd.DataFrame(position).rename(columns = {0:'st_position'}).set_index(tsla.index) frames = [close_price, st, st_signal, position] strategy = pd.concat(frames, join = 'inner', axis = 1) strategy.head() print(strategy[20:25]) tsla_ret = pd.DataFrame(np.diff(tsla['close'])).rename(columns = {0:'returns'}) st_strategy_ret = [] for i in range(len(tsla_ret)): returns = tsla_ret['returns'][i]*strategy['st_position'][i] st_strategy_ret.append(returns) st_strategy_ret_df = pd.DataFrame(st_strategy_ret).rename(columns = {0:'st_returns'}) investment_value = 100000 number_of_stocks = floor(investment_value/tsla['close'][-1]) st_investment_ret = [] for i in range(len(st_strategy_ret_df['st_returns'])): returns = number_of_stocks*st_strategy_ret_df['st_returns'][i] st_investment_ret.append(returns) st_investment_ret_df = pd.DataFrame(st_investment_ret).rename(columns = {0:'investment_returns'}) total_investment_ret = round(sum(st_investment_ret_df['investment_returns']), 2) profit_percentage = floor((total_investment_ret/investment_value)*100) print(cl('Profit gained from the ST strategy by investing $100k in TSLA : {}'.format(total_investment_ret), attrs = ['bold'])) print(cl('Profit percentage of the ST strategy : {}%'.format(profit_percentage), attrs = ['bold'])) def get_benchmark(start_date, investment_value): spy = get_historical_data('SPY', start_date)['close'] benchmark = pd.DataFrame(np.diff(spy)).rename(columns = {0:'benchmark_returns'}) investment_value = investment_value number_of_stocks = floor(investment_value/spy[-1]) benchmark_investment_ret = [] for i in range(len(benchmark['benchmark_returns'])): returns = number_of_stocks*benchmark['benchmark_returns'][i] benchmark_investment_ret.append(returns) benchmark_investment_ret_df = pd.DataFrame(benchmark_investment_ret).rename(columns = {0:'investment_returns'}) return benchmark_investment_ret_df benchmark = get_benchmark('2020-01-01', 100000) investment_value = 100000 total_benchmark_investment_ret = round(sum(benchmark['investment_returns']), 2) benchmark_profit_percentage = floor((total_benchmark_investment_ret/investment_value)*100) print(cl('Benchmark profit by investing $100k : {}'.format(total_benchmark_investment_ret), attrs = ['bold'])) print(cl('Benchmark Profit percentage : {}%'.format(benchmark_profit_percentage), attrs = ['bold'])) print(cl('ST Strategy profit is {}% higher than the Benchmark Profit'.format(profit_percentage - benchmark_profit_percentage), attrs = ['bold']))"
    },
    {
        "link": "https://codearmo.com/python-tutorial/super-trend-strategy-python",
        "document": "Are you looking to take your cryptocurrency trading to the next level? Look no further than Bybit, the leading crypto derivatives exchange. With advanced trading tools, low fees, and a user-friendly platform, Bybit makes it easy to trade Bitcoin, Ethereum, and other popular cryptocurrencies. And if you sign up using our affiliate link and use the referral code CODEARMO, you'll receive exclusive benefits and bonuses up to $30,000 to help you get started. Don't miss out on this opportunity to join one of the fastest-growing communities in crypto trading. Sign up for Bybit today and start trading like a pro!\n\nWhat is the SuperTrend Indicator?\n\nThe Supertrend indicator is a technical analysis tool that uses a combination of moving averages and ATR (average true range) to identify the trend direction and generate buy and sell signals. The Supertrend formula involves two parameters, namely the period for ATR calculation and the multiplier. The ATR value is calculated based on the high, low, and closing prices of the security, and then multiplied by the multiplier to get the final value. The Supertrend line is plotted by adding the ATR value to the moving average for a bullish trend and subtracting the ATR value from the moving average for a bearish trend. The buy and sell signals are generated when the price crosses above or below the Supertrend line. The Supertrend is a popular indicator among traders and can be used in various trading strategies.\n\nIn order to prevent repetition we will use the same logic for data collection as shown in the Bollinger Bands article. For the purposes of this article we will take 4 hour candles for ETHUSDT to create the super-trend indicator.\n\nTo calculate the Supertrend indicator using pandas_ta, we'll first need to import the library. Once we have pandas_ta installed and imported, we can use the supertrend() function to calculate the indicator. We'll pass in the high, low, and close prices of ETHUSDT, along with a factor that determines the sensitivity of the indicator. Once the Supertrend values are calculated, we can plot them using matplotlib. We'll create a new figure and axis object, and use the plot() function to plot the Supertrend values against the date range of the security's prices. With just a few lines of code, we can quickly calculate and plot the Supertrend indicator for any coin we're interested in analyzing.\n\nIn the following analysis, we will compute the simple returns for a buy-and-hold strategy on ETH. Additionally, we will evaluate the returns from a long-short strategy that relies on the SuperTrend indicator, where the direction is denoted as either 1 or -1, corresponding to long or short positions, respectively. To ensure that we avoid lookahead bias, we must shift the returns as demonstrated below, given that the high, low, and close values for a given period are not available until the end of the period.\n\nAs illustrated in the plot above, a buy-and-hold strategy appears to outperform the strategy of solely following the Supertrend indicator. It is also important to note that we haven't included fees. However, it is important to note that the Supertrend indicator is typically utilized in conjunction with other technical indicators or for establishing stop loss/take profit levels. We urge the reader to experiment with this indicator in tandem with other technical indicators to evaluate its effectiveness in their investment strategy."
    },
    {
        "link": "https://levelup.gitconnected.com/step-by-step-implementation-of-the-supertrend-indicator-in-python-656aa678c111",
        "document": "A Step-By-Step Guide to Implementing the SuperTrend Indicator in Python\n\nI’ve backtested a lot of trading strategies based on an extensive amount of technical indicators. Among those, some indicators’ results are off the charts and I term these as premium indicators. In today’s article, we are going to discuss a trend-following indicator that joins this exclusive list of premium indicators concerning its performance and efficiency. It’s none other than the SuperTrend indicator.\n\nWe will first explore what this indicator is all about and its complex calculation. Then, we will move on to building the indicator from scratch in Python and construct a simple trading strategy based on it. Finally, we will backtest it on the stock of Tesla and compare the strategy’s performance with the returns of SPY ETF (an ETF specifically designed to track the movements of the S&P 500 market index). Without further ado, let’s hop into the article.\n\nBefore moving on, if you want to backtest your trading strategies without any coding, there is a solution for it. It is BacktestZone. It is a platform to backtest any number of trading strategies on different types of tradeable assets for free without coding. You can use the tool right…"
    }
]