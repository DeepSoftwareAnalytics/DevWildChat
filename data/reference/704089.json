[
    {
        "link": "https://stackoverflow.com/questions/55465234/how-to-fix-a-subscript-out-of-range-error-for-vectors-in-c",
        "document": "Out of range error for vectors in my code. What can I do to stop the error? I've checked so many forums.\n\nI've already done some basic trials to see if something else was the problem. Starting at 0. Wrote the code from scratch again. Used other IDEs.\n\nResults always lead me to line 1733 of the vector file, and I'm unsure if more error will come after fixing the error."
    },
    {
        "link": "https://stackoverflow.com/questions/16861260/vector-subscript-out-of-range-c-error",
        "document": "The problem is when you erase an element from the vector it reduces the size of the vector. The index goes from 0 to , where the is computed in the outer loop. In the inner loop the size of will be reduced when you erase an element from , but is not updated to reflect the reduced size of contours. That's why you get vector subscript out of range error."
    },
    {
        "link": "https://quora.com/How-can-you-prevent-a-vector-subscript-out-of-a-range-problem-in-C++",
        "document": "Something went wrong. Wait a moment and try again."
    },
    {
        "link": "https://cboard.cprogramming.com/cplusplus-programming/176543-expression-vector-subscript-out-range-[debug-assertion-fail].html",
        "document": ""
    },
    {
        "link": "https://gamedev.stackexchange.com/questions/138809/vector-subscript-out-of-range",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://geeksforgeeks.org/vector-in-cpp-stl",
        "document": "In C++, vector is a dynamic array that stores collection of elements same type in contiguous memory. It has the ability to resize itself automatically when an element is inserted or deleted.\n\nVector is defined as the std::vector class template inside the <vector> header file.\n\nwhere T is the type of elements and v is the name assigned to the vector.\n\nCreating a vector involves creating an instance of std::vector class. This requires us to provide the type of elements as template parameter.\n\nWe can also provide the values to be stored in the vector inside {} curly braces. This process is called initialization.\n\nIn the above example,\n• vector<int> v2(5, 9) creates a vector of size 5 where each element initialized to 9.\n\nMore ways to declare and initialize vectors are discussed in this article – 8 Ways to Initialize Vector in C++\n\nAn element can be inserted into a vector using vector insert() method which takes linear time. But for the insertion at the end, the vector push_back() method can be used. It is much faster, taking only constant time.\n\nMore ways to insert an element in the vector are discussed in the article – Different Ways to Add Elements in a Vector\n\nJust like arrays, vector elements can be accessed using their index inside the [] subscript operator. This method is fast but doesn’t check whether the given index exists in the vector or not. So, there is another member method vector at() for safely accessing elements.\n\nTo know more about accessing vector elements, refer to the article – Different Ways to Access Elements in Vector\n\nUpdating elements is very similar to the accessing except that we use an assignment operator to assign a new value. It uses the same methods: [] subscript operator and vector at().\n\nMore methods to update vector elements are discussed in this article – Different Ways to Update Vector Elements\n\nOne of the common problems with arrays was to keep a separate variable to store the size information. Vector provides the solution to this problem by providing size() method.\n\nVector in C++ can be traversed using indexes in a loop. The indexes start from 0 and go up to vector size – 1. To iterate through this range, we can use a loop and determine the size of the vector using the vector size()method.\n\nWe can also use a range-based loop for simple traversal. More ways to traverse vectors are discussed in this article – Different Ways to Iterate Through Vector\n\nAn element can be deleted from a vector using vector erase() but this method needs iterator to the element to be deleted. If only the value of the element is known, then find() function is used to find the position of this element.\n\nFor the deletion at the end, the vector pop_back() method can be used, and it is much faster, taking only constant time.\n\nTo know more about the deletion of an element in the vector, refer to this article – Different Ways to Remove Elements from Vector\n\nVector is one of the most frequently used containers in C++. It is used in many situations for different purposes. The following examples aim to help you master vector operations beyond the basics.\n\nThe below table lists the time complexity of the above operations on a vector:\n\nVectors can be passed to a function as arguments just like any other variable in C++. But it is recommended to pass the vector by reference so as to avoid the copying of all elements which can be expensive if the vector is large. Refer to this article to know more – Passing Vector to a Function\n\nVector internal working is very interesting and useful to select and optimize its usage. Understanding the internal memory management also helps in modifying the default mechanism of vector to suits our needs. Refer to this article to know more – Internal Working of Vector\n\nJust like arrays, we can also create multidimensional vectors in C++. Each element of multidimensional vector can be visualized as the collection of vectors with dimension one less that the current vector. For example, 2D vectors are the collection of 1D vectors, while 3D vectors are the collection of 2D vectors and so on.\n\nWith the addition of each dimension, the complexity of operations on the vectors also increases.\n\nRefer to this article to know more – Multidimensional Vectors in C++\n\nFollowing is the list of all member functions of std::vector class in C++:\n\nAdds an element to the end of the vector. Removes the last element of the vector. Returns the number of elements in the vector. Returns the maximum number of elements that the vector can hold. Changes the size of the vector. Checks if the vector is empty. Accesses the element at a specific position, with bounds checking. Accesses the first element of the vector. Accesses the last element of the vector. Returns an iterator pointing to the first element of the vector. Returns an iterator pointing to the past-the-end element of the vector. Returns a reverse iterator pointing to the last element of the vector. Returns a reverse iterator pointing to the element preceding the first element of the vector. Inserts elements at a specific position in the vector. Removes elements from a specific position or range in the vector. Swaps the contents of the vector with those of another vector. Removes all elements from the vector. Constructs and inserts an element in the vector. Constructs and inserts an element at the end of the vector. Assigns new values to the vector elements by replacing old ones. Returns the size of the storage space currently allocated to the vector. Requests that the vector capacity be at least enough to contain a specified number of elements. Returns a direct pointer to the memory array used internally by the vector to store its owned elements. Returns a copy of the allocator object associated with the vector."
    },
    {
        "link": "https://stackoverflow.com/questions/31663770/c-safety-of-accessing-element-of-vector-via-pointers",
        "document": "In a C++ project of mine, I am using a to hold a bunch of s which hold a number of elements for a simple game (ie: tic-tac-toe, coordinates, vs , etc). ie:\n\nEvery time during a single game, whenever a player makes a move (ie: places an or ), the information is stored in the via , to create an \"undo\" feature, which currently works as expected.\n\nIn some parts of my undo/redo logic, I am using functions which traverse the , find the appropriate element, and return a pointer directly to that element.\n\nIs it safe, so long as I'm correctly managing the , access to it, NULL-ing out all the pointers that point to elements of the , or are there inherent risks? My concern is that if extend the game, which I plan to do (ie: use the same code for a chess game I'm working on), and that the gets too large and needs to be automagically re-allocated, the pointers will become invalid (ie: entire list is moved to a new contiguous block that can fit all the elements), or is there some type of smart-pointer interface to to accomplish the same thing?"
    },
    {
        "link": "https://stackoverflow.com/questions/34527968/individual-elements-in-a-stdvector-c",
        "document": "You ask for the ... The answer is: There is no general\n\nThe is not always the same. It really depends on what your code is doing.\n\nThe also depends on what you want to achieve, e.g. best performance, best code maintainability, best code readability and so on.\n\nThe is also based on a personal view.\n\nSo just to repeat the answer: There is no general\n\nOther answers mentions a number of different posibilities. I'll add one more."
    },
    {
        "link": "https://en.cppreference.com/w/cpp/container/vector",
        "document": "The elements are stored contiguously, which means that elements can be accessed not only through iterators, but also using offsets to regular pointers to elements. This means that a pointer to an element of a vector may be passed to any function that expects a pointer to an element of an array.\n\nThe storage of the vector is handled automatically, being expanded as needed. Vectors usually occupy more space than static arrays, because more memory is allocated to handle future growth. This way a vector does not need to reallocate each time an element is inserted, but only when the additional memory is exhausted. The total amount of allocated memory can be queried using capacity() function. Extra memory can be returned to the system via a call to shrink_to_fit()[1].\n\nReallocations are usually costly operations in terms of performance. The reserve() function can be used to eliminate reallocations if the number of elements is known beforehand.\n\nThe complexity (efficiency) of common operations on vectors is as follows:\n• Insertion or removal of elements at the end - amortized constant .\n• Insertion or removal of elements - linear in the distance to the end of the vector .\n\n(for other than bool) meets the requirements of Container, AllocatorAwareContainer(since C++11), SequenceContainer, ContiguousContainer(since C++17) and ReversibleContainer.\n\nAll member functions of are constexpr: it is possible to create and use objects in the evaluation of a constant expression. However, objects generally cannot be constexpr, because any dynamically allocated storage must be released in the same evaluation of constant expression.\n• is In libstdc++,is not available in C++98 mode.\n\nThe type of the elements. must meet the requirements of and . The requirements that are imposed on the elements depend on the actual operations performed on the container. Generally, it is required that element type is a complete type and meets the requirements of , but many member functions impose stricter requirements. The requirements that are imposed on the elements depend on the actual operations performed on the container. Generally, it is required that element type meets the requirements of Erasable, but many member functions impose stricter requirements. This container (but not its members) can be instantiated with an incomplete element type if the allocator satisfies the allocator completeness requirements. An allocator that is used to acquire/release memory and to construct/destroy the elements in that memory. The type must meet the requirements of . if is not the same as .\n\nThe standard library provides a specialization of for the type bool, which may be optimized for space efficiency.\n\nThe following behavior-changing defect reports were applied retroactively to previously published C++ standards."
    },
    {
        "link": "https://geeksforgeeks.org/how-to-access-element-in-vector-using-index-in-cpp",
        "document": "How to Access Elements in a Vector in C++?\n\nIn C++, vector provides fast access to its elements using their position as indexes. In this article, we will discuss different ways to access the element in vector using index in C++.\n\nThe simplest method to access the elements of vector is by using vector operator [] with the corresponding index value. Let’s take a look at a simple example:\n\nExplanation: Indexes of vectors in C++ starts from 0 and goes till (size - 1). This method does check whether the given index exists in the std::vector or not leading undefined behaviour if the index is invalid.\n\nThere are also some other methods in C++ by which we can access\n\nVector elements can also access using vector::at() method. This method also takes the index but if the index is invalid it throws std::out_of_range exception.\n\nExplanation: The element at index 3 is accessed without any problem, but when we tried to access the element at index 10, we got the out-of-range error.\n\nIterators are like pointers to the elements of the vector which can be deference to access the element it points to. The vector begin() functions returns iterator to the first element. It can be incremented to point to the element at desired index.\n\nwhere, idx is the index whose value to be accessed.\n\nThis method also does not perform bound checking.\n\nThe vector front() method can be used to quickly access the first element of the vector without bothering about index.\n\nMake sure that the vector is not empty while using vector front() method, otherwise it may cause segmentation fault.\n\nJust like vector front() method, vector back() method can be used to quickly access the last element of the vector without bothering about index."
    },
    {
        "link": "https://stackoverflow.com/questions/70359446/how-to-optimize-this-radix-sort-algorithm-in-c",
        "document": "As suggested in the comments, first thing is to get the API right.\n\ncan be replaced by , which uses iterators, and doesn't make a copy of the input.\n\nOther suspicious thing is . If memory so permits, at least reserve the maximum amount of elements in each queue -- otherwise the queues need to copy old data when resizing.\n\nThen if possible, using raw pointers with no out of range check, you can and with and My observation is that while STL is correctly implemented and is fast to develop with, the support of dynamic memory allocation in insertions practically always degrades performance compared to known static allocations.\n\nThird thing is to specialise the radix for powers of two, since now the division is strength reduced to shift, and the modulus is strength reduced to logical and (by some constants, which need to be calculated).\n\nEspecially when radix is a power of two, and possibly otherwise too, I guess it's not useful to calculate conditionally: ."
    },
    {
        "link": "https://geeksforgeeks.org/cpp-program-for-radix-sort",
        "document": "Radix Sort is a sorting technique in which we sort the elements by processing each and every digit of that element. It is not a comparison-based sorting algorithm which means we do not compare the elements in a radix sort in order to sort them. Here, we apply counting sort to every digit of an element starting from the Least Significant Digit (LSB) to the Most Significant Digit (MSB) or vice versa.\n\nAlgorithm for Radix Sort in C++\n\nThe Radix Sort is based on the idea that by sorting the dataset on the basis of each decimal place, it will be eventually completely sorted. Radix sort uses the counting sort algorithm as the subroutine.\n\nThe algorithm of the radix sort can be written as:\n\nWorking of Radix Sort in C++\n\nWe will understand the working of Radix Sort in C++ using an example.\n\nWe take an array named 'count' whose size is 10 as the values will range from 0-9 always. Initially, we will initialize the count array with 0. Then we will update the count array with respect to the frequency of the digits. After that, we will perform prefix sum in the same way we do in a counting sort.\n\nwhere d is the no. of digits, n is the total no. of elements and k is the base of the number system.\n\nThe radix sort has the following benefits:\n• It is faster than other comparison-based sorting algorithms.\n\nThe radix sort also has some limitations which are as follows:\n• Space Complexity is higher as compared to the other sorting algorithms."
    },
    {
        "link": "https://stackoverflow.com/questions/78643069/optimization-of-radix-sort-implementation-slower-than-expected-compared-to-stan",
        "document": "The main problem in terms of performance is that your code does a lot of memory allocations. It is very costly. Each vector in your 2D vector you grow dynamically. It means that on the first call it allocates space for or elements, then on the third call allocates space for elements and copies all the date it had into a newly allocated memory location. Then and so on. The values above are implementation dependent and are not precise. But the idea is correct. Vector expansion is costly.\n\n, contrary, is implemented completely inplace. It does not allocate any additional space to sort a range.\n\nYou can avoid those allocations by calling on vectors you plan to populate. This solution works fine, if you know how many elements will your vector have. Or can come up with some estimation based on some empirical knowledge. So your loops will look like this:\n\nThis change to the way how you populate your vectors will improve your code performance significanlty.\n\nIf you do not have any estimation of your vector future size, you can try to use instead of . It has different allocation policy, which in your case may end up in less allocations in total. allocates memory in a fixed size chunks and does not copy data from old place to a new one. So sometimes, when you have to deal with dynamic growth it can be faster. But you will pay with slower access to elements and slower iteration through the .\n\nIt was a long time ago, when I wrote my radix sort. But I would also suggest you to split your elements into \"words\" for radix sort differently. Now you do a dynamic split. I mean, your \"word\" size is defined by , which you pass during runtime. You can fix that size based on the type size. You are using . Split each value into four byte words. With that you will fix the size of your \"inner\" vectors by and will be able to use instead . It will require a bit more work, in terms that you will need to go through the array 4 times, but it will also have better memory locality, so can end up good.\n\nPerformance is about experimenting and measuring. Very few things are guaranteed, but many can be worth of trying :)"
    },
    {
        "link": "https://github.com/eloj/radix-sorting",
        "document": "TODO: These notes (and code) are incomplete. My goal is to eventually provide a step-by-step guide and introduction to a simple radix sort implementation, and try to cover the basic issues which are sometimes glossed over in text books and courses. Furthermore, WHILE THIS NOTE PERSISTS, I MAY FORCE PUSH TO MASTER\n\nThese are my notes on engineering a radix sort.\n\nThe ideas behind radix sorting are not new in any way, but seems to have become, if not forgotten, so at least under-utilized, as over the years quicksort became the go-to \"default\" sorting algorithm for internal sorting.\n\nUnless otherwise specified, this code is written foremost to be clear and easy to understand.\n\nAll code is provided under the MIT License.\n• All together now; Radix sort\n\nThis code can sort 40 million 32-bit integers in under half a second using a single core of an Intel i5-3570T, a low-TDP CPU from 2012 using DDR3-1333. requires ~3.5s for the same task (with the caveat that it's in-place).\n\nThe included will build all the code on any reasonably up-to-date linux installation.\n\nThe default build enables a lot of compiler warning flags and optimizations, but care has been taken to make sure the examples are easy to build and run individually too;\n\nThe program and the benchmark expects a file named to exist. This file is generated by running or .\n\nSee the section on C++ Implementation for more information on how to use the test program and benchmark.\n\nPossibly the simplest way to sort an array of integers, is to simply count how many there are of each, and use those counts to write the result.\n\nThis is the most basic counting sort.\n\nYou could easily extend this code to work with 16-bit values, but if you try to push much further the drawbacks of this counting sort become fairly obvious; you need a location to store the count for each unique integer. For 8- and 16-bit numbers this would amount to =1KiB and =256KiB of memory. For 32-bit integers, it'd require =16GiB of memory. Multiply by two if you need 64- instead of 32-bit counters.\n\nAgain, we're only sorting an array of integers, nothing more. It's not immediately obvious how we could use this to sort records with multiple pieces of data.\n\nAs presented, this sort is in-place, but since it's not moving any elements, it doesn't really make sense to think of it as being stable or unstable.\n\nAn in-place sort is a sorting algorithm where the amount of extra memory used does not depend on the size of the input.\n\nA stable sort is one where records with equal keys keep their relative position in the sorted output.\n\nTo get us closer to radix sorting, we now need to consider a slightly more general variant where we're, at least conceptually, rearranging input elements:\n\nWe have introduced a separate output array, which means we are no longer in-place. This auxiliary array is required; the algorithm would break if we tried to write directly into the input array.\n\nHowever, the main difference between this and the first variant is that we're no longer directly writing the output from the counts. Instead the counts are re-processed into a prefix sum (or exclusive scan) in the second loop.\n\nThis gives us, for each input value, its first location in the sorted output array, i.e the value of tells us the array index at which to write the first j to the output, because contains the sum count of all elements that would precede ('sort lower than') j.\n\nFor instance, will always be zero, because any will always end up in the first position in the output (we're assuming only non-negative integers for now). will contain how many zeroes precede the first , will contain how many zeroes and ones precede the first , and so on.\n\nIn the sorting loop, we look up the output location for the key of the entry we're sorting, and write the entry there. We then increase the count of the prefix sum for that key by one, which guarantees that the next same-keyed entry is written just after.\n\nBecause we are processing the input entries in order, from the lowest to the highest index, and preserving this order when we write them out, this sort is in essence stable. That said, it's a bit of a pointless distinction since without any other data associated with the keys, there's nothing that distinguishes same keys from one another.\n\nWith a few basic modifications, we arrive at:\n\nWe are now sorting an array of , not an array of octets. The name and the type of the struct here is arbitrary; it's only used to show that we're not restricted to sorting arrays of integers.\n\nThe primary modification to the sorting function is the small addition of a function , which returns the key for a given record.\n\nThe main insight you should take away from this is that, to sort record types, we just need some way to extract or derive a key for each entry.\n\nWe're still restricted to integer keys. We rely on there being some sort of mapping from our records (or entries) to the integers which orders the records the way we require.\n\nHere we still use a single octet inside the as the key. Associated with each key is a short string. This allows us to demonstrate that a) sorting keys with associated data is not a problem, and b) the sort is indeed stable.\n\nRunning the full program demonstrates that each like-key is output in the same order it came in the input array, i.e the sort is stable.\n\nNow we are ready to take the step from counting sorts to radix sorts.\n\nOn a high level, the radix sort we'll cover next uses the counting sort we have already discussed, but overcome the inherent limitation of counting sorts to deal with large magnitude (or wide) keys by using multiple passes, each pass processing only a part of the key.\n\nSome texts describe this as looking at the individual digits of an integer key, which you can process digit-by-digit via a series of modulo (remainder) and division operations with the number 10, the base or radix.\n\nIn a computer we deal with bits, which means the base is inherently two, not ten.\n\nBecause working with individual bits is in some sense \"slow\", we group multiple of them together into units that are either faster and/or more convenient to work with. One such grouping is into strings of eight bits, called octets or bytes.\n\nAn octet can represent or different values. In other words, just as processing a base-10 number digit-by-digit is operating in radix-10, processing a binary number in units of eight bits means operating in radix-256.\n\nSince we're not going to do any math with the keys, it may help to conceptually consider them simply as bit-strings instead of numbers. This gets rid of some baggage which isn't useful to the task at hand.\n\nBecause we're using a computer and operate on bits, instead of division and modulo operations, we use bit-shifts and bit-masking.\n\nBelow is a table of random 32-bit keys written out in hexadecimal, or base-16, which is a convenient notation for us. In hexadecimal a group of four bits is represented with a symbol (digit) from 0-F (0-9 plus A-F), and consequently a group of eight bits is represented by two such symbols.\n\nIf you consider this table, with the four 8-bit wide columns marked A through D, there's a choice to be made; if we're going to process these keys top to bottom, one column at a time, in which order do we process the columns?\n\nThis choice gives rise to the two main classes of radix sorts, those that are Least Significant Bits (LSB, bottom-up) first and those that are Most Significant Bits (MSB, top-down) first. Sometimes 'digit' is substituted for 'bits', it's the same thing.\n\nIf you have prior experience you may already know that, based on the material presented so far, we're going down the LSB path, meaning we'll process the columns from right to left; D, C, B, A.\n\nIn our counting sorts, the key width and the radix (or column) width were the same; 8-bits. In a radix sort the column width will be less or equal to the key width, and in a LSB radix sort we'll be forced to make multiple passes over the input to make up the difference. The wider our radix the more memory (to track counts), but fewer passes we'll need. This is a tradeoff.\n\nThe assertion then, and we will demonstrate this to be true, is that if we apply counting sort by column D, and then apply counting sort on that result by column C, and so forth, after the last column (A) is processed, our data will be sorted and this sort is stable.\n\nThe function builds on in a straight-forward manner by introducing four counting sort passes. The outer (pass) loop is unrolled by design, to show off the pattern.\n\nThe four histograms are generated in one pass through the input. These are re-processed into prefix sums in a separate step. A speed vs memory trade-off can be had by not pre-computing all the histograms.\n\nWe then sort columns D through A, swapping (a.k.a ping-ponging) the input and output buffer between the passes.\n\nAfter the 4:th (final) sorting pass, since this is an even number, the final result is available in the input buffer.\n\nA quick performance note: Having four distinct arrays for the histograms will almost certainly generate imperfect code, e.g you're likely to end up with separate calls to .\n\nExtending to larger keys, e.g 64-bit keys and beyond, can be achieved by simply adding more passes, but for completeness, let us instead look at how we can augment the function to sort in multiple passes via a key-shift argument, as this is a very common implementation detail:\n\nAt each point we retrieve the key via , we shift out any bits we've already processed in a previous pass, passed in as .\n\nWe can then sort 64-bit wide by calling the sort function twice as such:\n\nThe first call will sort on bits 0-31 of the key, the second call on bits 32-63.\n\nThis is only possible because the sort is stable.\n\nSo far we've mostly had our code rearrange the values in the input array itself, but it's sometimes desirable to be able to treat the input as immutable.\n\nThe obvious solution is to generate an array of pointers into the input and then sort those pointers. This is usually how we do things when sorting record types (or strings) in a C-style language, since rearranging these implies copying a lot of data, which can be expensive, and because we can have multiple pointer arrays representing different sort orders over the same input.\n\nAn alternative is to sort using indeces instead. You can think of it as assigning a number between and to each object to be sorted, and then returning a permutation of these numbers that represents the sorted order. Some know this by the name argsort.\n\nExample: Take as input the array . Assuming zero-based indexing, the rank-array representing an ascending sort of the input is . I.e =\n\nHaving the sort return indeces is also useful if we need to sort something which is split over different arrays, since the result can be applied to the original input as well as any 'parallel' arrays. This is demonstrated in the example code below, where we decouple from the struct whose key we're sorting on.\n\nSo the goal is to modify our sorting function to return instead of permuting . We can achieve this with quite minor changes:\n\nFirst note in the prototype that we declare the input array as const. This means we can't write to , and hence we guarantee that the input is undisturbed.\n\nInstead of the old array we accept an array , which has been allocated to be twice the number of entries to be sorted. It's twice the length of the original array because we still need space to ping-pong reads and writes while sorting, and the input array is no longer available for writing.\n\nIn the first pass we read the input in order and write out indeces in the correct positions in the indeces buffer. In subsequent passes we alternate reading via, and writing the indeces buffers.\n\nAs with sorting via pointers, the extra indirection from looking up the key via the indeces array is likely to have quite negative implications for performance due to the added data-dependency. There are solutions to this, which we'll have to revisit here at a later date.\n\nOn the plus side, we have now decoupled the size of the auxiliary buffer(s) from the size of the objects in the input array. Yes, we need to allocate a buffer twice the length of that when we're sorting by value, but we only need room for two indeces per entry, so the size of the auxilary buffer(s) is directly related to the number of objects being sorted. So for example, if we're sorting fewer than 2^16 objects, we can use 16-bit indeces to save space and improve cache behaviour.\n\nFirst let's get the question of sorting strings out of the way.\n\nLSB radix sort is inherently columnar. The keys must all be the same width, and narrower is better to keep the number of passes down. To sort variable length keys you would first have to pad all keys on the left (MSD) side until they're the length of the longest key.\n\nThis is to say, an LSB implemention is not well suited to sort character strings of any significant length. For those, a MSB radix sort is better.\n\nFor sorting floats, doubles or other fixed-width keys, or changing the sort order, we can still use the LSB implementation as-is. The trick is to map the keys onto the unsigned integers.\n\nI'll use the term key-derivation for this, because we're not only extracting different parts of a key; using this function to manipulate the key in different ways to derive a sort key is how we're able to handle signed integers, floating point keys, and chose a sort order.\n\nThe natural ordering for the unsigned integers is in ascending order. If we instead want to sort in descending (reverse) order, simply have the key-derivation function return the bitwise inverse (or complement) of the key:\n\nNote that for like-keys, the first record in the forward-direction of the input will also be the first record in the output. If this is not what you want, I suggest using a standard ascending sort and reading the result backwards, which obviously will give you the last like-key record first.\n\nTo treat the key as a signed integer, we need to manipulate the sign-bit, which is the top-most 'most significant' bit, since by default this is set for negative numbers, meaning they would appear at the end of the result. Using the xor operator we flip the top bit, which neatly solves the problem:\n\nThese can be combined to handle signed integers in descending order:\n\nTo sort IEEE 754 single-precision (32-bit) floats (a.k.a binary32) in their natural order we need to invert the key if the sign-bit is set, else we need to flip the sign bit. (via Radix Tricks):\n\nThis looks complex, but the left side of the parenthetical converts a set sign-bit to an all-set bitmask (-1 equals ~0) which causes the to invert the whole key. The second expression in the parenthetical (after the bitwise or) sets the sign bit, which is a no-op if it was already set, but otherwise ensures that the flips the sign-bit only.\n\nAs an implementation detail for C and C++, is the floating point key reinterpreted as an unsigned 32-bit integer to allow the bit-manipulation. This sort of type-punning, if done naively, can be considered undefined behaviour (UB). Post C++20 you should have the option to use to do this in a well-defined way, but without that option we instead use into a local temporary. This pattern is recognized by compilers, but as always you should inspect the generated code to make sure.\n\nSo yes, you can sort with NaNs. Isn't life great?\n\nAll of these of course extends naturally to 64-bit keys, just change the shifts from 31 to 63 and adjust the types involved accordingly.\n\nIn this section I'll talk about some optimizations that I have tried, or that may be worth investigating.\n\nAt this point it must be noted that none of the code in this repository even attempts to defeat side-channel leaks, and some of the following optimizations add data-dependencies that will definitely open the door for timing attacks should you apply them in a cryptographic context.\n\nI have observed a few different radix sort implementations, and some of them have a larger fixed overhead than others. This gives rise to the idea of hybrid sorts, where you redirect small workloads (e.g N < 100) to say an insertion sort. This is often a part of MSB radix sorts.\n\nIt's also possible to do one MSB pass and then LSB sort the sub-results. This saves memory and memory management from doing all MSB passes, and should improve cache locality for the LSB passes. In the one implementation I've tried, the fixed overhead was quite high. (TODO: experiment/determine exact reason)\n\nSince we have to scan once through the input to build our histograms, it's relatively easy to add code there to detect if the input is already sorted/reverse-sorted, and special case that.\n\nOne way to implement it is to initialize a variable to the number of elements to be sorted, and decrement it every time the current element is less-than-or-equal to the next one, care of bounds. After a pass through the input, you would have a count of the number of elements that need to be sorted. If this number is less than two, you're done.\n\nIf there's a user-defined key-derivation function, apply it to the values being compared.\n\nPushing further, say trying to detect already sorted columns, didn't seem worth the effort in my experiments. You don't want to add too many conditionals to the histogram loop.\n\nIf every key has the same value for a key-radix aligned portion, then the sort loop for that column will simply be a copy from one buffer to another, which is a waste.\n\nFortunately detecting a 'trivial pass' is easy, and does not -- as is sometimes shown -- require a loop over the histogram looking for a non-zero count and checking if it's equal the number of keys being sorted.\n\nInstead we can simply sample any key, and check the corresponding histogram entries directly.\n\nWe take the first key (any will do), extract all the subkeys using whatever key-radix we're using, and then we directly probe those entries in the histogram(s). No searching required.\n\nIff the corresponding count is equal to the number of keys to be sorted, all the values in the column are the same and it can be skipped.\n\nNotice how you could sample any key, and the result would be the same.\n\nUnlikely to be a win in the common case, but as a potential micro-optimization you could any two keys and ignore histograms for which the combined bits are not all zero.\n\nSkipping columns has the side-effect of the final sorted result ending up in either the original input buffer or the auxiliary buffer, depending on whether you sorted an even or odd number of columns. I prefer to have the sort function return the pointer to the result, rather than add a copy step.\n\nThis section is under development, speculative, and has not been implemented\n\nThe opportunity for column-skipping is fast and easy to detect, and efficient when activated, but suffers from limited opportunity in practice due to the requirement that every input entry has the same value in the column. The higher the radix (wider column), the fewer opportunities for skipping.\n\nIf we reduced the column width to one single bit, then any column with all 0s or all 1s could be skipped, at the cost of having to do key-width number of passes worst-case.\n\nThe insight here is that we only care about columns with mixed values, so we should never have to do more passes than there are such columns. The problem is that we do not want to use radix-1 because looking at a single bit at a time is not efficient in practice.\n\nHowever, this gives rise to the following idea; we should be able to re-arrange the bit-columns in the key, as long as we preserve the relative order of the underlying sort keys, such that we maximize opportunity for column skipping.\n\nIn other words, if we know which 1-bit columns are relevant, i.e those which are not all the same bit-value, then we could partition the keys such that all relevant columns move to towards the LSB, and all irrelevant towards the MSB, and as long as we preserve the relative order of relevant columns, this should create opportunity for a wider radix to skip columns at the MSB end of the key.\n\nCalculating a mask of which columns are relevant can be done efficiently in one pass using simple bit-operations. We already do this pass to generate histograms, so little performance is lost here.\n\nThe mask tells us the maximum amount of bit-compaction that can be achieved, and so we can decide to skip it if it wouldn't expose any whole radix-N columns.\n\nOne basic first approach would be to use the mask to shift out/ignore low bits that are the same, as long as we can shift enough that a full column becomes available for skipping.\n\nMore optimally, the mask can be used together with a parallel bit-extraction instruction, where available, to pack the keys. On AMD64 this instruction is called and is part of the BMI2 instruction set.\n\nThis is an active area of research.\n\nUnlike a typical implementation, which would likely repeatedly call a counting sort and therefore need as many histogramming passes as there are sorting passes, the idea of pre-calculating the histograms for multiple sorting passes at once comes naturally when writing the radix sort in the unrolled form.\n\nReducing the size of the histograms by using the smallest counter type possible will have a positive performance impact due to better cache utilization. If you can get away with 32-bit or even 16-bit counters, it will almost certainly pay off. In an experiment, halving the histogram size by changing the counter type from to improved performance when sorting N<100 items by ~33%. This effect fades out as the input size grows in proportion to the histograms.\n\nIt's possible to do the histogramming for the next pass within the sort loop of the current pass, reducing the memory footprint to two histogram buffers in the unrolled form.\n\nEASTL, while having a conventional outer sorting loop, uses this fusing approach anyway, noting that it \"avoids memory traffic / cache pressure of reading keys in a separate operation\".\n\nUsing multiples of eight bits for the radix is convenient, but not required. If we do, we limit ourselves to 8-bit or 16-bit wide radixes in practice. Using an intermediate size such as 11-bits with three passes saves us one pass for every 32-bits of key and may fit a low-level cache (L1/L2) better, but also makes it less likely for the column skipping to kick in, and adds a few extra masking operations.\n\nGoing narrower could allow us to skip more columns in common workloads. There's definitely room for experimentation in this area, maybe even trying non-uniformly wide radixes (8-16-8) or dynamic selection of radix widths. If the data you're sorting isn't random, adapting the radix to the data may yield positive results, e.g if you know you are sorting floats in the range -1 to +1, or only care about a certain number of bits of precision, then this may be exploitable.\n\nThat said, in my limited initial testing, neither 4- nor 11-bit wide radix showed any promise at all in sorting random 32-bit integers. Quite the contrary, both were bad across the board.\n\nThe 4-bit version never had much going for it, so no big surprise there. Perhaps if you have to sort on a tiny microprocessor it may be worth going down in radix, but on a modern \"big CPU\" I'm confident saying it is not.\n\nThe 11-bit version on the other hand was surprisingly bad, and was never even close to beating the standard 8-bit radix implementation, even when reducing the counter width down to 32-bits to compensate for the larger number of buckets. This variant is one you sometimes see in the wild, and I can guarantee you that it's almost certainly a pessimization over just using octets.\n\nMy current interpretation as to why, is that increasing L1D cache pressure with larger histograms is actually detrimental to overall performance.\n\nThere is perhaps a world with bigger caches and faster memory where going wider is better, but for now it seems eight bits reigns supreme.\n\nInstead of applying the key-derivation function on each access, you could parameterize the sort on two functions; One to rewrite the input buffer into the desired key-derivation, and the other to transform the final output buffer back to the original form.\n\nThe savings are supposed to come from not having to apply the key-derivation function to each key during the intermediate sorting passes. In my testing a performance increase from this micro-optimization -- rewriting in the histogramming loop and the last sorting pass -- could not be measured even at 40 million keys. It's possible that this is worth it on less capable hardware.\n\nIt's also possible to change the underlying code instead of manipulating the key. You can reverse the sort-order by reversing the prefix sum, or simply reading the final result backwards if possible.\n\nThe 2000-era Radix Sort Revisited presents a direct way to change the code to handle floats.\n\nWorking primarily on IA-32 and AMD64/x86-64 CPUs, I've never had a good experience with adding manual prefetching (i.e ).\n\nMichael Herf reports a \"25% speedup\" from adding calls to his code that was running on a Pentium 3.\n\nI'll put this in the TBD column for now.\n\nSee \"Prefix Sum with SIMD\" for various approaches to using SIMD (AVX2 specifically) to calculate prefix sums.\n\nThere are optimized histogram functions out there, and with newer instruction set extensions it certainly seems more viable, but I have yet to personally explore this space.\n\nFor other architectures this may be more or less viable.\n\nTODO: This code is very much a work in progress, used to test various techniques, and does NOT represent a final 'product'.\n\nBy default we build an executable called . This is a test harness of sorts, with some options to let you test different setups.\n\nThe first argument is the number of integers to sort (from ), with zero meaning all:\n\nTo sort all the available data as unsigned 16-bit integers using for memory allocation, and forcing a column skip:\n\nThe second argument is a flag controlling the use of to map the input file and allocate memory. Defaults to .\n\nThe third argument is a flag that controls the use of hugepages, i.e very large memory pages. The presence of this feature is not guaranteed, and allocation may fail on some systems when used in combination with . Defaults to .\n\nThe fourth argument is the , i.e, what type to interpret the input file as an array of. Defaults to .\n\nThe fifth argument is a mask expressed in hexadecimal that will be bitwise -ed against the input before it's sorted. This can be used to demonstrate the column-skipping functionality, e.g by passing 0x00FFFFFF the MSD column should be skipped. Defaults to no masking.\n\nThe Make target will build and run a benchmark comparing the (WIP) C++ implementation against and stdlib .\n\nThis code is built on top of and requires the prior installation of the Google benchmark microbenchmark support library, which is available as on Debian and Ubuntu.\n\nIt's not worth talking about specific numbers at this point in time, but some general notes are in order.\n\nYou will notice that there's a cut-off point, usually after a few hundred or so keys, where radix sort starts beating the other algorithms. This is due to the overhead of initializing, generating and accessing histograms. We can always implement a hybrid which switches to a sort that handles small inputs better, giving us the best of both worlds.\n\nThe cost of allocating the auxiliary sorting buffer is not included in the timings by default. If you care about that the benchmark can be easily updated to move the aux buffer allocation into the benchmark loop. In practice, repeated allocation and deallocation of the same-sized block is likely to be immaterial, especially when there is little other pressure on the memory allocator during the benchmark.\n\nBecause we're sorting random data, the column-skipping optimization and pre-sorted detection is extremely unlikely to kick in, so while the benchmark is realistic, it is by no means a best-case scenario.\n\nThe main downside to out-of-place implementations is that the natural interface is different from \"regular\" in-place sorts. This becomes a problem when you need to conform to pre-existing interfaces in e.g programming language standard libraries and tools. You can work around this in various ways, but this either sacrifices performance, or complicates the implementation. Take the hit of allocating and deallocating the auxilary storage every call, or keep a lazily or pre-allocated, and possibly growing, block of memory around that can be reused.\n\nThey are also, as a general rule, not great for very small inputs.\n\nConsider what it would mean if in a counting sort, instead of a typical 32- or 64-bit counter, we only had one bit per integer.\n\nInstead of an array of counters, we have a bitmap. Instead of increasing a counter each time we see a value, we simply mark it off as present in the bitmap.\n\nThis would only work if there were zero or one occurrence of each possible input value. More correctly; it would only work if we're okay with removing any duplicates in the input, leaving only unique values.\n\nSince we're using a single bit per value we have extend the magnitude of keys we can handle per unit memory by a factor of 32 or 64. We can now go up to 24-bit keys in only 2MiB memory.\n\nJon Bentley originally presented a scenario where this approach could be useful in his book Programming Pearls, 2nd ed, section 1.2, and his Dr.Dobb's column Algorithm Alley."
    },
    {
        "link": "https://cboard.cprogramming.com/c-programming/178915-binary-radix-sort.html",
        "document": ""
    }
]