[
    {
        "link": "https://medium.com/@zhaodongyu/optimize-sgemm-on-risc-v-platform-b0098630b444",
        "document": "The related code is located in ./prepare/ .\n\nI use the Allwinner Nezha D1 development board, and downloaded the cross-compilation link from here.\n\nFor detailed instructions, please refer to the readme.\n\nI conducted memory bandwidth tests on the development board using the following projects:\n\nRoofline proposes a method for quantitative analysis using “Operational Intensity” and provides a formula for the theoretical performance limit achievable on computational platforms.\n\nAccording to OpenPPL Public Course | RISC-V Technical Analysis:\n• The computing power of D1 can reach 4 GFlops (@ 1GHz).\n\nAlthough I measured the highest as 2.592 GB/s, there may be some problems somewhere? Let’s trust Sensetime for now, temporarily accept its value.\n\nTake step0 as an example, you need to edit the Makefile first to configure your cross-compilation chain.\n\nThis version seems to be the most intuitive to me, after all, this is how I learned, understood, and computed matrix multiplication:\n\nI think version 0 very well explains the formula $C_{mn} = \\sum_{k=1}^{K} A_{mk}B_{kn}$.\n\nHowever, this version has obvious shortcomings: on a platform with a theoretical computing power of 4 GFLOPS, it only achieves a maximum computational performance of 0.03 GFLOPS. This is because the access to matrix B has a very low cache hit rate, i.e., “poor spatial locality”. Throughout the calculation, it is equivalent to accessing matrix B many, many times.\n\nIt is advisable to access the elements of multi-dimensional arrays in sequential order. This can improve the spatial locality of memory access and make it more friendly to the cache.\n\nFurthermore, it can be observed that with the increase in size, the performance fluctuates significantly. Analysis of the data shows that when m=n=k is 128 164 192 228 256 288 320 352 384, the performance is poor. These numbers differ by 32, and 32 * 4 (sizeof(float)) = 128 B.\n\nIt is speculated that the performance fluctuation is related to cacheline and hardware prefetching — cacheline = 64B, after cache miss, hardware prefetching, i.e., HWPrefetcher, reads one more cacheline.\n\nReusing data in the cache is the most basic and efficient use of cache. For nested loops, loop interchange, loop reversal, loop fusion, loop distribution, loop tiling, loop unrolling and jam, etc., can be used to improve program performance.\n\nSelecting an appropriate loop transformation method can both maintain the semantics of the program and improve its performance.\n\nCompared with version 0, version 1 has better spatial locality for the operation on matrix B, and the performance has been greatly improved (especially for larger sizes, while for m = n = k <= 68, the efficiency of version 0 is higher).\n\nAdjusting the order of m, n, and k does not affect the result (i.e., maintaining the semantics of the program), but it can affect the performance.\n\nTesting the performance of different loop orders (using the Allwinner Nezha D1 platform, with m=n=k=512 as an example)\n\nHowever, the hardware utilization of version 1 is still very low, and further optimizations are needed.\n\nTo avoid unnecessary cache swapping, blocking processing is performed. Discussing Why Blocking Matrix Optimization Works is a good read, I recommend learning from it.\n\nAfter performing block operations in version 2, the performance is still not satisfactory. This is because, although this version superficially implements blocking logic, there are still some small tricks in the calculation within the block that have not been applied.\n\nSeveral tricks are mentioned in BLISlab-tutorial:\n\nAfter using these tricks, this version has significantly improved performance!\n\nHowever, for larger matrix sizes, the performance of this version is still relatively low. Upon investigation, for example, after accessing B[0,0], B[0,1], B[0,2], B[0,3], when accessing B[1,0], when the size is large, there must be a cache miss. Therefore, it would be great if the data could be rearranged in advance.\n\nI assume matrix B is parameter, so we can perform the pack operation in advance. Version 4 prepack matrix B, leading to further performance improvement!\n\nThe reason for the performance improvement is evident: there is a significant reduction in cache misses when accessing matrix B. This is the first time I deeply realized the importance of prepacking neural network weights before model inference.\n\nIt can be seen that when the size is relatively large, the performance still declines. This should be due to a high number of cache misses when accessing matrix A. Should we pack A?\n\nI assume matrix A is input, so packing A cannot be done in advance and must be included in the overall timing. Is it necessary?\n\nHere, since matrix A is assumed to be an input, packing A needs to be performed during computation, and this time consumption needs to be included in the timing.\n\nThe results are still pleasing, especially with large matrix sizes, achieving further performance improvements.\n\nI initially approached this experiment with a trial-and-error mindset, considering the additional read of A and writing of packA. It seems the main challenge ahead lies in combating cache misses.\n\nThe current optimization direction has reached its limit. It’s worth trying to do some preload during the calculation process.\n\nNext, we’ll move to assembly, work on vector calculations, and implement preload in assembly.\n\nBrief explanation: A is not packed, but B is prepacked with 16 numbers.\n\nRegarding the use of rvv instructions, I believe vsetvli is essential, and vfmacc.vf is the mainstay.\n\nI have learned a lot from OpenPPL Course | RISC-V Technical Analysis. They are truly professional! I recommend learning theoretical guidance and knowledge points from them, paying tribute to OpenPPL!\n\nAs for assembly operators, there are many details in assembly, and I strongly complain: writing assembly is really annoying! Especially the debugging process, it’s torturous. The last time I wrote assembly was during my undergraduate classes. Picking it up again brings some novelty and excitement, and being able to control the execution of operators at a very fine granularity gives a great sense of accomplishment.\n\nRegarding how the assembly files are implemented specifically, I believe the fastest way is to look at the assembly code. I won’t explain it further here.\n\nIt should be noted that this version’s performance is very poor. Why is that? It’s another issue of loop order.\n\nBrief explanation: A is not packed, but B is prepacked with 16 numbers.\n\nReversing the order of loops, starting with the n-direction followed by the m-direction, significantly improves performance.\n\nHowever, the performance of large-sized matrices is still not very good. The root cause remains in memory access. The computation of large-sized matrices in the roofline model is considered compute-bound, where ideally the compute time and memory access time should overlap as much as possible. Currently, a significant amount of time is spent on memory access (mostly due to cache miss!).\n\nBrief Explanation: Matrix A is not packed, while Matrix B undergoes prepackaging of 16 elements and includes a preload operation.\n\nThe performance is explosive! It reaches a maximum of 2.212 GFLOPS.\n\nInserting some load operations between vfmacc.vf instructions preloads the data that will be used later into the cache, significantly reducing cache miss.\n\nInitially, I was puzzled — how can the compute time and memory access time overlap when the code seems to execute sequentially? It wasn’t until later that I understood the essence here, which lies in the principle of cacheline. Indeed, foundational knowledge is crucial!\n\nBased on previous experience, an attempt was made to pack Matrix A, but surprisingly, the results were not very good. A brief analysis suggests that the preload for Matrix A in this version of the assembly code might not be optimized.\n\nIn the previous version, although A wasn’t packed, there was preload for A’s 4 rows, which also addressed the pain point of cache miss for Matrix A."
    },
    {
        "link": "https://codasip.com/2024/03/20/a-custom-risc-v-vector-instruction",
        "document": "A novel AI-acceleration paper presents a method to optimize sparse matrix multiplication for machine learning models, particularly focusing on structured sparsity. Structured sparsity involves a predefined pattern of zero values in the matrix, unlike unstructured sparsity where zeros can occur anywhere. The research was conducted by Democritus University of Thrace (DUTH) in Greece and was sponsored by Codasip University Program.\n\nStructured sparsity has emerged as a promising approach to streamline the complexity of modern Machine Learning (ML) applications and facilitate the handling of sparse data in hardware. Accelerating ML models, whether for training or inference, heavily relies on efficient execution of equivalent matrix multiplications, which are often performed on vector processors or custom matrix engines.\n\nThe aim of this study was to integrate the simplicity of structured sparsity into existing vector execution flow and vector processing units (VPUs), thus expediting the corresponding matrix multiplications with minimal redesign in mind. To achieve this goal, a novel vector index-multiply-accumulate instruction is introduced. This instruction facilitates low-cost indirect reads from the vector register file, thereby reducing unnecessary memory traffic and enhancing data locality.\n• Vectorized sparse matrix multiplication: DUTH adapted a vectorized row-based matrix multiplication algorithm to handle structured sparse data. This involved loading only the nonzero elements and their column indexes from one sparse matrix and selecting corresponding rows from the other matrix for multiplication.\n• Introduction of a new instruction: They introduced a new vector instruction called vindexmac (vector index-multiply-accumulate) to optimize the computation from point 1. This instruction allows for low-cost indirect reads from the vector register file, reducing unnecessary memory traffic and instruction overhead.\n• Hardware implementation: The proposed vector instruction is integrated into a high-end decoupled RISC-V vector processor at minimal hardware cost. Evaluation using Gem5 simulation demonstrated significant speedups when executing Convolutional Neural Networks (CNNs) pruned for structured sparsity.\n• Optimization for local data usage: The researchers preloaded tiles of one of the matrices into the vector register file to improve data locality and reduce memory traffic during computation. This optimization exploits the regular structure of the sparse matrix A to determine which rows of B need to be preloaded.\n\nThere are various approaches to implementing vectorized matrix multiplications with sparse data. One particularly effective method is the row-wise approach, also referred to as Gustavson’s algorithm, which has demonstrated high efficiency in calculating the matrix product A × B. The product of the matrix multiplication is produced row-wise, as follows:\n\nYou vector-load the first row of A, and then load all rows of B. Each row of B is multiplied with one of the elements of the loaded row of A. The results are accumulated row-wise in C. To avoid multiplying the zero elements the following algorithm can be used with which you only load the rows of B that correspond to non-zero columns of A:\n\nThe structured-sparse format of matrix A helps in eliminating unnecessary multiplications in sparse-dense multiplication. However, a critical bottleneck arises from the still relatively frequent vector loads from memory for the rows of matrix B, as indicated in lines 8 and 9 of Algorithm 2. To address this challenge, we can exploit the structured sparsity of matrix A to minimize memory traffic and enable computations to utilize local data already present in the vector register file.\n\nThe vindexmac instruction streamlines the process by replacing the three instructions, including a vector load from memory, from lines 8 to 10 of Algorithm 2. Consequently, the need for vector loading from memory is eliminated with the caveat of needing to preload a tile of matrix B. Furthermore, the vindexmac instruction efficiently computes a vector containing partial results corresponding to a row of the result matrix C.\n\nSince B is preloaded and accessing it is done through the vector register file reads, we can see in Algorithm 3 that the number of times B rows are loaded is now outside of the inner for-loop, meaning much less loads overall. As the number of zero elements of both A and B is known beforehand and is done in a very structured manner, it means this preloading and implementing the new instruction can be done very efficiently.\n\nThis is just a very brief and high-level overview of the technical details and more can be inferred from the actual paper.\n\nThe newly proposed instruction has been seamlessly integrated into a decoupled RISC-V vector processor with minimal hardware overhead. Extensive evaluations have showcased substantial speedups ranging from 1.80× to 2.14× compared to state-of-the-art vectorized kernels. These improvements are particularly pronounced when executing layers with varying degrees of sparsity sourced from cutting-edge Convolutional Neural Networks (CNNs).\n\nAdditionally, optimizations regarding the sparse data placement, across the scalar and vector register files, are planned for future work.\n\nSoon, this new instruction will be integrated into a VPU extended Codasip L31 core using Codasip Studio, further enhancing the capabilities of the vector processor for sparse matrix multiplication tasks.\n\nThe collaboration between DUTH University and Codasip’s University showcases how industry tools and sponsorship can help drive academic research and development of novel and state-of-the-art solutions in processor and system design for AI inference acceleration and in turn how new ideas can be integrated into existing industry products."
    },
    {
        "link": "https://github.com/XUANTIE-RV/riscv-matrix-extension-spec",
        "document": "This is a matrix extension proposal for AI applications under RISC-V architecture. The extension is currently at version 0.6.0. Compared to the last version, there are following updates:\n• Separated source and accumulation registers\n• The source and accumulation registers are separated to tile register and accumulation register, supporting source and destination registers of different sizes;\n• More flexible register shape\n• The number of rows and columns in the matrix registers is adjustable, no longer limited to rows = RLEN/32. By adjusting the number of rows and columns, different sizes of matrix multiplication can be formed, achieving full coverage from pure outer products to pure inner products;\n• More Element-wise instruction\n• New element-wise instructions have been added to facilitate operator fusion;\n\nThe extension is still under construction, and this is a preview demo project. Some key directories are shown below.\n\nSeveral demos in binaries are provided to evaluate RISC-V Matrix Extension's performance. You can quickly start it with the following instructions.\n\nPlease refer to the demos/README for details.\n\nThis project is built using AsciiDoctor (Ruby). The repository has been setup to build the PDF on checkin using GitHub actions. Workflow dependencies are located in the dependencies directory.\n\nFor more information on AsciiDoctor, specification guidelines, or building locally, see the RISC-V Documentation Developer Guide.\n\nRISC-V Matrix Extension Specification is kept in ./spec. The recommended method for building the PDF files is to use the Docker Image, as described in the RISC-V Docs Base Container Image repository.\n\nUser guide and reference manual for RISC-V Matrix Extension tools are kept in ./doc.\n\nThe final documents form of PDF can be generated using the command under corresponding folder. The generation method of each document is as follows.\n\nCompile and install qemu. Please refer to the Xuantie qemu project for details.\n\nGet your own case and compile into matrix.elf. Both intrinsic and nn libraries can be used to perform this step. Please refer to T-HEAD GNU Compiler Toolchain or HHB and SHL for details.\n\nEvaluation matrix performance on qemu with RISC-V Matrix Extension(with vector length set to VLEN and matrix length set to RLEN)"
    },
    {
        "link": "https://stackoverflow.com/questions/64520575/efficient-matrix-multiplication-in-risc-v",
        "document": "I am struggling with matrix multiplication in RISC-V\n\nThe goal is to reduce the clock cycles\n• Below 20,000,000 cycles (2%)\n• Below 18,000,000 cycles (2%)\n• Below 16,000,000 cycles (2%)\n• Below 14,000,000 cycles (2%)\n• Below 12,000,000 cycles (2%)\n• Below 10,000,000 cycles (1%)\n• Below 9,000,000 cycles (1%)\n• Below 8,000,000 cycles (1%)\n• Below 7,000,000 cycles (1%)\n• Below 6,000,000 cycles (1%)\n\nand bruteforce in C with O3 is 16M\n\nI know there's the a better algorithm to do matrix multiplication o(n^3)\n\nbut here n is only 128, I wonder if there's other way to reduce cycles by the O(n^3) algo\n\nis there anything can I do to boost my code?"
    },
    {
        "link": "https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?article=8901&context=etd_theses",
        "document": ""
    },
    {
        "link": "https://shakti.org.in/docs/risc-v-asm-manual.pdf",
        "document": ""
    },
    {
        "link": "https://five-embeddev.com/riscv-user-isa-manual/Priv-v1.12/f.html",
        "document": "This chapter describes the standard instruction-set extension for single-precision floating-point, which is named “F” and adds single-precision floating-point computational instructions compliant with the IEEE 754-2008 arithmetic standard [ieee754-2008]. The F extension depends on the “Zicsr” extension for control and status register access.\n\nThe F extension adds 32 floating-point registers, – , each 32 bits wide, and a floating-point control and status register , which contains the operating mode and exception status of the floating-point unit. This additional state is shown in Figure 1.1. We use the term FLEN to describe the width of the floating-point registers in the RISC-V ISA, and FLEN=32 for the F single-precision floating-point extension. Most floating-point instructions operate on values in the floating-point register file. Floating-point load and store instructions transfer floating-point values between registers and memory. Instructions to transfer values to and from the integer register file are also provided.\n\nThe floating-point control and status register, , is a RISC-V control and status register (CSR). It is a 32-bit read/write register that selects the dynamic rounding mode for floating-point arithmetic operations and holds the accrued exception flags, as shown in Figure 1.2. The register can be read and written with the FRCSR and FSCSR instructions, which are assembler pseudoinstructions built on the underlying CSR access instructions. FRCSR reads by copying it into integer register rd. FSCSR swaps the value in by copying the original value into integer register rd, and then writing a new value obtained from integer register rs1 into . The fields within the can also be accessed individually through different CSR addresses, and separate assembler pseudoinstructions are defined for these accesses. The FRRM instruction reads the Rounding Mode field and copies it into the least-significant three bits of integer register rd, with zero in all other bits. FSRM swaps the value in by copying the original value into integer register rd, and then writing a new value obtained from the three least-significant bits of integer register rs1 into . FRFLAGS and FSFLAGS are defined analogously for the Accrued Exception Flags field . Bits 31–8 of the are reserved for other standard extensions. If these extensions are not present, implementations shall ignore writes to these bits and supply a zero value when read. Standard software should preserve the contents of these bits. Floating-point operations use either a static rounding mode encoded in the instruction, or a dynamic rounding mode held in . Rounding modes are encoded as shown in Table 1.3. A value of 111 in the instruction’s rm field selects the dynamic rounding mode held in . The behavior of floating-point instructions that depend on rounding mode when executed with a reserved rounding mode is reserved, including both static reserved rounding modes (101–110) and dynamic reserved rounding modes (101–111). Some instructions, including widening conversions, have the rm field but are nevertheless mathematically unaffected by the rounding mode; software should set their rm field to RNE (000) but implementations must treat the rm field as usual (in particular, with regard to decoding legal vs. reserved encodings). The accrued exception flags indicate the exception conditions that have arisen on any floating-point arithmetic instruction since the field was last reset by software, as shown in Table 1.4. The base RISC-V ISA does not support generating a trap on the setting of a floating-point exception flag.\n\nFloating-point loads and stores use the same base+offset addressing mode as the integer base ISAs, with a base address in register rs1 and a 12-bit signed byte offset. The FLW instruction loads a single-precision floating-point value from memory into floating-point register rd. FSW stores a single-precision value from floating-point register rs2 to memory. FLW and FSW are only guaranteed to execute atomically if the effective address is naturally aligned. FLW and FSW do not modify the bits being transferred; in particular, the payloads of non-canonical NaNs are preserved. As described in Section [sec:rv32:ldst], the EEI defines whether misaligned floating-point loads and stores are handled invisibly or raise a contained or fatal trap.\n\nFloating-point arithmetic instructions with one or two source operands use the R-type format with the OP-FP major opcode. FADD.S and FMUL.S perform single-precision floating-point addition and multiplication respectively, between rs1 and rs2. FSUB.S performs the single-precision floating-point subtraction of rs2 from rs1. FDIV.S performs the single-precision floating-point division of rs1 by rs2. FSQRT.S computes the square root of rs1. In each case, the result is written to rd. The 2-bit floating-point format field fmt is encoded as shown in Table [tab:fmt]. It is set to S (00) for all instructions in the F extension. All floating-point operations that perform rounding can select the rounding mode using the rm field with the encoding shown in Table 1.3. Floating-point minimum-number and maximum-number instructions FMIN.S and FMAX.S write, respectively, the smaller or larger of rs1 and rs2 to rd. For the purposes of these instructions only, the value − 0.0 is considered to be less than the value + 0.0. If both inputs are NaNs, the result is the canonical NaN. If only one operand is a NaN, the result is the non-NaN operand. Signaling NaN inputs set the invalid operation exception flag, even when the result is not NaN. Floating-point fused multiply-add instructions require a new standard instruction format. R4-type instructions specify three source registers (rs1, rs2, and rs3) and a destination register (rd). This format is only used by the floating-point fused multiply-add instructions. FMADD.S multiplies the values in rs1 and rs2, adds the value in rs3, and writes the final result to rd. FMADD.S computes (rs1×rs2)+rs3. FMSUB.S multiplies the values in rs1 and rs2, subtracts the value in rs3, and writes the final result to rd. FMSUB.S computes (rs1×rs2)-rs3. FNMSUB.S multiplies the values in rs1 and rs2, negates the product, adds the value in rs3, and writes the final result to rd. FNMSUB.S computes -(rs1×rs2)+rs3. FNMADD.S multiplies the values in rs1 and rs2, negates the product, subtracts the value in rs3, and writes the final result to rd. FNMADD.S computes -(rs1×rs2)-rs3. The fused multiply-add instructions must set the invalid operation exception flag when the multiplicands are ∞ and zero, even when the addend is a quiet NaN.\n\nFloating-point-to-integer and integer-to-floating-point conversion instructions are encoded in the OP-FP major opcode space. FCVT.W.S or FCVT.L.S converts a floating-point number in floating-point register rs1 to a signed 32-bit or 64-bit integer, respectively, in integer register rd. FCVT.S.W or FCVT.S.L converts a 32-bit or 64-bit signed integer, respectively, in integer register rs1 into a floating-point number in floating-point register rd. FCVT.WU.S, FCVT.LU.S, FCVT.S.WU, and FCVT.S.LU variants convert to or from unsigned integer values. For XLEN > 32, FCVT.W[U].S sign-extends the 32-bit result to the destination register width. FCVT.L[U].S and FCVT.S.L[U] are RV64-only instructions. If the rounded result is not representable in the destination format, it is clipped to the nearest value and the invalid flag is set. Table [tab:int_conv] gives the range of valid inputs for FCVT.int.S and the behavior for invalid inputs. All floating-point to integer and integer to floating-point conversion instructions round according to the rm field. A floating-point register can be initialized to floating-point positive zero using FCVT.S.W rd, , which will never set any exception flags. All floating-point conversion instructions set the Inexact exception flag if the rounded result differs from the operand value and the Invalid exception flag is not set. Floating-point to floating-point sign-injection instructions, FSGNJ.S, FSGNJN.S, and FSGNJX.S, produce a result that takes all bits except the sign bit from rs1. For FSGNJ, the result’s sign bit is rs2’s sign bit; for FSGNJN, the result’s sign bit is the opposite of rs2’s sign bit; and for FSGNJX, the sign bit is the XOR of the sign bits of rs1 and rs2. Sign-injection instructions do not set floating-point exception flags, nor do they canonicalize NaNs. Note, FSGNJ.S rx, ry, ry moves ry to rx (assembler pseudoinstruction FMV.S rx, ry); FSGNJN.S rx, ry, ry moves the negation of ry to rx (assembler pseudoinstruction FNEG.S rx, ry); and FSGNJX.S rx, ry, ry moves the absolute value of ry to rx (assembler pseudoinstruction FABS.S rx, ry). Instructions are provided to move bit patterns between the floating-point and integer registers. FMV.X.W moves the single-precision value in floating-point register rs1 represented in IEEE 754-2008 encoding to the lower 32 bits of integer register rd. The bits are not modified in the transfer, and in particular, the payloads of non-canonical NaNs are preserved. For RV64, the higher 32 bits of the destination register are filled with copies of the floating-point number’s sign bit. FMV.W.X moves the single-precision value encoded in IEEE 754-2008 standard encoding from the lower 32 bits of integer register rs1 to the floating-point register rd. The bits are not modified in the transfer, and in particular, the payloads of non-canonical NaNs are preserved.\n\nThe FCLASS.S instruction examines the value in floating-point register rs1 and writes to integer register rd a 10-bit mask that indicates the class of the floating-point number. The format of the mask is described in Table [tab:fclass]. The corresponding bit in rd will be set if the property is true and clear otherwise. All other bits in rd are cleared. Note that exactly one bit in rd will be set. FCLASS.S does not set the floating-point exception flags."
    },
    {
        "link": "https://skaminsky115.github.io/teaching/cs61c/resources/fa19/disc03.pdf",
        "document": ""
    },
    {
        "link": "https://cse.iitd.ac.in/~srsarangi/archbook/chapters/riscv.pdf",
        "document": ""
    },
    {
        "link": "https://content.riscv.org/wp-content/uploads/2019/06/riscv-spec.pdf",
        "document": ""
    }
]