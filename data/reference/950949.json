[
    {
        "link": "https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-example",
        "document": "This topic provides a series of examples that illustrate how to use the Snowflake Connector to perform standard Snowflake operations such as user login, database and table creation, warehouse creation, data insertion/loading, and querying.\n\nThe sample code at the end of this topic combines the examples into a single, working Python program.\n\nSnowflake Connector for Python version 3.14.0 introduced the connection parameter that specifies how the connector should set file permissions when downloading files for a Snowflake stage with the GET command. These files are always owned by the same user who runs the Python process. By default, the parameter is to provide a more secure and strict file permission, which means that only the owner has read and write permissions of the downloaded files. Other groups and users have no permissions for the files downloaded with the GET command. If your organization requires less restrictive file permissions for the files, you can set the parameter to . Enabling this parameter sets the file permissions for the files downloaded from a stage to , which allows the owner to read and write the files, but allows others only to read them. This setting might be necessary, for example, for some ETL tools that run under a different system user who needs to be able to read and process the downloaded files. If you are unsure of which value to use, consult with the team responsible for your organization’s applicable security policy.\n\nTo specify values to be used in a SQL statement, you can include literals in the statement, or you can bind variables. When you bind variables, you put one or more placeholders in the text of the SQL statement, and then specify the variable (the value to be used) for each placeholder. The following example contrasts the use of literals and binding: There is an upper limit to the size of data that you can bind, or that you can combine in a batch. For details, see Limits on Query Text Size. Snowflake supports the following types of binding:\n• None and , which bind data on the client.\n• None and , which bind data on the server. Each of these is explained below. Both binding and binding bind data on the client side rather than on the server side. By default, the Snowflake Connector for Python supports both and , so you can use or as the placeholder. For example:\n• None Using as the placeholder:\n• None Using as the placeholder: With and , you can also use a list object to bind data for the IN operator: The percent character (“%”) is both a wildcard character for SQL LIKE and a format binding character for Python. If you use format binding, and if your SQL command contains the percent character, you might need to escape the percent character. For example, if your SQL statement is: then your Python code should look like the following (note the extra percent sign to escape the original percent sign): Both binding and binding bind data on the server side rather than on the client side:\n• None For binding, use a question mark character ( ) to indicate where in the string you want a variable’s value inserted.\n• None For binding, use a colon ( ) followed by a number to indicate the position of the variable that you want substituted at that position. For example, specifies the second variable. Use numeric binding to bind the same value more than once in the same query. For example, if you have a long VARCHAR or BINARY or semi-structured value that you want to use more than once, then binding allows you to send the value to the server once and use it multiple times. The next sections explain how to use and binding:\n• None Using qmark or numeric binding with datetime objects\n• None Using bind variables with the IN operator To use or style binding, you can either execute one of the following or set as part of the connection parameters when calling . If you set to or , you must use or (where is replaced with a number) as the placeholders, respectively.\n• None Using as the placeholder:\n• None Using as the placeholder: The following query shows how to use binding to reuse a variable: Using bind variables with the IN operator¶ and (server side binding) do not support the use of bind variables with the IN operator. If you need to use bind variables with the IN operator, use client side binding ( or ). In your application code, you can insert multiple rows in a single batch. To do this, use parameters for values in an INSERT statement. For example, the following statement uses placeholders for binding in an INSERT statement: Then, to specify the data that should be inserted, define a variable that is a sequence of sequences (for example, a list of tuples): As shown in the example above, each item in the list is a tuple that contains the column values for a row to be inserted. To perform the binding, call the method, passing the variable as the second argument. For example: If you are binding data on the server (i.e. by using or binding), the connector can optimize the performance of batch inserts through binding. When you use this technique to insert a large number of values, the driver can improve performance by streaming the data (without creating files on the local machine) to a temporary stage for ingestion. The driver automatically does this when the number of values exceeds a threshold. In addition, the current database and schema for the session must be set. If these are not set, the CREATE TEMPORARY STAGE command executed by the driver can fail with the following error: CREATE TEMPORARY STAGE SYSTEM$BIND file_format=(type=csv field_optionally_enclosed_by='\"') Cannot perform CREATE STAGE. This session does not have a current schema. Call 'USE SCHEMA', or use a qualified name. For alternative ways to load data into the Snowflake database (including bulk loading using the COPY command), see Load data into Snowflake. Avoid binding data using Python’s formatting function because you risk SQL injection. For example: Instead, store the values in variables and then bind those variables using qmark or numeric binding style.\n\nTo retrieve metadata about each column in the result set (e.g. the name, type, precision, scale, etc. of each column), use one of the following approaches:\n• None To access the metadata after calling the method to execute the query, use the attribute of the object.\n• None To access the metadata without having to execute the query, call the method. The method is available in the Snowflake Connector for Python 2.4.6 and more recent versions. The attribute is set to one of the following values:\n• None Version 2.4.6 and later: A list of ResultMetadata objects. (The method also returns this list.) Each tuple and object contains the metadata for a column (the column name, data type, etc.). You can access the metadata by index or, in 2.4.6 and later versions, by attribute. The following examples demostrate how to access the metadata from the returned tuples and objects. Example: Getting the column name metadata by index (versions 2.4.5 and earlier): The following example uses the attribute to retrieve the list of column names after executing a query. The attribute is a list of tuples, and the example accesses the column name from the first value in each tuple. Example: Getting the column name metadata by attribute (versions 2.4.6 and later): The following example uses the attribute to retrieve the list of column names after executing a query. The attribute is a list of ResultMetaData objects, and the example accesses the column name from the attribute of each object. Example: Getting the column name metadata without executing the query (versions 2.4.6 and later): The following example uses the method to retrieve the list of column names without executing a query. The method returns a list of ResultMetaData objects, and the example accesses the column name from the attribute of each object.\n\nThe following sample code combines many of the examples described in the previous sections into a working python program. This example contains two parts:\n• None A parent class (“python_veritas_base”) contains the code for many common operations, such as connecting to the server.\n• None A child class (“python_connector_example”) represents the custom portions of a particular client, for example, querying a table. This sample code is imported directly from one of our tests to help ensure that it is has been executed on a recent build of the product. Because this is taken from a test, it includes a small amount of code to set an alternative port and protocol used in some tests. Users should not set the protocol or port number; instead, omit these and use the defaults. This also contains some section markers (sometimes called “snippet tags”) to identify code that can be imported independently into the documentation. Section markers typically look similar to: These section markers are not required in user code. The first part of the code sample contains the common subroutines to:\n• None Read command-line arguments (for example, “–warehouse MyWarehouse”) that contain connection information.\n• None Create and use a warehouse, database, and schema.\n• None Drop the schema, database, and warehouse when you are done with them. This is the Base/Parent class for programs that use the Snowflake This class is intended primarily for: This does any required initialization steps, which in this class is Most tests follow the same basic pattern in this main() method: * Set up, e.g. use (or create and use) the warehouse, database, * Run the queries (or do the other tasks, e.g. load data). * Clean up. In this test/demo, we drop the warehouse, database, and schema. In a customer scenario, you'd typically clean up temporary tables, etc., but wouldn't drop your database. # Read the connection parameters (e.g. user ID) from the command line # and environment variables, then connect to Snowflake. # Set up anything we need (e.g. a separate schema for the test/demo). # Do the \"real work\", for example, create a table, insert rows, SELECT # Clean up. In this case, we drop the temporary warehouse, database, and Read the command-line arguments and store them in a dictionary. Command-line arguments should come in pairs, e.g.: # Strip off the leading \"--\" from the tag, e.g. from \"--user\". This gets account identifier and login information from the environment variables and command-line parameters, connects to the argv: This is usually sys.argv, which contains the command-line parameters. It could be an equivalent substitute if you get the parameter information from another source. # Get account identifier and login information from environment variables and command-line parameters. # For information about account identifiers, see # Get the password from an appropriate environment variable, if # Get the other login info etc. from the command line. \"ERROR: Please pass the following command-line parameters: # If the password is set by both command line and env var, the # command-line value takes precedence over (is written over) the # If the password wasn't set either in the environment var or on # If the PORT is set but the protocol is not, we ignore the PORT (bug!!). Set up to run a test. You can override this method with one appropriate to your test/demo. Your sub-class should override this to include the code required for your documentation sample or your test case. This default method does a very simple self-test that shows that the # This is an example of an SQL statement we might want to run. # Get the results (should be only one): Clean up after a test. You can override this method with one appropriate to your test/demo. Create the temporary schema, database, and warehouse that we use # Create a database, schema, and warehouse if they don't already exist. Drop the temporary schema, database, and warehouse that we create The second part of the code sample creates a table, inserts rows into it, etc.: # Import the base class that contains methods used in many tests and code This is a simple example program that shows how to use the Snowflake To run this sample, do the following:\n• None Copy the first piece of code to a file named “python_veritas_base.py”.\n• None Copy the second piece of code to a file named “python_connector_example.py”\n• None Set the SNOWSQL_PWD environment variable to your password, for example:\n• None Execute the program using a command line similar to the following (replace the user and account information with your own user and account information, of course). This deletes the warehouse, database, and schema at the end of the program! Do not use the name of an existing database because you will lose it! Here is a longer example: In the section where you set your account and login information, make sure to replace the variables as needed to match your Snowflake login information (name, password, etc.). This example uses the format() function to compose the statement. If your environment has a risk of SQL injection attacks, you might prefer to bind values rather than use format(). # Only required if you copy data from your S3 bucket # Creating a database, schema, and warehouse if none exists # Using the database, schema and warehouse # replace <s3_bucket> with the name of your bucket)"
    },
    {
        "link": "https://docs.snowflake.com/en/developer-guide/python-connector/python-connector",
        "document": "This driver currently does not support GCP regional endpoints. Please ensure that any workloads using through this driver do not require support for regional endpoints on GCP. If you have questions about this, please contact Snowflake Support.\n\nThe Snowflake Connector for Python provides an interface for developing Python applications that can connect to Snowflake and perform all standard operations. It provides a programming alternative to developing applications in Java or C/C++ using the Snowflake JDBC or ODBC drivers.\n\nThe connector is a native, pure Python package that has no dependencies on JDBC or ODBC. It can be installed using on Linux, MacOS, and Windows platforms where Python is installed.\n\nThe connector supports developing applications using the Python Database API v2 specification (PEP-249), including using the following standard API objects:\n\nFor more information, see PEP-249.\n\nSnowSQL, the command line client provided by Snowflake, is an example of an application developed using the connector."
    },
    {
        "link": "https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-connect",
        "document": "Connecting to Snowflake with the Python Connector¶\n\nThis topic explains the various ways you can connect to Snowflake with the Python connector.\n\nVerifying the network connection to Snowflake with SnowCD¶ After configuring your driver, you can evaluate and troubleshoot your network connectivity to Snowflake using SnowCD. You can use SnowCD during the initial configuration process and on-demand to evaluate and troubleshoot your network connection to Snowflake.\n\nTo import the module, execute the following command: You can get login information from environment variables, the command line, a configuration file, or another appropriate source. For example: For the ACCOUNT parameter, use your account identifier. Note that the account identifier does not include the suffix. For details and examples, see Usage notes for the account parameter (for the connect method). For descriptions of available connector parameters, see the methods. If you copy data from your own Amazon S3 bucket, then you need the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. If your data is stored in a Microsoft Azure container, provide the credentials directly in the COPY statement. After reading the connection information, connect using either the default authenticator or federated authentication (if enabled).\n\nThe Python connector lets you add connection definitions to a configuration file. A connection definition refers to a collection of connection-related parameters. Snowflake Python libraries currently support TOML version 1.0.0. For more information about file formats, see TOML (Tom’s Obvious Minimal Language). The Python connector looks for the file in the following locations, in order:\n• None If a directory exists on your machine, the Python Connector uses the file. You can override the default directory by setting the location in the environment variable.\n• None Otherwise, the Python Connector uses the file in the one of the following locations, based on your operating system:\n• None Linux: , but you can update it with XDG vars\n• None In a text editor, open the file for editing. For example, to open the file in the Linux vi editor:\n• You can generate the basic settings for the TOML configuration file in Snowsight. For information, see Configuring a client, driver, library, or third-party application to connect to Snowflake. For example, to add a Snowflake connection called with the account , user , and password credentials, as well as database information, add the following lines to the configuration file: Connection definitions support the same configuration options available in the snowflake.connector.connect method.\n• None Save changes to the file.\n• None In your Python code, supply the connection name to , similar to the following: You can also override values defined for the connection in the file, as follows:\n\nYou can set a connection as the default, so you don’t have to specify one every time you call to connect to Snowflake. You can define a default connection in any of the following ways, which are listed in increasing order of precedence:\n• \n• None In the file, create the connection definition and give it the name , as shown:\n• None Specify a named connection as the default connection in the Snowflake file, in the same directory as the file.\n• None Open the file for editing; then:\n• None Set the parameter similar to the following:\n• Sometimes you might want to override the default connection temporarily, such as trying a test connection, without needing to change the normal default connection. You can override the default connection specified in the and files by setting the environment variable as follows: To use the default connection, execute Python code similar to the following: If you choose to rely on a default connection, you cannot override connection parameters, such as , , or .\n\nTo use a proxy server, configure the following environment variables: Snowflake’s security model does not allow Secure Sockets Layer (SSL) proxies (using an HTTPS certificate). Your proxy server must use a publicly-available Certificate Authority (CA), reducing potential security risks such as a MITM (Man In The Middle) attack through a compromised proxy. If you must use your SSL proxy, we strongly recommend that you update the server policy to pass through the Snowflake certificate such that no certificate is altered in the middle of communications. Optionally can be used to bypass the proxy for specific communications. For example, access to Amazon S3 can bypass the proxy server by specifying . does not support wildcards. Each value specified should be one of the following:\n• None The end of a hostname (or a complete hostname), for example:\n• None An IP address, for example: If more than one value is specified, values should be separated by commas, for example:\n\nCalling submits a login request. If a login request fails, the connector can resend the connection request. The following parameters set time limits after which the connector stops retrying requests:\n• None : Specifies how long, in seconds, to keep resending the connection request. If the connection is unsuccessful within that time, the connector fails with a timeout error after completing the current attempt instead of continuing to retry the login request. After the timeout passes, further retries are prevented. However, the current ongoing attempt terminates naturally.\n• None : Specifies how long to wait for network issues to resolve for other requests, such as query requests from . When seconds have passed, if the current attempt fails, a timeout occurs, and the request in question is not retried. After seconds pass, the current attempt is still allowed to finish (fail on its own), after which the timeout occurs.\n• None : Specifies the connection and request timeouts at the socket level. The following example overrides the for the SNOWFLAKE_JWT authenticator: # this request itself stops retrying after 60 seconds as it is a login request The following example demonstrates the effect of setting to a large value: # even though login_timeout is 1, connect will take up to n*300 seconds before failing # this issue arises because socket operations cannot be cancelled once started The following example shows how to override the socket timeout for the SNOWFLAKE_JWT authenticator: # socket timeout for this request is still 300 seconds Note that the environment variable limits the maximum number of retry attempts for login requests. If a request has not timed out but the maximum number of retry attempts is reached, the request immediately fails. The default value is 1, meaning the connector makes only one retry attempt.\n\nIn some situations, you might want to vary the rate or frequency the connector uses to retry failed requests due to timeouts. For example, if you notice that very large numbers of attempts occur concurrently, you can spread those requests out by defining a retry backoff policy. A backoff policy specifies the time to wait between retry attempts. The Snowflake Connector for Python implements backoff policies with the connection parameter that specifies a Python generator function. The generator function lets you specify how long to wait (back off) before sending the next retry request. Snowflake provides the following helpers to create predefined generator functions with your desired parameters. You can use these if you do not want to create your own:\n• None , which increases the backoff duration by a constant each iteration.\n• None , which multiplies the backoff duration by a constant each iteration.\n• None , which randomly chooses between incrementing the backoff duration with and leaving it unchanged each iteration. These predefined generator functions use the following parameters to specify their behaviors:\n• None : Coefficient for incrementing the backoff time. The effect depends on implementation (default = ); adds the value, while multiplies the value.\n• None : Whether to enable jitter on computed durations (default = ). For more information about jitter in exponential backoff, see the AWS Exponential Backoff And Jitter article. For example, you can use the policy with default values or with custom values, as shown: You can also create your own backoff policy generator functions, similar to the following that defines the generator function: You then set the connection parameter to the name of your generator function as follows:\n\nWhen the driver connects, Snowflake sends a certificate to confirm that the connection is to Snowflake rather than to a host that is impersonating Snowflake. The driver sends that certificate to an OCSP (Online Certificate Status Protocol) server to verify that the certificate has not been revoked. If the driver cannot reach the OCSP server to verify the certificate, the driver can “fail open” or “fail closed”. Versions of the Snowflake Connector for Python prior to 1.8.0 default to fail-close mode. Versions 1.8.0 and later default to fail-open. You can override the default behavior by setting the optional connection parameter when calling the connect() method. For example: The driver or connector version and its configuration both determine the OCSP behavior. For more information about the driver or connector version, their configuration, and OCSP behavior, see OCSP Configuration. To ensure all communications are secure, the Snowflake Connector for Python uses the HTTPS protocol to connect to Snowflake, as well as to connect to all other services (e.g. Amazon S3 for staging data files and Okta for federated authentication). In addition to the regular HTTPS protocol, the connector also checks the TLS/SSL certificate revocation status on each connection via OCSP (Online Certificate Status Protocol) and aborts the connection if it finds the certificate is revoked or the OCSP status is not reliable. Because each Snowflake connection triggers up to three round trips with the OCSP server, multiple levels of cache for OCSP responses have been introduced to reduce the network overhead added to the connection:\n• None Memory cache, which persists for the life of the process.\n• None File cache, which persists until the cache directory (e.g. ) is purged. Caching also addresses availability issues for OCSP servers (i.e. in the event the actual OCSP server is down). As long as the cache is valid, the connector can still validate the certificate revocation status. If none of the cache layers contain the OCSP response, the client attempts to fetch the validation status directly from the CA’s OCSP server. By default, the file cache is enabled in the following locations, so no additional configuration tasks are required: However, if you want to specify a different location and/or file name for the OCSP response cache file, the method accepts the parameter, which specifies the path and name for the OCSP cache file in the form of a URI. The OCSP response cache server is currently supported by the Snowflake Connector for Python 1.6.0 and higher. The memory and file types of OCSP cache work well for applications connected to Snowflake using one of the clients Snowflake provides, with a persistent host. However, they don’t work in dynamically-provisioned environments such as AWS Lambda or Docker. To address this situation, Snowflake provides a third level of caching: the OCSP response cache server. The OCSP response cache server fetches OCSP responses hourly from the CA’s OCSP servers and stores them for 24 hours. Clients can then request the validation status of a given Snowflake certificate from this server cache. If your server policy denies access to most or all external IP addresses and web sites, you must allow the cache server address to allow normal service operation. The cache server URL is . If you need to disable the cache server for any reason, set the environment variable to . Note that the value is case-sensitive and must be in lowercase.\n\nRunning diagnostics for a connection requires the following: If you encounter connectivity issues, you can run diagnostics directly within the connector. Snowflake Support might also request this information to help you with connectivity issues. The diagnostics collection uses the following connection parameters:\n• None : Absolute path to a JSON file containing the output of or . Required only if the user defined in the connection does not have permission to run the system allowlist functions or if connecting to the account URL fails.\n• \n• None If desired, change the location for the generated report by setting the parameter.\n• None Review the diagnostic test output of the generated file located in the specified log path. You can review the report for any connectivity issues and discuss them with your network team. You can also provide the report to Snowflake Support for additional assistance."
    },
    {
        "link": "https://pypi.org/project/snowflake-connector-python",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://github.com/snowflakedb/snowflake-connector-python/blob/master/DESCRIPTION.md",
        "document": "This package includes the Snowflake Connector for Python, which conforms to the Python DB API 2.0 specification: https://www.python.org/dev/peps/pep-0249/\n\nSnowflake Documentation is available at: https://docs.snowflake.com/\n\nSource code is also available at: https://github.com/snowflakedb/snowflake-connector-python\n• \n• Added a <19.0.0 pin to pyarrow as a workaround to a bug affecting Azure Batch.\n• Fixed a bug where privatelink OCSP Cache url could not be determined if privatelink account name was specified in uppercase.\n• Added connection parameter that restores the previous behaviour of saving files downloaded with GET with 644 permissions.\n• Deprecated connection property and replaced it with with the same behavior as the former property.\n• \n• Changed not to use scoped temporary objects.\n• \n• Remedied SQL injection vulnerability in snowflake.connector.pandas_tools.write_pandas. See more https://github.com/snowflakedb/snowflake-connector-python/security/advisories/GHSA-2vpq-fh52-j3wv\n• Remedied vulnerability in deserialization of the OCSP response cache. See more: https://github.com/snowflakedb/snowflake-connector-python/security/advisories/GHSA-m4f6-vcj4-w5mx\n• Remedied vulnerability connected to cache files permissions. See more: https://github.com/snowflakedb/snowflake-connector-python/security/advisories/GHSA-r2x6-cjg7-8r43\n• \n• Added a feature to limit the sizes of IO-bound ThreadPoolExecutors during PUT and GET commands.\n• Updated README.md to include instructions on how to verify package signatures using .\n• Updated the log level for cursor's chunk rowcount from INFO to DEBUG.\n• Added a feature to verify if the connection is still good enough to send queries over.\n• Added support for base64-encoded DER private key strings in the authentication type.\n• \n• Fixed a bug where multipart uploads to Azure would be missing their MD5 hashes.\n• Fixed a bug where OpenTelemetry header injection would sometimes cause Exceptions to be thrown.\n• Fixed a bug where OCSP checks would throw TypeError and make mainly GCP blob storage unreachable.\n• \n• Improved the error message for SSL-related issues to provide clearer guidance when an SSL error occurs.\n• \n• Improved error handling for asynchronous queries, providing more detailed and informative error messages when an async query fails.\n• Improved inference of top-level domains for accounts specifying a region in China, now defaulting to snowflakecomputing.cn.\n• Improved implementation of the to reduce the likelihood of collisions.\n• Updated the log level for OCSP fail-open warning messages from ERROR to WARNING.\n• \n• Fixed a bug that logged the session token when renewing a session.\n• Fixed a bug where disabling client telemetry did not work.\n• Fixed a bug where passing as a string raised a during the login retry step.\n• Use instead of for default config file location resolution.\n• Removed reference to script (its backing module was already removed long ago)\n• Enhanced retry mechanism for handling transient network failures during query result polling when no server response is received.\n• \n• Set default connection timeout of 10 seconds and socket read timeout of 10 minutes for HTTP calls in file transfer.\n• Fixed a bug that specifying client_session_keep_alive_heartbeat_frequency in snowflake-sqlalchemy could crash the connector.\n• \n• Added support for connection parameter to read an OAuth token from a file when connecting to Snowflake.\n• Added support for connection parameter to allow debugging raw arrow data in case of arrow data parsing failure.\n• Added support for connection parameter to disable SAML URL check in OKTA authentication.\n• Fixed a bug that OCSP certificate signed using SHA384 algorithm cannot be verified.\n• Fixed a bug that status code shown as uploaded when PUT command failed with 400 error.\n• Fixed a bug that a PermissionError was raised when the current user does not have the right permission on parent directory of config file path.\n• Fixed a bug that OCSP GET url is not encoded correctly when it contains a slash.\n• Fixed a bug that an SSO URL didn't accept in a query parameter, for instance, .\n• \n• Removed an incorrect error log message that could occur during arrow data conversion.\n• \n• Fixed an issue that caused a HTTP 400 error when connecting to a China endpoint.\n• \n• Added easy logging configuration so that users can easily generate log file by setup log config in .\n• \n• Reverted the change \"Updated to skip TABLE IF NOT EXISTS in truncate mode.\" introduced in v3.8.0 (yanked) as it's a breaking change. will be fixed in the future in a non-breaking way.\n• \n• Improved auth in containerized environments\n• Instruct browser to not fetch on success page\n• Add flag (usage: ) to set the underlying socket's flag (described in the socket man page)\n• Useful when the randomized port used in the localhost callback url is being followed before the container engine completes port forwarding to host\n• Statically map a port between your host and container and allow that port to be reused in rapid succession with:\n• Add flag (usage: ) to make a non-blocking socket.recv call and retry on Error\n• Consider using this if running in a containerized environment and externalbrowser auth frequently hangs while waiting for callback\n• NOTE: this has not been tested extensively, but has been shown to improve the experience when using WSL\n• Updated diagnostics to use system$allowlist instead of system$whitelist.\n• Updated to skip TABLE IF NOT EXISTS in truncate mode.\n• Improved cleanup logic for connection to rely on interpreter shutdown instead of the method.\n• Updated the logging level from INFO to DEBUG when logging the executed query using .\n• Fixed a bug that the truncated password in log is not masked.\n• \n• Added a new boolean parameter to to force returning in case of zero rows.\n• Cleanup some C++ code warnings and performance issues.\n• Added support for connecting using an existing connection via the session and master token.\n• Added support for connecting to Snowflake by authenticating with multiple SAML IDP using external browser.\n• \n• Added a new flag to class, that keeps track of whether the connection's master token has expired.\n• Fixed a bug where date insertion failed when date format is set and qmark style binding is used.\n• \n• Version 3.5.0 is the snowflake-connector-python purely built upon apache arrow-nanoarrow project.\n• Reduced the wheel size to ~1MB and installation size to ~5MB.\n• Deprecated the usage of the following class/variable/environment variable for the sake of pure nanoarrow converter:\n• \n• Removed dependencies on pycryptodomex and oscrypto. All connections now go through OpenSSL via the cryptography library, which was already a dependency.\n• Fixed issue with ingesting files over 80 GB to S3.\n• Added the argument to allowing for configurable backoff policy between retries of failed requests. See available implementations in the module.\n• Added the argument to specifying socket read and connect timeout.\n• Fixed and behaviour. Retries of login and network requests are now properly halted after these timeouts expire.\n• \n• Added for non-Windows platforms command suggestions (chown/chmod) for insufficient file permissions of config files.\n• Fixed issue that arrow iterator causes when the c extensions are not compiled.\n• \n• Introduced the environment variable to allows switching between the nanoarrow converter and the arrow converter. Valid values include:\n• , which uses the converter configured in the server.\n• , which uses arrow converter, overriding the server setting.\n• , which uses the nanoarrow converter, overriding the server setting.\n• Introduced the enum, whose members include:\n• , which uses the converter configured in the server.\n• , which uses arrow converter, overriding the server setting.\n• , which uses the nanoarrow converter, overriding the server setting.\n• Introduced the module variable to allow switching between the nanoarrow converter and the arrow converter. It works in conjunction with the enum.\n• The newly-introduced environment variable, enum, and module variable are temporary. They will be removed in a future release when switch from arrow to nanoarrow for data conversion is complete.\n• \n• Fixed a bug where url port and path were ignored in private link oscp retry.\n• Bumped platformdirs dependency from >=2.6.0,<3.9.0 to >=2.6.0,<4.0.0.0 and made necessary changes to allow this.\n• Removed the deprecation warning from the vendored urllib3 about urllib3.contrib.pyopenssl deprecation.\n• \n• Made the -> renaming more consistent in module.\n• \n• Fixed a bug in retry logic for okta authentication to refresh token.\n• Support when constructing in addition to raw bytes.\n• Fixed a bug when connecting through SOCKS5 proxy, the attribute is missing on .\n• Cherry-picked https://github.com/urllib3/urllib3/commit/fd2759aa16b12b33298900c77d29b3813c6582de onto vendored urllib3 (v1.26.15) to enable enforce_content_length by default.\n• \n• Added a feature that lets you add connection definitions to the configuration file. A connection definition refers to a collection of connection parameters, for example, if you wanted to define a connection named `prod``: By default, we look for the file in the location specified in the environment variable (default: ). If this folder does not exist, the Python connector looks for the file in the platformdirs location, as follows:\n• On Linux: , but follows XDG settings You can determine which file is used by running the following command:\n• Improved OCSP response caching to reduce the times of disk writing.\n• Added a parameter in that skips session deletion when client connection closes.\n• Tightened our pinning of platformdirs, to prevent their new releases breaking us.\n• Fixed a bug where SFPlatformDirs would incorrectly append application_name/version to its path.\n• Added retry reason for queries that are retried by the client.\n• Fixed a bug where fails when user does not have the privilege to create stage or file format in the target schema, but has the right privilege for the current schema.\n• Worked around a segfault which sometimes occurred during cache serialization in multi-threaded scenarios.\n• Fixed a bug about deleting the temporary files happened when running PUT command.\n• Allowed to pass to and .\n• Fixed a bug where pickle.dump segfaults during cache serialization in multi-threaded scenarios.\n• Improved retry logic for okta authentication to refresh token if authentication gets throttled.\n• Note that this release does not include the changes introduced in the previous 3.1.0a1 release. Those will be released at a later time.\n• \n• Fixed a bug in which could modify the argument statement_params dictionary object when executing a multistatement query.\n• Added the json_result_force_utf8_decoding connection parameter to force decoding JSON content in utf-8 when the result format is JSON.\n• Fixed a bug in which we cannot call before fetching the result of the first query if the cursor runs an async multistatement query.\n• Fixed a bug when was not called before yielding results of .\n• Fixed a bug where some ResultMetadata fields were marked as required when they were optional.\n• \n• Fixed a bug that prints error in logs for GET command on GCS.\n• Added a parameter that allows users to skip file uploads to stage if file exists on stage and contents of the file match.\n• Fixed a bug that occurred when writing a Pandas DataFrame with non-default index in .\n• Fixed a bug that occurred when writing a Pandas DataFrame with column names containing double quotes in .\n• Fixed a bug that occurred when writing a Pandas DataFrame with binary data in .\n• Improved GET logging to warn when downloading multiple files with the same name.\n• \n• Fixed a memory leak in the logging module of the Cython extension.\n• Fixed a bug where the command on AWS raised when uploading file composed of multiple parts.\n• Fixed a bug of incorrect type hints of and .\n• Fixed a bug where swallows the final line break in the case when there are no space between lines.\n• Improved logging to mask tokens in case of errors.\n• Validate SSO URL before opening it in the browser for External browser authenticator.\n• \n• Improved the robustness of OCSP response caching to handle errors in cases of serialization and deserialization.\n• Errors raised now have a query field that contains the SQL query that caused them when available.\n• Fixed a bug where MFA token caching would refuse to work until restarted instead of reauthenticating.\n• Replaced the dependency on setuptools in favor of packaging.\n• Fixed a bug where should pass keyword arguments instead of positional arguments when calling .\n• \n• Fixed a bug where write_pandas did not use user-specified schema and database to create intermediate objects\n• Fixed a bug where HTTP response code of 429 were not retried\n• Fixed a bug where MFA token caching was not working\n• During browser-based authentication, the SSO url is now printed before opening it in the browser\n• Increased the level of a log for when ArrowResult cannot be imported\n• \n• Fixed a bug where the permission of the file downloaded via GET command is changed\n• Reworked authentication internals to allow users to plug custom key-pair authenticators\n• Multi-statement query execution is now supported through and\n• The Snowflake parameter can be altered at the account, session, or statement level. An additional argument, , can be provided to to use this parameter at the statement level. It must be provided to to submit a multi-statement query through the method. Note that bulk insert optimizations available through are not available when submitting multi-statement queries.\n• By default the parameter is 1, meaning only a single query can be submitted at a time\n• Set to 0 to submit any number of statements in a multi-statement query\n• Set to >1 to submit the specified exact number of statements in a multi-statement query\n• Bindings are accepted in the same way for multi-statements as they are for single statement queries\n• Asynchronous multi-statement query execution is supported. Users should still use to retrieve results\n• To access the results of each query, users can call as specified in the DB 2.0 API (PEP-249), to iterate through each statements results\n• The first statement's results are accessible immediately after calling (or if asynchronous) through the existing methods\n• \n• During the execution of GET commands we no longer resolve target location on the local machine\n• Improved performance of regexes used for PUT/GET SQL statement detection. CVE-2022-42965\n• \n• Fixed a bug where write_pandas wouldn't write an empty DataFrame to Snowflake\n• When closing connection async query status checking is now parallelized\n• Fixed a bug where test logging would be enabled on Jenkins workers in non-Snowflake Jenkins machines\n• Enhanced the atomicity of write_pandas when overwrite is set to True\n• \n• Fixed a bug where rowcount was deleted when the cursor was closed\n• Fixed a bug where extTypeName was used even when it was empty\n• The write_pandas function now supports providing additional arguments to be used by DataFrame.to_parquet\n• All optional parameters of write_pandas can now be provided to pd_writer and make_pd_writer to be used with DataFrame.to_sql\n• \n• Fixed a bug where timestamps fetched as pandas.DataFrame or pyarrow.Table would overflow for the sake of unnecessary precision. In the case where an overflow cannot be prevented a clear error will be raised now.\n• The write_pandas function now supports transient tables through the new table_type argument which supersedes create_temp_table argument\n• Fixed a bug where calling fetch_pandas_batches incorrectly raised NotSupportedError after an async query was executed\n• \n• Release wheels are now built on manylinux2014\n• Updated vendored library versions requests to 2.28.1 and urllib3 to 1.26.10\n• Added attribute to in compliance with PEP249.\n• Fixed a bug where gzip compressed http requests might be garbled by an unflushed buffer\n• \n• Fixed a bug where errors raised during get_results_from_sfqid() were missing errno\n• \n• Fixed an error message that appears when pandas optional dependency group is required but is not installed\n• Fixed a bug where decryption took place before decompression when downloading files from stages\n• Extra named arguments given executemany() are now forwarded to execute()\n• Automatically sets the application name to streamlit when streamlit is imported and application name was not explicitly set\n• \n• Fixed a bug where partner name (from SF_PARTNER environmental variable) was set after connection was established\n• \n• Added an option for partners to inject their name through an environmental variable (SF_PARTNER)\n• Fixed a bug where we would not wait for input if a browser window couldn't be opened for SSO login\n• Fixed a bug where final Arrow table would contain duplicate index numbers when using fetch_pandas_all\n• \n• Fixed a bug where circular reference would prevent garbage collection on some objects\n• Fixed a bug where was thrown when executing against a closed cursor instead of\n• Fixed a bug where calling would crash if an iterator was supplied as args\n• Fixed a bug where violating constraint raised instead of\n• \n• Fixed a bug where timezone was missing from retrieved Timestamp_TZ columns\n• Fixed a bug where a long running PUT/GET command could hit a Storage Credential Error while renewing credentials\n• Fixed a bug where py.typed was not being included in our release wheels\n• Fixed a bug where negative numbers were mangled when fetched with the connection parameter arrow_number_to_decimal\n• Improved the error message that is encountered when running GET for a non-existing file\n• Fixed rendering of our long description for PyPi\n• Fixed a bug where DUO authentication ran into errors if sms authentication was disabled for the user\n• Add the ability to auto-create a table when writing a pandas DataFrame to a Snowflake table\n• Bumped the maximum dependency version of numpy from <1.22.0 to <1.23.0\n• \n• Fixed an issue bug where _get_query_status failed if there was a network error.\n• Added the interpolate_empty_sequences connection parameter to control interpolating empty sequences into queries.\n• Fixed an issue where where BLOCKED was considered to be an error by is_an_error.\n• Fixed an issue where dbapi.Binary returned a string instead of bytes.\n• Fixed issue so that fetch functions now return a typed DataFrames and pyarrow Tables for empty results.\n• \n• Fixed a bug where uploading a streaming file with multiple parts did not work.\n• JWT tokens are now regenerated when a request is retired.\n• Updated URL escaping when uploading to AWS S3 to match how S3 escapes URLs.\n• Blocked queries are now be considered to be still running.\n• Snowflake specific exceptions are now set using Exception arguments.\n• Fixed an issue where use_s3_regional_url was not set correctly by the connector.\n• \n• Removing cloud sdks.snowflake-connector-python will not install them anymore. Recreate your virtualenv to get rid of unnecessary dependencies.\n• Fixed a bug where error number would not be added to Exception messages.\n• Fixed a bug where client_prefetch_threads parameter was not respected when pre-fetching results.\n• \n• Fixed a bug where GET commands would fail to download files from sub directories from stages.\n• Added a feature where where the connector will print the url it tried to open when it is unable to open it for external browser authentication.\n• \n• Force cast a column into integer in write_pandas to avoid a rare behavior that would lead to crashing.\n• Implement AWS signature V4 to new SDKless PUT and GET.\n• Fixed a bug where error logs would be printed for query executions that produce no results.\n• Fixed a bug where the temporary stage for bulk array inserts exists.\n• \n• Internal change to the implementation of result fetching.\n• Internal change to the implementation for PUT and GET. A new connection parameter use_new_put_get was added to toggle between implementations.\n• Fixed a bug where executemany did not detect the type of data it was inserting.\n• Updated the minimum Mac OSX build target from 10.13 to 10.14.\n• \n• Fixes Python Connector bug that prevents the connector from using AWS S3 Regional URL. The driver currently overrides the regional URL information with the default S3 URL causing failure in PUT.\n• \n• Fixed a bug in write_pandas when quote_identifiers is set to True the function would not actually quote column names.\n• \n• Changed default value of client_session_keep_alive to None.\n• Added the ability to retrieve metadata/schema without executing the query (describe method).\n• \n• Fix for incorrect JWT token invalidity when an account alias with a dash in it is used for regionless account URL.\n• \n• Fixed a segfault issue when using DictCursor and arrow result format with out of range dates.\n• \n• Uses s3 regional URL in private links when a param is set.\n• \n• Added support for using the PUT command with a file-like object.\n• Removed the pytz pin because it doesn't follow semantic versioning release format.\n• \n• For dependency checking, increased the version condition for the pyjwt package from <2.0.0 to <3.0.0.\n• \n• The fix to add proper proxy CONNECT headers for connections made over proxies.\n• \n• Send all Python Connector exceptions to in-band or out-of-band telemetry.\n• Vendoring requests and urllib3 to contain OCSP monkey patching to our library only.\n• \n• Relaxed the boto3 dependency pin up to the next major release.\n• Relaxed the cffi dependency pin up to the next major release.\n• \n• Fixed a bug that was preventing the connector from working on Windows with Python 3.8.\n• For dependency checking, increased the version condition for the cryptography package from <3.0.0 to <4.0.0.\n• For dependency checking, increased the version condition for the pandas package from <1.1 to <1.2.\n• \n• Updated the dependency on the cryptography package from version 2.9.2 to 3.2.1.\n• \n• Added an optional parameter to the write_pandas function to specify that identifiers should not be quoted before being sent to the server.\n• The write_pandas function now honors default and auto-increment values for columns when inserting new rows.\n• Enabled the runtime pyarrow version verification to fail gracefully. Fixed a bug with AWS glue environment.\n• Upgraded the version of boto3 from 1.14.47 to 1.15.9.\n• Upgraded the version of idna from 2.9 to 2.10.\n• \n• In the Connection object, the execute_stream and execute_string methods now filter out empty lines from their inputs.\n• \n• Fixed a bug where a file handler was not closed properly.\n• \n• Fixed a bug where 2 constants were removed by mistake.\n• \n• When the log level is set to DEBUG, log the OOB telemetry entries that are sent to Snowflake.\n• Fixed a bug in the PUT command where long running PUTs would fail to re-authenticate to GCP for storage.\n• \n• Improved an error message for when \"pandas\" optional dependency group is not installed and user tries to fetch data into a pandas DataFrame. It'll now point user to our online documentation.\n• \n• Connection parameter validate_default_parameters now verifies known connection parameter names and types. It emits warnings for anything unexpected types or names.\n• Fixed an issue in write_pandas with location determination when database, or schema name was included.\n• Fixed an issue where uploading a file with special UTF-8 characters in their names corrupted file.\n• \n• Switched docstring style to Google from Epydoc and added automated tests to enforce the standard.\n• \n• Support azure-storage-blob v12 as well as v2 (for Python 3.5.0-3.5.1) by Python Connector\n• Fixed a bug where temporary directory path was not Windows compatible in write_pandas function\n• Added out of band telemetry error reporting of unknown errors\n• \n• Update Pyarrow version from 0.16.0 to 0.17.0 for Python connector\n• Missing keyring dependency will not raise an exception, only emit a debug log from now on.\n• \n• Added more efficient way to ingest a pandas.Dataframe into Snowflake, located in snowflake.connector.pandas_tools\n• More restrictive application name enforcement and standardizing it with other Snowflake drivers\n• Added checking and warning for users when they have a wrong version of pyarrow installed\n• \n• Emit warning only if trying to set different setting of use_openssl_only parameter\n• \n• Add use_openssl_only connection parameter, which disables the usage of pure Python cryptographic libraries for FIPS\n• Add manylinux1 as well as manylinux2010\n• Fix a bug where a certificate file was opened and never closed in snowflake-connector-python.\n• \n• AWS: When OVERWRITE is false, which is set by default, the file is uploaded if no same file name exists in the stage. This used to check the content signature but it will no longer check. Azure and GCP already work this way.\n• Document Python connector dependencies on our GitHub page in addition to Snowflake docs.\n• Fix GCP exception using the Python connector to PUT a file in a stage with auto_compress=false.\n• Fix wrong result bug while using fetch_pandas_all() to get fixed numbers with large scales.\n• \n• Fix the arrow bundling issue for python connector on mac.\n• Fix the arrow dll bundle issue on windows.Add more logging.\n• \n• Add support for GCS PUT and GET for private preview.\n• Support fetch as numpy value in arrow result format.\n• Fix NameError: name 'EmptyPyArrowIterator' is not defined for Mac.\n• Return empty dataframe for fetch_pandas_all() api if result set is empty.\n• \n• Update the release note that 1.9.0 was removed\n• Handle year out of range correctly in arrow result format\n• \n• Use new query result format parameter in python tests\n• \n• Fix for ,Pandas fetch API did not handle the case that first chunk is empty correctly.\n• Updated with botocore, boto3 and requests packages to the latest version.\n• \n• Fix sessions remaining open even if they are disposed manually. Retry deleting session if the connection is explicitly closed.\n• Fix memory leak in the new fetch pandas API\n• Ensure that the cython components are present for Conda package\n• \n• Fix SF_OCSP_RESPONSE_CACHE_DIR referring to the OCSP cache response file directory and not the top level of directory.\n• Move AWS_ID and AWS_SECRET_KEY to their newer versions in the Python client\n• Update USER-AGENT to be consistent with new format\n• \n• Implement converter for all arrow data types in python connector extension\n• Fix RevokedCertificateError OOB Telemetry events are not sent\n• Make tzinfo class at the module level instead of inlining\n• \n• Rewrote validateDefaultParameters to validate the database, schema and warehouse at connection time. False by default.\n• \n• Python3.4 using requests 2.21.0 needs older version of urllib3\n• Use Account Name for Global URL\n• \n• Fixed DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated\n• \n• Fix the incorrect custom Server URL in Python Driver for Privatelink\n• \n• Update copyright year from 2018 to 2019 for Python\n• \n• Added SnowflakeNullConverter for Python Connector to skip all client side conversions\n• Fixed the hang when region=us-west-2 is specified.\n• \n• Fixed the epoch time to datetime object converter for Windoww\n• \n• Catch socket.EAI_NONAME for localhost socket and raise a better error message\n• \n• Fixed a backslash followed by a quote in a literal was not taken into account.\n• Added to each HTTP request for tracing.\n• \n• Changed most INFO logs to DEBUG. Added INFO for key operations.\n• Fixed the URL query parser to get multiple values.\n• \n• Enforce virtual host URL for PUT and GET.\n• Added retryCount, clientStarTime for query-request for better service.\n• \n• Replaced with to avoid namespace conflict with .\n• Fixed hang if the connection is not explicitly closed since 1.6.4.\n• \n• Fixed the current object cache in the connection for id token use.\n• \n• Fixed div by zero for Azure PUT command.\n• Cache id token for SSO. This feature is WIP.\n• \n• Ensure the type of attribute is .\n• \n• Fixed PUT command error 'Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.' for Azure deployment.\n• \n• Fixed object has no attribute errors in Python3 for Azure deployment.\n• Removed ContentEncoding=gzip from the header for PUT command. This caused COPY failure if autocompress=false.\n• \n• Fixed TypeError: list indices must be integers or slices, not str. PR/Issue 75 (@daniel-sali).\n• \n• Relaxed version requirements. No 3.5.0 should be used.\n• \n• Pulled back for OCSP check in Python 2. Python 3 continue using for better performance.\n• Limit the upper bound of version to less than 3.5.0 for Issue 65.\n• \n• Fixed failue in case HOME/USERPROFILE is not set.\n• \n• Removed and from the dependency.\n• \n• Started replacing with Not activated yet.\n• \n• Added Azure support for PUT and GET commands.\n• \n• Updated Fed/SSO parameters. The production version of Fed/SSO from Python Connector requires this version.\n• Set CLIENT_APP_ID and CLIENT_APP_VERSION in all requests\n• Support new behaviors of newer version of . Relaxed the dependency.\n• Making socket timeout same as the login time\n• Fixed the case where no error message is attached.\n• \n• Refresh AWS token in PUT command if S3UploadFailedError includes the ExpiredToken error\n• Retry all of 5xx in connection\n• \n• Cleaned up logger by moving instance to module.\n• \n• Upgraded SSL wrapper with the latest urllib3 pyopenssl glue module. It uses kqueue, epoll or poll in replacement of select to read data from socket if available.\n• \n• Changed the log levels for some messages from ERROR to DEBUG to address confusion as real incidents. In fact, they are not real issues but signals for connection retry.\n• Added to the dependent component list to mitigate CA root certificate out of date issue.\n• Set the maximum versions of dependent components and .\n• Added a connection parameter to validate the default database, schema and warehouse. If the specified object doesn't exist, it raises an error.\n• \n• Relaxed the version of dependent components and\n• \n• Pinned and versions to 0.2.3 and 0.0.9, respectively\n• \n• Relaxed the versions of dependent components , , and and\n• \n• Fixed OCSP response cache file not found issue on Windows. Drive letter was taken off\n• \n• Set autocommit and abort_detached_query session parameters in authentication time if specified\n• Fixed cross region stage issue. Could not get files in us-west-2 region S3 bucket from us-east-1\n• \n• Fixed timestamp format FF to honor the scale of data type\n• Improved the security of OKTA authentication with hostname verifications\n• Retry PUT on the error 10053 with lower concurrency\n• \n• Use proxy parameters for PUT and GET commands.\n• Added and to the results from query results.\n• Fixed the connection timeout calculation based on and .\n• Improved error messages in case of 403, 502 and 504 HTTP reponse code.\n• Upgraded to 1.7.2, to 1.4.4 and to 1.5.14.\n• \n• Added and parameters to the objects.\n• \n• Fixed parameter. One character was truncated from the tail of account name\n• \n• Fixed the regression in 1.3.8 that caused intermittent 504 errors\n• \n• Compress data in HTTP requests at all times except empty data or OKTA request\n• Refactored FIXED, REAL and TIMESTAMP data fetch to improve performance. This mainly impacts SnowSQL\n• Increased the retry counter for OCSP servers to mitigate intermittent failure\n• \n• Upgraded to 1.4.93 to fix and to 1.4.3 to fix the HTTPS request failure in Python 3.6\n• \n• Convert non-UTF-8 data in the large result set chunk to Unicode replacement characters to avoid decode error.\n• Use package to support both PY2 and PY3 for some functions\n• Fixed OverflowError caused by invalid range of timetamp data for SnowSQL.\n• \n• Increased the validity date acceptance window to prevent OCSP returning invalid responses due to out-of-scope validity dates for certificates.\n• \n• Upgraded to 1.5.3, to 16.2.0 and to 1.9.1.\n• \n• Not to compress file in PUT command\n• \n• Increased the stability of PUT and GET commands\n• \n• Set the signature version to v4 to AWS client. This impacts , commands and fetching large result set.\n• \n• Added support for the data type, which enables support for more Python data types:\n• \n• and can be used for binding.\n• is also used for fetching data type.\n• \n• can be used for binding\n• is used for fetching data type.\n• Added and connection parameters for proxy servers that require authentication.\n• \n• Added and to run multiple statements in a string and stream.\n• Increased the stability of fetching data for Python 2.\n• \n• Force OCSP cache invalidation after 24 hours for better security.\n• Use in PUT and GET if Transfer acceleration is enabled for the S3 bucket.\n• Fixed the side effect of that loads in the current directory.\n• \n• Fixed the AWS token renewal issue with PUT command when uploading uncompressed large files.\n• \n• Added retry for errors and in PUT and GET, respectively.\n• \n• Added parameter to Connection so that you can specify the maximum number of HTTP/HTTPS connections in the pool.\n• \n• Fixed 404 issue in GET command. An extra slash character changed the S3 path and failed to identify the file to download.\n• \n• Upgraded to 1.3.1 and and 1.4.22.\n• Added data type binding support. , and can be bound and fetched.\n• \n• Fixed OCSP revocation check issue with the new certificate and AWS S3.\n• Upgraded to 1.3.1 and to 16.0.0.\n• \n• Replaced the self contained packages in with the dependency of 1.3.0 and 1.4.2.\n• \n• Added support for data type, which is now a Snowflake supported data type. This feature requires a server upgrade.\n• Added to fetch the results in instead of .\n• Added compression to the SQL text and commands.\n• \n• Upgraded to 1.2.2 and to 1.5.2.\n• Fixed the conversion from to datetime in queries.\n• \n• Time out all HTTPS requests so that the Python Connector can retry the job or recheck the status.\n• Fixed the location of encrypted data for command. They used to be in the same directory as the source data files.\n• Added support for renewing the AWS token used in commands if the token expires.\n• \n• Added support for the data type (i.e. or ). This changes the behavior of the binding for the type object:\n• Previously, was bound as a numeric value (i.e. for , for ).\n• Now, is bound as native SQL data (i.e. or ).\n• Added the method to the object:\n• By default, mode is ON (i.e. each DML statement commits the change).\n• If mode is OFF, the and methods are enabled.\n• Avoid segfault issue for 1.2 in Mac OSX by using 1.1 until resolved."
    },
    {
        "link": "https://community.snowflake.com/s/article/Snowflake-Security-Overview-and-Best-Practices",
        "document": ""
    },
    {
        "link": "https://community.fabric.microsoft.com/t5/Power-Query/Avoiding-including-Snowflake-credentials-in-Python-script-when/td-p/4396489",
        "document": "Thank you for reaching out to the Microsoft Fabric Community Forum.\n\nThank you for raising your concern. We really apologize for the inconvenience caused. After reviewing the details you provided, please follow the steps below, which might resolve the issue.\n• Store your Snowflake credentials in environment variables to avoid hardcoding them in your script. Use a configuration file for your credentials and ensure it's excluded from version control by adding it to .gitignore.\n• Use key pair authentication instead of username and password for better security and to avoid storing passwords.\n• Utilize secrets management services like aws secrets manager, azure key vault, or Hashi corp vault to securely store and retrieve your credentials.\n• Snowflake now allows managing secrets directly within the platform, enabling secure storage of sensitive information like API keys or passwords.\n\nBy implementing these methods, you can ensure that your Snowflake credentials are handled securely and not exposed within your Python scripts.\n\nIf this post helps, then please give us Kudos and consider Accept it as a solution to help the other members find it more quickly."
    },
    {
        "link": "https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-connect",
        "document": "Connecting to Snowflake with the Python Connector¶\n\nThis topic explains the various ways you can connect to Snowflake with the Python connector.\n\nVerifying the network connection to Snowflake with SnowCD¶ After configuring your driver, you can evaluate and troubleshoot your network connectivity to Snowflake using SnowCD. You can use SnowCD during the initial configuration process and on-demand to evaluate and troubleshoot your network connection to Snowflake.\n\nTo import the module, execute the following command: You can get login information from environment variables, the command line, a configuration file, or another appropriate source. For example: For the ACCOUNT parameter, use your account identifier. Note that the account identifier does not include the suffix. For details and examples, see Usage notes for the account parameter (for the connect method). For descriptions of available connector parameters, see the methods. If you copy data from your own Amazon S3 bucket, then you need the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. If your data is stored in a Microsoft Azure container, provide the credentials directly in the COPY statement. After reading the connection information, connect using either the default authenticator or federated authentication (if enabled).\n\nThe Python connector lets you add connection definitions to a configuration file. A connection definition refers to a collection of connection-related parameters. Snowflake Python libraries currently support TOML version 1.0.0. For more information about file formats, see TOML (Tom’s Obvious Minimal Language). The Python connector looks for the file in the following locations, in order:\n• None If a directory exists on your machine, the Python Connector uses the file. You can override the default directory by setting the location in the environment variable.\n• None Otherwise, the Python Connector uses the file in the one of the following locations, based on your operating system:\n• None Linux: , but you can update it with XDG vars\n• None In a text editor, open the file for editing. For example, to open the file in the Linux vi editor:\n• You can generate the basic settings for the TOML configuration file in Snowsight. For information, see Configuring a client, driver, library, or third-party application to connect to Snowflake. For example, to add a Snowflake connection called with the account , user , and password credentials, as well as database information, add the following lines to the configuration file: Connection definitions support the same configuration options available in the snowflake.connector.connect method.\n• None Save changes to the file.\n• None In your Python code, supply the connection name to , similar to the following: You can also override values defined for the connection in the file, as follows:\n\nYou can set a connection as the default, so you don’t have to specify one every time you call to connect to Snowflake. You can define a default connection in any of the following ways, which are listed in increasing order of precedence:\n• \n• None In the file, create the connection definition and give it the name , as shown:\n• None Specify a named connection as the default connection in the Snowflake file, in the same directory as the file.\n• None Open the file for editing; then:\n• None Set the parameter similar to the following:\n• Sometimes you might want to override the default connection temporarily, such as trying a test connection, without needing to change the normal default connection. You can override the default connection specified in the and files by setting the environment variable as follows: To use the default connection, execute Python code similar to the following: If you choose to rely on a default connection, you cannot override connection parameters, such as , , or .\n\nTo use a proxy server, configure the following environment variables: Snowflake’s security model does not allow Secure Sockets Layer (SSL) proxies (using an HTTPS certificate). Your proxy server must use a publicly-available Certificate Authority (CA), reducing potential security risks such as a MITM (Man In The Middle) attack through a compromised proxy. If you must use your SSL proxy, we strongly recommend that you update the server policy to pass through the Snowflake certificate such that no certificate is altered in the middle of communications. Optionally can be used to bypass the proxy for specific communications. For example, access to Amazon S3 can bypass the proxy server by specifying . does not support wildcards. Each value specified should be one of the following:\n• None The end of a hostname (or a complete hostname), for example:\n• None An IP address, for example: If more than one value is specified, values should be separated by commas, for example:\n\nCalling submits a login request. If a login request fails, the connector can resend the connection request. The following parameters set time limits after which the connector stops retrying requests:\n• None : Specifies how long, in seconds, to keep resending the connection request. If the connection is unsuccessful within that time, the connector fails with a timeout error after completing the current attempt instead of continuing to retry the login request. After the timeout passes, further retries are prevented. However, the current ongoing attempt terminates naturally.\n• None : Specifies how long to wait for network issues to resolve for other requests, such as query requests from . When seconds have passed, if the current attempt fails, a timeout occurs, and the request in question is not retried. After seconds pass, the current attempt is still allowed to finish (fail on its own), after which the timeout occurs.\n• None : Specifies the connection and request timeouts at the socket level. The following example overrides the for the SNOWFLAKE_JWT authenticator: # this request itself stops retrying after 60 seconds as it is a login request The following example demonstrates the effect of setting to a large value: # even though login_timeout is 1, connect will take up to n*300 seconds before failing # this issue arises because socket operations cannot be cancelled once started The following example shows how to override the socket timeout for the SNOWFLAKE_JWT authenticator: # socket timeout for this request is still 300 seconds Note that the environment variable limits the maximum number of retry attempts for login requests. If a request has not timed out but the maximum number of retry attempts is reached, the request immediately fails. The default value is 1, meaning the connector makes only one retry attempt.\n\nIn some situations, you might want to vary the rate or frequency the connector uses to retry failed requests due to timeouts. For example, if you notice that very large numbers of attempts occur concurrently, you can spread those requests out by defining a retry backoff policy. A backoff policy specifies the time to wait between retry attempts. The Snowflake Connector for Python implements backoff policies with the connection parameter that specifies a Python generator function. The generator function lets you specify how long to wait (back off) before sending the next retry request. Snowflake provides the following helpers to create predefined generator functions with your desired parameters. You can use these if you do not want to create your own:\n• None , which increases the backoff duration by a constant each iteration.\n• None , which multiplies the backoff duration by a constant each iteration.\n• None , which randomly chooses between incrementing the backoff duration with and leaving it unchanged each iteration. These predefined generator functions use the following parameters to specify their behaviors:\n• None : Coefficient for incrementing the backoff time. The effect depends on implementation (default = ); adds the value, while multiplies the value.\n• None : Whether to enable jitter on computed durations (default = ). For more information about jitter in exponential backoff, see the AWS Exponential Backoff And Jitter article. For example, you can use the policy with default values or with custom values, as shown: You can also create your own backoff policy generator functions, similar to the following that defines the generator function: You then set the connection parameter to the name of your generator function as follows:\n\nWhen the driver connects, Snowflake sends a certificate to confirm that the connection is to Snowflake rather than to a host that is impersonating Snowflake. The driver sends that certificate to an OCSP (Online Certificate Status Protocol) server to verify that the certificate has not been revoked. If the driver cannot reach the OCSP server to verify the certificate, the driver can “fail open” or “fail closed”. Versions of the Snowflake Connector for Python prior to 1.8.0 default to fail-close mode. Versions 1.8.0 and later default to fail-open. You can override the default behavior by setting the optional connection parameter when calling the connect() method. For example: The driver or connector version and its configuration both determine the OCSP behavior. For more information about the driver or connector version, their configuration, and OCSP behavior, see OCSP Configuration. To ensure all communications are secure, the Snowflake Connector for Python uses the HTTPS protocol to connect to Snowflake, as well as to connect to all other services (e.g. Amazon S3 for staging data files and Okta for federated authentication). In addition to the regular HTTPS protocol, the connector also checks the TLS/SSL certificate revocation status on each connection via OCSP (Online Certificate Status Protocol) and aborts the connection if it finds the certificate is revoked or the OCSP status is not reliable. Because each Snowflake connection triggers up to three round trips with the OCSP server, multiple levels of cache for OCSP responses have been introduced to reduce the network overhead added to the connection:\n• None Memory cache, which persists for the life of the process.\n• None File cache, which persists until the cache directory (e.g. ) is purged. Caching also addresses availability issues for OCSP servers (i.e. in the event the actual OCSP server is down). As long as the cache is valid, the connector can still validate the certificate revocation status. If none of the cache layers contain the OCSP response, the client attempts to fetch the validation status directly from the CA’s OCSP server. By default, the file cache is enabled in the following locations, so no additional configuration tasks are required: However, if you want to specify a different location and/or file name for the OCSP response cache file, the method accepts the parameter, which specifies the path and name for the OCSP cache file in the form of a URI. The OCSP response cache server is currently supported by the Snowflake Connector for Python 1.6.0 and higher. The memory and file types of OCSP cache work well for applications connected to Snowflake using one of the clients Snowflake provides, with a persistent host. However, they don’t work in dynamically-provisioned environments such as AWS Lambda or Docker. To address this situation, Snowflake provides a third level of caching: the OCSP response cache server. The OCSP response cache server fetches OCSP responses hourly from the CA’s OCSP servers and stores them for 24 hours. Clients can then request the validation status of a given Snowflake certificate from this server cache. If your server policy denies access to most or all external IP addresses and web sites, you must allow the cache server address to allow normal service operation. The cache server URL is . If you need to disable the cache server for any reason, set the environment variable to . Note that the value is case-sensitive and must be in lowercase.\n\nRunning diagnostics for a connection requires the following: If you encounter connectivity issues, you can run diagnostics directly within the connector. Snowflake Support might also request this information to help you with connectivity issues. The diagnostics collection uses the following connection parameters:\n• None : Absolute path to a JSON file containing the output of or . Required only if the user defined in the connection does not have permission to run the system allowlist functions or if connecting to the account URL fails.\n• \n• None If desired, change the location for the generated report by setting the parameter.\n• None Review the diagnostic test output of the generated file located in the specified log path. You can review the report for any connectivity issues and discuss them with your network team. You can also provide the report to Snowflake Support for additional assistance."
    },
    {
        "link": "https://stackoverflow.com/questions/7014953/i-need-to-securely-store-a-username-and-password-in-python-what-are-my-options",
        "document": "There are a few options for storing passwords and other secrets that a Python program needs to use, particularly a program that needs to run in the background where it can't just ask the user to type in the password.\n• Checking the password in to source control where other developers or even the public can see it.\n• Other users on the same server reading the password from a configuration file or source code.\n• Having the password in a source file where others can see it over your shoulder while you are editing it.\n\nThis isn't always an option, but it's probably the best. Your private key is never transmitted over the network, SSH just runs mathematical calculations to prove that you have the right key.\n\nIn order to make it work, you need the following:\n• The database or whatever you are accessing needs to be accessible by SSH. Try searching for \"SSH\" plus whatever service you are accessing. For example, \"ssh postgresql\". If this isn't a feature on your database, move on to the next option.\n• Create an account to run the service that will make calls to the database, and generate an SSH key.\n• Either add the public key to the service you're going to call, or create a local account on that server, and install the public key there.\n\nThis one is the simplest, so it might be a good place to start. It's described well in the Twelve Factor App. The basic idea is that your source code just pulls the password or other secrets from environment variables, and then you configure those environment variables on each system where you run the program. It might also be a nice touch if you use default values that will work for most developers. You have to balance that against making your software \"secure by default\".\n\nHere's an example that pulls the server, user name, and password from environment variables.\n\nLook up how to set environment variables in your operating system, and consider running the service under its own account. That way you don't have sensitive data in environment variables when you run programs in your own account. When you do set up those environment variables, take extra care that other users can't read them. Check file permissions, for example. Of course any users with root permission will be able to read them, but that can't be helped. If you're using systemd, look at the service unit, and be careful to use instead of for any secrets. values can be viewed by any user with .\n\nThis is very similar to the environment variables, but you read the secrets from a text file. I still find the environment variables more flexible for things like deployment tools and continuous integration servers. If you decide to use a configuration file, Python supports several formats in the standard library, like TOML, JSON, INI, netrc, and XML. You can also find external packages like PyYAML. Personally, I find JSON and YAML the simplest to use, and YAML allows comments. TOML support was added to the core libraries in 3.11, but I haven't tried it yet.\n\nThree things to consider with configuration files:\n• Where is the file? Maybe a default location like , and a command-line option to use a different location.\n• Make sure other users can't read the file.\n• Obviously, don't commit the configuration file to source code. You might want to commit a template that users can copy to their home directory.\n\nSome projects just put their secrets right into a Python module.\n\nThen import that module to get the values.\n\nOne project that uses this technique is Django. Obviously, you shouldn't commit to source control, although you might want to commit a file called that users can copy and modify.\n\nI see a few problems with this technique:\n• Developers might accidentally commit the file to source control. Adding it to reduces that risk.\n• Some of your code is not under source control. If you're disciplined and only put strings and numbers in here, that won't be a problem. If you start writing logging filter classes in here, stop!\n\nIf your project already uses this technique, it's easy to transition to environment variables. Just move all the setting values to environment variables, and change the Python module to read from those environment variables."
    },
    {
        "link": "https://medium.com/jungletronics/how-to-securely-save-credentials-in-python-dd5c6983741a",
        "document": "How To Securely Save Credentials in Python\n\nThe command is used in Ruby on Rails to open the credentials file in the Visual Studio Code editor, and the flag makes the command wait until the editor is closed before proceeding. Here is my first attempt, followed by a script run by Chris Oliver explaining everything in detail.\n\nIn Python, there isn’t a direct equivalent for editing files with an external editor like this, but you can achieve similar functionality by using the module to open a file with VS Code or any other text editor.\n\nHere’s how you can do it in Python:\n\nThis Python script opens the specified file in VS Code and waits for the editor to close before continuing with the rest of the script, similar to the Rails command.\n\nTo securely save credentials like API tokens, passwords, or other sensitive data in Python, you should avoid hard-coding these values directly into your scripts. Instead, consider using environment variables, encrypted storage, or a configuration management system. Here are a few secure methods to handle credentials in Python:\n\nEnvironment variables are a common way to store sensitive data securely. This approach keeps credentials out of your codebase and allows you to access them dynamically.\n\nYou can set environment variables in your operating system or through a shell before running your Python script:\n\nUse the module to access these variables in your Python script:\n\nA file can store environment variables locally. This file should be kept secure and not included in version control.\n\nUse the library to load environment variables from the file:\n\nThen, use it in your script:\n\nFor storing credentials securely on your system, use the library, which integrates with your operating system’s credential store (like macOS Keychain, Windows Credential Locker, or Linux Secret Service).\n\nReplace and with appropriate identifiers for your application.\n\nYou can also store sensitive data in encrypted files using libraries like . This method requires a decryption step each time you need access to the data.\n\nFor highly sensitive or production environments, use secrets management tools like AWS Secrets Manager, Azure Key Vault, Google Cloud Secret Manager, or HashiCorp Vault. These services provide more robust security and management capabilities.\n• Environment Variables and .env files: Simple and effective for many use cases.\n\nChoose the method that best fits your security requirements and deployment environment.\n\n01#Episode#PurePythonSeries — Send Email in Python — Using Jupyter Notebook — How To Send Gmail In Python\n\n02#Episode#PurePythonSeries — Automate Your Email With Python & Outlook — How To Create An Email Trigger System in Python\n\n03#Episode#PurePythonSeries — Manipulating Files With Python — Manage Your Lovely Photos With Python!\n\n05#Episode#PurePythonSeries — Is This Leap Year? Python Calendar — How To Calculate If The Year Is Leap Year and How Many Days Are In The Month\n\n08#Episode#PurePythonSeries — Decorator in Python — How To Simplifying Your Code And Boost Your Function\n\n11#Episode#PurePythonSeries — Python — Send Email Using SMTP — Send Mail To Any Internet Machine (SMTP or ESMTP)\n\n15#Episode#PurePythonSeries — ISS Tracking Project — Get an Email alert when International Space Station (ISS) is above of us in the sky, at night\n\n18#Episode#PurePythonSeries — Python — Efficient File Handling in Python — Best Practices and Common Methods (this one)\n\n19#Episode#PurePythonSeries — Python — How To Securely Save Credentials in Python — Like API tokens, passwords, or other sensitive data (this one)\n\nThe command is typically used in a shell script or a configuration file to set an environment variable. To use this in Python, you don't directly save this in a Python file, but rather in a file that initializes your environment before running your Python script.\n\nHere are a few common options for where to save this command:\n\nIf you want to set the environment variable globally and persistently across sessions, you should add the command to your shell profile file. The specific file depends on your operating system and the shell you’re using:\n• For shell on Linux or macOS: Add to or .\n• For shell on macOS: Add to .\n• For shell on Windows (using Git Bash or WSL): Add to .\n• For on Windows: Set a persistent environment variable using the PS drive.\n\nExample for Linux/macOS using bash:\n\nAdd the following line at the end of the file:\n• Save the file and close the editor.\n• Reload the file to apply the changes:\n\nIf you want to set the environment variable only for specific sessions or scripts, you can create a shell script:\n\nAdd the export command to the script:\n\nRun the script before your Python script to set the environment variable:\n\nIf you prefer to use a file with Python, the environment variables are stored there, and then loaded in your Python script using a library like .\n\nLoad the file in your Python script:"
    }
]