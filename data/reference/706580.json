[
    {
        "link": "https://docs.python-telegram-bot.org/en/v13.13",
        "document": "If you’re just starting out with the library, we recommend following our “Your first Bot” tutorial that you can find on our wiki. On our wiki you will also find guides like how to use handlers, webhooks, emoji, proxies and much more.\n\nA great way to learn is by looking at examples. Ours can be found in our examples folder on Github."
    },
    {
        "link": "https://docs.python-telegram-bot.org",
        "document": "We have made you a wrapper you can’t refuse\n\nWe have a vibrant community of developers helping each other in our Telegram group. Join us!\n\nStay tuned for library updates and new releases on our Telegram Channel.\n\nYou can install or upgrade via To install a pre-release, use the flag in addition. You can also install from source, though this is usually not necessary. To enable you to verify that a release file that you downloaded was indeed provided by the team, we have taken the following measures. Starting with v21.4, all releases are signed via sigstore. The corresponding signature files are uploaded to the GitHub releases page. To verify the signature, please install the sigstore Python client and follow the instructions for verifying signatures from GitHub Actions. As input for the parameter, please use the value . Earlier releases are signed with a GPG key. The signatures are uploaded to both the GitHub releases page and the PyPI project and end with a suffix . Please find the public keys here. The keys are named in the format . In addition, the GitHub release page also contains the sha1 hashes of the release files in the files with the suffix . tries to use as few 3rd party dependencies as possible. However, for some features using a 3rd party library is more sane than implementing the functionality again. As these features are optional, the corresponding 3rd party dependencies are not installed by default. Instead, they are listed as optional dependencies. This allows to avoid unnecessary dependency conflicts for users who don’t need the optional features. The only required dependency is httpx ~= 0.27 for , the default networking backend. is most useful when used along with additional libraries. To minimize dependency conflicts, we try to be liberal in terms of version requirements on the (optional) dependencies. On the other hand, we have to ensure stability of , which is why we do apply version bounds. If you encounter dependency conflicts due to these bounds, feel free to reach out. PTB can be installed with optional dependencies:\n• None installs the cryptography>=39.0.1 library. Use this, if you want to use Telegram Passport related functionality.\n• None installs httpx[socks]. Use this, if you want to work behind a Socks5 server.\n• None installs httpx[http2]. Use this, if you want to use HTTP/2.\n• None installs aiolimiter~=1.1,<1.3. Use this, if you want to use .\n• None installs the tornado~=6.4 library. Use this, if you want to use / .\n• None installs the cachetools>=5.3.3,<5.6.0 library. Use this, if you want to use arbitrary callback_data.\n• None installs the APScheduler>=3.10.4,<3.12.0 library. Use this, if you want to use the . To install multiple optional dependencies, separate them by commas, e.g. .\n• None installs all optional dependencies that are related to , i.e. .\n\nOnce you have installed the library, you can begin working with it - so let’s get started! Our Wiki contains an Introduction to the API explaining how the pure Bot API can be accessed via . Moreover, the Tutorial: Your first Bot gives an introduction on how chatbots can be easily programmed with the help of the module.\n• None The package documentation is the technical reference for . It contains descriptions of all available classes, modules, methods and arguments as well as the changelog.\n• None The wiki is home to number of more elaborate introductions of the different features of and other useful resources that go beyond the technical documentation.\n• None Our examples section contains several examples that showcase the different features of both the Bot API and . Even if it is not your approach for learning, please take a look at . It is the de facto base for most of the bots out there. The code for these examples is released to the public domain, so you can start by grabbing the code and building on top of it.\n• None The official Telegram Bot API documentation is of course always worth a read. If the resources mentioned above don’t answer your questions or simply overwhelm you, there are several ways of getting help.\n• None We have a vibrant community of developers helping each other in our Telegram group. Join us! Asking a question here is often the quickest way to get a pointer in the right direction.\n• None You can even ask for help on Stack Overflow using the python-telegram-bot tag. Since v20.0, is built on top of Pythons module. Because is in general single-threaded, does currently not aim to be thread-safe. Noteworthy parts of API that are likely to cause issues (e.g. race conditions) when used in a multi-threaded setting include:\n• None all classes in the module that allow to add/remove allowed users/chats at runtime\n\nOccasionally we are asked if we accept donations to support the development. While we appreciate the thought, maintaining PTB is our hobby, and we have almost no running costs for it. We therefore have nothing set up to accept donations. If you still want to donate, we kindly ask you to donate to another open source project/initiative of your choice instead.\n\nYou may copy, distribute and modify the software provided that modifications are described and licensed for free under LGPL-3. Derivative works (including modifications or anything statically linked to the library) can only be redistributed under LGPL-3, but applications that use the library don’t have to be."
    },
    {
        "link": "https://pypi.org/project/python-telegram-bot/13.14",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://pypi.org/project/python-telegram-bot/13.15",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://media.readthedocs.org/pdf/python-telegram-bot/latest/python-telegram-bot.pdf",
        "document": ""
    },
    {
        "link": "https://geeksforgeeks.org/exception-handling-of-python-requests-module",
        "document": "Python request module is a simple and elegant Python HTTP library. It provides methods for accessing Web resources via HTTP. In the following article, we will use the HTTP GET method in the Request module. This method requests data from the server and the Exception handling comes in handy when the response is not successful. Here, we will go through such situations. We will use Python’s try and except functionality to explore the exceptions that arise from the Requests module.\n• url: Returns the URL of the response\n• raise_for_status(): If an error occur, this method returns a HTTPError object\n• request: Returns the request object that requested this response\n• status_code: Returns a number that indicates the status (200 is OK, 404 is Not Found)\n\n\n\nThe first thing to know is that the response code is 200 if the request is successful.\n\nHere, we tried the following URL sequence and then passed this variable to the Python requests module using raised_for_status(). If the try part is successful, we will get the response code 200, if the page that we requested doesn’t exist. This is an HTTP error, which was handled by the Request module’s exception HTTPError and you probably got the error 404.\n\nYou could also use a general exception from the Request module. That is requests.exceptions.RequestException.\n\nNow, you may have noticed that there is an argument ‘timeout’ passed into the Request module. We could prescribe a time limit for the requested connection to respond. If this has not happened, we could catch that using the exception requests.exceptions.ReadTimeout. To demonstrate this let us find a website that responds successfully.\n\nIf we change timeout = 0.01, the same code would return, because the request could not possibly be that fast.\n\nAnother common error is that we might not specify HTTPS or HTTP in the URL. For example, We cause use requests.exceptions.MissingSchema to catch this exception.\n\nLet us say that there is a site that doesn’t exist. Here, the error will occur even when you can’t make a connection because of the lack of an internet connection\n\nHere, We put together everything we tried so far the idea is that the exceptions are handled according to the specificity.\n\nFor example, url = “https://www.gle.com”, When this code is run for this URL will produce an Exception request. Whereas, In the absence of connection requests.exceptions.ConnectionError will print the Connection Error, and when the connection is not made the general exception is handled by requests.exceptions.RequestException.\n\nNote: The output may change according to requests."
    },
    {
        "link": "https://stackoverflow.com/questions/16511337/correct-way-to-try-except-using-python-requests-module",
        "document": "Here's a generic way to do things which at least means that you don't have to surround each and every call with :\n• Be aware of which is a builtin, nothing to do with the class *. I assume the latter is more common in this context but have no real idea...\n• When examining a non- returned exception, , the superclass of all the exceptions (including ), is not according to the docs. Maybe it has changed since the accepted answer.**\n• Obviously this assumes a logger has been configured.\n• shows the call stack from here to point of failure, but tells you nothing about the call stack which called this function. However, the little-known and extremely useful optional kwarg will also print the stack up to the call.\n\n*I looked at the source code: subclasses the single class , which subclasses the single class (builtin)\n\n**However at the bottom of this page you find \"requests.exceptions.RequestException\" at the time of writing (2022-02)... but it links to the above page: confusing.\n\nFirst you check : if it's something funny has happened and will be an exception which has to be acted on in some way depending on context. If is then will be a Response object.\n\nAdvanced version, when json object is expected to be returned\n\nA couple of things are tedious with regard to the above: 1) how to stipulate that only one or more status codes are acceptable? 2) how to specify that the JSON dict returned must contain a certain structure of keys and perhaps subdictionaries? In the case of Elasticsearch, for example, you often have JSON objects returned and it is tedious to check that all the keys are actually present before obtaining the values for them.\n\nThis is therefore built on the above simple function.\n\nIf I uncomment the \"cats_and_dogs\" line it returns because this supposedly required key is missing. Conversely, if the required_dict check passes you can be sure that \"_source\" and \"es_version\" will not produce nasty s. You also know that key \"found\" had the value .\n\nNB the stipulating keys and values can be a nested to any depth; and a value \"None\" means \"value can be anything but at least check that the key is present at this location in the delivered dict\". \n\n It is also possible to make the code cope with s of s in the recursive function but this makes things a bit too complicated to include here, and will arguably be a very marginal requirement.\n\nThis uses a method other than \"get\" and, because it creates a new resource, the status code should be 201, not 200. NB the parameter can either be an or a (or indeed , meaning \"any status code\")."
    },
    {
        "link": "https://realpython.com/python-requests",
        "document": "The Requests library is the de facto standard for making HTTP requests in Python. It abstracts the complexities of making requests behind a beautiful, simple API so that you can focus on interacting with services and consuming data in your application.\n\nThroughout this tutorial, you’ll see some of the most useful features that Requests has to offer as well as ways to customize and optimize those features for different situations that you may come across. You’ll also learn how to use Requests in an efficient way as well as how to prevent requests to external services from slowing down your application.\n\nIn this tutorial, you’ll learn how to:\n• Make requests using the most common HTTP methods\n• Customize your requests’ headers and data using the query string and message body\n• Inspect data from your requests and responses\n• Configure your requests to help prevent your application from backing up or slowing down\n\nFor the best experience working through this tutorial, you should have basic general knowledge of HTTP. That said, you still may be able to follow along fine without it.\n\nIn the upcoming sections, you’ll see how you can install and use in your application. If you want to play with the code examples that you’ll see in this tutorial, as well as some additional ones, then you can download the code examples and work with them locally:\n\nEven though the Requests library is a common staple for many Python developers, it’s not included in Python’s standard library. There are good reasons for that decision, primarily that the library can continue to evolve more freely as a self-standing project. Note: Requests doesn’t support asynchronous HTTP requests directly. If you need async support in your program, you should try out AIOHTTP or HTTPX. The latter library is broadly compatible with Requests’ syntax. Because Requests is a third-party library, you need to install it before you can use it in your code. As a good practice, you should install external packages into a virtual environment, but you may choose to install into your global environment if you’re planning to use it across multiple projects. Whether you’re working in a virtual environment or not, you’ll need to install : Once has finished installing , you can use it in your application. Importing looks like this: Now that you’re all set up, it’s time to begin your journey through Requests. Your first goal will be learning how to make a request.\n\nA is a powerful object for inspecting the results of the request. Make that same request again, but this time store the return value in a variable so that you can get a closer look at its attributes and behaviors: In this example, you’ve captured the return value of , which is an instance of , and stored it in a variable called . You can now use to see a lot of information about the results of your request. The first bit of information that you can gather from is the status code. A status code informs you of the status of the request. For example, a status means that your request was successful, whereas a status means that the resource you were looking for wasn’t found. There are many other possible status codes as well to give you specific insights into what happened with your request. By accessing , you can see the status code that the server returned: returned , which means that your request was successful and the server responded with the data that you were requesting. Sometimes, you might want to use this information to make decisions in your code: With this logic, if the server returns a status code, then your program will print . If the result is a , then your program will print . Requests goes one step further in simplifying this process for you. If you use a instance in a conditional expression, then it’ll evaluate to if the status code was smaller than , and otherwise. Therefore, you can simplify the last example by rewriting the statement: In the code snippet above, you implicitly check whether the of is between and . If it’s not, then you raise an exception that includes the non-success status code in an f-string. Note: This truth value test is possible because is an overloaded method on . This means that the adapted default behavior of takes the status code into account when determining the truth value of the object. Keep in mind that this method is not verifying that the status code is equal to . The reason for this is that other status codes within the to range, such as and , are also considered successful in the sense that they provide some workable response. For example, the status code tells you that the response was successful, but there’s no content to return in the message body. So, make sure you use this convenient shorthand only if you want to know if the request was generally successful. Then, if necessary, you’ll need to handle the response appropriately based on the status code. Let’s say you don’t want to check the response’s status code in an statement. Instead, you want to use Request’s built-in capacities to raise an exception if the request was unsuccessful. You can do this using : If you invoke , then Requests will raise an for status codes between and . If the status code indicates a successful request, then the program will proceed without raising that exception. Now, you know a lot about how to deal with the status code of the response that you got back from the server. However, when you make a request, you rarely only care about the status code of the response. Usually, you want to see more. Next, you’ll see how to view the actual data that the server sent back in the body of the response. The response of a request often has some valuable information, known as a payload, in the message body. Using the attributes and methods of , you can view the payload in a variety of different formats. To see the response’s content in , you use : While gives you access to the raw bytes of the response payload, you’ll often want to convert them into a string using a character encoding such as UTF-8. will do that for you when you access : Because the decoding of to a requires an encoding scheme, Requests will try to guess the encoding based on the response’s headers if you don’t specify one. You can provide an explicit encoding by setting before accessing : If you take a look at the response, then you’ll see that it’s actually serialized JSON content. To get a dictionary, you could take the that you retrieved from and deserialize it using . However, a simpler way to accomplish this task is to use : The of the return value of is a dictionary, so you can access values in the object by key: You can do a lot with status codes and message bodies. But, if you need more information, like metadata about the response itself, then you’ll need to look at the response’s headers. The response headers can give you useful information, such as the content type of the response payload and a time limit on how long to cache the response. To view these headers, access : returns a dictionary-like object, allowing you to access header values by key. For example, to see the content type of the response payload, you can access : There’s something special about this dictionary-like headers object, though. The HTTP specification defines headers as case-insensitive, which means that you’re able to access these headers without worrying about their capitalization: Whether you use the key or , you’ll get the same value. Now that you’ve seen the most useful attributes and methods of in action, you already have a good overview of Requests’ basic usage. You can get content from the Internet and work with the response that you receive. But there’s more to the Internet than plain and straightforward URLs. In the next section, you’ll take a step back and see how your responses change when you customize your requests to account for query string parameters.\n\nAccording to the HTTP specification, , , and the less common requests pass their data through the message body rather than through parameters in the query string. Using Requests, you’ll pass the payload to the corresponding function’s parameter. takes a dictionary, a list of tuples, bytes, or a file-like object. You’ll want to adapt the data that send in the body of your request to the specific needs of the service that you’re interacting with. For example, if your request’s content type is , then you can send the form data as a dictionary: You can also send that same data as a list of tuples: If, however, you need to send JSON data, then you can use the parameter. When you pass JSON data via , Requests will serialize your data and add the correct header for you. Like you learned earlier, the httpbin service accepts test requests and responds with data about the requests. For instance, you can use it to inspect a basic request: You can see from the response that the server received your request data and headers as you sent them. Requests also provides this information to you in the form of a that you’ll inspect in more detail in the next section.\n\nAuthentication helps a service understand who you are. Typically, you provide your credentials to a server by passing data through the header or a custom header defined by the service. All the functions of Requests that you’ve seen to this point provide a parameter called , which allows you to pass your credentials: The request succeeds if the credentials that you pass in the tuple to are valid. When you pass your credentials in a tuple to the parameter, Requests applies the credentials using HTTP’s Basic access authentication scheme under the hood. You may wonder where the string that Requests set as the value for your header comes from. In short, it’s a Base64-encoded string of the username and password with the prefix :\n• First, Requests combines the username and password that you provided, putting a colon in between them. So for the username and password , this becomes .\n• Then, Requests encodes this string in Base64 using . The encoding converts the string to .\n• Finally, Requests adds in front of this Base64 string. This is how the final value for the header becomes in the example shown above. HTTP Basic authentication isn’t very secure, because you can decode the username and password from the Base64 string. That’s why it’s important to always send these requests over HTTPS, which provides an additional layer of security by encrypting the entire HTTP request. You could make the same request by passing explicit Basic authentication credentials using : Though you don’t need to be explicit for Basic authentication, you may want to authenticate using another method. Requests provides other methods of authentication out of the box, such as and . A real-world example of an API that requires authentication is GitHub’s authenticated user API. This endpoint provides information about the authenticated user’s profile. If you try to make a request without credentials, then you’ll see that the status code is : If you don’t provide authentication credentials when accessing a service that requires them, then you’ll get an HTTP error code as a response. To make a request to GitHub’s authenticated user API, you first need to generate a personal access token with the read:user scope. Then you can pass this token as the second element in a tuple to : Like you learned previously, this approach passes the credentials to , which expects a username and a password and sends the credentials as a Base64-encoded string with the prefix : This works, but it’s not the right way to authenticate with a Bearer token—and using an empty string input for the superfluous username is awkward. With Requests, you can supply your own authentication mechanism to fix that. To try this out, create a subclass of and implement : \"\"\"Attach an API token to the Authorization header.\"\"\" Here, your custom mechanism receives a token, then includes that token in the header of your request, also setting the recommended prefix to the string. You can now use this custom token authentication to make your call to GitHub’s authenticated user API: Your custom created a well-formatted string for the header. You can now use this more intuitive way of interacting with a token-based authentication scheme such as the one that parts of GitHub’s API require. Note: While you could construct the authentication string outside of a custom authentication class and pass it directly with , this appoach is discouraged because it can lead to unexpected behavior. When you attempt to set your authentication credentials directly using , then Requests may internally overwrite your input. This can happen, for example, if you have a file that provides authentication credentials. Requests will attempt to get the credentials from the file if you don’t provide an authentication method using . Bad authentication mechanisms can lead to security vulnerabilities. Unless a service requires a custom authentication mechanism for some reason, you’ll always want to use a tried-and-true auth scheme like the built-in Basic authentication or OAuth, for example through Requests-OAuthlib. While you’re thinking about security, consider dealing with SSL certificates using Requests.\n\nAnytime the data that you’re trying to send or receive is sensitive, security is important. The way that you communicate with secure sites over HTTP is by establishing an encrypted connection using SSL, which means that verifying the target server’s SSL certificate is critical. The good news is that Requests does this for you by default. However, there are some cases where you might want to change this behavior. If you want to disable SSL certificate verification, then you pass to the parameter of the request function: InsecureRequestWarning: Unverified HTTPS request is being made to host Requests even warns you when you’re making an insecure request to help you keep your data safe! Note: Requests uses a package called to provide certificate authorities. This lets Requests know which authorities it can trust. Therefore, you should update frequently to keep your connections as secure as possible. Now that you know how to make all sorts of HTTP requests using Requests, authenticated or not, you may wonder about how you can make sure that your program works as quickly as possible. In the next section, you’ll learn about a few ways that you can improve performance with the help of Requests.\n\nWhen using Requests, especially in a production application environment, it’s important to consider performance implications. Features like timeout control, sessions, and retry limits can help you keep your application running smoothly. When you make an inline request to an external service, your system will need to wait for the response before moving on. If your application waits too long for that response, requests to your service could back up, your user experience could suffer, or your background jobs could hang. By default, Requests will wait indefinitely on the response, so you should almost always specify a timeout duration to prevent these issues from happening. To set the request’s timeout, use the parameter. can be an integer or float representing the number of seconds to wait on a response before timing out: In the first request, the request will time out after 1 second. In the second request, the request will time out after 3.05 seconds. You can also pass a tuple to with the following two elements:\n• Connect timeout: The time it allows for the client to establish a connection to the server\n• Read timeout: The time it’ll wait on a response once your client has established a connection Both of these elements should be numbers, and can be of type or : If the request establishes a connection within 3.05 seconds and receives data within 5 seconds of the connection being established, then the response will be returned as it was before. If the request times out, then the function will raise a exception: \"The request did not time out\" Your program can catch the exception and respond accordingly. Until now, you’ve been dealing with high-level APIs such as and . These functions are abstractions of what’s going on when you make your requests. They hide implementation details, such as how connections are managed, so that you don’t have to worry about them. Underneath those abstractions is a class called . If you need to fine-tune your control over how requests are being made or improve the performance of your requests, you may need to use a instance directly. Sessions are used to persist parameters across requests. For example, if you want to use the same authentication across multiple requests, then you can use a session: In this code example, you use a context manager to ensure that the session releases the resources when it doesn’t need them anymore. In line 7, you log in using your custom . You only need to log in once per session, and then you can make multiple authenticated requests. Requests will persist the credentials while the session exists. You then make two requests to the authenticated user API in lines 9 and 10 using instead of . The primary performance optimization of sessions comes in the form of persistent connections. When your app makes a connection to a server using a , it keeps that connection around in a connection pool. When your app wants to connect to the same server again, it’ll reuse a connection from the pool rather than establishing a new one. When a request fails, you may want your application to retry the same request. However, Requests won’t do this for you by default. To apply this functionality, you need to implement a custom transport adapter. Transport adapters let you define a set of configurations for each service that you’re interacting with. For example, say you want all requests to to retry two times before finally raising a . You’d build a transport adapter, set its parameter, and mount it to an existing : In this example, you’ve set up your session so that it’ll retry a maximum of two times when your request to GitHub’s API doesn’t work as expected. When you mount the —in this case, —to , then will adhere to its configuration for each request to . Note: While the implementation shown above works, you won’t see any effect of the retry behavior unless there’s something wrong with your network connection or GitHub’s servers. If you want to play around with code that builds on top of this example, and you’d like to inspect when the retries happen, then you’re in luck. You can download the materials of this tutorial and take a look at : Get Your Code: Click here to download the free sample code that shows you how to use Python’s Requests library. The code in this file improves on the example shown above by using the underlying to further customize the retry functionality. It also adds logging to display debugging output, which gives you a chance to monitor when Python attempted the retries. Requests comes packaged with intuitive implementations for timeouts, transport adapters, and sessions that can help you keep your code efficient and your application resilient."
    },
    {
        "link": "https://datacamp.com/tutorial/making-http-requests-in-python",
        "document": "Master the basics of data analysis with Python in just four hours. This online course will introduce the Python interface and explore popular packages."
    },
    {
        "link": "https://docs.chainstack.com/docs/best-practices-for-error-handling-in-api-requests",
        "document": "\n• Explains how Chainstack’s global node feature can boost your DApp’s reliability by balancing traffic automatically based on user location.\n• Demonstrates a JavaScript load balancer script using multiple Chainstack endpoints, distributing requests across different regions to avoid single-point failures.\n• Shows examples with both web3.js and ethers.js, detailing how to fail over to the next endpoint if one fails.\n• Concludes that both global nodes and custom load-balancing approaches help ensure your blockchain app can handle high traffic and unexpected downtimes.\n\nIn the world of API requests, error handling is not just a best practice—it's a necessity. Effectively handling HTTP status codes is crucial for ensuring smooth and reliable communication between clients and servers. Whether you're a seasoned developer or just starting out, understanding how to automate the retrieval of response codes from any request can help you build more robust applications, implement effective retry logic, and create comprehensive error backlogs. This guide will walk you through the best practices for error handling in API requests, with a focus on handling HTTP status codes and implementing retry logic.\n\nHTTP status codes are the server's way of telling the client about the status of the operation it requested. They play a vital role in API requests as they can indicate success, failure, or need for further action. By properly handling these status codes, you can ensure your application responds appropriately to each possible outcome of an API request. This can significantly enhance the user experience and the overall performance of your application.\n\nHTTP status codes are grouped into five major categories, each representing a specific class of responses. These include:\n• 1xx (informational) — the request has been received and understood, and the client should continue the process.\n• 2xx (success) — the action was successfully received, understood, and accepted.\n• 3xx (redirection) — the client must take additional action to complete the request.\n• 4xx (client errors) — the request contains bad syntax or cannot be fulfilled.\n• 5xx (server errors) — the server failed to fulfill an apparently valid request.\n\nUnderstanding these status codes and how to handle them is the first step toward effective error handling in API requests. In the following sections, we'll dive deeper into how to retrieve and handle these status codes in your Python code and how to implement a retry logic for temporary failures.\n\nBefore we can handle HTTP status codes, we first need to know how to retrieve them. In Python, this can be done using the attribute of the response object. This attribute holds the status code that the server returned for the HTTP request.\n\nLet's consider a scenario where we're interested in getting the logs of the latest block. We can do this using the following Python code:\n\nIf the above code is successfully run, it will output the logs for the latest block. This means that the response code received by the client (you, who made the request) was equal to 200. To retrieve the response code of the request presented above, we can simply use the following:\n\nThis will store the HTTP status code of the response in the variable. Now that we know how to retrieve the status code of a response, we can move on to handling these codes and analyzing error responses.\n\nIn addition to dealing with response codes, it's also important to analyze other information in the response to understand and deal with errors. This can be particularly useful when the server returns a 4xx or 5xx status code, indicating a client or server error.\n\nFor instance, let's consider a possible response for a request that contains an error content in the output:\n\nIn this case, the server returned a JSON object with an field, which contains further information about the error that occurred. We can extract this information in our Python code like this:\n\nIn this code, we first check if the response's status code is 200, indicating a successful request. If it is, we parse the JSON content of the response and check if it contains an field. If it does, we store the content of this field in the variable. This information can be used to implement a retry logic and keep a record of whenever those errors happen in time.\n\nIncorporating retry logic into your code can significantly enhance the reliability of your application. By leveraging the tools and techniques we have discussed, you can implement a retry mechanism that automatically handles temporary failures and retries the request when necessary. This can reduce the impact of temporary failures on you, increase system availability, and ensure data integrity. In the worst-case scenario, this enables you to keep track of the errors you face with precise timestamps for such incidents.\n\nImplementing retry logic is particularly important when dealing with 5xx server errors. These errors indicate a problem with the server and are often temporary. By implementing a retry logic, your application can automatically retry the request after a short delay, giving the server a chance to recover. This can significantly improve the user experience by reducing the number of failed requests the user has to deal with.\n\nNow that we understand the importance of implementing retry logic let's dive into how to implement it in our Python code. Our retry logic aims to automatically retry the request when a temporary failure occurs. This can be a 5xx server error, a connection error, or any other type of error that we deem temporary.\n\nHere's an example of how to implement retry logic in Python using both the response code and error messages to determine when to retry a request:\n\nThe retry logic is governed by a for loop that runs up to a predefined maximum number of attempts (the variable). For each iteration of the loop, which represents an attempt to fetch the logs, the code performs the following steps:\n• A request is sent to the Ethereum node with the defined headers and payload.\n• If the HTTP status code of the response is not 200 (indicating a successful request), the code prints a message indicating that the request failed and the current attempt number. Then, it waits for the specified delay period (the variable) before proceeding to the next iteration of the loop. This delay provides a pause before retrying, which can be helpful in cases where the server might be temporarily overloaded or experiencing other transient issues.\n• If the status code is 200 (indicating a successful request), the response is parsed into JSON format and checked for an key. If is present, the code prints a message with the error details and the current attempt number, waits for the specified delay period, and proceeds to the next iteration of the loop. This handles cases where the request was technically successful, but the response indicates an error condition that might be resolved with a retry.\n• If there's no key in the response but the is empty, the code prints a message indicating this fact and the current attempt number, waits for the specified delay period, and proceeds to the next iteration of the loop. This handles situations where the request was successful and didn't result in an error but didn't provide any logs to process.\n\nIf the function hasn't returned by the end of the loop (meaning it hasn't successfully processed a set of logs), it will have retried the request the maximum number of times. At this point, the function will exit, and the code will continue, effectively giving up on fetching logs after exhausting all the allowed attempts.\n\nUsing response code and error messages in retry logic Using response code and error messages in retry logic\n\nAs you can see in the above example, we use both the response code and error messages in our retry logic. The response code allows us to determine whether the request was successful, while the error messages provide more detailed information about what went wrong.\n\nBy using both of these pieces of information, we can make our retry logic more intelligent and effective. For example, we can decide to retry the request immediately if the error message indicates a temporary problem with the server or wait for a longer delay if the error message indicates a more serious problem.\n\nIn addition, by logging the error messages, we can keep a record of the errors that occurred, which can be useful for debugging and improving our application.\n\nWhile handling HTTP status codes and implementing retry logic can significantly improve the reliability of your application, there are a few common problems and gotchas that you should be aware of.\n\nAnother common problem is the lack of effective retry logic and robust error backlogs. Without these, your application may not be able to recover from temporary failures, resulting in poor user experience and potential data loss.\n\nAn effective retry logic should take into account the nature of the error and adjust its behavior accordingly. For example, if the error is temporary (such as a 5xx server error), the retry logic should wait for a short delay before retrying the request. If the error is permanent (such as a 4xx client error), the retry logic should not retry the request and should log the error and notify the user.\n\nA robust error backlog, on the other hand, can help you keep track of the errors that occur in your application, allowing you to debug and fix issues more effectively. It can also provide valuable insights into the performance and reliability of your application, helping you identify areas for improvement.\n\nHandling HTTP status codes and implementing retry logic are crucial aspects of working with API requests. They ensure smooth and reliable communication between clients and servers and enhance your applications' overall performance and resilience. As the volume and complexity of data continue to increase, the importance of these practices cannot be overstated.\n\nRemember, the key to effective error handling is understanding the different types of HTTP status codes and how to handle them. This includes knowing how to retrieve these codes, analyze error responses, and implement robust retry logic. By doing so, you can build applications that are capable of handling temporary failures and maintaining data integrity, even in the face of increasing data volume and complexity.\n\nHowever, it's also important to be aware of the common challenges and gotchas associated with these practices. This includes dealing with the constantly growing data in Web3, implementing effective retry logic, and maintaining robust error backlogs. By being aware of these challenges and knowing how to handle them, you can ensure that your applications remain reliable and resilient, no matter what comes their way.\n\nIn conclusion, while error handling in API requests can be complex, it's an essential skill for any developer working with APIs. By following the best practices outlined in this guide, you can ensure that your applications are well-equipped to handle any errors that may occur, resulting in a better user experience and a more reliable application."
    },
    {
        "link": "https://blog.vidocsecurity.com/blog/api-security-best-practices-for-python-developers-part-ii",
        "document": "Part II of the Developer’s Guide for a secure API implementation. Devs are the core of web applications, that's why you should continue learning how to prevent common attacks and secure your endpoints correctly. Avoid deploying vulnerable code by taking into account these Security Best Practices.\n\nWe are here to show you the OWASP Top 10 API Security to develop secured coding and stand out from other developers. We covered the first five Security Best Practices in a previous blogpost. Make sure to give it a look if you haven't already. Know your vulnerabilities!\n\nObjects in web applications usually contain many properties, which sometimes need to be updated by the users, like , or . However there are some properties that are set by the application, and should not be modified or accessed by the user. Like , or\n\nAn API endpoint is vulnerable to Mass Assignment if it automatically converts client input into internal object properties, without first validating which parameters are being updated and saved.\n\nBad actors can modify request's bodies to update or overwrite sensitive object’s properties that developers never intended to expose, leading to privilege escalation, data tampering, security mechanisms bypass, and more.\n\nIn the example above , the application thinks that is secure, with only as body because there is only one \"Email Input\" displayed on the web's UI for users to update their emails when necessary. However, instead of checking that is the only key in the JSON input, or just grabbing that key, the back-end is appending both pieces of data, without verifying the information they contain.\n\nIf instead of a user, we have a bad actor using the API then it may try to update more sensitive information to gain privileged access to the application. Modifying to contain both and values like , when processed, it will not only update the user's email, but add an admin key to the object if it didn't exist or override it with the new value if present. Making the user now have administrator privileges.\n\nHow to fix this vulnerability\n\nOther tips for fixing and preventing Mass Assignment vulnerabilities:\n• If possible, avoid using functions that automatically bind a client’s input into code variables or internal objects.\n• Whitelist only the properties that should be updated by the client.\n• Use built-in features to blacklist properties that should not be accessed by clients.\n• If applicable, explicitly define and enforce schemas for the input data payloads.\n\nThis vulnerability can take many forms and shapes. Attackers will often attempt to find unpatched versions of frameworks or dependencies the application is using; common configurations or debugging endpoints; or unprotected files and directories to gain unauthorized access or knowledge of your system.\n\nBecause of this, security misconfiguration can happen at any level of the API stack, from the network level to the application level. And they can not only expose sensitive user data, but also system details that may lead to full server compromise.\n\nFor example, in the case above, you can see several security misconfigurations, which also lead to further exploitation:\n\n1. An admin panel that is open to the internet, instead of locally accessible, it may even use default credentials. \n\n2. It's using insecure protocol because it's not communicating via .\n\n3. Framework fingerprints are visible, for both PHP and MySQL. If you dig deeper, you can also check the NGINX information.\n\n4. The previous point leads to further exploitation because you can see with a quick search that phpMyAdmin 2.2.0 has a directory traversal vulnerability and MySQL 4.0.26 has more than one known vulnerability.\n\n5. There is also leakage of sensitive data, because you can check runtime information, system variables and processes; as well as the endpoint.\n\nOther tips for fixing and preventing Security Misconfiguration vulnerabilities:\n• Make sure debugging is turned off on production environments.\n• Use the respective secure headers via a secure communication channel for all API interactions, including static assets.\n• Make sure your dependencies and frameworks are up to date.\n• Lock down every environment with its own proper configuration depending on its usage.\n• Erase all default configuration that is not being actively used.\n• Review and update configurations across the entire API stack. The review should include: orchestration files, API components, and cloud services (e.g., S3 bucket permissions).\n• Continuously assess the effectiveness of the configuration and settings in all environments.\n\nBad actors will feed the API with malicious code through whatever injection vectors are available, such as user inputs, url parameters, integrated services configurations, etc, expecting it to be sent to an interpreter that executes them.\n\nThis happens mainly because client-supplied data is not validated, filtered, or sanitized by the API back-end before being used or concatenated to SQL/NoSQL/LDAP queries, OS commands, XML parsers, and Object Relational Mapping (ORM)/Object Document Mapper (ODM). As mentioned in the Part I, don't only trust on your front-end for input validations, they can be easily bypassed, make sure to always validate in the back-end as well.\n\nIn this case, you can see that if user sends a POST request body like , then it would get deleted due to the system's call to the command. However, the back-end logic is not validating whether the input is safe or if it contains something that shouldn't be there. \n\nFor example, what would happen if the user sends a POST request body like . You are right, the application would run the command to delete and then it would proceed to delete the whole directory of the server.\n\nHow to fix this vulnerability\n\nOther tips for fixing and preventing Injection vulnerabilities:\n• Validate, filter, and sanitize all client-provided data, or other data coming from integrated systems.\n• Special characters should be escaped using the specific syntax for the target interpreter.\n• Always limit the number of returned records to prevent mass disclosure in case of injection.\n• Validate incoming data using sufficient filters to only allow valid values for each input parameter.\n• Define data types and strict patterns for all string parameters.\n\nIf there is no updated documentation of all API versions deployed it's quite easy to miss one or two things along the way. The lack of assets inventory and retire strategies leads to running unpatched systems, resulting in leakage of sensitive data.\n\nIt’s common to find unnecessarily exposed API hosts because of modern concepts like microservices, which make applications easy to deploy and independent from each other. Old API versions are an easy way to compromise systems without having to fight state-of-the-art security mechanisms, which might be in place to protect the most recent API versions.\n\nBad actors may gain access to sensitive data, or even takeover the server through old, unpatched API versions still connected to valid databases or that have in place less security configurations or backdoors to make testing easier.\n\nOther tips for fixing and preventing Improper Assets Management vulnerabilities:\n• Inventory all API hosts and document important aspects of each one of them, focusing on the API environment (e.g., production, staging, test, development), who should have network access to the host (e.g., public, internal, partners) and the API version.\n• Inventory integrated services and document important aspects such as their role in the system, what data is exchanged (data flow), and its sensitivity.\n• Document all aspects of your API such as authentication, errors, redirects, rate limiting, cross-origin resource sharing (CORS) policy and endpoints, including their parameters, requests, and responses.\n• Generate documentation automatically by adopting open standards and available libraries. Include the documentation build in your CI/CD pipeline.\n• Use external protection measures such as API security firewalls for all exposed versions of your APIs, not just for the current production version.\n• Avoid using production data with non-production API deployments. If this is unavoidable, these endpoints should get the same security treatment as the production ones.\n• When newer versions of APIs include security improvements, perform risk analysis to make the decision of the mitigation actions required for the older version: for example, whether it is possible to backport the improvements without breaking API compatibility or you need to take the older version out quickly and force all clients to move to the latest version.\n\nAttackers may take advantage of lack of logging and monitoring to abuse systems without being noticed, as it's almost impossible to track suspicious activities and respond to them in a timely fashion.\n\nWithout visibility over ongoing malicious activities, attackers have plenty of time to fully compromise systems without being detected.\n\nOther tips for fixing and preventing Insufficient Logging & Monitoring vulnerabilities:\n• Logs should be written using a format suited to be consumed by a log management solution, and should include enough detail to identify the malicious actor.\n• Logs should be handled as sensitive data, and their integrity should be guaranteed at rest and transit.\n• Configure a system to continuously monitor the infrastructure, network, and the API functioning.\n• Use a Security Information and Event Management (SIEM) system to aggregate and manage logs from all components of the API stack and hosts.\n• Configure custom dashboards and alerts, enabling suspicious activities to be detected and responded to earlier.\n\nIf you ended up here with us is because you're interested in improving your developer skills by implementing Security Best Practices. We guided you here, now it's your turn to assess your API and patch any vulnerabilities you may have.\n\nDid you like what you read? Because we have several technical blogs like this one waiting for you. Check them out here!\n\n\n\nCheck out our other social media platforms to stay connected:‎"
    },
    {
        "link": "https://reddit.com/r/learnpython/comments/o6k1wa/best_practices_for_pythonbased_client",
        "document": "TL;dr: What would be the best practices for writing a Python application that interfaces with a public service, when that service requires each app to have an app API key, and all users of the app must use the same key?\n\nLet's use the example of a Twitter client written in Python. The Twitter API requires that all apps have an API key, and all instances of the app use the same key. Additionally, users do have user keys based on their logins, which the app can store securely. The issue is the global API key.\n• It is definitely easy to keep the API key out of source control by using gitignore and some secure method of automated builds to pull in the key. However, that still means the distributed package will contain the API key somewhere.\n• Some other posts suggested running an intermediate server that manages the API key, and having the app communicate with that server. There are serious security and privacy concerns here when it comes to an app like Twitter; for example, an intermediate server would by design have access to direct message conversations among other issues.\n• Loss of the API key (someone finding a way to extract it from the code) compromises the entire application; a bad actor could use the API key to abuse API access, thus locking out all users of the app.\n\nSimple obfuscation of the API key through some sort of Python string manipulation might help the extremely casual hacker from just using something like strings on the pyc/pyo files. However, is that sufficient for general use when it comes to securely storing an API key?\n\nEDIT: An additional note - even obfuscating the key in code would be quite easy to defeat. A hacker could simply import the .pyc file, use dir() to determine its functions, and ultimately just run the de-obfuscation function and get the key out. Same would apply even if the API key were stored obfuscated in an external text file. Even if the de-obfuscation were in a C file, it would still be possible to call the .pyd file from Python and de-obfuscate it.\n\nThis is not a problem unique to Python; many other services require you to get an app ID and app key, and many other programming languages thus need to be able to somehow securely store that API key in a widely distributed application.\n\nHow is this done?"
    },
    {
        "link": "https://quora.com/What-are-some-of-the-best-practices-for-working-with-APIs-in-Python",
        "document": "Something went wrong. Wait a moment and try again."
    },
    {
        "link": "https://curity.io/resources/learn/api-security-best-practices",
        "document": "With the rising threat of cyberattacks, securing APIs has become business-critical. Especially as many security reports indicate that web APIs are quite vulnerable. Thankfully, by following a few best practices, API providers can ward off many potential vulnerabilities. Below, we cover top API security best practices, which are good things to keep in mind when designing and creating APIs.\n\nOur first recommendation is to always put your API behind a gateway. API gateways centralize traffic features and apply them to every request that hits your API. These features may be security-related, like rate limiting, blocking malicious clients, and proper logging. Or, they may be more practical and business-related, like path and headers rewriting, gathering business metrics, and so on.\n\nNot having these controls could easily result in a serious security threat. Without a gateway, API providers would have to reinforce each endpoint with these features one-by-one. An API gateway eases the process of adding or fixing these features. Thankfully, there are plenty of API gateway products available on the market.\n\nNext, do not let your APIs or gateways issue access or refresh tokens. A centralized OAuth server should always issue such tokens. Issuing tokens requires many complex processes: authenticating the client, authenticating the user, authorizing the client, signing the tokens, and other operations. All these functions require access to different data, such as client information or the preferred authentication mechanism. Furthermore, if many entities issue and sign tokens, it becomes increasingly challenging to manage all the credentials used for signing. Only one entity can safely handle these processes — an OAuth server.\n\nWhen APIs are concerned, using JSON Web Tokens (JWTs) as access and refresh tokens is a good practice. Services that receive JWTs can leverage claim information to make informed business decisions: Is the caller allowed to access this resource? What data can the caller retrieve?\n\nHowever, when tokens are exposed outside your infrastructure and especially when exposed to third-party clients, you should use opaque tokens instead of JWTs. Information in a JWT is easy to decode and thus available to everyone. If JWT data is public, privacy becomes a concern. You must ensure that no sensitive data ends up in the JWT's claims. What is more, if you share JWTs with third-party clients, chances are that they will start depending on the data in the JWT. It might become a liability, even if the data is not sensitive. Once integrators start depending on the contents of a JWT, changing the token's claims could result in a breaking change, requiring costly implementation upgrades in all third-party clients.\n\nIf you want to use opaque tokens externally but also benefit from JWTs in your internal communication, you can use one of two approaches: the phantom token approach or the split token approach. Both involve an API gateway in the process of translating an opaque token into a JWT.\n\nWhenever a service calls another service, you need to consider the access token to send to the upstream service. In small setups with only a couple of APIs you might forward the original access token. However, this solution does not scale well, since reusing the same token gives the receiving service all permissions that are associated with the access token.\n\nInstead, use the token exchange flow to obtain tokens that contain enough information for the receiving service to authorize the request, but do not allow it to reuse the token elsewhere. This is especially important when requests cross security boundaries, like when you call services external to your organization.\n\nWhen using token exchange, never allow services to handle the same token that the client sends to the API. For example, use the phantom token approach described earlier. Otherwise, your services will be able to call your publicly-facing API endpoints creating a vector for abuse. You can also utilize other token sharing techniques, like embedded tokens to achieve similar results.\n\nOAuth scopes limit the capabilities of an access token. If stolen client credentials have limited scopes, an attacker will have much less power. Therefore, you should always issue tokens with limited capabilities. Verification of token scopes can be done at the API gateway to limit the malicious traffic reaching your API. You should use scopes during coarse-grained access control. This control could include checking whether a request with a given access token can query a given resource or verifying the client can use a given Content-Type.\n\n6. Use Claims for Fine-Grained Access Control at the API Level\n\nYou should always implement fine-grained access control at the API level. This access control complements any control done at the API gateway level, and should be architected so that even if a malicious request slips through the gateway, the API will still reject it. This practice safeguards against situations in which attackers bypass the gateway.\n\nA fine-grained access control focuses on securing an API from a business perspective. The API should verify whether the request can reach the given endpoint. It should also check whether the caller has rights to the data and what information can be returned based on the caller's identity (both for the client and user). The 2019 OWASP Top 10 API Security Vulnerabilities lists broken object level authorization (BOLA) as the top API vulnerability, so it's worth remembering this one.\n\n​Zero-trust is not just a buzzword — your API should limit trust to incoming traffic. Period. One of the steps toward building zero-trust is using HTTPS for all API traffic. If possible, use HTTPS internally so that traffic between services cannot be sniffed.\n\nYour services should always verify incoming JWTs, even if they are transformed from an opaque token by the gateway. This again helps to mitigate situations where a request manages to bypass your gateway, preventing a malicious actor from operating inside your company or infrastructure.\n\nZero-trust also means that your services should deny access by default. Then use claims-based access control to allow access to requests that fulfill concrete access control policies.\n\nProper JWT validation is crucial for the security of your APIs. Yet, if every team implements their own JWT validation solution, you risk increasing overall system vulnerability. Mistakes are more common, and it's difficult to fix bugs.\n\nInstead, create a company-wide solution for JWT validation, preferably based on libraries available on the market and tailored to your API's needs. Standardizing a company-wide JWT validation process will help guarantee the same level of security across all your endpoints. When issues arise, teams can resolve them more quickly. For security-sensitive tasks like JWT validation, quick threat resolution is incredibly important.\n\nDo not mix authentication methods for the same resources. Authentication methods can have different security levels. For example, consider Basic authentication versus multi-factor authentication. If you have a resource secured with a higher level of trust, like a JWT with limited scopes, but allow access with a lower level of trust, this can lead to API abuse. In some cases, this could be a significant security risk.\n\nDo not leave any of your APIs unprotected. Even internal APIs should have protections implemented. This way, you're sure that the API is protected from any threat from inside your organization.\n\nAPIs are commonly created for internal use only and made available to the public later on. In such scenarios, proper API security tends to be overlooked. When published externally, the API becomes vulnerable to attacks.\n\nRemember that security by obscurity is not recommended. Just because you create a complicated name for an endpoint or use an obscure Content-Type does not mean the API will be secure. It's only a matter of time before someone finds the endpoint and abuses it.\n\nIf you have internal clients operating only inside your network, you can have your OAuth server issue JWTs for such clients instead of opaque tokens. This will avoid unnecessary token translations. However, you should only apply this strategy if the JWTs do not leave your network. If you have external clients, or if the tokens are used externally, you should hide them behind an opaque token, as noted before.\n\nTo verify a JWT's integrity, an API must access a public key (if the JWT is asymmetrically signed, as recommended). You can accomplish this in a couple of ways: you can hardcode the key's value or query some endpoint at your service startup and cache the result.\n\nThe recommended method is to obtain a key from a JWKS endpoint exposed by the OAuth server. The API should cache the downloaded key to limit unnecessary traffic but should query the JWKS endpoint again whenever it finds a signing key it doesn't know.\n\nThis allows for a simple key rotation, which the OAuth server can handle on-demand without impeding the API services. Using key sets instead of keys also allows a seamless key rotation for the clients. The OAuth server can begin issuing new tokens signed with a new key but existing tokens will remain valid as long as the old public key is part of the key set.\n\nMaintaining high standards for your APIs, both from a security and design point of view, is not a trivial task. Therefore, consider splitting responsibility between different groups of people and having other teams audit your APIs.\n\nThere are different approaches to setting up governance over your API. You could have a dedicated team of API experts review the design and security aspects, or create a guild of API experts picked from different groups to offer guidance. However you organize governance, ensure you always have additional eyes checking your APIs.\n\nAs defined by the JWT specification, a claim is a piece of information asserted about a subject. It's good practice to have these claims asserted by a centralized OAuth server — this makes it easier to control which claims appear in your tokens. This is important for privacy and security reasons.\n\nWhether calling internal or external services, all APIs should only use claims asserted by the centralized server and should not add additional information nor issue tokens. Managing claims centrally allows you to control the information flowing between the APIs to ensure they do not leak excess data.\n\n15. Abuse Doesn't Have to Be a Breach\n\nJust because your API security isn't breached doesn't mean that everything is fine. You should gather metrics and log usage of your API to catch any unwanted behavior. Watch out for requests iterating over your IDs, requests with unexpected headers or data, customers creating many clients to circumvent rate limits, and other suspicious cues. Losing data due to API abuse can be just as harmful to your business as a hacker breaking through the security.\n\nAlthough not concerning APIs directly, an important part of a secure API is how securely access tokens are handled by clients. If access tokens can easily be stolen, they can then be used to steal data from an API. Mobile and backend clients can store those tokens pretty securely, but it is not the case with browser-based applications. Single Page Applications developers often wonder how to securely keep tokens in the browser, which should be treated as a hostile environment. The OAuth for Browser-Based Apps specification currently recommends keeping the tokens out of the browser altogether. This can be achieved by introducing lightweight backend components that are capable of safeguarding the tokens and issuing secure cookie-based sessions. The pattern is known as backend-for-frontend or token handler.\n\nSecuring an API with high-standard security is a paramount concern. As seen above, there are many technical strategies to consider when designing your authorization processes, which, if undermined, can directly affect API security. A stronger foundation is only made possible with a secure, centralized OAuth server responsible for token issuance and claims assertion. Many suggestions also revolve around treating internal APIs with the same care as public-facing endpoints. By following these protective measures, you can sufficiently safeguard APIs and thwart unwanted behavior."
    },
    {
        "link": "https://hackernoon.com/5-best-practices-for-integrating-with-external-apis",
        "document": "The modern world requires fast and cheap delivery of value to the end-user. That’s why we test tens of hypotheses per week in IT companies. For fast experiments, we usually prefer to use a ready-made solution instead of a self-developed one. Therefore, there is always a need to integrate with external services via API. And today I’d like to talk about best practices for these integrations.\n\nTimeouts are a crucial part of your fault tolerance. You should set it for all external calls. Otherwise, an external service can hang up and you will be frozen with it. For example, if you use Golang, then your code would be something like that:\n\nAny external service (even Google or Amazon) can be down. You should consider the fallback logic for 5xx responses or unexpected responses. For instance, you can return a default response object or do some fallback job.\n\nEvery extra API call is an overhead to you and the external systems. Pore over the API docs to find batch methods for your needs.\n\nFor example, 1 call to create one item takes 20ms. Therefore, the synchronous creation of 10 items would take 200ms (actually it will take more because on load external services usually start to throttle your requests). But you can use the batch API method and create 10 items per single request and it takes 50 ms.\n\nUsually, when your requests count is increasing the difference becomes much more prominent. It can save you a tremendous amount of execution time. In the corner case if there is no batch method, try to parallel your requests.\n\nMost services have API limits. Investigate them and calculate how your requests will be placed within the limits. There is a useful lib in Go that can help you to control the API calls count.\n\nEven if an external service returns successful responses, it can have issues with performance sometimes. For cases like these, you should use metrics and alerts on your side to see when it happens and react quickly.\n\nIn my team, we prefer to use widespread solutions like Prometheus and Grafana:\n\nWith data in Prometheus, we can set up alerts in Grafana when an external service is down or its response is taking too long."
    }
]