[
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html",
        "document": "Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv. If you want to pass in a path object, pandas accepts any . By file-like object, we refer to objects with a method, such as a file handle (e.g. via builtin function) or .\n\nRow number(s) containing column labels and marking the start of the data (zero-indexed). Default behavior is to infer the column names: if no are passed the behavior is identical to and column names are inferred from the first line of the file, if column names are passed explicitly to then the behavior is identical to . Explicitly pass to be able to replace existing names. The header can be a list of integers that specify row locations for a on the columns e.g. . Intervening rows that are not specified will be skipped (e.g. 2 in this example is skipped). Note that this parameter ignores commented lines and empty lines if , so denotes the first line of data rather than the first line of the file.\n\nColumn(s) to use as row label(s), denoted either by column labels or column indices. If a sequence of labels or indices is given, will be formed for the row labels. Note: can be used to force pandas to not use the first column as the index, e.g., when you have a malformed file with delimiters at the end of each line.\n\nSubset of columns to select, denoted either by column labels or column indices. If list-like, all elements must either be positional (i.e. integer indices into the document columns) or strings that correspond to column names provided either by the user in or inferred from the document header row(s). If are given, the document header row(s) are not taken into account. For example, a valid list-like parameter would be or . Element order is ignored, so is the same as . To instantiate a from with element order preserved use for columns in order or for order. If callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to . An example of a valid callable argument would be . Using this parameter results in much faster parsing time and lower memory usage.\n\nWhether or not to include the default values when parsing the data. Depending on whether is passed in, the behavior is as follows:\n• None If is , and are specified, is appended to the default values used for parsing.\n• None If is , and are not specified, only the default values are used for parsing.\n• None If is , and are specified, only the values specified are used for parsing.\n• None If is , and are not specified, no strings will be parsed as . Note that if is passed in as , the and parameters will be ignored.\n\nFunction to use for converting a sequence of string columns to an array of instances. The default uses to do the conversion. pandas will try to call in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by ) as arguments; 2) concatenate (row-wise) the string values from the columns defined by into a single array and pass that; and 3) call once for each row using one or more strings (corresponding to the columns defined by ) as arguments. Deprecated since version 2.0.0: Use instead, or read in as and then apply as-needed.\n\nFor on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’ (otherwise no compression). If using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in. Set to for no decompression. Can also be a dict with key set to one of { , , , , , } and other key-value pairs are forwarded to , , , , or , respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: ."
    },
    {
        "link": "https://pandas.pydata.org/docs/user_guide/missing_data.html",
        "document": "pandas uses different sentinel values to represent a missing (also referred to as NA) depending on the data type. for NumPy data types. The disadvantage of using NumPy data types is that the original data type will be coerced to or . for NumPy , , and . For typing applications, use . for , (and other bit widths), and . These types will maintain the original data type of the data. For typing applications, use . To detect these missing value, use the or methods. or will also consider a missing value. Equality compaisons between , , and do not act like Therefore, an equality comparison between a or with one of these missing values does not provide the same information as or .\n\nExperimental: the behaviour of can still change without warning. Starting from pandas 1.0, an experimental value (singleton) is available to represent scalar missing values. The goal of is provide a “missing” indicator that can be used consistently across data types (instead of , or depending on the data type). For example, when having missing values in a with the nullable integer dtype, it will use : Currently, pandas does not yet use those data types using by default a or , so you need to specify the dtype explicitly. An easy way to convert to those dtypes is explained in the conversion section. In general, missing values propagate in operations involving . When one of the operands is unknown, the outcome of the operation is also unknown. For example, propagates in arithmetic operations, similarly to : There are a few special cases when the result is known, even when one of the operands is . In equality and comparison operations, also propagates. This deviates from the behaviour of , where comparisons with always return . To check if a value is equal to , use An exception on this basic propagation rule are reductions (such as the mean or the minimum), where pandas defaults to skipping missing values. See the calculation section for more. For logical operations, follows the rules of the three-valued logic (or Kleene logic, similarly to R, SQL and Julia). This logic means to only propagate missing values when it is logically required. For example, for the logical “or” operation ( ), if one of the operands is , we already know the result will be , regardless of the other value (so regardless the missing value would be or ). In this case, does not propagate: On the other hand, if one of the operands is , the result depends on the value of the other operand. Therefore, in this case propagates: The behaviour of the logical “and” operation ( ) can be derived using similar logic (where now will not propagate if one of the operands is already ): Since the actual value of an NA is unknown, it is ambiguous to convert NA to a boolean value. Traceback (most recent call last) in : boolean value of NA is ambiguous This also means that cannot be used in a context where it is evaluated to a boolean, such as where can potentially be . In such cases, can be used to check for or being can be avoided, for example by filling missing values beforehand. A similar situation occurs when using or objects in statements, see Using if/truth statements with pandas. implements NumPy’s protocol. Most ufuncs work with , and generally return : Currently, ufuncs involving an ndarray and will return an object-dtype filled with NA values. The return type here may change to return a different array type in the future. See DataFrame interoperability with NumPy functions for more on ufuncs. If you have a or using , and in that can convert data to use the data types that use such as or . This is especially helpful after reading in data sets from IO methods where data types were inferred. In this example, while the dtypes of all columns are changed, we show the results for the first 10 columns.\n\nNA values can be replaced with corresponding value from a or where the index and column aligns between the original object and the filled object. can also be used to fill NA values.Same result as above. and fills NA values using various interpolation methods. Interpolation relative to a in the is available by setting If you have scipy installed, you can pass the name of a 1-d interpolation routine to . as specified in the scipy interpolation documentation and reference guide. The appropriate interpolation method will depend on the data type. If you are dealing with a time series that is growing at an increasing rate, use . If you have values approximating a cumulative distribution function, use . To fill missing values with goal of smooth plotting use . When interpolating via a polynomial or spline approximation, you must also specify the degree or order of the approximation: Interpolating new observations from expanding data with . accepts a keyword argument to limit the number of consecutive values filled since the last valid observation By default, values are filled in a direction. Use parameter to fill or from directions. By default, values are filled whether they are surrounded by existing valid values or outside existing valid values. The parameter restricts filling to either inside or outside values. # fill one consecutive inside value in both directions # fill all consecutive outside values in both directions and can be used similar to and to replace or insert missing values. Replacing more than one value is possible by passing a list. Python strings prefixed with the character such as are “raw” strings. They have different semantics regarding backslashes than strings without this prefix. Backslashes in raw strings will be interpreted as an escaped backslash, e.g., . Replace the ‘.’ with with regular expression that removes surrounding whitespace Pass nested dictionaries of regular expressions that use the keyword. Pass a list of regular expressions that will replace matches with a scalar. All of the regular expression examples can also be passed with the argument as the argument. In this case the argument must be passed explicitly by name or must be a nested dictionary. A regular expression object from is a valid input as well."
    },
    {
        "link": "https://geeksforgeeks.org/python-read-csv-using-pandas-read_csv",
        "document": "CSV files are the Comma Separated Files. It allows users to load tabular data into a DataFrame, which is a powerful structure for data manipulation and analysis. To access data from the CSV file, we require a function read_csv() from Pandas that retrieves data in the form of the data frame. Here’s a quick example to get you started.\n\nSuppose you have a file named people.csv First, we must import the Pandas library. then using Pandas load this data into a DataFrame as follows:\n\nfunction in Pandas is used to read data from CSV files into a Pandas DataFrame. A DataFrame is a powerful data structure that allows you to manipulate and analyze tabular data efficiently. CSV files are plain-text files where each row represents a record, and columns are separated by commas (or other delimiters).\n\nHere is the Pandas read CSV syntax with its parameters.\n\nThe parameter allows to load only specific columns from a CSV file. This reduces memory usage and processing time by importing only the required data.\n\nThe parameter sets one or more columns as the DataFrame index, making the specified column(s) act as row labels for easier data referencing.\n\nThe parameter replaces specified strings (e.g., , ) with , enabling consistent handling of missing or incomplete data during analysis.\\\n\nWe won’t got nan values as there is no missing value in our dataset.\n\nIn this example, we will take a CSV file and then add some special characters to see how the sep parameter works.\n\nThe sample data is stored in a multi-line string for demonstration purposes.\n• Engine argument is used because the default C engine does not support regular expressions for delimiters.\n\nThe parameter limits the number of rows read from a file, enabling quick previews or partial data loading for large datasets. Here, we just display only 5 rows using nrows parameter.\n\nThe parameter skips unnecessary rows at the start of a file, which is useful for ignoring metadata or extra headers that are not part of the dataset.\n\nThe parameter converts date columns into datetime objects, simplifying operations like filtering, sorting, or time-based analysis.\n\nPandas allows you to directly read a CSV file hosted on the internet using the file’s URL. This can be incredibly useful when working with datasets shared on websites, cloud storage, or public repositories like GitHub.\n\nHow to read pandas DataFrame to csv?\n\nHow to extract data from CSV file in Python using pandas?\n\nHow to read CSV string in pandas?"
    },
    {
        "link": "https://stackoverflow.com/questions/27228964/read-csv-data-with-missing-values-into-python-using-pandas",
        "document": "I have a CSV-file looking like this:\n\nAll quoted entries are strings. Non-quoted entries are numerical. Empty fields are missing values (NaN), Quoted empty fields still should be considered as empty strings. I tried to read it in with pandas read_csv but I cannot get it working the way I would like to have it... It still consideres ,\"\", and ,, as NaN, while it's not true for the first one.\n\nCan anybody help? Is it possible at all?"
    },
    {
        "link": "https://pyimagesearch.com/2024/05/16/read-csv-file-using-pandas-read_csv-pd-read_csv",
        "document": "In this tutorial, we delve into the powerful data manipulation capabilities of Python’s Pandas library, specifically focusing on the function. By the end of this tutorial, you will have a thorough understanding of the function, a versatile tool in the arsenal of any data scientist or analyst.\n\nThe function is one of the most commonly used pandas functions, particularly for data preprocessing. It is invaluable for tasks such as importing data from CSV files into the Python environment for further analysis. This function is capable of reading a CSV file from both your local machine and from a URL directly. What’s more, using pandas to read csv files comes with a plethora of options to customize your data loading process to fit your specific needs.\n\nWe will explore the different parameters and options available in the function, learn how to handle large datasets, and deal with different types of data. Whether you’re a beginner just starting out or a seasoned data science professional, understanding the pandas read csv function is crucial to efficient data analysis.\n\nUnleash the power of the function, and redefine the way you handle, manipulate, and analyze data.\n\nThings to Be Aware of When Using Pandas Concat\n\nWhen using the function ( ) to read a CSV file into a DataFrame, there are several important things to be aware of:\n• Delimiter and Encoding: Always specify the appropriate delimiter and encoding parameters when using . The default delimiter is a comma, but CSV files can also use other delimiters like tabs or semicolons. Additionally, ensure the encoding matches the file’s encoding to correctly read special characters.\n• Handling Missing Data: Be mindful of how missing data is represented in your CSV file. By default, considers empty strings, NA, and NULL values as missing data. You can customize how missing values are handled using parameters like and .\n• Parsing Dates and Times: When working with date and time data in CSV files, specify the parameter in to ensure the correct parsing of date and time columns. This will allow you to work with the data as datetime objects in the DataFrame.\n\nBy paying attention to these key considerations when using , you can effectively use Pandas to read csv files into DataFrames while ensuring data integrity and proper handling of various data types.\n\nTo follow this guide, you need to have the Pandas library installed on your system.\n\nIf you need help configuring your development environment for Pandas, we highly recommend that you read our pip install Pandas guide — it will have you up and running in minutes.\n\nNeed Help Configuring Your Development Environment?\n\nAll that said, are you:\n• Wanting to skip the hassle of fighting with the command line, package managers, and virtual environments?\n• Ready to run the code immediately on your Windows, macOS, or Linux system?\n\nGain access to Jupyter Notebooks for this tutorial and other PyImageSearch guides pre-configured to run on Google Colab’s ecosystem right in your web browser! No installation required.\n\nAnd best of all, these Jupyter Notebooks will run on Windows, macOS, and Linux!\n\nWe first need to review our project directory structure.\n\nStart by accessing this tutorial’s “Downloads” section to retrieve the source code and example images.\n\nFrom there, take a look at the directory structure:\n\nSimple Example of Using pandas read_csv\n\nThis example demonstrates how to use to load a simple dataset. We will use a CSV file that contains movie ratings. The goal is to load this data into a pandas DataFrame and print basic information about the data.\n\nLine 2: First, we import the pandas package using the alias. This package provides data structures and data analysis tools.\n\nLine 8: A CSV file named ‘movie_ratings.csv’ is loaded into a DataFrame called ‘movie_ratings’. This file likely contains movie rating data with columns like ‘Title’, ‘Year’, ‘Rating’, and ‘Reviewer’.\n\nLine 11: The function is used to display the first few rows of the ‘movie_ratings’ DataFrame. This provides a quick look at the data.\n\nLine 14: The function is used to display basic information about the ‘movie_ratings’ DataFrame. This includes data types of columns and the presence of any missing values. This helps in understanding the structure and completeness of the dataset.\n\nWhen you run this code, you’ll see an output similar to the following:\n\nThis output show how handles importing a csv file and how to display information about the csv data. Now let’s move on to some more advanced capabilities.\n\nThe parameter in allows you to skip specific rows while reading a CSV file. This is particularly useful for ignoring metadata, extra headers, or any irrelevant rows at the beginning of the file. By skipping these rows, you can focus on the actual data and clean up your dataset for further analysis.\n\nHere’s an example of using :\n\nWhen to Use\n• Ignoring metadata or additional headers at the start of the file.\n\nThe result of the above code will skip the first two lines, ensuring only the main data is loaded into the DataFrame.\n\nWe will explore more advanced features of , we’ll use a dataset that includes mixed data types, handling dates, and missing values. We’ll focus on a dataset about video game sales, which includes release dates, platforms, sales figures, and missing values in some entries. This will allow us to demonstrate how to handle these complexities using .\n\nLine 2: We import the pandas library, which is a powerful tool for data manipulation and analysis in Python.\n\nLines 6-22: We create a DataFrame named by reading data from a CSV file named ‘video_game_sales.csv’ using the function. The parameter is set to , which instructs pandas to parse the ‘Release_Date’ column as a datetime object. The parameter is specified as a dictionary to define the data types of various columns. This helps in optimizing memory usage and ensures that each column is read with the appropriate type:\n\nThe parameter is used to specify additional strings that should be recognized as NaN (missing values) in the dataset. In this case, ‘n/a’, ‘NA’, and ‘–‘ are treated as missing values.\n\nLine 25: We print the first few rows of the DataFrame using the method to get a quick look at the data and verify that it has been loaded correctly.\n\nLine 28: We print the information about the DataFrame using the method. This provides a summary of the DataFrame, including the column names, data types, and the number of non-null values in each column. This is useful to confirm that the data types are correctly set and to check for any missing values.\n\nWhen you run this code, you’ll see the following output:\n\nThis output illustrates how ‘pandas read_csv’ handles importing a csv file that contains multiple data types, datetime and treating specific strings like ‘n/a’, ‘NA’, and ‘–‘ as missing values.\n\nWe’ll cover how to read large datasets with chunking and handle non-standard CSV files that use different delimiters or encodings. These examples are highly relevant to readers dealing with diverse and potentially challenging datasets.\n\nThe parameter is crucial for optimizing memory usage and performance when dealing with large datasets by allowing users to load only the necessary columns. This can significantly improve efficiency, especially when working with datasets containing numerous columns. Below is a simple example to demonstrate how to use the parameter with .\n\nBy specifying only the required columns using , you can:\n• Speed up the data import process, making it more efficient for large datasets.\n\nExample 2: Using to Set a Column as the Index\n\nThe parameter in allows you to specify a column to be used as the index of the resulting DataFrame. This is particularly helpful when one column in your dataset contains unique identifiers or meaningful labels, such as IDs, dates, or names. By setting such a column as the index, you can simplify data referencing and improve the readability of your DataFrame.\n\nHere’s how to use the parameter:\n• Improves DataFrame organization for operations like grouping or merging.\n\nIf your dataset contains multiple columns you’d like to use as a hierarchical index, you can pass a list of column names to .\n\nThis feature is essential for organizing and working efficiently with your data, making a vital tool in the data scientist’s arsenal.\n\nThis technique is useful for managing memory when working with very large datasets. By specifying chunksize, pd.read_csv returns an iterable object, allowing you to process the data in manageable parts.\n\nThe output will be displayed in chucks as follows:\n\nUsing the chuck parameter for it allows you to more easily manage and process large amounts of csv data.\n\nHandling files with different delimiters and encodings is common. Adjusting the and parameters in lets you adapt to these variations seamlessly.\n\nHere is an example of non standard csv delimiter data being used in this example:\n\nLine 5: On this line we utilize the function from the pandas library to read a CSV file named ‘odd_delimiter_sales.csv’. The function is called with three arguments. The first argument is the file path as a string. The second argument specifies the delimiter used in the CSV file, which in this case is a semicolon (‘;’). The third argument sets the encoding to ‘UTF-16’ to handle any encoding issues.\n\nOn Line 8, we use the function to display the first few rows of the DataFrame . The method is called on to retrieve the first five rows, which is the default behavior of this method. This allows us to verify that the data was loaded correctly.\n\nWhen you run this code, you’ll see the following output:\n\nNow that you know the basics and some advanced techniques of how to use Pandas to read csv files. We are going to look into an alternative solution to pandas to read csv files.\n\nWe are going to explore alternatives to using , we’ll delve into using Dask. Dask is a powerful parallel computing library in Python that can handle large datasets efficiently, making it an excellent alternative for cases where Pandas might struggle with memory issues.\n\nWe will use the large dataset we created earlier for the chunking example ( ) to demonstrate how Dask can be used for similar tasks but more efficiently in terms of memory management and parallel processing.\n\nUsing Dask to Read and Process Large Datasets\n\nHere’s how you can use Dask to achieve similar functionality to but with the capability to handle larger datasets more efficiently:\n\nWhy Dask is a Better Approach for Large Datasets\n• Scalability: Dask can scale up to clusters of machines and handle computations on datasets that are much larger than the available memory, whereas pandas is limited by the size of the machine’s RAM.\n• Lazy Evaluation: Dask operations are lazy, meaning they build a task graph and execute it only when you explicitly compute the results. This allows Dask to optimize the operations and manage resources more efficiently.\n• Parallel Computing: Dask can automatically divide data and computation over multiple cores or different machines, providing significant speed-ups especially for large-scale data.\n\nThis makes Dask an excellent alternative to when working with very large data sets or in distributed computing environments where parallel processing can significantly speed up data manipulations.\n\nIn this tutorial, we delve into using function to effectively manage CSV data. We start with a straightforward example, loading and examining a movie ratings dataset to demonstrate basic Pandas functions. We then advance to a video game sales dataset, where we explore more complex features such as handling mixed data types, parsing dates, and managing missing values.\n\nWe also provide practical advice on reading large datasets through chunking and tackling non-standard CSV files with unusual delimiters and encodings. These techniques are essential for dealing with diverse datasets efficiently.\n\nLastly, we introduce Dask as a robust alternative for processing large datasets, highlighting its advantages in scalability, lazy evaluation, and parallel computing. This makes Dask an excellent option for large-scale data tasks where Pandas may fall short.\n\nThis guide aims to equip you with the skills to enhance your data handling capabilities and tackle complex data challenges using Pandas. By mastering these steps, you’ll be well-equipped to handle CSV data efficiently in your data analysis projects. For more details on the function, refer to the official documentation.\n\nTo download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), simply enter your email address in the form below!"
    },
    {
        "link": "https://machinelearningmastery.com/normalize-standardize-time-series-data-python",
        "document": "Some machine learning algorithms will achieve better performance if your time series data has a consistent scale or distribution.\n\nTwo techniques that you can use to consistently rescale your time series data are normalization and standardization.\n\nIn this tutorial, you will discover how you can apply normalization and standardization rescaling to your time series data in Python.\n\nAfter completing this tutorial, you will know:\n• The limitations of normalization and expectations of your data for using standardization.\n• What parameters are required and how to manually calculate normalized and standardized values.\n• How to normalize and standardize your time series data using scikit-learn in Python.\n\nKick-start your project with my new book Time Series Forecasting With Python, including step-by-step tutorials and the Python source code files for all examples.\n• Updated Aug/2019: Updated data loading to use new API.\n\nThis dataset describes the minimum daily temperatures over 10 years (1981-1990) in the city Melbourne, Australia.\n\nThe units are in degrees Celsius and there are 3,650 observations. The source of the data is credited as the Australian Bureau of Meteorology.\n\nBelow is a sample of the first 5 rows of data, including the header row.\n\nBelow is a plot of the entire dataset.\n\nThe dataset shows a strong seasonality component and has a nice, fine-grained detail to work with.\n\nThis tutorial assumes that the dataset is in your current working directory with the filename “daily-minimum-temperatures-in-me.csv“.\n\nNormalization is a rescaling of the data from the original range so that all values are within the range of 0 and 1.\n\nNormalization can be useful, and even required in some machine learning algorithms when your time series data has input values with differing scales.It may be required for algorithms, like k-Nearest neighbors, which uses distance calculations and Linear Regression and Artificial Neural Networks that weight input values.\n\nNormalization requires that you know or are able to accurately estimate the minimum and maximum observable values. You may be able to estimate these values from your available data. If your time series is trending up or down, estimating these expected values may be difficult and normalization may not be the best method to use on your problem.\n\nA value is normalized as follows:\n\nWhere the minimum and maximum values pertain to the value x being normalized.\n\nFor example, for the temperature data, we could guesstimate the min and max observable values as 30 and -10, which are greatly over and under-estimated. We can then normalize any value like 18.8 as follows:\n\nYou can see that if an x value is provided that is outside the bounds of the minimum and maximum values, that the resulting value will not be in the range of 0 and 1. You could check for these observations prior to making predictions and either remove them from the dataset or limit them to the pre-defined maximum or minimum values.\n\nYou can normalize your dataset using the scikit-learn object MinMaxScaler.\n\nGood practice usage with the MinMaxScaler and other rescaling techniques is as follows:\n• Fit the scaler using available training data. For normalization, this means the training data will be used to estimate the minimum and maximum observable values. This is done by calling the fit() function,\n• Apply the scale to training data. This means you can use the normalized data to train your model. This is done by calling the transform() function\n• Apply the scale to data going forward. This means you can prepare new data in the future on which you want to make predictions.\n\nIf needed, the transform can be inverted. This is useful for converting predictions back into their original scale for reporting or plotting. This can be done by calling the inverse_transform() function.\n\nBelow is an example of normalizing the Minimum Daily Temperatures dataset.\n\nThe scaler requires data to be provided as a matrix of rows and columns. The loaded time series data is loaded as a Pandas Series. It must then be reshaped into a matrix of one column with 3,650 rows.\n\nThe reshaped dataset is then used to fit the scaler, the dataset is normalized, then the normalization transform is inverted to show the original values again.\n\nRunning the example prints the first 5 rows from the loaded dataset, shows the same 5 values in their normalized form, then the values back in their original scale using the inverse transform.\n\nWe can also see that the minimum and maximum values of the dataset are 0 and 26.3 respectively.\n\nThere is another type of rescaling that is more robust to new values being outside the range of expected values; this is called Standardization. We will look at that next.\n\nStandardizing a dataset involves rescaling the distribution of values so that the mean of observed values is 0 and the standard deviation is 1.\n\nThis can be thought of as subtracting the mean value or centering the data.\n\nLike normalization, standardization can be useful, and even required in some machine learning algorithms when your time series data has input values with differing scales.\n\nStandardization assumes that your observations fit a Gaussian distribution (bell curve) with a well behaved mean and standard deviation. You can still standardize your time series data if this expectation is not met, but you may not get reliable results.\n\nThis includes algorithms like Support Vector Machines, Linear and Logistic Regression, and other algorithms that assume or have improved performance with Gaussian data.\n\nStandardization requires that you know or are able to accurately estimate the mean and standard deviation of observable values. You may be able to estimate these values from your training data.\n\nA value is standardized as follows:\n\nWhere the mean is calculated as:\n\nAnd the standard_deviation is calculated as:\n\nFor example, we can plot a histogram of the Minimum Daily Temperatures dataset as follows:\n\nRunning the code gives the following plot that shows a Gaussian distribution of the dataset, as assumed by standardization.\n\nWe can guesstimate a mean temperature of 10 and a standard deviation of about 5. Using these values, we can standardize the first value in the dataset of 20.7 as follows:\n\nThe mean and standard deviation estimates of a dataset can be more robust to new data than the minimum and maximum.\n\nYou can standardize your dataset using the scikit-learn object StandardScaler.\n\nBelow is an example of standardizing the Minimum Daily Temperatures dataset.\n\nRunning the example prints the first 5 rows of the dataset, prints the same values standardized, then prints the values back in their original scale.\n\nWe can see that the estimated mean and standard deviation were 11.1 and 4.0 respectively.\n\nIn this tutorial, you discovered how to normalize and standardize time series data in Python.\n• That some machine learning algorithms perform better or even require rescaled data when modeling.\n• How to manually calculate the parameters required for normalization and standardization.\n• How to normalize and standardize time series data using scikit-learn in Python.\n\nDo you have any questions about rescaling time series data or about this post?\n\n Ask your questions in the comments and I will do my best to answer."
    },
    {
        "link": "https://medium.com/@nomannayeem/comprehensive-guide-to-time-series-data-analytics-and-forecasting-with-python-2c82de2c8517",
        "document": "Welcome to this comprehensive guide on time series data analytics and forecasting using Python. Whether you are a seasoned data analyst or a business analyst looking to dive deeper into time series analysis, this guide is tailored for you. We will walk you through the essentials of time series data, from understanding its fundamental components to applying sophisticated forecasting techniques.\n• What time series data is and its unique characteristics.\n• How to preprocess and clean your data for accurate analysis.\n• Techniques for exploring and visualizing your data.\n• How to evaluate and select the best model for your data.\n\nBy the end of this guide, you’ll be equipped with the knowledge and tools to perform robust time series analysis and make accurate forecasts that can drive valuable insights for your business.\n\nIn this section, we’ll dive into what time series data is and why it’s essential in data analysis and forecasting.\n\nTime series data is a sequence of data points collected or recorded at specific time intervals. Examples include daily stock prices, monthly sales figures, yearly climate data, and many more. The primary characteristic of time series data is its temporal order, meaning the sequence in which the data points are recorded matters.\n\nTime series data has several unique characteristics that distinguish it from other types of data:\n• Trend: This is the long-term movement or direction in the data. For example, the general increase in a company’s sales over several years.\n• Seasonality: These are patterns that repeat at regular intervals, such as higher ice cream sales during the summer.\n• Cyclic Patterns: Unlike seasonality, cyclic patterns are not of a fixed period. These could be influenced by economic cycles or other factors.\n• Irregular Components: These are random or unpredictable variations in the data.\n\nLet’s create a simple time series dataset using Python to illustrate these concepts.\n\nThis code snippet creates a univariate time series data with 100 daily records starting from January 1, 2020.\n\nBefore diving into the analysis and forecasting, it’s essential to preprocess your time series data to ensure accuracy and reliability. This section will cover how to handle missing values, outliers, and transform your data for better analysis.\n\nTime series data often comes from various sources such as databases, APIs, or CSV files. The first step is to load your data into a suitable format, usually a Pandas DataFrame.\n\nMissing values and outliers can significantly affect your analysis. Here are common methods to handle them:\n• Filling Missing Values: Use methods like forward fill, backward fill, or interpolation.\n• Removing Outliers: Detect and remove outliers using statistical methods like Z-score or IQR (Interquartile Range).\n\nTransforming your data can help in identifying patterns and making it ready for analysis. Common transformations include:\n• Smoothing: Techniques like moving average can help in reducing noise.\n• Differencing: Used to remove trends and seasonality.\n• Scaling and Normalization: Ensure your data fits within a specific range for better model performance.\n\nIn this section, we’ll explore various techniques to understand and visualize your time series data. EDA helps uncover patterns, trends, and relationships that can inform your forecasting models.\n\nVisualizing your time series data is crucial to grasp its underlying patterns and characteristics. Here are some essential visualization techniques:\n• Line Plots: The most straightforward way to visualize time series data.\n• Autocorrelation Plots: To check the correlation of the series with its past values.\n\nLet’s use Python to create these visualizations.\n\nEDA provides insights into the structure and characteristics of your time series data. For example:\n• Line Plots: Help identify overall trends and any apparent seasonality.\n• Autocorrelation and Partial Autocorrelation Plots: Show the relationship between current and past values of the series, indicating potential lags to include in models.\n\nTime series decomposition is a crucial step in understanding the underlying components of your data. By breaking down a time series into its constituent parts, you can gain insights into the trend, seasonality, and residual (irregular) components. This process helps in better understanding and forecasting the data.\n\nThere are two main models for decomposing a time series: additive and multiplicative.\n\n1. Additive Model: This model assumes that the components add together to produce the time series:\n\nwhere Y(t)Y(t)Y(t) is the observed time series, T(t)T(t)T(t) is the trend component, S(t)S(t)S(t) is the seasonal component, and R(t)R(t)R(t) is the residual component.\n\n2. Multiplicative Model: This model assumes that the components multiply together to produce the time series:\n\nThe multiplicative model is useful when the seasonal variations are proportional to the level of the trend.\n\nLet’s decompose the time series using Python’s library. We'll use the realistic dummy dataset we created earlier.\n• Trend: The long-term progression of the series.\n• Seasonality: The repeating short-term cycle in the series.\n• Residuals: The remaining noise after removing the trend and seasonality.\n\nThese components can be analyzed separately to better understand the behavior of the time series and to improve forecasting models.\n\nForecasting is the process of making predictions about future values based on historical time series data. There are various methods to achieve this, ranging from simple statistical models to advanced machine learning techniques. In this section, we’ll explore some of the most commonly used forecasting methods.\n\nLet’s focus on ARIMA and SARIMA models for this section, as they are widely used and relatively straightforward to implement with Python.\n\nWe’ll use the library to demonstrate how to fit ARIMA and SARIMA models to our time series data.\n• ARIMA Model: Suitable for non-seasonal data, ARIMA can model the autocorrelations in the data.\n• SARIMA Model: Extends ARIMA to handle seasonality, making it ideal for data with seasonal patterns.\n\nSelecting the best forecasting model is crucial for making accurate predictions. In this section, we’ll explore various techniques and metrics for evaluating time series models and selecting the most suitable one.\n\n1. Mean Absolute Error (MAE): Measures the average magnitude of the errors in a set of predictions, without considering their direction.\n\n2. Mean Squared Error (MSE): Measures the average of the squares of the errors, giving more weight to larger errors.\n\n3. Root Mean Squared Error (RMSE): The square root of the mean squared error.\n• Rolling Forecast Origin: A method where the model is re-estimated for each prediction point, and the forecast is made for the next time step.\n• Time Series Split: Splitting the time series data into training and testing sets based on time.\n• Residual Analysis: Check if the residuals (errors) from the model are white noise (i.e., they should be uncorrelated and normally distributed with a mean of zero).\n• Autocorrelation of Residuals: Use autocorrelation plots to check for patterns in the residuals.\n\nLet’s evaluate the ARIMA and SARIMA models we fitted in the previous section.\n\nThe visual above shows the residuals from the ARIMA and SARIMA models. The evaluation metrics are as follows:\n• MAPE: NaN (due to division by zero or very small actual values)\n• MAPE: NaN (due to division by zero or very small actual values)\n\nThe residual plots indicate how well the models fit the data, with the goal being that the residuals should ideally resemble white noise.\n• Residual Analysis: Helps in diagnosing issues with the model and ensuring that the residuals are uncorrelated and normally distributed.\n\nIn this section, we’ll walk through a practical implementation of time series forecasting using a real-world example. We’ll demonstrate the entire workflow, from data loading and preprocessing to model building, evaluation, and forecasting.\n\nWe’ll use the following Python libraries:\n\nLet’s assume we have a dataset of monthly sales data for a retail store over the last 10 years. We’ll use this data to forecast future sales.\n\nThis code will walk you through the entire process of time series forecasting using a realistic dummy dataset, including data loading, preprocessing, EDA, decomposition, model building, evaluation, and plotting the forecast.\n• Practical Application: Shows how to apply the techniques learned in previous sections to a real-world scenario.\n\nIn this final section, we’ll discuss some best practices to follow when performing time series analysis and forecasting, as well as common pitfalls to avoid.\n• Understand Your Data: Before diving into analysis, take the time to understand the nature of your time series data. Recognize patterns such as trends, seasonality, and cycles.\n• Proper Preprocessing: Always clean your data to handle missing values and outliers. Ensure your data is stationary if required by the model you choose.\n• Model Selection: Choose the right model based on the characteristics of your data. Use simpler models like ARIMA for non-seasonal data and SARIMA for seasonal data.\n• Cross-Validation: Use cross-validation techniques to evaluate the performance of your models. Rolling forecast origin and time series split are common methods.\n• Residual Analysis: After fitting your model, analyze the residuals to ensure they are white noise. This indicates that the model has captured all the underlying patterns in the data.\n• Regular Updates: Time series data can change over time, so it’s crucial to regularly update your models with new data to maintain their accuracy.\n• Automate Where Possible: Use automated tools and scripts to streamline your workflow, especially for data preprocessing and model evaluation.\n• Overfitting: Creating a model that is too complex can lead to overfitting, where the model performs well on the training data but poorly on new data. Use simpler models and regularization techniques to avoid this.\n• Ignoring Seasonality: Failing to account for seasonal patterns can lead to inaccurate forecasts. Always check for seasonality in your data and use appropriate models like SARIMA if necessary.\n• Neglecting Model Diagnostics: Skipping residual analysis and other diagnostics can lead to misleading results. Always perform thorough diagnostics to validate your model.\n• Using Inappropriate Metrics: Different metrics provide different insights. Using only one metric can give an incomplete picture of model performance. Use multiple evaluation metrics like MAE, MSE, RMSE, and MAPE.\n• Data Leakage: Ensure that future data does not influence the model training. This can lead to over-optimistic performance estimates. Properly split your data into training and test sets.\n• Static Models for Dynamic Data: Static models can become obsolete as new data becomes available. Regularly update your models to adapt to new trends and patterns."
    },
    {
        "link": "https://analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization",
        "document": "Feature Scaling is a critical step in building accurate and effective machine learning models. One key aspect of feature engineering is scaling, normalization, and standardization, which involves transforming the data to make it more suitable for modeling. These techniques can help to improve model performance, reduce the impact of outliers, and ensure that the data is on the same scale. In this article, we will explore the concepts of scaling, normalization, and standardization, including why they are important and how to apply them to different types of data. By the end of this article, you’ll have a thorough understanding of these essential feature engineering techniques and be able to apply them to your own machine learning projects. Also in the article you will get to know about the data standardization vs normalization and with these difference you will get clear understanding of feature scaling.\n\nIn this article, you will learn about feature scaling, its importance in machine learning, and how scaling in machine learning can enhance model performance.\n\nFeature scaling is a preprocessing technique that transforms feature values to a similar scale, ensuring all features contribute equally to the model. It’s essential for datasets with features of varying ranges, units, or magnitudes. Common techniques include standardization, normalization, and min-max scaling. This process improves model performance, convergence, and prevents bias from features with larger values.\n\nWhy Should we Use Feature Scaling?\n\nSome machine learning algorithms are sensitive to feature scaling, while others are virtually invariant. Let’s explore these in more depth:\n\nMachine learning algorithms like linear regression, logistic regression, neural network, PCA (principal component analysis), etc., that use gradient descent as an optimization technique require data to be scaled. Take a look at the formula for gradient descent below:\n\nThe presence of feature value X in the formula will affect the step size of the gradient descent. The difference in the ranges of features will cause different step sizes for each feature. To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model.\n\nDistance algorithms like KNN, K-means clustering, and SVM(support vector machines) are most affected by the range of features. This is because, behind the scenes, they are using distances between data points to determine their similarity.\n\nFor example, let’s say we have data containing high school CGPA scores of students (ranging from 0 to 5) and their future incomes (in thousands Rupees):\n\nSince both the features have different scales, there is a chance that higher weightage is given to features with higher magnitudes. This will impact the performance of the machine learning algorithm; obviously, we do not want our algorithm to be biased towards one feature.\n\nThe effect of scaling is conspicuous when we compare the Euclidean distance between data points for students A and B, and between B and C, before and after scaling, as shown below:\n\nTree-based algorithms, on the other hand, are fairly insensitive to the scale of the features. Think about it, a decision tree only splits a node based on a single feature. The decision tree splits a node on a feature that increases the homogeneity of the node. Other features do not influence this split on a feature.\n\nSo, the remaining features have virtually no effect on the split. This is what makes them invariant to the scale of the features!\n\nNormalization, a vital aspect of Feature Scaling, is a data preprocessing technique employed to standardize the values of features in a dataset, bringing them to a common scale. This process enhances data analysis and modeling accuracy by mitigating the influence of varying scales on machine learning models.\n\nNormalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n\nHere, Xmax and Xmin are the maximum and the minimum values of the feature, respectively.\n• When the value of X is the minimum value in the column, the numerator will be 0, and hence X’ is 0\n• On the other hand, when the value of X is the maximum value in the column, the numerator is equal to the denominator, and thus the value of X’ is 1\n• If the value of X is between the minimum and the maximum value, then the value of X’ is between 0 and 1\n\nStandardization is another Feature scaling method where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero, and the resultant distribution has a unit standard deviation.\n\n\n\n is the mean of the feature values and is the standard deviation of the feature values. Note that, in this case, the values are not restricted to a particular range.\n\nNow, the big question in your mind must be when should we use normalization and when should we use standardization? Let’s find out!\n\nHowever, at the end of the day, the choice of using normalization or standardization will depend on your problem and the machine learning algorithm you are using. There is no hard and fast rule to tell you when to normalize or standardize your data. You can always start by fitting your model to raw, normalized, and standardized data and comparing the performance for the best results.\n\nIt is a good practice to fit the scaler on the training data and then use it to transform the testing data. This would avoid any data leakage during the model testing process. Also, the scaling of target values is generally not required.\n\nNow comes the fun part – putting what we have learned into practice. I will be applying feature scaling to a few machine-learning algorithms on the Big Mart dataset. I’ve taken on the DataHack platform.\n\nI will skip the preprocessing steps since they are out of the scope of this tutorial. But you can find them neatly explained in this article. Those steps will enable you to reach the top 20 percentile on the hackathon leaderboard, so that’s worth checking out!\n\nSo, let’s first split our data into training and testing sets:\n\nBefore moving to the feature scaling part, let’s glance at the details of our data using the pd.describe() method:\n\nWe can see that there is a huge difference in the range of values present in our numerical features: Item_Visibility, Item_Weight, Item_MRP, and Outlet_Establishment_Year. Let’s try and fix that using feature scaling!\n\nNote: You will notice negative values in the Item_Visibility feature because I have taken log-transformation to deal with the skewness in the feature.\n\nTo normalize your data, you need to import the MinMaxScaler from the sklearn library and apply it to our dataset. So, let’s do that!\n\nLet’s see how normalization has affected our dataset:\n\nAll the features now have a minimum value of 0 and a maximum value of 1. Perfect!\n\nTry out the above code in the live coding window below!!\n\nNext, let’s try to standardize our data.\n\nTo standardize your data, you need to import the StandardScaler from the sklearn library and apply it to our dataset. Here’s how you can do it:\n\nYou would have noticed that I only applied standardization to my numerical columns, not the other One-Hot Encoded features. Standardizing the One-Hot encoded features would mean assigning a distribution to categorical features. You don’t want to do that!\n\nBut why did I not do the same while normalizing the data? Because One-Hot encoded features are already in the range between 0 to 1. So, normalization would not affect their value.\n\nRight, let’s have a look at how standardization has transformed our data:\n\nThe numerical features are now centered on the mean with a unit standard deviation. Awesome!\n\nIt is always great to visualize your data to understand the distribution present. We can see the comparison between our unscaled and scaled data using boxplots.\n\nYou can notice how scaling the features brings everything into perspective. The features are now more comparable and will have a similar effect on the learning models.\n\nIt’s now time to train some machine learning algorithms on our data to compare the effects of different Feature scaling techniques on the algorithm’s performance. I want to see the effect of scaling on three algorithms in particular: K-Nearest Neighbors, Support Vector Regressor, and Decision Tree.\n\nNow, let’s delve into training machine learning algorithms on our dataset to assess the impact of various scaling techniques on their performance. Specifically, I aim to observe the effects of scaling on three key algorithms: K-Nearest Neighbors, Support Vector Regressor, and Decision Tree. This analysis will provide valuable insights into the significance of feature scaling in machine learning and how it influences the outcomes of these algorithms.\n\nAs we saw before, KNN is a distance-based algorithm that is affected by the range of features. Let’s see how it performs on our data before and after scaling:\n\nYou can see that scaling the features has brought down the RMSE score of our KNN model. Specifically, the normalized data performs a tad bit better than the standardized data.\n\nNote: I am measuring the RMSE here because this competition evaluates the RMSE.\n\nSVR is another distance-based algorithm. So let’s check out whether it works better with normalization or standardization:\n\nWe can see that scaling the features does bring down the RMSE score. And the standardized data has performed better than the normalized data. Why do you think that’s the case?\n\nThe sklearn documentation states that SVM, with RBF kernel, assumes that all the features are centered around zero and variance is of the same order. This is because a feature with a variance greater than that of others prevents the estimator from learning from all the features. Great!\n\nWe already know that a Decision tree is invariant to feature scaling. But I wanted to show a practical example of how it performs on the data:\n\nYou can see that the RMSE score has not moved an inch on scaling the features. So rest assured when you are using tree-based algorithms on your data!\n\nThis tutorial covered the relevance of using feature scaling on your data and how normalization and standardization have varying effects on the working of machine learning algorithms. Remember that there is no correct answer to when to use normalization over standardization and vice-versa. It all depends on your data and the algorithm you are using.\n\nHope you get a clear understanding of how to normalize data and why feature scaling is important in machine learning. You’ll learn simple ways to do feature normalization to improve your models.\n\nTo enhance your skills in feature engineering and other key data science techniques, consider enrolling in our Data Science Black Belt program. Our comprehensive curriculum covers all aspects of data science, including advanced topics such as feature engineering, machine learning, and deep learning. With hands-on projects and mentorship, you’ll gain practical experience and the skills you need to succeed in this exciting field. Enroll today and take your data science skills to the next level!"
    },
    {
        "link": "https://stackoverflow.com/questions/19256930/python-how-to-normalize-time-series-data",
        "document": "Following my previous comment, here it is a (not optimized) python function that does scaling and/or normalization: ( it needs a pandas DataFrame as input, and it’s doesn’t check that, so it raises errors if supplied with another object type. If you need to use a list or numpy.array you need to modify it. But you could convert those objects to pandas.DataFrame() first.\n\nThis function is slow, so it’s advisable run it just once and store the results."
    },
    {
        "link": "https://geeksforgeeks.org/ml-feature-scaling-part-2",
        "document": "Feature Scaling is a technique to standardize the independent features present in the data. It is performed during the data pre-processing to handle highly varying values. If feature scaling is not done then machine learning algorithm tends to use greater values as higher and consider smaller values as lower regardless of the unit of the values. For example it will take 10 m and 10 cm both as same regardless of their unit. In this article we will learn about different techniques which are used to perform feature scaling.\n• None We should first select the maximum absolute value out of all the entries of a particular measure.\n• None Then after this we divide each entry of the column by this maximum value.\n\nAfter performing the above-mentioned two steps we will observe that each entry of the column lies in the range of -1 to 1. But this method is not used that often the reason behind this is that it is too sensitive to the outliers. And while dealing with the real-world data presence of outliers is a very common thing.\n\nFor the demonstration purpose we will use the dataset which you can download from here. This dataset is a simpler version of the original house price prediction dataset having only two columns from the original dataset. The first five rows of the original data are shown below:\n\nNow let’s apply the first method which is of the absolute maximum scaling. For this first, we are supposed to evaluate the absolute maximum values of the columns.\n\nNow we are supposed to subtract these values from the data and then divide the results from the maximum values as well.\n\nThis method of scaling requires below two-step:\n• None First we are supposed to find the minimum and the maximum value of the column.\n• None Then we will subtract the minimum value from the entry and divide the result by the difference between the maximum and the minimum value.\n\nAs we are using the maximum and the minimum value this method is also prone to outliers but the range in which the data will range after performing the above two steps is between 0 to 1.\n\nThis method is more or less the same as the previous method but here instead of the minimum value we subtract each entry by the mean value of the whole data and then divide the results by the difference between the minimum and the maximum value.\n\nThis method of scaling is basically based on the central tendencies and variance of the data.\n• None First we should calculate the of the data we would like to normalize it.\n• None Then we are supposed to subtract the mean value from each entry and then divide the result by the standard deviation.\n\nThis helps us achieve a normal distribution of the data with a mean equal to zero and a standard deviation equal to 1.\n\nIn this method of scaling, we use two main statistical measures of the data.\n\nAfter calculating these two values we are supposed to subtract the median from each entry and then divide the result by the interquartile range.\n\nIn conclusion scaling, normalization and standardization are essential feature engineering techniques that ensure data is well-prepared for machine learning models. They help improve model performance, enhance convergence and reduce biases. Choosing the right method depends on your data and algorithm.\n\nIn machine learning feature scaling is used for number of purposes:\n• Range: Scaling guarantees that all features are on a comparable scale and have comparable ranges. This process is known as feature normalisation. This is significant because the magnitude of the features has an impact on many machine learning techniques. Larger scale features may dominate the learning process and have an excessive impact on the outcomes.\n• Algorithm performance improvement: When the features are scaled several machine learning methods including gradient descent-based algorithms, distance-based algorithms (such k-nearest neighbours) and support vector machines perform better or converge more quickly. The algorithm’s performance can be enhanced by scaling the features which prevent the convergence of the algorithm to the ideal outcome.\n• Preventing numerical instability: Numerical instability can be prevented by avoiding significant scale disparities between features. For examples include distance calculations where having features with differing scales can result in numerical overflow or underflow problems. Stable computations are required to mitigate this issue by scaling the features.\n• Equal importance: Scaling features makes sure that each characteristic is given the same consideration during the learning process. Without scaling bigger scale features could dominate the learning producing skewed outcomes. This bias is removed through scaling and each feature contributes fairly to model predictions."
    }
]