[
    {
        "link": "https://geeksforgeeks.org/thread-synchronization-in-cpp",
        "document": "In C++ multithreading, synchronization between multiple threads is necessary for the smooth, predictable, and reliable execution of the program. It allows the multiple threads to work together in conjunction by having a proper way of communication between them. If we do not synchronize the threads working on a shared resource properly, it may cause some of the concurrency problems.\n\nConsider that there is a program that is being used for managing the banking data. Now, assume that two threads are being used for credit and debit in a bank account. Now, assume the following sequence of transactions of a user:\n\nNow, assume that there is no synchronization between the processes running on the different threads. So, it is possible that the sequence of the operation can be:\n\nDue to this change in the sequence, the user wont be able to withdraw his/her money from the bank account even though he had credited enough money in the account. Here, both credit_process and debit_process threads can be synchronized so that when there is a simultaneous credit and debit requests, it should first execute the credit request.\n\nThere can be many such concurrency issues that arises due to non-synchronized operations. Some of them are:\n\nIn C++, thread synchronization is possible using the following methods:\n\nMutex is a synchronization primitive that locks the access to the shared resource if some thread is already accessing it.\n\nLet us take an example of a code snippet given below -\n\nIn the above code snippet, we created two threads and we have an add function which in turn adds the values. Both the threads, t1 and t2 go to the add function simultaneously. Now, it becomes difficult to decide which thread will execute first and modify the value of num as both of them move to the add function simultaneously. Such a condition is called race condition. Thus, we should apply thread synchronization to avoid this race condition.\n\nHere, we will apply mutex to achieve synchronization.\n\nNow, both the threads will not be able to access the critical section at the same time as we have applied lock . Thus, here using mutex we were able to achieve thread synchronization.\n\nThe condition variable is another such synchronization primitive but it is mainly used to notify the threads about the state of the shared data. It is used with the mutex locks to create processes that automatically wait and notify the state of the resource to each other.\n\nHere, in this program we first create two threads and then we try to perform addition first followed by subtraction. But as we can see in the above program, we passed the thread t2 first and then t1. Assuming thread t2 goes to the sub() function first, it first locks the mutex and then checks the condition whether the val is 0 or not. As the val is 0 initially, the predicate returns false and as soon as it returns false, it releases the mutex and waits for the condition to be true i.e., val!=0. Now as the mutex is released, addition is performed in the add() function and after that notify_one() gets executed which notifies the waiting thread which in turn tries to get the lock and again checks the condition. This way the process continues.\n\nOne of the best use case of condition variable is Producer-Consumer Problem.\n\nThe std::future and std::promise are used to return the data from a task executed on the other thread. The std::promise is used to sent the data and the std::future is used to receive the data on the main process. The std::future get() method can be used to retrieve the data returned by the process and is able to hold the current process till the value is returned.\n\nThis method is generally preferred of over the condition variable when we only want the task to be executed once.\n\nHere, in the above program, we try to find the number of even numbers in the given range. We first create a promise object and then we create a future object from that promise object. We send the promise object to the thread and then once we are ready with the value (after the function has been executed) ,we set the promise object. Then we create a future object from that promise. Finally we get the output from the future object to get our answer.\n\nThread Synchronization is necessary in the cases when we have multiple threads working on the shared data(critical section) so that the behaviour is defined. We have seen few common method of thread synchronization in C++. Although, it can be done manually using naive and bruteforce methods like (continuously updated some flag when done when done working on the shared data), the above discussed method provides more refined and tested ways of thread synchronization in C++."
    },
    {
        "link": "https://stackoverflow.com/questions/25493572/how-to-multithread-file-processing-in-c",
        "document": "Looking into your code I assume you produce one output file from one input. In such case you do not need to write multithreaded code to check if processing multiple files at once will speed up the process. Just modify your program to accept file name as a parameter and run multiple of them in parallel. But unless you are reading/writing from/to SSD drive such parallel processing most probably would slow process down, as hard-drive will have to switch between reading/writing for multiple positions, and head positioning is slow.\n\nIt is not clear what you are doing on processing, but if it takes 100% CPU then you most probably will speed up process significantly by processing one file by multiple threads. You would have one thread reading, then thread pool processing, then one thread writing. Tricky part would be to synchronize data and make it not appear in output file in wrong order."
    },
    {
        "link": "https://medium.com/codex/c-multithreading-the-simple-way-95aa1f7304a2",
        "document": "Multithreading is one of the most powerful and vital capabilities of nearly any computer processor that exists today. Multithreading allows software to execute different code simultaneously in the same program. Web servers, web browsers, databases, mobile applications, and just about any production grade software wouldn’t function as well as it does without multithreading.\n\nMultithreading often carries a reputation for being difficult. Compared with other concepts in software development, one could certainly make a case for that. However, multithreading isn’t really that different from general programming. It’s just potentially more dangerous . Learning to protect against the danger, though, can allow one to implement far more powerful algorithms and programs than you could in a single threaded manner.\n\nTo understand multithreading, it’s best to start from the least dangerous concepts and proceed toward the most potentially dangerous ones. This allows one to get comfortable with threading and work their way toward more critical and cautious code writing.\n\nPerhaps the least threatening form of multithreading is concurrency. Concurrency is generally meant to mean multiple threads running at the same time, but not sharing any resources. This means no data structures, memory, or another is shared between threads. Concurrency is commonly utilized for tasks that can be split up between threads and worked on independently.\n\nTo illustrate this, let’s look at the example of each thread getting a pointer to an integer, and the thread increments that integer, then stops. Each thread gets to run until it increments the number a few hundred times. Then, those threads, often called “worker” threads, get joined by the main thread. All of the threads work simultaneously.\n\nIf you are new to multithreading, there’s a few parts of this code that might not make sense. The method is probably one of them. An important detail to understand about starting new threads is that they work and function entirely separately from the main thread, the thread which begins in . Because they are entirely separate, we have to decide a point in which we want to wait for them to complete their assigned work.\n\nThink of similarly to how two people might split up to do their own separate tasks, then “join” back together later on. If you are traveling or going somewhere with a friend, you don’t want top just abandon them! You should ideally wait for them to catch up again. The same logic goes for threads. Anytime additional threads are created, there’s an obligation to direct how you want the central, main thread to act in accordance with them.\n\nDo you always have to join threads ? No. Actually, there is one other option. Just like with the friends example, it’s possible a friend might want to go their own way back home, and not meet back up with you. In the case of threads, that’s called detaching. Detaching a thread means allowing it to work and complete it’s work independently of the main thread. But, this can be dangerous. Take the following example, very similar to the one for , for instance.\n\nThe first risk here is using the heap-allocated after it’s deleted. Unlike , does not make the calling thread stop or wait for anything. This means as soon as the third call to ends, the calling thread will delete the array. If the created threads haven’t finished their work, they will be writing to a deleted array, which corrupts memory.\n\nThe second risk here is that the created threads can keep running even after the main thread finishes, if their work is not completed. Or they might be killed as soon as main ends. This is undefined behavior according to the C++ standard. Regardless what a specific compiler might guarantee, undefined behavior is something to avoid. There are valid use cases for , but any of them require some other form of synchronization between threads to be reliable.\n\nA resource where two different threads can access the same memory address is called a shared resource. It’s critical to note the emphasis on address. In the prior example shown here with multiple threads accessing the same array, that is not a shared resource because no two threads are reading or writing from the same memory address. The array could have just been four separate integer pointers, there’s nothing about an array in and of itself that makes it a shared resource.\n\nUnlike concurrency, a shared resource is used when it’s desirable for threads to perform work on the same data or object. This means objects which are not allocated on a thread’s own stack, and only one’s visible to other threads. What can make this tricky to understand is, although both threads can access some resource, they can never see the other threads accessing that resource.\n\nA great example of a shared resource in real life is an airport runway at night. A runway has blinking lights to help guide planes align with it as they prepare for landing. But it’s very difficult if not impossible for other planes to see each other at night, due to the darkness and the speed at which they travel. If a plane were to attempt to land on the runway at the same time as another plane, it would be disastrous. The only way planes avoid such collisions is to coordinate through air traffic control.\n\nThreads function the same way in the sense they depend on synchronization mechanisms to coordinate access to resources, such as not writing to them at the exact same time. The mechanism we will discuss here, which is perhaps the most common one, is a mutex. A mutex, known by the type , allows threads to acquire locks. Locks are a form of control that allows only one thread to proceed through a section of code at a time. Let’s look at this example.\n\nIn the above example, the and methods of the class both happen while the calling thread constructs a lock on the mutex associated with the queue. This lock is best used as an RAII style object, where it’s only active under some scope of code. Once the program finishes that scope, the lock guard object is destroyed, allowing another thread to construct and acquire a lock on the mutex. This pattern continues to satisfy the condition only one thread can modify the queue at a time.\n\nEven though they do sound really neat and straight forward, mutexes can still be dangerous. When a thread acquires a lock on a mutex, it’s responsible for freeing or destroying that lock so other threads can access the secured scope of code as well. What happens if a thread never frees the lock it acquired ? Well, something really bad.\n\nA leaked lock is when a thread locks a mutex but then that lock for some reason can never be unlocked. If this happens, all the threads will block and wait on the mutex indefinitely, making no progress or doing any work whatsoever.\n\nThe rule of thumb for mutexes is to think carefully and critically about what a thread does when it places a lock on a mutex. It’s vital that a thread only locks when it absolutely requires single thread access, and while doing so, does work as fast as possible. While mutexes provide a means to safely access the same resource, they do so at a performance cost.\n\nAre there other ways to prevent illegal, dual access to resources among multiple threads ? Yes. But that’s a topic for another post."
    },
    {
        "link": "https://stackoverflow.com/questions/72319088/c-prioritize-data-synchronization-between-threads",
        "document": "Normally in such a scenario you use a reader-writer-lock. This allows either a read by all readers in parallel or a write by a single writer.\n\nBut that does nothing to stop a writer from holding the lock for minutes if it so desires. Forcing the writer out of the lock is probably also not a good idea. The object is probably in some inconsistent state mid changed.\n\nThere is another synchronization method called read-copy-update that might help. This allows writers to modify element without being blocked by readers. The drawback is that you might get some readers still reading the old data and others reading the new data for some time.\n\nIt also might be problematic with multiple writers if they try to change the same member. The slower writer might have computed all the needed updates only to notice some other thread changes the object. It then has to start over wasting all the time it already spend.\n\nNote: copying the element can be done in constant time, certainly under 1ms. So you can guarantee readers are never blocked for long. By releasing the write lock first you guarantee readers to read between any 2 writes, assuming the RW lock is designed with the same principle.\n\nSo I would suggest another solution I call write-intent-locking:\n\nYou start with a RW lock but add a lock to handle write-intent. Any writer can acquire the write-intent lock at any time, but only one of them, it's exclusive. Once a write holds the write-intent lock it copies the element and starts modifying the copy. It can take as long as it wants to do that as it's not blocking any readers. It does block other writers though.\n\nWhen all the modifications are done the writer acquires the write lock and then quickly copies, moves or replaces the element with the prepared copy. It then releases the write and write-intent lock, unblocking both the readers and writers that want to access the same element."
    },
    {
        "link": "https://chrizog.com/cpp-thread-synchronization",
        "document": "Three Ways For C++ Thread Synchronization in C++11 and C++14\n\nIn this article I will give you an overview about different ways of synchronizing C++ threads. Wile C++ is a powerful language and you can get almost anything done with it, it's daunting for beginners to capture all the possible options provided by the language. For that reason I provide an overview and comparison of different C++ synchronization mechanisms in one place.\n\nWith threading in C++ there are mainly two tasks for the developer:\n\nProtecting shared data is achieved by mutual exclusion. By avoiding that multiple threads access the same data at the same time race conditions are prevent. Two read operations are permitted. But two write operations or one write and one read operation lead to race conditions. For using locks and mutexes I can recommend this article.\n\nThis article is about the second issue: Synchronizing concurrent operations in C++ which means coordinating the order of events between multiple threads. It happens that one thread must wait on another thread to complete a task before it can continue its own task. For example one thread indexes data in the background and the second thread needs to use the indexed data. The first thread is maybe not ready yet with indexing and the second thread shall wait on the result of the first thread.\n\nMany of the synchronization problems can be broken down to the producer-consumer pattern: A consumer thread is waiting for a condition to be true. The producer thread is setting the condition. How can you achieve the waiting on a condition in C++? And how can you pass the results of tasks between different threads? I will explain three different methods:\n• Periodically checking the condition (the naive and worst approach)\n\nImagine you are waiting for the parcel delivery at your home for your long awaited packet (a brand new PlayStation 5). In order to not miss the delivery, there are several options. One option, which is also the worst idea, is to sit at your front window all day and starr at the entrance. As soon as you see the delivery car, you run to the door and open it. The drawbacks of this approach are obvious: Your eyes will get tired since you are starring out of the window all day. You also cannot do anything else in the meantime but only wait for the packet. A lot of wasted time.\n\nYou can put this example in relation to C++ threads. The waiting thread (= consumer) is checking the condition to be true continuously in a while loop. The producing thread is setting the condition as soon as it is ready. The condition can be a simple boolean flag protected by a mutex. The condition could be also checking if a queue is empty. The queue is protected by a mutex. See the following code for an example:\n\nAt (1) in main we are starting two threads: the producing and the consuming thread.\n\nAt (2) a new Packet is put into the queue periodically. For the write operation to the queue, the mutex m is locked using a std::lock_guard. The sleep of 1 second simulates a long running task (e.g. the parcel delivery must drive to your house).\n\nIn (3) the mutex is periodically locked and it is tested if there is a packet in the queue. If there is a packet in the queue it is taken out of the queue and processed.\n\nWhen running the above code, the CPU usage will likely jump to 100% for that process due to the while-loop in the consuming-thread. It is reading the value of packet_queue.empty() all time. You might think: \"Hmm, I could simply put a sleep in the recipient's while loop - problem solved!\"\n\nWith the sleep at (4) you will reduce the CPU usage as the consuming thread will go into sleep state. But you will introduce an undesired delay and unnecessary wakeups. In the worst case, you need to wait 100 milliseconds to react to the newly arrived packet.\n\nYou can decrease the sleep time, but the question will be always: How fast is fast enough and what delay is acceptable?\n\nThis naive approach is usually considered as bad design.\n• Condition variables can be used multiple times (in comparison to futures which are for one-shot events)\n• Take care of spurious and lost wakeups\n• Having a network thread that receives data regularly and notifies the main worker thread\n\nFollowing the previous story of waiting for the packet, wouldn't it be a lot better if you'd just get notified by the door bell by the postman? That would make a lot more sense. While waiting for the door bell to ring, you could even clean the house in the meantime.\n\nWith C++ threads this can be achieved using a condition variable. A condition variable has a wait-function, that you can use to block the consuming thread. Using the wait-function your thread is consuming no CPU resources.\n\nIn the producing thread you call a notify function on the condition variable. This will cause the consuming thread to wake up and continue after the wait call.\n\nAt (1) in main we are starting two threads again: a producing and a consuming thread.\n\nAt (2) in the consuming thread, you must use a std::unique_lock and acquire the lock on the mutex m. Condition variables only work with std::unique_lock. Afterwards you call the wait function and pass the lock and a predicate.\n\nA predicate can be a function that returns a boolean. The predicate shall return true when the thread should not wait anymore, i.e. when the event happens. For that purpose we use a lambda expression that returns true if there is a Packet in the queue: .\n\nThe wait function releases the mutex and suspends the thread. There is no active waiting (meaning actively reading the variable as it is done in the naive approach) and therefore no CPU resources are wasted. When a wake-up happens, the lock is automatically reacquired. You then take the element out of the queue and release the lock. Since we take out the element by a copy, the which represents the processing of the packet does not need to be inside the critical section protected by the mutex.\n\nThere are existing variants and that allow you to have a timeout on the wait operation as well. Read more at cppreference.\n\nAt (3) the mutex is locked on the producer thread. Here a lock_guard is enough, since you only need to protect the critical section for pushing a new packet to the . Afterwards call in order to wakeup the waiting thread. There is also the possibility that multiple threads wait on the condition variable. Then you can use instead of .\n\nOne issue that comes with condition variables are lost wakeups. A lost wakeup happens when the notify call executes before the wait call. In that case the condition variable would wait forever or until the next notify call. The originally intended wakeup was lost.\n\nFor that reason you should always use the wait function overload with the predicate that I used in the example above. There is also a wait function without the predicate that is not recommended to use.\n\nWith the predicate the wakeup is not lost even if the notify call executes before the wait call. Because of the condition that is evaluated in the predicate the wait function returns immediately and the waiting thread continues its work.\n\nIt can also happen that the waiting thread wakes up from waiting on the condition variable for no reason. It wakes up even if no notification was intended.\n\nThat's another reason to always use wait function overload with the predicate. If the thread wakes up, it checks if the predicate returns true. Since it was a spurious wakeup the condition is not satisfied and the thread is suspended again and waits. Without the predicate the waiting thread would continue its work before notify_one was called. With the predicate this is prevented.\n• Do multithreading without using std::thread\n• Only one-shot events are possible\n\nA very nice and simple way to synchronize concurrent tasks is std::future<T >. A future represents a value that may be computed in the future, e.g. it could be the result of a computation that returns an integer (std::future<int > ). This can be the case when you want to execute a long running computation in a separate thread and return the result from the thread back to the main thread. Or it can be simply an event that you want to wait for where no result is returned from the thread (std::future<void > ).\n\nThe major difference compared to condition variables is that futures can be only used once. With the condition variable the parcel delivery thread could produce packets multiple times and notify the waiting thread. With an std::future you could only deliver a single packet.\n\nAdditionally with std::future you may not even need to use std::thread which keeps the complexity of your code low and understandable.\n\nThere are different ways to get a std::future. The easiest way is to use . With std::async you handover a task (a Callable) and a future is returned. On the future you can call the get() function to retrieve the result that is returned from the thread. The get() call blocks the waiting thread until the task from std::async is finished.\n\nWithout an std::future and std::async you would normally need a mutex and modify shared data in order to return a result from the thread. With std::async and std::future this is not needed at all and very easy. Compare the following code to the condition variable example. It's more compact and more understandable:\n\nIf you only want to wait for the result you can also use future.wait() instead of future.get(). Wait does not consume the result of the future in contrast to future.get().\n\nSometimes you cannot use std::async when you cannot set the future's value by a return-statement from the task running in a separate thread. This can be the case when you want to set the value two or more futures from one thread. In that case you can use a pair or multiple pairs of std::promise and std::future.\n\nstd::promise is the other side of the coin of an std::future. Whereas std::future behaves like a receiver, std::promise can be seen as the sender.\n\nFor using promise-future pairs, you first create a promise and use the get_future() function to retrieve the future. On the future you use get() or wait(). On the promise you use set_value(). Have a look on the example:\n\nNote the difference to std::async before: Instead of returning the value from the task, now you pass a promise to the std::thread. Inside the thread you use set_value(..) to resolve the future.\n\nWhen you start the thread you move the promise to the thread. Promises cannot be copied.\n\nWe had a look on three different ways of C++ thread synchronization:\n• The naive approach that better should not be used\n• Condition variables, that are ideal for recurring events in combination with worker threads\n• Futures with either std::async or std::promise (for one-shot events)\n\nFor simplicity reasons you can strive for whenever it's possible. It's the simplest approach and least likely to introduce bugs. You don't need to use mutexes and locks with std::async.\n\nIf std::async cannot be used, use a condition variable as a synchronization mechanism. With condition variables make sure to use a predicate to test the condition to be protected against lost and spurious wakeups."
    },
    {
        "link": "https://en.cppreference.com/w/cpp/thread/mutex",
        "document": "The class is a synchronization primitive that can be used to protect shared data from being simultaneously accessed by multiple threads.\n• A calling thread owns a from the time that it successfully calls either or until it calls .\n• When a thread owns a , all other threads will block (for calls to ) or receive a return value (for ) if they attempt to claim ownership of the .\n• A calling thread must not own the prior to calling or .\n\nThe behavior of a program is undefined if a is destroyed while still owned by any threads, or a thread terminates while owning a . The class satisfies all requirements of Mutex and StandardLayoutType.\n\nis neither copyable nor movable.\n\nis usually not accessed directly: std::unique_lock, std::lock_guard, or std::scoped_lock(since C++17) manage locking in a more exception-safe manner."
    },
    {
        "link": "https://stackoverflow.com/questions/49439929/managing-threads-while-practicing-modern-c17s-best-practices",
        "document": "Originally I had thought about designing a class to store along with the and that they would work with. The class was to be responsible for the managing of memory, access, transferring, releasing, locking, unlocking, joining, and other typical common functionalities of the associated types within the standard multithreading library. It was originally intended to associate the containing thread and its id with a specific set of resources that a particular thread has access to.\n\nAfter reading through the docs on about , , , , , etc. and now knowing that and are non copyable and the fact that if I template the class to store arbitrary , , or s as a within this class's container that the class instantiation of this intended singleton would only be able to store a specific function signature limiting it to not be able to instantiate any other declaration signature.\n\nConcerning these behaviors and properties of the multithreading library within the standard library for , , , , etc... it came to mind that I'm overthinking the overall design of this class.\n\nYou can refer to my initial design attempt via this previously asked question of mine. Storing arbitrary function objects into a class member container without knowing their declaration signature, This should give you an idea of what I was attempting to do.\n\nUnderstanding a little more about their behaviors, properties and responsibilities I would like to know if the following would be appropriate for the intended design process.\n\nInstead of storing any , , , or ; would it make more sense to just store the generated of the , and have my manager class act more like a monitoring, recording, and reporting type class instead?\n\nMy new intentions would be that the container would store the thread's ID in an associated map as its key along with an associated common struct. The struct would contain a property list of all the responsibilities and actions of the combined resources. This may then allow support for some of the following features: a priority queue, a task scheduler, a dispatcher of commands to send and fetch resources knowing if the thread is available or not, where these types of actions will not be done by this class directly but through generic function templates.\n\nWith my intention to implement this kind of design while trying to maintain the best practices of modern c++ targeting c++17; would this kind of design be appropriate for proper class design to be generic, modular, portable and efficient for use?"
    },
    {
        "link": "https://medium.com/@abhishekjainindore24/mutex-in-c-threads-part-1-45aeac3ab62d",
        "document": "Mutex stands for mutual exclusion. When multiple threads share resources (such as variables or data structures), they may try to access and modify these resources simultaneously. This can result in data races, where the result of the program becomes unpredictable.\n\nRace condition is a situation where two or more threads/process happen to change a common data at the same time. If there is a race condition then we have to protect it and the protected section is called critical section/region\n\nIn the code above, two threads ( and ) increment the same variable. However, since the is being accessed and modified concurrently, the output may be inconsistent each time the program runs. This inconsistency occurs because of a data race. Without synchronization, the threads may read the same value before either has updated it, causing the final result to be incorrect.\n\nA mutex ensures that only one thread can access a shared resource at a time. The other threads must wait until the mutex is unlocked. This prevents data races and ensures the integrity of shared data.\n\nHow to Use a Mutex\n\nTo use a mutex in C++, we include the library. The basic operations are:\n• : Locks the mutex, blocking other threads from accessing the critical section.\n• : Unlocks the mutex, allowing other threads to access the critical section.\n\nIn this version, the ensures that when one thread is updating the , the other thread is blocked from accessing it until the first thread calls . As a result, the final value of the counter will be consistent and correct.\n\nIn this version, the ensures that when one thread is updating the , the other thread is blocked from accessing it until the first thread calls . As a result, the final value of the counter will be consistent and correct.\n\nManually locking and unlocking a mutex can be error-prone, especially if an exception occurs between the and calls, leaving the mutex locked indefinitely. To avoid this, C++ provides the class, which automatically locks a mutex when the object is created and unlocks it when the object goes out of scope.\n\nWith , the mutex is automatically unlocked when the object goes out of scope, making the code more robust and less prone to mistakes.\n\nA deadlock occurs when two or more threads are waiting for each other to release a mutex, causing both threads to remain blocked indefinitely. Deadlocks often happen when two or more mutexes are locked in different orders by different threads.\n\nIn this code, locks first, and locks first. If both threads try to acquire the second mutex while holding the first, they will end up waiting for each other indefinitely, resulting in a deadlock.\n\nTo avoid this, you can:\n• Always lock mutexes in the same order in all threads.\n• Use , which locks multiple mutexes in a deadlock-free way.\n\nIn addition to , the C++ Standard Library provides several other mutex types for specific use cases:\n• : Allows the same thread to lock the mutex multiple times. This is useful when a function that locks a mutex calls itself recursively.\n• : Allows the thread to attempt locking for a specified duration or until a timeout.\n• (C++17): Allows multiple readers but only one writer at a time. Useful for scenarios where multiple threads need read access, but write access is rare."
    },
    {
        "link": "https://en.cppreference.com/w/cpp/thread",
        "document": "C++ includes built-in support for threads, atomic operations, mutual exclusion, condition variables, and futures.\n\nThreads enable programs to execute across several processor cores.\n\nThe components stop source, stop token, and stop callback can be used to asynchronously request that an operation stops execution in a timely manner, typically because the result is no longer required. Such a request is called a stop request.\n\nThese components specify the semantics of shared access to a stop state. Any object modeling any of these components that refer to the same stop state is an associated stop source, stop token, or stop callback, respectively.\n• to cooperatively cancel the execution such as for ,\n\nIn fact, they do not even need to be used to \"stop\" anything, but can instead be used for a thread-safe one-time function(s) invocation trigger, for example.\n\nThese components are provided for fine-grained atomic operations allowing for lockless concurrent programming. Each atomic operation is indivisible with regards to any other atomic operation that involves the same object. Atomic objects are free of data races.\n\nNeither the macro, nor any of the non-macro global namespace declarations are provided by any C++ standard library header other than .\n\nMutual exclusion algorithms prevent multiple threads from simultaneously accessing shared resources. This prevents data races and provides support for synchronization between threads.\n\nA condition variable is a synchronization primitive that allows multiple threads to communicate with each other. It allows some number of threads to wait (possibly with a timeout) for notification from another thread that they may proceed. A condition variable is always associated with a mutex.\n\nA semaphore is a lightweight synchronization primitive used to constrain concurrent access to a shared resource. When either would suffice, a semaphore can be more efficient than a condition variable.\n\nLatches and barriers are thread coordination mechanisms that allow any number of threads to block until an expected number of threads arrive. A latch cannot be reused, while a barrier can be used repeatedly.\n\nThe standard library provides facilities to obtain values that are returned and to catch exceptions that are thrown by asynchronous tasks (i.e. functions launched in separate threads). These values are communicated in a shared state, in which the asynchronous task may write its return value or store an exception, and which may be examined, waited for, and otherwise manipulated by other threads that hold instances of std::future or std::shared_future that reference that shared state.\n\nSafe-reclamation techniques are most frequently used to straightforwardly resolve access-deletion races."
    },
    {
        "link": "https://stackoverflow.com/questions/4989451/mutex-example-tutorial",
        "document": "While a mutex may be used to solve other problems, the primary reason they exist is to provide mutual exclusion and thereby solve what is known as a race condition. When two (or more) threads or processes are attempting to access the same variable concurrently, we have potential for a race condition. Consider the following code\n\nThe internals of this function look so simple. It's only one statement. However, a typical pseudo-assembly language equivalent might be:\n\nBecause the equivalent assembly-language instructions are all required to perform the increment operation on i, we say that incrementing i is a non-atmoic operation. An atomic operation is one that can be completed on the hardware with a gurantee of not being interrupted once the instruction execution has begun. Incrementing i consists of a chain of 3 atomic instructions. In a concurrent system where several threads are calling the function, problems arise when a thread reads or writes at the wrong time. Imagine we have two threads running simultaneoulsy and one calls the function immediately after the other. Let's also say that we have i initialized to 0. Also assume that we have plenty of registers and that the two threads are using completely different registers, so there will be no collisions. The actual timing of these events may be:\n\nWhat's happened is that we have two threads incrementing i concurrently, our function gets called twice, but the outcome is inconsistent with that fact. It looks like the function was only called once. This is because the atomicity is \"broken\" at the machine level, meaning threads can interrupt each other or work together at the wrong times.\n\nWe need a mechanism to solve this. We need to impose some ordering to the instructions above. One common mechanism is to block all threads except one. Pthread mutex uses this mechanism.\n\nAny thread which has to execute some lines of code which may unsafely modify shared values by other threads at the same time (using the phone to talk to his wife), should first be made acquire a lock on a mutex. In this way, any thread that requires access to the shared data must pass through the mutex lock. Only then will a thread be able to execute the code. This section of code is called a critical section.\n\nOnce the thread has executed the critical section, it should release the lock on the mutex so that another thread can acquire a lock on the mutex.\n\nThe concept of having a mutex seems a bit odd when considering humans seeking exclusive access to real, physical objects but when programming, we must be intentional. Concurrent threads and processes don't have the social and cultural upbringing that we do, so we must force them to share data nicely.\n\nSo technically speaking, how does a mutex work? Doesn't it suffer from the same race conditions that we mentioned earlier? Isn't pthread_mutex_lock() a bit more complex that a simple increment of a variable?\n\nTechnically speaking, we need some hardware support to help us out. The hardware designers give us machine instructions that do more than one thing but are guranteed to be atomic. A classic example of such an instruction is the test-and-set (TAS). When trying to acquire a lock on a resource, we might use the TAS might check to see if a value in memory is 0. If it is, that would be our signal that the resource is in use and we do nothing (or more accurately, we wait by some mechanism. A pthreads mutex will put us into a special queue in the operating system and will notify us when the resource becomes available. Dumber systems may require us to do a tight spin loop, testing the condition over and over). If the value in memory is not 0, the TAS sets the location to something other than 0 without using any other instructions. It's like combining two assembly instructions into 1 to give us atomicity. Thus, testing and changing the value (if changing is appropriate) cannot be interrupted once it has begun. We can build mutexes on top of such an instruction.\n\nNote: some sections may appear similar to an earlier answer. I accepted his invite to edit, he preferred the original way it was, so I'm keeping what I had which is infused with a little bit of his verbiage."
    }
]