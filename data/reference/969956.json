[
    {
        "link": "https://tensorflow.org/addons/tutorials/optimizers_cyclicallearningrate",
        "document": "Warning: This project is deprecated . TensorFlow Addons has stopped development, The project will only be providing minimal maintenance releases until May 2024. See the full announcement here or on github .\n\nThis tutorial demonstrates the use of Cyclical Learning Rate from the Addons package.\n\nIt has been shown it is beneficial to adjust the learning rate as training progresses for a neural network. It has manifold benefits ranging from saddle point recovery to preventing numerical instabilities that may arise during backpropagation. But how does one know how much to adjust with respect to a particular training timestamp? In 2015, Leslie Smith noticed that you would want to increase the learning rate to traverse faster across the loss landscape but you would also want to reduce the learning rate when approaching convergence. To realize this idea, he proposed Cyclical Learning Rates (CLR) where you would adjust the learning rate with respect to the cycles of a function. For a visual demonstration, you can check out this blog. CLR is now available as a TensorFlow API. For more details, check out the original paper here.\n\nIn the interest of reproducibility, the initial model weights are serialized which you will be using to conduct our experiments.\n\nThe module return a direct schedule that can be passed to an optimizer. The schedule takes a step as its input and outputs a value calculated using CLR formula as laid out in the paper.\n\nHere, you specify the lower and upper bounds of the learning rate and the schedule will oscillate in between that range ([1e-4, 1e-2] in this case). is used to define the function that would scale up and scale down the learning rate within a given cycle. defines the duration of a single cycle. A of 2 means you need a total of 4 iterations to complete one cycle. The recommended value for is as follows:\n\nwhere factor lies within the [2, 8] range.\n\nIn the same CLR paper, Leslie also presented a simple and elegant method to choose the bounds for learning rate. You are encouraged to check it out as well. This blog post provides a nice introduction to the method.\n\nBelow, you visualize how the schedule looks like.\n\nIn order to better visualize the effect of CLR, you can plot the schedule with an increased number of steps.\n\nThe function you are using in this tutorial is referred to as the method in the CLR paper. There are other two functions there were explored namely and (short for exponential).\n\nAs expected the loss starts higher than the usual and then it stabilizes as the cycles progress. You can confirm this visually with the plots below.\n\nEven though for this toy example, you did not see the effects of CLR much but be noted that it is one of the main ingredients behind Super Convergence and can have a really good impact when training in large-scale settings."
    },
    {
        "link": "https://github.com/mhmoodlan/cyclic-learning-rate",
        "document": "TensorFlow implementation of cyclic learning rate from the paper: Smith, Leslie N. \"Cyclical learning rates for training neural networks.\" 2017.\n\nCLR is used to enhance the way the learning rate is scheduled during training, to provide better convergence and help in regularizing deep learning models. It eliminates the need to experimentally find the best values for the global learning rate. Allowing the learning rate to cyclically vary between lower and upper boundary values. The idea is to divide the training process into cycles determined by a stepsize parameter, which defines the number of iterations in half a cycle. The author claims that it is often good to set the stepsize to:\n\nThe learning rate is computed as:\n\nThe author proposes three variations of this policy:\n• 'triangular': Default, linearly increasing then linearly decreasing the learning rate at each cycle.\n• 'triangular2': The same as the triangular policy except that the learning rate difference is cut in half at the end of each cycle. This means the learning rate difference drops after each cycle.\n• 'exp_range': The learning rate varies between the minimum and maximum boundaries and each boundary value declines by an exponential factor of:\n\nWhere global_step is a number indicating the current iteration and gamma is a constant passed as an argument to the CLR callback.\n\nUpgrade to the latest version of TensorFlow:\n\nThis project is licensed under the MIT License - see the LICENSE.md file for details"
    },
    {
        "link": "https://medium.com/towards-data-science/super-convergence-with-cyclical-learning-rates-in-tensorflow-c1932b858252",
        "document": "Super-Convergence using Cyclical Learning Rate schedules is one of the most useful techniques in deep learning and very often overlooked. It allows for rapid prototyping of network architectures, loss function engineering, data augmentation experiments and training production ready models in orders of magnitude less training time and epochs.\n\nIt is demonstrated within this article and the accompanying code that it is possible to train deep neural network models for complex tasks such as super resolution from initialised weights (i.e. not pre-trained or transferred) within minutes.\n\nYou can connect with me on LinkedIn or follow me on Twitter.\n\nThe techniques for training in this way are from research that is a few years old. In 2015 Leslie Smith published a paper titled Cyclical Learning Rates for Training Neural Networks that outlines methods to train deep neural network models quicker (with fewer epochs) by cycling up to very high learning rates. Sadly this went widely unnoticed by the majority of deep learning practitioners.\n\nThis technique is so effective that a state of the art super resolution model can be trained in as low as 16 epochs in approximately 4 minutes from initialisation not using a pre-trained model. With impressive results such as the following.\n\nExample super resolution performed on crops of images from the DIV2K validation set\n\nGenerated images from a IDN deep neural network architecture based on the Fast and Accurate Single Image Super-Resolution via Information Distillation Network paper whose model was trained for just 16 epochs on a training set of 800 images in the DIV2K training set. This is not a model trained using transfer learning.\n\nThe learning rate used for training cycled between 7e-3 and 7e-4 with a final cycle between 7e-5 and 7e-6.\n\n7e-3 is much higher than the learning rates traditionally used for training these deep neural networks, which typically might be 2e-4 then decaying piecewise.\n\nThe training time does depend on hardware, this research was carried out using an NVidia 3080RTX for the 4 minute training time figures.\n\nUsually a high learning rate can cause unstable training and result in a model that is diverged and unable to be trained. A small learning rate may never converge or may get stuck on a sub-optimal model. Hence moderate learning rates are chosen and used over many epochs, for example 10,000 epochs is not uncommon. Stepped learning rates are not uncommon, to have a few steps lowering the learning rate during training such as the Piecewise Constant Decay learning rate schedule.\n\nWith a Cyclical Learning Rate schedule the high learning rate is only maintained for a short time and helps to avoid the training becoming unstable, exploding gradients and the model diverging. The higher learning rate even for short times allows a close to optimal model to be trained and found in a relatively short number of epochs.\n\nThere are other factors as well such as having a deep neural network architecture that can train quickly, finding and using the most optimal initialisation. Using super convergence allows you to find and test these easily and quickly.\n\nHow many experiments in research don’t result in being fruitful, being able to carry out more and more varied experiments in less time is invaluable to researchers.\n\nIf you are training upon Cloud Infrastructure then there’s cost savings in training faster and less energy use, which can’t be a bad thing when cloud datacentres make up over 1% of World’s energy usage.\n\nThe Cyclical Learning Rate schedule has been popularised to some practitioners and students with PyTorch by Jeremy Howard in his Fastai libraries and course, in particular with using transfer learning and re-training modified existing (pre-trained) models. After moving to implementing deep learning in TensorFlow alongside PyTorch I found it surprising not to find much evidence of Super Convergence being used in TensorFlow 2 nor many practical examples, hence the inspiration to write this and hopefully help others.\n\nBeing able to experiment rapidly testing more architectures, varying them and tuning their hyperparameters is possible with using a Cyclical Learning Rate schedule to achieve super convergence.\n\nEngineering effective Loss functions can be complex, especially with blended and weighted values as outlined in my article here and being able to evaluate adjustments potentially in minutes, rather than hours or days increases productivity and allows experiments to be more varied and more detailed.\n\nData augmentation is another very effective tool, which is under utilised by many deep learning practitioners. Using a Cyclical Learning Rate schedule to achieve super convergence allows rapid testing of different data augmentation to see how effective each augmentation technique is and to tune the hyperparameters of the augmentation. For example, finding an appropriate delta for a random brightness augmentation.\n\nThis is fairly much the extent of the documentation for TensorFlow 2’s Cyclical Learning rate schedule. It is good its been implemented and is available in the TensorFlow addons.\n\nThe implementation isn’t too complex if you know what the parameters are and what to specify, which the documentation is somewhat lacking upon.\n\nYou’ll need to install TensorFlow addons with pip or otherwise.\n\nThen import the Cyclical learning rate in the code\n\nThen create a Cyclical Learning Rate schedule, each parameter is detailed further below. I believe that maybe a lack of clarity on these parameters is one of the reasons this TensorFlow learning rate schedule is not in wider use.\n\nThen this is specified as the optimiser for example with the Adam algorithm (implemented within tf.keras.optimizers.Adam) for training.\n\nInitial learning rate is the base or lowest learning rate and the learning rate at the start of each cycle of training.\n\nMaximal learning rate is the highest learning rate and the learning rate at the middle of the first cycle of training and subsequent depending upon your scale function.\n\nThe step size is half of a cycle. The step size is the number of iterations used for each step and a cycle consists of two such steps: one in which the learning rate increases and the other in which it decreases. In the paper Leslie Smith suggests experiments indicate a step size equal to 2 to 10 times the number of iterations in an epoch is good.\n\nTherefore the step size should be set as the iterations within each epoch (in TensorFlow 2, the number of items in training set divided by the batch size) multiplied by a factor between 2 and 10.\n\nThe Scale function is the function controlling the change from the initial learning rate to the maximal learning rate and back to the initial learning rate. In the paper this is one of triangular, triangular 2 or exponential range. In my own experiments with models for generative images (for example super resolution) I have found Triangular 2 to be most effective.\n\nTriangular 2: A basic triangular cycle that scales initial amplitude by half with each cycle:\n\nExponential range: A cycle that scales initial amplitude by gamma to the power of the cycle iterations with each cycle:\n\nThe number of training steps should therefore be the step size multiplied by 2 (two steps in a cycle) then multiplied by 3 or more(cycles). This results in a minimum of 12 epochs of training.\n\nIt is of interest, through my experiments I have often found 3 cycles trains a higher performing model then 4 cycles, training for more cycles has small increases in accuracy/performance.\n\nThis parameter should be determined by how important a very small increase in accuracy is against longer training times.\n\nAlso, in the paper Leslie Smith states:\n\nIts very important to get this calculation correct to ensure train stops at the end of a cycle.\n\nAn observation Leslie Smith makes in the paper is that in experiments 8 or 10 epochs within each step often only has marginal improvements over 2 epochs. This parameter should be determined by how important a very small increase in accuracy is.\n\nIn 2017 Leslie Smith and Nicholay Topin published a paper titled “Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates”. This paper describes how to use a Learning Rate range test to check if super converge is possible for a model and a method to select the initial and maximum learning rates. Again sadly this went widely unnoticed by the majority of deep learning practitioners.\n\nIn the paper it suggests (CLR being Cyclical Learning Rate):\n\nWithin my own experiments and research I have found reducing the learning rate by an order of magnitude, so 10 times lower works more effectively.\n\nThere are many ways to implement a learning rate finder, often graphs from Tensorboard are examined to find the optimal maximum learning rate. Usually the learning rate is slowly increased with each batch size of training until the loss stops improving and the model diverges. Then the maximum learning rate is chosen as a value slightly less than this.\n\nIn the paper it suggests using one final cycle with a learning rate several orders of magnitude lower, so perhaps 100 or 1000 times lower.\n\nThis takes the previous example of 12 training epochs up to 16 epochs, which is still a spectacularly low number of training epochs.\n\nModels trained with Super-convergence may not be better than traditional piecewise training carried out for thousands of times of more epochs. From my experiments and research I’ve found the models are at best approaching the models trained with piecewise training.\n\nHowever they are likely to be close enough in many circumstances. Getting close to 99% as accurate in such a short training period is likely to be enough and might actually result is less over fitting with models that can generalise better.\n\nHere is my example implementation demonstrating Super-Convergence using Cyclical Learning Rate schedules:\n\nThis is an improved version of the IDN deep neural network from the Fast and Accurate Single Image Super-Resolution via Information Distillation Network paper by Zheng Hui, Xiumei Wang and Xinbo Gao.\n\nI am using the SSIM to gauge quality as I don’t believe a mean average of PSNR across images is accurate as it is a logarithmic based metric. Comparing PSNR across multiple images may not be that meaningful.\n\nThis modified/improved version of IDN is the result of research I am carrying out which will be detailed in another future article.\n\nYou can connect with me on LinkedIn or follow me on Twitter."
    },
    {
        "link": "https://pyimagesearch.com/2019/07/29/cyclical-learning-rates-with-keras-and-deep-learning",
        "document": "In this tutorial, you will learn how to use Cyclical Learning Rates (CLR) and Keras to train your own neural networks. Using Cyclical Learning Rates you can dramatically reduce the number of experiments required to tune and find an optimal learning rate for your model.\n\nToday is part two in our three-part series on tuning learning rates for deep neural networks:\n\nLast week we discussed the concept of learning rate schedules and how we can decay and decrease our learning rate over time according to a set function (i.e., linear, polynomial, or step decrease).\n\nHowever, there are two problems with basic learning rate schedules:\n• We don’t know what the optimal initial learning rate is.\n• Monotonically decreasing our learning rate may lead to our network getting “stuck” in plateaus of the loss landscape.\n\nCyclical Learning Rates take a different approach. Using CLRs, we now:\n• Allow the learning rate to cyclically oscillate between the two bounds\n\nIn practice, using Cyclical Learning Rates leads to faster convergence and with fewer experiments/hyperparameter updates.\n\nAnd when we combine CLRs with next week’s technique on automatically finding optimal learning rates, you may never need to tune your learning rates again! (or at least run far fewer experiments to tune them).\n\nTo learn how to use Cyclical Learning Rates with Keras, just keep reading!\n\n2020-06-11 Update: This blog post is now TensorFlow 2+ compatible!\n\nIn the first part of this tutorial, we’ll discuss Cyclical Learning Rates, including:\n• Why should we use Cyclical Learning Rates?\n• How do we use Cyclical Learning Rates with Keras?\n\nFrom there, we’ll implement CLRs and train a variation of GoogLeNet on the CIFAR-10 dataset — I’ll even point out how to use Cyclical Learning Rates with your own custom datasets.\n\nFinally, we’ll review the results of our experiments and you’ll see firsthand how CLRs can reduce the number of learning rate trials you need to perform to find an optimal learning rate range.\n\nAs we discussed in last week’s post, we can define learning rate schedules that monotonically decrease our learning rate after each epoch.\n\nBy decreasing our learning rate over time we can allow our model to (ideally) descend into lower areas of the loss landscape.\n\nIn practice; however, there are a few problems with a monotonically decreasing learning rate:\n• First, our model and optimizer are still sensitive to our initial choice in learning rate.\n• Second, we don’t know what the initial learning rate should be — we may need to perform 10s to 100s of experiments just to find our initial learning rate.\n• Finally, there is no guarantee that our model will descend into areas of low loss when lowering the learning rate.\n\nTo address these issues, Leslie Smith of the NRL introduced Cyclical Learning Rates in his 2015 paper, Cyclical Learning Rates for Training Neural Networks.\n\nNow, instead of monotonically decreasing our learning rate, we instead:\n• Define the lower bound on our learning rate (called “base_lr”).\n• Define the upper bound on the learning rate (called the “max_lr”).\n• Allow the learning rate to oscillate back and forth between these two bounds when training, slowly increasing and decreasing the learning rate after every batch update.\n\nAn example of a Cyclical Learning Rate can be seen in Figure 1.\n\nNotice how our learning rate follows a triangular pattern. First, the learning rate is very small. Then, over time, the learning rate continues to grow until it hits the maximum value. The learning rate then descends back down to the base value. This cyclical pattern continues throughout training.\n\nWhy should we use Cyclical Learning Rates?\n\nAs mentioned above, Cyclical Learning Rates enables our learning rate to oscillate back and forth between a lower and upper bound.\n\nSo, why bother going through all the trouble?\n\nWhy not just monotonically decrease our learning rate, just as we’ve always done?\n\nThe first reason is that our network may become stuck in either saddle points or local minima, and the low learning rate may not be sufficient to break out of the area and descend into areas of the loss landscape with lower loss.\n\nSecondly, our model and optimizer may be very sensitive to our initial learning rate choice. If we make a poor initial choice in learning rate, our model may be stuck from the very start.\n\nInstead, we can use Cyclical Learning Rates to oscillate our learning rate between upper and lower bounds, enabling us to:\n• Have more freedom in our initial learning rate choices.\n• Break out of saddle points and local minima.\n\nIn practice, using CLRs leads to far fewer learning rate tuning experiments along with near identical accuracy to exhaustive hyperparameter tuning.\n\nHow do we use Cyclical Learning Rates?\n\nWe’ll be using Brad Kenstler’s implementation of Cyclical Learning Rates for Keras.\n\nIn order to use this implementation we need to define a few values first:\n• Batch size: Number of training examples to use in a single forward and backward pass of the network during training.\n• Batch/Iteration: Number of weight updates per epoch (i.e., # of total training examples divided by the batch size).\n• Cycle: Number of iterations it takes for our learning rate to go from the lower bound, ascend to the upper bound, and then descend back to the lower bound again.\n• Step size: Number of iterations in a half cycle. Leslie Smith, the creator of CLRs, recommends that the step_size should be ). In practice, I have found that step sizes of either 4 or 8 work well in most situations.\n\nWith these terms defined, let’s see how they work together to define a Cyclical Learning Rate policy.\n\nOur learning rate starts off at the base value and then starts to increase.\n\nWe reach the maximum learning rate value halfway through the cycle (i.e., the step size, or number of iterations in a half cycle). Once the maximum learning rate is hit, we then decrease the learning rate back to the base value. Again, it takes a half cycle to return to the base learning rate.\n\nThis entire process repeats (i.e., cyclical) until training is terminated.\n\nThe “triangular2” CLR policy is similar to the standard “triangular” policy, but instead cuts our max learning rate bound in half after every cycle.\n\nThe argument here is that we get the best of both worlds:\n\nWe can oscillate our learning rate to break out of saddle points/local minima…\n\n…and at the same time decrease our learning rate, enabling us to descend into lower loss areas of the loss landscape.\n\nFurthermore, reducing our maximum learning rate over time helps stabilize our training. Later epochs with the “triangular” policy may exhibit large jumps in both loss and accuracy — the “triangular2” policy will help stabilize these jumps.\n\nThe “exp_range” Cyclical Learning Rate policy is similar to the “triangular2” policy, but, as the name suggests, instead follows an exponential decay, giving you more fine-tuned control in the rate of decline in max learning rate.\n\nNote: In practice, I don’t use the “exp_range” policy — the “triangular” and “triangular2” policies are more than sufficient in the vast majority of projects.\n\nHow do I install Cyclical Learning Rates on my system?\n\nThe Cyclical Learning Rate implementation we are using is not pip-installable.\n\nInstead, you can either:\n• Use the “Downloads” section to grab the file and associated code/data for this tutorial.\n• Download the file from the GitHub repo (linked to above) and insert it into your project.\n\nFrom there, let’s move on to training our first CNN using a Cyclical Learning Rate.\n\nTo configure your system for this tutorial, I first recommend following either of these tutorials:\n• How to install TensorFlow 2.0 on Ubuntu\n• How to install TensorFlow 2.0 on macOS\n\nEither tutorial will help you configure you system with all the necessary software for this blog post in a convenient Python virtual environment.\n\nPlease note that PyImageSearch does not recommend or support Windows for CV/DL projects.\n\nGo ahead and run the command from within the directory to print our project structure:\n\nThe directory will contain our CLR and accuracy/loss plots.\n\nThe module contains our cyclical learning rate callback class, MiniGoogLeNet CNN, and configuration file:\n• The file contains the Cyclical Learning Rate callback which will update our learning rate automatically at the end of each batch update.\n• The file holds the CNN which we will train using CIFAR-10 data. We will not review MiniGoogLeNet today — please refer to Deep Learning for Computer Vision with Python to learn more about this CNN architecture.\n• Our is simply a Python file containing configuration variables — we’ll review it in the next section.\n\nOur training script, , trains MiniGoogLeNet using the CIFAR-10 dataset. The training script takes advantage of our CLR callback and configuration.\n\nBefore we implement our training script, let’s first review our configuration file:\n\nWe will use the module in our config so that we can construct operating system-agnostic paths directly (Line 2).\n\nFrom there, our CIFAR-10 are defined (Lines 5 and 6).\n\nThe and define our base learning rate and maximum learning rate, respectively (Lines 10 and 11). I know these learning rates will work well when training MiniGoogLeNet per the experiments I have already run for Deep Learning for Computer Vision with Python — next week I will show you how to automatically find these values.\n\nThe (Line 12) is the number of training examples per batch update.\n\nWe then have the which is the number of batch updates in a half cycle (Line 13).\n\nThe controls our Cyclical Learning Rate policy (Line 14). Here we are using the policy, as discussed in the previous section.\n\nWe can calculate the number of full CLR cycles in a given number of epochs via:\n\nFor example, with and , there will be a total of 6 full cycles: .\n\nWe’ll plot a training history accuracy/loss plot as well as a cyclical learning rate plot. You may specify the paths + filenames of the plots on Lines 19 and 20.\n\nWith our configuration defined, we can move on to implementing our training script.\n\nOpen up and insert the following code:\n\nLines 2-15 import our necessary packages. Most notably our (from the file) is imported via Line 7. The backend is set on Line 3 so that our plots can be written to disk at the end of the training process.\n\nLines 20-22 load the CIFAR-10 image dataset. The data is pre-split into training and testing sets.\n\nFrom there, we calculate the and apply mean subtraction (Lines 25-27). Mean subtraction is a normalization/scaling technique that results in improved model accuracy. For more details, please refer to the Practitioner Bundle of Deep Learning for Computer Vision with Python.\n\nNext, we initialize our data augmentation object (Lines 35-37). Data augmentation increases model generalization by producing randomly mutated images from your dataset during training. I’ve written about data augmentation in-depth in Deep Learning for Computer Vision with Python as well as two blog posts (How to use Keras fit and fit_generator (a hands-on tutorial) and Keras ImageDataGenerator and Data Augmentation).\n\nLet’s initialize (1) our model, and (2) our cyclical learning rate callback:\n\nOur is initialized with stochastic gradient descent ( ) optimization and loss (Lines 41-44). If you have only two classes in your dataset, be sure to set .\n\nNext, we initialize the cyclical learning rate callback via Lines 48-52. The CLR parameters are provided to the constructor. Now is a great time to review them at the top of the “How do we use Cyclical Learning Rates?” section above. The follow’s Leslie Smith’s recommendation of setting it to be a multiple of the number of batch updates per epoch.\n\nLet’s train and evaluate our model using CLR now:\n\n2020-06-11 Update: Formerly, TensorFlow/Keras required use of a method called in order to accomplish data augmentation. Now, the method can handle data augmentation as well, making for more-consistent code. This also applies to the migration from to . Be sure to check out my articles about fit and fit_generator as well as data augmentation.\n\nLines 56-62 launch training using the callback and data augmentation.\n\nThen Lines 66-68 evaluate the network on the testing set and print a .\n\n2020-06-11 Update: In order for this plotting snippet to be TensorFlow 2+ compatible the dictionary keys are updated to fully spell out “accuracy” sans “acc” (i.e., and ). It is semi-confusing that “val” is not spelled out as “validation”; we have to learn to love and live with the API and always remember that it is a work in progress that many developers around the world contribute to.\n• Training accuracy/loss history (Lines 71-82). The standard plot format included in most of my tutorials and every experiment of my deep learning book.\n• Learning rate history (Lines 86-91). This plot will help us to visually verify that our learning rate is oscillating according to our intentions.\n\nWe are now ready to train our CNN using Cyclical Learning Rates with Keras!\n\nMake sure you’ve used the “Downloads” section of this post to download the source code — from there, open up a terminal and execute the following command:\n\nAs you can see, by using the “triangular” CLR policy we are obtaining 91% accuracy on our CIFAR-10 testing set.\n\nThe following figure shows the learning rate plot, demonstrating how it cyclically starts at our lower learning rate bound, increases to the maximum value at half a cycle, and then decreases again to the lower bound, completing the cycle:\n\nExamining our training history you can see the cyclical behavior of the learning rate:\n\nNotice the “wave” in the training accuracy and validation accuracy — the bottom of the wave is our base learning rate, the top of the wave is the upper bound on the learning rate, and the bottom of the wave, just before the next one starts, is the lower learning rate.\n\nJust for fun, go back to Line 14 of the file and update the to be instead of :\n\nFrom there, train the network:\n\nThis time we are obtaining 90% accuracy, slightly lower than using the “triangular” policy.\n\nOur learning rate plot visualizes how our learning rate is cyclically updated:\n\nNotice at after each complete cycle the maximum learning rate is halved. Since our maximum learning rate is decreasing after every cycle, our “waves” in the training and validation accuracy will be much less pronounced:\n\nWhile the “triangular” Cyclical Learning Rate policy obtained slightly better accuracy, it also exhibited far much fluctuation and had more risk of overfitting.\n\nIn contrast, the “triangular2” policy, while being less accurate, is more stable in its training.\n\nWhen performing your own experiments with Cyclical Learning Rates I suggest you test both policies and choose the one that balances both accuracy and stability (i.e., stable training with less risk of overfitting).\n\nIn next week’s tutorial, I’ll show you how you can automatically define your minimum and maximum learning rate bounds with Cyclical Learning Rates.\n\nIn this tutorial, you learned how to use Cyclical Learning Rates (CLRs) with Keras.\n\nUnlike standard learning rate decay schedules, which monotonically decrease our learning rate, CLRs instead:\n• Allow the learning rate to cyclically oscillate between the two bounds\n\nCyclical Learning rates often lead to faster convergence with fewer experiments and hyperparameter tuning.\n\nHow do we know that the optimal lower and upper bounds of the learning rate are?\n\nThat’s a great question — and I’ll be answering it in next week’s post where I’ll show you how to automatically find optimal learning rate values.\n\nTo download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), just enter your email address in the form below."
    },
    {
        "link": "https://medium.com/data-science/super-convergence-with-cyclical-learning-rates-in-tensorflow-c1932b858252",
        "document": "Super-Convergence using Cyclical Learning Rate schedules is one of the most useful techniques in deep learning and very often overlooked. It allows for rapid prototyping of network architectures, loss function engineering, data augmentation experiments and training production ready models in orders of magnitude less training time and epochs.\n\nIt is demonstrated within this article and the accompanying code that it is possible to train deep neural network models for complex tasks such as super resolution from initialised weights (i.e. not pre-trained or transferred) within minutes.\n\nYou can connect with me on LinkedIn or follow me on Twitter.\n\nThe techniques for training in this way are from research that is a few years old. In 2015 Leslie Smith published a paper titled Cyclical Learning Rates for Training Neural Networks that outlines methods to train deep neural network models quicker (with fewer epochs) by cycling up to very high learning rates. Sadly this went widely unnoticed by the majority of deep learning practitioners.\n\nThis technique is so effective that a state of the art super resolution model can be trained in as low as 16 epochs in approximately 4 minutes from initialisation not using a pre-trained model. With impressive results such as the following.\n\nExample super resolution performed on crops of images from the DIV2K validation set\n\nGenerated images from a IDN deep neural network architecture based on the Fast and Accurate Single Image Super-Resolution via Information Distillation Network paper whose model was trained for just 16 epochs on a training set of 800 images in the DIV2K training set. This is not a model trained using transfer learning.\n\nThe learning rate used for training cycled between 7e-3 and 7e-4 with a final cycle between 7e-5 and 7e-6.\n\n7e-3 is much higher than the learning rates traditionally used for training these deep neural networks, which typically might be 2e-4 then decaying piecewise.\n\nThe training time does depend on hardware, this research was carried out using an NVidia 3080RTX for the 4 minute training time figures.\n\nUsually a high learning rate can cause unstable training and result in a model that is diverged and unable to be trained. A small learning rate may never converge or may get stuck on a sub-optimal model. Hence moderate learning rates are chosen and used over many epochs, for example 10,000 epochs is not uncommon. Stepped learning rates are not uncommon, to have a few steps lowering the learning rate during training such as the Piecewise Constant Decay learning rate schedule.\n\nWith a Cyclical Learning Rate schedule the high learning rate is only maintained for a short time and helps to avoid the training becoming unstable, exploding gradients and the model diverging. The higher learning rate even for short times allows a close to optimal model to be trained and found in a relatively short number of epochs.\n\nThere are other factors as well such as having a deep neural network architecture that can train quickly, finding and using the most optimal initialisation. Using super convergence allows you to find and test these easily and quickly.\n\nHow many experiments in research don’t result in being fruitful, being able to carry out more and more varied experiments in less time is invaluable to researchers.\n\nIf you are training upon Cloud Infrastructure then there’s cost savings in training faster and less energy use, which can’t be a bad thing when cloud datacentres make up over 1% of World’s energy usage.\n\nThe Cyclical Learning Rate schedule has been popularised to some practitioners and students with PyTorch by Jeremy Howard in his Fastai libraries and course, in particular with using transfer learning and re-training modified existing (pre-trained) models. After moving to implementing deep learning in TensorFlow alongside PyTorch I found it surprising not to find much evidence of Super Convergence being used in TensorFlow 2 nor many practical examples, hence the inspiration to write this and hopefully help others.\n\nBeing able to experiment rapidly testing more architectures, varying them and tuning their hyperparameters is possible with using a Cyclical Learning Rate schedule to achieve super convergence.\n\nEngineering effective Loss functions can be complex, especially with blended and weighted values as outlined in my article here and being able to evaluate adjustments potentially in minutes, rather than hours or days increases productivity and allows experiments to be more varied and more detailed.\n\nData augmentation is another very effective tool, which is under utilised by many deep learning practitioners. Using a Cyclical Learning Rate schedule to achieve super convergence allows rapid testing of different data augmentation to see how effective each augmentation technique is and to tune the hyperparameters of the augmentation. For example, finding an appropriate delta for a random brightness augmentation.\n\nThis is fairly much the extent of the documentation for TensorFlow 2’s Cyclical Learning rate schedule. It is good its been implemented and is available in the TensorFlow addons.\n\nThe implementation isn’t too complex if you know what the parameters are and what to specify, which the documentation is somewhat lacking upon.\n\nYou’ll need to install TensorFlow addons with pip or otherwise.\n\nThen import the Cyclical learning rate in the code\n\nThen create a Cyclical Learning Rate schedule, each parameter is detailed further below. I believe that maybe a lack of clarity on these parameters is one of the reasons this TensorFlow learning rate schedule is not in wider use.\n\nThen this is specified as the optimiser for example with the Adam algorithm (implemented within tf.keras.optimizers.Adam) for training.\n\nInitial learning rate is the base or lowest learning rate and the learning rate at the start of each cycle of training.\n\nMaximal learning rate is the highest learning rate and the learning rate at the middle of the first cycle of training and subsequent depending upon your scale function.\n\nThe step size is half of a cycle. The step size is the number of iterations used for each step and a cycle consists of two such steps: one in which the learning rate increases and the other in which it decreases. In the paper Leslie Smith suggests experiments indicate a step size equal to 2 to 10 times the number of iterations in an epoch is good.\n\nTherefore the step size should be set as the iterations within each epoch (in TensorFlow 2, the number of items in training set divided by the batch size) multiplied by a factor between 2 and 10.\n\nThe Scale function is the function controlling the change from the initial learning rate to the maximal learning rate and back to the initial learning rate. In the paper this is one of triangular, triangular 2 or exponential range. In my own experiments with models for generative images (for example super resolution) I have found Triangular 2 to be most effective.\n\nTriangular 2: A basic triangular cycle that scales initial amplitude by half with each cycle:\n\nExponential range: A cycle that scales initial amplitude by gamma to the power of the cycle iterations with each cycle:\n\nThe number of training steps should therefore be the step size multiplied by 2 (two steps in a cycle) then multiplied by 3 or more(cycles). This results in a minimum of 12 epochs of training.\n\nIt is of interest, through my experiments I have often found 3 cycles trains a higher performing model then 4 cycles, training for more cycles has small increases in accuracy/performance.\n\nThis parameter should be determined by how important a very small increase in accuracy is against longer training times.\n\nAlso, in the paper Leslie Smith states:\n\nIts very important to get this calculation correct to ensure train stops at the end of a cycle.\n\nAn observation Leslie Smith makes in the paper is that in experiments 8 or 10 epochs within each step often only has marginal improvements over 2 epochs. This parameter should be determined by how important a very small increase in accuracy is.\n\nIn 2017 Leslie Smith and Nicholay Topin published a paper titled “Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates”. This paper describes how to use a Learning Rate range test to check if super converge is possible for a model and a method to select the initial and maximum learning rates. Again sadly this went widely unnoticed by the majority of deep learning practitioners.\n\nIn the paper it suggests (CLR being Cyclical Learning Rate):\n\nWithin my own experiments and research I have found reducing the learning rate by an order of magnitude, so 10 times lower works more effectively.\n\nThere are many ways to implement a learning rate finder, often graphs from Tensorboard are examined to find the optimal maximum learning rate. Usually the learning rate is slowly increased with each batch size of training until the loss stops improving and the model diverges. Then the maximum learning rate is chosen as a value slightly less than this.\n\nIn the paper it suggests using one final cycle with a learning rate several orders of magnitude lower, so perhaps 100 or 1000 times lower.\n\nThis takes the previous example of 12 training epochs up to 16 epochs, which is still a spectacularly low number of training epochs.\n\nModels trained with Super-convergence may not be better than traditional piecewise training carried out for thousands of times of more epochs. From my experiments and research I’ve found the models are at best approaching the models trained with piecewise training.\n\nHowever they are likely to be close enough in many circumstances. Getting close to 99% as accurate in such a short training period is likely to be enough and might actually result is less over fitting with models that can generalise better.\n\nHere is my example implementation demonstrating Super-Convergence using Cyclical Learning Rate schedules:\n\nThis is an improved version of the IDN deep neural network from the Fast and Accurate Single Image Super-Resolution via Information Distillation Network paper by Zheng Hui, Xiumei Wang and Xinbo Gao.\n\nI am using the SSIM to gauge quality as I don’t believe a mean average of PSNR across images is accurate as it is a logarithmic based metric. Comparing PSNR across multiple images may not be that meaningful.\n\nThis modified/improved version of IDN is the result of research I am carrying out which will be detailed in another future article.\n\nYou can connect with me on LinkedIn or follow me on Twitter."
    },
    {
        "link": "https://machinelearningmastery.com/using-learning-rate-schedule-in-pytorch-training",
        "document": "The classical algorithm to train neural networks is called stochastic gradient descent. It has been well established that you can achieve increased performance and faster training on some problems by using a learning rate that changes during training.\n\nIn this post, you will discover what is learning rate schedule and how you can use different learning rate schedules for your neural network models in PyTorch.\n\nAfter reading this post, you will know:\n• The role of learning rate schedule in model training\n• How to use learning rate schedule in PyTorch training loop\n• How to set up your own learning rate schedule\n\nThis post is divided into three parts; they are\n\nGradient descent is an algorithm of numerical optimization. What it does is to update parameters using the formula:\n\nIn this formula, $w$ is the parameter, e.g., the weight in a neural network, and $y$ is the objective, e.g., the loss function. What it does is to move $w$ to the direction that you can minimize $y$. The direction is provided by the differentiation, $\\dfrac{dy}{dw}$, but how much you should move $w$ is controlled by the learning rate $\\alpha$.\n\nAn easy start is to use a constant learning rate in gradient descent algorithm. But you can do better with a learning rate schedule. A schedule is to make learning rate adaptive to the gradient descent optimization procedure, so you can increase performance and reduce training time.\n\nIn the neural network training process, data is feed into the network in batches, with many batches in one epoch. Each batch triggers one training step, which the gradient descent algorithm updates the parameters once. However, usually the learning rate schedule is updated once for each training epoch only.\n\nYou can update the learning rate as frequent as each step but usually it is updated once per epoch because you want to know how the network performs in order to determine how the learning rate should update. Regularly, a model is evaluated with validation dataset once per epoch.\n\nThere are multiple ways of making learning rate adaptive. At the beginning of training, you may prefer a larger learning rate so you improve the network coarsely to speed up the progress. In a very complex neural network model, you may also prefer to gradually increasse the learning rate at the beginning because you need the network to explore on the different dimensions of prediction. At the end of training, however, you always want to have the learning rate smaller. Since at that time, you are about to get the best performance from the model and it is easy to overshoot if the learning rate is large.\n\nTherefore, the simplest and perhaps most used adaptation of the learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used and decreasing the learning rate so that a smaller rate and, therefore, smaller training updates are made to weights later in the training procedure.\n\nThis has the effect of quickly learning good weights early and fine-tuning them later.\n\nNext, let’s look at how you can set up learning rate schedules in PyTorch.\n\n\n\nKick-start your project with my book Deep Learning with PyTorch. It provides self-study tutorials with working code.\n\nIn PyTorch, a model is updated by an optimizer and learning rate is a parameter of the optimizer. Learning rate schedule is an algorithm to update the learning rate in an optimizer.\n\nBelow is an example of creating a learning rate schedule:\n\nThere are many learning rate scheduler provided by PyTorch in submodule. All the scheduler needs the optimizer to update as first argument. Depends on the scheduler, you may need to provide more arguments to set up one.\n\nLet’s start with an example model. In below, a model is to solve the ionosphere binary classification problem. This is a small dataset that you can download from the UCI Machine Learning repository. Place the data file in your working directory with the filename .\n\nThe ionosphere dataset is good for practicing with neural networks because all the input values are small numerical values of the same scale.\n\nA small neural network model is constructed with a single hidden layer with 34 neurons, using the ReLU activation function. The output layer has a single neuron and uses the sigmoid activation function in order to output probability-like values.\n\nPlain stochastic gradient descent algorithm is used, with a fixed learning rate 0.1. The model is trained for 50 epochs. The state parameters of an optimizer can be found in ; which the learning rate is a floating point value at . At the end of each epoch, the learning rate from the optimizer is printed.\n\nThe complete example is listed below.\n\nYou can confirm that the learning rate didn’t change over the entire training process. Let’s make the training process start with a larger learning rate and end with a smaller rate. To introduce a learning rate scheduler, you need to run its function in the training loop. The code above is modified into the following:\n\nIn the above, is used. It is a linear rate scheduler and it takes three additional parameters, the , , and . You set to 1.0, to 0.5, and to 30, therefore it will make a multiplicative factor decrease from 1.0 to 0.5, in 10 equal steps. After 10 steps, the factor will stay at 0.5. This factor is then multiplied to the original learning rate at the optimizer. Hence you will see the learning rate decreased from $0.1\\times 1.0 = 0.1$ to $0.1\\times 0.5 = 0.05$.\n\nBesides , you can also use , its syntax is:\n\nIf you replaced with this, you will see the learning rate updated as follows:\n\nIn which the learning rate is updated by multiplying with a constant factor in each scheduler update.\n\nThere is no general rule that a particular learning rate schedule works the best. Sometimes, you like to have a special learning rate schedule that PyTorch didn’t provide. A custom learning rate schedule can be defined using a custom function. For example, you want to have a learning rate that:\n\non epoch $n$, which $lr_0$ is the initial learning rate, at epoch 0, and $\\alpha$ is a constant. You can implement a function that given the epoch $n$ calculate learning rate $lr_n$:\n\nThen, you can set up a to update the learning rate according to this function:\n\nModifying the previous example to use , you have the following:\n\nNote that although the function provided to assumes an argument , it is not tied to the epoch in the training loop but simply counts how many times you invoked .\n\nThis section lists some tips and tricks to consider when using learning rate schedules with neural networks.\n• Increase the initial learning rate. Because the learning rate will very likely decrease, start with a larger value to decrease from. A larger learning rate will result in a lot larger changes to the weights, at least in the beginning, allowing you to benefit from the fine-tuning later.\n• Use a large momentum. Many optimizers can consider momentum. Using a larger momentum value will help the optimization algorithm continue to make updates in the right direction when your learning rate shrinks to small values.\n• Experiment with different schedules. It will not be clear which learning rate schedule to use, so try a few with different configuration options and see what works best on your problem. Also, try schedules that change exponentially and even schedules that respond to the accuracy of your model on the training or test datasets.\n\nBelow is the documentation for more details on using learning rates in PyTorch:\n• How to adjust learning rate, from PyTorch documentation\n\nIn this post, you discovered learning rate schedules for training neural network models.\n\nAfter reading this post, you learned:\n• How to set up learning rate schedule in PyTorch"
    },
    {
        "link": "https://medium.com/data-scientists-diary/guide-to-pytorch-learning-rate-scheduling-b5d2a42f56d4",
        "document": "I understand that learning data science can be really challenging… …especially when you are just starting out. But it doesn’t have to be this way. That’s why I spent weeks creating a 46-week Data Science Roadmap with projects and study resources for getting your first data science job. If that’s not enough, I’ve also added: 4. A Discord community to help our data scientist buddies get access to study resources, projects, and job referrals. Click here to access everything! Now, let’s get back to the blog:\n\n“Training a neural network is like steering a ship; too fast, and you might miss the mark; too slow, and you’ll drift away. Learning rate scheduling is your way of keeping the ship steady as you navigate complex waters.” You probably already know that choosing the right learning rate is critical — too high, and your model diverges; too low, and training becomes painfully slow. That’s where learning rate scheduling becomes a game-changer, particularly in advanced training setups. By dynamically adjusting the learning rate, we can better handle the plateaus and sudden changes that training a complex model often demands. Here’s the deal: PyTorch offers several key learning rate schedulers, each designed for a specific purpose. In this guide, we’ll skip the basics and dive straight into the powerful schedulers that experienced data scientists like yourself will actually use in practice. Let’s focus on three essentials:\n• StepLR: For steady decay at regular intervals, ideal for large training runs where gradual learning rate reduction is critical.\n• ReduceLROnPlateau: Perfect when you need dynamic adjustment based on validation performance, such as in cases where the model hits a performance plateau. With each scheduler, I’ll provide code you can use immediately, along with practical insights to help you decide when each one makes sense in a real-world training scenario.\n\nEssential PyTorch Learning Rate Schedulers and When to Use Each “Let’s not just set a learning rate. Let’s make it adapt, evolve, and work for us as we train.” When you’re dealing with large datasets and high epoch counts, keeping a steady pace by reducing the learning rate at regular intervals can stabilize training and avoid overshooting the target loss. Here’s how you set up in PyTorch: import torch\n\nimport torch.optim as optim\n\n\n\n# Suppose your model and optimizer are already defined\n\nmodel = ...\n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\n\n\n# StepLR: Reduces learning rate every 10 epochs by a factor of 0.1\n\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n\n\n# Example training loop with StepLR integration\n\nfor epoch in range(50): # Let's assume 50 epochs\n\n # Training code here\n\n ...\n\n\n\n # Step the scheduler at the end of each epoch\n\n scheduler.step()\n\n print(f\"Epoch {epoch+1}, Learning Rate: {scheduler.get_last_lr()[0]}\") This scheduler keeps things simple. Every epochs, the learning rate drops by . In practice, is incredibly effective for models that benefit from gradual but consistent adjustments. Tip: Keep an eye on the ; too frequent, and you risk slowing training unnecessarily; too infrequent, and you may miss the stability benefits. This might surprise you: sometimes, a gentle decay just isn’t enough, and you need a more aggressive approach. applies an exponential decay, continuously lowering the learning rate, making it useful for models that require rapid convergence but can handle more aggressive adjustments. # Define the same model and optimizer as before\n\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n\n\n# ExponentialLR with a decay rate of 0.95\n\nscheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n\n\n\n# Training loop example\n\nfor epoch in range(50):\n\n # Training code here\n\n ...\n\n\n\n # Step the scheduler\n\n scheduler.step()\n\n print(f\"Epoch {epoch+1}, Learning Rate: {scheduler.get_last_lr()[0]}\") When should you use ? Think of it for models where early rapid learning is beneficial, but you still want to taper off the learning rate as the model converges. Caution: Be mindful of the decay rate—setting too low can lead to premature convergence and stunted learning. A between 0.9 and 0.99 often strikes the right balance, but experimenting within that range will help fine-tune results. You might be wondering why we’d need a scheduler that responds to the model’s performance. Here’s why: in cases where the model’s training or validation loss hits a plateau, a well-timed reduction in the learning rate can push the model past these sticking points. monitors a metric, such as validation loss, and only reduces the learning rate when the improvement stalls. # Assuming model and optimizer are defined\n\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n\n\n# ReduceLROnPlateau scheduler with validation loss monitoring\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, cooldown=2, threshold=0.01)\n\n\n\n# Example training loop\n\nfor epoch in range(50):\n\n # Training code here\n\n ...\n\n val_loss = compute_validation_loss() # Define this based on your validation metric\n\n\n\n # Step the scheduler based on validation loss\n\n scheduler.step(val_loss)\n\n print(f\"Epoch {epoch+1}, Learning Rate: {optimizer.param_groups[0]['lr']}\") ReduceLROnPlateau can be incredibly valuable for models that show early promise but tend to plateau before reaching optimal performance. Tuning the , , and parameters can make or break this approach:\n• Patience: Number of epochs to wait before reducing the rate.\n• Factor: Multiplicative factor by which the learning rate decreases.\n• Cooldown: Ensures that once the rate is reduced, it won’t reduce again immediately. By adjusting these parameters, you can tailor to be responsive yet controlled, preventing the model from stagnating without over-reacting to minor fluctuations. With these schedulers and code examples, you’ll have complete control over your learning rate, allowing you to navigate different stages of training and adapt as needed. Each approach offers a unique benefit, so as you choose a scheduler, consider both the architecture of your model and the characteristics of your data. Let’s keep building on this momentum as we dive into custom schedulers next, where I’ll show you how to tailor learning rate adjustments precisely to fit your unique needs.\n\n“Sometimes, the best learning rate schedule is the one you create yourself.” Here’s the deal: PyTorch’s built-in schedulers are fantastic, but if you’re looking to optimize a model with unique patterns or specific performance metrics, a custom scheduler could be the answer. Creating your own learning rate scheduler lets you adjust precisely based on how your model is performing at different stages. Suppose you want a learning rate that decreases at specific intervals but also ramps up when your model shows a sudden improvement in validation performance. Here’s a simple example of how to create such a custom scheduler in PyTorch: import torch\n\nfrom torch.optim import Optimizer\n\n\n\nclass CustomLRScheduler(torch.optim.lr_scheduler._LRScheduler):\n\n def __init__(self, optimizer, base_lr=0.1, ramp_up_epochs=5, decay_factor=0.5, last_epoch=-1):\n\n self.base_lr = base_lr\n\n self.ramp_up_epochs = ramp_up_epochs\n\n self.decay_factor = decay_factor\n\n super(CustomLRScheduler, self).__init__(optimizer, last_epoch)\n\n\n\n def get_lr(self):\n\n # Custom rule for updating learning rate\n\n if self.last_epoch < self.ramp_up_epochs:\n\n # Linearly increase learning rate for the initial ramp_up_epochs\n\n return [self.base_lr * (self.last_epoch + 1) / self.ramp_up_epochs for _ in self.base_lrs]\n\n else:\n\n # Decay the learning rate by decay_factor for each subsequent epoch\n\n return [base_lr * (self.decay_factor ** (self.last_epoch - self.ramp_up_epochs)) for base_lr in self.base_lrs]\n• Ramp Up Phase: For the first few epochs, the learning rate linearly increases to allow the model to quickly learn key features.\n• Decay Phase: After the ramp-up, it gradually decreases to focus on refinement. Using the Custom Scheduler in Your Training Loop # Model and optimizer setup\n\nmodel = ...\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\nscheduler = CustomLRScheduler(optimizer, base_lr=0.1, ramp_up_epochs=5, decay_factor=0.5)\n\n\n\nfor epoch in range(50):\n\n # Training code here\n\n ...\n\n \n\n # Step the scheduler\n\n scheduler.step()\n\n print(f\"Epoch {epoch+1}, Learning Rate: {scheduler.get_last_lr()[0]}\") Custom schedulers open up a world of flexibility. The real benefit here is that you’re not constrained by predefined decay rules; instead, you can respond to model performance dynamically. If you’re in a situation where the model’s accuracy needs a gentle push or a sudden boost, creating a scheduler tailored to these shifts is a huge advantage.\n• : Controls how long the scheduler takes to reach full learning rate.\n• : Adjusts how aggressively the learning rate drops once the ramp-up phase is over. Building a custom scheduler takes a bit of experimentation, but once fine-tuned, it can lead to noticeably smoother convergence.\n\n“To step or not to step — that’s the question when integrating schedulers.” You might be wondering: where exactly should you call ? This simple question can trip up even seasoned data scientists because the answer varies depending on your scheduler’s purpose and whether you’re stepping by epoch or batch. Let’s start with a scheduler that steps at the end of each epoch, such as or . Here’s a sample training loop: for epoch in range(50): # Total epochs\n\n for batch in train_loader:\n\n # Forward pass, loss calculation, and backpropagation\n\n optimizer.zero_grad()\n\n outputs = model(batch)\n\n loss = loss_fn(outputs, batch_labels)\n\n loss.backward()\n\n optimizer.step()\n\n \n\n # Step the scheduler at the end of each epoch\n\n scheduler.step()\n\n print(f\"Epoch {epoch+1}, Learning Rate: {scheduler.get_last_lr()[0]}\") Calling at the end of each epoch means your learning rate will only update once per epoch, which is typical for schedules that rely on longer-term adjustments. This might surprise you, but some models — especially those with a high batch size — benefit from adjusting the learning rate after each batch rather than each epoch. For instance, often requires a per-batch step for optimal performance. for epoch in range(50): # Total epochs\n\n for batch in train_loader:\n\n # Forward pass, loss calculation, and backpropagation\n\n optimizer.zero_grad()\n\n outputs = model(batch)\n\n loss = loss_fn(outputs, batch_labels)\n\n loss.backward()\n\n optimizer.step()\n\n\n\n # Step the scheduler after each batch\n\n scheduler.step()\n\n print(f\"Batch {batch_num}, Learning Rate: {scheduler.get_last_lr()[0]}\")\n• Placing in the wrong spot can lead to unexpected learning rate values. If your scheduler is configured to step per epoch but you place inside the batch loop, the learning rate will change far more often than intended.\n• Schedulers like require validation loss as input. Ensure you run after calculating validation metrics; otherwise, it won’t know when to adjust the learning rate. Example for in Validation-Dependent Scheduling: for epoch in range(50): # Total epochs\n\n for batch in train_loader:\n\n # Training loop code here\n\n ...\n\n \n\n # Validation phase\n\n val_loss = validate_model(model, val_loader) # Define validation logic\n\n\n\n # Scheduler step with validation loss\n\n scheduler.step(val_loss)\n\n print(f\"Epoch {epoch+1}, Validation Loss: {val_loss}, Learning Rate: {optimizer.param_groups[0]['lr']}\") 3. Ignoring the Order of Scheduler and Optimizer Steps\n• In batch-level scheduling, always call before . Reversing this order can result in a misaligned learning rate adjustment. By keeping these details in mind, you’ll ensure the learning rate scheduler behaves exactly as intended, seamlessly integrated into your training loop for optimal control.\n\n“Sometimes, one scheduler isn’t enough. Just like seasons in a year, training often goes through phases that demand unique learning rate adjustments.” Combining schedulers can unlock a powerful multi-phase training approach, where you adjust the learning rate differently during each stage. Imagine a scenario where you initially want a steady, controlled decay for early training, but as your model approaches convergence, you need an adaptive adjustment based on validation performance. Here, combining with can give you that fine-grained control. In this example, we’ll use during the initial training phase to decay the learning rate every few epochs, and then switch to for the fine-tuning phase, responding to validation loss stagnation. import torch\n\nimport torch.optim as optim\n\n\n\n# Model and optimizer setup\n\nmodel = ...\n\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n\n\n# StepLR: Decay every 10 epochs by 0.5\n\nstep_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n\n# ReduceLROnPlateau: Reduce when validation loss plateaus\n\nplateau_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3, cooldown=1)\n\n\n\nfor epoch in range(50):\n\n # Training phase\n\n model.train()\n\n for batch in train_loader:\n\n optimizer.zero_grad()\n\n outputs = model(batch)\n\n loss = loss_fn(outputs, batch_labels)\n\n loss.backward()\n\n optimizer.step()\n\n\n\n # Step the StepLR scheduler every epoch\n\n step_scheduler.step()\n\n\n\n # Validation phase\n\n model.eval()\n\n val_loss = compute_validation_loss(model, val_loader) # Define validation loss calculation\n\n \n\n # ReduceLROnPlateau scheduler based on validation loss\n\n plateau_scheduler.step(val_loss)\n\n\n\n print(f\"Epoch {epoch+1}, Learning Rate: {optimizer.param_groups[0]['lr']}\") When and Why to Combine Schedulers\n• Control vs. Adaptability: gives you control over decay timing, ideal for models that need gradual reduction over fixed intervals. Once fine-tuning begins, adapts the learning rate more dynamically.\n• Performance: This combination can reduce the time to convergence since the adaptive adjustment prevents overfitting during late-stage training.\n• Scheduler Order: Here, we call first to decay the learning rate. When is active, it can override StepLR if validation metrics require adjustment. Combining schedulers offers you the flexibility to create unique learning rate schedules that evolve with your model’s training phases. Keep experimenting with different combinations to see how your model responds!\n\nExperimenting with OneCycleLR for Optimal Training in One Epoch “You might be wondering: can a single training cycle really make that much of a difference? With OneCycleLR, the answer is a confident yes.” OneCycleLR is particularly powerful for short, intense training cycles where the model benefits from a sharp increase in the learning rate at the start, followed by a symmetric decline. This technique, pioneered by Leslie Smith, takes advantage of a unique scheduling curve to speed up convergence while reducing overfitting. OneCycleLR is designed for scenarios where you want the model to reach optimal performance within a single training cycle (like in transfer learning). By setting the learning rate to peak midway through the training, it forces the model to adapt quickly and then fine-tune as the rate declines. Here’s a typical OneCycleLR setup for a transfer learning scenario, where the model’s parameters need quick adaptation before stabilizing. # Assuming model and optimizer are defined\n\noptimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n\nmax_lr = 0.01 # Maximum learning rate\n\nsteps_per_epoch = len(train_loader) # Number of batches per epoch\n\nepochs = 10\n\n\n\n# OneCycleLR setup\n\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, steps_per_epoch=steps_per_epoch, epochs=epochs, anneal_strategy='linear')\n\n\n\nfor epoch in range(epochs):\n\n for batch in train_loader:\n\n optimizer.zero_grad()\n\n outputs = model(batch)\n\n loss = loss_fn(outputs, batch_labels)\n\n loss.backward()\n\n optimizer.step()\n\n \n\n # Step the scheduler every batch\n\n scheduler.step()\n\n print(f\"Learning Rate: {scheduler.get_last_lr()[0]}\")\n• Max Learning Rate ( ): This controls the peak of the learning rate cycle. OneCycleLR will increase the learning rate to this value by the midpoint of training.\n• Steps Per Epoch and Epochs: Essential for OneCycleLR to compute the total number of iterations for the cycle.\n• Anneal Strategy: Defines the curve of learning rate reduction after reaching the peak. Using a or strategy affects the slope of decline, with providing a smoother taper.\n• If you’re unsure, a good rule of thumb is to set to about 10 times your base learning rate. Too high, and your model might overshoot during training; too low, and you won’t see the full benefits of OneCycleLR’s aggressive adaptation.\n• Although PyTorch can compute this for you using and , defining it explicitly can sometimes offer finer control, especially when experimenting with different epoch lengths.\n• A strategy gives a more natural, gradual decline that can work better for models requiring fine adjustments, while is more straightforward and useful for shorter training cycles."
    },
    {
        "link": "https://kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/69576720/implementing-custom-learning-rate-scheduler-in-pytorch",
        "document": "Since this is a scheduler used in a popular paper (Attention is all you need), reasonably good implementations already exist online.\n\nYou can grab a PyTorch implementation from this repository by @jadore801120.\n\nOnce you have it, then simply\n\nalso make sure to invoke the scheduler at the right time"
    },
    {
        "link": "https://github.com/pytorch/pytorch/blob/main/torch/optim/lr_scheduler.py",
        "document": ""
    },
    {
        "link": "https://neptune.ai/blog/how-to-choose-a-learning-rate-scheduler",
        "document": "Researchers generally agree that neural network models are difficult to train. One of the biggest issues is the large number of hyperparameters to specify and optimize. The list goes on, including the number of hidden layers, activation functions, optimizers, learning rate, and regularization.\n\nTuning these hyperparameters can significantly improve neural network models. For us, as data scientists, building neural network models is about solving an optimization problem. We want to find the minima (global or sometimes local) of the objective function by gradient-based methods, such as gradient descent.\n\nOf all the gradient descent hyperparameters, the learning rate is one of the most critical ones for good model performance. In this article, we will explore this parameter and explain why scheduling our learning rate during model training is crucial.\n\nMoving from there, we’ll see how to schedule learning rates by implementing and using various schedulers in Keras. We will then create experiments in Neptune to compare how these schedulers perform.\n\nWhat is the learning rate in neural networks?\n\nWhat is the learning rate, and what does it do to a neural network? The learning rate (or step size) is explained as the magnitude of change/update to model weights during the backpropagation training process. As a configurable hyperparameter, the learning rate is usually specified as a positive value less than 1.0.\n\nIn back-propagation, model weights are updated to reduce the error estimates of our loss function. Rather than changing the weights using the full amount, we multiply it by some learning rate value. For example, setting the learning rate to 0.5 would mean updating (usually subtracting) the weights with 0.5*estimated weight errors (i.e., gradients or total error change w.r.t. the weights).\n\nThe learning rate controls how big of a step it takes for an optimizer to reach a minimum of the loss function. What does this do to our optimization algorithm? Look at these graphs:\n• With a large learning rate (on the right), the algorithm learns fast, but it may also cause the algorithm to oscillate around or even jump over the minima. Even worse, a high learning rate equals large weight updates, which might cause the weights to overflow.\n• On the contrary, with a small learning rate (on the left), updates to the weights are small, which will guide the optimizer gradually towards the minima. However, the optimizer may take too long to converge or get stuck in a plateau or undesirable local minima;\n• A good learning rate is a tradeoff between the coverage rate and overshooting (in the middle). It’s not too small so that our algorithm can converge swiftly, and it’s not too large so that our algorithm won’t jump back and forth without reaching a minimum.\n\nAlthough the theoretical principle of finding an appropriate learning rate is straightforward (not too large, not too small), it’s easier said than done! Learning rate scheduling can help solve this problem.\n\nA learning rate schedule is a predefined framework that adjusts the learning rate between epochs or iterations as the training progresses. Two of the most common techniques for learning rate scheduling are:\n• Constant learning rate: as the name suggests, we initialize a learning rate and don’t change it during training;\n• Learning rate decay: we select an initial learning rate, then gradually reduce it in accordance with a scheduler.\n\nKnowing what learning rate schedules are, you must be wondering why we need to decrease the learning rate in the first place. Well, in a neural network, our model weights are updated as:\n\nwhere eta is the learning rate, and partial derivative is the gradient.\n\nThis is good for the training process. Early in the training, the learning rate is set to be large in order to reach a set of weights that are good enough. Over time, these weights are fine-tuned to reach higher accuracy by leveraging a small learning rate.\n\nFor the demonstration purpose, we will be working with the popular Fashion-MINIST data that comes with Keras. This dataset consists of 70,000 images (the training set and testing set are 60,000 and 10,000, respectively). These images are 28×28 pixels and are grouped into ten classes.\n\nTo compare our model performance with different learning rate schedulers, we’ll track our experiments in Neptune. Neptune monitors everything model-related. Refer to the Quickstart documentation page for detailed step-by-step instructions on how to get your Neptune projects set up and configured with Python.\n\nSpecifically, create a project named “learning-rate-scheduling” under your own workspace and save your credentials based on those instructions. Now, open a new Python script or a Jupyter Notebook (I prefer a notebook) and define a function to create Neptune Run objects using your credentials:\n\nThen, run the below snippet to import other necessary packages:\n\nNext, we’ll load the dataset with some utility functions available in Keras.\n\nTo reduce the runtime, our model will be trained against 20,000 images rather than the entire 60,000. Thus, we will randomly select 20,000 data records using the code below.\n\nOn top of that, we will also define several helper functions to save and plot the learning rate as training goes:\n• The current dataset is normalized by dividing it by 255; Thus, it’s rescaled to a range of 0-1;\n• We defined a function get_lr_metric() to save and print out the learning rate as a part of our Keras metrics. Don’t worry too much about its implementation; it simply captures the learning rate from our given optimizer at every iteration.\n\nIn addition, let’s also create a helper function to log learning rates and model performance charts to Neptune throughout our experiments:\n\nHaving the dataset and helper functions ready to go, we can now build a neural network model as an image classifier. For simplicity, our current model contains two hidden layers and an output layer with the ‘softmax’ activation function for multi-class classification:\n\nHere’s the model structure, which is a reasonably simple network.\n\nAs aforementioned, the constant schedule is the simplest scheme among all learning rate schedulers. To set a performance baseline, we will train the model using a learning rate of 0.01 consistently through all epochs:\n• created a Neptune experiment under our credentials to track the base model performance;\n• specified the learning rate using the arg. in the standard SGD optimizer in Keras;\n• added the lr_metric as a user-defined metric to monitor, which enables learning rate information to be shown in the training verbatim;\n• logged the learning rate and performance charts (loss and accuracy curves) in Neptune.\n\nLooking at the training progress, we can confirm that the current learning rate is fixed at 0.01 without changing:\n\nIn our Neptune experiment, we’ll find the following performance charts:\n\nAs learning unfolds, training loss decreases and accuracy increases (the downward trend in accuracy suggests that we need to do more epochs); nonetheless, when it comes to the validation set, model performance doesn’t change too much (just over 80%). This will be our baseline model for benchmarking with the decay schedulers later.\n\nKeras offers a built-in standard decay policy, and it can be enabled using the ExponentialDecay scheduler. First, the scheduler must be defined with logic that specifies how often the decay must happen. A good rule is to decay at every epoch, as written in the decay_rate parameter. Also, how much to decay is specified in the decay_rate parameter. The lower this parameter is, the faster the learning rate declines.\n\nOnce you define the scheduler, you can pass it to the SGD optimizer’s learning_rate parameter:\n\nIn the rest of this article, we’ll create our own schedulers using the Callback() functionality in Keras to gain a deeper understanding of how different schedulers work.\n\nThe underlying mechanism of learning rate decay is to reduce the learning rate as epochs increase. So, we basically want to specify our learning rate to be some decreasing function of epochs.\n\nAmong all potential candidates, a linear function is the most straightforward one, so the learning rate linearly decreases with epochs. Due to its simplicity, linear decay is usually considered the first attempt to experiment with.\n\nWith this scheme, the learning rate will decay to zero by the end of the training epochs. To implement linear decay:\n\nHere, we defined a class LRPolynomialDecay, where the power argument controls how fast the decay would be; that is, a smaller power makes the learning rate decay more slowly, yet a larger power makes the decay more quickly.\n\nSetting the power equal to 1 gives us linear decay, the plot of which is shown below.\n\nTo train our model with this custom linear decay, we need to:\n• Pass the result to the argument of the method of our model.\n\nRunning this model, we can see the following performance chart in our Neptune project:\n\nFrom the loss and accuracy curves on the validation set, we observe:\n• Both metrics fluctuate during the entire training process;\n• After about 30 epochs, model overfitting occurs, where training loss continues to decrease while validation loss starts to increase (and accuracy is almost flat).\n\nThis pattern indicates that our model is diverging as training goes on, and it’s most likely because the learning rate is too high.\n\nShould we reduce the learning rate as a linear function of epochs? Maybe not. It works better to have a policy where the learning rate decays faster when training begins and then gradually flattens out to a small value towards the end of the training.\n\nThis is the basic concept of non-linear decay, among which the most commonly used ones are time-based and exponential decay.\n\nThe formula for time-based decay is defined as:\n\nwhere decay is a parameter that is normally calculated as:\n\nLet’s specify the following parameters:\n\nthen this chart shows the generated learning rate curve:\n\nAs compared to the linear function, time-based decay causes the learning rate to decrease faster upon training start and much slower later. As before, let’s pass this scheduler to the LearningRateScheduler callback and log the performance charts to Neptune.\n\nHere’s the performance of this model:\n\nAs we can see, this model fits better than the linear decay one against the validation set. A couple of observations,\n• Learning almost stops at around 30 epochs as our learning rate is reduced to values close to zero;\n• Similar to the linear scenario, there are some large fluctuations when the training starts.\n\nNow, is there a way to smooth out these fluctuations? Let’s turn to exponential decay, which is defined as an exponential function of the number of epochs:\n\nAgain, specifying initial_learning_rate = 0.5 and epochs = 100 will produce the following decay curve (vs. linear and time-based decays):\n\nThe exponential scheme offers an even smoother decay path at the beginning, which should lead to a smoother training curve. Let’s run this model to find out if this is the case:\n\nHere is our result:\n\nIt’s easier to see that the training curve from exponential decay is much smoother than that from time-based decay. Overall, the exponential decay outperforms slightly.\n\nSo far, we have only looked at the continuous decay policies; how about a discrete one? Next, we’ll move on to a popular discrete staircase decay, a.k.a., step-based decay.\n\nUnder this policy, our learning rate is scheduled to reduce a certain amount every N epochs:\n\nHere, the drop_rate specifies the amount that the learning rate is modified, and the epochs_drop specifies how frequent the modification is.\n\nSame as above, setting our initial_learning_rate = 0.5 and epochs = 100 generates this step-looking learning curve:\n\nHowever, we will keep the initial learning rate at 0.1 for performance reasons and set the epoch drop rate to 20. Passing this to our model:\n\nWe would have performance charts quite similar to the linear decay, where our model almost overfits.\n\nWith various decay schemes implemented, we can now bring things together to compare how the model performs.\n\nBased on our experiments, it appears that overall, the learning stops at approximately 60 epochs; thus, for easy visualization, I have rerun all our experiments with 60 epochs:\n\nTo compare all the metrics of these experiments, you can click on the eye icons of each and switch to the charts tab:\n\nPerformance charts above from the current exercise imply that the linear decay (labeled as polynomial decay) performs the best, followed by the time-based decay.\n\nBesides SGD with a learning rate scheduler, the second most influential optimization technique is adaptive optimizers, such as AdaGrad, RMSprop, or Adam. These optimizers approximate the gradient using model internal feedback; this means that they’re almost parameter-free and are incompatible with our learning rate schedulers as opposed to SGD.\n\nAmong all the adaptive optimizers, Adam has been a favorite of machine learning practitioners. Although details about this optimizer are beyond the scope of this article, it’s worth mentioning that Adam updates the learning rate separately for each model parameter/weight. This implies that with Adam, the learning rate may first increase at early layers and thus help improve the efficiency of deep neural networks.\n\nNow for good measure, let’s train our model with the Keras default Adam optimizer as the last experiment:\n\nNow, undoubtedly, this Adam learner makes our model diverge fairly quickly:\n\nAs you can see, the test loss reaches its low after only about 10 epochs, and the model starts overfitting. This suggests that we need to introduce some regularization into our model architecture.\n\nDespite being a highly effective learner, Adam isn’t always the optimal choice right off the bat without hyperparameter tuning. SGD, on the other hand, can perform significantly better with tuned learning rates or decay schedulers.\n\nWith all our experiments, we should get a better understanding as to how important learning rate schedules are; an excessively aggressive decay results in optimizers never reaching the minima, whereas a slow decay leads to chaotic updates without significant improvement.\n• To select a learning rate schedule, a common practice is to start with a value that’s not too small, e.g., 0.5, and then exponentially lower it to get the smaller values, such as 0.01, 0.001, 0.0001;\n• Although oftentimes being the default optimizer in deep learning applications, under the hood does not necessarily outperforms all the time; it can cause model divergence.\n• To build an effective model, we should also factor in other hyperparameters, such as momentum and regularization parameters (dropout, early stostopping, etc.).\n\nFinally, it’s worth mentioning that the current result is based on one neural network and dataset. When it comes to other models using other datasets, the optimal learning rate schedule may differ. Nevertheless, this article should provide you with a guide as to how to systematically choose a learning rate scheduler that best suits your specific model and dataset."
    },
    {
        "link": "https://cameronrwolfe.substack.com/p/the-best-learning-rate-schedules",
        "document": "This newsletter is supported by Alegion. As a research scientist at Alegion, I work on a range of problems from online learning to diffusion models. Feel free to check out our data annotation platform or contact me about potential collaboration/opportunities!\n\nWelcome to the Deep (Learning) Focus newsletter. Each issue picks a single topic in deep learning research and comprehensively overviews related research. Feel free to subscribe to the newsletter, share it, or follow me on twitter if you enjoy it!\n\nAnybody that has trained a neural network knows that properly setting the learning rate during training is a pivotal aspect of getting the neural network to perform well. Additionally, the learning rate is typically varied along the training trajectory according to some learning rate schedule. The choice of this schedule also has a large impact on the quality of training.\n\nMost practitioners adopt a few, widely-used strategies for the learning rate schedule during training; e.g., step decay or cosine annealing. Many of these schedules are curated for a particular benchmark, where they have been determined empirically to maximize test accuracy after years of research. But, these strategies often fail to generalize to other experimental settings, raising an important question: what are the most consistent and useful learning rate schedules for training neural networks?\n\nWithin this overview, we will look at recent research into various learning rate schedules that can be used to train neural networks. Such research has discovered numerous strategies for the learning rate that are both highly effective and easy to use; e.g., cyclical or triangular learning rate schedules. By studying these methods, we will arrive at several practical takeaways, providing simple tricks that can be immediately applied to improving neural network training.\n\nTo supplement this overview, I have implemented the main learning rate schedules that we will explore within a repository found here. These code examples are somewhat minimal, but they are sufficient to implement any of the learning rate schedules discussed in this overview without much effort.\n\nIn a supervised learning setting, the goal of neural network training is to produce a neural network that, given some data as input, can predict the ground truth label associated with that data. One example of this would be training a neural network to correctly predict whether an image contains a cat or a dog based upon a large dataset of labeled images of cats and dogs.\n\nThe basic components of neural network training, depicted above, are as follows:\n• None Neural Network: takes some data as input and transforms this data based on its internal parameters/weights to produce some output.\n• None Dataset: a large set of examples of input-output data pairs (e.g., images and their corresponding classifications).\n• None Optimizer: used to update the neural network’s internal parameters such that its predictions become more accurate.\n• None Hyperparameters: external parameters that are set by the deep learning practitioner to control relevant details of the training process.\n\nUsually, a neural network begins training with all of its parameters randomly initialized. To learn more meaningful parameters, the neural network is shown samples of data from the dataset. For each of these samples, the neural network attempts to predict the correct output, then the optimizer updates the neural network’s parameters to improve this prediction.\n\nThis process of updating the neural network’s parameters such that it can better match the known outputs within a dataset is referred to as training. The process repeats iteratively, typically until the neural network has looped over the entire dataset – referred to as an epoch of training – multiple times.\n\nAlthough this description of neural network training is not comprehensive, it should provide enough intuition to make it through this overview. Many extensive tutorials on neural network training exist online. My favorite tutorial by-far is from the “Practical Deep Learning for Coders” course by Jeremy Howard and fast.ai; see the link to the video below.\n\nModel parameters are updated by the optimizer during training. Hyperparameters, in contrast, are “extra” parameters that we, the deep learning practitioner, have control over. But, what can we actually control with hyperparameters? One common hyperparameter, which is relevant to this overview, is the learning rate.\n\nwhat is the learning rate? Put simply, each time the optimizer updates the neural network’s parameters, the learning rate controls the size of this update. Should we update the parameters a lot, a little bit, or somewhere in the middle? We make this choice by setting the learning rate.\n\nselecting a good learning rate. Setting the learning rate is one of the most important aspects of training a neural network. If we choose a value that is too large, training will diverge. On the other hand, a learning rate that is too small can yield poor performance and slow training. We must choose a learning rate that is large enough to provide regularization benefits to the training process and converge quickly, while not being too large such that the training process becomes unstable.\n\nHyperparameters like the learning rate are typically selected using a simple approach called grid search. The basic idea is to:\n• None Define a range of potential values for each hyperparameter\n• None Select a discrete set of values to test within this range\n• None Test all combinations of possible hyperparameter values\n• None Choose the best hyperparameter setting based on validation set performance\n\nGrid search is a simple, exhaustive search for the best hyperparameters. See the illustration below for an example of grid search over potential learning rate values.\n\nA similar approach can be applied to many hyperparameters at once by following a similar approach and testing all possible combinations of hyperparameter values.\n\nGrid search is computationally inefficient, as it requires the neural network to be retrained for each hyperparameter setting. To avoid this cost, many deep learning practitioners adopt a “guess and check” approach of trying several hyperparameters within a reasonable range and seeing what works. Alternative methodologies for selecting optimal hyperparameters have been proposed [5], but grid search or guess and check procedures are commonly used due to their simplicity.\n\nAfter selecting a learning rate, we typically should not maintain this same learning rate throughout the entire training process. Rather, conventional wisdom suggests that we should (i) select an initial learning rate, then (ii) decay this learning rate throughout the training process [1]. The function by which we perform this decay is referred to as the learning rate schedule.\n\nMany different learning rate schedules have been proposed over the years; e.g., step decay (i.e., decaying the learning rate by 10X a few times during training) or cosine annealing; see the figure below. In this overview, we will explore a number of recently proposed schedules that perform especially well.\n\nadaptive optimization techniques. Neural network training according to stochastic gradient descent (SGD) selects a single, global learning rate that is used for updating all model parameters. Beyond SGD, adaptive optimization techniques have been proposed (e.g., RMSProp or Adam [6]), which use training statistics to dynamically adjust the learning rate used for each of a model’s parameters. Most of the results outlined within this overview apply to both adaptive and SGD-style optimizers.\n\nIn this section, we will see several examples of recently proposed learning rate schedules. These include strategies like cyclical or triangular learning rates, as well as different profiles for learning rate decay. The optimal learning rate strategy is highly-dependent upon the domain and experimental settings, but we will see that several high-level takeaways can be drawn by studying the empirical results of many different learning rate strategies.\n\nAuthors in [1] propose a new method for handling the learning rate during neural network training: cyclically varying it between a minimum and a maximum value according to a smooth schedule. Prior to this work, most practitioners adopted the popular strategy of (i) setting the learning rate to an initially large value, then (ii) decaying the learning rate as training proceeds.\n\nIn [1], we throw away this rule-of-thumb in favor of a cyclical strategy. Cycling the learning rate in this way is somewhat counterintuitive — increasing the learning rate during training damages model performance, right? Despite temporarily degrading network performance as the learning rate increases, cyclical learning rate schedules actually provide a lot of benefits over the full course of training, as we will see in [1].\n\nCyclical learning rates introduce three new hyperparameters: stepsize, minimum learning rate, and maximum learning rate. The resulting schedule is “triangular”, meaning that the learning rate is increased/decreased in adjacent cycles; see above. The stepsize can be set somewhere between 2-10 training epochs, while the range for the learning rate is typically discovered via a learning rate range test (see Section 3.3 of [1]).\n\nIncreasing the learning rate temporarily degrades model performance. Once the learning rate has decayed again, however, the model’s performance will recover and improve. With this in mind, we see in the experimental results of [1] that models trained with cyclical learning rates follow a cyclical pattern in their performance. Model performance peaks at the end of each cycle (i.e., when the learning rate decays back to the minimum value) and becomes somewhat worse at intermediate stages of the cycle (i.e., when the learning rate is increased); see below.\n\nThe results in [1] reveal that cyclical learning rates benefit model performance over the course of training. Models trained via cyclical learning rates reach higher levels of performance faster than models trained with other learning rate strategies; see the figure below. In other words, the anytime performance of models trained with cyclical learning rates is really good!\n\nIn larger-scale experiments on ImageNet, cyclical learning rates still provide benefits, though they are a bit less pronounced.\n\nThe authors in [2] propose a simple restarting technique for the learning rate, called stochastic gradient descent with restarts (SGDR), in which the learning rate is periodically reset to its original value and scheduled to decrease. This technique employs the following steps:\n• None Decay the learning rate according to some fixed schedule\n• None Reset the learning rate to its original value after the end of the decay schedule\n• None Return to step #1 (i.e., decay the learning rate again)\n\nA depiction of different schedules that follow this strategy is provided below.\n\nWe can notice a few things about the schedules above. First, a cosine decay schedule is always used in [2] (the plot’s y-axis is in log scale). Additionally, the length of each decay schedule may increase as training progresses. Concretely, authors in [2] define the length of the first decay cycle as , then multiply this length by during each successive decay cycle; see below for a depiction.\n\nTo follow the terminology of [1], the stepsize of SGDR may increase after each cycle. Unlike [1], however, SGDR is not triangular (i.e., each cycle just decays the learning rate).\n\nIn experiments on CIFAR10/100, we can see that SGDR learning rate schedules yield good model performance more quickly than step decay schedules — SGDR has good anytime performance. The models obtained after each decay cycle perform well and continue to get better in successive decay cycles.\n\nGoing beyond these initial results, we can study model ensembles formed by taking “snapshots” at the end of each decay cycle. In particular, we can save a copy of the model’s state after each decay cycle within an SGDR schedule. Then, after training is complete, we can average the predictions of each of these models at inference time, forming an ensemble/group of models; see the link below for more details on the idea of ensembles.\n\nBy forming model ensembles in this way, we can achieve pretty significant reductions in test error on CIFAR10; see below.\n\nAdditionally, the snapshots from SGDR seem to provide a set of models with diverse predictions. Forming an ensemble in this way actually outperforms the normal approach of adding independent, fully-trained models into an ensemble.\n\nSuper-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates\n\nThe authors in [3] study an interesting approach for training neural networks that allows the speed of training to be increased by an order of magnitude. The basic approach — originally outlined in [8] — is to perform a single, triangular learning rate cycle with a large maximum learning rate, then allow the learning rate to decay below the minimum value of this cycle at the end of training; see below for an illustration.\n\nIn addition, the momentum is cycled in the opposite direction of the learning rate (typically in the range [0.85, 0.95]). This approach of jointly cycling the learning rate and momentum is referred to as “1cycle”. The authors in [3] show that it can be used to achieve “super-convergence” (i.e., extremely fast convergence to a high-performing solution).\n\nFor example, we see in experiments on CIFAR10 that 1cycle can achieve better performance than baseline learning rate strategies with 8X fewer training iterations. Using different 1cycle step sizes can yield even further speedups in training, though the accuracy level varies depending on the step size.\n\nWe can observe similar results on a few different architectures and datasets. See the table below, where 1cycle again yields good performance in a surprisingly small number of training epochs.\n\nCurrently, it is not clear whether super-convergence is achievable in a wide number of experimental settings, as experiments provided in [3] are somewhat limited in scale and variety. Nonetheless, we can probably all agree that the super-convergence phenomenon is quite interesting. In fact, the result was so interesting that it was even popularized and studied in depth by the fast.ai community.\n\nWithin [4], authors (including myself) consider the problem of properly scheduling the learning rate given different budget regimes (i.e., small, medium, or large number of training epochs). You might be thinking: why would we consider this setting? Well, oftentimes the optimal number of training epochs is not known ahead of time. Plus, we might be working with a fixed monetary budget that limits the number of training epochs we can perform.\n\nTo find the best budget-agnostic learning rate schedules, we must first define the space of possible learning rate schedules that will be considered. In [4], we do this by decomposing a learning rate schedule into two components:\n• None Profile: the function according to which the learning rate is varied throughout training.\n• None Sampling Rate: the frequency with which the learning rate is updated according to the chosen profile.\n\nSuch a decomposition can be used to describe nearly all fixed-structure learning rate schedules. Different profile and sampling rate combinations are depicted below. Higher sampling rates cause the schedule to match the underlying profile more closely.\n\nAuthors in [4] consider learning rate schedules formed with different sampling rates and three function profiles — exponential (i.e., produces step schedules), linear, and REX (i.e., a novel profile defined in [4]); see the figure above.\n\nFrom here, the authors train a Resnet20/38 on CIFAR10 with different sampling rate and profile combinations. In these experiments, we see that step decay schedules (i.e., exponential profile with a low sampling rate) only perform well given a low sampling rate and many training epochs. REX schedules with every iteration sampling perform well in all different epoch settings.\n\nPrior work indicated that a linear decay schedule is best for low-budget training settings (i.e., training with fewer epochs) [9]. In [4], we can see that REX is actually a better choice, as it avoids decaying the learning rate too early during training.\n\nFrom here, authors in [4] consider a variety of popular learning rate schedules, as shown in the figure below.\n\nThese schedules are tested across a variety of domains and training epoch budgets. When the performance is aggregated across all experiments, we get the results shown below.\n\nImmediately, we see that REX achieves shockingly consistent performance across different budget regimes and experimental domains. No other learning rate schedule achieves close to the same ratio of top-1/3 finishes across experiments, revealing that REX is a good domain/budget-agnostic learning rate schedule.\n\nBeyond the consistency of REX, these results teach us something more general: commonly-used learning rate strategies don’t generalize well across experimental settings. Each schedule (even REX, though to a lesser degree) performs best in only a small number of cases, revealing that selecting the proper learning rate strategy for any particular setting is incredibly important.\n\nProperly handling the learning rate is arguably the most important aspect of neural network training. Within this overview, we have learned about several practical learning rate schedules for training deep networks. Studying this line of work provides takeaways that are simple to understand, easy to implement, and highly effective. Some of these basic takeaways are outlined below.\n\nChoose a good learning rate. Properly setting the learning rate is one of the most important aspects of training a high-performing neural network. Choosing a poor initial learning rate or using the wrong learning rate schedule drastically deteriorates model performance.\n\nThe “default” schedule isn’t always best. Many experimental settings have a “default” learning rate schedule that we tend to adopt without much thought; e.g., step decay schedules for training CNNs for image classification. We should be aware that the performance of these schedules may deteriorate drastically as experimental settings change; e.g., for budgeted settings, REX-based schedules significantly outperform step decay. As practitioners, we should always be mindful of our chosen learning rate schedule to truly maximize our model’s performance.\n\nCyclical schedules are awesome. Cyclical or triangular learning rate schedules (e.g., as in [2] or [3]) are really useful because:\n• None They often match or exceed state-of-the-art performance\n\nUsing cyclical learning rate strategies, models reach their best performance at the end of each decay cycle. We can simply continue training for any given number of cycles until we are happy with the network’s performance. The optimal amount of training need not be known a priori, which is often useful in practice.\n\nThere’s a lot to explore out there. Although learning rate strategies have been widely studied, it seems like there is still more out there to be discovered. For example, we have seen that adopting alternative decay profiles benefits budgeted settings [4] and cyclical strategies may even be used to achieve super-convergence in some cases [3]. My question is: what more can be discovered? It seems like there are really interesting strategies (e.g., fractal learning rates [7]) that are yet to be explored.\n\nAs a supplement to this overview, I created a lightweight code repository for reproducing some of the different learning rate schedules, which includes:\n• None Functions for adjusting the learning rate/momentum in PyTorch optimizers\n• None Working examples for common learning rate schedules we have seen in this overview\n\nAlthough a bit minimal, this code provides everything that’s needed to implement and use any of the learning rate strategies we have studied so far. A link to the repository is provided below.\n\nIf you’re not interested in using this code, you can also use the learning rate schedulers directly implemented within PyTorch.\n\nNew to the newsletter?\n\nHello! I am Cameron R. Wolfe, a research scientist at Alegion and PhD student at Rice University studying the empirical and theoretical foundations of deep learning. This is the Deep (Learning) Focus newsletter, where I pick a single, bi-weekly topic in deep learning research, provide an understanding of relevant background information, then overview a handful of popular papers on the topic. If you like this newsletter, please subscribe, share it with your friends, or follow me on twitter!\n\n[1] Smith, Leslie N. \"Cyclical learning rates for training neural networks.\" 2017 IEEE winter conference on applications of computer vision (WACV). IEEE, 2017.\n\n[3] Smith, Leslie N., and Nicholay Topin. \"Super-convergence: Very fast training of neural networks using large learning rates.\" Artificial intelligence and machine learning for multi-domain operations applications. Vol. 11006. SPIE, 2019.\n\n[4] Chen, John, Cameron Wolfe, and Tasos Kyrillidis. \"REX: Revisiting Budgeted Training with an Improved Schedule.\" Proceedings of Machine Learning and Systems 4 (2022): 64-76.\n\n[5] Yu, Tong, and Hong Zhu. \"Hyper-parameter optimization: A review of algorithms and applications.\" arXiv preprint arXiv:2003.05689 (2020).\n\n[7] Agarwal, Naman, Surbhi Goel, and Cyril Zhang. \"Acceleration via fractal learning rate schedules.\" International Conference on Machine Learning. PMLR, 2021."
    },
    {
        "link": "https://medium.com/@zhonghong9998/adaptive-learning-rate-scheduling-optimizing-training-in-deep-networks-14d4f95a45d6",
        "document": "When it comes to training deep neural networks, one of the crucial factors that significantly influences model performance is the learning rate.\n\nThe learning rate determines the size of the steps taken during the optimization process and plays a pivotal role in determining how quickly or slowly a model converges to the optimal solution.\n\nIn recent years, adaptive learning rate scheduling techniques have gained prominence for their effectiveness in optimizing the training process and improving model performance.\n\nBefore delving into adaptive learning rate scheduling, let’s first understand why the learning rate is so important in training deep neural networks.\n\nIn essence, the learning rate controls the amount by which we update the parameters of the model during each iteration of the optimization algorithm, such as stochastic gradient descent (SGD) or its variants.\n\nA learning rate that is too high can lead to overshooting the optimal solution, causing oscillations or divergence in the optimization process.\n\nOn the other hand, a learning rate that is too low can result in slow convergence and potentially get stuck in local minima.\n\nThe Need for Adaptive Learning Rate Scheduling\n\nTraditional approaches to setting the learning rate often involve manually tuning hyperparameters based on heuristics or trial and error.\n\nHowever, in practice, finding the optimal learning rate can be challenging, especially when dealing with large and complex datasets or architectures.\n\nThis is where adaptive learning rate scheduling techniques come into play.\n\nInstead of relying on a fixed learning rate throughout the training process, these techniques dynamically adjust the learning rate based on the feedback received during training.\n\nBy adapting the learning rate according to the characteristics of the optimization landscape, adaptive scheduling methods aim to achieve faster convergence and improved generalization performance.\n\nThere are several popular adaptive learning rate scheduling techniques that have been widely adopted in the deep learning community. Let’s take a closer look at some of them:\n\nLearning rate schedulers dynamically adjust the learning rate during training according to a predefined schedule.\n\nOne common example is the step decay scheduler, which reduces the learning rate by a certain factor after a fixed number of epochs or iterations.\n\nAnother popular scheduler is the exponential decay scheduler, which exponentially decreases the learning rate over time.\n\nAdaptive optimization algorithms, such as AdaGrad, RMSprop, and Adam, adaptively adjust the learning rates for each parameter based on the historical gradients.\n\nThese algorithms are particularly effective in dealing with sparse gradients or non-stationary optimization landscapes.\n\nCyclical learning rate policies, introduced by Smith et al. in their paper “Cyclical Learning Rates for Training Neural Networks”, involve cyclically varying the learning rate between two extreme values.\n\nThis approach has been shown to improve both the convergence speed and the final performance of deep neural networks.\n\nThe learning rate range test, proposed by Leslie N. Smith in “Cyclical Learning Rates for Training Neural Networks”, involves gradually increasing the learning rate during the training process and monitoring the loss.\n\nThis allows practitioners to determine a suitable learning rate range for training the model effectively.\n\nWhile adaptive learning rate scheduling techniques offer significant benefits in terms of convergence speed and performance, it’s essential to keep a few practical considerations in mind:\n• Monitor Model Performance: Continuously monitor the performance of your model during training to ensure that the chosen learning rate scheduling technique is effectively improving convergence and generalization.\n• Experiment with Different Schedulers: Don’t hesitate to experiment with different learning rate schedulers and hyperparameters to find the combination that works best for your specific dataset and model architecture.\n• Regularization Techniques: Consider incorporating regularization techniques, such as dropout or weight decay, in conjunction with adaptive learning rate scheduling to prevent overfitting and improve model generalization.\n• Use Pretrained Models: When working with limited training data, consider leveraging pretrained models and fine-tuning them using adaptive learning rate scheduling techniques. This can significantly accelerate the training process and improve performance, especially in tasks such as transfer learning.\n\nNow that we’ve discussed the theory behind adaptive learning rate scheduling, let’s see how we can implement some of these techniques in Python using popular deep learning frameworks such as TensorFlow and PyTorch.\n\nHere’s an example of how you can implement a simple step decay learning rate scheduler using TensorFlow:\n\nIn this example, we define a step decay function that reduces the learning rate by a factor of 0.5 every 10 epochs. We then create a learning rate scheduler callback using this function and pass it to the method of our model during training.\n\nHere’s how you can implement a similar step decay learning rate scheduler using PyTorch:\n\nIn this PyTorch example, we define a step decay function similar to the TensorFlow example and create a scheduler with a step size of 10 epochs and a reduction factor of 0.5. We then update the learning rate at the end of each epoch during training.\n\nAdaptive learning rate scheduling is a powerful technique for optimizing the training process of deep neural networks.\n\nBy dynamically adjusting the learning rate based on the feedback received during training, these techniques can lead to faster convergence and better generalization, ultimately resulting in more accurate and robust models.\n\nIn this article, we’ve explored the importance of learning rate scheduling, discussed various adaptive techniques, and provided practical examples of how to implement them using popular deep learning frameworks such as TensorFlow and PyTorch.\n\nAdditionally, we’ve highlighted some best practices and considerations for effectively applying these techniques in real-world projects.\n\nBy mastering adaptive learning rate scheduling and experimenting with different techniques and configurations, you can significantly enhance the performance of your deep learning models and tackle a wide range of tasks and applications.\n\nFor further reading on this topic, I recommend checking out the following resources:\n• “On the Importance of the Learning Rate for Training Neural Networks”\n• “Understanding the Dynamics of Learning Rate on Deep Learning Neural Networks”"
    },
    {
        "link": "https://reddit.com/r/MachineLearning/comments/xeyzf7/d_how_does_one_choose_a_learning_rate_schedule",
        "document": "I'm currently using AdamW and find that an exponential decay schedule gives far better results than a fixed learning rate. I've used optuna to do bayesian optimization of my hyper-parameters including learning rate schedule and I just choose 300 epochs so trials would complete in a reasonable amount of time. However, I can train the winner above 1000 epochs and the validation loss continues to drop (around 1700 it starts to overfit). I imagine if I did another search over learning rate schedules using 2000 epochs that I'd get a different schedule and that would continue to do better if trained even longer as well.\n\nWith Optuna, I stopped using pruners (like asynchronous successive halving) because I don't think the validation loss early in the training process says much about the final performance, and instead would be biased towards schedules with fast decays.\n\nSo how do these projects that commit to a model and train it for days or weeks choose a schedule that isn't going to be 100x to cautious or end up overfitting 20% into their training budget?\n\nI'd imagine there's a method for dynamically adjusting the learning rate based on generalization error and the derivative of validation loss. I.E. if it starts overfitting then bump up the learning rate to get out of that local minimum and try to settle into a new one. But I haven't found any papers in that direction."
    },
    {
        "link": "https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras",
        "document": "The classical algorithm to train neural networks is called stochastic gradient descent. It has been well established that you can achieve increased performance and faster training on some problems by using a learning rate that changes during training.\n\nIn this post, you will discover how you can use different learning rate schedules for your neural network models in Python using the Keras deep learning library.\n\nAfter reading this post, you will know:\n• How to configure and evaluate a time-based learning rate schedule\n• How to configure and evaluate a drop-based learning rate schedule\n\nKick-start your project with my new book Deep Learning With Python, including step-by-step tutorials and the Python source code files for all examples.\n\nAdapting the learning rate for your stochastic gradient descent optimization procedure can increase performance and reduce training time.\n\nSometimes, this is called learning rate annealing or adaptive learning rates. Here, this approach is called a learning rate schedule, where the default schedule uses a constant learning rate to update network weights for each training epoch.\n\nThe simplest and perhaps most used adaptation of the learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used and decreasing the learning rate so that a smaller rate and, therefore, smaller training updates are made to weights later in the training procedure.\n\nThis has the effect of quickly learning good weights early and fine-tuning them later.\n\nTwo popular and easy-to-use learning rate schedules are as follows:\n• Decrease the learning rate gradually based on the epoch\n• Decrease the learning rate using punctuated large drops at specific epochs\n\nNext, let’s look at how you can use each of these learning rate schedules in turn with Keras.\n\nThe stochastic gradient descent optimization algorithm implementation in the SGD class has an argument called decay. This argument is used in the time-based learning rate decay schedule equation as follows:\n\nWhen the decay argument is zero (the default), this does not affect the learning rate.\n\nWhen the decay argument is specified, it will decrease the learning rate from the previous epoch by the given fixed amount.\n\nFor example, if you use the initial learning rate value of 0.1 and the decay of 0.001, the first five epochs will adapt the learning rate as follows:\n\nExtending this out to 100 epochs will produce the following graph of learning rate (y-axis) versus epoch (x-axis):\n\nYou can create a nice default schedule by setting the decay value as follows:\n\nThe example below demonstrates using the time-based learning rate adaptation schedule in Keras.\n\nIt is demonstrated in the Ionosphere binary classification problem. This is a small dataset that you can download from the UCI Machine Learning repository. Place the data file in your working directory with the filename ionosphere.csv.\n\nThe ionosphere dataset is good for practicing with neural networks because all the input values are small numerical values of the same scale.\n\nA small neural network model is constructed with a single hidden layer with 34 neurons, using the rectifier activation function. The output layer has a single neuron and uses the sigmoid activation function in order to output probability-like values.\n\nThe learning rate for stochastic gradient descent has been set to a higher value of 0.1. The model is trained for 50 epochs, and the decay argument has been set to 0.002, calculated as 0.1/50. Additionally, it can be a good idea to use momentum when using an adaptive learning rate. In this case, we use a momentum value of 0.8.\n\nThe complete example is listed below.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nThe model is trained on 67% of the dataset and evaluated using a 33% validation dataset.\n\nRunning the example shows a classification accuracy of 99.14%. This is higher than the baseline of 95.69% without the learning rate decay or momentum.\n\nAnother popular learning rate schedule used with deep learning models is systematically dropping the learning rate at specific times during training.\n\nOften this method is implemented by dropping the learning rate by half every fixed number of epochs. For example, we may have an initial learning rate of 0.1 and drop it by 0.5 every ten epochs. The first ten epochs of training would use a value of 0.1, and in the next ten epochs, a learning rate of 0.05 would be used, and so on.\n\nIf you plot the learning rates for this example out to 100 epochs, you get the graph below showing the learning rate (y-axis) versus epoch (x-axis).\n\nYou can implement this in Keras using the LearningRateScheduler callback when fitting the model.\n\nThe LearningRateScheduler callback allows you to define a function to call that takes the epoch number as an argument and returns the learning rate to use in stochastic gradient descent. When used, the learning rate specified by stochastic gradient descent is ignored.\n\nIn the code below, we use the same example as before of a single hidden layer network on the Ionosphere dataset. A new step_decay() function is defined that implements the equation:\n\nHere, the InitialLearningRate is the initial learning rate (such as 0.1), the DropRate is the amount that the learning rate is modified each time it is changed (such as 0.5), Epoch is the current epoch number, and EpochDrop is how often to change the learning rate (such as 10).\n\nNotice that the learning rate in the SGD class is set to 0 to clearly indicate that it is not used. Nevertheless, you can set a momentum term in SGD if you want to use momentum with this learning rate schedule.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example results in a classification accuracy of 99.14% on the validation dataset, again an improvement over the baseline for the model of the problem.\n\nThis section lists some tips and tricks to consider when using learning rate schedules with neural networks.\n• Increase the initial learning rate. Because the learning rate will very likely decrease, start with a larger value to decrease from. A larger learning rate will result in a lot larger changes to the weights, at least in the beginning, allowing you to benefit from the fine-tuning later.\n• Use a large momentum. Using a larger momentum value will help the optimization algorithm continue to make updates in the right direction when your learning rate shrinks to small values.\n• Experiment with different schedules. It will not be clear which learning rate schedule to use, so try a few with different configuration options and see what works best on your problem. Also, try schedules that change exponentially and even schedules that respond to the accuracy of your model on the training or test datasets.\n\nIn this post, you discovered learning rate schedules for training neural network models.\n\nAfter reading this post, you learned:\n• How to configure and use a time-based learning rate schedule in Keras\n• How to develop your own drop-based learning rate schedule in Keras\n\nDo you have any questions about learning rate schedules for neural networks or this post? Ask your question in the comments, and I will do my best to answer."
    }
]