[
    {
        "link": "https://multimedia.tencent.com/products/free-viewpoint-video",
        "document": "We would like to use performance and analytics cookies (“Cookies”) to help us recognize whether you are a returning visitor and to track the number of website views and visits. For more information about the Cookies we use and your options (including how to change your preferences) see our Cookies Policy"
    },
    {
        "link": "https://learn.microsoft.com/en-us/windows/mixed-reality/images/high-quality-streamable-free-viewpoint-video.pdf",
        "document": ""
    },
    {
        "link": "http://ip.hhi.de/imagecom_G1/assets/pdfs/icip04_free_viewpoint.pdf",
        "document": ""
    },
    {
        "link": "https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=0e0e654df8a34b7d90918450bf4325a67cc69b90",
        "document": ""
    },
    {
        "link": "https://ieeexplore.ieee.org/document/7051585",
        "document": ""
    },
    {
        "link": "https://link.springer.com/article/10.1007/s11042-025-20667-8",
        "document": "Free-viewpoint video (FVV) is an advanced type of video content that allows users to interactively navigate and view a scene from different viewpoints in real-time. FVV is often used for immersive applications, such as virtual reality, telepresence, and sports broadcasting. FVV content can be captured using different techniques, among which using an array of cameras and depth sensors is the typical, and then synthesized into novel viewpoints using depth image-based rendering (DIBR) techniques. However, FVV content has high spatiotemporal complexity, which poses significant challenges for storage, interactivity, transmission, and rendering. To address these challenges, several free-viewpoint video streaming methods have been proposed in recent years. These methods aim to efficiently compress and transmit FVV content over the network, while maintaining high visual quality and low latency.\n\nStandardization organizations, such as ISO-MPEG, and ITU-T VCEG, are also active in the investigation of various aspects of free viewpoint visual media technology (FTV) standardization [10]. However, their activity focuses mainly on compression technologies, view synthesis issues, and video quality evaluation, while FVV streaming is not in the main scope. In 2017, FTV activity moved to MPEG Immersive Video (MIV), as part 12 of the ISO/IEC MPEG-I family of standards [11]. The MIV standards are under development with the goal of providing the capability to compress a representation of a real or virtual 3D scene captured by multiple real or virtual cameras. The key feature of immersive video playback is that it allows viewers to have control over the position and orientation of the view of the content.\n\nDesigning FVV streaming systems poses several challenges in terms of video quality, real-time operation, and cost. Achieving high-quality synthesized streams depends on the quality and quantity of cameras used in the video acquisition stage. However, this increases the overall cost of the acquisition setup, as well as the resources required for processing (compression and rendering) and network bandwidth. Furthermore, the complexity of the synthesis algorithms is so high that it can prevent real-time operation, even with high-end computing resources. Some commercial systems were also developed, such as Intel True View [12] or 4DReplay [13], however, they have oriented their application towards the quality end, sacrificing real-time operation and user interactivity.\n\nDIBR is the most commonly used synthesis algorithm for virtual view synthesis that generates the point clouds for each view based on texture view and depth map. A DIBR-based FVV service model is built from four main blocks: acquisition and compression, streaming, head-tracking, and viewpoint synthesis.\n\nIn an FVV system, the acquisition block aims to yield a data format that comprises both multiview texture and geometry information. To capture the scene, general color cameras are combined with active depth sensors. Depth views are typically monochromatic images taking values in the range of 0 and 255, as illustrated in Fig. 1, with the corresponding color image [14]. Various technologies are available for capturing depth data in real-time, which can be categorized as active or passive depending on whether the devices emit signals onto the scene or not [15]. Active depth sensing devices include time-of-flight (ToF) cameras, LIDAR sensors, and structured light cameras, while stereo and plenoptic cameras are passive depth estimation devices.\n\nCamera density is a crucial factor that has a significant impact on the quality of the synthesized view. However, as the number of cameras increases, more processing is required to take advantage of all the cameras. The camera baseline is an important parameter in FVV systems as it affects the parallax between views and determines the angular resolution of the captured scene. A larger camera baseline results in greater parallax between views and finer angular resolution, which in turn allows for more precise viewpoint synthesis. However, increasing the camera baseline also has practical limitations. There is a classical trade-off to be considered between costs, complexity, required network capacity, and quality metrics such as navigation range and virtual view quality.\n\nThree different categories of image-based 3D representation formats can be distinguished: two-view stereo video, multi-view video, and multi-view video-plus-depth (MVD). Two-view stereo video, also known as stereoscopic video, is the simplest scenario, consisting of two videos representing the left and right views from slightly different viewpoints. The coding process leverages the temporal and spatial redundancies of frames. In the case of multi-view video, which uses a set of synchronized cameras to capture the same scene from different viewpoints, the same approach can be applied. An effective method for encoding two or more videos depicting the same scene from various viewpoints is known as multi-view video coding (MVC) [16]. Multi-view video-plus-depth (MVD) [17] representation uses per-pixel depth map sequences associated with multi-view texture video. In the case of MVD, each stream is encoded by taking advantage of the inter-view and intra-view correlation among all frames in the depth and color information from different views to remove the temporal and spatial redundancy. The multi-view versions of standard video codecs like H.264/AVC or H.265/HEVC, make possible to achieve up to 30% reduction in bitrate for dense multiview arrays as compared to simulcast [18]. However, this gain in bitrate comes at the cost of an increase in encoder complexity.\n\nEfficient data compression is essential not just for storage but also for network delivery of multi-view video streams. To create a synthesized virtual viewpoint from the available camera views, camera streams must be delivered to the renderer, which can be located in a central media server, user equipment, or distributed in the network [5].\n\nIn server-based approaches [19, 20], a central media server processes all camera views and associated depth map sequences and generates a requested virtual viewpoint stream for each user based on the desired viewpoint coordinates provided by the clients. The synthesized views are delivered as conventional 2D streams to the users. The limitation of the server-based solution is that the scalability of this approach may be restricted by the computational capacity of the media server.\n\nIn case of the client-based models [21,22,23], camera streams and depth sequences are delivered to the clients to generate virtual views independently. This delivery scheme avoids the limited resource capacity problem of a centralized media server, but it requires large network traffic to transmit all requested reference streams to the clients.\n\nTo reduce the overall traffic load, it is possible to deliver only the required camera sequences and corresponding depth images to the users. However, this approach requires adaptive camera handover methods that continuously change the delivered set of camera streams depending on the current viewpoint. In our previous work [5], we proposed an optimized proxy server-based distributed viewpoint synthesis solutions for this issue combined with predictive camera selection using threshold areas to prefetch camera view sequences that are likely to be required for the view synthesis. Another solution for reducing the network traffic is to use multicast end-to-end delivery with advanced multicast group management methods to handle continuously changing requested camera streams and avoid late camera view delivery as it was investigated in [6]. While in our previous approaches, the user-specific virtual view was synthesized from neighboring camera views, the novelty of the current work is that the used camera-pair baseline dynamically changes based on other users’ requested viewpoints. The goal is similar: minimize the network traffic, but even at the price of slight video quality degradation.\n\nIn general, the effectiveness of multicast delivery can be described by the so-called multicast power law. Chuang and Sirbu [24] conducted experiments on various real and simulated network topologies to assess the efficiency of multicast communications. They measured the average number of links, denoted as \\(L_N(n)\\), in the multicast tree required to reach n randomly selected destination hosts from a given source. The efficiency gain of multicast over unicast can be evaluated by examining the deviation of \\(L_N(n)\\) from linear growth (as observed in unicast). Based on extensive simulations, Chuang and Sirbu [24] experimentally concluded that the empirical power law for small values of n is \\(L_N(n)/U_N(n) \\sim n^{0.8}\\), where \\(U_N\\) represents the average number of links in a unicast path. The equation is however not general and depends on the structural properties of multicast trees.\n\nAlthough multicast delivery is a very efficient way of serving multiple customers simultaneously, only a few papers investigate multicast FVV streaming approach, such as [14, 25, 26] including our previous works as well.\n\nThe third approach is a distributed model that uses network servers (e.g., cloud or edge) for view synthesis [15, 27,28,29]. In this model, the client does not directly connect to the media server but instead requests the generation of the stream with the desired viewpoint from the most suitable server. In the case of remote rendering, bandwidth and computation issues can be resolved because all virtual views are synthesized remotely on a powerful server at the cost of increased latency.\n\nSimilarly to conventional 2D video, the choice of media transport technology (including protocol, strategy, etc.) for transmission depends on the objectives and characteristics of the application. In real-time FVV systems, compressed video is usually transmitted using the RTP/UDP, while on-demand services have less strict latency demands, thus HTTP-based streaming (HLS, DASH) is also an option.\n\nViewpoint-tracking is a pivotal component in the functionality and user experience of FVV services. These technologies allow users to navigate and explore video content dynamically, offering an immersive and interactive experience akin to real-life observation. Tracking systems sense the position and orientation of the user, especially head and eye movements. By accurately tracking the position and orientation of a user’s head, FVV systems can adjust the displayed perspective in real-time, ensuring that the scene responds naturally to the user’s movements. This responsiveness is crucial for maintaining immersion, as any lag or inaccuracy can disrupt the sense of presence and engagement.\n\nIn real-life implementations, head-tracking and viewpoint-tracking can be achieved through sensor-based or vision-based methods [30]. One common approach is using sensors mounted on the user’s body, typically a head-mounted display (HMD). This technology is often seen in VR headsets like the Oculus Rift or HTC Vive, where a combination of external sensors and onboard accelerometers and gyroscopes provide precise tracking data. An HMD-based immersive visualization system was developed by Usón et al. [31] that enables point-of-view control with the user’s natural position and visualization of live volumetric content in a 3D environment. Synthetic views are generated in real-time by the FVV system and streamed with low latency protocols to a Meta Quest 3 HMD using a WebRTC-based server. The performance tests conducted on the implemented system showed that their approach is able to provide HD resolution (1080p@30fps) with low end-to-end delay (\\(\\sim \\)380 ms).\n\nAnother approach involves the use of computer vision techniques, where cameras analyze the user’s facial features and movements to infer head position and orientation. Such a head-tracking-based multi-view display system was developed by [32] using a head-tracking camera placed in front of the observer. The deployed RealSense SR300 camera with a built-in face-tracking algorithm was able to perfectly detect the user’s head and eye gaze direction even in low light and minimal occlusion conditions.\n\nAdvanced implementations may also incorporate prediction algorithms to improve tracking accuracy and robustness, particularly in complex or cluttered environments. Kurutepe et al. [20] proposed a selective FVV streaming system based on multi-view video coding and user head-tracking. In the proposed system, the client predicts the future user’s head position using a Kalman filter-based predictor and sends the requested viewpoint to the server over a feedback channel. A more simple and less precise tracking system was proposed by Hamza et al. [33] based on dead reckoning. Their method calculates the head’s current position by using a previously determined position and advancing that position based on known or estimated velocities over a duration of elapsed time and course. By tracking the user’s viewpoint positions, dead reckoning can enable the client to predict the future path by assuming that the user maintains the current view-switching velocity.\n\nMulti-view-video-plus-depth (MVD) representation is commonly used for free-viewpoint image synthesis. The synthesis process involves backprojecting image pixels to the 3D space using per-pixel depth information and rendering them from the virtual viewpoint, which is often referred to as 3D warping or depth-image-based rendering (DIBR). The viewpoint synthesizing process utilizes the color image, depth map, and camera calibration information to project any pixel of the image into 3D space and then project it back onto a virtual camera plane to create a new virtual image. The method can be conceptually understood as a two-step process [34]: (i.) 3D image warping, which uses depth data and camera parameters to back-project pixel samples from reference images to their proper 3D locations and then re-project them onto the new synthesized image space; (ii.) reconstruction and re-sampling, which involves determining the pixel sample values in the synthesized image.\n\nStandard 3D warping cannot produce high-quality virtual viewpoint images when the virtual viewpoint is physically distant from the reference viewpoints due to holes created by the 3D warping process, resulting in disocclusion holes and non-disocclusion [35]. The quality of the rendered view depends on the efficiency of the hole-filling and inpainting methods.\n\nInter-camera distance (baseline) has a significant effect on the achieved view quality [36]. Domański et al. [37] investigated the synthesis of views along the path between physical cameras using sparse camera setups. Their research highlighted the relationship between the baseline, depth resolution, and occlusion size, all of which affect the performance of viewpoint synthesis. They demonstrated that occluded regions across neighboring camera views can have a negative impact on depth estimation and viewpoint synthesis. As a result, a non-uniform arrangement of cameras clustered pairwise over the scene is recommended for better viewpoint synthesis results. Similar results were obtained in [35, 38,39,40].\n\nAlthough there are several tools for viewpoint synthesis developed for research purposes, the most well-known implementation is the View Synthesis Reference Software (VSRS) [41] provided by the ISO/IEC Moving Pictures Experts Group (MPEG). The aim of the implementation is to allow researchers to evaluate and compare the performance of different algorithms under different conditions."
    },
    {
        "link": "https://global.canon/en/news/2019/20190917.html",
        "document": "TOKYO, September 17, 2019—Canon Inc. announced today that during seven matches of Rugby World Cup 2019, to be held from September 20 to November 2, the company will provide highlight footage created by the Free Viewpoint Video System to the International Games Broadcast Services (IGBS). These video highlights feature viewpoints and angles not possible with conventional cameras, effectively conveying the thrill of such sports as rugby and delivering an experience that feels like being right on the field.\n\nCanon will create Free Viewpoint Video content during all seven of the tournament matches to be held at the International Stadium Yokohama. Captured video footage will be curated into highlight reels that display the excitement and superb skill of each play from the ideal viewpoint and angle. Generated video will be provided to IGBS, who will then distribute the content to rights holders for broadcast, streaming, news, sports commentary programs and other forms of content viewing. In addition, content will be made available on the Canon homepage approximately 24 hours after the conclusion of each match.\n\n* The Rugby World Cup 2019 Canon website is no longer available.\n\nCanon has continued to refine such aspects of the Free Viewpoint Video System as camera positioning, method of image capture, image-processing algorithms and hardware to achieve higher image quality and faster content generation workflows. The system is now capable of delivering high-quality, newsworthy content to broadcasters and content distributors. With the system, Canon is able to generate Free Viewpoint Video content and freely control its time and positioning. This technological evolution has been recognized by IGBS, thus laying the foundation for full-scale professional use by broadcasters.\n\nWith this tournament, Canon marks its entry into the volumetric video1 market, in which the company will begin business via the creation and delivery of sports video content to broadcast television and Over the Top2 media services. Going forward, Canon aims to apply this technology and provide video for live broadcast and video replay, alongside a wide range of event and entertainment genres beyond sports.\n\nThe system comprises multiple high-resolution cameras set up around a stadium, which are connected to a network and controlled via software to simultaneously capture the game from multiple viewpoints. Afterward, image processing technology renders the videos as high-resolution 3D spatial data within which users can freely move a virtual camera around the 3D space, resulting in video that can be viewed from various different angles and viewpoints."
    },
    {
        "link": "http://ip.hhi.de/imagecom_G1/assets/pdfs/icip04_free_viewpoint.pdf",
        "document": ""
    },
    {
        "link": "https://studios.disneyresearch.com/wp-content/uploads/2019/03/3D-video-and-free-viewpoint-video%E2%80%94From-capture-to-display.pdf",
        "document": ""
    },
    {
        "link": "https://cgl.ethz.ch/Downloads/Publications/Papers/2011/Kus11/Kus11.pdf",
        "document": ""
    }
]