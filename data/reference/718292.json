[
    {
        "link": "https://helm.sh/docs/chart_template_guide",
        "document": "This guide provides an introduction to Helm's chart templates, with emphasis on the template language.\n\nTemplates generate manifest files, which are YAML-formatted resource descriptions that Kubernetes can understand. We'll look at how templates are structured, how they can be used, how to write Go templates, and how to debug your work.\n\nThis guide focuses on the following concepts:\n\nThis guide is oriented toward learning the ins and outs of the Helm template language. Other guides provide introductory material, examples, and best practices."
    },
    {
        "link": "https://helm.sh/docs/chart_template_guide/getting_started",
        "document": "In this section of the guide, we'll create a chart and then add a first template. The chart we created here will be used throughout the rest of the guide.\n\nTo get going, let's take a brief look at a Helm chart.\n\nAs described in the Charts Guide, Helm charts are structured like this:\n\nThe directory is for template files. When Helm evaluates a chart, it will send all of the files in the directory through the template rendering engine. It then collects the results of those templates and sends them on to Kubernetes.\n\nThe file is also important to templates. This file contains the default values for a chart. These values may be overridden by users during or .\n\nThe file contains a description of the chart. You can access it from within a template.\n\nThe directory may contain other charts (which we call subcharts). Later in this guide we will see how those work when it comes to template rendering.\n\nFor this guide, we'll create a simple chart called , and then we'll create some templates inside of the chart.\n\nIf you take a look at the directory, you'll notice a few files already there.\n• : The \"help text\" for your chart. This will be displayed to your users when they run .\n• : A basic manifest for creating a service endpoint for your deployment\n• : A place to put template helpers that you can re-use throughout the chart\n\nAnd what we're going to do is... remove them all! That way we can work through our tutorial from scratch. We'll actually create our own and as we go.\n\nWhen you're writing production grade charts, having basic versions of these charts can be really useful. So in your day-to-day chart authoring, you probably won't want to remove them.\n\nThe first template we are going to create will be a . In Kubernetes, a ConfigMap is simply an object for storing configuration data. Other things, like pods, can access the data in a ConfigMap.\n\nBecause ConfigMaps are basic resources, they make a great starting point for us.\n\nTIP: Template names do not follow a rigid naming pattern. However, we recommend using the extension for YAML files and for helpers.\n\nThe YAML file above is a bare-bones ConfigMap, having the minimal necessary fields. By virtue of the fact that this file is in the directory, it will be sent through the template engine.\n\nIt is just fine to put a plain YAML file like this in the directory. When Helm reads this template, it will simply send it to Kubernetes as-is.\n\nWith this simple template, we now have an installable chart. And we can install it like this:\n\nUsing Helm, we can retrieve the release and see the actual template that was loaded.\n\nThe command takes a release name ( ) and prints out all of the Kubernetes resources that were uploaded to the server. Each file begins with to indicate the start of a YAML document, and then is followed by an automatically generated comment line that tells us what template file generated this YAML document.\n\nFrom there on, we can see that the YAML data is exactly what we put in our file.\n\nNow we can uninstall our release: .\n\nHard-coding the into a resource is usually considered to be bad practice. Names should be unique to a release. So we might want to generate a name field by inserting the release name.\n\nTIP: The field is limited to 63 characters because of limitations to the DNS system. For that reason, release names are limited to 53 characters. Kubernetes 1.3 and earlier limited to only 24 characters (thus 14 character names).\n\nThe big change comes in the value of the field, which is now .\n\nThe template directive injects the release name into the template. The values that are passed into a template can be thought of as namespaced objects, where a dot ( ) separates each namespaced element.\n\nThe leading dot before indicates that we start with the top-most namespace for this scope (we'll talk about scope in a bit). So we could read as \"start at the top namespace, find the object, then look inside of it for an object called \".\n\nThe object is one of the built-in objects for Helm, and we'll cover it in more depth later. But for now, it is sufficient to say that this will display the release name that the library assigns to our release.\n\nNow when we install our resource, we'll immediately see the result of using this template directive:\n\nYou can run to see the entire generated YAML.\n\nNote that the ConfigMap inside Kubernetes name is instead of previously.\n\nAt this point, we've seen templates at their most basic: YAML files that have template directives embedded in and . In the next part, we'll take a deeper look into templates. But before moving on, there's one quick trick that can make building templates faster: When you want to test the template rendering, but not actually install anything, you can use . This will render the templates. But instead of installing the chart, it will return the rendered template to you so you can see the output:\n\nUsing will make it easier to test your code, but it won't ensure that Kubernetes itself will accept the templates you generate. It's best not to assume that your chart will install just because works.\n\nIn the Chart Template Guide, we take the basic chart we defined here and explore the Helm template language in detail. And we'll get started with built-in objects."
    },
    {
        "link": "https://github.com/helm/charts/blob/master/stable/mysql/templates/deployment.yaml",
        "document": "This repository was archived by the owner on Feb 22, 2022. It is now read-only.\n• Notifications You must be signed in to change notification settings {{- if not (and .Values.allowEmptyRootPassword (not .Values.mysqlRootPassword)) }} {{- if not (and .Values.allowEmptyRootPassword (not .Values.mysqlPassword)) }}\n\nYou can’t perform that action at this time."
    },
    {
        "link": "https://devtron.ai/blog/helm-chart-deployment",
        "document": "In the dynamic landscape of Kubernetes, managing applications and their myriad configurations can become a complex endeavor. Enter Helm, a potent package manager for Kubernetes, with Helm charts at the core of its utility. This article delves into the essence of Helm charts, underscores their widespread adoption, evaluates alternatives, and conducts a comparative analysis to spotlight the advantages and disadvantages between Helm charts and their alternatives.\n\nHelm simplifies Kubernetes deployments, but debugging a broken release is difficult. Devtron eliminates the guesswork in debugging, and your work becomes significantly easier. With Devtron, you can quickly diagnose issues and repair them in a timely way.\n\nHelm charts are a collection of YAML files that describe a related set of Kubernetes resources, including metadata, configuration files, and templates for creating Kubernetes deployments. A chart may encompass all necessities for running a web server, a database, and the backend services required by an application, bundling these specifications alongside a default configuration into a singular, manageable entity. Charts simplify the Kubernetes application deployment process, enabling users to employ simple commands for installation, upgrade, and management.\n• Simplification: Abstracting the complexity of deploying applications on Kubernetes, transforming a multi-step operation into a straightforward command line (CLI) instruction.\n• Versioning and Rollbacks: Support for versioning with Helm charts facilitates easy rollbacks to previous versions of an application, leveraging helm rollback commands for efficient management.\n• Template Engine: With the built-in template engine, users can customize installations via the file without modifying the chart itself, promoting reuse across various environments.\n• Dependency Management: Charts can specify dependencies on other charts or docker images, automating the deployment of complex applications and microservices.\n\nThe allure of Helm charts for Kubernetes users lies in their ability to streamline application deployment and management:\n• DevOps Workflow Integration: Seamlessly integrates into CI/CD pipelines, enhancing DevOps practices by automating helm deployments through commands like and .\n• Comprehensive Management: From helm repo addition to helm release tracking, charts provide a unified approach to managing lifecycle events, including upgrades and rollbacks, with detailed documentation available through helm docs.\n• Community and Support: Helm's strong community contributes to a vast helm repo on GitHub, offering extensive resources, tutorials, and support for new and experienced users alike.\n\nWhile Helm charts reign supreme for many, alternatives exist catering to specific needs and preferences:\n• Kustomize: Offers a declarative approach to configuration, focusing on customization of Kubernetes objects without the overhead of templating.\n• Terraform: An infrastructure as code tool supporting Kubernetes, allowing users to manage a wide array of resources across service providers like Azure.\n• Pulumi: Enables infrastructure definition using general-purpose programming languages, presenting an alternative approach to Kubernetes resource management.\n• Ansible: Renowned for its simplicity, Ansible manages Kubernetes resources with ease, appealing for its agentless architecture and automation capabilities beyond Kubernetes.\n• Strong community support and a large repository of existing charts for common applications.\n• The templating syntax can be complex for newcomers.\n• Over-reliance on charts for simple tasks can sometimes complicate matters.\n• Leaner, more focused on configuration customization without additional templating.\n• Limited in scope compared to Helm, focusing on customization over package management.\n• Can require more boilerplate for complex deployments.\n• Infrastructure as code approach appeals to users seeking consistency across environments.\n• Learning curve and complexity can be higher due to broader scope.\n• May provide more functionality than needed for Kubernetes-only deployments.\n• Strong community and wide adoption in automation beyond Kubernetes.\n• Lacks some of the Kubernetes-specific features and optimizations found in Helm.\n• Can be slower for complex deployments due to its agentless nature.\n\nThe Helm client is a command-line interface (CLI) tool that users interact with to manage Helm charts and their deployments on Kubernetes. It acts as the primary interface for executing various Helm commands to manage Kubernetes applications.\n\nIt processes the commands issued by the user, such as installing, upgrading, rolling back deployments, or packaging Helm charts. The Helm client communicates with the Helm library to execute these commands, which in turn interacts with the Kubernetes API.\n• Functionality: This command packages a Helm chart into a chart archive (a file). It collects all the files and directories within a chart directory, compresses them, and creates a versioned chart archive. This command is essential for chart distribution and versioning.\n• Relationship: The command is used by chart developers to prepare a chart for distribution. Once packaged, the chart can be distributed via Helm chart repositories or directly to users. The Helm client utilizes this packaged chart for deployment.\n• Functionality: This command is used to deploy a Helm chart onto a Kubernetes cluster. It reads the specified chart (either from a local directory, a package, or a chart repository), processes the configuration options provided by the user, and then deploys the resulting set of Kubernetes resources to the cluster.\n• Relationship: After a chart is packaged with the command, it can be installed using the command. The Helm client interprets the command, processes the chart and its configurations (values), and manages the deployment process to Kubernetes.\n• Functionality: This command renders the chart templates locally and displays the output. It allows users to see the generated Kubernetes manifest files without actually deploying them to a cluster. This is useful for debugging or for understanding what resources will be created or modified.\n• Relationship: The command can be used as a preliminary step before to verify what Kubernetes resources will be deployed. It leverages the same chart packaging managed by the command but doesn't require interaction with a Kubernetes cluster for its execution. It provides a way for users to inspect or audit the deployment manifests for correctness or compliance before actual deployment.\n\nThe workflow typically starts with chart developers using to prepare and version their charts. Users, then, can use the Helm client to execute the command to deploy these packaged charts onto their Kubernetes clusters. Before installation, users might opt to run with their chart to preview the Kubernetes resources that will be created without making any changes to their cluster. This entire process underscores Helm's capability to streamline Kubernetes application deployment and management through an integrated set of tools and commands.\n\nHow to Make Helm Deployments Ridiculously Easy With Devtron\n\nBuilding a Helm chart and managing it, are two different aspects of packaging an application for Kubernetes. Helm is efficient enough to package a complex K8s application and deploy it. When it comes down to app management, there are certain challenges with Helm. In this short example, we will see how easy it is to deploy and manage a custom mongodb chart using Devtron.\n\nDevtron comes with a number of pre-added, commonly used Helm charts that can be seen in the `Charts` section. But if you want to install your custom chart, let's say MongoDB, it can be easily achieved using Devtron.\n\nTo add any custom chart, go to → → → →\n\nNow that we have added our custom chart repository i.e, , let’s deploy it.\n\nTo deploy any chart through Devtron, goto → → → → .\n\nFor instance, I want to deploy Percona's MongoDB operator. Let’s see how we can deploy it with just a few clicks.\n\nStep-2: Click on to edit default values as per your requirements\n\nStep-3: Edit the default configuration and click on . You can also see the description for all the parameters, from the sidebar - README of the particular chart.\n\nStep-4: Congratulations, you have successfully deployed! You can see all the details of your deployed chart such as current status, different workloads, manifests, events, logs, deployment history, etc within Devtron dashboard itself.\n\nChoosing between Helm charts and alternatives boils down to specific project requirements, preferences, and infrastructure. Helm offers a robust, feature-rich platform ideal for managing complex Kubernetes applications, supported by a comprehensive ecosystem of tools and community resources. For those seeking simplicity and integration within DevOps workflows, Devtron's Helm Chart Store presents an attractive alternative, streamlining application deployment and management in Kubernetes environments.\n\nReach out to us through the Discord channel and refer to Devtron documentation, to find all the features of Devtron.\n\nIf you'd like to continue learning more about Helm, read this blog post on the changes made to Helm 3."
    },
    {
        "link": "https://helm.sh/docs/topics/charts",
        "document": "Helm uses a packaging format called charts. A chart is a collection of files that describe a related set of Kubernetes resources. A single chart might be used to deploy something simple, like a memcached pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.\n\nCharts are created as files laid out in a particular directory tree. They can be packaged into versioned archives to be deployed.\n\nIf you want to download and look at the files for a published chart, without installing it, you can do so with .\n\nThis document explains the chart format, and provides basic guidance for building charts with Helm.\n\nA chart is organized as a collection of files inside of a directory. The directory name is the name of the chart (without versioning information). Thus, a chart describing WordPress would be stored in a directory.\n\nInside of this directory, Helm will expect a structure that matches this:\n\nHelm reserves use of the , , and directories, and of the listed file names. Other files will be left as they are.\n\nThe file is required for a chart. It contains the following fields:\n\nAs of v3.3.2, additional fields are not allowed. The recommended approach is to add custom metadata in .\n\nEvery chart must have a version number. A version must follow the SemVer 2 standard. Unlike Helm Classic, Helm v2 and later uses version numbers as release markers. Packages in repositories are identified by name plus version.\n\nFor example, an chart whose version field is set to will be named:\n\nMore complex SemVer 2 names are also supported, such as . But non-SemVer names are explicitly disallowed by the system.\n\nNOTE: Whereas Helm Classic and Deployment Manager were both very GitHub oriented when it came to charts, Helm v2 and later does not rely upon or require GitHub or even Git. Consequently, it does not use Git SHAs for versioning at all.\n\nThe field inside of the is used by many of the Helm tools, including the CLI. When generating a package, the command will use the version that it finds in the as a token in the package name. The system assumes that the version number in the chart package name matches the version number in the . Failure to meet this assumption will cause an error.\n\nThe field should be for Helm charts that require at least Helm 3. Charts supporting previous Helm versions have an set to and are still installable by Helm 3.\n\nChanges from to :\n• A field defining chart dependencies, which were located in a separate file for charts (see Chart Dependencies).\n• The field, discriminating application and library charts (see Chart Types).\n\nNote that the field is not related to the field. It is a way of specifying the version of the application. For example, the chart may have an , indicating that the version of Drupal included in the chart (by default) is . This field is informational, and has no impact on chart version calculations. Wrapping the version in quotes is highly recommended. It forces the YAML parser to treat the version number as a string. Leaving it unquoted can lead to parsing issues in some cases. For example, YAML interprets as a floating point value, and a git commit SHA like as scientific notation.\n\nAs of Helm v3.5.0, wraps the default field in quotes.\n\nThe optional field can define semver constraints on supported Kubernetes versions. Helm will validate the version constraints when installing the chart and fail if the cluster runs an unsupported Kubernetes version.\n\nVersion constraints may comprise space separated AND comparisons such as\n\nwhich themselves can be combined with the OR operator like in the following example\n\nIn this example the version is excluded, which can make sense if a bug in certain versions is known to prevent the chart from running properly.\n\nApart from version constrains employing operators the following shorthand notations are supported\n• hyphen ranges for closed intervals, where is equivalent to .\n• wildcards , and , where is equivalent to .\n• tilde ranges (patch version changes allowed), where is equivalent to .\n• caret ranges (minor version changes allowed), where is equivalent to .\n\nFor a detailed explanation of supported semver constraints see Masterminds/semver.\n\nWhen managing charts in a Chart Repository, it is sometimes necessary to deprecate a chart. The optional field in can be used to mark a chart as deprecated. If the latest version of a chart in the repository is marked as deprecated, then the chart as a whole is considered to be deprecated. The chart name can be later reused by publishing a newer version that is not marked as deprecated. The workflow for deprecating charts is:\n• Update chart's to mark the chart as deprecated, bumping the version\n• Release the new chart version in the Chart Repository\n• Remove the chart from the source repository (e.g. git)\n\nThe field defines the type of chart. There are two types: and . Application is the default type and it is the standard chart which can be operated on fully. The library chart provides utilities or functions for the chart builder. A library chart differs from an application chart because it is not installable and usually doesn't contain any resource objects.\n\nNote: An application chart can be used as a library chart. This is enabled by setting the type to . The chart will then be rendered as a library chart where all utilities and functions can be leveraged. All resource objects of the chart will not be rendered.\n\nCharts can also contain files that describe the installation, configuration, usage and license of a chart.\n\nA LICENSE is a plain text file containing the license for the chart. The chart can contain a license as it may have programming logic in the templates and would therefore not be configuration only. There can also be separate license(s) for the application installed by the chart, if required.\n\nA README for a chart should be formatted in Markdown (README.md), and should generally contain:\n• A description of the application or service the chart provides\n• Any prerequisites or requirements to run the chart\n• Descriptions of options in and default values\n• Any other information that may be relevant to the installation or configuration of the chart\n\nWhen hubs and other user interfaces display details about a chart that detail is pulled from the content in the file.\n\nThe chart can also contain a short plain text file that will be printed out after installation, and when viewing the status of a release. This file is evaluated as a template, and can be used to display usage notes, next steps, or any other information relevant to a release of the chart. For example, instructions could be provided for connecting to a database, or accessing a web UI. Since this file is printed to STDOUT when running or , it is recommended to keep the content brief and point to the README for greater detail.\n\nIn Helm, one chart may depend on any number of other charts. These dependencies can be dynamically linked using the field in or brought in to the directory and managed manually.\n\nThe charts required by the current chart are defined as a list in the field.\n• The field is the name of the chart you want.\n• The field is the version of the chart you want.\n• The field is the full URL to the chart repository. Note that you must also use to add that repo locally.\n• You might use the name of the repo instead of URL\n\nOnce you have defined dependencies, you can run and it will use your dependency file to download all the specified charts into your directory for you.\n\nWhen retrieves charts, it will store them as chart archives in the directory. So for the example above, one would expect to see the following files in the charts directory:\n\nIn addition to the other fields above, each requirements entry may contain the optional field .\n\nAdding an alias for a dependency chart would put a chart in dependencies using alias as name of new dependency.\n\nOne can use in cases where they need to access a chart with other name(s).\n\nIn the above example we will get 3 dependencies in all for :\n\nThe manual way of achieving this is by copy/pasting the same chart in the directory multiple times with different names.\n\nIn addition to the other fields above, each requirements entry may contain the optional fields and .\n\nAll charts are loaded by default. If or fields are present, they will be evaluated and used to control loading for the chart(s) they are applied to.\n\nCondition - The condition field holds one or more YAML paths (delimited by commas). If this path exists in the top parent's values and resolves to a boolean value, the chart will be enabled or disabled based on that boolean value. Only the first valid path found in the list is evaluated and if no paths exist then the condition has no effect.\n\nTags - The tags field is a YAML list of labels to associate with this chart. In the top parent's values, all charts with tags can be enabled or disabled by specifying the tag and a boolean value.\n\nIn the above example all charts with the tag would be disabled but since the path evaluates to 'true' in the parent's values, the condition will override the tag and will be enabled.\n\nSince is tagged with and that tag evaluates to , will be enabled. Also note that although has a condition specified, there is no corresponding path and value in the parent's values so that condition has no effect.\n\nThe parameter can be used as usual to alter tag and condition values.\n• Conditions (when set in values) always override tags. The first condition path that exists wins and subsequent ones for that chart are ignored.\n• Tags are evaluated as 'if any of the chart's tags are true then enable the chart'.\n• Tags and conditions values must be set in the top parent's values.\n• The key in values must be a top level key. Globals and nested tables are not currently supported.\n\nIn some cases it is desirable to allow a child chart's values to propagate to the parent chart and be shared as common defaults. An additional benefit of using the format is that it will enable future tooling to introspect user-settable values.\n\nThe keys containing the values to be imported can be specified in the parent chart's in the field using a YAML list. Each item in the list is a key which is imported from the child chart's field.\n\nTo import values not contained in the key, use the child-parent format. Examples of both formats are described below.\n\nIf a child chart's file contains an field at the root, its contents may be imported directly into the parent's values by specifying the keys to import as in the example below:\n\nSince we are specifying the key in our import list, Helm looks in the field of the child chart for key and imports its contents.\n\nThe final parent values would contain our exported field:\n\nPlease note the parent key is not contained in the parent's final values. If you need to specify the parent key, use the 'child-parent' format.\n\nTo access values that are not contained in the key of the child chart's values, you will need to specify the source key of the values to be imported ( ) and the destination path in the parent chart's values ( ).\n\nThe in the example below instructs Helm to take any values found at path and copy them to the parent's values at the path specified in\n\nIn the above example, values found at in the subchart1's values will be imported to the key in the parent chart's values as detailed below:\n\nThe parent chart's resulting values would be:\n\nThe parent's final values now contains the and fields imported from subchart1.\n\nIf more control over dependencies is desired, these dependencies can be expressed explicitly by copying the dependency charts into the directory.\n\nA dependency should be an unpacked chart directory but its name cannot start with or . Such files are ignored by the chart loader.\n\nFor example, if the WordPress chart depends on the Apache chart, the Apache chart (of the correct version) is supplied in the WordPress chart's directory:\n\nThe example above shows how the WordPress chart expresses its dependency on Apache and MySQL by including those charts inside of its directory.\n\nTIP: To drop a dependency into your directory, use the command\n\nThe above sections explain how to specify chart dependencies, but how does this affect chart installation using and ?\n\nSuppose that a chart named \"A\" creates the following Kubernetes objects\n\nFurthermore, A is dependent on chart B that creates objects\n\nAfter installation/upgrade of chart A a single Helm release is created/modified. The release will create/update all of the above Kubernetes objects in the following order:\n\nThis is because when Helm installs/upgrades charts, the Kubernetes objects from the charts and all its dependencies are\n• sorted by type followed by name; and then\n\nHence a single release is created with all the objects for the chart and its dependencies.\n\nThe install order of Kubernetes types is given by the enumeration InstallOrder in kind_sorter.go (see the Helm source file).\n\nHelm Chart templates are written in the Go template language, with the addition of 50 or so add-on template functions from the Sprig library and a few other specialized functions.\n\nAll template files are stored in a chart's folder. When Helm renders the charts, it will pass every file in that directory through the template engine.\n\nValues for the templates are supplied two ways:\n• Chart developers may supply a file called inside of a chart. This file can contain default values.\n• Chart users may supply a YAML file that contains values. This can be provided on the command line with .\n\nWhen a user supplies custom values, these values will override the values in the chart's file.\n\nTemplate files follow the standard conventions for writing Go templates (see the text/template Go package documentation for details). An example template file might look something like this:\n\nThe above example, based loosely on https://github.com/deis/charts, is a template for a Kubernetes replication controller. It can use the following four template values (usually defined in a file):\n• : The source registry for the Docker image.\n• : The tag for the docker image.\n• : The storage backend, whose default is set to\n\nAll of these values are defined by the template author. Helm does not require or dictate parameters.\n\nTo see many working charts, check out the CNCF Artifact Hub.\n\nValues that are supplied via a file (or via the flag) are accessible from the object in a template. But there are other pre-defined pieces of data you can access in your templates.\n\nThe following values are pre-defined, are available to every template, and cannot be overridden. As with all values, the names are case sensitive.\n• : The name of the release (not the chart)\n• : The namespace the chart was released to.\n• : The service that conducted the release.\n• : This is set to true if the current operation is an upgrade or rollback.\n• : This is set to true if the current operation is an install.\n• : The contents of the . Thus, the chart version is obtainable as and the maintainers are in .\n• : A map-like object containing all non-special files in the chart. This will not give you access to templates, but will give you access to additional files that are present (unless they are excluded using ). Files can be accessed using or using the function. You can also access the contents of the file as using\n• : A map-like object that contains information about the versions of Kubernetes ( ) and the supported Kubernetes API versions ( )\n\nNOTE: Any unknown fields will be dropped. They will not be accessible inside of the object. Thus, cannot be used to pass arbitrarily structured data into the template. The values file can be used for that, though.\n\nConsidering the template in the previous section, a file that supplies the necessary values would look like this:\n\nA values file is formatted in YAML. A chart may include a default file. The Helm install command allows a user to override values by supplying additional YAML values:\n\nWhen values are passed in this way, they will be merged into the default values file. For example, consider a file that looks like this:\n\nWhen this is merged with the in the chart, the resulting generated content will be:\n\nNote that only the last field was overridden.\n\nNOTE: The default values file included inside of a chart must be named . But files specified on the command line can be named anything.\n\nNOTE: If the flag is used on or , those values are simply converted to YAML on the client side.\n\nNOTE: If any required entries in the values file exist, they can be declared as required in the chart template by using the 'required' function\n\nAny of these values are then accessible inside of templates using the object:\n\nValues files can declare values for the top-level chart, as well as for any of the charts that are included in that chart's directory. Or, to phrase it differently, a values file can supply values to the chart as well as to any of its dependencies. For example, the demonstration WordPress chart above has both and as dependencies. The values file could supply values to all of these components:\n\nCharts at a higher level have access to all of the variables defined beneath. So the WordPress chart can access the MySQL password as . But lower level charts cannot access things in parent charts, so MySQL will not be able to access the property. Nor, for that matter, can it access .\n\nValues are namespaced, but namespaces are pruned. So for the WordPress chart, it can access the MySQL password field as . But for the MySQL chart, the scope of the values has been reduced and the namespace prefix removed, so it will see the password field simply as .\n\nAs of 2.0.0-Alpha.2, Helm supports special \"global\" value. Consider this modified version of the previous example:\n\nThe above adds a section with the value . This value is available to all charts as .\n\nFor example, the templates may access as , and so can the chart. Effectively, the values file above is regenerated like this:\n\nThis provides a way of sharing one top-level variable with all subcharts, which is useful for things like setting properties like labels.\n\nIf a subchart declares a global variable, that global will be passed downward (to the subchart's subcharts), but not upward to the parent chart. There is no way for a subchart to influence the values of the parent chart.\n\nAlso, global variables of parent charts take precedence over the global variables from subcharts.\n\nSometimes, a chart maintainer might want to define a structure on their values. This can be done by defining a schema in the file. A schema is represented as a JSON Schema. It might look something like this:\n\nThis schema will be applied to the values to validate it. Validation occurs when any of the following commands are invoked:\n\nAn example of a file that meets the requirements of this schema might look something like this:\n\nNote that the schema is applied to the final object, and not just to the file. This means that the following file is valid, given that the chart is installed with the appropriate option shown below.\n\nFurthermore, the final object is checked against all subchart schemas. This means that restrictions on a subchart can't be circumvented by a parent chart. This also works backwards - if a subchart has a requirement that is not met in the subchart's file, the parent chart must satisfy those restrictions in order to be valid.\n\nWhen it comes to writing templates, values, and schema files, there are several standard references that will help you out.\n\nKubernetes provides a mechanism for declaring new types of Kubernetes objects. Using CustomResourceDefinitions (CRDs), Kubernetes developers can declare custom resource types.\n\nIn Helm 3, CRDs are treated as a special kind of object. They are installed before the rest of the chart, and are subject to some limitations.\n\nCRD YAML files should be placed in the directory inside of a chart. Multiple CRDs (separated by YAML start and end markers) may be placed in the same file. Helm will attempt to load all of the files in the CRD directory into Kubernetes.\n\nCRD files cannot be templated. They must be plain YAML documents.\n\nWhen Helm installs a new chart, it will upload the CRDs, pause until the CRDs are made available by the API server, and then start the template engine, render the rest of the chart, and upload it to Kubernetes. Because of this ordering, CRD information is available in the object in Helm templates, and Helm templates may create new instances of objects that were declared in CRDs.\n\nFor example, if your chart had a CRD for in the directory, you may create instances of the kind in the directory:\n\nThe file must contain the CRD with no template directives:\n\nThen the template may create a new (using templates as usual):\n\nHelm will make sure that the kind has been installed and is available from the Kubernetes API server before it proceeds installing the things in .\n\nUnlike most objects in Kubernetes, CRDs are installed globally. For that reason, Helm takes a very cautious approach in managing CRDs. CRDs are subject to the following limitations:\n• CRDs are never reinstalled. If Helm determines that the CRDs in the directory are already present (regardless of version), Helm will not attempt to install or upgrade.\n• CRDs are never installed on upgrade or rollback. Helm will only create CRDs on installation operations.\n• CRDs are never deleted. Deleting a CRD automatically deletes all of the CRD's contents across all namespaces in the cluster. Consequently, Helm will not delete CRDs.\n\nOperators who want to upgrade or delete CRDs are encouraged to do this manually and with great care.\n\nThe tool has several commands for working with charts.\n\nIt can create a new chart for you:\n\nOnce you have edited a chart, can package it into a chart archive for you:\n\nYou can also use to help you find issues with your chart's formatting or information:\n\nA chart repository is an HTTP server that houses one or more packaged charts. While can be used to manage local chart directories, when it comes to sharing charts, the preferred mechanism is a chart repository.\n\nAny HTTP server that can serve YAML files and tar files and can answer GET requests can be used as a repository server. The Helm team has tested some servers, including Google Cloud Storage with website mode enabled, and S3 with website mode enabled.\n\nA repository is characterized primarily by the presence of a special file called that has a list of all of the packages supplied by the repository, together with metadata that allows retrieving and verifying those packages.\n\nOn the client side, repositories are managed with the commands. However, Helm does not provide tools for uploading charts to remote repository servers. This is because doing so would add substantial requirements to an implementing server, and thus raise the barrier for setting up a repository.\n\nThe command takes an optional option that lets you specify a \"starter chart\". Also, the starter option has a short alias .\n\nStarters are just regular charts, but are located in . As a chart developer, you may author charts that are specifically designed to be used as starters. Such charts should be designed with the following considerations in mind:\n• The will be overwritten by the generator.\n• Users will expect to modify such a chart's contents, so documentation should indicate how users can do so.\n• All occurrences of will be replaced with the specified chart name so that starter charts can be used as templates, except for some variable files. For example, if you use custom files in the directory or certain files, will NOT override inside them. Additionally, the chart description is not inherited.\n\nCurrently the only way to add a chart to is to manually copy it there. In your chart's documentation, you may want to explain that process."
    },
    {
        "link": "https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node",
        "document": "You can constrain a Pod so that it is restricted to run on particular node(s), or to prefer to run on particular nodes. There are several ways to do this and the recommended approaches all use label selectors to facilitate the selection. Often, you do not need to set any such constraints; the scheduler will automatically do a reasonable placement (for example, spreading your Pods across nodes so as not place Pods on a node with insufficient free resources). However, there are some circumstances where you may want to control which node the Pod deploys to, for example, to ensure that a Pod ends up on a node with an SSD attached to it, or to co-locate Pods from two different services that communicate a lot into the same availability zone.\n\nYou can use any of the following methods to choose where Kubernetes schedules specific Pods:\n\nLike many other Kubernetes objects, nodes have labels. You can attach labels manually. Kubernetes also populates a standard set of labels on all nodes in a cluster.\n\nThe value of these labels is cloud provider specific and is not guaranteed to be reliable. For example, the value of may be the same as the node name in some environments and a different value in other environments.\n\nAdding labels to nodes allows you to target Pods for scheduling on specific nodes or groups of nodes. You can use this functionality to ensure that specific Pods only run on nodes with certain isolation, security, or regulatory properties.\n\nIf you use labels for node isolation, choose label keys that the kubelet cannot modify. This prevents a compromised node from setting those labels on itself so that the scheduler schedules workloads onto the compromised node.\n\nThe admission plugin prevents the kubelet from setting or modifying labels with a prefix.\n\nTo make use of that label prefix for node isolation:\n• Ensure you are using the Node authorizer and have enabled the admission plugin.\n• Add labels with the prefix to your nodes, and use those labels in your node selectors. For example, or .\n\nis the simplest recommended form of node selection constraint. You can add the field to your Pod specification and specify the node labels you want the target node to have. Kubernetes only schedules the Pod onto nodes that have each of the labels you specify.\n\nSee Assign Pods to Nodes for more information.\n\nis the simplest way to constrain Pods to nodes with specific labels. Affinity and anti-affinity expands the types of constraints you can define. Some of the benefits of affinity and anti-affinity include:\n• The affinity/anti-affinity language is more expressive. only selects nodes with all the specified labels. Affinity/anti-affinity gives you more control over the selection logic.\n• You can indicate that a rule is soft or preferred, so that the scheduler still schedules the Pod even if it can't find a matching node.\n• You can constrain a Pod using labels on other Pods running on the node (or other topological domain), instead of just node labels, which allows you to define rules for which Pods can be co-located on a node.\n\nThe affinity feature consists of two types of affinity:\n• Node affinity functions like the field but is more expressive and allows you to specify soft rules.\n• Inter-pod affinity/anti-affinity allows you to constrain Pods against labels on other Pods.\n\nNode affinity is conceptually similar to , allowing you to constrain which nodes your Pod can be scheduled on based on node labels. There are two types of node affinity:\n• : The scheduler can't schedule the Pod unless the rule is met. This functions like , but with a more expressive syntax.\n• : The scheduler tries to find a node that meets the rule. If a matching node is not available, the scheduler still schedules the Pod.\n\nIn the preceding types, means that if the node labels change after Kubernetes schedules the Pod, the Pod continues to run.\n\nYou can specify node affinities using the field in your Pod spec.\n\nFor example, consider the following Pod spec:\n\nIn this example, the following rules apply:\n• The node must have a label with the key and the value of that label must be either or .\n• The node preferably has a label with the key and the value .\n\nYou can use the field to specify a logical operator for Kubernetes to use when interpreting the rules. You can use , , , , and .\n\nRead Operators to learn more about how these work.\n\nand allow you to define node anti-affinity behavior. Alternatively, you can use node taints to repel Pods from specific nodes.\n\nSee Assign Pods to Nodes using Node Affinity for more information.\n\nYou can specify a between 1 and 100 for each instance of the affinity type. When the scheduler finds nodes that meet all the other scheduling requirements of the Pod, the scheduler iterates through every preferred rule that the node satisfies and adds the value of the for that expression to a sum.\n\nThe final sum is added to the score of other priority functions for the node. Nodes with the highest total score are prioritized when the scheduler makes a scheduling decision for the Pod.\n\nFor example, consider the following Pod spec:\n\nIf there are two possible nodes that match the rule, one with the label and another with the label, the scheduler considers the of each node and adds the weight to the other scores for that node, and schedules the Pod onto the node with the highest final score.\n\nIf you want Kubernetes to successfully schedule the Pods in this example, you must have existing nodes with the label.\n\nWhen configuring multiple scheduling profiles, you can associate a profile with a node affinity, which is useful if a profile only applies to a specific set of nodes. To do so, add an to the field of the plugin in the scheduler configuration. For example:\n\nThe is applied to all Pods that set to , in addition to the NodeAffinity specified in the PodSpec. That is, in order to match the Pod, nodes need to satisfy and the Pod's .\n\nSince the is not visible to end users, its behavior might be unexpected to them. Use node labels that have a clear correlation to the scheduler profile name.\n\nInter-pod affinity and anti-affinity allow you to constrain which nodes your Pods can be scheduled on based on the labels of Pods already running on that node, instead of the node labels.\n\nInter-pod affinity and anti-affinity rules take the form \"this Pod should (or, in the case of anti-affinity, should not) run in an X if that X is already running one or more Pods that meet rule Y\", where X is a topology domain like node, rack, cloud provider zone or region, or similar and Y is the rule Kubernetes tries to satisfy.\n\nYou express these rules (Y) as label selectors with an optional associated list of namespaces. Pods are namespaced objects in Kubernetes, so Pod labels also implicitly have namespaces. Any label selectors for Pod labels should specify the namespaces in which Kubernetes should look for those labels.\n\nYou express the topology domain (X) using a , which is the key for the node label that the system uses to denote the domain. For examples, see Well-Known Labels, Annotations and Taints.\n\nInter-pod affinity and anti-affinity require substantial amounts of processing which can slow down scheduling in large clusters significantly. We do not recommend using them in clusters larger than several hundred nodes.\n\nPod anti-affinity requires nodes to be consistently labeled, in other words, every node in the cluster must have an appropriate label matching . If some or all nodes are missing the specified label, it can lead to unintended behavior.\n\nSimilar to node affinity are two types of Pod affinity and anti-affinity as follows:\n\nFor example, you could use affinity to tell the scheduler to co-locate Pods of two services in the same cloud provider zone because they communicate with each other a lot. Similarly, you could use anti-affinity to spread Pods from a service across multiple cloud provider zones.\n\nTo use inter-pod affinity, use the field in the Pod spec. For inter-pod anti-affinity, use the field in the Pod spec.\n\nScheduling a group of pods with inter-pod affinity to themselves\n\nIf the current Pod being scheduled is the first in a series that have affinity to themselves, it is allowed to be scheduled if it passes all other affinity checks. This is determined by verifying that no other pod in the cluster matches the namespace and selector of this pod, that the pod matches its own terms, and the chosen node matches all requested topologies. This ensures that there will not be a deadlock even if all the pods have inter-pod affinity specified.\n\nConsider the following Pod spec:\n\nThis example defines one Pod affinity rule and one Pod anti-affinity rule. The Pod affinity rule uses the \"hard\" , while the anti-affinity rule uses the \"soft\" .\n\nThe affinity rule specifies that the scheduler is allowed to place the example Pod on a node only if that node belongs to a specific zone where other Pods have been labeled with . For instance, if we have a cluster with a designated zone, let's call it \"Zone V,\" consisting of nodes labeled with , the scheduler can assign the Pod to any node within Zone V, as long as there is at least one Pod within Zone V already labeled with . Conversely, if there are no Pods with labels in Zone V, the scheduler will not assign the example Pod to any node in that zone.\n\nThe anti-affinity rule specifies that the scheduler should try to avoid scheduling the Pod on a node if that node belongs to a specific zone where other Pods have been labeled with . For instance, if we have a cluster with a designated zone, let's call it \"Zone R,\" consisting of nodes labeled with , the scheduler should avoid assigning the Pod to any node within Zone R, as long as there is at least one Pod within Zone R already labeled with . Conversely, the anti-affinity rule does not impact scheduling into Zone R if there are no Pods with labels.\n\nTo get yourself more familiar with the examples of Pod affinity and anti-affinity, refer to the design proposal.\n\nYou can use the , , and values in the field for Pod affinity and anti-affinity.\n\nRead Operators to learn more about how these work.\n\nIn principle, the can be any allowed label key with the following exceptions for performance and security reasons:\n• For Pod affinity and anti-affinity, an empty field is not allowed in both and .\n• For Pod anti-affinity rules, the admission controller limits to . You can modify or disable the admission controller if you want to allow custom topologies.\n\nIn addition to and , you can optionally specify a list of namespaces which the should match against using the field at the same level as and . If omitted or empty, defaults to the namespace of the Pod where the affinity/anti-affinity definition appears.\n\nYou can also select matching namespaces using , which is a label query over the set of namespaces. The affinity term is applied to namespaces selected by both and the field. Note that an empty ({}) matches all namespaces, while a null or empty list and null matches the namespace of the Pod where the rule is defined.\n\nKubernetes includes an optional field for Pod affinity or anti-affinity. The field specifies keys for the labels that should match with the incoming Pod's labels, when satisfying the Pod (anti)affinity.\n\nThe keys are used to look up values from the pod labels; those key-value labels are combined (using ) with the match restrictions defined using the field. The combined filtering selects the set of existing pods that will be taken into Pod (anti)affinity calculation.\n\nIt's not recommended to use with labels that might be updated directly on pods. Even if you edit the pod's label that is specified at directly, (that is, not via a deployment), kube-apiserver doesn't reflect the label update onto the merged .\n\nA common use case is to use with (set on Pods managed as part of a Deployment, where the value is unique for each revision). Using in allows you to target the Pods that belong to the same revision as the incoming Pod, so that a rolling upgrade won't break affinity.\n\nKubernetes includes an optional field for Pod affinity or anti-affinity. The field specifies keys for the labels that should not match with the incoming Pod's labels, when satisfying the Pod (anti)affinity.\n\nIt's not recommended to use with labels that might be updated directly on pods. Even if you edit the pod's label that is specified at directly, (that is, not via a deployment), kube-apiserver doesn't reflect the label update onto the merged .\n\nOne example use case is to ensure Pods go to the topology domain (node, zone, etc) where only Pods from the same tenant or team are scheduled in. In other words, you want to avoid running Pods from two different tenants on the same topology domain at the same time.\n\nInter-pod affinity and anti-affinity can be even more useful when they are used with higher level collections such as ReplicaSets, StatefulSets, Deployments, etc. These rules allow you to configure that a set of workloads should be co-located in the same defined topology; for example, preferring to place two related Pods onto the same node.\n\nFor example: imagine a three-node cluster. You use the cluster to run a web application and also an in-memory cache (such as Redis). For this example, also assume that latency between the web application and the memory cache should be as low as is practical. You could use inter-pod affinity and anti-affinity to co-locate the web servers with the cache as much as possible.\n\nIn the following example Deployment for the Redis cache, the replicas get the label . The rule tells the scheduler to avoid placing multiple replicas with the label on a single node. This creates each cache in a separate node.\n\nThe following example Deployment for the web servers creates replicas with the label . The Pod affinity rule tells the scheduler to place each replica on a node that has a Pod with the label . The Pod anti-affinity rule tells the scheduler never to place multiple servers on a single node.\n\nCreating the two preceding Deployments results in the following cluster layout, where each web server is co-located with a cache, on three separate nodes.\n\nThe overall effect is that each cache instance is likely to be accessed by a single client that is running on the same node. This approach aims to minimize both skew (imbalanced load) and latency.\n\nYou might have other reasons to use Pod anti-affinity. See the ZooKeeper tutorial for an example of a StatefulSet configured with anti-affinity for high availability, using the same technique as this example.\n\nis a more direct form of node selection than affinity or . is a field in the Pod spec. If the field is not empty, the scheduler ignores the Pod and the kubelet on the named node tries to place the Pod on that node. Using overrules using or affinity and anti-affinity rules.\n\nSome of the limitations of using to select nodes are:\n• If the named node does not exist, the Pod will not run, and in some cases may be automatically deleted.\n• If the named node does not have the resources to accommodate the Pod, the Pod will fail and its reason will indicate why, for example OutOfmemory or OutOfcpu.\n• Node names in cloud environments are not always predictable or stable.\n\nHere is an example of a Pod spec using the field:\n\nThe above Pod will only run on the node .\n\nYou can use topology spread constraints to control how Pods are spread across your cluster among failure-domains such as regions, zones, nodes, or among any other topology domains that you define. You might do this to improve performance, expected availability, or overall utilization.\n\nRead Pod topology spread constraints to learn more about how these work.\n\nThe following are all the logical operators that you can use in the field for and mentioned above.\n\nThe following operators can only be used with .\n\nand operators will not work with non-integer values. If the given value doesn't parse as an integer, the pod will fail to get scheduled. Also, and are not available for .\n• Read more about taints and tolerations.\n• Read the design docs for node affinity and for inter-pod affinity/anti-affinity.\n• Learn about how the topology manager takes part in node-level resource allocation decisions.\n• Learn how to use nodeSelector.\n• Learn how to use affinity and anti-affinity."
    },
    {
        "link": "https://densify.com/kubernetes-autoscaling/kubernetes-affinity",
        "document": "Kubernetes (K8s) scheduler often uses simple rules based on resource availability to place pods on nodes.\n\n That’s fine for many use cases. However, sometimes, you may need to go beyond simple resource-based scheduling.\n\nFor example, what if you want to schedule pods on specific nodes? Or, what if you want to avoid deploying specific pods on the same nodes?\n\nThat’s where Kubernetes affinity and Kubernetes anti-affinity come in. They are advanced K8s scheduling techniques that can help you create flexible scheduling policies.\n\nIn general, affinity enables the Kubernetes scheduler to place a pod either on a group of nodes or a pod relative to the\n\n placement of other pods. To control pod placements on a group of nodes, a user needs to use node affinity rules. In contrast,\n\n pod affinity or pod anti-affinity rules provide the ability to control pod placements relative to other pods.\n\nIn this article, to help you hit the ground running with Kubernetes affinity, we’ll take a closer look at advanced scheduling methods,\n\n deep-dive into the different types of K8s affinity, and provide a node affinity demo so you can get hands-on with Kubernetes advanced scheduling.\n\nBefore we jump into Kubernetes affinity, let’s take a step back and review the different custom pod scheduling techniques\n\n available to us. In the table below, you’ll see a breakdown of the various advanced scheduling methods,\n\n including Node Affinity and Pod Affinity/Anti-Affinity.\n\nNow let’s take a closer look at each technique.\n\nWith Taints, nodes have control over pod placement. Taints allow nodes to define which pods can be placed on them and which pods are repelled away from them.\n\nFor example, suppose you have a node with special hardware and want the scheduler only to deploy pods requiring the special hardware.\n\n You can use Tolerations for the node’s Taints to meet this requirement.\n\nThe pods that require special hardware must define toleration for the Taints on those nodes. With the Toleration, the nodes will allow the pods to run on them.\n\nNodeSlector is the simplest way to schedule a pod on a node. It works by defining a label for a node and then binding the\n\n pod to that node by matching the label in the pod spec. It is a simple approach that works in some cases. However, it is not flexible enough for complex pod scheduling.\n\nNode affinity rules use labels on nodes and label selectors in the pod specification file. Nodes don’t have control over the placement. If the scheduler places a\n\n pod using the node affinity rule and the rule is no longer valid later (e.g., due to a change of labels), the pod will continue to run on that node.\n\nThere are many use cases for node affinity, including placing a pod on a group of nodes with a specific CPU/GPU and placing pods on nodes in a particular availability zone.\n\nThere are two types of Node Affinity rules:\n\nRequired rules must always be met for the scheduler to place the pod. With preferred rules the scheduler will try to enforce the rule, but doesn’t guarantee the enforcement.\n\nPod Affinity and Anti-Affinity enable the creation of rules that control where to place\n\n the pods relative to other pods. A user must label the nodes and use label selectors in pod specifications.\n\nPod Affinity/Anti-Affinity allows a pod to specify an affinity (or anti-affinity) towards a\n\n set of pods. As with Node Affinity, the node does not have control over the placement of the pod.\n\nAffinity rules work based on labels. With an affinity rule, the scheduler can place the pod on\n\n the same node as other pods if the label on the new pod matches the label on the other pod.\n\nAn anti-affinity rule tells the scheduler not to place the new pod on the same node if the label on the\n\n new pod matches the label on another pod. Anti-affinity allows you to keep pods away from each other. Anti-affinity is useful in cases such as:\n\n avoiding placing a pod that will interfere in the performance of an existing pod on the same node.\n\nLike Node Affinity rules, Pod affinity rules also have “Required” and “Preferred” modes. In the “Required” mode, a\n\n rule must be met for the scheduler to place the pod. In “Preferred” mode, scheduling based on the rule is not guaranteed.\n\nNow that you understand Kubernetes affinity let’s jump into our Node Affinity demo.\n\nIn this demo, we’ll go through step-by-step instructions to:\n• Explain how Node Affinity rules work by labeling some nodes in the cluster\n• See that the pods are placed on the nodes with given labels.\n\nFirst let us see the list of nodes in the cluster:\n\nLabel two nodes for node affinity demo, so that the pods can be placed on labeled nodes by affinity rule:\n\nNext, create a namespace for the demo:\n\nNext, create the following deployment file with node affinity in the pod spec:\n\nNow, create the deployment using the YAML file:\n\nNext, see if the deployment was created successfully:\n\nNext, get the list of pods created:\n\nNext, to check if the pod affinity rule worked, get the list of pods on nodes and make sure the pods were placed on nodes with label app=frontend, i.e worker3, worker4 in this case:\n\nLet’s assume we have a Redis cache for web applications and we need to run three replicas of Redis but we need to make\n\n sure that each replica runs on a different node, we make use of pod anti-affinity here.\n\nRedis deployment with pod anti-affinity rule, so that each replica lands on a different node:\n\nCreate the deployment using the yaml file above:\n\nGet the deployment is up\n\nCheck if each pod is running on different node:\n\nWe noticed that each pod is running on a different node.\n\nNext, let’s assume we have a web server running and we need to make sure that each web server pod co-locates\n\n with each Redis cache pod, but at the same time, we need to make sure two web server pods don’t run\n\n on the same node, for this to happen we make use of pod affinity and pod anti-affinity both as below.\n\nCreate the deployment file with affinity and anti-affinity rules:\n\nList the pods along with nodes and check that they are placed as required by rules:\n\nWe noticed that each web pod is co-located with a redis pod, and no two web pods are running on the same node; this was done by using pod affinity and anti-affinity rules.\n\nAffinity and anti-affinity provide flexible ways to schedule pods on nodes or place pods relative to other pods. You can use affinity rules to optimize\n\n the pod placement on worker nodes for performance, fault tolerance, or other complex scheduling requirements."
    },
    {
        "link": "https://docs.openshift.com/container-platform/4.12/nodes/scheduling/nodes-scheduler-pod-affinity.html",
        "document": "We’re taking you to the new home of OpenShift documentation at docs.redhat.com\n\nThis may take a few seconds. If you aren’t redirected automatically,\n\nyou can continue to the new page here."
    },
    {
        "link": "https://blog.kubecost.com/blog/kubernetes-node-affinity",
        "document": "Pod scheduling is one of the most important aspects of Kubernetes cluster management. How pods are distributed across nodes directly impacts performance and resource utilization. Kubernetes node affinity is an advanced scheduling feature that helps administrators optimize the distribution of pods across a cluster.\n\nThis article will review scheduling basics, Kubernetes node affinity and anti-affinity, pod affinity and anti-affinity, and provide practical examples to help you get comfortable using this cluster scheduling feature.\n\nWhat is scheduling in Kubernetes?\n\nKubernetes scheduling is the process of selecting a suitable node to run pods.\n\nTo understand Kubernetes node affinity, you first need to understand the basics of Kubernetes scheduling created to automate the process of pod placement. Kube-scheduler is the default scheduler in K8s, but administrators can use custom schedulers, too.\n\nThe most basic approach to scheduling is through the nodeSelector available in Kubernetes since version 1.0. With nodeSelector, users can define label-key value pairs in nodes and use these labels to match when scheduling pods.\n\nYou can specify the nodeSelector in the PodSpec using a key-value pair. If the key-value pair matches exactly the label defined in the node, the pod will get matched to the specific node. You can use the following command to add labels to the nodes.\n\nThe PodSpec for the nodeSelector is as follows.\n\nUsing the nodeSelector is the recommended way to match pods with nodes for simple use cases in small Kubernetes clusters. However, this method quickly becomes inadequate to facilitate complex use cases and larger K8s clusters. With Kubernetes affinity, administrators gain greater control over the pod scheduling process.\n\nWhat are Kubernetes affinity and anti-affinity?\n\nThe affinity and anti-affinity features in Kubernetes provide administrators with more granular scheduling functionality.\n\nWith affinity and anti-affinity, administrators can:\n• Create “preferred” and “required” rules for a greater variety of matching conditions.\n• Match labels of pods running within nodes and determine the scheduling location of new pods.\n\nThere are two types of affinity: Kubernetes node affinity and Kubernetes pod affinity. Since the naming may be misleading, it’s important to realize that both features are meant from the pod’s perspective. Node affinity attracts pods to nodes, and pod affinity attracts pods to pods.\n\nKubernetes node affinity is a feature that enables administrators to match pods according to the labels on nodes.\n\nIt is similar to nodeSelector but provides more granular control over the selection process. Node affinity enables a conditional approach with logical operators in the matching process, while nodeSelector is limited to looking for exact label key-value pair matches. Node affinity is specified in the PodSpec using the nodeAffinity field in the affinity section.\n\nWhat are pod affinity and anti-affinity?\n\nPod affinity provides administrators with a way to configure the scheduling process using labels configured on the existing pods within a node.\n\nA pod with a label key-value pair matching a condition can be scheduled on a node containing the matching labels.\n\nAs the name suggests, pod anti-affinity simply offers the opposite functionality. It prevents pods from being placed on the same node. For example, it can avoid placing two pods of the same application on the same Kubernetes node to offer redundancy in a failure scenario (more on this later).\n\nPod affinity and anti-affinity are also specified within the affinity section using the podAffinity and podAntiAffinity fields in the PodSpec.\n\nBoth node affinity and pod affinity can set required and preferred rules that act as hard and soft rules. You can configure these rules with the following fields in a pod manifest:\n\nThese rules are valid for both node and pod affinity and can be configured using the respective affinity fields. Below are example rules to demonstrate this configuration method.\n\nThis rule shown below defines the following conditions:\n• For a pod to be placed in a node, the node must have the value “app-worker-node” within the name label indicated by the required rule in the pod manifest.\n• Nodes containing the key type with the value “app-01” are preferred.\n\nThis rule shown below defines the following conditions:\n• A pod will only be scheduled on nodes where an existing pod has the key name with the value “web-app”.\n• Pods that contain the key type and the value “primary” in the matching nodes are preferred.\n• The topologyKey topology.kubernetes.io/zone ensures that the pod is scheduled only if the node is within the same zone as one of the existing pods with the matching key-value pairs. Note that some constraints are applied to the topology due to performance and security considerations found in the Kubernetes documentation.\n• The weight field is used to define the priority of the conditions. Weight can be any value between 1 to 100. Kubernetes scheduler will prefer a node with the highest calculated weight value corresponding to the matching conditions.\n\nOur examples focused on the use case for node affinity which is designed to either attract or assign a pod to a node. The use case for anti-affinity may be less clear at this point. Conceptually, anti-affinity provides similar functionality as taints and tolerations in Kubernetes. Both features prevent pods from being scheduled on specific nodes. The primary difference is that anti-affinity uses matching conditions based on labels while taints are applied to the node and match tolerations defined in pod manifests.\n\nIf you want to stop pods from being scheduled on specific nodes, taints and tolerations are a better option. However, affinity, anti-affinity, taints, and tolerations are not mutually exclusive. You can combine them to facilitate complex scheduling scenarios. For example, suppose you want to reserve nodes in a Kubernetes cluster for an Elastic Search application but not host more than two instances of that particular application on the same node (to protect the application against a single node failure). In that scenario, you would take the following steps:\n• Use taints on the reserved nodes and tolerations on the Elastic Search pods (this prevents any other application from placing pods on your reserved nodes).\n• Use node affinity to assign or attract Elastic Search pods to those nodes (toleration along won’t assign those pods to the nodes so you must use affinity)\n• Use pod anti-affinity to avoid having more than one Elastic Search pod on each node.\n\nAs you can see, the combination provides a new level of dynamic control over pod scheduling.\n\nHow to use affinity in Kubernetes\n\nLet’s review how to utilize affinity in Kubernetes using an example. For this tutorial, we’ll use a Kubernetes cluster created with minikube v1.24.0 and Kubernetes 1.22.3 on Docker 20.10.8 on a Windows 10 environment. You can follow along in any compatible Kubernetes environment.\n\nNode affinity depends on the labels specified on the node.\n\nTo begin, a label to the nodes using the command shown below:\n\nHere, we have labeled the node containing the key name with the value app-worker-node.\n\nYou can see the labels configured on the specific node using the command:\n\nNext, create a manifest that uses node affinity to match the pods with this specific node with the label. The manifest below defines a rule that enforces the given condition. The pod will only get scheduled on a node containing a label with the key name and the value .\n\nWe used the operator in this manifest, but node affinity supports the following operators:\n\nAffinity can be configured using nodeSelectorTerms or matchExpression. The difference is that the pod can get scheduled to a node if any defined conditions match when multiple rules are configured using nodeSelectorTerms. On the other hand, the pod will get scheduled only if all the matchExpression rules are satisfied when using matchExpression.\n\nWith the support of kube-schedule to configure multiple scheduling profiles, you can configure Kubernetes node affinity to provide different affinity rules for the specific scheduler profile. You can do this using the NodeAffinity plugin in the scheduler configuration.\n\nThere are two scheduler profiles in the configuration below: and . We have configured the NodeAffinity plugin to the . Therefore, the added affinity will only be applied to pods that set to . If the pod has its own node affinity configured in the PodSpec, both the rule configured in the PodSpec, and the scheduler-specific rules must match before a pod can be scheduled on a node.\n\nConsider a use case where you want to schedule pods so all the pods on a node relate to the same application. You can do this with pod affinity. In fact, pod affinity is the preferred way to co-locate pods.\n\nYou can create an affinity rule that looks for key-value pairs from the pods currently residing in the node and schedules the pod to the node if a matching key-value pair is found. In the manifest below, pods are only scheduled to a node if there is a pod in the node with the key and the value .\n\nWhile we used the operator here, pod affinity supports the following operations:\n\nNow let’s look at a pod anti-affinity configuration. Suppose you want to schedule exactly one database pod per node. You can meet this requirement using anti-affinity, which will stop pods from getting scheduled if a pod within the node matches the specified rule. In the manifest below, a new pod will not be scheduled on the node if a pod has a key and the value .\n\nBoth pod affinity and anti-affinity rules can be configured in the same pod manifest to customize the scheduling process further. The manifest below consists of an affinity rule and an anti-affinity rule.\n\nThe pod affinity rule ensures that pods will only get scheduled to a node with at least a single pod with the matching key-value pair . Meanwhile, the anti-affinity rule will try to keep pods from getting scheduled on nodes with containers with the key-value pair .\n\nAffinity is one of the key features available in Kubernetes to customize and better control the pod scheduling process. Kubernetes pod and node affinity and anti-affinity rules enable cluster administrators to control where pods are allowed to be scheduled. Specifying multiple rules helps facilitate a wide range of scheduling configurations. Additionally, affinity can be combined with other features such as taints and tolerations to gain even greater control over the scheduling process. As a result, Kubernetes node affinity and anti-affinity and pod affinity and anti-affinity are important tools in a Kubernetes administrator’s toolbox."
    },
    {
        "link": "https://medium.com/@wasiualhasib/optimizing-pod-placement-with-kubernetes-affinity-and-anti-affinity-rules-1940d931f64e",
        "document": "In Kubernetes, affinity and anti-affinity are powerful features used to control how Pods are scheduled and placed within the cluster. These features allow you to set rules that encourage or require certain placement constraints, which Kubernetes scheduler obeys when placing Pods onto nodes.\n\nWhat are Affinity and Anti-affinity?\n\nAffinity: Affinity is used to attract Pods to certain nodes, whether based on the node’s attributes (node affinity) or based on the labels of other Pods running on those nodes (inter-pod affinity). This can ensure that Pods are placed on nodes that meet specific criteria, such as proximity to necessary resources or other Pods they need to communicate with.\n\nAnti-affinity: Anti-affinity, on the other hand, is used to repel Pods from certain nodes. This is useful for spreading Pods across nodes to ensure high availability and resilience, preventing single points of failure.\n\nWhy are They Important?\n\n1. Performance Optimization: Affinity can be used to place Pods on nodes with specific hardware (like GPUs), reducing latency or improving performance.\n\n2. High Availability: By using anti-affinity, you can spread replicas of an application across different nodes or even across different zones, which increases the application’s fault tolerance.\n\n3. Cost Efficiency: In cloud environments, affinity rules can help ensure that Pods are placed on reserved or spot instances to reduce costs.\n\n4. Legal and Regulatory Compliance: Affinity rules can ensure that data and applications reside on physically appropriate locations to meet legal or corporate policies.\n\nHow to Use Affinity and Anti-affinity\n\nAffinity and anti-affinity rules are defined in the Pod’s spec under the `affinity` field. There are two types of affinities:\n\n1. Node Affinity: Attracts Pods to nodes with specific labels.\n\n2. Pod Affinity and Anti-affinity: Attracts or repels Pods from nodes based on the labels of other Pods that are already running on those nodes.\n\nExample: Node Affinity\n\nHere’s an example of using node affinity to schedule a Pod only on nodes that have a label `disktype: ssd`, ensuring the Pod runs on a node with an SSD drive for performance reasons.\n\nExample: Pod Anti-affinity\n\nThis example demonstrates using pod anti-affinity to ensure that no two nginx Pods are placed on the same node, enhancing high availability.\n\nKey Parameters in Affinity and Anti-affinity\n\n- `requiredDuringSchedulingIgnoredDuringExecution`: The rule must be met for the Pod to be scheduled on a node. If not possible, the Pod will not be scheduled.\n\n- `preferredDuringSchedulingIgnoredDuringExecution`: The scheduler will try to enforce the rules but will not guarantee it. If the rule cannot be met, the Pod will still be scheduled.\n\n- `topologyKey`: Used in pod affinity/anti-affinity to describe the scope in which to evaluate the label selectors. Commonly, this is set to `”kubernetes.io/hostname”` to indicate that the rule should apply per-node.\n\nAffinity in Kubernetes is used to decide where to place Pods, based on specific rules that attract them to certain nodes or other Pods. This can be for reasons such as performance optimization, proximity to needed services, or specific hardware requirements. Pod Anti-affinity is used to ensure high availability by spreading Pods across different nodes or failure domains. This prevents a single node failure from affecting all instances or replicas of an application, thereby enhancing the resilience and reliability of your applications.\n\nThese settings help Kubernetes make smart scheduling decisions to optimize the performance and reliability of the applications running in your cluster.\n\nConclusion\n\nUsing affinity and anti-affinity helps optimize the distribution of workloads across a Kubernetes cluster, ensuring that Pods are scheduled where they can perform best or where they meet specific operational or compliance requirements. These features are critical for managing complex applications at scale in production environments."
    }
]