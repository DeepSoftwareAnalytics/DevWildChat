[
    {
        "link": "https://man7.org/linux/man-pages/man2/mmap.2.html",
        "document": "Pages that refer to this page: memusage(1), alloc_hugepages(2), arch_prctl(2), clone(2), execve(2), fcntl(2), fork(2), futex(2), get_mempolicy(2), getpagesize(2), getrlimit(2), ioctl_userfaultfd(2), io_uring_register(2), io_uring_setup(2), madvise(2), mbind(2), memfd_create(2), memfd_secret(2), mincore(2), mlock(2), mmap2(2), mprotect(2), mremap(2), msync(2), open(2), perf_event_open(2), personality(2), posix_fadvise(2), PR_SET_MM_ARG_START(2const), PR_SET_MM_START_CODE(2const), PR_SET_TAGGED_ADDR_CTRL(2const), readahead(2), remap_file_pages(2), seccomp(2), sendfile(2), set_mempolicy(2), shmget(2), shmop(2), statx(2), syscalls(2), UFFDIO_API(2const), uselib(2), userfaultfd(2), vfork(2), avc_init(3), avc_open(3), cap_launch(3), fopen(3), io_uring_queue_exit(3), io_uring_queue_init(3), io_uring_queue_init_mem(3), io_uring_queue_init_params(3), mallinfo(3), malloc(3), malloc_stats(3), mallopt(3), numa(3), off_t(3type), pthread_attr_setguardsize(3), pthread_attr_setstack(3), selinux_status_open(3), sem_init(3), shm_open(3), core(5), proc(5), proc_meminfo(5), proc_pid_map_files(5), proc_pid_maps(5), proc_sys_kernel(5), proc_sys_vm(5), systemd.exec(5), tmpfs(5), capabilities(7), fanotify(7), file-hierarchy(7), futex(7), inode(7), inotify(7), io_uring(7), pkeys(7), shm_overview(7), spufs(7), ld.so(8), netsniff-ng(8), setarch(8), trafgen(8), xfs_io(8)"
    },
    {
        "link": "https://en.wikipedia.org/wiki/Mmap",
        "document": "In computing, is a POSIX-compliant Unix system call that maps files or devices into memory. It is a method of memory-mapped file I/O. It implements demand paging because file contents are not immediately read from disk and initially use no physical RAM at all. The actual reads from disk are performed after a specific location is accessed, in a lazy manner. After the mapping is no longer needed, the pointers must be unmapped with . Protection information—for example, marking mapped regions as executable—can be managed using , and special treatment can be enforced using .\n\nIn Linux, macOS and the BSDs, can create several types of mappings. Other operating systems may only support a subset of these; for example, shared mappings may not be practical in an operating system without a global VFS or I/O cache.\n\nThe original design of memory-mapped files came from the TOPS-20 operating system. and associated systems calls were designed as part of the Berkeley Software Distribution (BSD) version of Unix. Their API was already described in the 4.2BSD System Manual, even though it was neither implemented in that release, nor in 4.3BSD.[1] Sun Microsystems had implemented this very API, though, in their SunOS operating system. The BSD developers at University of California, Berkeley unsuccessfully requested Sun to donate its implementation; 4.3BSD-Reno was instead shipped with an implementation based on the virtual memory system of Mach.[2]\n\nFile-backed mapping maps an area of the process's virtual memory to files; that is, reading those areas of memory causes the file to be read. It is the default mapping type.\n\nAnonymous mapping maps an area of the process's virtual memory not backed by any file, made available via the / flags. The contents are initialized to zero.[3] In this respect an anonymous mapping is similar to malloc, and is used in some malloc implementations for certain allocations, particularly large ones.\n\nIf the mapping is shared (the flag is set), then it is preserved when a process is forked (using a system call). Therefore, writes to a mapped area in one process are immediately visible in all related (parent, child or sibling) processes. If the mapping is shared and backed by a file (not ) the underlying file medium is only guaranteed to be written after it is passed to the system call. In contrast, if the mapping is private (the flag is set), the changes will neither be seen by other processes nor written to the file.\n\nA process reading from, or writing to, the underlying file will not always see the same data as a different process that has mapped the file, since segments of the file are copied into RAM and only periodically flushed to disk. Synchronization can be forced with a call to .\n\nUsing mmap on files can significantly reduce memory overhead for applications accessing the same file; they can share the memory area the file encompasses, instead of loading the file for each application that wants access to it. This means that mmap(2) is sometimes used for Interprocess Communication (IPC). On modern operating systems, mmap(2) is typically preferred to the System V IPC Shared Memory facility.[4]\n\nThe main difference between System V shared memory (shmem) and memory mapped I/O (mmap) is that System V shared memory is persistent: unless explicitly removed by a process, it is kept in memory and remains available until the system is shut down. mmap'd memory is not persistent between application executions (unless it is backed by a file).\n\n/* This example shows how an mmap of /dev/zero is equivalent to using anonymous memory (MAP_ANON) not connected to any file. N.B. MAP_ANONYMOUS or MAP_ANON are supported by most UNIX /* Does not work on OS X or macOS, where you can't mmap over /dev/zero */\n\nThe mmap system call has been used in various database implementations as an alternative for implementing a buffer pool, although this created a different set of problems that could realistically only be fixed using a buffer pool.[5]\n• Virtual memory for when there is more address space than physical memory\n• Paging for the implementation of virtual memory\n• Windows\n• MapViewOfFile win32 function is somewhat equivalent to mmap."
    },
    {
        "link": "https://programmingappliedai.substack.com/p/what-is-mmap-in-linux-and-how-it",
        "document": "This allows a program to directly access file data as if it were part of the program’s memory, bypassing explicit read and write system calls.\n\nInstead of copying file contents between disk and memory buffers, provides a way to map the file directly into memory, enabling efficient I/O operations.\n• None\n• None When a file is mapped using , the operating system loads the file's contents into memory (if not already in memory).\n• None The program accesses the file content via memory pointers, and any changes are either immediately or lazily written back to the file, depending on the configuration.\n• None\n• None In Linux and Unix-like systems, the system call is used:\n• None\n• None\n• None : Starting address of the mapped area (usually , letting the OS choose).\n• None : File descriptor of the file to be mapped.\n• None : Offset in the file from where mapping begins.\n• None\n• None When done, the memory region should be unmapped with to release resources:\n• None\n• None Accessing file contents through memory pointers is faster than traditional I/O operations.\n• None\n• None Accessing files becomes as simple as manipulating arrays in memory, eliminating the need for repetitive read/write operations.\n• None\n• None Pages of the file are loaded into memory only when accessed, saving memory and reducing startup time for large files.\n• None\n• None With the flag, multiple processes can share the same memory-mapped file, making it useful for inter-process communication (IPC).\n• None\n• None Changes made in memory can be automatically reflected in the file (if is used), simplifying file synchronization.\n• None\n• None Databases like SQLite and LMDB use to map database files into memory, enabling efficient access and updates.\n• None\n• None Operating systems use to load executables and shared libraries into memory on demand.\n• None\n• None For analyzing large datasets, allows efficient loading and traversal without manually managing buffer reads.\n• None\n• None Accessing large media files like videos and images benefits from 's on-demand page loading. import mmap # Open a file and map it into memory with open('example.txt', 'r+b') as f: # Memory-map the file mmapped_file = mmap.mmap(f.fileno(), 0) # Access the file like a byte array print(mmapped_file[:10]) # Read the first 10 bytes mmapped_file.close()\n• None\n• None For extremely large files, the addressable space may be limited by available memory or system address space (especially on 32-bit systems).\n• None\n• None Improperly handled errors during file access or memory writes (e.g., segmentation faults) can crash the application.\n• None\n• None Accessing unmapped pages triggers page faults, which may introduce latency if the file is not already in memory.\n\nWhy is Useful in System Design\n• None\n• None Wide-column and document-oriented databases often use for fast read/write access and efficient storage management.\n• None Example: MongoDB used for its storage engine before WiredTiger.\n• None\n• None In messaging systems like Apache Kafka, can help map log segments into memory, reducing read overhead.\n• None\n• None For streaming platforms like YouTube or Netflix, is used to access large video files efficiently, enabling adaptive bitrate streaming.\n• None\n• None Useful in analytics systems for efficient traversal of massive datasets without loading entire files into memory.\n\nis a powerful tool for efficient file I/O, enabling direct access to file data in memory. It shines in scenarios requiring high performance, low latency, and shared access to data. While its use introduces complexity in handling, the benefits in terms of performance and resource optimization make it a key technique in systems like databases, analytics platforms, and multimedia applications."
    },
    {
        "link": "https://comp.anu.edu.au/courses/comp2310/labs/07-mmio",
        "document": "In this week’s lab, you will:\n• Learn the system calls for file I/O in Linux\n• Learn the concept of standard I/O streams, and learn about file I/O utilities in the C standard I/O library\n• Understand the typical uses and advantages of file streams\n• Understand the uses and advantages of memory mapped I/O and the mmap() system call\n• Practice the uses of file I/O and mmap() by writing some interesting C programs\n\nClone the lab pack 3 repo from here.\n\nInput/Output (I/O): This specific tutorial covers file input/output (I/O) in C. You will learn how to read from and write to files stored on a storage device such as a disk drive. A file is a versatile abstraction on Unix-like systems. Many objects (including regular files containing user data, devices, and pipes for inter-process communication) are represented as files. We are concerned in this tutorial with the so-called . A regular file contains bytes of data organized into a linear array called a byte stream. Linux does not impose any other structure on the files beyond the byte stream. Bytes may have value and represent anything from students’ marks to photos.\n\nToday, we will do basic file operations such as open, close, read, and write. We will also learn two ways of performing read/write I/O on modern computer systems. The first method uses system calls for I/O (affectionately known as ). The second method is (or for short). It uses the virtual memory abstraction and the operating system’s page faulting mechanism for performing I/O. The former is sometimes called because it explicitly uses system calls for reading and writing files. Mmio is also called because, from the programmer’s perspective, there is no distinction between accessing regular memory (i.e., allocated via malloc) and disk-resident files. In implicit I/O, both malloc’ed memory and files can be accessed via pointers, and pointer arithmetic works for both. Today, you will learn to perform this magic!\n\nThis first exercise is a revision exercise for how string handling works in C. You first looked at implementing a few string utilities all the way back in Lab 1. Some of these functions are already included in the C standard library, but it’s good to practice doing them on your own!\n\nWe will use the most basic method of accessing files via and system calls. Before a file can be read or written it must be opened via the system call. Once the program is done using the file, it must be closed via the system call.\n\nA file is opened via the system call.\n\nThe system call returns a file descriptor (an integer) to make it easy for the program to do operations on the file. The filepath and its name are only needed for opening a file. The operating system kernel maintains a per-process list of open files, called the file table. This table is indexed via file descriptors (a.k.a., fds). An fds has an int type in C. Each Linux process has a maximum number of files that it may open. File descriptors start at 0.\n\nAs an example, you can open a file somewhere in your home directory as follows:\n\nNote that we have opened the file for reading only by specifying the flag. We can also open the file for writing only with flag. Finally, we can open the file for both reading and writing with the flag. You can see the full list of with the command. You should try to read about the flags: , , , and .\n\nIf there is an error in opening a file, the system call returns -1.\n\nReading data from the file is done via the system call:\n\nThe system call reads the number of bytes specified by into a buffer set up by the program from an open file with descriptor (or -1 on error). The buffer must be large enough to read the required number of bytes. On success, the number of bytes written into is returned. The file position is advanced by the number of bytes read from .\n\nThere are two problems with the above example.\n• The system call may return with less than bytes. We will not delve into the full list of reasons for that scenario here, but common reasons are:\n• not enough bytes (i.e., equal to ) are available\n• the system call is rudely interrupted for some reason.\n• There is a possibility of return value of 0 when using . A return value of 0 means the end-of-file (EOF) is reached.\n\nNote that we need to handle the above two cases. We also need to handle any potential errors. The following code solves these problems:\n\nThe loop reads bytes from the current file position of into , which should be bytes in length. It continues reading until it reads all bytes, or until is reached. Note the adjustments to and in the case more than zero, but less than bytes are read. prints a message on standard error.\n\nWhat’s the deal with errno (again)?#\n\nThe C library provides support for detecting and reacting to errors in the execution of system calls. Below is a description of from the Linux manual page.\n\nIn our example code, means that a signal was received before any bytes were read and the system call can be reissued.\n\nAs with reads, the most basic usage of the system call is simple (see below). We leave it to you to check for possible occurrence of partial writes and errors.\n\nAfter a program has finished working with a file descriptor, it can unmap (close) the file descriptor from the associated file via the system call.\n\nNow that you know how to use the , , and system calls to perform I/O on files, this is a simple warm-up exercise to practice using them. The currently incomplete program provided in will take two file names as command line arguments. The first of which is the source file and the second the destination file. The function which checks whether the program has been given the correct number of arguments has been written for you.\n\nYou will need to complete the function so that:\n• It opens both files using . Make sure the appropriate flags are set for both calls — the source file should be open in read-only mode and the destination file in write-only mode. If either call to fails, use the function to print an error and exit with return code .\n• After opening both files, the system call should be used to read one character at a time in a loop from . The return value of this function should be stored in the variable .\n• After each call to the value of should be checked. If the end-of-file has been reached, the function should return. If an error has occurred, an error message should be printed using and the program should exit.\n• Otherwise, the single byte read should then be written to the destination file using the system call. Store the return value in and check this value for errors as well.\n\nAnother system call that will be useful when performing I/O is . The system call returns a number of different pieces of information about the status of the file from the given file descriptor.\n\nThere is also the similar system call. The only difference to is that takes the name of the file as its first argument rather than a file descriptor open to that file.\n\nEach open file has an associated meta-data called the file or . When the file is first opened, the file position is zero. Each read and write operation starts at a specific offset (current position). As bytes in the file are read from or written to (e.g., via read() and write() syscalls), the file offset advances accordingly.Typically, I/O occurs linearly through a file, and the file position updates automatically. Some applications may want to jump around (seek) in a file, accessing random locations. The system call sets the file position of a file descriptor to a given value. It performs no other action and initiates no I/O.\n\nStorage devices such as disks and solid-state drives are much slower than byte-addressable main memory (DRAM). The operating system kernel maintains an in-memory data structure called the that stores (caches) recently accessed files from disk. The page cache allows the kernel to fulfill subsequent read requests for the file content from memory, avoiding repeated disk accesses. Therefore, when you issue the system call in the above examples, the operating system first transfers file data from the disk into the page cache. It then copies data from the page cache to the user-specified buffer in the application memory. We call the data structure page cache because a page is the basic unit of memory allocation inside the kernel.\n\nThe kernel also optimizes write I/O operations by buffering them in the page cache. Imagine a program issuing a batch of small writes (e.g., 32 bytes). If the kernel accesses the slow disk on every write, then the system operates very inefficiently. Indeed, the kernel performs write buffering. When a program issues a write request, the data is copied into an in-memory buffer, and the buffer is marked dirty, denoting that the in-memory copy is newer than the disk-resident copy. Subsequent writes hit in the buffer, avoiding disk accesses again. At some point, the kernel decides to the dirty buffer(s) to disk. In modern Linux kernels, the page cache and the buffer subsystem have been unified as a single structure (page cache). Before Linux kernel 2.4, there was both a page cache and a buffer cache.\n\nIn summary, syscall I/O results in an extra copy from the kernel space to the user space (in addition to disk transfer). We refer to buffering in the page cache as kernel-level buffering.\n\nTo understand the motivation behind file streams, we need to briefly visit block devices and filesystem blocks.\n\nWe consider storage devices, such as disks and solid-state drives (SSDs) block devices. The smallest addressable unit on a block device is the sector. We can access any (random) block. The sector is a physical attribute of the device (set at manufacturing time). Historically, disks have a typical sector size of 512 bytes. All I/O operations on a block device must occur at the granularity of one or more sectors. The other type of I/O device is the character device. A character device delivers or accepts a stream of characters. A keyboard is an example of a character device. If the user types “ ,”, for example, an application would want to read from the keyboard device the , the , and finally the , in exactly that order. A block structure would make little sense and reading/writing characters in any order would also make little sense. Other examples of character devices include rats (for pointing), printers, and network interfaces.\n\nA filesystem is a collection of files and directories organized into a hierarchy. Filesystems (that exist on top of block devices) use the abstraction of blocks for transferring file data to and from storage devices. The smallest addressable unit on a filesystem is a block. Blocks are power-of-two multiple of sector sizes. The most common block size on Linux is 4 KB. Inside the kernel, all I/O operations happen at the granularity of blocks. If an application requests 32 bytes, the kernel must transfer a block from the underlying storage (block) device. The same is true for writes. Everything with regards to storage I/O happens at the block granularity.\n\nIf an application performs partial block operations, i.e., reading and writing less than the block size, this results in an inefficiency. The operating system needs to do extra work to make things happen at the block boundary. On the other hand, user-space applications operate in terms of high-level abstractions, such as, structs and arrays and strings, whose size is typically much smaller than the block.\n\nThere is one more issue with small I/O requests, e.g., requesting a single byte from a device. Each such request results in a system call, and too many system calls are bad for performance and slow down applications. Programs that need to issue many small I/O requests typically resort to . This type of buffering refers to buffering done in the user space, either manually by the programmer, or transparently in a library. Note that this buffering is on top of the buffering done by the kernel in the page cache. The two serve different purposes!\n\nHere, we will explore a platform-independent user-buffering solution provided by the standard C I/O library (called ). This means that C library routines internally perform buffering of user requests, and only invoke the kernel via system calls for reading/writing blocks. The result is a reduction in the frequency of system calls. The downside of user-level buffering is that there are two copies in addition to the disk transfer. One from the page cache to the standard library buffer and the second from the standard I/O library buffer to the buffer provided by the user (programmer).\n\nStandard I/O library routines do not work directly with file descriptors. They instead use file pointers which are represented by the object defined in .\n\nFiles are opened for reading or writing via :\n\nThis function opens the file with the behaviour given by and associates a new stream with it. The argument describes how to open the give file. The argument can be specified as one of the following strings (reproduced from the man page):\n\nOnce a file stream is opened, you can read from a stream via and write to a stream via .\n\nThe standard I/O library provides a number of powerful routines for manipulating file data via streams. You should explore the full set of routines here.\n\nIn the first program, a new file is created. We declare a structure with three numbers - , and , and define it in the main function as num. Inside the for loop we write the value to the file using .\n\nThe first parameter takes the address of and the second parameter takes the size of the structure . Since we’re only inserting one instance of , the third parameter is 1. The last parameter points to the file we’re writing to. Finally, we close the file.\n\nIn the second program, we read from the same file and loop through the records one by one. In each iteration of the for loop we read bytes and store them at the address of . We print the values at each iteration.\n\nThe program provided as in your lab repository builds a linked list of student records in memory. The program contains an empty function . The argument is the file name that the data should be written to. Using file stream I/O, write the and to this file by completing the definition of the function. This process of writing in-memory data structures to a storage-resident file after eliminating needless meta-data (such as pointers between student records) is called .\n\nThis will be very similar to the code in , so make sure you read through and understand that first.\n\nThe inverse of serialization is . The file contains the same record struct definition and functions for inserting and printing items in the linked list. You will need to implement the function such that it reads serialized data from the given file and populates the linked list pointed to by . This will be similar to the code.\n\nWe will now discuss a second way to perform I/O! This alternative to standard file I/O is to map a file into memory, establishing a one-to-one correspondence between a memory address and a byte in a file. The programmer can then access the file directly through memory, similar to any other memory-resident data. The ability to map disk-backed files into memory is truly phenomenal. It allows direct computation over files via load/store instructions. Let’s see how we can map a storage-backed file into memory!\n\nYou should already be familiar with , as it is what you used in your first assignment. Here we will focus using to read from and write to files.\n\nLinux implements the POSIX-standard system call for mapping files into memory. is a system call that can be used by a user process to ask the operating system kernel to a file into the memory (i.e., address space) of that process. The system call can also be used to allocate memory (an anonymous mapping), which was the subject of the first assignment. It is important to remember that the mapped pages are not actually brought into physical memory until they are referenced. Therefore, lazily loads pages into memory (a.k.a., ).\n\nThe following figure illustrates the use of the mmap() system call:\n\nHere is the function prototype for :\n\nYou are (hopefully) already familiar with some of these arguments, but we will explain them here for your benefit anyway:\n• - This argument is a hint to the operating system kernel to use this address at which the virtual mapping should start in the virtual memory (i.e. the virtual address space) of the process. The value can be specified as to indicate that the kernel can place the virtual mapping anywhere it sees fit. If not , then should be a multiple of the page size.\n• - This argument specifies the length as number of bytes for the mapping. This length should be a multiple of the page size.\n• - The protection for the mapped memory. The value of is the bitwise or of various of the following single-bit values:\n• - Enable the contents of the mapped memory to be readable by the process.\n• - Enable the contents of the mapped memory to be writable by the process.\n• - Enable the contents of the mapped memory to be executable by the process as CPU machine instructions.\n• - Various options controlling the mapping. Some of the more common values are:\n• - Allocate anonymous memory; the pages are not backed by any file.\n• - The default setting; it need not be specified. The mapped region is backed by a regular file.\n• - Don’t interpret addr as a hint: place the mapping at exactly that address, which must be a multiple of the page size.\n• - Modifications to the mapped memory region are not visible to other processes mapping the same file.\n• - Modifications to the mapped memory region are visible to other processes mapping the same file and are eventually reflected in the file.\n• - The open file descriptor for the file from which to populate the memory region. If MAP_ANONYMOUS is specified, then is ignored.\n• - If this is not an anonymous mapping, the memory mapped region will be populated with data starting at position offset bytes from the beginning of the file open as file descriptor . Should be a multiple of the page size.\n\nOn success, returns a pointer to the mapped area in virtual memory. On error, the value is returned and is set to indicate the reason. You can find full details on the system call by using the command.\n\nis a system call used to unmap memory previously mapped with . The call removes the mapping for the memory from the address space of the calling process process:\n\nThere are two arguments to the munmap() system call:\n• - The address of the memory to unmap from the calling process’s virtual mapping. Should be a multiple of the page size.\n• - The length of the memory (number of bytes) to unmap from the calling process’s virtual mapping. Should be a multiple of the page size.\n\nOn success () returns 0, on failure -1 and is set to indicate the reason. If successful, future accesses to the unmapped memory area will result in a segmentation fault ( ).\n\nThere are three primary advantages of using to access files on disk.\n• One advantage of using is . If no memory within a certain page is ever referenced, then that page is never loaded into physical memory. This can be crucial in certain applications in terms of saving both memory and time. In standard file I/O (or syscall-based I/O), programmers may end up bringing file data that is never used.\n• On modern systems, memory mapped I/O is typically faster compared to syscall-based I/O especially for large files. There are at least two sources of :\n• Traditional I/O involves a lot of system calls (e.g., calls to read()) to load data into memory. We have seen in the lectures that there are costs associated with systems calls, including switching from user to kernel mode, and such costs as error checking within the functions themselves.\n• Loading data into main memory also has to go through several layers of software abstraction, with the data being copied around in various buffers in the operating system before finally being placed in memory, which clearly will slow down the program. In fact, in the case of user-buffered I/O, there are two copies, one from the kernel page cache to the C library buffer, and another one from the C library buffer to the programmer’s buffer in the application code. Using avoids the overheads due to extra copying.\n• Programmers find it convenient to manipulate files using pointers and pointer arithmetic due to its versatility. Also, there is no need for programmers to maintain additional user-level buffers as the operating system manages the buffers in the page cache and returns a pointer to the user.\n\nLet’s see an example of memory-mapped I/O and do some exercises.\n\nLook at the program in your repository. This program demonstrates reading data from a file using . This program calls the system call to open its own source code file (the file ) and then gets the size (in bytes) of this file using the system call. The program then uses to map that file into the process’s memory, and then prints out the contents of the file from the memory into which the file has been mapped.\n\nUnfortunately, there are two bugs in the program! Attempting to run this program as-is will cause it to crash.\n\nIn the file we provide an incomplete implementation of a program very similar to the one from Exercise 2. This will take two a source file name and a destination file name as arguments. The contents of the source file will be written to the destination file. Instead of using and , fast copy should use and .\n\nFill in the rest of the code so that file copying works. You can confirm if things are working properly by using the Unix/Linux command ( will provide the manual page).\n\nThe program provided in the repository maps a file into the program’s virtual memory. It maps the file with the flag set and both read and write permissions enabled.\n\nWrite a program that reads lines of text from a file on disk and builds a data structure in memory. This data structure contains all the unique words encountered so far, and how many times each occurred. The data structure appears in memory as follows:\n\nUse the C library function to read one line of text at a time. Write helper functions to handle splitting lines into separate words, inserting them into the linked list or updating the count of an existing word. Make sure you take proper care of buffer size for storing the newly read tweet, and for detecting the end of word, end of line, and end of file."
    },
    {
        "link": "https://reddit.com/r/embeddedlinux/comments/s4kpgz/what_is_the_purpose_of_mmap_and_munmap_system",
        "document": "Create your account and connect with a world of communities.\n\nBy continuing, you agree to our\n\nand acknowledge that you understand the"
    },
    {
        "link": "https://stackoverflow.com/questions/41103881/how-to-handle-memory-management-via-mmap-properly",
        "document": "I'm trying to write my own and implementation for the sake of learning, with just and (since and are obsoletes). I've read a fair amount of documentation on the subject, but every example I see either use or doesn't explain very well how to handle large zones of mapped memory.\n\nThe idea of what I'm trying to write is this: I first map a big zone (i.e. 512 pages); this zone will contains all allocations between 1 and 992 bytes, in 16 bytes increments. I'll do the same later with a 4096 pages zone for bigger allocations (or mmap directly if the requested size is bigger than a page). So I need a way to store informations about every chunk that I allocate or free.\n\nMy question is, how do I handle these informations properly ?\n\nMy problematics are: If I create a linked list, how do I allocate more space for each node ? Or do I need to copy it to the mapped zone ? If so, how can I juggle between data space and reserved space ? Or is it better to use a static sized array ? Problem with this is that my zone's size depends on the page size."
    },
    {
        "link": "https://design-reuse.com/articles/25090/dynamic-memory-allocation-fragmentation-c.html",
        "document": "In C and C++, it can be very convenient to allocate and de-allocate blocks of memory as and when needed. This is certainly standard practice in both languages and almost unavoidable in C++. However, the handling of such dynamic memory can be problematic and inefficient. For desktop applications, where memory is freely available, these difficulties can be ignored. For embedded - generally real time - applications, ignoring the issues is not an option.\n\nDynamic memory allocation tends to be nondeterministic; the time taken to allocate memory may not be predictable and the memory pool may become fragmented, resulting in unexpected allocation failures. In this session the problems will be outlined in detail and an approach to deterministic dynamic memory allocation detailed.\n\nIt may be useful to think in terms of data memory in C and C++ as being divided into three separate spaces:\n\nStatic memory. This is where variables, which are defined outside of functions, are located. The keyword static does not generally affect where such variables are located; it specifies their scope to be local to the current module. Variables that are defined inside of a function, which are explicitly declared static, are also stored in static memory. Commonly, static memory is located at the beginning of the RAM area. The actual allocation of addresses to variables is performed by the embedded software development toolkit: a collaboration between the compiler and the linker. Normally, program sections are used to control placement, but more advanced techniques, like Fine Grain Allocation, give more control. Commonly, all the remaining memory, which is not used for static storage, is used to constitute the dynamic storage area, which accommodates the other two memory spaces.\n\nAutomatic variables. Variables defined inside a function, which are not declared static, are automatic. There is a keyword to explicitly declare such a variable – auto – but it is almost never used. Automatic variables (and function parameters) are usually stored on the stack. The stack is normally located using the linker. The end of the dynamic storage area is typically used for the stack. Compiler optimizations may result in variables being stored in registers for part or all of their lifetimes; this may also be suggested by using the keyword register.\n\nThe heap. The remainder of the dynamic storage area is commonly allocated to the heap, from which application programs may dynamically allocate memory, as required.\n\nIn C, dynamic memory is allocated from the heap using some standard library functions. The two key dynamic memory functions are malloc() and free().\n\nThe malloc() function takes a single parameter, which is the size of the requested memory area in bytes. It returns a pointer to the allocated memory. If the allocation fails, it returns NULL. The prototype for the standard library function is like this:\n\nThe free() function takes the pointer returned by malloc() and de-allocates the memory. No indication of success or failure is returned. The function prototype is like this:\n\nTo illustrate the use of these functions, here is some code to statically define an array and set the fourth element’s value:\n\nThe following code does the same job using dynamic memory allocation:\n\nThe pointer de-referencing syntax is hard to read, so normal array referencing syntax may be used, as [ and ] are just operators:\n\nWhen the array is no longer needed, the memory may be de-allocated thus:\n\nAssigning NULL to the pointer is not compulsory, but is good practice, as it will cause an error to be generated if the pointer is erroneous utilized after the memory has been de-allocated.\n\nThe amount of heap space actually allocated by malloc() is normally one word larger than that requested. The additional word is used to hold the size of the allocation and is for later use by free(). This “size word” precedes the data area to which malloc() returns a pointer.\n\nThere are two other variants of the malloc() function: calloc() and realloc().\n\nThe calloc() function does basically the same job as malloc(), except that it takes two parameters – the number of array elements and the size of each element – instead of a single parameter (which is the product of these two values). The allocated memory is also initialized to zeros. Here is the prototype:\n\nThe realloc() function resizes a memory allocation previously made by malloc(). It takes as parameters a pointer to the memory area and the new size that is required. If the size is reduced, data may be lost. If the size is increased and the function is unable to extend the existing allocation, it will automatically allocate a new memory area and copy data across. In any case, it returns a pointer to the allocated memory. Here is the prototype:\n\nManagement of dynamic memory in C++ is quite similar to C in most respects. Although the library functions are likely to be available, C++ has two additional operators – new and delete – which enable code to be written more clearly, succinctly and flexibly, with less likelihood of errors. The new operator can be used in three ways:\n\np_var = new typename;\n\n p_var = new type(initializer);\n\n p_array = new type [size];\n\nIn the first two cases, space for a single object is allocated; the second one includes initialization. The third case is the mechanism for allocating space for an array of objects.\n\nThe delete operator can be invoked in two ways:\n\nThe first is for a single object; the second deallocates the space used by an array. It is very important to use the correct de-allocator in each case.\n\nThere is no operator that provides the functionality of the C realloc() function.\n\nHere is the code to dynamically allocate an array and initialize the fourth element:\n\nUsing the array access notation is natural. De-allocation is performed thus:\n\nAgain, assigning NULL to the pointer after deallocation is just good programming practice. Another option for managing dynamic memory in C++ is the use the Standard Template Library. This may be inadvisable for real time embedded systems.\n\nAs a general rule, dynamic behavior is troublesome in real time embedded systems. The two key areas of concern are determination of the action to be taken on resource exhaustion and nondeterministic execution performance.\n\nThere are a number of problems with dynamic memory allocation in a real time system. The standard library functions (malloc() and free()) are not normally reentrant, which would be problematic in a multithreaded application. If the source code is available, this should be straightforward to rectify by locking resources using RTOS facilities (like a semaphore). A more intractable problem is associated with the performance of malloc(). Its behavior is unpredictable, as the time it takes to allocate memory is extremely variable. Such nondeterministic behavior is intolerable in real time systems.\n\nWithout great care, it is easy to introduce memory leaks into application code implemented using malloc() and free(). This is caused by memory being allocated and never being deallocated. Such errors tend to cause a gradual performance degradation and eventual failure. This type of bug can be very hard to locate.\n\nMemory allocation failure is a concern. Unlike a desktop application, most embedded systems do not have the opportunity to pop up a dialog and discuss options with the user. Often, resetting is the only option, which is unattractive. If allocation failures are encountered during testing, care must be taken with diagnosing their cause. It may be that there is simply insufficient memory available – this suggests various courses of action. However, it may be that there is sufficient memory, but not available in one contiguous chunk that can satisfy the allocation request. This situation is called memory fragmentation.\n\nThe best way to understand memory fragmentation is to look at an example. For this example, it is assumed hat there is a 10K heap. First, an area of 3K is requested, thus:\n\nThen, a further 4K is requested:\n\n3K of memory is now free.\n\nSome time later, the first memory allocation, pointed to by p1, is de-allocated:\n\nThis leaves 6K of memory free in two 3K chunks. A further request for a 4K allocation is issued:\n\nThis results in a failure – NULL is returned into p1 – because, even though 6K of memory is available, there is not a 4K contiguous block available. This is memory fragmentation.\n\nIt would seem that an obvious solution would be to de-fragment the memory, merging the two 3K blocks to make a single one of 6K. However, this is not possible because it would entail moving the 4K block to which p2 points. Moving it would change its address, so any code that has taken a copy of the pointer would then be broken. In other languages (such as Visual Basic, Java and C#), there are defragmentation (or “garbage collection”) facilities. This is only possible because these languages do not support direct pointers, so moving the data has no adverse effect upon application code. This defragmentation may occur when a memory allocation fails or there may be a periodic garbage collection process that is run. In either case, this would severely compromise real time performance and determinism.\n\nA real time operating system may provide a service which is effectively a reentrant form of malloc(). However, it is unlikely that this facility would be deterministic.\n\nMemory management facilities that are compatible with real time requirements – i.e. they are deterministic – are usually provided. This is most commonly a scheme which allocates blocks – or “partitions” – of memory under the control of the OS.\n\nTypically, block memory allocation is performed using a “partition pool”, which is defined statically or dynamically and configured to contain a specified number of blocks of a specified fixed size. For Nucleus OS, the API call to define a partition pool has the following prototype:\n\nThis is most clearly understood by means of an example:\n\nThis creates a partition pool with the descriptor MyPool, containing 2000 bytes of memory, filled with partitions of size 40 bytes (i.e. there are 50 partitions). The pool is located at address 0xB000. The pool is configured such that, if a task attempts to allocate a block, when there are none available, and it requests to be suspended on the allocation API call, suspended tasks will be woken up in a first-in, first-out order. The other option would have been task priority order.\n\nAnother API call is available to request allocation of a partition. Here is an example using Nucleus OS:\n\nThis requests the allocation of a partition from MyPool. When successful, a pointer to the allocated block is returned in ptr. If no memory is available, the task is suspended, because NU_SUSPEND was specified; other options, which may have been selected, would have been to suspend with a timeout or to simply return with an error.\n\nWhen the partition is no longer required, it may be de-allocated thus:\n\nIf a task of higher priority was suspended pending availability of a partition, it would now be run. There is no possibility for fragmentation, as only fixed size blocks are available. The only failure mode is true resource exhaustion, which may be controlled and contained using task suspend, as shown.\n\nAdditional API calls are available which can provide the application code with information about the status of the partition pool – for example, how many free partitions are currently available. Care is required in allocating and de-allocating partitions, as the possibility for the introduction of memory leaks remains.\n\nThe potential for programmer error resulting in a memory leak when using partition pools is recognized by vendors of real time operating systems. Typically, a profiler tool is available which assists with the location and rectification of such bugs.\n\nHaving identified a number of problems with dynamic memory behavior in real time systems, some possible solutions and better approaches can be proposed.\n\nIt is possible to use partition memory allocation to implement malloc() in a robust and deterministic fashion. The idea is to define a series of partition pools with block sizes in a geometric progression; e.g. 32, 64, 128, 256 bytes. A malloc() function may be written to deterministically select the correct pool to provide enough space for a given allocation request. This approach takes advantage of the deterministic behavior of the partition allocation API call, the robust error handling (e.g. task suspend) and the immunity from fragmentation offered by block memory.\n\nC and C++ use memory in various ways, both static and dynamic. Dynamic memory includes stack and heap.\n\nDynamic behavior in embedded real time systems is generally a source of concern, as it tends to be non-deterministic and failure is hard to contain.\n\nUsing the facilities provided by most real time operating systems, a dynamic memory facility may be implemented which is deterministic, immune from fragmentation and with good error handling."
    },
    {
        "link": "https://reddit.com/r/AskProgramming/comments/qe2nxe/what_are_some_examples_of_how_cc_memory",
        "document": "Just did an online course to learn c so I know basic syntax, how to use pointers etc. Did the classic example of creating a linked list using structs+pointers. But you could have done the same thing in python, all the pointers did is allow functions to edit variables without returning them (which of course other languages can do). There's also malloc but again, it's not like other languages don't dynamically allocate memory for variables/arrays right?\n\nWould they be needed if you had limited RAM, on a microcontroller? In which case, what would be an actual example of using them to be more efficient with memory?\n\nto put it more directly, what's something you can do with c that you can't do with python?"
    },
    {
        "link": "https://stackoverflow.com/questions/10218374/use-cases-of-mmap",
        "document": "mmap takes memory management out of the hands of the programmer to a large extent, and puts it in the hands of the OS.\n\nIt's about demand paging using the virtual memory subsystem from disk to physical memory.\n\nSo to look at the 11111th byte of a file, instead of seeking and reading, you can mmap and use an array index. The OS will keep surroundiung data in its \"buffer cache\" (page cache really).\n\nThe example's a little messy because it was written at a time when Linux had mmap support in its kernel, but the C library didn't yet have a stub for calling it. But you can pretty much ignore mmap.c. The example uses mmap to set pixels on and off using a monochromatic display adapter.\n\nAnother reasonable use is for a bloom filter: http://stromberg.dnsalias.org/~strombrg/drs-bloom-filter/\n\n...but on 32 bit OS's, the maximum size of an mmap'd memory region kinda hurts."
    },
    {
        "link": "https://softwareverify.com/blog/memory-fragmentation-your-worst-nightmare",
        "document": "We first know when someone has a memory fragmentation problem when we receive an email that goes something like this:\n\nAt this point, we ask a few questions and make sure it’s not just a problem with holding onto data until the last minute before freeing it which is causing these reports. Once we’ve ruled that out, we mention the dreaded F word. Fragmentation.\n\nOver the last 11 years, we’ve written many emails to people explaining memory fragmentation and what you can do to mitigate it. I thought we should put this information out there for you all to benefit from. As usual, if you have anything to add, any extra techniques or insights, or constructive criticism, please add a comment or email support.\n\nBut I’m working with Linux, or an embedded system\n\nThe code examples and the test application (see below) are written for Microsoft Windows.\n\nBut, the general principles and the mitigation techniques apply to your software regardless of Operating System. As such, please keep reading. If you’re using Linux, the test application to generate memory fragmentation should run under Wine.\n\nBecause memory fragmentation takes various forms ranging from very subtle to downright blatant in-your-face fail I’ve created a test application you can use to generate on-demand memory fragmentation. You can then play with this application and the suggested tools and techniques to understand fragmentation analysis. And you can analyse the code for the test application to see just how easy it is to create memory fragmentation by using careless allocation strategies.\n\nLearn more about the mvFragmentation memory fragmentation test application.\n\nMemory fragmentation is when the sum of the available space in a memory heap is large enough to satisfy a memory allocation request but the size of any individual fragment (or contiguous fragments) is too small to satisfy that memory allocation request. This probably sounds confusing, so I will illustrate how this situation can arise.\n\nTo simplify the explanation let us consider a simple computer system that can only allocate 8 chunks of memory, each 1KB in size.\n\nAt the program start, the program hasn’t allocated any memory. The memory landscape looks like this:\n\nThe application allocates 1KB, 4KB, and 2KB. The memory landscape looks like this:\n\nThe program then deallocates 4KB, 1KB. The memory landscape now looks like this:\n\nThe program needs to do some other work before repeating its task. It allocates 1KB to store some data during the next task. The memory landscape now looks like this:\n\nNow the first task needs to repeat. The application wants to allocate the 1KB, 4KB workspace that is used last time around (and deallocated at the end of the task). The program allocates 1KB. The memory landscape looks like this:\n\nNow the program wants to allocate 4KB. But although there is 4KB of free space, it is not contiguous. The memory is fragmented.\n\nThis is a simplified example. Because I’ve used power-of-two memory allocation sizes this example probably doesn’t exist in real life because many memory allocators use power-of-two sizing to place different allocations in different memory bins. But in terms of demonstrating what causes memory fragmentation, this example demonstrates it perfectly.\n\nThe previous section provided an overview of what memory fragmentation is. Now we investigate some of the causes of memory fragmentation.\n\nMany memory allocators return memory blocks aligned on specific memory boundaries. For example, aligned with the width of the computer architecture pointer size. For example on a 32-bit chip, the alignment would be 4 bytes; on a 64-bit chip, the alignment would be 8 bytes.\n\nSome allocators allow the allocation requestor to specify the alignment of the allocation being made.\n\nWhatever the reason for the alignment it stands to reason that to satisfy that alignment sometimes there will be unused (or wasted) space immediately prior to the base address of the memory allocation. This wasted space will be after an earlier memory allocation. The space will often be too small for the memory allocator to use to satisfy a memory request.\n\nFor the memory heap to manage itself, the heap must also use memory. For some heaps, the heap uses memory in a separate space from the heap itself. An example of this is the release mode Microsoft C Runtime heap. In other heaps, the heap uses the heap memory to manage itself. An example of this is the debug mode Microsoft Runtime heap. When heap management is done in the heap each allocation adds an overhead for the amount of memory required to satisfy the allocation and to manage the allocation. This overhead increases the size of the allocation and may result in the total size required not matching the allocator’s perfect allocation size, resulting in wasted space.\n\nRelated to heap workspace is heap guard space. This is typically found in debug mode heaps. The guard space is a block of a few bytes before the block and after the block. The guard space is filled with a known value that can be checked at any time to see if it has been modified. This is typically used to detect buffer overruns. The guard space increases the size of the allocation required and may result in the total size required not matching the allocator’s perfect allocation size, resulting in wasted space.\n\nEach memory allocator has its own strategy for deciding how to allocate memory and provide it to the software calling the allocator. Each allocator strategy will be optimal for some types of usage and less useful or even dangerous in other circumstances. A common strategy is to allocate memory in bins, each bin being of a particular size and each size being twice that of the previous bin.\n\nAllocations that fit into a particular bin but which cannot be served by a smaller bin are served by that bin. Bins may be created ahead of time or on a just-in-time basis. If a bin does not exist to satisfy a strategy a new bin will be created. This new bin will be twice the size of the previous bin. Repeat until you get to the bin size you need. This has been shown to be quite a useful strategy. The problem is that this strategy can allocate from bins that are much larger than is required to satisfy an allocation, thus leaving large chunks of memory unused.\n\nEven though you may be using an allocator with a great pedigree and superb performance, at the end of the day the allocator can only do so much when presented with a particular sequence of allocations, reallocations and deallocations by an application. The behaviour of your software and the memory allocation characteristics it exhibits can contribute greatly to the lack of memory fragmentation your application experiences or it can cause the very memory fragmentation problems that are causing you to lose your hair.\n\nAs such although most people don’t pay much attention to memory fragmentation because you often do not need to, you need to be aware of what memory fragmentation is, what causes it and how to mitigate memory fragmentation on the (hopefully) few occasions you encounter it.\n\nAlthough at the start of this article I said we try to rule out memory leaks as a possible cause you cannot discount them. The cause of the fragmentation could be something as simple as failing to deallocate a small allocation that if deallocated would allow a much larger allocation to be allocated at the same address each time through a computation loop.\n\nAn example of this would be a process that each time through its loop (say serving a web page) allocates 1MB to do some work, then 1 byte, then deallocates the 1MB but does not deallocate the 1 byte. Each 1-byte allocation will be locking up a region of memory larger than 1 byte (the bin size from which that 1 byte was allocated). Over time this will eventually turn into a significant memory leak, but before you get to that point there will be a lot of memory fragmentation caused by the wasted memory associated with each 1-byte leak.\n\nBecause of this, you must always consider memory leaks before you start thinking about changing your memory allocator or using private/custom heaps. Fixing any memory leak is:\n• A lot cheaper than changing your code to accommodate alternate memory allocation strategies (see later for details).\n\nThe Win32 function VirtualAlloc() can be used to allocate new blocks of committed memory. When this happens a new block that is large enough to satisfy the memory requirement is allocated. These blocks are allocated in a multiple of the minimum allocation size. The minimum allocation size is found by calling GetSystemInfo() and examining the returned dwAllocationGranularity value. This is typically 64KB.\n\nFor most calls to VirtualAlloc() that allocate new chunks of memory, the requested size will be less than the allocated size, resulting in the allocated block and a smaller block that comes after the allocated block. The smaller block’s creation is implicit. If the caller of VirtualAlloc() doesn’t know about this, they will be accidentally creating wasted regions in the memory space because although the memory is usable, there is no way to find the wasted regions address unless you calculate it at the time of the original block allocation. This is best explained with an example:\n\nIf you commit a 24KB block of memory with VirtualAlloc():\n\nThe first block is pointed to by the pointer returned from VirtualAlloc().\n\nThe second block isn’t pointed to by anything. You can calculate where it is if you know about the allocation and the size of the allocation.\n\nThe odd wasted block is of not much concern (there is one after every DLL), but wasted blocks that result from treating VirtuaAlloc() like a regular heap will cause memory fragmentation. The best solution is to use a regular heap to provide these allocations, or to create a custom heap to manage the allocations from a VirtualAlloc() backed heap.\n\nWe’ve written an in-depth exploration of how VirtualAlloc() can cause wasted memory, plus example mitigation techniques.\n\nWasted blocks can be identified by VM Validator and VMMap (they are called “unknown” in VMMap).\n\nMemory allocation lifetime also plays a part in memory fragmentation. Objects with short lifetimes only occupy space in the heap for a short period of time. As such their effect on fragmentation is minimal. But objects that live for a long period (or forever in the case of leaked memory), prevent the larger space of free memory around them from forming a contiguous free memory region that could satisfy a memory request.\n\nSolutions to this problem are to where possible allocate all long-lived objects in their own heap so that they do not affect the fragmentation of other heaps. If you can’t do this, try to allocate the long-lived objects before any other objects so that they (hopefully) get allocated at one end of the heap or the other (implementation dependent).\n\nIs fragmentation affected by the amount of memory in my computer?\n\nThe amount of memory in your computer will not affect whether you suffer memory fragmentation. Memory fragmentation is caused by a combination of the allocation strategy used by the allocator you are using, the sizes and alignments of the internal structures, combined with the memory allocation behaviour of your software application.\n\nThat said the more memory you have the longer it will be before you feel the effects of memory fragmentation. That isn’t necessarily a good thing. The sooner you know about it the sooner you can fix it.\n\nConversely, if you don’t have a lot of memory you may not experience memory fragmentation because your program doesn’t have enough workspace to get into a situation where memory fragmentation is an issue. Given the memory that most modern PCs have these days, I doubt this situation will be facing you.\n\nMemory fragmentation affects all computer programs that use a dynamic memory allocator that does not use garbage collection (or similar mechanisms) to remove memory fragmentation by compacting the memory heap.\n\nIt is important to note that some garbage-collected allocators have a Large Object Heap which is used to handle large memory allocations. Examples of this are the Microsoft .Net Runtime and Java. These Large Object Heaps are not compacted. As a result, even these garbage collected heaps can suffer memory fragmentation, but only for large objects. What constitutes “large” is implementation dependent. For .Net, “large” means 85,000 bytes.\n\nSystems that do not use dynamic memory allocators do not suffer from memory fragmentation. Examples of these are many small embedded systems. Although an embedded system in the late 1980s was an 8-bit 6801 with 64KB of RAM programmed in assembler, whereas now it’s a 32-bit ARM with 256MB RAM and a C compiler. So today, it’s quite possible your embedded system is at risk from memory fragmentation whereas the devices I worked on 25 years ago were not at risk.\n\nWhen is fragmentation more likely to be a problem?\n\nFragmentation is more likely to be a problem when your application makes a series of allocations and deallocations such that each time an allocation is made it cannot re-use space that was left by a previous deallocation of a similar (or larger) size block.\n\nOr put another way if you have a large range of widely differing memory sizes in your program’s memory allocation behaviour you probably stand a higher chance of suffering from memory fragmentation than if all your memory allocations are of similar sizes.\n\nHow can I detect if my program is suffering from fragmentation?\n\nThere are telltale signs that your program may be suffering from fragmentation:\n• One sign of memory fragmentation is that your program may start to run a lot slower. This is because the allocator has to spend more time searching for a suitable place to put each memory allocation. You’ll notice this for applications that have a very subtle form of memory fragmentation which only wastes small amounts of memory for each fragment.\n• Another sign of memory fragmentation is that some memory allocations fail but most memory allocations succeed. Yet when you examine the amount of memory used by your program there always seems to be enough memory to satisfy even the memory allocation calls that failed. The type of fragmentation that wastes large amounts of memory and prevent large allocations from happening – these programs tend to run at full speed and then just fail to allocate memory. Much less subtle, but easier to identify the problem.\n• If you are using a custom heap and have access to some heap diagnostics then you can perform the following calculation to determine if that heap is fragmented. Find the largest free block size in the heap (not a block that is in use). Find the total free space in the heap. If the largest free block size is small compared to the total free size then you probably have a fragmentation problem. What defines “small”? Well, that is for you to decide based on your understanding of the application you are working on. No absolute values. Sorry.\n\nThere are several methods you can use to detect memory fragmentation. These all involve the use of free tools and/or commercial tools. Firstly we need to establish that the software does not suffer from any memory leaks and also does not suffer from any resource (handle) leaks. You can do this with your favourite memory leak tool, for example, Memory Validator.\n\nOnce you know there are no leaks occurring when you run the software we can turn our attention to the memory allocation behaviour of the software. We can inspect this using various tools. These are listed in the order they were created.\n\nTask Manager can be used for identifying trends in memory usage. Both the graphical display and the various memory counters can help you.\n\nThere are various memory-related counters:\n\nThe counters you are interested in are Private Working Set and Memory Commit Size.\n\nThe other counters may be increasing, or decreasing, but they are irrelevant. We are concerned about ever-increasing application memory use. As such we want to know the private amount of memory in use – the memory that is not shared with other applications. The commit size also shows you the amount of memory in use instead of being reserved for possible use. Another counter that also reflects the total memory size of the process is Virtual Memory Size (VM Size).\n\nIf these values continue to increase but your memory leak tool shows that you have no memory leaks then your application is almost certainly suffering from memory fragmentation.\n\nVM Validator is a free software tool for visualising virtual memory. We wrote this tool over 20 years ago so that it was easy to visualize memory fragmentation problems that would cause memory allocation failures when allocating large blocks of memory. Using the virtual view you can watch your application’s memory usage. This is particularly useful when you watch what happens when you load a large image (satellite photo), do some work, unload it, do some work, and then load another image. If you are suffering from fragmentation, you can see the image doesn’t reload in the same place each time. VM Validator provides a view of the page fault behaviour of your software and the following three views which will be useful for investigating memory fragmentation.\n• Summary view. Examine the Wasted statistic in the Virtual Memory tile. Click on the Wasted bar to view the statistic on the Pages view.\n• Virtual view. A graphical view of virtual memory. Using this view you can watch your application’s memory usage. This is particularly useful when you watch what happens when you load a large image (satellite photo), do some work, unload it, do some work, and then load another image. If you are suffering from fragmentation, you can see the image doesn’t reload in the same place each time.\n• Pages view. A breakdown of memory pages by memory region. Change the Type filter to Private, then scan all columns looking at the Description column, looking for “Free (Wasted)”. Ignore all the entries immediately after a DLL. Anything that remains may be wasted memory causing memory fragmentation.\n• Paragraphs view. A breakdown of memory paragraphs by memory region. Memory Paragraphs are the minimum size (64KB) allocated by VirtualAlloc(). Change the Type filter to Private, then scan all columns looking at the Description column, looking for “Free (Wasted)”. Ignore all the entries immediately after a DLL. Anything that remains may be wasted memory causing memory fragmentation.\n\nMemory Validator is our memory leak detection tool. Memory Validator also has a similar view to the VM Validator tool. This view is the virtual view and shows:\n• Virtual view. A graphical view of virtual memory. Using this view you can watch your application’s memory usage. This is particularly useful when you watch what happens when you load a large image (satellite photo), do some work, unload it, do some work, and then load another image. If you are suffering fragmentation you can see the image doesn’t reload in the same place each time.\n• Paragraphs view. A breakdown of memory paragraphs by memory region. Memory Paragraphs are the minimum size (64KB) allocated by VirtualAlloc().\n• Sandbar view. A visualization of the memory collected by Memory Validator so that you can see the memory gaps (or sandbars) between each currently active object – this is the Pages view (not the Pages subtab on the Virtual view).\n\nProcess Explorer from SysInternals can also be used to monitor memory. If you go to the View menu then choose Select Columns… then go to the Process Memory tab you can select which values you want to view. Selecting Virtual Size allows you to see the total size of your application’s virtual memory. You can save these values for later use by going to the View menu and then choosing Save Column Sets….\n\nDouble-clicking a graph will display the resource monitor so that you can inspect the data more clearly.\n\nVMMap is another SysInternals tool that shows you the virtual memory map of your application. This is similar to VM Validator but very different in appearance. You can use it in a similar way to how we described above.\n\nVMMap also has a “fragmentation view” which you can access from the View menu. This is similar to the VM Validator Virtual view.\n\nIt’s almost impossible to prevent memory fragmentation before seeing it because it is a function of your application’s behaviour. However, once you’ve ruled out memory and resource leaks and established that memory fragmentation is the problem then there are various tactics and strategies you can use to mitigate the memory fragmentation.\n\nYou’re no doubt familiar with the phrase that the worst type of performance optimisation is premature optimisation. This is also true of memory fragmentation. Do not try to guess ahead of time which parts of your program will cause fragmentation and which parts won’t. You almost certainly won’t get it right. This will mean wasted effort on custom heaps for areas that don’t need it. And most likely a more complex implementation than required. Much better to write your software, then observe its behaviour and address the behaviour you find, if you need to.\n\nThere are a variety of different approaches that can be taken to mitigate memory fragmentation. You can use each approach on its own or in conjunction with other approaches listed here. None of these approaches is mutually exclusive.\n\nThe Windows Low Fragmentation Heap (LFH) was introduced with Windows XP. It was also backported to Windows 2000 SP4 although I doubt many of you reading this will still be working on Windows 2000, although many of you are still working on Windows XP (after all, your customers still are!).\n\nThe LFH can be enabled or disabled using HeapSetInformation.\n\nNote that you cannot enable the LFH for heaps that have the HEAP_NO_SERIALIZE flag set.\n\nProbably the easiest and simplest approach to take is to try swapping out the memory manager for a different memory manager. There are commercial and open-source heap managers available. Commercial:\n• Cherrystone’s Extensible Scalable Allocator (ESA). I think Cherrystone are out of business. The link we had no longer works.\n• MicroQuill’s SmartHeap. The link was broken (under maintenance) the last time we checked. http://www.microquill.com/smartheap/index.html\n• Hoard memory allocator. GPL and commercial licences are available.\n\nI’m not saying that you should try one of these allocators. I have no idea how simple or complex it is to replace your allocator with another. But if it is simple to replace, then trying another allocator to see if that allocator handles your application’s memory allocation behaviour such that your memory fragmentation problems are solved. That may be a good, effective use of your time.\n\nYou could try writing your own heap manager to reduce memory fragmentation. But I don’t recommend it. This is a non-trivial task (even if it seems trivial at first glance) if you want to have good CPU performance, good memory performance, and good robustness and good allocation strategy. There are companies whose entire business model provides high-performance heap managers. If a business can be built on this, you can bet it’s not a trivial job.\n\nThat said, if you can find a special edge case (as we have, see Linear Heap below), then writing your own custom heap manager can be very helpful.\n\nRather than just use malloc, new etc to allocate in the C runtime heap you could choose to do all allocations for specific objects in a specific heap created by using HeapCreate(); This is useful because it forces all allocations of a specific size and type into one heap. Thus the allocation behaviour that was causing fragmentation in one heap is now split among many heaps and may not cause fragmentation when split like that.\n\nThen when you are at a suitable point where you can destroy the heap you can do that, and then re-create the heap effectively setting fragmentation for that heap to zero.\n\nThis is a variation of the previous topic. You override operator new and operator delete to place different object types in different heaps. There are many ways you can set this up. This is a simple example where you set the heap for the class using a static function. Derive all other classes for this heap from this base class.\n\nReduce the number of allocations and deallocations\n\nIf you can reduce the number of memory allocations and memory deallocations you are reducing the chance for fragmentation to occur. As such anything you can do to reduce how often you allocate or deallocate memory will usually help. From this stems the concept of memory pools and reuse.\n\nIf you have commonly used chunks of memory of the same size that are allocated and deallocated frequently then you may be better off reusing the allocated memory rather than deallocating it and then reallocating it. This places less stress on the memory allocator, is faster and reduces fragmentation.\n\nIf you are reusing a large number of memory allocations you’ll probably need to have a manager class for each group of allocations so that you can ask for a new object to work with. We do this as part of our communications buffer handling in our software tools.\n\nAnother variation on reducing the number of memory allocations and deallocations is to reuse objects. This reduces fragmentation. There are a few ways to reuse objects. You can simply reuse the object you have. To do this you may reinitialise it by copying a different object to it, or you may call a method to reset the object. We’ve seen cases of people calling the object destructor to destroy the object contents – this works because they don’t call delete, thus the memory is not deallocated.\n\nProbably not the most common practice you’ll see. We prefer to implement a dedicated reset() / flush() method which resets the object. We typically call that from the destructor.\n\nIf you are reusing a large number of objects you’ll probably need to have a manager class for each group of objects so that you can ask for a new object to work with. We do this as part of our communications buffer handling in our software tools.\n\nSometimes it’s better to plan ahead and allocate all the objects ahead of time. These objects then live in a pool. When an object is needed the code asks the pool manager for an object. The object is used. When the object is no longer needed it is given back to the pool manager. The same strategy can be applied to memory chunks of given sizes.\n\nThis can be particularly effective if you allocate all the objects or memory blocks in one allocation and then divide that allocation into the appropriate number of memory blocks or objects. This allows no scope for fragmentation within the large allocation.\n\nIf you are using custom heaps to store data of a particular type if you can completely destroy the heap at a particularly opportune moment and then recreate it then you can effectively set the heap fragmentation to zero for that heap. Good opportunities for this are when you close a document or when data queues get empty.\n\nYou can use what we call a linear heap to provide a zero fragmentation heap. A linear heap can however only be used in a restricted set of circumstances.\n\nA linear heap is a memory heap that allows you to dynamically allocate memory with the proviso that you must deallocate memory in the order it was allocated. Memory cannot be reallocated, expanded or compacted in place (no support for realloc() or __expand()). These restrictions mean that the heap can contain many allocations and each allocation sits immediately after the previous allocation. There is never any gap between the end of one allocation and the start of another allocation (except for alignment purposes). Deallocations simply remove the data from the start of the heap. The heap is split into pages. A page is created when the current page is full and cannot hold any new allocations. As memory is deallocated from a heap page the page holds fewer data until eventually, it holds no data. When a heap page is empty it is discarded to either the free list for reuse or it is decommitted back to the operating system for reuse.\n\nThis type of heap is very fast to use as it doesn’t need to think about the best fit, find an unused block that’s the right size or any of the other housekeeping tasks that most memory allocators have to do. The heap also doesn’t use any of the power of 2 or other strategies to manage memory. Memory is simply allocated in a linear fashion, marching through the memory space the heap is using. When that space is exhausted more is requested and the same procedure is followed. The heap never suffers from memory fragmentation.\n\nWe use linear heaps in all our inter-process communications queues. Memory Validator in particular puts quite a stress on the communications queues due to the fact it can queue up to 1 million items before switching to synchronous communications. One of our customers runs tests that monitor multiple billions of events over several days. Part of what allows that to happen is despite the wide variety of data sizes (many of which are defined by the data in the customer’s application) our monitoring software does not suffer from memory fragmentation in these key high-use components.\n\nSo far as we are aware the linear heap is our own invention. We haven’t heard of anyone using them before.\n\nIf you can intern various objects such that for each use of the object a single instance can be used this can prevent fragmentation caused by the creation and destruction of many instances of such objects.\n\nExample: A classic case for interning is the use of strings. Consider that you have an application that needs to process a large number of strings but the application does not know the content of the strings but the application does know that any duplicates can be reduced to a single copy. A good example would be a debugging symbol handler. You may have 100 classes but the full symbol name for each method is className::methodName so className can be interned. What about the method names? These can also be interned so that any references to the method name are only stored once.\n\nThere are some useful side effects of this technique:\n• You can easily store these interned objects in their own heap allowing you to deallocate all objects just by destroying the heap, reducing any fragmentation in that heap to zero.\n\nWe use a variant of this technique to manage the symbols in our software tools.\n\nIf you are using VirtualAlloc() to allocate large blocks of memory (for loading data into or for implementing a custom heap) it may be worth trying the MEM_TOP_DOWN_FLAG to force VirtualAlloc() to allocate blocks at the top of the address space. This means the addresses of any VirtualAlloc’d allocations will not be near any allocations made by the C runtime or HeapAlloc() etc. This could prove to be quite useful in many situations for preventing memory fragmentation.\n\nCaveat. Depending on the behaviour of your program using VirtualAlloc() with the MEM_TOP_DOWN flag may not be a good idea – it could cause things to be much slower. Read this informative blog posting before proceeding. Summary: If using VirtualAlloc() with MEM_TOP_DOWN a little bit that’s OK, but using it to make a lot of allocations in a short amount of time, could be very slow.\n\nTo inform your decision for the above-mentioned strategies and tactics you could also examine the number of allocations and objects of different sizes to try to identify any commonality in allocation sizes. You could also try to identify the application hotspots – places where the application performs the most of its allocations and see if you can then optimise these to use object/memory pools or if you can reuse a memory/object allocation rather than deallocating it and then reallocating it later.\n\nWe don’t know of any tools that can do this apart from our memory tool Memory Validator.\n• The types tab will give you the breakdown of the number of objects of each type allocated.\n• The sizes tab will give you the same information for each memory allocation of a particular size (this data includes object sizes).\n• The locations tab will give you the same information for each memory allocation at a particular filename and line number.\n• The hotspots tab, if you set it to display All Allocations will show you a hierarchical allocation tree showing you the hotspot locations for allocations, reallocations and deallocations. This allows you to identify which functions are allocating the most objects and the call stack for that allocation.\n\nOnce you know this information you make much more informed decisions about which objects/allocations should have their own private heap space, which ones should be in memory pools and which ones should be left alone.\n\nIs it possible to guarantee zero fragmentation?\n\nThe only way to guarantee zero fragmentation is to either write your software in a language (or style) that does not use dynamic memory allocation or to use an appropriate technique to mitigate any fragmentation you may experience. By far the best technique is to destroy each heap when you get an opportunity to do so. This resets fragmentation for the memory controlled by the heap to zero.\n\nWhat about .Net – can that suffer from fragmentation?\n\nYes. The .Net Large Object Heap (LOH) can suffer from fragmentation because the LOH is never compacted after a garbage collection.\n\nAlso in the regular .Net heap pinned objects cannot be moved. Objects that cannot be moved prevent the heap from being compacted in the most optimal manner. Depending on how your objects are pinned this could cause quite bad fragmentation of the .Net heap.\n\nIf you do need to pin objects you may want to think about moving those objects into the native heap and then using the techniques in this article to ensure they all end up in the same place using an object pool etc. This would move the pinned objects out of the .Net heap and allow .Net heap compaction to proceed as normal.\n\nHow can I prevent fragmentation in the Large Object Heap for C#?\n\nWith the .Net Large Object Heap (LOH) it really depends on what data you’ve got in the LOH as to what you can do to mitigate the memory fragmentation.\n\nYou should definitely consider object reuse and object pooling (as mentioned above).\n\nArrays of type double with 1000 objects or more are placed on the Large Object Heap. Try to keep all your double arrays smaller than 1000 items.\n\nObjects 85,000 bytes or larger are placed on the Large Object Heap. Arrays can easily exceed 85,000 bytes so you should be careful about creating arrays with more than 10,000 items. Alternative arrangements that split one large array into several smaller arrays that are managed by a parent object that provides an array-style interface would prevent these arrays from entering the Large Object Heap as each individual array would be below the threshold for entering the Large Object Heap.\n\nStarting with .Net 4.51 Preview there is a special option to force the Large Object Heap to compact itself. This is not automatic but controlled by the software engineer via an API call.\n\nI hope you now have a better understanding of the cause of memory fragmentation and what you can do to improve any memory fragmentation issues you may be facing. If you can use a linear heap it’s an excellent, high-speed solution. If you can’t then look at drop-in replacement heaps or assigning objects to specific custom heaps, object reuse, memory reuse and pooling."
    }
]