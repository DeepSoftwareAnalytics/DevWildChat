[
    {
        "link": "https://postgresql.org/docs/current/sql-createtable.html",
        "document": "The optional clause specifies a list of tables from which the new table automatically inherits all columns. Parent tables can be plain tables or foreign tables. Use of creates a persistent relationship between the new child table and its parent table(s). Schema modifications to the parent(s) normally propagate to children as well, and by default the data of the child table is included in scans of the parent(s). If the same column name exists in more than one parent table, an error is reported unless the data types of the columns match in each of the parent tables. If there is no conflict, then the duplicate columns are merged to form a single column in the new table. If the column name list of the new table contains a column name that is also inherited, the data type must likewise match the inherited column(s), and the column definitions are merged into one. If the new table explicitly specifies a default value for the column, this default overrides any defaults from inherited declarations of the column. Otherwise, any parents that specify default values for the column must all specify the same default, or an error will be reported. constraints are merged in essentially the same way as columns: if multiple parent tables and/or the new table definition contain identically-named constraints, these constraints must all have the same check expression, or an error will be reported. Constraints having the same name and expression will be merged into one copy. A constraint marked in a parent will not be considered. Notice that an unnamed constraint in the new table will never be merged, since a unique name will always be chosen for it. Column settings are also copied from parent tables. If a column in the parent table is an identity column, that property is not inherited. A column in the child table can be declared identity column if desired.\n\nCreates the table as a partition of the specified parent table. The table can be created either as a partition for specific values using or as a default partition using . Any indexes, constraints and user-defined row-level triggers that exist in the parent table are cloned on the new partition. The must correspond to the partitioning method and partition key of the parent table, and must not overlap with any existing partition of that parent. The form with is used for list partitioning, the form with and is used for range partitioning, and the form with is used for hash partitioning. is any variable-free expression (subqueries, window functions, aggregate functions, and set-returning functions are not allowed). Its data type must match the data type of the corresponding partition key column. The expression is evaluated once at table creation time, so it can even contain volatile expressions such as . When creating a list partition, can be specified to signify that the partition allows the partition key column to be null. However, there cannot be more than one such list partition for a given parent table. cannot be specified for range partitions. When creating a range partition, the lower bound specified with is an inclusive bound, whereas the upper bound specified with is an exclusive bound. That is, the values specified in the list are valid values of the corresponding partition key columns for this partition, whereas those in the list are not. Note that this statement must be understood according to the rules of row-wise comparison (Section 9.25.5). For example, given , a partition bound allows with any , with any non-null , and with any . The special values and may be used when creating a range partition to indicate that there is no lower or upper bound on the column's value. For example, a partition defined using allows any values less than 10, and a partition defined using allows any values greater than or equal to 10. When creating a range partition involving more than one column, it can also make sense to use as part of the lower bound, and as part of the upper bound. For example, a partition defined using allows any rows where the first partition key column is greater than 0 and less than or equal to 10. Similarly, a partition defined using allows any rows where the first partition key column starts with \"a\". Note that if or is used for one column of a partitioning bound, the same value must be used for all subsequent columns. For example, is not a valid bound; you should write . Also note that some element types, such as , have a notion of \"infinity\", which is just another value that can be stored. This is different from and , which are not real values that can be stored, but rather they are ways of saying that the value is unbounded. can be thought of as being greater than any other value, including \"infinity\" and as being less than any other value, including \"minus infinity\". Thus the range is not an empty range; it allows precisely one value to be stored — \"infinity\". If is specified, the table will be created as the default partition of the parent table. This option is not available for hash-partitioned tables. A partition key value not fitting into any other partition of the given parent will be routed to the default partition. When a table has an existing partition and a new partition is added to it, the default partition must be scanned to verify that it does not contain any rows which properly belong in the new partition. If the default partition contains a large number of rows, this may be slow. The scan will be skipped if the default partition is a foreign table or if it has a constraint which proves that it cannot contain rows which should be placed in the new partition. When creating a hash partition, a modulus and remainder must be specified. The modulus must be a positive integer, and the remainder must be a non-negative integer less than the modulus. Typically, when initially setting up a hash-partitioned table, you should choose a modulus equal to the number of partitions and assign every table the same modulus and a different remainder (see examples, below). However, it is not required that every partition have the same modulus, only that every modulus which occurs among the partitions of a hash-partitioned table is a factor of the next larger modulus. This allows the number of partitions to be increased incrementally without needing to move all the data at once. For example, suppose you have a hash-partitioned table with 8 partitions, each of which has modulus 8, but find it necessary to increase the number of partitions to 16. You can detach one of the modulus-8 partitions, create two new modulus-16 partitions covering the same portion of the key space (one with a remainder equal to the remainder of the detached partition, and the other with a remainder equal to that value plus 8), and repopulate them with data. You can then repeat this -- perhaps at a later time -- for each modulus-8 partition until none remain. While this may still involve a large amount of data movement at each step, it is still better than having to create a whole new table and move all the data at once. A partition must have the same column names and types as the partitioned table to which it belongs. Modifications to the column names or types of a partitioned table will automatically propagate to all partitions. constraints will be inherited automatically by every partition, but an individual partition may specify additional constraints; additional constraints with the same name and condition as in the parent will be merged with the parent constraint. Defaults may be specified separately for each partition. But note that a partition's default value is not applied when inserting a tuple through a partitioned table. Rows inserted into a partitioned table will be automatically routed to the correct partition. If no suitable partition exists, an error will occur. Operations such as which normally affect a table and all of its inheritance children will cascade to all partitions, but may also be performed on an individual partition. Note that creating a partition using requires taking an lock on the parent partitioned table. Likewise, dropping a partition with requires taking an lock on the parent table. It is possible to use to perform these operations with a weaker lock, thus reducing interference with concurrent operations on the partitioned table.\n\nThe clause specifies a table from which the new table automatically copies all column names, their data types, and their not-null constraints. Unlike , the new table and original table are completely decoupled after creation is complete. Changes to the original table will not be applied to the new table, and it is not possible to include data of the new table in scans of the original table. Also unlike , columns and constraints copied by are not merged with similarly named columns and constraints. If the same name is specified explicitly or in another clause, an error is signaled. The optional clauses specify which additional properties of the original table to copy. Specifying copies the property, specifying omits the property. is the default. If multiple specifications are made for the same kind of object, the last one is used. The available options are: Comments for the copied columns, constraints, and indexes will be copied. The default behavior is to exclude comments, resulting in the copied columns and constraints in the new table having no comments. Compression method of the columns will be copied. The default behavior is to exclude compression methods, resulting in columns having the default compression method. constraints will be copied. No distinction is made between column constraints and table constraints. Not-null constraints are always copied to the new table. Default expressions for the copied column definitions will be copied. Otherwise, default expressions are not copied, resulting in the copied columns in the new table having null defaults. Note that copying defaults that call database-modification functions, such as , may create a functional linkage between the original and new tables. Any generation expressions of copied column definitions will be copied. By default, new columns will be regular base columns. Any identity specifications of copied column definitions will be copied. A new sequence is created for each identity column of the new table, separate from the sequences associated with the old table. Indexes, , , and constraints on the original table will be created on the new table. Names for the new indexes and constraints are chosen according to the default rules, regardless of how the originals were named. (This behavior avoids possible duplicate-name failures for the new indexes.) Extended statistics are copied to the new table. settings for the copied column definitions will be copied. The default behavior is to exclude settings, resulting in the copied columns in the new table having type-specific default settings. For more on settings, see Section 65.2. is an abbreviated form selecting all the available individual options. (It could be useful to write individual clauses after to select all but some specific options.) The clause can also be used to copy column definitions from views, foreign tables, or composite types. Inapplicable options (e.g., from a view) are ignored.\n\nThe constraint specifies that a group of one or more columns of a table can contain only unique values. The behavior of a unique table constraint is the same as that of a unique column constraint, with the additional capability to span multiple columns. The constraint therefore enforces that any two rows must differ in at least one of these columns. For the purpose of a unique constraint, null values are not considered equal, unless is specified. Each unique constraint should name a set of columns that is different from the set of columns named by any other unique or primary key constraint defined for the table. (Otherwise, redundant unique constraints will be discarded.) When establishing a unique constraint for a multi-level partition hierarchy, all the columns in the partition key of the target partitioned table, as well as those of all its descendant partitioned tables, must be included in the constraint definition. Adding a unique constraint will automatically create a unique btree index on the column or group of columns used in the constraint. The created index has the same name as the unique constraint. The optional clause adds to that index one or more columns that are simply “payload”: uniqueness is not enforced on them, and the index cannot be searched on the basis of those columns. However they can be retrieved by an index-only scan. Note that although the constraint is not enforced on included columns, it still depends on them. Consequently, some operations on such columns (e.g., ) can cause cascaded constraint and index deletion.\n\nThe clause defines an exclusion constraint, which guarantees that if any two rows are compared on the specified column(s) or expression(s) using the specified operator(s), not all of these comparisons will return . If all of the specified operators test for equality, this is equivalent to a constraint, although an ordinary unique constraint will be faster. However, exclusion constraints can specify constraints that are more general than simple equality. For example, you can specify a constraint that no two rows in the table contain overlapping circles (see Section 8.8) by using the operator. The operator(s) are required to be commutative. Exclusion constraints are implemented using an index that has the same name as the constraint, so each specified operator must be associated with an appropriate operator class (see Section 11.10) for the index access method . Each defines a column of the index, so it can optionally specify a collation, an operator class, operator class parameters, and/or ordering options; these are described fully under CREATE INDEX. The access method must support (see Chapter 62); at present this means cannot be used. Although it's allowed, there is little point in using B-tree or hash indexes with an exclusion constraint, because this does nothing that an ordinary unique constraint doesn't do better. So in practice the access method will always be or . The allows you to specify an exclusion constraint on a subset of the table; internally this creates a partial index. Note that parentheses are required around the predicate.\n\nThese clauses specify a foreign key constraint, which requires that a group of one or more columns of the new table must only contain values that match values in the referenced column(s) of some row of the referenced table. If the list is omitted, the primary key of the is used. Otherwise, the list must refer to the columns of a non-deferrable unique or primary key constraint or be the columns of a non-partial unique index. The user must have permission on the referenced table (either the whole table, or the specific referenced columns). The addition of a foreign key constraint requires a lock on the referenced table. Note that foreign key constraints cannot be defined between temporary tables and permanent tables. A value inserted into the referencing column(s) is matched against the values of the referenced table and referenced columns using the given match type. There are three match types: , , and (which is the default). will not allow one column of a multicolumn foreign key to be null unless all foreign key columns are null; if they are all null, the row is not required to have a match in the referenced table. allows any of the foreign key columns to be null; if any of them are null, the row is not required to have a match in the referenced table. is not yet implemented. (Of course, constraints can be applied to the referencing column(s) to prevent these cases from arising.) In addition, when the data in the referenced columns is changed, certain actions are performed on the data in this table's columns. The clause specifies the action to perform when a referenced row in the referenced table is being deleted. Likewise, the clause specifies the action to perform when a referenced column in the referenced table is being updated to a new value. If the row is updated, but the referenced column is not actually changed, no action is done. Referential actions other than the check cannot be deferred, even if the constraint is declared deferrable. There are the following possible actions for each clause: Produce an error indicating that the deletion or update would create a foreign key constraint violation. If the constraint is deferred, this error will be produced at constraint check time if there still exist any referencing rows. This is the default action. Produce an error indicating that the deletion or update would create a foreign key constraint violation. This is the same as except that the check is not deferrable. Delete any rows referencing the deleted row, or update the values of the referencing column(s) to the new values of the referenced columns, respectively. Set all of the referencing columns, or a specified subset of the referencing columns, to null. A subset of columns can only be specified for actions. Set all of the referencing columns, or a specified subset of the referencing columns, to their default values. A subset of columns can only be specified for actions. (There must be a row in the referenced table matching the default values, if they are not null, or the operation will fail.) If the referenced column(s) are changed frequently, it might be wise to add an index to the referencing column(s) so that referential actions associated with the foreign key constraint can be performed more efficiently."
    },
    {
        "link": "https://postgresql.org/docs/current/tutorial-table.html",
        "document": "You can create a new table by specifying the table name, along with all column names and their types:\n\nYou can enter this into with the line breaks. will recognize that the command is not terminated until the semicolon.\n\nWhite space (i.e., spaces, tabs, and newlines) can be used freely in SQL commands. That means you can type the command aligned differently than above, or even all on one line. Two dashes (“ ”) introduce comments. Whatever follows them is ignored up to the end of the line. SQL is case-insensitive about key words and identifiers, except when identifiers are double-quoted to preserve the case (not done above).\n\nspecifies a data type that can store arbitrary character strings up to 80 characters in length. is the normal integer type. is a type for storing single precision floating-point numbers. should be self-explanatory. (Yes, the column of type is also named . This might be convenient or confusing — you choose.)\n\nPostgreSQL supports the standard types , , , , , , , , , and , as well as other types of general utility and a rich set of geometric types. PostgreSQL can be customized with an arbitrary number of user-defined data types. Consequently, type names are not key words in the syntax, except where required to support special cases in the standard.\n\nThe second example will store cities and their associated geographical location:\n\nThe type is an example of a PostgreSQL-specific data type.\n\nFinally, it should be mentioned that if you don't need a table any longer or want to recreate it differently you can remove it using the following command:"
    },
    {
        "link": "https://w3schools.com/postgresql/postgresql_create_table.php",
        "document": "To create a new database table using the SQL Shell, make sure you are connected to the database. If not, follow the steps in the Get Started chapter of this tutorial.\n\nOnce you are connected, you are ready to write SQL statements!\n\nThe following SQL statement will create a table named in your PostgreSQL database:\n\nWhen you execute the above statement, an empty table named cars will be created, and the SQL Shell application will return the following:\n\nIn the SQL Shell application on your computer the operation above might look like this:\n\nThe above SQL statement created an empty table with three fields: , , and .\n\nWhen creating fields in a table we have to specify the data type of each field.\n\nFor and we are expecting string values, and string values are specified with the keyword.\n\nWe also have to specify the number of characters allowed in a string field, and since we do not know exactly, we just set it to 255.\n\nFor we are expecting integer values (numbers without decimals), and integer values are specified with the keyword.\n\nYou can \"display\" the empty table you just created with another SQL statement:\n\nWhich will give you this result:\n\nIn the SQL Shell application on your computer the operation above might look like this:\n\nIn the next chapters we will learn how to insert data into a table, and also more on how to retrieve data from a table."
    },
    {
        "link": "https://neon.tech/postgresql/postgresql-tutorial/postgresql-create-table",
        "document": "Summary: in this tutorial, you will learn how to use the PostgreSQL CREATE TABLE statement to create a new table.\n\nTypically, a relational database consists of multiple related tables. Tables allow you to store structured data like customers, products, and employees.\n\nTo create a new table, you use the statement. Here’s the basic syntax of the statement:\n\nFirst, specify the name of the table that you want to create after the keywords. The table name must be unique in a schema. If you create a table with a name that already exists, you’ll get an error.\n\nA schema is a named collection of database objects including tables. If you create a table without a schema, it defaults to public. You’ll learn more about the schema in the schema tutorial.\n\nSecond, use the option to create a new table only if it does not exist. When you use the option and the table already exists, PostgreSQL will issue a notice instead of an error.\n\nThird, specify table columns separated by commas. Each column definition consists of the column name, data type, size, and constraint.\n\nThe constraint of a column specifies a rule that is applied to data within a column to ensure data integrity. The column constraints include primary key, foreign key, not null, unique, check, and default.\n\nFor example, the constraint ensures that the values in a column cannot be NULL.\n\nFinally, specify constraints for the table including primary key, foreign key, and check constraints.\n\nA table constraint is a rule that is applied to the data within the table to maintain data integrity.\n\nNote that some column constraints can be defined as table constraints such as primary key, foreign key, unique, and check constraints.\n• NOT NULL– ensures that the values in a column cannot be .\n• UNIQUE – ensures the values in a column are unique across the rows within the same table.\n• PRIMARY KEY – a primary key column uniquely identifies rows in a table. A table can have one and only one primary key. The primary key constraint allows you to define the primary key of a table.\n• CHECK – ensures the data must satisfy a boolean expression. For example, the value in the price column must be zero or positive.\n• FOREIGN KEY – ensures that the values in a column or a group of columns from a table exist in a column or group of columns in another table. Unlike the primary key, a table can have many foreign keys.\n\nTable constraints are similar to column constraints except that you can include more than one column in the table constraint.\n\nWe will create a new table called in the sample database. The table has the following columns:\n\nThe following example uses the statement to create the table:\n\nTo create a table in a database, you need to execute the statement using a PostgreSQL client such as psql and pgAdmin.\n\nWe’ll show you step-by-step how to create the table using the psql client tool.\n\nFirst, open the Command Prompt on Windows or Terminal on Unix-like systems and connect to the PostgreSQL:\n\nIt’ll prompt you to enter a password for the user .\n\nWhen you enter a password correctly, you’ll see the following command prompt:\n\nSecond, connect to the database:\n\nThird, enter the following statement and press Enter:\n\nThe output indicates that the table has been created.\n\nTo view the accounts table, you can use the command:\n• Use the statement to create a new table.\n• Use the option to create the new table only if it does not exist."
    },
    {
        "link": "https://postgresql.org/docs/8.1/sql-createtable.html",
        "document": "This documentation is for an unsupported version of PostgreSQL.You may want to view the same page for the current version, or one of the other supported versions listed above instead.\n\nwill create a new, initially empty table in the current database. The table will be owned by the user issuing the command. If a schema name is given (for example, ) then the table is created in the specified schema. Otherwise it is created in the current schema. Temporary tables exist in a special schema, so a schema name may not be given when creating a temporary table. The name of the table must be distinct from the name of any other table, sequence, index, or view in the same schema. also automatically creates a data type that represents the composite type corresponding to one row of the table. Therefore, tables cannot have the same name as any existing data type in the same schema. The optional constraint clauses specify constraints (tests) that new or updated rows must satisfy for an insert or update operation to succeed. A constraint is an SQL object that helps define the set of valid values in the table in various ways. There are two ways to define constraints: table constraints and column constraints. A column constraint is defined as part of a column definition. A table constraint definition is not tied to a particular column, and it can encompass more than one column. Every column constraint can also be written as a table constraint; a column constraint is only a notational convenience for use when the constraint only affects one column.\n\nIf specified, the table is created as a temporary table. Temporary tables are automatically dropped at the end of a session, or optionally at the end of the current transaction (see below). Existing permanent tables with the same name are not visible to the current session while the temporary table exists, unless they are referenced with schema-qualified names. Any indexes created on a temporary table are automatically temporary as well. Optionally, or can be written before or . This makes no difference in PostgreSQL, but see Compatibility. The name (optionally schema-qualified) of the table to be created. The name of a column to be created in the new table. The data type of the column. This may include array specifiers. For more information on the data types supported by PostgreSQL, refer to Chapter 8. The clause assigns a default data value for the column whose column definition it appears within. The value is any variable-free expression (subqueries and cross-references to other columns in the current table are not allowed). The data type of the default expression must match the data type of the column. The default expression will be used in any insert operation that does not specify a value for the column. If there is no default for a column, then the default is null. The optional clause specifies a list of tables from which the new table automatically inherits all columns. Use of creates a persistent relationship between the new child table and its parent table(s). Schema modifications to the parent(s) normally propagate to children as well, and by default the data of the child table is included in scans of the parent(s). If the same column name exists in more than one parent table, an error is reported unless the data types of the columns match in each of the parent tables. If there is no conflict, then the duplicate columns are merged to form a single column in the new table. If the column name list of the new table contains a column name that is also inherited, the data type must likewise match the inherited column(s), and the column definitions are merged into one. However, inherited and new column declarations of the same name need not specify identical constraints: all constraints provided from any declaration are merged together and all are applied to the new table. If the new table explicitly specifies a default value for the column, this default overrides any defaults from inherited declarations of the column. Otherwise, any parents that specify default values for the column must all specify the same default, or an error will be reported. The clause specifies a table from which the new table automatically copies all column names, their data types, and their not-null constraints. Unlike , the new table and original table are completely decoupled after creation is complete. Changes to the original table will not be applied to the new table, and it is not possible to include data of the new table in scans of the original table. Default expressions for the copied column definitions will only be copied if is specified. The default behavior is to exclude default expressions, resulting in all columns of the new table having null defaults. This optional clause specifies whether rows of the new table should have OIDs (object identifiers) assigned to them. If neither nor is specified, the default value depends upon the default_with_oids configuration parameter. (If the new table inherits from any tables that have OIDs, then is forced even if the command says .) If is specified or implied, the new table does not store OIDs and no OID will be assigned for a row inserted into it. This is generally considered worthwhile, since it will reduce OID consumption and thereby postpone the wraparound of the 32-bit OID counter. Once the counter wraps around, OIDs can no longer be assumed to be unique, which makes them considerably less useful. In addition, excluding OIDs from a table reduces the space required to store the table on disk by 4 bytes per row (on most machines), slightly improving performance. To remove OIDs from a table after it has been created, use ALTER TABLE. An optional name for a column or table constraint. If not specified, the system generates a name. The column is not allowed to contain null values. The column is allowed to contain null values. This is the default. This clause is only provided for compatibility with non-standard SQL databases. Its use is discouraged in new applications. The constraint specifies that a group of one or more columns of a table may contain only unique values. The behavior of the unique table constraint is the same as that for column constraints, with the additional capability to span multiple columns. For the purpose of a unique constraint, null values are not considered equal. Each unique table constraint must name a set of columns that is different from the set of columns named by any other unique or primary key constraint defined for the table. (Otherwise it would just be the same constraint listed twice.) The primary key constraint specifies that a column or columns of a table may contain only unique (non-duplicate), nonnull values. Technically, is merely a combination of and , but identifying a set of columns as primary key also provides metadata about the design of the schema, as a primary key implies that other tables may rely on this set of columns as a unique identifier for rows. Only one primary key can be specified for a table, whether as a column constraint or a table constraint. The primary key constraint should name a set of columns that is different from other sets of columns named by any unique constraint defined for the same table. The clause specifies an expression producing a Boolean result which new or updated rows must satisfy for an insert or update operation to succeed. Expressions evaluating to TRUE or UNKNOWN succeed. Should any row of an insert or update operation produce a FALSE result an error exception is raised and the insert or update does not alter the database. A check constraint specified as a column constraint should reference that column's value only, while an expression appearing in a table constraint may reference multiple columns. Currently, expressions cannot contain subqueries nor refer to variables other than columns of the current row. These clauses specify a foreign key constraint, which requires that a group of one or more columns of the new table must only contain values that match values in the referenced column(s) of some row of the referenced table. If is omitted, the primary key of the is used. The referenced columns must be the columns of a unique or primary key constraint in the referenced table. Note that foreign key constraints may not be defined between temporary tables and permanent tables. A value inserted into the referencing column(s) is matched against the values of the referenced table and referenced columns using the given match type. There are three match types: , , and , which is also the default. will not allow one column of a multicolumn foreign key to be null unless all foreign key columns are null. allows some foreign key columns to be null while other parts of the foreign key are not null. is not yet implemented. In addition, when the data in the referenced columns is changed, certain actions are performed on the data in this table's columns. The clause specifies the action to perform when a referenced row in the referenced table is being deleted. Likewise, the clause specifies the action to perform when a referenced column in the referenced table is being updated to a new value. If the row is updated, but the referenced column is not actually changed, no action is done. Referential actions other than the check cannot be deferred, even if the constraint is declared deferrable. There are the following possible actions for each clause: Produce an error indicating that the deletion or update would create a foreign key constraint violation. If the constraint is deferred, this error will be produced at constraint check time if there still exist any referencing rows. This is the default action. Produce an error indicating that the deletion or update would create a foreign key constraint violation. This is the same as except that the check is not deferrable. Delete any rows referencing the deleted row, or update the value of the referencing column to the new value of the referenced column, respectively. Set the referencing column(s) to their default values. If the referenced column(s) are changed frequently, it may be wise to add an index to the foreign key column so that referential actions associated with the foreign key column can be performed more efficiently. This controls whether the constraint can be deferred. A constraint that is not deferrable will be checked immediately after every command. Checking of constraints that are deferrable may be postponed until the end of the transaction (using the SET CONSTRAINTS command). is the default. Only foreign key constraints currently accept this clause. All other constraint types are not deferrable. If a constraint is deferrable, this clause specifies the default time to check the constraint. If the constraint is , it is checked after each statement. This is the default. If the constraint is , it is checked only at the end of the transaction. The constraint check time can be altered with the SET CONSTRAINTS command. The behavior of temporary tables at the end of a transaction block can be controlled using . The three options are: No special action is taken at the ends of transactions. This is the default behavior. All rows in the temporary table will be deleted at the end of each transaction block. Essentially, an automatic TRUNCATE is done at each commit. The temporary table will be dropped at the end of the current transaction block. The is the name of the tablespace in which the new table is to be created. If not specified, default_tablespace is used, or the database's default tablespace if is an empty string. This clause allows selection of the tablespace in which the index associated with a or constraint will be created. If not specified, default_tablespace is used, or the database's default tablespace if is an empty string.\n\nUsing OIDs in new applications is not recommended: where possible, using a or other sequence generator as the table's primary key is preferred. However, if your application does make use of OIDs to identify specific rows of a table, it is recommended to create a unique constraint on the column of that table, to ensure that OIDs in the table will indeed uniquely identify rows even after counter wraparound. Avoid assuming that OIDs are unique across tables; if you need a database-wide unique identifier, use the combination of and row OID for the purpose. Tip: The use of is not recommended for tables with no primary key, since without either an OID or a unique data key, it is difficult to identify specific rows. PostgreSQL automatically creates an index for each unique constraint and primary key constraint to enforce uniqueness. Thus, it is not necessary to create an index explicitly for primary key columns. (See CREATE INDEX for more information.) Unique constraints and primary keys are not inherited in the current implementation. This makes the combination of inheritance and unique constraints rather dysfunctional. A table cannot have more than 1600 columns. (In practice, the effective limit is lower because of tuple-length constraints.)\n\nCREATE TABLE films ( code char(5) CONSTRAINT firstkey PRIMARY KEY, title varchar(40) NOT NULL, did integer NOT NULL, date_prod date, kind varchar(10), len interval hour to minute ); CREATE TABLE distributors ( did integer PRIMARY KEY DEFAULT nextval('serial'), name varchar(40) NOT NULL CHECK (name <> '') ); Define a unique table constraint for the table . Unique table constraints can be defined on one or more columns of the table. CREATE TABLE distributors ( did integer CHECK (did > 100), name varchar(40) ); CREATE TABLE distributors ( did integer, name varchar(40) CONSTRAINT con1 CHECK (did > 100 AND name <> '') ); Define a primary key table constraint for the table . Primary key table constraints can be defined on one or more columns of the table. Define a primary key constraint for table . The following two examples are equivalent, the first using the table constraint syntax, the second the column constraint syntax. This assigns a literal constant default value for the column , arranges for the default value of column to be generated by selecting the next value of a sequence object, and makes the default value of be the time at which the row is inserted. Define two column constraints on the table , one of which is explicitly given a name: CREATE TABLE distributors ( did integer CONSTRAINT no_null NOT NULL, name varchar(40) NOT NULL ); The above is equivalent to the following specified as a table constraint:\n\nThe command conforms to the standard, with exceptions listed below. Although the syntax of resembles that of the SQL standard, the effect is not the same. In the standard, temporary tables are defined just once and automatically exist (starting with empty contents) in every session that needs them. PostgreSQL instead requires each session to issue its own command for each temporary table to be used. This allows different sessions to use the same temporary table name for different purposes, whereas the standard's approach constrains all instances of a given temporary table name to have the same table structure. The standard's definition of the behavior of temporary tables is widely ignored. PostgreSQL's behavior on this point is similar to that of several other SQL databases. The standard's distinction between global and local temporary tables is not in PostgreSQL, since that distinction depends on the concept of modules, which PostgreSQL does not have. For compatibility's sake, PostgreSQL will accept the and keywords in a temporary table declaration, but they have no effect. The clause for temporary tables also resembles the SQL standard, but has some differences. If the clause is omitted, SQL specifies that the default behavior is . However, the default behavior in PostgreSQL is . The option does not exist in SQL. The SQL standard says that column constraints may only refer to the column they apply to; only table constraints may refer to multiple columns. PostgreSQL does not enforce this restriction; it treats column and table check constraints alike. The \"constraint\" (actually a non-constraint) is a PostgreSQL extension to the SQL standard that is included for compatibility with some other database systems (and for symmetry with the constraint). Since it is the default for any column, its presence is simply noise. Multiple inheritance via the clause is a PostgreSQL language extension. SQL:1999 and later define single inheritance using a different syntax and different semantics. SQL:1999-style inheritance is not yet supported by PostgreSQL. The PostgreSQL concept of OIDs is not standard. PostgreSQL allows a table of no columns to be created (for example, ). This is an extension from the SQL standard, which does not allow zero-column tables. Zero-column tables are not in themselves very useful, but disallowing them creates odd special cases for , so it seems cleaner to ignore this spec restriction. The PostgreSQL concept of tablespaces is not part of the standard. Hence, the clauses and are extensions."
    },
    {
        "link": "https://blog.skyvia.com/complete-guide-on-how-to-import-and-export-csv-files-to-postgresql",
        "document": "This article is a complete guide for you to import and export CSV files to the Postgres database. We will look into different methods to perform this task. Every method is illustrated with examples and images to better understand it for readers. Some methods use a command-line interface that is more compatible with the database administrators, while others use graphical user interfaces and third-party tools that are easy to understand for non-technical audiences as well.\n\nAfter reading this article, you will be able to import CSV file to PostgreSQL and export Postgres data to CSV. A CSV (Comma-separated values) file generally refers to a file where the columns and their corresponding values are separated by a specific character like a comma.\n\nYou can import and export data, using different methods such as command line i.e. psql, using the user interface of pgadmin, and doing it automatically at the scheduled intervals with the help of the Skyvia tool. First, we will consider the Postgres export to csv and later focus on the data import to the Postgres database.\n• How to Copy Postgres Table to CSV File via Command Line\n• How to Import CSV to Postgres Table\n• Import from CSV File to Postgresql by Command Line Using COPY Statement\n• Import and Export CSV to Postgresql with pgAdmin 4\n\nTo get started with the activity, let us create a table in Postgres that will be used to export to csv. We will create a table named employees containing its details.\n\nWe will use this table to import and export the contents. You can download a sample csv file here to use it.\n\nHow to Copy Postgres Table to CSV File via Command Line\n\nThis section will guide you on how to export data from PostgreSQL to CSV. The data can be exported with or without headers. It can be done from the client-side as well as the server-side. The COPY command can help you to export a table to CSV. It will read the contents of the table and export as CSV. The export process from Postgres to CSV, using the copy command, is pretty straightforward, and we can repeat it to export all tables to CSV.\n\nThere are two different variants of the command, one for the client-side and the other for the server-side. When we are using the command for the client-side import/export, it will export a PostgreSQL table to CSV and save it on the client computer. While using the server-side command, it will run on the server and copy to CSV on the server end. Let us consider the copy query in the below sections.\n\nPsql \\copy command is used when you want to export the data from Postgres table to a CSV file on a client machine. To use this command, you will need access to the psql prompt. You will understand it more with the following psql copy examples.\n\nTo copy the entire table to a CSV file, use \\copy. This will copy the contents of a table to the client computer as a CSV file. The file will not contain the headers of the table.\n\nYou can also write a query instead of copying the entire table.\n\nAs output, it will show the total number of records that have been copied using psql copy.\n\nIn the above section, we exported the table data without headers. Now we will export the data including the headers. We will use the copy command to export CSV with headers.\n\nTo get the headers, we need to add the ‘header’ keyword after the ‘csv’ keyword in the command. It looks like this:\n\nAfter executing the above commands, you will be able to export the table to CSV with headers.\n\nThis section will instruct on how to export the data to a CSV file in the server, using the PostgreSQL copy command. It will copy the contents to a specified location on the server.\n\nTo do this, you need to connect to the database of the server through a Postgres shell prompt. Look at the below Postgres copy example for a better understanding of how to do it.\n\nThe difference between the two commands, i.e. COPY and /copy is that the former is server-based and the latter is client-based. We demonstrated copy command examples for both, client-based as well as server-based exports.\n\nHow to Import CSV to Postgres Table\n\nSo far, we have seen how to export the data to a CSV file. Now we shift our focus to importing data from CSV to Postgres table. There are different methods like using the command-line interface, third-party tools, etc., to import CSV to PostgreSQL. We will consider it more in the below sections. By using these methods, we cannot create a table from the CSV file in Postgres. The table will need to be created first then we can import the data into the table otherwise you might see an error while importing the data.\n\nImport from CSV File to Postgresql by Command Line Using COPY Statement\n\nWe can make use of the PostgreSQL copy command to copy from the CSV file and import it to Postgres. We can import the data into the table with or without headers provided the CSV should be in the same format. While exporting the data, we use the TO keyword in the copy command, whereas while importing the data into a Postgres table, we need to use a FROM keyword.\n\nUsing the above commands, one will be able to copy from a CSV file, using the PostgreSQL command line.\n\nImport and Export CSV to Postgresql with pgAdmin 4\n\npgAdmin is an open-source tool that helps in the administration and management of Postgres databases and their development. In this section, we will focus on creating a table, importing and exporting the table contents.\n\nTo create a table in pgAdmin, follow the below steps:\n\n1. In the schema section, select a table and right-click to see options related to tables. Hover on the Create option, then click on Table to open the wizard for creating a table.\n\n2. Enter table-specific details like table name, columns, etc. Click on Save to create the table.\n\nTo import CSV to PostgreSQL with pgAdmin, follow the below steps:\n• After creating the table, right-click on the table to see the options. Choose the Import/Export option to open the wizard.\n• Click on the Import/Export flag button to import the data. Select the file that needs to be imported. Enter the delimiter of the file and enable the header option if the file consists of the headers. Click on OK to start the importing process.\n• After clicking on OK, you will notice a popup showing that the process has been successfully completed. This process demonstrates how you can import CSV into PostgreSQL, using pgAdmin 4.\n\nTo export CSV using pgAdmin, follow the below steps:\n• Open the wizard by clicking on the Import/Export option shown by right-clicking the Table.\n• Specify the path where the file needs to be saved. In the Miscellaneous section, select Headers to export the headers along with the data. Click on OK to start the exporting process. The data will be successfully exported into the file from the table.\n\nSkyvia is a cloud-based solution that helps you automatically import/export data from PostgreSQL to CSV files. In this section, first, we will focus on how to create a connection to PostgreSQL and Dropbox and, second, how to create a package to repeatedly execute data export from PostgreSQL to CSV files in your Dropbox file storage.\n\nBefore starting to use this solution, you need an account on Skyvia and Dropbox. Both are free to register.\n\n1. Click NEW and select Connection in the menu on the left.\n\n2. In the Select Connector page, select PostgreSQL. In the opened window, specify required PostgreSQL parameters as shown below and click Create Connection at the bottom left of the window.\n\n3. Next, create a connection to Dropbox in the same way as described in the steps above. But it is much easier to do, you simply sign in to your Dropbox account via OAuth 2.\n• Click NEW and select Export under Integration, i.e., you repeat almost the same actions as in Step 1 above.\n• In the open package editor window, under Source in the drop-down list, select the PostgreSQL connection created a bit earlier.\n• Move on to Target and click CSV to storage service. In the drop-down list, select Dropbox, and below, select the folder in your CSV file with exported data will be located to. In our case, it is the Employees folder.\n• Click Add new on the right top of the page to add a task where you can specify editor mode on how to export data – simply selecting an object you need in PostgreSQL or executing the entered query/command to export data to CSV. As you can see on the sample screenshot below, we select the Products object and apply the necessary filter settings. When the task is ready, we save the task.\n• Finally, schedule your package for automatic execution on certain days and at specific time. Enjoy the process with no more effort!\n\nUpon reading this article, the readers would get a grip on importing and exporting data from/to the CSV file using multiple methodologies. Import and export of data become a crucial task for many organizations as they need to import data from different locations for analytical purposes. For such tasks, it is better to opt for scheduled automated jobs that can be performed without any human intervention.\n\nIn the Comments section, share with us what kind of method you personally prefer to use amongst the ones listed above. You can also suggest some other methods, which you know to import/export the Postgres table from/to a CSV file."
    },
    {
        "link": "https://estuary.dev/csv-to-postgres",
        "document": "Many people and organizations prefer storing data in CSV because of its simplicity, compatibility, and database import/export support. While CSV files have their advantages, they also come with some drawbacks. For instance, CSV may not be suitable for complex data structures or scenarios where data relationships need to be preserved. \n\n\n\nImporting a CSV file into a robust database like allows for reliable and persistent data storage. By moving data from CSV to PostgreSQL, organizations can leverage powerful SQL queries to organize and analyze data, helping to extract insights and make data-driven decisions.\n\n\n\nIn this comprehensive guide, we’ll cover three methods to import CSV data into PostgreSQL: using no-code tools, the COPY command, and pgAdmin. It explains step-by-step processes, highlights challenges of manual methods, and emphasizes automation for seamless, real-time data integration.\n\nA Comma Separated Value (CSV) file is a lightweight text file used to store data separated by commas with a .csv extension. As data in a is stored using a comma-delimited format, it’s easier to convert your data into rows and columns. This enables you to easily read, edit, and transfer your data to various systems. As a result, CSV files are a popular choice for data professionals when importing and exporting data between databases.\n\nPostgreSQL, also commonly known as Postgres, is an open-source object-relational database management system (ORDBMS). It is a highly scalable database that can be used for both relational (SQL) as well as non-relational (JSON) querying. To help you perform complex operations on tables, PostgreSQL supports different SQL functions, such as user-defined functions, subqueries, and triggers. Because of such flexibility, it is used as a primary database for many web, mobile, and IoT applications.\n• None It is highly secure and reliable.\n• None PostgreSQL is free to download and use. \n\n\n\nLet's dive into the three methods for importing your CSV data into PostgreSQL:\n• None Method 2: Import CSV to PostgreSQL using the Copy command \n\n\n\nNo-code offer a range of pre-built connectors to facilitate data movement between platforms. Among top data integration tools for building pipelines, Estuary Flow stands out as a reliable, cost-effective choice.\n\n\n\nFlow is a powerful cloud-based data integration platform that can help you move data between different sources and destinations in real time. It allows you to capture data from a CSV hosted at any or from Google Sheets. What’s more, you can also use Flow to export data from PostgreSQL to CSV file if needed.\n\nHere’s how you can go about loading data from CSV to PostgreSQL using Estuary Flow:\n\nStep 1: Sign Up or Sign In\n\nto your Estuary account or .\n\nFrom your Estuary dashboard, navigate to \"Sources\" and click \"+ New Capture.\"\n\nIf your CSV is hosted online (e.g., on a website), select the \"HTTP File\" connector. If it's in Google Sheets, choose the \"Google Sheets\" connector.\n\nProvide the necessary details like the CSV file URL or Google Sheets link.\n\nClick \"Next,\" then \"Save and Publish\" to complete the capture setup.\n\nSwitch to the \"Destinations\" tab and click \"+ New Materialization.\"\n\nSince you're copying your data from CSV to PostgreSQL, search for PostgreSQL in the Search Connectors box. Click on the Materialization button of the PostgreSQL connector. You will be redirected to the PostgreSQL materialization connector page.\n\nBefore you proceed to connect PostgreSQL, make sure you have completed the . Once you are done with the necessary steps, you can proceed to set up your destination.\n\nOn the Create Materialization page, fill in the details such as the Name of the connector, Address, Username, and Password of the database. If your Flow collections aren’t automatically selected for materialization, you can use the Source Collections option to select your collections.\n\nClick on the Next button. Then, click on Save and Publish. After completing these steps, Estuary Flow will continuously replicate your CSV data to PostgreSQL in real time.\n\nReady to simplify your data integration? Try Estuary Flow for free today and start importing your CSV data into PostgreSQL with ease! Sign Up for Free. Need assistance? Our team is here to help—Contact Us\n• None uses open-source data connectors that are compatible across platforms and work in real-time. This reduces the repetitive manual efforts to reliably transfer data.\n• None Scalability and Performance: Flow offers high scalability with performance and ensures you can handle large volumes of data in milliseconds.\n\nBONUS: If you need to load data from Google Sheets to PostgreSQL, check out .\n\nMethod 2: Import CSV to PostgreSQL Using the Copy Command\n\nUsing the COPY command to replicate data from CSV to Postgres is one of the simplest manual methods available. To copy the data, all you need is PostgreSQL superuser access.\n\nThe COPY command in PostgreSQL is used to copy data between a PostgreSQL table/database and any source/destination file. Based on your requirement, the COPY command can be used as COPY-TO or COPY-FROM. While COPY-TO copies the data from the table to a specified file, COPY-FROM copies the data from a file to the table.\n\nFor a better understanding, let’s consider the following CSV file data and PostgreSQL table.\n\nStep 1: Assume your CSV file data is structured as follows:\n\nStep 2: Open the command-line tool in your machine and connect it with your local PostgreSQL server using the following command:\n\nStep 3: On execution of the above command, you will be prompted to enter the password.\n\nStep 4: Now, you’re connected to the PostgreSQL server. The command prompt should now appear as follows:\n\nStep 5: Create a table in PostgreSQL using the following command:\n\nUpon successful execution, a table named will be created, with as the primary key.\n\nStep 6: To check the list of databases, enter the below command:\n\nAfter executing the above command, you can see a list of databases. To check if the table is successfully created in the database, run the \\dt command.\n\nOnce the PostgreSQL table is ready, you can use the COPY command to move data from CSV to PostgreSQL.\n\nStep 7: To import the CSV file into the PostgreSQL student_info table, use the below COPY command in your command prompt:\n\nThe output COPY 4 indicates that all four rows from the CSV file have been copied to your PostgreSQL table.\n\nLet’s understand the above query in more detail:\n• None COPY-FROM: Used to copy data from a file to the PostgreSQL table.\n• None student_info: This is the table name where you want to move CSV data. Mentioning column names will copy data from the CSV file to the PostgreSQL table in the given sequence.\n• None C:\\Program Files\\PostgreSQL\\Sample1.csv: The path and name of the CSV file from where data is to be imported into the PostgreSQL table.\n• None DELIMITER ‘,’: It defines how the values in the CSV file are separated.\n• None CSV: Used to specify that data is imported from a CSV file.\n• None HEADER: Specifies that the CSV file contains a header line with the names of each column. So, the PostgreSQL table will import CSV data from the second row.\n\nStep 8: To check if the tables are imported successfully, run the following command.\n\nThat’s it! You’ve successfully moved CSV file data to the PostgreSQL table. Similarly, you can also use the COPY command to copy PostgreSQL output to CSV.\n\npgAdmin is an open-source tool for administrating and managing your PostgreSQL databases. You can easily download it from their .\n\nFollow the steps below to copy data from CSV to the PostgreSQL table using pgAdmin.\n\nStep 1: With pgAdmin, you can directly create a table using its GUI. In the schema section, navigate to Tables > Create > Table.\n\nStep 2: You will be redirected to the Create-Table window. Enter the Name and Schema of the table.\n\nStep 3: Now, click on the Save button to create a table.\n\nStep 4: After creating the table, right-click on the table name and select the Import/Export option.\n\nStep 5: An Import/Export Data window with two tabs—Options and Columns—will be opened for that specific table. Select the Options window and enable the Import option from the Import/Export toggle button. Now, mention the path of the file that needs to be imported. Select the format of the file as CSV and specify the delimiter of the file. If your file consists of the headers, enable the header option.\n\nStep 6: Click on the OK button to start the importing process.\n\nA popup window will appear once the data from the CSV file has been successfully copied to PostgreSQL.\n\npgAdmin not only allows you to copy data from CSV to PostgreSQL but also lets you make PostgreSQL copy to CSV.\n\nWhile the two manual methods to copy CSV to PostgreSQL are quite simple, there are a few challenges involved.\n• None Manual methods typically limit the ability to clean and transform data during the import process.\n• None To achieve no loss data transfer, you would need to continuously review and manage the end-to-end data flow. This requires significant effort, which can be time-consuming.\n• For real-time use cases, it would require you to import data manually as soon as the CSV source file data is updated. This necessitates repeating the entire migration process, which can be tedious.\n\nCopying CSV to PostgreSQL shouldn’t be a challenging task, especially if you use the right approach. This guide covered multiple methods for PostgreSQL import CSV to table, including the COPY command, pgAdmin, and no-code tools. \n\n\n\nIf you’re considering a less manual approach, no-code platforms like Estuary ensure data consistency and real-time updates, allowing you to automate your data pipeline and focus on core business objectives.\n\nEstuary Flow allows you to connect data from various sources to PostgreSQL seamlessly. It offers an automated, streamlined solution that helps save both time and hassle. Get integrated, try Flow for free!"
    },
    {
        "link": "https://postgresql.org/docs/current/sql-copy.html",
        "document": "moves data between PostgreSQL tables and standard file-system files. copies the contents of a table to a file, while copies data from a file to a table (appending the data to whatever is in the table already). can also copy the results of a query. If a column list is specified, copies only the data in the specified columns to the file. For , each field in the file is inserted, in order, into the specified column. Table columns not specified in the column list will receive their default values. with a file name instructs the PostgreSQL server to directly read from or write to a file. The file must be accessible by the PostgreSQL user (the user ID the server runs as) and the name must be specified from the viewpoint of the server. When is specified, the server executes the given command and reads from the standard output of the program, or writes to the standard input of the program. The command must be specified from the viewpoint of the server, and be executable by the PostgreSQL user. When or is specified, data is transmitted via the connection between the client and the server. Each backend running will report its progress in the view. See Section 27.4.3 for details. By default, will fail if it encounters an error during processing. For use cases where a best-effort attempt at loading the entire file is desired, the clause can be used to specify some other behavior.\n\nThe name (optionally schema-qualified) of an existing table. An optional list of columns to be copied. If no column list is specified, all columns of the table except generated columns will be copied. A , , , , , or command whose results are to be copied. Note that parentheses are required around the query. For , , , and queries a clause must be provided, and the target relation must not have a conditional rule, nor an rule, nor an rule that expands to multiple statements. The path name of the input or output file. An input file name can be an absolute or relative path, but an output file name must be an absolute path. Windows users might need to use an string and double any backslashes used in the path name. A command to execute. In , the input is read from standard output of the command, and in , the output is written to the standard input of the command. Note that the command is invoked by the shell, so if you need to pass any arguments that come from an untrusted source, you must be careful to strip or escape any special characters that might have a special meaning for the shell. For security reasons, it is best to use a fixed command string, or at least avoid including any user input in it. Specifies that input comes from the client application. Specifies that output goes to the client application. Specifies whether the selected option should be turned on or off. You can write , , or to enable the option, and , , or to disable it. The value can also be omitted, in which case is assumed. Selects the data format to be read or written: , (Comma Separated Values), or . The default is . Requests copying the data with rows already frozen, just as they would be after running the command. This is intended as a performance option for initial data loading. Rows will be frozen only if the table being loaded has been created or truncated in the current subtransaction, there are no cursors open and there are no older snapshots held by this transaction. It is currently not possible to perform a on a partitioned table. This option is only allowed in . Note that all other sessions will immediately be able to see the data once it has been successfully loaded. This violates the normal rules of MVCC visibility and users should be aware of the potential problems this might cause. Specifies the character that separates columns within each row (line) of the file. The default is a tab character in text format, a comma in format. This must be a single one-byte character. This option is not allowed when using format. Specifies the string that represents a null value. The default is (backslash-N) in text format, and an unquoted empty string in format. You might prefer an empty string even in text format for cases where you don't want to distinguish nulls from empty strings. This option is not allowed when using format. When using , any data item that matches this string will be stored as a null value, so you should make sure that you use the same string as you used with . Specifies the string that represents a default value. Each time the string is found in the input file, the default value of the corresponding column will be used. This option is allowed only in , and only when not using format. Specifies that the file contains a header line with the names of each column in the file. On output, the first line contains the column names from the table. On input, the first line is discarded when this option is set to (or equivalent Boolean value). If this option is set to , the number and names of the columns in the header line must match the actual column names of the table, in order; otherwise an error is raised. This option is not allowed when using format. The option is only valid for commands. Specifies the quoting character to be used when a data value is quoted. The default is double-quote. This must be a single one-byte character. This option is allowed only when using format. Specifies the character that should appear before a data character that matches the value. The default is the same as the value (so that the quoting character is doubled if it appears in the data). This must be a single one-byte character. This option is allowed only when using format. Forces quoting to be used for all non- values in each specified column. output is never quoted. If is specified, non- values will be quoted in all columns. This option is allowed only in , and only when using format. Do not match the specified columns' values against the null string. In the default case where the null string is empty, this means that empty values will be read as zero-length strings rather than nulls, even when they are not quoted. If is specified, the option will be applied to all columns. This option is allowed only in , and only when using format. Match the specified columns' values against the null string, even if it has been quoted, and if a match is found set the value to . In the default case where the null string is empty, this converts a quoted empty string into NULL. If is specified, the option will be applied to all columns. This option is allowed only in , and only when using format. Specifies how to behave when encountering an error converting a column's input value into its data type. An value of means fail the command, while means discard the input row and continue with the next one. The default is . The option is applicable only for when the is or . A message containing the ignored row count is emitted at the end of the if at least one row was discarded. When option is set to , a message containing the line of the input file and the column name whose input conversion has failed is emitted for each discarded row. Specifies that the file is encoded in the . If this option is omitted, the current client encoding is used. See the Notes below for more details. Specify the amount of messages emitted by a command: or . If is specified, additional messages are emitted during processing. This is currently used in command when option is set to . The optional clause has the general form where is any expression that evaluates to a result of type . Any row that does not satisfy this condition will not be inserted to the table. A row satisfies the condition if it returns true when the actual row values are substituted for any variable references. Currently, subqueries are not allowed in expressions, and the evaluation does not see any changes made by the itself (this matters when the expression contains calls to functions).\n\ncan be used only with plain tables, not views, and does not copy rows from child tables or child partitions. For example, copies the same rows as . The syntax can be used to dump all of the rows in an inheritance hierarchy, partitioned table, or view. can be used with plain, foreign, or partitioned tables or with views that have triggers. You must have select privilege on the table whose values are read by , and insert privilege on the table into which values are inserted by . It is sufficient to have column privileges on the column(s) listed in the command. If row-level security is enabled for the table, the relevant policies will apply to statements. Currently, is not supported for tables with row-level security. Use equivalent statements instead. Files named in a command are read or written directly by the server, not by the client application. Therefore, they must reside on or be accessible to the database server machine, not the client. They must be accessible to and readable or writable by the PostgreSQL user (the user ID the server runs as), not the client. Similarly, the command specified with is executed directly by the server, not by the client application, must be executable by the PostgreSQL user. naming a file or command is only allowed to database superusers or users who are granted one of the roles , , or , since it allows reading or writing any file or running a program that the server has privileges to access. Do not confuse with the psql instruction . invokes or , and then fetches/stores the data in a file accessible to the psql client. Thus, file accessibility and access rights depend on the client rather than the server when is used. It is recommended that the file name used in always be specified as an absolute path. This is enforced by the server in the case of , but for you do have the option of reading from a file specified by a relative path. The path will be interpreted relative to the working directory of the server process (normally the cluster's data directory), not the client's working directory. Executing a command with might be restricted by the operating system's access control mechanisms, such as SELinux. will invoke any triggers and check constraints on the destination table. However, it will not invoke rules. For identity columns, the command will always write the column values provided in the input data, like the option . input and output is affected by . To ensure portability to other PostgreSQL installations that might use non-default settings, should be set to before using . It is also a good idea to avoid dumping data with set to , because negative interval values might be misinterpreted by a server that has a different setting for . Input data is interpreted according to option or the current client encoding, and output data is encoded in or the current client encoding, even if the data does not pass through the client but is read from or written to a file directly by the server. The command physically inserts input rows into the table as it progresses. If the command fails, these rows are left in a deleted state; these rows will not be visible, but still occupy disk space. This might amount to considerable wasted disk space if the failure happened well into a large copy operation. should be used to recover the wasted space. and can be used simultaneously on the same column. This results in converting quoted null strings to null values and unquoted null strings to empty strings.\n\nWhen the format is used, the data read or written is a text file with one line per table row. Columns in a row are separated by the delimiter character. The column values themselves are strings generated by the output function, or acceptable to the input function, of each attribute's data type. The specified null string is used in place of columns that are null. will raise an error if any line of the input file contains more or fewer columns than are expected. End of data can be represented by a single line containing just backslash-period ( ). An end-of-data marker is not necessary when reading from a file, since the end of file serves perfectly well; it is needed only when copying data to or from client applications using pre-3.0 client protocol. Backslash characters ( ) can be used in the data to quote data characters that might otherwise be taken as row or column delimiters. In particular, the following characters must be preceded by a backslash if they appear as part of a column value: backslash itself, newline, carriage return, and the current delimiter character. The specified null string is sent by without adding any backslashes; conversely, matches the input against the null string before removing backslashes. Therefore, a null string such as cannot be confused with the actual data value (which would be represented as ). The following special backslash sequences are recognized by : Backslash followed by one to three octal digits specifies the byte with that numeric code Backslash followed by one or two hex digits specifies the byte with that numeric code Presently, will never emit an octal or hex-digits backslash sequence, but it does use the other sequences listed above for those control characters. Any other backslashed character that is not mentioned in the above table will be taken to represent itself. However, beware of adding backslashes unnecessarily, since that might accidentally produce a string matching the end-of-data marker ( ) or the null string ( by default). These strings will be recognized before any other backslash processing is done. It is strongly recommended that applications generating data convert data newlines and carriage returns to the and sequences respectively. At present it is possible to represent a data carriage return by a backslash and carriage return, and to represent a data newline by a backslash and newline. However, these representations might not be accepted in future releases. They are also highly vulnerable to corruption if the file is transferred across different machines (for example, from Unix to Windows or vice versa). All backslash sequences are interpreted after encoding conversion. The bytes specified with the octal and hex-digit backslash sequences must form valid characters in the database encoding. will terminate each row with a Unix-style newline (“ ”). Servers running on Microsoft Windows instead output carriage return/newline (“ ”), but only for to a server file; for consistency across platforms, always sends “ ” regardless of server platform. can handle lines ending with newlines, carriage returns, or carriage return/newlines. To reduce the risk of error due to un-backslashed newlines or carriage returns that were meant as data, will complain if the line endings in the input are not all alike. This format option is used for importing and exporting the Comma Separated Value ( ) file format used by many other programs, such as spreadsheets. Instead of the escaping rules used by PostgreSQL's standard text format, it produces and recognizes the common escaping mechanism. The values in each record are separated by the character. If the value contains the delimiter character, the character, the string, a carriage return, or line feed character, then the whole value is prefixed and suffixed by the character, and any occurrence within the value of a character or the character is preceded by the escape character. You can also use to force quotes when outputting non- values in specific columns. The format has no standard way to distinguish a value from an empty string. PostgreSQL's handles this by quoting. A is output as the parameter string and is not quoted, while a non- value matching the parameter string is quoted. For example, with the default settings, a is written as an unquoted empty string, while an empty string data value is written with double quotes ( ). Reading values follows similar rules. You can use to prevent input comparisons for specific columns. You can also use to convert quoted null string data values to . Because backslash is not a special character in the format, , the end-of-data marker, could also appear as a data value. To avoid any misinterpretation, a data value appearing as a lone entry on a line is automatically quoted on output, and on input, if quoted, is not interpreted as the end-of-data marker. If you are loading a file created by another application that has a single unquoted column and might have a value of , you might need to quote that value in the input file. In format, all characters are significant. A quoted value surrounded by white space, or any characters other than , will include those characters. This can cause errors if you import data from a system that pads lines with white space out to some fixed width. If such a situation arises you might need to preprocess the file to remove the trailing white space, before importing the data into PostgreSQL. format will both recognize and produce files with quoted values containing embedded carriage returns and line feeds. Thus the files are not strictly one line per table row like text-format files. Many programs produce strange and occasionally perverse files, so the file format is more a convention than a standard. Thus you might encounter some files that cannot be imported using this mechanism, and might produce files that other programs cannot process. The format option causes all data to be stored/read as binary format rather than as text. It is somewhat faster than the text and formats, but a binary-format file is less portable across machine architectures and PostgreSQL versions. Also, the binary format is very data type specific; for example it will not work to output binary data from a column and read it into an column, even though that would work fine in text format. The file format consists of a file header, zero or more tuples containing the row data, and a file trailer. Headers and data are in network byte order. PostgreSQL releases before 7.4 used a different binary file format. The file header consists of 15 bytes of fixed fields, followed by a variable-length header extension area. The fixed fields are: 11-byte sequence — note that the zero byte is a required part of the signature. (The signature is designed to allow easy identification of files that have been munged by a non-8-bit-clean transfer. This signature will be changed by end-of-line-translation filters, dropped zero bytes, dropped high bits, or parity changes.) 32-bit integer bit mask to denote important aspects of the file format. Bits are numbered from 0 ( ) to 31 ( ). Note that this field is stored in network byte order (most significant byte first), as are all the integer fields used in the file format. Bits 16–31 are reserved to denote critical file format issues; a reader should abort if it finds an unexpected bit set in this range. Bits 0–15 are reserved to signal backwards-compatible format issues; a reader should simply ignore any unexpected bits set in this range. Currently only one flag bit is defined, and the rest must be zero: If 1, OIDs are included in the data; if 0, not. Oid system columns are not supported in PostgreSQL anymore, but the format still contains the indicator. 32-bit integer, length in bytes of remainder of header, not including self. Currently, this is zero, and the first tuple follows immediately. Future changes to the format might allow additional data to be present in the header. A reader should silently skip over any header extension data it does not know what to do with. The header extension area is envisioned to contain a sequence of self-identifying chunks. The flags field is not intended to tell readers what is in the extension area. Specific design of header extension contents is left for a later release. This design allows for both backwards-compatible header additions (add header extension chunks, or set low-order flag bits) and non-backwards-compatible changes (set high-order flag bits to signal such changes, and add supporting data to the extension area if needed). Each tuple begins with a 16-bit integer count of the number of fields in the tuple. (Presently, all tuples in a table will have the same count, but that might not always be true.) Then, repeated for each field in the tuple, there is a 32-bit length word followed by that many bytes of field data. (The length word does not include itself, and can be zero.) As a special case, -1 indicates a NULL field value. No value bytes follow in the NULL case. There is no alignment padding or any other extra data between fields. Presently, all data values in a binary-format file are assumed to be in binary format (format code one). It is anticipated that a future extension might add a header field that allows per-column format codes to be specified. To determine the appropriate binary format for the actual tuple data you should consult the PostgreSQL source, in particular the and functions for each column's data type (typically these functions are found in the directory of the source distribution). If OIDs are included in the file, the OID field immediately follows the field-count word. It is a normal field except that it's not included in the field-count. Note that oid system columns are not supported in current versions of PostgreSQL. The file trailer consists of a 16-bit integer word containing -1. This is easily distinguished from a tuple's field-count word. A reader should report an error if a field-count word is neither -1 nor the expected number of columns. This provides an extra check against somehow getting out of sync with the data.\n\nThe following example copies a table to the client using the vertical bar ( ) as the field delimiter: To copy data from a file into the table: To copy into a file just the countries whose names start with 'A': COPY (SELECT * FROM country WHERE country_name LIKE 'A%') TO '/usr1/proj/bray/sql/a_list_countries.copy'; To copy into a compressed file, you can pipe the output through an external compression program: Here is a sample of data suitable for copying into a table from : Note that the white space on each line is actually a tab character. The following is the same data, output in binary format. The data is shown after filtering through the Unix utility . The table has three columns; the first has type , the second has type , and the third has type . All the rows have a null value in the third column.\n\nThere is no statement in the SQL standard. The following syntax was used before PostgreSQL version 9.0 and is still supported: COPY [ ( [, ...] ) ] FROM { ' ' | STDIN } [ [ WITH ] [ BINARY ] [ DELIMITER [ AS ] ' ' ] [ NULL [ AS ] ' ' ] [ CSV [ HEADER ] [ QUOTE [ AS ] ' ' ] [ ESCAPE [ AS ] ' ' ] [ FORCE NOT NULL [, ...] ] ] ] COPY { [ ( [, ...] ) ] | ( ) } TO { ' ' | STDOUT } [ [ WITH ] [ BINARY ] [ DELIMITER [ AS ] ' ' ] [ NULL [ AS ] ' ' ] [ CSV [ HEADER ] [ QUOTE [ AS ] ' ' ] [ ESCAPE [ AS ] ' ' ] [ FORCE QUOTE { [, ...] | * } ] ] ] Note that in this syntax, and are treated as independent keywords, not as arguments of a option. The following syntax was used before PostgreSQL version 7.3 and is still supported: COPY [ BINARY ] FROM { ' ' | STDIN } [ [USING] DELIMITERS ' ' ] [ WITH NULL AS ' ' ] COPY [ BINARY ] TO { ' ' | STDOUT } [ [USING] DELIMITERS ' ' ] [ WITH NULL AS ' ' ]"
    },
    {
        "link": "https://stackoverflow.com/questions/2987433/how-to-import-csv-file-data-into-a-postgresql-table",
        "document": "How can I write a stored procedure that imports data from a CSV file and populates the table?\n\nIf you don't have permission to use (which work on the db server), you can use instead (which works in the db client). Using the same example as Bozhidar Batsov: Copy data from your CSV file to the table: Mind that \\copy ... must be written in one line and without a ; at the end! You can also specify the columns to read: See the documentation for COPY: Do not confuse COPY with the psql instruction \\copy. \\copy invokes COPY FROM STDIN or COPY TO STDOUT, and then fetches/stores the data in a file accessible to the psql client. Thus, file accessibility and access rights depend on the client rather than the server when \\copy is used. For identity columns, the COPY FROM command will always write the column values provided in the input data, like the INSERT option OVERRIDING SYSTEM VALUE.\n\nOne quick way of doing this is with the Python Pandas library (version 0.15 or above works best). This will handle creating the columns for you - although obviously the choices it makes for data types might not be what you want. If it doesn't quite do what you want you can always use the 'create table' code generated as a template. import pandas as pd df = pd.read_csv('mypath.csv') df.columns = [c.lower() for c in df.columns] # PostgreSQL doesn't like capitals or spaces from sqlalchemy import create_engine engine = create_engine('postgresql://username:password@localhost:5432/dbname') df.to_sql(\"my_table_name\", engine) And here's some code that shows you how to set various options: # Set it so the raw SQL output is logged import logging logging.basicConfig() logging.getLogger('sqlalchemy.engine').setLevel(logging.INFO) df.to_sql(\"my_table_name2\", engine, if_exists=\"append\", # Options are ‘fail’, ‘replace’, ‘append’, default ‘fail’ index = False, # Do not output the index of the dataframe dtype = {'col1': sqlalchemy.types.NUMERIC, 'col2': sqlalchemy.types.String}) # Datatypes should be SQLAlchemy types\n\nMost other solutions here require that you create the table in advance/manually. This may not be practical in some cases (e.g., if you have a lot of columns in the destination table). So, the approach below may come handy. Providing the path and column count of your CSV file, you can use the following function to load your table to a temp table that will be named as : The top row is assumed to have the column names. create or replace function data.load_csv_file ( target_table text, csv_path text, col_count integer ) returns void as $$ declare iter integer; -- dummy integer to iterate columns with col text; -- variable to keep the column name at each iteration col_first text; -- first column name, e.g., top left corner on a csv file or spreadsheet begin create table temp_table (); -- add just enough number of columns for iter in 1..col_count loop execute format('alter table temp_table add column col_%s text;', iter); end loop; -- copy the data from csv file execute format('copy temp_table from %L with delimiter '','' quote ''\"'' csv ', csv_path); iter := 1; col_first := (select col_1 from temp_table limit 1); -- update the column names based on the first row which has the column names for col in execute format('select unnest(string_to_array(trim(temp_table::text, ''()''), '','')) from temp_table where col_1 = %L', col_first) loop execute format('alter table temp_table rename column col_%s to %s', iter, col); iter := iter + 1; end loop; -- delete the columns row execute format('delete from temp_table where %s = %L', col_first, col_first); -- change the temp table name to the name given as parameter, if not blank if length(target_table) > 0 then execute format('alter table temp_table rename to %I', target_table); end if; end; $$ language plpgsql;\n\nYou could also use pgAdmin, which offers a GUI to do the import. That's shown in this SO thread. The advantage of using pgAdmin is that it also works for remote databases. Much like the previous solutions though, you would need to have your table on the database already. Each person has his own solution, but I usually open the CSV file in Excel, copy the headers, paste special with transposition on a different worksheet, place the corresponding data type on the next column, and then just copy and paste that to a text editor together with the appropriate SQL table creation query like so: CREATE TABLE my_table ( /* Paste data from Excel here for example ... */ col_1 bigint, col_2 bigint, /* ... */ col_n bigint )\n\nDBeaver Community Edition (dbeaver.io) makes it trivial to connect to a database, then import a CSV file for upload to a PostgreSQL database. It also makes it easy to issue queries, retrieve data, and download result sets to CSV, JSON, SQL, or other common data formats. It is a FOSS multi-platform database tool for SQL programmers, DBAs and analysts that supports all popular databases: MySQL, PostgreSQL, SQLite, Oracle, DB2, SQL Server, Sybase, MS Access, Teradata, Firebird, Hive, Presto, etc. It's a viable FOSS competitor to TOAD for Postgres, TOAD for SQL Server, or Toad for Oracle. I have no affiliation with DBeaver. I love the price (FREE!) and full functionality, but I wish they would open up this DBeaver/Eclipse application more and make it easy to add analytics widgets to DBeaver / Eclipse, rather than requiring users to pay for the $199 annual subscription just to create graphs and charts directly within the application. My Java coding skills are rusty and I don't feel like taking weeks to relearn how to build Eclipse widgets, (only to find that DBeaver has probably disabled the ability to add third-party widgets to the DBeaver Community Edition.)"
    },
    {
        "link": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/PostgreSQL.Procedural.Importing.Copy.html",
        "document": "Using the \\copy command to import data to a table on a PostgreSQL DB instance\n\nThe PostgreSQL command is a meta-command available from the interactive client tool. You can use to import data into a table on your RDS for PostgreSQL DB instance. To use the command, you need to first create the table structure on the target DB instance so that has a destination for the data being copied.\n\nYou can use to load data from a comma-separated values (CSV) file, such as one that's been exported and saved to your client workstation.\n\nTo import the CSV data to the target RDS for PostgreSQL DB instance, first connect to the target DB instance using .\n\nYou then run command with the following parameters to identify the target for the data and its format.\n\nIf your CSV file has column heading information, you can use this version of the command and parameters.\n\nCreating a new DB instance in the Database Preview environment using command with the meta-command as shown in the following examples. This example uses source-table as the source table name, source-table.csv as the .csv file, and target-db as the target database:\n\nFor complete details about the command, see the psql page in the PostgreSQL documentation, in the Meta-Commands section."
    }
]