[
    {
        "link": "https://github.com/NVlabs/stylegan2",
        "document": "Analyzing and Improving the Image Quality of StyleGAN\n\n Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila\n\n\n\nAbstract: The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent vectors to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably detect if an image is generated by a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.\n\nFor business inquiries, please visit our website and submit the form: NVIDIA Research Licensing\n\n★★★ NEW: StyleGAN2-ADA-PyTorch is now available; see the full list of versions here ★★★\n• Both Linux and Windows are supported. Linux is recommended for performance and compatibility reasons.\n• 64-bit Python 3.6 installation. We recommend Anaconda3 with numpy 1.14.3 or newer.\n• We recommend TensorFlow 1.14, which we used for all experiments in the paper, but TensorFlow 1.15 is also supported on Linux. TensorFlow 2.x is not supported.\n• On Windows you need to use TensorFlow 1.14, as the standard 1.15 installation does not include necessary C++ headers.\n• One or more high-end NVIDIA GPUs, NVIDIA drivers, CUDA 10.0 toolkit and cuDNN 7.5. To reproduce the results reported in the paper, you need an NVIDIA GPU with at least 16 GB of DRAM.\n• Docker users: use the provided Dockerfile to build an image with the required library dependencies.\n\nStyleGAN2 relies on custom TensorFlow ops that are compiled on the fly using NVCC. To test that your NVCC installation is working correctly, run:\n\nOn Windows, the compilation requires Microsoft Visual Studio to be in . We recommend installing Visual Studio Community Edition and adding into using .\n\nPre-trained networks are stored as files on the StyleGAN2 Google Drive folder. Below, you can either reference them directly using the syntax , or download them manually and reference by filename.\n\nThe results are placed in . You can change the location with . For example, .\n\nYou can import the networks in your own Python code using . For this to work, you need to include the source directory in and create a default TensorFlow session by calling . See run_generator.py and pretrained_networks.py for examples.\n\nDatasets are stored as multi-resolution TFRecords, similar to the original StyleGAN. Each dataset consists of multiple files stored under a common directory, e.g., . In the following sections, the datasets are referenced using a combination of and arguments, e.g., .\n\nFFHQ. To download the Flickr-Faces-HQ dataset as multi-resolution TFRecords, run:\n\nLSUN. Download the desired LSUN categories in LMDB format from the LSUN project page. To convert the data to multi-resolution TFRecords, run:\n\nCustom. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords, run:\n\nTo find the matching latent vectors for a set of images, run:\n\nTo reproduce the training runs for config F in Tables 1 and 3, run:\n\nFor other configurations, see .\n\nWe have verified that the results match the paper when training with 1, 2, 4, or 8 GPUs. Note that training FFHQ at 1024×1024 resolution requires GPU(s) with at least 16 GB of memory. The following table lists typical training times using NVIDIA DGX-1 with 8 Tesla V100 GPUs:\n\nTraining curves for FFHQ config F (StyleGAN2) compared to original StyleGAN using 8 GPUs:\n\nAfter training, the resulting networks can be used the same way as the official pre-trained networks:\n\nTo reproduce the numbers for config F in Tables 1 and 3, run:\n\nFor other configurations, see the StyleGAN2 Google Drive folder.\n\nNote that the metrics are evaluated using a different random seed each time, so the results will vary between runs. In the paper, we reported the average result of running each metric 10 times. The following table lists the available metrics along with their expected runtimes and random variation:\n\nNote that some of the metrics cache dataset-specific data on the disk, and they will take somewhat longer when run for the first time.\n\nThis work is made available under the Nvidia Source Code License-NC. To view a copy of this license, visit https://nvlabs.github.io/stylegan2/license.html\n\nWe thank Ming-Yu Liu for an early review, Timo Viitanen for his help with code release, and Tero Kuosmanen for compute infrastructure."
    },
    {
        "link": "https://github.com/NVlabs/stylegan2-ada-pytorch",
        "document": "Abstract: Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42.\n\nFor business inquiries, please visit our website and submit the form: NVIDIA Research Licensing\n\nThis repository is a faithful reimplementation of StyleGAN2-ADA in PyTorch, focusing on correctness, performance, and compatibility.\n• Extensive verification of image quality, training curves, and quality metrics against the TensorFlow version.\n• Results are expected to match in all cases, excluding the effects of pseudo-random numbers and floating-point arithmetic.\n• Training is typically 5%–30% faster compared to the TensorFlow version on NVIDIA Tesla V100 GPUs.\n• Inference is up to 35% faster in high resolutions, but it may be slightly slower in low resolutions.\n• GPU memory usage is comparable to the TensorFlow version.\n• Faster startup time when training new networks (<50s), and also when using pre-trained networks (<4s).\n• New command line options for tweaking the training performance.\n• Compatible with old network pickles created using the TensorFlow version.\n• New ZIP/PNG based dataset format for maximal interoperability with existing 3rd party tools.\n• TFRecords datasets are no longer supported — they need to be converted to the new format.\n• New JSON-based format for logs, metrics, and training curves.\n• Training curves are also exported in the old TFEvents format if TensorBoard is installed.\n• Command line syntax is mostly unchanged, with a few exceptions (e.g., ).\n• Truncation is now disabled by default.\n• Linux and Windows are supported, but we recommend Linux for performance and compatibility reasons.\n• 1–8 high-end NVIDIA GPUs with at least 12 GB of memory. We have done all testing and development using NVIDIA DGX-1 with 8 Tesla V100 GPUs.\n• 64-bit Python 3.7 and PyTorch 1.7.1. See https://pytorch.org/ for PyTorch install instructions.\n• CUDA toolkit 11.0 or later. Use at least version 11.1 if running on RTX 3090. (Why is a separate CUDA toolkit installation required? See comments in #2.)\n• Python libraries: . We use the Anaconda3 2020.11 distribution which installs most of these by default.\n• Docker users: use the provided Dockerfile to build an image with the required library dependencies.\n\nThe code relies heavily on custom PyTorch extensions that are compiled on the fly using NVCC. On Windows, the compilation requires Microsoft Visual Studio. We recommend installing Visual Studio Community Edition and adding it into using .\n\nPre-trained networks are stored as files that can be referenced using local filenames or URLs:\n\nOutputs from the above commands are placed under , controlled by . Downloaded network pickles are cached under , which can be overridden by setting the environment variable. The default PyTorch extension build directory is , which can be overridden by setting .\n\nDocker: You can run the above curated image example using Docker as follows:\n\nNote: The Docker image requires NVIDIA driver release or later.\n\nLegacy networks: The above commands can load most of the network pickles created using the previous TensorFlow versions of StyleGAN2 and StyleGAN2-ADA. However, for future compatibility, we recommend converting such legacy pickles into the new format used by the PyTorch version:\n\nTo find the matching latent vector for a given image file, run:\n\nFor optimal results, the target image should be cropped and aligned similar to the FFHQ dataset. The above command saves the projection target , result , latent vector , and progression video . You can render the resulting latent vector by specifying for :\n\nYou can use pre-trained networks in your own Python code as follows:\n\nThe above code requires and to be accessible via . It does not need source code for the networks themselves — their class definitions are loaded from the pickle via .\n\nThe pickle contains three networks. and are instantaneous snapshots taken during training, and represents a moving average of the generator weights over several training steps. The networks are regular instances of , with all of their parameters and buffers placed on the CPU at import and gradient computation disabled by default.\n\nThe generator consists of two submodules, and , that can be executed separately. They also support various additional options:\n\nPlease refer to , , and for further examples.\n\nDatasets are stored as uncompressed ZIP archives containing uncompressed PNG files and a metadata file for labels.\n\nCustom datasets can be created from a folder containing images; see for more information. Alternatively, the folder can also be used directly as a dataset, without running it through first, but doing so may lead to suboptimal performance.\n\nLegacy TFRecords datasets are not supported — see below for instructions on how to convert them.\n\nStep 2: Extract images from TFRecords using from the TensorFlow version of StyleGAN2-ADA:\n\nStep 3: Create ZIP archive using from this repository:\n\nCIFAR-10: Download the CIFAR-10 python version and convert to ZIP archive:\n\nLSUN: Download the desired categories from the LSUN project page and convert to ZIP archive:\n\nStep 2: Extract 512x512 resolution crops using from the TensorFlow version of StyleGAN2-ADA:\n\nStep 3: Create ZIP archive using from this repository:\n\nIn its most basic form, training new networks boils down to:\n\nThe first command is optional; it validates the arguments, prints out the training configuration, and exits. The second command kicks off the actual training.\n\nIn this example, the results are saved to a newly created directory , controlled by . The training exports network pickles ( ) and example images ( ) at regular intervals (controlled by ). For each pickle, it also evaluates FID (controlled by ) and logs the resulting scores in (as well as TFEvents if TensorBoard is installed).\n\nThe name of the output directory reflects the training configuration. For example, indicates that the base configuration was , meaning that the hyperparameters were selected automatically for training on one GPU. The base configuration is controlled by :\n\nThe training configuration can be further customized with additional command line options:\n• amplifies the dataset with x-flips. Often beneficial, even with ADA.\n• overrides R1 gamma. We recommend trying a couple of different values for each new dataset.\n• enables pixel blitting but disables all other augmentations.\n\nPlease refer to for the full list.\n\nThe total training time depends heavily on resolution, number of GPUs, dataset, desired quality, and hyperparameters. The following table lists expected wallclock times to reach different points in the training, measured in thousands of real images shown to the discriminator (\"kimg\"):\n\nThe above measurements were done using NVIDIA Tesla V100 GPUs with default settings ( ). \"sec/kimg\" shows the expected range of variation in raw training performance, as reported in . \"GPU mem\" and \"CPU mem\" show the highest observed memory consumption, excluding the peak at the beginning caused by .\n\nIn typical cases, 25000 kimg or more is needed to reach convergence, but the results are already quite reasonable around 5000 kimg. 1000 kimg is often enough for transfer learning, which tends to converge significantly faster. The following figure shows example convergence curves for different datasets as a function of wallclock time, using the same settings as above:\n\nNote: serves as a reasonable first guess for the hyperparameters but it does not necessarily lead to optimal results for a given dataset. For example, yields considerably better FID for FFHQ-140k at 1024x1024 than illustrated above. We recommend trying out at least a few different values of for each new dataset.\n\nBy default, automatically computes FID for each network pickle exported during training. We recommend inspecting (or TensorBoard) at regular intervals to monitor the training progress. When desired, the automatic computation can be disabled with to speed up the training slightly (3%–9%).\n\nAdditional quality metrics can also be computed after the training:\n\nThe first example looks up the training configuration and performs the same operation as if had been specified during training. The second example downloads a pre-trained network pickle, in which case the values of and must be specified explicitly.\n\nNote that many of the metrics have a significant one-off cost when calculating them for the first time for a new dataset (up to 30min). Also note that the evaluation is done using a different random seed each time, so the results will vary if the same metric is computed multiple times.\n\nWe employ the following metrics in the ADA paper. Execution time and GPU memory usage is reported for one NVIDIA Tesla V100 GPU at 1024x1024 resolution:\n\nIn addition, the following metrics from the StyleGAN and StyleGAN2 papers are also supported:\n• GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium, Heusel et al. 2017\n• Improved Precision and Recall Metric for Assessing Generative Models, Kynkäänniemi et al. 2019\n\nThis work is made available under the Nvidia Source Code License.\n\nThis is a research reference implementation and is treated as a one-time code drop. As such, we do not accept outside code contributions in the form of pull requests.\n\nWe thank David Luebke for helpful comments; Tero Kuosmanen and Sabu Nadarajan for their support with compute infrastructure; and Edgar Schönfeld for guidance on setting up unconditional BigGAN."
    },
    {
        "link": "https://developer.nvidia.com/blog/synthesizing-high-resolution-images-with-stylegan2",
        "document": "GANs have captured the world’s imagination. Their ability to dream up realistic images of landscapes, cars, cats, people, and even video games, represents a significant step in artificial intelligence.\n\nOver the years, NVIDIA researchers have contributed several breakthroughs to GANs.\n\nThis new project called StyleGAN2, presented at CVPR 2020, uses transfer learning to generate a seemingly infinite numbers of portraits in an infinite variety of painting styles. The work builds on the team’s previously published StyleGAN project.\n\nShown in this new demo, the resulting model allows the user to create and fluidly explore portraits. This is done by separately controlling the content, identity, expression, and pose of the subject. Users can also modify the artistic style, color scheme, and appearance of brush strokes.\n\nThe model was trained using an NVIDIA DGX system comprised of eight NVIDIA V100 GPUs, with the cuDNN-accelerated TensorFlow deep learning framework.\n\n“Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality,” the researchers stated in their paper, Analyzing and Improving the Image Quality of StyleGAN.\n\nBecause of its interactivity, the resulting network can be a powerful tool for artistic expression, the researchers stated in the video.\n\nThe implementation and trained models are available on the StyleGAN2 GitHub repo. If you are attending CVPR 2020, you can also watch a talk by the authors."
    },
    {
        "link": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/research/models/stylegan2",
        "document": "StyleGAN2 pretrained models for FFHQ (aligned & unaligned), AFHQv2, CelebA-HQ, BreCaHAD, CIFAR-10, LSUN dogs, and MetFaces (aligned & unaligned) datasets. This model is ready for non-commercial uses.\n\nWe observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.\n\nYou can see the details of this model on this link: https://nvlabs.github.io/stylegan3 and the related paper can be find here: https://nvlabs.github.io/stylegan3/\n\nYou can train new networks using train.py. This release contains an interactive model visualization tool that can be used to explore various characteristics of a trained model. To start it, run:\n\nStyleGAN2 pretrained models for these datasets: FFHQ (aligned & unaligned), AFHQv2, CelebA-HQ, BreCaHAD, CIFAR-10, LSUN dogs, and MetFaces (aligned & unaligned) datasets.\n\nthe result quality and training time depend heavily on the exact set of options. The most important ones (--gpus, --batch, and --gamma) must be specified explicitly, and they should be selected with care. See python train.py --help for the full list of options and Training configurations for general guidelines & recommendations, along with the expected training speed & memory usage in different scenarios.\n\nThe results of each training run are saved to a newly created directory, for example ~/training-runs/00000-stylegan3-t-afhqv2-512x512-gpus8-batch32-gamma8.2. The training loop exports network pickles (network-snapshot-.pkl) and random image grids (fakes.png) at regular intervals (controlled by --snap). For each exported pickle, it evaluates FID (controlled by --metrics) and logs the result in metric-fid50k_full.jsonl. It also records various statistics in training_stats.jsonl, as well as *.tfevents if TensorBoard is installed.\n\nHow to Use this Model\n• Linux and Windows are supported, but we recommend Linux for performance and compatibility reasons.\n• 1–8 high-end NVIDIA GPUs with at least 12 GB of memory. We have done all testing and development using Tesla V100 and A100 GPUs.\n• 64-bit Python 3.8 and PyTorch 1.9.0 (or later). See https://pytorch.org for PyTorch install instructions.\n• GCC 7 or later (Linux) or Visual Studio (Windows) compilers. Recommended GCC version depends on CUDA version.\n\nDatasets are stored as uncompressed ZIP archives containing uncompressed PNG files and a metadata file dataset.json for labels.\n\nyou can check the output of the model in the paper at this address: https://arxiv.org/abs/2106.12423\n\nThis work is made available under the Nvidia Source Code License.\n\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards here. Please report security vulnerabilities or NVIDIA AI Concerns here."
    },
    {
        "link": "https://medium.com/data-science/stylegan-v2-notes-on-training-and-latent-space-exploration-e51cf96584b3",
        "document": "What: in this entry I’m going to present notes, thoughts and experimental results I collected while training multiple StyleGAN models and exploring the learned latent space.\n\nWhy: this is a dump of ideas/considerations that can range from the obvious to the “ holy moly”, meant to possibly provide insight — or starting point for discussions — for other people out there with similar interests and intents. As such it can be skimmed through to see if anything is of interest.\n\nWho: many considerations apply to both StyleGAN v1 and v2, but all generated results are from v2 models, unless explicitly specified.\n• Encoder for v1 (+ directions learning) and encoder for v2\n\nAll of the content here has been generated on top of such repos via my customized Jupyter notebooks.\n\nWorth mentioning people who diligently shared code, experiments and valuable suggestions: Gwern (TWDNE), pbaylies, gpt2ent, xsteenbrugge, Veqtor, Norod78, roadrunner01. All more than worth following."
    },
    {
        "link": "https://geeksforgeeks.org/image-generation-using-generative-adversarial-networks-gans",
        "document": "Generative Adversarial Networks (GANs) represent a revolutionary approach to artificial intelligence particularly for generating images. Introduced in 2014 GANs have significantly advanced the ability to create realistic and high-quality images from random noise.\n\nIn this article, we are going to train GANs model on MNIST dataset for generating images.\n\nGenerative Adversarial Networks (GANs) consist of two neural networks—the Generator and the Discriminator—that compete with each other. The Generator creates images from random noise aiming to make them look as realistic as possible while the Discriminator evaluates these images to determine whether they are real or fake.\n\nTraining Generative Adversarial Networks (GANs) is an iterative process that revolves around the interaction between two neural networks:\n\nThe Discriminator starts by being trained on a dataset containing real images. Its goal is to differentiate between these real images and fake images generated by the Generator. Through backpropagation and gradient descent, the Discriminator adjusts its parameters to improve its ability to accurately classify real and generated images.\n\nConcurrently the Generator is trained to produce images that are increasingly difficult for the Discriminator to distinguish from real images. Initially the Generator generates random noise but as training progresses it learns to generate images that resemble those in the training dataset. The Generator's parameters are adjusted based on the feedback from the Discriminator, optimizing the Generator's ability to create more realistic and high-quality images.\n\nImport necessary libraries including TensorFlow, Keras layers and models, NumPy for numerical operations and Matplotlib for plotting.\n\nProper data preparation is crucial for the successful training of neural networks. For the MNIST dataset the preprocessing steps include loading the dataset reshaping the images to ensure they are in the correct format for TensorFlow processing and normalizing the pixel values to the range [0,1]. Normalization helps stabilize the training process by keeping the input values small.\n\nThis step involves defining the architecture for both the generator and the discriminator using convolutional neural network (CNN) layers, tailored to efficiently process and generate image data.\n\nThe generator’s role in a GAN is to synthesize new images that mimic the distribution of a given dataset. In this case, we use convolutional transpose layers, which are effective for upscaling the input and creating detailed images from a lower-dimensional noise vector.\n• Reshape : Transforms the feature map into a 3D shape that can be processed by convolutional layers.\n• Conv2DTranspose Layers : These layers perform upscaling and convolution simultaneously, gradually increasing the resolution of the generated image.\n• BatchNormalization : Stabilizes the learning process and helps in faster convergence.\n• Activation Functions : 'ReLU' is used for non-linearity in intermediate layers, while 'sigmoid' is used in the output layer to normalize the pixel values between 0 and 1.\n\nThe discriminator is a binary classifier that determines whether a given image is real (from the dataset) or fake (generated by the generator).\n• Conv2D Layers : Perform convolutions with a stride of 2 to downsample the image, reducing its dimensionality and increasing the field of view of the filters.\n• BatchNormalization : Used here as well to ensure stable training.\n• Flatten : Converts the 2D feature maps into a 1D feature vector necessary for classification.\n• Dense Output Layer : Outputs a single probability indicating the likelihood that the input image is real.\n\nFirst compile and set up the combined GAN model which connects the generator and discriminator. This setup is crucial for training the generator while keeping the discriminator's parameters fixed during the generator's training updates.\n\nThe training loop involves alternately training the discriminator and the generator. The discriminator learns to distinguish real images from the fake ones produced by the generator. Simultaneously, the generator learns to fool the discriminator by generating increasingly realistic images.\n• Mode Collapse: It occurs when the Generator produces limited varieties of outputs failing to explore the full diversity of the data distribution.\n• Training Instability: This manifests as oscillations or divergence in training where the Generator and Discriminator struggle to reach equilibrium.\n• Hyperparameter Sensitivity: It include parameters such as learning rates and network architectures that significantly impact GANs' performance and stability."
    },
    {
        "link": "https://realpython.com/generative-adversarial-networks",
        "document": "Generative adversarial networks (GANs) are neural networks that generate material, such as images, music, speech, or text, that is similar to what humans produce.\n\nGANs have been an active topic of research in recent years. Facebook’s AI research director Yann LeCun called adversarial training “the most interesting idea in the last 10 years” in the field of machine learning. Below, you’ll learn how GANs work before implementing two generative models of your own.\n• What a generative model is and how it differs from a discriminative model\n• How GANs are structured and trained\n• How to build your own GAN using PyTorch\n• How to train your GAN for practical applications using a GPU and PyTorch\n\nIf you’ve studied neural networks, then most of the applications you’ve come across were likely implemented using discriminative models. Generative adversarial networks, on the other hand, are part of a different class of models known as generative models. Discriminative models are those used for most supervised classification or regression problems. As an example of a classification problem, suppose you’d like to train a model to classify images of handwritten digits from 0 to 9. For that, you could use a labeled dataset containing images of handwritten digits and their associated labels indicating which digit each image represents. During the training process, you’d use an algorithm to adjust the model’s parameters. The goal would be to minimize a loss function so that the model learns the probability distribution of the output given the input. After the training phase, you could use the model to classify a new handwritten digit image by estimating the most probable digit the input corresponds to, as illustrated in the figure below: You can picture discriminative models for classification problems as blocks that use the training data to learn the boundaries between classes. They then use these boundaries to discriminate an input and predict its class. In mathematical terms, discriminative models learn the conditional probability P(y|x) of the output y given the input x. Besides neural networks, other structures can be used as discriminative models such as logistic regression models and support vector machines (SVMs). Generative models like GANs, however, are trained to describe how a dataset is generated in terms of a probabilistic model. By sampling from a generative model, you’re able to generate new data. While discriminative models are used for supervised learning, generative models are often used with unlabeled datasets and can be seen as a form of unsupervised learning. Using the dataset of handwritten digits, you could train a generative model to generate new digits. During the training phase, you’d use some algorithm to adjust the model’s parameters to minimize a loss function and learn the probability distribution of the training set. Then, with the model trained, you could generate new samples, as illustrated in the following figure: To output new samples, generative models usually consider a stochastic, or random, element that influences the samples generated by the model. The random samples used to drive the generator are obtained from a latent space in which the vectors represent a kind of compressed form of the generated samples. Unlike discriminative models, generative models learn the probability P(x) of the input data x, and by having the distribution of the input data, they’re able to generate new data instances. Note: Generative models can also be used with labeled datasets. When they are, they’re trained to learn the probability P(x|y) of the input x given the output y. They can also be used for classification tasks, but in general, discriminative models perform better when it comes to classification. You can find more information on the relative strengths and weaknesses of discriminative and generative classifiers in the article On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes. Although GANs have received a lot of attention in recent years, they’re not the only architecture that can be used as a generative model. Besides GANs, there are various other generative model architectures such as:\n• Models that predict the next word in a sequence, like GPT-2 However, GANs have attracted the most public interest of late due to the exciting results in image and video generation. Now that you know the basics of generative models, you’ll see how GANs work and how to train them.\n\nGenerative adversarial networks consist of an overall structure composed of two neural networks, one called the generator and the other called the discriminator. The role of the generator is to estimate the probability distribution of the real samples in order to provide generated samples resembling real data. The discriminator, in turn, is trained to estimate the probability that a given sample came from the real data rather than being provided by the generator. These structures are called generative adversarial networks because the generator and discriminator are trained to compete with each other: the generator tries to get better at fooling the discriminator, while the discriminator tries to get better at identifying generated samples. To understand how GAN training works, consider a toy example with a dataset composed of two-dimensional samples (x₁, x₂), with x₁ in the interval from 0 to 2π and x₂ = sin(x₁), as illustrated in the following figure: As you can see, this dataset consists of points (x₁, x₂) located over a sine curve, having a very particular distribution. The overall structure of a GAN to generate pairs (x̃₁, x̃₂) resembling the samples of the dataset is shown in the following figure: The generator G is fed with random data from a latent space, and its role is to generate data resembling the real samples. In this example, you have a two-dimensional latent space, so that the generator is fed with random (z₁, z₂) pairs and is required to transform them so that they resemble the real samples. The structure of the neural network G can be arbitrary, allowing you to use neural networks as a multilayer perceptron (MLP), a convolutional neural network (CNN), or any other structure as long as the dimensions of the input and output match the dimensions of the latent space and the real data. The discriminator D is fed with either real samples from the training dataset or generated samples provided by G. Its role is to estimate the probability that the input belongs to the real dataset. The training is performed so that D outputs 1 when it’s fed a real sample and 0 when it’s fed a generated sample. As with G, you can choose an arbitrary neural network structure for D as long as it respects the necessary input and output dimensions. In this example, the input is two-dimensional. For a binary discriminator, the output may be a scalar ranging from 0 to 1. The GAN training process consists of a two-player minimax game in which D is adapted to minimize the discrimination error between real and generated samples, and G is adapted to maximize the probability of D making a mistake. Although the dataset containing the real data isn’t labeled, the training processes for D and G are performed in a supervised way. At each step in the training, D and G have their parameters updated. In fact, in the original GAN proposal, the parameters of D are updated k times, while the parameters of G are updated only once for each training step. However, to make the training simpler, you can consider k equal to 1. To train D, at each iteration you label some real samples taken from the training data as 1 and some generated samples provided by G as 0. This way, you can use a conventional supervised training framework to update the parameters of D in order to minimize a loss function, as shown in the following scheme: For each batch of training data containing labeled real and generated samples, you update the parameters of D to minimize a loss function. After the parameters of D are updated, you train G to produce better generated samples. The output of G is connected to D, whose parameters are kept frozen, as depicted here: You can imagine the system composed of G and D as a single classification system that receives random samples as input and outputs the classification, which in this case can be interpreted as a probability. When G does a good enough job to fool D, the output probability should be close to 1. You could also use a conventional supervised training framework here: the dataset to train the classification system composed of G and D would be provided by random input samples, and the label associated with each input sample would be 1. During training, as the parameters of D and G are updated, it’s expected that the generated samples given by G will more closely resemble the real data, and D will have more trouble distinguishing between real and generated data. Now that you know how GANs work, you’re ready to implement your own using PyTorch.\n\nAs a first experiment with generative adversarial networks, you’ll implement the example described in the previous section. To run the example, you’re going to use the PyTorch library, which you can install using the Anaconda Python distribution and the conda package and environment management system. To learn more about Anaconda and conda, check out the tutorial on Setting Up Python for Machine Learning on Windows. To begin, create a conda environment and activate it: After you activate the conda environment, your prompt will show its name, . Then you can install the necessary packages inside the environment: Since PyTorch is a very actively developed framework, the API may change on new releases. To ensure the example code will run, you install the specific version . Besides PyTorch, you’re going to use Matplotlib to work with plots and a Jupyter Notebook to run the code in an interactive environment. Doing so isn’t mandatory, but it facilitates working on machine learning projects. For a refresher on working with Matplotlib and Jupyter Notebooks, take a look at Python Plotting With Matplotlib (Guide) and Jupyter Notebook: An Introduction. Before opening Jupyter Notebook, you need to register the conda environment so that you can create Notebooks using it as the kernel. To do that, with the environment activated, run the following command: Now you can open Jupyter Notebook by running . Create a new Notebook by clicking New and then selecting gan. Inside the Notebook, begin by importing the necessary libraries: Here, you import the PyTorch library with . You also import just to be able to set up the neural networks in a less verbose way. Then you import to obtain the value of the pi constant, and you import the Matplotlib plotting tools as as usual. It’s a good practice to set up a random generator seed so that the experiment can be replicated identically on any machine. To do that in PyTorch, run the following code: The number represents the random seed used to initialize the random number generator, which is used to initialize the neural network’s weights. Despite the random nature of the experiment, it must provide the same results as long as the same seed is used. Now that the environment is set, you can prepare the training data. The training data is composed of pairs (x₁, x₂) so that x₂ consists of the value of the sine of x₁ for x₁ in the interval from 0 to 2π. You can implement it as follows: Here, you compose a training set with pairs (x₁, x₂). In line 2, you initialize , a tensor with dimensions of rows and columns, all containing zeros. A tensor is a multidimensional array similar to a NumPy array. In line 3, you use the first column of to store random values in the interval from to . Then, in line 4, you calculate the second column of the tensor as the sine of the first column. Next, you’ll need a tensor of labels, which are required by PyTorch’s data loader. Since GANs make use of unsupervised learning techniques, the labels can be anything. They won’t be used, after all. In line 5, you create , a tensor filled with zeros. Finally, in lines 6 to 8, you create as a list of tuples, with each row of and represented in each tuple as expected by PyTorch’s data loader. You can examine the training data by plotting each point (x₁, x₂): The output should be something similar to the following figure: With , you can create a PyTorch data loader: Here, you create a data loader called , which will shuffle the data from and return batches of samples that you’ll use to train the neural networks. After setting up the training data, you need to create the neural networks for the discriminator and generator that will compose the GAN. In the following section, you’ll implement the discriminator. In PyTorch, the neural network models are represented by classes that inherit from , so you’ll have to define a class to create the discriminator. For more information on defining classes, take a look at Object-Oriented Programming (OOP) in Python. The discriminator is a model with a two-dimensional input and a one-dimensional output. It’ll receive a sample from the real data or from the generator and will provide the probability that the sample belongs to the real training data. The code below shows how to create a discriminator: You use to build the model. First, you need to call to run from . The discriminator you’re using is an MLP neural network defined in a sequential way using . It has the following characteristics:\n• Lines 5 and 6: The input is two-dimensional, and the first hidden layer is composed of neurons with ReLU activation.\n• Lines 8, 9, 11, and 12: The second and third hidden layers are composed of and neurons, respectively, with ReLU activation.\n• Lines 14 and 15: The output is composed of a single neuron with sigmoidal activation to represent a probability.\n• Lines 7, 10, and 13: After the first, second, and third hidden layers, you use dropout to avoid overfitting. Finally, you use to describe how the output of the model is calculated. Here, represents the input of the model, which is a two-dimensional tensor. In this implementation, the output is obtained by feeding the input to the model you’ve defined without any other processing. After declaring the discriminator class, you should instantiate a object: represents an instance of the neural network you’ve defined and is ready to be trained. However, before you implement the training loop, your GAN also needs a generator. You’ll implement one in the next section. In generative adversarial networks, the generator is the model that takes samples from a latent space as its input and generates data resembling the data in the training set. In this case, it’s a model with a two-dimensional input, which will receive random points (z₁, z₂), and a two-dimensional output that must provide (x̃₁, x̃₂) points resembling those from the training data. The implementation is similar to what you did for the discriminator. First, you have to create a class that inherits from , defining the neural network architecture, and then you need instantiate a object: Here, represents the generator neural network. It’s composed of two hidden layers with and neurons, both with ReLU activation, and a linear activation layer with neurons in the output. This way, the output will consist of a vector with two elements that can be any value ranging from negative infinity to infinity, which will represent (x̃₁, x̃₂). Now that you’ve defined the models for the discriminator and generator, you’re ready to perform the training! Before training the models, you need to set up some parameters to use during training: Here you set up the following parameters:\n• Line 1 sets the learning rate ( ), which you’ll use to adapt the network weights.\n• Line 2 sets the number of epochs ( ), which defines how many repetitions of training using the whole training set will be performed.\n• Line 3 assigns the variable to the binary cross-entropy function , which is the loss function that you’ll use to train the models. The binary cross-entropy function is a suitable loss function for training the discriminator because it considers a binary classification task. It’s also suitable for training the generator since it feeds its output to the discriminator, which provides a binary observable output. PyTorch implements various weight update rules for model training in . You’ll use the Adam algorithm to train the discriminator and generator models. To create the optimizers using , run the following lines: Finally, you need to implement a training loop in which training samples are fed to the models, and their weights are updated to minimize the loss function: For GANs, you update the parameters of the discriminator and the generator at each training iteration. As is generally done for all neural networks, the training process consists of two loops, one for the training epochs and the other for the batches for each epoch. Inside the inner loop, you begin preparing the data to train the discriminator:\n• Line 2: You get the real samples of the current batch from the data loader and assign them to . Notice that the first dimension of the tensor has the number of elements equal to . This is the standard way of organizing data in PyTorch, with each line of the tensor representing one sample from the batch.\n• Line 4: You use to create labels with the value for the real samples, and then you assign the labels to .\n• Lines 5 and 6: You create the generated samples by storing random data in , which you then feed to the generator to obtain .\n• Line 7: You use to assign the value to the labels for the generated samples, and then you store the labels in .\n• Lines 8 to 11: You concatenate the real and generated samples and labels and store them in and , which you’ll use to train the discriminator. Next, in lines 14 to 19, you train the discriminator:\n• Line 14: In PyTorch, it’s necessary to clear the gradients at each training step to avoid accumulating them. You do this using .\n• Line 15: You calculate the output of the discriminator using the training data in .\n• Lines 16 and 17: You calculate the loss function using the output from the model in and the labels in .\n• Line 18: You calculate the gradients to update the weights with .\n• Line 19: You update the discriminator weights by calling . Next, in line 22, you prepare the data to train the generator. You store random data in , with a number of lines equal to . You use two columns since you’re providing two-dimensional data as input to the generator. You train the generator in lines 25 to 32:\n• Line 25: You clear the gradients with .\n• Line 26: You feed the generator with and store its output in .\n• Line 27: You feed the generator’s output into the discriminator and store its output in , which you’ll use as the output of the whole model.\n• Lines 28 to 30: You calculate the loss function using the output of the classification system stored in and the labels in , which are all equal to .\n• Lines 31 and 32: You calculate the gradients and update the generator weights. Remember that when you trained the generator, you kept the discriminator weights frozen since you created with its first argument equal to . Finally, on lines 35 to 37, you display the values of the discriminator and generator loss functions at the end of each ten epochs. Since the models used in this example have few parameters, the training will be complete in a few minutes. In the following section, you’ll use the trained GAN to generate some samples. Checking the Samples Generated by the GAN Generative adversarial networks are designed to generate data. So, after the training process is finished, you can get some random samples from the latent space and feed them to the generator to obtain some generated samples: Then you can plot the generated samples and check if they resemble the training data. Before plotting the data, you’ll need to use to return a tensor from the PyTorch computational graph, which you’ll then use to calculate the gradients: The output should be similar to the following figure: You can see the distribution of the generated data resembles the one from the real data. By using a fixed latent space samples tensor and feeding it to the generator at the end of each epoch during the training process, you can visualize the evolution of the training: Note that at the beginning of the training process, the generated data distribution is very different from the real data. However, as the training progresses, the generator learns the real data distribution. Now that you’ve done your first implementation of a generative adversarial network, you’ll go through a more practical application using images.\n\nGenerative adversarial networks can also generate high-dimensional samples such as images. In this example, you’re going to use a GAN to generate images of handwritten digits. For that, you’ll train the models using the MNIST dataset of handwritten digits, which is included in the package. To begin, you need to install in the activated conda environment: Again, you’re using a specific version of to assure the example code will run, just like you did with . With the environment set up, you can start implementing the models in Jupyter Notebook. Open it and create a new Notebook by clicking on New and then selecting gan. As in the previous example, you start by importing the necessary libraries: Besides the libraries you’ve imported before, you’re going to need and to obtain the training data and perform image conversions. Again, set up the random generator seed to be able to replicate the experiment: Since this example uses images in the training set, the models need to be more complex, with a larger number of parameters. This makes the training process slower, taking about two minutes per epoch when running on CPU. You’ll need about fifty epochs to obtain a relevant result, so the total training time when using a CPU is around one hundred minutes. To reduce the training time, you can use a GPU to train the model if you have one available. However, you’ll need to manually move tensors and models to the GPU in order to use them in the training process. You can ensure your code will run on either setup by creating a object that points either to the CPU or, if one is available, to the GPU: Later, you’ll use this to set where tensors and models should be created, using the GPU if available. Now that the basic environment is set, you can prepare the training data. The MNIST dataset consists of 28 × 28 pixel grayscale images of handwritten digits from 0 to 9. To use them with PyTorch, you’ll need to perform some conversions. For that, you define , a function to be used when loading the data: The function has two parts:\n• converts the range of the tensor coefficients. The original coefficients given by range from 0 to 1, and since the image backgrounds are black, most of the coefficients are equal to 0 when they’re represented using this range. changes the range of the coefficients to -1 to 1 by subtracting from the original coefficients and dividing the result by . With this transformation, the number of elements equal to 0 in the input samples is dramatically reduced, which helps in training the models. The arguments of are two tuples, and , with representing the number of channels of the images. Grayscale images such as those in MNIST dataset have only one channel, so the tuples have only one value. Then, for each channel of the image, subtracts from the coefficients and divides the result by . Now you can load the training data using and perform the conversions using : The argument ensures that the first time you run the above code, the MNIST dataset will be downloaded and stored in the current directory, as indicated by the argument . Now that you’ve created , you can create the data loader as you did before: You can use Matplotlib to plot some samples of the training data. To improve the visualization, you can use to reverse the color map and plot the digits in black over a white background: The output should be something similar to the following: As you can see, there are digits with different handwriting styles. As the GAN learns the distribution of the data, it’ll also generate digits with different handwriting styles. Now that you’ve prepared the training data, you can implement the discriminator and generator models. Implementing the Discriminator and the Generator In this case, the discriminator is an MLP neural network that receives a 28 × 28 pixel image and provides the probability of the image belonging to the real training data. You can define the model with the following code: To input the image coefficients into the MLP neural network, you vectorize them so that the neural network receives vectors with coefficients. The vectorization occurs in the first line of , as the call to converts the shape of the input tensor. In this case, the original shape of the input is 32 × 1 × 28 × 28, where 32 is the batch size you’ve set up. After the conversion, the shape of becomes 32 × 784, with each line representing the coefficients of an image of the training set. To run the discriminator model using the GPU, you have to instantiate it and send it to the GPU with . To use a GPU when there’s one available, you can send the model to the object you created earlier: Since the generator is going to generate more complex data, it’s necessary to increase the dimensions of the input from the latent space. In this case, the generator is going to be fed a 100-dimensional input and will provide an output with 784 coefficients, which will be organized in a 28 × 28 tensor representing an image. In line 12, you use the hyperbolic tangent function as the activation of the output layer since the output coefficients should be in the interval from -1 to 1. In line 20, you instantiate the generator and send it to to use the GPU if one is available. Now that you have the models defined, you’ll train them using the training data. To train the models, you need to define the training parameters and optimizers like you did in the previous example: To obtain a better result, you decrease the learning rate from the previous example. You also set the number of epochs to to reduce the training time. The training loop is very similar to the one you used in the previous example. In the highlighted lines, you send the training data to to use the GPU if available: Some of the tensors don’t need to be sent to the GPU explicitly with . This is the case with in line 11, which will already be sent to an available GPU since and were sent to the GPU previously. Since this example features more complex models, the training may take a bit more time. After it finishes, you can check the results by generating some samples of handwritten digits. Checking the Samples Generated by the GAN To generate handwritten digits, you have to take some random samples from the latent space and feed them to the generator: To plot , you need to move the data back to the CPU in case it’s running on the GPU. For that, you can simply call . As you did previously, you also need to call before using Matplotlib to plot the data: The output should be digits resembling the training data, as in the following figure: After fifty epochs of training, there are several generated digits that resemble the real ones. You can improve the results by considering more training epochs. As with the previous example, by using a fixed latent space samples tensor and feeding it to the generator at the end of each epoch during the training process, you can visualize the evolution of the training: You can see that at the beginning of the training process, the generated images are completely random. As the training progresses, the generator learns the distribution of the real data, and at about twenty epochs, some generated digits already resemble real data."
    },
    {
        "link": "https://blog.ovhcloud.com/understanding-image-generation-beginner-guide-generative-adversarial-networks-gan",
        "document": "All the code related to this article is available in our dedicated GitHub repository. You can reproduce all the experiments with OVHcloud AI Notebooks.\n\nHave you ever been amazed by what generative artificial intelligence could do, and wondered how it can generate realistic images 🤯🎨?\n\nIn this tutorial, we will embark on an exciting journey into the world of Generating Adversarial Networks (GANs), a revolutionary concept in generative AI. No prior experience is necessary to follow along. We will walk you through every step, starting with the basic concepts and gradually building up to the implementation of Deep Convolutional GANs (DCGANs).\n\nBy the end of this tutorial, you will be able to generate your own images!\n\nGANs have been introduced by Ian Goodfellow et al. in 2014 in the paper Generative Adversarial Nets. GANs have become very popular last years, allowing us, for example, to:\n• Transfer image style of one image to another (Black and white to color)\n\nWhat is a GAN and how it works?\n\nA GAN is composed of two main components: a generator G and a discriminator D.\n\nEach component is a neural network, but their roles are different:\n• The purpose of the generator G is to reproduce the data distribution of the training data 𝑥, to generate synthetic samples for the same data distribution. These data are often images, but can also be audio or text.\n• On the other hand, the discriminator D is a kind of judge who will estimate whether a sample 𝑥 is real or fake (has been generated). It is in fact a classifier that will say if a sample comes from the real data distribution or the generator.\n\nDuring training, the generator starts with a vector of random noise (z) as input and produces synthetic samples G(z).\n\nAs training progresses, it refines its output, making the generated data G(z) more and more similar to the real data. The goal of the generator is to outsmart the discriminator into classifying its generated samples as real.\n\nMeanwhile, the discriminator is presented with both real samples from the training data and fake samples from the generator. As it learns to discriminate between the two, it provides feedback to the generator about the quality of its generated samples. This is why the term “adversarial“ is used here.\n\nIn fact, GANs come from game theory, where D and G are playing a two-player minimax game with the following value function:\n\nAs we can observe, the discriminator aims to maximize the V function. To do this, it must maximize each of the two parts of the equation that will be added together. This means maximizing log(D(x)), so D(x) in the first equation part (probability of real data), and minimizing the D(G(z)) in the second part (probability of fake data).\n\nSimultaneously, the generator tries to minimize the function. It only comes into play in the second part of the function, where it tries to obtain the highest value of D(G(z)) in order to fool the discriminator.\n\nThis constant confrontation between the generator and the discriminator creates an iterative learning process, where the generator gradually improves to produce increasingly realistic G(z) samples, and the discriminator becomes increasingly accurate in its distinction of the data presented to it.\n\nIn an ideal scenario, this iterative process would reach an equilibrium point, where the generator produces data that is indistinguishable from real data, and the discriminator’s performance is 50% (random guessing).\n\nGANs may not always reach this equilibrium due to the training process being sensitive to factors (architecture, hyperparameters, dataset complexity). The generator and discriminator may reach a dead end, oscillating between solutions or facing mode collapse, resulting in limited sample diversity. Also, it is important that discriminator does not start off too strong, otherwise the generator will not get any information on how to improve itself, since it does not know what the real data looks like, as shown in the illustration above.\n\nDCGAN has been introduced in 2016 by Alec Radford et al. in the paper Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.\n\nIts new convolutional architecture has considerably improved the quality and stability of image synthesis compared to classical GANs. Here are the major changes:\n• Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator), making them exceptionally well-suited for image generation tasks.\n• Use batchnorm in both the generator and the discriminator.\n• Use ReLU activation in generator for all layers except for the output, which uses tanh.\n• Use LeakyReLU activation in the discriminator for all layer.\n\nThe operation principles of these layers will not be explained in this tutorial.\n\nNow that we know the concept of image generation, let’s try to put it into practice!\n\nIn this tutorial, we will implement a DCGAN architecture and train it on a medical dataset to generate new images. This dataset is the Chest X-Ray Pneumonia. All the code explained here will run on a single GPU, linked to OVHcloud AI Notebooks, and is given in our GitHub repository.\n\n1 – Explore dataset and prepare it for training\n\nThe Chest X-Ray Pneumonia dataset contains 5,863 X-Ray images. This may not be sufficient for training a robust DCGAN, but we are going to try! Indeed, the DCGAN research paper is conducting its study on a dataset of over 60,000 images.\n\nAdditionally, it is important to consider that the dataset contains two classes (Pneumonia/Normal). While we will not separate the classes for data quantity purposes, improving our network’s performance could be beneficial. Furthermore, it is advisable to verify if the classes are well-balanced.\n\nOnly the training subset will be used here (5,221 images). Let’s take a look at our images:\n\nWe notice that we have quite similar images. The backgrounds are identical, and the chests are often centered in the same way, which should help the network learn.\n\nData pre-processing is a crucial step when you want to facilitate and accelerate model convergence and obtain high-quality results. This pre-processing can be broken down into various generic operations that are commonly applied.\n\nEach image in the dataset will be transformed. They are then assembled in packets of 128 images, which we call batches. This avoids loading the dataset all at once, which could use up a lot of memory. This also makes the most of GPUs parallelism.\n• Resize images to (64x64xchannels), dimensions expected by our DCGAN. This avoids keeping the original dimensions of the images, which are all different. This also reduces the number of pixels which accelerates the model training (computation cost).\n• Standardize & Normalize the image’s pixel values, which improves training performance in AI.\n\nIf original images are smaller than the desired size, transformation will pad the images to reach the specified size.\n\nWe won’t show you the code for these transformations here, but as mentioned earlier, you can find it in its entirety on our GitHub repository. You can reproduce all the experiments with OVHcloud AI Notebooks.\n\nNow that the images are ready, we can define our DCGAN:\n\nAs shown in the image above, the generator architecture is designed to take a random noise vector z as input and transform it into a (3x64x64) image, which is the same size as the images in the training dataset.\n\nTo do this, it uses transposed convolutions (also falsely known as deconvolutions) to progressively upsample the noise vector z until it reaches the desired output image size. In fact, the transposed convolutions help the generator capture complex patterns and generate realistic images during the training process.\n\nThe final Tanh() activation function ensures that the pixel values of the generated images are in the range [-1, 1], which also corresponds to our transformed training images (we had normalized them).\n\nThe code for implementing this generator is given in its entirety on our GitHub repository.\n\nAs a reminder, the discriminator acts as a sample classifier. Its aim is to distinguish the data generated by the generator from the real data in the training dataset.\n\nAs shown in the image above, the discriminator takes an input image of size (3x64x64) and outputs a probability, indicating if the input image is real (1) or fake (0).\n\nTo do this, it uses convolutional layers, batch normalization layers, and LeakyReLU functions, which are presented in the paper as architecture guidelines to follow. Each convolutional block is designed to capture features of the input images, moving from low-level features such as edges and textures for the first blocks, to more abstract and complex features such as shapes and objects for the last.\n\nProbability is obtained thanks to the use of the sigmoid activation, which squashes the output to the range [0, 1].\n\nThe code for implementing this discriminator is given in its entirety on our GitHub repository.\n\nNow that we have our adversarial networks, we need to define the loss function.\n\nThe adversarial loss V(D, G) can be approximated using the Binary Cross Entropy (BCE) loss function, which is commonly used for GANs because it measures the binary cross-entropy between the discriminator’s output (probability) and the ground truth labels during training (here we fix real=1 or fake=0). It will calculate the loss for both the generator and the discriminator during backpropagation.\n\nBCE Loss is computed with the following equation, where target is the ground truth label (1 or 0), and ŷ is the discriminator’s probability output:\n\nIf we compare this equation to our previous V(D, G) objective, we can see that BCE loss term for real data samples corresponds to the first term in V(D, G), log(D(x)), and the BCE loss term for fake data samples corresponds to the second term in V(D, G), log(1 – D(G(z))).\n\nIn this binary case, the BCE can be represented by two distinct curves, which describe how the loss varies as a function of the predictions ŷ of the model. The first shows the loss as a function of the calculated probability, for a synthetic sample (label y = 0). The second describes the loss for a real sample (label y = 1).\n\nWe can see that the further the prediction ŷ is from the actual label assigned (target), the greater the loss. On the other hand, a prediction that is close to the truth will generate a loss very close to zero, which will not impact the model since it appears to classify the samples successfully.\n\nDuring training, the goal is to minimize the BCE loss. This way, the discriminator will learn to correctly classify real and generated samples, while the generator will learn to generate samples that can “fool” the discriminator into classifying them as real.\n\nHyperparameters were chosen according to the indications given by in the DCGAN paper.\n\nWe are now ready to train our DCGAN !\n\nTo monitor the generator’s learning progress, we will create a constant noise vector, denoted as .\n\nDuring the training loop, we will regularly feed this into the generator. Using a same constant vector makes it possible to generate similar images each time, and to observe the evolution of the samples produced by the generator over the training cycles.\n\nAlso, we will compute the BCE Loss of the Discriminator and the Generator separately. This will enable them to improve over the training cycles. For each batch, these losses will be calculated and saved into lists, enabling us to plot the losses after training for each training iteration.\n\nThanks to our fixed noise vector, we were able to capture the evolution of the generated images, providing an overview of how the model learned to reproduce the distribution of training data over time.\n\nHere are the samples generated by our model during training, when fed with a fixed noise, over 100 epochs. For visualization, a display of 9 generated images was chosen :\n\nAt the start of the training process (epoch 1), the images generated show the characteristics of the random noise vector.\n\nAs the training progresses, the weights of the discriminator and generator are updated. Noticeable changes occur in the generated images. Epochs 5, 10 and 20 show quick and subtle evolution of the model, which begins to capture more distinct shapes and structures.\n\nNext epochs show an improvement in edges and details. Generated samples become sharper and more identifiable, and by epoch 100 the images are quite realistic despite the limited data available (5,221 images).\n\nDo not hesitate to play with the hyperparameters to try and vary your results! You can also check out the GAN hacks repo, which shares many tips dedicated to training GANs. Training time will vary according to your resources and the number of images.\n\nOnce the generator has been trained over 100 epochs, we are free to generated unlimited new images, based on a new random noise vector each time.\n\nIn order to retain only relevant samples, a data post-processing step was set up to assess the quality of the images generated. All generated images were sent to the trained discriminator. Its job is to evaluate the probability of the generated samples, and keep only those which have obtained a probability greater than a fixed threshold (0.8 for example).\n\nThis way, we have obtained the following images, compared to the original ones. We can see that despite the small number of images in our dataset, the model was able to identify and learn the distribution of the real images data and reproduce them in a realistic way:\n\nOriginal dataset images (left), compared with images selected from generated samples (right)\n\nA DCGAN model (and GANs in general) can be evaluated in several ways. A research paper has been published on this subject.\n\nOn the quantitative side, the evolution of the BCE loss of the generator and the discriminator provide indications of the quality of the model during training.\n\nThe evolution of these losses is illustrated in the figure below, where the discriminator losses are shown in orange and the generator losses in blue, over a total of 4100 iterations. Each iteration corresponds to a complete pass of the dataset, which is split into 41 batches of 128 images. Since the model has been trained over 100 epochs, loss tracking is available over 4100 iterations (41*100).\n\nAt the start of training, both curves show high loss values, indicating an unstable start of the DCGAN. This results in very unrealistic images being generated, where the nature of the random noise is still too present (see epoch 1 on the previous image). The discriminator is therefore too powerful for the moment.\n\nA few iterations later, the losses converge towards lower values, demonstrating the improvement in the model’s performance.\n\nHowever, from epoch 10, a trend emerges. The discriminator loss begins to decrease very slightly, indicating an improvement in its ability to determine which samples are genuine and which are synthetic. On the other hand, the generator’s loss shows a slight increase, suggesting that it needs to improve in order to generate images capable of deceiving its adversary.\n\nMore generally, fluctuations are observed throughout training due to the competitive nature of the network, where the generator and discriminator are constantly adjusting relative to each other. These moments of fluctuation may reflect attempts to adjust the two networks. Unfortunately, they do not ultimately appear to lead to an overall reduction in network loss.\n\nLosses are not the only performance indicator. They are often insufficient to assess the visual quality of the images generated.\n\nThis is confirmed by an analysis of the previous graphs, where we inevitably notice that the images generated at epoch 10 are not the most realistic, while the loss is approximately the same as that obtained at epoch 100.\n\nOne commonly used method is human visual assessment. However, this manual assessment has a number of limitations. It is subjective, does not fully reflect the capabilities of the models, cannot be reproduced and is expensive.\n\nResearch is therefore focusing on finding new, more reliable and less costly methods. This is particularly the case with CAPTCHAs, tests designed to check whether a user is a human or a robot before accessing content. These tests sometimes present pairs of generated and real images where the user has to indicate which of the two seems more authentic. This ultimately amounts to training a discriminator and a generator manually.\n\nAll the code related to this article is available in our dedicated GitHub repository. You can reproduce all the experiments with OVHcloud AI Notebooks.\n\nI hope you have enjoyed this post!\n\nYou are now more comfortable with image generation and the concept of Generative Adversarial Networks! Now you know how to generate images from your own dataset, even if it’s not very large!\n\nYou can train your own network on your dataset and generate images of faces, objects and landscapes. Happy GANning! 🎨🚀\n\nYou can check our other computer vision articles to learn how to:"
    },
    {
        "link": "https://medium.com/data-science-at-microsoft/synthetic-data-generation-using-generative-adversarial-networks-gans-part-2-9a078741d3ce",
        "document": "Generative Adversarial Networks — GANs — employ a deep learning model to generate synthetic data that mimics real data. They have multiple applications, including processing and working with images, text, and other data. The goal of this two-part article series is to give beginners a complete understanding of GANs. In the first article of the series, my colleague Daniel Huang introduced GANs at a beginner level, provided an overview of how they work, and covered various use cases. In this article, I take a deep dive into the inner workings of GANs, providing more information to enable you to gain a depth of understanding that will allow you to begin using GANs in your own work.\n\nA major category of Machine Learning (ML) techniques consists of unsupervised methods. In these methods, the training dataset contains only samples of data (e.g., payroll data or images, among others) and there are no “ground truth” labels for training purposes (e.g., labels such as “high” or “low” for payroll data, or labels such as “dog” or “cat” for images of animals). In contrast, in supervised techniques, a model is trained by using both the training dataset and the ground truth labels such that the labels are used to correct the prediction output of the model.\n\nGenerative methods are unsupervised learning techniques focusing on discovering the patterns (without knowing the patterns in advance) and latent features of a dataset (including image, text, or tabular data) such that a trained model can generate new data instances with characteristics similar to the original data. Models predicting the next word in a sentence, using techniques such as Latent Dirichlet Allocation (LDA) and Variational Autoencoders (VAE), are examples of generative models.\n\nDiscriminative models, in contrast, are supervised techniques addressing the data classification problem. Given input such as a dataset of animal images, they can classify each image as a dog or cat, for example. Techniques such as logistic regression, Random Forest (RF), and Support Vector Machines (SVM) are examples of discriminative models.\n\nGenerative Adversarial Networks — GANs for short — use a generative method combined with a deep learning–based ML approach. Notably for GANs, however, is that the GANs training process of the generative model is actually formulated as a supervised process, not an unsupervised one as is typical of generative models.\n• Generator: A generative model to learn the latent features of a target dataset, which, after training, are used to generate new data instances like the original training data.\n• Discriminator: A classification model aiming to distinguish real (the original dataset) and fake (synthetic data from the generator) data, which is discarded after training.\n\nFigure 1 shows the overall architecture of GANs models.\n\nThe training of GANs is based on a zero-sum or minimax game with two players, each one (G and D) trying to maximize its own benefits. The game converges when both players reach a point that changing their actions (updating the weights of neural networks) does not bring more benefits (or the loss functions for G and D cannot be further minimized). This point is the Nash equilibrium for the following equation:\n\nThis equation shows that G tries to minimize the loss function while D tries to maximize it.\n\nThe Generator is a neural network model responsible for generating realistic samples from the target domain. The input for the generator model is a vector randomly sampled from a uniform or Gaussian distribution. This vector is used as a starting point for the G model to generate synthetic data in the problem domain. This random vector represents a compressed version of features of the outputs referred to as latent features or a latent vector. In fact, during the training process, the Generator converts this random vector to meaningful data points (e.g., human face images). In this way, each new random vector drawn from the latent space (e.g., Gaussian distribution) is converted to a new output in the problem domain.\n\nFigure 2 shows a sample architecture of a Generator composed of different layers (transpose convolutional, dense layers, and leaky_relu activation function). The input for this architecture is a latent vector of size 100 drawn from a uniform distribution and the output is set as a 28 by 28 image, to be compatible with the input shape of the Discriminator.\n\nThe input of the Generator is a latent vector, and its output (consisting of fake samples) is directly used as the input of the Discriminator. The Discriminator receives input from both the real dataset and the Generator to classify them as real and fake, respectively. In GANs architecture, the Generator (G) tries to minimize the second term of the main loss function (1) but as the first term is independent of G, the effective loss function is:\n\nThe Generator’s loss has one part: The average of inverted probabilities of the fake samples, and during the training process, G aims to minimize this term (which is the common part of the loss function with D). This term is usually represented as g_loss in implementations.\n\nThe Discriminator functions as a classifier to distinguish the real samples (in the original dataset) from the fake ones (from the Generator model). The inputs for this component are 1) samples from the original dataset, and 2) synthetic data from the Generator component. As this component is a supervised method (or classifier), it uses a binary label representation, with label 1 for real inputs and label 0 for fake inputs. Like any other neural network, a loss function is needed to train the Discriminator. The following formula demonstrates the loss function as the Discriminator aims to maximize it.\n\nThe Discriminator’s loss has two terms: 1) the average of the log probability of real samples, and 2) the average of the log of inverted probabilities for the fake samples. These two terms are usually represented as d_loss_real and d_loss_fake in implementations.\n\nFigure 3 shows a sample architecture of a Discriminator composed of different layers (consisting of convolutional, dropout, fully connected layers, and leaky_relu activation function). The input for this architecture is a 28 by 28 image and the output (taking the form of the sigmoid function) is a 1 by 1 output representing whether the input is fake or real.\n\nThe training algorithm is an alternative training process based on the backpropagation technique. Alternative training means the Generator and Discriminator are trained in one loop, one after the other. In this way we train the Discriminator for one step (using a mini-batch of input data) and then train the Generator for one step (using a mini-batch of latent vectors), repeating these steps until the model converges. During Discriminator training, the Generator is frozen and does not train, and vice versa.\n\nReal samples are random mini-batches of the original dataset from the problem domain (e.g., images or tabular data), which are sampled to be classified as the real class by the Discriminator.\n\nFake samples consist of a sample of synthetic data from the Generator (e.g., fake images or tabular data) output to be classified as a fake class. It is worth mentioning that the quality of fake samples is intended to be improved during the training process, and so if we visualize the fake samples from the early training steps of the Generator, we see they are noisy and not meaningful in comparison to real data samples, which are always high-quality data in comparison.\n\nThe Discriminator is connected to two loss functions. One is used to train the Discriminator, and one is used to train the Generator. During the training of the Discriminator, the Generator is frozen and does not train, and the Discriminator classifies its two input sources as real and fake classes. The Discriminator network weights are updated and trained using the backpropagation mechanism and regular stochastic gradient descent (SGD). The loss function used in backpropagation penalizes (via error correction) the Discriminator model for cases in which the Discriminator incorrectly classifies the fake samples as real and vice versa. This error correction is high at the beginning of the training process and gradually converges to near zero (ideally).\n\nTraining the Generator is more complicated than the Discriminator. To train a neural network model using the backpropagation method, we need to calculate the error value of the model’s output using a loss function and then update the model weights by propagating the gradients across the neural layers, aiming to minimize the error value. In GANs architecture, the generator is not directly connected to a loss function. In fact, the Generator is used as an input of the Discriminator and the loss function connected to the Discriminator is used to train the Generator. These are the steps used to train the Generator:\n• A batch of random latent vectors (z) is drawn from a distribution, such as a Gaussian distribution.\n• A batch of fake samples (G(z)) is generated from the Generator.\n• A batch of fake data is fed to the Discriminator to be classified as real.\n• The Generator loss function (connected to the Discriminator) is calculated, and the error is backpropagated to the Generator layers while the Discriminator is frozen (does not update). In this way, the loss function penalizes the Generator for samples the Discriminator classifies as fake. This is the critical point where the Generator weights are updated to generate samples that can fool the Discriminator as real samples.\n\nFigure 6 shows the complete Generator and Discriminator training steps. The first lines show the code that defines the G and D and GAN, which actually consists of D(G(z)), as well the real and fake labels. The training process contains many iterations (an “epoch”), which is a hyperparameter not known in advance. At the start of each iteration, we must prepare the real and fake mini-batches (shown in lines 4–6). The fake samples are outputs of G, with a random latent vector as the input. In the early stages, in which G is not yet trained, the fake samples are garbage.\n\nWith the real and fake samples, we start by training the Discriminator with two labels: Label 1 for real samples and label 0 for fake samples (shown in lines 7 and 8). The train_on_batch() function implements the cross-entropy binary loss to calculate the gradients of the D (with regard to the loss function) and backpropagates them to the layers of D to train it as well. Next, after locking D (as shown on line 9), G — which is D(G(z)) because G is updated through D — and the loss function are connected to D, which is then trained. These steps are repeated until the model converges or fails (in the process of looking at d_loss_fake, d_loss_real, and g_loss).\n\nWhile GANs introduce a variety of benefits in areas such data privacy, image inpainting, or data augmentation, they also suffer from some training challenges. The main training challenges of GANs are (but are not limited to) mode collapse and non-convergence.\n\nWhen we train GANs, with different random latent vectors as inputs of the Generator component we expect the Generator to produce different outputs (e.g, different images) as well. But there are cases where the Generator starts producing the same output or a limited list of outputs again and again, which is called mode collapse. For images, this means that the outputs of the Generator share the same texture, color, and image features. The mode collapse can even be a full collapse to only one image (i.e., the same images with only tiny and neglectable differences) or it can be a partial collapse to multiple images. Figure 7 shows a partial mode collapse in which images with the same underline color are similar images.\n\nResolving the mode collapse problem is an active research area and one of the most important challenges in GANs research. There are some early solutions to address mode collapse:\n\nUnrolled GANs: An unrolled GAN is like having multiple copies (e.g., three or five) of the Discriminator grouped together and coupled with the Generator. The Generator is updated through all these Discriminators using the backpropagation mechanism. In this way the Generator can predict how the Discriminator will be updated in the following k steps. This introduces a “surrogate loss function” for the Generator (while Discrimination still uses the regular loss function of the GAN).\n\nIn this technique, the Generator is updated through multiple Discriminator optimization iterations and the Discriminator is updated just once. These differences break the sync between the Generator and Discriminator updates, which can prevent the Generator from collapsing into a local minimum and makes the training more stable. Figure 8 compares the visualized heatmaps of the Generator distribution in the GAN, showing ten unrolled Discriminators on the top row and a standard GAN on the bottom row. As we can see, the distribution of the Generator in the unrolled version gradually reaches the target or desired distribution in the last (Target) column — but the standard GAN fails.\n\nThis is how the authors describe it in their paper: “In the unrolled case, however, this undesirable behavior no longer occurs. Now G’s actions take into account how D will respond. G will try to make steps that D will have a hard time responding to. This extra information helps the generator spread its mass to make the next D step less effective instead of collapsing to a point.”\n\nThe sample code implementing the unrolled GAN can be found here.\n\nWasserstein Loss: The Wasserstein distance approximates the Earth mover’s distance (EMD). In GANs based on the Wasserstein loss function (WGAN), the Discriminator (which is called Critic) does not classify its inputs (real and fake samples) into probabilities between 0 and 1. The output of the Discriminator is a distance value and can be any number, and so the Discriminator does not have a sigmoid function in the last layer of its neural network. The Discriminator tries to maximize the distance metric of D(x) – D(G(z)) and the Generator tries to minimize this distance, or more precisely, it tries to maximize the D(G(z)) term. The output of the Discriminator does not need to be between 0 and 1 (to be used by the binary cross-entropy loss) and can be any number. Also, the Discriminator can be optimized without limitation by Generator updates. This leads the WGANs to have a reduced vanishing gradient effect and helps the Generator receive good feedback from the Discriminator, reducing the chance of getting stuck in a local minimum that can lead to mode collapse.\n\nThe Wasserstein GANs also have some considerations and limitations. They must observe the requirements of the Lipschitz constraint, which can be satisfied either by gradient clipping or by the gradient penalty. Moreover, momentum-based optimizers like Adam cannot be used, which means that the RMSProp optimizer is a good choice to be used instead with these GANs.\n\nNon-convergence failure is the case in which the Generator and the Discriminator cannot find an equilibrium point during the training process, causing their loss functions to fluctuate. GANs are based on a zero-sum game, and the game stabilizes only when both players reach a point that changing their actions does not bring more benefits.\n\nStable Discriminator loss values are around 0.5 (which means G can generate high-quality fake samples and D cannot distinguish almost any of them), which is hard to achieve, but loss values around 0.7 or 0.8 are also acceptable. Loss values in the range of 1.0–1.5 for Generator are good numbers for many cases. In non-convergence failure, the Discriminator’s loss may go to zero or the Generators’ loss may rise continuously. This can happen at the beginning of the training process or in cases in which the Generator produces garbage outputs that are easy for the Discriminator to classify as fake samples. As the training (updating the neural network weights) process starts from the D loss function, when D can classify G outputs easily, the D error rate is close to zero and the gradients propagated from the Discriminator to the Generator are not large or powerful enough to train the Generator. For some GANs this instability starts at early epochs and then recovers later, while for others this instability continues throughout the entire training process, leading to a training halt.\n\nEvaluation of GANs means assessing a trained Generator instead of the training process. It means that once the Generator is trained, we can evaluate the quality of the Generator’s outputs. During GAN training, the loss value of the Generator and Discriminator show the effectiveness of the training process and whether G and D are converging or not. The methods explained below are mainly used for GAN-generated images. This paper by Ali Borji has covered most of the GAN evaluation metrics, such as Fréchet Inception Distance (FID), AM Score, Maximum Mean Discrepancy, and Number of Statistically-Different Bins (NDB).\n\nIn this approach, which is applicable only to GANs generating synthetic images, we can visually assess the quality and diversity (regarding mode collapse) of the generated outputs. While this approach is straightforward and quite simple to apply, it has some drawbacks:\n• It is time consuming and the number of images that can be evaluated in a particular time is limited (although this can be improved by adding more reviewers).\n• The image-reviewing results can be biased by how human reviewers analyze the images and find the features they are expected to evaluate.\n\nThe notion of an inception score (IS) is introduced in the paper “Improved Techniques for Training GANs” and is based on using a pre-trained model (such as Google Inception v3) to evaluate the results of images generated by a GAN. This score is applied to all the images generated by a GAN and the average of all those individual scores is the final score of the GAN under test. The range of scores is between zero and the number of image classes in the training dataset of the pre-trained model; for example, using a pre-trained Inception v3 as the classifier model with the ILSVRC 2012 training dataset, which has 1000 classes, or with the CIFAR-10 dataset, which has 10 classes. This score measures two aspects of the generated images:\n• Image quality: Can each generated image be classified as one specific class? The output of the pre-trained model (e.g., Inception v3) for each generated image is a likelihood vector of class probabilities (each between 0.0 And 1.0 with a cumulative sum to 1.0). If the image has high quality, then only one index of this likelihood vector has high value, and all other values are small; for example, an image with a clear picture of a cat with a prediction vector of [0.90, 0.05, 0.02, 0.03] versus an image with no clear picture of a cat with probability vector of [0.35, 0.25, 0.15, 0.25] for an image set with four image classes (cat, tiger, lion, and jaguar).\n• Image diversity: Do the generated images cover a wide range of classes? By merging all the individual prediction vectors across all generated images, we have a probability vector (marginal distribution) representing the class diversity of the generated images. If all image classes are presented, then this averaged vector has uniform distribution (e.g., [0.24, 0.25, 0.26, 0.25]), but if some classes are repeated more than others (making things less diverse), then it is like having a vector with high numbers for only more repeated classes (e.g., [0.70, 0.10, 0.03, 0.07]).\n\nNow we have one label prediction vector for each image (as a conditional probability p(y|x) vector for y as label and x as the image) and one marginal distribution vector for all images (p(y)). For an ideal GAN we want to have one specific and distinct class for each image and high diversity across all the generated images. In this case, each image prediction label should be a narrow distribution (one large number and many small numbers) and the marginal distribution should be close to a uniform distribution (covering all image classes). Because of this difference the distance between these two vectors is high. To measure this distance, researchers have used the Kullback-Leibler (KL) divergence to measure the similarity or difference of these two probability vectors. Averaging all the individual scores (KL distances) shapes the final inception score (IS). Mathematically, the KL formula is:\n\nTo apply this formula for the inception score, P(x) is the image prediction label vector, and Q(x) is the marginal distribution across all images.\n\nIf the generated images are of high quality and simultaneously cover a wide range of classes, then the individual KL distance is high, and the final inception score will be high as well. If either the quality or the diversity of images is low, then the final score will be low as well.\n\nThe IS score is highly dependent on the pre-trained classifier and its training dataset. This introduces some limitations for the inception score, such as:\n• If the generated images are not like the training data set images, then it can lead to low scores.\n• If the generated images cover all the classes but inside each class, they have very few diversities, and it still leads to a high score. Diversity inside classes is not considered.\n• If the classifier failed to detect some features of the images but still generates narrow prediction vectors (one class with high confidence), such as classifying the animals with double faces as well as single face ones, then it still leads to high scores.\n\nFrom the first introduction of GANs architecture in 2014, many improvements and applications have been developed by researchers. Here some useful applications are introduced.\n\nResearchers describing their work in the paper “Large Scale GAN Training for High Fidelity Natural Image Synthesis” introduced the “BigGAN” model as one highly befitted from scaling up the GANs (i.e., increasing the batch size and number of channels). The result is very realistic and high-resolution images with high inception scores of 166.5 for 128 by 128 images.\n\nAuthors describing their work in the paper “Context Encoders: Feature Learning by Inpainting” have developed a visual feature learning architecture called “context encoders” based on encoder-decoder architecture, which can be used for photo inpainting and filling in the missing parts of images. The proposed architecture leverages training convolutional neural networks using an adversarial loss function, plus a reconstruction loss (like auto encoders).\n\nThe StackGAN architecture introduced in the paper “StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks” demonstrates a GAN that generates images conditioned on text descriptions. The StackGAN architecture is based on the more general Conditional GAN architecture. In conditional GANs, the Generator and Discriminator have additional input variables c as G(z, c) and D(x, c), which allows them to generate images based on a specific condition. StackGAN introduces a “conditioning augmentation” technique that receives the text embeddings and outputs a conditioning variable that is paired with the regular latent vector (drawn from normal or uniform distribution) used to train the GANs.\n\nGenerating frontal views of human faces from different face poses and angles has many applications in facial recognition systems. Researchers describing their work in the paper “Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis” have developed a GAN architecture called a Two-Pathway Generative Adversarial Network (TP-GAN) to address this issue. At the core of the architecture is a synthetic loss function composed of pixel-wise loss, identity loss, symmetry loss, and adversarial loss functions.\n\nIn this article I have taken a deep dive into GANs, providing an overview of the inner workings of GANs architecture and how GANs are typically used. This article builds on the information presented by my colleague Daniel Huang in the introductory article of this two-part article series. Below I present some additional information on GANs, including demos, references, and useful links. By knowing how GANs function, we hope you will be equipped to use GANs in your own work.\n• Image-to-image translation with conditional adversarial networks.” Proceedings of the IEEE conference on computer vision and pattern recognition, 2017.\n• Storygan: “A sequential conditional gan for story visualization.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n• StyleGAN: Karras, Tero, Samuli Laine, and Timo Aila. “A style-based generator architecture for generative adversarial networks.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n• Anime GAN: Jin, Yanghua, et al. “Towards the automatic anime characters creation with generative adversarial networks.” arXiv preprint arXiv:1708.05509 (2017).\n\nFor an introduction to GANs, please see the introductory article of this two-part article series:"
    },
    {
        "link": "https://machinelearningmastery.com/generative_adversarial_networks",
        "document": "More generally, GANs are a model architecture for training a generative model, and it is most common to use deep learning models in this architecture, such as convolutional neural networks or CNNs for short.\n\nGANs are a clever way of training a generative model by framing the problem as supervised learning with two sub-models: the generator model that we train to generate new examples, and the discriminator model that tries to classify examples as either real (from your dataset) or fake (generated).\n\nThe two models are trained together in a zero-sum game, adversarially, until the discriminator model is fooled about half the time, meaning the generator model is generating plausible examples.\n\nThis is most unlike training “normal” neural network models that involve training the model to minimize loss to some point of convergence.\n\n…so, Why are Generative Adversarial Networks so Compelling?\n\nOne of the many major advancements in the use of deep learning methods in domains such as computer vision is a technique called data augmentation.\n\nSuccessful generative modeling provides an alternative and potentially more domain-specific approach for data augmentation.\n\nIn complex domains or domains with a limited amount of data, generative modeling provides a path towards more training for modeling. GANs have seen much success in this use case in domains such as deep reinforcement learning.\n\nThere are many research reasons why GANs are interesting, important, and require further study.\n\nAmong these reasons is GANs successful ability to model high-dimensional data, handle missing data, and the capacity of GANs to provide multi-modal outputs or “multiple plausible answers“.\n\nPerhaps the most compelling application of GANs is in conditional GANs for tasks that require the generation of new examples. Three examples include:\n\nPerhaps the most compelling reason that GANs are widely studied, developed, and used is because of their success. GANs have been able to generate photos so realistic that humans are unable to tell that they are of objects, scenes, and people that do not exist in real life.\n\nAstonishing is not a sufficient adjective for their capability and success.\n\nThe Challenge of Getting Started with GANs\n\nThe study and application of GANs is very new.\n\nThe technique was only first described just a few years ago.\n\nBecause the field is so young, it can be challenging to know how to get started, what to focus on, and how to best use the available techniques.\n\nThere are no good theories for how to implement and configure GAN models. All advice for applying GAN models is based on hard earned empirical findings, the same as any nascent field of study. This makes it both exciting and frustrating.\n\nIt’s exciting because although the results achieved so far, such as the automatic synthesis of large photo-realistic faces and translation of photographs from day to night, we have only scratched the surface on the capabilities of these methods.\n\nIt is frustrating because the models are fussy and prone to failure modes, even after all care was taken in the choice of model architecture, model configuration hyperparameters, and data preparation.\n\nI study the field and carefully designed a book to give you the foundation required to begin developing and applying generative adversarial networks quickly on your own projects.\n\nSo, how can you get started and get good at using GANs fast?\n\nThis is the book I wish I had when I was getting started with Generative Adversarial Networks.\n\nThis book was born out of one thought:\n\nHow can I get you to be proficient with GANs as fast as possible?\n\nThe Machine Learning Mastery method describes that the best way of learning this material is by doing. This means the focus of the book is hands-on with projects and tutorials.\n\nThis book was designed to teach you step-by-step how to develop Generative Adversarial Networks using modern deep learning methods for your own computer vision projects.\n\nYou will be led along the critical path from a practitioner interested in GANs to a practitioner that can confidently design, configure, train and use GAN models.\n\nThis is the fastest process that I can devise for getting you proficient with Generative Adversarial Networks.\n\nDevelop Real Practical Skills That You Can Apply Immediately, such as:\n\n…so, is this book right for YOU?\n\n Who Is This Book For?\n\nLet’s make sure you are in the right place.\n\nThis book is for developers that know some applied machine learning and some deep learning.\n\nMaybe you want or need to start using GANs for image synthesis or translation on your research project or on a project at work. This book was written to help you do that quickly and efficiently by compressing years of knowledge and experience into a laser-focused course of hands-on tutorials.\n\nThis guide was written in the top-down and results-first style that you’re used to from Machine Learning Mastery.\n\nThe lessons in this book assume a few things about you.\n\n…so what will YOU know after reading it?\n\n About Your Learning Outcomes\n\nThis book will teach you how to get results.\n\nAfter reading and working through this book,\n\n you will know:\n\nThis book will NOT teach you how to be a research scientist nor all the theory behind why specific methods work (if such theories exist for GANs). For that, I would recommend good research papers and textbooks.\n\nThis new understanding of applied deep learning methods will impact your practice of working with GANs in the following ways:\n\nThis book is not a substitute for an undergraduate course in deep learning, computer vision, or GANs, nor is it a textbook for such courses, although it could be a useful complement. For a good list of top textbooks and other resources, see the “Further Reading” section at the end of each tutorial lesson.\n\n…so what is in the Ebook?\n\n 29 Step-by-Step Tutorials to Transform You into a GAN Practitioner\n\nThis book was designed around major deep learning techniques that are directly relevant to Generative Adversarial Networks.\n\nThere are a lot of things you could learn about GANs, from theory to abstract concepts to APIs. My goal is to take you straight to developing an intuition for the elements you must understand with laser-focused tutorials.\n\nThe tutorials were designed to focus on how to get results with deep learning methods. As such, they will give you the tools to both rapidly understand and apply each technique or operation. There is a mixture of both tutorial lessons and projects to both introduce the methods and give plenty of examples and opportunities to practice using them.\n\nEach of the tutorials is designed to take you about one hour to read through and complete, excluding running time and the extensions and further reading sections.\n\nYou can choose to work through the lessons one per day, one per week, or at your own pace. I think momentum is critically important, and this book is intended to be read and used, not to sit idle. I would recommend picking a schedule and sticking to it.\n\nThe tutorials are divided into 7 parts; they are:\n\nHi, I'm Jason Brownlee. I run this site and I wrote and published this book.\n\nI live in Australia with my wife and sons. I love to read books, write tutorials, and develop systems.\n\nI have a computer science and software engineering background as well as Masters and PhD degrees in Artificial Intelligence with a focus on stochastic optimization.\n\nI've written books on algorithms, won and ranked well in competitions, consulted for startups, and spent years in industry. (Yes, I have spend a long time building and maintaining REAL operational systems!)\n\nI get a lot of satisfaction helping developers get started and get really good at applied machine learning.\n\nI teach an unconventional top-down and results-first approach to machine learning where we start by working through tutorials and problems, then later wade into theory as we need it.\n\nI'm here to help if you ever have any questions. I want you to be awesome at machine learning.\n\nDo you want to take a closer look at the book? Download a free sample chapter PDF.\n\nEnter your email address and your sample chapter will be sent to your inbox.\n\nClick Here to Download Your Sample Chapter\n\nCheck Out What Customers Are Saying:\n\nYou're Not Alone in Choosing Machine Learning Mastery\n\nTrusted by\n\n...students and faculty from universities like:\n\nand many thousands more...\n\nPlus, as you should expect of any great product on the market, every Machine Learning Mastery Ebook\n\ncomes with the surest sign of confidence: my gold-standard 100% money-back guarantee.\n\nIf you're not happy with your purchase of any of the Machine Learning Mastery Ebooks,\n\njust email me within 90 days of buying, and I'll give you your money back ASAP.\n\nNo waiting. No questions asked. No risk.\n\n…it’s time to take the next step.\n\n Bring to Your NOW!\n\nAll prices are in US Dollars (USD).\n\nHey, can you build a predictive model for this?\n\nImagine you had the to say:\n\n\"YES!\"\n\n...and .\n\nI have been there. It feels great!\n\nHow much is that worth to you?\n\nThe industry is demanding skills in machine learning.\n\nThe market wants people that can deliver results, not write academic papers.\n\nBusiness knows what these skills are worth and are paying sky-high starting salaries.\n\nA Data Scientists Salary Begins at:\n\n$100,000 to $150,000.\n\nA Machine Learning Engineers Salary is Even Higher.\n\nWhat Are Your Alternatives?\n\nYou made it this far.\n\nYou're ready to take action.\n\nBut, what are your alternatives? What options are there?\n\n(1) A Theoretical Textbook for $100+ \n\n...it's boring, math-heavy and you'll probably never finish it.\n\n(2) An On-site Boot Camp for $10,000+ \n\n...it's full of young kids, you must travel and it can take months.\n\n(3) A Higher Degree for $100,000+ \n\n...it's expensive, takes years, and you'll be an academic.\n\nFor the Hands-On Skills You Get...\n\nAnd the Speed of Results You See...\n\nAnd the Low Price You Pay...\n\nAnd they work. That's why I offer the money-back guarantee.\n\nProfessionals Stay On Top Of Their Field\n\n Get The Training You Need!\n\nYou don't want to fall behind or miss the opportunity.\n\nDo you have another question?"
    }
]