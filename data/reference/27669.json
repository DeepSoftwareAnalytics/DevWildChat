[
    {
        "link": "https://ffmpeg.org/doxygen/6.0/group__lavc__packet.html",
        "document": "A fallback track indicates an alternate track to use when the current track can not be decoded for some reason. e.g. no decoder available for codec."
    },
    {
        "link": "https://ffmpeg.org/doxygen/3.3/group__lavc__encdec.html",
        "document": "The avcodec_send_packet()/avcodec_receive_frame()/avcodec_send_frame()/ avcodec_receive_packet() functions provide an encode/decode API, which decouples input and output. More...\n\nThe avcodec_send_packet()/avcodec_receive_frame()/avcodec_send_frame()/ avcodec_receive_packet() functions provide an encode/decode API, which decouples input and output.\n\nThe API is very similar for encoding/decoding and audio/video, and works as follows:\n• Set up and open the AVCodecContext as usual.\n• Send valid input:\n• For decoding, call avcodec_send_packet() to give the decoder raw compressed data in an AVPacket.\n• For encoding, call avcodec_send_frame() to give the encoder an AVFrame containing uncompressed audio or video. In both cases, it is recommended that AVPackets and AVFrames are refcounted, or libavcodec might have to copy the input data. (libavformat always returns refcounted AVPackets, and av_frame_get_buffer() allocates refcounted AVFrames.)\n• Receive output in a loop. Periodically call one of the avcodec_receive_*() functions and process their output:\n• For decoding, call avcodec_receive_frame(). On success, it will return an AVFrame containing uncompressed audio or video data.\n• For encoding, call avcodec_receive_packet(). On success, it will return an AVPacket with a compressed frame. Repeat this call until it returns AVERROR(EAGAIN) or an error. The AVERROR(EAGAIN) return value means that new input data is required to return new output. In this case, continue with sending input. For each input frame/packet, the codec will typically return 1 output frame/packet, but it can also be 0 or more than 1.\n\nAt the beginning of decoding or encoding, the codec might accept multiple input frames/packets without returning a frame, until its internal buffers are filled. This situation is handled transparently if you follow the steps outlined above.\n\nIn theory, sending input can result in EAGAIN - this should happen only if not all output was received. You can use this to structure alternative decode or encode loops other than the one suggested above. For example, you could try sending new input on each iteration, and try to receive output if that returns EAGAIN.\n\nEnd of stream situations. These require \"flushing\" (aka draining) the codec, as the codec might buffer multiple frames or packets internally for performance or out of necessity (consider B-frames). This is handled as follows:\n• Instead of valid input, send NULL to the avcodec_send_packet() (decoding) or avcodec_send_frame() (encoding) functions. This will enter draining mode.\n• Call avcodec_receive_frame() (decoding) or avcodec_receive_packet() (encoding) in a loop until AVERROR_EOF is returned. The functions will not return AVERROR(EAGAIN), unless you forgot to enter draining mode.\n• Before decoding can be resumed again, the codec has to be reset with avcodec_flush_buffers().\n\nUsing the API as outlined above is highly recommended. But it is also possible to call functions outside of this rigid schema. For example, you can call avcodec_send_packet() repeatedly without calling avcodec_receive_frame(). In this case, avcodec_send_packet() will succeed until the codec's internal buffer has been filled up (which is typically of size 1 per output frame, after initial input), and then reject input with AVERROR(EAGAIN). Once it starts rejecting input, you have no choice but to read at least some output.\n\nNot all codecs will follow a rigid and predictable dataflow; the only guarantee is that an AVERROR(EAGAIN) return value on a send/receive call on one end implies that a receive/send call on the other end will succeed, or at least will not fail with AVERROR(EAGAIN). In general, no codec will permit unlimited buffering of input or output.\n\nThis API replaces the following legacy functions:\n• avcodec_decode_video2() and avcodec_decode_audio4(): Use avcodec_send_packet() to feed input to the decoder, then use avcodec_receive_frame() to receive decoded frames after each packet. Unlike with the old video decoding API, multiple frames might result from a packet. For audio, splitting the input packet into frames by partially decoding packets becomes transparent to the API user. You never need to feed an AVPacket to the API twice (unless it is rejected with AVERROR(EAGAIN) - then no data was read from the packet). Additionally, sending a flush/draining packet is required only once.\n• avcodec_encode_video2()/avcodec_encode_audio2(): Use avcodec_send_frame() to feed input to the encoder, then use avcodec_receive_packet() to receive encoded packets. Providing user-allocated buffers for avcodec_receive_packet() is not possible.\n• The new API does not handle subtitles yet.\n\nMixing new and old function calls on the same AVCodecContext is not allowed, and will result in undefined behavior.\n\nSome codecs might require using the new API; using the old API will return an error when calling it. All codecs support the new API.\n\nA codec is not allowed to return AVERROR(EAGAIN) for both sending and receiving. This would be an invalid state, which could put the codec user into an endless loop. The API has no concept of time either: it cannot happen that trying to do avcodec_send_packet() results in AVERROR(EAGAIN), but a repeated call 1 second later accepts the packet (with no other receive/flush API calls involved). The API is a strict state machine, and the passage of time is not supposed to influence it. Some timing-dependent behavior might still be deemed acceptable in certain cases. But it must never result in both send/receive returning EAGAIN at the same time at any point. It must also absolutely be avoided that the current state is \"unstable\" and can \"flip-flop\" between the send/receive APIs allowing progress. For example, it's not allowed that the codec randomly decides that it actually wants to consume a packet now instead of returning a frame, after it just returned AVERROR(EAGAIN) on an avcodec_send_packet() call."
    },
    {
        "link": "https://stackoverflow.com/questions/73296394/ffmpeglibav-creating-avpackets-from-network-packetstcp-using-av-parser-parse",
        "document": "I am developing one software that takes frames using OpenCV then sends them over network. I was using RTMP protocol for that, i have created one middle rtmp server and solved it.However, i had to solve the problem using TCP. On the sender side I do not face any problem, i can smoothly convert From cv::Mat-> AVframe->Avpacket and send it over network. However, on the receiver side, i can not create AVframes. This is my sender code(dont have any issue, it sends over TCP AS EXPECTED):\n\nThis is my receiver code that i have too many questions:\n\nAs i understand, this function takes bytes untill it can create AVpacket.(In my case this function for twice, in the first one reads as big as my buffer size (4096), at the second one it reads about ~2900, then goes the decoding line.) My questions about this function, It takes AVpacket.pts, AVpacket.dst and AVpacket.pos but, while i am receiving some bytes how can I pass those variables.(I am already trying to fill AVpacket). After passing this function, i am having error in int response = avcodec_send_packet(pCodecContext, pPacket), line. The error:The error And sometimes getting this error :The error-2\n\nIn addition to that, i have checked the CodecContext, i have hardcoded every property on the receiver side, so they are the same.(Which also thisis another thing that confuses me, because AVpacket should have already have the context information right?)"
    },
    {
        "link": "https://github.com/FFmpeg/FFmpeg/blob/master/doc/APIchanges",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/10015769/ffmpeg-avpacket-control",
        "document": "According to the text in avcodec.h file, there are some decoders may support multiple frames in one , but method decode only first frame... I must get all of them.\n\nIn source code of libavcodec, parameter AVPacket noticed as , so while decoding this packet decoder cant change AVPacket's fields, can I change maybe an offset of packet data or delete already recived data for making decoder read in loop all frames in the packet???"
    },
    {
        "link": "https://stackoverflow.com/questions/49439929/managing-threads-while-practicing-modern-c17s-best-practices",
        "document": "Originally I had thought about designing a class to store along with the and that they would work with. The class was to be responsible for the managing of memory, access, transferring, releasing, locking, unlocking, joining, and other typical common functionalities of the associated types within the standard multithreading library. It was originally intended to associate the containing thread and its id with a specific set of resources that a particular thread has access to.\n\nAfter reading through the docs on about , , , , , etc. and now knowing that and are non copyable and the fact that if I template the class to store arbitrary , , or s as a within this class's container that the class instantiation of this intended singleton would only be able to store a specific function signature limiting it to not be able to instantiate any other declaration signature.\n\nConcerning these behaviors and properties of the multithreading library within the standard library for , , , , etc... it came to mind that I'm overthinking the overall design of this class.\n\nYou can refer to my initial design attempt via this previously asked question of mine. Storing arbitrary function objects into a class member container without knowing their declaration signature, This should give you an idea of what I was attempting to do.\n\nUnderstanding a little more about their behaviors, properties and responsibilities I would like to know if the following would be appropriate for the intended design process.\n\nInstead of storing any , , , or ; would it make more sense to just store the generated of the , and have my manager class act more like a monitoring, recording, and reporting type class instead?\n\nMy new intentions would be that the container would store the thread's ID in an associated map as its key along with an associated common struct. The struct would contain a property list of all the responsibilities and actions of the combined resources. This may then allow support for some of the following features: a priority queue, a task scheduler, a dispatcher of commands to send and fetch resources knowing if the thread is available or not, where these types of actions will not be done by this class directly but through generic function templates.\n\nWith my intention to implement this kind of design while trying to maintain the best practices of modern c++ targeting c++17; would this kind of design be appropriate for proper class design to be generic, modular, portable and efficient for use?"
    },
    {
        "link": "https://medium.com/@pauljlucas/advanced-thread-safety-in-c-4cbab821356e",
        "document": "C++ supports writing programs where parts of the code run concurrently using threads. When writing such programs, you have to take additional steps to ensure that data shared among threads can not cause race conditions. Typically, race conditions are avoided by proper use of mutexes and locks. However, in high-performance code, mutexes can sometimes be too costly, especially with high-contention data. C++ supports alternatives.\n\nNote: what follows will not describe how to create or join threads since those things are largely straightforward. Instead, what follows will describe how to write programs safely in a multithreaded program.\n\nIn the late 1960s, early 1970s, programs (running on computers such as the PDP-7 and later the PDP-11) were:\n• Single-threaded. (While nascent “threads” appeared in 1966, POSIX threads didn’t arrive until 1995.)\n• Executed sequentially. (That is, machine code instructions generated by compilers were executed one at a time and in the same order as the original statements in your program were written.)\n• Running in flat (non-hierarchical) memory. (CPU caches didn’t exist until the late 1970s and didn’t go mainstream until 1993 with the Intel Pentium.)\n\nIn order not to have overall performance constrained by memory, CPU designers had to employ mitigation tactics of:\n\nAll of these tactics are done entirely by the hardware. You as the programmer have no way either to know or really influence what’s going on under the hood.\n\nAdditionally, compiler implementors also employ their own mitigation tactics of:\n• … and many more.\n\nThe compiler is free to do any optimization so long as it makes no change to the observable behavior of your program (the “as-if rule”).\n\nAs long as your program has only a single thread, you can remain blissfully unaware of either the hardware or compiler tactics being employed. However, once you have multiple threads, you must become acutely aware of what’s going on under the hood.\n\nWhen doing multithreaded programming, you’ll hear the term “atomic” used. What, exactly, is meant by “atomic” anyway? There are three related, but distinct, meanings:\n• A value operation (read or write) completes with no possible intervening operation by another thread, e.g., writing single-byte values on any CPU, or 16- or 32-bit values on a 32-bit CPU, or 64-bit values on a 64-bit CPU.\n• An updated value is visible to other CPUs.\n• Multiple value operations complete with no possible intervening operation by another thread, e.g., updating A and B together (“transactional”).\n\n“Atomic” always means #1; it usually also means #2; or all three.\n\nConsider the following code where the main thread spawns another. Both threads run so long as remains . To shut down, the main thread sets to . In response, both loops will exit and the main thread will join the other thread. Unfortunately, the code is wrong. (More unfortunately, I’ve seen such code in production.)\n\nIt’s wrong because is shared by multiple threads improperly. Some programmers mistakenly think along the lines of:\n\nThe problem is that while that’s actually true, it’s insufficient. A is atomic only by meaning #1; it’s not atomic by meaning #2. That is, just because a is updated by one thread does not mean that updated value is visible to other threads (running on other CPUs).\n\nI’ve also seen code where programmers knew they had to do something more to make such code thread-safe. Unfortunately, that something turned out to be:\n\nThat is, they inserted because they kind-of understand what it does, but not what it’s for. The attempted use of to make something thread-safe is always wrong in C++.\n\nThe classic way to fix this code (correctly) would be to use a mutex:\n\nThe mutexes and locks make be atomic by meaning #1 and meaning #2. While not necessary for this program, mutexes and locks also make things atomic by meaning #3 as well, so they’re the most general, one-size-fits-all tool for making programs thread-safe. However, C++ offers alternative tools that provide a more custom fit especially useful in high-performance code.\n\nWriting safe multithreaded code even with ordinary mutexes and locks is hard to get right, even for experts. Writing safe multithreaded code using advanced thread-safety techniques is even harder! Before considering them, you should:\n• Know that using efficient algorithms matters far more than locking technique. For example, an N⋅log(N) algorithm with slower mutexes will still very likely be more performant than an N^2 algorithm with faster locking (or lock-free) techniques.\n• Measure to see if your code is spending too much time locking. (Remember: results are CPU-dependent!)\n• Only if locking takes a significant percentage of time, then consider the following techniques.\n\nAn Even Better Fix:\n\nWhile use of mutexes and locks is correct, they’re fairly heavy-weight. In the case of using a , a much lighter-weight fix is:\n\nUse of does not make atomic by meaning #1 (it already is); it makes it atomic by meaning #2.\n\nThe class is specialized for all built-in types ( , , , etc.) and all pointer types ( for any ). It can even be used for your own type , but only when:\n• is trivially copyable (that is, if would correctly copy ).\n\nWhen is an integral or pointer type, has conventional operators overloaded for it:\n\nThe operators are shorthands for the member functions shown in the comments. While the operators are convenient, I always use the member functions to make it obvious in code that the variable being manipulated is a .\n\nis not thread-safe because the read of and the write to are not atomic by meaning #3.\n\nEven though offers better performance over mutexes and locks, it’s not guaranteed to. For a type on a specific CPU under specific conditions, and check. Those conditions include:\n• Accessing unaligned values, e.g., reading an at a memory address that’s not evenly divisible by (assuming the CPU can even do it at all without generating a bus error).\n• Runtime CPU dispatching, e.g., a particular program during initialization may detect whether the CPU supports the instruction and, if not, may be forced to use locks.\n\nThe only type that’s guaranteed to be always lock-free on all CPUs under all conditions is that’s a special-case of . (An example using is shown later.)\n\nAtomic (by any meaning) is not enough to ensure thread-safety because the order of memory operations does not necessarily match the order of statements in a program. Why not? Because the hardware, the compiler, or both, may reorder things to improve performance. The “as-if” rule holds only from the perspective of a single thread.\n\nMemory barriers (aka, “fences”) help ensure thread-safety by selectively prohibiting reordering of memory operations across the barrier. They also provide some synchronization among threads. C++ provides .\n\nThe sometimes confusing thing about memory barriers is that they’re about controlling the order of operations, not the operations themselves.\n\nThis is the safest memory order which is why it’s the default. For example, the signature of is:\n\nIt’s also the least efficient because it establishes a global memory ordering (bottleneck) across all threads. In specific cases, more efficient memory orders can be used.\n\nThis is the least safe memory order in that it does not guarantee operation order or synchronization, but still guarantees modification order. What good is that? A typical use-case is incrementing reference counts:\n\nAt this point, you might ask something along the lines of:\n\nThe answer is no for two reasons:\n• Each increment is still atomic (by meanings #1 and #2) so each thread always sees the latest value. What does is allow the hardware or the compiler to reorder other operations having nothing to do with either before or after its increment to improve overall performance, not the performance of specifically.\n• Because is part of a private data structure, it’s guaranteed that no actions are ever conditionally taken by another thread based on its current value. Said another way, when only performing an increment and not altering the control flow in any thread, then use of is safe.\n\nAn example of when is not safe is when decrementing reference counts. (More later.)\n\nNow that you hopefully understand , you should never use it unless you can prove your use of it is correct and it actually significantly improves performance. Correct use of is very hard to do.\n\nThese memory orders are safer and always used in pairs:\n• (used with ) is used to “publish” information: no accesses can be reordered after.\n• (used with ) is used to “subscribe” to information: no accesses can be reordered before.\n\nIn this example, is shared among two threads and that acts as a semaphore:\n• The sets then signals that the data is ready (or “publishes” it). The write to “happens before” the write to because the use of guarantees that the write to can not be reordered after .\n• Meanwhile, the busy waits for the signal on , then safely reads . The read from “happens before” the read from because the use of guarantees that the read from can not be reordered before .\n• itself need not be atomic. (This is particularly useful for data that can not be made atomic either because it’s too big or is not trivially copyable.)\n• You can set any amount of data, then “publish” all of it simultaneously.\n\nAt this point, you might ask whether busy waiting is efficient. In general, it’s efficient when the waits are short — shorter than the time it would take to lock and unlock a mutex.\n\nA general spinlock (a kind of busy waiting) can be implemented using :\n\nThis memory order is a special case of that allows operations to be dependency-ordered that can be more efficient on weakly ordered CPUs (ARM, PowerPC; but not x86). From the earlier example:\n\nThere’s no actual dependency between and other than what’s in our minds — which the compiler has no way to know. To create an actual dependency that the compiler can use to keep things in dependency order, we can instead do:\n\nNow, we’ve created an actual dependency using a pointer and a pointed-to value in that the value of the pointer has to be loaded before the pointer is dereferenced. The compiler can preserve this order without using an additional and more expensive memory barrier.\n\nThis memory order is and combined into one used for read-modify-write operations: no accesses can be reordered before or after. A typical use-case is decrementing reference counts:\n\nThe reason can’t be used here is because a use of the reference counted object in another thread could be reordered to be after the happens resulting in undefined behavior.\n\nUsing with is atomic without a barrier; using is a barrier without a specific atomic. It’s useful if you want to update several atomic values together. For example, instead of doing:\n\nyou can do:\n\nBut there is another difference. Given:\n\nNo operations in (1) will be reordered after the store-release of into (2); however, store operations in (2) can be reordered before the release into (1).\n\nimposes stronger ordering guarantees than an operation on an atomic with the same memory order. Given:\n\nNo operations in (1) will be reordered after all subsequent store operations into (3) and store operations in (3) can not be reordered before the fence into (1). However, since controls store operations only, load operations in (2) can be reordered before the release into (1).\n\nThe rules for are similar except it controls load operations only and enforces ordering in the opposite direction. The differences are subtle. In general, an acquire or release associated with a particular atomic is preferred.\n\nCompare-and-swap (CAS), as its name suggests, is both compare and swap operations done together atomically (by all atomic meanings). Conceptually, it’s implemented as:\n\nexcept done atomically. The idea is that you check the value of an atomic variable and:\n• If it’s the value you expect (or hope for), then (and only then) set it to a new desired value; or:\n• If it’s not the value you expect (or hope for), it means some other thread changed the value, so do nothing. You are free to reattempt setting the value.\n\nThere are actually two flavors: and . (More on the difference later.)\n\nFor example, we can reimplement the previous class using CAS:\n\nUsing CAS, we check the value of :\n• If it’s , it means it’s currently unlocked, so set it to to indicate it’s now locked and return .\n• If it’s , it means it’s currently locked (by another thread), so do not set the value and return . Note that in this case, the first parameter of is an in/out parameter and it’s set to the atomic’s current value (here, ) even though we don’t care. This forces us to reset to prior to another attempt.\n\nOne of the primary use-cases for CAS is that it allows you to implement lock-free operations on data structures. For example, part of a lock-free implementation might be:\n\nAfter a new node is created, we try to update :\n• If it’s still equal to (the original value of ), update to point to using the memory order specified by the third argument.\n• If it’s not equal, it means another thread snuck in and updated to point to different new node, so do nothing and reattempt. (Note that has been updated to be the new pointing to the different node using the memory order specified by the fourth argument.)\n\nThe existence of implies there’s a — and there is. The difference between them is:\n• fails (returns ) only if the current value is not the expected value.\n• may also fail if there’s a “spurious failure.”\n\nA “spurious failure” is quirk on weakly ordered CPUs (e.g., ARM and PowerPC; but not x86) where the operation fails for reasons other than the value not being the expected value.\n\nSo if never fails spuriously, why does exist? Before answering that question, let’s look at a conceptual implementation of them both:\n\nAssume there’s only that implements a weak version of CAS:\n• , however, wraps it with a loop that effectively filters out spurious failures. The important thing to remember here is that there’s a loop.\n\nGiven that, the benefits of are:\n• Spurious failures tend not to happen all that often.\n• When your code is using a loop anyway, the weak version will yield better performance on weakly ordered CPUs (ARM, PowerPC; but not x86 — but it’s no worse).\n• Detects the ABA Problem (on weak CPUs) — more later.\n\nSo then why does exist?\n• If you have a loop only to filter out spurious failures, don’t: use .\n• But if you have a loop anyway, use .\n• However, if handling a spurious failure is expensive (for example, if you have to discard and reconstruct a new object), use .\n• But doesn’t detect the ABA Problem (more later).\n\nGiven all that, you might wonder when would you ever not have a loop for ? One example is implementing :\n\nThe ABA Problem can be illustrated as follows. Suppose thread 1 performs the following steps:\n• Read the same memory location again (value is still “A”).\n\nThe problem is suppose thread 2 performs the following steps while thread 1 is doing its step 2:\n• Write “B” to the same memory location.\n• Write “A” to the same memory location.\n\nBy the time thread 1 does its step 3, it reads “A” and believes nothing has changed — even though it has. You might now ask:\n\nThe answer is: sometimes it doesn’t — but sometimes it does.\n\nConsider the code from earlier (repeated here for convenience):\n\nand the following illustration:\n\nState (1) shows the initial conditions where A and B are nodes on the stack, (H) points to A, and (N) has its also point to A. The dotted box contains and that are being compared. State (3) shows the desired final state where points to and points to A.\n\nBut what if another thread sneaks in before the loop is entered, pushes a new node X shown by state (2), then immediately pops it? Both and still point to A, so will be set to — which is correct. In this case, the ABA Problem isn’t actually a problem.\n\nBut now consider what the code for might be:\n\nand the following illustration:\n\nState (1) shows the initial conditions where A, B, and C are nodes on the stack and (H) and (F) both point to A. The dotted box contains and that are being compared. State (5) shows the desired final state where points to B.\n\nBut what if another thread sneaks in before the loop is entered, pops A and B shown by state (2), then pushes A shown by state (3)? Both and still point to A, so will be set to — which is wrong! Why? Originally, pointed to A whose pointed to B, hence (the desired argument in the compare) is B. But B was deleted in (2) so we’ll end up in state (4) with being a dangling pointer to B that was deleted. In this case, the ABA Problem is actually a problem! (It wasn’t a problem for because the desired value of could never become stale.)\n\nEven worse, there’s no easy fix for this. In this case, the problem is that part of the desired value expression can change. While stays the same, what points to can change. Detecting ABA Problems is hard, even for experts.\n\nHow can this be fixed?\n• Give up and just use a mutex and locks.\n\nA versioned pointer is an ordinary pointer plus an additional “version number” such that every time the value of the pointer changes, the version number is incremented. Conceptually, something like:\n\nThen use instead of :\n\nThis would work because, even though the part of would still point to A, its part would be different, so and would not compare equal, so would not be set to B, the stale value of .\n\nThe caveats of , however, are:\n• This will work only if the CPU supports double-pointer-wide (16 bytes on a 64-bit CPU) CAS (which, for example, x86_64 does via the instruction).\n• As mentioned, a specialization of requires to be trivially copyable — which is why can’t be implemented.\n\nFor many CPUs, the L1 cache is “chunked” into cache lines typically ranging from 16–64K in size each. To read a given memory location from main memory into the cache, the location and the surrounding chunk-sized bytes are all read in together. Similarly, to write a given memory location from the cache into main memory, the entire cache line is written. For code that exhibits locality of reference, the chunking yields a performance gain. However, in some cases, it can yield a performance loss.\n\nAssume that your code has one thread pushing items onto the tail of the queue (repeatedly updating ) and a second thread popping items from the head of the queue (repeatedly updating ).\n\nIn the code as given, and will very likely reside on the same cache line. This means updating one will invalidate the entire cache line adding otherwise unnecessary contention for the other.\n\nIn a case such as this, you want and to reside on different cache lines so updating one doesn’t affect the other. You can actually achieve that by using and :\n\nThis will waste a little bit of memory, but ensure that and reside on different cache lines.\n• Your algorithm matters far more than locking technique.\n• Thread-safety in general and using atomics and barriers directly specifically is very hard to get right, even for experts.\n• Before using advanced thread-safety techniques, measure to see if your code is spending too much time locking. (Remember: results are CPU-dependent!)\n• Only if locking takes a significant percentage of time, then consider atomics and barriers.\n• Atomics and barriers are two sides of the same coin.\n• Be aware of the ABA Problem."
    },
    {
        "link": "https://stackoverflow.com/questions/19422684/stdthread-management-usage-and-best-practice",
        "document": "One of the problems with using as an unadorned local variable is that it is not exception safe. I will admit that I am often guilty of this myself when demonstrating small little HelloWorlds.\n\nHowever it is good to know exactly what you're getting into, so here is a more detailed explanation of the exception safety aspects of using :\n\nIn the above example, I have a \"large\" program, which \"occasionally\" throws an exception. Typically I want to catch and handle exceptions before they bubble up to . However as a last resort, itself is wrapped up in a try-catch-all. In this example I simply print out that something really bad has happened and quit. In a more realistic example you might give your client a chance to save work, or free up memory or disk space, launch a different process that files a bug report, etc.\n\nLooks good, right? Unfortunately wrong. When you run this, the output is:\n\nI didn't give my client the notification that something went wrong before returning from normally. I was expecting this output:\n\nAs it turns out, looks like this:\n\nSo when throws, runs during stack unwinding, and without getting called. Thus calls .\n\nDon't ask me why. It is a long story, and I lack the objectivity to tell it unbiasedly.\n\nRegardless, you have to know about this behavior, and guard against it.\n\nOne possible solution is to go back to the wrapper design, perhaps using private inheritance as first proposed by the OP and warned against in other answers:\n\nThe intent is to create a new type, say that behaves just like a , except for its destructor. should call either or in its destructor. I've chosen above. Now one can simply substitute for in my example code:\n\nand the output is now:\n\nWhy choose instead of in ?\n\nIf you use , then the stack unwinding of the main thread will block until completes. This may be what you want, or it may not be. I can not answer this question for you. Only you can decide this design issue for your application. Consider:\n\nThe thread will still throw an exception under , but now it will hang during unwinding, and only 10 minutes later print out:\n\nand exit. Perhaps because of references or resources that are used within , this is what you need to have happen. But if it is not, then you can instead:\n\nand then your program will immediately output \"unexpected exception caught\" and return, even though is still busy crunching away (after returns will be forcefully canceled as part of a normal shutdown of the application).\n\nPerhaps you need for some of your threads and for others. Perhaps this leads you to two -like wrappers, or to a policy-based wrapper. The committee was unable to form a consensus for a solution, and so you must decide what is right for you, or live with .\n\nThis makes direct use of very, very low-level behavior. Ok for Hello World, but in a real application, best encapsulated away in a mid-level handler, either via private inheritance or as a private data member. The good news is that in C++11 that mid-level handler can now be written portably (on top of ) instead of writing down to the OS or a 3rd-party lib as is necessary in C++98/03."
    },
    {
        "link": "https://johnfarrier.com/powerful-tips-and-techniques-for-stdmutex-in-c",
        "document": "Concurrency and parallelism have become vital aspects of modern C++ programming, demanding robust mechanisms for handling multi-threaded environments. Enter , a fundamental synchronization primitive in the C++ Standard Library. When utilized correctly, ensures that only one thread accesses a particular resource at any given time, preventing data races and ensuring thread safety.\n\nThis article is tailored for experienced C++ developers looking to enhance their understanding of . (And developers who want to build Technical Capital in their projects!) We will explore its core concepts, various types, and effective techniques for seamless integration into your applications. As multi-threading becomes increasingly prevalent in software development, mastering is essential for writing efficient and safe code.\n\nUnderstanding how to work with effectively can significantly improve the performance and reliability of your software. Let’s embark on this journey to unlock the full potential of in C++.\n\nA mutex, short for mutual exclusion, is a synchronization primitive used to control access to a shared resource by multiple threads in a concurrent programming environment. In C++, is provided by the standard library as a means to implement this mutual exclusion. By locking a mutex, a thread ensures that other threads attempting to lock the same mutex will be blocked until the mutex is unlocked.\n\nin C++ is designed to have exclusive, non-recursive ownership semantics. This means that once a thread has locked a , no other thread can lock it until it is explicitly unlocked by the owning thread. Unlike recursive mutexes, does not allow the same thread to lock it multiple times without first unlocking it. This exclusivity ensures that the critical section guarded by the mutex is accessed by only one thread at a time, preventing race conditions.\n\nImproper handling of can lead to undefined behavior, making it crucial to understand and correctly implement mutex usage. Common pitfalls include:\n• Double locking: Attempting to lock a that the current thread has already locked can cause a program to deadlock or exhibit undefined behavior.\n• Unlocking by a different thread: Only the thread that has locked a should unlock it. Unlocking a mutex from a different thread leads to undefined behavior.\n• Not unlocking a mutex: Failing to unlock a will result in other threads being blocked indefinitely when they try to acquire the same mutex.\n\nProper usage of is essential for writing safe and efficient concurrent programs in C++. By understanding its core characteristics and avoiding common mistakes, developers can ensure their applications remain robust and maintainable.\n\nTypes of Mutex in C++\n\nWhen it comes to synchronizing threads and ensuring safe access to shared resources in C++, the makes an appearance as a crucial tool. However, is not the only type available in the C++ Standard Library. Let’s explore the different types of mutexes provided by the library and their use cases.\n\nis the simplest and most commonly used mutex type. It provides exclusive, non-recursive ownership semantics. This means a thread must acquire a before accessing the shared resource and release it once done. Failing to release the mutex can result in deadlocks, preventing other threads from accessing the resource. Here’s an example of using :\n\nHere, ensures that and do not interleave their outputs.\n\nextends by offering the ability to attempt to lock the mutex for a specified period or until a specific point in time. This can be helpful in scenarios where a thread should not wait indefinitely to acquire a lock. Instead, it can perform other tasks if it fails to acquire the mutex within a given time frame. Here’s how you can use :\n\nIn this code, attempts to lock the mutex for one second.\n\nA allows the same thread to acquire the same mutex multiple times without causing a deadlock. This is useful in recursive code where a function that holds a mutex lock may need to call itself or another function that also tries to acquire the same mutex. Here’s an example:\n\nSimilar to , allows the same thread to lock the same mutex multiple times but also includes timed locking capabilities. This type combines the functionalities of both and . Let’s look at an example of its usage:\n\nThis example demonstrates attempting to lock a recursive mutex with a timeout period. By understanding these variants, you can choose the most suitable mutex type for your specific synchronization needs, enhancing the efficiency and reliability of your C++ applications.\n\nTypes of Locks in C++\n\nLocks are essential when working with to ensure thread safety and proper synchronization in concurrent C++ programming. Here, we explore the primary types of locks you can use with :\n\nThe is a simple, lightweight locking mechanism that provides a convenient way to manage the ownership of a mutex. When an instance of is created, it locks the mutex, ensuring that the current thread has exclusive access to the protected resource. When the instance goes out of scope, the destructor automatically releases the lock, preventing any possible resource leaks or deadlock situations.\n\noffers more flexibility compared to . While locks the mutex upon creation and releases it upon destruction, allows deferred locking, timed locking, and manual unlocking. This flexibility can be beneficial in complex scenarios where you might need to lock and unlock the mutex multiple times within the same scope.\n\nis used with shared mutexes (such as ), and allows multiple threads to hold the same mutex in a read-only mode simultaneously. This is particularly useful in scenarios where resources are read more frequently than they are modified. However, cannot be used with directly, requiring instead.\n\nUnderstanding these lock types enables developers to harness the full potential of and related synchronization primitives in C++. By choosing the proper lock based on the requirements, you can write efficient, thread-safe code that minimizes contention and maximizes performance.\n\nsimplifies managing multiple mutexes simultaneously by acquiring all the locks in a consistent order, preventing deadlocks. It locks the mutexes at the start of the scope and releases them automatically when the scope ends.\n\nA spin lock is a low-level synchronization primitive that keeps a thread in a busy-wait loop until it successfully acquires the lock. Unlike a traditional mutex, which may put a thread to sleep if the lock is unavailable, a spin lock continues “spinning” (repeatedly checking the lock status) until it can proceed. This can be more efficient for short critical sections where the wait time is minimal, as it avoids the overhead of context switching between threads. However, spin locks can become costly if contention is high, as they consume CPU cycles without performing useful work.\n\nWhen to Use Spin Locks\n\nSpin locks are ideal for scenarios where:\n• Critical sections are very short, and the lock is expected to be held for a brief time.\n• The overhead of suspending and resuming threads (as done with mutexes) would outweigh the cost of spinning.\n• High performance is required, and the risk of contention is low.\n\nHowever, they should be avoided in cases where:\n• There is high contention among threads, which could lead to performance degradation from excessive spinning.\n• Locks may be held for longer durations.\n\nHere, the function sets the lock and returns its previous value. If the lock was already set, the thread keeps spinning (busy-waiting) until it acquires the lock. After completing the critical section, the lock is released using , allowing other threads to proceed.\n• Suitable for real-time systems where thread suspension is undesirable.\n• Can waste CPU cycles if contention is high.\n• Not ideal for long critical sections or when multiple threads contend for the same resource.\n\n, introduced in C++20, provides a synchronization mechanism for coordinating threads in phases. It ensures that a group of threads must reach a specific point (the barrier) before any can continue. After all threads arrive, they proceed together, enabling phased parallel processing.\n\nUsing in conjunction with is a powerful technique for managing thread synchronization and communication. The allows threads to efficiently wait for and be notified of specific conditions. This setup is particularly useful for scenarios where threads must wait for certain conditions before proceeding.\n\nA is an object used to block one or more threads until another thread modifies a shared variable and notifies the condition variable. It is a synchronization primitive that enables blocking of threads but efficiently releases the CPU while waiting.\n\nTo use a , you need to associate it with an . Here is a simple example to illustrate the usage:\n\nIn this example, we see how is used to protect the shared variable. allows threads to wait until they are notified. is called by the threads to wait until is set to true. Finally, wakes all waiting threads once the condition is met.\n\nBenefits of Using std::mutex with std::condition_variable\n• Thread Efficiency: A condition variable allows threads to sleep while waiting, reducing CPU consumption significantly compared to busy-waiting.\n• Inter-Thread Communication: Facilitates communication between threads, enabling them to coordinate their actions based on shared states.\n• Avoid Spurious Wakes: Use a while-loop for checking the condition before proceeding after . This ensures that the thread only proceeds when the condition is actually met.\n• Minimal Scope: Hold the mutex lock for the minimal possible duration to minimize contention.\n• Clear Signaling: Use if only one thread needs to be awakened, and if multiple threads must proceed.\n\nIntegrating with enriches your multithreading toolkit. By mastering this technique, your C++ applications will be able to handle more complex synchronization scenarios efficiently and reliably.\n\nBest Practices and Tips Using std::mutex\n\nEffectively utilizing in C++ requires adhering to several best practices to ensure safe and efficient concurrency management. Below are some essential tips for using :\n\nA fundamental rule when using is to avoid deadlocks, which occur when two or more threads are stuck waiting for each other indefinitely. To prevent this:\n• Consistent Locking Order: Always lock multiple mutexes in a predefined order across all threads. This practice reduces the risk of circular dependencies.\n• Use std::lock(): The function can lock multiple objects simultaneously, minimizing the risk of deadlocks by avoiding intermediate states.\n\nAdhering to the RAII principle is crucial for managing resources like . RAII ensures that resources are properly released when they go out of scope:\n\nstd::lock_guard: Use for simple mutex locking. It locks the mutex upon construction and automatically releases it when the goes out of scope.\n\nstd::unique_lock: For more complex scenarios where you need to lock and unlock the mutex multiple times within a block, use . It provides more flexibility than .\n\nWhile is essential for thread synchronization, it can also introduce performance bottlenecks if not used correctly:\n\nMinimize Locked Code: Only protect the smallest possible critical section. Extensive locking can lead to contention, decreasing performance\n\nAvoid Unnecessary Locks: Ensure that only shared resources that need protection are surrounded by . Overuse of mutexes can serialize your code and degrade performance.\n\nImplementing these best practices when working with will help ensure safe, efficient, and performant multi-threaded applications in C++.\n\nMastering the use of is fundamental for developing robust multithreaded applications in C++. Proper usage of ensures that critical sections of code are executed atomically, thereby preventing race conditions and data corruption. By understanding the different types of mutexes and locks, such as and , developers can choose the most appropriate synchronization mechanism for their specific use case.\n\nIt’s essential to adhere to best practices when working with to avoid common pitfalls such as deadlocks and performance bottlenecks. Utilizing techniques like RAII (Resource Acquisition Is Initialization) can help manage the lifecycle of mutex locks efficiently and prevent resource leaks.\n\nAdditionally, integrating with can offer more sophisticated synchronization by allowing threads to wait for specific conditions to be met before proceeding. This combination is especially useful in producer-consumer scenarios and other complex multithreaded applications.\n\nBy consistently applying the strategies and techniques discussed, you can significantly improve the reliability and performance of your multithreaded C++ programs. Embrace these practices to make the most of and elevate your concurrency programming skills.\n\nLearn More about the C++ Standard Library! Boost your C++ knowledge with my new book: Data Structures and Algorithms with the C++ STL!"
    },
    {
        "link": "https://linkedin.com/pulse/mastering-multithreading-concurrency-c-comprehensive-deep-bajwa",
        "document": "Multithreading and concurrency stand as fundamental pillars in the domain of modern software development. These concepts play a pivotal role in optimizing application performance. Within the robust confines of the C++ programming language, multithreading and concurrency are not just features to explore but essential components that empower developers to craft highly efficient and responsive software. In this comprehensive article, we embark on an extensive journey into the realm of multithreading and concurrency in C++, starting with the basics and venturing into advanced topics.\n\nAt its core, multithreading is the art of executing multiple threads concurrently within a single program. Threads, akin to nimble and lightweight processes, coexist within the same memory space. This arrangement allows them to communicate and collaborate seamlessly, performing tasks simultaneously. Multithreading, particularly on multi-core processors, can yield remarkable gains in application performance.\n\nC++ equips programmers with a toolkit for managing threads. This toolkit encompasses essential functions such as thread creation, joining, detaching, and termination. Proficiency in utilizing these functions is imperative for mastering multithreading.\n\nWhen multiple threads engage in simultaneous access and modification of shared data, the peril of data races looms, resulting in undefined behavior. The salvation lies in synchronization mechanisms such as mutexes and locks.\n\nMutexes, the sentinels of multithreading, stand as guardians protecting shared data. C++ extends its benevolence with std::mutex, std::unique_lock, and std::lock_guard to streamline the management of mutexes and locks efficiently. Behold a vivid example:\n\nCondition variables (`std::condition_variable`) offer threads the ability to pause gracefully until specific conditions are met. They emerge as invaluable allies, particularly in scenarios where one thread produces data, while another thread consumes it.\n\nAn essential distinction lies in understanding the difference between parallelism (the simultaneous execution of threads on multiple cores) and concurrency (the efficient orchestration of multiple tasks). This comprehension is pivotal for optimizing application performance.\n\nThread pools usher in the age of efficiency by simplifying the management and reuse of a fixed number of threads for tasks. They mitigate the overhead of thread creation and destruction. C++ provides libraries such as <thread> and <future> to embrace the world of thread pools.\n\nMultithreading and concurrency in C++ are formidable tools for crafting high-performance applications. While they present challenges like data races and synchronization, a comprehensive understanding of these concepts, coupled with the tools and techniques furnished by C++, empowers you to unleash the full potential of multithreading. The result is software that not only exemplifies efficiency but also responsiveness. Continuous practice and experimentation are your steadfast companions on the journey to mastering this indispensable facet of C++ programming."
    }
]