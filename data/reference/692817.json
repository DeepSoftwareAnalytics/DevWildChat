[
    {
        "link": "https://redhat.com/en/blog/customize-user-environments",
        "document": "System security is a major concern. As a sysadmin, as I've stated before, it's your primary concern. Adding users to a system decreases security. Your job is to create a usable but secure environment for your users. You have corporate assets to protect, systems to maintain, and users to satisfy. There's often a conflict among those three aspects of system administration, as you well know. One way to satisfy all three is to customize your user's environments by implementing and enforcing a corporate standard. Striking a balance between user productivity and system security isn't easy. I can't write specifications for your particular situation, but I can show you where to make the necessary changes so that you can do so.\n\nThis article covers customizing your user's environments using files found in the and directories. With a fresh system install, you'll find three files under /etc/skel: , , and . When you create a new user account on a system, these three files are copied to the user's home directory and are owned by the user. In case you don't know, so-called dot files (those named with a preceding dot (.) are hidden from standard file lists. To see them, you must use the switch with the ls command.\n\nAs you can see, these files are owned by root and can only be edited or changed by the root user.\n\nThe file is the most important of the three files listed. It's most important because it is the only \"required\" file in the list. It executes every time the user logs into a system, it launches the file, and defines and exports the PATH variable. Its default settings are simple.\n\nThe can also be used to define a custom shell prompt, define one's editor of choice, or anything else that you want to place into the file for the user.\n\n[ You might also like: Linux environment variable tips and tricks ]\n\nThe contents of the .bashrc file, by default, only call the file. The file consists of settings that can be configured for all users.\n\nYou could call other files configured for certain user groups as well. For example, if a user is a member of the finance group, you could call a file to set up a particular set of variables for all users in the finance group.\n\nThe listing for is far too long for this venue, but you can look at it and see what it does. The file refers to the file for more environment variables and settings. Both files come with the following warning.\n\nSo, you see, customizing a user environment is not as simple as you thought.\n\nIf you list the files in , you'll see the following:\n\nYou can see that many of the files are for use in the C shell. The most important file for this article's focus is . The contents of is listed below.\n\nAs you can see from the message, if you want to override any currently configured envvar (Environment variables) entries with a corporate standard, make entries in this file to do that.\n\nAs users learn more about their environments and Google things, they'll customize their own—often to their detriment. You have to strike a balance between being a laissez-faire sysadmin and a heavy-handed dictator sysadmin. You want users to be productive but have some limited control over their own environments. My suggestion to make both sides happy is to set all corporate standard user environment parameters in and in that you don't want to be edited or changed.\n\nRealize that the , , and are user-editable files. The only way around this is to change permission on those files with a root user script after you create the accounts. In other words, run a script after you create a user account to change the permissions on the files to . The user won't be able to alter the files.\n\nIf you want to lock down the environment by changing the files to root ownership, you could create a new file in such as a file that the user can edit and include it in the file.\n\n[ Want to learn more about security? Check out the IT security and compliance checklist. ]\n\nCustomizing a user's environment can enhance system security and standardize what users see and how they interact with a system. Granting shell access to a production system has its own implications, but you must provide resources for your users to maximize their productivity and maximize system security. If you find that perfect balance, please write it up into an article for Enable Sysadmin. In my own experience, every user feels that they are the exception to the corporate standard and, pretty soon, you have a bunch of exceptions and no corporate standard at all."
    },
    {
        "link": "https://github.com/CLIUtils/envmodule_setup/blob/master/docs/CentOS7.md",
        "document": "This is a guide to setting up Lmod (lua environment modules) on a CentOS7 system. I've used a similar procedure to set them up on a Mac, as well, so this is still a useful guide to the workings of Lmod if you use a different system; mostly paths will change. On a Mac, you'll want to install from the tap in .\n\nThere are several good pages covering environment modules (TCL version), but not many that use the newer Lua syntax. This document aims to fill that roll.\n\nTo install Lmod, you should add the epel repository and then do to set it up. It will add an init script to to activate the command automatically for all users. If you need to customize anything, like the paths included in the , the scripts added here are and . You might find that removing the last entry in the modulepath init script ( ) is useful, as the is better set by the profile script already sourced.\n\nI'll be showing you how to set up the directories in , since that is already prepared for us. We'll make two directories, and . should be the only one in your module path; I perfer not to have in the , as that will add clutter when looking at the available modules (and it is already available as ).\n\nInside , you'll make directories for your compilers, such as . Inside , we'll set up the default 4.8 compiler by creating a file called . This is what it should look like:\n\nThis has a lot more than is needed, but provides an example of everything you need. This is in Lua syntax, so the items like the multiline string and the local statement for defining local variables might be a little unfamiliar to Python users, but otherwise it's a lot like Python.\n• The command, which means only one module marked with this string can be loaded at a time.\n• The commands, which set environment variables and forget the previous setting\n• Another option is , which remember the previous setting for unloading\n• Another similar command is , which clears a variable on loading\n• The command , which adds a path to the beginning of an environment variable, and removes it on unloading\n• There is also a matching command\n• The fact that I'm changing in a module is special to Lmod; it will also unload all modules in the added path when it is removed!\n\nI've also chosen to use a prefix and a version to make this easy to change. The commmand is from one of the optional Lua libraries that Lmod includes automatically.\n\nFeel free to add more compilers to if needed.\n\nYou'll notice that my compiler loaded it's matching directory in Compiler; that's where packages that are compiled with this compiler live. Matching names in multiple compiler directories will correctly swap if you swap compiliers! Inside a directory, add the modules that you want to use.\n\nIf you want the default module to be something other than the latest version number, make a symbolic link in the directory pointing to the file you want; if you want to alias a simpler version number, like 6 instead of 6.08.02, you can also make a similar symbolic link.\n\nMany packages use a shell script to set up the environment. Lmod can capture those changes and write (most) of the file for you. To run it, load the included package and then run and it will write a modulefile for you. You can use to see the options. If you like the prefix suggestion I used above, I have written my own version of this script that includes this, available as an example in a Plumbum branch here. It also can save an environment to a file, then load and compare, creating the file that describes the changes.\n\nSome common commands you'll want are:\n• Prepares a compiler, makes the packages built with that compiler available\n• Shows all the currently used modules\n• Saves the current list of packages (can save to a name, or without a name is your personal default)\n• Loads a (named or default) package list - note the system default is"
    },
    {
        "link": "https://stackoverflow.com/questions/48554652/centos-setting-global-environment-variables-at-startup",
        "document": "Write a startup script to start the server. export all variables in a startup script before starting the server. Maybe something like this: //startup.js\n\nWhen you use this script to start the server, these variables will be exported before the startup, and will be available in the process environment.\n\nOne more way could be using something like ansible for the deployment. With that, you can use any other file to export variables before running the startup script."
    },
    {
        "link": "https://cherryservers.com/blog/how-to-set-list-and-manage-linux-environment-variables",
        "document": "Environment variables are a key aspect of Linux system administration and provide engineers with a powerful tool for customizing systems. In this Linux tutorial, we’ll take a closer look at what Linux environment variables are, how they differ from shell variables, and how you can list, set, and manage them.\n\n#What are Environment Variables in Linux?\n\nLinux environment variables are dynamic variables used by a shell and its child processes. Environment variables define a variety of aspects related to how a Linux system works. For example, a user’s default shell is defined in the SHELL variable. Similarly, many administrators use the JAVA_HOME= environment variable to point programs to specific versions of Java on a system.\n\nA few key points to keep in mind when working with Linux environment variables are:\n• You can specify multiple values for a single variable by separating them with colons like this:\n• By convention, but not rule, environment variable names are always capitalized.\n• Shell variables are not the same as environment variables. Shell variables only apply to the current shell, not any child processes. We’ll take a closer look at this topic in the Linux shell variables vs. Linux environment variables section.\n\nWith all that in mind, let’s move on to the Linux environment variables setup tutorial.\n\nOur examples use Ubuntu 20.04 and GNU Bash 5.0.17, but you can follow along on most modern Linux systems. There are multiple methods for listing Linux environment variables here we will demonstrate some of the most common.\n\nNow that we know what environment variables in Linux are, what they are used for, and have defined the prerequisites needed to set and list Linux environment variables, let’s look at ways to get you started.\n\nYou can use to list all your current Linux environment variables by running the command without any options or parameters.\n\nThe output of should look similar to this:\n\nYou can also use to display individual environment variables. The basic syntax to display a single Linux environment variable with is:\n\nFor example, to print the environment variable, use this command:\n\nThe output should display your shell’s current working directory.\n\nThe env command can list all environment variables with output similar to . To use env to list all your Linux environment variables, run the command with no options or parameters, like this:\n\nIn most cases, the output should match the output of except for the variable. This happens because the variable is a special Bash parameter used to invoke a shell or shell script. If you use env on a modern Linux system, the value will match the location of the binary (e.g. /usr/bin/env). If you use , it will match the location of the printenv binary (e.g./usr/bin/printenv).\n\nOne of the quickest ways to display a single Linux environment variable is to use the command. The basic syntax is .\n\nFor example, to print the PATH environment variable, use this command:\n\nThe output should look similar to:\n\nThe command also works for shell variables that are NOT environment variables. For example, if we create a BREAKFAST variable and set it to 'Pepper & Egg', we can see the output with but not with .\n• - this built-in shell utility can display shell and environment variables using this command.\n• - displays the names of all your shell and environment variables.\n\nThe basic command to set a Linux environment variable is\n\nFor example, to create an environment variable named SANDWICH with a value of Pepper And Egg, use this command:\n\nTo confirm that the environment variable was created, use the command\n\nThe output should be as follows:\n\nYou can create shell variables using this basic command structure .\n\nFor example, to create a shell variable named COOLSERVER with a value of Cherry, use this command:\n\nYou can confirm the variable now exists using this command:\n\nThe output should be:\n\nTo confirm the variable is not an environment variable, execute this command:\n\nBecause the variable isn’t an environment variable, there should be no output:\n\n#How to Convert a Shell Variable to an Environment Variable\n\nTo convert a shell variable to an environment variable, the basic command syntax is .\n\nFor example, to convert the COOLSERVER variable from the previous example to an environment variable, use this command:\n\nNow, the command should print Cherry as the output:\n\nWith what we have covered so far, you can see shell variables and environment variables are different things. Now, let’s take a closer look at the difference and why it matters.\n\nThe key difference between Linux shell variables and Linux environment variables is: shell variables are not shared with a shell’s child processes, environment variables are shared with a shell's child processes.\n\nTo demonstrate why that difference is important, let’s walk through an example with a simple bash script.\n\nFirst, create a variable named CLOUD and set the value to Cherry Servers!.\n\nNext, use the command to display the variable.\n\nHere’s what that looks like end-to-end:\n\nNow, create a bash script that includes the same command. The script should look like this:\n\nMake the script executable with this command:\n\nThe output should be blank, like this:\n\nThe reason the output is blank is because is a shell variable. It is contained within the current shell instance. Meanwhile, the script initiates a new process,and, since the environment variable is not set, the script doesn’t print anything.\n\nLet’s make an environment variable with this command (note do NOT use the $ before the variable name):\n\nNow, run the test.sh script again.\n\nYou should see this output:\n\nThe basic syntax to delete an existing Linux environment variable is .\n\nFor example, to unset our COOLSERVER variable, use this command:\n\nTo confirm the variable is now deleted, execute this command:\n\nBecause the variable isn’t an environment variable, there should be no output:\n\nThe environment variables we’ve created so far will only last for the duration of your shell session. If you log out or reboot, you would need to recreate them if you want to use them.\n\nFortunately, there are several ways to make Linux environment variables persist across different shell sessions.\n\n#Use .profile to Make Environment Variables Permanent for Login Shells\n\nThe file exists in a user’s home directory. You can add environment variables for a user by editing their file to include export commands to set environment variables.\n\nFor example, to make our COOLSERVER environment variable permanent for our current user, follow this process:\n• Add the following line to the bottom of the file:\n\nThe changes will take effect on the next login. You can make them take effect for the current terminal using this command:\n\n#Use .bashrc to Make Environment Variables Permanent for Non-login Interactive Shells\n\nFor non-login shells, you can follow the same process as login shells, but instead of editing file, edit .\n\n⚠️ Warning: Any shell on the system can access system-wide variables, so set them with caution.\n\nTo set system-wide Linux environment variables, you can edit the file. Instead of appending export commands to the file, append the pair to the end of the file.\n\nFor example, to make our COOLSERVER environment variable permanent for the entire system, follow this process:\n• Add the following line to the bottom of the file\n\nAlso read: How to install deb file on Ubuntu 22.04\n\nNow that you know how to list, set, and manage Linux environment variables, you can use them to customize your systems and scripts. To take a deeper dive on some of the topics we reviewed here, check out the man pages for bash, export, and env.\n\nFor more content like this, subscribe to the Cherry Servers blog!"
    },
    {
        "link": "https://redhat.com/en/blog/linux-environment-variables",
        "document": "Environment variables exist to enhance and to standardize your shell environment on Linux systems. There are standard environment variables that the system sets up for you, but you can also set up your own environment variables, or optionally change the default ones to meet your needs.\n\nIf you want to see your environment variables, use the command and look for the words in all caps in the output's far left. These are your environment variables, and their values are to the right:\n\nI have omitted the output of the variable because it is so long. Try this command on your system to see what the full output looks like.\n\nMany environment variables are set and then exported from the file and the file. There is a line in that reads:\n\nTo make permanent changes to the environment variables for all new accounts, go to your files, such as , and change the ones that are already there or enter the new ones. When you create new users, these files will be copied to the new user's home directory.\n\nTo call the value of a single environment variable, enter the following command, using (Shell Level) as an example:\n\nThis variable changes depending on how many subshells you have open. For example, enter twice and then issue the command again:\n\nA shell level of three means that you are two subshells deep, so type twice to return to your regular shell.\n\n[Want to try out Red Hat Enterprise Linux? Download it now for free.]\n\nThe variable contains the search path for executing commands and scripts. To see your , enter:\n\nTemporarily change your by entering the following command to add :\n\nThe change is temporary for the current session. It isn't permanent because it's not entered into the file. To make the change permanent, enter the command into your home directory's file.\n\nWhen you do this, you're creating a new variable by appending a directory to the current variable, . A colon ( ) separates entries.\n\nI had a theory that I think has been dispelled by my own good self. My theory was that the commands and probably just read and echoed the contents of the shell variables and or , respectively. To my surprise, after looking at the source code, they don't. Maybe I should rewrite them to do just that. There's no reason to add multiple libraries and almost 400 lines of C code to display the working directory. You can just read and echo that to the screen (stdout). The same goes for with either or .\n\nIf you want to have a look at the source code for yourself, it's on GitHub and other places. If you find that these programs (or others) do use shell variables, I'd love to know about it. Admittedly, I'm not that great at reading C source code, so they could very well use shell variables and I'd never know it. They just didn't seem to from what I read and could understand.\n\nIn this last environment variable overview, I want to show you how the variable comes in handy. You don't have to stay in your default shell, which is likely Bash. You can enter into and work in any shell that's installed on the system. To find out which shells are installed on your system, use the following command:\n\nAll of those are actually Bash, so don't get excited. If you're lucky, you might also see entries for , , , , and .\n\nYou can use any of these shells and have different things going on in each one if you're so inclined. But, let's say that you're a Solaris admin and you want to use the Korn shell. You can change your default shell to using the command:\n\nNow, if you type , the response will be , so you have to log out and log in again to see the change. Once you log out and log in, you will receive a different response to .\n\nYou can enter other shells and should report your current shell and , which will keep you oriented as to how many shells deep you are.\n\n[ Learn how to manage your Linux environment for success. ]\n\nYou can set your own variables at the command line per session, or make them permanent by placing them into the file, , or whichever startup file you use for your default shell. On the command line, enter your environment variable and its value as you did earlier when changing the variable.\n\nShell or environment variables are helpful to users, sysadmins, and programmers alike. They are useful on the command line and in scripts. I've used them over the years for many different purposes, and although some of them are probably a little unconventional, they worked and still do. Create your own or use the ones given to you by the system and installed applications. They truly can enrich your Linux user experience.\n\nAs a side note on variables and shells, does anyone think that those who program in JSON should only be allowed to use the Bourne Shell? Discuss."
    },
    {
        "link": "https://redhat.com/en/blog/error-handling-bash-scripting",
        "document": "Scripting is one of the key tools for a sysadmin to manage a set of day-to-day activities such as running backups, adding users/groups, installing/updating packages, etc. While writing a script, error handling is one of the crucial things to manage.\n\nThis article shows some basic/intermediate techniques of dealing with error handling in Bash scripting. I discuss how to obtain the error codes, get verbose output while executing the script, deal with the debug function, and standard error redirection. Using these techniques, sysadmins can make their daily work easy.\n\nIn Bash scripting, prints the exit status. If it returns zero, it means there is no error. If it is non-zero, then you can conclude the earlier task has some issue.\n\nA basic example is as follows:\n\nIf you run the above script once, it will print because the directory does not exist, therefore the script will create it. Naturally, you will get a non-zero value if you run the script a second time, as seen below:\n\nIt is always recommended to enable the debug mode by adding the option to your shell script as below:\n\nYou can write a debug function as below, which helps to call it anytime, using the example below:\n\nYou can redirect all the system errors to a custom file using standard errors, which can be denoted by the number 2. Execute it in normal Bash commands, as demonstrated below:\n\nMost of the time, it is difficult to find the exact line number in scripts. To print the line number with the error, use the PS4 option (supported with Bash 4.1 or later). Example below:\n\nYou can easily see the line number while reading the errors:\n\n[ Get this free ebook: Managing your Kubernetes clusters for dummies. ]\n\nManaging errors is a key skill for administrators when writing scripts. These tips should help make your life easier when troubleshooting Bash scripts or even general commands."
    },
    {
        "link": "https://dev.to/unfor19/writing-bash-scripts-like-a-pro-part-2-error-handling-46ff",
        "document": "In the previous blog post of this series, we covered the basics of how to write a proper Bash script, with a consistent naming convention. Writing Bash scripts consistently while learning new terms can be a significant challenge when doing it from scratch. But fear not! We're here to guide you on becoming a Bash scripting master!\n\nAnd the journey of loving Bash continues!\n\nBefore diving into the fascinating error-handling world in Bash scripts, let's set the \"definition of done.\" By the end of this blog post, we aim to achieve the following:\n• Take advantage of STDERR and STDOUT to handle errors.\n• Allow script execution to continue even if there's an error.\n• Invoke an error handler (function) according to an error message.\n\nBash scripts return an exit status or exit code after execution. An exit code 0 means success, while a non-zero exit code indicates an error. Understanding exit codes is fundamental to effective error handling.\n\nWhen a command succeeds, it returns an exit code of 0, indicating success:\n\n\n\nIf the ls command fails, it returns a non-zero exit code, indicating an error:\n\n\n\nUsing Exit Codes to Our Advantage\n\nOne way to handle errors is by adding the set -e option to your Bash scripts. When enabled, it ensures that the script will terminate immediately if any command exits with a non-zero status. It's like a safety net that automatically catches errors and stops the script from continuing.\n\n\n\nIn this example, the command attempts to list the contents of a non-existent directory, causing an error. Due to , the script will stop executing after encountering the error, and the last line won't be printed.\n\nWe covered a few new characters and terms, so let's make sure we fully understand what they do:\n• The $HOME variable exists on any POSIX system, so I used it to demonstrate how Bash can use a Global Environment Variable that contains your \"home directory path\".\n• The $? character is an exit status variable which stores the exit code of the previous command.\n• The option set -e forces the script to stop executing when encountering any error.\n\nOften, you might want to capture the output (both STDOUT and STDERR) of a command and handle it differently based on whether it succeeded or failed (raised an error).\n\nWe can use the expression $(subcommand) to execute a command and then capture its output into a variable. The important part is to redirect to by adding to the end of the \"subcommand\".\n\n\n\nIn this example, the command attempts to list the contents of a non-existent directory. The output (including the error message) is captured in the response variable. We then check the exit status using and print either or accordingly.\n\nSo far, we covered to terminate execution on an error and redirect error output to standard output , but what happens if we combine them?\n\nYou may want to continue executing the script even if a command fails and save the error message for later use in an error handler. The technique comes to the rescue! It allows the script to proceed without terminating, even if a command exits with a non-zero status. That is an excellent technique for handling errors according to their content.\n\nIn the following example, we attempt to ping each server, with a timeout of 1 second, using the ping command. If the server is reachable, we should print \"Response - ${response}.\". Though what happens if the ping fails? How do we handle that? Let's solve it with a use-case scenario!\n\nTo make it authentic as possible, I added an array of servers with the expression.\n\nAfter that, I used the for do; something; done loop to iterate over the servers. For each iteration, we ping a server and redirect to to capture the error's output to the variable .\n\nAnd the final tweak was to add inside the evaluation so that even if the command fails, its output is saved in the variable.\n\n\n\nThe response will always be evaluated as because of this part:\n\n\n\nThe best way to fix it is to analyze a successful response message and set it as an \"indicator of a successful response\", and in any other case, the script should fail with an error message.\n\nIn the case of running , a successful response can be considered as:\n\n\n\nThe analysis should be done for a specific use case; this approach assists with handling unknown errors by setting a single source of truth for a successful response and considering anything else as an error.\n\nHere's the final version of the code, with a few upgrades to the output:\n\n\n\nYou've just learned how to use a Bash pipe | to pass data to the grep command. The trick is to and pipe it with to the command like this:\n\n\n\nYou've also learned about /dev/null, a black hole where you can redirect output that shouldn't be printed or saved anywhere.\n\nA more complex scenario may require dedicated error handlers, for example, executing an HTTP Request with curl, and handling HTTP Responses; You can create a custom function to handle specific responses gracefully, like this:\n\n\n\nThis blog post took it up a notch; you've learned several terms and can now handle errors in Bash like a Pro! There are still more tricks in this error-handling mix, like trapping CTRL-C error; we'll discover more about that and other ways to handle errors using Bash scripts."
    },
    {
        "link": "https://unix.stackexchange.com/questions/79315/when-to-use-redirection-to-stderr-in-shell-scripts",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://stackoverflow.com/questions/64786/error-handling-in-bash",
        "document": "Do you have a better error handling routine that you use in Bash scripts?\n\nHe suggests using the following function for error handling in Bash:\n\nWhat is your favorite method to handle errors in Bash? The best example of handling errors I have found on the web was written by William Shotts, Jr at http://www.linuxcommand.org .\n\nThe community reviewed whether to reopen this question 3 years ago and left it closed:\n\nWant to improve this question? Update the question so it can be answered with facts and citations by editing this post .\n\n. This question is opinion-based . It is not currently accepting answers.\n\ntempfiles=( ) cleanup() { rm -f \"${tempfiles[@]}\" } trap cleanup 0 error() { local parent_lineno=\"$1\" local message=\"$2\" local code=\"${3:-1}\" if [[ -n \"$message\" ]] ; then echo \"Error on or near line ${parent_lineno}: ${message}; exiting with status ${code}\" else echo \"Error on or near line ${parent_lineno}; exiting with status ${code}\" fi exit \"${code}\" } trap 'error ${LINENO}' ERR ...then, whenever you create a temporary file: and will be deleted on exit, and the current line number will be printed. ( will likewise give you exit-on-error behavior, though it comes with serious caveats and weakens code's predictability and portability). You can either let the trap call for you (in which case it uses the default exit code of 1 and no message) or call it yourself and provide explicit values; for instance: will exit with status 2, and give an explicit message. Alternatively and give the first lines of the trap a little modification to trap all non-zero exit codes across the board (mind non-error non-zero exit codes): This then is also \"compatible\" with .\n\nReading all the answers on this page inspired me a lot.\n\n \n\n So, here's my hint:\n\n \n\n file content: lib.trap.sh lib_name='trap' lib_version=20121026 stderr_log=\"/dev/shm/stderr.log\" # # TO BE SOURCED ONLY ONCE: # ###~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## if test \"${g_libs[$lib_name]+_}\"; then return 0 else if test ${#g_libs[@]} == 0; then declare -A g_libs fi g_libs[$lib_name]=$lib_version fi # # MAIN CODE: # ###~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## set -o pipefail # trace ERR through pipes set -o errtrace # trace ERR through 'time command' and other functions set -o nounset ## set -u : exit the script if you try to use an uninitialised variable set -o errexit ## set -e : exit the script if any statement returns a non-true return value exec 2>\"$stderr_log\" ###~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## # # FUNCTION: EXIT_HANDLER # ###~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## function exit_handler () { local error_code=\"$?\" test $error_code == 0 && return; # # LOCAL VARIABLES: # ------------------------------------------------------------------ # local i=0 local regex='' local mem='' local error_file='' local error_lineno='' local error_message='unknown' local lineno='' # # PRINT THE HEADER: # ------------------------------------------------------------------ # # Color the output if it's an interactive terminal test -t 1 && tput bold; tput setf 4 ## red bold echo -e \"\n\n(!) EXIT HANDLER:\n\n\" # # GETTING LAST ERROR OCCURRED: # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ # # # Read last file from the error log # ------------------------------------------------------------------ # if test -f \"$stderr_log\" then stderr=$( tail -n 1 \"$stderr_log\" ) rm \"$stderr_log\" fi # # Managing the line to extract information: # ------------------------------------------------------------------ # if test -n \"$stderr\" then # Exploding stderr on : mem=\"$IFS\" local shrunk_stderr=$( echo \"$stderr\" | sed 's/\\: /\\:/g' ) IFS=':' local stderr_parts=( $shrunk_stderr ) IFS=\"$mem\" # Storing information on the error error_file=\"${stderr_parts[0]}\" error_lineno=\"${stderr_parts[1]}\" error_message=\"\" for (( i = 3; i <= ${#stderr_parts[@]}; i++ )) do error_message=\"$error_message \"${stderr_parts[$i-1]}\": \" done # Removing last ':' (colon character) error_message=\"${error_message%:*}\" # Trim error_message=\"$( echo \"$error_message\" | sed -e 's/^[ \\t]*//' | sed -e 's/[ \\t]*$//' )\" fi # # GETTING BACKTRACE: # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ # _backtrace=$( backtrace 2 ) # # MANAGING THE OUTPUT: # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ # local lineno=\"\" regex='^([a-z]{1,}) ([0-9]{1,})$' if [[ $error_lineno =~ $regex ]] # The error line was found on the log # (e.g. type 'ff' without quotes wherever) # -------------------------------------------------------------- then local row=\"${BASH_REMATCH[1]}\" lineno=\"${BASH_REMATCH[2]}\" echo -e \"FILE:\\t\\t${error_file}\" echo -e \"${row^^}:\\t\\t${lineno}\n\n\" echo -e \"ERROR CODE:\\t${error_code}\" test -t 1 && tput setf 6 ## white yellow echo -e \"ERROR MESSAGE:\n\n$error_message\" else regex=\"^${error_file}\\$|^${error_file}\\s+|\\s+${error_file}\\s+|\\s+${error_file}\\$\" if [[ \"$_backtrace\" =~ $regex ]] # The file was found on the log but not the error line # (could not reproduce this case so far) # ------------------------------------------------------ then echo -e \"FILE:\\t\\t$error_file\" echo -e \"ROW:\\t\\tunknown\n\n\" echo -e \"ERROR CODE:\\t${error_code}\" test -t 1 && tput setf 6 ## white yellow echo -e \"ERROR MESSAGE:\n\n${stderr}\" # Neither the error line nor the error file was found on the log # (e.g. type 'cp ffd fdf' without quotes wherever) # ------------------------------------------------------ else # # The error file is the first on backtrace list: # Exploding backtrace on newlines mem=$IFS IFS=' ' # # Substring: I keep only the carriage return # (others needed only for tabbing purpose) IFS=${IFS:0:1} local lines=( $_backtrace ) IFS=$mem error_file=\"\" if test -n \"${lines[1]}\" then array=( ${lines[1]} ) for (( i=2; i<${#array[@]}; i++ )) do error_file=\"$error_file ${array[$i]}\" done # Trim error_file=\"$( echo \"$error_file\" | sed -e 's/^[ \\t]*//' | sed -e 's/[ \\t]*$//' )\" fi echo -e \"FILE:\\t\\t$error_file\" echo -e \"ROW:\\t\\tunknown\n\n\" echo -e \"ERROR CODE:\\t${error_code}\" test -t 1 && tput setf 6 ## white yellow if test -n \"${stderr}\" then echo -e \"ERROR MESSAGE:\n\n${stderr}\" else echo -e \"ERROR MESSAGE:\n\n${error_message}\" fi fi fi # # PRINTING THE BACKTRACE: # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ # test -t 1 && tput setf 7 ## white bold echo -e \"\n\n$_backtrace\n\n\" # # EXITING: # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ # test -t 1 && tput setf 4 ## red bold echo \"Exiting!\" test -t 1 && tput sgr0 # Reset terminal exit \"$error_code\" } trap exit_handler EXIT # ! ! ! TRAP EXIT ! ! ! trap exit ERR # ! ! ! TRAP ERR ! ! ! ###~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## # # FUNCTION: BACKTRACE # ###~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~## function backtrace { local _start_from_=0 local params=( \"$@\" ) if (( \"${#params[@]}\" >= \"1\" )) then _start_from_=\"$1\" fi local i=0 local first=false while caller $i > /dev/null do if test -n \"$_start_from_\" && (( \"$i\" + 1 >= \"$_start_from_\" )) then if test \"$first\" == false then echo \"BACKTRACE IS:\" first=true fi caller $i fi let \"i=i+1\" done } return 0 #!/bin/bash source 'lib.trap.sh' echo \"doing something wrong now ..\" echo \"$foo\" exit 0 doing something wrong now .. (!) EXIT HANDLER: FILE: trap-test.sh LINE: 6 ERROR CODE: 1 ERROR MESSAGE: foo: unassigned variable BACKTRACE IS: 1 main trap-test.sh Exiting! \n\n As you can see from the screenshot below, the output is colored and the error message comes in the used language.\n\n \n\n\n\nInspired by the ideas presented here, I have developed a readable and convenient way to handle errors in bash scripts in my bash boilerplate project. By simply sourcing the library, you get the following out of the box (i.e. it will halt execution on any error, as if using thanks to a on and some bash-fu): There are some extra features that help handle errors, such as try and catch, or the throw keyword, that allows you to break execution at a point to see the backtrace. Plus, if the terminal supports it, it spits out powerline emojis, colors parts of the output for great readability, and underlines the method that caused the exception in the context of the line of code. The downside is - it's not portable - the code works in bash, probably >= 4 only (but I'd imagine it could be ported with some effort to bash 3). The code is separated into multiple files for better handling, but I was inspired by the backtrace idea from the answer above by Luca Borrione. To read more or take a look at the source, see GitHub:\n\nI prefer something really easy to call. So I use something that looks a little complicated, but is easy to use. I usually just copy-and-paste the code below into my scripts. An explanation follows the code. #This function is used to cleanly exit any script. It does this displaying a # given error message, and exiting with an error code. function error_exit { echo echo \"$@\" exit 1 } #Trap the killer signals so that we can exit with a good message. trap \"error_exit 'Received signal SIGHUP'\" SIGHUP trap \"error_exit 'Received signal SIGINT'\" SIGINT trap \"error_exit 'Received signal SIGTERM'\" SIGTERM #Alias the function so that it will print a message with the following format: #prog-name(@line#): message #We have to explicitly allow aliases, we do this because they make calling the #function much easier (see example). shopt -s expand_aliases alias die='error_exit \"Error ${0}(@`echo $(( $LINENO - 1 ))`):\"' I usually put a call to the cleanup function in side the error_exit function, but this varies from script to script so I left it out. The traps catch the common terminating signals and make sure everything gets cleaned up. The alias is what does the real magic. I like to check everything for failure. So in general I call programs in an \"if !\" type statement. By subtracting 1 from the line number the alias will tell me where the failure occurred. It is also dead simple to call, and pretty much idiot proof. Below is an example (just replace /bin/false with whatever you are going to call). #This is an example useage, it will print out #Error prog-name (@1): Who knew false is false. if ! /bin/false ; then die \"Who knew false is false.\" fi\n\nNot sure if this will be helpful to you, but I modified some of the suggested functions here in order to include the check for the error (exit code from prior command) within it. On each \"check\" I also pass as a parameter the \"message\" of what the error is for logging purposes. Now to call it within the same script (or in another one if I use ) I simply write the name of the function and pass a message as parameter, like this: #!/bin/bash cd /home/myuser/afolder error_exit \"Unable to switch to folder\" rm * error_exit \"Unable to delete all files\" Using this I was able to create a really robust bash file for some automated process and it will stop in case of errors and notify me ( will do that)\n\nSometimes , , and not work properly because they attempt to add automatic error detection to the shell. This does not work well in practice. In my opinion, instead of using and other stuffs, you should write your own error checking code. If you wise to use , be aware of potential gotchas. To avoid Error while running the code you can use or \n\n in Linux is a null device file. This will discard anything written to it and will return EOF on reading. you can use this at end of the command For you can use or to achieve Similar behaviour use can use && like this if [[ Condition ]]; then # if true else # if false fi show output of the last command ,it return 1 or 0\n\nThis function has been serving me rather well recently: action () { # Test if the first parameter is non-zero # and return straight away if so if test $1 -ne 0 then return $1 fi # Discard the control parameter # and execute the rest shift 1 \"$@\" local status=$? # Test the exit status of the command run # and display an error message on failure if test ${status} -ne 0 then echo Command \\\"\"$@\"\\\" failed >&2 fi return ${status} } You call it by appending 0 or the last return value to the name of the command to run, so you can chain commands without having to check for error values. With this, this statement block: If any of the commands fail, the error code is simply passed to the end of the block. I find it useful when you don't want subsequent commands to execute if an earlier one failed, but you also don't want the script to exit straight away (for example, inside a loop).\n\nUsing trap is not always an option. For example, if you're writing some kind of re-usable function that needs error handling and that can be called from any script (after sourcing the file with helper functions), that function cannot assume anything about exit time of the outer script, which makes using traps very difficult. Another disadvantage of using traps is bad composability, as you risk overwriting previous trap that might be set earlier up in the caller chain. There is a little trick that can be used to do proper error handling without traps. As you may already know from other answers, doesn't work inside commands if you use operator after them, even if you run them in a subshell; e.g., this wouldn't work: #!/bin/sh # prints: # # --> outer # --> inner # ./so_1.sh: line 16: some_failed_command: command not found # <-- inner # <-- outer set -e outer() { echo '--> outer' (inner) || { exit_code=$? echo '--> cleanup' return $exit_code } echo '<-- outer' } inner() { set -e echo '--> inner' some_failed_command echo '<-- inner' } outer But operator is needed to prevent returning from the outer function before cleanup. The trick is to run the inner command in background, and then immediately wait for it. The builtin will return the exit code of the inner command, and now you're using after , not the inner function, so works properly inside the latter: #!/bin/sh # prints: # # --> outer # --> inner # ./so_2.sh: line 27: some_failed_command: command not found # --> cleanup set -e outer() { echo '--> outer' inner & wait $! || { exit_code=$? echo '--> cleanup' return $exit_code } echo '<-- outer' } inner() { set -e echo '--> inner' some_failed_command echo '<-- inner' } outer Here is the generic function that builds upon this idea. It should work in all POSIX-compatible shells if you remove keywords, i.e. replace all with just : # [CLEANUP=cleanup_cmd] run cmd [args...] # # `cmd` and `args...` A command to run and its arguments. # # `cleanup_cmd` A command that is called after cmd has exited, # and gets passed the same arguments as cmd. Additionally, the # following environment variables are available to that command: # # - `RUN_CMD` contains the `cmd` that was passed to `run`; # - `RUN_EXIT_CODE` contains the exit code of the command. # # If `cleanup_cmd` is set, `run` will return the exit code of that # command. Otherwise, it will return the exit code of `cmd`. # run() { local cmd=\"$1\"; shift local exit_code=0 local e_was_set=1; if ! is_shell_attribute_set e; then set -e e_was_set=0 fi \"$cmd\" \"$@\" & wait $! || { exit_code=$? } if [ \"$e_was_set\" = 0 ] && is_shell_attribute_set e; then set +e fi if [ -n \"$CLEANUP\" ]; then RUN_CMD=\"$cmd\" RUN_EXIT_CODE=\"$exit_code\" \"$CLEANUP\" \"$@\" return $? fi return $exit_code } is_shell_attribute_set() { # attribute, like \"x\" case \"$-\" in *\"$1\"*) return 0 ;; *) return 1 ;; esac } #!/bin/sh set -e # Source the file with the definition of `run` (previous code snippet). # Alternatively, you may paste that code directly here and comment the next line. . ./utils.sh main() { echo \"--> main: $@\" CLEANUP=cleanup run inner \"$@\" echo \"<-- main\" } inner() { echo \"--> inner: $@\" sleep 0.5; if [ \"$1\" = 'fail' ]; then oh_my_god_look_at_this fi echo \"<-- inner\" } cleanup() { echo \"--> cleanup: $@\" echo \" RUN_CMD = '$RUN_CMD'\" echo \" RUN_EXIT_CODE = $RUN_EXIT_CODE\" sleep 0.3 echo '<-- cleanup' return $RUN_EXIT_CODE } main \"$@\" $ ./so_3 fail; echo \"exit code: $?\" --> main: fail --> inner: fail ./so_3: line 15: oh_my_god_look_at_this: command not found --> cleanup: fail RUN_CMD = 'inner' RUN_EXIT_CODE = 127 <-- cleanup exit code: 127 $ ./so_3 pass; echo \"exit code: $?\" --> main: pass --> inner: pass <-- inner --> cleanup: pass RUN_CMD = 'inner' RUN_EXIT_CODE = 0 <-- cleanup <-- main exit code: 0 The only thing that you need to be aware of when using this method is that all modifications of Shell variables done from the command you pass to will not propagate to the calling function, because the command runs in a subshell."
    },
    {
        "link": "https://redhat.com/it/blog/error-handling-bash-scripting",
        "document": "Scripting is one of the key tools for a sysadmin to manage a set of day-to-day activities such as running backups, adding users/groups, installing/updating packages, etc. While writing a script, error handling is one of the crucial things to manage.\n\nThis article shows some basic/intermediate techniques of dealing with error handling in Bash scripting. I discuss how to obtain the error codes, get verbose output while executing the script, deal with the debug function, and standard error redirection. Using these techniques, sysadmins can make their daily work easy.\n\nIn Bash scripting, prints the exit status. If it returns zero, it means there is no error. If it is non-zero, then you can conclude the earlier task has some issue.\n\nA basic example is as follows:\n\nIf you run the above script once, it will print because the directory does not exist, therefore the script will create it. Naturally, you will get a non-zero value if you run the script a second time, as seen below:\n\nIt is always recommended to enable the debug mode by adding the option to your shell script as below:\n\nYou can write a debug function as below, which helps to call it anytime, using the example below:\n\nYou can redirect all the system errors to a custom file using standard errors, which can be denoted by the number 2. Execute it in normal Bash commands, as demonstrated below:\n\nMost of the time, it is difficult to find the exact line number in scripts. To print the line number with the error, use the PS4 option (supported with Bash 4.1 or later). Example below:\n\nYou can easily see the line number while reading the errors:\n\n[ Get this free ebook: Managing your Kubernetes clusters for dummies. ]\n\nManaging errors is a key skill for administrators when writing scripts. These tips should help make your life easier when troubleshooting Bash scripts or even general commands."
    }
]