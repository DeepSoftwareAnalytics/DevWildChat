[
    {
        "link": "https://docs.python-telegram-bot.org/en/v13.13",
        "document": "If you’re just starting out with the library, we recommend following our “Your first Bot” tutorial that you can find on our wiki. On our wiki you will also find guides like how to use handlers, webhooks, emoji, proxies and much more.\n\nA great way to learn is by looking at examples. Ours can be found in our examples folder on Github."
    },
    {
        "link": "https://docs.python-telegram-bot.org/_/downloads/en/v13.13/pdf",
        "document": ""
    },
    {
        "link": "https://python-telegram-bot.readthedocs.io/_/downloads/en/v13.2/pdf",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/78432185/how-to-correctly-filter-messages-using-messagehandler-from-telegram-ext",
        "document": "Im doing my bot using Python3 and telegram-python-bot v.21.0.1 library using modules telegram and telegram.ext. Now Im facing problem with creating custom filter for filter out messages from admin\\owner of this chat.\n\nHere is my code:\n\nAs far as I understand, to create your own filter, you need to create your own subclass from BaseFilter and implement the filter method, which will return a bool. In the example above, everything is simple, it always returns false, just to make sure it works. Then the MessageHandler gets the message and logically should pass it into filters, in this case, into the initialized NotPrivilegedUserFilter as the variable my_filter. However, in runtime, this does not work, there are no errors or exceptions, and my handle_message callback, which processes the message (writes the status of the member who wrote in the chat), is always called.\n\nActually, the question is - what am I doing wrong? My task is to filter out messages sent by members with the status of ChatMemberStatus.ADMINISTRATOR, ChatMemberStatus.OWNER.\n\nI've read the documentation, and it seems to be described exactly as I do, plus I've read the docstrings in the library in the BaseFilter and MessageHandler classes. I asked ChatGPT, but it couldn't come up with anything either; I tried all the options he suggested. Also, I debugged with prints (lol) the class NotPrivilegedUserFilter to make sure it gets called during the operation of MessageHandler, but that did not happen. I also looked at code examples on the python-telegram-bot project's GitHub and found no examples of creating custom filters.\n\nI looked at what options are offered on Stack Overflow... Well, you can of course create a function that returns a list of chat admin ids and other privileged users and simply filter by checking the id against the list, but the question is how to write your own filters. It used to be possible to do so, maybe something has changed, or I am misunderstanding something."
    },
    {
        "link": "https://python-telegram-bot.readthedocs.io/_/downloads/en/v13.7/pdf",
        "document": ""
    },
    {
        "link": "https://realpython.com/image-processing-with-the-python-pillow-library",
        "document": "Python Pillow allows you to manipulate images and perform basic image processing tasks. As a fork of the Python Imaging Library (PIL), Pillow supports image formats like JPEG, PNG, and more, enabling you to read, edit, and save images. With Python Pillow, you can crop, resize, rotate, and apply filters to images, making it a versatile tool for image manipulation.\n\nPillow is often used for high-level image processing tasks and exploratory work. While not the fastest library, it offers a gentle learning curve and a comprehensive set of features for basic to intermediate image processing needs. You can enhance its capabilities by integrating it with NumPy for pixel-level manipulations and creating animations.\n\nBy the end of this tutorial, you’ll understand that:\n• Python Pillow is used for image manipulation and basic image processing.\n• Pillow offers reasonable speed for its intended use cases.\n• PIL is the original library, while Pillow is its actively maintained fork.\n• You read an image in Python Pillow using from the PIL module.\n• Pillow is used for its ease of use, versatility, and integration with NumPy.\n\nWith these insights, you’re ready to dive into the world of image processing with Python Pillow. You’ll use several images in this tutorial, which you can download from the tutorial’s image repository:\n\nWith these images in hand, you’re now ready to get started with Pillow.\n\nThe Python Pillow library is a fork of an older library called PIL. PIL stands for Python Imaging Library, and it’s the original library that enabled Python to deal with images. PIL was discontinued in 2011 and only supports Python 2. To use its developers’ own description, Pillow is the friendly PIL fork that kept the library alive and includes support for Python 3. There’s more than one module in Python to deal with images and perform image processing. If you want to deal with images directly by manipulating their pixels, then you can use NumPy and SciPy. Other popular libraries for image processing are OpenCV, scikit-image, and Mahotas. Some of these libraries are faster and more powerful than Pillow. However, Pillow remains an important tool for dealing with images. It provides image processing features that are similar to ones found in image processing software such as Photoshop. Pillow is often the preferred option for high-level image processing tasks that don’t require more advanced image processing expertise. It’s also often used for exploratory work when dealing with images. Pillow also has the advantage of being widely used by the Python community, and it doesn’t have the same steep learning curve as some of the other image processing libraries. You’ll need to install the library before you can use it. You can install Pillow using within a virtual environment: Now that you’ve installed the package, you’re ready to start familiarizing yourself with the Python Pillow library and perform basic manipulations of images. The Module and Class in Pillow The main class defined in Pillow is the class. When you read an image using Pillow, the image is stored in an object of type . For the code in this section, you’ll need the image file named (image credit), which you can find in the image repository for this tutorial: Get Images: Click here to get access to the images that you’ll manipulate and process with Pillow. You can place this image file in the project folder that you’re working in. When exploring images with Pillow, it’s best to use an interactive REPL environment. You’ll start by opening the image that you just downloaded: You might expect to import from Pillow instead of from PIL. You did install , after all, not . However, Pillow is a fork of the PIL library. Therefore, you’ll still need to use when importing into your code. You call the function to read the image from the file and to read the image into memory so that the file can now be closed. You use a statement to create a context manager to ensure the file is closed as soon as it’s no longer needed. In this example, the object is a JPEG image-specific type that’s a subclass of the class, as you confirm with the call to . Note that both the class and the module where the class is defined share the same name, . You can display the image using : The method saves the image as a temporary file and displays it using your operating system’s native software for dealing with images. When you run the code above, you’ll see the following image displayed: On some systems, calling will block the REPL until you close the image. This depends on the operating system and the default image viewing software that you’re using. You’ll need to be familiar with three key properties when dealing with images in the Python Pillow library. You can explore these using the class attributes , , and : The format of an image shows what type of image you’re dealing with. In this case, the format of the image is . The size shows the width and height of the image in pixels. The mode of this image is . You’ll learn more about modes shortly. Often, you may need to crop and resize images. The class has two methods that you can use to perform these operations, and : The argument to must be a 4-tuple that defines the left, upper, right, and bottom edges of the region that you wish to crop. The coordinate system used in Pillow assigns the coordinates (0, 0) to the pixel in the upper-left corner. This is the same coordinate system that’s usually used for two-dimensional arrays. The 4-tuple represents the following section of the image: The new image that returns in the code above has a size of pixels. The cropped image shows only one of the buildings from the original picture: In the code above, you also change the resolution of the cropped image using , which needs a tuple as a required argument. The tuple that you use as an argument defines the new width and height of the image in pixels. In the example above, you’re setting the new width and height to a quarter of their original values using the floor division operator ( ) and the attributes and . The final call to displays the cropped and resized image: There are additional optional parameters that you can use with to control how the image is resampled. Alternatively, you can achieve similar scaling using : The argument determines the factor by which you scale the image down. If you prefer to set a maximum size rather than a scaling factor, then you can use . The size of the thumbnail will be smaller than or equal to the size that you set. Note: The method changes the object in place and doesn’t return a new object. However, , , and all return a new object. Not all methods in the Pillow library behave in the same way. Once you’re happy with your returned image, you can save any of the objects to file using : Once you call the method, it creates the image files in your project folder. In this example, one of the images is a JPEG image and the other is a PNG image. The extension that you use as a filname automatically determines the file format, or you can specify the format as an additional optional argument. You can manipulate the image beyond cropping and resizing. Another common requirement is to rotate or flip the image. You can use the method for some transformations. Go ahead and carry on with the same REPL session that you started in the previous section: This code displays the following image: There are seven options that you can pass as arguments to :\n• : Flips the image left to right, resulting in a mirror image\n• : Rotates the image by 270 degrees counterclockwise, which is the same as 90 degrees clockwise\n• : Transposes the rows and columns using the top-left pixel as the origin, with the top-left pixel being the same in the transposed image as in the original image\n• : Transposes the rows and columns using the bottom-left pixel as the origin, with the bottom-left pixel being the one that remains fixed between the original and modified versions All the rotation options above define rotations in steps of 90 degrees. If you need to rotate an image by another angle, then you can use : This method call rotates the image by 45 degrees counterclockwise, giving the following image: The object returned is the same size as the original . Therefore, the corners of the image are missing in this display. You can change this behavior using the named parameter: This method returns a larger image that fully contains the rotated image: You can customize the rotation further with additional optional parameters. You can now change the size and orientation of an image. In the next section, you’ll learn about different types of images in the Python Pillow library. Bands and Modes of an Image in the Python Pillow Library An image is a two-dimensional array of pixels, where each pixel corresponds to a color. Each pixel can be represented by one or more values. For example, in an RGB image, each pixel is represented by three values corresponding to the red, green, and blue values for that pixel. Therefore, the object for an RBG image contains three bands, one for each color. An RGB image of size pixels is represented by a array of values. RGBA images also include the alpha value, which contains information about the transparency for each pixel. An RGBA image has four bands, one for each of the colors and a fourth one containing the alpha values. Each band has the same dimensions as the image dimensions. Therefore, an RGBA image of size pixels is represented by a array of values. The mode of an image describes what type of image you’re working with. Pillow supports most standard modes, including black-and-white (binary), grayscale, RGB, RGBA, and CMYK. You can see the full list of supported modes in the Pillow documentation on modes. You can find out how many bands are in an object using the method, and you can convert between modes using . Now you’ll use the image named (image credit) from the image repository for this tutorial: This image’s mode is also RGB. You can convert this image into other modes. This code uses the same REPL session that you started in the previous sections: You call twice to convert the RGB image into a CMYK and a grayscale version. The CMYK image looks similar to the original image but is encoded using the mode that’s common for printed material rather than digital displays. The conversion to grayscale gives the following output: The outputs from the calls to confirm that there are three bands in the RGB image, four bands in the CMYK image, and one band in the grayscale image. You can separate an image into its bands using and combine separate bands back into an object using . When you use , the method returns all the bands as separate objects. You can confirm this by displaying the string representation of one of the objects returned: The mode of the object that returns is , indicating this is a grayscale image, or an image that only displays the luminance values of each pixel. Now, you can create three new RGB images showing the red, green, and blue channels separately using , which is a function in the module: The first argument in determines the mode of the image that you want to create. The second argument contains the individual bands that you want to merge into a single image. The red band alone, stored in the variable , is a grayscale image with mode L. To create the image showing only the red channel, you merge the red band from the original image with green and blue bands that only contain zeros. To create a band containing zeros everywhere, you use the method. This method needs a function as an argument. The function that you use determines how each point transforms. In this case, you use a function to map each point to . When you merge the red band with green and blue bands containing zeros, you get an RGB image called . Therefore, the RGB image that you create only has non-zero values in the red channel, but because it’s still an RGB image, it’ll display in color. You also repeat a similar process to obtain and , which contain RGB images with the green and blue channels from the original image. The code displays the following three images: The red image contains a strong signal in the pixels that represent the strawberry, because these pixels are mostly red. The green and blue channels show these pixels as dark because they have small values. The exceptions are those pixels that represent the reflection of the light on the surface of the strawberry as these pixels are nearly white. Creating the side-by-side displays shown in this tutorialShow/Hide In this tutorial, when there are several images output in the code that need to be displayed next to one another to make comparisons easier, the images are displayed side by side rather than as separate images. These side-by-side displays were created using Pillow itself. You can use the function , shown below, to merge several images into a single display: The first parameter in uses the unpacking operator ( ) so that any number of objects of type can be used as input arguments. The keyword parameter can be set to if you want to tile the images vertically rather than horizontally. This function assumes that all images have the same size. The overall size of the display is calculated from the size of the images and the number of images used. You then create a new object with the same mode as the original images and with the size of the overal display. The loop pastes the images that you input when you call the function into the final display. The function returns the final object containing all the images side by side. The image in the main article showing the three color channels for the strawberry image was obtained by calling the function as follows: This function was used to generate all the displays that show more than one image in this tutorial.\n\nYou’ve learned how to crop and rotate images, resize them, and extract color bands from color images. However, none of the actions that you’ve taken so far have made any changes to the content of the image. In this section, you’ll learn about image processing features in the Python Pillow library. You’ll use the module in Pillow. One of the methods that’s used in image processing is image convolution using kernels. The aim of this tutorial is not to give a detailed explanation of image processing theory. If you’re interested in the science of image processing, one of the best resources that you can use is Digital Image Processing by Gonzalez and Woods. In this section, you’ll learn the basics of how you can use convolution kernels to perform image processing. But what’s a convolution kernel? A kernel is a matrix: You can consider a simple image to understand the process of convolution using kernels. The image has a size of pixels and contains a vertical line and a dot. The line is four pixels wide, and the dot consists of a pixel square. The image below is enlarged for display purposes: You can place the kernel anywhere on the image and use the location of the kernel’s central cell as a reference. The diagram below is a representation of the top-left portion of the image: The elements in this diagram represent different aspects of the image and the kernel:\n• The white squares represent pixels in the image that have a value of .\n• The red squares represent pixels in the image that have a value of . These make up the dot in the image shown above.\n• Each purple region represents the kernel. This kernel consists of a region, and each cell in the kernel has a value of . The diagram shows the kernel in three different positions labeled 1, 2, and 3. A new image can be created as a result of the convolution of the image with the kernel. You can understand the convolution process through the following steps:\n• Locate kernel: Consider one of the kernel locations and look at the image pixels covered by the kernel’s nine cells.\n• Multiply kernel and pixel values: Multiply the values in each of the kernel’s cells with the corresponding pixel values in the image. You’ll have nine values from the nine multiplications.\n• Sum results of multiplications: Add those nine values together. The result will be the value of the pixel in the new image that has the same coordinates as the kernel’s center pixel.\n• Repeat for all pixels: Repeat the process for every pixel in the image, moving the kernel each time so that the kernel’s central cell corresponds to a different image pixel each time. You can see this process with the three kernel positions labeled 1, 2, and 3 in diagram above. Consider the kernel position labeled 1. The position of this kernel is , which is the position of its central cell because it’s in the fourth row (index = ) and the third column (index = ). Each image pixel in the region covered by the kernel has a value of zero. Therefore, all the multiplications from step 2 will be zero, and their addition will also be zero. The new image will have a value of zero at pixel . The scenario is different for the other kernel positions shown. Next, consider the kernel labeled 2, located at . One of the image pixels overlapping this is not zero. The multiplication of this pixel value with the kernel value will give . The eight remaining multiplications are still zero because the image pixels are zero. Therefore, the value of the pixel at position in the new image will be . The third kernel position illustrated above is at . There are four non-zero image pixels overlapping with this kernel. Each one has a value of , so the multiplication result will again be for each of those pixel positions. The overall result for this kernel position is . The new image will have this value at . The diagram and the discussion above only consider three kernel positions. The convolution process repeats this process for every possible kernel position in the image. This gives a value for each pixel position in the new image. The result of the convolution is shown on the right in the following image, with the original image on the left: The kernel that you used is a box blur kernel. The factor of is there so that the overall weighting of the kernel is . The result of the convolution is a blurred version of the original image. There are other kernels that perform different functions, including different blurring methods, edge detection, sharpening, and more. The Python Pillow library has several built-in kernels and functions that’ll perform the convolution described above. You don’t need to understand the math of filtering through convolution to use these filters, but it always helps to know what’s happening behind the scenes when using these tools. The next sections will look at the kernels and image filtering capabilities available in the module in Pillow. You’ll return to using the image of the buildings that you used at the beginning of this tutorial. You can start a new REPL session for this section: In addition to , you also import the module from Pillow. You can use the method to apply filtering to the image. This method needs a convolution kernel as its argument, and you can use one of the several kernels available in the module in Pillow. The first set of filters that you’ll learn about deal with blurring, sharpening, and smoothing an image. You can blur the image using the predefined filter: The displayed image is a blurred version of the original one. You can zoom in to observe the difference in more detail using and then display the images again using : The two cropped images show the difference between the two versions: You can customize the type and amount of blurring that you need using or : You can see the three blurred images below, shown in the same order as in the code above: The filter is similar to the one described in the previous section introducing convolution kernels. The argument is the radius of the box blur filter. In the earlier section discussing kernels, the box blur filter that you used was a filter. This means that it had a radius of , because the filter extends by one pixel from the center. The blurred images show that the box blur filter with a radius of produces an image that’s more blurred than the image generated by the box blur filter with radius . You can also use the filter, which uses a Gaussian blur kernel. The Gaussian kernel puts more weight on the pixels at the center of the kernel than those at the edges, and this leads to smoother blurring than what’s obtained with the box blur. For this reason, Gaussian blurring can give better results in many cases. What if you want to sharpen an image? In that case, you can use the filter and compare the result with the original image: You’re comparing a cropped version of both images showing a small portion of the building. The sharpened image is on the right: Perhaps instead of sharpening an image, you need to smooth it. You can achieve this by passing as an argument for : Below, you can see the original image on the left and the smoothed image on the right: You’ll see an application of the smooth filter in the next section, in which you’ll learn about more filters in the module. These filters act on the edges of objects in the image. When you look at an image, it’s relatively easy to determine the edges of objects within that image. It’s also possible for an algorithm to detect edges automatically using edge detection kernels. The module in Pillow has a predefined kernel to achieve this. In this section, you’ll use the image of the buildings again and convert it to grayscale before you apply the edge detection filter. You can carry on with the REPL session from the previous section: The result is an image showing the edges from the original image: This filter identifies the edges in the image. You can obtain a better outcome by applying the filter before finding the edges: You can see a comparison of the original grayscale image and the two edge detection results below. The version with smoothing before edge detection is shown at the bottom: You can also enhance the edges of the original image with the filter: You used the smoothed version of the grayscale image to enhance the edges. A portion of the original grayscale image and the image with the edges enhanced are shown side by side below. The image with edge enhancement is on the right: Another predefined filter in that deals with object edges is . You can pass it as an argument to as you did with the other filters in this section: You’re using the smoothed, grayscale version as a starting point for this filter. You can see the embossed image below, which shows a different effect using the edges in the image: In this section, you’ve learned about several filters available in the module that you can apply to images. There are other filters that you can use to process images. You can see a list of all the filters available in the documentation.\n\nImage Segmentation and Superimposition: An Example In this section, you’ll use the image files named (image credit) and (image credit), which you can find in the image repository for this tutorial: Get Images: Click here to get access to the images that you’ll manipulate and process with Pillow. You can use the Python Pillow library to extract the cat from the first image and place it on the floor of the monastery courtyard. You’ll use a number of image processing techniques to achieve this. You’ll start by working on . You’ll need to remove the picture of the cat from the background using image segmentation techniques. In this example, you’ll segment the image using thresholding techniques. First, you can crop the image to a smaller one to remove some of the background. You can start a new REPL session for this project: The cropped image contains the cat and some of the background that’s too close to the cat for you to crop it: Each pixel in a color image is represented digitally by three numbers corresponding to the red, green, and blue values of that pixel. Thresholding is the process of converting all the pixels to either the maximum or minimum value depending on whether they’re higher or lower than a certain number. It’s easier to do this on a grayscale image: You achieve thresholding by calling to convert each pixel in the grayscale image into either or . The conversion depends on whether the value in the grayscale image is greater or smaller than the threshold value. The threshold value in this example is . The figure below shows the grayscale image and the result from the thresholding process: In this example, all the points in the grayscale image that had a pixel value greater than are converted to white, and all other pixels are changed to black. You can change the sensitivity of the thresholding process by varying the threshold value. Thresholding can be used to segment images when the object to segment is distinct from the background. You can achieve better results with versions of the original image that have higher contrast. In this example, you can achieve higher contrast by thresholding the blue channel of the original image rather than the grayscale image, because the dominant colors in the background are brown and green colors, which have a weak blue component. You can extract the red, green, and blue channels from the color image as you did earlier: The red, green, and blue channels are shown below, from left to right. All three are displayed as grayscale images: The blue channel has a higher contrast between the pixels representing the cat and those representing the background. You can use the blue channel image to threshold: You use a threshold value of in this example. You also convert the image into a binary mode using as an argument to . The pixels in a binary image can only have the values of or . Note: When dealing with certain image formats, such as JPEG, that rely on lossy compression, the images may vary slightly depending on which JPEG decoders you’re using. Different operating systems often come with different default JPEG decoders. Therefore, the results that you get when processing images may vary depending on the operating system and JPEG decoder that you’re using. You may need to slightly adjust the threshold value if your results do not match the ones shown in this tutorial. The result of thresholding is the following: You can identify the cat in this black-and-white image. However, you’d like to have an image in which all the pixels that correspond to the cat are white and all other pixels are black. In this image, you still have black regions in the area which corresponds to the cat, such as where the eyes, nose and mouth are, and you also still have white pixels elsewhere in the image. You can use the image processing techniques called erosion and dilation to create a better mask that represents the cat. You’ll learn about these two techniques in the next section. You can look at the image file called , which you can download from the repository linked to this tutorial: The left-hand side of this binary image shows a white dot on a black background, while the right-hand side shows a black hole in a solid white section. Erosion is the process of removing white pixels from the boundaries in an image. You can achieve this in a binary image by using as an argument for the method. This filter replaces the value of a pixel with the minimum value of the nine pixels in the array centered around the pixel. In a binary image, this means that a pixel will have the value of zero if any of its neighboring pixels are zero. You can see the effect of erosion by applying several times to the image. You should continue with the same REPL session as in the previous section: You’ve applied the filter three times using a loop. This code gives the following output: The dot has shrunk but the hole has grown as a result of erosion. Dilation is the opposite process to erosion. White pixels are added to the boundaries in a binary image. You can achieve dilation by using , which converts a pixel to white if any of its neighbors are white. You can apply dilation to the same image containing a dot and a hole, which you can open and load again: The dot has now grown bigger, and the hole has shrunk: You can use erosion and dilation together to fill in holes and remove small objects from a binary image. Using the image with a dot and hole, you can perform ten erosion cycles to remove the dot, followed by ten dilation cycles to restore the hole to its original size: You perform ten erosion cycles with the first loop. The image at this stage is the following: The dot has disappeared, and the hole is larger than it was in the original image. The second loop performs ten dilation cycles, which return the hole to its original size: However, the dot is no longer present in the image. The erosions and dilations have modified the image to keep the hole but remove the dot. The number of erosions and dilations needed depends on the image and what you want to achieve. Often, you’ll need to find the right combination through trial and error. You can define functions to perform several cycles of erosion and dilation: These functions make it easier to experiment with erosion and dilation for an image. You’ll use these functions in the next section as you continue working on placing the cat into the monastery. You can use a sequence of erosions and dilations on the threshold image that you obtained earlier to remove parts of the mask that don’t represent the cat and to fill in any gaps in the region containing the cat. Once you’ve experimented with erosion and dilation, you’ll be able to use educated guesses in a trial-and-error process to find the best combination of erosions and dilations to achieve the ideal mask. Starting with the image , which you obtained earlier, you can start with a series of erosions to remove the white pixels that represent the background in the original image. You should continue working in the same REPL session as in the previous sections: The eroded threshold image no longer contains white pixels representing the background of the image: However, the remaining mask is smaller than the overall outline of the cat and has holes and gaps within it. You can perform dilations to fill the gaps: The fifty-eight cycles of dilation filled all the holes in the mask to give the following image: However, this mask is too big. You can therefore finish the process with a series of erosions: The result is a mask that you can use to segment the image of the cat: You can avoid the sharp edges of a binary mask by blurring this mask. You’ll have to convert it from a binary image into a grayscale image first: The filter returns the following mask: The mask now looks like a cat! Now you’re ready to extract the image of the cat from its background: First, you create a blank image with the same size as . You create a new object from by using and setting all values to zero. Next, you use the function in to create an image made up from both and using to determine which parts of each image are used. The composite image is shown below: You’ve segmented the image of the cat and extracted the cat from its background. You can go a step further and paste the segmented image of the cat into the image of the monastery courtyard from the image repository for this tutorial: You’ve used to paste an image onto another one. This method can be used with three arguments:\n• The first argument is the image that you want to paste in. You’re resizing the image to one-fifth of its size using the integer division operator ( ).\n• The second argument is the location in the main image where you want to paste the second picture. The tuple includes the coordinates within the main image where you want to place the top-left corner of the image that you’re pasting in.\n• The third argument provides the mask that you wish to use if you don’t want to paste the entire image. You’ve used the mask that you obtained from the process of thresholding, erosion, and dilation to paste the cat without its background. The output is the following image: You’ve segmented the cat from one image and placed it into another image to show the cat sitting quietly in the monastery courtyard rather than in the field where it was sitting in the original image. Your final task in this example is to add the Real Python logo as a watermark to the image. You can get the image file with the Real Python logo from the repository accompanying this tutorial: Get Images: Click here to get access to the images that you’ll manipulate and process with Pillow. You should continue working in the same REPL session: This is the full-size logo in color: You can change the image to grayscale and threshold it using to transform it into a black-and-white image. You also reduce its size and transform it into a contour image: The output shows the contour from the Real Python logo. The contour is ideal for using as a watermark on your image: To use this as a watermark, you’ll need to reverse the colors so that the background is black and only the outline that you want to keep is white. You can achieve this using again: You’ve converted the pixels that had a value of and assigned them the value , converting them from white to black pixels. You set the remaining pixels to white. The reversed outline logo is shown below: Your final step is to paste this outline onto the image of the cat sitting in the monastery courtyard. You can use again: The first argument in indicates the image that you wish to paste in, and the third argument represents the mask. In this case, you’re using the same image as a mask because the image is a binary image. The second argument provides the top-left coordinates of the region where you want to paste the image. The watermark has a rectangular outline, which is a result of the contour filter that you used earlier. If you prefer to remove this outline, you can crop the image using . This is an exercise that you can try on your own.\n\nPillow has an extensive selection of built-in functions and filters. However, there are times when you need to go further and manipulate images beyond the features that are already available in Pillow. You can manipulate the image further with the help of NumPy. NumPy is a very popular Python library for dealing with numeric arrays, and it’s an ideal tool to use with Pillow. You can learn more about NumPy in NumPy Tutorial: Your First Steps Into Data Science in Python. When you convert an image into a NumPy array, you can perform any transformations that you require directly on the pixels in the array. Once you’ve completed your processing in NumPy, you can convert the array back into an object using Pillow. You need to install NumPy for this section: Now that you’ve installed NumPy, you’re ready to use Pillow and NumPy to spot the difference between two images. Using NumPy to Subtract Images From Each Other See if you can spot the differences between the following two images: This isn’t a hard one! However, you decide to cheat and write a Python program to solve the puzzle for you. You can download the image files and (image credit) from the repository accompanying this tutorial: Get Images: Click here to get access to the images that you’ll manipulate and process with Pillow. Your first step is to read the images using Pillow and convert them to NumPy arrays: Since and are objects of type , you can manipulate them using all the tools that you have available in NumPy. You can subtract one array from the other to show the pixels that differ between the two images: When you subtract an array from another one of the same size, the result is another array with the same shape as the original arrays. You can convert this array into an image using in Pillow: The result of subtracting one NumPy array from another and converting into a Pillow is the difference image shown below: The difference image only shows three regions from the original image. These regions highlight the differences between the two images. You can also see some noise surrounding the cloud and the fence, which is due to small changes in the original JPEG compression in the region surrounding these items. You can go further and create images from scratch using NumPy and Pillow. You can start by creating a grayscale image. In this example, you’ll create a simple image containing a square, but you can create more elaborate images in the same way: You create an array of size containing zeros everywhere. Next, you set the value of a set of pixels at the center of the array to . You can index NumPy arrays using both rows and columns. In this example, the first slice, , represents the rows to . The second slice, , which follows the comma, represents the columns to . You can use to convert the NumPy array into an object of type . The output from the code above is shown below: You’ve created a grayscale image containing a square. The mode of the image is inferred automatically when you use . In this case, mode is used, which corresponds to an image with 32-bit floating-point pixels. You can convert this to a simpler grayscale image with 8-bit pixels if you wish: You can also go further and create a color image. You can repeat the process above to create three images, one corresponding to the red channel, another to the green, and a final one corresponding to the blue channel: You create an object from each NumPy array and convert the images to mode , which represents grayscale. Now, you can combine these three separate images into one RGB image using : The first argument in is the mode of the image output. The second argument is a sequence with the individual single-band images. This code creates the following image: You’ve combined the separate bands into an RGB color image. In the next section, you’ll go a step further and create a GIF animation using NumPy and Pillow. In the previous section, you created a color image containing three overlapping squares of different colors. In this section, you’ll create an animation showing those three squares merging into a single white square. You’ll create several versions of the images containing three squares, and the location of the squares will vary slightly between successive images: You create an empty list called , which you’ll use to store the various images that you generate. Within the loop, you create NumPy arrays for the red, green, and blue channels, as you did in the previous section. The array containing the green layer is always the same and represents a square in the center of the image. The red square starts in a position displaced to the top-left of the center. In each successive frame, the red square moves closer to the center until it reaches the center in the final iteration of the loop. The blue square is initially shifted toward the bottom-right then moves towards the center with each iteration. Note that in this example, you’re iterating over , which means that the variable increases in steps of two. You learned earlier that you can save an object to file using . You can use the same function to save to a GIF file that includes a sequence of images. You call on the first image in the sequence, which is the first image that you stored in the list : The first argument in is the filename for the file that you want to save. The extension in the filename tells what file format it needs to output. You also include two keyword arguments in :\n• ensures that all the images in the sequence are saved, and not just the first one.\n• allows you to append the remaining images in the sequence to the GIF file. This code saves to file, and you can then open the GIF file with any image software. The GIF should loop by default, but on some systems you’ll need to add the keyword argument to to make sure the GIF loops. The animation that you get is the following one: The three squares with different colors merge into a single white square. Can you create your own animation using different shapes and different colors?"
    },
    {
        "link": "https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html",
        "document": "The Python Imaging Library supports a wide variety of raster file formats. Over 30 different file formats can be identified and read by the library. Write support is less extensive, but most common interchange and presentation formats are supported.\n\nThe function identifies files from their contents, not their names, but the method looks at the name to determine which format to use, unless the format is given explicitly.\n\nWhen an image is opened from a file, only that instance of the image is considered to have the format. Copies of the image will contain data loaded from the file, but not the file itself, meaning that it can no longer be considered to be in the original format. So if is called on an image, or another method internally creates a copy of the image, then any methods or attributes specific to the format will no longer be present. The (file pointer) attribute will no longer be present, and the attribute will be .\n\nBLP is the Blizzard Mipmap Format, a texture format used in World of Warcraft. Pillow supports reading Compressed or raw images, and all types of images. Pillow supports writing BLP images. The method can take the following keyword arguments: If present and set to “BLP1”, images will be saved as BLP1. Otherwise, images will be saved as BLP2. Pillow reads and writes Windows and OS/2 BMP files containing , , , or data. 16-colour images are read as images. Support for reading 8-bit run-length encoding was added in Pillow 9.1.0. Support for reading 4-bit run-length encoding was added in Pillow 9.3.0. The method sets the following properties: Set to 1 if the file is a 256-color run-length encoded image. Set to 2 if the file is a 16-color run-length encoded image. DDS is a popular container texture format used in video games and natively supported by DirectX. DXT1 and DXT5 pixel formats can be read, only in mode. Added in version 3.4.0: DXT3 images can be read in mode and DX10 images can be read in and mode. Added in version 6.0.0: Uncompressed images can be read. Added in version 8.3.0: BC5S images can be opened in mode, and uncompressed images can be read. Uncompressed data can also be saved to image files. Added in version 9.3.0: ATI1 images can be opened in mode and ATI2 images can be opened in mode. Added in version 9.4.0: Uncompressed (“luminance”) and images can be opened and saved. Added in version 10.1.0: BC5U can be read in mode, and 8-bit color indexed images can be read in mode. Pillow reads and writes DIB files. DIB files are similar to BMP files, so see above for more information. Pillow identifies EPS files containing image data, and can read files that contain embedded raster images (ImageData descriptors). If Ghostscript is available, other EPS files can be read as well. The EPS driver can also write EPS images. The EPS driver can read EPS images in , , and mode, but Ghostscript may convert the images to mode rather than leaving them in the original color space. The EPS driver can write images in , and modes. To use Ghostscript, Pillow searches for the “gs” executable. On Windows, it also searches for “gswin32c” and “gswin64c”. To customise this behaviour, will set the name of the executable to use. will prevent Ghostscript use. If Ghostscript is available, you can call the method with the following parameters to affect how Ghostscript renders the EPS. Affects the scale of the resultant rasterized image. If the EPS suggests that the image be rendered at 100px x 100px, setting this parameter to 2 will make the Ghostscript render a 200px x 200px image instead. The relative position of the bounding box is maintained: If true, generates an RGBA image with a transparent background, instead of the default behaviour of an RGB image with a white background. Pillow reads GIF87a and GIF89a versions of the GIF file format. The library writes files in GIF87a by default, unless GIF89a features are used or GIF89a is already in use. Files are written with LZW encoding. GIF files are initially read as grayscale ( ) or palette mode ( ) images. Seeking to later frames in a image will change the image to (or if the first frame had transparency). mode images are changed to because each frame of a GIF may contain its own individual palette of up to 256 colors. When a new frame is placed onto a previous frame, those colors may combine to exceed the mode limit of 256 colors. Instead, the image is converted to handle this. If you would prefer the first image frame to be as well, so that every frame is converted to or mode, there is a setting available: GIF frames do not always contain individual palettes however. If there is only a global palette, then all of the colors can fit within mode. If you would prefer the frames to be kept as in that case, there is also a setting available: To restore the default behavior, where mode images are only converted to or after the first frame: The method sets the following properties: Transparency color index. This key is omitted if the image is not transparent. May not be present. The time to display the current frame of the GIF, in milliseconds. May not be present. The number of times the GIF should loop. 0 means that it will loop forever. May not be present. A comment about the image. This is the last comment found before the current frame’s image. May not be present. Contains application specific information. The GIF loader supports the and methods. You can combine these methods to seek to the next frame ( ). raises an if you try to seek after the last frame. When calling to write a GIF file, the following options are available: If present and true, all frames of the image will be saved. If not, then only the first frame of a multiframe image will be saved. A list of images to append as additional frames. Each of the images in the list can be single or multiframe images. This is currently supported for GIF, PDF, PNG, TIFF, and WebP. It is also supported for ICO and ICNS. If images are passed in of relevant sizes, they will be used instead of scaling down the main image. Whether or not to include local color table. Whether or not the image is interlaced. By default, it is, unless the image is less than 16 pixels in width or height. Indicates the way in which the graphic is to be treated after being displayed. Pass a single integer for a constant disposal, or a list or tuple to set the disposal for each frame separately. Use the specified palette for the saved image. The palette should be a bytes or bytearray object containing the palette entries in RGBRGB… form. It should be no more than 768 bytes. Alternately, the palette can be passed in as an object. Whether to attempt to compress the palette by eliminating unused colors (this is only useful if the palette can be compressed to the next smaller power of 2 elements) and whether to mark all pixels that are not new in the next frame as transparent. This is attempted by default, unless a palette is specified as an option or as part of the first image’s dictionary. Note that if the image you are saving comes from an existing GIF, it may have the following properties in its dictionary. For these options, if you do not pass them in, they will default to their values. The display duration of each frame of the multiframe gif, in milliseconds. Pass a single integer for a constant duration, or a list or tuple to set the duration for each frame separately. Integer number of times the GIF should loop. 0 means that it will loop forever. If omitted or , the image will not loop. The GIF loader creates an image memory the same size as the GIF file’s logical screen size, and pastes the actual pixel data (the local image) into this image. If you only want the actual pixel rectangle, you can crop the image: # only read the first \"local image\" from this GIF file Pillow reads and writes macOS files. By default, the largest available icon is read, though you can override this by setting the property before calling . The method sets the following property: Prior to version 8.3.0, Pillow could only write ICNS files on macOS. A list of supported sizes found in this icon file; these are a 3-tuple, , where is 2 for a retina icon and 1 for a standard icon. You can call the method with the following parameter. Affects the scale of the resultant image. If the size is set to , after loading at scale 2, the final value of will be . The method can take the following keyword arguments: A list of images to replace the scaled down versions of the image. The order of the images does not matter, as their use is determined by the size of each image. ICO is used to store icons on Windows. The largest available icon is read. The method supports the following options: A list of sizes including in this ico file; these are a 2-tuple, ; Default to . Any sizes bigger than the original size or 256 will be ignored. The method can take the following keyword arguments: A list of images to replace the scaled down versions of the image. The order of the images does not matter, as their use is determined by the size of each image. By default, the image data will be saved in PNG format. With a bitmap format of “bmp”, image data will be saved in BMP format instead. IM is a format used by LabEye and other applications based on the IFUNC image processing library. The library reads and writes most uncompressed interchange versions of this format. IM is the only format that can store all internal Pillow formats. Pillow reads JPEG, JFIF, and Adobe JPEG files containing , , or data. It writes standard and progressive JFIF files. Using the method, you can speed things up by converting images to , and resize images to 1/2, 1/4 or 1/8 of their original size while loading them. By default Pillow doesn’t allow loading of truncated JPEG files, set to override this. The method may set the following properties if available: JFIF application marker found. If the file is not a JFIF file, this key is not present. A tuple representing the pixel density of the image, in units specified by jfif_unit. A tuple representing the reported pixel density in pixels per inch, if the file is a jfif file and the units are in inches. Adobe application marker found. If the file is not an Adobe JPEG file, this key is not present. Indicates that this is a progressive JPEG file. The ICC color profile for the image. The method supports the following options: The image quality, on a scale from 0 (worst) to 95 (best), or the string . The default is 75. Values above 95 should be avoided; 100 disables portions of the JPEG compression algorithm, and results in large files with hardly any gain in image quality. The value is only valid for JPEG files and will retain the original image quality level, subsampling, and qtables. If present and true, indicates that the encoder should make an extra pass over the image in order to select optimal encoder settings. If present and true, indicates that this image should be stored as a progressive JPEG file. If present and true, the image is stored with the provided ICC profile. If this parameter is not provided, the image will be saved with no profile attached. To preserve the existing profile: If present, the image will be stored with the provided raw EXIF data. By default, libjpeg converts images with an RGB color space to YCbCr. If this option is present and true, those images will be stored as RGB instead. When this option is enabled, attempting to chroma-subsample RGB images with the option will raise an . If present, sets the subsampling for the encoder.\n• None : Only valid for JPEG files, will retain the original image setting. If absent, the setting will be determined by libjpeg or libjpeg-turbo. If present, emit a restart marker whenever the specified number of MCU blocks has been produced. If present, emit a restart marker whenever the specified number of MCU rows has been produced. If present, sets the qtables for the encoder. This is listed as an advanced option for wizards in the JPEG documentation. Use with caution. can be one of several types of values:\n• None a list, tuple, or dictionary (with integer keys = range(len(keys))) of lists of 64 integers. There must be between 2 and 4 tables. Allows storing images without quantization and Huffman tables, or with these tables but without image data. This is useful for container formats or network protocols that handle tables separately and share them between images. To enable JPEG support, you need to build and install the IJG JPEG library before building the Python Imaging Library. See the distribution README for details. Pillow reads and writes JPEG 2000 files containing , , , , or data. When reading, data is converted to or depending on whether or not there is an alpha channel. Added in version 8.3.0: Pillow can read (but not write) , , and images with subsampled components. Added in version 10.4.0: Pillow can read images with OpenJPEG 2.5.1 and later. Added in version 11.1.0: Pillow can write images with OpenJPEG 2.5.3 and later. Pillow supports JPEG 2000 raw codestreams ( files), as well as boxed JPEG 2000 files ( or files). When loading, if you set the on the image prior to the method being invoked, you can ask Pillow to convert the image to either or rather than choosing for itself. It is also possible to set to the number of resolutions to discard (each one reduces the size of the resulting image by a factor of 2), and to specify the number of quality layers to load. The method supports the following options: The image offset, as a tuple of integers, e.g. (16, 16) The tile offset, again as a 2-tuple of integers. The tile size as a 2-tuple. If not specified, or if set to None, the image will be saved without tiling. Either or depending on the units you want to use to specify image quality. A sequence of numbers, each of which represents either an approximate size reduction (if quality mode is ) or a signal to noise ratio value in decibels. If not specified, defaults to a single layer of full quality. The number of different image resolutions to be stored (which corresponds to the number of Discrete Wavelet Transform decompositions plus one). The code-block size as a 2-tuple. Minimum size is 4 x 4, maximum is 1024 x 1024, with the additional restriction that no code-block may have more than 4096 coefficients (i.e. the product of the two numbers must be no greater than 4096). The precinct size as a 2-tuple. Must be a power of two along both axes, and must be greater than the code-block size. If , use the lossy discrete waveform transformation DWT 9-7. Defaults to , which uses the lossless DWT 5-3. If then enable multiple component transformation when encoding, otherwise use for no component transformation (default). If MCT is enabled and is then the Irreversible Color Transformation will be applied, otherwise encoding will use the Reversible Color Transformation. MCT works best with a of and is only applicable when the image data has 3 components. Controls the progression order; must be one of , , , , . The letters stand for Component, Position, Resolution and Layer respectively and control the order of encoding, the idea being that e.g. an image encoded using LRCP mode can have its quality layers decoded as they arrive at the decoder, while one encoded using RLCP mode will have increasing resolutions decoded as they arrive, and so on. If true, then tell the encoder to save the image as signed. Set the encoder to produce output compliant with the digital cinema specifications. The options here are (the default), for 24fps 2K, for 48fps 2K, and for 24fps 4K. Note that for compliant 2K files, at least one of your image dimensions must match 2048 x 1080, while for compliant 4K files, at least one of the dimensions must match 4096 x 2160. If then don’t wrap the raw codestream in the JP2 file format when saving, otherwise the extension of the filename will be used to determine the format (default). Adds a custom comment to the file, replacing the default “Created by OpenJPEG version” comment. If and OpenJPEG 2.4.0 or later is available, then include a PLT (packet length, tile-part header) marker in the produced file. Defaults to . To enable JPEG 2000 support, you need to build and install the OpenJPEG library, version 2.0.0 or higher, before building the Python Imaging Library. Windows users can install the OpenJPEG binaries available on the OpenJPEG website, but must add them to their PATH in order to use Pillow (if you fail to do this, you will get errors about not being able to load the DLL). Pillow reads and writes Multi Picture Object (MPO) files. When first opened, it loads the primary image. The and methods may be used to read other pictures from the file. The pictures are zero-indexed and random access is supported. When calling to write an MPO file, by default only the first frame of a multiframe image will be saved. If the argument is present and true, then all frames will be saved, and the following option will also be available. A list of images to append as additional pictures. Each of the images in the list can be single or multiframe images. Pillow identifies and reads MSP files from Windows 1 and 2. The library writes uncompressed (Windows 1) versions of this format. Pillow reads and writes PCX files containing , , , or data. The function sets the following properties: The absolute value of the number stored in the Scale Factor / Endianness line. Pillow identifies, reads, and writes PNG files containing , , , , , or data. Interlaced files are supported as of v1.1.7. As of Pillow 6.0, EXIF data can be read from PNG images. However, unlike other image formats, EXIF data is not guaranteed to be present in until has been called. By default Pillow doesn’t allow loading of truncated PNG files, set to override this. The function sets the following properties, when appropriate: The chromaticity points, as an 8 tuple of floats. ( , , , , , , , ) The sRGB rendering intent as an integer. For images: Either the palette index for full transparent pixels, or a byte string with alpha values for each palette entry. For , , and images, the color that represents full transparent pixels in this image. This key is omitted if the image is not a transparent palette image. also sets to a dictionary of the values of the , , and chunks of the PNG image. Individual compressed chunks are limited to a decompressed size of , by default 1MB, to prevent decompression bombs. Additionally, the total size of all of the text chunks is limited to , defaulting to 64MB. The method supports the following options: If present and true, instructs the PNG writer to make the output file as small as possible. This includes extra processing in order to find optimal encoder settings. For , , , , and images, this option controls what color from the image to mark as transparent. For images, this can be a either the palette index, or a byte string with alpha values for each palette entry. A tuple of two numbers corresponding to the desired dpi in each direction. ZLIB compression level, a number between 0 and 9: 1 gives best speed, 9 gives best compression, 0 gives no compression at all. Default is 6. When option is True has no effect (it is set to 9 regardless of a value passed). The ICC Profile to include in the saved file. The exif data to include in the saved file. For images, this option controls how many bits to store. If omitted, the PNG writer uses 8 bits (256 colors). To enable PNG support, you need to build and install the ZLIB compression library before building the Python Imaging Library. See the installation documentation for details. The PNG loader includes limited support for reading and writing Animated Portable Network Graphics (APNG) files. When an APNG file is loaded, will return . The value of the property will be when the property is greater than 1. For APNG files, the property depends on both the animation frame count as well as the presence or absence of a default image. See the property documentation below for more details. The and methods are supported. raises an if you try to seek after the last frame. These properties will be set for APNG frames, where applicable: Specifies whether or not this APNG file contains a separate default image, which is not a part of the actual APNG animation. When an APNG file contains a default image, the initially loaded image (i.e. the result of ) will be the default image. To account for the presence of the default image, the property will be set to , where is the actual APNG animation frame count. To load the first APNG animation frame, must be called.\n• None - The APNG contains default image, which is not an animation frame.\n• None - The APNG does not contain a default image. The property will be set to the actual APNG animation frame count. The initially loaded image (i.e. ) will be the first APNG animation frame. The number of times to loop this APNG, 0 indicates infinite looping. The time to display this APNG frame (in milliseconds). The APNG loader returns images the same size as the APNG file’s logical screen size. The returned image contains the pixel data for a given frame, after applying any APNG frame disposal and frame blend operations (i.e. it contains what a web browser would render for this frame - the composite of all previous frames and this frame). Any APNG file containing sequence errors is treated as an invalid image. The APNG loader will not attempt to repair and reorder files containing sequence errors. When calling , by default only a single frame PNG file will be saved. To save an APNG file (including a single frame APNG), the parameter must be set to . The following parameters can also be set: Boolean value, specifying whether or not the base image is a default image. If , the base image will be used as the default image, and the first image from the sequence will be the first APNG animation frame. If , the base image will be used as the first APNG animation frame. Defaults to . A list or tuple of images to append as additional frames. Each of the images in the list can be single or multiframe images. The size of each frame should match the size of the base image. Also note that if a frame’s mode does not match that of the base image, the frame will be converted to the base image mode. Integer number of times to loop this APNG, 0 indicates infinite looping. Defaults to 0. Integer (or list or tuple of integers) length of time to display this APNG frame (in milliseconds). Defaults to 0. An integer (or list or tuple of integers) specifying the APNG disposal operation to be used for this frame before rendering the next frame. Defaults to 0.\n• None 0 ( , default) - No disposal is done on this frame before rendering the next frame.\n• None 1 ( ) - This frame’s modified region is cleared to fully transparent black before rendering the next frame.\n• None 2 ( ) - This frame’s modified region is reverted to the previous frame’s contents before rendering the next frame. An integer (or list or tuple of integers) specifying the APNG blend operation to be used for this frame before rendering the next frame. Defaults to 0.\n• None 0 ( ) - All color components of this frame, including alpha, overwrite the previous output image contents.\n• None 1 ( ) - This frame should be alpha composited with the previous output image contents. The , and parameters can be set to lists or tuples to specify values for each individual frame in the animation. The length of the list or tuple must be identical to the total number of actual frames in the APNG animation. If the APNG contains a default image (i.e. is set to ), these list or tuple parameters should not include an entry for the default image. Pillow reads and writes PBM, PGM, PPM and PNM files containing , , or data. “Raw” (P4 to P6) formats can be read, and are used when writing. Since Pillow 9.2.0, “plain” (P1 to P3) formats can be read as well. Pillow also reads SPIDER stack files containing sequences of SPIDER images. The and methods are supported, and random access is allowed. The method sets the following attributes: Set to 1 if the file is an image stack, else 0. Set to the number of images in the stack. A convenience method, , is provided for converting floating point data to byte data (mode ): The extension of SPIDER files may be any 3 alphanumeric characters. Therefore the output format must be specified explicitly: For more information about the SPIDER image processing package, see https://github.com/spider-em/SPIDER Pillow reads and writes TGA images containing , , , , and data. Pillow can read and write both uncompressed and run-length encoded TGAs. The method can take the following keyword arguments: If set to “tga_rle”, the file will be run-length encoded. If present and a positive number, the first pixel is for the top left corner, rather than the bottom left corner. Pillow reads and writes TIFF files. It can read both striped and tiled images, pixel and plane interleaved multi-band images. If you have libtiff and its headers installed, Pillow can read and write many kinds of compressed TIFF files. If not, Pillow will only read and write uncompressed files. Beginning in version 5.0.0, Pillow requires libtiff to read or write compressed files. Prior to that release, Pillow had buggy support for reading Packbits, LZW and JPEG compressed TIFFs without using libtiff. The method sets the following properties: Image resolution as an tuple, where applicable. You can use the attribute to get more detailed information about the image resolution. Image resolution as an tuple, where applicable. This is a measurement in whichever unit is specified by the file. The attribute contains a dictionary of TIFF metadata. The keys are numerical indexes from . Values are strings or numbers for single items, multiple values are returned in a tuple of values. Rational numbers are returned as a object. For compatibility with legacy code, the attribute contains a dictionary of decoded TIFF fields as returned prior to version 3.0.0. Values are returned as either strings or tuples of numeric values. Rational numbers are returned as a tuple of . The TIFF loader supports the and methods, taking and returning frame numbers within the image file. You can combine these methods to seek to the next frame ( ). Frames are numbered from 0 to , and can be accessed in any order. raises an if you try to seek after the last frame. The method can take the following keyword arguments: If true, Pillow will save all frames of the image to a multiframe tiff document. A list of images to append as additional frames. Each of the images in the list can be single or multiframe images. Note however, that for correct results, all the appended images should have the same and properties. A object or dict object containing tiff tags and values. The TIFF field type is autodetected for Numeric and string values, any other types require using an object and setting the type in with the appropriate numerical value from . Metadata values that are of the rational type should be passed in using a object. For compatibility with legacy code, a object may be passed in this field. However, this is deprecated. Previous versions only supported some tags when writing using libtiff. The supported list is found in . Added support for signed types (e.g. ) and multiple values. Multiple values for a single tag must be to as a tuple and require a matching type in tagtype. Alternate keyword to “tiffinfo”, for consistency with other formats. If true, the image will be saved as a BigTIFF. A string containing the desired compression method for the file. (valid only with libtiff installed) Valid compression methods are: , , , , , , , , , , , , , , The image quality for JPEG compression, on a scale from 0 (worst) to 100 (best). The default is 75. These arguments to set the tiff header fields are an alternative to using the general tags available through tiffinfo. The ICC Profile to include in the saved file. An integer. 1 for no unit, 2 for inches and 3 for centimeters. Either an integer or a float, used for both the x and y resolution. Either an integer or a float. Either an integer or a float. A tuple of , with inches as the resolution unit. For consistency with other image formats, the x and y resolutions of the dpi will be rounded to the nearest integer. Pillow reads and writes WebP files. Requires libwebp v0.5.0 or later. The method supports the following options: If present and true, instructs the WebP writer to use lossless compression. Integer, 0-100, defaults to 80. For lossy, 0 gives the smallest size and 100 the largest. For lossless, this parameter is the amount of effort put into the compression: 0 is the fastest, but gives larger files compared to the slowest, but best, 100. Integer, 0-100, defaults to 100. For lossy compression only. 0 gives the smallest size and 100 is lossless. If true, preserve the transparent RGB values. Otherwise, discard invisible RGB values for better compression. Defaults to false. The ICC Profile to include in the saved file. The exif data to include in the saved file. The XMP data to include in the saved file. When calling to write a WebP file, by default only the first frame of a multiframe image will be saved. If the argument is present and true, then all frames will be saved, and the following options will also be available. A list of images to append as additional frames. Each of the images in the list can be single or multiframe images. The display duration of each frame, in milliseconds. Pass a single integer for a constant duration, or a list or tuple to set the duration for each frame separately. Number of times to repeat the animation. Defaults to [0 = infinite]. Background color of the canvas, as an RGBA tuple with values in the range of (0-255). Minimum and maximum distance between consecutive key frames in the output. The library may insert some key frames as needed to satisfy this criteria. Note that these conditions should hold: kmax > kmin and kmin >= kmax / 2 + 1. Also, if kmax <= 0, then key-frame insertion is disabled; and if kmax == 1, then all frames will be key-frames (kmin value does not matter for these special cases). If true, use mixed compression mode; the encoder heuristically chooses between lossy and lossless for each frame.\n\nCUR is used to store cursors on Windows. The CUR decoder reads the largest available cursor. Animated cursors are not supported. DCX is a container file format for PCX files, defined by Intel. The DCX format is commonly used in fax applications. The DCX decoder can read files containing , , , or data. When the file is opened, only the first image is read. You can use or to read other images. Pillow identifies and reads FITS files, commonly used for astronomy. Uncompressed and GZIP_1 compressed images can be read. The method sets the following properties: The delay (in milliseconds) between each frame. Pillow reads Kodak FlashPix files. Only the highest resolution image is read from the file, and the viewing transform is not taken into account. To enable FPX support, you must install olefile. To enable full FlashPix support, you need to build and install the IJG JPEG library before building the Python Imaging Library. See the distribution README for details. The FTEX decoder reads textures used for 3D objects in Independence War 2: Edge Of Chaos. The plugin reads a single texture per file, in the compressed and uncompressed formats. The method sets the following properties: The spacing between the brushes, in pixels. Version 2 only. Pillow reads uncompressed GD2 files. Note that you must use to read such a file. The method sets the following properties: Transparency color index. This key is omitted if the image is not transparent. Pillow identifies and reads Microsoft Image Composer (MIC) files. When opened, the first sprite in the file is loaded. You can use and to read other sprites from the file. Note that there may be an embedded gamma of 2.2 in MIC files. To enable MIC support, you must install olefile. Pillow reads PhotoCD files containing data. This only reads the 768x512 resolution image from the file. Higher resolutions are encoded in a proprietary encoding. Pillow provides limited support for PIXAR raster files. The library can identify and read “dumped” RGB files. Pillow identifies and reads PSD files written by Adobe Photoshop 2.5 and 3.0. Pillow reads images in Quite OK Image format using a Python decoder. If you wish to write code specifically for this format, qoi is an alternative library that uses C to decode the image and interfaces with NumPy. Note that this file format cannot be automatically identified, so you must use the open function in the module to read files in this format. By default, a Quake2 standard palette is attached to the texture. To override the palette, use the method. On Windows, it can read WMF and EMF files. By default, it will load the image at 72 dpi. To load it at another resolution: To add other read or write support, use to register a WMF and EMF handler. Pillow reads X pixmap files (mode ) with 256 colors or less. The method sets the following properties: Transparency color index. This key is omitted if the image is not transparent."
    },
    {
        "link": "https://stackoverflow.com/questions/60359207/randomly-select-images-using-pil",
        "document": "I have a folder with over 100k images, I am wondering how do I use PIL to randomly select 5 images to display?\n\nThe code below will return me all codes which is not feasible though."
    },
    {
        "link": "https://geeksforgeeks.org/show-random-picture-from-a-folder-in-python",
        "document": "In this article, we are going to share with you the steps to create a simple script that selects and displays a random image from a specified folder using Python. By using the combination of os, random, and PIL libraries, you can easily do this, So, let's get started.\n\nUsing the Python Pillow library, we can show the picture randomly from any folder. Here is the step-by-step procedure for doing this.\n\nStep 2: Getting the image path randomly from the folder\n\nStep 4: Integrating both function to showing the random Image\n\n# Filter the list to get only image files \"No images found in the specified folder.\" \"An error occurred while displaying the image: \" The specified folder does not exist:\n\nNOTE: The folder structure of showing a Random Image from a Folder will be like this,\n\nHere is the video of real output of the code"
    },
    {
        "link": "https://pillow.readthedocs.io/en/stable/reference/Image.html",
        "document": "The module provides a class with the same name which is used to represent a PIL image. The module also provides a number of factory functions, including functions to load images from files, and to create new images.\n\nInstances of the class have the following attributes: The filename or path of the source file. Only images created with the factory function have a filename attribute. If the input is a file like object, the filename attribute is set to an empty string. The file format of the source file. For images created by the library itself (via a factory function, or by running a method on an existing image), this attribute is set to . Image mode. This is a string specifying the pixel format used by the image. Typical values are “1”, “L”, “RGB”, or “CMYK.” See Modes for a full list. Image size, in pixels. The size is given as a 2-tuple (width, height). Colour palette table, if any. If mode is “P” or “PA”, this should be an instance of the class. Otherwise, it should be set to . A dictionary holding data associated with the image. This dictionary is used by file handlers to pass on various non-image information read from the file. See documentation for the various file handlers for details. Most methods ignore the dictionary when returning new images; since the keys are not standardized, it’s not possible for a method to know if the operation affects the dictionary. If you need the information later on, keep a reference to the info dictionary returned from the open method. Unless noted elsewhere, this dictionary does not affect saving files. if this image has more than one frame, or otherwise. This attribute is only defined by image plugins that support animated images. Plugins may leave this attribute undefined if they don’t support loading animated images, even if the given format supports animated images. Given that this attribute is not present for all images use to check if Pillow is aware of multiple frames in an image regardless of its format. The number of frames in this image. This attribute is only defined by image plugins that support animated images. Plugins may leave this attribute undefined if they don’t support loading animated images, even if the given format supports animated images. Given that this attribute is not present for all images use to check the number of frames that Pillow is aware of in an image regardless of its format. Determine if an image has transparency data, whether in the form of an alpha channel, a palette with an alpha channel, or a “transparency” key in the info dictionary. Note the image might still appear solid, if all of the values shown within are opaque."
    }
]