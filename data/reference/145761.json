[
    {
        "link": "https://convin.ai/blog/conversational-ai-training",
        "document": "Training a conversational AI model is a multi-step process. Each stage ensures the AI understands human language and can deliver contextually relevant responses. Below are the essential steps involved:\n\nThe first step in training conversational AI is gathering a wide range of conversation data. This could include historical chat logs, recorded phone calls, or other interactions between agents and customers. The goal is to capture a variety of conversational tones, styles, and contexts to ensure the AI can handle real-world scenarios.\n\nData diversity is crucial. A well-trained AI must be able to interact with customers of different backgrounds, speaking different languages or dialects. For example, an AI in a global support center must understand American English and regional variations and accents. The more diverse the dataset, the more robust the AI becomes.\n\nOnce the data is collected, it must be labeled and annotated. This process involves tagging each part of the conversation with its corresponding intent or meaning. For example, if a customer asks for a product price, the label might be \"price inquiry.\" This labeling is critical because it helps the AI understand what customers are asking for.\n\nAnnotation adds further depth to the data. Sentiment analysis is often included at this stage, helping the AI recognize if a customer is frustrated, neutral, or satisfied. Accurate annotation allows the AI to respond appropriately, improving customer satisfaction.\n\nPreprocessing is a crucial step in which the collected data is cleaned and prepared for training. This includes removing noise from voice data, standardizing text formats, and ensuring that only relevant parts of conversations are used. Preprocessing might involve filtering background noise or adjusting volume levels in voice data.\n\nAnother part of preprocessing is normalizing the language used in conversations. Slang, abbreviations, and informal language must be translated into formats the AI can recognize and learn from. This step ensures that the AI model isn’t confused by colloquial language or regional slang.\n\nOnce the data is prepared, it's time to train the model. The AI is fed these labeled datasets and uses them to learn how to interpret and respond to similar conversations in the future. Deep learning models, neural networks, or other algorithms are used during this phase to fine-tune the AI's responses.\n\nThe training process involves teaching the AI to recognize what customers are asking and the context behind their questions. For instance, if a customer asks, \"Can I get a refund?\" rather than \"Can I get more information on returns?\" the AI must differentiate between a direct request and an inquiry for details.\n\nAfter the initial training, the AI model must be continuously fine-tuned based on real-world interactions. This stage is critical because language and customer expectations evolve. Regular updates and retraining using new data help the AI improve its performance and accuracy.\n\nFor example, if the AI frequently misinterprets a certain query type, additional training can correct these errors. Continuous learning ensures the AI remains relevant and accurate, providing better customer experiences over time.\n\nBefore deploying the AI system, it must be tested and validated. This phase involves running the AI through various simulated customer interactions to check its performance. Testing ensures the AI responds appropriately to queries and can handle complex conversations.\n\nValidation also involves stress-testing the AI under high-volume scenarios, ensuring it can maintain accuracy and speed even during peak times. Identifying weak spots in the AI’s understanding at this stage allows developers to retrain and improve the model before it goes live."
    },
    {
        "link": "https://medium.com/wluper/methods-to-grow-your-own-data-sets-for-conversational-ai-1bfeac9508ef",
        "document": "Methods to grow your own data sets for Conversational AI Scaling from tiny to large amounts of NLP and Dialogue data Supervised Learning algorithms require a significant amount of labelled training examples to properly approximate a function that is robust to the richness and variation inherent in natural languages. Providing too few examples may produce a model which fails to generalise to underlying patterns, making it brittle and easily broken when exposed to unseen examples encountered in the wild. However, collecting and annotating training data within a domain demands considerable time and resources. Fortunately, there are a range of high-quality augmentation techniques to artificially inflate textual datasets, including methods using state-of-the-art language models like BERT and GPT-2. Given a tiny dataset of just three conversations: Open-AI’s massive GPT-2 language model was trained on so much data that it is able to generate very realistic sentences. We can use this fact to produce new variant examples by extending each conversation’s final sentence (e.g. “i’m just in a bad mood” → “…because I lost in the qualifiers”):\n\nSimilarly, we can use it to additional sentences to the end of the conversation too. In fact, each time you run this language model, you get slightly different results so you could re-run this augmentation method multiple times to introduce even more variant conversations into your dataset. First we will need to install GPT-2-simple, an open-source library designed to make accessing this powerful language model very easy. We download the 774M parameter version of the model and load it up. We create a small function which takes an example conversation as an input and calls GPT-2 to generate the next 100 words which it thinks could follow on from this conversation (we have chosen to extend the conversation by a single sentence [:n+1], but feel-free to modify this to extend the conversation more). Google’s BERT is another powerful language model which has revolutionised NLP but it is trained slightly differently to other language models, like GPT-2. This difference makes it well suited for predicting masked words; wherein a word is masked (hidden) and BERT uses the words surrounding that masked word to predict what the masked word could be. E.g. “One day she [MASK] down the hall” → “One day she ran down the hall” We inserted masks between words in a complete sentence and could trick BERT into predicting new words and extending the sentence from the middle (as opposed to the end). E.g. “the fox” → “the [MASK] fox” → “the brown fox” → “the [MASK] brown fox” → “the striped brown fox” …\n\nTo do this method, we must first install the open-source library, pytorch-pretrained-bert, and download the language model along with its accompanying tokeniser. We also need to create a function which appropriately formats the input string with special tokens ([CLS], [SEP]), splits the string into tokens (using the model’s accompanying tokeniser) and then inserts a special mask token ([MASK]) to indicate the word we wish the model to predict. For the output of the model, we want to return the sentence with an additional word inserted between two other words. Therefore, we create a function which converts the token indexes back into words (using the same tokeniser), and then fetch the token index predicted by the model corresponding to the location of the mask token (and convert it to a word in the same way). We then join the words together into a single string, clean it up a bit by removing any special tokenisation symbols (e.g. ## , [CLS] at the beginning, [SEP] at the end, etc). We now define a function to connect everything together; the input formatting, the BERT model, the output formatting. You may have noticed that the function requires you to specify which position you want to insert the mask. Well, why not each and every position in turn? This is what this next function does; iteratively creating variants by placing a mask at every index in the sentence until it reaches the end and fails, at which point it returns all the newly created variants. Back translation (a.k.a. spinning) uses a machine translation model to translate a sentence into a foreign language and then to back again into the original language. We found this method very good at producing natural sounding sentences which are grammatically consistent yet slightly different from the original.\n\nFirst we need to import textblob, or any other open-source library with access to free translation. Next, we define a function which translates the sentence into some specified foreign language and back again into English (the assumed original language). This could also be extended to include translations into more than one foreign language before being translated back into English. def _spin_text(text, foreign_language):\n\n try:\n\n spun_text = _clean_word(\n\n TextBlob(\n\n TextBlob(text).translate(\n\n from_lang=\"en\",\n\n to=foreign_language\n\n ).raw\n\n ).translate(\n\n from_lang=foreign_language,\n\n to=\"en\"\n\n ).raw\n\n )\n\n return spun_text if spun_text != _clean_word(text) else None\n\n except:\n\n return None If the translation failed, or the spun sentence turns out identical to the original (disregarding formatting or punctuation changes), then the function returns None. A more classical technique is to pick a word in the sentence and substitute it for one of its synonyms. This method can produce a huge number of variations by substituting one word in the conversation at at a time (e.g. “I’m just in a bad mood” → “I’m simply in a bad mood”). “hi …” -> “how do you do …”\n\nWe first download the relevant files from NLTK (an older yet giant NLP library). We define a function which fetches synonyms for a word from WordNet (a massive, hand-curated web of words and their various relations with one another) def synonyms(word, pos_tag):\n\n return list(\n\n {\n\n lemma.replace(\"_\",\" \").replace(\"-\",\" \") for synset in wn.synsets(\n\n _clean_word(word),\n\n pos_tag,\n\n ) for lemma in synset.lemma_names()\n\n }\n\n ) We have added a filter here which takes into account the word’s part-of-speech before fetching any synonyms (i.e. is it a noun, verb, adjective, adverb, etc). E.g. The word “test” can be used as a noun or verb, etc: We then define another function to automatically infer the part-of-speech tag: Fortunately, NLTK has some pre-trained POS Taggers which considerably simplifies our lives: This function takes in tokens, as opposed to a string, so be sure to split the string into words or use one of NLTK’s provided tokenisers (nltk.word_tokenize(some_string_to_be_tokenised)). The function outputs the resulting POS tags returned by the tagger but first converts them into the POS notation required for compatibility with WordNet: def _convert_nltk_to_wordnet_tag(pos_tag):\n\n if pos_tag.startswith(\"N\"):\n\n return wn.NOUN\n\n if pos_tag.startswith(\"V\"):\n\n return wn.VERB\n\n if pos_tag.startswith(\"R\"):\n\n return wn.ADV\n\n if pos_tag.startswith(\"J\"):\n\n return wn.ADJ Our final technique is a very basic one which can be applied to any time-series data (like conversations). The position of each sentence (or data point) in the conversation (training example) is simply shifted (offset) by one place to produce a valid variant. E.g. “hi”, “how are you”, “fine thanks”… → “how are you”, “fine thanks”… You can also combine time-series samples (conversations) by appending them to each other, producing a “new”, longer example: Finally, if you wish to make your model robust to textual errors which can occur in real-world scenarios, you can create variants by inserting textual noise (e.g. random spelling mistakes, additions, deletions and word order changes, etc). We at Wluper combined the above five techniques, added a little bit of magic sauce, and were able to achieve a 10,000x fold improvement in training data for actual conversations, without even exhausting the number of possible variants each of these five technique alone are able to offer. Powerful, yet efficient. As well as state-of-the-art language models, some of our unrevealed NLP augmentation methods involve multi-task learning, semi-supervised, and even unsupervised learning. One specific example would be Clustering, a great way to find natural variations for simple intents like greetings, requesting alternatives, etc. \n\nWe use clustering algorithms to analyse public data sets and discover groups of phrases which share some underlying semantic relationships. Although one will not be told explicitly which similarities these clusters share, they can be inferred using other techniques. Further, a seed phrase with a known class label can be used to solve such ambiguities. We are constantly striving to find efficient and effective methods to grow our quality data sets and allow our Dialogue system to become better, more accurate, and more robust. You can find code snippets for the above NLP and Dialogue data augmentation in the “dsag” repo on our Github page."
    },
    {
        "link": "https://discuss.huggingface.co/t/dataset-format-standards-for-chat-based-fine-tuned-llama-models/70638",
        "document": "I want to use a Llama-based model for text generation/chat-bot. I have my own data, and was curious how to format it as to get the best results out of my fine-tuning. Currently, I use \"[PCP] \" and \"[SR] \" to separate who is talking. Here’s a snippet of my code to do this, followed by an example of two conversations I might have in my dataset.\n\n[PCP] Some text some text some text [SR] Dear Lorem, ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco. [SR] Hi __, Thanks for getting back to GI. Given the clinical symptom along with patient’s age and documented BRBPR, GI will evaluate patient in GI clinic to consider scheduling diagnostic EGD/colonoscopy evaluation.\n\n &\n\n Best [PCP] Great!\n\nand (no data is real)…\n\n[PCP] 45 yo male already on Pantoprazole , has recurrence , worsening GERD with esophagitis. addendum: update:01/01/1970. patient tested negative for H.pylori in the stool but still having a lot of abdominal gas and eructation [SR] Dr Smith what are the pts GERD and “esophagitis” symptoms.\n\nThis is the only place I’ve seen something that has a little documentation on how to format data depending on what model and what task you are using. A question was asked here but never answered\n\nIt says use this for chat-based\n\n[INST]<<SYS>>\n\n You are a friendly chatbot that gives helpful answers\n\n <</SYS>> Hello[/INST]Hello, how are you?</s><s>[INST]Good, please tell me what 1+1 is.[/INST]1+1=2. Please let me know if you need anything else!</s>\n\nBut I am curious if there is any other documentation out there that details how and why data should be formatted a certain way for a certain type of model and task."
    },
    {
        "link": "https://huggingface.co/docs/trl/main/en/dataset_formats",
        "document": "and get access to the augmented documentation experience\n\nThis guide provides an overview of the dataset formats and types supported by each trainer in TRL.\n\nOverview of the dataset formats and types\n• The format of a dataset refers to how the data is structured, typically categorized as either standard or conversational.\n• The type is associated with the specific task the dataset is designed for, such as prompt-only or preference. Each type is characterized by its columns, which vary according to the task, as shown in the table.\n\nThe standard dataset format typically consists of plain text strings. The columns in the dataset vary depending on the task. This is the format expected by TRL trainers. Below are examples of standard dataset formats for different tasks:\n\nConversational datasets are used for tasks involving dialogues or chat interactions between users and assistants. Unlike standard dataset formats, these contain sequences of messages where each message has a (e.g., or ) and (the message text).\n\nJust like standard datasets, the columns in conversational datasets vary depending on the task. Below are examples of conversational dataset formats for different tasks:\n\nConversational datasets are useful for training chat models, but must be converted into a standard format before being used with TRL trainers. This is typically done using chat templates specific to the model being used. For more information, refer to the Working with conversational datasets in TRL section.\n\nA language modeling dataset consists of a column (or for conversational datasets) containing a full sequence of text.\n\nIn a prompt-only dataset, only the initial prompt (the question or partial sentence) is provided under the key . The training typically involves generating the completion based on this prompt, where the model learns to continue or complete the given input.\n\nFor examples of prompt-only datasets, refer to the Prompt-only datasets collection.\n\nFor examples of prompt-completion datasets, refer to the Prompt-completion datasets collection.\n\nA preference dataset is used for tasks where the model is trained to choose between two or more possible completions to the same prompt. This dataset includes a , a completion, and a completion. The model is trained to select the response over the response. Some dataset may not include the column, in which case the prompt is implicit and directly included in the and completions. We recommend using explicit prompts whenever possible.\n\nFor examples of preference datasets, refer to the Preference datasets collection.\n\nSome preference datasets can be found with the tag on Hugging Face Hub. You can also explore the librarian-bots’ DPO Collections to identify preference datasets.\n\nAn unpaired preference dataset is similar to a preference dataset but instead of having and completions for the same prompt, it includes a single and a indicating whether the completion is preferred or not.\n\nFor examples of unpaired preference datasets, refer to the Unpaired preference datasets collection.\n\nA stepwise (or process) supervision dataset is similar to an unpaired preference dataset but includes multiple steps of completions, each with its own label. This structure is useful for tasks that need detailed, step-by-step labeling, such as reasoning tasks. By evaluating each step separately and providing targeted labels, this approach helps identify precisely where the reasoning is correct and where errors occur, allowing for targeted feedback on each part of the reasoning process.\n\nFor examples of stepwise supervision datasets, refer to the Stepwise supervision datasets collection.\n\nWhich dataset type to use?\n\nChoosing the right dataset type depends on the task you are working on and the specific requirements of the TRL trainer you are using. Below is a brief overview of the dataset types supported by each TRL trainer.\n\nConversational datasets are increasingly common, especially for training chat models. However, some TRL trainers don’t support conversational datasets in their raw format. (For more information, see issue #2071.) These datasets must first be converted into a standard format. Fortunately, TRL offers tools to easily handle this conversion, which are detailed below.\n\nTo convert a conversational dataset into a standard dataset, you need to apply a chat template to the dataset. A chat template is a predefined structure that typically includes placeholders for user and assistant messages. This template is provided by the tokenizer of the model you use.\n\nFor detailed instructions on using chat templating, refer to the Chat templating section in the documentation.\n\nIn TRL, the method you apply to convert the dataset will vary depending on the task. Fortunately, TRL provides a helper function called apply_chat_template() to simplify this process. Here’s an example of how to use it:\n\nAlternatively, you can use the map method to apply the template across an entire dataset:\n\nUsing any dataset with TRL: preprocessing and conversion\n\nMany datasets come in formats tailored to specific tasks, which might not be directly compatible with TRL. To use such datasets with TRL, you may need to preprocess and convert them into the required format.\n\nTo make this easier, we provide a set of example scripts that cover common dataset conversions.\n\nLet’s take the UltraFeedback dataset as an example. Here’s a preview of the dataset:\n\nAs shown above, the dataset format does not match the expected structure. It’s not in a conversational format, the column names differ, and the results pertain to different models (e.g., Bard, GPT-4) and aspects (e.g., “helpfulness”, “honesty”).\n\nBy using the provided conversion script , you can transform this dataset into an unpaired preference type, and push it to the Hub:\n\nOnce converted, the dataset will look like this:\n\nNow, you can use this dataset with TRL!\n\nBy adapting the provided scripts or creating your own, you can convert any dataset into a format compatible with TRL.\n\nThis section provides example code to help you convert between different dataset types. While some conversions can be performed after applying the chat template (i.e., in the standard format), we recommend performing the conversion before applying the chat template to ensure it works consistently.\n\nFor simplicity, some of the examples below do not follow this recommendation and use the standard format. However, the conversions can be applied directly to the conversational format without modification.\n\nTo convert a prompt-completion dataset into a language modeling dataset, concatenate the prompt and the completion.\n\nTo convert a prompt-completion dataset into a prompt-only dataset, remove the completion.\n\nFrom preference with implicit prompt to language modeling dataset\n\nTo convert a preference with implicit prompt dataset into a language modeling dataset, remove the rejected, and rename the column to .\n\nFrom preference with implicit prompt to prompt-completion dataset\n\nTo convert a preference dataset with implicit prompt into a prompt-completion dataset, extract the prompt with extract_prompt(), remove the rejected, and rename the column to .\n\nFrom preference with implicit prompt to prompt-only dataset\n\nTo convert a preference dataset with implicit prompt into a prompt-only dataset, extract the prompt with extract_prompt(), and remove the rejected and the chosen.\n\nTo convert a preference dataset with implicit prompt into a preference dataset with explicit prompt, extract the prompt with extract_prompt().\n\nFrom preference with implicit prompt to unpaired preference dataset\n\nTo convert a preference dataset with implicit prompt into an unpaired preference dataset, extract the prompt with extract_prompt(), and unpair the dataset with unpair_preference_dataset().\n\nTo convert a preference dataset into a language modeling dataset, remove the rejected, concatenate the prompt and the chosen into the column.\n\nTo convert a preference dataset into a prompt-completion dataset, remove the rejected, and rename the column to .\n\nTo convert a preference dataset into a prompt-only dataset, remove the rejected and the chosen.\n\nTo convert a preference dataset with explicit prompt into a preference dataset with implicit prompt, concatenate the prompt to both chosen and rejected, and remove the prompt.\n\nTo convert dataset into an unpaired preference dataset, unpair the dataset with unpair_preference_dataset().\n\nTo convert an unpaired preference dataset into a language modeling dataset, concatenate prompts with good completions into the column, and remove the prompt, completion and label columns.\n\nTo convert an unpaired preference dataset into a prompt-completion dataset, filter for good labels, then remove the label columns.\n\nTo convert an unpaired preference dataset into a prompt-only dataset, remove the completion and the label columns.\n\nTo convert a stepwise supervision dataset into a language modeling dataset, concatenate prompts with good completions into the column.\n\nTo convert a stepwise supervision dataset into a prompt-completion dataset, join the good completions and remove the labels.\n\nFrom stepwise supervision to prompt only dataset\n\nTo convert a stepwise supervision dataset into a prompt-only dataset, remove the completions and the labels.\n\nTo convert a stepwise supervision dataset into an unpaired preference dataset, join the completions and merge the labels.\n\nThe method for merging the labels depends on the specific task. In this example, we use the logical AND operation. This means that if the step labels indicate the correctness of individual steps, the resulting label will reflect the correctness of the entire sequence.\n\nSome trainers also support fine-tuning vision-language models (VLMs) using image-text pairs. In this scenario, it’s recommended to use a conversational format, as each model handles image placeholders in text differently.\n\nA conversational vision dataset differs from a standard conversational dataset in two key ways:\n• The dataset must contain the key with the image data.\n• The field in messages must be a list of dictionaries, where each dictionary specifies the type of data: or .\n\nAn example of a conversational vision dataset is the openbmb/RLAIF-V-Dataset. Below is an embedded view of the dataset’s training data, allowing you to explore it directly:"
    },
    {
        "link": "https://github.com/PolyAI-LDN/conversational-datasets",
        "document": "This repository provides tools to create reproducible datasets for training and evaluating models of conversational response. This includes:\n• OpenSubtitles - over 400 million lines from movie and television subtitles (available in English and other languages)\n• Amazon QA - over 3.6 million question-response pairs in the context of Amazon products\n\nMachine learning methods work best with large datasets such as these. At PolyAI we train models of conversational response on huge conversational datasets and then adapt these models to domain-specific tasks in conversational AI. This general approach of pre-training large models on huge datasets has long been popular in the image community and is now taking off in the NLP community.\n\nRather than providing the raw processed data, we provide scripts and instructions to generate the data yourself. This allows you to view and potentially manipulate the pre-processing and filtering. The instructions define standard datasets, with deterministic train/test splits, which can be used to define reproducible evaluations in research papers.\n\nEach dataset has its own directory, which contains a dataflow script, instructions for running it, and unit tests.\n\nNote that these are the dataset sizes after filtering and other processing. For instance, the Reddit dataset is based on a raw database of 3.7 billion comments, but consists of 726 million examples because the script filters out long comments, short comments, uninformative comments (such as , and comments with no replies.\n\nBenchmark results for each of the datasets can be found in .\n\nThis repo contains scripts for creating datasets in a standard format - any dataset in this format is referred to elsewhere as simply a conversational dataset.\n\nDatasets are stored either as:\n• JSON text files, with one example per line\n• or as Tensorflow record files containing serialized tensorflow example protocol buffers.\n\nThe training set is stored as one collection of examples, and the test set as another. Examples are shuffled randomly (and not necessarily reproducibly) among the files. The train/test split is always deterministic, so that whenever the dataset is generated, the same train/test split is created.\n\nEach example contains a conversational context and a response that goes with that context. For example:\n\nExplicitly, each example contains a number of string features:\n• A feature, the most recent text in the conversational context\n• A feature, the text that is in direct response to the .\n• A number of extra context features, , etc. going back in time through the conversation. They are named in reverse order so that always refers to the most recent extra context, so that no padding needs to be done, and datasets with different numbers of extra contexts can be mixed.\n\nDepending on the dataset, there may be some extra features also included in each example. For instance, in Reddit the author of the context and response are identified using additional features.\n\nFor use outside of tensorflow, the JSON format may be preferable. To get JSON format datasets, use in the dataset's script. Each line will contain a single JSON object.\n\nBelow is some example python code for reading a JSON format dataset.\n\nThe and scripts demonstrate how to read a Tensorflow example format conversational dataset in Python, using functions from the tensorflow library.\n\nYou can use to compute the number of examples in a tensorflow record file:\n\nIt can also be used to display the examples in a readable format:\n\nBelow is some example tensorflow code for reading a conversational dataset into a tensorflow graph:\n\nConversational datasets are created using Apache Beam pipeline scripts, run on Google Dataflow. This parallelises the data processing pipeline across many worker machines. Apache Beam requires python 2.7, so you will need to set up a python 2.7 virtual environment:\n\nThe Dataflow scripts write conversational datasets to Google cloud storage, so you will need to create a bucket to save the dataset to.\n\nDataflow will run workers on multiple Compute Engine instances, so make sure you have a sufficient quota of machines. The READMEs for individual datasets give an idea of how many workers are required, and how long each dataflow job should take.\n\nLastly, you will need to set up authentication by creating a service account with access to Dataflow and Cloud Storage, and set :\n\nThis should be enough to follow the instructions for creating each individual dataset.\n\nOf course you may evaluate your models in any way you like. However, when publishing results, we encourage you to include the 1-of-100 ranking accuracy, which is becoming a research community standard.\n\nThe 1-of-100 ranking accuracy is a Recall@k metric. In general Recall@k takes N responses to the given conversational context, where only one response is relevant. It indicates whether the relevant response occurs in the top k ranked candidate responses. The 1-of-100 metric is obtained when k=1 and N=100. This effectively means that, for each query, we indicate if the correct response is the top ranked response among 100 candidates. The final score is the average across all queries.\n\nThe 1-of-100 metric is computed using random batches of 100 examples so that the responses from other examples in the batch are used as random negative candidates. This allows for efficiently computing the metric across many examples in batches. While it is not guaranteed that the random negatives will indeed be 'true' negatives, the 1-of-100 metric still provides a useful evaluation signal that correlates with downstream tasks.\n\nThe following tensorflow code shows how this metric can be computed for a dot-product style encoder model, where the score for each context and response is a dot product between corresponding vectors:\n\nSee also the baselines for example code computing the 1-of-100 metric.\n\nMany studies have used Recall@k in the context of retrieval-based dialogue, including the following papers:\n• The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems, Lowe et al. SIGDIAL 2015.\n• Strategy and Policy Learning for Non-task-oriented Conversational Systems, Yu et al. SIGDIAL 2016.\n• Training End-to-End Dialogue Systems with the Ubuntu Dialogue Corpus, Lowe et al. Dialogue and Discourse 2017.\n• Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots, Wu et al. ACL 2017.\n• Improving Response Selection in Multi-turn Dialogue Systems by Incorporating Domain Knowledge, Chaudhuri et al. CoNLL 2018.\n• Customized Nonlinear Bandits for Online Response Selection in Neural Conversational Models, Liu et al. AAAI 2018.\n• Multi-representation Fusion Network for Multi-Turn Response Selection in Retrieval-based Chatbots, Tao et al. WSDM 2019.\n• Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network, Zhou et al. ACL 2018.\n\nThe following papers use the 1-of-100 ranking accuracy in particular:\n• Conversational Contextual Cues: The Case of Personalization and History for Response Ranking., Al-Rfou et al. arXiv pre-print 2016.\n• Question-Answer Selection in User to User Marketplace Conversations, Kumar et al. IWSDS 2018.\n• Learning Semantic Textual Similarity from Conversations.. Yang et al. Workshop on Representation Learning for NLP 2018.\n\nWhen using these datasets in your work, please cite our paper, A Repository of Conversational Datasets:\n\nWe happily accept contributions in the form of pull requests. Each pull request is tested in CircleCI - it is first linted with , and then the unit tests are run. In particular we would be interested in:\n• adaptations to the scripts so that they work better in your environment (e.g. other Apache Beam runners, other cloud storage solutions, other example formats)\n• results from your methods in the benchmarks the benchmarks page.\n• code for new baselines and improvements to existing baselines"
    },
    {
        "link": "https://deasylabs.com/blog/best-practices-for-creating-high-quality-training-datasets",
        "document": "Creating high-quality training datasets is foundational for the performance of AI models, especially for enterprises operating in regulated industries like finance, healthcare, and government services where the accuracy and reliability of AI systems are paramount. Proper dataset creation is imperative to ensure data-driven decisions are accurate and predictive models perform optimally. This article elucidates the best practices for creating high-quality AI training datasets, emphasizing advanced techniques and real-world applications.\n\nLabel hierarchies are crucial for organizing training data in a multi-level framework, enabling more nuanced classification tasks and enhancing model accuracy. Organizing labels in a hierarchical structure mirrors human cognitive processes, giving the model context about relationships among different classes. For instance, when distinguishing between vehicle types, a hierarchy might first differentiate between 'Vehicle' and 'Non-Vehicle', then classify 'Vehicle' into 'Car', 'Truck', and 'Motorcycle', and further subdivide 'Car' into 'Sedan', 'SUV', etc. This structure helps in the progressive refinement of the model, effectively reducing classification errors.\n\nConsistency in data annotation is critical for maintaining the reliability of the training dataset. Using an automated labeling workflow, such as the one provided by Deasie , helps maintain high standards in data labeling by providing structured guidelines and automated checks. Such workflows minimize human error and assure a higher degree of reproducibility. Employing expert annotators, especially in specialized fields like medical imaging, further enhances the quality and consistency of labels.\n\nAn effective training dataset must represent the problem domain's variability. It is essential to include examples of all classes, reflecting different conditions and variations seen in real-world scenarios. For instance, in our opinion, balancing the dataset to avoid bias towards any class ensures the model does not become skewed towards more frequent classes, improving its generalization ability.\n\nData augmentation is critical in enhancing training datasets, especially in scenarios where data collection is expensive or time-consuming. Augmentation techniques, such as rotating, flipping, or adding noise to images, artificially increase the dataset size and diversity. This helps the model become more robust by exposing it to a wider variety of examples without needing to collect new data. Techniques like Synthetic Minority Over-sampling Technique (SMOTE) are also used to balance class distributions by generating synthetic examples for minority classes.\n\nUnstructured data, which includes text, images, and other non-tabular forms, poses unique challenges in dataset creation. Converting unstructured data into a structured format suitable for machine learning involves several steps, including data cleaning, feature extraction, and annotation. For text data, this might involve tokenization, stop-word removal, and stemming or lemmatization. In the case of image data, preprocessing steps like normalization, resizing, and cropping are crucial. Utilizing automated tools like Deasie can streamline the process of organizing and labeling unstructured data, thus improving overall dataset quality.\n\nMetadata can significantly enhance the processes of data organization and retrieval, thereby improving the model's accuracy and scalability. Metadata, such as timestamps, geolocation, or device information, provides additional context that can be instrumental in understanding data points. In retrieval-augmented generation (RAG) models, accurate and comprehensive metadata can enhance both the precision and relevance of the information retrieved, leading to improved results and efficiency.\n\nIn regulated industries, the security and privacy of training data are paramount. It is essential to comply with industry standards and regulations, such as HIPAA for healthcare data or GDPR for data within the European Union. Implementing robust encryption practices, access controls, and de-identification techniques ensure that sensitive information is protected. Permissioned access and audit logs help in monitoring and controlling data usage, thereby maintaining compliance with regulatory requirements.\n\nUsing advanced labeling tools such as those offered by Deasie can enhance the efficiency and quality of data annotation processes. These tools often come equipped with features like machine-assisted annotation, hierarchical labeling support, and real-time quality checks. Implementing such tools can reduce annotation time, minimize errors, and ensure the dataset is comprehensive and well-structured.\n\nTo illustrate these practices, consider a case study in oncology imaging. A project was aimed at developing a model to differentiate between various types of tumors. The team adopted a hierarchical labeling system, starting with broad categories (e.g., 'Benign' and 'Malignant') and proceeding to more specific tumor types (e.g., 'Melanoma', 'Basal Cell Carcinoma'). Using Deasie's automated labeling workflow, they achieved consistent and high-quality annotations. By incorporating metadata such as patient demographics and imaging device specifications, the model's performance improved significantly in identifying and classifying tumors. The dataset was rigorously balanced and augmented through techniques like image rotation and noise addition, ensuring robustness and generalization.\n\nIn our experience, adhering to these best practices enabled the oncology imaging project to achieve a 12% improvement in classification accuracy and a 20% reduction in training time compared to traditional methods of dataset creation. The model's enhanced performance was attributed to both the structured label hierarchy and the comprehensive metadata utilized.\n\nCreating high-quality training datasets is not merely a preliminary step in the AI development process but a strategic necessity. Ensuring datasets are well-structured, balanced, and enriched with metadata not only improves model performance but also ensures compliance with regulatory standards. As AI continues to evolve, the emphasis on sophisticated data handling practices will be critical in developing reliable and effective AI systems.\n\nIn our opinion, advancing these best practices will be instrumental for enterprises seeking to harness AI's full potential, particularly in sectors where data precision and compliance are vital. By focusing on the quality and integrity of their training datasets, organizations can create solutions that are not only technologically advanced but also robust and reliable in real-world applications."
    },
    {
        "link": "https://medium.com/data-science/a-beginners-guide-to-building-high-quality-datasets-for-machine-learning-586a2ce7a565",
        "document": "Tools and techniques for data cleaning, visualization, augmentation, and synthetic data generation Finding the real source of complexity on data can look a lot like playing “data detective” until you find the “golden key” that unlocks the really useful insights. Photo by Michael Dziedzic on Unsplash Smart Data over Big Data. That’s the postulate of the “Data-Centric AI” paradigm. More than just “simply preprocessing” data, data scientists should a build a continuous and systematic practice of understanding and improving their datasets. This will ultimately drive our focus from blindly pursuing higher classification results by throwing ever more complex algorithms to the problem to a deep understanding of why the classification results are what they are, what is indeed the source of complexity of the problem, and how we can adjust the data so that classifiers can learn the problem better, thus increasing their performance. If you’re new to machine learning, this might seem a little bit daunting: “What are the best practices of building high-quality datasets and how to put them in place?” In this tutorial, we’ll go through a simple case of leveraging the Data-Centric AI paradigm to achieve high-quality data and improve our machine learning classification results. Following the mantra of Data-Centric AI — it’s all about the data—at no point will we delve into the model itself (honestly, it will be a simple decision tree). We’ll use the Pima Indians Diabetes Dataset , freely available on Kaggle (License: CC0: Public Domain). You’ll can also find all the code and additional materials at the Data-Centric AI Community GitHub.\n\nBefore we start curating our dataset, we need to understand the problem we’re trying to solve and the peculiarities of the data we’re working with. Thoroughly understanding our data characteristics, problem complexity, and use case domain is one of the first principles of Data-Centric AI. This will help us determine the next steps to move along your machine learning pipeline. When it comes to data profiling, there are several interesting open-source tools that you can explore: I’ve made a review of a few myself, including , , , , and . I currently use mostly ydata-profiling: I find it to be a top-notch tool for data practitioners that, rather than making us jump through pandas hoops to get the most of our data’s characteristics and visualizations, lets us do it all in a few lines of code. First, you’ll need to install ydata-profiling (better use a virtual environment for this — if you don’t know how, you can check this 2-min video, or this full tutorial if you’ve never worked in conda environments before): Then, we can get a complete overview of the data by saving a report of all the characteristics and visualizations we need to get started: The data report let’s us know immediately the overall characteristics of our data and highlights some warnings that we might need to take into account: The datasets contains 768 observations and 9 variables/features. While 8 are numeric, 1 is identified as categorical ( seems to be our target). There are no duplicate rows and apparently there are no missing values. Finally, some warning sare found among the the features. Moreover, several features have a large number of . Now it’s time to play data detective. are rather expected in biological features, but how about these values? Looking at some of the features highlighted (e.g., ), we can see that these values are quite far off from the overall distribution. And resorting to domain knowledge, these “0” values are actually nonsensical: a 0 value is OK for but for BMI, Glucose, Insulin, Blood Pressure, or Skin Thickness is invalid. YData Profiling Report: BMI feature, indicating that zero values are rather “off” from the distribution. We quickly realize what these zeros are encoding: missing data. For now we’ll get on fixing this issue, but a thorough process of EDA can comprehend a lot more. Check this Essential Guide to Exploratory Data Analysis to see what else you could uncover from your data.\n\nNow that we found that some columns have invalid zero values, we can start by handling the missing data issue on our dataset. Many machine learning models and scikit-learn estimators do not natively support missing values, so we need to handle these NaNs somehow before feeding our dataset to the estimator. First, lets mark these 0 values as NaN values: Now, we can use data imputation to replace the NaN observations with plausible replacement values. The “no free lunch” theorem tells us that there is no best solution for every situation — we should investigate how different solutions affect the complexity of our training data and determine what boosts our machine learning model the best. That’s actually another principle of Data-Centric AI: constant iteration and improvement. For now will use a very simple method — — to replace the zero values with the mean value of each feature. This is a very naive approach that is likely to create some undesired “spikes” in our distributions, but the goal is simply to showcase how to highlight and impute missing data, we can try better approaches later on: Now, we can then try out a very simple decision tree classifier and see what would be the baseline of our classification results. As a side note, decision trees can be extended to support missing values naturally, via surrogate splits or other methods. Indeed, in scikit-learn’s documentation seems like decision trees do have built-in support for missing values in some conditions in the current version ( ). However, as I was using version , I stumbled upon this error: Still, even if the NaN values are dealt with internally, training your models with missing data is not a good practice, as it will jeopardize the concepts that the model learns from messy and limited information. The classification results are not great. Keep in mind that we’re using a simple decision tree, but still… there is a significant different between the prediction for our target categories. Why is the classifier performing better for class “0” than class “1”?\n\nIf we were paying attention in Step 1 (maybe you’ve discovered it already), our target class, , is imbalanced. Maybe not at the point to trigger a warning in the default settings (the default threshold is ), but enough to still bias the classifier towards the majority class, neglecting the minority one. This is clear from the data visualization presented in the Profiling Report: YData Profiling Report: Outcome classes “0” and “1” are not equally represented. Classifiers will naturally be more biased towards the well-represented class, “0”, neglecting class “1”. Note that while missing data can be caused by several errors during data collection, transmission or storage, the class imbalance may reflect a natural characteristic of the domain: e.g., there are simply less patients diagnosed with diabetes in this medical center. Nevertheless, it is still important to act over the training data to guarantee that the model does not overlook the minority cases: in fact, that’s what we’re trying to predict more accurately. A false positive is bad since it will give the wrong information to a healthy patient the she has diabetes. But when additional tests are made, this will be just a “scare”. However, in this case, a false negative is worse. We’d be telling a patient with diabetes that everything is OK, she passes undiagnosed, and the disease progresses. One way to increase these numbers is by using data oversampling techniques. Data Oversampling is a popular technique among data practitioners to adjust the distributions of a dataset — i.e., the ratio between the existing classes or categories in data — thus mitigating the imbalanced data problem. And is just one of many interesting and useful applications of Synthetic Data. While synthetic data can have several interpretations — e.g., “fake data”, “dummy data”, “simulated data” — here we’re referring to “data-driven” synthetic data generation. In that sense, synthetic data is artificially generated that that preserves the characteristics of real data — its structure, statistical properties, dependencies, and correlations. There are a plethora of methods and open-source tools to generate synthetic data — , , , , and are just some that I’ve experimented with in the past. And again… there are “no free lunches”: choosing the most appropriate method will invariably depend on the objective for which the synthetic data is needed. To have a quick grasp of how synthetic data can be used for augmentation, we’ll leverage the package, and experiment with their Gaussian Mixture Models. First, we’ll need to install the package: And once that’s done, creating synthetic data is super straightforward: After we have our synthetic data, we can simply take out a subset of the newly generated minority class samples generated by sampling from the synthetic data, and add it to the training data to create a balanced (i.e., 50%-50%) distribution: Let’s see how that impacts the learning of our decision tree and its subsequent results: Note how such a simple modification of our training set resulted in a performance boost of our F-score in 10% and a significant improvement of the minority class sensitivity results (from 53% to 73%). Here lies the beauty of the Data-Centric AI paradigm: without ever touching our model parametrization, we’ve significantly improved the quality of our training set with very simple heuristics and standard techniques — imagine what we could do with more advanced strategies and specialized data preparation pipelines! Sure, class 0's recall suffered a bit, but ultimately, we need this model to be more sensitive than specific (i.e., detect better the positive class than the negative class) due to the particular constraints that we’re dealing with: disease diagnosis — again, another principle of Data-Centric AI: methods and results need to be evaluated based on the domain’s needs and constraints.\n\nThroughout this article, we’ve experimented with the Data-Centric AI paradigm with a very hands-on, practical use case. We started, as always, by understanding our data. We discovered, investigated, and solved particular data quality issues such as missing data, and improving our training data with synthetic data to overcome the imbalanced nature of the domain. Of course, for such a quick and simple case study, we focused on simple heuristics to get the job done, but the work of a data scientist never stop theres. How would the results change if we considered a different imputation method? How could we have achieved a better fit in our synthetic data generation? Should we have balanced both classes equally or perhaps increase the representation of the minority class even higher? Could some feature transformation or dimensionality reduction helped the classification result? Should we have removed some cofounding features? All of these questions seem unknowable at the start of any machine learning project. But as we start to measure and uncover the source of complexity in each dataset, we get better insights on what methods could improve classification results (a sort of “meta-learning” approach). And sure enough, the data needs to be manipulated and improved according to both the data characteristics and the ultimate goal of the project. Producing a pre-defined pipeline and treating data preparation like a one-fits-all solution is similar to flying blind. Instead, a skilled data scientist continuously plays data detective and tries to find the best techniques based on the clues that the data leaves out for us to catch. And it usually does. We just need to keep our eyes sharp! I hope you’ve enjoyed the tutorial, and as always, feedback, questions, and suggestions are much appreciated. Let me know what other topics you would like me to write about in the comments!"
    },
    {
        "link": "https://alphabold.com/the-complete-guide-to-preparing-your-data-for-ai-success",
        "document": "In our data-driven world, the success of any AI system depends heavily on the quality of the data it’s trained on. AI data preparation—the process of cleaning, organizing, and structuring raw data—is the essential foundation that enables AI models to generate accurate, reliable insights. Without properly prepared data, even the most advanced AI algorithms can fail, producing misleading results and missing key opportunities. Many companies already possess vast amounts of data, but the real challenge lies in making that data usable for AI. Organizations often struggle with the initial steps—cleaning out inaccuracies, organizing disparate data sources, and ensuring the data is structured in a way that AI systems can learn from effectively. This article will address these pain points, providing a comprehensive guide on how to prepare data for AI, from collection to labeling and beyond. The purpose of this guide is to walk you through the critical steps of AI data preparation, helping your organization transform raw data into a powerful resource for AI systems. By following best practices, you’ll set the stage for AI projects that deliver meaningful, actionable insights while avoiding the pitfalls of poor data quality.\n\nAt the core of any successful AI project is high-quality data. AI models learn from data, and the quality, accuracy, and relevance of that data determine how well the AI can identify patterns and make predictions. Whether it’s a machine learning algorithm for customer segmentation or a neural network for image recognition, clean and well-prepared data enables these systems to function efficiently and generate actionable insights. According to a study by MIT Sloan, nearly 85% of AI projects fail to deliver due to poor data quality. When data is inconsistent, incomplete, or poorly structured, the AI model is fed with flawed information, which leads to inaccurate outcomes. The success of AI models depends as much on the data they’re trained on as the algorithms themselves. In fact, experts estimate that data scientists spend 60-80% of their time on data preparation precisely because it’s so critical to performance.\n\nEnsuring that your AI system produces fair and unbiased results is crucial, especially as more businesses rely on AI for decision-making in areas like hiring, credit scoring, and law enforcement. AI systems are only as good as the data they are trained on, and if the data reflects historical biases, the AI model can perpetuate those biases, leading to discriminatory outcomes. Importance of Diverse, Unbiased Datasets: Training AI models on homogeneous or biased datasets can lead to unfair treatment of certain groups or inaccurate predictions. For example, an AI model trained only on data from one geographic region might not perform well when deployed globally.\n• Bias Audits: Regularly audit your data for potential biases in demographic factors like race, gender, and socioeconomic status.\n• Re-Sampling Data: Balance your data by ensuring that all relevant groups are equally represented, particularly in cases where some classes (e.g., minority groups) may be underrepresented.\n• Fairness Metrics: Use fairness metrics like demographic parity or equal opportunity to ensure your AI model’s predictions are equitable. Real-World Example: A notable instance of AI bias was Amazon’s AI hiring tool, which favored male candidates for technical positions. This was due to the AI being trained on historical resumes that reflected gender imbalances in the tech industry. The tool had to be scrapped after it was found to reinforce this bias, highlighting the need for careful bias detection in AI data preparation."
    },
    {
        "link": "https://medium.com/@miriam.santos/a-beginners-guide-to-building-high-quality-datasets-for-machine-learning-586a2ce7a565",
        "document": "Tools and techniques for data cleaning, visualization, augmentation, and synthetic data generation Finding the real source of complexity on data can look a lot like playing “data detective” until you find the “golden key” that unlocks the really useful insights. Photo by Michael Dziedzic on Unsplash Smart Data over Big Data. That’s the postulate of the “Data-Centric AI” paradigm. More than just “simply preprocessing” data, data scientists should a build a continuous and systematic practice of understanding and improving their datasets. This will ultimately drive our focus from blindly pursuing higher classification results by throwing ever more complex algorithms to the problem to a deep understanding of why the classification results are what they are, what is indeed the source of complexity of the problem, and how we can adjust the data so that classifiers can learn the problem better, thus increasing their performance. If you’re new to machine learning, this might seem a little bit daunting: “What are the best practices of building high-quality datasets and how to put them in place?” In this tutorial, we’ll go through a simple case of leveraging the Data-Centric AI paradigm to achieve high-quality data and improve our machine learning classification results. Following the mantra of Data-Centric AI — it’s all about the data—at no point will we delve into the model itself (honestly, it will be a simple decision tree). We’ll use the Pima Indians Diabetes Dataset , freely available on Kaggle (License: CC0: Public Domain). You’ll can also find all the code and additional materials at the Data-Centric AI Community GitHub.\n\nBefore we start curating our dataset, we need to understand the problem we’re trying to solve and the peculiarities of the data we’re working with. Thoroughly understanding our data characteristics, problem complexity, and use case domain is one of the first principles of Data-Centric AI. This will help us determine the next steps to move along your machine learning pipeline. When it comes to data profiling, there are several interesting open-source tools that you can explore: I’ve made a review of a few myself, including , , , , and . I currently use mostly ydata-profiling: I find it to be a top-notch tool for data practitioners that, rather than making us jump through pandas hoops to get the most of our data’s characteristics and visualizations, lets us do it all in a few lines of code. First, you’ll need to install ydata-profiling (better use a virtual environment for this — if you don’t know how, you can check this 2-min video, or this full tutorial if you’ve never worked in conda environments before): Then, we can get a complete overview of the data by saving a report of all the characteristics and visualizations we need to get started: The data report let’s us know immediately the overall characteristics of our data and highlights some warnings that we might need to take into account: The datasets contains 768 observations and 9 variables/features. While 8 are numeric, 1 is identified as categorical ( seems to be our target). There are no duplicate rows and apparently there are no missing values. Finally, some warning sare found among the the features. Moreover, several features have a large number of . Now it’s time to play data detective. are rather expected in biological features, but how about these values? Looking at some of the features highlighted (e.g., ), we can see that these values are quite far off from the overall distribution. And resorting to domain knowledge, these “0” values are actually nonsensical: a 0 value is OK for but for BMI, Glucose, Insulin, Blood Pressure, or Skin Thickness is invalid. YData Profiling Report: BMI feature, indicating that zero values are rather “off” from the distribution. We quickly realize what these zeros are encoding: missing data. For now we’ll get on fixing this issue, but a thorough process of EDA can comprehend a lot more. Check this Essential Guide to Exploratory Data Analysis to see what else you could uncover from your data.\n\nNow that we found that some columns have invalid zero values, we can start by handling the missing data issue on our dataset. Many machine learning models and scikit-learn estimators do not natively support missing values, so we need to handle these NaNs somehow before feeding our dataset to the estimator. First, lets mark these 0 values as NaN values: Now, we can use data imputation to replace the NaN observations with plausible replacement values. The “no free lunch” theorem tells us that there is no best solution for every situation — we should investigate how different solutions affect the complexity of our training data and determine what boosts our machine learning model the best. That’s actually another principle of Data-Centric AI: constant iteration and improvement. For now will use a very simple method — — to replace the zero values with the mean value of each feature. This is a very naive approach that is likely to create some undesired “spikes” in our distributions, but the goal is simply to showcase how to highlight and impute missing data, we can try better approaches later on: Now, we can then try out a very simple decision tree classifier and see what would be the baseline of our classification results. As a side note, decision trees can be extended to support missing values naturally, via surrogate splits or other methods. Indeed, in scikit-learn’s documentation seems like decision trees do have built-in support for missing values in some conditions in the current version ( ). However, as I was using version , I stumbled upon this error: Still, even if the NaN values are dealt with internally, training your models with missing data is not a good practice, as it will jeopardize the concepts that the model learns from messy and limited information. The classification results are not great. Keep in mind that we’re using a simple decision tree, but still… there is a significant different between the prediction for our target categories. Why is the classifier performing better for class “0” than class “1”?\n\nIf we were paying attention in Step 1 (maybe you’ve discovered it already), our target class, , is imbalanced. Maybe not at the point to trigger a warning in the default settings (the default threshold is ), but enough to still bias the classifier towards the majority class, neglecting the minority one. This is clear from the data visualization presented in the Profiling Report: YData Profiling Report: Outcome classes “0” and “1” are not equally represented. Classifiers will naturally be more biased towards the well-represented class, “0”, neglecting class “1”. Note that while missing data can be caused by several errors during data collection, transmission or storage, the class imbalance may reflect a natural characteristic of the domain: e.g., there are simply less patients diagnosed with diabetes in this medical center. Nevertheless, it is still important to act over the training data to guarantee that the model does not overlook the minority cases: in fact, that’s what we’re trying to predict more accurately. A false positive is bad since it will give the wrong information to a healthy patient the she has diabetes. But when additional tests are made, this will be just a “scare”. However, in this case, a false negative is worse. We’d be telling a patient with diabetes that everything is OK, she passes undiagnosed, and the disease progresses. One way to increase these numbers is by using data oversampling techniques. Data Oversampling is a popular technique among data practitioners to adjust the distributions of a dataset — i.e., the ratio between the existing classes or categories in data — thus mitigating the imbalanced data problem. And is just one of many interesting and useful applications of Synthetic Data. While synthetic data can have several interpretations — e.g., “fake data”, “dummy data”, “simulated data” — here we’re referring to “data-driven” synthetic data generation. In that sense, synthetic data is artificially generated that that preserves the characteristics of real data — its structure, statistical properties, dependencies, and correlations. There are a plethora of methods and open-source tools to generate synthetic data — , , , , and are just some that I’ve experimented with in the past. And again… there are “no free lunches”: choosing the most appropriate method will invariably depend on the objective for which the synthetic data is needed. To have a quick grasp of how synthetic data can be used for augmentation, we’ll leverage the package, and experiment with their Gaussian Mixture Models. First, we’ll need to install the package: And once that’s done, creating synthetic data is super straightforward: After we have our synthetic data, we can simply take out a subset of the newly generated minority class samples generated by sampling from the synthetic data, and add it to the training data to create a balanced (i.e., 50%-50%) distribution: Let’s see how that impacts the learning of our decision tree and its subsequent results: Note how such a simple modification of our training set resulted in a performance boost of our F-score in 10% and a significant improvement of the minority class sensitivity results (from 53% to 73%). Here lies the beauty of the Data-Centric AI paradigm: without ever touching our model parametrization, we’ve significantly improved the quality of our training set with very simple heuristics and standard techniques — imagine what we could do with more advanced strategies and specialized data preparation pipelines! Sure, class 0's recall suffered a bit, but ultimately, we need this model to be more sensitive than specific (i.e., detect better the positive class than the negative class) due to the particular constraints that we’re dealing with: disease diagnosis — again, another principle of Data-Centric AI: methods and results need to be evaluated based on the domain’s needs and constraints.\n\nThroughout this article, we’ve experimented with the Data-Centric AI paradigm with a very hands-on, practical use case. We started, as always, by understanding our data. We discovered, investigated, and solved particular data quality issues such as missing data, and improving our training data with synthetic data to overcome the imbalanced nature of the domain. Of course, for such a quick and simple case study, we focused on simple heuristics to get the job done, but the work of a data scientist never stop theres. How would the results change if we considered a different imputation method? How could we have achieved a better fit in our synthetic data generation? Should we have balanced both classes equally or perhaps increase the representation of the minority class even higher? Could some feature transformation or dimensionality reduction helped the classification result? Should we have removed some cofounding features? All of these questions seem unknowable at the start of any machine learning project. But as we start to measure and uncover the source of complexity in each dataset, we get better insights on what methods could improve classification results (a sort of “meta-learning” approach). And sure enough, the data needs to be manipulated and improved according to both the data characteristics and the ultimate goal of the project. Producing a pre-defined pipeline and treating data preparation like a one-fits-all solution is similar to flying blind. Instead, a skilled data scientist continuously plays data detective and tries to find the best techniques based on the clues that the data leaves out for us to catch. And it usually does. We just need to keep our eyes sharp! I hope you’ve enjoyed the tutorial, and as always, feedback, questions, and suggestions are much appreciated. Let me know what other topics you would like me to write about in the comments!"
    },
    {
        "link": "https://rws.com/artificial-intelligence/train-ai-data-services/blog/ensuring-high-quality-ai-data-the-foundation-of-ai-success",
        "document": "There's no shortage of stories of how AI can go wrong – with results that may be funny or serious, pleasing or horrifying. One thing these incidents often have in common is a problem with the data used to train the AI model. There are countless ways in which issues with AI training data – or the way it is prepared – can ultimately affect AI performance. This is why AI data quality is paramount: it can make or break your AI project. So let's dig into AI data quality. What are the characteristics of high-quality training data? What is the real-world impact of poor-quality data, and the benefits of using high-quality data? And how do you ensure that the data for your next AI project is of sufficient quality?\n\nWhat does high-quality AI data look like? Through our work collecting, generating, preparing and validating data for AI training, we see directly how nuanced the issue of data quality is. High-quality AI training data should be:\n• Accurate. As obvious as it seems that AI data should be up-to-date and error-free, it’s not so easy to be confident that this is the case. Badly labelled data is a common problem for AI projects, for example. Together with other data quality issues, inaccurate data can derail any AI initiative.\n• Consistent. The data used to train the AI model should be consistent in format, structure and quality, because inconsistencies can cause the AI model to pick up on differences that shouldn’t make a difference – and learn the wrong thing. Ensuring AI data consistency can be especially challenging because the datasets can be so large, but it's an absolute must.\n• Relevant. If you’re training an AI model to identify dogs in images, supplying it with images of cats won’t be relevant to the purpose and could confuse the model. Similarly, for a voice recognition model learning to understand English, audio data in French would not be relevant. Remember that the same data (or features of data) might be relevant for one purpose and irrelevant for another.\n• Balanced. AI training data should include an equal spread of positive and negative examples so that the model can learn from both. Well-balanced AI training data prevents the model from being biased towards a certain outcome, leading to better performance when faced with real-world data after training. Balance and bias in AI data are clearly related.\n• Comprehensive. Besides being relevant, AI training data should cover a broad spectrum of situations that the AI model might encounter when it's applied in the real world. The less comprehensive an AI dataset is for the intended purpose, the more likely the model is to respond inappropriately. For example, if a large language model is asked a question that its training data doesn't adequately cover, it will still generate an answer, but this answer may not be aligned with the real world. The wider the variety of relevant scenarios you train a model with, the better its performance will be.\n• Diverse and representative. When creating a comprehensive dataset, it's critical to ensure that the data fully represents the diversity of those who will ultimately use and be affected by the model. This starts with data acquisition: if you're only using American data or only have Americans generating data, don't be surprised if it doesn't adequately represent Norwegians. Neglecting diversity will usually lead to the AI model failing to adapt to novel situations or different user groups.\n• Bias-free. As explained when discussing the challenge of bias, bias can be so insidious that it takes specific focus and effort to catch and address it in AI datasets. Failure to do so risks the AI model producing biased decisions and predictions. It may prove impossible to remove bias entirely, but it’s important to do everything possible to minimize it, including ensuring that your AI training data is consistent, balanced, comprehensive, diverse and representative. The different dimensions of AI data quality can overlap or be closely related. The root cause of bias might be a lack of balance, diversity or even consistency. Or a lack of consistency might create a problem of relevance. Even so, they are distinct concepts, as the following examples show.\n\nHere are three classic examples of AI gone wrong, used to illustrate the different dimensions of AI data quality. An AI-powered facial recognition system fails to recognize dark-skinned people because it has been trained on a dataset comprising images of light-skinned individuals only. This limits its applicability and accuracy in real-world scenarios. Clearly the training data here is not:\n• Diverse and representative, since the skin tones used for training are too narrow a spectrum for the intended use of the system.\n• Bias-free, because of its lack of representative diversity. An AI-powered recruitment tool, designed to choose the top candidates from job applications in a fair and unbiased manner, is discovered to be ignoring quality female candidates. The data used to train the model was a decade's worth of recruitment data that reflected historical gender biases in recruitment, where male candidates were favoured over equally qualified female candidates. Here the training data is not:\n• Relevant to the system's purpose of equal opportunity employment, despite relevance in terms of job roles and industries. Because the data overwhelmingly reflected historically unequal employment opportunities, it was an unsuitable training set.\n• Balanced, because it doesn't include a balance of examples of equally qualified women and men being selected.\n• Bias-free, because of the historical gender bias reflected in the data. An AI-powered loans approval system is found to be denying loans to people from a specific location, even when they have a good credit history. Its training data is discovered to include a large number of past defaulters from that location. The AI has therefore learned to associate the location with poor creditworthiness, to the extent that this is overriding actual evidence of creditworthiness. Here the training data is not:\n• Relevant – or at least, not entirely relevant. Geographic location has been inadvertently linked to creditworthiness, when it isn't a reliable signifier in individual cases.\n• Balanced, because it lacks examples of creditworthy individuals from this specific location.\n• Bias-free, because the issue of relevance has created a bias.\n\nBenefits of getting it right from the start Because high-quality data is the foundation of any successful AI project, it’s well worth investing time and resources to get it right. For those who develop, use and are affected by the AI model, the benefits include:\n• More efficient development. Using high-quality AI data from the start eliminates or reduces the need to revisit data preparation to solve issues that occur during training. The time taken up front to ensure data quality is more than made up for by faster development and deployment of a reliable AI solution.\n• Improved performance and decision-making. AI models trained with high-quality data are more likely to make good predictions or uncover genuine insights. Whether in a business setting, a hospital looking for a diagnosis, a predictive policing scenario or a household planning a holiday, more reliable AI performance leads to better choices made by the people acting on the model's output, and better outcomes for those affected by their decisions.\n• Greater trust. More reliable and fair AI models naturally increase trust in AI, which is critical to the widespread adoption of AI technologies.\n• Cost savings. Curating and cleaning AI data to ensure its quality might initially seem costly, but it can lead to significant savings in the long run, starting with more efficient development (faster, therefore more cost-effective). By developing a more reliable model you also reduce the risk of costly corrections of errors after deployment, as well as the costs of regaining customer trust and repairing the reputation of your brand. Fundamentally, high-quality AI training data saves money by preventing expensive mistakes."
    }
]