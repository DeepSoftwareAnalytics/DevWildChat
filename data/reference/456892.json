[
    {
        "link": "https://medium.com/@maleeshadesilva21/preprocessing-steps-for-natural-language-processing-nlp-a-beginners-guide-d6d9bf7689c9",
        "document": "Machine Learning heavily relies on the quality of the data fed into it, and thus, data preprocessing plays a crucial role in ensuring the accuracy and efficiency of the model. In this article, we will discuss the main text preprocessing techniques used in NLP.\n\nIn this step, we will perform fundamental actions to clean the text. These actions involve transforming all the text to lowercase, eliminating characters that do not qualify as words or whitespace, as well as removing any numerical digits present.\n\nPython is a case sensitive programming language. Therefore, to avoid any issues and ensure consistency in the processing of the text, we convert all the text to lowercase.\n\nThis way, “Free” and “free” will be treated as the same word, and our data analysis will be more accurate and reliable.\n\nWhen building a model, URLs are typically not relevant and can be removed from the text data.\n\nFor removing URLs we can use ‘regex’ library.\n\nIt is essential to remove any characters that are not considered as words or whitespace from the text dataset.\n\nThese non-word and non-whitespace characters can include punctuation marks, symbols, and other special characters that do not provide any meaningful information for our analysis.\n\nIt is important to remove all numerical digits from the text dataset. This is because, in most cases, numerical values do not provide any significant meaning to the text analysis process.\n\nMoreover, they can interfere with natural language processing algorithms, which are designed to understand and process text-based information.\n\nTokenization is the process of breaking down large blocks of text such as paragraphs and sentences into smaller, more manageable units.\n\nIn this step, we will be applying word tokenization to split the data in the ‘Message’ column into words.\n\nBy performing word tokenization, we can obtain a more accurate representation of the underlying patterns and trends present in the text data.\n\nStopwords refer to the most commonly occurring words in any natural language.\n\nFor the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document. Therefore, removing stopwords can help us to focus on the most important information in the text and improve the accuracy of our analysis.\n\nOne of the advantages of removing stopwords is that it can reduce the size of the dataset, which in turn reduces the training time required for natural language processing models.\n\nVarious libraries such as ‘Natural Language Toolkit’ (NLTK), ‘spaCy’, and ‘Scikit-Learn’ can be used to remove stopwords.\n\nIn this example, we will use the NLTK library to remove stopwords in the ‘Message’ column of our dataset.\n\nWhat’s the difference between Stemming and Lemmatization?\n\nThere are various algorithms that can be used for stemming,\n\nLet’s take a look at how we can use ‘Porter Stemmer’ algorithm on our dataset.\n\nSome basic rules defined under the Porter Stemmer algorithm are,\n\nNext, let’s take a look at how we can implement Lemmatization for the same dataset.\n\nThe above code segments will produce outputs as shown below.\n\nNote that, we only use either Stemming or Lemmatization on our dataset based on the requirement.\n\nIn this article we discussed main preprocessing steps in building an NLP model, which include text cleaning, tokenization, stopword removal, and stemming/lemmatization. Implementing these steps can help improve model accuracy by reducing the noise in the text data and converting it into a structured format that can be easily analyzed by the model."
    },
    {
        "link": "https://stackoverflow.com/questions/46203023/how-can-i-make-my-python-nltk-pre-processing-code-more-efficient",
        "document": "The message you see comes up all too often when you work with a dataframe. It means pandas is not sure your operation is safe, but it is not sure that it is a problem, either. Figure out a work-around to be safe, but that's not the source of your performance problem.\n\nI didn't profile your code, but your function in particular is horrible. Why do you keep splitting, processing and re-joining? Tokenize once, filter the tokens, then join the final result if you must.\n\nIn addition to the redundant splitting-joining, you do it inefficiently by needlessly building a temporary array that you pass to . Use a generator instead (i.e., leave out the square brackets) and your performance should improve dramatically. For example, instead of you can write:\n\nI can't go into more detail because to be frank, your tokenization is a mess. When and how will you apply , after you've removed the punctuation? Also the line I rewrote above is (and was) trying to \"singularize\" indidivual letters, if I'm not mistaken. Think more carefully about what you're doing, work with tokens as I recommended (consider using -- but it's not as fast as a single ), and inspect the intermediate steps."
    },
    {
        "link": "https://guides.library.upenn.edu/penntdm/python/nltk",
        "document": "NLTK is a free, open-source library for advanced Natural Language Processing (NLP) in Python. It can help simplify textual data and gain in-depth information from input messages. Because of its powerful features, NLTK has been called “a wonderful tool for teaching and working in, computational linguistics using Python,” and “an amazing library to play with natural language.” The NLTK Google Colab Notebook version of the tutorial is also available if you would like to follow along. It is often the case that your input data is unstructured. Therefore, you will first need to preprocess it before doing any analysis. In this tutorial, we will be going over basic text preprocessing techniques that NLTK supports. Then, we will also be introducing some of the advanced features of NLKT:\n• Sentence detection and Tokenization: NLTK can break the input text into linguistically meaningful or basic units for future analyses.\n• Stop word removal: NLTK can remove the common words in English so that they would not distort tasks such as word frequency analysis.\n• Part-of-speech tagging: NLTK analyzes the grammatical role each word plays in a sentence.\n• Word frequencies: NLTK can give insights into word patterns in the text.\n• Lemmatization: NLTK can reduce inflected forms of a word into root words called a lemma.\n• Chunking and Chinking: NLTK allows you to identify or exclude phrases with specific patterns in a textual input.\n\nStop words are the most common words in a language. Examples of stop words are the, who, too, and is. We usually remove the stop words because they are not significant in many text mining tasks such as word frequency analysis. You can identify and remove stop words by using NLTK's list of stop words after tokenizing the text. nltk.download( ) nltk.corpus stopwords nltk.tokenize word_tokenize nltk.download( ) output = ( visit. His visit was to an apple farm while on a fruitarian diet.\" ) # Create a string object. By default, the function breaks sentences by periods. # Customize text or read files as needed # Get a list of stop words in English stop_words = (stopwords.words( )) sentences = word_tokenize(output) token sentences: token stop_words: (token)\n\nStemming refers to a text processing task that reduces words to their root. For example, the words \"adventure\", \"adventurer\", and \"adventurous\" share the root adventur.” Stemming allows us to reduce the complexity of the textual data so that we do not have to worry about the details of how each word was used. nltk.stem PorterStemmer nltk.tokenize word_tokenize nltk.download( ) output = (\"Please share with us the adventurous adventures of adventurer Tom\") # Create a string object. By default, the function breaks sentences by periods. # Customize text or read files as needed stemmer = PorterStemmer() words = word_tokenize(output) word words: (stemmer.stem(word)) Take a look at this example as introduced by Real Python: nltk.stem PorterStemmer nltk.tokenize word_tokenize nltk.download( ) output = ( \"The crew of the USS Discovery discovered many discoveries. Discovering is what explorers do\" ) # Create a string object. By default, the function breaks sentences by periods. stemmer = PorterStemmer() words = word_tokenize(output) word words: (stemmer.stem(word)) Note that the 'discovery' was stemmed to 'discoveri' when 'discovering' was stemmed to 'discov.' Why would it happen? Well, there are two major flaws of stemming: understemming and overstemming:\n• Understemming: understemming happens when two related words that should be reduced to the same root are not stemmed to the same root.\n• Overstemming: overstemming happens when two unrelated words were reduced to the same stem when they should not be. We need to be more careful when analyzing the stem words.\n\nLemmatization is the process of reducing inflected forms of a word while ensuring that the reduced form belongs to a language. This reduced form or root word is called a lemma. For example, “visits”, “visiting”, and “visited” are all forms of “visit” (lemma). Here, \"visit\" is the lemma. The inflection of a word also reduces numbers (car vs cars). Lemmatization is an important step because it helps you reduce the inflected forms of a word so that they can be analyzed in the text more efficiently. To perform lemmatization, use the NLTK function WordNetLemmatizer() on the tokenized object: nltk nltk.stem WordNetLemmatizer nltk.download( ) nltk.download( ) output = ( \"Apple's name was inspired by visits. His visits was to an apple farm while on a fruitarian diet.\" ) # Create a string object. By default, the function breaks sentences by periods. lemmatizer = WordNetLemmatizer() words = word_tokenize(output) word words: print(lemmatizer.lemmatize(word))\n\nUnlike tokenization, which allows you to identify every single word and sentence, chunking allows you to identify phrases in a textual input. Chunking allows you to extract a word or group of words that work as a unit to perform a grammatical function. The following examples are all examples of phrases: nltk nltk.tokenize word_tokenize nltk.tree Tree IPython.display display svgling nltk.download( ) output = ( \"Apple's name was inspired by visits. His visits was to an apple farm while on a fruitarian diet.\" ) # Create a string object. By default, the function breaks sentences by periods. # Customize text or read files as needed words = word_tokenize(output) tag = nltk.pos_tag(words) To perform chunking, you will have to first define a chunk grammar that tells Python which format of grammatical unit you would like to extract. The rule we defined means that, the pattern we are extracting is a noun phrase:\n• can have any number (*) of adjectives (<JJ>) Likewise, the rule \"NP: {<DT>*<JJ>*<NN>}\" means that the phrase is a noun phrase where:\n• starts with any number of (*) determiner (<DT>)\n• can have any number (*) of adjectives (<JJ>) Please refer to this diagram for more information:"
    },
    {
        "link": "https://nltk.org/book/ch03.html",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/39452842/text-pre-processing-with-nltk",
        "document": "What is df? a list of tweets? You maybe should consider cleaning the tweet one after the other and not as a list of tweets. It would be easier to have a function .\n\nnltk provides a TweetTokenizer to clean the tweets.\n\nthe \"re\" package provides good solutions to use regex.\n\nI advice you to create a variable for an easier use of\n\nDeleting stopwords in a sentence is described [here] (Stopword removal with NLTK): clean_wordlist = [i for i in sentence.lower().split() if i not in stopwords]\n\nIf you want to use regex (with the re package), you can\n• None create a regex pattern composed of all the stopwords (out of the tweet_clean function): \n\n (?siu) for multiline, ignorecase, unicode\n• None and use this pattern to clean any string\n\nTo remove 1 words tweet you could only keep the one longest than 1 word:"
    },
    {
        "link": "https://huggingface.co/docs/transformers/en/tasks/question_answering",
        "document": "and get access to the augmented documentation experience\n\nQuestion answering tasks return an answer given a question. If you’ve ever asked a virtual assistant like Alexa, Siri or Google what the weather is, then you’ve used a question answering model before. There are two common types of question answering tasks:\n• Extractive: extract the answer from the given context.\n• Abstractive: generate an answer from the context that correctly answers the question.\n\nThis guide will show you how to:\n• Finetune DistilBERT on the SQuAD dataset for extractive question answering.\n• Use your finetuned model for inference.\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:\n\nStart by loading a smaller subset of the SQuAD dataset from the 🤗 Datasets library. This’ll give you a chance to experiment and make sure everything works before spending more time training on the full dataset.\n\nSplit the dataset’s split into a train and test set with the train_test_split method:\n\nThen take a look at an example:\n\nsquad[ ][ ] { : { : [ ], : [ ]}, : 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.' , : , : 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?' , : }\n\nThere are several important fields here:\n• : the starting location of the answer token and the answer text.\n• : background information from which the model needs to extract the answer.\n\nThe next step is to load a DistilBERT tokenizer to process the and fields:\n\nThere are a few preprocessing steps particular to question answering tasks you should be aware of:\n• Some examples in a dataset may have a very long that exceeds the maximum input length of the model. To deal with longer sequences, truncate only the by setting .\n• Next, map the start and end positions of the answer to the original by setting .\n• With the mapping in hand, now you can find the start and end tokens of the answer. Use the method to find which part of the offset corresponds to the and which corresponds to the .\n\nHere is how you can create a function to truncate and map the start and end tokens of the to the :\n\nTo apply the preprocessing function over the entire dataset, use 🤗 Datasets map function. You can speed up the function by setting to process multiple elements of the dataset at once. Remove any columns you don’t need:\n\nNow create a batch of examples using DefaultDataCollator. Unlike other data collators in 🤗 Transformers, the DefaultDataCollator does not apply any additional preprocessing such as padding.\n\nIf you aren’t familiar with finetuning a model with the Trainer, take a look at the basic tutorial here! You’re ready to start training your model now! Load DistilBERT with AutoModelForQuestionAnswering: At this point, only three steps remain:\n• Define your training hyperparameters in TrainingArguments. The only required parameter is which specifies where to save your model. You’ll push this model to the Hub by setting (you need to be signed in to Hugging Face to upload your model).\n• Pass the training arguments to Trainer along with the model, dataset, tokenizer, and data collator. Once training is completed, share your model to the Hub with the push_to_hub() method so everyone can use your model: If you aren’t familiar with finetuning a model with Keras, take a look at the basic tutorial here! To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters: To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters: Then you can load DistilBERT with TFAutoModelForQuestionAnswering: Convert your datasets to the format with prepare_tf_dataset(): Configure the model for training with : The last thing to setup before you start training is to provide a way to push your model to the Hub. This can be done by specifying where to push your model and tokenizer in the PushToHubCallback: Finally, you’re ready to start training your model! Call with your training and validation datasets, the number of epochs, and your callback to finetune the model: Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n\nEvaluation for question answering requires a significant amount of postprocessing. To avoid taking up too much of your time, this guide skips the evaluation step. The Trainer still calculates the evaluation loss during training so you’re not completely in the dark about your model’s performance.\n\nIf you have more time and you’re interested in how to evaluate your model for question answering, take a look at the Question answering chapter from the 🤗 Hugging Face Course!\n\nGreat, now that you’ve finetuned a model, you can use it for inference!\n\nCome up with a question and some context you’d like the model to predict:\n\nThe simplest way to try out your finetuned model for inference is to use it in a pipeline(). Instantiate a for question answering with your model, and pass your text to it:\n\nYou can also manually replicate the results of the if you’d like:"
    },
    {
        "link": "https://medium.com/@lokaregns/question-answering-with-hugging-face-transformers-a-beginners-guide-487ae1a91b9a",
        "document": "What We Will Cover in This Blog\n• Review of what Question Answering is and where we use it.\n• How to use Hugging face Transformers for Question Answering with just a few lines of code.\n\nThe main focus of this blog, using a very high level interface for transformers which is the Hugging face pipeline. Using this interface you will see that we can generate answers from given question and corpus of text with just 1 or 2 lines of code.\n\nWhat is Question Answering in NLP?\n\nQuestion Answering (QA) in Natural Language Processing (NLP) is a task that involves using computational methods to automatically answer questions posed in natural language. This task involves identifying the relevant information within a large corpus of text and extracting the answer that best addresses the question. It involves a combination of various NLP techniques, such as information retrieval, information extraction, and text understanding, to accurately answer questions based on the context and meaning of the input text. QA systems are widely used in applications such as customer service, knowledge management, and education, providing quick and accurate answers to a wide range of questions.\n\nQuestion Answering (QA) in NLP involves identifying the answer to a question from a large corpus of text, such as a document or a collection of documents. The process typically involves the following steps:\n• Question Analysis: The question is analyzed to identify its type (e.g., factual, definition, comparison, etc.) and the information required to find the answer (e.g., named entities, keywords, etc.).\n• Information Retrieval: The corpus is searched to retrieve the relevant documents or text snippets that contain the information needed to answer the question.\n• Candidate Answer Generation: Possible answers are extracted from the retrieved text snippets and candidate answers are generated.\n• Answer Selection: The candidate answers are evaluated against the question and the best answer is selected based on various criteria, such as relevance, accuracy, and confidence.\n\nThe process can be aided by techniques such as natural language processing (NLP), information retrieval (IR), and machine learning (ML), among others. The performance of QA systems can be improved by using large pre-trained language models, such as BERT or GPT, to encode the context of the question and candidate answers.\n\nUse of the Question Answering\n• Customer Service: QA systems can be used to automate customer service by providing instant answers to common questions.\n• Knowledge Management: QA systems can be used to manage and retrieve information from large knowledge bases, such as company FAQs and product manuals.\n• Education: QA systems can be used to provide students with instant answers to their questions and help them learn more effectively.\n• Healthcare: QA systems can be used to provide healthcare professionals with instant answers to clinical questions, improving patient outcomes and reducing diagnostic errors.\n• News and Media: QA systems can be used to provide quick answers to factual questions about current events, politics, sports, and more.\n• Legal: QA systems can be used to provide lawyers and legal researchers with instant answers to legal questions, improving the speed and efficiency of their work.\n\nOverall, QA systems aim to improve decision-making, increase productivity, and provide fast and accurate answers to questions, making it a valuable tool in a wide range of industries and domains.\n\nAdvantages of using Transformers for Question Answering\n\nTransformers are widely used for Question Answering (QA) for several reasons:\n• Contextual Embeddings: Transformers can capture the context of a question and candidate answer, allowing them to better understand the relationship between the two and make more accurate predictions.\n• Pre-training: Transformers can be pre-trained on large amounts of text data, giving them a strong understanding of language and enabling them to perform well on a wide range of tasks, including QA.\n• Attention Mechanisms: Transformers use attention mechanisms, which allow them to focus on specific parts of the input when making predictions, leading to improved performance on QA tasks.\n• Transfer Learning: Transformers can be fine-tuned for specific QA tasks, allowing for transfer learning from a pre-trained model and reducing the amount of labeled data needed for training.\n• Performance: Transformers have demonstrated state-of-the-art performance on a wide range of QA tasks, outperforming traditional methods in terms of accuracy and speed.\n\nOverall, the use of transformers for QA allows for more accurate, fast, and scalable solutions compared to traditional methods, making it a popular choice in many applications.\n\nLet’s begin with installing transformers as we regularly do.\n\nThe command is used to install the package, which provides access to state-of-the-art Transformer-based models for NLP tasks, including QA.\n\nOnce the package is installed, you can import and use the Transformer-based models in your own projects.\n\nThe line “from transformers import pipeline” imports the pipeline module from the Transformers library, which provides an easy-to-use interface for common NLP tasks, including QA. As we have not explicitly supplied any model, by default it will select distilbert.\n\nThe “pipeline” function is then used to load a pre-trained QA pipeline by passing in the argument “question-answering”. This pipeline is a combination of a pre-trained language model, such as BERT or GPT, and a classifier that predicts the likelihood of a candidate answer being the correct one.\n\nOnce the QA pipeline is loaded, it can be used to answer questions by passing a question and a context (e.g., a document or a paragraph) to the pipeline, and it will return the answer with the highest predicted probability.\n\nThis code provides a simple and convenient way to get started with Question Answering using pre-trained models, without the need to implement and train the models from scratch.\n\nLet’s try with simple context and question.\n\nThe code above is using the Question Answering (QA) pipeline loaded in the previous code to answer a question.\n• Context: The variable “ctx” stores the context in which the question will be answered. In this case, it is a sentence stating “My name is Ganesh and I am studying Data Science”.\n• Question: The variable “que” stores the question to be answered. In this case, it is “What is Ganesh studying?”\n• QA Pipeline: The QA pipeline stored in the variable “qa” is called with the context and question as input. The “context” argument is set to “ctx” and the “question” argument is set to “que”.\n• Output: The QA pipeline will return the answer with the highest predicted probability, along with the start and end position of the answer in the context, and the confidence score.\n\nThe code demonstrates how to use the pre-trained QA pipeline to answer questions in a given context. The pipeline uses the context and question to generate candidate answers, and then uses the pre-trained model to predict the likelihood of each candidate answer being the correct one. The answer with the highest predicted probability is returned as the final answer.\n\nThe output is the result of using the Question Answering (QA) pipeline to answer the question.\n• Score: The ‘score’ field represents the confidence score of the predicted answer, with a value between 0 and 1. A higher score indicates a higher level of confidence in the answer. In this case, the score is 0.99887, which is close to 1, indicating a high level of confidence in the answer.\n• Start: The ‘start’ field represents the starting position of the answer in the context. In this case, the start position is 36, which is the first character of the word “Data”.\n• End: The ‘end’ field represents the ending position of the answer in the context. In this case, the end position is 48, which is the last character of the word “Science”.\n• Answer: The ‘answer’ field represents the final answer to the question. In this case, the answer is “Data Science”, which is the correct answer to the question “What is Ganesh studying?”\n\nThe output provides the answer to the question, along with the confidence score, start and end positions in the context, allowing the user to assess the reliability of the answer and use it in further applications as needed.\n\nFor the next portion of the script we are going to use a snippet from Wikipedia's page of Elon Musk.\n\ncontext = '''Elon Reeve Musk FRS (/ˈiːlɒn/ EE-lon; born June 28, 1971) is a business magnate and investor. He is the founder, CEO and chief engineer of SpaceX; \n\n angel investor, CEO and product architect of Tesla, Inc.; owner and CEO of Twitter, Inc.; founder of The Boring Company; co-founder of Neuralink \n\n and OpenAI; and president of the philanthropic Musk Foundation. With an estimated net worth of around $175 billion as of February 3, 2023, primarily \n\n from his ownership stakes in Tesla and SpaceX,[4][5] Musk is the second-wealthiest person in the world, according to both the Bloomberg Billionaires \n\n Index and Forbes's real-time billionaires list.[6][7]\n\n Musk was born in Pretoria, South Africa, and briefly attended at the University of Pretoria before moving to Canada at age 18, acquiring citizenship \n\n through his Canadian-born mother. Two years later, he matriculated at Queen's University and transferred to the University of Pennsylvania, \n\n where he received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University. After two days, he \n\n dropped out and with his brother Kimbal, co-founded the online city guide software company Zip2. In 1999, Zip2 was acquired by Compaq for $307 million \n\n and Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal, which eBay acquired for $1.5 billion in 2002.\n\n With $175.8 million, Musk founded SpaceX in 2002, a spaceflight services company. In 2004, he was an early investor in the electric vehicle manufacturer \n\n Tesla Motors, Inc. (now Tesla, Inc.). He became its chairman and product architect, assuming the position of CEO in 2008. In 2006, he helped create \n\n SolarCity, a solar energy company that was later acquired by Tesla and became Tesla Energy. In 2015, he co-founded OpenAI, a nonprofit artificial \n\n intelligence research company. The following year, he co-founded Neuralink—a neurotechnology company developing brain–computer interfaces—and The Boring \n\n Company, a tunnel construction company. Musk has also proposed a hyperloop high-speed vactrain transportation system. In 2022, his acquisition of Twitter \n\n for $44 billion was completed.'''\n\n\n\n\n\nYou can see that, the answer is correct. But, the QA model is not a generative model, it only selects an answer from the context and returns it. Additionally, it can only handle one continuous string as the context, so if the answer is present in multiple locations in the context, the model will only return the answer from one location. These limitations should be kept in mind when using the QA model.\n\nHere also answer is correct.\n\nAs we know Elon Musk has invested in multiple companies, but our model only lists two because the answer is located in multiple contexts.\n\nLet’s ask a question for which the answer is not present in the context.\n\nAs we can observe, the model provided an incorrect answer because the information was not present in the context. It’s important to note that even if the answer is not available in the context, the model may still provide an answer. However, it may not be accurate. One can verify the accuracy of the model’s response by checking its confidence score. If the score is low, it’s advisable to verify the answer before accepting it.\n\nThat’s it for now.\n\nBeginner's section is completed. Next I plan to write additional blogs on Transformers where I will demonstrate how to fine-tune transformer models on custom datasets, as well as how to create transformer models from scratch and train them on custom datasets.\n\nIf you enjoyed this post, be sure to check out some of my other blog posts for more insights and information. I’m constantly exploring new topics and writing about my findings, so there’s always something new and interesting to discover."
    },
    {
        "link": "https://digitalocean.com/community/tutorials/how-to-train-question-answering-machine-learning-models",
        "document": "Question-Answering Models are machine or deep learning models that can answer questions given some context, and sometimes without any context (e.g. open-domain QA). They can extract answer phrases from paragraphs, paraphrase the answer generatively, or choose one option out of a list of given options, and so on. It all depends on the dataset it was trained on (e.g. SQuAD, CoQA, etc.) or the problem it was trained for, or to some extent the neural network architecture. So, for example, if you feed this paragraph (context) to your model trained to extract answer phrases from context, and ask a question like “What is a question-answering model?”, it should output the first line of this paragraph.\n\nSuch models need to understand the structure of the language, have a semantic understanding of the context and the questions, have an ability to locate the position of an answer phrase, and much more. So without any doubt, it is difficult to train models that perform these tasks. Fortunately, the concept of attention in neural networks has been a lifesaver for such difficult tasks. Since its introduction for sequence modeling tasks, lots of RNN networks with sophisticated attention mechanisms like R-NET, FusionNet, etc. have shown great improvement in QA tasks. However, a completely new neural network architecture based on attention, specifically self-attention, called Transformer, has been the real game-changer in NLP. Here I will discuss one such variant of the Transformer architecture called BERT, with a brief overview of its architecture, how it performs a question answering task, and then write our code to train such a model to answer COVID-19 related questions from research papers.\n• Basic Knowledge of NLP: Familiarity with Natural Language Processing concepts, including tokenization, embeddings, and transformers.\n• Python Programming: Understanding of Python, including libraries such as NumPy, pandas, and scikit-learn.\n• Experience with PyTorch or TensorFlow: Basic understanding of deep learning frameworks like PyTorch or TensorFlow, as BERT models are typically implemented in these.\n• Understanding of BERT: Awareness of the BERT (Bidirectional Encoder Representations from Transformers) architecture, including its use of attention mechanisms and pre-training/fine-tuning processes.\n• Access to Data and Compute Resources: A dataset suitable for question-answering tasks (like SQuAD) and access to GPU resources for efficient training.\n\nBefore jumping to BERT, let us understand what language models are and how Transformers come into the picture.\n\nA language model is a probabilistic model that learns the probability of the occurrence of a sentence, or sequence of tokens, based on the examples of text it has seen during training. For example:\n\nThese language models, if big enough and trained on a sufficiently large dataset, can start understanding any language and its intricacies really well. Traditionally RNNs were used to train such models due to the sequential structure of language, but they are slow to train (due to sequential processing of each token) and sometimes difficult to converge (due to vanishing/exploding gradients).\n\nHowever, different variants of Transformers, with their ability to process tokens in parallel and impressive performance due to self-attention mechanism and different pre-training objectives, have made training large models (and sometimes really really large models), which understand natural language really well, possible. Different Transformer-based language models, with small changes in their architecture and pre-training objective, perform differently on different types of tasks. BERT (Bidirectional Encoder Representations from Transformers) is one such model. BERT has been trained using the Transformer Encoder architecture, with Masked Language Modelling (MLM) and the Next Sentence Prediction (NSP) pre-training objective.\n\nNow that we know what BERT is, let us go through its architecture and pre-training objectives briefly. BERT uses Transformer Encoder from the original Transformer paper. An Encoder has a stack of encoder blocks (where the output of one block is fed as the input to the next block), and each encoder block is composed of two neural network layers. First there is a self-attention layer (which is the magic operation that makes transformers so powerful) and then a simple feed-forward layer. After each layer, there is a residual connection and a layer normalization operation as shown in the figure below.\n\nOne Encoder Block (source). Here X1, X2 are input vectors. One vector for each token.\n\nSo, for each encoder layer, the number (with a maximum limit of 512) of input vectors and output vectors is always the same. And before the first encoder layer, the input vector for each token is obtained by adding token embedding, positional embedding, and segment embedding. These vectors are processed in parallel inside each encoder layer using matrix multiplications, and the obtained output vectors are fed to the next encoder block. After being processed sequentially through N such blocks, the obtained output vectors start understanding natural language very well.\n\nThis is a very compressed overview of the BERT architecture, focusing only on the ideas we need to understand the rest of the blog. For a more elaborate discussion on how different operations happen in each layer, multi-head self-attention, and understanding parallel token processing, please check out Jay Alammar’s Blog.\n\nA pre-training objective is a task on which a model is trained before being fine-tuned for the end task. GPT models are trained on a Generative Pre-Training task (hence the name GPT) i.e. generating the next token given previous tokens, before being fine-tuned on, say, SST-2 (sentence classification data) to classify sentences.\n\nSimilarly, BERT uses MLM and NSP as its pre-training objectives. It uses a few special tokens like CLS, SEP, and MASK to complete these objectives. We will see the use of these tokens as we go through the pre-training objectives. But before proceeding, we should know that each tokenized sample fed to BERT is appended with a CLS token in the beginning and the output vector of CLS from BERT is used for different classification tasks. Now let’s start with MLM.\n\nIn the MLM objective, a percentage of tokens are masked i.e. replaced with special token MASK, and the model is asked to predict the correct token in place of MASK. To accomplish this a masked language model head is added over the final encoder block, which calculates a probability distribution over the vocabulary only for the output vectors (output from the final encoder block) of MASK tokens. And in NSP, the two sentences tokenized and the SEP token appended at their end are concatenated and fed to BERT. The output vector of the CLS token is then used to calculate the probability of whether the second sentence in the pair is the subsequent sentence in the original document. For both the objectives, standard cross-entropy loss with AdamW optimizer is used to train the weights.\n\nThe above pre-training objectives are really powerful in capturing the semantics of the natural language in comparison to other pre-training objectives, e.g. the generative pre-training objective. Hence, many models with similar or slightly tweaked pre-training objectives, with more or less the same architecture as BERT, have been trained to achieve SOTA results on many NLP tasks. RoBERTA, SpanBERT, DistilBERT, ALBERT etc. are a few of them.\n\nAfter being trained on such pre-training objectives, these models are fine-tuned on special tasks like question answering, name entity recognition, etc. Here we will see how BERT is trained on a Question-Answering objective.\n\nAs mentioned before, the QA task can be framed in different ways. Here I will be focusing on context-based question answering, where questions are asked from a given paragraph. SQuAD is a popular dataset for this task which contains many paragraphs of text, different questions related to the paragraphs, their answers, and the start index of answers in the paragraph. There are two versions of SQuAD, SQuAD1.1 and SQuAD2.0, with the main difference being that SQuAD2.0 contains over 50,000 unanswerable questions that look similar to the answerable ones. So to do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. Both datasets are publicly available and can be downloaded from here. Here, I will be using SQuAD2.0.\n\nTo perform the QA task we add a new question-answering head on top of BERT, just the way we added a masked language model head for performing the MLM task. The purpose of this question-answering head is to find the start token and end token of an answer for a given paragraph, for example:\n\nEverything that comes in between, including the start and end token, is considered an answer.\n\nInside the question answering head are two sets of weights, one for the start token and another for the end token, which have the same dimensions as the output embeddings. The output embeddings of all the tokens are fed to this head, and a dot product is calculated between them and the set of weights for the start and end token, separately. In other words, the dot product between the start token weight and output embeddings is taken, and the dot product between the end token weight and output embeddings is also taken. Then a softmax activation is applied to produce a probability distribution over all the tokens for the start and end token set (each set also separately). The tokens with the maximum probability are chosen as the start and end token, respectively. In this process, it may so happen that the end token could appear before the start token. In that case an empty string is output as the predicted answer. The figures below should make the operations clearer.\n\nIn popular implementations, this head is implemented as a feed-forward layer that takes the input of the same dimension as the BERT output embeddings and returns a two-dimensional vector, which is then fed to the softmax layer. The complete BERT SQuAD model is finetuned using cross-entropy loss for the start and end tokens.\n\nWe will be using Hugging Face’s Transformers library for training our QA model. We will also be using BioBERT, which is a language model based on BERT, with the only difference being that it has been finetuned with MLM and NSP objectives on different combinations of general & biomedical domain corpora. Different domains have specific jargons and terms which occur very rarely in standard English, and if they occur it could mean different things, or imply different contexts. Hence, models like BioBERT, LegalBERT, etc. have been trained to learn such nuances of the domain-specific text so that domain-specific NLP tasks could be performed with better accuracy.\n\nHere we aim to use the QA model to extract relevant information from COVID-19 research literature. Hence, we will be finetuning BioBERT using Hugging Face’s Transformers library on SQuADv2 data.\n\nIn the examples section of the Transformers repository, Hugging Face has already provided a script, , to train the QA model on SQuAD data. This script can be run easily using the below command.\n\nOne can understand most of the parameters from their names. For more details on the parameters and an exhaustive list of parameters that can be adjusted, one can refer to the github repo for the code examples.\n\nUsing this script, the model can be easily finetuned to perform the QA task. However, running this script is RAM heavy, because tries to process the complete SQuAD data at once and requires more than 12GB of RAM. So, I have modified and added a new function named which can process SQuAD data in batches. You can check out these methods below.\n\nBasically, the added modifications run the same method on mini-batches of data and save the created features in a folder. One can define the minibatch size by adding the below line at line 660 in , and providing an argument in the command I mentioned above.\n\nThe modified can be downloaded from here.\n\nOur trained model was able to achieve an F1 score of 70 and an Exact Match of 67.8 on SQuADv2 data after 4 epochs, using the default hyperparameters mentioned in the Now let us see the performance of this trained model on some research articles from the COVID-19 Open Research Dataset Challenge (CORD-19). Below are examples of some sample texts obtained from research articles, questions asked on the sample text, and the predicted answer.\n\nContext: Conclusion : Our study firstly demonstrated the regional disparity of COVID - 19 in Chongqing municipality and further thoroughly compared the differences between severe and non - severe patients. The 28 - day mortality of COVID - 19 patients from 3 designed hospitals of Chongqing is 1. 5 %, lower than that of Hubei province and mainland China including Hubei province. However, the 28 - mortality of severe patients was relatively high, with much higher when complications occurred. Notably, the 28 - mortality of critically severe patients complicated with severe ARDS is considerably as high as 44. 4 %. Therefore, early diagnosis and intensive care of critically severe COVID - 19 cases, especially those combined with ARDS, will be considerably essential to reduce mortality.\n\n Question: What is the mortality for ARDS?\n\n Predicted Answer: 44.4 % Context: This is a retrospective study from 3 patients with 2019 - nCoV infection admitted to Renmin Hospital of Wuhan University, a COVID - 2019 designated hospital in Wuhan, from January 31 to February 6, 2020. All patients were diagnosed and classified based on the Diagnosis and Treatment of New Coronavirus Pneumonia ( 6th edition ) published by the National Health Commission of China4. We recorded the epidemiological history, demographic features, clinical characteristics, symptoms and signs, treatment and clinical outcome in detail. Additionally, we found that the proportion of probiotics was significantly reduced, such as Bifidobacterium, Lactobacillus, and Eubacterium, and the proportion of conditioned pathogenic bacteria was significantly increased, such as Corynebacterium of Actinobacteria and Ruthenibacterium of Firmicutes. Notably, all patients died.\n\n _Question:_What is the mortality of ARDS caused by viral infections?\n\n Predicted Answer: all patients died. Context: Meanwhile, numbers of patients with COVID - 19 infection had chronic comorbidities, mainly hypertension, diabetes and cardiovascular disease, which is similar to MERS - COV population. Those results indicate that older adult males with chronic underlying disease might be more susceptibility to COVID - 19 or MERS - COV… CC - BY - NC - ND 4. 0 In terms of laboratory testing, reduced lymphocytes and increased CRP were found in both COVID - 19 and MERS - COV patients. This result indicates that COVID - 19 might be associated with cellular immune response, mainly act on lymphocytes like MERS - COV does $[ 48 ]$. The cells infected by viruses induce the release of numbers of pro - inflammatory cytokines and inflammation storm in the body. Moreover, increased cytokines might make damage to related organs such as liver $[ 49 ]$. Our results demonstrated that abnormal value of AST was found in MERS - COV population, but not in COVID - 19 population. The possible reason is that the follow - up time of COVID - 19 population was too short, and the liver.\n\n Question: What kind of cytokines play a major role in host response?\n\n Predicted Answer: pro - inflammatory\n\nIn this article we briefly went through the architecture of BERT, saw how BERT performs on a question-answering task, trained a version of the BERT model (Bio-BERT) on SQuADv2 data using (which reduces the RAM usage), and saw the performance of the trained model on texts from COVID-related research articles. The performance of such models depends to a large extent on the context and relevant question fed to the model. Here, contexts were manually extracted from articles and fed to the model. In later articles we will see a deep learning based approach to find the most appropriate paragraph from research articles, given a specific question."
    },
    {
        "link": "https://huggingface.co/learn/nlp-course/en/chapter7/7",
        "document": "Time to look at question answering! This task comes in many flavors, but the one we’ll focus on in this section is called extractive question answering. This involves posing questions about a document and identifying the answers as spans of text in the document itself.\n\nWe will fine-tune a BERT model on the SQuAD dataset, which consists of questions posed by crowdworkers on a set of Wikipedia articles. This will give us a model able to compute predictions like this one:\n\nThis is actually showcasing the model that was trained and uploaded to the Hub using the code shown in this section. You can find it and double-check the predictions here.\n\nThe dataset that is used the most as an academic benchmark for extractive question answering is SQuAD, so that’s the one we’ll use here. There is also a harder SQuAD v2 benchmark, which includes questions that don’t have an answer. As long as your own dataset contains a column for contexts, a column for questions, and a column for answers, you should be able to adapt the steps below.\n\nAs usual, we can download and cache the dataset in just one step thanks to :\n\nWe can then have a look at this object to learn more about the SQuAD dataset:\n\nIt looks like we have everything we need with the , , and fields, so let’s print those for the first element of our training set:\n\nThe and fields are very straightforward to use. The field is a bit trickier as it comports a dictionary with two fields that are both lists. This is the format that will be expected by the metric during evaluation; if you are using your own data, you don’t necessarily need to worry about putting the answers in the same format. The field is rather obvious, and the field contains the starting character index of each answer in the context.\n\nDuring training, there is only one possible answer. We can double-check this by using the method:\n\nFor evaluation, however, there are several possible answers for each sample, which may be the same or different:\n\nWe won’t dive into the evaluation script as it will all be wrapped up by a 🤗 Datasets metric for us, but the short version is that some of the questions have several possible answers, and this script will compare a predicted answer to all the acceptable answers and take the best score. If we take a look at the sample at index 2, for instance:\n\nwe can see that the answer can indeed be one of the three possibilities we saw before.\n\nLet’s start with preprocessing the training data. The hard part will be to generate labels for the question’s answer, which will be the start and end positions of the tokens corresponding to the answer inside the context.\n\nBut let’s not get ahead of ourselves. First, we need to convert the text in the input into IDs the model can make sense of, using a tokenizer:\n\nAs mentioned previously, we’ll be fine-tuning a BERT model, but you can use any other model type as long as it has a fast tokenizer implemented. You can see all the architectures that come with a fast version in this big table, and to check that the object you’re using is indeed backed by 🤗 Tokenizers you can look at its attribute:\n\nWe can pass to our tokenizer the question and the context together, and it will properly insert the special tokens to form a sentence like this:\n\nThe labels will then be the index of the tokens starting and ending the answer, and the model will be tasked to predicted one start and end logit per token in the input, with the theoretical labels being as follow:\n\nIn this case the context is not too long, but some of the examples in the dataset have very long contexts that will exceed the maximum length we set (which is 384 in this case). As we saw in Chapter 6 when we explored the internals of the pipeline, we will deal with long contexts by creating several training features from one sample of our dataset, with a sliding window between them.\n\nTo see how this works using the current example, we can limit the length to 100 and use a sliding window of 50 tokens. As a reminder, we use:\n• to set the maximum length (here 100)\n• to truncate the context (which is in the second position) when the question with its context is too long\n• to set the number of overlapping tokens between two successive chunks (here 50)\n• to let the tokenizer know we want the overflowing tokens\n\nAs we can see, our example has been in split into four inputs, each of them containing the question and some part of the context. Note that the answer to the question (“Bernadette Soubirous”) only appears in the third and last inputs, so by dealing with long contexts in this way we will create some training examples where the answer is not included in the context. For those examples, the labels will be (so we predict the token). We will also set those labels in the unfortunate case where the answer has been truncated so that we only have the start (or end) of it. For the examples where the answer is fully in the context, the labels will be the index of the token where the answer starts and the index of the token where the answer ends.\n\nThe dataset provides us with the start character of the answer in the context, and by adding the length of the answer, we can find the end character in the context. To map those to token indices, we will need to use the offset mappings we studied in Chapter 6. We can have our tokenizer return these by passing along :\n\nAs we can see, we get back the usual input IDs, token type IDs, and attention mask, as well as the offset mapping we required and an extra key, . The corresponding value will be of use to us when we tokenize several texts at the same time (which we should do to benefit from the fact that our tokenizer is backed by Rust). Since one sample can give several features, it maps each feature to the example it originated from. Because here we only tokenized one example, we get a list of s:\n\nBut if we tokenize more examples, this will become more useful:\n\nAs we can see, the first three examples (at indices 2, 3, and 4 in the training set) each gave four features and the last example (at index 5 in the training set) gave 7 features.\n\nThis information will be useful to map each feature we get to its corresponding label. As mentioned earlier, those labels are:\n• if the answer is not in the corresponding span of the context\n• if the answer is in the corresponding span of the context, with being the index of the token (in the input IDs) at the start of the answer and being the index of the token (in the input IDs) where the answer ends\n\nTo determine which of these is the case and, if relevant, the positions of the tokens, we first find the indices that start and end the context in the input IDs. We could use the token type IDs to do this, but since those do not necessarily exist for all models (DistilBERT does not require them, for instance), we’ll instead use the method of the our tokenizer returns.\n\nOnce we have those token indices, we look at the corresponding offsets, which are tuples of two integers representing the span of characters inside the original context. We can thus detect if the chunk of the context in this feature starts after the answer or ends before the answer begins (in which case the label is ). If that’s not the case, we loop to find the first and last token of the answer:\n\nLet’s take a look at a few results to verify that our approach is correct. For the first feature we find as labels, so let’s compare the theoretical answer with the decoded span of tokens from 83 to 85 (inclusive):\n\nSo that’s a match! Now let’s check index 4, where we set the labels to , which means the answer is not in the context chunk of that feature:\n\nIndeed, we don’t see the answer inside the context.\n\nNow that we have seen step by step how to preprocess our training data, we can group it in a function we will apply on the whole training dataset. We’ll pad every feature to the maximum length we set, as most of the contexts will be long (and the corresponding samples will be split into several features), so there is no real benefit to applying dynamic padding here:\n\nNote that we defined two constants to determine the maximum length used as well as the length of the sliding window, and that we added a tiny bit of cleanup before tokenizing: some of the questions in the SQuAD dataset have extra spaces at the beginning and the end that don’t add anything (and take up space when being tokenized if you use a model like RoBERTa), so we removed those extra spaces.\n\nTo apply this function to the whole training set, we use the method with the flag. It’s necessary here as we are changing the length of the dataset (since one example can give several training features):\n\nAs we can see, the preprocessing added roughly 1,000 features. Our training set is now ready to be used — let’s dig into the preprocessing of the validation set!\n\nPreprocessing the validation data will be slightly easier as we don’t need to generate labels (unless we want to compute a validation loss, but that number won’t really help us understand how good the model is). The real joy will be to interpret the predictions of the model into spans of the original context. For this, we will just need to store both the offset mappings and some way to match each created feature to the original example it comes from. Since there is an ID column in the original dataset, we’ll use that ID.\n\nThe only thing we’ll add here is a tiny bit of cleanup of the offset mappings. They will contain offsets for the question and the context, but once we’re in the post-processing stage we won’t have any way to know which part of the input IDs corresponded to the context and which part was the question (the method we used is available for the output of the tokenizer only). So, we’ll set the offsets corresponding to the question to :\n\nWe can apply this function on the whole validation dataset like before:\n\nIn this case we’ve only added a couple of hundred samples, so it appears the contexts in the validation dataset are a bit shorter.\n\nNow that we have preprocessed all the data, we can get to the training.\n\nFine-tuning the model with the Trainer API\n\nThe training code for this example will look a lot like the code in the previous sections — the hardest thing will be to write the function. Since we padded all the samples to the maximum length we set, there is no data collator to define, so this metric computation is really the only thing we have to worry about. The difficult part will be to post-process the model predictions into spans of text in the original examples; once we have done that, the metric from the 🤗 Datasets library will do most of the work for us.\n\nThe model will output logits for the start and end positions of the answer in the input IDs, as we saw during our exploration of the pipeline. The post-processing step will be similar to what we did there, so here’s a quick reminder of the actions we took:\n• We masked the start and end logits corresponding to tokens outside of the context.\n• We then converted the start and end logits into probabilities using a softmax.\n• We attributed a score to each pair by taking the product of the corresponding two probabilities.\n• We looked for the pair with the maximum score that yielded a valid answer (e.g., a lower than ).\n\nHere we will change this process slightly because we don’t need to compute actual scores (just the predicted answer). This means we can skip the softmax step. To go faster, we also won’t score all the possible pairs, but only the ones corresponding to the highest logits (with ). Since we will skip the softmax, those scores will be logit scores, and will be obtained by taking the sum of the start and end logits (instead of the product, because of the rule log(ab)=log(a)+log(b) ).\n\nTo demonstrate all of this, we will need some kind of predictions. Since we have not trained our model yet, we are going to use the default model for the QA pipeline to generate some predictions on a small part of the validation set. We can use the same processing function as before; because it relies on the global constant , we just have to change that object to the tokenizer of the model we want to use temporarily:\n\nNow that the preprocessing is done, we change the tokenizer back to the one we originally picked:\n\nWe then remove the columns of our that are not expected by the model, build a batch with all of that small validation set, and pass it through the model. If a GPU is available, we use it to go faster:\n\nSince the will give us predictions as NumPy arrays, we grab the start and end logits and convert them to that format:\n\nNow, we need to find the predicted answer for each example in our . One example may have been split into several features in , so the first step is to map each example in to the corresponding features in :\n\nWith this in hand, we can really get to work by looping through all the examples and, for each example, through all the associated features. As we said before, we’ll look at the logit scores for the start logits and end logits, excluding positions that give:\n• An answer that wouldn’t be inside the context\n• An answer that is too long (we limit the possibilities at )\n\nOnce we have all the scored possible answers for one example, we just pick the one with the best logit score:\n\nThe final format of the predicted answers is the one that will be expected by the metric we will use. As usual, we can load it with the help of the 🤗 Evaluate library:\n\nThis metric expects the predicted answers in the format we saw above (a list of dictionaries with one key for the ID of the example and one key for the predicted text) and the theoretical answers in the format below (a list of dictionaries with one key for the ID of the example and one key for the possible answers):\n\nWe can now check that we get sensible results by looking at the first element of both lists:\n\nNot too bad! Now let’s have a look at the score the metric gives us:\n\nAgain, that’s rather good considering that according to its paper DistilBERT fine-tuned on SQuAD obtains 79.1 and 86.9 for those scores on the whole dataset.\n\nNow let’s put everything we just did in a function that we will use in the . Normally, that function only receives a tuple with logits and labels. Here we will need a bit more, as we have to look in the dataset of features for the offset and in the dataset of examples for the original contexts, so we won’t be able to use this function to get regular evaluation results during training. We will only use it at the end of training to check the results.\n\nThe function groups the same steps as before; we just add a small check in case we don’t come up with any valid answers (in which case we predict an empty string).\n\nWe can check it works on our predictions:\n\nLooking good! Now let’s use this to fine-tune our model.\n\nWe are now ready to train our model. Let’s create it first, using the class like before:\n\nAs usual, we get a warning that some weights are not used (the ones from the pretraining head) and some others are initialized randomly (the ones for the question answering head). You should be used to this by now, but that means this model is not ready to be used just yet and needs fine-tuning — good thing we’re about to do that!\n\nTo be able to push our model to the Hub, we’ll need to log in to Hugging Face. If you’re running this code in a notebook, you can do so with the following utility function, which displays a widget where you can enter your login credentials:\n\nIf you aren’t working in a notebook, just type the following line in your terminal:\n\nOnce this is done, we can define our . As we said when we defined our function to compute the metric, we won’t be able to have a regular evaluation loop because of the signature of the function. We could write our own subclass of to do this (an approach you can find in the question answering example script), but that’s a bit too long for this section. Instead, we will only evaluate the model at the end of training here and show you how to do a regular evaluation in “A custom training loop” below.\n\nThis is really where the API shows its limits and the 🤗 Accelerate library shines: customizing the class to a specific use case can be painful, but tweaking a fully exposed training loop is easy.\n\nLet’s take a look at our :\n\nWe’ve seen most of these before: we set some hyperparameters (like the learning rate, the number of epochs we train for, and some weight decay) and indicate that we want to save the model at the end of every epoch, skip evaluation, and upload our results to the Model Hub. We also enable mixed-precision training with , as it can speed up the training nicely on a recent GPU.\n\nBy default, the repository used will be in your namespace and named after the output directory you set, so in our case it will be in . We can override this by passing a ; for instance, to push the model to the organization we used (which is the model we linked to at the beginning of this section).\n\nFinally, we just pass everything to the class and launch the training:\n\nNote that while the training happens, each time the model is saved (here, every epoch) it is uploaded to the Hub in the background. This way, you will be able to to resume your training on another machine if necessary. The whole training takes a while (a little over an hour on a Titan RTX), so you can grab a coffee or reread some of the parts of the course that you’ve found more challenging while it proceeds. Also note that as soon as the first epoch is finished, you will see some weights uploaded to the Hub and you can start playing with your model on its page.\n\nOnce the training is complete, we can finally evaluate our model (and pray we didn’t spend all that compute time on nothing). The method of the will return a tuple where the first elements will be the predictions of the model (here a pair with the start and end logits). We send this to our function:\n\nGreat! As a comparison, the baseline scores reported in the BERT article for this model are 80.8 and 88.5, so we’re right where we should be.\n\nFinally, we use the method to make sure we upload the latest version of the model:\n\nThis returns the URL of the commit it just did, if you want to inspect it:\n\nThe also drafts a model card with all the evaluation results and uploads it.\n\nAt this stage, you can use the inference widget on the Model Hub to test the model and share it with your friends, family, and favorite pets. You have successfully fine-tuned a model on a question answering task — congratulations!\n\nIf you want to dive a bit more deeply into the training loop, we will now show you how to do the same thing using 🤗 Accelerate.\n\nLet’s now have a look at the full training loop, so you can easily customize the parts you need. It will look a lot like the training loop in Chapter 3, with the exception of the evaluation loop. We will be able to evaluate the model regularly since we’re not constrained by the class anymore.\n\nFirst we need to build the s from our datasets. We set the format of those datasets to , and remove the columns in the validation set that are not used by the model. Then, we can use the provided by Transformers as a and shuffle the training set, but not the validation set:\n\nNext we reinstantiate our model, to make sure we’re not continuing the fine-tuning from before but starting from the BERT pretrained model again:\n\nThen we will need an optimizer. As usual we use the classic , which is like Adam, but with a fix in the way weight decay is applied:\n\nOnce we have all those objects, we can send them to the method. Remember that if you want to train on TPUs in a Colab notebook, you will need to move all of this code into a training function, and that shouldn’t execute any cell that instantiates an . We can force mixed-precision training by passing to the (or, if you are executing the code as a script, just make sure to fill in the 🤗 Accelerate appropriately).\n\nAs you should know from the previous sections, we can only use the length to compute the number of training steps after it has gone through the method. We use the same linear schedule as in the previous sections:\n\nTo push our model to the Hub, we will need to create a object in a working folder. First log in to the Hugging Face Hub, if you’re not logged in already. We’ll determine the repository name from the model ID we want to give our model (feel free to replace the with your own choice; it just needs to contain your username, which is what the function does):\n\nThen we can clone that repository in a local folder. If it already exists, this local folder should be a clone of the repository we are working with:\n\nWe can now upload anything we save in by calling the method. This will help us upload the intermediate models at the end of each epoch.\n\nWe are now ready to write the full training loop. After defining a progress bar to follow how training goes, the loop has three parts:\n• The training in itself, which is the classic iteration over the , forward pass through the model, then backward pass and optimizer step.\n• The evaluation, in which we gather all the values for and before converting them to NumPy arrays. Once the evaluation loop is finished, we concatenate all the results. Note that we need to truncate because the may have added a few samples at the end to ensure we have the same number of examples in each process.\n• Saving and uploading, where we first save the model and the tokenizer, then call . As we did before, we use the argument to tell the 🤗 Hub library to push in an asynchronous process. This way, training continues normally and this (long) instruction is executed in the background.\n\nHere’s the complete code for the training loop:\n\nIn case this is the first time you’re seeing a model saved with 🤗 Accelerate, let’s take a moment to inspect the three lines of code that go with it:\n\nThe first line is self-explanatory: it tells all the processes to wait until everyone is at that stage before continuing. This is to make sure we have the same model in every process before saving. Then we grab the , which is the base model we defined. The method changes the model to work in distributed training, so it won’t have the method anymore; the method undoes that step. Lastly, we call but tell that method to use instead of .\n\nOnce this is done, you should have a model that produces results pretty similar to the one trained with the . You can check the model we trained using this code at huggingface-course/bert-finetuned-squad-accelerate. And if you want to test out any tweaks to the training loop, you can directly implement them by editing the code shown above!\n\nWe’ve already shown you how you can use the model we fine-tuned on the Model Hub with the inference widget. To use it locally in a , you just have to specify the model identifier:\n\nGreat! Our model is working as well as the default one for this pipeline!"
    },
    {
        "link": "https://geeksforgeeks.org/how-to-use-hugging-face-model-for-question-answering",
        "document": "How to Use Hugging Face Model for Question Answering\n\nIn the rapidly advancing field of natural language processing (NLP), question answering systems are becoming increasingly sophisticated and accessible. Hugging Face, a leader in the AI community, provides an array of pre-trained models through its Transformers library, making it easier for developers to implement complex NLP tasks like question answering. This article provides a comprehensive guide on how to use Hugging Face models for question answering.\n\nHugging Face Transformers is a state-of-the-art library that offers a wide range of pre-trained models designed for various NLP tasks. These models are built on transformer architectures, which have shown remarkable success in understanding the context and semantics of text.\n\nBefore diving into coding, ensure that you have Python installed, along with the library. You can install it using pip:\n\nFor question answering, the most commonly used models are based on the BERT (Bidirectional Encoder Representations from Transformers) architecture, such as or its variants like which is a lighter version. Hugging Face also offers specialized question answering models like trained on the SQuAD 2.0 dataset.\n\nStart by importing the necessary components from the transformers library.\n\nUse the Hugging Face models to build a pipeline for answering questions. This will let you pose queries in light of the information supplied.\n\nDefine a context string that contains the information from which the model will extract answers. Here, the context is about GeeksforGeeks, a popular platform for learning and practicing computer science.\n\nStep 4: Ask Questions and Get Answers\n\nCreate a list of questions related to the context, and then use the pipeline to get answers for each. The pipeline uses the context to find and return the most relevant answer to each question.\n\nAsk a question that is unrelated to the defined context to see how the model handles it. Since the context does not contain the answer, the model might not return a relevant response.\n\nWhen a Hugging Face question-answering model incorrectly answers an out-of-context question like \"What is the capital of India?\" with \"GeeksforGeeks,\" it usually reflects certain limitations in the model's design:\n• Context Limitation: The model attempts to find an answer using the provided context, regardless of its relevance to the question, leading to mismatched or irrelevant answers.\n• Model Training: Models are trained to always produce an answer from the available context, without the capability to identify if the question is unrelated to that context.\n• Lack of Rejection Mechanism: Standard QA pipelines are designed to provide the \"best\" answer possible from the context, even if it's incorrect or nonsensical, as they lack a mechanism to reject a question as unanswerable based on the context.\n• Fine-tuning : For more accurate results, consider fine-tuning the model on a domain-specific dataset.\n• Handling Long Texts : If your context is very long, consider strategies like sliding window or top-N paragraph selection to manage the input size.\n• Performance Optimization : Utilize hardware acceleration like GPUs to speed up inference, especially when dealing with multiple requests.\n\nHugging Face's Transformers library significantly simplifies the implementation of question answering systems. By following this guide, you can start leveraging powerful transformer models to build robust NLP applications capable of understanding and responding to user queries effectively. Whether for customer support automation, educational tools, or data analysis, the possibilities are extensive."
    },
    {
        "link": "https://spacy.io/api/entityrecognizer",
        "document": "A transition-based named entity recognition component. The entity recognizer identifies non-overlapping labelled spans of tokens. The transition-based algorithm used encodes certain assumptions that are effective for “traditional” named entity recognition tasks, but may not be a good fit for every span identification problem. Specifically, the loss function optimizes for whole entity accuracy, so if your inter-annotator agreement on boundary tokens is low, the component will likely perform poorly on your problem. The transition-based algorithm also assumes that the most decisive information about your entities will be close to their initial tokens. If your entities are long and characterized by tokens in their middle, the component will likely not be a good fit for your task.\n\nPredictions will be saved to as a tuple. Each label will also be reflected to each underlying token, where it is saved in the and fields. Note that by definition each token can only have one label. When setting to create training data, all the spans must be valid and non-overlapping, or an error will be thrown. An enum encoding of the IOB part of the named entity tag. The IOB part of the named entity tag. The label part of the named entity tag (hash). The label part of the named entity tag.\n\nThe default config is defined by the pipeline component factory and describes how the component should be configured. You can override its settings via the argument on or in your for training. See the model architectures documentation for details on the architectures and their arguments and hyperparameters. A list of transition names. Inferred from the data if not provided. Defaults to . During training, cut long sequences into shorter segments by creating intermediate states based on the gold-standard history. The model is not very sensitive to this parameter, so you usually won’t need to change it. Defaults to . The powering the pipeline component. Defaults to . This key refers to a in that specifies incorrect spans. The NER will learn not to predict (exactly) those spans. Defaults to .\n\nCreate a new pipeline instance. In your application, you would normally use a shortcut for this and instantiate the component using its string name and . String name of the component instance. Used to add entries to the during training. A list of transition names. Inferred from the data if set to , which is the default. During training, cut long sequences into shorter segments by creating intermediate states based on the gold-standard history. The model is not very sensitive to this parameter, so you usually won’t need to change it. Defaults to . Identifies spans that are known to be incorrect entity annotations. The incorrect entity annotations can be stored in the span group in , under this key. Defaults to .\n\nApply the pipe to one document. The document is modified in place and returned. This usually happens under the hood when the object is called on a text and all pipeline components are applied to the in order. Both and delegate to the and methods.\n\nApply the pipe to a stream of documents. This usually happens under the hood when the object is called on a text and all pipeline components are applied to the in order. Both and delegate to the and methods. The number of documents to buffer. Defaults to .\n\nInitialize the component for training. should be a function that returns an iterable of objects. At least one example should be supplied. The data examples are used to initialize the model of the component and can either be the full training data or a representative sample. Initialization includes validating the network, inferring missing shapes and setting up the label scheme based on the data. This method is typically called by and lets you customize arguments it receives via the block in the config. Function that returns gold-standard annotations in the form of objects. Must contain at least one . The label information to add to the component, as provided by the property after initialization. To generate a reusable JSON file from your data, you should run the command. If no labels are provided, the callback is used to extract the labels from the data, which may be a lot slower.\n\nAdd a new label to the pipe. Note that you don’t have to call this method if you provide a representative data sample to the method. In this case, all labels found in the sample will be automatically added to the model, and the output dimension will be inferred automatically.\n\nChange the output dimension of the component’s model by calling the model’s attribute . This is a function that takes the original model and the new output dimension , and changes the model in place. When resizing an already trained model, care should be taken to avoid the “catastrophic forgetting” problem.\n\nDuring serialization, spaCy will export several data fields used to restore different aspects of the object. If needed, you can exclude them from serialization by passing in the string names via the argument. The config file. You usually don’t want to exclude this. The binary model data. You usually don’t want to exclude this."
    },
    {
        "link": "https://spacy.io/usage/spacy-101",
        "document": "spaCy provides a variety of linguistic annotations to give you insights into a text’s grammatical structure. This includes the word types, like the parts of speech, and how the words are related to each other. For example, if you’re analyzing text, it makes a huge difference whether a noun is the subject of a sentence, or the object – or whether “google” is used as a verb, or refers to the website or company in a specific context. Once you’ve downloaded and installed a trained pipeline, you can load it via . This will return a object containing all components and data needed to process text. We usually call it . Calling the object on a string of text will return a processed : Even though a is processed – e.g. split into individual words and annotated – it still holds all information of the original text, like whitespace characters. You can always get the offset of a token into the original string, or reconstruct the original by joining the tokens and their trailing whitespace. This way, you’ll never lose any information when processing text with spaCy. During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off – whereas “U.K.” should remain one token. Each consists of individual tokens, and we can iterate over them: First, the raw text is split on whitespace characters, similar to . Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n• Does the substring match a tokenizer exception rule? For example, “don’t” does not contain whitespace, but should be split into two tokens, “do” and “n’t”, while “U.K.” should always remain one token.\n• Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes. If there’s a match, the rule is applied and the tokenizer continues its loop, starting with the newly split substrings. This way, spaCy can split complex, nested tokens like combinations of abbreviations and multiple punctuation marks.\n• Tokenizer exception: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied. While punctuation rules are usually pretty general, tokenizer exceptions strongly depend on the specifics of the individual language. This is why each available language has its own subclass, like or , that loads in lists of hard-coded data and exception rules. To learn more about how spaCy’s tokenization rules work in detail, how to customize and replace the default tokenizer and how to add language-specific data, see the usage guides on language data and customizing the tokenizer. After tokenization, spaCy can parse and tag a given . This is where the trained pipeline and its statistical models come in, which enable spaCy to make predictions of which tag or label most likely applies in this context. A trained component includes binary data that is produced by showing a system enough examples for it to make predictions that generalize across the language – for example, a word following “the” in English is most likely a noun. Linguistic annotations are available as attributes. Like many NLP libraries, spaCy encodes all strings to hash values to reduce memory usage and improve efficiency. So to get the readable string representation of an attribute, we need to add an underscore to its name:\n• Lemma: The base form of the word.\n• is alpha: Is the token an alpha character?\n• is stop: Is the token part of a stop list, i.e. the most common words of the language? Most of the tags and labels look pretty abstract, and they vary between languages. will show you a short description – for example, returns “verb, 3rd person singular present”. Using spaCy’s built-in displaCy visualizer, here’s what our example sentence and its dependencies look like: To learn more about part-of-speech tagging and rule-based morphology, and how to navigate and use the parse tree effectively, see the usage guides on part-of-speech tagging and using the dependency parse. A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case. Named entities are available as the property of a :\n• Start: Index of start of entity in the .\n• End: Index of end of entity in the . Using spaCy’s built-in displaCy visualizer, here’s what our example sentence and its named entities look like: is looking at buying startup for To learn more about entity recognition in spaCy, how to add your own entities to a document and how to train and update the entity predictions of a model, see the usage guides on named entity recognition and training pipelines. Similarity is determined by comparing word vectors or “word embeddings”, multi-dimensional meaning representations of a word. Word vectors can be generated using an algorithm like word2vec and usually look like this: To make them compact and fast, spaCy’s small pipeline packages (all packages that end in ) don’t ship with word vectors, and only include context-sensitive tensors. This means you can still use the methods to compare documents, spans and tokens – but the result won’t be as good, and individual tokens won’t have any vectors assigned. So in order to use real word vectors, you need to download a larger pipeline package: Pipeline packages that come with built-in word vectors make them available as the attribute. and will default to an average of their token vectors. You can also check if a token has a vector assigned, and get the L2 norm, which can be used to normalize vectors.\n• has vector: Does the token have a vector representation?\n• Vector norm: The L2 norm of the token’s vector (the square root of the sum of the values squared) The words “dog”, “cat” and “banana” are all pretty common in English, so they’re part of the pipeline’s vocabulary, and come with a vector. The word “afskfsd” on the other hand is a lot less common and out-of-vocabulary – so its vector representation consists of 300 dimensions of , which means it’s practically nonexistent. If your application will benefit from a large vocabulary with more vectors, you should consider using one of the larger pipeline packages or loading in a full vector package, for example, , which includes 685k unique vectors. spaCy is able to compare two objects, and make a prediction of how similar they are. Predicting similarity is useful for building recommendation systems or flagging duplicates. For example, you can suggest a user content that’s similar to what they’re currently looking at, or label a support ticket as a duplicate if it’s very similar to an already existing one. Each , , and comes with a method that lets you compare it with another object, and determine the similarity. Of course similarity is always subjective – whether two words, spans or documents are similar really depends on how you’re looking at it. spaCy’s similarity implementation usually assumes a pretty general-purpose definition of similarity.\n• Compare two different tokens and try to find the two most dissimilar tokens in the texts with the lowest similarity score (according to the vectors).\n• Compare the similarity of two objects, entries in the vocabulary. You can get a lexeme via the attribute of a token. You should see that the similarity results are identical to the token similarity. What to expect from similarity results Computing similarity scores can be helpful in many situations, but it’s also important to maintain realistic expectations about what information it can provide. Words can be related to each other in many ways, so a single “similarity” score will always be a mix of different signals, and vectors trained on different data can produce very different results that may not be useful for your purpose. Here are some important considerations to keep in mind:\n• There’s no objective definition of similarity. Whether “I like burgers” and “I like pasta” is similar depends on your application. Both talk about food preferences, which makes them very similar – but if you’re analyzing mentions of food, those sentences are pretty dissimilar, because they talk about very different foods.\n• The similarity of and objects defaults to the average of the token vectors. This means that the vector for “fast food” is the average of the vectors for “fast” and “food”, which isn’t necessarily representative of the phrase “fast food”.\n• Vector averaging means that the vector of multiple tokens is insensitive to the order of the words. Two documents expressing the same meaning with dissimilar wording will return a lower similarity score than two documents that happen to contain the same words while expressing different meanings. is a library developed by us that builds on top of spaCy and lets you train and query more interesting and detailed word vectors. It combines noun phrases like “fast food” or “fair game” and includes the part-of-speech tags and entity labels. The library also includes annotation recipes for our annotation tool Prodigy that let you evaluate vectors and create terminology lists. For more details, check out our blog post. To explore the semantic similarities across all Reddit comments of 2015 and 2019, see the interactive demo. To learn more about word vectors, how to customize them and how to load your own vectors into spaCy, see the usage guide on using word vectors and semantic similarities.\n\nWhen you call on a text, spaCy first tokenizes the text to produce a object. The is then processed in several different steps – this is also referred to as the processing pipeline. The pipeline used by the trained pipelines typically include a tagger, a lemmatizer, a parser and an entity recognizer. Each pipeline component returns the processed , which is then passed on to the next component.\n• Name: ID of the pipeline component.\n• Creates: Objects, attributes and properties modified and set by the component. The capabilities of a processing pipeline always depend on the components, their models and how they were trained. For example, a pipeline for named entity recognition needs to include a trained named entity recognizer component with a statistical model and weights that enable it to make predictions of entity labels. This is why each pipeline specifies its components and their settings in the config: Does the order of pipeline components matter? ¶ The statistical components like the tagger or parser are typically independent and don’t share any data between each other. For example, the named entity recognizer doesn’t use any features set by the tagger and parser, and so on. This means that you can swap them, or remove single components from the pipeline without affecting the others. However, components may share a “token-to-vector” component like or . You can read more about this in the docs on embedding layers. Custom components may also depend on annotations set by other components. For example, a custom lemmatizer may need the part-of-speech tags assigned, so it’ll only work if it’s added after the tagger. The parser will respect pre-defined sentence boundaries, so if a previous component in the pipeline sets them, its dependency predictions may be different. Similarly, it matters if you add the before or after the statistical entity recognizer: if it’s added before, the entity recognizer will take the existing entities into account when making predictions. The , which resolves named entities to knowledge base IDs, should be preceded by a pipeline component that recognizes entities such as the . Why is the tokenizer special? ¶ The tokenizer is a “special” component and isn’t part of the regular pipeline. It also doesn’t show up in . The reason is that there can only really be one tokenizer, and while all other pipeline components take a and return it, the tokenizer takes a string of text and turns it into a . You can still customize the tokenizer, though. is writable, so you can either create your own class from scratch, or even replace it with an entirely custom function. To learn more about how processing pipelines work in detail, how to enable and disable their components, and how to create your own, see the usage guide on language processing pipelines.\n\nThe central data structures in spaCy are the class, the and the object. The class is used to process a text and turn it into a object. It’s typically stored as a variable called . The object owns the sequence of tokens and all their annotations. By centralizing strings, word vectors and lexical attributes in the , we avoid storing multiple copies of this data. This saves memory, and ensures there’s a single source of truth. Text annotations are also designed to allow a single source of truth: the object owns the data, and and are views that point into it. The object is constructed by the , and then modified in place by the components of the pipeline. The object coordinates these components. It takes raw text and sends it through the pipeline, returning an annotated document. It also orchestrates training and serialization. A collection of objects for efficient binary serialization. Also used for . A collection of training annotations, containing two objects: the reference data and the predictions. Processing class that turns text into objects. Different languages implement their own subclasses of it. The variable is typically called . An entry in the vocabulary. It’s a word type with no context, as opposed to a word token. It therefore has no part-of-speech tag, dependency parse etc. The processing pipeline consists of one or more pipeline components that are called on the in order. The tokenizer runs before the components. Pipeline components can be added using . They can contain a statistical model and trained weights, or only make rule-based modifications to the . spaCy provides a range of built-in components for different language processing tasks and also allows adding custom components. Add entity spans to the using token-based rules or exact phrase matches. Determine the base forms of words using rules and lookups. Predict categories or labels over the whole document. Segment raw text and create objects from the words. Class that all trainable pipeline components inherit from. Use a transformer model and set its outputs. Automatically apply something to the , e.g. to merge spans of tokens. Matchers help you find and extract information from objects based on match patterns describing the sequences you’re looking for. A matcher operates on a and gives you access to the matched tokens in context. Match sequences of tokens based on dependency trees using Semgrex operators. Match sequences of tokens, based on pattern rules, similar to regular expressions. Class for managing annotated corpora for training and evaluation data. Abstract base class for storage and retrieval of data for entity linking. Implementation of storing all data in memory. Container for convenient access to large lookup tables and dictionaries. Store morphological analyses and map them to and from hash values. Map strings to and from hash values. The shared vocabulary that stores strings and gives you access to objects.\n\nWhenever possible, spaCy tries to store data in a vocabulary, the , that will be shared by multiple documents. To save memory, spaCy also encodes all strings to hash values – in this case for example, “coffee” has the hash . Entity labels like “ORG” and part-of-speech tags like “VERB” are also encoded. Internally, spaCy only “speaks” in hash values.\n• Token: A word, punctuation mark etc. in context, including its attributes, tags and dependencies.\n• Lexeme: A “word type” with no context. Includes the word shape and flags, e.g. if it’s lowercase, a digit or punctuation.\n• StringStore: The dictionary mapping hash values to strings, for example → “coffee”. If you process lots of documents containing the word “coffee” in all kinds of different contexts, storing the exact string “coffee” every time would take up way too much space. So instead, spaCy hashes the string and stores it in the . You can think of the as a lookup table that works in both directions – you can look up a string to get its hash, or a hash to get its string: Now that all strings are encoded, the entries in the vocabulary don’t need to include the word text themselves. Instead, they can look it up in the via its hash value. Each entry in the vocabulary, also called , contains the context-independent information about a word. For example, no matter if “love” is used as a verb or a noun in some context, its spelling and whether it consists of alphabetic characters won’t ever change. Its hash value will also always be the same.\n• Text: The original text of the lexeme.\n• Orth: The hash value of the lexeme.\n• Shape: The abstract word shape of the lexeme.\n• Prefix: By default, the first letter of the word string.\n• Suffix: By default, the last three letters of the word string.\n• is alpha: Does the lexeme consist of alphabetic characters?\n• is digit: Does the lexeme consist of digits? The mapping of words to hashes doesn’t depend on any state. To make sure each value is unique, spaCy uses a hash function to calculate the hash based on the word string. This also means that the hash for “coffee” will always be the same, no matter which pipeline you’re using or how you’ve configured spaCy. However, hashes cannot be reversed and there’s no way to resolve back to “coffee”. All spaCy can do is look it up in the vocabulary. That’s why you always need to make sure all objects you create have access to the same vocabulary. If they don’t, spaCy might not be able to find the strings it needs. If the vocabulary doesn’t contain a string for , spaCy will raise an error. You can re-add “coffee” manually, but this only works if you actually know that the document contains that word. To prevent this problem, spaCy will also export the when you save a or object. This will give you the object and its encoded annotations, plus the “key” to decode it.\n\nspaCy’s tagger, parser, text categorizer and many other components are powered by statistical models. Every “decision” these components make – for example, which part-of-speech tag to assign, or whether a word is a named entity – is a prediction based on the model’s current weight values. The weight values are estimated based on examples the model has seen during training. To train a model, you first need training data – examples of text, and the labels you want the model to predict. This could be a part-of-speech tag, a named entity or any other information. Training is an iterative process in which the model’s predictions are compared against the reference annotations in order to estimate the gradient of the loss. The gradient of the loss is then used to calculate the gradient of the weights through backpropagation. The gradients indicate how the weight values should be changed so that the model’s predictions become more similar to the reference labels over time.\n• Text: The input text the model should predict a label for.\n• Label: The label the model should predict.\n• Gradient: The direction and rate of change for a numeric value. Minimising the gradient of the weights should result in predictions that are closer to the reference labels on the training data. When training a model, we don’t just want it to memorize our examples – we want it to come up with a theory that can be generalized across unseen data. After all, we don’t just want the model to learn that this one instance of “Amazon” right here is a company – we want it to learn that “Amazon”, in contexts like this, is most likely a company. That’s why the training data should always be representative of the data we want to process. A model trained on Wikipedia, where sentences in the first person are extremely rare, will likely perform badly on Twitter. Similarly, a model trained on romantic novels will likely perform badly on legal text. This also means that in order to know how the model is performing, and whether it’s learning the right things, you don’t only need training data – you’ll also need evaluation data. If you only test the model with the data it was trained on, you’ll have no idea how well it’s generalizing. If you want to train a model from scratch, you usually need at least a few hundred examples for both training and evaluation. To learn more about training and updating pipelines, how to create training data and how to improve spaCy’s named models, see the usage guides on training. Training config files include all settings and hyperparameters for training your pipeline. Instead of providing lots of arguments on the command line, you only need to pass your file to . This also makes it easy to integrate custom models and architectures, written in your framework of choice. A pipeline’s is considered the “single source of truth”, both at training and runtime. For more details on spaCy’s configuration system and how to use it to customize your pipeline components, component models, training settings and hyperparameters, see the training config usage guide. spaCy’s class helps you implement your own trainable components that have their own model instance, make predictions over objects and can be updated using . This lets you plug fully custom machine learning components into your pipeline that can be configured via a single training config. To learn more about how to implement your own model architectures and use them to power custom trainable components, see the usage guides on the trainable component API and implementing layers and architectures for trainable components.\n\nWe’re very happy to see the spaCy community grow and include a mix of people from all kinds of different backgrounds – computational linguistics, data science, deep learning, research and more. If you’d like to get involved, below are some answers to the most important questions and resources for further reading. Bugs suck, and we’re doing our best to continuously improve the tests and fix bugs as soon as possible. Before you submit an issue, do a quick search and check if the problem has already been reported. If you’re having installation or loading problems, make sure to also check out the troubleshooting guide. Help with spaCy is available via the following platforms: How do I know if something is a bug? Of course, it’s always hard to know for sure, so don’t worry – we’re not going to be mad if a bug report turns out to be a typo in your code. As a simple rule, any C-level error without a Python traceback, like a segmentation fault or memory error, is always a spaCy bug. Because models are statistical, their performance will never be perfect. However, if you come across patterns that might indicate an underlying issue, please do file a report. Similarly, we also care about behaviors that contradict our docs.\n• Stack Overflow: Usage questions and everything related to problems with your specific code. The Stack Overflow community is much larger than ours, so if your problem can be solved by others, you’ll receive help much quicker.\n• : General discussion, project ideas and usage questions. Meet other community members to get help with a specific code implementation, discuss ideas for new projects/plugins, support more languages, and share best practices.\n• : Bug reports and improvement suggestions, i.e. everything that’s likely spaCy’s fault. This also includes problems with the trained pipelines beyond statistical imprecisions, like patterns that point to a bug. Please understand that we won’t be able to provide individual support via email. We also believe that help is much more valuable if it’s shared publicly, so that more people can benefit from it. If you come across an issue and you think you might be able to help, consider posting a quick update with your solution. No matter how simple, it can easily save someone a lot of time and headache – and the next time you need help, they might repay the favor. How can I contribute to spaCy? You don’t have to be an NLP expert or Python pro to contribute, and we’re happy to help you get started. If you’re new to spaCy, a good place to start is the label on GitHub, which we use to tag bugs and feature requests that are easy and self-contained. We also appreciate contributions to the docs – whether it’s fixing a typo, improving an example or adding additional explanations. You’ll find a “Suggest edits” link at the bottom of each page that points you to the source. Another way of getting involved is to help us improve the language data – especially if you happen to speak one of the languages currently in alpha support. Even adding simple tokenizer exceptions, stop words or lemmatizer data can make a big difference. It will also make it easier for us to provide a trained pipeline for the language in the future. Submitting a test that documents a bug or performance issue, or covers functionality that’s especially important for your application is also very helpful. This way, you’ll also make sure we never accidentally introduce regressions to the parts of the library that you care about the most. For more details on the types of contributions we’re looking for, the code conventions and other useful tips, make sure to check out the contributing guidelines. spaCy adheres to the Contributor Covenant Code of Conduct. By participating, you are expected to uphold this code. I’ve built something cool with spaCy – how can I get the word out? First, congrats – we’d love to check it out! If you think your project would be a good fit for the spaCy Universe, feel free to submit it! Tutorials are also incredibly valuable to other users and a great way to get exposure. So we strongly encourage writing up your experiences, or sharing your code and some tips and tricks on your blog. Since our website is open-source, you can add your project or tutorial by making a pull request on GitHub. If you would like to use the spaCy logo on your site, please get in touch and ask us first. However, if you want to show support and tell others that your project is using spaCy, you can grab one of our spaCy badges here:"
    },
    {
        "link": "https://spacy.io",
        "document": "spaCy is designed to help you do real work — to build real products, or gather real insights. The library respects your time, and tries to avoid wasting it. It's easy to install, and its API is simple and productive. spaCy excels at large-scale information extraction tasks. It's written from the ground up in carefully memory-managed Cython. If your application needs to process entire web dumps, spaCy is the library you want to be using. Since its release in 2015, spaCy has become an industry standard with a huge ecosystem. Choose from a variety of plugins, integrate with your machine learning stack and build custom components and workflows.\n\nspaCy v3.0 introduces a comprehensive and extensible system for configuring your training runs. Your configuration file will describe every detail of your training run, with no hidden defaults, making it easy to rerun your experiments and track changes. You can use the quickstart widget or the command to get started, or clone a project template for an end-to-end workflow.\n\n\n\n\n\n\n\n The easiest way to get started is to clone a project template and run it – for example, this template for training a part-of-speech tagger and dependency parser on a Universal Dependencies treebank. spaCy's new project system gives you a smooth path from prototype to production. It lets you keep track of all those data transformation, preprocessing and training steps, so you can make sure your project is always ready to hand over for automation. It features source asset download, command execution, checksum verification, and caching with a variety of backends and integrations.\n\nGet a custom spaCy pipeline, tailor-made for your NLP problem by spaCy's core developers.\n• Streamlined. Nobody knows spaCy better than we do. Send us your pipeline requirements and we'll be ready to start producing your solution in no time at all.\n• Production ready. spaCy pipelines are robust and easy to deploy. You'll get a complete spaCy project folder which is ready to .\n• Predictable. You'll know exactly what you're going to get and what it's going to cost. We quote fees up-front, let you try before you buy, and don't charge for over-runs at our end — all the risk is on us.\n• Maintainable. spaCy is an industry standard, and we'll deliver your pipeline with full code, data, tests and documentation, so your team can retrain, update and extend the solution as your requirements change. In this free and interactive online course you’ll learn how to use spaCy to build advanced natural language understanding systems, using both rule-based and machine learning approaches. It includes 55 exercises featuring videos, slide decks, multiple-choice questions and interactive coding practice in the browser.\n\nspaCy v3.0 introduces transformer-based pipelines that bring spaCy's accuracy right up to the current state-of-the-art. You can also use a CPU-optimized pipeline, which is less accurate but much cheaper to run."
    },
    {
        "link": "https://medium.com/ubiai-nlp/fine-tuning-spacy-models-customizing-named-entity-recognition-for-domain-specific-data-3d17c5fc72ae",
        "document": "In the realm of Natural Language Processing (NLP), a foundational endeavor involves extracting meaningful insights from textual data. At the core of numerous NLP applications lies Named Entity Recognition (NER), a pivotal technique that plays a crucial role in recognizing and classifying entities such as names, dates, and locations embedded within textual content.\n\nNamed Entity Recognition (NER) represents a specialized domain within natural language processing that focuses on identifying and categorizing named entities within textual content. These entities cover various categories, including individual names, organizational titles, geographic locations, dates, numerical values, and more.\n\nThe importance of NER is pervasive across a range of applications, spanning information extraction, question answering, chatbots, sentiment analysis, and recommendation systems. This underscores its crucial role in advancing multiple facets of natural language understanding and utilization.\n\nSpaCy stands out as a leading natural language processing (NLP) library, celebrated for its efficiency and adaptability in addressing diverse linguistic tasks. Developed by Explosion AI, SpaCy is strategically crafted with a focus on creating production-ready applications, establishing it as a preferred choice for researchers, developers, and businesses alike.\n\nAs an open-source library, SpaCy provides pre-trained models for essential tasks like part-of-speech tagging, named entity recognition, and dependency parsing. Its distinguishing features include exceptional speed and memory efficiency, enabling it to handle substantial volumes of text in real-time effectively. Additionally, SpaCy’s user-friendly interface, extensive language support, and seamless integration with deep learning frameworks contribute significantly to its popularity within the NLP community. Notably, SpaCy’s support for custom training and fine-tuning, as demonstrated in this article, enhances its versatility, making it valuable in various scenarios for extracting insights and information from textual data.\n\nWe begin by importing the essential library, spacy, which is the core component for natural language processing tasks.\n\nFollowing that, we download the large English language model, en_core_web_lg, which encompasses a comprehensive set of linguistic features.\n\nOnce the download is complete, we initialize and load the model using the spacy.load(“en_core_web_lg”) command.\n\nWe then create a SpaCy Doc object by applying the loaded SpaCy model to a given text string. Specifically, the text “UBIAI is cool” undergoes processing through the pipeline, and the resulting document is stored in the variable doc.\n\nIn SpaCy, the “displacy” module serves as a tool for visualizing linguistic annotations, encompassing named entities, dependencies, and other linguistic features. This module offers a convenient means to visually represent the structure and relationships within a given text.\n\nIn the next example, we apply the “displacy” module to the `doc` object that has been created, providing a visual representation of the linguistic annotations and enhancing the understanding of the text’s structural elements.\n\nAs output we get :\n\nWhile pre-trained models provide a robust foundation for various NLP tasks, the necessity for customization arises from the unique requirements of specific applications and domains. This section presents arguments for why fine-tuning pre-trained Named Entity Recognition (NER) models is crucial:\n\n1. **Domain Specificity:**\n\n — Excels in domain-specific data by being trained on specialized content, leading to enhanced accuracy in recognizing entities within that particular domain.\n\n2. **Improved Precision:**\n\n — Enables higher precision by tailoring the model to recognize specific entity types crucial to your application, reducing the risk of false positives.\n\n3. **Data Control:**\n\n — Provides control over the quality of training data and annotation, ensuring that the model is exposed to relevant and accurate examples.\n\n4. **Adaptability:**\n\n — Allows continuous updates and fine-tuning as the data evolves, ensuring that the model stays relevant and effective over time.\n\nWhile pre-trained NER models proficiently recognize common entity types like persons, organizations, and locations, they may fall short when faced with the need to identify unique entities specific to your domain. This emphasizes the importance of fine-tuning NER models on domain-specific data.\n\nIn this section we will guide you on how to fine-tune a spaCy NER model en_core_web_lg on your own data.\n\nOur initial task involves instructing spaCy on how to recognize words associated with specific tags. To accomplish this, we need to create a JSON file containing examples, with each annotated entry specifying tags and their corresponding indices.\n\nThe file UBIAI_TEST.json follows a specific format, containing sentences under the “content” field along with corresponding labels specified in the “tag_name” field. The “start” attribute represents the position of the starting character, while “end” indicates where the label ends in the sentence. This structured format allows for precise annotation of entities within the textual content.\n\nThis code snippet is instrumental in preparing the training data in the correct format for training a SpaCy Named Entity Recognition (NER) model.\n\nFor that first example the output would be :\n\nIn this segment, the training data is transformed into SpaCy’s efficient DocBin format, a binary structure designed for storing Doc objects. The process unfolds as follows:\n\nExample on a json file containing 7 training examples:\n\nNow it is time to create the Training Configuration:\n\nStart by creating these two configuration files (The configuration is from the official spaCy documentation) .\n\nNext, execute the following command within the notebook code block to initialize spaCy, utilizing the specified configuration file. This configuration file is essential for training the spaCy model with the custom features we have generated.\n\nOnce the training is done, 2 folders named model-best and model-last are going to be generated.\n\nNow all you have to do is to load and test the fine-tuned model.\n\nIn this section, we will offer step-by-step guidance on fine-tuning a spaCy Named Entity Recognition (NER) model (en_core_web_trf) using your custom data. The initial three steps mirror those of using tok2vec, with the only distinction being the switch from the en_core_web_lg model to the en_core_web_trf.\n\nNow, it is time to create the training configuration:\n• Choose your desired language and set the component to ‘ner.’\n\n2. Depending on your system specifications, opt for either CPU or GPU.\n\n3. Save this configuration as ‘base_config.cfg.’\n\nTo complete the configuration with the default settings for the rest of the system parameters, execute the following command in the command line to generate the ‘config.cfg’ file.\n\nTrain the model using the command line by specifying the training and development data paths in the configuration file.\n\nAdditionally, you can set parameters such as batch size, max steps, epochs, patience, and more directly in the configuration file.\n\nNow all you have to do is to load and test the fine-tuned model.\n\nIn conclusion, by leveraging natural language processing libraries such as spaCy and blending domain expertise with the adaptability provided by custom Named Entity Recognition (NER) models, you have the potential to enhance accuracy and achieve context-aware natural language understanding in diverse real-world scenarios."
    },
    {
        "link": "https://geeksforgeeks.org/named-entity-recognition",
        "document": ""
    }
]