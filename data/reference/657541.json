[
    {
        "link": "https://ubuntu.com/server/docs/about-logical-volume-management-lvm",
        "document": "You should have been redirected."
    },
    {
        "link": "https://documentation.ubuntu.com/server/how-to/storage/manage-logical-volumes",
        "document": "The Ubuntu Server installer has the ability to set up and install to LVM partitions, and this is the supported way of doing so. If you would like to know more about any of the topics in this page, refer to our explanation of logical volume management (LVM).\n\nFirst, you need a physical volume. Typically you start with a hard disk, and create a regular partition whose type is “LVM” on it. You can create it with or , and usually only want one LVM-type partition in the whole disk, since LVM will handle subdividing it into logical volumes. In , you need to check the flag when creating the partition, and with , tag the type with code . Once you have your LVM partition, you need to initialise it as a physical volume. Assuming this partition is :\n\nNow you want to create a logical volume from some of the free space in : This creates a logical volume named in volume group using 5 GB of space. You can find the block device for this logical volume in . You might also want to try the and commands, which list the logical volumes and physical volumes respectively, and their more detailed variants; and .\n\nYou can extend a logical volume with: This will add 5 GB to the logical volume in the volume group, and will automatically resize the underlying filesystem (if supported). The space is allocated from free space anywhere in the volume group. You can specify an absolute size instead of a relative size if you want by omitting the leading . If you have multiple physical volumes you can add the names of one (or more) of them to the end of the command to limit which ones are used to fulfil the request.\n\nIf you only have one physical volume then you are unlikely to ever need to move, but if you add a new disk, you might want to. To move the logical volume off of physical volume , you can run: If you omit the argument, then all logical volumes on the physical volume will be moved. If you only have one other physical volume then that is where it will be moved to. Otherwise you can add the name of one or more specific physical volumes that should be used to satisfy the request, instead of any physical volume in the volume group with free space. This process can be resumed safely if interrupted by a crash or power failure, and can be done while the logical volume(s) in question are in use. You can also add to perform the move in the background and return immediately, or to have it print how much progress it has made every seconds. If you background the move, you can check its progress with the command.\n\nWhen you create a snapshot, you create a new logical volume to act as a clone of the original logical volume. The snapshot volume does not initially use any space, but as changes are made to the original volume, the changed blocks are copied to the snapshot volume before they are changed in order to preserve them. This means that the more changes you make to the origin, the more space the snapshot needs. If the snapshot volume uses all of the space allocated to it, then the snapshot is broken and can not be used any more, leaving you with only the modified origin. The command will tell you how much space has been used in a snapshot logical volume. If it starts to get full, you might want to extend it with the command. To create a snapshot of the bar logical volume and name it , run: This will create a snapshot named of the original logical volume and allocate 5 GB of space for it. Since the snapshot volume only stores the areas of the disk that have changed since it was created, it can be much smaller than the original volume. While you have the snapshot, you can mount it if you wish to see the original filesystem as it appeared when you made the snapshot. In the above example you would mount the device. You can modify the snapshot without affecting the original, and the original without affecting the snapshot. For example, if you take a snapshot of your root logical volume, make changes to your system, and then decide you would like to revert the system back to its previous state, you can merge the snapshot back into the original volume, which effectively reverts it to the state it was in when you made the snapshot. To do this, you can run: If the origin volume of is in use, it will inform you that the merge will take place the next time the volumes are activated. If this is the root volume, then you will need to reboot for this to happen. At the next boot, the volume will be activated and the merge will begin in the background, so your system will boot up as if you had never made the changes since the snapshot was created, and the actual data movement will take place in the background while you work."
    },
    {
        "link": "https://help.ubuntu.com/community/UbuntuDesktopLVM",
        "document": "LVM stands for Logical Volume Manager (or Logical Volume Management). Instead of using all physical volumes for your system (hard to change, static), take one (or many) disk(s) and have that drive logically divided for expandability, better monitoring and all around better disk management on your computer. If you wish to learn more, please refer to the links below:\n• Keep the terminal open as we will be creating partitions using the command fdisk. In the example below, I will be setting up my desktop using the /dev/sda patition (to find yours, do the command 'sudo fdisk -l' and make sure you know much your storage is on the machine so you can figure out what physical device you want to use): ubuntu@ubuntu:~$ sudo fdisk /dev/sda Device contains neither a valid DOS partition table, nor Sun, SGI or OSF disklabel Building a new DOS disklabel with disk identifier 0x568311d6. Changes will remain in memory only, until you decide to write them. After that, of course, the previous content won't be recoverable. Warning: invalid flag 0x0000 of partition table 4 will be corrected by w(rite) WARNING: DOS-compatible mode is deprecated. It's strongly recommended to switch off the mode (command 'c') and change display units to sectors (command 'u'). Command (m for help): n Command action e extended p primary partition (1-4) p Partition number (1-4): 1 First cylinder (1-5221, default 1): Using default value 1 Last cylinder, +cylinders or +size{K,M,G} (1-5221, default 5221): +1G Command (m for help): n Command action e extended p primary partition (1-4) p Partition number (1-4): 2 First cylinder (133-5221, default 133): Using default value 133 Last cylinder, +cylinders or +size{K,M,G} (133-5221, default 5221): Using default value 5221 Command (m for help): t Partition number (1-4): 2 Hex code (type L to list codes): 8e Changed system type of partition 2 to 8e (Linux LVM) Command (m for help): p Disk /dev/sda: 42.9 GB, 42949672960 bytes 255 heads, 63 sectors/track, 5221 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x568311d6 Device Boot Start End Blocks Id System /dev/sda1 1 132 1060258+ 83 Linux /dev/sda2 133 5221 40877392+ 8e Linux LVM Command (m for help): w The partition table has been altered! Calling ioctl() to re-read partition table. Syncing disks. The reason for this setup is explained below:\n• This part will help set up your logical volumes for your desktop. In this example, there will be three different logical volumes using one volume group and one physical volume: Create the physical volume: This gives the LVM an understand of what physical volumes it has to offer for volume groups and logical volumes during your setup. Create a group using our newly create physical volume: This part creates a new volume group which is tided to a physical volume. A physical disk can not be can not be used on multiple violume groups (unless you partition that disk before hand like we did above). The name of this volume group is 'sysvg' but you can name it whatever you wish. Create the swap logical volume using the newly create sysvg group: I need a swap for the system so 1GB will do in this case (you can change the size to whatever you want). The name can be whatever you want it to be but I like to keep a naming scheme when creating VG's and LV's. Then assign it to a VG group that you made from before and you will have your first LV done. Create the root logical volume using the newly create sysvg group: Again, like the swap but this will be used for my root file system. The size and name is up to you and make sure you point it to your VG that was created above. Create the home logical volume using the newly create sysvg group: Without getting into a lot of details, this will create a logical volume called 'home' using 100% of whats left on my VG. The percentage and name is up to you and make sure you point it to your VG that was created above. You do not need to use the rest of your disk space on home or any of the above created.\n• None The partition /dev/sda1 will be the /boot partition for your system and you can select whatever file system you wish (I would advise on ext4 or better). Now lets scroll up to the top and highlight the /dev/mapper/sysvg-lvhome partition that has the \"ext4\" next to it. Press the \"Change...\" button and see picture 1.4 for what this partition should look like:\n• None This will be the /home directory for my system using ext4. Again, the file system is your choice and doesn't need to be ext4. Do this same process for /dev/mapper/sysvg-root and /dev/mapper/sysvg-lvswap by highlighting them and pressing the \"Change...\" button. See pictures 1.5 and picture 1.6 for what you should set up for both:\n• None If you are all set up for your system to have all proper logical volumes, you press the \"Install Now\" button to start the installation process. This installation will ask for you additional information like your timezone, username, etc... please fill that in with what you want for your system. Once you are done, you can sit back and wait for the whole thing to comeplete. Once the installation process is complete, you will see picture 1.7 in front of you:"
    },
    {
        "link": "https://tldp.org/HOWTO/LVM-HOWTO",
        "document": ""
    },
    {
        "link": "https://askubuntu.com/questions/3596/what-is-lvm-and-what-is-it-used-for",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://redhat.com/en/blog/creating-logical-volumes",
        "document": "Logical Volumes ( ) are the final storage unit in the standard LVM architecture. These units are created from the volume group, which is made up of physical volumes ( ). If you have been following along in the series, you will have initialized your physical volumes and combined them into a volume group ( ). We will be continuing our LVM exploration by further splicing our newly created volume group into various logical volumes.\n\nIf you are interested in the previous articles, you can review part 1 and part 2.\n\nAs with all things, variation is the spice of life, and that's no different when it comes to technology—specifically, logical volumes. You have a few different options at your disposal here, and they each have unique use cases that sysadmins can employ to best fit a given situation. Your options are as follows:\n\nI will be discussing each of these volume types in further detail, walking you through examples of when and why you would want to use each one. I will also walk you through a basic configuration of each. Let's get to it!\n\nLinear logical volumes are the LVM default when it comes to logical volume creation. They are generally used to combine one or more disks to create one usable storage unit. We created a volume group called in our last article. That volume group was created by joining two unique physical volumes. Here I am going to use a small portion of that volume group to create a linear logical volume titled, very creatively, . Seen below:\n\nYou can use the for detailed information on the logical volumes currently in existence on your system.\n\nYou can also use the command if verbosity isn't your thing:\n\nThese are the most common logical volume types and are very straightforward to create.\n\nStriped logical volumes allow the administrator to control the way that the data is written to the physical volumes. For high volume read/write scenarios, striped logical volumes would be ideal, as they allow for read and write operations to be done in parallel.\n\nWhen using striped logical volumes, you can set the number of stripes (this number cannot exceed the number of physical volumes) and the stripe size. This allows the user a greater level of control over how I/O is performed on the system.\n\nHere, we are going to create a striped logical volume of 500Mb. The denotes the number of stripes (because we only have two physical volumes, we are using two). The denotes the size of the stripes as the default 64Kb. We named the striped volume , and it is a part of the volume group .\n\nNow, using the command, you can see both the linear volume and the newly created striped volume:\n\nThese kinds of logical volumes are really helpful in the right circumstances! If you need high volume read/write, consider striping your volumes.\n\nMirrored logical volumes do exactly what you would expect them to do. They allow you to \"reflect\" the data on one device to an identical copy. This ensures your data is available. If one part of the mirror breaks, the remaining drive changes its characteristics to that of a linear volume and is still accessible. LVM keeps a log on what data is where, which allows the changes to be persistent. Let's look at how to create a mirror with LVM.\n\nYou can see that we created a mirror drive of , named the mirror , and created it on the volume group. All of this was done using the same command from the previous examples. We can verify creation by using the command.\n\nSo we looked at logical volumes as a whole, the three kinds of logical volumes that LVM allows you to create, and how to configure these volumes. LVM allows you to create a storage unit to fit almost any need you may have as an administrator, and that's what makes it such a great utility. I recommend that you give LVM a try the next time you need to accomplish any disk manipulation. In my opinion, there is no better tool for the job!\n\n[ Want to try out Red Hat Enterprise Linux? Download it now for free. ]"
    },
    {
        "link": "https://tecmint.com/manage-and-create-lvm-parition-using-vgcreate-lvcreate-and-lvextend",
        "document": "Because of the changes in the LFCS exam requirements effective Feb. 2, 2016, we are adding the necessary topics to the LFCS series published here. To prepare for this exam, your are highly encouraged to use the LFCE series as well.\n\nOne of the most important decisions while installing a Linux system is the amount of storage space to be allocated for system files, home directories, and others. If you make a mistake at that point, growing a partition that has run out of space can be burdensome and somewhat risky.\n\nLogical Volumes Management (also known as LVM), which have become a default for the installation of most (if not all) Linux distributions, have numerous advantages over traditional partitioning management. Perhaps the most distinguishing feature of LVM is that it allows logical divisions to be resized (reduced or increased) at will without much hassle.\n\nThe structure of the LVM consists of:\n• One or more entire hard disks or partitions are configured as physical volumes (PVs).\n• A volume group (VG) is created using one or more physical volumes. You can think of a volume group as a single storage unit.\n• Multiple logical volumes can then be created in a volume group. Each logical volume is somewhat equivalent to a traditional partition – with the advantage that it can be resized at will as we mentioned earlier.\n\nIn this article we will use three disks of 8 GB each (/dev/sdb, /dev/sdc, and /dev/sdd) to create three physical volumes. You can either create the PVs directly on top of the device, or partition it first.\n\nAlthough we have chosen to go with the first method, if you decide to go with the second (as explained in Part 4 – Create Partitions and File Systems in Linux of this series) make sure to configure each partition as type .\n\nTo create physical volumes on top of /dev/sdb, /dev/sdc, and /dev/sdd, do:\n\nYou can list the newly created PVs with:\n\nand get detailed information about each PV with:\n\nIf you omit as parameter, you will get information about all the PVs.\n\nTo create a volume group named using and (we will save for later to illustrate the possibility of adding other devices to expand storage capacity when needed):\n\nAs it was the case with physical volumes, you can also view information about this volume group by issuing:\n\nSince is formed with two 8 GB disks, it will appear as a single 16 GB drive:\n\nWhen it comes to creating logical volumes, the distribution of space must take into consideration both current and future needs. It is considered good practice to name each logical volume according to its intended use.\n\nFor example, let’s create two LVs named (10 GB) and (remaining space), which we can use later to store project documentation and system backups, respectively.\n\nThe option is used to indicate a name for the LV, whereas sets a fixed size and (lowercase L) is used to indicate a percentage of the remaining space in the container VG.\n\nAs before, you can view the list of LVs and basic information with:\n\nTo view information about a single LV, use lvdisplay with the VG and LV as parameters, as follows:\n\nIn the image above we can see that the LVs were created as storage devices (refer to the LV Path line). Before each logical volume can be used, we need to create a filesystem on top of it.\n\nWe’ll use ext4 as an example here since it allows us both to increase and reduce the size of each LV (as opposed to xfs that only allows to increase the size):\n\nIn the next section we will explain how to resize logical volumes and add extra physical storage space when the need arises to do so.\n\nNow picture the following scenario. You are starting to run out of space in , while you have plenty of space available in . Due to the nature of LVM, we can easily reduce the size of the latter (say 2.5 GB) and allocate it for the former, while resizing each filesystem at the same time.\n\nFortunately, this is as easy as doing:\n\nIt is important to include the minus or plus signs while resizing a logical volume. Otherwise, you’re setting a fixed size for the LV instead of resizing it.\n\nIt can happen that you arrive at a point when resizing logical volumes cannot solve your storage needs anymore and you need to buy an extra storage device. Keeping it simple, you will need another disk. We are going to simulate this situation by adding the remaining PV from our initial setup ( ).\n\nTo add to , do\n\nIf you run before and after the previous command, you will see the increase in the size of the VG:\n\nNow you can use the newly added space to resize the existing LVs according to your needs, or to create additional ones as needed.\n\nMounting Logical Volumes on Boot and on Demand\n\nOf course there would be no point in creating logical volumes if we are not going to actually use them! To better identify a logical volume we will need to find out what its (a non-changing attribute that uniquely identifies a formatted storage device) is.\n\nTo do that, use blkid followed by the path to each device:\n\nand insert the corresponding entries in (make sure to use the UUIDs obtained before):\n\nThen save the changes and mount the LVs:\n\nWhen it comes to actually using the LVs, you will need to assign proper permissions as explained in Part 8 – Manage Users and Groups in Linux of this series.\n\nIn this article we have introduced Logical Volume Management, a versatile tool to manage storage devices that provides scalability. When combined with RAID (which we explained in Part 6 – Create and Manage RAID in Linux of this series), you can enjoy not only scalability (provided by LVM) but also redundancy (offered by RAID).\n\nIn this type of setup, you will typically find LVM on top of RAID, that is, configure RAID first and then configure LVM on top of it.\n\nIf you have questions about this article, or suggestions to improve it, feel free to reach us using the comment form below."
    },
    {
        "link": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html-single/configuring_and_managing_logical_volumes/index",
        "document": "You can create and use volume groups (VGs) to manage and resize multiple physical volumes (PVs) combined into a single storage entity. Extents are the smallest units of space that you can allocate in LVM. Physical extents (PE) and logical extents (LE) has the default size of 4 MiB that you can configure. All extents have the same size. When you create a logical volume (LV) within a VG, LVM allocates physical extents on the PVs. The logical extents within the LV correspond one-to-one with physical extents in the VG. You do not need to specify the PEs to create LVs. LVM will locate the available PEs and piece them together to create a LV of the requested size. Within a VG, you can create multiple LVs, each acting like a traditional partition but with the ability to span across physical volumes and resize dynamically. VGs can manage the allocation of disk space automatically. You can use the command to create a volume group (VG). You can adjust the extent size for very large or very small volumes to optimize performance and storage efficiency. You can specify the extent size when creating a VG. To change the extent size you must re-create the volume group.\n• One or more physical volumes are created. For more information about creating physical volumes, see Creating LVM physical volume.\n• None List and identify the PV that you want to include in the VG:\n• None Replace VolumeGroupName with the name of the volume group that you want to create. Replace PhysicalVolumeName with the name of the PV. To specify the extent size when creating a VG, use the option. Replace ExtentSize with the size of the extent. If you provide no size suffix, the command defaults to MB.\n• None Verify that the VG is created:\n• , , and man pages on your system Create volume groups from one or more physical drives or other storage devices. Logical volumes are created from volume groups. Each volume group can include multiple logical volumes.\n• None You have installed the RHEL 8 web console. For instructions, see Installing and enabling the web console.\n• The package is installed on your system.\n• Physical drives or other types of storage devices from which you want to create volume groups.\n• None Log in to the RHEL 8 web console. For details, see Logging in to the web console.\n• In the table, click the menu button.\n• In the field, enter a name for the volume group. The name must not include spaces.\n• None Select the drives you want to combine to create the volume group. The RHEL web console displays only unused block devices. If you do not see your device in the list, make sure that it is not being used by your system, or format it to be empty and unused. Used devices include, for example:\n• Physical volumes being a member of another software RAID device\n• On the page, check whether the new volume group is listed in the table. You can use the command to rename a volume group (VG).\n• One or more physical volumes are created. For more information about creating physical volumes, see Creating LVM physical volume.\n• The volume group is created. For more information about creating volume groups, see Section 3.1, “Creating an LVM volume group”.\n• None List and identify the VG that you want to rename:\n• None Replace OldVolumeGroupName with the name of the VG. Replace NewVolumeGroupName with the new name for the VG.\n• None Verify that the VG has a new name: You can use the command to add physical volumes (PVs) to a volume group (VG).\n• One or more physical volumes are created. For more information about creating physical volumes, see Creating LVM physical volume.\n• The volume group is created. For more information about creating volume groups, see Section 3.1, “Creating an LVM volume group”.\n• None List and identify the VG that you want to extend:\n• None List and identify the PVs that you want to add to the VG:\n• None Replace VolumeGroupName with the name of the VG. Replace PhysicalVolumeName with the name of the PV.\n• None Verify that the VG now includes the new PV: You can combine two existing volume groups (VGs) with the command. The source volume will be merged into the destination volume.\n• One or more physical volumes are created. For more information about creating physical volumes, see Creating LVM physical volume.\n• Two or more volume group are created. For more information about creating volume groups, see Section 3.1, “Creating an LVM volume group”.\n• None List and identify the VG that you want to merge:\n• None Merge the source VG into the destination VG: Replace VolumeGroupName2 with the name of the source VG. Replace VolumeGroupName1 with the name of the destination VG.\n• None Verify that the VG now includes the new PV: To remove unused physical volumes (PVs) from a volume group (VG), use the command. The command shrinks a volume group’s capacity by removing one or more empty physical volumes. This frees those physical volumes to be used in different volume groups or to be removed from the system.\n• None If the physical volume is still being used, migrate the data to another physical volume from the same volume group:\n• None If there are not enough free extents on the other physical volumes in the existing volume group:\n• None Add the newly created physical volume to the volume group:\n• None Move the data from /dev/vdb3 to /dev/vdb4:\n• None Remove the physical volume /dev/vdb3 from the volume group:\n• None Verify that the /dev/vdb3 physical volume is removed from the VolumeGroupName volume group:\n• , , and man pages on your system If there is enough unused space on the physical volumes, a new volume group can be created without adding new disks. In the initial setup, the volume group VolumeGroupName1 consists of /dev/vdb1, /dev/vdb2, and /dev/vdb3. After completing this procedure, the volume group VolumeGroupName1 will consist of /dev/vdb1 and /dev/vdb2, and the second volume group, VolumeGroupName2, will consist of /dev/vdb3.\n• You have sufficient space in the volume group. Use the command to determine how much free space is currently available in the volume group.\n• Depending on the free capacity in the existing physical volume, move all the used physical extents to other physical volume using the command. For more information, see Removing physical volumes from a volume group.\n• None Split the existing volume group VolumeGroupName1 to the new volume group VolumeGroupName2: If you have created a logical volume using the existing volume group, use the following command to deactivate the logical volume:\n• None View the attributes of the two volume groups:\n• None Verify that the newly created volume group VolumeGroupName2 consists of /dev/vdb3 physical volume:\n• , , and man pages on your system You can move an entire LVM volume group (VG) to another system using the following commands: Use this command on an existing system to make an inactive VG inaccessible to the system. Once the VG is inaccessible, you can detach its physical volumes (PV). Use this command on the other system to make the VG, which was inactive in the old system, accessible in the new system.\n• No users are accessing files on the active volumes in the volume group that you are moving.\n• None Deactivate all logical volumes in the volume group, which prevents any further activity on the volume group:\n• None Export the volume group to prevent it from being accessed by the system from which you are removing it:\n• None # PV /dev/sda1 is in exported VG VolumeGroupName [17.15 GB / 7.15 GB free] PV /dev/sdc1 is in exported VG VolumeGroupName [17.15 GB / 15.15 GB free] PV /dev/sdd1 is in exported VG VolumeGroupName [17.15 GB / 15.15 GB free] ...\n• Shut down your system and unplug the disks that make up the volume group and connect them to the new system.\n• None Plug the disks into the new system and import the volume group to make it accessible to the new system: You can use the argument of the command to import volume groups that are missing physical volumes and subsequently run the command.\n• None Mount the file system to make it available for use:\n• , , and man pages on your system You can remove an existing volume group using the command. Only volume groups that do not contain logical volumes can be removed.\n• None Ensure the volume group does not contain logical volumes: Replace VolumeGroupName with the name of the volume group.\n• None Replace VolumeGroupName with the name of the volume group. In a cluster environment, LVM uses the <qualifier> to coordinate access to volume groups shared among multiple machines. You must stop the before removing a volume group to make sure no other node is trying to access or modify it during the removal process.\n• The volume group contains no logical volumes.\n• None Ensure the volume group does not contain logical volumes: Replace VolumeGroupName with the name of the volume group.\n• None Stop the on all nodes except the node where you are removing the volume group: Replace VolumeGroupName with the name of the volume group and wait for the lock to stop.\n• None Replace VolumeGroupName with the name of the volume group.\n\nWith LVM, you can do the following tasks:\n• Create new logical volumes to extend storage capabilities of your system\n• Safely remove volumes when they are no longer needed\n• Activate or deactivate volumes to control the system’s access to its data With the Logical Volume Manager (LVM), you can manage disk storage in a flexible and efficient way that traditional partitioning schemes cannot offer. Below is a summary of key LVM features that are used for storage management and optimization. Concatenation involves combining space from one or more physical volumes into a singular logical volume, effectively merging the physical storage. Striping optimizes data I/O efficiency by distributing data across multiple physical volumes. This method enhances performance for sequential reads and writes by allowing parallel I/O operations. LVM supports RAID levels 0, 1, 4, 5, 6, and 10. When you create a RAID logical volume, LVM creates a metadata subvolume that is one extent in size for every data or parity subvolume in the array. Thin provisioning enables the creation of logical volumes that are larger than the available physical storage. With thin provisioning, the system dynamically allocates storage based on actual usage instead of allocating a predetermined amount upfront. With LVM snapshots, you can create point-in-time copies of logical volumes. A snapshot starts empty. As changes occur on the original logical volume, the snapshot captures the pre-change states through copy-on-write (CoW), growing only with changes to preserve the state of the original logical volume. LVM supports the use of fast block devices, such as SSD drives as write-back or write-through caches for larger slower block devices. Users can create cache logical volumes to improve the performance of their existing logical volumes or create new cache logical volumes composed of a small and fast device coupled with a large and slow device. LVM provides a flexible approach to handling disk storage by abstracting the physical layer into logical volumes that can be created and adjusted based on your needs. With linear logical volumes (LVs), you can merge multiple physical storage units into one virtual storage space. You can easily expand or reduce linear LVs to accommodate the data requirements.\n• The volume group is created. For more information, see Creating LVM volume group.\n• None List the names of volume groups and their size:\n• None Replace LogicalVolumeName with the name of the LV. Replace VolumeSize with the size for the LV. If no size suffix is provided the command defaults to MB. Replace VolumeGroupName with the name of the volume group.\n• None Verify that the linear LV is created: 4.2.2. Creating or resizing a logical volume by using the RHEL system role Use the role to perform the following tasks:\n• To create an LVM logical volume in a volume group consisting of many disks\n• To resize an existing file system on LVM\n• To express an LVM volume size in percentage of the pool’s total size If the volume group does not exist, the role creates it. If a logical volume exists in the volume group, it is resized if the size does not match what is specified in the playbook. If you are reducing a logical volume, to prevent data loss you must ensure that the file system on that logical volume is not using the space in the logical volume that is being reduced.\n• You have prepared the control node and the managed nodes\n• You are logged in to the control node as a user who can run playbooks on the managed nodes.\n• The account you use to connect to the managed nodes has permissions on them.\n• None Create a playbook file, for example , with the following content: --- - name: Manage local storage hosts: managed-node-01.example.com tasks: - name: Create logical volume ansible.builtin.include_role: name: rhel-system-roles.storage vars: storage_pools: - name: myvg disks: - sda - sdb - sdc volumes: - name: mylv size: 2G fs_type: ext4 mount_point: /mnt/data The settings specified in the example playbook include the following: You must specify the size by using units (for example, GiB) or percentage (for example, 60%). For details about all variables used in the playbook, see the file on the control node.\n• None Note that this command only validates the syntax and does not protect against a wrong but valid configuration.\n• None Verify that specified volume has been created or resized to the requested size: With striped logical volume (LV), you can distribute the data across multiple physical volumes (PVs), potentially increasing the read and write speed by utilizing the bandwidth of multiple disks simultaneously. When creating a striped LV, it is important to consider the stripe number and size. The stripe number is the count of PVs across which data is distributed. Increasing the stripe number can enhance performance by utilizing multiple disks concurrently. Stripe size is the size of the data chunk written to each disk in the stripe set before moving to the next disk and is specified in kilobytes (KB). The optimal stripe size depends on your workload and the filesystem block size. The default is 64KB and can be adjusted.\n• None List the names of volume groups and their size:\n• None Replace NumberOfStripes with the number of stripes. Replace StripeSize with the stripe size in kilobytes. The is not a required option. If you do not specify the stripe size it defaults to 64KB. Replace LogicalVolumeName with the name of the LV. Replace VolumeGroupName with the name of the volume group.\n• None Verify that the striped LV is created: RAID logical volumes enable you to use multiple disks for redundancy and performance. LVM supports various RAID levels, including RAID0, RAID1, RAID4, RAID5, RAID6, and RAID10. With LVM you can create striped RAIDs (RAID0, RAID4, RAID5, RAID6), mirrored RAID (RAID1), or a combination of both (RAID10). RAID 4, RAID 5, and RAID 6 offer fault tolerance by storing parity data that can be used to reconstruct lost information in case of a disk failure. When creating RAID LVs, place each stripe on a separate PV. The number of stripes equals to the number of PVs that should be in the volume group (VG). Uses first device to store parity Uses an extra device to store parity Uses two extra devices to store parity\n• None List the names of volume groups and their size:\n• None\n• None Replace level with the RAID level 0, 4, 5, or 6. Replace NumberOfStripes with the number of stripes. Replace StripeSize with the stripe size in kilobytes. Replace Size with the size of the LV. Replace LogicalVolumeName with the name of the LV.\n• None Replace MirrorsNumber with the number of mirrors. Replace Size with the size of the LV. Replace LogicalVolumeName with the name of the LV.\n• None To create a mirrored and striped RAID, use: Replace MirrorsNumber with the number of mirrors. Replace NumberOfStripes with the number of stripes. Replace StripeSize with the stripe size in kilobytes. Replace Size with the size of the LV. Replace LogicalVolumeName with the name of the LV.\n• None Verify that the RAID LV is created: Under thin provisioning, physical extents (PEs) from a volume group (VG) are allocated to create a thin pool with a specific physical size. Logical volumes (LVs) are then allocated from this thin pool based on a virtual size, not limited by the pool’s physical capacity. With this, the virtual size of each thin LV can exceed the actual size of the thin pool leading to over-provisioning, when the collective virtual sizes of all thin LVs surpasses the physical capacity of the thin pool. Therefore, it is essential to monitor both logical and physical usage closely to avoid running out of space and outages. Thin provisioning optimizes storage efficiency by allocating space as needed, lowering initial costs and improving resource utilization. However, when using thin LVs, beware of the following drawbacks:\n• Improper discard handling can block the release of unused storage space, causing full allocation of the space over time.\n• Copy on Write (CoW) operation can be slower on file systems with snapshots.\n• Data blocks can be intermixed between multiple file systems leading to random access limitations.\n• You have created a physical volume. For more information, see Creating LVM physical volume.\n• You have created a volume group. For more information, see Creating LVM volume group.\n• You have created a logical volume. For more information, see Creating logical volumes.\n• None List the names of volume groups and their size:\n• None Replace PoolSize with the maximum amount of disk space the thin pool can use. Replace ThinPoolName with the name for the thin pool. Replace VolumeGroupName with the name of the volume group.\n• None Replace MaxVolumeSize with the maximum size the volume can grow to within the thin pool. Replace ThinPoolName with the name for the thin pool. Replace VolumeGroupName with the name of the volume group. You can create other thin LVs within the same thin pool.\n• None Verify that the thin LV is created: With Logical Volume Manager (LVM), you can resize logical volumes (LVs) as needed without affecting the data stored on them. You can extend linear (thick) LVs and their snapshots with the command.\n• None Ensure your volume group has enough space to extend your LV:\n• None Extend the linear LV and resize the file system: Replace AdditionalSize with how much space to add to the LV. The default unit of measurement is megabytes, but you can specify other units. Replace VolumeGroupName with the name of the volume group. Replace LogicalVolumeName with the name of the thin volume.\n• None Verify that the linear LV is extended: You can extend the thin logical volume (LV) with the command.\n• None Ensure the thin pool has enough space for the data you plan to add:\n• None Extend the thin LV and resize the file system: Replace AdditionalSize with how much space to add to the LV. The default unit of measurement is megabytes, but you can specify other units. Replace VolumeGroupName with the name of the volume group. Replace ThinVolumeName with the name of the thin volume. The virtual size of thin logical volumes can exceed the physical capacity of the thin pool resulting in over-provisioning. To prevent running out of space, you must monitor and periodically extend the capacity of the thin pool. The metric indicates the percentage of the allocated data space that the thin pool currently uses. The metric reflects the percentage of space used for storing metadata, which is essential for managing the mappings within the thin pool. Monitoring these metrics is vital to ensure efficient storage management and to avoid capacity issues. LVM provides the option to manually extend the data or metadata capacity as needed. Alternatively, you can enable monitoring and automate the expansion of your thin pool. Logical Volume Manager (LVM) provides the option to manually extend the data segment, the metadata segment, or the thin pool. You can use the command to extend the thin pool.\n• None Display the data and metadata space used:\n• None Replace Size with the new size for your thin pool. Replace VolumeGroupName with the name of the volume group. Replace ThinPoolName with the name of the thin pool. The data size will be extended. The metadata size will be extended if necessary.\n• None Verify that the thin pool is extended: You can use the command to extend the segment.\n• None Replace Size with the size for your data segment. Replace VolumeGroupName with name of the volume group. Replace ThinPoolName with the name of the thin pool.\n• None Verify that the segment is extended: You can use the command to extend the segment.\n• None Replace Size with the size for your metadata segment. Replace VolumeGroupName with name of the volume group. Replace ThinPoolName with the name of the thin pool.\n• None Verify that the segment is extended: You can automate the expansion of your thin pool by enabling monitoring and setting the and the configuration parameters.\n• None Check if the thin pool is monitored:\n• None Replace VolumeGroupName with the name of the volume group. Replace ThinPoolName with the name of the thin pool.\n• As the user, open the file in an editor of your choice.\n• None Uncomment the and lines and set each parameter to a required value: determines the percentage at which LVM starts to auto-extend the thin pool. For example, setting it to 70 means LVM will try to extend the thin pool when it reaches 70% capacity. specifies by what percentage the thin pool should be extended when it reaches threshold. For example, setting it to 20 means the thin pool will be increased by 20% of its current size.\n• Save the changes and exit the editor. When the size of the LV is reduced, the freed up logical extents are returned to the volume group and then can be used by other LVs. Data stored in the reduced area is lost. Always back up the data and resize the file system before proceeding.\n• None List the logical volumes and their volume groups:\n• None Check where the logical volume is mounted: Replace /dev/VolumeGroupName/LogicalVolumeName with the path to your logical volume.\n• None Replace /MountPoint with the mounting point for your logical volume.\n• None Resize the LV and the file system: Replace TargetSize with the new size of the LV. Replace VolumeGroupName/LogicalVolumeName with the path to your logical volume.\n• None Replace /MountPoint with the mounting point for your file system.\n• None Verify the space usage of the file system: # Filesystem Type Size Used Avail Use% Mounted on /dev/mapper/VolumeGroupName-NewLogicalVolumeName ext4 2.9G 139K 2.7G 1% /MountPoint Replace /MountPoint with the mounting point for your logical volume.\n• None Verify the size of the LV: You can rename an existing logical volume, including snapshots, using the command.\n• None List the logical volumes and their volume groups:\n• None Replace VolumeGroupName with name of the volume group. Replace LogicalVolumeName with the name of the logical volume. Replace NewLogicalVolumeName with the new logical volume name.\n• None Verify that the logical volume is renamed: You can remove an existing logical volume, including snapshots, using the command.\n• None List the logical volumes and their paths:\n• None Check where the logical volume is mounted: Replace /dev/VolumeGroupName/LogicalVolumeName with the path to your logical volume.\n• None Replace /MountPoint with the mounting point for your logical volume.\n• None Replace VolumeGroupName/LogicalVolumeName with the path to your logical volume. You can activate the logical volume with the command.\n• None List the logical volumes, their volume groups, and their paths:\n• None Replace VolumeGroupName with the name of the volume group. Replace LogicalVolumeName with the name of the logical volume. When activating a thin LV that was created as a snapshot of another LV, you might need to use the option to activate it.\n• None Verify that the LV is active: Replace VolumeGroupName with the name of the volume group. Replace LogicalVolumeName with the name of the logical volume. By default, when you create a logical volume, it is in an active state. You can deactivate the logical volume with the command. Deactivating a logical volume with active mounts or in use can lead to data inconsistencies and system errors.\n• None List the logical volumes, their volume groups, and their paths:\n• None Check where the logical volume is mounted: Replace /dev/VolumeGroupName/LogicalVolumeName with the path to your logical volume.\n• None Replace /MountPoint with the mounting point for your logical volume.\n• None Replace VolumeGroupName with name of the volume group. Replace LogicalVolumeName with the name of the logical volume.\n• None Verify that the LV is not active: Replace VolumeGroupName with name of the volume group. Replace LogicalVolumeName with the name of the logical volume.\n• Snapshots, which are point-in-time copies of logical volumes (LVs)\n• Caching, with which you can use faster storage as a cache for slower storage A snapshot is a logical volume (LV) that mirrors the content of another LV at a specific point in time. When you create a snapshot, you are creating a new LV that serves as a point-in-time copy of another LV. Initially, the snapshot LV contains no actual data. Instead, it references the data blocks of the original LV at the moment of snapshot creation. It is important to regularly monitor the snapshot’s storage usage. If a snapshot reaches 100% of its allocated space, it will become invalid. It is essential to extend the snapshot before it gets completely filled. This can be done manually by using the command or automatically via the file. When data on the original LV changes, the copy-on-write (CoW) system copies the original, unchanged data to the snapshot before the change is made. This way, the snapshot grows in size only as changes occur, storing the state of the original volume at the time of the snapshot’s creation. Thick snapshots are a type of LV that requires you to allocate some amount of storage space upfront. This amount can later be extended or reduced, however, you should consider what type of changes you intend to make to the original LV. This helps you to avoid either wasting resources by allocating too much space or needing to frequently increase the snapshot size if you allocate too little. Thin snapshots are a type of LV created from an existing thin provisioned LV. Thin snapshots do not require allocating extra space upfront. Initially, both the original LV and its snapshot share the same data blocks. When changes are made to the original LV, it writes new data to different blocks, while the snapshot continues to reference the original blocks, preserving a point-in-time view of the LV’s data at the snapshot creation. Thin provisioning is a method of optimizing and managing storage efficiently by allocating disk space on an as-needed basis. This means that you can create multiple LVs without needing to allocate a large amount of storage upfront for each LV. The storage is shared among all LVs in a thin pool, making it a more efficient use of resources. A thin pool allocates space on-demand to its LVs. The choice between thick or thin LV snapshots is directly determined by the type of LV you are taking a snapshot of. If your original LV is a thick LV, your snapshots will be thick. If your original LV is thin, your snapshots will be thin. When you create a thick LV snapshot, it is important to consider the storage requirements and the intended lifespan of your snapshot. You need to allocate enough storage for it based on the expected changes to the original volume. The snapshot must have a sufficient size to capture changes during its intended lifespan, but it cannot exceed the size of the original LV. If you expect a low rate of change, a smaller snapshot size of 10%-15% might be sufficient. For LVs with a high rate of change, you might need to allocate 30% or more. It is essential to extend the snapshot before it gets completely filled. If a snapshot reaches 100% of its allocated space, it becomes invalid. You can monitor the snapshot capacity with the command. You can create a thick LV snapshot with the command.\n• You have created a physical volume. For more information, see Creating LVM physical volume.\n• You have created a volume group. For more information, see Creating LVM volume group.\n• You have created a logical volume. For more information, see Creating logical volumes.\n• None Identify the LV of which you want to create a snapshot: The size of the snapshot cannot exceed the size of the LV.\n• None Replace SnapshotSize with the size you want to allocate for the snapshot (e.g. 10G). Replace SnapshotName with the name you want to give to the snapshot logical volume. Replace VolumeGroupName with the name of the volume group that contains the original logical volume. Replace LogicalVolumeName with the name of the logical volume that you want to create a snapshot of.\n• None Verify that the snapshot is created: If a snapshot reaches 100% of its allocated space, it becomes invalid. It is essential to extend the snapshot before it gets completely filled. This can be done manually by using the command.\n• None List the names of volume groups, logical volumes, source volumes for snapshots, their usage percentages, and sizes:\n• None Replace AdditionalSize with how much space to add to the snapshot (for example, +1G). Replace VolumeGroupName with the name of the volume group. Replace SnapshotName with the name of the snapshot.\n• None Verify that the LV is extended: If a snapshot reaches 100% of its allocated space, it becomes invalid. It is essential to extend the snapshot before it gets completely filled. This can be done automatically.\n• As the user, open the file in an editor of your choice.\n• None Uncomment the and lines and set each parameter to a required value: determines the percentage at which LVM starts to auto-extend the snapshot. For example, setting the parameter to 70 means that LVM will try to extend the snapshot when it reaches 70% capacity. specifies by what percentage the snapshot should be extended when it reaches the threshold. For example, setting the parameter to 20 means the snapshot will be increased by 20% of its current size.\n• Save the changes and exit the editor. You can merge thick LV snapshot into the original logical volume from which the snapshot was created. The process of merging means that the original LV is reverted to the state it was in when the snapshot was created. Once the merge is complete, the snapshot is removed. The merge between the original and snapshot LV is postponed if either is active. It only proceeds once the LVs are reactivated and not in use.\n• None List the LVs, their volume groups, and their paths:\n• None Check where the LVs are mounted: Replace /dev/VolumeGroupName/LogicalVolumeName with the path to your logical volume. Replace /dev/VolumeGroupName/SnapshotName with the path to your snapshot.\n• None Replace /LogicalVolume/MountPoint with the mounting point for your logical volume. Replace /Snapshot/MountPoint with the mounting point for your snapshot.\n• None Replace VolumeGroupName with the name of the volume group. Replace LogicalVolumeName with the name of the logical volume. Replace SnapshotName with the name of your snapshot.\n• None Merge the thick LV snapshot into the origin: Replace SnapshotName with the name of the snapshot.\n• None Replace VolumeGroupName with the name of the volume group. Replace LogicalVolumeName with the name of the logical volume.\n• None Replace /LogicalVolume/MountPoint with the mounting point for your logical volume.\n• None Verify that the snapshot is removed: Thin provisioning is appropriate where storage efficiency is a priority. Storage space dynamic allocation reduces initial storage costs and maximizes the use of available storage resources. In environments with dynamic workloads or where storage grows over time, thin provisioning allows for flexibility. It enables the storage system to adapt to changing needs without requiring large upfront allocations of the storage space. With dynamic allocation, over-provisioning is possible, where the total size of all LVs can exceed the physical size of the thin pool, under the assumption that not all space will be utilized at the same time. You can create a thin LV snapshot with the command. When creating a thin LV snapshot, avoid specifying the snapshot size. Including a size parameter results in the creation of a thick snapshot instead.\n• You have created a physical volume. For more information, see Creating LVM physical volume.\n• You have created a volume group. For more information, see Creating LVM volume group.\n• You have created a logical volume. For more information, see Creating logical volumes.\n• None Identify the LV of which you want to create a snapshot:\n• None Replace SnapshotName with the name you want to give to the snapshot logical volume. Replace VolumeGroupName with the name of the volume group that contains the original logical volume. Replace ThinVolumeName with the name of the thin logical volume that you want to create a snapshot of.\n• None Verify that the snapshot is created: You can merge thin LV snapshot into the original logical volume from which the snapshot was created. The process of merging means that the original LV is reverted to the state it was in when the snapshot was created. Once the merge is complete, the snapshot is removed.\n• None List the LVs, their volume groups, and their paths:\n• None Check where the original LV is mounted: Replace VolumeGroupName/ThinVolumeName with the path to your logical volume.\n• None Replace /ThinLogicalVolume/MountPoint with the mounting point for your logical volume. Replace /ThinSnapshot/MountPoint with the mounting point for your snapshot.\n• None Replace VolumeGroupName with the name of the volume group. Replace ThinLogicalVolumeName with the name of the logical volume.\n• None Merge the thin LV snapshot into the origin: Replace VolumeGroupName with the name of the volume group. Replace ThinSnapshotName with the name of the snapshot.\n• None Replace /ThinLogicalVolume/MountPoint with the mounting point for your logical volume.\n• None Verify that the original LV is merged: You can cache logical volumes by using the or targets. utilizes faster storage device (SSD) as cache for a slower storage device (HDD). It caches read and write data, optimizing access times for frequently used data. It is beneficial in mixed workload environments where enhancing read and write operations can lead to significant performance improvements. optimizes write operations by using a faster storage medium (SSD) to temporarily hold write data before it is committed to the primary storage device (HDD). It is beneficial for write-intensive applications where write performance can slow down the data transfer process. When caching LV with , a cache pool is created. A cache pool is a LV that combines both the cache data, which stores the actual cached content, and cache metadata, which tracks what content is stored in the cache. This pool is then associated with a specific LV to cache its data. targets two types of blocks: frequently accessed (hot) blocks are moved to the cache, while less frequently accessed (cold) blocks remain on the slower device.\n• None Display the LV you want to cache and its volume group:\n• None Replace CachePoolName with the name of the cache pool. Replace Size with the size for your cache pool. Replace VolumeGroupName with the name of the volume group. Replace /FastDevicePath with the path to your fast device, for example SSD or NVME.\n• None Attach the cache pool to the LV:\n• None Verify that the LV is now cached: When caching LVs with , a caching layer between the logical volume and the physical storage device is created. operates by temporarily storing write operations in a faster storage medium, such as an SSD, before eventually writing them back to the primary storage device, optimizing write-intensive workloads.\n• None Display the logical volume you want to cache and its volume group:\n• None Replace CacheVolumeName with the name of the cache volume. Replace Size with the size for your cache pool. Replace VolumeGroupName with the name of the volume group. Replace /FastDevicePath with the path to your fast device, for example SSD or NVME.\n• None Attach the cache volume to the LV: Replace CacheVolumeName with the name of the cache volume. Replace VolumeGroupName with the name of the volume group. Replace LogicalVolumeName with the name of the logical volume.\n• None Verify that the LV is now cached: Use two main ways to remove caching from a LV. You can detach the cache from the LV but preserve the cache volume itself. In this case the LV will no longer benefit from the caching mechanism but the cache volume and its data will remain intact. While the cache volume is preserved, the data within the cache cannot be reused and will be erased the next time it is used in a caching setup. You can detaches the cache from the LV and remove the cache volume entirely. This action effectively destroys the cache, freeing up the space.\n• None\n• None To detach the cached volume, use:\n• None To detach and remove the cached volume, use: Replace VolumeGroupName with the name of the volume group. Replace LogicalVolumeName with the name of the logical volume.\n• None Verify that the LV is not cached: You can create custom thin pools to have a better control over the storage.\n• None Replace ThinPoolDataName with the name for your thin pool data LV. Replace Size with the size for your LV. Replace VolumeGroupName with the name of your volume group.\n• None Verify that the custom thin pool is created:\n\nYou can create and manage Redundant Array of Independent Disks (RAID) volumes by using logical volume manager (LVM). LVM supports RAID levels 0, 1, 4, 5, 6, and 10. An LVM RAID volume has the following characteristics:\n• LVM creates and manages RAID logical volumes that leverage the Multiple Devices (MD) kernel drivers.\n• You can temporarily split RAID1 images from the array and merge them back into the array later.\n• RAID logical volumes are not cluster-aware. Although you can create and activate RAID logical volumes exclusively on one machine, you cannot activate them simultaneously on more than one machine.\n• When you create a RAID logical volume (LV), LVM creates a metadata subvolume that is one extent in size for every data or parity subvolume in the array. For example, creating a 2-way RAID1 array results in two metadata subvolumes ( and ) and two data subvolumes ( and ). The following are the supported configurations by RAID, including levels 0, 1, 4, 5, 6, 10, and linear: RAID level 0, often called striping, is a performance-oriented striped data mapping technique. This means the data being written to the array is broken down into stripes and written across the member disks of the array, allowing high I/O performance at low inherent cost but provides no redundancy. RAID level 0 implementations only stripe the data across the member devices up to the size of the smallest device in the array. This means that if you have multiple devices with slightly different sizes, each device gets treated as though it was the same size as the smallest drive. Therefore, the common storage capacity of a level 0 array is the total capacity of all disks. If the member disks have a different size, then the RAID0 uses all the space of those disks using the available zones. RAID level 1, or mirroring, provides redundancy by writing identical data to each member disk of the array, leaving a mirrored copy on each disk. Mirroring remains popular due to its simplicity and high level of data availability. Level 1 operates with two or more disks, and provides very good data reliability and improves performance for read-intensive applications but at relatively high costs. RAID level 1 is costly because you write the same information to all of the disks in the array, which provides data reliability, but in a much less space-efficient manner than parity based RAID levels such as level 5. However, this space inefficiency comes with a performance benefit, which is parity-based RAID levels that consume considerably more CPU power in order to generate the parity while RAID level 1 simply writes the same data more than once to the multiple RAID members with very little CPU overhead. As such, RAID level 1 can outperform the parity-based RAID levels on machines where software RAID is employed and CPU resources on the machine are consistently taxed with operations other than RAID activities. The storage capacity of the level 1 array is equal to the capacity of the smallest mirrored hard disk in a hardware RAID or the smallest mirrored partition in a software RAID. Level 1 redundancy is the highest possible among all RAID types, with the array being able to operate with only a single disk present. Level 4 uses parity concentrated on a single disk drive to protect data. Parity information is calculated based on the content of the rest of the member disks in the array. This information can then be used to reconstruct data when one disk in the array fails. The reconstructed data can then be used to satisfy I/O requests to the failed disk before it is replaced and to repopulate the failed disk after it has been replaced. Since the dedicated parity disk represents an inherent bottleneck on all write transactions to the RAID array, level 4 is seldom used without accompanying technologies such as write-back caching. Or it is used in specific circumstances where the system administrator is intentionally designing the software RAID device with this bottleneck in mind such as an array that has little to no write transactions once the array is populated with data. RAID level 4 is so rarely used that it is not available as an option in Anaconda. However, it could be created manually by the user if needed. The storage capacity of hardware RAID level 4 is equal to the capacity of the smallest member partition multiplied by the number of partitions minus one. The performance of a RAID level 4 array is always asymmetrical, which means reads outperform writes. This is because write operations consume extra CPU resources and main memory bandwidth when generating parity, and then also consume extra bus bandwidth when writing the actual data to disks because you are not only writing the data, but also the parity. Read operations need only read the data and not the parity unless the array is in a degraded state. As a result, read operations generate less traffic to the drives and across the buses of the computer for the same amount of data transfer under normal operating conditions. This is the most common type of RAID. By distributing parity across all the member disk drives of an array, RAID level 5 eliminates the write bottleneck inherent in level 4. The only performance bottleneck is the parity calculation process itself. Modern CPUs can calculate parity very fast. However, if you have a large number of disks in a RAID 5 array such that the combined aggregate data transfer speed across all devices is high enough, parity calculation can be a bottleneck. Level 5 has asymmetrical performance, and reads substantially outperforming writes. The storage capacity of RAID level 5 is calculated the same way as with level 4. This is a common level of RAID when data redundancy and preservation, and not performance, are the paramount concerns, but where the space inefficiency of level 1 is not acceptable. Level 6 uses a complex parity scheme to be able to recover from the loss of any two drives in the array. This complex parity scheme creates a significantly higher CPU burden on software RAID devices and also imposes an increased burden during write transactions. As such, level 6 is considerably more asymmetrical in performance than levels 4 and 5. The total capacity of a RAID level 6 array is calculated similarly to RAID level 5 and 4, except that you must subtract two devices instead of one from the device count for the extra parity storage space. This RAID level attempts to combine the performance advantages of level 0 with the redundancy of level 1. It also reduces some of the space wasted in level 1 arrays with more than two devices. With level 10, it is possible, for example, to create a 3-drive array configured to store only two copies of each piece of data, which then allows the overall array size to be 1.5 times the size of the smallest devices instead of only equal to the smallest device, similar to a 3-device, level 1 array. This avoids CPU process usage to calculate parity similar to RAID level 6, but it is less space efficient. The creation of RAID level 10 is not supported during installation. It is possible to create one manually after installation. Linear RAID is a grouping of drives to create a larger virtual drive. In linear RAID, the chunks are allocated sequentially from one member drive, going to the next drive only when the first is completely filled. This grouping provides no performance benefit, as it is unlikely that any I/O operations split between member drives. Linear RAID also offers no redundancy and decreases reliability. If any one member drive fails, the entire array cannot be used and data can be lost. The capacity is the total of all member disks. To create a RAID logical volume, you can specify a RAID type by using the argument of the command. For most users, specifying one of the five available primary types, which are , , , , and , should be sufficient. The following table describes the possible RAID segment types. RAID1 mirroring. This is the default value for the argument of the command, when you specify the argument without specifying striping.\n• It is same as .\n• It is same as .\n• Striped mirrors. This is the default value for the argument of the command if you specify the argument along with the number of stripes that is greater than 1. Striping. RAID0 spreads logical volume data across multiple data subvolumes in units of stripe size. This is used to increase performance. Logical volume data is lost if any of the data subvolumes fail. You can create a RAID0 striped logical volume using the command. The following table describes different parameters, which you can use while creating a RAID0 striped logical volume. Specifying creates a RAID0 volume without metadata volumes. Specifying creates a RAID0 volume with metadata volumes. Since RAID0 is non-resilient, it does not store any mirrored data blocks as RAID1/10 or calculate and store any parity blocks as RAID4/5/6 do. Hence, it does not need metadata volumes to keep state about resynchronization progress of mirrored or parity blocks. Metadata volumes become mandatory on a conversion from RAID0 to RAID4/5/6/10. Specifying preallocates those metadata volumes to prevent a respective allocation failure. Specifies the number of devices to spread the logical volume across. Specifies the size of each stripe in kilobytes. This is the amount of data that is written to one device before moving to the next device. Specifies the volume group to use. Specifies the devices to use. If this is not specified, LVM will choose the number of devices specified by the Stripes option, one for each stripe. You can create RAID1 arrays with multiple numbers of copies, according to the value you specify for the argument. Similarly, you can specify the number of stripes for a RAID 0, 4, 5, 6, and 10 logical volume with the argument. You can also specify the stripe size with the argument. The following procedure describes different ways to create different types of RAID logical volume.\n• None Create a 2-way RAID. The following command creates a 2-way RAID1 array, named my_lv, in the volume group my_vg, that is 1G in size:\n• None Create a RAID5 array with stripes. The following command creates a RAID5 array with three stripes and one implicit parity drive, named my_lv, in the volume group my_vg, that is 1G in size. Note that you can specify the number of stripes similar to an LVM striped volume. The correct number of parity drives is added automatically.\n• None Create a RAID6 array with stripes. The following command creates a RAID6 array with three 3 stripes and two implicit parity drives, named my_lv, in the volume group my_vg, that is 1G one gigabyte in size:\n• None Display the LVM device my_vg/my_lv, which is a 2-way RAID1 array:\n• and man pages on your system 9.5. Configuring an LVM pool with RAID by using the RHEL system role With the system role, you can configure an LVM pool with RAID on RHEL by using Red Hat Ansible Automation Platform. You can set up an Ansible playbook with the available parameters to configure an LVM pool with RAID.\n• You have prepared the control node and the managed nodes\n• You are logged in to the control node as a user who can run playbooks on the managed nodes.\n• The account you use to connect to the managed nodes has permissions on them.\n• None Create a playbook file, for example , with the following content: --- - name: Manage local storage hosts: managed-node-01.example.com tasks: - name: Configure LVM pool with RAID ansible.builtin.include_role: name: rhel-system-roles.storage vars: storage_safe_mode: false storage_pools: - name: my_pool type: lvm disks: [sdh, sdi] raid_level: raid1 volumes: - name: my_volume size: \"1 GiB\" mount_point: \"/mnt/app/shared\" fs_type: xfs state: present For details about all variables used in the playbook, see the file on the control node.\n• None Note that this command only validates the syntax and does not protect against a wrong but valid configuration.\n• None Verify that your pool is on RAID: A RAID0 logical volume spreads logical volume data across multiple data subvolumes in units of stripe size. The following procedure creates an LVM RAID0 logical volume called mylv that stripes data across the disks.\n• You have created three or more physical volumes. For more information about creating physical volumes, see Creating LVM physical volume.\n• You have created the volume group. For more information, see Creating LVM volume group.\n• None Create a RAID0 logical volume from the existing volume group. The following command creates the RAID0 volume mylv from the volume group myvg, which is 2G in size, with three stripes and a stripe size of 4kB:\n• None Create a file system on the RAID0 logical volume. The following command creates an ext4 file system on the logical volume:\n• None Mount the logical volume and report the file system disk space usage: # # Filesystem 1K-blocks Used Available Use% Mounted on /dev/mapper/my_vg-mylv 2002684 6168 1875072 1% /mnt 9.7. Configuring a stripe size for RAID LVM volumes by using the RHEL system role With the system role, you can configure a stripe size for RAID LVM volumes on RHEL by using Red Hat Ansible Automation Platform. You can set up an Ansible playbook with the available parameters to configure an LVM pool with RAID.\n• You have prepared the control node and the managed nodes\n• You are logged in to the control node as a user who can run playbooks on the managed nodes.\n• The account you use to connect to the managed nodes has permissions on them.\n• None Create a playbook file, for example , with the following content: --- - name: Manage local storage hosts: managed-node-01.example.com tasks: - name: Configure stripe size for RAID LVM volumes ansible.builtin.include_role: name: rhel-system-roles.storage vars: storage_safe_mode: false storage_pools: - name: my_pool type: lvm disks: [sdh, sdi] volumes: - name: my_volume size: \"1 GiB\" mount_point: \"/mnt/app/shared\" fs_type: xfs raid_level: raid0 raid_stripe_size: \"256 KiB\" state: present For details about all variables used in the playbook, see the file on the control node.\n• None Note that this command only validates the syntax and does not protect against a wrong but valid configuration.\n• None Verify that stripe size is set to the required size: Soft corruption in data storage implies that the data retrieved from a storage device is different from the data written to that device. The corrupted data can exist indefinitely on storage devices. You might not discover this corrupted data until you retrieve and attempt to use this data. Depending on the type of configuration, a Redundant Array of Independent Disks (RAID) logical volume(LV) prevents data loss when a device fails. If a device consisting of a RAID array fails, the data can be recovered from other devices that are part of that RAID LV. However, a RAID configuration does not ensure the integrity of the data itself. Soft corruption, silent corruption, soft errors, and silent errors are terms that describe data that has become corrupted, even if the system design and software continues to function as expected. When creating a new RAID LV with DM integrity or adding integrity to an existing RAID LV, consider the following points:\n• The integrity metadata requires additional storage space. For each RAID image, every 500MB data requires 4MB of additional storage space because of the checksums that get added to the data.\n• While some RAID configurations are impacted more than others, adding DM integrity impacts performance due to latency when accessing the data. A RAID1 configuration typically offers better performance than RAID5 or its variants.\n• The RAID integrity block size also impacts performance. Configuring a larger RAID integrity block size offers better performance. However, a smaller RAID integrity block size offers greater backward compatibility.\n• There are two integrity modes available: or . The integrity mode typically offers better performance than mode. If you experience performance issues, either use RAID1 with integrity or test the performance of a particular RAID configuration to ensure that it meets your requirements. When you create a RAID LV with device mapper (DM) integrity or add integrity to an existing RAID logical volume (LV), it mitigates the risk of losing data due to soft corruption. Wait for the integrity synchronization and the RAID metadata to complete before using the LV. Otherwise, the background initialization might impact the LV’s performance. Device mapper (DM) integrity is used with RAID levels 1, 4, 5, 6, and 10 to mitigate or prevent data loss due to soft corruption. The RAID layer ensures that a non-corrupted copy of the data can fix the soft corruption errors.\n• None Create a RAID LV with DM integrity. The following example creates a new RAID LV with integrity named test-lv in the my_vg volume group, with a usable size of 256M and RAID level 1: To add DM integrity to an existing RAID LV, use the following command: Adding integrity to a RAID LV limits the number of operations that you can perform on that RAID LV.\n• None Optional: Remove the integrity before performing certain operations.\n• None\n• None View information about the test-lv RAID LV that was created in the my_vg volume group: The following describes different options from this output: It is the list of attributes under the Attr column indicates that the RAID image is using integrity. The integrity stores the checksums in the RAID LV. It indicates the synchronization progress for both the top level RAID LV and for each RAID image. It is is indicated in the LV column by . It ensures that the synchronization progress displays 100% for the top level RAID LV and for each RAID image.\n• None Display the type for each RAID LV:\n• None There is an incremental counter that counts the number of mismatches detected on each RAID image. View the data mismatches detected by integrity from under my_vg/test-lv: In this example, the integrity has not detected any data mismatches and thus the counter shows zero (0).\n• None View the data integrity information in the log files, as shown in the following examples: Example 9.1. Example of dm-integrity mismatches from the kernel message logs Example 9.2. Example of dm-integrity data corrections from the kernel message logs\n• and man pages on your system LVM supports RAID takeover, which means converting a RAID logical volume from one RAID level to another, for example, from RAID 5 to RAID 6. You can change the RAID level to increase or decrease resilience to device failures.\n• None # Using default stripesize 64.00 KiB. Rounding size 500.00 MiB (125 extents) up to stripe boundary size 504.00 MiB (126 extents). Logical volume \"my_lv\" created.\n• None Convert the RAID logical volume to another RAID level: # Using default stripesize 64.00 KiB. Replaced LV type raid6 (same as raid6_zr) with possible type raid6_ls_6. Repeat this command to convert to raid6 after an interim conversion has finished. Are you sure you want to convert raid5 LV my_vg/my_lv to raid6_ls_6 type? [y/n]: y Logical volume my_vg/my_lv successfully converted.\n• None Optional: If this command prompts to repeat the conversion, run:\n• None View the RAID logical volume with the converted RAID level:\n• and man pages on your system You can convert an existing linear logical volume to a RAID logical volume. To perform this operation, use the argument of the command. RAID logical volumes are composed of metadata and data subvolume pairs. When you convert a linear device to a RAID1 array, it creates a new metadata subvolume and associates it with the original logical volume on one of the same physical volumes that the linear volume is on. The additional images are added in a metadata/data subvolume pair. If the metadata image that pairs with the original logical volume cannot be placed on the same physical volume, the fails.\n• None View the logical volume device that needs to be converted:\n• None Convert the linear logical volume to a RAID device. The following command converts the linear logical volume my_lv in volume group __my_vg, to a 2-way RAID1 array: # Are you sure you want to convert linear LV my_vg/my_lv to raid1 with 2 images enhancing resilience? [y/n]: y Logical volume successfully converted.\n• None Ensure if the logical volume is converted to a RAID device: 9.12. Converting an LVM RAID1 logical volume to an LVM linear logical volume You can convert an existing RAID1 LVM logical volume to an LVM linear logical volume. To perform this operation, use the command and specify the argument. This removes all the RAID data subvolumes and all the RAID metadata subvolumes that make up the RAID array, leaving the top-level RAID1 image as the linear logical volume.\n• None Convert an existing RAID1 LVM logical volume to an LVM linear logical volume. The following command converts the LVM RAID1 logical volume my_vg/my_lv to an LVM linear device: # Are you sure you want to convert raid1 LV my_vg/my_lv to type linear losing all resilience? [y/n]: y Logical volume my_vg/my_lv successfully converted. When you convert an LVM RAID1 logical volume to an LVM linear volume, you can also specify which physical volumes to remove. In the following example, the command specifies that you want to remove /dev/sde1, leaving /dev/sdf1 as the physical volume that makes up the linear device:\n• None Verify if the RAID1 logical volume was converted to an LVM linear device: You can convert an existing mirrored LVM device with a segment type mirror to a RAID1 LVM device. To perform this operation, use the command with the raid1 argument. This renames the mirror subvolumes named to RAID subvolumes named . In addition, it also removes the mirror log and creates metadata subvolumes named for the data subvolumes on the same physical volumes as the corresponding data subvolumes.\n• None # Are you sure you want to convert mirror LV my_vg/my_lv to raid1 type? [y/n]: y Logical volume successfully converted.\n• None Verify if the mirrored logical volume is converted to a RAID1 logical volume: 9.14. Changing the number of images in an existing RAID1 device You can change the number of images in an existing RAID1 array, similar to the way you can change the number of images in the implementation of LVM mirroring. When you add images to a RAID1 logical volume with the command, you can perform the following operations:\n• specify the total number of images for the resulting device,\n• how many images to add to the device, and\n• can optionally specify on which physical volumes the new metadata/data image pairs reside.\n• None Display the LVM device my_vg/my_lv, which is a 2-way RAID1 array: Metadata subvolumes named always exist on the same physical devices as their data subvolume counterparts . The metadata/data subvolume pairs will not be created on the same physical volumes as those from another metadata/data subvolume pair in the RAID array unless you specify anywhere.\n• None # Are you sure you want to convert raid1 LV to 3 images enhancing resilience? [y/n]: y Logical volume successfully converted. The following are a few examples of changing the number of images in an existing RAID1 device:\n• None You can also specify which physical volumes to use while adding an image to RAID. The following command converts the 2-way RAID1 logical volume my_vg/my_lv to a 3-way RAID1 logical volume by specifying the physical volume /dev/sdd1 to use for the array:\n• None # Are you sure you want to convert raid1 LV my_vg/my_lv to 2 images reducing resilience? [y/n]: y Logical volume successfully converted.\n• None Convert the 3-way RAID1 logical volume into a 2-way RAID1 logical volume by specifying the physical volume /dev/sde1, which contains the image to remove: Additionally, when you remove an image and its associated metadata subvolume volume, any higher-numbered images will be shifted down to fill the slot. Removing from a 3-way RAID1 array that consists of , , and results in a RAID1 array that consists of and . The subvolume will be renamed and take over the empty slot, becoming .\n• None View the RAID1 device after changing the number of images in an existing RAID1 device: You can split off an image of a RAID logical volume to form a new logical volume. When you are removing a RAID image from an existing RAID1 logical volume or removing a RAID data subvolume and its associated metadata subvolume from the middle of the device, any higher numbered images will be shifted down to fill the slot. The index numbers on the logical volumes that make up a RAID array will thus be an unbroken sequence of integers. You cannot split off a RAID image if the RAID1 array is not yet in sync.\n• None Display the LVM device my_vg/my_lv, which is a 2-way RAID1 array:\n• None\n• None The following example splits a 2-way RAID1 logical volume, my_lv, into two linear logical volumes, my_lv and new: # Are you sure you want to split raid1 LV losing all resilience? [y/n]: y\n• None The following example splits a 3-way RAID1 logical volume, my_lv, into a 2-way RAID1 logical volume, my_lv, and a linear logical volume, new:\n• None View the logical volume after you split off an image of a RAID logical volume: You can temporarily split off an image of a RAID1 array for read-only use while tracking any changes by using the argument with the argument of the command. Using this feature, you can merge the image into an array at a later time while resyncing only those portions of the array that have changed since the image was split. When you split off a RAID image with the argument, you can specify which image to split but you cannot change the name of the volume being split. In addition, the resulting volumes have the following constraints:\n• The new volume you create is read-only.\n• You cannot resize the new volume.\n• You cannot rename the remaining array.\n• You cannot resize the remaining array.\n• You can activate the new volume and the remaining array independently. You can merge an image that was split off. When you merge the image, only the portions of the array that have changed since the image was split are resynced.\n• None Split an image from the created RAID logical volume and track the changes to the remaining array: # my_lv_rimage_2 split from my_lv for read-only purposes. Use 'lvconvert --merge my_vg/my_lv_rimage_2' to merge back into my_lv\n• None Optional: View the logical volume after splitting the image:\n• None Merge the volume back into the array: You can set the field to the parameter in the file. With this preference, the system attempts to replace the failed device with a spare device from the volume group. If there is no spare device, the system log includes this information.\n• None View the RAID logical volume if the /dev/sdb device fails: # /dev/sdb: open failed: No such device or address Couldn't find device with uuid A4kRl2-vIzA-uyCb-cci7-bOod-H5tX-IzH4Ee. WARNING: Couldn't find all devices for LV my_vg/my_lv_rimage_1 while checking used and assumed devices. WARNING: Couldn't find all devices for LV my_vg/my_lv_rmeta_1 while checking used and assumed devices. LV Copy% Devices my_lv 100.00 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0) [my_lv_rimage_0] [unknown](1) [my_lv_rimage_1] /dev/sdc1(1) [...] You can also view the system log for the error messages if the /dev/sdb device fails.\n• None Set the field to in the file: If you set to but there are no spare devices, the allocation fails, leaving the logical volume as it is. If the allocation fails, you can fix and replace the failed device by using the command. For more information, see Replacing a failed RAID device in a logical volume.\n• None Verify if the failed device is now replaced with a new device from the volume group: Even though the failed device is now replaced, the display still indicates that LVM could not find the failed device because the device is not yet removed from the volume group. You can remove the failed device from the volume group by executing the command. You can set the field to the parameter in the file. With this preference, the system adds a warning to the system log that indicates a failed device. Based on the warning, you can determine the further steps. By default, the value of the field is in .\n• None Set the raid_fault_policy field to warn in the lvm.conf file: # # This configuration option has an automatic default value. raid_fault_policy = \"warn\"\n• None View the system log to display error messages if the /dev/sdb device fails: # Apr 14 18:48:59 virt-506 kernel: sd 25:0:0:0: rejecting I/O to offline device Apr 14 18:48:59 virt-506 kernel: I/O error, dev sdb, sector 8200 op 0x1:(WRITE) flags 0x20800 phys_seg 0 prio class 2 [...] Apr 14 18:48:59 virt-506 dmeventd[91060]: WARNING: VG my_vg is missing PV 9R2TVV-bwfn-Bdyj-Gucu-1p4F-qJ2Q-82kCAF (last written to /dev/sdb). Apr 14 18:48:59 virt-506 dmeventd[91060]: WARNING: Couldn't find device with uuid 9R2TVV-bwfn-Bdyj-Gucu-1p4F-qJ2Q-82kCAF. Apr 14 18:48:59 virt-506 dmeventd[91060]: Use 'lvconvert --repair my_vg/ly_lv' to replace failed device. If the /dev/sdb device fails, the system log displays error messages. In this case, however, LVM will not automatically attempt to repair the RAID device by replacing one of the images. Instead, if the device has failed you can replace the device with the argument of the command. For more information, see Replacing a failed RAID device in a logical volume. You can replace a working RAID device in a logical volume by using the argument of the command. In the case of RAID device failure, the following commands do not work.\n• The RAID device has not failed.\n• None Replace the RAID device with any of the following methods depending on your requirements:\n• None Replace a RAID1 device by specifying the physical volume that you want to replace:\n• None Replace a RAID1 device by specifying the physical volume to use for the replacement:\n• None Replace multiple RAID devices at a time by specifying multiple replace arguments:\n• None Examine the RAID1 array after specifying the physical volume that you wanted to replace:\n• None Examine the RAID1 array after specifying the physical volume to use for the replacement:\n• None Examine the RAID1 array after replacing multiple RAID devices at a time: RAID is not similar to traditional LVM mirroring. In case of LVM mirroring, remove the failed devices. Otherwise, the mirrored logical volume would hang while RAID arrays continue running with failed devices. For RAID levels other than RAID1, removing a device would mean converting to a lower RAID level, for example, from RAID6 to RAID5, or from RAID4 or RAID5 to RAID0. Instead of removing a failed device and allocating a replacement, with LVM, you can replace a failed device that serves as a physical volume in a RAID logical volume by using the argument of the command.\n• None The volume group includes a physical volume that provides enough free capacity to replace the failed device. If no physical volume with enough free extents is available on the volume group, add a new, sufficiently large physical volume by using the utility.\n• None View the RAID logical volume after the /dev/sdc device fails: # /dev/sdc: open failed: No such device or address Couldn't find device with uuid A4kRl2-vIzA-uyCb-cci7-bOod-H5tX-IzH4Ee. WARNING: Couldn't find all devices for LV my_vg/my_lv_rimage_1 while checking used and assumed devices. WARNING: Couldn't find all devices for LV my_vg/my_lv_rmeta_1 while checking used and assumed devices. LV Cpy%Sync Devices my_lv 100.00 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0) [my_lv_rimage_0] /dev/sde1(1) [my_lv_rimage_1] [unknown](1) [my_lv_rimage_2] /dev/sdd1(1) [my_lv_rmeta_0] /dev/sde1(0) [my_lv_rmeta_1] [unknown](0) [my_lv_rmeta_2] /dev/sdd1(0)\n• None # /dev/sdc: open failed: No such device or address Couldn't find device with uuid A4kRl2-vIzA-uyCb-cci7-bOod-H5tX-IzH4Ee. WARNING: Couldn't find all devices for LV my_vg/my_lv_rimage_1 while checking used and assumed devices. WARNING: Couldn't find all devices for LV my_vg/my_lv_rmeta_1 while checking used and assumed devices. Attempt to replace failed RAID images (requires full device resync)? [y/n]: y Faulty devices in my_vg/my_lv successfully replaced.\n• None Optional: Manually specify the physical volume that replaces the failed device:\n• None Examine the logical volume with the replacement: # /dev/sdc: open failed: No such device or address /dev/sdc1: open failed: No such device or address Couldn't find device with uuid A4kRl2-vIzA-uyCb-cci7-bOod-H5tX-IzH4Ee. LV Cpy%Sync Devices my_lv 43.79 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0) [my_lv_rimage_0] /dev/sde1(1) [my_lv_rimage_1] /dev/sdb1(1) [my_lv_rimage_2] /dev/sdd1(1) [my_lv_rmeta_0] /dev/sde1(0) [my_lv_rmeta_1] /dev/sdb1(0) [my_lv_rmeta_2] /dev/sdd1(0) Until you remove the failed device from the volume group, LVM utilities still indicate that LVM cannot find the failed device.\n• None Remove the failed device from the volume group:\n• None View the available physical volumes after removing the failed device:\n• None Examine the logical volume after the replacing the failed device:\n• and man pages on your system LVM provides scrubbing support for RAID logical volumes. RAID scrubbing is the process of reading all the data and parity blocks in an array and checking to see whether they are coherent. The command initiates a background synchronization action on the array.\n• None Optional: Control the rate at which a RAID logical volume is initialized by setting any one of the following options:\n• sets the maximum recovery rate for a RAID logical volume so that it will not expel nominal I/O operations.\n• None sets the minimum recovery rate for a RAID logical volume to ensure that I/O for sync operations achieves a minimum throughput, even when heavy nominal I/O is present Replace 4K with the recovery rate value, which is an amount per second for each device in the array. If you provide no suffix, the options assume kiB per second per device. When you perform a RAID scrubbing operation, the background I/O required by the actions can crowd out other I/O to LVM devices, such as updates to volume group metadata. This might cause the other LVM operations to slow down. You can also use these maximum and minimum I/O rate while creating a RAID device. For example, creates a 2-way RAID10 array my_lv, which is in the volume group my_vg with 3 stripes that is 10G in size with a maximum recovery rate of 128 kiB/sec/device.\n• None Display the number of discrepancies in the array, without repairing them: This command initiates a background synchronization action on the array.\n• Optional: View the file for the kernel messages.\n• None Correct the discrepancies in the array: This command repairs or replaces failed devices in a RAID logical volume. You can view the file for the kernel messages after executing this command.\n• and man pages on your system You can control the I/O operations for a device in a RAID1 logical volume by using the and parameters of the command. The following is the format for using these parameters: Marks a device in a RAID1 logical volume as and avoids all read actions to these drives unless necessary. Setting this parameter keeps the number of I/O operations to the drive to a minimum. Use the command to set this parameter. You can set the attribute in the following ways: By default, the value of the attribute is yes for the specified physical volume in the logical volume. To remove the flag, append to the physical volume. To toggle the value of the attribute, specify the argument. You can use this argument more than one time in a single command, for example, . With this, it is possible to toggle the attributes for all the physical volumes in a logical volume at once. Specifies the maximum number of pending writes marked as . These are the number of write operations applicable to devices in a RAID1 logical volume. After the value of this parameter exceeds, all write actions to the constituent devices complete synchronously before the RAID array notifies for completion of all write actions. You can set this parameter by using the command. Setting the attribute’s value to zero clears the preference. With this setting, the system chooses the value arbitrarily. RAID reshaping means changing attributes of a RAID logical volume without changing the RAID level. Some attributes that you can change include RAID layout, stripe size, and number of stripes.\n• None # Using default stripesize 64.00 KiB. Rounding size 500.00 MiB (125 extents) up to stripe boundary size 504.00 MiB (126 extents). Logical volume \"my_lv\" created.\n• None Optional: View the images and of the RAID logical volume:\n• None Modify the attributes of the RAID logical volume by using the following ways depending on your requirement:\n• None Modify the images of the RAID logical volume: # Using default stripesize 64.00 KiB. WARNING: Adding stripes to active logical volume my_vg/my_lv will grow it from 126 to 189 extents! Run \"lvresize -l126 my_vg/my_lv\" to shrink it or use the additional capacity. Are you sure you want to add 1 images to raid5 LV my_vg/my_lv? [y/n]: y Logical volume my_vg/my_lv successfully converted.\n• None Modify the of the RAID logical volume: # Converting stripesize 64.00 KiB of raid5 LV my_vg/my_lv to 128.00 KiB. Are you sure you want to convert raid5 LV my_vg/my_lv? [y/n]: y Logical volume my_vg/my_lv successfully converted.\n• None View the images and of the RAID logical volume:\n• None View the RAID logical volume after modifying the attribute:\n• None View the RAID logical volume after modifying the attribute:\n• None View the RAID logical volume after modifying the attribute:\n• and man pages on your system When you create a RAID logical volume, the parameter from the file represents the region size for the RAID logical volume. After you created a RAID logical volume, you can change the region size of the volume. This parameter defines the granularity to keep track of the dirty or clean state. Dirty bits in the bitmap define the work set to synchronize after a dirty shutdown of a RAID volume, for example, a system failure. If you set to a higher value, it reduces the size of bitmap as well as the congestion. But it impacts the operation during resynchronizing the region because writes to RAID are postponed until synchronizing the region finishes.\n• None The Region column indicates the raid_region_size parameter’s value.\n• None # Do you really want to change the region_size 512.00 KiB of LV my_vg/my_lv to 4.00 MiB? [y/n]: y Changed region size on RAID LV my_vg/my_lv to 4.00 MiB.\n• None # Do you really want to deactivate logical volume my_vg/my_lv to resync it? [y/n]: y\n• None The Region column indicates the changed value of the parameter.\n• None View the parameter’s value in the file:\n\nYou can use Logical Volume Manager (LVM) tools to troubleshoot a variety of issues in LVM volumes and groups. If an LVM command is not working as expected, you can gather diagnostics in the following ways.\n• None Use the following methods to gather different kinds of diagnostic data:\n• Add the argument to any LVM command to increase the verbosity level of the command output. Verbosity can be further increased by adding additional . A maximum of four such is allowed, for example, .\n• In the section of the configuration file, increase the value of the option. This causes LVM to provide more details in the system log.\n• None If the problem is related to the logical volume activation, enable LVM to log messages during the activation:\n• Set the option in the section of the configuration file.\n• Execute the LVM command with the option.\n• None If you do not reset the option to , the system might become unresponsive during low memory situations.\n• Examine the last backup of the LVM metadata in the directory and archived versions in the directory.\n• Check the cache file for a record of which devices have physical volumes on them. Troubleshooting information about a failed Logical Volume Manager (LVM) volume can help you determine the reason of the failure. You can check the following examples of the most common LVM volume failures. In this example, one of the devices that made up the volume group myvg failed. The volume group usability then depends on the type of failure. For example, the volume group is still usable if RAID volumes are also involved. You can also see information about the failed device. # /dev/vdb1: open failed: No such device or address /dev/vdb1: open failed: No such device or address WARNING: Couldn't find device with uuid 42B7bu-YCMp-CEVD-CmKH-2rk6-fiO9-z1lf4s. WARNING: VG myvg is missing PV 42B7bu-YCMp-CEVD-CmKH-2rk6-fiO9-z1lf4s (last written to /dev/sdb1). WARNING: Couldn't find all devices for LV myvg/mylv while checking used and assumed devices. VG #PV #LV #SN Attr VSize VFree Devices myvg 2 2 0 wz-pn- <3.64t <3.60t [unknown](0) myvg 2 2 0 wz-pn- <3.64t <3.60t [unknown](5120),/dev/vdb1(0) In this example, one of the devices failed. This can be a reason for the logical volume in the volume group to fail. The command output shows the failed logical volumes. # /dev/vdb1: open failed: No such device or address /dev/vdb1: open failed: No such device or address WARNING: Couldn't find device with uuid 42B7bu-YCMp-CEVD-CmKH-2rk6-fiO9-z1lf4s. WARNING: VG myvg is missing PV 42B7bu-YCMp-CEVD-CmKH-2rk6-fiO9-z1lf4s (last written to /dev/sdb1). WARNING: Couldn't find all devices for LV myvg/mylv while checking used and assumed devices. LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert Devices mylv myvg -wi-a---p- 20.00g [unknown](0) [unknown](5120),/dev/sdc1(0) The following examples show the command output from the and utilities when an image of a RAID logical volume has failed. The logical volume is still usable. # Error reading device /dev/sdc1 at 0 length 4. Error reading device /dev/sdc1 at 4096 length 4. Couldn't find device with uuid b2J8oD-vdjw-tGCA-ema3-iXob-Jc6M-TC07Rn. WARNING: Couldn't find all devices for LV myvg/my_raid1_rimage_1 while checking used and assumed devices. WARNING: Couldn't find all devices for LV myvg/my_raid1_rmeta_1 while checking used and assumed devices. PV VG Fmt Attr PSize PFree /dev/sda2 rhel_bp-01 lvm2 a-- <464.76g 4.00m /dev/sdb1 myvg lvm2 a-- <836.69g 736.68g /dev/sdd1 myvg lvm2 a-- <836.69g <836.69g /dev/sde1 myvg lvm2 a-- <836.69g <836.69g [unknown] myvg lvm2 a-m <836.69g 736.68g # Couldn't find device with uuid b2J8oD-vdjw-tGCA-ema3-iXob-Jc6M-TC07Rn. WARNING: Couldn't find all devices for LV myvg/my_raid1_rimage_1 while checking used and assumed devices. WARNING: Couldn't find all devices for LV myvg/my_raid1_rmeta_1 while checking used and assumed devices. LV VG Attr LSize Devices my_raid1 myvg rwi-a-r-p- 100.00g my_raid1_rimage_0(0),my_raid1_rimage_1(0) [my_raid1_rimage_0] myvg iwi-aor--- 100.00g /dev/sdb1(1) [my_raid1_rimage_1] myvg Iwi-aor-p- 100.00g [unknown](1) [my_raid1_rmeta_0] myvg ewi-aor--- 4.00m /dev/sdb1(0) [my_raid1_rmeta_1] myvg ewi-aor-p- 4.00m [unknown](0) If a physical volume fails, you can activate the remaining physical volumes in the volume group and remove all the logical volumes that used that physical volume from the volume group.\n• None Activate the remaining physical volumes in the volume group:\n• None Check which logical volumes will be removed:\n• None Remove all the logical volumes that used the lost physical volume from the volume group:\n• None Optional: If you accidentally removed logical volumes that you wanted to keep, you can reverse the operation: If you remove a thin pool, LVM cannot reverse the operation. If the volume group’s metadata area of a physical volume is accidentally overwritten or otherwise destroyed, you get an error message indicating that the metadata area is incorrect, or that the system was unable to find a physical volume with a particular UUID. This procedure finds the latest archived metadata of a physical volume that is missing or corrupted.\n• None Find the archived metadata file of the volume group that contains the physical volume. The archived metadata files are located at the path: Replace 00000-1248998876 with the backup-number. Select the last known valid metadata file, which has the highest number for the volume group.\n• None Find the UUID of the physical volume. Use one of the following methods.\n• Examine the archived metadata file. Find the UUID as the value labeled in the section of the volume group configuration.\n• None Deactivate the volume group using the option: # PARTIAL MODE. Incomplete logical volumes will be processed. WARNING: Couldn't find device with uuid . WARNING: VG is missing PV (last written to ). 0 logical volume(s) in volume group \" \" now active This procedure restores metadata on a physical volume that is either corrupted or replaced with a new device. You might be able to recover the data from the physical volume by rewriting the metadata area on the physical volume. Do not attempt this procedure on a working LVM logical volume. You will lose your data if you specify the incorrect UUID.\n• You have identified the metadata of the missing physical volume. For details, see Finding the metadata of a missing LVM physical volume.\n• None Restore the metadata on the physical volume: The command overwrites only the LVM metadata areas and does not affect the existing data areas. The following example labels the device as a physical volume with the following properties:\n• None The metadata information contained in , which is the most recent good archived metadata for the volume group\n• None Restore the metadata of the volume group:\n• None Display the logical volumes on the volume group: The logical volumes are currently inactive. For example:\n• None If the segment type of the logical volumes is RAID, resynchronize the logical volumes:\n• If the on-disk LVM metadata takes at least as much space as what overrode it, this procedure can recover the physical volume. If what overrode the metadata went past the metadata area, the data on the volume may have been affected. You might be able to use the command to recover that data. LVM commands that report the space usage in volume groups round the reported number to decimal places to provide human-readable output. This includes the and utilities. As a result of the rounding, the reported value of free space might be larger than what the physical extents on the volume group provide. If you attempt to create a logical volume the size of the reported free space, you might get the following error: To work around the error, you must examine the number of free physical extents on the volume group, which is the accurate value of free space. You can then use the number of extents to create the logical volume successfully. 13.7. Preventing the rounding error when creating an LVM volume When creating an LVM logical volume, you can specify the number of logical extents of the logical volume to avoid rounding error.\n• None Find the number of free physical extents in the volume group: For example, the following volume group has 8780 free physical extents:\n• None Create the logical volume. Enter the volume size in extents rather than bytes. Example 13.6. Creating a logical volume by specifying the number of extents Example 13.7. Creating a logical volume to occupy all the remaining space Alternatively, you can extend the logical volume to use a percentage of the remaining free space in the volume group. For example:\n• None Check the number of extents that the volume group now uses: 13.8. LVM metadata and their location on disk LVM headers and metadata areas are available in different offsets and sizes.\n• Is found in and structures.\n• Is in the second 512-byte sector of the disk. Note that if a non-default location was specified when creating the physical volume (PV), the header can also be in the first or third sector.\n• Begins 4096 bytes from the start of the disk.\n• Ends 1 MiB from the start of the disk.\n• Begins with a 512 byte sector containing the structure. A metadata text area begins after the sector and goes to the end of the metadata area. LVM VG metadata text is written in a circular fashion into the metadata text area. The points to the location of the latest VG metadata within the text area. You can print LVM headers from a disk by using the command. This command prints , , , and the location of metadata text if found. Bad fields are printed with the prefix. The LVM metadata area offset will match the page size of the machine that created the PV, so the metadata area can also begin 8K, 16K or 64K from the start of the disk. Larger or smaller metadata areas can be specified when creating the PV, in which case the metadata area may end at locations other than 1 MiB. The specifies the size of the metadata area. When creating a PV, a second metadata area can be optionally enabled at the end of the disk. The contains the locations of the metadata areas. Choose one of the following procedures to extract VG metadata from a disk, depending on your situation. For information about how to save extracted metadata, see Saving extracted metadata to a file. For repair, you can use backup files in without extracting metadata from disk.\n• None Print the locations of all metadata copies found in the metadata area, based on finding a valid : Example 13.9. Locations of metadata copies in the metadata area # metadata at 4608 length 815 crc 29fcd7ab vg test seqno 1 id FaCsSz-1ZZn-mTO4-Xl4i-zb6G-BYat-u53Fxv metadata at 5632 length 1144 crc 50ea61c3 vg test seqno 2 id FaCsSz-1ZZn-mTO4-Xl4i-zb6G-BYat-u53Fxv metadata at 7168 length 1450 crc 5652ea55 vg test seqno 3 id FaCsSz-1ZZn-mTO4-Xl4i-zb6G-BYat-u53Fxv\n• None Search for all copies of metadata in the metadata area without using an , for example, if headers are missing or damaged: Example 13.10. Copies of metadata in the metadata area without using an # Searching for metadata at offset 4096 size 1044480 metadata at 4608 length 815 crc 29fcd7ab vg test seqno 1 id FaCsSz-1ZZn-mTO4-Xl4i-zb6G-BYat-u53Fxv metadata at 5632 length 1144 crc 50ea61c3 vg test seqno 2 id FaCsSz-1ZZn-mTO4-Xl4i-zb6G-BYat-u53Fxv metadata at 7168 length 1450 crc 5652ea55 vg test seqno 3 id FaCsSz-1ZZn-mTO4-Xl4i-zb6G-BYat-u53Fxv\n• None Include the option in the command to show the description from each copy of metadata: Example 13.11. Showing description from each copy of metadata This file can be used for repair. The first metadata area is used by default for dump metadata. If the disk has a second metadata area at the end of the disk, you can use the option to use the second metadata area for dump metadata instead. If you need to use dumped metadata for repair, it is required to save extracted metadata to a file with the option and the option.\n• If is added to , the raw metadata is written to the named file. You can use this file for repair.\n• If is added to or , then raw metadata from all locations is written to the named file.\n• None To save one instance of metadata text from add where is from the listing output \"metadata at <offset>\". Example 13.12. Output of the command # Searching for metadata at offset 4096 size 1044480 metadata at 5632 length 1144 crc 50ea61c3 vg test seqno 2 id FaCsSz-1ZZn-mTO4-Xl4i-zb6G-BYat-u53Fxv # test { id = \"FaCsSz-1ZZn-mTO4-Xl4i-zb6G-BYat-u53Fxv\" 13.11. Repairing a disk with damaged LVM headers and metadata using the pvcreate and the vgcfgrestore commands You can restore metadata and headers on a physical volume that is either corrupted or replaced with a new device. You might be able to recover the data from the physical volume by rewriting the metadata area on the physical volume. These instructions should be used with extreme caution, and only if you are familiar with the implications of each command, the current layout of the volumes, the layout that you need to achieve, and the contents of the backup metadata file. These commands have the potential to corrupt data, and as such, it is recommended that you contact Red Hat Global Support Services for assistance in troubleshooting.\n• You have identified the metadata of the missing physical volume. For details, see Finding the metadata of a missing LVM physical volume.\n• None Collect the following information needed for the and commands. You can collect the information about your disk and UUID by running the command.\n• is the path to the most recent metadata backup file for the VG, for example,\n• is the name of the VG that has the damaged or missing PV.\n• of the PV that was damaged on this device is the value taken from the output of the command.\n• is the name of the disk where the PV is supposed to be, for example, . Be certain this is the correct disk, or seek help, otherwise following these steps may lead to data loss.\n• None Optionally, verify that the headers are valid:\n• None Restore the VG metadata on the disk: If there is no metadata backup file for the VG, you can get one by using the procedure in Saving extracted metadata to a file.\n• None To verify that the new physical volume is intact and the volume group is functioning correctly, check the output of the following command:\n• How to repair metadata on physical volume online? (Red Hat Knowledgebase)\n• How do I restore a volume group in Red Hat Enterprise Linux if one of the physical volumes that constitute the volume group has failed? (Red Hat Knowledgebase) 13.12. Repairing a disk with damaged LVM headers and metadata using the pvck command This is an alternative to the Repairing a disk with damaged LVM headers and metadata using the pvcreate and the vgcfgrestore commands. There may be cases where the and the commands do not work. This method is more targeted at the damaged disk. This method uses a metadata input file that was extracted by , or a backup file from . When possible, use metadata saved by from another PV in the same VG, or from a second metadata area on the PV. For more information, see Saving extracted metadata to a file.\n• None Repair the headers and metadata on the disk:\n• is a file containing the most recent metadata for the VG. This can be , or it can be a file containing raw metadata text from the command output.\n• is the name of the disk where the PV is supposed to be, for example, . To prevent data loss, verify that is the correct disk. If you are not certain the disk is correct, contact Red Hat Support. If the metadata file is a backup file, the should be run on each PV that holds metadata in VG. If the metadata file is raw metadata that has been extracted from another PV, the needs to be run only on the damaged PV.\n• None To check that the new physical volume is intact and the volume group is functioning correctly, check outputs of the following commands:\n• How to repair metadata on physical volume online? (Red Hat Knowledgebase)\n• How do I restore a volume group in Red Hat Enterprise Linux if one of the physical volumes that constitute the volume group has failed? (Red Hat Knowledgebase) You can troubleshoot various issues in LVM RAID devices to correct data errors, recover devices, or replace failed devices. LVM provides scrubbing support for RAID logical volumes. RAID scrubbing is the process of reading all the data and parity blocks in an array and checking to see whether they are coherent. The command initiates a background synchronization action on the array.\n• None Optional: Control the rate at which a RAID logical volume is initialized by setting any one of the following options:\n• sets the maximum recovery rate for a RAID logical volume so that it will not expel nominal I/O operations.\n• None sets the minimum recovery rate for a RAID logical volume to ensure that I/O for sync operations achieves a minimum throughput, even when heavy nominal I/O is present Replace 4K with the recovery rate value, which is an amount per second for each device in the array. If you provide no suffix, the options assume kiB per second per device. When you perform a RAID scrubbing operation, the background I/O required by the actions can crowd out other I/O to LVM devices, such as updates to volume group metadata. This might cause the other LVM operations to slow down. You can also use these maximum and minimum I/O rate while creating a RAID device. For example, creates a 2-way RAID10 array my_lv, which is in the volume group my_vg with 3 stripes that is 10G in size with a maximum recovery rate of 128 kiB/sec/device.\n• None Display the number of discrepancies in the array, without repairing them: This command initiates a background synchronization action on the array.\n• Optional: View the file for the kernel messages.\n• None Correct the discrepancies in the array: This command repairs or replaces failed devices in a RAID logical volume. You can view the file for the kernel messages after executing this command.\n• and man pages on your system RAID is not similar to traditional LVM mirroring. In case of LVM mirroring, remove the failed devices. Otherwise, the mirrored logical volume would hang while RAID arrays continue running with failed devices. For RAID levels other than RAID1, removing a device would mean converting to a lower RAID level, for example, from RAID6 to RAID5, or from RAID4 or RAID5 to RAID0. Instead of removing a failed device and allocating a replacement, with LVM, you can replace a failed device that serves as a physical volume in a RAID logical volume by using the argument of the command.\n• None The volume group includes a physical volume that provides enough free capacity to replace the failed device. If no physical volume with enough free extents is available on the volume group, add a new, sufficiently large physical volume by using the utility.\n• None View the RAID logical volume after the /dev/sdc device fails: # /dev/sdc: open failed: No such device or address Couldn't find device with uuid A4kRl2-vIzA-uyCb-cci7-bOod-H5tX-IzH4Ee. WARNING: Couldn't find all devices for LV my_vg/my_lv_rimage_1 while checking used and assumed devices. WARNING: Couldn't find all devices for LV my_vg/my_lv_rmeta_1 while checking used and assumed devices. LV Cpy%Sync Devices my_lv 100.00 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0) [my_lv_rimage_0] /dev/sde1(1) [my_lv_rimage_1] [unknown](1) [my_lv_rimage_2] /dev/sdd1(1) [my_lv_rmeta_0] /dev/sde1(0) [my_lv_rmeta_1] [unknown](0) [my_lv_rmeta_2] /dev/sdd1(0)\n• None # /dev/sdc: open failed: No such device or address Couldn't find device with uuid A4kRl2-vIzA-uyCb-cci7-bOod-H5tX-IzH4Ee. WARNING: Couldn't find all devices for LV my_vg/my_lv_rimage_1 while checking used and assumed devices. WARNING: Couldn't find all devices for LV my_vg/my_lv_rmeta_1 while checking used and assumed devices. Attempt to replace failed RAID images (requires full device resync)? [y/n]: y Faulty devices in my_vg/my_lv successfully replaced.\n• None Optional: Manually specify the physical volume that replaces the failed device:\n• None Examine the logical volume with the replacement: # /dev/sdc: open failed: No such device or address /dev/sdc1: open failed: No such device or address Couldn't find device with uuid A4kRl2-vIzA-uyCb-cci7-bOod-H5tX-IzH4Ee. LV Cpy%Sync Devices my_lv 43.79 my_lv_rimage_0(0),my_lv_rimage_1(0),my_lv_rimage_2(0) [my_lv_rimage_0] /dev/sde1(1) [my_lv_rimage_1] /dev/sdb1(1) [my_lv_rimage_2] /dev/sdd1(1) [my_lv_rmeta_0] /dev/sde1(0) [my_lv_rmeta_1] /dev/sdb1(0) [my_lv_rmeta_2] /dev/sdd1(0) Until you remove the failed device from the volume group, LVM utilities still indicate that LVM cannot find the failed device.\n• None Remove the failed device from the volume group:\n• None View the available physical volumes after removing the failed device:\n• None Examine the logical volume after the replacing the failed device:\n• and man pages on your system When using LVM with multipathed storage, LVM commands that list a volume group or logical volume might display messages such as the following: Found duplicate PV GDjTZf7Y03GJHjteqOwrye2dcSCjdaUi: using /dev/dm-5 not /dev/sdd Found duplicate PV GDjTZf7Y03GJHjteqOwrye2dcSCjdaUi: using /dev/emcpowerb not /dev/sde Found duplicate PV GDjTZf7Y03GJHjteqOwrye2dcSCjdaUi: using /dev/sddlmab not /dev/sdf You can troubleshoot these warnings to understand why LVM displays them, or to hide the warnings. When a multipath software such as Device Mapper Multipath (DM Multipath), EMC PowerPath, or Hitachi Dynamic Link Manager (HDLM) manages storage devices on the system, each path to a particular logical unit (LUN) is registered as a different SCSI device. The multipath software then creates a new device that maps to those individual paths. Because each LUN has multiple device nodes in the directory that point to the same underlying data, all the device nodes contain the same LVM metadata. Table 13.1. Example device mappings in different multipath software As a result of the multiple device nodes, LVM tools find the same metadata multiple times and report them as duplicates. LVM displays the duplicate PV warnings in either of the following cases: Single paths to the same device The two devices displayed in the output are both single paths to the same device. The following example shows a duplicate PV warning in which the duplicate devices are both single paths to the same device. If you list the current DM Multipath topology using the command, you can find both and under the same multipath map. These duplicate messages are only warnings and do not mean that the LVM operation has failed. Rather, they are alerting you that LVM uses only one of the devices as a physical volume and ignores the others. If the messages indicate that LVM chooses the incorrect device or if the warnings are disruptive to users, you can apply a filter. The filter configures LVM to search only the necessary devices for physical volumes, and to leave out any underlying paths to multipath devices. As a result, the warnings no longer appear. The two devices displayed in the output are both multipath maps. The following examples show a duplicate PV warning for two devices that are both multipath maps. The duplicate physical volumes are located on two different devices rather than on two different paths to the same device. Found duplicate PV GDjTZf7Y03GJHjteqOwrye2dcSCjdaUi: using not Found duplicate PV GDjTZf7Y03GJHjteqOwrye2dcSCjdaUi: using not This situation is more serious than duplicate warnings for devices that are both single paths to the same device. These warnings often mean that the machine is accessing devices that it should not access: for example, LUN clones or mirrors. Unless you clearly know which devices you should remove from the machine, this situation might be unrecoverable. Red Hat recommends that you contact Red Hat Technical Support to address this issue. The following examples show LVM device filters that avoid the duplicate physical volume warnings that are caused by multiple storage paths to a single logical unit (LUN). You can configure the filter for logical volume manager (LVM) to check metadata for all devices. Metadata includes local hard disk drive with the root volume group on it and any multipath devices. By rejecting the underlying paths to a multipath device (such as , ), you can avoid these duplicate PV warnings, because LVM finds each unique metadata area once on the multipath device itself.\n• None To accept the second partition on the first hard disk drive and any device mapper (DM) Multipath devices and reject everything else, enter:\n• None To accept all HP SmartArray controllers and any EMC PowerPath devices, enter:\n• None To accept any partitions on the first IDE drive and any multipath devices, enter:"
    },
    {
        "link": "https://medium.com/@yhakimi/lvm-how-to-create-and-extend-a-logical-volume-in-linux-9744f27eacfe",
        "document": "Let’s get started by talking a bit about LVM\n\nLVM deals with the storage in a way that is, by far, more efficient than traditional disk management. With standard disk partitioning, the storage capacity is based on the individual disk capacity, but with LVM, the storage space is managed by combining all the available physical hard drives as if they are part of a pool, making them usable as a whole, instead of handling them individually.\n\nLet’s say we have four 1TB drives, with a traditional disk scheme you would handle them individually but with LVM, these four 1TB drives would be considered to be this one 4TB single chunk or aggregated storage capacity. This gives us greater flexibility and control over the disk layout and allows us to manipulate disks in an easier way. One of the main benefits of using LVM is the ability to effortlessly grow the filesystem.\n\nTo understand and use LVM we need to comprehend three main components, they are interconnected and together make what we call a Logical Volume, these components are:\n\nPhysical Volume: These are the base block used to create an LVM, physical volumes or “PVs” are simply your physical storage devices, whether it is SSD or HDD drives. For a hard drive to be considered a physical volume, it has to be initialized marked as a physical volume so it can be used by the LVM.\n\nVolume Group: We can think of a volume group “VG” as a pool that is comprised of physical volumes. Let’s say we three 1TB SSD hard drives that are part of a volume group, this “VG” will show up as having a consolidated storage capacity of 3TB, which will then be used to create logical volumes.\n\nLogical Volumes: When our VG is created, we can finally create a logical volume. We can carve one or more logical volumes from a single volume group. A logical volume will be treated as a traditional partition that would be then mounted on a directory and be in use.\n\nThe figure below illustrates the structure of a Logical Volume:\n\nIt starts from physical devices, the hard drives, that are used to create our Physical Volumes or PVs. In this example, we have three separate HDDs, each is used to create one Physical Volume. The partition /dev/sda2 will make our first PV, /dev/sdb1 the second and sdc1 the third.\n\nMoving up we have two separate Volume groups and each of these have their distinctive Logical Volumes, as we said earlier, LVs are carved out from Volume groups, we may have one or many Logical Volumes coming from one VG.\n\nThe final layer in this abstraction is the Logical Volume, for example, lv-data that comes from the vg-data volume group, once lv-data is ready it can be formatted and used as a mount point, in this case: /dev/vg-data/lv-data."
    },
    {
        "link": "https://askubuntu.com/questions/219881/how-can-i-create-one-logical-volume-over-two-disks-using-lvm",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    }
]