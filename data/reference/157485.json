[
    {
        "link": "https://pytorch.org/docs/stable/nn.html",
        "document": "These are the basic building blocks for graphs:\n\nApplies a 1D convolution over an input signal composed of several input planes. Applies a 2D convolution over an input signal composed of several input planes. Applies a 3D convolution over an input signal composed of several input planes. Applies a 1D transposed convolution operator over an input image composed of several input planes. Applies a 2D transposed convolution operator over an input image composed of several input planes. Applies a 3D transposed convolution operator over an input image composed of several input planes. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. Combines an array of sliding local blocks into a large containing tensor.\n\nApplies a 1D max pooling over an input signal composed of several input planes. Applies a 2D max pooling over an input signal composed of several input planes. Applies a 3D max pooling over an input signal composed of several input planes. Applies a 1D average pooling over an input signal composed of several input planes. Applies a 2D average pooling over an input signal composed of several input planes. Applies a 3D average pooling over an input signal composed of several input planes. Applies a 2D fractional max pooling over an input signal composed of several input planes. Applies a 3D fractional max pooling over an input signal composed of several input planes. Applies a 1D power-average pooling over an input signal composed of several input planes. Applies a 2D power-average pooling over an input signal composed of several input planes. Applies a 3D power-average pooling over an input signal composed of several input planes. Applies a 1D adaptive max pooling over an input signal composed of several input planes. Applies a 2D adaptive max pooling over an input signal composed of several input planes. Applies a 3D adaptive max pooling over an input signal composed of several input planes. Applies a 1D adaptive average pooling over an input signal composed of several input planes. Applies a 2D adaptive average pooling over an input signal composed of several input planes. Applies a 3D adaptive average pooling over an input signal composed of several input planes.\n\nPads the input tensor using the reflection of the input boundary. Pads the input tensor using the reflection of the input boundary. Pads the input tensor using the reflection of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with a constant value. Pads the input tensor boundaries with a constant value. Pads the input tensor boundaries with a constant value. Pads the input tensor using circular padding of the input boundary. Pads the input tensor using circular padding of the input boundary. Pads the input tensor using circular padding of the input boundary.\n\nCreates a criterion that measures the mean absolute error (MAE) between each element in the input x and target y. Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input x and target y. This criterion computes the cross entropy loss between input logits and target. Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities: This loss combines a layer and the in one single class. Creates a criterion that measures the loss given inputs x1, x2, two 1D mini-batch or 0D , and a label 1D mini-batch or 0D y (containing 1 or -1). Measures the loss given an input tensor x and a labels tensor y (containing 1 or -1). Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input x (a 2D mini-batch ) and output y (which is a 2D of target class indices). Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. Creates a criterion that optimizes a two-class classification logistic loss between input tensor x and target tensor y (containing 1 or -1). Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input x and target y of size (N,C). Creates a criterion that measures the loss given input tensors x1​, x2​ and a label y with values 1 or -1. Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input x (a 2D mini-batch ) and output y (which is a 1D tensor of target class indices, 0≤y≤x.size(1)−1): Creates a criterion that measures the triplet loss given an input tensors x1, x2, x3 and a margin with a value greater than 0. Creates a criterion that measures the triplet loss given input tensors a, p, and n (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\"distance function\") used to compute the relationship between the anchor and positive example (\"positive distance\") and the anchor and negative example (\"negative distance\").\n\nClip the gradient norm of an iterable of parameters. Clip the gradient norm of an iterable of parameters. Clip the gradients of an iterable of parameters at specified value. Compute the norm of an iterable of tensors. Scale the gradients of an iterable of parameters given a pre-calculated total norm and desired max norm. Utility functions to flatten and unflatten Module parameters to and from a single vector. Flatten an iterable of parameters into a single vector. Copy slices of a vector into an iterable of parameters. Fuse a convolutional module and a BatchNorm module into a single, new convolutional module. Fuse convolutional module parameters and BatchNorm module parameters into new convolutional module parameters. Fuse a linear module and a BatchNorm module into a single, new linear module. Fuse linear module parameters and BatchNorm module parameters into new linear module parameters. Convert of to The conversion recursively applies to nested , including . Utility functions to apply and remove weight normalization from Module parameters. Apply weight normalization to a parameter in the given module. Apply spectral normalization to a parameter in the given module. Given a module class object and args / kwargs, instantiate the module without initializing parameters / buffers. Abstract base class for creation of new pruning techniques. Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. Prune (currently unpruned) units in a tensor at random. Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. Prune entire (currently unpruned) channels in a tensor at random. Prune entire (currently unpruned) channels in a tensor based on their L -norm. Prune tensor by removing units with the lowest L1-norm. Prune tensor by removing random channels along the specified dimension. Prune tensor by removing channels with the lowest L -norm along the specified dimension. Globally prunes tensors corresponding to all parameters in by applying the specified . Prune tensor corresponding to parameter called in by applying the pre-computed mask in . Remove the pruning reparameterization from a module and the pruning method from the forward hook. Check if a module is pruned by looking for pruning pre-hooks. Parametrizations implemented using the new parametrization functionality in . Apply an orthogonal or unitary parametrization to a matrix or a batch of matrices. Apply weight normalization to a parameter in the given module. Apply spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules. Note that these functions can be used to parametrize a given Parameter or Buffer given a specific function that maps from an input space to the parametrized space. They are not parameterizations that would transform an object into a parameter. See the Parametrizations tutorial for more information on how to implement your own parametrizations. Remove the parametrizations on a tensor in a module. Context manager that enables the caching system within parametrizations registered with . A sequential container that holds and manages the original parameters or buffers of a parametrized . Utility functions to call a given Module in a stateless manner. Perform a functional call on the module by replacing the module parameters and buffers with the provided ones. Holds the data and list of of a packed sequence."
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html",
        "document": "Click here to download the full example code\n\nNeural networks can be constructed using the package.\n\nNow that you had a glimpse of , depends on to define models and differentiate them. An contains layers, and a method that returns the .\n\nFor example, look at this network that classifies digit images:\n\nIt is a simple feed-forward network. It takes the input, feeds it through several layers one after the other, and then finally gives the output.\n\nA typical training procedure for a neural network is as follows:\n• None Define the neural network that has some learnable parameters (or weights)\n• None Compute the loss (how far is the output from being correct)\n• None Update the weights of the network, typically using a simple update rule:\n\nA loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target. There are several different loss functions under the nn package . A simple loss is: which computes the mean-squared error between the output and the target. # make it the same shape as output Now, if you follow in the backward direction, using its attribute, you will see a graph of computations that looks like this: So, when we call , the whole graph is differentiated w.r.t. the neural net parameters, and all Tensors in the graph that have will have their Tensor accumulated with the gradient. For illustration, let us follow a few steps backward: <MseLossBackward0 object at 0x7fa1139a9d80> <AddmmBackward0 object at 0x7fa1139a8b50> <AccumulateGrad object at 0x7fa1139ab2b0>\n\nTo backpropagate the error all we have to do is to . You need to clear the existing gradients though, else gradients will be accumulated to existing gradients. Now we shall call , and have a look at conv1’s bias gradients before and after the backward. # zeroes the gradient buffers of all parameters conv1.bias.grad before backward None conv1.bias.grad after backward tensor([ 0.0081, -0.0080, -0.0039, 0.0150, 0.0003, -0.0105]) Now, we have seen how to use loss functions. The neural network package contains various modules and loss functions that form the building blocks of deep neural networks. A full list with documentation is here. The only thing left to learn is:\n• None Updating the weights of the network"
    },
    {
        "link": "https://isip.piconepress.com/courses/temple/ece_4822/resources/books/Deep-Learning-with-PyTorch.pdf",
        "document": ""
    },
    {
        "link": "https://docs.alliancecan.ca/wiki/PyTorch",
        "document": "PyTorch is a Python package that provides two high-level features:\n\nIf you are porting a PyTorch program to one of our clusters, you should follow our tutorial on the subject.\n\nPyTorch has a distant connection with Torch, but for all practical purposes you can treat them as separate projects.\n\nPyTorch developers also offer LibTorch, which allows one to implement extensions to PyTorch using C++, and to implement pure C++ machine learning applications. Models written in Python using PyTorch can be converted and used in pure C++ through TorchScript.\n\nTo see the latest version of PyTorch that we have built:\n\nFor more information, see Available wheels.\n\nThe preferred option is to install it using the Python wheel as follows:\n\nNote: There are known issues with PyTorch 1.10 on our clusters (except for Narval). If you encounter problems while using distributed training, or if you get an error containing , we recommend installing PyTorch 1.9.1 using .\n\nIn addition to , you can install , and :\n\nHere is an example of a job submission script using the python wheel, with a virtual environment inside a job:\n\nThe Python script has the form\n\nYou can then submit a PyTorch job with:\n\nOn version 1.7.0 PyTorch has introduced support for Nvidia's TensorFloat-32 (TF32) Mode, which in turn is available only on Ampere and later Nvidia GPU architectures. This mode of executing tensor operations has been shown to yield up to 20x speed-ups compared to equivalent single precision (FP32) operations and is enabled by default in PyTorch versions 1.7.x up to 1.11.x. However, such gains in performance come at the cost of potentially decreased accuracy in the results of operations, which may become problematic in cases such as when dealing with ill-conditioned matrices, or when performing long sequences of tensor operations as is common in deep learning models. Following calls from its user community, TF32 is now disabled by default for matrix multiplications, but still enabled by default for convolutions starting with PyTorch version 1.12.0.\n\nAs of October 2022, our only cluster equipped with Ampere GPUs is Narval. When using PyTorch on Narval, users should be cognizant of the following:\n• You may notice a significant slowdown when running the exact same GPU-enabled code with and .\n• You may get different results when running the exact same GPU-enabled code with and .\n\nTo enable or disable TF32 on set the following flags to or accordingly:\n\nFor more information, see PyTorch's official documentation\n\nPyTorch natively supports parallelizing work across multiple CPUs in two ways: intra-op parallelism and inter-op parallelism.\n• intra-op refers to PyTorch's parallel implementations of operators commonly used in Deep Learning, such as matrix multiplication and convolution, using OpenMP directly or through low-level libraries like MKL and OneDNN. Whenever you run PyTorch code that performs such operations, they will automatically leverage multi-threading over as many CPU cores as are available to your job.\n• inter-op parallelism on the other hand refers to PyTorch's ability to execute different parts of your code concurrently. This modality of parallelism typically requires that you explicitly design your program such that different parts can run in parallel. Examples include code that leverages PyTorch's Just-In-Time compiler to run asynchronous tasks in a TorchScript program.\n\nWith small scale models, we strongly recommend using multiple CPUs instead of using a GPU. While training will almost certainly run faster on a GPU (except in cases where the model is very small), if your model and your dataset are not large enough, the speed up relative to CPU will likely not be very significant and your job will end up using only a small portion of the GPU's compute capabilities. This might not be an issue on your own workstation, but in a shared environment like our HPC clusters, this means you are unnecessarily blocking a resource that another user may need to run actual large scale computations! Furthermore, you would be unnecessarily using up your group's allocation and affecting the priority of your colleagues' jobs.\n\nThe code example below contains many opportunities for intra-op parallelism. By simply requesting more CPUs and without any code changes, we can observe the effect of PyTorch's native support for parallelism on performance:\n\nThere is a common misconception that you should definitely use a GPU for model training if one is available. While this may almost always hold true (training very small models is often faster on one or more CPUs) on your own local workstation equipped with a GPU, it is not the case on our HPC clusters.\n\nSimply put, you should not ask for a GPU if your code is not capable of making a reasonable use of its compute capacity.\n\nGPUs draw their performance advantage in Deep Learning tasks mainly from two sources:\n• Their ability to parallelize the execution of certain key numerical operations, such as multiply-accumulate, over many thousands of compute cores compared to the single-digit count of cores available in most common CPUs.\n• A much higher memory bandwidth than CPUs, which allows GPUs to efficiently use their massive number of cores to process much larger amounts of data per compute cycle.\n\nLike in the multi-cpu case, PyTorch contains parallel implementations of operators commonly used in Deep Learning, such as matrix multiplication and convolution, using GPU-specific libraries like CUDNN or MIOpen, depending on the hardware platform. This means that for a learning task to be worth running on a GPU, it must be composed of elements that scale out with massive parallelism in terms of the number of operations that can be performed in parallel, the amount of data they require, or, ideally, both. Concretely this means, for example, large models (with large numbers of units and layers), large inputs, or, ideally, both.\n\nIn the example below, we adapt the multi-cpu code from the previous section to run on one GPU and examine its performance. We can observe that two parameters play an important role: and . The first influences performance by increasing the size of our inputs at each iteration, thus putting more of the GPU's capacity to use. The second influences performance by streamlining the movement of our inputs from the Host's (or the CPU's) memory to the GPU's memory, thus reducing the amount of time the GPU sits idle waiting for data to process.\n\nTwo takeaways emerge from this:\n• Increase your to as much as you can fit in the GPU's memory to optimize your compute performance.\n• Use a with as many workers as you have to streamline feeding data to the GPU.\n\nOf course, is also an important parameter with respect to a model's performance on a given task (accuracy, error, etc.) and different schools of thought have different views on the impact of using large batches. This page will not go into this subject, but if you have reason to believe that a small (relative to space in GPU memory) batch size is best for your application, skip to Data Parallelism with a single GPU to see how to maximize GPU utilization with small inputs.\n\nIn cases where a model is fairly small, such that it does not take up a large portion of GPU memory and it cannot use a reasonable amount of its compute capacity, it is not advisable to use a GPU. Use one or more CPUs instead. However, in a scenario where you have such a model, but have a very large dataset and wish to perform training with a small batch size, taking advantage of Data parallelism on a GPU becomes a viable option.\n\nData Parallelism, in this context, refers to methods to perform training over multiple replicas of a model in parallel, where each replica receives a different chunk of training data at each iteration. Gradients are then aggregated at the end of an iteration and the parameters of all replicas are updated in a synchronous or asynchronous fashion, depending on the method. Using this approach may provide a significant speed-up by iterating through all examples in a large dataset approximately N times faster, where N is the number of model replicas. An important caveat of this approach, is that in order to get a trained model that is equivalent to the same model trained without Data Parallelism, the user must scale either the learning rate or the desired batch size in function of the number of replicas. See this discussion for more information.\n\nPyTorch has implementations of Data Parallelism methods, with the class being the one recommended by PyTorch maintainers for best performance. Designed to work with multiple GPUs, it can be also be used with a single GPU.\n\nIn the example that follows, we adapt the single GPU code from the previous section to use Data Parallelism. This task is fairly small - with a batch size of 512 images, our model takes up about 1GB of GPU memory space, and it uses only about 6% of its compute capacity during training. This is a model that should not be trained on our clusters. However, using Data Parallelism, we can fit up to 14 or 15 replicas of this model on a V100 GPU with 16GB memory and increase our resource usage, while getting a nice speed-up. We use Nvidia's Multi-Process Service (MPS), along with MPI to efficiently place multiple model replicas on one GPU:\n\nThere is a known issue with our PyTorch 1.10 wheel . Multi-GPU code that uses DistributedDataParallel running with this PyTorch version may fail unpredictably if the backend is set to or . We recommend using our latest PyTorch build instead of version 1.10 on all GP clusters.\n\nData Parallelism, in this context, refers to methods to perform training over multiple replicas of a model in parallel, where each replica receives a different chunk of training data at each iteration. Gradients are then aggregated at the end of an iteration and the parameters of all replicas are updated in a synchronous or asynchronous fashion, depending on the method. Using this approach may provide a significant speed-up by iterating through all examples in a large dataset approximately N times faster, where N is the number of model replicas. An important caveat of this approach, is that in order to get a trained model that is equivalent to the same model trained without Data Parallelism, the user must scale either the learning rate or the desired batch size in function of the number of replicas. See this discussion for more information. In the multiple-GPU case, each GPU hosts a replica of your model. Consequently, the model must be small enough to fit inside the memory of a single GPU. Refer to the Model Parallelism section for options to train very large models that do not fit inside a single GPU.\n\nThere are several ways to perform Data Parallelism using PyTorch. This section features tutorials on three of them: using the DistributedDataParallel class, using the PyTorch Lightning package and using the Horovod package.\n\nThe DistributedDataParallel class is the way recommended by PyTorch maintainers to use multiple GPUs, whether they are all on a single node, or distributed across multiple nodes.\n\nThe Python script has the form\n\nPyTorch Lightning is a Python package that provides interfaces to PyTorch to make many common, but otherwise code-heavy tasks, more straightforward. This includes training on multiple GPUs. The following is the same tutorial from the section above, but using PyTorch Lightning instead of explicitly leveraging the DistributedDataParallel class:\n\nHorovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. Its API allows you to retain the level of control over your training code that provides, but makes writing your scripts easier by abstracting away the need to directly configure process groups and dealing with the cluster scheduler's environment variables. It also features distributed optimizers, which may increase performance in some cases. The following is the same example as above, re-implemented using Horovod:\n\nIn cases where a model is too large to fit inside a single GPU, you can split it into multiple parts and load each one onto a separate GPU. In the example below, we revisit the code example from previous sections to illustrate how this works: we will split a Convolutional Neural Network in two parts - the convolutional/pooling layers and the densely connected feedforward layers. This job will request 2 GPUs and each of the two parts of the model will be loaded on its own GPU. We will also add code to perform pipeline parallelism and minimize as much as possible the amount of time the second GPU sits idle waiting for the outputs of the first. To do this, we will create a separate for each part of our model, create a sequence of modules by wrapping our model parts with , then use to break each input batch into chunks and feed them in parallel to all parts of our model.\n\nIn cases where a model is too large to fit inside a Single GPU and, additionally, the goal is to train such a model using a very large training set, combining model parallelism with data parallelism becomes a viable option to achieve high performance. The idea is straightforward: you will split a large model into smaller parts, give each part its own GPU, perform pipeline parallelism on the inputs, then, additionally, you will create replicas of this whole process, which will be trained in parallel over separate subsets of the training set. As in the example from the previous section, gradients are computed independently within each replica, then an aggregation of these gradients is used to update all replicas synchronously or asynchronously, depending on the method used. The main difference here is that each model replica lives in more than one GPU.\n\nThe following example is a reprise of the ones from previous sections. Here we combine Torch RPC and DistributedDataParallel to split a model in two parts, then train four replicas of the model distributed over two nodes in parallel. In other words, we will have 2 model replicas spanning 2 GPUs on each node. An important caveat of using Torch RPC is that currently it only supports splitting models inside a single node. For very large models that do not fit inside the combined memory space of all GPUs of a single compute node, see the next section on DeepSpeed.\n\nDeepSpeed is a deep learning training optimization library, providing the means to train massive billion parameter models at scale. Fully compatible with PyTorch, DeepSpeed features implementations of novel memory-efficient distributed training methods, based on the Zero Redundancy Optimizer (ZeRO) concept. Through the use of ZeRO, DeepSpeed enables distributed storage and computing of different elements of a training task - such as optimizer states, model weights, model gradients and model activations - across multiple devices, including GPU, CPU, local hard disk, and/or combinations of these devices. This \"pooling\" of resources, notably for storage, allows models with massive amounts of parameters to be trained efficiently, across multiple nodes, without explicitly handling Model, Pipeline or Data Parallelism in your code. The examples below show how to take advantage of DeepSpeed and its implementations of ZeRO variants through its PyTorch Lightning interface for ease of use.\n\nIn the following example, we use ZeRO Stage 3 to train a model using a \"pool\" of 4 GPUs. Stage 3 means all three of: optimizer states; model parameters; and model gradients will be split (sharded) between all 4 GPUs. This is more memory-efficient than pure Data Parallelism, where we would have a full replica of the model loaded on each GPU. Using DeepSpeed's optimizer instead of a native PyTorch one, performance is comparable with pure Data Parallelism. DeepSpeed's optimizers are JIT compiled at run-time and you must load the module where <version> must match the version used to build the PyTorch install you are using.\n\nZeRO with offload to CPU\n\nIn this example, we will again use ZeRO stage 3, but this time we enable offloading model parameters and optimizers states to the CPU. This means that the compute node's memory will be available to store these tensors while they are not required by any GPU computations, and additionally, optimizer steps will be computed on the CPU. For practical purposes, you can think of this as though your GPUs were gaining an extra 32GB of memory. This takes even more pressure off from GPU memory and would allow you to increase your batch size, for example, or increase the size of the model. Using DeepSpeed's optimizer instead of a native PyTorch one, performance remains at par with pure Data Parallelism. DeepSpeed's optimizers are JIT compiled at run-time and you must load the module where <version> must match the version used to build the PyTorch install you are using.\n\nZeRO with offload to NVMe\n\nIn this example, we use ZeRO stage 3 yet again, but this time we enable offloading model parameters and optimizers states to the local disk. This means that the compute node's local disk storage will be available to store these tensors while they are not required by any GPU computations. As before, optimizer steps will be computed on the CPU. Again, for practical purposes, you can think of this as extending GPU memory by however much storage is available on the local disk, though this time performance will significantly degrade. This approach works best (i.e., performance degradation is least noticeable) on NVMe-enabled drives, which have higher throughput and faster response times, but it can be used with any type of storage.\n\nWhether or not you expect your code to run for long time periods, it is a good habit to create Checkpoints during training. A checkpoint is a snapshot of your model at a given point during the training process (after a certain number of iterations or after a number of epochs) that is saved to disk and can be loaded at a later time. It is a handy way of breaking up jobs that are expected to run for a very long time, into multiple shorter jobs that may get allocated on the cluster more quickly. It is also a good way of avoiding losing progress in case of unexpected errors in your code or node failures.\n\nTo create a checkpoint when training with , we recommend using the callbacks parameter of the class. The following example shows how to instruct PyTorch to create a checkpoint at the end of every training epoch. Make sure the path where you want to create the checkpoint exists.\n\nThis code snippet will also load a checkpoint from , if there is one, and continue training from that point. For more information, please refer to the official PyTorch Lightning documentation.\n\nPlease refer to the official PyTorch documentation for examples on how to create and load checkpoints inside of a training loop.\n\nCheckpointing can also be done while running a distributed training program. With PyTorch Lightning, no extra code is required other than using the checkpoint callback as described above. If you are using DistributedDataParallel or Horovod however, checkpointing should be done only by one process (one of the ranks) of your program, since all ranks will have the same state at the end of each iteration. The following example uses the first process (rank 0) to create a checkpoint:\n\nYou must be careful when loading a checkpoint created in this manner. If a process tries to load a checkpoint that has not yet been saved by another, you may see errors or get wrong results. To avoid this, you can add a barrier to your code to make sure the process that will create the checkpoint finishes writing it to disk before other processes attempt to load it. Also note that will attempt to load tensors to the GPU that saved them originally ( in this case) by default. To avoid issues, pass to to load tensors on the correct GPU for each rank.\n\nOn AVX512 hardware (Béluga, Skylake or V100 nodes), older versions of Pytorch (less than v1.0.1) using older libraries (cuDNN < v7.5 or MAGMA < v2.5) may considerably leak memory resulting in an out-of-memory exception and death of your tasks. Please upgrade to the latest version.\n\nThere are cases where we get this kind of error:\n\nA C++ exception is thrown instead of a Python exception. This might happen when programming in C++ with libtorch, but it is unexpected when programming in Python. We cannot see the Python traceback, which makes it difficult to pinpoint the cause of the error in our python script. On Graham, it has been observed that using PyTorch 1.9.1 (instead of PyTorch 1.10.x) helps: it allows to get the Python traceback.\n\nLibTorch allows one to implement both C++ extensions to PyTorch and pure C++ machine learning applications. It contains \"all headers, libraries and CMake configuration files required to depend on PyTorch\", as described in the documentation.\n\nHow to use LibTorch\n\nLoad the modules required by Libtorch, then install PyTorch in a Python virtual environment:\n\nCreate the following two files:\n\nWith the python virtualenv activated, configure the project and compile the program:\n\nTo test an application with CUDA, request an interactive job with a GPU."
    },
    {
        "link": "https://pytorch.org/docs/stable/notes/modules.html",
        "document": "PyTorch uses modules to represent neural networks. Modules are:\n• None Building blocks of stateful computation. PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for easy construction of elaborate, multi-layer neural networks.\n• None Tightly integrated with PyTorch’s autograd system. Modules make it simple to specify learnable parameters for PyTorch’s Optimizers to update.\n• None Easy to work with and transform. Modules are straightforward to save and restore, transfer between CPU / GPU / TPU devices, prune, quantize, and more.\n\nThis note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch, many topics in this note are elaborated on in other notes or tutorials, and links to many of those documents are provided here as well.\n\nTo get started, let’s look at a simpler, custom version of PyTorch’s module. This module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules:\n• None It inherits from the base Module class. All modules should subclass for composability with other modules.\n• None It defines some “state” that is used in computation. Here, the state consists of randomly-initialized and tensors that define the affine transformation. Because each of these is defined as a , they are registered for the module and will automatically be tracked and returned from calls to . Parameters can be considered the “learnable” aspects of the module’s computation (more on this later). Note that modules are not required to have state, and can also be stateless.\n• None It defines a forward() function that performs the computation. For this affine transformation module, the input is matrix-multiplied with the parameter (using the short-hand notation) and added to the parameter to produce the output. More generally, the implementation for a module can perform arbitrary computation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be constructed and called: Note that the module itself is callable, and that calling it invokes its function. This name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module. The “forward pass” is responsible for applying the computation represented by the module to the given input(s) (as shown in the above snippet). The “backward pass” computes gradients of module outputs with respect to its inputs, which can be used for “training” parameters through gradient descent methods. PyTorch’s autograd system automatically takes care of this backward pass computation, so it is not required to manually implement a function for each module. The process of training module parameters through successive forward / backward passes is covered in detail in Neural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call to or , where the latter includes each parameter’s name: In general, the parameters registered by a module are aspects of the module’s computation that should be “learned”. A later section of this note shows how to update these parameters using one of PyTorch’s Optimizers. Before we get to that, however, let’s first examine how modules can be composed with one another.\n\nModules can contain other modules, making them useful building blocks for developing more elaborate functionality. The simplest way to do this is using the module. It allows us to chain together multiple modules: Note that automatically feeds the output of the first module as input into the , and the output of that as input into the second module. As shown, it is limited to in-order chaining of modules with a single input and output. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives full flexibility on how submodules are used for a module’s computation. For example, here’s a simple neural network implemented as a custom module: This module is composed of two “children” or “submodules” ( and ) that define the layers of the neural network and are utilized for computation within the module’s method. Immediate children of a module can be iterated through via a call to or : To go deeper than just the immediate children, and recursively iterate through a module and its child modules: Sometimes, it’s necessary for a module to dynamically define submodules. The and modules are useful here; they register submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules. This means that calls to and will recursively include child parameters, allowing for convenient optimization of all parameters within the network: It’s also easy to move all parameters to a different device or change their precision using : More generally, an arbitrary function can be applied to a module and its submodules recursively by using the function. For example, to apply custom initialization to parameters of a module and its submodules: # Note that no_grad() is used here to avoid tracking this computation in the autograd graph. # Apply the function recursively on the module and its submodules. These examples show how elaborate neural networks can be formed through module composition and conveniently manipulated. To allow for quick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of performant modules within the namespace that perform common neural network operations like pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out:\n\nOnce a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch’s Optimizers from : # Create the network (from previous section) and optimizer # to output the constant zero function # After training, switch the module to eval mode to do inference, compute performance metrics, etc. # (see discussion below for a description of training and evaluation modes) In this simplified example, the network learns to simply output zero, as any non-zero output is “penalized” according to its absolute value by employing as a loss function. While this is not a very interesting task, the key parts of training are present:\n• None An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network’s parameters are associated with it.\n• None\n• None calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network’s parameters have changed. In particular, examining the value of ‘s parameter shows that its values are now much closer to 0 (as may be expected): Note that the above process is done entirely while the network module is in “training mode”. Modules default to training mode and can be switched between training and evaluation modes using and . They can behave differently depending on which mode they are in. For example, the module maintains a running mean and variance during training that are not updated when the module is in evaluation mode. In general, modules should be in training mode during training and only switched to evaluation mode for inference or evaluation. Below is an example of a custom module that behaves differently between the two modes: Training neural networks can often be tricky. For more information, check out:\n\nIn the previous section, we demonstrated training a module’s “parameters”, or learnable aspects of computation. Now, if we want to save the trained model to disk, we can do so by saving its (i.e. “state dictionary”): # Load the module later on A module’s contains state that affects its computation. This includes, but is not limited to, the module’s parameters. For some modules, it may be useful to have state beyond parameters that affects module computation but is not learnable. For such cases, PyTorch provides the concept of “buffers”, both “persistent” and “non-persistent”. Following is an overview of the various types of state a module can have:\n• None Parameters: learnable aspects of computation; contained within the\n• \n• None Persistent buffers: contained within the (i.e. serialized when saving & loading)\n• None Non-persistent buffers: not contained within the (i.e. left out of serialization) As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want the current value of the running mean to be considered part of the module’s so that it will be restored when loading a serialized form of the module, but we don’t want it to be learnable. This snippet shows how to use to accomplish this: Now, the current value of the running mean is considered part of the module’s and will be properly restored when loading the module from disk: # Serialized form will contain the 'mean' tensor As mentioned previously, buffers can be left out of the module’s by marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied with : # Moves all module parameters and buffers to the specified device / dtype Buffers of a module can be iterated over using or . The following class demonstrates the various ways of registering parameters and buffers within a module: # Setting a nn.Parameter as an attribute of the module automatically registers the tensor # as a parameter of the module. # Reserves the \"param3\" attribute as a parameter, preventing it from being set to anything # except a parameter. \"None\" entries like this will not be present in the module's state_dict. # Registers a persistent buffer (one that appears in the module's state_dict). # Registers a non-persistent buffer (one that does not appear in the module's state_dict). # Reserves the \"buffer3\" attribute as a buffer, preventing it from being set to anything # except a buffer. \"None\" entries like this will not be present in the module's state_dict. # Adding a submodule registers its parameters as parameters of the module. # Note that non-persistent buffer \"buffer2\" and reserved attributes \"param3\" and \"buffer3\" do # not appear in the state_dict. For more information, check out:\n\nBy default, parameters and floating-point buffers for modules provided by are initialized during module instantiation as 32-bit floating point values on the CPU using an initialization scheme determined to perform well historically for the module type. For certain use cases, it may be desired to initialize with a different dtype, device (e.g. GPU), or initialization technique. Note that the device and dtype options demonstrated above also apply to any floating-point buffers registered for the module: While module writers can use any device or dtype to initialize parameters in their custom modules, good practice is to use and by default as well. Optionally, you can provide full flexibility in these areas for your custom module by conforming to the convention demonstrated above that all modules follow:\n• None Provide a constructor kwarg that applies to any parameters / buffers registered by the module.\n• None Provide a constructor kwarg that applies to any parameters / floating-point buffers registered by the module.\n• None Only use initialization functions (i.e. functions from ) on parameters and buffers within the module’s constructor. Note that this is only required to use ; see this page for an explanation. For more information, check out:\n\nIn Neural Network Training with Modules, we demonstrated the training process for a module, which iteratively performs forward and backward passes, updating module parameters each iteration. For more control over this process, PyTorch provides “hooks” that can perform arbitrary computation during a forward or backward pass, even modifying how the pass is done if desired. Some useful examples for this functionality include debugging, visualizing activations, examining gradients in-depth, etc. Hooks can be added to modules you haven’t written yourself, meaning this functionality can be applied to third-party or PyTorch-provided modules. PyTorch provides two types of hooks for modules:\n• None Forward hooks are called during the forward pass. They can be installed for a given module with and . These hooks will be called respectively just before the forward function is called and just after it is called. Alternatively, these hooks can be installed globally for all modules with the analogous and functions.\n• None Backward hooks are called during the backward pass. They can be installed with and . These hooks will be called when the backward for this Module has been computed. will allow the user to access the gradients for outputs while will allow the user to access the gradients both the inputs and outputs. Alternatively, they can be installed globally for all modules with and . All hooks allow the user to return an updated value that will be used throughout the remaining computation. Thus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or modify some inputs/outputs without having to change the module’s function. Below is an example demonstrating usage of forward and backward hooks: # Allows for examination and modification of the input before the forward pass. # Note that inputs are always wrapped in a tuple. # Allows for examination of inputs / outputs and modification of the outputs # after the forward pass. Note that inputs are always wrapped in a tuple while outputs # Allows for examination of grad_inputs / grad_outputs and modification of # grad_inputs used in the rest of the backwards pass. Note that grad_inputs and # grad_outputs are always wrapped in tuples. # Run input through module before and after adding hooks. # Note that the modified input results in a different output. # Remove hooks; note that the output here matches the output before adding hooks."
    },
    {
        "link": "https://discuss.pytorch.org/t/custom-loss-functions/29387",
        "document": "I’m implementing a custom loss function in Pytorch 0.4. Reading the docs and the forums, it seems that there are two ways to define a custom loss function:\n• Extending Module and implementing only the forward method. With that in mind, my questions are:\n• Can I write a python function that takes my model outputs as inputs and use torch.* functions to compute my loss function (without extending Function or Module)? If not, why?\n\nThis is a quite simple implementation of custom loss functions while there are not extra parameters. Could you please share some solutions to fix this problem?\n\n For example, in keras, you can implement weighted loss by following: Besides, BCELoss may doesn’t suit this case.\n\nThe argument in has the shape of the input batch, since the loss functions take floating point targets, which does not correspond to a class weighting schema. on the other side is closer to a class weighting, as it only weights the positive examples. Furthermore, you can balance the recall and precision changing the argument.\n\ni mean, for each batch the input of the loss function is a list of all predictions and labels in the current batch, and the loss is built for input of only one prediction and label so how it should be implement? and another thing - how the backward() of costume function should be implemented?\n\ni mean, for each batch the input of the loss function is a list of all predictions and labels in the current batch, and the loss is built for input of only one prediction and label so how it should be implement? and another thing - how the backward() of costume function should be implemented?\n\n@netaglazer\n\n I believe if you are worried about the first dimension being the Batch index, pytorch automatically extracts the individual predictions and accumulated the loss as batch loss. So, you can write your loss function assuming your batch has only one sample.\n\n @ptrblck could you please correct me if my understanding about loss function above is wrong?"
    },
    {
        "link": "https://neptune.ai/blog/pytorch-loss-functions",
        "document": "Your neural networks can do a lot of different tasks. Whether it’s classifying data, like grouping pictures of animals into cats and dogs, regression tasks, like predicting monthly revenues, or anything else. Every task has a different output and needs a different type of loss function.\n\nThe way you configure your loss functions can make or break the performance of your algorithm. By correctly configuring the loss function, you can make sure your model will work how you want it to.\n\nLuckily for us, there are loss functions we can use to make the most of machine learning tasks.\n\nIn this article, we’ll talk about popular loss functions in PyTorch, and about building custom loss functions. Once you’re done reading, you should know which one to choose for your project.\n\nWhat are the loss functions?\n\nBefore we jump into PyTorch specifics, let’s refresh our memory of what loss functions are.\n\nLoss functions are used to gauge the error between the prediction output and the provided target value. A loss function tells us how far the algorithm model is from realizing the expected outcome. The word ‘loss’ means the penalty that the model gets for failing to yield the desired results.\n\nFor example, a loss function (let’s call it J) can take the following two parameters:\n\nThis function will determine your model’s performance by comparing its predicted output with the expected output. If the deviation between y_pred and y is very large, the loss value will be very high.\n\nIf the deviation is small or the values are nearly identical, it’ll output a very low loss value. Therefore, you need to use a loss function that can penalize a model properly when it is training on the provided dataset.\n\nLoss functions change based on the problem statement that your algorithm is trying to solve.\n\nPyTorch’s torch.nn module has multiple standard loss functions that you can use in your project.\n\nTo add them, you need to first import the libraries:\n\nNext, define the type of loss you want to use. Here’s how to define the mean absolute error loss function:\n\nAfter adding a function, you can use it to accomplish your specific task.\n\nWhich loss functions are available in PyTorch?\n\nBroadly speaking, loss functions in PyTorch are divided into two main categories: regression losses and classification losses.\n\nRegression loss functions are used when the model is predicting a continuous value, like the age of a person.\n\nClassification loss functions are used when the model is predicting a discrete value, such as whether an email is spam or not.\n\nRanking loss functions are used when the model is predicting the relative distances between inputs, such as ranking products according to their relevance on an e-commerce search page.\n\nNow we’ll explore the different types of loss functions in PyTorch, and how to use them:\n\nThe Mean Absolute Error (MAE), also called L1 Loss, computes the average of the sum of absolute differences between actual values and predicted values.\n\nIt checks the size of errors in a set of predicted values, without caring about their positive or negative direction. If the absolute values of the errors are not used, then negative values could cancel out the positive values.\n\nThe Pytorch L1 Loss is expressed as:\n\nx represents the actual value and y the predicted value.\n\nWhen could it be used?\n• Regression problems, especially when the distribution of the target variable has outliers, such as small or big values that are a great distance from the mean value. It is considered to be more robust to outliers.\n\nThe Mean Squared Error (MSE), also called L2 Loss, computes the average of the squared differences between actual values and predicted values.\n\nPytorch MSE Loss always outputs a positive result, regardless of the sign of actual and predicted values. To enhance the accuracy of the model, you should try to reduce the L2 Loss—a perfect value is 0.0.\n\nThe squaring implies that larger mistakes produce even larger errors than smaller ones. If the classifier is off by 100, the error is 10,000. If it’s off by 0.1, the error is 0.01. This punishes the model for making big mistakes and encourages small mistakes.\n\nThe Pytorch L2 Loss is expressed as:\n\nx represents the actual value and y the predicted value.\n\nWhen could it be used?\n• MSE is the default loss function for most Pytorch regression problems.\n\nThe Negative Log-Likelihood Loss function (NLL) is applied only on models with the softmax function as an output activation layer. Softmax refers to an activation function that calculates the normalized exponential function of every unit in the layer.\n\nThe Softmax function is expressed as:\n\nThe function takes an input vector of size N, and then modifies the values such that every one of them falls between 0 and 1. Furthermore, it normalizes the output such that the sum of the N values of the vector equals to 1.\n\nNLL uses a negative connotation since the probabilities (or likelihoods) vary between zero and one, and the logarithms of values in this range are negative. In the end, the loss value becomes positive.\n\nIn NLL, minimizing the loss function assists us get a better output. The negative log likelihood is retrieved from approximating the maximum likelihood estimation (MLE). This means that we try to maximize the model’s log likelihood, and as a result, minimize the NLL.\n\nIn NLL, the model is punished for making the correct prediction with smaller probabilities and encouraged for making the prediction with higher probabilities. The logarithm does the punishment.\n\nNLL does not only care about the prediction being correct but also about the model being certain about the prediction with a high score.\n\nThe Pytorch NLL Loss is expressed as:\n\nwhere x is the input, y is the target, w is the weight, and N is the batch size.\n\nWhen could it be used?\n\nThis loss function computes the difference between two probability distributions for a provided set of occurrences or random variables.\n\nIt is used to work out a score that summarizes the average difference between the predicted values and the actual values. To enhance the accuracy of the model, you should try to minimize the score—the cross-entropy score is between 0 and 1, and a perfect value is 0.\n\nOther loss functions, like the squared loss, punish incorrect predictions. Cross-Entropy penalizes greatly for being very confident and wrong.\n\nUnlike the Negative Log-Likelihood Loss, which doesn’t punish based on prediction confidence, Cross-Entropy punishes incorrect but confident predictions, as well as correct but less confident predictions.\n\nThe Cross-Entropy function has a wide range of variants, of which the most common type is the Binary Cross-Entropy (BCE). The BCE Loss is mainly used for binary classification models; that is, models having only 2 classes.\n\nThe Pytorch Cross-Entropy Loss is expressed as:\n\nWhere x is the input, y is the target, w is the weight, C is the number of classes, and N spans the mini-batch dimension.\n\nWhen could it be used?\n• Binary classification tasks, for which it’s the default loss function in Pytorch.\n• Creating confident models—the prediction will be accurate and with a higher probability.\n\nThe Hinge Embedding Loss is used for computing the loss when there is an input tensor, x, and a labels tensor, y. Target values are between {1, -1}, which makes it good for binary classification tasks.\n\nWith the Hinge Loss function, you can give more error whenever a difference exists in the sign between the actual class values and the predicted class values. This motivates examples to have the right sign.\n\nThe Hinge Embedding Loss is expressed as:\n\nWhen could it be used?\n• Classification problems, especially when determining if two inputs are dissimilar or similar.\n\nThe Margin Ranking Loss computes a criterion to predict the relative distances between inputs. This is different from other loss functions, like MSE or Cross-Entropy, which learn to predict directly from a given set of inputs.\n\nWith the Margin Ranking Loss, you can calculate the loss provided there are inputs x1, x2, as well as a label tensor, y (containing 1 or -1).\n\nWhen y == 1, the first input will be assumed as a larger value. It’ll be ranked higher than the second input. If y == -1, the second input will be ranked higher.\n\nThe Pytorch Margin Ranking Loss is expressed as:\n\nWhen could it be used?\n\nThe Triplet Margin Loss computes a criterion for measuring the triplet loss in models. With this loss function, you can calculate the loss provided there are input tensors, x1, x2, x3, as well as margin with a value greater than zero.\n\nThe Pytorch Triplet Margin Loss is expressed as:\n\nWhen could it be used?\n• It is used in content-based retrieval problems\n\nThe Kullback-Leibler Divergence, shortened to KL Divergence, computes the difference between two probability distributions.\n\nWith this loss function, you can compute the amount of lost information (expressed in bits) in case the predicted probability distribution is utilized to estimate the expected target probability distribution.\n\nIts output tells you the proximity of two probability distributions. If the predicted probability distribution is very far from the true probability distribution, it’ll lead to a big loss. If the value of KL Divergence is zero, it implies that the probability distributions are the same.\n\nKL Divergence behaves just like Cross-Entropy Loss, with a key difference in how they handle predicted and actual probability. Cross-Entropy punishes the model according to the confidence of predictions, and KL Divergence doesn’t. KL Divergence only assesses how the probability distribution prediction is different from the distribution of ground truth.\n\nThe KL Divergence Loss is expressed as:\n\nx represents the true label’s probability and y represents the predicted label’s probability.\n\nWhen could it be used?\n• If you want to make sure that the distribution of predictions is similar to that of training data\n\nHow to create a custom loss function in PyTorch?\n\nPyTorch lets you create your own custom loss functions to implement in your projects.\n\nHere’s how you can create your own simple Cross-Entropy Loss function.\n\nYou can also create other advanced PyTorch custom loss functions.\n\nLet’s modify the Dice coefficient, which computes the similarity between two samples, to act as a loss function for binary classification problems:\n\nIt is quite obvious that while training a model, one needs to keep an eye on the loss function values to track the model’s performance. As the loss value keeps decreasing, the model keeps getting better. There are a number of ways that we can do this. Let’s take a look at them.\n\nFor this, we will be training a simple Neural Network created in PyTorch which will perform classification on the famous Iris dataset.\n\nMaking the required imports for getting the dataset.\n\nScaling the dataset to have mean=0 and variance=1, gives quick model convergence.\n\nSplitting the dataset into train and test in an 80-20 ratio.\n\nMaking the necessary imports for our Neural Network and its training.\n\nDefining functions for getting accuracy and training the network.\n\nAbove, we have used print statements in the train_network function to monitor the loss as well as accuracy. Let’s see this in action:\n\nWe get an output like this:\n\nSince we’ve stored the intermediate values in lists, we can also plot the metrics using Matplotlib:\n\nThis will give us a graph that helps us analyze the correlation between loss and accuracy:\n\nThis method gets the job done. But if we train several model versions with different parameters, or have to analyze the model’s performance over time, we need a more capable experiment tracking solution.\n\nA simpler way to monitor your metrics would be to log them in a service like neptune.ai and focus on more important tasks, such as building and training the model.\n\nTo get started with Neptune, we need to install a couple of libraries:\n\nneptune is a Python SDK to authorize communication between your scripts and Neptune. We will need python-dotenv for managing environment variables, such as Neptune project name and your API token.\n\nFirst, you need to retrieve those variables from your Neptune account:\n\nHere’s how it looks in the UI.\n\n3. Copy your API key and the project name which is in the format of workspace-name/project-name.\n\n4. Return to your code editor and create a file named .env in your working directory:\n\n5. Paste the following contents into the file:\n\nNext, we need to initialize a new Neptune run using the init_run() method. The method requires your API token and the project name, which we retrieve using the os and python-dotenv libraries:\n\nrun is an instance of a Run object which we can use to log any metadata related to our ML experiments including:\n\nThe syntax to log metadata is very intuitive as the Run object can be thought of as a special dictionary (don’t run the below snippet just yet):\n\nIf you need them later on, you can retrieve the logged details using a similar syntax:\n\nIn the next sections, we will use this pattern of code to capture our model’s training process to Neptune.\n\nTo log the loss of our model, we need to add a couple of lines to the train_network function (notice the three lines where we use the run object):\n\nLet’s rerun our model training and inspect the data that ends up in our Neptune project:\n\nUsing the PyTorch integration for advanced logging\n\nFor more sophisticated logging features such as automated capture of model parameters, logging frequency configuration, and model checkpointing, you can use neptune_pytorch library:\n\nThe NeptuneLogger class requires both the run and model objects to enable logging. Then, it can automatically capture model parameters and gradients at a frequency specified by log_freq.\n\nUsing the neptune_callback object requires us to change the lines of code where the run object is used. The PyTorch integration collects all the data under a specific key of the run object defined by neptune_callback.base_namespace, so we replace run[‘key’] by run[neptune_callback.base_namespace][‘key’]. For example:\n\nWith those lines changed, we can retrain the model:\n\nYou can view the result in the Neptune app.\n\nTo stop the connection to Neptune and sync all data, call the stop() method:\n\nUsing the `neptune_pytorch` integration is the recommended method for logging PyTorch models. It gives you finer control over metadata generated during training and allows you to log more challenging artifacts such as model checkpoints and predictions in a formatted syntax.\n\nThis article covered the most common loss functions in machine learning and how to use them in PyTorch. Choosing a loss function depends on the problem type like regression, classification or ranking. If none of the functions in today’s list don’t meet your requirements, PyTorch allows creating custom loss functions as well.\n\nLoss functions are critical in informing you about the performance of your model. Therefore, you will spend a lot of time monitoring the loss and changing your model training strategy accordingly. And in our view, the best way to monitor loss is by using an experiment tracking tool such as Neptune."
    },
    {
        "link": "https://discuss.pytorch.org/t/how-to-implement-a-custom-loss-in-pytorch/197938",
        "document": "I’m currently working on implementing a custom loss function for my project. The idea is to add a loss function with a set of existing ones. Specifically, I’m introducing a novel component calculated as a weight * MSE between two maps obtained online during training. However, I’ve encountered an issue where adjusting the weight of this new loss term doesn’t seem to have any impact on the final loss and accuracy of my model. Even when I set the weight to 1 or a significantly larger value, the results remain unchanged. More precisely, even when the seeds are fixed, we can see a contribution from the added loss (weight*MSE) in the result. The problem appears to be centered around the weight itself, which seems to have no influence on the outcome. The code follows this reasoning: loss = a * loss1 + b * loss2 + c * loss3 + d * loss4 #where d * loss4 is the custom implementation added by us loss1 = nn.CrossEntropyLoss() loss2 = nn.KLDivLoss() loss3 = nn.MSE() loss4 = nn.MSE() the types are the following: and the loss4 implementation has this structure: Do you have any advice on how to proceed?\n\n Thanks in advance\n\nthe weight of this new loss term doesn’t seem to have any impact on the final loss and accuracy of my model.\n\n …\n\n loss4 = criterion(torch.tensor(factor1, requires_grad=True).cuda(0, non_blocking=True), torch.tensor(factor2, requires_grad=True).cuda(0, non_blocking=True)) The factory function instantiates new tensors from \n\n and that are no longer connected to the computation graph. Setting\n\n for these new tensors doesn’t reconnect them to\n\n the existing computation graph, but, rather, makes them new leaves of the\n\n computation graph. So when you call on the full , gradients do not\n\n backpropagate through and via the term. So\n\n the weight of will, indeed, have no effect on your model’s training. Why are you creating new tensors for ? The fix might be as simple as:\n\nIt sounds to me like you’re trying to make a smaller network(or curve fitting) in order to optimize your loss function(s). In order to do that, we need to determine some method to train the smaller network on. First, we can build such a model with the class. class CustomLoss(nn.Module): def __init__(self): super().__init__() self.loss1 = nn.CrossEntropyLoss() self.loss2 = nn.KLDivLoss() self.loss3 = nn.MSE() self.loss4 = nn.MSE() #setting these to 0.25 so their total starts out as 1.0. self.a = nn.Parameter(0.25) self.b = nn.Parameter(0.25) self.c = nn.Parameter(0.25) self.d = nn.Parameter(0.25) def forward(self, x): return self.loss1(x) * self.a + self.loss2(x) * self.b + self.loss3(x) * self.c + self.loss4(x) * self.d Note: The above is by no means an endorsement of your setup. My initial thoughts without running this is that you’ll want something that self-regulates or constrains the total of (self.a + self.b + self.c + self.d) to be equal to 1 at all times. This way you can prevent exploding your loss function. Here’s an alternative function that might accomplish that: Second, you obviously will not have pre-designated targets for your custom loss model. So a reinforcement learning method is in order. And your main model’s training is an iterative process. By that, I mean that the benefit of any changes made to these values during training of the custom loss model might not make itself manifest until many iterations have been processed. Because of this, a DQN or PPO might be the best approach for training the custom loss model. PPO is more stable. Here is a PyTorch tutorial for that: Reinforcement Learning (PPO) with TorchRL Tutorial — torchrl main documentation\n\nHi Frank! Thanks a lot for the answer, I’ll explain myself better.\n\n Factor1 and factor2 are two images generated from two networks (one of which is frozen). My aim is to make the first and second network-generated images close to each other, so I’m using the MSE with a weight. I’m very interested in what you were saying about the loss4 term. The two tensors (factor1 and factor2) are generated at each epoch, and added in the following way: From your experience, can you see if this is done correctly? Unfortunately, the “d” weight seems to have no influence at all on the results. Should I define the factor tensors differently?\n\nHi Johnson, thanks a lot for the answer. To further clarify, our primary objective is to bring two images, generated during training, closer together by optimizing an additional loss term. I appreciate your suggestion to regularize the function by dividing it by the sum, I intend to incorporate it into the implementation. Regarding the TorchRL implementation, you mean to give the a,b,c,d parameters to PPO algorithm to fine-tune them online during training?\n\nThe way I’d set up a problem like this is:\n• Find the best a to d parameters. During this stage, set your main model parameters as small as possible. You want to train a lot of throw-away models quickly. Use a PPO algorithm for training the Custom Loss model.\n• Train the main model. Set the Custom Loss model parameters to static. And scale up the model you intend to train to the size you want. Note of Caution: There is an implicit assumption I’ve made here, which I think is very reasonable to assume: that is that the best combination of various loss functions will be the same, regardless of the scale of the model being trained, given the same data and problem."
    },
    {
        "link": "https://saturncloud.io/blog/custom-loss-function-in-pytorch-a-comprehensive-guide",
        "document": "As a data scientist or software engineer, you might have come across situations where the standard loss functions available in PyTorch are not enough to capture the nuances of your problem statement. In such cases, you can create custom loss functions in PyTorch to optimize your model’s performance.\n\nAs a data scientist or software engineer, you might have come across situations where the standard loss functions available in PyTorch are not enough to capture the nuances of your problem statement. In such cases, you can create custom loss functions in PyTorch to optimize your model’s performance.\n\nIn this blog post, we will be discussing how to create custom loss functions in PyTorch and integrate them into your neural network model.\n\nA loss function, also known as a cost function or objective function, is used to quantify the difference between the predicted and actual output of a machine learning model. The goal of training a machine learning model is to minimize the value of the loss function, which indicates that the model is making accurate predictions.\n\nPyTorch offers a wide range of loss functions for different problem statements, such as Mean Squared Error (MSE) for regression problems and Cross-Entropy Loss for classification problems. However, there are situations where these standard loss functions are not suitable for your problem statement.\n\nA custom loss function in PyTorch is a user-defined function that measures the difference between the predicted output of the neural network and the actual output. You can create custom loss functions in PyTorch by inheriting the class and implementing the method.\n\nHere’s an example of a custom loss function for a binary classification problem:\n\nIn this example, are the predicted outputs of the neural network, and are the actual outputs. The loss calculation is performed using the binary cross-entropy loss formula, which penalizes the model for making incorrect predictions.\n\nOnce you have defined your custom loss function, you can integrate it into your neural network model by passing it as an argument to the parameter of the optimizer.\n\nCustom loss functions can offer several advantages over standard loss functions in PyTorch.\n\nWith custom loss functions, you have complete control over the loss calculation process. You can define the loss function in a way that suits your problem statement the best.\n\nIn some cases, using a custom loss function can lead to improved model performance. This is because the custom loss function can capture the nuances of the problem statement better than the standard loss functions.\n\nCustom loss functions can be used with complex neural network models, such as GANs and LSTMs, where standard loss functions may not be sufficient.\n\nIn this blog post, we discussed how to create custom loss functions in PyTorch. Custom loss functions can offer several advantages over standard loss functions, such as flexibility, improved performance, and compatibility with complex models.\n\nWhen creating a custom loss function, it is essential to ensure that the function is differentiable, as PyTorch uses automatic differentiation to optimize the model. Additionally, the custom loss function should be designed to suit your problem statement the best.\n\nBy implementing custom loss functions, you can enhance your machine learning models and achieve better results.\n\nSaturn Cloud is your all-in-one solution for data science & ML development, deployment, and data pipelines in the cloud. Spin up a notebook with 4TB of RAM, add a GPU, connect to a distributed cluster of workers, and more. Request a demo today to learn more."
    },
    {
        "link": "https://stackoverflow.com/questions/53215816/making-custom-non-trivial-loss-function-in-pytorch",
        "document": "@Shai already summed it up: Your loss function is not differentiable.\n\nOne way to think about it is that your loss function should be plottable, and the \"downhill\" slope should \"roll\" toward the desired model output. In order to plot your loss function, fix then plot where is your loss function, and make sure your plotted loss function has the slope as desired. In your case, it sounds like you want to weight the the loss more strongly when it is on the wrong side of the threshold. As long as you can plot it, and the slope is always downhill toward your target value (no flat spots or uphill slopes on the way from a valid prediction to the target value), your model should learn from it.\n\nAlso note that if you're just trying to take into account some business objective which prioritizes precision over recall, you could accomplish this by training to convergence with cross entropy or some well-known loss function, and then by tuning your model threshold based on your use case. A higher threshold would normally prioritize precision, and a lower threshold would normally prioritize recall. After you've trained, you can then evaluate your model at a variety of thresholds and choose the most appropriate."
    }
]