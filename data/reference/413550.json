[
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html",
        "document": "Note: this implementation is restricted to the binary classification task.\n\nRead more in the User Guide.\n\nTrue binary labels. If labels are not either {-1, 1} or {0, 1}, then pos_label should be explicitly given. Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by “decision_function” on some classifiers). For decision_function scores, values greater than or equal to zero should indicate the positive class. The label of the positive class. When , if is in {-1, 1} or {0, 1}, is set to 1, otherwise an error will be raised. Whether to drop some suboptimal thresholds which would not appear on a plotted ROC curve. This is useful in order to create lighter ROC curves. Increasing false positive rates such that element i is the false positive rate of predictions with score >= . Increasing true positive rates such that element is the true positive rate of predictions with score >= . Decreasing thresholds on the decision function used to compute fpr and tpr. represents no instances being predicted and is arbitrarily set to .\n\nSince the thresholds are sorted from low to high values, they are reversed upon returning them to ensure they correspond to both and , which are sorted in reversed order during their calculation.\n\nAn arbitrary threshold is added for the case and to ensure that the curve starts at . This threshold corresponds to the ."
    },
    {
        "link": "https://r-craft.org/scikit-learn-roc_curve-explained",
        "document": "This tutorial will show you how to use the Scikit Learn roc_curve function.\n\nIt will explain the syntax of the function and show an example of how to use it.\n\nThe tutorial is organized into sections, so if you need something specific, you can click on any of the following links.\n\nI’m going to show you how to use the Scikit-learn roc_curve function later in the tutorial (so you’re welcome to skip ahead to the syntax section).\n\nBut it might be helpful for you to understand some of the conceptual foundations of ROC curves first.\n\nWith that in mind, let’s quickly review ROC curves and where they fit into building and evaluating classification systems.\n\nClassifiers are supervised learning systems that input data and then output a categorical label. Said differently, classifiers are systems that predict categories.\n\nIn binary classification, which is arguably the most common type of classification, there are two possible outcomes for the outputs.\n\nHaving said that, we sometimes encode those binary outcomes in different ways depending on the problem.\n\nSome examples of binary encodings are:\n\nBut whether you use 0 and 1, and , or any other binary representation, what’s crucial is the underlying concept that the predicted outputs of binary classifiers can only take 2 possible values.\n\nIt’s also important to realize that when a classifier makes a prediction, that prediction can be correct or incorrect.\n\nFour Types of Correct and Incorrect Predictions\n\nFor a simple binary classification system, there are actually 4 possible correct and incorrect predictions.\n\nTo illustrate this, let’s assume that we’re working with a very general classifier that only outputs 2 predictions: and .\n\nFor example, if we were working with an email spam classifier, the classifier would output if it thought that a particular email was spam, and it would output if it though that the email was not spam.\n\nIn such a case, there are 4 types of correct and incorrect predictions, as follows:\n• Correctly predict if the actual email is spam.\n• Correctly predict if the actual email is not spam.\n• Incorrectly predict if the actual email is not spam.\n• Incorrectly predict if the actual email is spam.\n\nThese different types of correct and incorrect predictions are important for understanding ROC curves, so let’s quickly discuss them.\n\nEach of these different correct and incorrect prediction types have names.\n\nIf we’re strictly using the generalized terminology of positive/negative, then the 4 prediction types described above can be described more generally as follows:\n• True Positive: Correctly predict if the actual value is positive.\n• True Negative: Correctly predict if the actual value is negative.\n• False Positive: Incorrectly predict if the actual value is negative.\n• False Negative: Incorrectly predict if the actual value is positive.\n\nThese concepts of True Positive, True Negative, False Positive, and False Negative are very important in classification.\n\nThey show up over and over in different concepts like Precision, Recall, Sensitivity and Specificity …\n\nAnd they’re very important for understanding ROC curves.\n\nWhat is An ROC Curve?\n\nAn ROC curve – which stands for Receiver Operating Characteristic is a visual diagnostic tool that we use to evaluate classifiers.\n\nOriginally used in signal detection during World War II, ROC curves are now very commonly used across machine learning.\n\nAn ROC curve visualizes the performance of a classification model for different values of classification threshold.\n\nRemember: most classification models produce a probability score – sometimes known as a confidence score – that measures the confidence that the model has that a particular example should be labeled with the positive class.\n\nWhen we use a classifier, we compare that confidence score against a threshold – a threshold that we can choose and we can optionally change – to decide if a particular example will be predicted as positive.\n\nIf the classifier’s confidence score is higher than the threshold for a particular example, then that classifier will get the positive label. If the confidence score is lower than the threshold, the example gets the negative label.(Note, what I just wrote applies to binary classification. Multiclass classification is a little more complicated.)\n\nImportantly though, we get to choose the threshold.\n\nAnd the model will have different True Positive Rate and False Positive rates for different thresholds.\n\nAn ROC Curve Plots True Positive Rate and False Positive Rate For Different Classification Thresholds\n\nThe ROC curve plots the True Positive Rate and False Positive Rate for all of those different classification thresholds.\n\nSo the ROC curve gives us a visual representation of a classifier’s performance across different thresholds.\n\nIn turn this allows us to see the possible tradeoffs in choosing one classification threshold vs another, because increasing the threshold will decrease true positives (since fewer samples will meet the higher threshold) while also decreasing false positives (which can be a good thing). But decreasing the threshold will increase true positives while also increasing false positives.\n\nThere’s always a tradeoff when we choose a particular classification threshold, and the ROC curve helps us see and understand those tradeoffs for a particular classifier.\n\nIn order to plot an ROC curve, we need to:\n• make predictions with that model on the basis of input feature data\n• compare the predictions vs the actual values (which we should have when we train the classifier)\n• compute both the True Positive Rate and the False Positive Rate for a range of classification thresholds between 0 and 1\n• plot the True Positive Rate vs False Positive Rate for those thresholds, with FPR on the x-axis and TPR on the y-axis\n\nPerhaps the most critical step that’s specific to building an ROC curve is that fourth step.\n\nWe need a way to compute TPR and FPR for that range of thresholds.\n\nHow do we do that?\n\nIn Python, we can use the Scikit-learn roc_curve function.\n\nLet’s take a look at how it works.\n\nHere, we’ll cover the syntax of the Scikit Learn roc_curve function.\n\nBefore we look at the syntax, I want to remind you that in order to use the roc_curve function, you first need to import it.\n\nTo do that, you can use the following code:\n\nOnce you’ve imported it, you’ll be ready to use it.\n\nAssuming that you’ve imported the roc_curve function as described above, you use it as follows:\n\nYou call the function as .\n\nInside the parenthesis, you need to provide a set of “y labels” as the first argument.\n\nThe second argument is a set of “y scores,” which can actually take a few different forms. I’ll explain that more soon.\n\nThen after the y labels and y scores, there are a few optional parameters.\n\nTo understand all of these arguments better, let’s look at each parameter and input individually.\n\nThere are 2 required arguments, and a few optional parameters for the Python roc_curve function:\n\nLet’s look at each one individually.\n\nThe argument is actual labels for the classification dataset that you’re using.\n\nTypically, after you split your data into training and test datasets, the values that we use for will be the y labels for the test dataset.\n\nIn terms of the structure of this input, it should be a Numpy array with a length equal to the number of samples in your dataset.\n\nThe argument is a Numpy array that contains the “scores” produced by the model for each example.\n\nSpecifically though, these scores can be any of the following:\n• “decision scores”, which we can obtain by using the Scikit Learn decision_function\n\nAll of these will work, but which ones will be available to generate and use will depend on the exact classifier that you use.\n\nStructurally, this will be a Numpy array with the same shape as .\n\nThe parameter enables you to specify the label of the positive class.\n\nBy default, this is set to . In this case, if the possible y labels are or , the system will assume that is the positive class.\n\nThe parameter allows you to specify an array of sample weights.\n\nStructurally, this will be a 1-dimensional Numpy array or array-like object with a length equal to the number of examples (i.e., one weight per example).\n\nThe parameter allows you to specify whether or not to “drop suboptimal threshold” values from the ROC curve. This creates a “lighter” ROC curve.\n\nBy default, this parameter is set to\n\nNow, let’s look at the output of the Scikit Learn roc_curve function.\n\nLet me quickly explain these with a little more detail\n\nThis array contains the false positive rates for the model.\n\nThese are organized in increasing order, such that the ith element is the false positive rate for predictions with a score greater than or equal to .\n\nThis array contains the true positive rates for the model.\n\nThese are organized in increasing order, such that the ith element is the true positive rate for predictions with a score greater than or equal to .\n\nThis array contains the thresholds for the decision function that are used to compute the false positive and true positive rates.\n\nAs noted in the official documentation, “ represents no instances being predicted and is arbitrarily set to max(y_score) + 1.”\n\nExamples of How to Generate ROC data with Sklearn roc_curve\n\nOk. Now that we’ve looked at the syntax, let’s take a look at an example or two of how to use the Scikit Learn roc_curve function\n\nBefore you run the examples, make sure to import the Scikit Learn roc_curve as well as a few other tools from Scikit Learn.\n\nWe’ll also import Seaborn Objects, which we’ll use to plot the ROC curve later.\n\nOk. Let’s look at the first example.\n\nHere, we’re going to create the data for an ROC curve for a simple binary classifier.\n\nTo do this, we need to:\n\nWe’ll also plot the ROC curve at the end.\n\nFirst, we need to create the right inputs for the ROC curve function.\n\nUltimately, we’re going to use two inputs:\n• probabilities for the positive class, as predicted by the model\n\nSo to get these, we’ll use make_classification to create a binary dataset.\n\nWe’ll use train_test_split to split the data into a training dataset and test dataset.\n\nAnd finally, we’ll predict the y-test values on the basis of the X_test dataset, using the Scikit Learn predict_proba method.\n\nNotice as well that at the end, we’re retrieving the probabilities for the positive class, and saving that information as .\n\nWe’ll use and as the inputs to .\n\nOk. Now we’re ready to use the function.\n\nHere, we’re going to call with as the first input and as the second input.\n\nAfter you run this, you can print out the contents of each.\n\nHere is the “false positive rate” data:\n\nHere is the “true positive rate” data:\n\nAnd here are the thresholds:\n\nPrinting them out might help familiarize you with the output, but in this form, the data probably doesn’t mean much.\n\nHere, we’ll plot the ROC data with Seaborn Objects:\n\nEffectively here, we’re plotting the “false positive rate” on the x-axis, and we’re plotting the “true positive rate” on the y-axis.\n\nFor more information on exactly how we’re plotting this data, check out our tutorial how to plot an ROC curve in Python, using Seaborn.\n\nDo you have any other questions about Python ROC curves?\n\nIf so, leave your questions in the comments section at the bottom of the page.\n\nFor more Machine Learning tutorials, sign up for our email list\n\nIf you’re trying to upskill and learn machine learning, then ROC curves are a useful tool.\n\nBut, you’ll still need to learn a lot more.\n\nSo if you want to master machine learning, sign up for our email list.\n\nWhen you sign up, you’ll get free tutorials on:\n\nWe publish free machine learning and data science tutorials every week, and when you sign up for our email list, we’ll deliver those free tutorials right to your inbox.\n\n[Now that we have a solid understanding of the fundamental concepts in binary classification, such as True Positive, True Negative, False Positive, and False Negative, let’s delve into a powerful tool used to evaluate the performance of binary classifiers: the ROC curve.\n\nWhat is an ROC Curve?\n\nROC stands for Receiver Operating Characteristic. It’s a graphical representation that illustrates the diagnostic ability of a binary classification system as its discrimination threshold is varied. In simpler terms, the ROC curve helps us see how well our classifier can distinguish between the two classes at different threshold levels.\n\nWhy is the Threshold Important?\n\nIn many classification systems, the prediction is not just a hard “yes” or “no”. Instead, it’s a probability score indicating the likelihood of an instance belonging to the positive class. This score is then compared to a threshold to decide the final classification. By varying this threshold, we can influence our model’s sensitivity (true positive rate) and specificity (false positive rate).\n\nThe ROC curve is plotted with the True Positive Rate (TPR) on the y-axis and the False Positive Rate (FPR) on the x-axis. Here’s what they mean:\n\nTrue Positive Rate (Sensitivity or Recall): It measures the proportion of actual positives that are correctly classified. It’s calculated as:\n\n T\n\n P\n\n R\n\n =\n\n T\n\n P\n\n T\n\n P\n\n +\n\n F\n\n N\n\n TPR=\n\n TP+FN\n\n TP\n\n ​\n\nFalse Positive Rate: It measures the proportion of actual negatives that are incorrectly classified as positive. It’s calculated as:\n\n F\n\n P\n\n R\n\n =\n\n F\n\n P\n\n F\n\n P\n\n +\n\n T\n\n N\n\n FPR=\n\n FP+TN\n\n FP\n\n ​\n\nAs the threshold is varied, the TPR and FPR will change, and plotting these changes gives us the ROC curve.\n\nIn an ideal world, a perfect classifier would have an ROC curve that goes straight up the y-axis (TPR = 1) and then to the left along the x-axis (FPR = 0). This would mean that our classifier can identify all positive cases perfectly without any false positives. On the contrary, a purely random classifier (think of flipping a coin to decide) will have an ROC curve that’s a diagonal line from the bottom-left to the top-right. This means it has an equal chance of getting a true positive as it does a false positive.\n\nOne of the most significant metrics associated with the ROC curve is the Area Under the Curve (AUC). The AUC gives a single number summary of the overall performance of the classifier. An AUC of 1.0 indicates a perfect classifier, while an AUC of 0.5 indicates no discriminative power (similar to random guessing).\n\nIn conclusion, ROC curves, coupled with the AUC metric, provide a comprehensive view of your model’s performance across all possible thresholds. By considering the balance between the true positive rate and the false positive rate, we can make more informed decisions about the optimal threshold and, consequently, the trade-offs we’re willing to accept in our classification tasks.]\n\n[Explanation of what KEYWORD does]"
    },
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html",
        "document": "Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n\nNote: this implementation can be used with binary, multiclass and multilabel classification, but some restrictions apply (see Parameters).\n\nRead more in the User Guide.\n\nTrue labels or binary label indicators. The binary and multiclass cases expect labels with shape (n_samples,) while the multilabel case expects binary label indicators with shape (n_samples, n_classes).\n• None In the binary case, it corresponds to an array of shape . Both probability estimates and non-thresholded decision values can be provided. The probability estimates correspond to the probability of the class with the greater label, i.e. and thus . The decision values corresponds to the output of . See more information in the User guide;\n• None In the multiclass case, it corresponds to an array of shape of probability estimates provided by the method. The probability estimates must sum to 1 across the possible classes. In addition, the order of the class scores must correspond to the order of , if provided, or else to the numerical or lexicographical order of the labels in . See more information in the User guide;\n• None In the multilabel case, it corresponds to an array of shape . Probability estimates are provided by the method and the non-thresholded decision values by the method. The probability estimates correspond to the probability of the class with the greater label for each output of the classifier. See more information in the User guide. If , the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Note: multiclass ROC AUC currently only handles the ‘macro’ and ‘weighted’ averages. For multiclass targets, is only implemented for and is only implemented for . Calculate metrics globally by considering each element of the label indicator matrix as a label. Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account. Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). Calculate metrics for each instance, and find their average. Will be ignored when is binary. If not , the standardized partial AUC [2] over the range [0, max_fpr] is returned. For the multiclass case, , should be either equal to or as AUC ROC partial computation currently is not supported for multiclass. Only used for multiclass targets. Determines the type of configuration to use. The default value raises an error, so either or must be passed explicitly. Stands for One-vs-rest. Computes the AUC of each class against the rest [3] [4]. This treats the multiclass case in the same way as the multilabel case. Sensitive to class imbalance even when , because class imbalance affects the composition of each of the ‘rest’ groupings. Stands for One-vs-one. Computes the average AUC of all possible pairwise combinations of classes [5]. Insensitive to class imbalance when . Only used for multiclass targets. List of labels that index the classes in . If , the numerical or lexicographical order of the labels in is used.\n\nThe Gini Coefficient is a summary measure of the ranking ability of binary classifiers. It is expressed using the area under of the ROC as follows:\n\nWhere G is the Gini coefficient and AUC is the ROC-AUC score. This normalisation will ensure that random guessing will yield a score of 0 in expectation, and it is upper bounded by 1."
    },
    {
        "link": "https://plotly.com/python/roc-and-pr-curves",
        "document": "Before diving into the receiver operating characteristic (ROC) curve, we will look at two plots that will give some context to the thresholds mechanism behind the ROC and PR curves.\n\nIn the histogram, we observe that the score spread such that most of the positive labels are binned near 1, and a lot of the negative labels are close to 0. When we set a threshold on the score, all of the bins to its left will be classified as 0's, and everything to the right will be 1's. There are obviously a few outliers, such as negative samples that our model gave a high score, and samples with a low score. If we set a threshold right in the middle, those outliers will respectively become false positives and .\n\nAs we adjust thresholds, the number of positive positives will increase or decrease, and at the same time the number of true positives will also change; this is shown in the second plot. As you can see, the model seems to perform fairly well, because the true positive rate decreases slowly, whereas the false positive rate decreases sharply as we increase the threshold. Those two lines each represent a dimension of the ROC curve."
    },
    {
        "link": "https://scikit-learn.org/stable/api/index.html",
        "document": ""
    },
    {
        "link": "https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc",
        "document": "The previous section presented a set of model metrics, all calculated at a single classification threshold value. But if you want to evaluate a model's quality across all possible thresholds, you need different tools.\n\nThe ROC curve is a visual representation of model performance across all thresholds. The long version of the name, receiver operating characteristic, is a holdover from WWII radar detection.\n\nThe ROC curve is drawn by calculating the true positive rate (TPR) and false positive rate (FPR) at every possible threshold (in practice, at selected intervals), then graphing TPR over FPR. A perfect model, which at some threshold has a TPR of 1.0 and a FPR of 0.0, can be represented by either a point at (0, 1) if all other thresholds are ignored, or by the following:\n\nThe area under the ROC curve (AUC) represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative.\n\nThe perfect model above, containing a square with sides of length 1, has an area under the curve (AUC) of 1.0. This means there is a 100% probability that the model will correctly rank a randomly chosen positive example higher than a randomly chosen negative example. In other words, looking at the spread of data points below, AUC gives the probability that the model will place a randomly chosen square to the right of a randomly chosen circle, independent of where the threshold is set.\n\nIn more concrete terms, a spam classifier with AUC of 1.0 always assigns a random spam email a higher probability of being spam than a random legitimate email. The actual classification of each email depends on the threshold that you choose.\n\nFor a binary classifier, a model that does exactly as well as random guesses or coin flips has a ROC that is a diagonal line from (0,0) to (1,1). The AUC is 0.5, representing a 50% probability of correctly ranking a random positive and negative example.\n\nIn the spam classifier example, a spam classifier with AUC of 0.5 assigns a random spam email a higher probability of being spam than a random legitimate email only half the time.\n\nAUC and ROC for choosing model and threshold\n\nAUC is a useful measure for comparing the performance of two different models, as long as the dataset is roughly balanced. (See Precision-recall curve, above, for imbalanced datasets.) The model with greater area under the curve is generally the better one.\n\nThe points on a ROC curve closest to (0,1) represent a range of the best-performing thresholds for the given model. As discussed in the Thresholds, Confusion matrix and Choice of metric and tradeoffs sections, the threshold you choose depends on which metric is most important to the specific use case. Consider the points A, B, and C in the following diagram, each representing a threshold:\n\nIf false positives (false alarms) are highly costly, it may make sense to choose a threshold that gives a lower FPR, like the one at point A, even if TPR is reduced. Conversely, if false positives are cheap and false negatives (missed true positives) highly costly, the threshold for point C, which maximizes TPR, may be preferable. If the costs are roughly equivalent, point B may offer the best balance between TPR and FPR.\n\nHere is the ROC curve for the data we have seen before:"
    },
    {
        "link": "https://evidentlyai.com/classification-metrics/explain-roc-curve",
        "document": "This chapter covers how to plot the ROC curve, compute the ROC AUC and interpret it. We will also showcase it using the open-source Evidently Python library .\n\nThe ROC AUC score is a popular metric to evaluate the performance of binary classifiers. To compute it, you must measure the area under the ROC curve, which shows the classifier's performance at varying decision thresholds.\n\nWant to keep tabs on your classification models? Automate the quality checks with Evidently Cloud. Powered by the leading open-source Evidently library with 20m+ downloads.\n\nWant to keep tabs on your classification models? Automate the quality checks with Evidently Cloud. Powered by the leading open-source Evidently library with 20m+ downloads.\n\nThe ROC curve stands for the Receiver Operating Characteristic curve. It is a graphical representation of the performance of a binary classifier at different classification thresholds.\n\nThe curve plots the possible True Positive rates (TPR) against the False Positive rates (FPR).\n\nHere is how the curve can look:\n\nEach point on the curve represents a specific decision threshold with a corresponding True Positive rate and False Positive rate.\n\nROC AUC stands for Receiver Operating Characteristic Area Under the Curve.\n\nROC AUC score is a single number that summarizes the classifier's performance across all possible classification thresholds. To get the score, you must measure the area under the ROC curve.\n\nROC AUC score shows how well the classifier distinguishes positive and negative classes. It can take values from 0 to 1.\n\nA higher ROC AUC indicates better performance. A perfect model would have an AUC of 1, while a random model would have an AUC of 0.5.\n\nTo understand the ROC AUC metric, it helps to understand the ROC curve first.\n\nHow does the ROC curve work?\n\nLet’s explain it step by step! We will cover:\n• What TPR and FPR are, and how to calculate them\n\nThe ROC curve plots the True Positive rate (TPR) against the False Positive rate (FPR) at various classification thresholds. You can derive TPR and FPR from a confusion matrix.\n\nA confusion matrix summarizes all correct and false predictions generated for a specific dataset. Here is an example of a matrix generated for a spam prediction use case:\n\nYou can calculate the True Positive and False Positive rates directly from the matrix.\n\nTPR (True Positive rate, also known as recall) shows the share of detected true positives. For example, the share of emails correctly labeled as spam out of all spam emails in the dataset.\n\nTo compute the TPR, you must divide the number of True Positives by the total number of objects of the target class – both identified (True Positives) and missed (False Negatives).\n\nFPR (False Positive rate) shows the share of objects falsely assigned a positive class out of all objects of the negative class. For example, the proportion of legitimate emails falsely labeled as spam.\n\nYou can calculate the FPR by dividing the number of False Positives by the total number of objects of the negative class in the dataset.\n\nYou can think of the FPR as a \"false alarm rate.\"\n\nTo create the ROC curve, you need to plot the FPR values against TPR values at different decision thresholds.\n\nYou might ask, what do \"different\" TPR and FPR values mean? Did we not just calculate them once and for all?\n\nIn fact, we calculated the values for a given confusion matrix at a given decision threshold. But for a probabilistic classification model, these TPR and FPR values are not set in stone.\n\nYou can vary the decision threshold that defines how to convert the model predictions into labels. This, in turn, can change the number of errors the model makes.\n\nA probabilistic classification model returns a number from 0 to 1 for each object. For example, for each email, it predicts how likely this email is spam. For a given email, it can be 0.1, 0.55, 0.99, or any other number.\n\nYou then have to decide at which probability you convert this prediction to a label. For instance, you can label all emails with a predicted probability of over 0.5 as spam. Or, you can only apply this decision when the score is 0.8 or higher.\n\nThis choice is what sets the classification threshold.\n\nAs you change the threshold, you will usually get new combinations of errors of different types (and new confusion matrices)!\n\nWhen you set the threshold higher, you make the model \"more conservative.\" It assigns the True label when it is \"more confident.\" But as a consequence, you typically lower recall: you detect fewer examples of the target class overall.\n\nWhen you set the threshold lower, you make the model \"less strict.\" It assigns the True label more often, even when \"less confident.\" Consequently, you increase recall: you will detect more examples of the target class. However, this may also lead to lower precision, as the model may make more False Positive predictions.\n\nTPR and FPR change in the same direction. The higher the recall (TPR), the higher the rate of false positive errors (FPR). The lower the recall, the fewer false alarms the model gives.\n\nNow, let’s get back to the curve!\n\nThe ROC curve illustrates this trade-off between the TPR and FPR we just explored. Unless your model is near-perfect, you have to balance the two. As you try to increase the TPR (i.e., correctly identify more positive cases), the FPR may also increase (i.e., you get more false alarms).\n\nThe ROC curve is a visual representation of this choice. Each point on the curve corresponds to a combination of TPR and FPR values at a specific decision threshold.\n\nTo create the curve, you should plot the FPR values as the x-axis and the TPR values as the y-axis.\n\nIf we continue with the example above, here is how it can look.\n\nSince our imaginary model does fairly well, most values are \"crowded\" to the left.\n\nThe left side of the curve corresponds to the more \"confident\" thresholds: a higher threshold leads to lower recall and fewer false positive errors. The extreme point is when both recall and FPR are 0. In this case, there are no correct detections but also no false ones.\n\nThe right side of the curve represents the \"less strict\" scenarios when the threshold is low. Both recall and False Positive rates are higher, ultimately reaching 100%. If you put the threshold at 0, the model will always predict a positive class: both recall, and the FPR will be 1.\n\nWhen you increase the threshold, you move left on the curve. If you decrease the threshold, you move to the right.\n\nNow, let’s take a look at the perfect scenario.\n\nIf our model is correct in all the predictions, all the time, it means that the TPR is always 1.0, and FPR is 0. It finds all the cases and never gives false alarms.\n\nHere is how the ROC curve would look.\n\nNow, let’s look at the worst-case scenario.\n\nLet’s say our model is random. In other words, it cannot distinguish between the two classes, and its predictions are no better than chance.\n\nA genuinely random model will predict the positive and negative classes with equal probability.\n\nThe ROC curve, in this case, will look like a diagonal line connecting points (0,0) and (1,1). For a random classifier, the TPR is equal to the FPR because it makes the same number of true and false positive predictions for any threshold value. As the classification threshold changes, the TPR goes up or down in the same proportion as the FPR.\n\nMost real-world models will fall somewhere between the two extremes. The better the model can distinguish between positive and negative classes, the closer the curve is to the top left corner of the graph.\n\nA ROC curve is a two-dimensional reflection of classifier performance across different thresholds. It is convenient to get a single metric to summarize it.\n\nThis is what the ROC AUC score does.\n\nHow to get the ROC AUC score?\n\nA ROC AUC score is a single metric to summarize the performance of a classifier across different thresholds. To compute the score, you must measure the area under the ROC curve.\n\nThere are different methods to calculate the ROC AUC score, but a common one is a trapezoidal rule. This involves approximating the area under the ROC curve by dividing it into trapezoids with vertical lines at the FPR values and horizontal lines at the TPR values. Then, you compute the area by summing the areas of the trapezoids.\n\nYou can compute ROC AUC in Python using sklearn.\n\nIf we return to our extreme \"perfect\" and \"random\" example, computing the ROC AUC score is easy. In the perfect scenario, we measure the square area: ROC AUC is 1. In the random scenario, it is precisely half: ROC AUC is 0.5.\n\nThe ROC AUC score can range from 0 to 1. A score of 0.5 indicates random guessing, and a score of 1 indicates perfect performance.\n\nA score slightly above 0.5 shows that a model has at least \"some\" (albeit small) predictive power. This is generally inadequate for any real applications.\n\nAs a rule of thumb, a ROC AUC score above 0.8 is considered good, while a score above 0.9 is considered great.\n\nHowever, the usefulness of the model depends on the specific problem and use case. There is no standard. You should interpret the ROC AUC score in context, together with other classification quality metrics, such as accuracy, precision, or recall.\n\nThe intuition behind ROC AUC is that it measures how well a binary classifier can distinguish or separate between the positive and negative classes.\n\nIt reflects the probability that the model will correctly rank a randomly chosen positive instance higher than a random negative one.\n\nFor example, this is how the model predictions might look, arranged by the predicted output scores.\n\nROC AUC reflects the likelihood that a random positive (red) instance will be located to the right of a random negative (gray) instance.\n\nIt shows how well a model can produce good relative scores and generally assign higher probabilities to positive instances over negative ones.\n\nIn the above picture, the classifier is not perfect but \"directionally correct.\" It ranks most negative instances lower than positive ones.\n\nThe ideal situation is to have all positive instances ranked higher than all negative instances, resulting in an AUC of 1.0.\n\nIt’s worth noting that even a perfect ROC AUC does not mean the predictions are well-calibrated. A well-calibrated classifier produces predicted probabilities that reflect the actual probabilities of the events. Say, if it predicts that an event has a 70% chance of occurring, it should be correct about 70% of the time. ROC AUC is not a calibration measure.\n\nROC AUC score, instead, shows how well a model can produce relative scores that help discriminate between positive or negative instances.\n\nLet’s sum up the important properties of the metric.\n\nHere are some advantages of the ROC AUC score.\n• A single number. ROC AUC reflects the model quality in one number. It is convenient to use a single metric, especially when comparing multiple models.\n• Does not change with the classification threshold. Unlike precision and recall, ROC AUC stays the same. In fact, it sums up the performance across the different classification thresholds. It is a valuable \"overall\" quality measure, whereas precision and recall provide a quality \"snapshot\" at a given decision threshold.\n• It is a suitable evaluation metric for imbalanced data. ROC AUC measures the model's ability to discriminate between the positive and negative classes, regardless of their relative frequencies in the dataset.\n• More tolerant to the drift in class balance. The ROC AUC generally remains more stable if the distribution of classes changes. This often happens in production use, for example, when fraud rates vary month-by-month. If they change significantly, the earlier chosen decision threshold might become inadequate. For example, if fraud becomes more prevalent, the recall of the fraud detection model might drop, as this metric uses the absolute number of actual fraud cases in the denominator. However, ROC AUC might remain stable, indicating that the model can still differentiate between the two classes despite the changes in their relative frequencies.\n• Scale-invariant. ROC AUC measures how well predictions are ranked rather than their absolute values. This way, it helps compare the quality of models that might output \"different ranges\" of predicted probabilities. It is typically relevant when you experiment with different models during the model training stage.\n\nThe metric also has a few downsides. As usual, a lot depends on the context!\n• ROC AUC is not intuitive. This metric can be hard to explain to business stakeholders and does not have an immediately interpretable meaning.\n• It does not consider the cost of errors. ROC AUC does not account for different types of errors and their consequences. In many scenarios, false negatives can be more costly than false positives, or vice versa. In this case, working to balance precision and recall and setting the appropriate classification threshold to minimize a certain type of error is often a more suitable approach. ROC AUC is not useful for this type of optimization.\n• It can be misleading if the class imbalance is severe. When the positive class is very small, ROC AUC can give a false impression of high quality. Imagine that a classifier predicts almost all instances as negative. TPR and FPR will be close to 0 because there are few positive predictions. As a result, the ROC curve will appear to \"hug\" the top left corner of the plot, giving the impression that the classifier is performing well and definitely better than random. However, though it correctly classifies most of the negative instances, it may miss most of the positives, which is likely more important for the model performance. In this case, it may be more appropriate to look at the precision-recall curve and rely on metrics like precision, recall, or F1-score to evaluate ML model quality.\n\nWhen to use ROC AUC\n\nConsidering all the above, ROC AUC is useful, but as usual, not a perfect metric.\n• During model training, it helps compare multiple ML models against each other.\n• ROC AUC is particularly useful when the goal is to rank predictions in order of their confidence level rather than produce well-calibrated probability estimates.\n• Both in training and production evaluation, ROC AUC helps provide a more complete picture of the model performance, giving a single metric that sums up the quality across different thresholds.\n\nHowever, there are limitations:\n• ROC AUC is less useful when you care about different costs of error and want to find the optimal threshold to optimize for the cost of a specific error.\n• It can be misleading when the data is heavily imbalanced (which is coincidentally often the cases where you ultimately care about different costs of errors).\n\nYou can use ROC AUC during production model monitoring as long as you have the true labels to compute it.\n\nHowever, a high ROC AUC score does not communicate all relevant aspects of the model quality. The score evaluates the degree of separability and does not consider the asymmetric costs of false positives and negatives. It captures, in one number, the quality of the model across all possible thresholds.\n\nIn many real-world scenarios, this overall performance is not relevant: you need to consider the costs of error and define a specific threshold to make automated decisions. Therefore, the ROC AUC score should be used with other metrics, such as precision and recall. You might also want to monitor precision and recall for specific important segments in your data (such as users in specific locations, premium users, etc.) to capture differences in performance.\n\nHowever, having ROC AUC as an additional metric might still be informative. For example, in cases where the shifting balance of classes might negatively impact recall, tracking ROC AUC might communicate whether the model itself remains reasonable.\n\nTo quickly calculate and visualize the ROC curve and ROC AUC score, as well as other metrics and plots to evaluate the quality of a classification model, you can use Evidently, an open-source Python library to evaluate, test and monitor ML models in production.\n\nYou will need to prepare your dataset that includes predicted values for each class and true labels and pass it to the tool. You will instantly get an interactive report that includes ROC AUC, accuracy, precision, recall, F1-score metrics as well as other visualizations. You can also integrate these model quality checks into your production pipelines."
    },
    {
        "link": "https://geeksforgeeks.org/auc-roc-curve",
        "document": "In machine learning model evaluation is crucial to ensure that the model performs well. Common evaluation metrics for classification tasks include accuracy, precision, recall, F1-score and AUC-ROC. In this article we’ll focus on the AUC-ROC curve a popular metric used to evaluate classification models.\n\nHow AUC-ROC curve is used?\n\nThe AUC-ROC curve is an essential tool used for evaluating the performance of binary classification models. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at different thresholds showing how well a model can distinguish between two classes such as positive and negative outcomes.\n\nIt provides a graphical representation of the model’s ability to distinguish between two classes like positive class for presence of a disease and negative class for absence of a disease.\n• Specificity : The proportion of actual negatives correctly identified by the model (inverse of FPR).\n• Sensitivity/Recall : The proportion of actual positives correctly identified by the model (same as TPR).\n\nThese terms are derived from the confusion matrix which provides the following values:\n• ROC Curve plots TPR vs. FPR at different thresholds. It represents the trade-off between the sensitivity and specificity of a classifier.\n• AUC (Area Under the Curve) measures the area under the ROC curve. A higher AUC value indicates better model performance as it suggests a greater ability to distinguish between classes. An AUC value of 1.0 indicates perfect performance while 0.5 suggests it is random guessing.\n\nAUC-ROC curve helps us understand how well a classification model distinguishes between the two classes (positive and negative).\n\nImagine we have 6 data points and out of these:\n• 3 belong to the positive class: Class 1 for people who have a disease.\n• 3 belong to the negative class: Class 0 for people who don’t have disease.\n\nNow the model will give each data point a predicted probability of belonging to Class 1 (the positive class). The AUC measures the model’s ability to assign higher predicted probabilities to the positive class than to the negative class. Here’s how it work:\n• Randomly choose a pair : Pick one data point from the positive class (Class 1) and one from the negative class (Class 0).\n• Check if the positive point has a higher predicted probability : If the model assigns a higher probability to the positive data point than to the negative one for correct ranking.\n• Repeat for all pairs : We do this for all possible pairs of positive and negative examples.\n\nWhen to Use AUC-ROC\n• None The dataset is balanced and the model needs to be evaluated across all thresholds.\n• None False positives and false negatives are of similar importance.\n• High AUC (close to 1) : The model effectively distinguishes between positive and negative instances.\n• Low AUC (close to 0) : The model struggles to differentiate between the two classes.\n• AUC around 0.5 : The model doesn’t learn any meaningful patterns i.e it is doing random guessing.\n\nIn short, the AUC gives you an overall idea of how well your model is doing at sorting positives and negatives, without being affected by the threshold you set for classification. A higher AUC means your model is doing good.\n\nImplementation using two different models\n\nIn order to train the Random Forest and Logistic Regression models and to present their ROC curves with AUC scores, the algorithm creates artificial binary classification data.\n\nUsing an 80-20 split ratio, the algorithm creates artificial binary classification data with 20 features, divides it into training and testing sets, and assigns a random seed to ensure reproducibility.\n\nUsing a fixed random seed to ensure repeatability, the method initializes and trains a logistic regression model on the training set. In a similar manner, it uses the training data and the same random seed to initialize and train a Random Forest model with 100 trees.\n\nUsing the test data and a trained Logistic Regression model, the code predicts the positive class’s probability. In a similar manner, using the test data, it uses the trained Random Forest model to produce projected probabilities for the positive class.\n\nUsing the test data, the code creates a DataFrame called test_df with columns labeled “True,” “Logistic,” and “RandomForest,” adding true labels and predicted probabilities from the Random Forest and Logistic Regression models.\n\nPlot the ROC Curve for the models\n\nThe code generates a plot with 8 by 6 inch figures. It computes the AUC and ROC curve for each model (Random Forest and Logistic Regression), then plots the ROC curve. The ROC curve for random guessing is also represented by a red dashed line, and labels, a title, and a legend are set for visualization.\n\nFor a multi-class setting, we can simply use one vs all methodology and you will have one ROC curve for each class. Let’s say you have four classes A, B, C and D then there would be ROC curves and corresponding AUC values for all the four classes, i.e. once A would be one class and B, C, and D combined would be the others class, similarly, B is one class and A, C, and D combined as others class, etc.\n\nThe general steps for using AUC-ROC in the context of a multiclass classification model are:\n• None For each class in your multiclass problem, treat it as the positive class while combining all other classes into the negative class.\n• None Train the binary classifier for each class against the rest of the classes.\n• None Here we plot the ROC curve for the given class against the rest.\n• None Plot the ROC curves for each class on the same graph. Each curve represents the discrimination performance of the model for a specific class.\n• None Examine the AUC scores for each class. A higher AUC score indicates better discrimination for that particular class.\n\nThe program creates artificial multiclass data, divides it into training and testing sets, and then uses the One-vs-Restclassifier technique to train classifiers for both Random Forest and Logistic Regression. Lastly, it plots the two models’ multiclass ROC curves to demonstrate how well they discriminate between various classes.\n\nThree classes and twenty features make up the synthetic multiclass data produced by the code. After label binarization, the data is divided into training and testing sets in an 80-20 ratio.\n\nThe program trains two multiclass models: a Random Forest model with 100 estimators and a Logistic Regression model with the One-vs-Rest approach. With the training set of data, both models are fitted.\n\nThe Random Forest and Logistic Regression models’ ROC curves and AUC scores are calculated by the code for each class. The multiclass ROC curves are then plotted showing the discrimination performance of each class and featuring a line that represents random guessing. The resulting plot offers a graphic evaluation of the models’ classification performance.\n\nWhat is the AUC-ROC curve?\n\nWhat does a perfect AUC-ROC curve look like?\n\nWhat does an AUC value of 0.5 signify?\n\nCan AUC-ROC be used for multiclass classification?\n\nHow is the AUC-ROC curve useful in model evaluation?"
    },
    {
        "link": "https://numberanalytics.com/blog/roc-curve-explained-key-metrics-evaluating-classifiers",
        "document": "In the continually evolving world of machine learning and data science, understanding the performance of classification models is paramount. One of the widely used tools that help in this evaluation is the Receiver Operating Characteristic (ROC) curve. This article demystifies the ROC curve by exploring its key metrics, historical background, and practical applications. Whether you’re a data scientist, a machine learning engineer, or simply curious about classifier evaluation, this article provides a comprehensive understanding that can help you improve your decision-making and model performance.\n\nThe ROC curve is much more than just a graphical plot; it represents years of evolution in diagnostic and classification analysis. Let’s dive into the history and terminology surrounding this classic tool.\n\nThe concept of the ROC curve emerged in the 1940s and 1950s in the field of signal detection theory, initially developed for radar operators during World War II. Over time, its application moved from military technology to psychology and ultimately to medical diagnostics and machine learning. The early adoption of ROC analysis in medical imaging and diagnostics underlines the enduring value of the method: to distinguish between a signal (true condition) and noise (errors) under circumstances of uncertainty.\n\nWith modern classification challenges in machine learning, the ROC curve has stayed relevant. Researchers and practitioners alike have valued its ability to visually represent the trade-offs between true positive rates (TPR) and false positive rates (FPR) across different decision thresholds.\n\nIn many real-world applications, decisions are seldom binary in terms of correctness. Classifiers output probabilities that need to be thresholded. As the threshold changes, so do the number of false negatives and false positives. The ROC curve provides a clear picture of how well a classifier distinguishes between the classes over all threshold levels.\n\nFor instance, consider two classifiers tasked with diagnosing a disease. If one model offers a high TPR but also a high FPR, it may lead to many unnecessary follow-ups and undue anxiety, even if it catches most true cases. The ROC curve allows practitioners to select an optimal threshold or compare multiple models to find the right balance, making it an essential part of the model evaluation toolkit.\n\nBefore diving into the metrics, it’s important to define some of the basic terminologies in classifier evaluation:\n• True Positive (TP): An instance where the condition is correctly predicted by the classifier.\n• False Positive (FP): An instance where the condition is incorrectly predicted (a type I error).\n• True Negative (TN): An instance where a negative condition is correctly predicted.\n• False Negative (FN): An instance where a positive condition is missed (a type II error).\n\nMathematically, the True Positive Rate (TPR), also known as sensitivity or recall, is defined as:\n\nThe False Positive Rate (FPR) is given by:\n\nThese rates are computed for every possible threshold decision, forming the ROC curve when plotted.\n\nTo truly appreciate the ROC curve, we must explore its fundamental metrics, particularly the true positive rate, the false positive rate, and the area under the curve.\n\nThe core of ROC curve analysis lies in understanding how a classifier performs by looking at TPR and FPR with respect to the threshold adjustments.\n• True Positive Rate (TPR) or Sensitivity: As mentioned, this measures the proportion of correctly identified positives out of all actual positives. A high TPR indicates that the classifier is catching most of the positive instances.\n• False Positive Rate (FPR): This quantifies the proportion of negatives that were incorrectly classified as positive. A low FPR is desirable because it means the classifier is not raising too many false alarms.\n\nAs the decision threshold is varied, we observe a trade-off between TPR and FPR. A more lenient threshold could result in a higher TPR but might also increase the FPR. Conversely, a stricter threshold might reduce the FPR at the cost of lowering the TPR.\n\nOne of the most significant metrics derived from the ROC curve is the Area Under the Curve (AUC). The AUC provides a single scalar value that summarizes the overall performance of a classifier, independent of a specific threshold. An AUC of 1 represents perfect classification, while an AUC of 0.5 suggests that the classifier performs no better than random guessing.\n\nIn mathematical terms, if we denote the ROC curve by an integral, the AUC is expressed as:\n\nThis integral computes the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example. Therefore, the larger the AUC, the better the classifier’s overall performance.\n\nWhile TPR (sensitivity) is already a central metric, another important measure is specificity, which indicates how well a classifier identifies negative instances. Specificity is calculated as:\n\nThis metric complements sensitivity. For any given threshold, a balance between sensitivity and specificity is crucial. In certain applications such as medical diagnostics, high sensitivity might be prioritized to ensure that as few true cases as possible are missed, even at the expense of lower specificity.\n\nA useful way to think about these relationships is to consider the confusion matrix, which summarizes the true positives, false positives, true negatives, and false negatives. Across the different thresholds, the ROC curve represents the dynamic balance between sensitivity and (1 – specificity), since:\n\nUnderstanding the theory behind ROC curves and key metrics is only half the battle. The power of the ROC analysis lies in its real-world applications where it serves as a tool for comparing classifiers and optimizing performance.\n\nMany modern statistical and machine learning software packages come with built-in functions for plotting ROC curves and calculating AUCs. Popular libraries and tools include:\n• Python with Scikit-learn: Scikit-learn’s and functions are extensively used in the Python community. Users can compute true and false positive rates and plot ROC curves with a few lines of code.\n• R with the ROCR Package: For those using R, the ROCR package provides robust tools for ROC analysis. Functions within this package allow for dynamic and interactive plotting, making it easier to analyze model performance.\n• MATLAB: MATLAB’s Statistics and Machine Learning Toolbox includes functions that enable the computation and visualization of ROC curves for both binary and multiclass classification problems.\n\nWhether you are working with Python, R, or MATLAB, the concepts remain consistent. The implementation might vary slightly, but the underlying principles of threshold selection, calculation of TPR and FPR, and deriving the AUC are universal across these tools.\n\nOne of the ROC curve’s most practical applications is for comparing different classifiers. When presented with two or more models, their ROC curves can be overlaid on the same plot. The model that consistently shows a ROC curve closer to the top-left corner (indicating higher TPR and lower FPR) is considered to have superior performance.\n\nFor example, if we have two classifiers, A and B, with respective AUC values of AUC_A and AUC_B, then:\n• If AUC_A > AUC_B, classifier A generally has better discriminative capability than classifier B.\n• If the ROC curves of the two classifiers cross, it might indicate that one model performs better in a region of high sensitivity while the other might excel in a region of high specificity. This insight is critical when the cost of false positives versus false negatives differs significantly.\n\nReal-world case studies further emphasize the utility of ROC curves. Consider a healthcare application where the objective is to predict a particular disease. By plotting the ROC curve, medical practitioners can identify the threshold that maximizes patient diagnosis while minimizing unnecessary treatments. For instance:\n• Cancer Detection: In radiology, ROC analysis aids radiologists in distinguishing between benign and malignant tumors. Studies have indicated that models with higher AUC values correlate with more reliable diagnostic tools.\n• Credit Card Fraud Detection: In finance, ROC curves help in fine-tuning fraud detection systems. Even a slight improvement in TPR at a maintained FPR can translate into significant financial savings and improved customer trust.\n• Spam Filtering: For email service providers, the balance between catching spam (high TPR) and not misclassifying important messages (low FPR) is critical. ROC analysis assists in identifying this balance, leading to more effective spam filters.\n\nBy bridging theoretical concepts with practical implementations, ROC analysis proves invaluable across various industries, ensuring that classifier evaluations are both rigorous and insightful.\n\nWhile ROC analysis offers substantial benefits, leveraging it effectively comes with its own set of challenges. Here, we provide practical tips to maximize the benefits of ROC analysis and discuss emerging trends that may influence future evaluation methods.\n• Threshold Optimization: Utilize the ROC curve to determine the optimal threshold for your specific application. Instead of relying on default thresholds (often 0.5), analyze the ROC curve to find a point that balances TPR and FPR in accordance with the cost of errors in your context.\n• Regular Re-Evaluation: As models get updated or retrained with new data, continuously re-assessing the ROC metrics ensures that the classifier remains robust over time.\n• Combining Multiple Metrics: While ROC is powerful, integrating insights from other evaluation metrics like Precision-Recall curves, F1 scores, and confusion matrices provides a more comprehensive picture. This multi-metric approach is especially critical in imbalanced datasets where true negatives are abundant or the positive class is rare.\n• Visual Exploration: Make use of visualization tools to overlay multiple ROC curves. Visual comparisons can reveal nuances that might be missed with mere numerical comparisons. Experiment with different plotting libraries to explore interactive and dynamic visualizations that facilitate deeper insights.\n\nDespite its benefits, ROC analysis comes with challenges:\n• Imbalanced Datasets: ROC curves can sometimes be misleading when dealing with heavily imbalanced classes. In such cases, the Precision-Recall curve might be a more appropriate metric for comparison.\n• Interpretation of AUC: While a higher AUC indicates better overall performance, it does not account for the precise region of the ROC curve that is most important for a particular application. When specific regions (like low FPR zones) are critical, more detailed analysis is required.\n• Threshold-Dependency: The insights gained from ROC analysis depend on the range of thresholds considered. It is essential to ensure that the thresholds span the full spectrum of predicted probabilities to avoid bias.\n\nA practical solution is to combine ROC analysis with other metrics and domain-specific knowledge. For instance, in a healthcare setting, understanding the relative importance of sensitivity versus specificity can guide the selection of an optimal threshold.\n\nThe field of model evaluation is continuously evolving, influenced by developments in both data complexity and analytical techniques. Some emerging trends include:\n• Automated Threshold Selection: Machine learning pipelines are increasingly integrating automated threshold selection methods. These methods dynamically adjust thresholds based on performance metrics and the cost associated with misclassifications, often using reinforcement learning techniques.\n• Hybrid Evaluation Metrics: Researchers are exploring hybrid metrics that combine the strengths of ROC curves with those of precision-recall curves. Such metrics may provide a more holistic view of classifier performance, especially in cases with significant class imbalance.\n• Interpretability and Explainability: With an increasing focus on explainable AI, visual tools that not only depict ROC curves but also explain the contribution of different features to misclassifications are gaining traction. These tools help stakeholders understand why a model might falter in certain scenarios, thereby guiding improvements.\n• Custom Evaluation Frameworks: Sectors such as finance and healthcare are moving toward customized evaluation frameworks that extend beyond conventional ROC analysis. These frameworks are tailored to the unique requirements of their domain, ensuring that the classifier performance aligns closely with business or clinical outcomes.\n\nThe future of classifier evaluation is likely to see a blend of traditional methods like ROC analysis with innovative techniques designed to handle the complexities of modern datasets. As these trends evolve, staying updated with the latest research and integrating new methods into existing workflows will be crucial for practitioners.\n\nThe Receiver Operating Characteristic curve is an indispensable tool in the realm of classifier evaluation. By offering insights into the trade-off between true positive rates and false positive rates, it allows for a nuanced understanding of model performance that goes beyond simple accuracy measures.\n\nThroughout this article, we explored the history and evolution of ROC analysis, delved into the mathematical underpinnings of key metrics such as TPR, FPR, and AUC, and discussed its practical applications through case studies in diverse fields. We also highlighted practical tips for improving classifier performance using ROC insights and outlined common challenges with potential solutions.\n\nUltimately, the true power of ROC analysis lies in its simplicity and interpretability. Whether you are evaluating new models, fine-tuning existing ones, or comparing different classifiers, understanding and utilizing ROC curves effectively can dramatically improve your decision-making process. By combining ROC analysis with emerging trends and other evaluation metrics, the path to robust and reliable classifier performance becomes clearer.\n\nEmbracing ROC analysis not only enhances your model evaluation skills but also positions you to navigate the complexities of modern machine learning applications. With continual advancements in both analytics and computational tools, the ROC curve remains a fundamental instrument in the journey toward building smarter, more accurate classifiers.\n\nWe hope this article has provided you with deeper insight into the intricacies of ROC curves and empowered you with the knowledge to apply these concepts in your own machine learning projects. As the landscape of data science evolves, so too will the techniques for evaluating classifiers. Keeping a firm grasp on these foundational concepts will ensure that your models are not only performing well statistically, but also resonating with real-world needs and applications.\n\nHappy modeling, and may your ROC curves always lead you to that optimal balance between sensitivity and specificity!\n• SPSS and SAS: For professionals and academics, IBM SPSS and SAS provide powerful statistical analysis tools that simplify complex data processing with advanced diagnostic capabilities. Both software packages feature a traditional menu-driven user interface (UI/UX), making them accessible for users who prefer a point-and-click approach over coding-based workflows.\n• Number Analytics: Number Analytics is an AI-powered statistical software that automates statistical model selection, result interpretation, and report documentation. Designed for business professionals with limited statistical background, it simplifies complex analyses with an intuitive, user-friendly approach. Try Number Analytics. (Number Analytics)"
    },
    {
        "link": "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python",
        "document": "It can be more flexible to predict probabilities of an observation belonging to each class in a classification problem rather than predicting classes directly.\n\nThis flexibility comes from the way that probabilities may be interpreted using different thresholds that allow the operator of the model to trade-off concerns in the errors made by the model, such as the number of false positives compared to the number of false negatives. This is required when using models where the cost of one error outweighs the cost of other types of errors.\n\nTwo diagnostic tools that help in the interpretation of probabilistic forecast for binary (two-class) classification predictive modeling problems are ROC Curves and Precision-Recall curves.\n\nIn this tutorial, you will discover ROC Curves, Precision-Recall Curves, and when to use each to interpret the prediction of probabilities for binary classification problems.\n\nAfter completing this tutorial, you will know:\n• ROC Curves summarize the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds.\n• Precision-Recall curves summarize the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds.\n• ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\n\nKick-start your project with my new book Probability for Machine Learning, including step-by-step tutorials and the Python source code files for all examples.\n• Update Aug/2018: Fixed bug in the representation of the no skill line for the precision-recall plot. Also fixed typo where I referred to ROC as relative rather than receiver (thanks spellcheck).\n• Update Nov/2018: Fixed description on interpreting size of values on each axis, thanks Karl Humphries.\n• Update Oct/2019: Updated ROC Curve and Precision Recall Curve plots to add labels, use a logistic regression model and actually compute the performance of the no skill classifier.\n• Update Nov/2019: Improved description of no skill classifier for precision-recall curve.\n• Update Oct/2023: Minor update on code to make it more Pythonic\n\nThis tutorial is divided into 6 parts; they are:\n• When to Use ROC vs. Precision-Recall Curves?\n\nIn a classification problem, we may decide to predict the class values directly.\n\nAlternately, it can be more flexible to predict the probabilities for each class instead. The reason for this is to provide the capability to choose and even calibrate the threshold for how to interpret the predicted probabilities.\n\nFor example, a default might be to use a threshold of 0.5, meaning that a probability in [0.0, 0.49] is a negative outcome (0) and a probability in [0.5, 1.0] is a positive outcome (1).\n\nThis threshold can be adjusted to tune the behavior of the model for a specific problem. An example would be to reduce more of one or another type of error.\n\nWhen making a prediction for a binary or two-class classification problem, there are two types of errors that we could make.\n• False Positive. Predict an event when there was no event.\n• False Negative. Predict no event when in fact there was an event.\n\nBy predicting probabilities and calibrating a threshold, a balance of these two concerns can be chosen by the operator of the model.\n\nFor example, in a smog prediction system, we may be far more concerned with having low false negatives than low false positives. A false negative would mean not warning about a smog day when in fact it is a high smog day, leading to health issues in the public that are unable to take precautions. A false positive means the public would take precautionary measures when they didn’t need to.\n\nA common way to compare models that predict probabilities for two-class problems is to use a ROC curve.\n\nA useful tool when predicting the probability of a binary outcome is the Receiver Operating Characteristic curve, or ROC curve.\n\nIt is a plot of the false positive rate (x-axis) versus the true positive rate (y-axis) for a number of different candidate threshold values between 0.0 and 1.0. Put another way, it plots the false alarm rate versus the hit rate.\n\nThe true positive rate is calculated as the number of true positives divided by the sum of the number of true positives and the number of false negatives. It describes how good the model is at predicting the positive class when the actual outcome is positive.\n\nThe true positive rate is also referred to as sensitivity.\n\nThe false positive rate is calculated as the number of false positives divided by the sum of the number of false positives and the number of true negatives.\n\nIt is also called the false alarm rate as it summarizes how often a positive class is predicted when the actual outcome is negative.\n\nThe false positive rate is also referred to as the inverted specificity where specificity is the total number of true negatives divided by the sum of the number of true negatives and false positives.\n\nThe ROC curve is a useful tool for a few reasons:\n• The curves of different models can be compared directly in general or for different thresholds.\n• The area under the curve (AUC) can be used as a summary of the model skill.\n\nThe shape of the curve contains a lot of information, including what we might care about most for a problem, the expected false positive rate, and the false negative rate.\n• Smaller values on the x-axis of the plot indicate lower false positives and higher true negatives.\n• Larger values on the y-axis of the plot indicate higher true positives and lower false negatives.\n\nIf you are confused, remember, when we predict a binary outcome, it is either a correct prediction (true positive) or not (false positive). There is a tension between these options, the same with true negative and false negative.\n\nA skilful model will assign a higher probability to a randomly chosen real positive occurrence than a negative occurrence on average. This is what we mean when we say that the model has skill. Generally, skilful models are represented by curves that bow up to the top left of the plot.\n\nA no-skill classifier is one that cannot discriminate between the classes and would predict a random class or a constant class in all cases. A model with no skill is represented at the point (0.5, 0.5). A model with no skill at each threshold is represented by a diagonal line from the bottom left of the plot to the top right and has an AUC of 0.5.\n\nA model with perfect skill is represented at a point (0,1). A model with perfect skill is represented by a line that travels from the bottom left of the plot to the top left and then across the top to the top right.\n\nAn operator may plot the ROC curve for the final model and choose a threshold that gives a desirable balance between the false positives and false negatives.\n\nWe can plot a ROC curve for a model in Python using the roc_curve() scikit-learn function.\n\nThe function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. The function returns the false positive rates for each threshold, true positive rates for each threshold and thresholds.\n\nThe AUC for the ROC can be calculated using the roc_auc_score() function.\n\nLike the roc_curve() function, the AUC function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. It returns the AUC score between 0.0 and 1.0 for no skill and perfect skill respectively.\n\nA complete example of calculating the ROC curve and ROC AUC for a Logistic Regression model on a small test problem is listed below.\n\nRunning the example prints the ROC AUC for the logistic regression model and the no skill classifier that only predicts 0 for all examples.\n\nA plot of the ROC curve for the model is also created showing that the model has skill.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nThere are many ways to evaluate the skill of a prediction model.\n\nAn approach in the related field of information retrieval (finding documents based on queries) measures precision and recall.\n\nThese measures are also useful in applied machine learning for evaluating binary classification models.\n\nPrecision is a ratio of the number of true positives divided by the sum of the true positives and false positives. It describes how good a model is at predicting the positive class. Precision is referred to as the positive predictive value.\n\nRecall is calculated as the ratio of the number of true positives divided by the sum of the true positives and the false negatives. Recall is the same as sensitivity.\n\nReviewing both precision and recall is useful in cases where there is an imbalance in the observations between the two classes. Specifically, there are many examples of no event (class 0) and only a few examples of an event (class 1).\n\nThe reason for this is that typically the large number of class 0 examples means we are less interested in the skill of the model at predicting class 0 correctly, e.g. high true negatives.\n\nKey to the calculation of precision and recall is that the calculations do not make use of the true negatives. It is only concerned with the correct prediction of the minority class, class 1.\n\nA precision-recall curve is a plot of the precision (y-axis) and the recall (x-axis) for different thresholds, much like the ROC curve.\n\nA no-skill classifier is one that cannot discriminate between the classes and would predict a random class or a constant class in all cases. The no-skill line changes based on the distribution of the positive to negative classes. It is a horizontal line with the value of the ratio of positive cases in the dataset. For a balanced dataset, this is 0.5.\n\n— The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets, 2015.\n\nA model with perfect skill is depicted as a point at (1,1). A skilful model is represented by a curve that bows towards (1,1) above the flat line of no skill.\n\nThere are also composite scores that attempt to summarize the precision and recall; two examples include:\n• F-Measure or F1 score: that calculates the harmonic mean of the precision and recall (harmonic mean because the precision and recall are rates).\n• Area Under Curve: like the AUC, summarizes the integral or an approximation of the area under the precision-recall curve.\n\nIn terms of model selection, F-Measure summarizes model skill for a specific probability threshold (e.g. 0.5), whereas the area under curve summarize the skill of a model across thresholds, like ROC AUC.\n\nThis makes precision-recall and a plot of precision vs. recall and summary measures useful tools for binary classification problems that have an imbalance in the observations for each class.\n\nPrecision and recall can be calculated in scikit-learn.\n\nThe precision and recall can be calculated for thresholds using the precision_recall_curve() function that takes the true output values and the probabilities for the positive class as input and returns the precision, recall and threshold values.\n\nThe F-Measure can be calculated by calling the f1_score() function that takes the true class values and the predicted class values as arguments.\n\nThe area under the precision-recall curve can be approximated by calling the auc() function and passing it the recall (x) and precision (y) values calculated for each threshold.\n\nWhen plotting precision and recall for each threshold as a curve, it is important that recall is provided as the x-axis and precision is provided as the y-axis.\n\nThe complete example of calculating precision-recall curves for a Logistic Regression model is listed below.\n\nRunning the example first prints the F1, area under curve (AUC) for the logistic regression model.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nWhen to Use ROC vs. Precision-Recall Curves?\n\nGenerally, the use of ROC curves and precision-recall curves are as follows:\n• ROC curves should be used when there are roughly equal numbers of observations for each class.\n• Precision-Recall curves should be used when there is a moderate to large class imbalance.\n\nThe reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.\n\n— The Relationship Between Precision-Recall and ROC Curves, 2006.\n\nSome go further and suggest that using a ROC curve with an imbalanced dataset might be deceptive and lead to incorrect interpretations of the model skill.\n\n— The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets, 2015.\n\nThe main reason for this optimistic picture is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\n\nWe can make this concrete with a short example.\n\nBelow is the same ROC Curve example with a modified problem where there is a ratio of about 100:1 ratio of class=0 to class=1 observations (specifically Class0=985, Class1=15).\n\nRunning the example suggests that the model has skill.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nIf you review the predictions, you will see that the model predicts the majority class (class 0) in all cases on the test set. The score is very misleading.\n\nA plot of the ROC Curve confirms the AUC interpretation of a skilful model for most probability thresholds.\n\nWe can also repeat the test of the same model on the same dataset and calculate a precision-recall curve and statistics instead.\n\nThe complete example is listed below.\n\nRunning the example first prints the F1 and AUC scores.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nWe can see that the model is penalized for predicting the majority class in all cases. The scores show that the model that looked good according to the ROC Curve is in fact barely skillful when considered using using precision and recall that focus on the positive class.\n\nThe plot of the precision-recall curve highlights that the model is just barely above the no skill line for most thresholds.\n\nThis is possible because the model predicts probabilities and is uncertain about some cases. These get exposed through the different thresholds evaluated in the construction of the curve, flipping some class 0 to class 1, offering some precision but very low recall.\n\nThis section provides more resources on the topic if you are looking to go deeper.\n• A critical investigation of recall and precision as measures of retrieval system performance, 1989.\n• The Relationship Between Precision-Recall and ROC Curves, 2006.\n• The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets, 2015.\n\nIn this tutorial, you discovered ROC Curves, Precision-Recall Curves, and when to use each to interpret the prediction of probabilities for binary classification problems.\n• ROC Curves summarize the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds.\n• Precision-Recall curves summarize the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds.\n• ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\n\nDo you have any questions?\n\n Ask your questions in the comments below and I will do my best to answer."
    }
]