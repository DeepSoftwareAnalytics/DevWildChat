[
    {
        "link": "https://stackoverflow.com/questions/53118858/bit-manipulations-good-practices",
        "document": "You could use bit-fields, despite what all the fear-mongers here have been saying. You would just need to know how the compiler(s) and system ABI(s) you intend your code to work with define the \"implementation defined\" aspects of bit-fields. Don't be scared off by pedants putting words like \"implementation defined\" in bold.\n\nHowever what others so far seem to have missed out on are the various aspects of how memory-mapped hardware devices might behave that can be counter-intuitive when dealing with a higher-level language like C and the optimization features such languages offer. For example every read or write of a hardware register may have side-effects sometimes even if bits are not changed on the write. Meanwhile the optimizer may make it difficult to tell when the generated code is actually reading or writing to the address of the register, and even when the C object describing the register is carefully qualified as , great care is required to control when I/O occurs.\n\nPerhaps you will need to use some specific technique defined by your compiler and system in order to properly manipulate memory-mapped hardware devices. This is the case for many embedded systems. In some cases compiler and system vendors will indeed use bit-fields, just as Linux does in some cases. I would suggest reading your compiler manual first.\n\nThe bit description table you quote appears to be for the control register of the the Intel Avalon DMA controller core. The \"read/write/clear\" column gives a hint as to how a particular bit behaves when it is read or written. The status register for that device has an example of a bit where writing a zero will clear a bit value, but it may not read back the same value as was written -- i.e. writing the register may have a side-effect in the device, depending on the value of the DONE bit. Interestingly they document the SOFTWARERESET bit as \"RW\", but then describe the procedure as writing a 1 to it twice to trigger the reset, and then they also warn Executing a DMA software reset when a DMA transfer is active may result in permanent bus lockup (until the next system reset). The SOFTWARERESET bit should therefore not be written except as a last resort. Managing a reset in C would take some careful coding no matter how you describe the register.\n\nAs for standards, well ISO/IEC have produced a \"technical report\" known as \"ISO/IEC TR 18037\", with the subtitle \"Extensions to support embedded processors\". It discusses a number of the issues related to using C to manage hardware addressing and device I/O, and specifically for the kinds of bit-mapped registers you mention in your question it documents a number of macros and techniques available through an include file they call . If your compiler provides such a header file, then you might be able to use these macros.\n\nThere are draft copies of TR 18037 available, the latest being TR 18037(2007), though it provides for rather dry reading. However it does contain an example implementation of .\n\nPerhaps a good example of a real-world implementation is in QNX. The QNX documentation offers a decent overview (and an example, though I would strongly suggest using s for integer values, never macros): QNX"
    },
    {
        "link": "https://embedded.com/bitwise-operations-on-device-registers",
        "document": ""
    },
    {
        "link": "https://blog.mbedded.ninja/programming/languages/c/bit-fields-and-bit-manipulation-masking",
        "document": "A bit field is a software/firmware variable that stores many bits in a consecutive memory structure.\n\nBit masking is the process of masking (selecting) specific bits in a bit field (a multi-bit software/firmware variable), e.g. a 32-bit integer, or ) to manipulate, without modifying the value of any bits that where not selected.\n\nBit masking is thought-of as a low-level operation, and is normally only seen in languages such C and C++ (higher level languages may still use bit masking, but the code is likely to be abstracted away from the user). C and C++ bit masking usually makes extensive use of pre-processor macros.\n\nThe bit-wise OR operation ( ) can be used to set bits in a variable to 1. The bits in the bit field which you wish to set to 1 are ORed with 1, while those you wish to leave unchanged are ORed with 0.\n\nThe bit-wise AND operation ( ) can be used to set bits in a variable to 0. The bits in the bitfield which you wish to clear are ANDed with 0, while those you wish to leave unchanged are ANDed with 1.\n\nThis following example uses a macro to perform the masking and setting of bits. A function could be used, but macros are generally more efficient due to the lack of a function call.\n\nThe maximum width (in bits) that the following bit field manipulation macros can work with. Creates a \"MAX_BIT_FIELD_WIDTH_BITS\"-wide bit mask for the specified bit range a to b (inclusive). //! BIT_MASK(5, 12) returns the value (if MAX_BIT_FIELD_WIDTH_BITS = 32). ONLY WORKS WITH UP TO \"MAX_BIT_FIELD_WIDTH_BITS\" WIDE BIT FIELDS Use this macro to set a number of bits within a bit field. //! will set bits 12 through to 14 with 1, 0 and 1 respectively. ONLY WORKS WITH UP TO \"MAX_BIT_FIELD_WIDTH_BITS\" WIDE BIT FIELDS\n\nYou can then use it as in the following:\n\nUse In The Industry\n\nMany microcontroller manufactures (e.g. Atmel), provide firmware libraries and example code which make extensive use of bitfield manipulation to modify the registers which control the hardware peripherals (e.g the register(s) which configures the baud rate, parity and num. stop bits for a UART)."
    },
    {
        "link": "https://stackoverflow.com/questions/1044654/bitfield-manipulation-in-c",
        "document": "Is there any more arguments for either side? In particular is there some possible failure, perhaps with endianness, that the bitmask method may miss but where the structure method is safe?\n\nThe interesting argument my coworker and I had about this is still unresolved. Both styles work, and I maintain the classic bitmask method is easy, safe, and clear. My coworker agrees it's common and easy, but the bitfield union method is worth the extra few lines to make it portable and safer.\n\nAn interesting blog post argues that this is error prone, difficult to maintain, and poor practice. The C language itself provides bit level access which is typesafe and portable:\n\nThe classic problem of testing and setting individual bits in an integer in C is perhaps one the most common intermediate-level programming skills. You set and test with simple bitmasks such as\n\nIf the issue is that setting and clearing bits is error prone, then the right thing to do is to write functions or macros to make sure you do it right. // off the top of my head #define SET_BIT(val, bitIndex) val |= (1 << bitIndex) #define CLEAR_BIT(val, bitIndex) val &= ~(1 << bitIndex) #define TOGGLE_BIT(val, bitIndex) val ^= (1 << bitIndex) #define BIT_IS_SET(val, bitIndex) (val & (1 << bitIndex)) Which makes your code readable if you don't mind that val has to be an lvalue except for BIT_IS_SET. If that doesn't make you happy, then you take out assignment, parenthesize it and use it as val = SET_BIT(val, someIndex); which will be equivalent. Really, the answer is to consider decoupling the what you want from how you want to do it.\n\nYou have to think about this from the perspective of a writer -- know your audience. So there are a couple of \"audiences\" to consider. First there's the classic C programmer, who have bitmasked their whole lives and could do it in their sleep. Second there's the newb, who has no idea what all this |, & stuff is. They were programming php at their last job and now they work for you. (I say this as a newb who does php) If you write to satisfy the first audience (that is bitmask-all-day-long), you'll make them very happy, and they'll be able to maintain the code blindfolded. However, the newb will likely need to overcome a large learning curve before they are able to maintain your code. They will need to learn about binary operators, how you use these operations to set/clear bits, etc. You're almost certainly going to have bugs introduced by the newb as he/she all the tricks required to get this to work. On the other hand, if you write to satisfy the second audience, the newbs will have an easier time maintaining the code. They'll have an easier time groking and the first audience will just get grumpy, but its hard to imagine they wouldn't be able to grok and maintain the new syntax. It's just much harder to screw up. There won't be new bugs, because the newb will more easily maintain the code. You'll just get lectures about how \"back in my day you needed a steady hand and a magnetized needle to set bits... we didn't even HAVE bitmasks!\" (thanks XKCD). So I would strongly recommend using the fields over the bitmasks to newb-safe your code.\n\nThe union usage has undefined behavior according to the ANSI C standard, and thus, should not be used (or at least not be considered portable). 1 The following are unspecified: — The value of padding bytes when storing values in structures or unions (6.2.6.1). — The value of a union member other than the last one stored into (6.2.6.1). 6 When a value is stored in an object of structure or union type, including in a member object, the bytes of the object representation that correspond to any padding bytes take unspecified values.[42]) The value of a structure or union object is never a trap representation, even though the value of a member of the structure or union object may be a trap representation. 7 When a value is stored in a member of an object of union type, the bytes of the object representation that do not correspond to that member but do correspond to other members take unspecified values. So, if you want to keep the bitfield ↔ integer correspondence, and to keep portability, I strongly suggest you to use the bitmasking method, that contrary to the linked blog post, it is not poor practice.\n\nWell you can't go wrong with structure mapping since both fields are accessable they can be used interchangably. One benefit for bit fields is that you can easily aggregate options: In some environments such as dealing with protocol options it can get quite old having to individually set options or use multiple parameters to ferry intermediate states to effect a final outcome. But sometimes setting flag.blah and having the list popup in your IDE is great especially if your like me and can't remember the name of the flag you want to set without constantly referencing the list. I personally will sometimes shy away from declaring boolean types because at some point I'll end up with the mistaken impression that the field I just toggled was not dependent (Think multi-thread concurrency) on the r/w status of other \"seemingly\" unrelated fields which happen to share the same 32-bit word. My vote is that it depends on the context of the situation and in some cases both approaches may work out great.\n\nI'm not adding much to what's already been said, except to emphasize two points: The compiler is free to arrange bits within a bitfield any way it wants. This mean if you're trying to manipulate bits in a microcontroller register, or if you want to send the bits to another processor (or even the same processor with a different compiler), you MUST use bitmasks. On the other hand, if you're trying to create a compact representation of bits and small integers for use within a single processor, bitfields are easier to maintain and thus less error prone, and -- with most compilers -- are at least as efficient as manually masking and shifting."
    },
    {
        "link": "https://geeksforgeeks.org/bit-fields-c",
        "document": "In C, we can specify the size (in bits) of the structure and union members. The idea of bit-field is to use memory efficiently when we know that the value of a field or group of fields will never exceed a limit or is within a small range. C Bit fields are used when the storage of our program is limited.\n\nBit-fields are variables that are defined using a predefined width or size. Format and the declaration of the bit-fields in C are shown below:\n• data_type: It is an integer type that determines the bit-field value which is to be interpreted. The type may be int, signed int, or unsigned int.\n• member_name: The member name is the name of the bit field.\n• width_of_bit-field: The number of bits in the bit-field. The width must be less than or equal to the bit width of the specified type.\n\nTo learn more about how to use bit fields in data structures and optimize memory usage, the C Programming Course Online with Data Structures provides comprehensive lessons on bitwise operations and memory management.\n• None If storage is limited, we can go for bit-field.\n• None When devices transmit status or information encoded into multiple bits for this type of situation bit-field is most efficient.\n• None Encryption routines need to access the bits within a byte in that situation bit-field is quite useful.\n\nExample of C Bit Fields\n\nIn this example, we compare the size difference between the structure that does not specify bit fields and the structure that has specified bit fields.\n\nConsider the following declaration of date without the use of bit fields.\n\nThe above representation of ‘date’ takes 12 bytes on a compiler whereas an unsigned int takes 4 bytes. Since we know that the value of d is always from 1 to 31, and the value of m is from 1 to 12, we can optimize the space using bit fields.\n\nThe below code defines a structure named date with a single member month. The month member is declared as a bit field with 4 bits.\n\nHowever, if the same code is written using signed int and the value of the fields goes beyond the bits allocated to the variable, something interesting can happen.\n\nBelow is the same code but with signed integers:\n\nThe output comes out to be negative. What happened behind is that the value 31 was stored in 5 bit signed integer which is equal to 11111. The MSB is a 1, so it’s a negative number and you need to calculate the 2’s complement of the binary number to get its actual value which is what is done internally.\n\nBy calculating 2’s complement you will arrive at the value 00001 which is equivalent to the decimal number 1 and since it was a negative number you get a -1. A similar thing happens to 12 in which case you get a 4-bit representation as 1100 and on calculating 2’s complement you get the value of -4.\n\n1. A special unnamed bit field of size 0 is used to force alignment on the next boundary.\n\nExample: The below code demonstrates how to force alignment to the next memory boundary using bit fields.\n\n2. We cannot have pointers to bit field members as they may not start at a byte boundary.\n\nExample: The below code demonstrates that taking the address of a bit field member directly is not allowed.\n\n3. It is implementation-defined to assign an out-of-range value to a bit field member.\n\nExample: The below code demonstrates the usage of bit fields within a structure and assigns an out-of-range value to one of the bit field members.\n\n4. Array of bit fields is not allowed.\n\nExample: The below C program defines an array of bit fields and fails in the compilation.\n\nQ1. Predict the output of the following program. Assume that unsigned int takes 4 bytes and long int takes 8 bytes.\n\nQ2. Predict the output of the following program.\n\nQ3. Predict the output of the following program."
    },
    {
        "link": "https://codeproject.com/Articles/1276428/Creating-Good-Control-Protocols",
        "document": "I am the author of a very powerful software-based automation platform called CQC. Over the many years of developing this large system, one thing that I've learned more about than I ever really wanted to is how to interface to devices, since getting information from and sending commands to devices is one of the core capabilities of any automation system.\n\nIn the process I have of course learned a lot about what makes a good protocol and what makes a bad one, definitely a lot of the latter. Many device manufacturers don't understand the needs of modern automation systems, and don't even seem to make any effort to do so. The creation of the control protocol is probably often just a side issue left to some junior developer, or worse someone who is working on the device itself and who is more interested in making his life simpler than than making the automation system's interface more robust and reasonable.\n\nAnd I've also had to create many protocols of my own as part of this system. Most of my own inter-product communications is based around my Object Request Broker technology (see my previous article for details on that.) Between these two ends of the stick, I've been beaten enough to have come to a fairly good understanding of the issues.\n\nThere are more things involved than covered here, but if you get this stuff right you'll be better off than most. And a lot of the other things are higher level issues that would sit on top of this. I.e.. once you've created a good protocol, how do you use that capability of sending and receiving messages reliably to good effect. For instance, allowing the automation system to query device capabilities so that it can auto-configure without user intervention.\n\nAt the most fundamental level, a control protocol defines the rules that two ‘parties’ use in order to have a coherent conversation. When two people meet on the street, there is an implicit control protocol between them that allows them to communicate (mostly) successfully. Sometimes they may not follow the rules as strictly as they should, as we all know too well, but mostly we all follow the protocol. The important aspects of such an exchange, as defined by the protocol, are:\n• Words Used. If the two sides don’t agree on the meaning of the individual sounds or bytes or electrical pulses they are throwing towards each other, not much useful is going to come of it, e.g.. two people who don’t speak the same language.\n• Sentence Boundaries. Even if both sides understand the words being used, if both sides cannot recognize the start and end of a chunk of words that are intended to be taken as a logical unit of communications, then dialogue will be very difficult. This is of course similar to punctuation in written information, used in the delimitation of phrases, sentences, and paragraphs.\n• Syntax. Even if both sides understand the individual words being used and how to break them out into consumable chunks, if the contents of those chunks are not arranged in some mutually understandable syntax, it will still be very difficult to communicate, e.g.. two people who speak the same language very badly and put the nouns and verbs in the wrong place and whatnot.\n• Flow Control. Some humans have problems with this one, but each side of the conversation can only absorb information (data) and react to it so quickly. So there must be some way to tell the other side, please slow down and let me get my head around what you’ve already told me before you tell me anything else. Note that this is different from sentence boundaries above, which is about delimiting individual groups of words intended to be consumed as a unit of communications, sentences basically. Flow control is about the rate of arrival of such sentences.\n• Acknowledgement. In even fairly casual conversation, and certainly in important conversations, we have means of acknowledgement, which are used by one side to give a positive indication to the other side that, yes, I have heard what you said and I acknowledge that I understand it. Or in some cases a negative acknowledgement, that you were not heard or understood.\n• Sanity Checks. We often, in our conversations will use various types of sanity checks, such as forcing the other side to regurgitate what we have said, to convince ourselves that they do understand us and/or haven’t missed important information, or use various gestures, expressions, cross references, and side commentary to insure that the information is correctly understood.\n\nAll of these concepts are completely relevant to the conversations had between a device being controlled and the thing doing the controlling, which we will henceforth refer to as the device and the controller for brevity, and to avoid being too specific about the nature of the two sides of the conversation. Both could be software programs on the same network or on the internet, both could be small embedded devices, hand held devices, or any combination of these. Each of the above concepts will be examined in more detail below in separate sections, so that they are well understood before moving on to higher level considerations.\n\nIn this section we will explore the most fundamental aspect of communications between controller and device, that of the words used in the communications. As a practical matter, what this ultimately comes down to is whether the protocol is ‘binary’ or ‘textual’. As a simple example it would be the difference between using a numerical 0 or 1 bit to indicate off or on vs. using text to indicate the same using perhaps the English words ‘Off’ and ‘On’ or the characters ‘0’ or ‘1’. All protocols are typically one or the other, though of course even a binary protocol must occasionally or commonly transmit text information within it. The differences will become more obvious in the next section where we discuss the level of syntax.\n• Of course ALL digital communications is really binary technically. All computers understand are bits and bytes. But computers deal with text quite easily via the concept of ‘text encodings’ which we’ll discuss below.\n\nMany people have trouble understanding this difference between binary and textual communications, and the important related concept of text encodings, so we’ll take a short detour to explain it. In order to have computers deal with text, given that it only really understands numbers, we have to come up with some scheme to represent (to encode) text as numbers. We could say that 1 is A, 2 is B, 3 is C, and so forth, with perhaps 27 through 37 being 0 to 9, and with a couple numbers for punctuation characters, and that could very easily be used to communicate information between two computers either directly or by storing it in a file that one writes and another reads back in, and so forth. As long as both the reader and writer agree on what numbers represent what characters, digits and punctuation (in other words the text encoding used), there is no ambiguity and we have successfully used numbers to encode text.\n\nThere are many different such text encodings out there, and you may see them referred to sometimes in e-mail or HTML files. They have names like UTF-8, ISO-8859-1, Unicode, Windows 1252, ASCII, and so forth. Don’t let them intimidate you since all they are is a mapping of meaningful characters to numbers so that text can be stored and manipulated in a computer. The only real concern you have is that when your computer or device or controller reads in some text, that it knows the encoding that the text was sent in so that it can correctly decode that text. In the e-mail and HTML worlds the MIME system provides this indication, and you will see something in the header of an HTML file like “text/html; charset=ISO-8859-1”, which means that the contents of the file is text, formatted as HTML, and the encoding is ISO-8859-1. That tells the receiving system how to interpret the text received.\n\nThere are some side effects of using a textual protocol, such as the representation of numbers or Boolean (off/on or false/true) flags. Whereas we could represent a number like 127 as a single byte in a binary protocol, that can’t be done in a textual protocol. Instead we have to use three bytes, which in our simple encoding above would be 28,29,34, i.e.. the values that represent the characters ‘1’, ‘2’, and ‘7’. We could save one byte in these situations by using a hexadecimal (base 16) representation which would only require two digits to represent a byte, from ‘00’ to ‘FF’.\n\nSo, clearly textual protocols can be more ‘verbose’. Whether that is an issue or not depends on the circumstances. In today’s high powered world it’s not generally an issue, particularly if the amount of information transmitted isn’t usually that large and therefore the size differential isn’t really substantial. It may also depend on the syntax of the text being used. XML and HTML, for instance, are fairly common and both can be quite verbose, perhaps taking up to ten or twenty or more times the number of bytes required to transmit the same information in a compact binary form, or even a more compact textual form.\n\nIf the communications medium used is very slow, verbosity can potentially become an issue, e.g.. protocols used over wide area wireless networks can be performance sensitive. For local area network, USB, or serial connections, it’s not generally too much of an issue.\n\nBinary protocols are capable of being highly compact generally, so verbosity isn’t too much of an issue, at least not from this word level perspective. However verbose your syntax is (covered in the next section), the use of a binary format will typically insure it can be transmitted in the least amount of bytes possible.\n\nIn a binary protocol, we run into the issue of word order. A single byte in a modern computer will likely always be 8 bits and can represent an unsigned value from either 0 to 255, or a signed value from -128 to 127. But, if you need to represent a value of greater range than that, you have to use more than one byte. When such values are written out to a file or transmitted over some medium to a receiver, one has to come first and the other has to come second (and so on if more than two are required.) If the two sides don’t agree they will interpret the numbers very differently. The order of these bytes is referred to as the ‘endianness’ of the protocol, where ‘big endian’ means that the most important byte comes first, and ‘little endian’ means that the least important byte comes first. So it is very important that a binary protocol define the endianness of the data, and that each side swap the bytes of any multi-byte numerical values as required on the way in and out.\n\nTextual protocols, though they use more bytes, don’t have this issue since they just spell out the numbers with textual digits, which the receiver must then convert to a binary value for internal use. The receiver understands that a number like -14 would be transmitted as characters in the natural written order, i.e.. ‘-‘, ‘1’, and ‘4’ (using whatever actual numbers those encode to in the chosen text encoding.) So text protocols are endian neutral.\n\nThere can also be some other issues such as the format of floating point numbers (if any are even involved in the control protocol and often they are not) or the form of negative numbers. There’s no negative sign in the binary world. Both sides have to agree that a given number is either a signed or unsigned value and interpret it accordingly. Signed numbers are encoded in a specific way, and both have to understand what form that is. But it’s very typical these days that any controller or computer or device is going to use the very common two’s compliment form for signed numbers and the IEEE format for floating point numbers. So this generally isn’t a worry. The device can indicate the use these forms in the protocol, and in the rare case of a controller that doesn’t use them natively, it can translate them as required.\n\nOne key ingredient to a good protocol is very strict definition of every aspect of the information being passed back and forth, and the ability to add new features to the protocol without breaking existing users of the old version. The biggest problem with textual protocols is that non-readable characters can be included into the text without being visually obvious. This may or may not affect any given consumer of the data, depending on what assumptions it makes in its parsing of the text. So the tests of the manufacturer may not catch a problem that ultimately breaks numerous existing controllers after some firmware update of the device.\n\nBinary protocols, which are generally defined by a ‘structure’, which is a programming term for a strict layout of values in memory and something that is supported directly in many programming languages, don’t tend to have this issue. They are very strictly defined in this way and there’s typically not not as much opportunity for assumptions to be made by the consumers of the data.\n\nWe’ll get into some specific examples in the Sentence Boundaries section below.\n\nWhich to Use?\n\nThere are good arguments for using either binary or textual form for your control protocol. The author would argue that binary protocols are typically more strictly defined and efficient. However, because of the capabilities (one could say limitations) of many modern programming languages, which do not allow for arbitrarily spelunking around in a block of memory for safety reasons, a textual format can often be more practical, since the block of memory can be just treated as a stream of text that can be parsed out sequentially using built in language capabilities.\n\nIn terms of complexity, binary protocols are typically more complex initially to get into, because a text protocol can often be easily explored using a simple text terminal program connected to the device. The text output of the device just shows up as lines of text on the terminal program. In theory a well written protocol document should make this unnecessary, but it still tends to make people comfortable, and allows for an easy visual sanity check during the development process.\n\nUltimately though, either will work well as long as they are of good quality. It’s generally a six of these, half dozen of the other, sort of proposition. In terms of what’s out in the wild, I would say that text based protocols outnumber binary probably ten to one if not considerably more. But that almost certainly has less to do with the inherent technical superiority of a textual protocol than the ease of implementation.\n\nOnce you have defined the form of the words of your protocol, you need need to provide a means for two sides of the conversation to be able to recognize coherent chunks of words intended to be consumed as a unit, e.g.. the sentences in the protocol language, often referred to as messages. This extraction of individual messages is a key element of both the controller and the device and must be done reliably, and both sides will typically have some low level code that handles this task, .e.g.. a GetMsg() function. Higher level code will call this function and expect it to give back the next message received (or perhaps an error that it didn’t receive anything or that what it got was malformed and so forth.)\n\nMaking this aspect of the protocol very unambiguous is a big part of good protocol design and it’s not terribly difficult to do. It will typically be done quite differently in binary vs. textual protocols, but sometimes the same sorts of schemes are used. We’ll discuss various strategies in the context of binary and textual formats.\n• Note that this is separate from sentence syntax, which we’ll discuss below. Though some consumers of messages may choose to parse and validate the contents of the message as they are receiving it, often the reading in of individual messages is a separate thing from the examining and validating of the contents of the message.\n\nThere are a number of strategies used in binary protocols. Because of the fact that binary protocols very easily allow for fixed layouts of data, what would be termed ‘structures’ in a programming language like C or C++, lots of binary protocols use some sort of structure to represent the data being transmitted. As long as both sides agree on that structure, it becomes fairly easy for both sides to send and receive messages, once any word order issues have been dealt with by flipping the order of bytes where needed.\n\nA very common scheme involves the definition of a ‘header’ that is the same for all messages, plus a message specific payload part that follows it. This header will typically at least include the type of message that the payload contains, and the number of payload bytes. This allows the receiver to generically read in and do basic validation of incoming messages. It can look at the information in the header, verify that it looks like a valid header, and then read in the number of payload bytes indicated. By having a distinct header and payload, a low level message reading function can just discard the header once it has verified that it has read a good message, leaving only the specific message part to further process.\n\nIn order to handle the problem of getting out of sync with the sender, the header will often contain various types of sanity checking information, which will be covered in a later section.\n\nAnother possible scheme, though less commonly used, is that of a start/end marker so that there is a start byte, followed by all of the bytes of the message, followed by an end byte. Getting in sync is fairly easy with such protocols since the receiver can just scan forward still it sees a start marker then read till an end marker is seen. Once the receiver has received the message, discarding the start/end markers, it should have a good message.\n\nThis scheme can be problematic in binary protocols due to possible ambiguity, which is discussed at the end of this section. Dealing with the ambiguity can offset any simplicity benefits this scheme might have for a binary protocol.\n\nThis is really sort of the same as Header+Payload, but it really doesn’t quite qualify as a header because it’s just a start byte followed by a number of bytes to read. So the reader looks for a start byte, reads the length byte that follows, then reads the indicated number of bytes.\n\nThis scheme, like the Start/End Marker scheme, suffers issues of ambiguity in binary protocols, exacerbated by the fact that if one incorrectly senses a start byte, and therefore reads a bogus following length byte, it could issue a read for the wrong amount of data and make the problem even worse.\n\nSometimes the start/end is combined with the Header and Payload scheme, which provides a sort of cross reference sanity check since the length byte in the header can be used to read that many bytes, the last of which should be the end byte if things are working correctly. If the start/end values are, say, 32 bit values with fairly randomly selected bit patterns, the odds of them just happening to occur in those locations are pretty small.\n\nThe textual strategies tend to be easier because of the simple fact that the textual encoding makes it fairly easy to guarantee that bytes used as start/end type markers will never be used in the actual body of the message. A binary protocol may legitimately use any of the possible 256 byte values in the body of a message. But a textual encoding generally only needs the upper and lower case characters, the digits, and maybe a few punctuation characters. That leaves plenty of the possible 256 values of a byte available for use as unique message delimiters. Some encodings have characters specifically designed for this purpose. These are typically ‘non-printable’ characters, i.e.. they are not used to convey text content, but purely to delimit text content and be removed before display. See the discussion of ambiguity below.\n\nThis is arguably the most commonly used strategy for textual protocols. Special byte values are used to mark the start and end of a message. This makes for a very easy sync strategy as long as ambiguity isn’t an issue. The receiver scans for the start marker, reads up to the next end marker, and he has a whole sentence of the protocol. The start/end markers are discarded, leaving just the message. There’s little danger of getting out of sync if one side connects in the middle of the other side sending data, since the receiver always just scans for a start marker and anything it sees before that marker is discarded.\n\nSometimes there is only an end marker, with the start marker being basically implicit. I.e.. the next byte after you read the end marker must be the first byte of the next message. This is not quite as robust since if junk data is somehow transmitted, perhaps partially seen incoming message upon first starting up, it cannot as easily be caught and discarded, until the program has gone through the work of trying to parse the incoming message.\n\nA common scheme here is a newline separator. Many systems have built in tools to parse out newline separated lines of incoming text, so this can make it easy to implement such schemes. But it’s not a recommended one, because it doesn’t make for a strongly defined format and such protocols can be easily broken by changes that were not foreseen by protocol implementers. Also many platforms don’t agree on what constitutes a new line, thus obviating the advantage of having a built in mechanism for parsing new line delimited values, and inconsistent newline values can be introduced without the manufacturer noticing this in testing.\n\nIt is possible to do a sort of header/length type of scheme, even with a textual protocol. It’s not so commonly done, because an unambiguous start/end or end marker scheme is so easy and works well, but there are devices out there that use this scheme, sometimes because they want to provide sanity check information that is separate from the main message payload, as in the binary world above.\n\nThere are some devices that might combine a binary plus text scheme. I.e.. they might use a simple binary header, which includes an indicator of the number of payload bytes to read, which is in some parseable text format. The XML Gateway Server of our own CQC product uses such a system. There is a binary header, followed by an XML based payload that the receiver parses out using an XML parser. This type of scheme combines some of the efficiency and convenience of binary for the reading in of messages, with the flexibility (but also the bloat) of text for the content of the payload.\n\nOf course any scheme where you use a language to describe other data expressed in the same languages raises issues of ambiguity. HTML is a case that many people may be familiar with. HTML uses text to describe aspects of other text, e.g.. the color, font size, etc… This requires that there be a way to distinguish between the data and the metadata (the data about the data.) The tags are the metadata, and the text actually displayed to the user is the data. Normally this is fine, until you have to display some text that might use some of the special delimiters, or even if you need to display HTML tags as the actual text, for instance a web site that demonstrates how to write HTML and therefore must treat HTML tags as both data and metadata.\n\nControl protocols suffer the same problem sometimes. If you have delimiter information such as start/end markers, you have to have a way for the receiver of the message to be sure that he is reading a message at the start of the message, not just picking up somewhere in the middle, and when to stop reading and process the message received.\n\nIn text protocols, this is not too hard. As mentioned above, most text encodings specifically include special characters not designed to display text, but to delimit text, and those can be used very conveniently in textual protocols. One trick commonly used in textual protocols designed around the ASCII text encoding (lots of them) is to use the STX/ETX characters (binary codes 2 and 3 in the ASCII encoding) as the start/end markers.\n\nIt is quite easy to arrange for these characters to never actually be used in the body of any legal message, so these markers are completely unambiguous. If a sender did use them, that sender would be so broken that it’s not worth worrying about. Many of the extant text encodings tend to include the ASCII set as part of it, and just extend it in one way or another, so the same STX/ETX characters are likely to be available in other commonly used encodings, such as Windows 1252, UTF-8, and Unicode. So a message in this scheme might look like this, where <STX> and <ETX> are spelled out for readability, though they would actually just be single bytes encoding those characters:\n\nFor binary protocols, the issue is a bit tricker, because any of the 0 to 255 values a byte can hold might legally be a value in the body of a message. A common scheme is to use something like the Header+Payload format discussed above, but that doesn’t guarantee complete non-ambiguity unless the magic marker values in the header are guaranteed not to ever exist in the payload of a message in the same distances from each other. In a protocol that fully defined any possible values sent and received, this can sometimes realistically be done, but sometimes not. And if it involves any user provided or external obtained content, it can be impossible to absolutely guarantee.\n\nSometimes further sanity checks are provided, as discussed in the Sanity Checks section below. But, failing all of that as a guarantee of non-ambiguity, sometimes a system of ‘escapement’ is used, to force the interpretation of a special value as non-special in a given context. There are various ways of doing this, but it’s unlikely to be of value to discuss them since you’d be far better off just avoiding the issue. If you do choose to use a binary protocol, it’s almost certain that a header plus payload plus possibly some extra sanity checks will more than suffice to the purpose. We’ll examine a sample binary protocol later in this document.\n\nThe most efficient protocols are those that include a length indicator, the reason being that those that don’t require the receiver to read the bytes one by one until it sees some sort of end marker. The receiver just has read a header’s worth of data, verify it using whatever means are provided, then pull out the payload length indicator and issue another single read for that many bytes.\n\nBut, on modern hardware, this efficiency argument probably is considered a distant second, or third or fourth, to concerns of simplicity of implementation. Since the number of bytes being transmitted per message (sentence) tends to be smallish, the overhead won’t be significant anyway. On a small embedded device perhaps it might be an advantage though if such bulk reads can be processed asynchronously while other housekeeping chores are being done.\n\nOnce you have decided on the binary vs. textual form of your protocol, and how to properly extract coherent sentences (messages), you then need to deal with the syntax of the message content itself. So we have a block of data, and we’ve discarded any of the message extraction housekeeping data, and just have the actual message contents, the payload. We need to parse the content of that message payload to see what the other side is trying to say. You could do this many ways, but most protocols end up doing it in the most obvious ways, because they are simple and work well\n\nHere are most of the sentences usually involved:\n• Do This Command. The controller needs to tell the device to do something, to change the channel, to turn off the light, etc…\n• Give Me Info. The controller needs to ask the device for something, what’s the current channel, what is the off/on state of the light, etc…\n• Here’s Some Info. The device either is responding to a query (previous item above) with the requested info, or it is is sending some unrequested notification of something having happened.\n• Configure Yourself. Some devices provide options in the protocol that affect how the device will respond or act, and therefore the controller needs to send commands to the device to set these options. These are syntactically probably the same as Do This Command, but with just some internal state of the device being the target of the command instead of the physical mechanism being controlled.\n• Acknowledgement. The device needs to acknowledge that it received a command or request, so it typically sends some kind of Acknowledgement (ACK) that all went fine, or a Negative Acknowledgement (NAK) that it didn’t understand what it was sent or couldn’t do what it was asked.\n\nThis list makes up pretty much all of what transpires between a controller and device. Not a very deep conversation, but an important one. Here are some basic examples of binary and textual syntax. We’ll get more deeply into the subject later, but these will serve as a starting point.\n\nIn a very simple binary protocol, you might have a set of numbers of that represents the commands, e.g.. 1 is do a command, 2 is give me info, 3 is here is some info. Another set of numbers represents the things that can be requested or set, e.g.. 1 is current channel, 2 is power, 3 is mute, etc…\n\nSo the most simple sentence might be the bytes 1,2,0, which might mean, turn off the power because the action is ‘do a command’, the target is the power state, and the value to set is zero for off. The acknowledgement of setting the state of something might be to just send out the same response as would have occurred if you requested the state, or it might be some simple ACK/NAK specialized message, perhaps with an error response indicator. To request the power, it might be 2,2, which would be a query command and the source being the power. And the response might be 3,2,1, meaning here is some info, the source is the power, and the value is 1, which means On in this case.\n\nSome devices will literally be that simple if they don’t have a lot of options. For instance a simple video switcher might have a protocol that simple. Of course there needs to be some more work to be able to correctly recognize the start and end of such messages. But, in terms of the actual message payloads, they can be that simplistic, and of course highly efficient since even with the sentence boundaries overhead you might be only sending four or five bytes.\n\nIn those cases where some text needs to be sent in a binary protocol, the protocol will usually either set aside a fixed number of bytes into which the text will be placed, with any unused bytes being set to some known unused text value, like zeros. Or, in some cases it will append the text to the end of the message, where it can be variable length, and will indicate in the fixed binary part of the message the number of variable bytes of text to expect at the end. The latter is often more efficient, but more complex to parse. So, for instance, if the device had a command (value of 4) to display a line of text on its front panel, the command might look like this (assuming our simple text encoding above is used):\n\nIn this example it is a do a command, and herer we’ll create a new ‘show text’ command with a value of 4, followed by an indicator of how many characters to expect (3), followed by that many characters, in this case the text for DAD. So the first three bytes of the message are the same syntax as above for a ‘do a command’ message, but in this case the protocol indicates that a show text command will have these trailing text bytes.\n• Of course the commas we are using in these binary examples are for readability purposes only, they wouldn’t be part of the actual data, which would just be consecutive binary bytes.\n\nThe ordering of values in a binary syntax generally isn’t that important. But, for the sake of those consumers of messages that do not have the ability to arbitrarily examine data within a block of memory, it may be more convenient for them to arrange the data such that the most important stuff comes first, such as the message type. This way, as it moves through the block of memory sequentially and pulls out values, they will see that important stuff first and can often then call other helper methods to parse out the remaining contents based on the type of message.\n\nAs discussed below for textual protocols, often a “verb noun value” type of ordering is desirable. The verb is the action to take and generally is the most important, the noun is the target or source of that action and is the next most important, and any modifiers or values are the least important, at least in terms of the natural order of parsing and validation of the contents.\n\nA textual syntax might be almost as simple as the above, however whereas a binary protocol might have absolutely no ambiguity as to what the words of the sentences are (in the above example each byte is a word in the sentence, so it’s pretty clear), a textual protocol may have to provide some sort of inter-word delimiter so that they can be pulled out of the incoming message successfully. Of course you could have a textual protocol in which each word of the sentence is a fixed length, except perhaps the last one. So you might have some commands like:\n\nIn this case, since the values for the action to take are all the same three characters long, there’s no ambiguity. You take the first three characters as the command, and the rest of the message is the value, no need to even separate the two, though they often would be for readability by humans. This sort of sentence format is not uncommon in simpler textual protocols. But, if you need to transmit more complex information you may need to deal with more than one variable length word in the sentence. Another common scheme is a comma or space delimited syntax, such as:\n\nHere the syntax is just two or three space separated words. This works well for the most part, however you do have to be concerned if the actual values you send can contain the delimiter character. This is the common problem of ‘meta languages’ which we will discuss in the next section. In many protocols this is not an issue since all of the data sent and received is defined by the protocol strictly and it can just be arranged that the delimiter will never be used. In some cases only the trailing part can contain arbitrary data so it’s not an issue. I.e.. you have action and source/target values that are completely defined by the protocol and never have spaces or commas, so they can easily be parsed out by space or comma separation. The value, the trailing part, is then just the remainder of the line, so no parsing is required and it doesn’t matter if it contains spaces or commas.\n\nThe ordering of the values is completely arbitrary of course, but it’s very common in most control protocols to put the action (verb) first, the target or source (noun) second, and any values being sent last. One benefit of it is that the receiver can validate the content of the incoming sentence as he parses through it, because the action to take is the most important aspect of the sentence and it defines what the valid source or target can be. Knowing the source or target tells the receiver what values are valid for the value if any.\n\nIt is also not uncommon for textual protocols to use a standard structured text format such as XML or JSON, in which case those define the structure of the content, and you use that structure to define the msgs you want to send and receive. Whether this is a good thing or not depends on circumstances. It will be considerably higher overhead, and maybe significant overkill for very simple devices. On the other hand, for more complex information it can be very useful.\n\nOne down side is that it means a potentially big chunk of code just to parse messages, which might be a burden in very small devices. XML parsers aren't trivial (if they are really compliant.) Obviously, as the device maker, you could require a very limited subset of XML and use something hand-hewn and much simpler.\n\nHere again, there is really no obvious winner, and the same pros and cons apply here as in the previous section, with binary tending towards more efficiency and being easier to very strictly define, and textual protocols being somewhat more verbose but easier probably to initially understand and possibly easier for modern, structure-less, languages to deal with.\n\nIn any conversation, sometimes people just talk too darn fast. The same can apply to controllable devices or automation systems. So there has to be some way to get the other side to slow down where that’s required. By using some very simple rules, this problem almost takes care of itself in most protocols, but there are still some issues to consider.\n\nIn some types of protocols, where one side basically does all of the talking, such as perhaps a sensor that just continuously sends out a stream of temperature values, if the sensor can send the values rapidly, it’s possible that the receiver could get overwhelmed. That’s not too likely in a modern computer based automation system, but it could possibly happen, and potentially more so in an embedded controller that cannot multi-task and therefore has to serially execute all tasks, including reading incoming data.\n\nFor such situations, there are two traditional schemes for getting the sender to shut up for a while. One is hardware based and used on devices that communicate through serial ports. There are hardware lines that can be used if both sides support them, in which the receiver can signal to the sender whether it is ready to accept data or not. This is something that is often handled, at least in modern computer operating systems, by the low level serial device drivers, so the application level program doesn’t have to worry too much about it.\n\nThe other scheme is to do it as part of the protocol itself, so that there are commands that can be sent to indicate that the other side should stop sending, and when it can start sending again. In this case, being part of the protocol itself, the application doing the communications must deal with the details. Though, in text oriented protocols the XON/XOFF special characters (similar to the STX/ETX special characters discussed above) can be used, and the low level serial port support libraries may handle those on behalf of the application as well.\n\nBut, having said all of that, don’t do any of that. In a typical device control situation, none of that is generally required or even desired. It can all be avoided by the simple expedient of requiring that all transmitted message be acknowledge, explicitly or implicitly. This is pretty easily done by having a protocol with the following characteristics:\n• All outgoing commands from the automation system to the device (to make something happen, set the volume, turn a light on), are explicitly acknowledged by the device, after it has done what it was asked to do, or a negative acknowledgement if it could not do what was asked.\n• All requests for information from the automation to the device are implicitly acknowledged by the return of the information requested (or a negative acknowledgement that it doesn’t have the information requested.)\n\nIn such a system, since the automation system cannot continue to the next step until the device replies, and such a reply is an implicit indication that the device is now ready for another command or request, a natural flow control is established that will insure that neither side talks the other to death. Such a scheme could be referred to by the term ‘call and response’, since every exchange begins with the controller making a call out to the device, and the device responding. Computer people might refer to it as ‘client/server’, in that one party (the controller) is a client actively making requests for services, and another party (the device) is a purely passive supplier of services. So the server in this case is very inattentive and only speaks when spoken to.\n\nMany devices use protocols based on this call and response scheme, because of its natural flow control and simplicity of implementation, on both sides. One side only has to send a message and expect a reply. The other side only has to wait for a request and send a reply back. There is a lot to be said for the simplicity of well defined rolls, and where possible this type of protocol should be used.\n\nHowever, there are some situations where it is not sufficient. In some cases the device must tell the controller important information without waiting around for the controller to ask for it. So you now have a situation in which the client side must both send messages and wait for a reply but also, while waiting for any reply, to be able to process an unsolicited incoming message. This can significantly increase the complexity of the code required to interface to such devices, but the complexity is sometimes necessary for practical reasons. Such notifications are often referred to as ‘asynchronous notifications’, because they can happen at any time, whereas in a call/response protocol the traffic in each direction is totally synchronized.\n\nHere are some common concerns that drive the selection of a pure call and response protocol vs. an asynchronous protocol.\n• Volume of Data. If a device only has, say, four pieces of information that can be set or queried by the controller, then there’s not much concern. But, if a device has a hundred or four hundred pieces of data, a pure call and response system can be a problem, because the controller cannot know something has changed until is explicitly asks for the piece of information that changed. If there are hundreds of possible piece of data that could change, the controller has to continually ask for them all all the time, making for a lot of traffic almost all of which is redundant.\n• Efficiency. Again, if there is a substantial volume of data, most of which is not changing most of the time, it’s a huge waste of time for both the controller to continually ask for all this data just to have to check it and see if any has changed, and for the device to continually cough up this information and transmit it. For small embedded devices that overhead might be pretty bad.\n• Latency. Latency in this case is the lag between when some state of the device changes and when the automation system sees that change and can react to it. If the device has a lot of values, the automation controller probably cannot ask for the whole chunk of values multiple times a second. It will have to round robin through the values, asking for either one at a time or chunks of them at a time. This might result in seconds of latency before the controller sees the change, depending on where it is in that round robin cycle when the change occurs.\n• Device Response Time. Even if the device doesn’t have lots of data values for the controller to read, if the response time is quite low, and if the controller’s continually banging on the device (to try to keep the latency down) just makes that situation worse, there’s still a good argument for having the device send out notifications of changes. Systems like Z-Wave or UPB are typical here, because of the potentially considerable number of devices in their networks and the quite low data rate.\n\nFor all these reasons, some devices really need to send out asynchronous notifications of changes. Note that this still leaves some issues that the controller needs to be concerned with, notably:\n• Startup State Acquisition. Even if the device sends out notifications of changes, the controller will still will likely need to get a full report of the state of the device upon controller startup, so that it can cache that data in its own memory. After that it will keep its cached values up to date based on incoming notifications, and possibly some active call/response polling. So using asynchronous notifications doesn’t relieve the device from the requirement of providing an efficient means to proactively get all the state information. Don’t make the controller spend two minutes just trying to get into sync with the device through slow and laborious single value queries of hundreds of values, before it can come online.\n• Keep Alive. If a device uses notifications and nothing is changing that requires reportage, the controller might not actually hear anything from the device for longs periods of time. So the controller still must periodically ‘ping’ the device to make sure it’s still alive and kicking. So the device should have some low overhead “are you alive” type of query available for this purpose. Alternatively, the device itself can guarantee that it will send out some type of message at least every X number of seconds, with some sort of empty ‘ping’ type message being used if it has nothing more useful to say. If the controller hasn’t seen a message in X seconds, it assumes the device is disconnected and tries actively trying to reconnect and doing the startup state acquisition steps.\n• Missing Notifications. It’s possible that a device might send a notification that somehow gets missed by the controller. Since the controller depends on these notifications, it might never know that it now is providing information that is out of sync with reality. This is often dealt with by using a ‘sequence counter’ value that is incremented and put into each outgoing message. The receiver maintains a similar counter and if the incoming message doesn’t have the expected sequence, something has gone wrong and the connection can be ‘recycled’ again in order to get back into sync. Such a scheme isn’t needed for call/response type protocols, since a lack of response is proof positive of a failure.\n\nSo, the bottom line here is that if your device has a fairly small amount of data, and it can either efficiently return it to the controller on a fairly rapid basis or the type of device means that considerable latency isn’t so much of an issue then stick with a simple call and response protocol. It’s easy to implement on both sides, and even a very simple controller can probably deal with it.\n\nFor example, a thermocouple probably doesn’t need to be polled rapidly since the temperature just isn’t going to change that rapidly. Every five seconds might just fine, or maybe every ten or fifteen seconds. And lots of other devices fall into the category of having a fairly small amount of data and being able to regurgitate it quite efficiently and rapidly without being overloaded. On the other hand, an A/V Processor, an automation panel product that offers lots of analog and/or digital I/O, a lighting system that supports hundreds of loads, etc… these types of products typically will need to support asynchronous notifications of changes, such as the commonly used Elk and Omni automation panels do.\n\nNote that one possible halfway scheme that some devices might choose to implement is to use a call and response system, but provide a special query not for a specific piece of information, but for any bits that have been changed since the last query. So the device would maintain a queue of changes. When the controller requests changes, those queued values are sent back and the queue is flushed, ready to start accumulating more changes. And, as always, this still does not relieve the device of the need to support efficient startup acquisition of the device’s full status. It also won't work with multiple parties asking for data.\n\nAnd, now we finally get back to flow control again. If the device sends asynchronous notifications, then that means that it is breaking that strict call and response scheme for natural flow control, and it’s possible that the controller could get overloaded. As mentioned previously, this isn’t very likely on a modern system, based on fast processors and the ability to multi-task. But it might be problematic for some controllers. You may want to deal with this issue by not sending notifications by default, but allowing the controller to explicitly ask for them to be sent, or by providing a configuration setting that can be set by the user via your own configuration interface or DIP switches and so forth.\n\nYou might think that you could just require that the controller acknowledge all notifications, but this would lead to a pretty instant death through deadlock’. As soon as the controller sends a request and then waits for a response, at the same time that the device sends a notification and waits for a response, they are now in a deadly embrace and neither will be able to respond to the other.\n\nAnother consideration if you send notifications is to make it clear to the controller whether a message is an async notification, or a response part of a call and response request from the controller. This makes it much easier for the controller to correctly sort out incoming messages. Some devices send exactly the same thing for an async notification that they would in response to a query for that value. This is convenient since the same code in the controller can handle both. But it can cause the controller to incorrectly think it has seen a response that it is waiting on, when it hasn’t yet. So mark responses and notifications explicitly so that they can be distinguished, though everything else about them should probably be the same for convenience.\n\nIn a way acknowledgements are part of flow control, as mentioned above. As long as the device acknowledges all commands (explicitly) and all requests (implicitly by returning the requested data), a natural flow control is maintained. However, acknowledgements are also part of error reporting. Typically a protocol will have a special positive (ACK) and negative (NAK) acknowledgement message, the latter being returned if the device cannot carry out a requested command, or provide requested information. In such cases, the negative acknowledgement (NAK) can return an error indicator that the controller can report or log for problem diagnosis.\n\nSome devices use as a positive command ACK the same response that they would send for a query of the thing affected. This is not horrible, but probably not optimal either. One reason for doing it might be to work around the fact that the actual value stored in the device isn’t exactly what the controller sent. That’s not common, and is highly undesirable, but might happen, e.g.. a set point that can only be set to even values, so sending 1 would actually set it to 2. By acknowledging the command to set the value with a value query response, the controller adjusts for this quirk in the device naturally since it will then be informed of the actual value set. Generally though a simple ack is more efficient and works as well. The controller already has the value since it just sent it out. It doesn’t need to be told about the value again. A positive acknowledgement lets the controller know it can store the value it just set, because that value was successfully stored in the device as well.\n\nIn the case of requesting information, typically a controller must be able to respond to either the actual response that it is waiting for, or a NAK response. So it does add a little extra complexity in that case. In the case of an outgoing command to the device, usually an ACK/NAK is all that is expected anyway. In most modern, exception based, languages the low level message reading method can watch for a NAK and throw an exception containing the error info, which means that usually only the highest level interface code needs to actually deal with NAK responses.\n\nProviding good error codes can very much help with in the field problem diagnosis. Many customers will not be capable of breaking out a port sniffer to watch communications and diagnose problems. The controller must of course as well be diligent about reporting such errors, though in some cases only when asked to provide more verbose reporting.\n\nDon’t ever have a situation where a controller sends a query or command, and you don’t respond if all goes well but you send a NAK if something fails. The controller cannot know whether it should wait or not. All messages from the controller to the device should be either responded to in some way, or not responded to at all (not recommended.) There can’t be anything in between because of the ambiguity it entails.\n\nIt’s not uncommon for protocols to include various sanity checking devices so that the recipient of a message can verify that it has clearly been received without any corruption, and that it hasn’t somehow gotten out of sync in its attempts to extra full messages from the incoming stream of data from the other side. For some types of devices, controlling critical processes, this can be a very important concern. Some of these sanity checks can be problematic for simple controllers, since they may require doing mathematical calculations that the controller just simply cannot do. To be friendly, it is typical for the device to send them out even though they may get ignored, and to accept messages either with or without them, and to only check them if received.\n\nA pretty common and easy to implement sanity check is the check sum. Some of the bytes of the message, such as just the payload part, are just added up, into an 8 or 16 bit value and that sum is transmitted along with the message (in a part not included in the check sum itself of course.) The receiver does the same for the same bytes of the message as received and compares his result with the one transmitted. If they are agree, that’s a pretty good sign that the message is correct. It’s not a guarantee since it’s not completely unrealistic that two arbitrary messages might generate the same check sum, but it’s a pretty good check that is simple to do (so even a simple controller might handle it) and efficient to calculate.\n\nUse of checksums really requires a header+payload sort of format to messages, since the checksum itself cannot be part of the bytes that are included in the checksum, or perhaps they are appended to the message in some way.\n\nCRCs are similar to checksums except that they don’t use a simple sum. Instead they will use an algorithm typically similar to what in software speak would be called a hash. These types of algorithms feed the bits of the message through what is sort of a ‘bit blender’ where the new bits interact with the previously consumed bits in such a way that it becomes vastly less likely that any two different sequences of bits could result in the same value.\n\nCRCs are obviously much more powerful in terms of ensuring the quality of the transmitted data. But even fairly complex automation controllers might not be able to generate them without allowing for the writing of custom code to do it, because each device might use a slightly different CRC algorithm. There are some standard ones but they aren’t universally used, or supported in a canned way in all controllers. So definitely here you would want to make the CRC optional.\n\nAs with checksums above, CRCs really requires a header+payload sort of format to messages, since the checksum itself cannot be part of the bytes that are included in the checksum, or perhaps they are appended to the message in some way.\n\nThis will be discussed below in the section on miscellaneous concerns, but effectively encryption is going to act sort of like a very large hashing operation, in which even a single bit getting scrambled as the encrypted data is transmitted will probably result in a complete or significant mangling of the decrypted data, making it clear it was corrupted in transmission.\n\nIn many binary protocols, magic values are placed into the data, such that the odds of those values appearing by accident if the communications gets out of sync are very low. This is not a particularly strong scheme, but it’s often used in addition to one of the other sanity checks and can often allow a bad message to be rejected very quickly without going through the overhead of calculating a checksum or CRC.\n\nEven when a message length indicator is in the message, an end marker can often be used as an extra sanity check. After the receiver sees the message length, it just blindly reads the rest of the message. If it doesn’t see the end marker at the end of the read in data, then the message is bad.\n\nIn addition to the above material, which covers very obvious and core issues of protocol design, this section will cover some less common ones that you might want to consider.\n\nSome devices will require encryption of all messages, both from the controller to it and vice versa. This may seem like overkill, and probably in most cases it is, but for some core devices that have control over your HVAC system, perhaps your security system, it may be something to consider. Note that this is really mostly only an issue for devices that use Ethernet for communications. For devices that use USB or serial connections, those are basically one to one connections from the controller to the device. So there’s not much concern that a third party will intercept that information.\n\nFor devices that are using the network, anyone with access to the network could potentially tap into that communications and eavesdrop or even take control over those devices. In some cases even here encryption is overkill since if you have someone breaking into your network you have bigger things to worry about. Deal with that for all the obvious reasons and you’ve dealt in the process with the potential hijacking of your automation devices. But, even so, there are considerations such as the wannabe hacker friends of your children, or the potential for someone hacking into your wireless network through accidental mis-configuration of the router potentially.\n\nIn any event, if the device is controlling systems in the user’s home such that they could be very concerned about these types of possibilities, you may consider implementing encryption. However, it would almost certainly need to be optional because some controllers may support encryption schemes like AES, Blowfish, TLS, etc…\n\nI believe that the only local device supported by CQC that currently requires encryption is the HAI Omni automation panel, which uses AES. Most cloud based protocols will use HTTPS or WSS, though cloud based protocols are undesirable for many other reasons.\n\nShort of encryption, you might choose to implement a login feature, where the controller must provide some login information before the device will correspond with it. This is more practical with mediums that have a distinct concept of connected or disconnected. The reason this is important is that the device must know if the connection is lost and go back into ‘waiting for connection’ mode which forces a new login to establish a ‘session’ as it is is usually called. As long as that session is active, the controller is assumed to be the same party that provided the login credentials. Serial ports, for instance, don’t really have this concept. The only way to know if someone is on the other side is if they respond in the defined way.\n\nThis section provides some general rules that controllable devices should follow in order to insure a high quality result. The previous discussion has been more about the actual for and flow of communications between the controller and device. But there are other, higher level concerns that are very important and that are very much part of the ‘contract’ between the two parties to abide by certain rules of protocol. Any deviation from these rules put extra burdens on the controller that must be understood, and that preferably are avoided altogether.\n\nThere is no one to one relationship between automation controller and device controlled in a modern, multi-user automation system, such as our CQC product. There are typically multiple users throughout the home, and the automation system is often configured to do things automatically at various times or in response to changes. So there’s not a guy sitting in front of a computer pressing a button to see the latest state of a device. In order to support from two to tens or even a hundred consumers of device information around the network, a modern controller cannot go out and talk to the device every time someone needs to know something about its state.\n\nInstead the controller caches its own copy of those aspects of the device’s state that it cares to expose, and hands out that cached information to consumers upon request. The controller must keep his copy of the device’s state up to date as much as is practical. This leads to two rules that well behaved devices should obey:\n• The device must provide an efficient means to bulk query the state of the device. When the controller starts up, it has to get its cached view of the device in place quickly. Don’t require two minutes or querying hundreds of individual values before the controller can come on line.\n• The device must provide an efficient means to keep up to date on changes in the device. If the device doesn’t have much data, then a simple ongoing poll is sufficient, but the device must be able to handle this at a fairly rapid rate without choking. If it cannot, or if it has a lot of data, it should support asynchronous notifications of changes to the controller.\n\nThese issues are discussed in detail in the Flow Control section above.\n\nThe controller is providing a means to get information from and send commands to the device. It is a fairly universal rule that the controller cannot send another query or command to the device until the device has acknowledged that it has completed the last one. If the device doesn’t support asynchronous notifications, then the controller has to actively poll the device state fairly rapidly in order to minimize latency (see latency discussion above.) If the device is slow to response, then outgoing commands are often significantly delayed, in order to allow outstanding polling queries to complete first. This causes the automation system to feel spongy and slow.\n\nSo the device must provide rapid responses to the controller if the controller is going to provide a high quality experience to the end user. This can be problematic in some cases since many devices cannot multi-task. So the design of the device must not place the needs of the automation system last. It should be designed such that responses can be rapidly provided, or it must provide asynchronous notifications to avoid the issue.\n\nThere are a lot of devices that are of the sort that CQC calls ‘dead if off’. This means that they do allow the controller to power them off and on via the control protocol, but that when in the off state they won’t respond to anything but a power on command. This is a very bad design for really obvious reasons. All the controller knows about a device is what it can know through the control protocol. A device that doesn’t respond to queries is no different from a device that has been disconnected, taken outside, and run over with the lawn mower. From the controller’s point of view it doesn’t exist.\n\nTherefore, if it doesn’t in fact exist, i.e.. it has failed because the communications has been severed through some physical or configuration change, or networking problem, then the controller has absolutely no idea until the user asks power the device on. At that point, after the user has sat down with his Banana Daiquiri and bowl of popcorn to watch a movie, the controller tells him it’s not working. This is something that the controller should have been able to tell him within a minute of its having failed. Of course watching a movie isn’t a mission critical activity, at least to some people, but other such scenarios could certainly fall into that category.\n\nSo please do not design such a device. At the very least, allow it to respond with some sort of “Sorry, I’m powered off right now” response, so that the controller can be sure it’s still present and responding. And provide a guaranteed minimum response time by which you will at least provide some sort of response, so that the controller can know that any failure to respond within that period of time is obviously an indication of trouble.\n\nSome devices, even if they use a strict call and response protocol, will still require that the controller respect some minimum interval between messages it sends to the device, often on the order of 100 to 500 milliseconds, sometimes more. This is really a sub-optimal design and to be avoided if at all possible. The call and response scheme should be sufficient to provide the needed flow control.\n\nHowever, presumably because some devices are both non-multi-threading and don’t even have the ability to buffer up incoming bytes from the control interface (serial port, USB port, whatever) in an interrupt driven way, that means that the device must actively be reading the serial port or incoming bytes will be lost. So, even though they have responded to a previous command or query, they now need to time to tend to other housekeeping chores before they can get back to looking for control messages again.\n\nIf there is such a limitation, it should be kept to an absolute minimum interval, preferably closer to 100 milliseconds than one second."
    },
    {
        "link": "https://reddit.com/r/embedded/comments/1d45tel/what_are_your_best_practices_to_make_uart_comms",
        "document": "I have been doing uart comms the basic way for a loong time: start of packet byte, length byte, data with escaped control bytes and crc. But as time goes on, I'm running into more and more challenging situations where this fails in critical scenarios. Sometimes the HW RX FIFO can't be cleared in time, for example. Other things happen sometimes that make my comms lose synchronization for a random length of time.\n\nSo what techniques do you use to bullet-proof your uart comms? I have read about using sequence numbers, end of packet bytes, and longer start of packet sequences. How well do these work for people? Are they worth it to implement in critical scenarios? Is there a comms library out there that helps with this?"
    },
    {
        "link": "https://codilime.com/blog/evolution-management-protocols-network-devices",
        "document": "Connecting and powering network devices (be it in the form of physical hardware or virtualized/containerized functions) is not enough to make them operational - the devices have to be configured before using them to forward / filter / load balance, etc. the traffic. And most likely, later one will need to reconfigure, monitor, diagnose and troubleshoot faults to ensure the network is always available, reliable, and secure. All those tasks have one thing in common - network management. As its name denotes, network management is the process of administering, monitoring, and optimizing a network to ensure its efficient operation and performance. Network devices have a management plane (to compare different planes check out our article: Management vs. Control vs. Data Planes in a Network Device) allowing those actions to be performed.\n\nImagine a scenario where a company's network spans multiple locations with numerous routers, switches, servers, and other devices. Each of them needs to be monitored for performance, configured for specific tasks, and troubleshot in case of issues. Without a standardized method of communication between these devices and a centralized management system, this task would quickly become unmanageable. Hence network management protocols, which provide the necessary framework to interact with network devices in a structured and efficient manner.\n\nNetwork management protocols define the rules and procedures for information and commands exchanged between network devices and network management systems (NMS). These protocols specify how data is exchanged, what types of information can be requested or modified, and the security measures in place to protect this communication. Their goal is to allow managing (configuration, maintenance, troubleshooting, etc.) network devices in an efficient and secure manner and to provide tools to build resilient, scalable and performant networks. Typical tasks supported by these protocols include:\n• Detect and respond to network events such as failures and security breaches\n\nNetwork management protocols facilitate the implementation of network management systems, which serve as the centralized application, providing a single interface through which administrators can manage their network elements.\n\nNetwork management protocols often rely on data modeling language for the definition of data exchanged between device and application. Data modeling language specifies rules and conventions which defines the entities, their attributes, constraints, relationships and hierarchical organization of data.\n\nNowadays, one of the most important data modeling languages is YANG (Yet Another Next Generation). It was developed as a data modeling language for the NETCONF protocol (YANG 1.0 specification was published in 2010 as RFC 6020 and the updated YANG 1.1 was published in 2016 as RFC 7950 ). However YANG is protocol independent and its data models can be used regardless of the underlying transport protocol.\n\nThere are a number of RFCs which define standard YANG data models. The OpenConfig project (open-source project with contributions from network operators, equipment vendors, etc.) is an important source of data models (for configuration and telemetry) which are vendor-neutral (written in YANG version 1.0). Additionally vendors create their own models or extend existing ones; e.g. OpenConfig models with data specific to their solutions.\n\nThe history of network management protocols is marked by a constant evolution to meet the growing demands of increasingly complex network devices. Early protocols laid the foundation for basic monitoring and management. However, as networks expanded and became more diverse (e.g. through virtualization or containerization of network functions), the limitations of those protocols became apparent. This led to the development of more advanced protocols that offer features such as improved security, support for more complex configurations, and better integration with modern software development practices.\n\nToday there exist numerous network management protocols, including both proprietary ones created by a specific vendor (and supported within their respective ecosystems) and industry-standard protocols supported by many vendors and applications.\n\nIn the forthcoming section, we delve into the world of network management protocols, exploring their features, strengths, weaknesses, and real-world applications. Our goal is to provide a comparison of the prominent network management protocols: SNMP, NETConf, RESTConf, and gNMI.\n\nSNMP stands for Simple Network Management Protocol. It is the first widely recognized network management protocol and was initially developed in the 1980s, with its first specification (known as SNMPv1) published in 1988. Since then new versions, namely SNMPv2 and SNMPv3, have been developed and published. The main characteristics of those versions are:\n• Added features like GETBULK for more efficient data retrieval.\n• SNMPv2c (commonly known as \"Community-based SNMPv2”) - version introduced for easier configuration (instead of the security model proposed by SNMP v2, uses community-base security scheme from SNMP v1), considered as the de facto SNMPv2 standard.\n• Adds enhanced security with the inclusion of authentication and encryption.\n• Includes support for User-Based Security Model (USM) for message security and the View-Based Access Control Model (VACM) for access control.\n\nNowadays it is common that all three versions are supported by networking devices.\n\nIn SNMP one can distinguish two types of elements:\n• SNMP Agent - runs on managed devices (like switches or routers) and has knowledge about the device and its operations. It answers requests (GET requests) and generates notifications (TRAP messages) for specific events.\n• SNMP Manager - software responsible for network management and monitoring through the SNMP protocol. It communicates with SNMP agents using GET, SET, and other SNMP operations.\n\nSNMP uses Structured Management Information - SMI (a subset of Abstract Syntax Notation One, in short ASN.1) to define its notation (type of data) and employs ASN.1 BER (Basic Encoding Rules) to encode messages. SNMP stores its data in a Management Information Base (MIB). MIBs specify the structure of data, which is organized in hierarchical, tree-like structures, with each node representing a different aspect of the device or network being managed. In SNMP, a managed object is represented as a variable, and the Object Identifier (OID) is used to uniquely identify each variable. In that notation each node in the tree is represented by a series of integers separated by dots, with each integer representing a sub-branch in the tree. For example 1.3.6.1.4.1.2021.4.5.0 is the OID for total RAM in a machine. Aside from the standard OIDs, MIBs can contain vendor-specific ones (supported/available only on devices from a given vendor).\n\nSNMP Agents use OIDs to report the value of managed objects to the SNMP Manager, and the Manager uses OIDs to retrieve particular values from the agent. Likewise, SNMP Manager can configure a network device by setting the values of a chosen parameters (through usage of OID). Additionally SNMP supports notifications (SNMP TRAP) when SNMP Agent sends information to SNMP Manager about certain events (e.g. a change of value of a given OID).\n\nSNMP primarily uses UDP (User Datagram Protocol) as its transport protocol. SNMP agent receives requests on UDP port 161, notifications (TRAP messages) are sent to SNMP manager port 162. SNMP can also use TCP (Transmission Control Protocol) for some operations; SNMPv3 in particular allows for the use of TCP as a transport protocol.\n\nUsage of SNMP as a network management protocol protocol has a number of benefits. However SNMP has its own limitations and drawbacks. One of them is the lack of standard discovery mechanism to find proper MIB modules for a given network device - a network administrator has to manually configure the OIDs of the devices within the monitoring system (in such cases, one can for example employ snmpwalk to retrieve a subtree of management values). It is worth noting that standard MIBs are also available, not tied to a specific vendor and device (proprietary MIBs typically extend the exposed properties of functionality of the devices of a given vendor).\n\nAnother limitation stems from the fact that SNMP has not been designed to retrieve huge amounts of data efficiently. Due to performance limitations when polling, SNMP is not well suited when managing large networks. Also it has limited filtering and retrieval capabilities. SNMP's primary focus is on monitoring, and its configuration capabilities are limited compared to more modern protocols.\n\nOne of the main concerns related to SNMP is security. From that perspective, only SNMPv3 should be used as it provides authentication and payload encryption.\n\nNETCONF (Network Configuration Protocol) is an IETF standard protocol described and published in 2006 in RFC 4741 (and later superseded by RFC 6241 with updates and extensions provided by RFC 6242 , RFC 7803 , RFC 8526 ). It was designed to address the limitations of older protocols, such as SNMP, by providing a more powerful and flexible way to configure, manage and automate network devices.\n\nIn the NETCONF protocol four layers are distinguished:\n\nThese layers have the following roles:\n• The Secure Transport layer provides a secure and reliable communication channel between client and server. NETCONF can use different transport protocols with SSH-based being the most common.\n• The Messages layer provides a transport-independent framing mechanism for encoding remote procedure calls (RPCs) and notifications.\n• The Operations layer specifies a set of base protocol operations which are run as RPC calls. Among others they include: , , , , , supporting a range of configuration and management tasks. Additionally, clients and servers may extend the base set of operations (customizations are supported).\n• The Content layer is responsible for configuration and notification data exchanged between devices.\n\nNETCONF follows the client-server model, where a client can be, for example, an application running on a separate system and the server is the network device implementing NETCONF. Communication between client and server is based on Remote Procedure Calls (RPC). The NETCONF client initiates operations to manage network devices, while the server responds to client requests.\n\nNETCONF distinguishes configuration data and state data (e.g. read-only status information and statistics). The protocol allows checking the configuration before applying it. This is possible thanks to the support of multiple datastores (like running, startup, and candidate, a vendor may implement different datastores). When a configuration from the candidate store can be applied and copied to the running datastore after being validated (and if validation fails it is not applied). Having different datastores allows managing each of them separately.\n\nOne of the strengths of NETCONF is the fact it is transaction-oriented, which allows atomic changes in configuration, meaning that when configuration changes are pushed to the network device(s), they are applied only if there are no errors / all operations succeed, otherwise all changes are withdrawn (there is no partial configuration change in such situation). This also means that a single transaction can contain different configuration operations and be pushed to multiple devices and be applied only if successful on all of them (e.g. the addition of a new VLAN on several switches, setting options for network interface, etc.)\n\nThe NETCONF protocol uses XML-based data models to describe device configuration and state information with YANG as a data modeling language. This facilitates writing automation scripts and tools that can interact with network devices and perform tasks such as configuration management, monitoring, and troubleshooting.\n\nServers (run on network devices) can send asynchronous notifications to clients about specific events, such as changes in device state or exceeded thresholds. These notifications, defined using YANG models, provide real-time updates to network management systems, aiding in proactive monitoring and troubleshooting.\n\nNETCONF uses a secure channel (using SSH / Transport Layer Security) for communication between client and server with authentication, encryption, and data integrity.\n\nWhile NETCONF offers significant advantages, it might be considered complex as it requires understanding YANG data modeling and XML messaging as well as XML-based messages which, though verbose, potentially increase bandwidth usage.\n\nRESTCONF is specified in RFC 8040 where is described as, “an HTTP-based protocol that provides a programmatic interface for accessing data defined in YANG, using the datastore concepts defined in NETCONF\".\n\nRESTCONF has been created to provide the HTTP-based REST API, allowing access to configuration data, state data, data-model-specific RPC operations, and event notifications in the networking device. REST stands for representational state transfer and is a type of software architecture that has been designed to ensure interoperability between different web applications. Nowadays it is one of the most common approaches used in internet systems. This also means that many automation solutions use that design principle. From that perspective, RESTCONF simplifies management tasks as it enables automation of network infrastructure using familiar RESTful API patterns.\n\nRESTCONF uses HTTP methods to provide CRUD (CREATE, READ, UPDATE and DELETE) operations on datastores containing YANG-defined data (compatible with a server that implements NETCONF datastores). This means that the operations applied by RESTCONF are the standard HTTP methods such as GET, POST, PUT, PATCH and DELETE.\n\nThe protocol works on hierarchical resources, where each resource is a manageable entity within the network device. A resource is defined as a collection of data and available methods on that data. Examples of resources include device configurations, interfaces, routing tables, and system status. Resources are identified and accessible by URIs (Uniform Resource Identifiers), e.g. https://example.com/restconf/data/ietf-interfaces:interfaces/interface=eth1 .\n\nRESTCONF uses client-server architecture and is stateless (operations are one-and-done, no active sessions are kept open with the client).\n\nRESTCONF does not support the concept of transactions. In fact, each RESTCONF call (be it GET, POST, PUT, PATCH and DELETE) is a transaction in itself and is directly applied. As a logical consequence, distributed transactions are not available - configuration is done operation by operation and device by device. Nor is a locking mechanism supported.\n\nRESTCONF does not provide validation options without activating a given configuration (validation is implicit, part of the RESTCONF calls which succeeds or fails).\n\nThe RESTCONF protocol allows data to be encoded with either JSON or XML. As a transport, HTTP or HTTPS is used.\n\nRESTCONF with its web-friendly architecture, use of standard HTTP methods, and lightweight data encoding make it a powerful tool for network administrators and developers. Its stateless nature allows it to scale effectively to manage large and complex networks. However, when compared to NETCONF, RESTCONF offers a less comprehensive set of operations for advanced configuration management (e.g. the handling of transactional operations and rollbacks is challenging). The verbose nature of JSON or XML encoding can result in increased bandwidth usage, especially in environments with frequent configuration changes (which might be an issue, especially in bandwidth-constrained use-cases).\n\ngNMI (gRPC Network Management Interface) is a network management protocol that leverages gRPC (framework implementing remote procedure call) to set, edit, and delete the configuration of network devices, and also to view operational data. The protocol is defined by the OpenConfig Working Group (gNMI specification ) and allows for configuration and telemetry data exchange between network devices and management systems.\n\ngRPC is an open-source remote procedure call (RPC) framework, initially developed by Google. It uses HTTP/2 as its transport protocol, allowing for efficient communication between clients and servers. gRPC offers features such as bi-directional streaming, authentication, and multiplexing.\n\nProtocol Buffers (aka protobuf) provides the data serialization in gRPC. The resulting data can be characterized by its compact binary representation, which significantly reduces the size of data payloads. This reduction in size translates to decreased bandwidth usage, thereby optimizing the efficiency of data transmission across the network. The schema of exchanged data is defined in protobuf .proto files, providing a clear specification of the structure and types of data elements.\n\nMoreover, Protocol Buffers are seamlessly integrated with YANG data models, which serve as the foundation for defining the structure of network configurations and operational data. In this collaboration, YANG models delineate the detailed structure and semantics of the data, while Protocol Buffers are responsible for the tasks of serialization and encoding.\n\ngNMI uses client-server architecture (with clients establishing a connection with gNMI servers). It implements four RPC methods:\n• Capabilities - exchanged by the client and server in the initial handshake to share capability information.\n• Get - used to retrieve configuration or operational data from network devices (gNMI server) by client.\n• Set - enables clients to modify configurations on network devices, clients send desired configuration changes, which are applied atomically on the device(s).\n• Subscribe - allows controlling subscriptions to data on the network devices by the client. Provides real-time telemetry streaming from devices to clients, which subscribe to specific paths and receive updates when the data changes.\n\ngNMI supports transitions - when a client sends a Set request to modify configurations on a network device, it can include multiple configuration changes within a single gNM request. These changes are bundled together, ensuring that either all changes are successfully applied or none at all.\n\nIn conclusion, gNMI stands as a contemporary network management protocol offering a unified service for state management, encompassing both streaming telemetry and configuration operations. gNMI boasts efficient data serialization capabilities, resulting in significantly smaller data sizes compared to protocols such as NETCONF. This efficiency is particularly advantageous for managing significant volumes of data and accommodating numerous concurrent connections, making it well-suited for large and dynamic network infrastructures.\n\nThe protocol's support for streaming telemetry is an important advantage, enabling continuous and real-time updates without the need for frequent polling. This feature enhances the responsiveness of network monitoring and facilitates timely decision-making based on up-to-date data. However, it is essential to note that implementing gNMI requires a degree of familiarity with the underlying technologies it relies upon, such as gRPC and Protocol Buffers. While these technologies offer powerful capabilities, they may present a learning curve for those new to them. Additionally, as a newer protocol in the network management landscape, gNMI may not yet have achieved the widespread adoption of more established protocols like SNMP or NETCONF.\n\nWhat is worth noting is that the OpenConfig working group has defined the specifications for two related management protocols:\n\nThey can be considered as extensions to the gRPC-based network management framework, providing specific functionalities tailored to network operations (gNOI allows execution of operational commands on network devices) and routing information management (gRIBI provides an API for adding and deleting routing entries).\n\nFor a comprehensive comparison of the above-considered network management protocols, the table below presents how they compare across key aspects.\n\nEach protocol offers some unique features, advantages and weaknesses, catering to different aspects of network management. SNMP remains a base protocol for network monitoring, with widespread adoption. However, it also poses important security challenges and has limited efficiency. NETCONF and RESTCONF provide standardized approaches to network configuration with YANG used as a modeling language. NETCONF offers transaction support and structured data handling. gNMI stands out as a modern protocol, leveraging gRPC for efficient communication and Protocol Buffers for data serialization. Its support for streaming telemetry and transactions makes it ideal for large and dynamic networks.\n\nWhen determining the optimal network management protocol for a given environment, it is essential to first assess the compatibility with the network elements. Each use case requires individual consideration. A general approach may involve the following considerations:\n• SNMP remains a widely adopted protocol, supported by numerous network devices. It is well-suited for traditional network environments focused on monitoring. However, taking into account security, only SNMPv3 should be considered.\n• For organizations transitioning to more modern infrastructures, both NETCONF and RESTCONF offer structured configuration capabilities. RESTCONF, with its RESTful design, presents a more web-friendly approach, while NETCONF is more comprehensive and provides support for more configuration options.\n• In dynamic and large networks, gNMI emerges as a compelling option. Its support for transactions and efficient data encoding ensures superior scalability and performance. Although, as a relatively new protocol, its support might be more limited.\n\nManagement protocols act as a bridge between network management systems and network devices, enabling efficient communication, centralized control, and ultimately, effective network management.\n\nThe evolution of network management protocols reflects the ever-changing landscape of networking requirements. While SNMP laid the foundation for network management, protocols like NETConf, RESTConf, and gNMI have emerged to address the limitations and cater to the demands of modern, dynamic networks. The choice of protocol depends on factors such as network complexity, scalability, security, and the need for real-time updates. As technology continues to advance, network management protocols will likely evolve further to meet the challenges of managing complex and diverse network infrastructures.\n\nThe coexistence of various protocols will almost certainly continue, as different environments have different requirements. One of the notable trends is the move towards a more standardized and efficient approach to network management. This is also highlighted by initiatives like OpenConfig, which not only defines network management protocol (gNMI as well as gNOI and gRIBI) but also common, vendor-neutral data models (YANG based).\n\nIn conclusion, the landscape of network management protocols is dynamic and diverse. Network administrators should assess their specific needs, consider the advantages and limitations of each protocol, and adopt a strategy that aligns with their network's requirements. With the right mix of protocols and technologies, they can navigate the complexities of modern networks with efficiency and agility."
    },
    {
        "link": "https://dynapar.com/technology/encoder_basics/encoder_protocol_interfaces",
        "document": ""
    },
    {
        "link": "https://lenovo.com/us/en/glossary/what-is-encode?srsltid=AfmBOoqasaRU6Wg709im_XsEsQ2LslfrxXdOWtE3H_E97KbbzcVdTLIF",
        "document": "Encode refers to the process of converting information or data into a specific format for various purposes. It's a crucial concept in technology and computing, playing a role in programming, data transmission, and more.\n\nHow does encoding work in programming?\n\nIn programming, encoding involves converting data into a specific format, often for storage or transmission. This process ensures that the information can be easily processed by computers. For instance, you might encode text in Unicode to represent characters in a standardized way, enabling cross-system compatibility.\n\nWhat is the role of encoding in data compression?\n\nEncoding plays a vital role in data compression by representing information in a more efficient manner. This reduces the amount of storage or bandwidth required. Popular techniques, like Huffman coding, use encoding to assign shorter codes to frequently occurring data patterns, optimizing space utilization.\n\nWhat role does encoding play in image and video processing?\n\nIn image and video processing, encoding involves representing visual data in a compressed format. This not only saves storage space but also facilitates faster transmission. Video codecs, such as H.264 or H.265, utilize encoding to compress video files, making streaming and storage more manageable.\n\nHow does encoding contribute to secure communication?\n\nEncoding is essential for secure communication, especially when transmitting sensitive data. Techniques like encryption use encoding to transform information into a format that is unintelligible without the proper decryption key. This safeguards data during transmission, ensuring only authorized parties can access and understand the content.\n\nWhat is the relationship between encoding and error detection/correction?\n\nEncoding plays a key role in error detection and correction. By adding redundancy to the encoded data, error-detecting codes can identify and potentially correct errors that may occur during transmission. This enhances the reliability of communication systems, ensuring data integrity even in the presence of disruptions.\n\nHow does encoding impact the representation of multimedia content on the web?\n\nWhen it comes to multimedia content on the web, encoding is crucial for efficient streaming. Video and audio files are often encoded using formats like MP3 or MP4 to reduce file sizes without compromising quality. This enables smoother playback and quicker loading times for online multimedia content.\n\nURL encoding involves converting special characters in a URL to a format that is compatible with web standards. This ensures that the URL can be transmitted and interpreted correctly. For example, spaces are encoded as \"%20,\" allowing browsers to handle and display URLs with special characters properly.\n\nHow does encoding relate to character sets in programming?\n\nIn programming, character sets define the repertoire of characters that can be used. Encoding comes into play to represent these characters digitally. Unicode, for instance, is a widely used encoding standard that assigns unique codes to characters from various writing systems, fostering a standardized way of handling text in different languages.\n\nWhat role does encoding play in networking protocols?\n\nNetworking protocols often rely on encoding to structure and transmit data between devices. For instance, in the transmission control protocol (TCP), data is encoded into packets for efficient and reliable communication across networks. This ensures that information is transmitted accurately and reassembled correctly at the receiving end.\n\nHow does encoding contribute to the functionality of quick response (QR) codes?\n\nQR codes use encoding to represent data, such as text or uniform resource locators (URLs), in a two-dimensional barcode. This encoding enables quick and reliable scanning by devices like smartphones. The structured format of QR codes, achieved through encoding, allows for easy retrieval of information encoded within the pattern.\n\nHow is encoding used in database systems?\n\nIn database systems, encoding is employed to store and retrieve data efficiently. Character encoding ensures that text data is represented consistently, preventing issues with data retrieval and display. Additionally, encoding techniques may be applied for data compression, optimizing storage space and retrieval times.\n\nHow does encoding impact the functionality of email systems?\n\nEmail systems utilize encoding to handle various types of content within messages. Multipurpose internet mail extensions (MIME) encoding, for instance, enables the inclusion of multimedia elements in emails. This ensures that attachments, images, and other content are transmitted and displayed correctly across different email clients.\n\nWhat is the significance of base64 encoding in web development?\n\nBase64 encoding is commonly used in web development to represent binary data, such as images or files, as American standard code for information interchange (ASCII) text. This facilitates the inclusion of binary data within text-based formats like JavaScript object notation (JSON) or eXtensible markup language (XML). Base64 encoding is a simple and widely supported method for embedding multimedia content directly into web pages or data exchanges.\n\nHow does encoding contribute to the security of password storage?\n\nIn password storage, encoding is applied for security purposes. Hashing algorithms, which involve one-way encoding, are commonly used to convert passwords into irreversible, unique strings of characters. This adds a layer of protection, as even if the encoded data is compromised, it is challenging to reverse-engineer and obtain the original password.\n\nWhat is the role of encoding in the execution of machine code by processors?\n\nMachine code, the low-level language understood by processors, relies on encoding to represent instructions. Each instruction is encoded into a specific binary format that the processor can interpret and execute. This encoding ensures that the processor accurately performs the intended operations, forming the basis for the execution of computer programs.\n\nHow does encoding contribute to the interoperability of different software applications?\n\nInteroperability between software applications is enhanced through standardized encoding formats. For example, JavaScript object notation (JSON) or eXtensible markup language (XML) serve as encoding formats for data interchange between different systems. This allows diverse software applications to communicate seamlessly, as they can interpret, and process data encoded in a universally understood format.\n\nWhat role does encoding play in the implementation of security certificates on the internet?\n\nSecurity certificates on the internet use encoding to secure communications through protocols like hypertext transfer protocol secure (HTTPS). The certificates, encoded in formats like X.509, contain cryptographic information. This encoding ensures that sensitive data, such as private keys and identity information, is transmitted securely between users and websites, safeguarding against unauthorized access and malicious activities.\n\nHow does encoding aid storage and retrieval in cloud computing?\n\nIn cloud computing, encoding is employed to structure and store data efficiently. Encoding techniques optimize data for storage, making it more economical and scalable. Additionally, during data retrieval, encoding ensures that information is accurately reconstructed, maintaining the integrity of stored data in the cloud environment.\n\nHow does encoding influence the development of virtual reality (VR) applications?\n\nIn VR applications, encoding is crucial for rendering realistic experiences. Video and audio content undergo encoding to be efficiently processed and transmitted to VR devices. This ensures that the immersive environments in VR applications are displayed with high quality and minimal latency, enhancing the overall user experience."
    }
]