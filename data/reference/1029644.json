[
    {
        "link": "https://keras.io/guides/sequential_model",
        "document": "Author: fchollet\n\n Date created: 2020/04/12\n\n Last modified: 2023/06/25\n\n Description: Complete guide to the Sequential model.\n\nWhen to use a Sequential model\n\nA model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n\nis equivalent to this function:\n\nA Sequential model is not appropriate when:\n• Your model has multiple inputs or multiple outputs\n• Any of your layers has multiple inputs or multiple outputs\n• You need to do layer sharing\n\nYou can create a Sequential model by passing a list of layers to the Sequential constructor:\n\nIts layers are accessible via the attribute:\n\nYou can also create a Sequential model incrementally via the method:\n\nNote that there's also a corresponding method to remove layers: a Sequential model behaves very much like a list of layers.\n\nAlso note that the Sequential constructor accepts a argument, just like any layer or model in Keras. This is useful to annotate TensorBoard graphs with semantically meaningful names.\n\nSpecifying the input shape in advance\n\nGenerally, all layers in Keras need to know the shape of their inputs in order to be able to create their weights. So when you create a layer like this, initially, it has no weights:\n\nIt creates its weights the first time it is called on an input, since the shape of the weights depends on the shape of the inputs:\n\nNaturally, this also applies to Sequential models. When you instantiate a Sequential model without an input shape, it isn't \"built\": it has no weights (and calling results in an error stating just this). The weights are created when the model first sees some input data:\n\nOnce a model is \"built\", you can call its method to display its contents:\n\nHowever, it can be very useful when building a Sequential model incrementally to be able to display the summary of the model so far, including the current output shape. In this case, you should start your model by passing an object to your model, so that it knows its input shape from the start:\n\nNote that the object is not displayed as part of , since it isn't a layer:\n\nModels built with a predefined input shape like this always have weights (even before seeing any data) and always have a defined output shape.\n\nIn general, it's a recommended best practice to always specify the input shape of a Sequential model in advance if you know what it is.\n\nWhen building a new Sequential architecture, it's useful to incrementally stack layers with and frequently print model summaries. For instance, this enables you to monitor how a stack of and layers is downsampling image feature maps:\n\nWhat to do once you have a model\n\nOnce your model architecture is ready, you will want to:\n• Train your model, evaluate it, and run inference. See our guide to training & evaluation with the built-in loops\n• Save your model to disk and restore it. See our guide to serialization & saving.\n\nOnce a Sequential model has been built, it behaves like a Functional API model. This means that every layer has an and attribute. These attributes can be used to do neat things, like quickly creating a model that extracts the outputs of all intermediate layers in a Sequential model:\n\nHere's a similar example that only extract features from one layer:\n\nTransfer learning consists of freezing the bottom layers in a model and only training the top layers. If you aren't familiar with it, make sure to read our guide to transfer learning.\n\nHere are two common transfer learning blueprint involving Sequential models.\n\nFirst, let's say that you have a Sequential model, and you want to freeze all layers except the last one. In this case, you would simply iterate over and set on each layer, except the last one. Like this:\n\nAnother common blueprint is to use a Sequential model to stack a pre-trained model and some freshly initialized classification layers. Like this:\n\nIf you do transfer learning, you will probably find yourself frequently using these two patterns.\n\nThat's about all you need to know about Sequential models!\n\nTo find out more about building models in Keras, see:\n• Guide to making new Layers & Models via subclassing"
    },
    {
        "link": "https://faroit.com/keras-docs/2.0.9",
        "document": "You have just found Keras.\n\nKeras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n\nUse Keras if you need a deep learning library that:\n• Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n• Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n• User friendliness. Keras is an API designed for human beings, not machines. It puts user experience front and center. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error.\n• Modularity. A model is understood as a sequence or a graph of standalone, fully-configurable modules that can be plugged together with as little restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions, regularization schemes are all standalone modules that you can combine to create new models.\n• Easy extensibility. New modules are simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making Keras suitable for advanced research.\n• Work with Python. No separate models configuration files in a declarative format. Models are described in Python code, which is compact, easier to debug, and allows for ease of extensibility.\n\nThe core data structure of Keras is a model, a way to organize layers. The simplest type of model is the model, a linear stack of layers. For more complex architectures, you should use the Keras functional API, which allows to build arbitrary graphs of layers.\n\nHere is the model:\n\nStacking layers is as easy as :\n\nOnce your model looks good, configure its learning process with :\n\nIf you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code).\n\nYou can now iterate on your training data in batches:\n\nAlternatively, you can feed batches to your model manually:\n\nEvaluate your performance in one line:\n\nOr generate predictions on new data:\n\nBuilding a question answering system, an image classification model, a Neural Turing Machine, or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful?\n\nFor a more in-depth tutorial about Keras, you can check out:\n• Getting started with the Sequential model\n• Getting started with the functional API\n\nIn the examples folder of the repository, you will find more advanced models: question-answering with memory networks, text generation with stacked LSTMs, etc.\n\nBefore installing Keras, please install one of its backend engines: TensorFlow, Theano, or CNTK. We recommend the TensorFlow backend.\n\nYou may also consider installing the following optional dependencies:\n• cuDNN (recommended if you plan on running Keras on GPU).\n• HDF5 and h5py (required if you plan on saving Keras models to disk).\n• graphviz and pydot (used by visualization utilities to plot model graphs).\n\nThen, you can install Keras itself. There are two ways to install Keras:\n\nIf you are using a virtualenv, you may want to avoid using sudo:\n\nThen, to the Keras folder and run the install command:\n\nSwitching from TensorFlow to CNTK or Theano\n\nBy default, Keras will use TensorFlow as its tensor manipulation library. Follow these instructions to configure the Keras backend.\n\nYou can ask questions and join the development discussion:\n• On the Keras Slack channel. Use this link to request an invitation to the channel.\n\nYou can also post bug reports and feature requests (only) in Github issues. Make sure to read our guidelines first.\n\nWhy this name, Keras?\n\nKeras (κέρας) means horn in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the Odyssey, where dream spirits (Oneiroi, singular Oneiros) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It's a play on the words κέρας (horn) / κραίνω (fulfill), and ἐλέφας (ivory) / ἐλεφαίρομαι (deceive).\n\nKeras was initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System)."
    },
    {
        "link": "https://keras.io/api/layers/core_layers/dense",
        "document": "implements the operation: where is the element-wise activation function passed as the argument, is a weights matrix created by the layer, and is a bias vector created by the layer (only applicable if is ).\n\nNote: If the input to the layer has a rank greater than 2, computes the dot product between the and the along the last axis of the and axis 0 of the (using ). For example, if input has dimensions , then we create a with shape , and the operates along axis 2 of the , on every sub-tensor of shape (there are such sub-tensors). The output in this case will have shape .\n• activation: Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: ).\n• use_bias: Boolean, whether the layer uses a bias vector.\n• activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\").\n• lora_rank: Optional integer. If set, the layer's forward pass will implement LoRA (Low-Rank Adaptation) with the provided rank. LoRA sets the layer's kernel to non-trainable and replaces it with a delta over the original kernel, obtained via multiplying two lower-rank trainable matrices. This can be useful to reduce the computation cost of fine-tuning large dense layers. You can also enable LoRA on an existing layer by calling .\n\nN-D tensor with shape: . The most common situation would be a 2D input with shape .\n\nN-D tensor with shape: . For instance, for a 2D input with shape , the output would have shape ."
    },
    {
        "link": "https://faroit.com/keras-docs/2.0.9/getting-started/sequential-model-guide",
        "document": "Getting started with the Keras Sequential model\n\nThe model is a linear stack of layers.\n\nYou can create a model by passing a list of layer instances to the constructor:\n\nYou can also simply add layers via the method:\n\nThe model needs to know what input shape it should expect. For this reason, the first layer in a model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. There are several possible ways to do this:\n• Pass an argument to the first layer. This is a shape tuple (a tuple of integers or entries, where indicates that any positive integer may be expected). In , the batch dimension is not included.\n• Some 2D layers, such as , support the specification of their input shape via the argument , and some 3D temporal layers support the arguments and .\n• If you ever need to specify a fixed batch size for your inputs (this is useful for stateful recurrent networks), you can pass a argument to a layer. If you pass both and to a layer, it will then expect every batch of inputs to have the batch shape .\n\nAs such, the following snippets are strictly equivalent:\n\nBefore training a model, you need to configure the learning process, which is done via the method. It receives three arguments:\n• An optimizer. This could be the string identifier of an existing optimizer (such as or ), or an instance of the class. See: optimizers.\n• A loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as or ), or it can be an objective function. See: losses.\n• A list of metrics. For any classification problem you will want to set this to . A metric could be the string identifier of an existing metric or a custom metric function.\n\nKeras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the function. Read its documentation here.\n\nHere are a few examples to get you started!\n\nIn the examples folder, you will also find example models for real datasets:\n\nIn this model, we stack 3 LSTM layers on top of each other, making the model capable of learning higher-level temporal representations.\n\nThe first two LSTMs return their full output sequences, but the last one only returns the last step in its output sequence, thus dropping the temporal dimension (i.e. converting the input sequence into a single vector).\n\nA stateful recurrent model is one for which the internal states (memories) obtained after processing a batch of samples are reused as initial states for the samples of the next batch. This allows to process longer sequences while keeping computational complexity manageable.\n\nYou can read more about stateful RNNs in the FAQ."
    },
    {
        "link": "https://github.com/keras-team/keras/releases",
        "document": "OpenVINO is now available as an infererence-only Keras backend. You can start using it by setting the field to in your config file.\n\nOpenVINO is a deep learning inference-only framework tailored for CPU (x86, ARM), certain GPUs (OpenCL capable, integrated and discrete) and certain AI accelerators (Intel NPU).\n\nBecause OpenVINO does not support gradients, you cannot use it for training (e.g. ) -- only inference. You can train your models with the JAX/TensorFlow/PyTorch backends, and when trained, reload them with the OpenVINO backend for inference on a target device supported by OpenVINO.\n\nYou can now export your Keras models to the ONNX format from the JAX, TensorFlow, and PyTorch backends.\n\nJust pass in your call:\n\nIt's now possible to easily integrate Keras models into Sciki-Learn pipelines! The following wrapper classes are available:\n• Add support for and with the PyTorch backend, supporting both the TF SavedModel format and the ONNX format.\n• @LavanyaKV1234 made their first contribution in #20553\n• @jakubxy08 made their first contribution in #20563\n• @dhantule made their first contribution in #20565\n• @roebel made their first contribution in #20575\n• @Surya2k1 made their first contribution in #20613\n• @edge7 made their first contribution in #20584\n• @adrinjalali made their first contribution in #20599\n• @mmicu made their first contribution in #20655\n• @rkazants made their first contribution in #19727\n• @lkk7 made their first contribution in #20682\n• @Furkan-rgb made their first contribution in #20684\n• @punkeel made their first contribution in #20694\n• @kas2020-commits made their first contribution in #20709"
    },
    {
        "link": "https://scikit-learn.org/stable/modules/multiclass.html",
        "document": "This section of the user guide covers functionality related to multi-learning problems, including multiclass, multilabel, and multioutput classification and regression.\n\nThe modules in this section implement meta-estimators, which require a base estimator to be provided in their constructor. Meta-estimators extend the functionality of the base estimator to support multi-learning problems, which is accomplished by transforming the multi-learning problem into a set of simpler problems, then fitting one estimator per problem.\n\nThis section covers two modules: and . The chart below demonstrates the problem types that each module is responsible for, and the corresponding meta-estimators that each module provides.\n\nThe table below provides a quick reference on the differences between problem types. More detailed explanations can be found in subsequent sections of this guide.\n\nBelow is a summary of scikit-learn estimators that have multi-learning support built-in, grouped by strategy. You don’t need the meta-estimators provided by this section if you’re using one of these estimators. However, meta-estimators can provide additional strategies beyond what is built-in:\n\nAll classifiers in scikit-learn do multiclass classification out-of-the-box. You don’t need to use the module unless you want to experiment with different multiclass strategies. Multiclass classification is a classification task with more than two classes. Each sample can only be labeled as one class. For example, classification using features extracted from a set of images of fruit, where each image may either be of an orange, an apple, or a pear. Each image is one sample and is labeled as one of the 3 possible classes. Multiclass classification makes the assumption that each sample is assigned to one and only one label - one sample cannot, for example, be both a pear and an apple. While all scikit-learn classifiers are capable of multiclass classification, the meta-estimators offered by permit changing the way they handle more than two classes because this may have an effect on classifier performance (either in terms of generalization error or required computational resources).\n• None 1d or column vector containing more than two discrete values. An example of a vector for 4 samples:\n• None Dense or sparse binary matrix of shape with a single sample per row, where each column represents one class. An example of both a dense and sparse binary matrix for 4 samples, where the columns, in order, are apple, orange, and pear: For more information about , refer to Transforming the prediction target (y). The one-vs-rest strategy, also known as one-vs-all, is implemented in . The strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and only one classifier, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy and is a fair default choice. Below is an example of multiclass learning using OvR: also supports multilabel classification. To use this feature, feed the classifier an indicator matrix, in which cell [i, j] indicates the presence of label j in sample i. constructs one classifier per pair of classes. At prediction time, the class which received the most votes is selected. In the event of a tie (among two classes with an equal number of votes), it selects the class with the highest aggregate classification confidence by summing over the pair-wise classification confidence levels computed by the underlying binary classifiers. Since it requires to fit classifiers, this method is usually slower than one-vs-the-rest, due to its O(n_classes^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which don’t scale well with . This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used times. The decision function is the result of a monotonic transformation of the one-versus-one classification. Below is an example of multiclass learning using OvO: Error-Correcting Output Code-based strategies are fairly different from one-vs-the-rest and one-vs-one. With these strategies, each class is represented in a Euclidean space, where each dimension can only be 0 or 1. Another way to put it is that each class is represented by a binary code (an array of 0 and 1). The matrix which keeps track of the location/code of each class is called the code book. The code size is the dimensionality of the aforementioned space. Intuitively, each class should be represented by a code as unique as possible and a good code book should be designed to optimize classification accuracy. In this implementation, we simply use a randomly-generated code book as advocated in although more elaborate methods may be added in the future. At fitting time, one binary classifier per bit in the code book is fitted. At prediction time, the classifiers are used to project new points in the class space and the class closest to the points is chosen. In , the attribute allows the user to control the number of classifiers which will be used. It is a percentage of the total number of classes. A number between 0 and 1 will require fewer classifiers than one-vs-the-rest. In theory, is sufficient to represent each class unambiguously. However, in practice, it may not lead to good accuracy since is much smaller than . A number greater than 1 will require more classifiers than one-vs-the-rest. In this case, some classifiers will in theory correct for the mistakes made by other classifiers, hence the name “error-correcting”. In practice, however, this may not happen as classifier mistakes will typically be correlated. The error-correcting output codes have a similar effect to bagging. Below is an example of multiclass learning using Output-Codes:\n\nMultilabel classification (closely related to multioutput classification) is a classification task labeling each sample with labels from possible classes, where can be 0 to inclusive. This can be thought of as predicting properties of a sample that are not mutually exclusive. Formally, a binary output is assigned to each class, for every sample. Positive classes are indicated with 1 and negative classes with 0 or -1. It is thus comparable to running binary classification tasks, for example with . This approach treats each label independently whereas multilabel classifiers may treat the multiple classes simultaneously, accounting for correlated behavior among them. For example, prediction of the topics relevant to a text document or video. The document or video may be about one of ‘religion’, ‘politics’, ‘finance’ or ‘education’, several of the topic classes or all of the topic classes. A valid representation of multilabel is an either dense or sparse binary matrix of shape . Each column represents a class. The ’s in each row denote the positive classes a sample has been labeled with. An example of a dense matrix for 3 samples: Dense binary matrices can also be created using . For more information, refer to Transforming the prediction target (y). An example of the same in sparse matrix form: Multilabel classification support can be added to any classifier with . This strategy consists of fitting one classifier per target. This allows multiple target variable classifications. The purpose of this class is to extend estimators to be able to estimate a series of target functions (f1,f2,f3…,fn) that are trained on a single X predictor matrix to predict a series of responses (y1,y2,y3…,yn). You can find a usage example for as part of the section on Multiclass-multioutput classification since it is a generalization of multilabel classification to multiclass outputs instead of binary outputs. Classifier chains (see ) are a way of combining a number of binary classifiers into a single multi-label model that is capable of exploiting correlations among targets. For a multi-label classification problem with N classes, N binary classifiers are assigned an integer between 0 and N-1. These integers define the order of models in the chain. Each classifier is then fit on the available training data plus the true labels of the classes whose models were assigned a lower number. When predicting, the true labels will not be available. Instead the predictions of each model are passed on to the subsequent models in the chain to be used as features. Clearly the order of the chain is important. The first model in the chain has no information about the other labels while the last model in the chain has features indicating the presence of all of the other labels. In general one does not know the optimal ordering of the models in the chain so typically many randomly ordered chains are fit and their predictions are averaged together.\n\nMulticlass-multioutput classification (also known as multitask classification) is a classification task which labels each sample with a set of non-binary properties. Both the number of properties and the number of classes per property is greater than 2. A single estimator thus handles several joint classification tasks. This is both a generalization of the multilabel classification task, which only considers binary attributes, as well as a generalization of the multiclass classification task, where only one property is considered. For example, classification of the properties “type of fruit” and “colour” for a set of images of fruit. The property “type of fruit” has the possible classes: “apple”, “pear” and “orange”. The property “colour” has the possible classes: “green”, “red”, “yellow” and “orange”. Each sample is an image of a fruit, a label is output for both properties and each label is one of the possible classes of the corresponding property. Note that all classifiers handling multiclass-multioutput (also known as multitask classification) tasks, support the multilabel classification task as a special case. Multitask classification is similar to the multioutput classification task with different model formulations. For more information, see the relevant estimator documentation. Below is an example of multiclass-multioutput classification: At present, no metric in supports the multiclass-multioutput classification task. A valid representation of multioutput is a dense matrix of shape of class labels. A column wise concatenation of 1d multiclass variables. An example of for 3 samples:"
    },
    {
        "link": "https://geeksforgeeks.org/multiclass-classification-using-scikit-learn",
        "document": "Problem – Given a dataset of m training examples, each of which contains information in the form of various features and a label. Each label corresponds to a class, to which the training example belongs. In multiclass classification, we have a finite set of classes. Each training example also has n features.\n\nFor example, in the case of identification of different types of fruits, “Shape”, “Color”, “Radius” can be featured, and “Apple”, “Orange”, “Banana” can be different class labels.\n\nIn a multiclass classification, we train a classifier using our training data and use this classifier for classifying new examples.\n\nAim of this article – We will use different multiclass classification methods such as, KNN, Decision trees, SVM, etc. We will compare their accuracy on test data. We will perform all this with sci-kit learn (Python). For information on how to install and use sci-kit learn, visit http://scikit-learn.org/stable/\n• None Split the dataset into “training” and “test” data.\n• None Train Decision tree, SVM, and KNN classifiers on the training data.\n• None Use the above classifiers to predict labels for the test data.\n\nDecision tree classifier – A decision tree classifier is a systematic approach for multiclass classification. It poses a set of questions to the dataset (related to its attributes/features). The decision tree classification algorithm can be visualized on a binary tree. On the root and each of the internal nodes, a question is posed and the data on that node is further split into separate records that have different characteristics. The leaves of the tree refer to the classes in which the dataset is split. In the following code snippet, we train a decision tree classifier in scikit-learn.\n\nSVM (Support vector machine) classifier – \n\nSVM (Support vector machine) is an efficient classification method when the feature vector is high dimensional. In sci-kit learn, we can specify the kernel function (here, linear). To know more about kernel functions and SVM refer – Kernel function | sci-kit learn and SVM.\n\nKNN (k-nearest neighbors) classifier – KNN or k-nearest neighbors is the simplest classification algorithm. This classification algorithm does not depend on the structure of the data. Whenever a new example is encountered, its k nearest neighbors from the training data are examined. Distance between two examples can be the euclidean distance between their feature vectors. The majority class among the k nearest neighbors is taken to be the class for the encountered example.\n\nNaive Bayes classifier – Naive Bayes classification method is based on Bayes’ theorem. It is termed as ‘Naive’ because it assumes independence between every pair of features in the data. Let (x , x , …, x ) be a feature vector and y be the class label corresponding to this feature vector.\n\nApplying Bayes’ theorem,\n\nSince, x , x , …, x are independent of each other,\n\nInserting proportionality by removing the P(x , …, x ) (since it is constant).\n\nTherefore, the class label is decided by,\n\nP(y) is the relative frequency of class label y in the training dataset.\n\nIn the case of the Gaussian Naive Bayes classifier, P(x | y) is calculated as,"
    },
    {
        "link": "https://stackoverflow.com/questions/49416716/labelbinarizer-yields-different-result-in-multiclass-example",
        "document": "When executing the multiclass example in the scikit-learn tutorial\n\nThis is all fine. Now with one-hot encoding:\n\nI would expect the label binarizer to only encode the target, but not having an influence on the classifier. However it yields a different result:\n\nNotebook on Google Colab (where the same code yields yet a different error, strangely):"
    },
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html",
        "document": "Several regression and binary classification algorithms are available in scikit-learn. A simple way to extend these algorithms to the multi-class classification case is to use the so-called one-vs-all scheme.\n\nAt learning time, this simply consists in learning one regressor or binary classifier per class. In doing so, one needs to convert multi-class labels to binary labels (belong or does not belong to the class). makes this process easy with the transform method.\n\nAt prediction time, one assigns the class for which the corresponding model gave the greatest confidence. makes this easy with the method.\n\nRead more in the User Guide.\n\nTarget values. All sparse matrices are converted to CSR before inverse transformation. Threshold used in the binary and multi-label cases. Use 0 when contains the output of decision_function (classifier). Use 0.5 when contains the output of predict_proba. If None, the threshold is assumed to be half way between neg_label and pos_label. Target values. Sparse matrix will be of CSR format. In the case when the binary labels are fractional (probabilistic), chooses the class with the greatest value. Typically, this allows to use the output of a linear model’s decision_function method directly as the input of .\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect."
    },
    {
        "link": "https://medium.com/towards-data-science/comprehensive-guide-to-multiclass-classification-with-sklearn-127cc500f362",
        "document": "Learn how to tackle any multiclass classification problem with Sklearn. The tutorial covers how to choose a model selection strategy, several multiclass evaluation metrics and how to use them finishing off with hyperparameter tuning to optimize for user-defined metrics.\n\nEven though multi-class classification is not as common, it certainly poses a much bigger challenge than binary classification problems. You can literally take my word for it because this article has been the most challenging post I have ever written (have written close to 70).\n\nI found that the topic of multiclass classification is deep and full of nuances. I have read so many articles, read multiple StackOverflow threads, created a few of my own, and spent several hours exploring the Sklearn user guide and doing experiments. The core topics of multiclass classification such as\n• filtering out a single metric that solves your business problem and…"
    },
    {
        "link": "https://geeksforgeeks.org/image-resizing-using-opencv-python",
        "document": ""
    },
    {
        "link": "https://cloudinary.com/guides/bulk-image-resize/python-image-resize-with-pillow-and-opencv",
        "document": "Python is a popular object-oriented programming language for image-related tasks for webpages, visualizations, or when using Python for machine-learning operations through frameworks like OpenCV and Scikit Learn.\n\nReducing the size of an image means changing its dimensions by removing its pixels. Scaling up an image increases the number of pixels but lowers quality. Either way, the image’s aspect ratio changes, which results in distortion.\n\nThis article describes how to resize images in bulk with the Pillow library, a popular fork of the Python Imaging Library (PIL); and, to maintain the quality and aspect ratio, in OpenCV, a robust library of programming functions for computer vision, neural networks, and other advanced image processing tasks. Also explained is how to resize and crop Python images with Cloudinary through automation.\n• Resize and Crop Images in Python With Cloudinary Through Automation\n\nPillow is a fork of the Python Imaging Library (PIL) that supports Python 3 and numerous image formats, including PNG, JPEG, TIFF, and PPM. When you load an image from a file, create a new image, or generate separate instances for images, you create an instance of PIL’s Image class.\n\nTo resize an image with Pillow’s method:\n• Load the image from a file with the function: The above command returns an object. In case of failure, the command returns an exception.\n• Call the method on the new image instance, passing a tuple argument with two integers to specify the width and height you desire: Note: Instead of modifying the image file, this function returns a separate instance with the new dimensions.\n\nThe method has two drawbacks, however:\n• Oftentimes, resizing to an exact width and height changes the image’s aspect ratio, leading to distortions.\n• If you set the size of the new instance to be larger than that of the original, “blows up” the instance, reducing its quality.\n\nAs a solution, resize the image with the more advanced Pillow method, :\n• Perform steps 1 and 2 of the above procedure.\n• Call the method on the instance, passing a tuple argument with two integers to specify the width and height you desire:\n\nAs shown under , the size of the new instance is 400×350 pixels. The aspect ratio of the original image remains unchanged. In addition, if the dimensions of the original are smaller than that specified for the new instance, instead of “blowing up” the image, returns an instance of the same size.\n\nOpenCV is an open-source computer-vision library with thousands of machine-learning and deep-learning algorithms for face detection, object recognition, and many other computer-vision tasks. Given that numerous computer-vision models require a certain size and quality level for their images, resizing is critical. To determine which image variation performs best, experiment with different sizes or resolutions.\n\nHere is the full syntax for the method in OpenCV:\n\nThe parameters are as follows:\n\nNote: Apply either or and , or all three.\n• Acquire a sample image and specify its current size:\n• Resize the image of, say, a size of 800×600 pixels, to 300×300 pixels:\n\nAs in the previous example on resizing images with Pillow’s resize() method, this procedure changes the aspect ratio, causing distortions. To maintain that ratio, run the following command to resize the image to 75% of its width and height:\n\nIn addition, for a resized instance that is larger than the original, you can customize the interpolation of the resize operation. Even though doing that causes quality loss, it might be the right choice for certain computer-vision applications.\n\nHere are the values for the argument:\n\nResize and Crop Python Images With Cloudinary Through Automation\n\nA cloud-based service for managing images and videos, Cloudinary offers a generous free-forever subscription plan. While on that platform, you can upload images and apply built-in effects, filters, and modifications.\n\nYou can also resize images through automation, focusing on the most important elements with AI, or adapt them to your website design by, for example, specifying the width, height, and aspect ratio as qualifiers for the new image instances. Cloudinary then automatically performs the resizing and cropping tasks to meet the criteria. No manual efforts are required.\n\nResizing it to 200×200 pixels with crop, scale, fill, and pad results in the following images:\n\nAutomatically determine what to keep in a banner crop\n\nTo automate image resizing and cropping on Cloudinary:\n• Set the transformation criteria for the above examples: # Focus on the model in a portrait crop. CloudinaryImage(\"docs/model.jpg\").image(gravity=\"person\", height=600, width=450, crop=\"fill\") # Detect the face for a thumbnail crop. CloudinaryImage(\"docs/model.jpg\").image(gravity=\"face\", height=250, width=250, crop=\"thumb\") # Crop to a banner, automatically focusing on a region of interest. CloudinaryImage(\"docs/model.jpg\").image(gravity=\"auto\", height=150, width=600, crop=\"fill\")\n\nPython offers numerous modules and libraries, such as Pillow and OpenCV, to resize images. While these tools are powerful and versatile, they require significant coding effort to handle various use cases, such as different image formats, sizes, and quality requirements. This can be a daunting task for developers looking to streamline their workflow rather than managing image processing code.\n\nThe Cloudinary API not only simplifies the process of resizing images but also provides a comprehensive suite of image management functionalities. With Cloudinary, you can easily handle image uploads, transformations, optimizations, and even advanced features like automatic format selection, watermarking, and responsive image delivery.\n\nReady to take your image processing to the next level? Sign up for a free Cloudinary account today and experience the ease and power of Cloudinary’s image management solutions. Start simplifying your workflow and focus on what truly matters—building great applications."
    },
    {
        "link": "https://opencv.org/blog/resizing-and-rescaling-images-with-opencv",
        "document": "Image resizing and rescaling are fundamental operations in computer vision, essential for optimizing performance, preparing data for machine learning, and enhancing visualization. OpenCV, one of the most powerful and widely used computer vision libraries, provides efficient methods to resize images while maintaining quality. This guide covers image resizing in OpenCV using Python and C++, explaining key functions, interpolation techniques, and best practices. Whether you’re working on high-performance applications in C++ or need quick prototyping in Python, this article provides hands-on examples and in-depth insights to help you master image scaling with OpenCV.\n\nSo, let’s dive in and scale our way to perfectly sized images!\n\nBefore using any OpenCV functions, we must first import the library. This is the essential first step to access all OpenCV functionalities.\n\nWe are assuming that you have already installed OpenCV on your device.\n\nIf not please refer the relevant links below:\n\nThe resize function in OpenCV is a versatile and efficient method for resizing or rescaling images to a desired dimension. Whether you need to reduce the size of an image for faster processing or enlarge it while maintaining quality, this function provides multiple interpolation techniques to achieve optimal results. It allows resizing based on absolute dimensions (width and height) or scaling factors (fx and fy), making it adaptable for various applications\n\nWhen resizing an image with OpenCV, the resize() function makes sure that the image is carefully reshaped while preserving as much detail as possible. Let’s break down its syntax\n• src (Source Image) – This is the input image that you want to resize.\n• dsize (Destination Size) – Here, you specify the exact (width, height) you want and the resize function reshapes the image to that shape.\n• dst (Destination Image – Optional) – While not commonly used, this argument can store the output image in an existing variable instead of creating a new one.\n• fx & fy (Scaling Factors – Optional) – Instead of specifying a fixed size, you can tell OpenCV to scale the image by a certain factor in the x (fx) and y (fy) directions. These arguments are responsible for stretching or shrinking of the images. If dsize is set, these are ignored—because OpenCV already knows the exact dimensions to use.\n• interpolation (Interpolation Method – Optional) – This determines how OpenCV fills in missing pixels when enlarging or merges them when shrinking.\n\nBelow are the syntaxes for resizing images in OpenCV using Python and C++, allowing you to scale images by specifying dimensions or scaling factors while choosing the appropriate interpolation method.\n\nBefore applying any resizing operations, let’s first read and display the original image to understand its dimensions.\n\nSometimes, high-resolution images, like the above don’t fit well on the screen. Now, let’s explore different resizing methods to scale the image properly and view it without issues.\n\nImage Resizing using the Width and Height\n\nBefore resizing an image, it’s essential to know its original dimensions. In OpenCV, images are represented as NumPy arrays in Python and as cv::Mat objects in C++. Interestingly, OpenCV follows a height × width × channels format, unlike some libraries (such as PIL) that use width × height.\n\nThis is because images in OpenCV behave like 2D arrays, where rows correspond to height and columns to width. You can retrieve these dimensions using image.shape in Python or image.rows and image.cols in C++. Understanding this structure ensures accurate resizing, preventing unexpected distortions in your final output.\n\nC++ Code for Resizing an Image\n\nYou may have noticed that we skipped the dst argument in the Python code but used it in the C++ code. This difference arises from how variables are handled in each language.\n\nIn Python, we can dynamically assign the resized image to a new variable without explicitly creating it beforehand. However, in C++, we must first declare a cv::Mat variable before passing it to the resize function.\n\nThe image has been resized using arbitrary width and height values without considering its aspect ratio (width-to-height proportion). As a result, the resizing process has distorted the image. To preserve the original proportions and prevent such distortions, it is essential to maintain the aspect ratio during resizing. Below is the corrected approach, where the aspect ratio is first calculated before resizing the image accordingly.\n\nInstead of defining both width and height manually, we only specify the new width and calculate the corresponding height. This ensures that the image scales proportionally, maintaining its original look.\n\nInstead of manually specifying width and height, we can scale the image up or down using the fx and fy parameters. Setting fx = fy ensures the aspect ratio is preserved.\n• If fx, fy < 1, the image is scaled down (shrunk).\n• If fx, fy > 1, the image is scaled up (enlarged).\n\nC++ Code for Resizing an Image\n\nInterpolation methods determine how new pixel values are calculated when resizing an image. Different methods have varying effects on image quality, sharpness, and smoothness.\n\nIn this case, instead of keeping equal scaling factors for width (fx) and height (fy), we will use different scaling values to see how interpolation behaves when the aspect ratio is altered. This will make the transformation more noticeable, especially in cases where stretching or shrinking occurs.\n\nTake a quick look into some commonly used interpolation methods:\n\nWe’ll be demonstrating INTER_LINEAR, INTER_CUBIC, INTER_NEAREST, and INTER_AREA in this article, but feel free to explore the INTER_LANCZOS4 method and others, and see how they compare!\n\nC++ Code for Resizing an Image\n\nWhile the differences between interpolation methods may not be easily noticeable in the video, you can observe subtle variations in image quality when experimenting on your own device.\n\nIn this article, we explored various image resizing techniques using OpenCV in both Python and C++. We learned how to resize images efficiently while preserving their aspect ratio and also observed the effects of distortion and stretching when the aspect ratio was not maintained.\n\nNow that you’ve mastered image resizing, why stop there? Try applying these techniques to videos and webcam feeds —scale, stretch, and experiment in real-time to see the magic of OpenCV in action!"
    },
    {
        "link": "https://analyticsvidhya.com/blog/2024/01/image-resizing-using-opencv-in-python",
        "document": "Image resizing is a crucial task in computer vision that involves changing the dimensions of an image while maintaining its aspect ratio. It is fundamental in various applications, including web development, computer vision tasks, and machine learning models. In this article, we will explore different image-resizing techniques using OpenCV, a popular library for computer vision tasks in Python.\n\nImage resizing plays a vital role in computer vision applications. It allows us to adjust the size of images to meet specific requirements, such as fitting images into a web page layout, preparing images for machine learning models, or resizing images for computer vision tasks like object detection and recognition.\n\nIn the next section, we’ll discuss this more thoroughly.\n\nNow, let’s dive into the practical implementation of image resizing using OpenCV in Python. Follow the steps below:\n\nBefore we begin, make sure you have OpenCV installed on your system. You can install it using pip:\n\nAdditionally, you may need to install other libraries, such as NumPy, for image manipulation tasks.\n\nTo resize an image, we first need to load it into our Python script. OpenCV provides the `imread` function to read an image from a file. We can then display the image using the `imshow` function.\n\nTo resize the image, we can use OpenCV’s `resize` function. We must specify the original image, target dimensions, and interpolation method as input parameters.\n\nAfter resizing the image, we can save it to a file using the `imwrite` function.\n\nOpenCV provides several techniques for resizing images, each serving different purposes. Let’s explore some of the commonly used techniques:\n\nResizing an image by scaling involves multiplying the width and height of the image by a scaling factor. This technique allows us to increase or decrease the size of an image while maintaining its aspect ratio. OpenCV provides the `resize` function, which takes the original image and the desired dimensions as input parameters.\n\nPreserving the aspect ratio of an image is crucial to avoid distortion. OpenCV provides a convenient method to resize images while preserving their aspect ratio. By specifying the desired width or height, OpenCV automatically adjusts the other dimension to maintain the aspect ratio.\n\nIn some cases, we may need to resize an image to specific custom dimensions. OpenCV allows us to resize images to any desired width and height by specifying the target dimensions explicitly.\n\nInterpolation methods are used to estimate pixel values when resizing an image. OpenCV provides various interpolation methods, such as nearest-neighbor, bilinear, and bicubic interpolation. These methods help in preserving image quality and reducing artifacts during the resizing process.\n\nWhile resizing images, we must consider certain challenges and maintain image quality. Let’s discuss some common challenges:\n• Maintaining Image Quality and Aspect Ratio: Maintaining image quality and aspect ratio is crucial to avoid distortion and artifacts. We can ensure high-quality resized images using appropriate interpolation methods and preserving the aspect ratio.\n• Handling Different Image Formats: Images can be in various formats, such as JPEG, PNG, or BMP. Handling different image formats correctly during resizing is essential to avoid compatibility issues.\n• Dealing with Memory Constraints: Resizing large images can consume significant memory. It is essential to optimize the resizing process to handle memory constraints efficiently, especially when working with limited resources.\n\nOpenCV offers advanced image resizing techniques that go beyond simple scaling. Let’s explore some of these techniques:\n\nContent-aware image resizing is a sophisticated technique that aims to resize images while preserving important content and structures, adapting the resizing process to the image’s content.\n• Preservation of Content: Unlike traditional resizing methods that may distort or crop important elements, content-aware resizing intelligently identifies and preserves regions of high importance in the image.\n• Seam Carving Algorithm: One of the popular approaches to content-aware resizing is the Seam Carving algorithm, which was briefly mentioned in the article. Seam carving identifies and removes or adds seams (paths of pixels) with low importance, allowing for non-uniform resizing.\n• Adaptive Resizing: Content-aware resizing adapts the resizing operation based on the image’s content. It may reduce the size of less important regions while maintaining the integrity of significant objects or structures.\n• Applications: Content-aware resizing is particularly useful in scenarios where preserving the content and structure of the image is crucial. It finds applications in photography, graphic design, and web development where maintaining the visual integrity of images during resizing is important.\n• Artifact Reduction: Similar to seam carving, content-aware resizing helps reduce artifacts that may occur in traditional resizing methods, ensuring a more visually pleasing result.\n• User Guidance: Some content-aware resizing tools allow users to guide the resizing process by specifying regions of the image that should be preserved or removed. This interactive approach provides more control over the final result.\n• Limitations: While content-aware resizing is powerful, it may face challenges in certain images or complex scenes. The effectiveness depends on the algorithm used and the ability to accurately identify the importance of different image regions.\n\nSeam carving is an advanced image-resizing technique that goes beyond traditional scaling. Unlike traditional methods that uniformly resize an image, seam carving aims to intelligently resize images by removing or adding seams, which are paths of pixels with low energy.\n• Energy Map: The energy of a pixel represents its importance in the image. An energy map is created by computing the gradient of the image, highlighting regions with high contrast and important features.\n• Dynamic Programming: Seam carving uses dynamic programming to find the optimal seam (path) to remove or duplicate in the image. The seam with the lowest accumulated energy is considered, ensuring that important features are preserved.\n• Non-Uniform Resizing: Seam carving allows for non-uniform resizing, meaning that different amounts can resize different image parts. This enables the preservation of important details while resizing less critical areas.\n• Artifact Reduction: Seam carving helps reduce artifacts that may occur in traditional resizing methods, especially in images with complex structures or objects.\n\nSuper-resolution is a technique that aims to enhance the resolution of an image, generating high-frequency details that may not be present in the original image.\n• Upsampling: Super-resolution involves increasing the spatial resolution of an image by upsampling, where finer details are generated between existing pixels.\n• Learning-Based Approaches: Modern super-resolution techniques often involve deep learning approaches. Convolutional Neural Networks (CNNs) are trained to learn the mapping between low-resolution and high-resolution image pairs.\n• Single Image Super-Resolution (SISR): Some techniques focus on enhancing the resolution of a single image without relying on additional high-resolution counterparts. These methods use learned priors to generate plausible high-resolution details.\n• Applications: Super-resolution is particularly useful in applications where high-quality images are required, such as medical imaging, satellite imagery, and surveillance.\n• Trade-offs: While super-resolution can enhance image details, it’s essential to note that it cannot recover information that is not present in the original low-resolution image. The results depend on the quality of the training data and the chosen super-resolution model.\n\nSeveral libraries and tools are available for image resizing. Let’s compare OpenCV with some popular alternatives:\n\nOpenCV and PIL/Pillow are widely used libraries for image-processing tasks. While OpenCV focuses more on computer vision tasks, PIL/Pillow provides a broader range of image manipulation functions. The choice between the two depends on the specific requirements of your project.\n\nScikit-image is another popular library for image-processing tasks in Python. It provides a comprehensive set of image resizing, filtering, and manipulation functions. OpenCV, on the other hand, is more specialized for computer vision tasks. The choice between the two depends on the specific needs of your project.\n\nImageMagick is a powerful command-line tool for image manipulation. It provides a wide range of functions for resizing, cropping, and transforming images. On the other hand, OpenCV is a Python library that offers similar functionality and additional computer vision capabilities. The choice between the two depends on your preferred programming language and the complexity of your project.\n\nImage resizing serves different purposes in various applications. Let’s explore some specific use cases:\n• Image Resizing for Web Applications: In web development, image resizing is essential to optimize the loading time of web pages. By resizing images to the appropriate dimensions, we can reduce the file size and improve the overall performance of web applications.\n• Image Resizing for Computer Vision Tasks: In computer vision tasks like object detection and recognition, resizing images to a specific size is often necessary. By resizing images to a consistent size, we can ensure that the input to our computer vision models remains consistent and accurate.\n• Image Resizing for Machine Learning Models: Machine learning models often require resizing images to a specific size before training or inference. By resizing images to a consistent size, we can ensure compatibility with our machine-learning models and improve their performance.\n\nImage resizing is a fundamental operation in computer vision that allows us to adjust the dimensions of images while maintaining their aspect ratio. OpenCV provides various techniques and functions for image resizing in Python. Following the steps outlined in this article, you can resize images efficiently for different applications. Consider the challenges and choose the appropriate techniques based on your specific requirements.\n\nElevate your skills, gain hands-on experience, and become a master in artificial intelligence and machine learning. Join now to access exclusive training, mentorship, and certification. Transform your career and embrace the future of technology. Enroll in the Certified AI & ML BlackBelt Plus Program today and shape the future of innovation!"
    },
    {
        "link": "https://bhavikjikadara.medium.com/image-processing-using-opencv-a-step-by-step-guide-e589b0acbbf3",
        "document": "Image processing is a crucial part of modern fields like AI, computer vision, and robotics. OpenCV, a powerful open-source library, allows developers to handle complex image tasks with ease. This blog will guide you through essential image processing techniques using Python, covering everything from reading and displaying images, converting color spaces, and resizing images, to more advanced tasks like edge detection, contour detection, and thresholding. With hands-on examples, you’ll learn to manipulate and enhance images effortlessly.\n\nOpenCV is a widely used open-source computer vision library that allows developers to manipulate images and video streams with minimal effort. It’s the go-to solution for many tasks such as image recognition, filtering, edge detection, and more. OpenCV is also cross-platform, making it a perfect choice for large-scale AI and machine-learning applications.\n\nBefore we start, let’s make sure OpenCV is installed. You can easily install it using :"
    }
]