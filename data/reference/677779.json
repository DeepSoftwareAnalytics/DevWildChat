[
    {
        "link": "https://keras.io/examples/vision/oxford_pets_image_segmentation",
        "document": "Author: fchollet\n\n Date created: 2019/03/20\n\n Last modified: 2020/04/20\n\n Description: Image segmentation model trained from scratch on the Oxford Pets dataset.\n\nⓘ This example uses Keras 3\n\nWhat does one input image and corresponding segmentation mask look like?\n\n# Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2: # For faster debugging, limit the size of data\n\n### [First half of the network: downsampling inputs] ### # Blocks 1, 2, 3 are identical apart from the feature depth. ### [Second half of the network: upsampling inputs] ###"
    },
    {
        "link": "https://digitalocean.com/community/tutorials/unet-architecture-image-segmentation",
        "document": "The applications of deep learning models and computer vision in the modern era are growing by leaps and bounds. Computer vision is one such field of artificial intelligence where we train our models to interpret real-life visual images. With the help of deep learning architectures like U-Net and CANet, we can achieve high-quality results on computer vision datasets to perform complex tasks. While computer vision is a humungous field with so much to offer and so many different, unique types of problems to solve, our focus for the next couple of articles will be on two architectures, namely U-Net and CANet, that are designed to solve the task of image segmentation.\n\nThe task in image segmentation is to take an image and divide it into several smaller fragments. These fragments or these multiple segments produced will help with the computation of image segmentation tasks. For image segmentation tasks, another essential requirement is the use of masks. With the help of masking, which is basically a binary image consisting of zero or non-zero values, we can obtain the desired result required for the segmentation task. Once we describe the most essential constituents of the image obtained during image segmentation with the help of images and their respective masks, we can achieve a multitude of future tasks with them.\n\nSome of the most crucial applications of image segmentation include machine vision, object detection, medical image segmentation, machine vision, face recognition, and so much more. Before you dive into this article, I would suggest checking out some optional pre-requisites to follow along with this article. Below is the list of the table of contents for understanding the list of concepts that we will cover in this article. It is recommended to follow through the entire article, but you can feel free to check out the specific sections if you know some concepts already.\n• Deep Learning: Familiarity with neural networks, particularly CNNs and object detection.\n• PyTorch or TensorFlow: Knowledge of either framework is required to follow along. Articles like this tutorial may be a good place to start.\n\nThe U-Net architecture, first published in the year 2015, has been a revolution in the field of deep learning. The architecture won the International Symposium on Biomedical Imaging (ISBI) cell tracking challenge of 2015 in numerous categories by a large margin. Some of their works include the segmentation of neuronal structures in electron microscopic stacks and transmitted light microscopy images.\n\nWith this U-Net architecture, the segmentation of images of sizes 512X512 can be computed with a modern GPU within small amounts of time. There have been many variants and modifications of this architecture due to its phenomenal success. Some of them include LadderNet, U-Net with attention, the recurrent and residual convolutional U-Net (R2-UNet), and U-Net with residual blocks or blocks with dense connections.\n\nAlthough U-Net is a significant accomplishment in the field of deep learning, it is equally essential to understand the previous methods that were employed for solving such kind of similar tasks. One of the primary examples that comes to end was the sliding window approach, which won the EM segmentation challenge at ISBI in the year 2012 by a large margin. The sliding window approach was able to generate a wide array of sample patches apart from the original training dataset.\n\nThis result was because it used the method of setting up the network of sliding window architecture by making the class label of each pixel as separate units by providing a local region (patch) around that pixel. Another achievement of this architecture was the fact that it could localize quite easily on any giving training dataset for the respective tasks.\n\nHowever, the sliding window approach suffered two main drawbacks that were countered by the U-Net architecture. Since each pixel was considered separately considered, the resulting patches we overlapping a lot. Hence, a lot of overall redundancy was produced. Another limitation was that the overall training procedure was quite slow and consumed a lot of time and resources. The feasibility of the working of the network is questionable due to the following reasons.\n\nThe U-Net is an elegant architecture that solves most of the occurring issues. It uses the concept of fully convolutional networks for this approach. The intent of the U-Net is to capture both the features of the context as well as the localization. This process is completed successfully by the type of architecture built. The main idea of the implementation is to utilize successive contracting layers, which are immediately followed by the upsampling operators for achieving higher resolution outputs on the input images.\n\nU-Net architecture from the following research paper\n\nBy having a brief look at the architecture shown in the image, we can notice why it is probably referred to as U-Net architecture. The shape of the so formed architecture is in the form of a ‘U’ and hence the following name. Just by looking at the structure and the numerous elements involved in the process of the construction of this architecture, we can understand that the network built is a fully convolutional network. They have not used any other layers such as dense or flatten or other similar layers. The visual representation shows an initial contracting path followed by an expanding path.\n\nThe architecture shows that an input image is passed through the model and then it is followed by a couple of convolutional layers with the ReLU activation function. We can notice that the image size is reducing from 572X572 to 570X570 and finally to 568X568. The reason for this reduction is because they have made use of unpadded convolutions (defined the convolutions as “valid”), which results in the reduction of the overall dimensionality. Apart from the Convolution blocks, we also notice that we have an encoder block on the left side followed by the decoder block on the right side.\n\nThe encoder block has a constant reduction of image size with the help of the max-pooling layers of strides 2. We also have repeated convolutional layers with an increasing number of filters in the encoder architecture. Once we reach the decoder aspect, we notice the number of filters in the convolutional layers start to decrease along with a gradual upsampling in the following layers all the way to the top. We also notice that the use of skip connections that connect the previous outputs with the layers in the decoder blocks.\n\nThis skip connection is a vital concept to preserve the loss from the previous layers so that they reflect stronger on the overall values. They are also scientifically proven to produce better results and lead to faster model convergence. In the final convolution block, we have a couple of convolutional layers followed by the final convolution layer. This layer has a filter of 2 with the appropriate function to display the resulting output. This final layer can be changed according to the desired purpose of the project you are trying to perform.\n\nIn this section of the article, we will look at the TensorFlow implementation of the U-Net architecture. While I am utilizing TensorFlow for computation of the model, you can choose any deep learning framework such as PyTorch for a similar implementation. We will look at the working of the U-Net architecture along with some other model structures with PyTorch in future articles. However, for this article, we will stick to the TensorFlow library. We will be importing all the required libraries and constructing our U-Net architecture from scratch. But, we will make some necessary changes that will improve the overall performance of the model as well as make it slightly less complex.\n\nIt is crucial to note that the U-Net model was introduced way back in 2015. Although its performance at that point in time was fabulous, the prominent methods and functions of deep learning have evolved simultaneously as well. Hence, there have many successful variants and versions of the U-Net architecture since its original creation to preserve certain image qualities while replicating, and in some scenarios, performing better than the original architecture.\n\nI will try to preserve most of the essential parameters and architectural elements of the original implementation of the U-Net architecture. However, there will be slight changes from the original content that will improve the modern efficiency and improve the speed as well the simplicity of the model. One of the changes that will be included in this structure is using the value of Convolution as “same” because many pieces of research in the future have shown that this particular change did not negatively impact the architectural build in any way. Also, since the concept of batch normalization was introduced in 2016, the original architecture did not use this aspect. But, our model implementation will include batch normalization as it yields the best results in most cases.\n\nFor building the U-Net architecture, we will utilize the TensorFlow deep learning framework, as discussed already. Hence, we will import the TensorFlow library for this purpose as well as the Keras framework, which is now an integral part of TensorFlow model structures. From our previous understanding of the U-Net architecture, we know that some of the essential imports include the convolutional layer, the max-pooling layer, an input layer, and the activation function ReLU for the basic modeling structure. We will then use some additional layers like the Conv2DTranspose layer, which will perform an upsampling for our desired decoder blocks. We will also make use of the Batch Normalization layers for stabilizing the training process and use the Concatenate layers for combining the necessary skip connections.\n\nAfter importing the required libraries, we can continue to build the U-Net architecture. You can either do this in one complete class by defining all the parameters and values accordingly in order and continuing the process until you reach the very end or you a few iterative blocks. I will be using the latter method as it is more convenient for most users to understand the model architecture of U-Net with the help of few blocks. We will utilize three iterative blocks as shown in the architecture representation, namely the convolution operation block, the encoder block, and the decoder block. With the help of these three blocks, we can build the U-Net architecture with ease. Let us now process and understand each of these function code blocks one by one.\n\nThe convolution operation block is used to perform the primary operation of taking the entered input parameters and processing a double layer of convolution operations. In this function, we have two arguments, namely the input for the convolution layer and the number of filters, which is by default 64. We will use the value of padding as same as discussed previously to maintain the same shapes as opposed to unpadded or valid convolutions. These convolutional layers are followed along by the Batch Normalization layer. These changes from the original model are made to gain the best outcomes possible. Finally, a ReLU activation layer is added into the mix as defined in the research paper. Let us explore the code block for building the convolution block.\n\nOur Next step will be to build the encoder and decoder blocks. These two functions are quite simple to construct. The encoder architecture will use consecutive inputs starting from the first layer all the way to the bottom. The encoder function as we have defined will have the convolutional block, i.e., two convolutional layers followed by their respective batch normalization and ReLU layers. Once we pass them through the convolution blocks, we will quickly downsample these elements, as mentioned in the research paper. We will use a max-pooling layer and stick to the parameters mentioned in the paper as the strides = 2. We will then return both the initial output and the max-pooled output, as we need the former for performing the skip connections.\n\nThe decoder block will include three arguments, namely the receiving inputs, the input of the skip connection, and the number of filters in the particular building block. We will upsample the entered input with the help of the Conv2DTranspose layers in our model. We will then concatenate both the receiving input and the newly upsampled layers to receive the final value of the skip connections. We will then use this combined function and perform our convolutional block operation to proceed to the next layer and return this output value.\n\nIf you are trying to build the entire U-Net architecture from scratch in a single layer, you might find that the overall structure is quite humungous because it consists of so many different blocks to be processed. By dividing our respective functions into three separate code blocks of convolutional operation, encoder structure, and decoder structure, we can construct the U-Net architecture with ease in a few lines of code. We will use the input layer, which will contain the respective shapes of our input image.\n\nAfter this step, we will collect all the primary outputs and the skip outputs to pass them on to further blocks. We will create the next block and construct the entire decoder architecture until we reach the output. The output will have the required dimensions according to our desired output. In this case, I have one output node with the sigmoid activation function. We will call the functional API modeling system to create our final model and return this model to the user for performing any task with the U-Net architecture.\n\nEnsure that your image shapes are divisible by at least 16 or multiples of 16. Since we are using four max-pooling layers during the down-sampling procedure, we don’t want to encounter the divisibility of any odd number shapes. Hence, it would be best to ensure that the sizes of your architecture are equivalent to sizes like (48, 48), (80,80), (160, 160), (256, 256), (512, 512), and other similar shapes. Let us try our model structure for an input shape of (160, 160, 3) and test the results. A summary of the model and its respective plot is obtained. You can see both these structures from the attached Jupyter Notebook. I will also include the model.png to show the particular plot of the entire architectural build.\n\nYou can view the summary and plots respectively with the above code blocks. Let us now explore a fun project with the U-Net architecture.\n\nFor this project, we will use the reference from Keras for an image segmentation project. The following link will guide you to the reference. For this project, we will extract the dataset and visualize the basic elements to get an overview of the structure. We can then proceed to build the data generator for loading for data from the dataset accordingly. We will then utilize the U-Net model that we built in the previous section and train this model until we reach a satisfactory result. Once our desired result is obtained, we will save this model and test it out on a validation sample. Let us get started with the implementation of the project!\n\nWe will use the Oxford pet dataset for this particular task of image segmentation. The Oxford pet dataset consists of 37 categories of pet dataset with roughly 200 images for each class. The images have a large variation in scale, pose, and lighting. To install the dataset locally on your system, you can download the images dataset from this link and the annotations dataset from the following link (TBA). Once you have the zip files downloaded successfully, you can unzip them twice with 7-zip or any other similar tool kit that you use on your operating system.\n\nIn the first code block, we will define the respective paths to the images and annotations directories. We will also define some essential parameters such as the image size, the batch size, and the number of classes. We will then ensure that all the elements in the dataset are arranged in the correct order for performing and obtaining a satisfactory image segmentation task. You can verify the images with their respective annotations by printing both the file paths to check if they produce the desired results.\n\nNow that we have collected and pre-processed our data for our project, our next step will be to take a brief look at the dataset. Let us analyze the dataset by displaying both an image and its respective segmented output. This segmented output with the masking is often times referred to as the ground truth annotation. We will utilize the I-Python display option along with the pillow library for randomly displaying a selected image. This simple code block is written as follows:\n\nIn this particular code block, instead of using data generators to prepare the data, we will utilize the Sequence operation from the Keras deep learning framework. This method is much safer for multi-processing in comparison to using the generator because they only train each sample once per epoch, which will avoid any unnecessary adjustments that we will need to make. We will then prepare the Sequence class in the utils module of Keras to compute, load, and vectorize batches of data. We will then construct the initialization function, an additional function to compute the length and the final function that will generate batches of data.\n\nIn the next step, we will define the split between the training and the validation data, respectively. We do this step to ensure that there is no corruption between the integrity of the elements in the train and test sets accordingly. Both of these data entities must be viewed separately so that the model does not get a peek at the testing data. In the validation set, we will also perform an optional shuffle operation that will mix up all the images in the dataset, and we can obtain random samples for both the train and the validation images. We will then call the training values and the validation values individually and store them in their respective variables.\n\nOnce we have completed the following steps, we can proceed to construct our U-Net architecture.\n\nThe U-Net model we will construct in this section is the exact same architecture as the one defined in the previous sections except for a few small modifications that we will discuss shortly. After the preparation of the dataset, we can construct our model accordingly. The model inculcates the image sizes and starts to struct the overall architecture through which our images will be passed. The only change you will need to make in the previously discussed architecture is as follows:\n\nWe are changing the final layer in the U-Net architecture to signify the total number of outputs that will be generated at the final step. Note that you could also utilize the SoftMax function for generating the final output for multi-class classification, and that is probably more accurate. However, you can see from the training results that this activation function works fine as well.\n\nIn the next step, we will compile and train the model to see its performance on the data. We are also using a checkpoint to save the model so that we can make predictions in the future. I interrupted the training procedure after 11 epochs as I was quite satisfied with the result obtained. You can choose to run it for more epochs if you want.\n\nFinally, let us visualize the results obtained.\n\nThe U-Net architecture is one of the most significant and revolutionary landmarks in the field of deep learning. While the initial research paper that introduced the U-Net architecture was to solve the task of Biomedical Image Segmentation, it was not limited to this single application. The model could and can still solve the most complex problems in deep learning. Although some of the elements in the original architecture are outdated, there are several variations of this architecture. These include LadderNet, U-Net with attention, the recurrent and residual convolutional U-Net (R2-UNet), and other similar networks which are derived successfully from the original U-Net Models.\n\nIn this article, we looked into a brief introduction of the U-Net modeling technique that finds fabulous utility in most modern tasks related to image segmentation. We then proceed to understand the construction and the main methodologies employed in the building of the U-Net architecture. We understood the numerous methodologies and techniques used to achieve the best results possible on the provided dataset. After this section, we learned how to build the U-Net architecture from scratch with various blocks to simplify the overall structure. Finally, we analyzed our constructed U-Net architecture with a simple example problem for image segmentation.\n\nIn the upcoming article, we will look into the CANet architecture for image segmentation and understand some of its core concepts. We will then proceed to build the entire architecture from scratch. Until then, have fun learning and exploring the world of Deep Learning!"
    },
    {
        "link": "https://pyimagesearch.com/2022/02/21/u-net-image-segmentation-in-keras",
        "document": "In this tutorial, you will learn how to create U-Net, an image segmentation model in TensorFlow 2 / Keras. We will first present a brief introduction on image segmentation, U-Net architecture, and then walk through the code implementation with a Colab notebook.\n\nTo learn how to implement a U-Net with TensorFlow 2 / Keras, just keep reading.\n\nImage segmentation is a computer vision task that segments an image into multiple areas by assigning a label to every pixel of the image. It provides much more information about an image than object detection, which draws a bounding box around the detected object, or image classification, which assigns a label to the object.\n\nSegmentation is useful and can be used in real-world applications such as medical imaging, clothes segmentation, flooding maps, self-driving cars, etc.\n\nThere are two types of image segmentation:\n• Instance segmentation: classify each pixel and differentiate each object instance.\n\nU-Net is a semantic segmentation technique originally proposed for medical imaging segmentation. It’s one of the earlier deep learning segmentation models, and the U-Net architecture is also used in many GAN variants such as the Pix2Pix generator.\n\nU-Net was introduced in the paper, U-Net: Convolutional Networks for Biomedical Image Segmentation. The model architecture is fairly simple: an encoder (for downsampling) and a decoder (for upsampling) with skip connections. As Figure 1 shows, it shapes like the letter U hence the name U-Net.\n\nThe gray arrows indicate the skip connections that concatenate the encoder feature map with the decoder, which helps the backward flow of gradients for improved training.\n\nNow that we have a basic understanding of semantic segmentation and the U-Net architecture, let’s implement a U-Net with TensorFlow 2 / Keras. Please follow the tutorial below with this Colab notebook.\n\nWe will be using Colab for model training, so make sure you set “Hardware accelerator” to “GPU under Runtime / change runtime type.” Then import the libraries and packages this project depends on:\n\nWe will use the Oxford-IIIT pet dataset, available as part of the TensorFlow Datasets (TFDS). It can be easily loaded with TFDS, and then with a bit of data preprocessing, ready for training segmentation models.\n\nWith just one line of code, we can use to load the dataset by specifying the name of the dataset, and get the dataset info by setting :\n\nPrint the dataset info with , and we will see all kinds of detailed information about the Oxford pet dataset. For example, in Figure 2, we can see there are a total of 7349 images with a built-in test/train split.\n\nLet’s first make a few changes to the downloaded data before we start training U-Net with it.\n\nFirst, we need to resize the images and masks to :\n\nWe then create a function to augment the dataset by flipping them horizontally:\n\nWe create a function to normalize the dataset by scaling the images to the range of and decreasing the image mask by :\n\nWe create two functions to preprocess the training and test datasets with a slight difference between the two – we only perform image augmentation on the training dataset.\n\nNow we are ready to build an input pipeline with by using the function:\n\nIf we execute , we will notice that the image is in the shape of of while the image mask is in the shape of with the data type of .\n\nWe define a batch size of and a buffer size of for creating batches of training and test datasets. With the original TFDS dataset, there are 3680 training samples and 3669 test samples, which are further split into validation/test sets. We will use the and the for training the U-Net model. After the training finishes, we will then use the to test the model predictions.\n\nNow the datasets are ready for training. Let’s visualize a random sample image and its mask from the training dataset, to get an idea of how the data looks.​​\n\nThe sample input image of a cat is in the shape of . The true mask has three segments: the green background; the purple foreground object, in this case, a cat; and the yellow outline. Figure 3 shows both the original input image and the true mask image.\n\nNow that we have the data ready for training, let’s define the U-Net model architecture. As mentioned earlier, the U-Net is shaped like a letter U with an encoder, decoder, and the skip connections between them. So we will create a few building blocks to make the U-Net model.\n\nFirst, we create a function with layers , which we will use in both the encoder (or the contracting path) and the bottleneck of the U-Net.\n\nThen we define a function for downsampling or feature extraction to be used in the encoder.\n\nFinally, we define an upsampling function for the decoder (or expanding path) of the U-Net.\n\nThere are three options for making a Keras model, as well explained in Adrian’s blog and the Keras documentation:\n• Functional API: more flexible and allows non-linear topology, shared layers, and multiple inputs or multi-outputs.\n• subclassing: most flexible and best for complex models that need custom training loops.\n\nU-Net has a fairly simple architecture; however, to create the skip connections between the encoder and decoder, we will need to concatenate some layers. So the Keras Functional API is most appropriate for this purpose.\n\nFirst, we create a function, specify the inputs, encoder layers, bottleneck, decoder layers, and finally the output layer with with activation of . Note the input image shape is . The output has three channels corresponding to the three classes that the model will classify each pixel for: background, foreground object, and object outline.\n\nWe call the function to create the model :\n\nAnd we can visualize the model architecture with to see each detail of the model. And we can use a Keras utils function called to generate a more visual diagram, including the skip connections. The generated image generated in Colab is rotated 90 degrees so that you can see U-shaped architecture in Figure 4 (see the details better in the Colab notebook):\n\nTo compile , we specify the optimizer, the loss function, and the accuracy metrics to track during training:\n\n\n\nWe train the by calling and training it for 20 epochs.\n\nAfter training for 20 epochs, we get a training accuracy and a validation accuracy of . The learning curve during training indicates that the model is doing well on both the training dataset and validation set, which indicates the model is generalizing well without much overfitting (Figure 5).\n\nNow that we have completed training the , let’s use it to make predictions on a few sample images of the test dataset.\n\nSee Figure 6 for the input images, the true masks, and the masks predicted by the U-Net model we trained.\n\nIn this post, you have learned how to load the Oxford-IIIT pet data with the TensorFlow dataset and how to train an image segmentation U-Net model from scratch. We created the U-Net with Keras Functional API and visualized the U-shaped architecture with skip connections. This post has been inspired by the official TensorFlow.org image segmentation tutorial and the U-Net tutorial on Keras.io, which uses for loading the data and has an Xception-style U-Net architecture. U-Net is a great start for learning semantic segmentation on images. To learn more about this topic, read segmentation papers on modern models such as DeepLab V3, HRNet, U2-Net, etc., among many other papers.\n\nTo download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), simply enter your email address in the form below!"
    },
    {
        "link": "https://analyticsvidhya.com/blog/2022/10/image-segmentation-with-u-net",
        "document": "In recent times, whenever we wish to perform image segmentation in machine learning, the first model we think of is the U-Net. It has been revolutionary in performance improvement compared to previous state-of-the-art methods. U-Net architecture for image segmentation is an encoder-decoder convolutional neural network with extensive medical imaging, autonomous driving, and satellite imaging applications.However, understanding how the U-Net performs segmentation is important, as all novel architectures post-U-Net develop on the same intuition. We will be diving in to understand how the u net segmentation performs image segmentation. To enhance our understanding, we will also apply the U-Net for the task of brain image segmentation.\n\nThis article was published as a part of the Data Science Blogathon.\n\nBefore we get to why U-Net is so popular when it comes to image segmentation tasks, let us understand what image segmentation is. Computer Vision has been one of the many exciting applications of machine intelligence. It has numerous applications in today’s world and makes our lives easier. Two of the most common computer vision tasks are image classification and object detection.\n\nImage classification for two classes involves predicting whether the image belongs to class A or B. The predicted label is assigned to the entire image. Classification is helpful when we want to see what class is in the image.\n\nObject detection, on the other hand, takes this further by predicting the object’s location in our input image. We localize objects within an image by drawing a bounding box around them. Detection is useful for locating and tracking the contents of the image.\n\nOne could think of image segmentation as combining classification and localization.\n\nImage segmentation involves partitioning the image into smaller parts called segments. Segmentation involves understanding what is given in an image at a pixel level. It provides fine-grained information about the image as well as the shapes and boundaries of the objects. The output of image segmentation is a mask where each element indicates which class that pixel belongs to. Let’s understand this with an example.\n\nInput and output of image segmentation, respectively\n\nGiven above, on the left is our input image of a cat. Our task is to separate the cat from the background. So we have two output classes – cat [1] and background [0]. However, in separating this cat from its background, we need to know the cat’s exact location in the image. We are to find answers to two questions-\n• “What” is in the input image?\n\nAns: Cat and Background\n• “Where” is that object in the input image?\n\nAns: The location of the cat in the image\n\nImage segmentation solves the above problem pixel by pixel. We wish to group similar pixels and separate dissimilar pixels. At each pixel, we will perform the classification task of whether that pixel is part of the cat or the background. Thus all the pixels which our model predicts as belonging to the cat will have the label 1, and the remaining pixels will have the label 0. In this process, we would have created a mask of our input image as shown above, and at the end of this pixel-wise classification, we also would have detected the cat’s exact location in our image.\n\nNow that we have understood segmentation let us understand the U-Net model.\n\nUnderstanding the Development and Advantages of U-Net\n\nOlaf Ronneberger and his team developed u net segmentation in 2015 for their work on biomedical images. It won the ISBI challenge by outperforming the sliding window technique by using fewer images and data augmentation to increase the model performance.\n\nSliding window architecture performs localization tasks well on any given training dataset. This architecture creates a local patch for each pixel, generating separate class labels for each pixel. However, this architecture has two main drawbacks: firstly, it generates a lot of overall redundancy due to overlapping patches. Secondly, the training procedure was slow, taking a lot of time and resources. These reasons made the architecture not feasible for various tasks. U-Net architecture for image segmentation overcomes these two drawbacks.\n\nU-Net Architecture and its Mechanisms for Image Segmentation\n\nWe initially talked about how segmentation consists of classification and localization. Let’s understand how a u net architecture performs these two tasks and why it is so apt for segmentation.\n\nU Net Architecture gets its name from its architecture. The “U” shaped model comprises convolutional layers and two networks. First is the encoder, which is followed by the decoder. With the U-Net, we can solve the above two questions of segmentation: “what” and “where.”\n\nThe encoder network is also called the contracting network. This network learns a feature map of the input image and tries to solve our first question- “what” is in the image? It is similar to any classification task we perform with convolutional neural networks except for the fact that in a U-Net, we do not have any fully connected layers in the end, as the output we require now is not the class label but a mask of the same size as our input image.\n\nThis encoder network consists of 4 encoder blocks. Each block contains two convolutional layers with a kernel size of 3*3 and valid padding, followed by a Relu activation function. This is inputted to a max pooling layer with a kernel size of 2*2. With the max pooling layer, we have halved the spatial dimensions learned, thereby reducing the computation cost of training the model.\n\nIn between the encoder and decoder network, we have the bottleneck layer. This is the bottommost layer, as we can see in the model above. It consists of 2 convolutional layers followed by Relu. The output of the bottleneck is the final feature map representation.\n\nNow, what makes U-Net so good at image segmentation is and What we have done till now is similar to any CNN. The skip connections and decoder network separates the u net architecture from other CNNs.\n\nThe decoder network is also called the expansive network. Our idea is to up sample our feature maps to the size of our input image. This network takes the feature map from the bottleneck layer and generates a segmentation mask with the help of skip connections. The decoder network tries to solve our second question-“where” is the object in the image? It consists of 4 decoder blocks. Each block starts with a transpose convolution ( indicated as up-conv in the diagram) with a kernel size of 2*2. After concatenating this output with the corresponding skip layer connection from the encoder block, the network utilizes two convolutional layers with a kernel size of 3*3, followed by a Relu activation function.\n\nSkip connections are indicated with a grey arrow in the model architecture. Skip connections help us use the contextual feature information collected in the encoder blocks to generate our segmentation map. The idea is to use our high-resolution features learned from the encoder blocks ( through skip connections ) to help us project our feature map ( output of the bottleneck layer). This helps us answer “where” is our object in the image?\n\nA 1*1 convolution follows the last decoder block with sigmoid activation which gives the output of a segmentation mask containing pixel-wise classification. This way, it could be said that the contracting path passes across information to the expansive path. And thus, we can capture both the feature information and localization with the help of a U-Net.\n\nLet’s take an application of U-Net to understand the model better.\n\nWe take the example of brain tumor segmentation. Brain tumor segmentation is a crucial task; early detection increases patients’ survival rates. Manually detecting these tumors is a tedious task. Automating this task using machine learning can help both doctors and patients. Our application tries to predict the brain tumor location using the U-Net.\n\nWe use a publicly available dataset. This brain tumor T1-Lighted CE-MRI image dataset consists of 3064 images. There are 1047 coronal images, 990 axial images, and 1027 saggital images. This dataset has a label for each image, identifying the type of tumor. These 3064 images belong to 233 patients. The dataset includes three types of tumors- 708 Meningiomas, 1426 Gliomas, and 930 Pituitary tumors, which are publicly available on: Click Here.\n\nThe size of each image is 512X512 pixels. Let’s break down each step of our segmentation project.\n\nWe download the dataset from the link given above. The dataset needs to be unzipped and made available. Our dataset is given in matlab format, so we convert this data into numpy arrays each for the images, labels, and masks. We finally display the size of each numpy .\n\nOur next step is pre-processing this data and visualizing this data. We display the result of each to understand our dataset better.\n\nNext, we normalize the input images. This step is followed by defining our evaluation metrics. We use binary cross-entropy, dice loss, and a custom loss function composed of these two. We also use a 80:10:10 ratio for our train:val:test split and display the size of each.\n\nNow coming to our U-Net model as detailed in our explanation above:\n\nAfter training this network for 40 epochs, we achieve a dice score of 0.67. Here are a few sample predictions on the test set.\n\nIn conclusion, our exploration of image segmentation and U-Net architecture reveals its dual role in classification and localization. By addressing “what” and “where” in segmentation, the encoder captures image content, while the decoder delineates object boundaries. Skip connections facilitate information flow between encoder and decoder, enhancing segmentation accuracy. While we applied U-Net to biomedical tasks, our understanding extends to broader segmentation applications. Future enhancements may involve pre-trained encoder weights and variations on U-Net architecture, yet its fundamental principles endure.\n\nThe media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion."
    },
    {
        "link": "https://geeksforgeeks.org/u-net-architecture-explained",
        "document": "U-Net is a widely used deep learning architecture that was first introduced in the “U-Net: Convolutional Networks for Biomedical Image Segmentation” paper. The primary purpose of this architecture was to address the challenge of limited annotated data in the medical field. This network was designed to effectively leverage a smaller amount of data while maintaining speed and accuracy.\n\nThe architecture of U-Net is unique in that it consists of a contracting path and an expansive path. The contracting path contains encoder layers that capture contextual information and reduce the spatial resolution of the input, while the expansive path contains decoder layers that decode the encoded data and use the information from the contracting path via skip connections to generate a segmentation map.\n\nThe contracting path in U-Net is responsible for identifying the relevant features in the input image. The encoder layers perform convolutional operations that reduce the spatial resolution of the feature maps while increasing their depth, thereby capturing increasingly abstract representations of the input. This contracting path is similar to the feedforward layers in other convolutional neural networks. On the other hand, the expansive path works on decoding the encoded data and locating the features while maintaining the spatial resolution of the input. The decoder layers in the expansive path upsample the feature maps, while also performing convolutional operations. The skip connections from the contracting path help to preserve the spatial information lost in the contracting path, which helps the decoder layers to locate the features more accurately.\n\nFigure 1 illustrates how the U-Net network converts a grayscale input image of size 572×572×1 into a binary segmented output map of size 388×388×2. We can notice that the output size is smaller than the input size because no padding is being used. However, if we use padding, we can maintain the input size. During the contracting path, the input image is progressively reduced in height and width but increased in the number of channels. This increase in channels allows the network to capture high-level features as it progresses down the path. At the bottleneck, a final convolution operation is performed to generate a 30×30×1024 shaped feature map. The expansive path then takes the feature map from the bottleneck and converts it back into an image of the same size as the original input. This is done using upsampling layers, which increase the spatial resolution of the feature map while reducing the number of channels. The skip connections from the contracting path are used to help the decoder layers locate and refine the features in the image. Finally, each pixel in the output image represents a label that corresponds to a particular object or class in the input image. In this case, the output map is a binary segmentation map where each pixel represents a foreground or background region.\n\nNext, we will implement the U-Net architecture using Python 3 and the TensorFlow library. The implementation can be divided into three parts. First, we will define the encoder block used in the contraction path. This block consists of two 3×3 convolution layers followed by a ReLU activation layer and a 2×2 max pooling layer. The second part is the decoder block, which takes the feature map from the lower layer, upconverts it, crops and concatenates it with the encoder data of the same level, and then performs two 3×3 convolution layers followed by ReLU activation. The third part is defining the U-Net model using these blocks.\n\nHere’s the code for the encoder block:\n\nUsing these blocks and defining a U-Net model and printing model summary.\n\nThe versatility and flexibility has enabled us to use this idea in various other domains beyond biomedical image segmentation. Some of the popular application involves image denoising, image-to-image translation, image super-resolution, object detection, and NLP. You can also explore some of these applications in the following articles:"
    },
    {
        "link": "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras",
        "document": "Dropout is a simple and powerful regularization technique for neural networks and deep learning models.\n\nIn this post, you will discover the Dropout regularization technique and how to apply it to your models in Python with Keras.\n\nAfter reading this post, you will know:\n• How to use Dropout on your input layers\n• How to use Dropout on your hidden layers\n• How to tune the dropout level on your problem\n\nKick-start your project with my new book Deep Learning With Python, including step-by-step tutorials and the Python source code files for all examples.\n\nDropout is a regularization technique for neural network models proposed by Srivastava et al. in their 2014 paper “Dropout: A Simple Way to Prevent Neural Networks from Overfitting” (download the PDF).\n\nDropout is a technique where randomly selected neurons are ignored during training. They are “dropped out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass, and any weight updates are not applied to the neuron on the backward pass.\n\nAs a neural network learns, neuron weights settle into their context within the network. Weights of neurons are tuned for specific features, providing some specialization. Neighboring neurons come to rely on this specialization, which, if taken too far, can result in a fragile model too specialized for the training data. This reliance on context for a neuron during training is referred to as complex co-adaptations.\n\nYou can imagine that if neurons are randomly dropped out of the network during training, other neurons will have to step in and handle the representation required to make predictions for the missing neurons. This is believed to result in multiple independent internal representations being learned by the network.\n\nThe effect is that the network becomes less sensitive to the specific weights of neurons. This, in turn, results in a network capable of better generalization and less likely to overfit the training data.\n\nDropout is easily implemented by randomly selecting nodes to be dropped out with a given probability (e.g., 20%) in each weight update cycle. This is how Dropout is implemented in Keras. Dropout is only used during the training of a model and is not used when evaluating the skill of the model.\n\nNext, let’s explore a few different ways of using Dropout in Keras.\n\nThe examples will use the Sonar dataset. This is a binary classification problem that aims to correctly identify rocks and mock-mines from sonar chirp returns. It is a good test dataset for neural networks because all the input values are numerical and have the same scale.\n\nThe dataset can be downloaded from the UCI Machine Learning repository. You can place the sonar dataset in your current working directory with the file name sonar.csv.\n\nYou will evaluate the developed models using scikit-learn with 10-fold cross validation in order to tease out differences in the results better.\n\nThere are 60 input values and a single output value. The input values are standardized before being used in the network. The baseline neural network model has two hidden layers, the first with 60 units and the second with 30. Stochastic gradient descent is used to train the model with a relatively low learning rate and momentum.\n\nThe full baseline model is listed below:\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example generates an estimated classification accuracy of 86%.\n\nUsing Dropout on the Visible Layer\n\nDropout can be applied to input neurons called the visible layer.\n\nIn the example below, a new Dropout layer between the input (or visible layer) and the first hidden layer was added. The dropout rate is set to 20%, meaning one in five inputs will be randomly excluded from each update cycle.\n\nAdditionally, as recommended in the original paper on Dropout, a constraint is imposed on the weights for each hidden layer, ensuring that the maximum norm of the weights does not exceed a value of 3. This is done by setting the kernel_constraint argument on the Dense class when constructing the layers.\n\nThe learning rate was lifted by one order of magnitude, and the momentum was increased to 0.9. These increases in the learning rate were also recommended in the original Dropout paper.\n\nContinuing from the baseline example above, the code below exercises the same network with input dropout:\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example provides a slight drop in classification accuracy, at least on a single test run.\n\nDropout can be applied to hidden neurons in the body of your network model.\n\nIn the example below, Dropout is applied between the two hidden layers and between the last hidden layer and the output layer. Again a dropout rate of 20% is used as is a weight constraint on those layers.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nYou can see that for this problem and the chosen network configuration, using Dropout in the hidden layers did not lift performance. In fact, performance was worse than the baseline.\n\nIt is possible that additional training epochs are required or that further tuning is required to the learning rate.\n\nDropout will randomly reset some of the input to zero. If you wonder what happens after you have finished training, the answer is nothing! In Keras, a layer can tell if the model is running in training mode or not. The Dropout layer will randomly reset some input only when the model runs for training. Otherwise, the Dropout layer works as a scaler to multiply all input by a factor such that the next layer will see input similar in scale. Precisely, if the dropout rate is $r$, the input will be scaled by a factor of $1-r$.\n\nThe original paper on Dropout provides experimental results on a suite of standard machine learning problems. As a result, they provide a number of useful heuristics to consider when using Dropout in practice.\n• Generally, use a small dropout value of 20%-50% of neurons, with 20% providing a good starting point. A probability too low has minimal effect, and a value too high results in under-learning by the network.\n• Use a larger network. You are likely to get better performance when Dropout is used on a larger network, giving the model more of an opportunity to learn independent representations.\n• Use Dropout on incoming (visible) as well as hidden units. Application of Dropout at each layer of the network has shown good results.\n• Use a large learning rate with decay and a large momentum. Increase your learning rate by a factor of 10 to 100 and use a high momentum value of 0.9 or 0.99.\n• Constrain the size of network weights. A large learning rate can result in very large network weights. Imposing a constraint on the size of network weights, such as max-norm regularization, with a size of 4 or 5 has been shown to improve results.\n\nBelow are resources you can use to learn more about Dropout in neural networks and deep learning models.\n• Dropout: A Simple Way to Prevent Neural Networks from Overfitting (original paper)\n• How does the dropout method work in deep learning? on Quora\n• Keras Training and Evaluation with Built-in Methods from TensorFlow documentation\n\nIn this post, you discovered the Dropout regularization technique for deep learning models. You learned:\n• What Dropout is and how it works\n• How you can use Dropout on your own deep learning models.\n• Tips for getting the best results from Dropout on your own models.\n\nDo you have any questions about Dropout or this post? Ask your questions in the comments, and I will do my best to answer."
    },
    {
        "link": "https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-dropout-with-keras.md",
        "document": "When you have a dataset of limited size, overfitting is quite a problem. That is, while your training results might be good, it's likely that they don't generalize to data that has not been seen during training.\n\nThis severely impacts the production usability of your machine learning module.\n\nFortunately, with regularization techniques, it's possible to reduce overfitting. Dropout is such a technique. In this blog post, we cover how to implement Keras based neural networks with Dropout. We do so by firstly recalling the basics of Dropout, to understand at a high level what we're working with. Secondly, we take a look at how Dropout is represented in the Keras API, followed by the design of a ConvNet classifier of the CIFAR-10 dataset. We subsequently provide the implementation with explained example code, and share the results of our training process.\n\nBefore discussing the implementation of Dropout in the Keras API, the design of our model and its implementation, let's first recall what Dropout is and how it works.\n\nIn our blog post \"What is Dropout? Reduce overfitting in your neural networks\", we looked at what Dropout is theoretically. In short, it's a regularizer technique that reduces the odds of overfitting by dropping out neurons at random, during every epoch (or, when using a minibatch approach, during every minibatch).\n\nDropping out neurons happens by attaching Bernoulli variables to the neural outputs (Srivastava et al., 2014). These variables, which take the value of [latex]1[/latex] with probability [latex]p[/latex] and 0 with [latex]1-p[/latex], help reduce overfitting by \"making the presence of other (..) units unreliable\". This way, neural networks cannot generate what Srivastava et al. call complex co-adaptations that do not generalize to unseen data.\n\nBy consequence, the occurrence of overfitting is reduced.\n\nLet's now continue with some Dropout best practices. If you wish to understand the concepts behind Dropout in more detail, I'd like to point you to this blog.\n\nWhen working on software projects, and hence when working on machine learning development, it's always best to take a look at some best practices. Srivastava et al. (2014), who discussed Dropout in their work \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", empirically found some best practices which we'll take into account in today's model:\n• While it's best to determine the value for parameter [latex]p[/latex] with a validation set, it's perfectly fine to set it to [latex]p \\approx 0.5[/latex]. This value has shown the best empirical results when being tested with the MNIST dataset.\n• To avoid holes in your input data, the authors argued that you best set [latex]p[/latex] for the input layer to [latex]1.0[/latex] - effectively the same as not applying Dropout there.\n• Dropout seems to work best when a combination of max-norm regularization (in Keras, with the MaxNorm constraint), high learning rates that decay to smaller values, and high momentum is used as well.\n\nAny optimizer can be used. Given the benefits of the Adam optimizer (momentum-like optimization with locally adapted weights), we're using that one today, as well as the best practices mentioned above.\n\nWithin Keras, Dropout is represented as one of the Core layers (Keras, n.d.):\n\nIt can be added to a Keras deep learning model with and contains the following attributes:\n• Rate: the parameter [latex]p[/latex] which determines the odds of dropping out neurons. When you did not validate which [latex]p[/latex] works best for you with a validation set, recall that it's best to set it to [latex]rate \\approx 0.5[/latex] for hidden layers and [latex]rate \\approx 0.1[/latex] for the input layer (note that [latex]rate \\approx 0.1[/latex] equals [latex]p \\approx 0.9[/latex] - Keras turns the logic upside down, making rate the odds of dropping out rather than keeping neurons!)\n• Noise shape: if you wish to share noise across one of (batch, timesteps, features), you can set the noise shape for this purpose. Read more about noise shape here.\n• Seed: if you wish to fixate the pseudo-random generator that determines whether the Bernoulli variables are 1 or 0 (e.g., to rule out issues with the number generator), then you can set some seed by specifying an integer value here.\n\nImportant: once more, the drop rate (or 'rate') in Keras determines the odds of dropping out neurons - instead of keeping them. In effect, with respect to the parameter [latex]p[/latex] defined by Srivastava et al. (2014) when discussing Dropout, thus effectively means [latex]1-p[/latex]. If 75% of the neurons are kept with [latex]p = 0.75[/latex], must be [latex]0.25[/latex].\n\nLet's now take a look how to create a neural network with Keras that makes use of Dropout for reducing overfitting. For this purpose, we're creating a convolutional neural network for image classification. Next, we discuss the dataset we're using today and the design of our model.\n\nThese are a few samples from the CIFAR-10 dataset, which we will use today:\n\nThe CIFAR-10 dataset is one of the standard machine learning datasets and contains thousands of small natural images, divided in 10 classes. For example, it contains pictures of cats, trucks, and ships. It's one of the default choices when you want to show how certain models work.\n\nNext, the architecture of our model. Today, it looks like this:\n\nThis architecture, which contains two Conv2D layers followed by Max Pooling, as well as two Densely-connected layers, worked best in some empirical testing up front - so I chose it to use in the real training process.\n\nNote that Dropout is applied with [latex]rate = 0.50[/latex], and that - which is not visible in this diagram - max-norm regularization is applied as well, in each layer (also the Dense ones). The Conv2D layers learn 64 filters each and convolve with a 3x3 kernel over the input. The max pooling pool size will be 2 x 2 pixels.\n\nThe activation functions in the hidden layer are ReLU, and by consequence, we use He uniform init as our weight initialization strategy.\n\nIf you wish to run today's model, you'll need Keras - one of the popular deep learning frameworks these days. For this to run, you'll need one of the backends (preferably Tensorflow) as well as Python (or, although not preferably, R).\n\nOpen up your Explorer, navigate to some folder, and create a file called . Now open this file in your code editor of choice. There we go, we can start coding :)\n\nThe first thing we need to do is to list our imports:\n\nWe'll use the deep learning framework, from which we'll use a variety of functionalities. From , we import the CIFAR-10 dataset. It's a nice shortcut: Keras contains API pointers to datasets like MNIST and CIFAR-10, which means that you can load them with only a few lines of code. This way, we don't get buried with a lot of data loading work, so that we can fully focus on creating the model.\n\nFrom , we import (the densely-connected layer type), (which serves to regularize), (to link the convolutional layers with the Dense ones), and finally and - the conv & related layers.\n\nWe also import the model, which allows us to stack the layers nicely on top of each other, from .\n\nNext, we import the Keras for some data preparation functionalities.\n\nFinally, we import the Constraints, which is a Dropout best practice and should improve the model significantly.\n\nNext, we can specify some configuration parameters for the model:\n\nCIFAR-10 samples are 32 pixels wide and 32 pixels high, and therefore we set . Batch size is set to 250, which empirically worked best for CIFAR-10 with my model. I set the number of epochs to 55, because - as we shall see - the differences between dropout and no dropout will be pretty clear by then.\n\nThe number of classes our model will be able to handle - - is 10, which is the number of classes supported by the CIFAR-10 dataset. Verbosity mode is set to 1 (or ), sending all output to screen. 20% of the training data will be used for validation purposes.\n\nFinally, is set to 2.0. This value specifies the maximum norm that is acceptable for the max-norm regularization with the MaxNorm Keras constraint. Empirically, I found that 2.0 is a good value for today's model. However, if you use it with some other model and/or another dataset, you must experiment a bit to find a suitable value yourself.\n\nThe next steps to add are related to loading and preparing the CIFAR-10 dataset:\n\nWith the Keras call, it's possible to load CIFAR-10 very easily into variables for features and targets, for the training and testing datasets.\n\nOnce the data has been loaded, we reshape it based on the backend we're using - i.e., Tensorflow, Theano and CNTK - so that no matter the backend, the data has a uniform shape.\n\nNext, we parse numbers as floats, which presumably speeds up the training process. Subsequently, we normalize the data, which neural networks appreciate. Finally, we apply , to ensure that categorical crossentropy loss can be used for this multiclass classification problem.\n\nOnce the data has been loaded, we can define the architecture:\n\nIt's in line with the architectural diagram we discussed earlier. It has two Conv2D and related layers, two Dense layers, and outputs a multiclass probability distribution for a sample, with the Softmax activation function.\n\nThe next step is to compile the model. Compiling, or configuring the model, allows you to specify a loss function, an optimizer and additional metrics, such as accuracy. As said, we use categorical crossentropy loss to determine the difference between prediction and actual target. Additionally, we use the Adam optimizer - pretty much one of the standard optimizers today.\n\nOnce our model has been configured, we can the training data to the model! We do so by specifying the and variables, as well as batch size, number of epochs, verbosity mode and the validation split. We set their values earlier.\n\nThe final step is adding a metric for evaluation with the test set - to identify how well it generalizes to data it has not seen before. This allows us to compare various models, which we will do next.\n\nIf you wish to copy the entire model at once, here you go:\n\nIt's now time to run the model. Open up a terminal, to the folder where you put your file, and execute . Training then starts!\n\nGenerally speaking, the models converge at accuracies of approximately 65-75%, not uncommon for the CIFAR-10 dataset. However, what's important is to see whether the model is actually overfitting - and we can do so by inspecting the loss value.\n\nBecause what worth has a model with 75% accuracy, when it is overconfident in terms of deteriorating loss? You'd still not benefit from it in practice.\n\nI ran the model multiple times, each time comparing two situations:\n• Dropout with vs Dropout without max-norm regularization.\n• Dropout with Adam optimizer vs Dropout with SGD optimizer.\n\nThe difference is enormous for the Dropout vs No dropout case, clearly demonstrating the benefits of Dropout for reducing overfitting. As you can see, and primarily by taking a look at the loss value, the model without Dropout starts overfitting pretty soon - and does so significantly.\n\nThe model with Dropout, however, shows no signs of overfitting, and loss keeps decreasing. You even end up with a model that significantly outperforms the no-Dropout case, even in terms of accuracy. That's great news - we didn't do all our work for nothing!\n\nLet's now take a look what happens when we apply max-norm regularization versus when we leave it out.\n\nAs you can see, the difference is less significant than with the Dropout/No-dropout case, but it still matters. Our [latex]norm = 2.0[/latex] max-norm regularization (i.e., our MaxNorm Keras constraint) ensures that overfitting does not happen, whereas the no-max-norm case starts overfitting slightly. Indeed, Srivastava et al.'s (2014) results can be confirmed: adding max-norm regularization to Dropout leads to even better performance.\n\nWell, the results for this one clearly indicate that Adam performs much better when Dropout is applied, compared to traditional SGD. Likely, this is the case because Adam combines momentum and local parameter updates - benefiting the training process irrespective of Dropout.\n\nIn today's blog post, we've seen how to implement Dropout with Keras. Based on some theory, we implemented a ConvNet with Python that makes use of Dropout to reduce the odds of overfitting.\n\nThat Dropout really works was confirmed by our experiments. Having trained on the CIFAR-10 dataset, the ConvNet we created experiences substantial overfitting when Dropout is omitted, while no overfitting is reported with Dropout added.\n\nMax-norm regularization indeed benefits Dropout, reducing the odds of overfitting even further. Finally, it's also become clear that when using Dropout, it might be a good idea to use Adam and not traditional SGD.\n\nThank you for reading MachineCurve today and I hope you've learnt something from this article! 😀 If you did, I'd be happy to hear from you - feel free to leave a comment in the comments box below. Thanks again, and happy engineering! 😎\n\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014, June 15). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Retrieved from http://jmlr.org/papers/v15/srivastava14a.html\n\nMachineCurve. (2019, December 16). What is Dropout? Reduce overfitting in your neural networks. Retrieved from https://www.machinecurve.com/index.php/2019/12/16/what-is-dropout-reduce-overfitting-in-your-neural-networks"
    },
    {
        "link": "https://machinelearningmastery.com/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks",
        "document": "Generative Adversarial Networks, or GANs, are an architecture for training generative models, such as deep convolutional neural networks for generating images.\n\nThe GAN architecture is comprised of both a generator and a discriminator model. The generator is responsible for creating new outputs, such as images, that plausibly could have come from the original dataset. The generator model is typically implemented using a deep convolutional neural network and results-specialized layers that learn to fill in features in an image rather than extract features from an input image.\n\nTwo common types of layers that can be used in the generator model are a upsample layer (UpSampling2D) that simply doubles the dimensions of the input and the transpose convolutional layer (Conv2DTranspose) that performs an inverse convolution operation.\n\nIn this tutorial, you will discover how to use UpSampling2D and Conv2DTranspose Layers in Generative Adversarial Networks when generating images.\n\nAfter completing this tutorial, you will know:\n• Generative models in the GAN architecture are required to upsample input data in order to generate an output image.\n• The Upsampling layer is a simple layer with no weights that will double the dimensions of input and can be used in a generative model when followed by a traditional convolutional layer.\n• The Transpose Convolutional layer is an inverse convolutional layer that will both upsample input and learn how to fill in details during the model training process.\n\nKick-start your project with my new book Generative Adversarial Networks with Python, including step-by-step tutorials and the Python source code files for all examples.\n\nThis tutorial is divided into three parts; they are:\n• Need for Upsampling in GANs\n• How to Use the Upsampling Layer\n• How to Use the Transpose Convolutional Layer\n\nNeed for Upsampling in Generative Adversarial Networks\n\nGenerative Adversarial Networks are an architecture for neural networks for training a generative model.\n\nThe architecture is comprised of a generator and a discriminator model, both of which are implemented as a deep convolutional neural network. The discriminator is responsible for classifying images as either real (from the domain) or fake (generated). The generator is responsible for generating new plausible examples from the problem domain.\n\nThe generator works by taking a random point from the latent space as input and outputting a complete image, in a one-shot manner.\n\nA traditional convolutional neural network for image classification, and related tasks, will use pooling layers to downsample input images. For example, an average pooling or max pooling layer will reduce the feature maps from a convolutional by half on each dimension, resulting in an output that is one quarter the area of the input.\n\nConvolutional layers themselves also perform a form of downsampling by applying each filter across the input images or feature maps; the resulting activations are an output feature map that is smaller because of the border effects. Often padding is used to counter this effect.\n\nThe generator model in a GAN requires an inverse operation of a pooling layer in a traditional convolutional layer. It needs a layer to translate from coarse salient features to a more dense and detailed output.\n\nA simple version of an unpooling or opposite pooling layer is called an upsampling layer. It works by repeating the rows and columns of the input.\n\nA more elaborate approach is to perform a backwards convolutional operation, originally referred to as a deconvolution, which is incorrect, but is more commonly referred to as a fractional convolutional layer or a transposed convolutional layer.\n\nBoth of these layers can be used on a GAN to perform the required upsampling operation to transform a small input into a large image output.\n\nIn the following sections, we will take a closer look at each and develop an intuition for how they work so that we can use them effectively in our GAN models.\n\nHow to Use the UpSampling2D Layer\n\nPerhaps the simplest way to upsample an input is to double each row and column.\n\nFor example, an input image with the shape 2×2 would be output as 4×4.\n\nWorked Example Using the UpSampling2D Layer\n\nThe Keras deep learning library provides this capability in a layer called UpSampling2D.\n\nIt can be added to a convolutional neural network and repeats the rows and columns provided as input in the output. For example:\n\nWe can demonstrate the behavior of this layer with a simple contrived example.\n\nFirst, we can define a contrived input image that is 2×2 pixels. We can use specific values for each pixel so that after upsampling, we can see exactly what effect the operation had on the input.\n\nOnce the image is defined, we must add a channel dimension (e.g. grayscale) and also a sample dimension (e.g. we have 1 sample) so that we can pass it as input to the model.\n\nWe can now define our model.\n\nThe model has only the UpSampling2D layer which takes 2×2 grayscale images as input directly and outputs the result of the upsampling operation.\n\nWe can then use the model to make a prediction, that is upsample a provided input image.\n\nThe output will have four dimensions, like the input, therefore, we can convert it back to a 2×2 array to make it easier to review the result.\n\nTying all of this together, the complete example of using the UpSampling2D layer in Keras is provided below.\n\nRunning the example first creates and summarizes our 2×2 input data.\n\nNext, the model is summarized. We can see that it will output a 4×4 result as we expect, and importantly, the layer has no parameters or model weights. This is because it is not learning anything; it is just doubling the input.\n\nFinally, the model is used to upsample our input, resulting in a doubling of each row and column for our input data, as we expected.\n\nBy default, the UpSampling2D will double each input dimension. This is defined by the ‘size‘ argument that is set to the tuple (2,2).\n\nYou may want to use different factors on each dimension, such as double the width and triple the height. This could be achieved by setting the ‘size‘ argument to (2, 3). The result of applying this operation to a 2×2 image would be a 4×6 output image (e.g. 2×2 and 2×3). For example:\n\nAdditionally, by default, the UpSampling2D layer will use a nearest neighbor algorithm to fill in the new rows and columns. This has the effect of simply doubling rows and columns, as described and is specified by the ‘interpolation‘ argument set to ‘nearest‘.\n\nAlternately, a bilinear interpolation method can be used which draws upon multiple surrounding points. This can be specified via setting the ‘interpolation‘ argument to ‘bilinear‘. For example:\n\nThe UpSampling2D layer is simple and effective, although does not perform any learning.\n\nIt is not able to fill in useful detail in the upsampling operation. To be useful in a GAN, each UpSampling2D layer must be followed by a Conv2D layer that will learn to interpret the doubled input and be trained to translate it into meaningful detail.\n\nWe can demonstrate this with an example.\n\nIn this case, our little GAN generator model must produce a 10×10 image and take a 100 element vector from the latent space as input.\n\nFirst, a Dense fully connected layer can be used to interpret the input vector and create a sufficient number of activations (outputs) that can be reshaped into a low-resolution version of our output image, in this case, 128 versions of a 5×5 image.\n\nNext, the 5×5 feature maps can be upsampled to a 10×10 feature map.\n\nFinally, the upsampled feature maps can be interpreted and filled in with hopefully useful detail by a Conv2D layer.\n\nThe Conv2D has a single feature map as output to create the single image we require.\n\nTying this together, the complete example is listed below.\n\nRunning the example creates the model and summarizes the output shape of each layer.\n\nWe can see that the Dense layer outputs 3,200 activations that are then reshaped into 128 feature maps with the shape 5×5.\n\nThe widths and heights are doubled to 10×10 by the UpSampling2D layer, resulting in a feature map with quadruple the area.\n\nFinally, the Conv2D processes these feature maps and adds in detail, outputting a single 10×10 image.\n\nHow to Use the Conv2DTranspose Layer\n\nThe Conv2DTranspose or transpose convolutional layer is more complex than a simple upsampling layer.\n\nA simple way to think about it is that it both performs the upsample operation and interprets the coarse input data to fill in the detail while it is upsampling. It is like a layer that combines the UpSampling2D and Conv2D layers into one layer. This is a crude understanding, but a practical starting point.\n\nIn fact, the transpose convolutional layer performs an inverse convolution operation.\n\nSpecifically, the forward and backward passes of the convolutional layer are reversed.\n\nIt is sometimes called a deconvolution or deconvolutional layer and models that use these layers can be referred to as deconvolutional networks, or deconvnets.\n\nReferring to this operation as a deconvolution is technically incorrect as a deconvolution is a specific mathematical operation not performed by this layer.\n\nIn fact, the traditional convolutional layer does not technically perform a convolutional operation, it performs a cross-correlation.\n\n— Is the deconvolution layer the same as a convolutional layer?, 2016.\n\nIt is a very flexible layer, although we will focus on its use in the generative models from upsampling an input image.\n\nThe transpose convolutional layer is much like a normal convolutional layer. It requires that you specify the number of filters and the kernel size of each filter. The key to the layer is the stride.\n\nTypically, the stride of a convolutional layer is (1×1), that is a filter is moved along one pixel horizontally for each read from left-to-right, then down pixel for the next row of reads. A stride of 2×2 on a normal convolutional layer has the effect of downsampling the input, much like a pooling layer. In fact, a 2×2 stride can be used instead of a pooling layer in the discriminator model.\n\nThe transpose convolutional layer is like an inverse convolutional layer. As such, you would intuitively think that a 2×2 stride would upsample the input instead of downsample, which is exactly what happens.\n\nStride or strides refers to the manner of a filter scanning across an input in a traditional convolutional layer. Whereas, in a transpose convolutional layer, stride refers to the manner in which outputs in the feature map are laid down.\n\nThis effect can be implemented with a normal convolutional layer using a fractional input stride (f), e.g. with a stride of f=1/2. When inverted, the output stride is set to the numerator of this fraction, e.g. f=2.\n\nOne way that this effect can be achieved with a normal convolutional layer is by inserting new rows and columns of 0.0 values in the input data.\n\nLet’s make this concrete with an example.\n\nConsider an input image wit the size 2×2 as follows:\n\nAssuming a single filter with a 1×1 kernel and model weights that result in no changes to the inputs when output (e.g. a model weight of 1.0 and a bias of 0.0), a transpose convolutional operation with an output stride of 1×1 will reproduce the output as-is:\n\nWith an output stride of (2,2), the 1×1 convolution requires the insertion of additional rows and columns into the input image so that the reads of the operation can be performed. Therefore, the input looks as follows:\n\nThe model can then read across this input using an output stride of (2,2) and will output a 4×4 image, in this case with no change as our model weights have no effect by design:\n\nWorked Example Using the Conv2DTranspose Layer\n\nKeras provides the transpose convolution capability via the Conv2DTranspose layer.\n\nIt can be added to your model directly; for example:\n\nWe can demonstrate the behavior of this layer with a simple contrived example.\n\nFirst, we can define a contrived input image that is 2×2 pixels, as we did in the previous section. We can use specific values for each pixel so that after the transpose convolutional operation, we can see exactly what effect the operation had on the input.\n\nOnce the image is defined, we must add a channel dimension (e.g. grayscale) and also a sample dimension (e.g. we have 1 sample) so that we can pass it as input to the model.\n\nWe can now define our model.\n\nThe model has only the Conv2DTranspose layer, which takes 2×2 grayscale images as input directly and outputs the result of the operation.\n\nThe Conv2DTranspose both upsamples and performs a convolution. As such, we must specify both the number of filters and the size of the filters as we do for Conv2D layers. Additionally, we must specify a stride of (2,2) because the upsampling is achieved by the stride behavior of the convolution on the input.\n\nSpecifying a stride of (2,2) has the effect of spacing out the input. Specifically, rows and columns of 0.0 values are inserted to achieve the desired stride.\n\nIn this example, we will use one filter, with a 1×1 kernel and a stride of 2×2 so that the 2×2 input image is upsampled to 4×4.\n\nTo make it clear what the Conv2DTranspose layer is doing, we will fix the single weight in the single filter to the value of 1.0 and use a bias value of 0.0.\n\nThese weights, along with a kernel size of (1,1) will mean that values in the input will be multiplied by 1 and output as-is, and the 0 values in the new rows and columns added via the stride of 2×2 will be output as 0 (e.g. 1 * 0 in each case).\n\nWe can then use the model to make a prediction, that is upsample a provided input image.\n\nThe output will have four dimensions, like the input, therefore, we can convert it back to a 2×2 array to make it easier to review the result.\n\nTying all of this together, the complete example of using the Conv2DTranspose layer in Keras is provided below.\n\nRunning the example first creates and summarizes our 2×2 input data.\n\nNext, the model is summarized. We can see that it will output a 4×4 result as we expect, and importantly, the layer two parameters or model weights. One for the single 1×1 filter and one for the bias. Unlike the UpSampling2D layer, the Conv2DTranspose will learn during training and will attempt to fill in detail as part of the upsampling process.\n\nFinally, the model is used to upsample our input. We can see that the calculations of the cells that involve real values as input result in the real value as output (e.g. 1×1, 1×2, etc.). We can see that where new rows and columns have been inserted by the stride of 2×2, that their 0.0 values multiplied by the 1.0 values in the single 1×1 filter have resulted in 0 values in the output.\n\nRemember: this is a contrived case where we artificially specified the model weights so that we could see the effect of the transpose convolutional operation.\n\nIn practice, we will use a large number of filters (e.g. 64 or 128), a larger kernel (e.g. 3×3, 5×5, etc.), and the layer will be initialized with random weights that will learn how to effectively upsample with detail during training.\n\nIn fact, you might imagine how different sized kernels will result in different sized outputs, more than doubling the width and height of the input. In this case, the ‘padding‘ argument of the layer can be set to ‘same‘ to force the output to have the desired (doubled) output shape; for example:\n\nThe Conv2DTranspose is more complex than the UpSampling2D layer, but it is also effective when used in GAN models, specifically the generator model.\n\nEither approach can be used, although the Conv2DTranspose layer is preferred, perhaps because of the simpler generator models and possibly better results, although GAN performance and skill is notoriously difficult to quantify.\n\nWe can demonstrate using the Conv2DTranspose layer in a generator model with another simple example.\n\nIn this case, our little GAN generator model must produce a 10×10 image and take a 100-element vector from the latent space as input, as in the previous UpSampling2D example.\n\nFirst, a Dense fully connected layer can be used to interpret the input vector and create a sufficient number of activations (outputs) that can be reshaped into a low-resolution version of our output image, in this case, 128 versions of a 5×5 image.\n\nNext, the 5×5 feature maps can be upsampled to a 10×10 feature map.\n\nWe will use a 3×3 kernel size for the single filter, which will result in a slightly larger than doubled width and height in the output feature map (11×11).\n\nTherefore, we will set ‘padding‘ to ‘same’ to ensure the output dimensions are 10×10 as required.\n\nTying this together, the complete example is listed below.\n\nRunning the example creates the model and summarizes the output shape of each layer.\n\nWe can see that the Dense layer outputs 3,200 activations that are then reshaped into 128 feature maps with the shape 5×5.\n\nThe widths and heights are doubled to 10×10 by the Conv2DTranspose layer resulting in a single feature map with quadruple the area.\n\nThis section provides more resources on the topic if you are looking to go deeper.\n• Is The Deconvolution Layer The Same As A Convolutional Layer?, 2016.\n\nIn this tutorial, you discovered how to use UpSampling2D and Conv2DTranspose Layers in Generative Adversarial Networks when generating images.\n• Generative models in the GAN architecture are required to upsample input data in order to generate an output image.\n• The Upsampling layer is a simple layer with no weights that will double the dimensions of input and can be used in a generative model when followed by a traditional convolutional layer.\n• The Transpose Convolutional layer is an inverse convolutional layer that will both upsample input and learn how to fill in details during the model training process.\n\nDo you have any questions?\n\n Ask your questions in the comments below and I will do my best to answer."
    },
    {
        "link": "https://medium.com/data-science/machine-learning-part-20-dropout-keras-layers-explained-8c9f6dc4c9ab",
        "document": "In the proceeding example, we’ll be using Keras to build a neural network with the goal of recognizing hand written digits.\n\nWe use Keras to import the data into our program. The data is already split into…"
    },
    {
        "link": "https://stackoverflow.com/questions/53654310/what-is-the-difference-between-upsampling2d-and-conv2dtranspose-functions-in-ker",
        "document": "UpSampling2D is just a simple scaling up of the image by using nearest neighbour or bilinear upsampling, so nothing smart. Advantage is it's cheap.\n\nConv2DTranspose is a convolution operation whose kernel is learnt (just like normal conv2d operation) while training your model. Using Conv2DTranspose will also upsample its input but the key difference is the model should learn what is the best upsampling for the job."
    }
]