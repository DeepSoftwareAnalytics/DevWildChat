[
    {
        "link": "https://stackoverflow.com/questions/39750550/node-express-rest-api-application-passing-parameters-to-get-method",
        "document": "I'm writing a sample nodejs / express REST interface or API for the purposes of learning. I've created a new route called \"emergency\".\n\nIn the file I have the following code:\n\nWhen I start the application and navigate to http://myserver/tutorial1/emergency everything works fine and I see the \"respond with a resource\" message.\n\nI'd like my application to be able to accept parameters as well. So fro example, when a user navigates to\n\nI want all emergency numbers to be queried and returned. But they should also be able to do this:\n\nand the system should query the database for emergency record 12345 and return the appropriate result set.\n\nIn order to accommodate both types of GET queries, I've changed the code to look like this:\n\nNow when I run the application, and browse to\n\nit works. However, browsing to\n\nDo I need to create two separate methods, one that accepts a parameter and one that doesn't? (aka method overloads?) Perhaps my understanding of REST is what's faulty. Should a GET request look like :\n\nor should it always look like this:\n\nMaybe the proper way to do a GET for all records is something like this:\n\nI'm trying to google my question right now as well, but I'm having a hard time expressing it succinctly enough to get an accurate search result set.\n\nThis is what my code looks like when I try to create two methods (and this works)\n\nBut this just feels odd because I guess I'm used to other frameworks in other languages where the system can check for empty params so you just need one method. this is not a complaint! just a comment that might explain why my brain is \"expecting\" the system to work a different way."
    },
    {
        "link": "https://blog.postman.com/how-to-create-a-rest-api-with-node-js-and-express",
        "document": "How to create a REST API with Node.js and Express\n\nNode.js is an ideal choice for developers who are looking to create fast and efficient web applications with RESTful APIs. In fact, it is the most widely used web development tool in the United States. But what makes Node.js so powerful? Why should you learn to use Node.js for building REST APIs, and what is the best way to go about it?\n\nIn this post, we’ll explore the answers to these questions and more. By the end, you’ll be able to set up a server, define routes, handle requests and responses, and work with databases. You can also get some hands-on experience by coding along with us and building a user management API with endpoints for user registration, login, profile management, and more.\n\nUse the links below to navigate to the section that interests you most:\n• Why use Node.js to build your REST API?\n• How to set up a Node.js app?\n• How to create a user management API with Node.js and Express?\n\nRelated: What is an API?\n\nREST, which stands for REpresentational State Transfer, is a software development architecture that defines a set of rules for communication between a client and a server. Let’s break this down a little more:\n• A REST client is a code or app used to communicate with REST servers.\n• A server contains resources that the client wants to access or change.\n• A resource is any information that the API can return.\n\nA REST API, also known as a RESTful API, is an API that conforms to the REST architecture. These APIs use the HTTP protocol to access and manipulate data on the server.\n\nThe essential components of a REST API include the HTTP method, endpoint, headers, and body. Here’s an example of a REST API that gives you a list of astronauts who are currently in space.\n\nThe HTTP method defines the action the client wants to make on the server, which includes creating, reading, updating, or deleting resources (CRUD). Here are four HTTP methods that are commonly used in REST APIs:\n• GET: used to retrieve resources.\n\nNow that we’ve covered the fundamentals of RESTful APIs, let’s look at why and how you can build them with Node.js.\n\nBy definition, Node.js is an open-source and cross-platform JavaScript runtime environment that runs based on Chrome’s V8 engine.\n\nTo break this down a little further, you can:\n• \n• Execute your JavaScript program or application on the server instead of the browser using Node.js (runtime environment).\n\nWhy should you use Node.js and Express to build your REST API?\n\nHere are four key advantages of Node.js and Express:\n• The ability to use a single language (JavaScript) for both client-side and server-side development.\n• Fast and powerful performance, owing to the ability to run multiple requests in parallel.\n• Middleware and routing capabilities that are built into Express to make API development quick and easy.\n• A large, active community of developers contributing to the ecosystem.\n\nAs you develop the user management API, you’ll start seeing these benefits quickly. Let’s get started.\n\nTo follow along, you need to have the following:\n\nIf you don’t have a Postman account yet, you can sign up for free here.\n\nThe first thing we’ll need to do is install Node.js on our machine. You can download the latest LTS version from the official Node.js website. Follow the prompts in the Node.js Installer and customize the defaults, if necessary. When you’re done, you should have installed Node.js, as well as NPM (Node Package Manager). You can verify the installation by running the following commands in your terminal:\n\nIf you see the versions of Node.js and NPM show up, your installation was successful.\n\nNext, we’ll create a new folder for the project by running the following command in your terminal (note that entering this command as-is will name your project “node rest api,” but you can change the name, if you’d like):\n\nTo navigate to your project, enter this command:\n\nTo initialize your app, run the following command in your terminal:\n\nYou will be prompted to enter your project name, description, and GitHub repository. You can accept the defaults by pressing Enter/Return, or customize them.\n\nNext, open this project in your editor, where you will see a new file called . This file contains the data you added about your project in the terminal. It also describes how you’re going to run the project and lists its dependencies (frameworks and libraries).\n\nFrom here on, you can run all your commands in your editor’s terminal.\n\nRun the following command to install the Express framework:\n\nWe’ll start by creating a new file named in the root of the project directory. We’ll use this file to set up the app. Then, we’ll load the dependencies so we can use them. In the file, add the following code to import Express:\n\nNow, let’s set up Express to create an app and configure it to parse requests with JSON payloads. Here’s the code you can add to do that:\n\nNow we need to make this application a server by getting it to listen for connections. To do this, we’ll connect to a port to listen for incoming requests.\n\nIn the file, we’ll add the following code to define the server code:\n\nWith the variable, we set up the port automatically by allowing the API to be deployed to a cloud platform like AWS or Azure. In case the variable is not set, we’ll default to using port 3000.\n\nNext, we’ll add the following code to the file in order to set up the server to listen on the specified port:\n\nLet’s start by defining a status endpoint to ensure the API is working.\n\nExpress lets you define routes using the function. Here, METHOD refers to the different HTTP methods, like GET, POST, PUT, and DELETE. For a GET request, you’d define the route by adding an function. This function has two parameters. We’ll use the first parameter to define the path. In this case, it is the endpoint:\n\nNext, we’ll add a callback function as the second parameter, which defines what we will do when the request is called. This function has two parameters: the request object (which contains details like the HTTP method, headers, and request body) and the response object (which defines the information that we want to send). The response (res) object contains different methods of sending a response to the client, such as , , and .\n\nHere’s what it looks like now:\n\nWith , we then define the response we want to return. But since we want to send back JSON, we’ll need to first define a JSON object. So, we define a status variable and create an object:\n\nChallenge for you: Go to your Postman account and test the endpoint with the GET request method. You should get a response that says “Running,” as shown below:\n\nIf you receive the expected response, congratulations! You’ve just created your first API.\n\nSimilarly, you can use , , and to handle other request methods.\n\nNow, we’ll create the following API endpoints for user management:\n• GET : Retrieving a user’s profile (restricted to the user themselves)\n• PATCH : Updating a user’s profile (restricted to the user themselves)\n• GET : Retrieving all users (available to all users)\n\nWe’ll also look at how you can use JSON Web Tokens (JWTs) to set up authentication for certain endpoints.\n\nWe recommend trying out the user management API to understand what responses you can expect.\n\nTo get started, clone the companion repository that contains the entire code for the app and use this tutorial to understand the basics of how we go about creating the user management app.\n\nOnce you’ve cloned the repository, navigate to the project, where you’ll find the following folders:\n• authorization: contains everything related to the and endpoints.\n• common: contains middlewares and models used for all endpoints.\n• storage: stores all the data for the app locally.\n• users: contains everything related to all of the user endpoints.\n\nRun to install the following libraries that we will be using:\n• Sequelize: A Node.js ORM (object-relational mapper) tool that helps with connecting to a database.\n• SQLite3: A library that helps us to create a database, define tables, and manage a SQLite database file.\n• jsonwebtoken: An implementation of JSON Web Token (JWT), an open standard (RFC 7519), used to securely transmit information between parties as a JSON object.\n• AJV (Another JSON Schema Validator): A library that helps us validate the payload received against a structure defined using a JSON Schema.\n\nIn real-world apps, we use databases to store data more efficiently. Since this is only a simple project, we wanted to keep things easy. So, we will build the API with SQLite, define the structure of the data in , and store data in .\n\nWe’ll start by creating a new file called . Next, in the same file, we will define our schema, which holds all the properties of a user (such as , , and ).\n\nWe’ll also specify the data type—and whether the data can be left void—for every user property. For the user ID, we’ll use auto-increment to automatically create a unique number when a new record is inserted into the table:\n\nSimilarly, you can also define other properties, such as , , , , , and , in the schema.\n\nIn the same file, we will define an initialize method to register our schema with the Sequelize library and return a model. The returned model can then be used to manipulate (i.e., create, read, update, and delete) the data stored in the database.\n\nHere’s how you can define a function to create a user:\n\nYou can also define wrapper functions that consume the Sequelize library to get and update entries in the database, just like we did for .\n\nNow, we’re all set to define operations with the module. Let’s look at how to define the endpoint.\n\nWe’ll start by creating a file named , which will hold all the controller functions for the different routes related to the user module, like , , etc.\n\nRefer to the example below to understand how to set up the controller function. This function uses the function created in the file above to fetch all the users from the table and return them in the response:\n\nAt the top of the file, add the following code to import the router:\n\nNext, we will import the in the file and define the route as shown below:\n\nNext, we need to register the file in our file so that any calls made to the endpoints are routed to the correct file. To do that, we add the following code:\n\nIn this tutorial, we’ll implement authentication and authorization using JSON Web Tokens (JWT). JWT generates a valid token for the user and ensures that only authenticated users can access specific API endpoints.\n\nWhen the user provides the necessary payload while registering or logging in, we will provide a token in return. The token typically expires after a period of time. To keep things simple, we will only focus on creating a single token.\n\nNow we can define the endpoint. The payload needs to contain , , , , , , and . Here’s an example:\n\nWe will set up a controller to create a new user in the User table using helper functions defined in —and also generate a JWT that will be returned as the response. Additionally, before storing the user in the table, we will hash the password using the SHA246 algorithm for better security:\n\nThe function used in the code above uses the jsonwebtoken library to generate a JWT that can be returned back to the user.\n\nNow, we need to create the route and invoke the controller function that we just created. We can do that by adding the following code in :\n\nLikewise, you can also define the endpoint.\n\nWe also need to register the file in our file so that any calls made to the or endpoints are routed to the correct file. To do that, we add the following code:\n\nWe need to make sure that only authenticated users can securely access data in the Users resource.\n\nLet us define a middleware that checks if the user is using a valid JWT. To do this, we’ll add the following code in :\n\nHere, we’re checking for the presence of auth headers. If no auth headers are provided, we return a 401 unauthorized error. Likewise, we also add checks and return the same error when a bearer auth header is not provided, or when a bearer auth header is provided, but a token is not provided.\n\nIn , we’ll check for permissions and validate if the user:\n• exists in our database (to access the endpoint).\n• has the required role (to access the and endpoints).\n\nHere, we’ll check if a user is in our database, and return a 403 forbidden error if not:\n\nNow, we’ll add the authentication middleware to the user’s module routes in :\n\nWe can update the endpoint created to the code given below:\n\nThis way, the endpoint can only be accessed by a logged in user who has the role.\n\nIf you’ve tried this out on your own, congratulations on creating your first app using Node.js! You now have a solid foundation for building powerful and scalable APIs for your web applications.\n\nYou can continue building this app by adding or updating user properties, filtering the users by a particular property (with a query parameter), or connecting to another database like MongoDB etc or MySQL. You can also check out the product directory app added in the Products folder.\n\nIf you have any questions, suggestions, or feedback for us, feel free to drop them in the comments below. We’d love to hear from you!\n\nWhat database should I use with Node.js?\n\nIn this tutorial, we used SQLite to store the data in a file. SQLite is a great library that allows you to create a database, define tables, and manage a SQLite database file. It also helps you connect with a range of databases— including MongoDB, MySQL, and PostgreSQL—and run queries to manage data in the database. However, you can use any other database based on your requirements by installing the appropriate Node.js driver and modifying the controller functions to interact with the database.\n\nWhat is the best framework for building REST APIs in Node.js?\n\nWhile Feathers, Nest, LoopBack, and Moleculer are great frameworks for building REST APIs, here are a few reasons why Express is the best option:\n• It offers features like Model-View-Controller (MVC), robust routing capabilities, and built-in middleware to enable you to create dynamic, scalable applications.\n• It has a large Express community that can help you be faster and more efficient.\n• It is easy to set up, learn, and use, making it an ideal choice for beginners.\n\nThis blog was co-authored by Arvind Kalra, who also worked on the companion code repository."
    },
    {
        "link": "https://medium.com/@onejohi/building-a-simple-rest-api-with-nodejs-and-express-da6273ed7ca9",
        "document": "Have you been working on front-end technologies and been feeling like you’re missing out something in the whole development process? Well, here’s a good place to start. If you’re been building apps using HTML, CSS and JavaScript, you may want to learn how to build servers that serve content to these front end technologies. For a more interactive in-depth tutorial, you can refer to this YouTube video I made for this article.\n\nFirst, you’ll need to recognize the difference between serving static assets and serving data. Serving static files is serving your HTML, CSS and JavaScript pages as they are. The reason they’re called static files is because they are not changed by the server nor run, they’re merely sent back as files for your browser to parse through. This is what you’ve most probably been doing without even realizing.\n\nExpress is a really cool Node framework that’s designed to help JavaScript developers create servers really quickly. NodeJS may be server side, but it can do a whole lot more than just serve pages and data. NodeJS is a powerful platform that lets you run JavaScript on your shell similar to how you’d run Python.\n\nTo get started, head over to https://nodejs.org and download the most stable release of NodeJS. It’s noteworthy to remember not to download the latest version since it may contain bugs and features that could be removed from the final version. Once you download NodeJS binaries, install it into your machine using the instructions provided on the page relative to your platform.\n\nTo confirm installation, close any open cmd instance that may be running and start a new instance. Type the following two commands to display the version of Node installed and NPM. NPM is the Node Package Manager and is a tool for installing, uninstalling and maintaining package modules for your app.\n\nThere are three ways to initialize a new express app. I’ll go over them briefly explaining each but will only showcase the second one.\n\nThe first way is creating the files by hand. A basic Node app contains a .js file and a package.json file. The package.json file contains a couple of properties. First one is name which holds the name of the app, second is version which shows the version of your app, a description of your app, main that points to the entry point of your application. There’s also scripts, that can be run when you need to perform some repetitive tasks, author name, licence, dependencies and devDependencies.\n\nThe package.json describes the app. It is very important. When uploading your app, your dependencies will be listed avoiding duplication and excessive data transfer. An angular 6 app node modules is around 230MB, that will take a lot of time to download or upload. So we omit these modules and just list them instead, then use the package.json to install the modules whenever we need to run the app on another machine. To understand this concept, I’ll explain it better when making an introduction to Git tutorial.\n\nThe second way you can initialize an app is using the npm tool. It’s the simplest but not the fastest way. All you have to do is open up your cmd in the folder you want to create your app in and type npm init to interactively create your package.json file.\n\nThe third way is the simplest, but a little complex for beginners as it creates files that you will be unfamiliar with especially if you’ve never done NodeJS. It also required you to install the express tool to generate a complete express template and not just the package.json.\n\nThis installs the express-generator tool that generates a complete express app. As I told you before, npm is a useful tool in installing modules that you might need. But wait a minute, what does the -g flag do? When installing modules to use in JS, you might want to use the modules in that specific app like installing mongoose so you can use mongoose methods to query data from your MongoDB instance. But then, you can also install modules/tools that you can use anywhere on your computer. These tools are available on the command line but only available if you install them globally. The -g flags specifies that you’re installing the module globally to use anywhere within your computer. Not using the -g flag will install the tool/module into that specific folder and will not be available outside that scope.\n\nPlease use the second method to generate a package.json file almost similar to the one pictured above. Second thing we want to do is install Express into our app. Note, we aren’t going to install express globally as we need to use it in this specific folder/app. Use the command below to install Express.\n\nThe save flag is used to edit your package.json file and add express as a dependency. After the installation is complete, open up your package.json to see express listed as a dependency. With this, you could send just your code and package.json file to a friend and request them to use npm to install the dependencies on their computer saving you some amount of data. To install, your friend will have to open up cmd inside the app folder and use the command npm install.\n\nThen create the app.js file or whatever you prefer naming it (default is index.js) and add in the following code.\n\nCongrats! You just made your first useless express server! So let’s go through the code and learn why our server is useless and why it’s not implementing the REST protocol yet. The first line requires express and uses the express variable to represent it. The second line initialized express using the brackets which initializes an express server and puts the initialized server into the variable app. So now whenever we want to use our express server, we would need to use the app variable which represents our app! We then set our app to listen to port 3000 and create a callback function that says our server is running on port 3000.\n\nYour app will now be accessible using http://localhost:3000, but hitting that endpoint now won’t get you anything since you haven’t configured your server to listen to any events.\n\nA server receives requests, processes them and returns a response. So you need to use routes to handle this requests. The requests have three major types, a GET request that get’s data, a POST request that sends data securely, a PUT request that updates data and a DELETE request that deletes data.\n\nLet’s create a simple GET request that returns a list of users. Under var app = express(), write down the following code.\n\nThis simple function makes the express app to use the url handle “/url” to trigger the callback that follows it. The callback accepts three parameters, req is the request body and carries information about the request. The res is the response body and is used to handle response functions like .render() to render templates and .json() to return json data.\n\nTo run your app, use the command below.\n\nThis is what your cmd should look like after running this command.\n\nThis means our app is now successfully running on port 3000. To view our data, open up your browser and enter http://localhost:3000/url. You’ll expect to see something like this on your browser.\n\nThe data returned is an array of string. This is raw data.\n\nHow it all fits as a REST based API.\n\nYou might be wondering where the REST attribute comes in. REST stands for REpresentational State Transfer. This means there is no state between the client and the server. There are no webpages served to be parsed, just data. And this gives you all the freedom you need. All you need to do is write some logic on a specific URL that connects to a database, uses it’s logic to process the data and return it in JSON format. Your client can now be an Android app made in Java, or a Windows desktop app made in C# or an Arduino project.\n\nThis is the whole point of using REST, it makes the connection stateless therefore any client that utilizes the HTTP protocol can access this data. Now you can iterate through the data and display it anywhere you want.\n\nUnbelievable as it seems, this is a basic REST based API. You make a request to a specific endpoint and get data back in a stateless manner. Very simply put. No complexity here. This is the most basic API you’ll need to do to understand how REST APIs work. In future tutorials, I’ll show you how to connect to a database, query data and return the data using REST protocol.\n\nBelow is an image of my PostMan app querying the same server.\n\nSo as you can see, our server isn’t restricted to browsers only. You can use Native apps and IoT devices as well to get data as long as it implements the HTTP protocol.\n\nIf Express is really exciting for you, you might wonder where to use this knowledge. Well, there are a number of ways I’d suggest. Express is a backend framework, you might want to expand to the frontend as well after learning how the backend works, that would make you a full stack developer that can develop both the technology that generates and stores data and the one that consumes and displays the data. The MEAN Stack would be a great model to learn with, although I must admit it is more of preference. The best thing about the MEAN Stack is the use of JavaScript syntax across your entire product, from frontend, backend to even the database. MongoDB saves data in BJSON which looks strikingly identical to JSON making it super easy to learn. If you choose to learn the MEAN Stack, here is a great article I made that get you started in a couple of minutes.\n\nThanks for going through my article, hope it’s helped you."
    },
    {
        "link": "https://expressjs.com/en/api.html",
        "document": "Creates an Express application. The function is a top-level function exported by the module.\n\nThe object conventionally denotes the Express application. Create it by calling the top-level function exported by the Express module:\n\nThe object has methods for\n• Routing HTTP requests; see for example, app.METHOD and app.param.\n\nIt also has settings (properties) that affect how the application behaves; for more information, see Application settings.\n\nAdd callback triggers to route parameters, where is the name of the parameter or an array of them, and is the callback function. The parameters of the callback function are the request object, the response object, the next middleware, the value of the parameter and the name of the parameter, in that order. If is an array, the trigger is registered for each parameter declared in it, in the order in which they are declared. Furthermore, for each declared parameter except the last one, a call to inside the callback will call the callback for the next declared parameter. For the last parameter, a call to will call the next middleware in place for the route currently being processed, just like it would if were just a string. For example, when is present in a route path, you may map user loading logic to automatically provide to the route, or perform validations on the parameter input. // try to get the user details from the User model and attach it to the request object Param callback functions are local to the router on which they are defined. They are not inherited by mounted apps or routers, nor are they triggered for route parameters inherited from parent routers. Hence, param callbacks defined on will be triggered only by route parameters defined on routes. All param callbacks will be called before any handler of any route in which the param occurs, and they will each be called only once in a request-response cycle, even if the parameter is matched in multiple routes, as shown in the following examples. On , the following is printed: CALLED ONLY ONCE although this matches and this matches too On , the following is printed: CALLED ONLY ONCE with 42 CALLED ONLY ONCE with 3 although this matches and this matches too The following section describes , which is deprecated as of v4.11.0. The behavior of the method can be altered entirely by passing only a function to . This function is a custom implementation of how should behave - it accepts two parameters and must return a middleware. The first parameter of this function is the name of the URL parameter that should be captured, the second parameter can be any JavaScript object which might be used for returning the middleware implementation. The middleware returned by the function decides the behavior of what happens when a URL parameter is captured. In this example, the signature is modified to . Instead of accepting a name and a callback, will now accept a name and a number. In this example, the signature remains the same, but instead of a middleware callback, a custom data type checking function has been defined to validate the data type of the user id. The ‘ ’ character can’t be used to capture a character in your capturing regexp. For example you can’t use to capture , use or instead (as in . // captures '1-a_6' and '543-az(ser\"-sder' but not '5-a s'\n\nThe object represents the HTTP request and has properties for the request query string, parameters, body, HTTP headers, and so on. In this documentation and by convention, the object is always referred to as (and the HTTP response is ) but its actual name is determined by the parameters to the callback function in which you’re working.\n\nBut you could just as well have:\n\nThe object is an enhanced version of Node’s own request object and supports all built-in fields and methods.\n\nThe object represents the HTTP response that an Express app sends when it gets an HTTP request.\n\nIn this documentation and by convention, the object is always referred to as (and the HTTP request is ) but its actual name is determined by the parameters to the callback function in which you’re working.\n\nBut you could just as well have:\n\nThe object is an enhanced version of Node’s own response object and supports all built-in fields and methods."
    },
    {
        "link": "https://geeksforgeeks.org/handling-requests-and-responses-in-node-js",
        "document": "Node.js is a powerful JavaScript runtime for building server-side applications. It provides an efficient way to handle HTTP requests and responses using the built-in http module or frameworks like Express.js.\n\nAn HTTP request is sent by a client (browser or API) to a server, and the server processes it to return an HTTP response. The response contains a status code, headers, and body content.\n• GET : Retrieves data without modifying it, ensuring the same result for multiple requests.\n• POST : Sends data to create a resource, potentially resulting in duplicates if repeated.\n• PUT : Fully updates an existing resource, replacing all its current data.\n• DELETE : Removes a resource from the server, ensuring consistent results across requests.\n• Status Code : Represents the outcome of the request (e.g., 200 OK, 404 Not Found, 500 Server Error).\n• Headers : Provide metadata about the response, such as content type and caching.\n• Body : Contains the actual data sent back to the client, in formats like JSON, HTML, or plain text.\n\nNode.js has a built-in http module to create an HTTP server and handle requests.\n\nCreates an HTTP server that listens for requests and responds with a message.\n\nDefines different behaviors for GET and POST requests.\n\nInitializes an Express server and sets up a basic route.\n\nProvides a centralized way to manage server errors.\n\nBest Practices for Handling Requests and Responses\n\nWhat is the difference between PUT and PATCH methods?\n\nWhen should I use POST instead of GET?\n\nHow do I handle JSON data in requests?\n\nWhat status code should I return for a successful POST request?\n\nHow can I handle multiple HTTP methods for the same endpoint?"
    },
    {
        "link": "https://mongodb.com/developer/languages/javascript/getting-started-with-mongodb-and-mongoose",
        "document": ""
    },
    {
        "link": "https://developer.mozilla.org/en-US/docs/Learn_web_development/Extensions/Server-side/Express_Nodejs/mongoose",
        "document": "Before you jump in and start coding the models, it's worth taking a few minutes to think about what data we need to store and the relationships between the different objects. We know that we need to store information about books (title, summary, author, genre, ISBN) and that we might have multiple copies available (with globally unique ids, availability statuses, etc.). We might need to store more information about the author than just their name, and there might be multiple authors with the same or similar names. We want to be able to sort information based on the book title, author, genre, and category. When designing your models it makes sense to have separate models for every \"object\" (a group of related information). In this case some obvious candidates for these models are books, book instances, and authors. You might also want to use models to represent selection-list options (e.g. like a drop-down list of choices), rather than hard-coding the choices into the website itself — this is recommended when all the options aren't known up front or may change. A good example is a genre (e.g. fantasy, science fiction, etc.). Once we've decided on our models and fields, we need to think about the relationships between them. With that in mind, the UML association diagram below shows the models we'll define in this case (as boxes). As discussed above, we've created models for the book (the generic details of the book), book instance (status of specific physical copies of the book available in the system), and author. We have also decided to have a model for the genre so that values can be created dynamically. We've decided not to have a model for the — we will hard code the acceptable values because we don't expect these to change. Within each of the boxes, you can see the model name, the field names and types, and also the methods and their return types. The diagram also shows the relationships between the models, including their multiplicities. The multiplicities are the numbers on the diagram showing the numbers (maximum and minimum) of each model that may be present in the relationship. For example, the connecting line between the boxes shows that and a are related. The numbers close to the model show that a must have zero or more s (as many as you like), while the numbers on the other end of the line next to the show that a book can have zero or more associated s. Note: As discussed in our Mongoose primer below it is often better to have the field that defines the relationship between the documents/models in just one model (you can still find the reverse relationship by searching for the associated in the other model). Below we have chosen to define the relationship between / and / in the Book schema, and the relationship between the / in the Schema. This choice was somewhat arbitrary — we could equally well have had the field in the other schema. Note: The next section provides a basic primer explaining how models are defined and used. As you read it, consider how we will construct each of the models in the diagram above.\n\nDatabase methods to create, find, update, or delete records are asynchronous. What this means is that the methods return immediately, and the code to handle the success or failure of the method runs at a later time when the operation completes. Other code can execute while the server is waiting for the database operation to complete, so the server can remain responsive to other requests. JavaScript has a number of mechanisms for supporting asynchronous behavior. Historically JavaScript relied heavily on passing callback functions to asynchronous methods to handle the success and error cases. In modern JavaScript callbacks have largely been replaced by Promises. Promises are objects that are (immediately) returned by an asynchronous method that represent its future state. When the operation completes, the promise object is \"settled\", and resolves an object that represents the result of the operation or an error. There are two main ways you can use promises to run code when a promise is settled, and we highly recommend that you read How to use promises for a high level overview of both approaches. In this tutorial, we'll primarily be using to wait on promise completion within an , because this leads to more readable and understandable asynchronous code. The way this approach works is that you use the keyword to mark a function as asynchronous, and then inside that function apply to any method that returns a promise. When the asynchronous function is executed its operation is paused at the first method until the promise settles. From the perspective of the surrounding code the asynchronous function then returns and the code after it is able to run. Later when the promise settles, the method inside the asynchronous function returns with the result, or an error is thrown if the promise was rejected. The code in the asynchronous function then executes until either another is encountered, at which point it will pause again, or until all the code in the function has been run. You can see how this works in the example below. is an asynchronous function that is called within a block. When is run, code execution is paused at until the promise resolves, at which point the code continues to and waits again. The code in the block runs if an error is thrown in the asynchronous function, and this will happen if the promise returned by either of the methods is rejected. The asynchronous methods above are run in sequence. If the methods don't depend on each other then you can run them in parallel and finish the whole operation more quickly. This is done using the method, which takes an iterable of promises as input and returns a single . This returned promise fulfills when all of the input's promises fulfill, with an array of the fulfillment values. It rejects when any of the input's promises rejects, with this first rejection reason. The code below shows how this works. First, we have two functions that return promises. We on both of them to complete using the promise returned by . Once they both complete returns and the results array is populated, the function then continues to the next , and waits until the promise returned by is settled. You would call the in a block to catch any errors. Promises with / allow both flexible and \"comprehensible\" control over asynchronous execution!\n\nModels are defined using the interface. The Schema allows you to define the fields stored in each document along with their validation requirements and default values. In addition, you can define static and instance helper methods to make it easier to work with your data types, and also virtual properties that you can use like any other field, but which aren't actually stored in the database (we'll discuss a bit further below). Schemas are then \"compiled\" into models using the method. Once you have a model you can use it to find, create, update, and delete objects of the given type. Note: Each model maps to a collection of documents in the MongoDB database. The documents will contain the fields/schema types defined in the model . The code fragment below shows how you might define a simple schema. First you mongoose, then use the Schema constructor to create a new schema instance, defining the various fields inside it in the constructor's object parameter. In the case above we just have two fields, a string and a date. In the next sections, we will show some of the other field types, validation, and other methods. Models are created from schemas using the method: The first argument is the singular name of the collection that will be created for your model (Mongoose will create the database collection for the model SomeModel above), and the second argument is the schema you want to use in creating the model. Note: Once you've defined your model classes you can use them to create, update, or delete records, and run queries to get all records or particular subsets of records. We'll show you how to do this in the Using models section, and when we create our views. A schema can have an arbitrary number of fields — each one represents a field in the documents stored in MongoDB. An example schema showing many of the common field types and how they are declared is shown below. const schema = new Schema({ name: String, binary: Buffer, living: Boolean, updated: { type: Date, default: Date.now() }, age: { type: Number, min: 18, max: 65, required: true }, mixed: Schema.Types.Mixed, _someId: Schema.Types.ObjectId, array: [], ofString: [String], // You can also have an array of each of the other types too. nested: { stuff: { type: String, lowercase: true, trim: true } }, }); Most of the SchemaTypes (the descriptors after \"type:\" or after field names) are self-explanatory. The exceptions are:\n• : Represents specific instances of a model in the database. For example, a book might use this to represent its author object. This will actually contain the unique ID ( ) for the specified object. We can use the method to pull in the associated information when needed.\n• : An array of items. You can perform JavaScript array operations on these models (push, pop, unshift, etc.). The examples above show an array of objects without a specified type and an array of objects, but you can have an array of any type of object. The code also shows both ways of declaring a field:\n• Field name and type as a key-value pair (i.e. as done with fields , and ).\n• Field name followed by an object defining the , and any other options for the field. Options include things like:\n• Whether the field is required\n• Whether fields should automatically be set to lowercase, uppercase, or trimmed (e.g. ) For more information about options see SchemaTypes (Mongoose docs). Mongoose provides built-in and custom validators, and synchronous and asynchronous validators. It allows you to specify both the acceptable range of values and the error message for validation failure in all cases.\n• All SchemaTypes have the built-in required validator. This is used to specify whether the field must be supplied in order to save a document.\n• \n• enum: specifies the set of allowed values for the field.\n• match: specifies a regular expression that the string must match.\n• maxLength and minLength for the string. The example below (slightly modified from the Mongoose documents) shows how you can specify some of the validator types and error messages: const breakfastSchema = new Schema({ eggs: { type: Number, min: [6, \"Too few eggs\"], max: 12, required: [true, \"Why no eggs?\"], }, drink: { type: String, enum: [\"Coffee\", \"Tea\", \"Water\"], }, }); For complete information on field validation see Validation (Mongoose docs). Virtual properties are document properties that you can get and set but that do not get persisted to MongoDB. The getters are useful for formatting or combining fields, while setters are useful for de-composing a single value into multiple values for storage. The example in the documentation constructs (and deconstructs) a full name virtual property from a first and last name field, which is easier and cleaner than constructing a full name every time one is used in a template. Note: We will use a virtual property in the library to define a unique URL for each model record using a path and the record's value. For more information see Virtuals (Mongoose documentation). A schema can also have instance methods, static methods, and query helpers. The instance and static methods are similar, but with the obvious difference that an instance method is associated with a particular record and has access to the current object. Query helpers allow you to extend mongoose's chainable query builder API (for example, allowing you to add a query \"byName\" in addition to the , and methods).\n\nOnce you've created a schema you can use it to create models. The model represents a collection of documents in the database that you can search, while the model's instances represent individual documents that you can save and retrieve. We provide a brief overview below. For more information see: Models (Mongoose docs). Note: Creation, update, deletion and querying of records are asynchronous operations that return a promise. The examples below show just the use of the relevant methods and (i.e. the essential code for using the methods). The surrounding and block to catch errors are omitted for clarity. For more information on using see Database APIs are asynchronous above. To create a record you can define an instance of the model and then call on it. The examples below assume is a model (with a single field ) that we have created from our schema. // Create an instance of model SomeModel const awesome_instance = new SomeModel({ name: \"awesome\" }); // Save the new model instance asynchronously await awesome_instance.save(); You can also use to define the model instance at the same time as you save it. Below we create just one, but you can create multiple instances by passing in an array of objects. Every model has an associated connection (this will be the default connection when you use ). You create a new connection and call on it to create the documents on a different database. You can access the fields in this new record using the dot syntax, and change the values. You have to call or to store modified values back to the database. // Access model field values using dot notation console.log(awesome_instance.name); //should log 'also_awesome' // Change record by modifying the fields, then calling save(). awesome_instance.name = \"New cool name\"; await awesome_instance.save(); You can search for records using query methods, specifying the query conditions as a JSON document. The code fragment below shows how you might find all athletes in a database that play tennis, returning just the fields for athlete name and age. Here we just specify one matching field (sport) but you can add more criteria, specify regular expression criteria, or remove the conditions altogether to return all athletes. const Athlete = mongoose.model(\"Athlete\", yourSchema); // find all athletes who play tennis, returning the 'name' and 'age' fields const tennisPlayers = await Athlete.find( { sport: \"Tennis\" }, \"name age\", ).exec(); Note: It is important to remember that not finding any results is not an error for a search — but it may be a fail-case in the context of your application. If your application expects a search to find a value you can check the number of entries returned in the result. Query APIs, such as , return a variable of type Query. You can use a query object to build up a query in parts before executing it with the method. executes the query and returns a promise that you can on for the result. // find all athletes that play tennis const query = Athlete.find({ sport: \"Tennis\" }); // selecting the 'name' and 'age' fields query.select(\"name age\"); // limit our results to 5 items query.limit(5); // sort by age query.sort({ age: -1 }); // execute the query at a later time query.exec(); Above we've defined the query conditions in the method. We can also do this using a function, and we can chain all the parts of our query together using the dot operator (.) rather than adding them separately. The code fragment below is the same as our query above, with an additional condition for the age. The method gets all matching records, but often you just want to get one match. The following methods query for a single record:\n• : Finds the document with the specified (every document has a unique ).\n• : Finds a single document that matches the specified criteria.\n• , , , : Finds a single document by or criteria and either updates or removes it. These are useful convenience functions for updating and removing records. Note: There is also a method that you can use to get the number of items that match conditions. This is useful if you want to perform a count without actually fetching the records. There is a lot more you can do with queries. For more information see: Queries (Mongoose docs). You can create references from one document/model instance to another using the schema field, or from one document to many using an array of . The field stores the id of the related model. If you need the actual content of the associated document, you can use the method in a query to replace the id with the actual data. For example, the following schema defines authors and stories. Each author can have multiple stories, which we represent as an array of . Each story can have a single author. The property tells the schema which model can be assigned to this field. const mongoose = require(\"mongoose\"); const Schema = mongoose.Schema; const authorSchema = new Schema({ name: String, stories: [{ type: Schema.Types.ObjectId, ref: \"Story\" }], }); const storySchema = new Schema({ author: { type: Schema.Types.ObjectId, ref: \"Author\" }, title: String, }); const Story = mongoose.model(\"Story\", storySchema); const Author = mongoose.model(\"Author\", authorSchema); We can save our references to the related document by assigning the value. Below we create an author, then a story, and assign the author id to our story's author field. const bob = new Author({ name: \"Bob Smith\" }); await bob.save(); // Bob now exists, so lets create a story const story = new Story({ title: \"Bob goes sledding\", author: bob._id, // assign the _id from our author Bob. This ID is created by default! }); await story.save(); Note: One great benefit of this style of programming is that we don't have to complicate the main path of our code with error checking. If any of the operations fail, the promise will reject and an error will be thrown. Our error handling code deals with that separately (usually in a block), so the intent of our code is very clear. Our story document now has an author referenced by the author document's ID. In order to get the author information in the story results we use , as shown below. Story.findOne({ title: \"Bob goes sledding\" }) .populate(\"author\") // Replace the author id with actual author information in results .exec(); Note: Astute readers will have noted that we added an author to our story, but we didn't do anything to add our story to our author's array. How then can we get all stories by a particular author? One way would be to add our story to the stories array, but this would result in us having two places where the information relating authors and stories needs to be maintained. A better way is to get the of our author, then use to search for this in the author field across all stories. This is almost everything you need to know about working with related items for this tutorial. For more detailed information see Population (Mongoose docs).\n\nNow that we understand something of what Mongoose can do and how we want to design our models, it's time to start work on the LocalLibrary website. The very first thing we want to do is set up a MongoDB database that we can use to store our library data. For this tutorial, we're going to use the MongoDB Atlas cloud-hosted sandbox database. This database tier is not considered suitable for production websites because it has no redundancy, but it is great for development and prototyping. We're using it here because it is free and easy to set up, and because MongoDB Atlas is a popular database as a service vendor that you might reasonably choose for your production database (other popular choices at the time of writing include ScaleGrid and ObjectRocket). Note: If you prefer, you can set up a MongoDB database locally by downloading and installing the appropriate binaries for your system. The rest of the instructions in this article would be similar, except for the database URL you would specify when connecting. In the Express Tutorial Part 7: Deploying to Production tutorial we host both the application and database on Railway, but we could equally well have used a database on MongoDB Atlas. You will first need to create an account with MongoDB Atlas (this is free, and just requires that you enter basic contact details and acknowledge their terms of service). After logging in, you'll be taken to the home screen:\n• Click the + Create button in the Overview section.\n• This will open the Deploy your cluster screen. Click on the M0 FREE option template.\n• Scroll down the page to see the different options you can choose.\n• You can change the name of your Cluster under Cluster Name. We are keeping it as for this tutorial.\n• Deselect the Preload sample dataset checkbox, as we'll import our own sample data later on\n• Select any provider and region from the Provider and Region sections. Different regions offer different providers.\n• Tags are optional. We will not use them here.\n• Click the Create deployment button (creation of the cluster will take some minutes).\n• This will open the Security Quickstart section.\n• Enter a username and password for your application to use to access the database (above we have created a new login \"cooluser\"). Remember to copy and store the credentials safely as we will need them later on. Click the Create User button. Note: Avoid using special characters in your MongoDB user password as mongoose may not parse the connection string properly.\n• Select Add by current IP address to allow access from your current computer\n• Enter in the IP Address field and then click the Add Entry button. This tells MongoDB that we want to allow access from anywhere. Note: It is a best practice to limit the IP addresses that can connect to your database and other resources. Here we allow a connection from anywhere because we don't know where the request will come from after deployment.\n• This will open the following screen. Click on the Go to Overview button.\n• You will return to the Overview screen. Click on the Database section under the Deployment menu on the left. Click the Browse Collections button.\n• This will open the Collections section. Click the Add My Own Data button.\n• This will open the Create Database screen.\n• Enter the name for the new database as .\n• Enter the name of the collection as .\n• Click the Create button to create the database.\n• You will return to the Collections screen with your database created.\n• Click the Overview tab to return to the cluster overview.\n• From the Cluster0 Overview screen click the Connect button.\n• This will open the Connect to Cluster0 screen.\n• Select the Drivers category, then the Driver Node.js and Version as shown.\n• DO NOT install the driver as suggested.\n• Click the Copy icon to copy the connection string.\n• Paste this in your local text editor.\n• Replace placeholder in the connection string with your user's password.\n• Insert the database name \"local_library\" in the path before the options ( )\n• Save the file containing this string somewhere safe. You have now created the database, and have a URL (with username and password) that can be used to access it. This will look something like:"
    },
    {
        "link": "https://mongoosejs.com/docs/models.html",
        "document": "Models are fancy constructors compiled from definitions. An instance of a model is called a document. Models are responsible for creating and reading documents from the underlying MongoDB database.\n\nWhen you call on a schema, Mongoose compiles a model for you.\n\nThe first argument is the singular name of the collection your model is for. Mongoose automatically looks for the plural, lowercased version of your model name. Thus, for the example above, the model Tank is for the tanks collection in the database.\n\nNote: The function makes a copy of . Make sure that you've added everything you want to , including hooks, before calling !\n\nAn instance of a model is called a document. Creating them and saving to the database is easy.\n\nNote that no tanks will be created/removed until the connection your model uses is open. Every model has an associated connection. When you use , your model will use the default mongoose connection.\n\nIf you create a custom connection, use that connection's function instead.\n\nFinding documents is easy with Mongoose, which supports the rich query syntax of MongoDB. Documents can be retrieved using a 's find, findById, findOne, or where static functions.\n\nSee the chapter on queries for more details on how to use the Query api.\n\nModels have static and functions for removing all documents matching the given .\n\nEach has its own method for modifying documents in the database without returning them to your application. See the API docs for more detail.\n\nIf you want to update a single document in the db and return it to your application, use findOneAndUpdate instead.\n\nChange streams provide a way for you to listen to all inserts and updates going through your MongoDB database. Note that change streams do not work unless you're connected to a MongoDB replica set.\n\nThe output from the above async function will look like what you see below.\n\nYou can read more about change streams in mongoose in this blog post.\n\nMongoDB Views are essentially read-only collections that contain data computed from other collections using aggregations. In Mongoose, you should define a separate Model for each of your Views. You can also create a View using .\n\nThe following example shows how you can create a new View on a Model that hides potentially sensitive information, like name and email.\n\nNote that Mongoose does not currently enforce that Views are read-only. If you attempt to a document from a View, you will get an error from the MongoDB server.\n\nThe API docs cover many additional methods available like count, mapReduce, aggregate, and more.\n\nNow that we've covered , let's take a look at Documents."
    },
    {
        "link": "https://geeksforgeeks.org/mongoose-schemas-creating-a-model",
        "document": "Mongoose is one of the most popular Object Data Modeling (ODM) libraries for MongoDB, providing schema-based solutions to model our application’s data. This allows us to define the structure of documents within a MongoDB collection, including validation, typecasting, and other powerful features that simplify database operations.\n\nWhat is Mongoose Schema and Model?\n\nBefore diving into creating a model, it’s important to understand the schema and model concepts in MongoDB:\n\nSchema: A MongoDB schema defines the structure of documents within a collection. It specifies the fields, their types, validation rules, default values, and other constraints.\n\nModel: A model is a wrapper around the schema that allows us to interact with the MongoDB database (CRUD operations like create, read, update, delete). It provides an interface for querying and manipulating the data.\n\nTo demonstrate how to create a model using Mongoose, follow these steps to set up the environment:\n\nFirst, we need to create a Node.js project if you haven’t done so already. Run the following command in your terminal:\n\nThis will generate a file, which will manage the project’s dependencies.\n\nWe can start the developement server using the below command.\n\nNote: The above command will not do anything right now since we have not written any code in our JavaScript file.\n\nWe need to install the required modules to use in our project. Run the following command to install mongoose and mongosd as dependencies.\n\nNow, create an app.js file where you will define the Mongoose schema and model and connect to MongoDB. Start by connecting to your MongoDB instance:\n\nTo create a schema and model in Mongoose, define a schema with fields like name, power type, gold, health, and mana for a “Mage.” Use mongoose.Schema() to set up the structure, ensuring required fields. Then, use mongoose.model() to create a model based on this schema, enabling interaction with the database.\n\nStep 5: Create the Model Using the Schema\n\nOnce the schema is defined, create a model using . This model will provide an interface to interact with the collection in the MongoDB database.\n\nStep 6: Create and Save Documents Using the Model\n\nTo create and save a model in Mongoose, instantiate an object from the model class using the new keyword, then call the save() method on this object to create a document in the corresponding MongoDB collection.\n\nWe can view the saved model and document by opening up the Studio 3T desktop application, clicking on connect, and then going through the following hierarchy and double click on the mode collection name.\n\nThis is what you will see when you double-click on the collection name. Notice that it has been given an _id field automatically.\n\nWe can create as many objects from the Mage class as you want, and call the save() method on them to create a document for each of them in the mages collection.\n\nBenefits of Using Mongoose Schema and Model\n\n1. Schema Validation: Mongoose automatically validates data based on the defined schema before saving it to the database.\n\n2. Ease of Use: Mongoose provides a simple interface to work with MongoDB, eliminating the need to write complex queries manually.\n\n3. Middleware Support: Mongoose supports middleware (pre and post hooks), allowing you to run custom logic before or after database operations.\n\nMongoose is a powerful tool for managing MongoDB collections in a Node.js application. By defining schemas and models, Mongoose allows us to structure your data, perform validation, and interact with MongoDB in a straightforward and efficient manner. With the ability to create complex models, define indexes, and validate documents, Mongoose significantly streamlines the development of MongoDB-based applications.\n\nWhat is MongoDB schema and model?\n\nHow do you create an index in Mongoose schema?\n\nHow to create a model in MongoDB?"
    },
    {
        "link": "https://ayoubkhial.com/blog/mean-web-app-part-7-designing-and-modeling-database-with-mongoose",
        "document": "In nearly all software applications, the data layer often acts as the cornerstone that underpins an application's business logic, user experience, and overall functionality. Effective database modeling is crucial for your application's scalability and long-term maintainability, whether you're developing a social media platform, designing a real-time analytics dashboard, or creating a bug-tracking system.\n\nWhile SQL databases have long offered a rich set of features to design complex relational models, the increasing popularity of NoSQL databases like MongoDB has necessitated different strategies for establishing relationships and enforcing data integrity.\n\nIn our previous article, we learned how to connect your Node.js application to MongoDB using Mongoose ODM. With that foundation laid, it's time to dig deeper into one of the most crucial aspects of any application: Database Modeling. This article will focus solely on using Mongoose's robust schema features to shape your database effectively.\n\nSo why is getting your database design right so important? Picture trying to build a house without any plans. You could start stacking bricks, but you'll likely end up with a shaky building. Just like that, a well-planned database is essential for a successful app. It helps it run smoothly and makes it easier to manage in the long run.\n\nBefore diving into Mongoose, establishing a well-designed MongoDB schema is essential. In the next section, we'll focus on the differences between SQL and MongoDB in database design, along with some best practices and pitfalls to avoid.\n\nA carefully planned database schema is crucial when developing an application, particularly a complex one. The schema shapes your data, which impacts how you query that data and the kind of insights you can extract from it.\n\nIf you come from an SQL background, you'll know that schema design focuses on normalization. This approach aims to reduce repeated data by spreading it across separate tables. Doing so keeps the data organized, minimizes duplication, and avoids issues when adding, changing, or deleting data. Reaching these organized stages, known as normal forms, is crucial for data integrity in SQL.\n\nIn NoSQL, especially in MongoDB, schema design takes a unique route. Here, your design decisions should align closely with your application's requirements. How the data is displayed in the application naturally guides how you should store it.\n\nOur system's Issues are closely linked to their related Comments and Activities. This means that when you display an Issue, you usually show its Comments and Activities right next to it. Instead of spreading this data across various MongoDB collections, it's wiser to include Comments and Activities directly in the Issue document. This approach makes fetching and displaying data easier, taking full advantage of what MongoDB offers.\n\nHOWEVER, if your application's use case doesn't require displaying Comments and Activities alongside Issues, separating them into distinct collections could be a wiser choice. For instance, if Comments are often accessed independently of Issues for analytics or reporting, keeping them in their own collection can improve data retrieval speed and manageability. The right design depends on your needs and how you intend to interact with the data.\n\nThe main reason you should be cautious is all about speed, or rather, the lack of it. When your data is spread across several collections, MongoDB uses the operator to stitch it together. On the Mongoose side, you've got the method doing the same job. Sounds great, but hold up, both these methods can slow you down big time, particularly as your data grows and gets more complex.\n\nHere's what an Issue document might look like in MongoDB compared to SQL:\n\nAlthough MongoDB provides design flexibility, some pitfalls should be avoided, especially when embedding documents. It's not always best to store all data in a single collection.\n\nTake a bug-tracking system as an example. Typically, you'd manage multiple Projects, each containing several related Issues. Each Issue falls under just one Project, creating a One-to-Many relationship.\n\nBuilding upon our earlier example, you might consider embedding Issues details directly within the Project documents. In such a design, the Project document would have an Issues array containing various issue subdocuments.\n\nWhile this embedded approach simplifies your queries, it's important to be cautious. Packing too much data into one collection comes with risks. For instance, you could run into MongoDB's 16MB document size limit, especially if your array keeps expanding. Additionally, if we were to establish an index on the array —say, based on the issue — its performance would degrade over time as more issues are added.\n\nAnother point to consider is how Issues operate independently. You might need to display, filter, sort, or edit them without tying them back to their respective Projects.\n\nA solution to these challenges is to form a distinct issues collection, allowing for more granular querying. We could also flip the model and embed the project detail in the issues documents instead.\n\nIn this design, each Issue would carry basic project information, ideal for scenarios where Issues and their associated Projects are frequently accessed together and where the Project details are unlikely to change. However, this approach could result in considerable data duplication.\n\nTo minimize data duplication, include only essential project details in each Issue and use a reference for complete project data when necessary. For instance, simply displaying the project's alongside an issue might suffice. This strategy is called The Extended Reference Pattern.\n\nTo completely understand our database, I have created a diagram displaying each collection and its corresponding fields and data types. To illustrate the relationships between collections, I have included arrows in the diagram. An arrow that ends with '1' indicates a one-to-one relationship, while an arrow that ends with 'n' signifies a one-to-many relationship.\n\nOur current schema might appear minimalistic for a bug-tracking system, but it serves as a solid foundation. Expect to update and enhance it down the line to include new features or make essential changes.\n• Duplication and Update Concerns: certain fields are denormalized to optimize read operations. This makes querying faster but can complicate updates. For instance, if a project's name changes, you'll need to update it in both the Projects collection and every Issue associated with that project. Handle these carefully to avoid data inconsistencies.\n• Embedding or Referencing: We've blended both approaches in our schema. This is common in MongoDB, but it's good to consistently evaluate if embedding or referencing is the right choice. For example, if grow significantly, consider segregating them into a new collection and linking them to Issue.\n• Array Growth: MongoDB imposes a document size cap. If arrays like or swell, consider alternative designs before hitting that limit.\n\nRemember that MongoDB schema should align closely with your application's specific needs for optimal performance. And remember, schema evolution is part of the development journey. Be prepared to adjust as your application's requirements shift.\n\nAs mentioned, our primary focus in this article is on schema modeling. Accordingly, we'll create two files for each component: and . These files lay the groundwork for defining your database schema and exporting the models. However, in a forthcoming article, we'll introduce the file. This file will serve as an intermediary layer that facilitates interactions with the database, enabling you to execute queries, perform updates, and more.\n\nAnd don't forget to add your components to the file so they can be recognized and imported by Node.js.\n\nHaving explored the principles of effective database design, let's now delve into how to model this architecture using Mongoose ODM.\n\nMongoose provides a powerful yet straightforward way to work with MongoDB databases in a Node.js environment. Understanding its core concepts —Model, Schema, and Document— is essential for appreciating how Mongoose streamlines data management and validation.\n\nIn Mongoose, a schema serves as the architectural plan that outlines the format of your data within a MongoDB collection. Every data operation and interaction in Mongoose is centered around this schema.\n\nEach Mongoose schema is tied to a MongoDB collection and defines the documents' shape, type, and characteristics within that collection. Within this schema, you lay out the fields, their data types, any validations, constraints, and even what are often called middleware or hooks.\n\nThe constructor from the Mongoose library provides an organized way to define the structure of MongoDB documents:\n\nAccording to this schema, each document in the linked MongoDB collection will include fields like , , , and . These fields encapsulate the range of attributes that a project document can possess. Individual documents might feature only a subset of these fields, depending on the situation.\n\nFrom the MongoDB angle, the Schema functions as a design template for its corresponding Collection, guaranteeing that all documents adhere to the specified structure.\n\nFields in the schema can vary from simple types like Strings to more complex structures like objects. Moreover, further validation and constraints can be incorporated for each field as required.\n\nThe concept of a Model goes beyond being merely a compiled Schema. It acts as the object constructor for generating documents and stands as the primary gateway for all operations on the corresponding MongoDB collection. In essence, the Model serves as a complete abstraction of the MongoDB collection it represents.\n\nHere, we utilize Mongoose's function to convert our pre-defined into a functional Model. This resulting Model comes packed with various methods, allowing extensive interactions with its corresponding MongoDB collection.\n\nThink of a Document as an individual entry or record within your database. It's born from a Model and embodies all the attributes outlined in the Schema. In practical terms, a Document functions as a live instance of your data that you can create, read, update, or delete directly within the application.\n\nTo illustrate, let's create a new Document using the Project model:\n\nMongoDB offers three primary methods for visualizing and interacting with your data: MongoDB Shell for command-line fans, MongoDB Compass for those who appreciate graphical interfaces, and MongoDB Atlas for users who prefer a cloud-based experience.\n\nMongosh, short for MongoDB Shell, provides a command-line interface (CLI) for MongoDB interactions. It's handy for hands-on tasks such as scripting and database administration. To install Mongosh, head over to MongoDB's official website or use a package manager like brew:\n\nOnce installed, you can connect to your MongoDB instance by running:\n\nTo switch to a specific database, run the command:\n\nOnce connected, you can execute MongoDB methods and queries. For example, run to list saved projects.\n\nIf you prefer a graphical interface for database management, MongoDB Compass offers an intuitive platform for visualizing data, executing queries, and managing indexes. Alternatively, if you're utilizing MongoDB Atlas's cloud solution, you can use its web-based interface, which closely resembles Compass and requires no additional installation.\n\nStarting from version 5.11.0, Mongoose has extended native support for TypeScript, effectively marrying TypeScript's strong typing capabilities with MongoDB's flexible data storage.\n\nTypeScript excels at providing type safety, and to capitalize on this strength, it's crucial to define interfaces that outline the expected structure of your documents. In essence, an interface acts as a strict blueprint, guaranteeing that each document you interact with adheres to a specified format.\n\nIn this example, the interface explicitly requires that each project have fields such as , , , and with their respective types.\n\nMongoose's interface brings its own set of built-in methods and properties, which apply to all Mongoose documents. By extending this interface, you signal to TypeScript about these standard properties.\n\nHere, combines the custom fields of and the standard methods and properties of Mongoose's , providing a cohesive model structure.\n\nTypeScript integration leads to a nuanced change in how you define Mongoose schemas. The schema is now tagged as type , tightening the type constraints on your documents.\n\nYou can easily build a Mongoose typed model once your schema and extended document interface are ready.\n\nThis setup not only ensures type-safe operations when dealing with Mongoose models and documents but also elevates the reliability of your code and minimizes errors.\n\nEnsuring data validation is crucial, especially when working with MongoDB, a database known for its flexibility. Mongoose offers robust schema validation features to maintain the integrity and reliability of your data, even in MongoDB's schema-less environment.\n\nMongoose allows you to set specific data types for each field in a schema, ranging from standard JavaScript types like , , , and to Mongoose-specific types such as and .\n\nTo gain a clearer understanding, let's examine the as it demonstrates a diverse range of field types:\n\nIn contrast to native data types, which directly represent the values stored, Mongoose's fulfills a specialized function. This 24-character hexadecimal string serves as a unique identifier for each document. MongoDB auto-generates these IDs whenever you create a new document.\n\nThe is vital in establishing relationships among various documents within your MongoDB database. In terms familiar to those who work with relational databases, it functions like a 'foreign key'. For instance, employs to create references to other collections, such as User and Project. This capability enables you to design a more interconnected data model on top of MongoDB's NoSQL foundation.\n\nMongoose automatically handles type-casting. For instance, if you provide a numeric String for a field, Mongoose will convert it into an actual Number. But there's a limit. If you insert a String that can't be parsed as a Number into a field, Mongoose will throw a validation error.\n\nCertain fields within a database schema are essential for a data record's integrity and meaningful representation. For instance, the and fields are non-negotiable in a schema. Mongoose addresses this necessity through the field option, ensuring that specific attributes are present before saving a document to the database.\n\nThe setting in a Mongoose schema can either be a boolean or a function that returns a boolean. A built-in required validator is attached to the property when set to , making it a mandatory field.\n\nIn the example of , fields like and are marked as required and must contain string values. If a field only has its type specified, such as , you can use shorthand notation to define it.\n\nBy default, fields that lack a attribute are considered optional. Therefore, in the corresponding TypeScript interface, the field should be marked as optional.\n\nAttempting to save a project that omits a required field like or will lead to a validation error. For example:\n\nThe attribute in Mongoose schemas adds an extra layer of data integrity. It restricts the values of a given field to a set list of acceptable options. Once you define an for a field, Mongoose deploys a validator that verifies if the field's value precisely aligns with one of the options listed in the array.\n\nTake the as an example. You can limit the field to only accept specific values like Active, Archived, On Hold, and Completed. Doing so ensures that the field will hold only those values in the pre-defined list.\n\nHere, the field in the interface uses the enum to define acceptable statuses. In the Mongoose schema, we use to generate an array of valid statuses dynamically based on the enum.\n\nIn addition to the frequently used built-in validators such as the ones we discussed. Mongoose provides an array of specialized validators to manage more complex data conditions. These include , , , and . For a comprehensive list and more details, see the official documentation.\n\nOne of Mongoose's most powerful capabilities is its extensibility. Although it offers a wide array of built-in validators, there are situations where you may require custom-made validation logic for your application. Mongoose makes it straightforward to define these specialized validators.\n\nTo build a custom validator, you must include a function in the schema definition. This function accepts the value to be validated as its first argument and should return a boolean indicating whether the validation is successful. If the function returns or throws an error, the validation is considered to have failed.\n\nFor instance, you want to ensure email addresses adhere to a specific format. You can use a regular expression to validate the structure. Here's an example within a schema:\n\nNow, let's consider a real-world example. Imagine a user is trying to register with an improperly formatted email address:\n\nIn this case, the email lacks a proper domain extension like \".com\". When the code runs, Mongoose will flag this as a validation error.\n\nTo specify a custom error message for the validator, for example, you can pass an array to the attribute. The first element of the array is the boolean flag indicating whether the field is required, and the second element is the custom error message.\n\nThis approach also works for other straightforward built-in validators like and . You can include a property within the object to set a custom error message for the enum validator.\n\nSimilarly, When using custom validation through the method, you can specify a custom error message by setting the property.\n\nMongoose offers basic templating capabilities for its error messages. Specifically, it substitutes the placeholder with the actual value undergoing validation.\n\nMongoose offers a range of settings that control how your data is saved and fetched from the database, assisting in data consistency. These options include text transformations such as , , and , as well as more general configurations like and .\n\nSetting the option to in your schema automatically converts all string characters to lowercase before storing them in the database. This is especially useful for fields like email addresses or usernames, where maintaining a standard format is beneficial. While we discuss here, it's worth noting that and work similarly for converting to uppercase and removing extra white spaces, respectively.\n\nIn this example, a username like \"RavenPHoenix\" would be automatically converted to \"ravenphoenix\" upon saving.\n\nThe attribute in a Mongoose schema allows you to specify a default value for a given path. This value can be a specific data type or function whose return value will be the default.\n\nIn this example, the field is automatically set to \"Active\" if no value is provided during document creation.\n\nThe option in Mongoose is a boolean that controls the default visibility of a field in query results. By setting it to , the field will not be included in query responses unlessexplicitly requested. This is particularly useful for sensitive data like passwords.\n\nFor instance, the field will be stored in the database when creating a new user. A subsequent query to fetch this user would include this password field by default.\n\nHowever, you can enhance data security by setting the attribute to for the field within the .\n\nAfter making this change, running this function will retrieve all user data except for the field unless you specifically request to include it in the query.\n\nWhile MongoDB's schema design favors embedded documents, there are situations where linking data across separate collections is necessary. Mongoose gracefully handles these relationships through its attribute within the definitions. This attribute indicates the model that should be referenced when populating the query results later.\n\nThe attribute acts as a pointer, connecting one schema to another. It specifies which model the refers to. Here's an example that illustrates this concept. In the following schema, the fields and contain . These ObjectIds point to documents in the Project and User collections, respectively.\n\nIn their default state, when you query for issue data, the and fields will yield ObjectIds:\n\nWith the attribute in place, you can leverage Mongoose's method to quickly fill in these fields when running queries:\n\nDesigning your MongoDB database with Mongoose offers you the choice between Subdocuments and Nested Objects for storing complex data. While both may seem similar, they serve distinct purposes and come with their own set of advantages and limitations.\n\nNested objects are straightforward structures that are easy to use and quick to retrieve. However, they lack the extensive functionalities available to subdocuments, such as custom validation, middleware, and schema-related features. If your use case is simple and doesn't require specialized behavior, nested objects can be a good fit.\n\nFor example, if each user has a profile picture with basic attributes like a URL and alternate text, a nested object would suffice:\n\nOn the other hand, subdocuments in Mongoose are more like autonomous documents that reside within a parent document. They benefit from their dedicated schema, allowing advanced features like middleware, custom validations, and virtuals.\n\nFor instance, if an issue contains a list of and , both of which could need timestamps, custom validations, or middleware, they are better modeled as subdocuments:\n\nThis way, the and fields are designed as subdocuments, each with specialized schema and features.\n\nMuch like the index in a book helps you quickly locate content without scanning every page, MongoDB indexes enable quick document retrieval without scanning the entire collection. They maintain a subset of the data in an organized manner, drastically reducing the number of documents the database engine has to sift through. Mongoose allows you to define indexes through its schema settings, including properties like and , to optimize database performance.\n\nBy setting the property for a particular field in your Mongoose schema, you're instructing MongoDB to generate an index for that attribute. This makes query operations on that field faster and more efficient.\n\nFor instance, consider a schema where we index the field:\n\nMongoDB quickly navigates its index tree by indexing the field to locate the relevant documents. This feature is handy in extensive collections where a non-indexed query might have to scan every document, resulting in significant delays.\n\nSeparately, the property ensures that all data for a given field are distinct across the collection. MongoDB will return an error if a duplicate value for a unique field is attempted.\n\nHere's how you'd define and as unique fields in the :\n\nAttempting to create a new user with an existing or would yield a MongoDB E11000 error, indicating a duplicate key violation.\n\nIn certain situations, you may want to ensure that a combination of fields is unique across all documents in a collection. MongoDB supports this requirement through the use of unique compound indexes.\n\nLet's say you have a number field which should be unique only withing the scope of a particular project. You could accomplish this by creating a unique compound index on both the and fields.\n\nThough indexes speed up data retrieval, they come with a trade-off: they require updates whenever data changes. This could impact the performance of data insertion or modification. Therefore, it's crucial to be selective about which fields to index. Prioritize fields frequently involved in search or sort operations to maintain an efficient database without unnecessary overhead.\n\nMongoose provides a wide range of options to customize your schema configuration. Below are some valuable options you might consider using to enhance your application: , , and .\n\nThe option in a Mongoose schema controls the automatic creation of indexes for the associated MongoDB collection. When set to , Mongoose will create the indexes for you. This is particularly useful for ensuring that indexes are created for you automatically without requiring manual intervention.\n\nBy default, is set to ; Therefore, upon compiling this model, the Projects collection is created with the specified indexes.\n\nWhile the automatic index-building feature is convenient, it's not always suitable for production databases for a couple of reasons:\n• Performance Overhead: Creating indexes is an operation that can consume significant CPU and memory resources, especially on extensive collections. This may interfere with the regular operations of your production database.\n• Production Concerns: Creating indexes automatically can be risky in production, especially when the database has existing data. It's often better to manage indexes manually.\n\nFor these reasons, you may choose to enable only in the development stage and disable it in production. The choice can be driven by environment variables:\n\nHere, is enabled solely when the application is in development mode. In a production environment where might be set to , it's crucial to manage indexes manually. This can be done using MongoDB's native methods, CLI utilities, or a structured database migration strategy.\n\nThe option, when set to true, ensures that the collection for the model is automatically created if it doesn't already exist.\n\nis set to by default; if you wish not to create collections automatically until you save your first document, set it to .\n\nIf is enabled ( or the default setting), it takes precedence over a value for . This makes sense, as the very action of creating an index necessitates the existence of the collection. Thus, even if is set to , Mongoose will create the collection if it doesn't already exist when indexing is triggered.\n\nMongoose automatically pluralizes the model name to derive the MongoDB collection name. While this default behavior simplifies the process for many developers, there may be instances where a custom collection name is necessary.\n\nMongoose offers the option within the schema options object in such cases. Utilizing this option allows you to explicitly define the collection name according to your needs.\n\nThe will correspond to a MongoDB collection named people in this configuration. This overrides the default name (users) that Mongoose would have automatically generated.\n\nCustomizing the collection name is especially beneficial in situations like:\n• Working with a pre-existing database where collection names are already established.\n• Get past Mongoose's default pluralization, which may not fit your specific naming rules.\n• Desiring more meaningful or descriptive collection names for easier database management.\n\nTracking when documents are created or modified is often an essential requirement in application development, especially in our case (bug-tracking app). Mongoose provides a straightforward way to handle this with its option. This feature automatically appends and fields to your schema when activated. Both of these fields have a data type.\n\nBy default, enabling timestamps will automatically create two fields: and .\n\nIf the default names and do not align with your naming conventions, Mongoose offers the flexibility to rename them.\n\nBy default, Mongoose relies on JavaScript's native function to determine the current time. You can customize this by setting your own function via the sub-option.\n\nIn this configuration, the function returns the current time in milliseconds since the Unix epoch. It's important to note that while returns a object, returns a representing the milliseconds.\n\nMongoose has much more to offer besides the topics we covered, like virtuals, aliases, hooks, methods, and plugins. We will cover some of them in future articles when needed. The following article will focus on effective and performant data querying by introducing the Data Access Objects (DAO) concept.\n\nYou can find the complete code source in this repository; feel free to give it a star ⭐️.\n\nIf you want to keep up with this series, consider subscribing to my newsletter to receive updates as soon as I publish an article."
    },
    {
        "link": "https://pdfkit.org",
        "document": "A JavaScript PDF generation library for Node and the browser.\n\nPDFKit is a PDF document generation library for Node and the browser that makes creating complex, multi-page, printable documents easy. The API embraces chainability, and includes both low level functions as well as abstractions for higher level functionality. The PDFKit API is designed to be simple, so generating complex documents is often as simple as a few function calls.\n\nCheck out some of the documentation and examples to see for yourself! You can also read the guide as a self-generated PDF with example output displayed inline. If you'd like to see how it was generated, check out the README in the docs folder.\n\nYou can also try out an interactive in-browser demo of PDFKit here.\n\nInstallation uses the npm package manager. Just type the following command after installing npm.\n• Font embedding\n• See fontkit for more details on advanced glyph layout support.\n• Image embedding\n• Supports JPEG and PNG files (including indexed PNGs, and PNGs with transparency)\n• Higher level APIs for creating tables and laying out content\n• Even more awesomeness, perhaps written by you! Please fork this repository and send me pull requests.\n\n// Pipe its output somewhere, like to a file or HTTP response // See below for browser usage doc.pipe(fs.createWriteStream('output.pdf'));\n\n// Embed a font, set the font size, and render some text doc .font('fonts/PalatinoBold.ttf') .fontSize(25) .text('Some text with an embedded font!', 100, 100);\n\n// Add an image, constrain it to a given size, and center it vertically and horizontally doc.image('path/to/image.png', { fit: [250, 300], align: 'center', valign: 'center' });\n\n// Add another page doc .addPage() .fontSize(25) .text('Here is some vector graphics...', 100, 100);\n\n// Apply some transforms and render an SVG path with the 'even-odd' fill rule doc .scale(0.6) .translate(470, -380) .path('M 250,75 L 323,301 131,161 369,161 177,301 z') .fill('red', 'even-odd') .restore();\n\n// Add some text with annotations doc .addPage() .fillColor('blue') .text('Here is a link!', 100, 100) .underline(100, 100, 160, 27, { color: '#0000FF' }) .link(100, 100, 160, 27, 'http://google.com/');\n\nThe PDF output from this example (with a few additions) shows the power of PDFKit — producing complex documents with a very small amount of code. For more, see the folder and the PDFKit programming guide.\n\nThere are three ways to use PDFKit in the browser:\n• Use Browserify. See demo source code and build script\n• Use webpack. See complete example.\n• Use prebuilt version. Distributed as file in the releases or in the package folder.\n\nIn addition to PDFKit, you'll need somewhere to stream the output to. HTML5 has a Blob object which can be used to store binary data, and get URLs to this data in order to display PDF output inside an iframe, or upload to a server, etc. In order to get a Blob from the output of PDFKit, you can use the blob-stream module.\n\nThe following example uses Browserify or webpack to load and . See here and here for examples of prebuilt version usage.\n\n// create a document the same way as above const doc = new PDFDocument();\n\n// add your content to the document here, as usual\n\n// get a blob when you are done doc.end(); stream.on('finish', function() { // get a blob you can do whatever you like with const blob = stream.toBlob('application/pdf');\n\n// or get a blob URL for display in the browser const url = stream.toBlobURL('application/pdf'); iframe.src = url; }); `\n\nYou can see an interactive in-browser demo of PDFKit here.\n\nNote that in order to Browserify a project using PDFKit, you need to install the module with npm, which is used to load built-in font data into the package. It is listed as a in PDFKit's , so it isn't installed by default for Node users. If you forget to install it, Browserify will print an error message.\n\nFor complete API documentation and more examples, see the PDFKit website.\n\nPDFKit is available under the MIT license."
    },
    {
        "link": "https://pdfkit.org/docs/getting_started.html",
        "document": "Installation uses the npm package manager. Just type the following command after installing npm.\n\nCreating a PDFKit document is quite simple. Just require the module in your JavaScript source file and create an instance of the class.\n\ninstances are readable Node streams. They don't get saved anywhere automatically, but you can call the method to send the output of the PDF document to another writable Node stream as it is being written. When you're done with your document, call the method to finalize it. Here is an example showing how to pipe to a file or an HTTP response.\n\nThe and methods found in PDFKit before version 0.5 are now deprecated.\n\nUsing PDFKit in the browser\n\nPDFKit can be used in the browser as well as in Node! There are two ways to use PDFKit in the browser. The first is to create an app using an module bundler like Browserify or Webpack. The second is to create a standalone pdfkit script as explained here.\n\nUsing PDFKit in the browser is exactly the same as using it in Node, except you'll want to pipe the output to a destination supported in the browser, such as a Blob. Blobs can be used to generate a URL to allow display of generated PDFs directly in the browser via an , or they can be used to upload the PDF to a server, or trigger a download in the user's browser.\n\nTo get a Blob from a , you should pipe it to a blob-stream, which is a module that generates a Blob from any Node-style stream. The following example uses Browserify to load and , but if you're not using Browserify, you can load them in whatever way you'd like (e.g. script tags).\n\nYou can see an interactive in-browser demo of PDFKit here.\n\nNote that in order to Browserify a project using PDFKit, you need to install the module with npm, which is used to load built-in font data into the package. It is listed as a in PDFKit's , so it isn't installed by default for Node users. If you forget to install it, Browserify will print an error message.\n\nThe first page of a PDFKit document is added for you automatically when you create the document unless you provide . Subsequent pages must be added by you. Luckily, it is quite simple!\n\nTo add some content every time a page is created, either by calling or automatically, you can use the event.\n\nYou can also set some options for the page, such as its size and orientation.\n\nThe property can be either (the default) or . The property can be either an array specifying in PDF points (72 per inch), or a string specifying a predefined size. A list of the predefined paper sizes can be seen here. The default is .\n\nPassing a page options object to the constructor will set the default paper size and layout for every page in the document, which is then overridden by individual options passed to the method.\n\nYou can set the page margins in two ways. The first is by setting the property (singular) to a number, which applies that margin to all edges. The other way is to set the property (plural) to an object with , , , and values. The default is a 1 inch (72 point) margin on all sides.\n\nPDFKit normally flushes pages to the output file immediately when a new page is created, making it impossible to jump back and add content to previous pages. This is normally not an issue, but in some circumstances it can be useful to add content to pages after the whole document, or a part of the document, has been created already. Examples include adding page numbers, or filling in other parts of information you don't have until the rest of the document has been created.\n\nPDFKit has a option in versions v0.7.0 and later that allows you to control when pages are flushed to the output file yourself rather than letting PDFKit handle that for you. To use it, just pass as an option to the constructor. Then, you can call to switch to a previous page (page numbers start at 0).\n\nWhen you're ready to flush the buffered pages to the output file, call . This method is automatically called by , so if you just want to buffer all pages in the document, you never need to call it. Finally, there is a method, which returns the range of pages that are currently buffered. Here is a small example that shows how you might add page numbers to a document.\n\nThe default font is 'Helvetica'. It can be configured by passing option\n\nPDF documents can have various metadata associated with them, such as the title, or author of the document. You can add that information by adding it to the object, or by passing an info object into the document at creation time.\n\nHere is a list of all of the properties you can add to the document metadata. According to the PDF spec, each property must have its first letter capitalized.\n• - the title of the document\n• - the name of the author\n• - the subject of the document\n• - keywords associated with the document\n• - the date the document was created (added automatically by PDFKit)\n• - the date the document was last modified\n\nPDF specification allow you to encrypt the PDF file and require a password when opening the file, and/or set permissions of what users can do with the PDF file. PDFKit implements standard security handler in PDF version 1.3 (40-bit RC4), version 1.4 (128-bit RC4), PDF version 1.7 (128-bit AES), and PDF version 1.7 ExtensionLevel 3 (256-bit AES).\n\nTo enable encryption, provide a user password when creating the in object. The PDF file will be encrypted when a user password is provided, and users will be prompted to enter the password to decrypt the file when opening it.\n\nTo set access privileges for the PDF file, you need to provide an owner password and permission settings in the object when creating . By default, all operations are disallowed. You need to explicitly allow certain operations.\n\nFollowing settings are allowed in object:\n• - whether printing is allowed. Specify to allow degraded printing, or to allow printing with high resolution\n• - whether modifying the file is allowed. Specify to allow modifying document content\n• - whether copying text or graphics is allowed. Specify to allow copying\n• - whether annotating, form filling is allowed. Specify to allow annotating and form filling\n• - whether form filling and signing is allowed. Specify to allow filling in form fields and signing\n• - whether copying text for accessibility is allowed. Specify to allow copying for accessibility\n• - whether assembling document is allowed. Specify to allow document assembly\n\nYou can specify either user password, owner password or both passwords. Behavior differs according to passwords you provides:\n• When only user password is provided, users with user password are able to decrypt the file and have full access to the document.\n• When only owner password is provided, users are able to decrypt and open the document without providing any password, but the access is limited to those operations explicitly permitted. Users with owner password have full access to the document.\n• When both passwords are provided, users with user password are able to decrypt the file but only have limited access to the file according to permission settings. Users with owner password have full access to the document.\n\nNote that PDF file itself cannot enforce access privileges. When file is decrypted, PDF viewer applications have full access to the file content, and it is up to viewer applications to respect permission settings.\n\nTo choose encryption method, you need to specify PDF version. PDFKit will choose best encryption method available in the PDF version you specified.\n\nWhen using PDF version 1.7 ExtensionLevel 3, password is truncated to 127 bytes of its UTF-8 representation. In older versions, password is truncated to 32 bytes, and only Latin-1 characters are allowed.\n\nPDF/A is a standard (ISO 19005-1:2005) which defines rules for electornic documents intended for long-term archiving. The restrictions on PDF/A documents are:\n\nCurrently, PDFKit aims to support PDF/A-1b, PDF/A-2b, PDF/A-3b and PDF/A-1a, PDF/A-2a, PDF/A-3a standards, also known as level B conformance and level A conformance, respectively.\n\nIn order to create PDF/A documents, set to either or for level B (basic) conformance, or for level A (accessible) conformance when creating the in object.\n\nSimilary, use or for PDF/A-2 level B conformance and for PDF/A-2 level A conformance. or can be used for PDF/A-3 level B conformance and for PDF/A-3 level A conformance.\n\nFuthermore, you will need to specify the other options relevant to the PDF/A subset you wish to use, for PDF/A-1 being:\n• set to at least\n\nFor PDF/A-2 and PDF/A-3, the needs to be set to at least and needs to be for level A conformance.\n\nIn order to verify the generated document for PDF/A and its subsets conformance, veraPDF is an excellent open source validator.\n\nPlease note that PDF/A requires fonts to be embedded, as such the standard fonts PDFKit comes with cannot be used because they are in AFM format, which only provides neccessary metrics, without the font data. You should use and use embeddable fonts such as .\n\nOnce you've created a instance, you can add content to the document. Check out the other sections described in this document to learn about each type of content you can add.\n\nThat's the basics! Now let's move on to PDFKit's powerful vector graphics abilities."
    },
    {
        "link": "https://geeksforgeeks.org/how-to-create-and-manipulate-pdf-documents-in-node-js-with-pdfkit-module",
        "document": "How to Create and Manipulate PDF Documents in Node.js with ‘PDFKit’ Module ?\n\nCreating and manipulating PDF documents programmatically is a common requirement in many web applications. Whether it’s generating invoices, reports, or complex forms, the ability to create PDFs directly from your server-side code can greatly enhance the functionality of your application. In this article, we will explore how to create and manipulate PDF documents in Node.js using the module.\n\nis a JavaScript library that provides an easy-to-use API for generating and modifying PDF documents in Node.js. It allows you to create complex PDF documents from scratch, including text, images, vector graphics, and more. is robust, flexible, and supports a wide range of PDF features.\n\nStep 4: Install the necessary packages/libraries in your project using the following commands.\n\nThe updated dependencies in package.json file will look like:\n\nAfter successful installation, we can use this package in our JavaScript application. To use this package, we have to import it first using the require function and we also have to import ‘fs’ module because we are interacting with the pdf file. Then create a new pdf document using the createWriteStream( ) function by passing the pdf file name in it, then add content in it. You can add text by using fontSize and text methods. To add images we have to use the image method by passing the file name and properties of the image.\n\nExample: Implementation to create and manipulate PDF Documents in NodeJs.\n\nRun the code: After successfully adding the content in the pdf file, we can run the javascript code using the command shown below. This will create a new pdf document having all the content described in the code.\n\nThe module is a powerful tool for creating and manipulating PDF documents in Node.js. It provides a wide range of features for adding text, images, shapes, and more to your PDFs. Whether you need to generate simple reports or complex documents, offers the functionality you need to get the job done."
    },
    {
        "link": "https://github.com/foliojs/pdfkit",
        "document": "A JavaScript PDF generation library for Node and the browser.\n\nPDFKit is a PDF document generation library for Node and the browser that makes creating complex, multi-page, printable documents easy. The API embraces chainability, and includes both low level functions as well as abstractions for higher level functionality. The PDFKit API is designed to be simple, so generating complex documents is often as simple as a few function calls.\n\nCheck out some of the documentation and examples to see for yourself! You can also read the guide as a self-generated PDF with example output displayed inline. If you'd like to see how it was generated, check out the README in the docs folder.\n\nYou can also try out an interactive in-browser demo of PDFKit here.\n\nInstallation uses the npm package manager. Just type the following command after installing npm.\n• Font embedding\n• See fontkit for more details on advanced glyph layout support.\n• Image embedding\n• Supports JPEG and PNG files (including indexed PNGs, and PNGs with transparency)\n• Higher level APIs for creating tables and laying out content\n• Even more awesomeness, perhaps written by you! Please fork this repository and send me pull requests.\n\n// Pipe its output somewhere, like to a file or HTTP response // See below for browser usage // Embed a font, set the font size, and render some text 'Some text with an embedded font!' // Add an image, constrain it to a given size, and center it vertically and horizontally : : : 'Here is some vector graphics...' // Apply some transforms and render an SVG path with the 'even-odd' fill rule :\n\nThe PDF output from this example (with a few additions) shows the power of PDFKit — producing complex documents with a very small amount of code. For more, see the folder and the PDFKit programming guide.\n\nThere are three ways to use PDFKit in the browser:\n• Use Browserify. See demo source code and build script\n• Use webpack. See complete example.\n• Use prebuilt version. Distributed as file in the releases or in the package folder.\n\nIn addition to PDFKit, you'll need somewhere to stream the output to. HTML5 has a Blob object which can be used to store binary data, and get URLs to this data in order to display PDF output inside an iframe, or upload to a server, etc. In order to get a Blob from the output of PDFKit, you can use the blob-stream module.\n\nThe following example uses Browserify or webpack to load and . See here and here for examples of prebuilt version usage.\n\nYou can see an interactive in-browser demo of PDFKit here.\n\nNote that in order to Browserify a project using PDFKit, you need to install the module with npm, which is used to load built-in font data into the package. It is listed as a in PDFKit's , so it isn't installed by default for Node users. If you forget to install it, Browserify will print an error message.\n\nFor complete API documentation and more examples, see the PDFKit website.\n\nPDFKit is available under the MIT license."
    },
    {
        "link": "https://pdforge.com/blog/how-to-generate-pdf-from-html-using-pdfkit-in-node-js",
        "document": "If you want to go deep on a full comparison between pdf libraries in javascript for 2025, you can check out this guide .\n\nIn contrast, PDFKit’s lightweight nature makes it a good fit for projects where performance and simplicity are key, especially for backend services or API-based PDF generation. Other good alternative to PDFKit with the same characteristics is jsPDF .\n\nLibraries like Playwright or Puppeteer are widely used for rendering HTML into PDFs via headless browser instances. These tools offer greater accuracy in rendering CSS and are excellent for visually rich documents but come at the cost of increased resource usage.\n\nWhile PDFKit is a great tool, it's important to be aware of alternatives.\n\nYou can check out the full documentation here .\n\nPDFKit is a powerful open-source library for generating PDF documents in Node.js. Unlike many PDF libraries, PDFKit operates with both flexibility and precision, allowing developers to build documents programmatically or by rendering existing HTML. It supports embedding images, fonts, and CSS styles, making it ideal for complex document layouts.\n\nBefore diving into generating PDFs, let’s get PDFKit up and running in your Node.js environment. Below is a step-by-step guide to install and configure the library.\n\nTo get started, initialize your Node.js project and install PDFKit using npm:\n\nOnce installed, require the library in your project:\n\nTo create a basic PDF, instantiate a new `PDFDocument` and pipe the output to a file:\n\nThis will generate a simple PDF containing the text \"Hello, PDFKit!\". However, the true power of PDFKit comes into play when you start generating more complex documents, especially when rendering HTML content.\n\nStructuring HTML effectively is essential for clean conversion to PDF. When preparing HTML for conversion, it's critical to use a layout structure that works well with both PDFKit and CSS rendering engines. A typical structure for an invoice might look like this:\n\nNow that we’ve structured our HTML, we can move on to converting it to a PDF document using PDFKit.\n\nRendering HTML in PDF Format: Key Techniques and Best Practices\n\nPDFKit doesn’t directly convert HTML to PDF like some tools, but it allows you to build the PDF programmatically. To achieve an HTML-to-PDF workflow, we can use libraries like to parse the HTML and render it with PDFKit. This involves first generating HTML and then programmatically placing that content into a PDFKit document.\n\nHandling CSS and Media Queries in HTML to PDF Conversion\n\nWhen converting HTML to PDF, PDFKit requires that you take special care with CSS. Inline styles and media queries may not always behave as expected. It's essential to keep CSS minimal and use PDF-friendly layouts. Avoid complex flexbox or grid layouts that may not render well outside of a browser context.\n\nPDFKit supports embedding custom fonts and images to enhance the PDF layout:\n\nYou can also customize headers, footers, and margins for each page in your PDF:\n\nPDFKit enables you to encrypt PDFs, adding an extra layer of security to sensitive documents:\n\nHow to Handle Large HTML Files and Pagination in Node.js\n\nWhen dealing with large HTML files, pagination becomes a key consideration. PDFKit can handle automatic pagination, ensuring content is split across pages without breaking mid-section:\n\nMake sure your image paths are correct relative to the current working directory, and that you’ve loaded the correct format (JPEG/PNG).\n\nUse `doc.moveDown()` or custom line spacing. Check if you’re specifying a `width` for text blocks. For HTML-based flows, ensure inline CSS is minimal.\n\nOptimize images, consider using standard fonts, or compress embedded images to reduce the file size.\n\nHow to Use a PDF API to Automate PDF Creation at Scale\n\nFor larger SaaS platforms requiring automated PDF generation at scale, integrating a PDF Generation API like pdforge can offload the heavy lifting. This approach is ideal for SaaS platforms with high volumes of PDF requests.\n\nWith pdforge, you can create beautiful reports with flexible layouts and complex components with an easy-to-use opinionated no-code builder. Let the AI do the heavy lifting by generating your templates, creating custom components or even filling all the variables for you.\n\nYou can handle high-volume PDF generation from a single backend call.\n\nHere’s an example of how to generate pdf with pdforge via an API call:\n\nYou can create your account, experience our no-code builder and create your first layout template without any upfront payment clicking here .\n\nPDFKit shines when flexibility and programmatic control are needed in PDF generation. It’s perfect for backend services or Node.js-based SaaS products. However, choosing the right tool depends on your specific requirements and scalability needs.\n\nIf CSS fidelity or browser-like rendering is crucial, tools like Playwright or Puppeteer might be a better fit.\n\nIf you don't want to waste time maintaining pdfs layouts and their infrastructure or if you don't want to keep track of best practices to generate PDFs at scale, third-party PDF APIs like pdforge will save you hours of work and deliver a high quality pdf layout."
    }
]