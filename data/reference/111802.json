[
    {
        "link": "https://datacamp.com/tutorial/markov-chains-python-tutorial",
        "document": "Master the basics of data analysis with Python in just four hours. This online course will introduce the Python interface and explore popular packages."
    },
    {
        "link": "https://stackoverflow.com/questions/61985501/generating-markov-transition-matrix-for-continuous-data-in-python",
        "document": "I am exploring the hidden markov model(HMM) to analyse the sequence of new cases and reproduction rate of covid-19. I have come across a scenarios where I need to generate a transition matrix for the continuous data.\n\nHow can I generate a Markov transformation matrix for continuous data using python or matlab(preferably python). I think matrix should be 3 be 3,showing the probability of moving from each state to the other 2 states.\n\nI am new to python and finding difficulty to do this. Is there a library that I can use for this purpose. I came across Generating Markov transition matrix in Python which is a similar question but it is for discrete data. I want to do something similar for a continuous data."
    },
    {
        "link": "https://github.com/fabrice-rossi/mixvlmc",
        "document": "implements variable length Markov chains (VLMC) and variable length Markov chains with covariates (COVLMC), as described in:\n\nincludes functionalities similar to the ones available in VLMC and PST. The main advantages of are the support of time varying covariates with COVLMC and the introduction of post-pruning of the models that enables fast model selection via information criteria.\n\nThe package can be installed from CRAN with:\n\nThe development version is available from GitHub:\n\nVariable length Markov chains (VLMC) are sparse high order Markov chains. They can be used to model time series (sequences) with discrete values (states) with a mix of small order dependencies for certain states and higher order dependencies for other states. For instance, with a binary time series, the probability of observing 1 at time could be constant whatever the older past states if the last one (at time ) was 1, but could depend on states at time and if the state was 0 at time . A collection of past states that determines completely the transition probabilities is a context of the VLMC. Read for details about contexts and context tree, and see for a more detailed introduction to VLMC.\n\nVLMC with covariates (COVLMC) are extension of VLMC in which transition probabilities (probabilities of the next state given the past) can be influenced by the past values of some covariates (in addition to the past values of the time series itself). Each context is associated to a logistic model that maps the (past values of the) covariates to transition probabilities.\n\nThe package is loaded in a standard way.\n\nThe main function of VLMC is which can be called on a time series represented by a numerical vector or a factor, for instance.\n\nThe default parameters of will tend to produce overly complex VLMC in order to avoid missing potential structure in the time series. In the example above, we expect the optimal VLMC to be a constant distribution as the sample is independent and uniformly distributed (it has no temporal structure). The default parameters give here an overly complex model, as illustrated by its text based representation\n\nThe representation uses simple ASCII art to display the contexts of the VLMC organized into a tree (see for a more detailed introduction):\n• the root ▪ corresponds to an empty context;\n• one can read contexts by following branches (represented by ─) down to their ends (the leaves): for instance is one of the contexts of the tree.\n\nHere the context is associated to the transition probabilities . This means that when one observes this context in the time series, it is always followed by a 0. Notice that contexts are extends to the left when we go down in the tree as deep nodes corresponds to older values. Some papers prefer to write contexts from the most recent value to the oldest one. With this convention, the “reverse” context corresponds to the sub time series . Unless otherwise specified, we write contexts in the temporal order.\n\nThe VLMC above is obviously overfitting to the time series, as illustrated by the 0/1 transition probabilities. A classical way to select a good model is to minimize the BIC. In this can be done easily using using ‘tune_vlmc()’ which fits first a complex VLMC and then prunes it (using a combination of and ), as follows (see for details):\n\nAs expected, we end up with a constant model.\n\nIn time series with actual temporal patterns, the optimal model will be more complex. As a very basic illustrative example, let us consider the time series and turn it into a binary one, with high activity associated to a number of sun spots larger than the median number.\n\nWe adjust automatically an optimal VLMC as follows:\n\nThe results of the pruning process can be represented graphically:\n\nThe plot shows that simpler models are too simple as the BIC increases when pruning becomes strong enough. The best model remains rather complex (as expected based on the periodicity of the Solar cycle):\n\nTo illustrate the use of covariates, we use the power consumption data set included in the package (see for details). We consider a week of electricity usage as follows:\n\nThe time series displays some typical patterns of electricity usage:\n• standard use between 0.4 and 2 kW;\n\nWe build a discrete time series from those (somewhat arbitrary) thresholds:\n\nThe best VLMC model is quite simple. It is almost a standard order one Markov chain, up to the order 2 context used when the active power is typical.\n\nAs pointed about above, low active power tend to correspond to night phase. We can include this information by introducing a day covariate as follows:\n\nA COVLMC is estimated using the function:\n\nThe model appears a bit complex. To get a more adapted model, we use a BIC based model selection as follows:\n\nAs in the VLMC case, the optimal model remains rather simple:\n• the high context do not use the covariate and is equivalent to the vlmc context;\n• the low context is more interesting: it does not switch to a high context (hence the single row of parameters) but uses the covariate. As expected, the probability of switching from low to typical is larger during the day;\n• the typical context is described in a more complex way that in the case of the vlmc as the transition probabilities depend on the previous state.\n\nVLMC models can also be used to sample new time series as in the VMLC bootstrap proposed by Bühlmann and Wyner. For instance, we can estimate the longest time period spent in the high active power regime. In this “predictive” setting, the AIC may be more adapted to select the best model. Notice that some quantities can be computed directly from the model in the VLMC case, using classical results on Markov Chains. See for details on sampling.\n\nWe first select two models based on the AIC.\n\nThe we sample 100 new time series for each model, using the function as follows:\n\nThen statistics can be computed on those time series. For instance, we look for the longest time period spent in the high active power regime.\n\nThe average longest time spent in high consecutively is\n• for the VLMC: 243.6 minutes with a standard error of 6.7337834;\n• for the VLMC with covariate: 286 minutes with a standard error of 8.9386235;\n\nThe following figure shows the distributions of the times obtained by both models as well as the observed value. The VLMC model with covariate is able to generate longer sequences in the high active power state than the bare VLMC model as the consequence of the sensitivity to the day/night schedule.\n\nThe VLMC with covariate can be used to investigate the effects of changes in those covariates. For instance, if the day time is longer, we expect high power usage to be less frequent. For instance, we simulate one week with a day time from 6:00 to 20:00 as follows.\n\nAs expected the distribution of the longest time spend consecutively in high power usage is shifted to lower values when the day length is increased."
    },
    {
        "link": "https://medium.com/@arjunprakash027/markov-chains-a-powerful-tool-for-modeling-sequential-data-202ee8e00b72",
        "document": "Markov chain is a type of mathematical model that describes a sequence of possible events that follow Markov property. Markov property states each event (about to happen) only depends on current state and not on previous states.\n\nLet's take an example, let's consider a coin that is biased 60% for head and 40% for tail. The continuous flip of the coin is an Markov chain, each value (head or tail) depends only on current flip and not on previous flips.\n\nA discrete time Markov chain is a sequence of random variables X1, X2, X3, … with the Markov property, such that the probability of moving to the next state depends only on the present state and not on the previous states. Putting this is mathematical probabilistic formula:\n\nas we can see probability of Xn+1 depends only on Xn that precedes it and not any other previous states.\n\nThe possible values of Xi forms a discrete set called as state space, and state space can be anything: numbers, words, weather anything you name.\n\nThe change from one state to another is called state transition and the probability associated with each transition is called transition probability.\n\nIf the Markov chain has N possible states, the matrix will be an N x N matrix, such that entry (I, J) is the probability of transitioning from state I to state J. Additionally, the transition matrix must be a stochastic matrix, a matrix whose entries in each row must add up to exactly 1, because each row is its own probability distribution.\n\nLets take example of a imaginary stock market where we have a idea of what will happen next day if we know what's happening today.\n\nIf its a bull market today, there is 30% change it will be bull market tomorrow, 40% chance it will be a neutral market and 30% chance it will be a bear market (see all add up to 100% since its a probability distribution)\n\nIf its bear market today, a 20% chance it will be bear tomorrow, 50% it will be bull and 30% it will be neutral.\n\nIf its a neural market today, 50% chance it will be a bull market, 20% it will be bear market and 30% it will be a neutral market.\n\nNow this is called state transition and to convert it into a state transition matrix, it will look like this:\n\nand depicting it in form of directed graph:\n\nHow to calculate the probability of next state\n\nNow lets calculate the probability of next state given current state using multiplication of matrix.\n\nthis is the possible states -> [bull,bear,neutral]\n\nlets say today we start with a bear market\n\nso, our current state is [0,1,0]\n\nand our transition matrix is\n\non multiplication we get [0.5,0.2,0.3]\n\nthis states that the probability of going from bear to bull market is 0.5, bear to bear is 0.2 and bear to neutral is 0.3\n\nWhat if I want to calculate probability for 3 days? Just do the same steps but with a little twist.\n\nwhen we started, our current state was [0,1,0]…and we got an probability for next state as [0.5,0.2,0.3], now to calculate probability of 3rd day, we consider [0.5,0.2,0.3] as our current state and multiply it with transition matrix to get probability of 3rd day.\n\nwe get probability as [0.33,0.38,0.38]\n\nto get probability of 4th day we take current state as [0.33,0.38,0.38] and multiply it with transition matrix and so on for n number of days.\n• This is the constructor method for the class.\n• It initializes the Markov chain with a transition matrix and a list of states.\n• The is a 2D NumPy array representing the transition probabilities between states. Each row represents the current state, and each column represents the next state.\n• The is a list of state names. The order of states in this list should correspond to the rows and columns of the transition matrix.\n• This method takes the current state as input and returns the next state based on the transition probabilities.\n• It first creates a one-hot encoded vector called , which represents the current state as a probability distribution with probability 1 for the current state and 0 for all other states.\n• It calculates the next state by multiplying with the .\n• The function is used to randomly choose the next state based on the probabilities in the vector.\n• The selected state name is returned.\n• This method generates a sequence of states starting from the for a total of steps.\n• It initializes the as a one-hot encoded vector for the starting state.\n• It maintains two lists: to store the generated sequence of states and to record the state at each step.\n• Calculates the next state by multiplying with the .\n• Randomly selects the next state based on the transition probabilities.\n• Appends the selected state to the list.\n• Updates the for the next iteration.\n• Finally, it returns both the generated sequence of states ( ) and the recorded state transitions ( ).\n• PageRank algorithm: Markov chains are used to rank web pages based on the number and quality of links pointing to them. The web is modeled as a directed graph, where pages are nodes and links are edges. The damping factor accounts for the possibility of a random jump to any page.\n• Stock market prediction: Markov chains can be used to model the behavior of the stock market, which can switch between different states, such as bull, bear, or stagnant. The transition probabilities can be estimated from historical data and used to forecast future market conditions.\n• Text generation: Markov chains can be used to generate random and meaningful text messages, such as those produced by bots on Reddit. The text is generated by using word-to-word probabilities learned from a large corpus of text data.\n• Election forecasting: Markov chains can be used to predict the outcome of elections, based on the past voting patterns and trends. Bootstrap percentiles can be used to calculate confidence intervals for these predictions.\n\nIf this blog was informational, please consider giving me a follow on medium and connect with me on LinkedIn"
    },
    {
        "link": "https://stackoverflow.com/questions/46657221/generating-markov-transition-matrix-in-python",
        "document": "Imagine I have a series of 4 possible Markovian states (A, B, C, D):\n\nHow can I generate a Markov transformation matrix using Python? The matrix must be 4 by 4, showing the probability of moving from each state to the other 3 states. I've been looking at many examples online but in all of them, the matrix is given, not calculated based on data. I also looked into hmmlearn but nowhere I read on how to have it spit out the transition matrix. Is there a library that I can use for this purpose?\n\nHere is an R code for the exact thing I am trying to do in Python: https://stats.stackexchange.com/questions/26722/calculate-transition-matrix-markov-in-r"
    },
    {
        "link": "https://stackoverflow.com/questions/52143556/markov-transition-probability-matrix-implementation-in-python",
        "document": "I am trying to calculate one-step, two-step transition probability matrices for a sequence as shown below :\n\nMy question, how do we calculate two step transition matrix. because when I manually calculate the matrix it is as below :\n\nHowever. np.dot(one_step_array,one_step_arrary) gives me a result which is different and as follows :\n\nPlease let me know which one is correct."
    },
    {
        "link": "https://stackoverflow.com/questions/46657221/generating-markov-transition-matrix-in-python",
        "document": "Imagine I have a series of 4 possible Markovian states (A, B, C, D):\n\nHow can I generate a Markov transformation matrix using Python? The matrix must be 4 by 4, showing the probability of moving from each state to the other 3 states. I've been looking at many examples online but in all of them, the matrix is given, not calculated based on data. I also looked into hmmlearn but nowhere I read on how to have it spit out the transition matrix. Is there a library that I can use for this purpose?\n\nHere is an R code for the exact thing I am trying to do in Python: https://stats.stackexchange.com/questions/26722/calculate-transition-matrix-markov-in-r"
    },
    {
        "link": "https://datacamp.com/tutorial/markov-chains-python-tutorial",
        "document": "Master the basics of data analysis with Python in just four hours. This online course will introduce the Python interface and explore popular packages."
    },
    {
        "link": "https://s2.smu.edu/~mitch/ftp_dir/pubs/syscon17.pdf",
        "document": ""
    },
    {
        "link": "https://almob.biomedcentral.com/articles/10.1186/s13015-015-0061-5",
        "document": "Working with big, dense transition matrices poses two connected problems: on the one hand, the storage size of the matrix may considerably slow down calculations or be altogether too big for the computer, on the other hand, the relevant information about the model may be difficult to extract from the great amount of data contained in the matrix. Visualization techniques for the interpretation of matrix data can, however, also help to find matrix properties which allow reducing the storage size, such as partial symmetry or the occurrence of many near-zero transition probabilities. We therefore start by describing the visualization techniques in the first part, and then move on to storage size reduction by sparse approximation in the second part of the results.\n\nAn intuitive first step in analyzing the transient behavior of a Markov chain model is a diagnostic visualization of the transition matrix. By summarizing results in an accessible way, the resulting diagram may ideally also provide a basis for direct biological interpretation. With one exception (landscape plot), all the following visualization methods are available using the functions histogrid, histo3d and networkplot (with its support function percolation) in the mamoth module; an example for the runtime of each method is given in Additional file 1.\n\nA heat map or histogram of the transition matrix, where the transition probabilities p are symbolized by color/ shade or height, is perhaps the easiest way to visualize it (Fig. 1). The resolution may be enhanced by an appropriate transformation of the range of values for p, for example by using a negative logarithm (\\([0;1] \\rightarrow [0; \\infty ]\\)) or a logit transformation (\\([0;1] \\rightarrow [-\\infty ; \\infty ]\\)).\n\nFor big matrices, heat maps can be costly to produce (memory size) and are often still not very clear, due to the large number of cases. However, they may help to recognize basic patterns (symmetries, groups of similar/more strongly connected states etc.) of potential value for finding more adapted visualizations/numerical methods.\n• In our example, the heat map shows that many of the transition probabilities in the matrix are, though not equal, very close to zero. After re-ordering the states, the partial symmetry of the matrix also becomes visible.\n\nThe duality between matrices and graphs (e.g. [7, 25]) provides an alternative for the visualization and mathematical analysis of either structure. In a graph \\(\\mathcal {G(V, E)}\\), the states of a Markov chain are thus represented as nodes/vertices \\(\\mathcal {V}\\) and the transitions as (weighted and directed) edges \\(\\mathcal {E}\\) connecting them, which is especially useful for sparse transition matrices.\n\nFor big, dense matrices, the number of edges in the resulting complete multidigraph (of edge multiplicity two) equals the number of entries in the transition matrix and is thus too big for easy interpretation. Concepts from network theory can be used to selectively display edges and summarize information about each state of the model system on the nodes. This leads to a variety of very clear synthetic representations constructed with different parameters and taking into account different time scales: from one generation (based on M) across t generations (based on \\(M^{t}\\)) up to the long-time equilibrium (dominant eigenvector of M, v).\n\nTo facilitate biological interpretation, we arranged the nodes of the network according to biological “meta data”. For our model example where states represent distributions of individuals on three genotypes (aa, aA, AA) under a constant population size (compositional data), we placed the nodes in a de Finetti diagram ([17], see Fig. 2), a specialized ternary plot for population genetics.\n\nThe following visualization techniques are based on selectively displaying the network’s edges:\n\nMost probable neighbor This is the analog to a nearest neighbor if distances (edge weights) represent probabilities. For each state i, there are one or several states j which have the highest probability to be the destination of a transition in the next time step; tracing these connections gives the expectation for the one-step transient behavior of the model.\n• In our example, the most likely state for the next generation (Fig. 2) is always on or very near to the Hardy-Weinberg Equilibrium, which is represented by the continuous black curve going through (1/4; 1/2; 1/4) in the diagram in Fig. 2a.\n\nMost probable path This is the counterpart of a shortest path if distances (edge weights) represent probabilities. For each non-commutative pair of states i and j, there exists at least one series of consecutive edges connecting i to j along which the product of the edge weights is maximal. It can be determined by using an “ordinary” shortest path algorithm (e.g. [26, 27]) on a negative log transform of the transition matrix. The most probable path is the most likely trajectory of the model system to get from one state to another.\n• In our example (Fig. 2), a change from a population with only the aa genotype to one with only the AA genotype would closely follow the Hardy-Weinberg curve.\n\nFlow threshold Using the smallest probability along the most likely path between two nodes i and j as a threshold, very rare transitions can be excluded.\n• In our example (Additional file 3), horizontal transitions along the base of the triangle, where no heterozygotes are produced despite of two homozygous genotypes being present in the population, would be excluded.\n\nThe following visualization techniques are based on changing the appearance of the network’s nodes:\n\nDegree For each node in a graph representing a dense matrix, the number of incoming (in-degree) and outgoing (out-degree) edges is normally (approximately) equal to the number of nodes (matrix rows/columns). This method should therefore be used in connection with selective edge plotting and interpreted according to context.\n• In our example (Fig. 2), the nodes with the highest in-degree are nearest neighbors to the largest number of nodes; if all states were equally likely at the current generation, those next to (0.25; 0.5; 0.25) on the Hardy-Weinberg curve would be the most likely in the next generation.\n\nBetweenness-centrality Based on the same concept as the most probable path, this can be redefined as the number of most probable paths passing through each node when connections between each pair of nodes are considered. It can be derived in a similar way as the most probable path, by applying a standard algorithm developed for additive distances to a negative log transform of the multiplicative probabilities in M. Nodes with a high betweenness-centrality represent frequent transient states.\n• In our example, these are all the states along the Hardy-Weinberg curve except for the fixation states (Additional file 4).\n\nProbabilities For each state i in the Markov chain model, several probabilities can be calculated—and displayed on the nodes—to describe both the transient and limiting behavior:\n\nprobability to stay for one time step \\(p_{\\text {stay}}(i) = p_{i,i}\\), the probabilities on the matrix diagonal; for each state i this is the probability that the system remains at state i for the next time step (“stickiness”). This probability allows the easy detection of (near-)absorptive states. In population genetics, the fixation states \\(\\lbrace (N;0;0),\\) \\((0;0;N)\\rbrace\\) are typical examples (Fig. 2). probability to leave in one time step \\(p_{\\text {out}}(i) = 1-p_{i,i},\\) the column sums of the matrix without the diagonal; for each state i this is the probability that the system changes state at the next time step (“conductivity”). Being the opposite of \\(p_{stay}\\), this probability allows the detection of states which are rarely occupied for consecutive time steps. In our example, these are the states where the population consists of an approximately even mixture of both homozygotes (central basis of the triangle) or only of heterozygotes (top of the triangle; Additional file 3). In contrast, the row sums of a left-stochastic matrix may exceed one and are thus not probabilities. As a result of the Markov property, a probability to arrive always depends on the state at the previous time step, which results in a number of possible definitions. probability to arrive from state j in one time step \\(p(i|j) = p_{j, i}, j \\in S\\), all probabilities in one column of the transition matrix; the probability distribution (mean, variance, skew according to arrangement of nodes) for transitions starting from one particular state. This allows the prediction of the most likely states for the next time step. In our example, the variance around the fixation states is much more limited than at the interior states of the triangle (Additional file 4). probability to arrive in one time step \\(p_{\\text {in}}(i) = 1/(|S|-1) \\cdot \\sum _j p_{j, i}\\) for \\(i \n\ne j\\), the row sums of the matrix divided by the number of other states; probability to arrive at state i if all previous states are equally likely. This shows states which are generally very likely destinations for one-step transitions. In our example, these are the states around the Hardy-Weinberg curve (additional file 3). probability to arrive in an infinite run \\(p_{\\text {in}}^{\\infty }(i) = \\sum _j p_{j, i} \\cdot v_{j}\\) for \\(i \n\ne j\\), the sum over the element-wise product of eigenvector and matrix row, without the diagonal; probabilities to arrive at state i if the likelihood of the previous states is distributed according to the limiting distribution. This shows the states which are the most frequent destination of transitions in an infinite run of the model. In our example, these are the two states next to the fixation states where there is exactly one “foreign” allele (Additional file 4). \\(p^{\\infty }(i) = v_{i}\\), the eigenvector; probability to find the system at state i after infinitely many time steps, or proportion of time spent in each state averaged over infinitely many time steps (limiting distribution). This is the prediction for the most likely states independently of the start state. As is well known for our example, these are the fixation states (Additional file 3).\n\nExpected time to first passage To calculate the expected time to arrive at a certain (group of) states from any other, the “target” states are considered absorptive (first passage time, [7]). Based on the sub-matrix \\(M'\\) including only the transition probabilities between non-target states, the times \\(t_{\\text {target}}\\) are\n\nwhere \\(\\mathbf {1}\\) is a row vector of ones matching the dimension of \\(M'\\) and I is the corresponding unit matrix. The first passage times of the target states are zero.\n• For our example, plotting the expected time to the fixation states shows that it depends predominantly on the current state’s allele frequencies (Additional file 4).\n\nCombining length and direction of the transitions in the most probable neighbor plot (Fig. 2) gives a three dimensional “landscape” illustrating the most probable dynamics of the Markov chain, similar to the “gravity well” plots known from physics. The expected changes in the genotype frequencies are thus represented in a more intuitive fashion, by imagining the population as a small ball rolling on a “landscape” from “hills” to “valleys”. Elevations h are derived from the equality of potential and kinetic energy, which resolves to\n\nfor a single time step, approximating gravitational acceleration by 10. For each model state/node, the distances d are given by the changes in genotype frequencies when moving to the most probable neighbor\n\nThe “landscape” is subsequently drawn as a triangular grid, using the elevation at each state/node as support. To improve readability, h can be rescaled by a constant factor and the landscape colored according to the relative elevation (taking the center of each triangle as reference). The resulting “landscape” shows only the (deterministic) expected dynamics of the Markov chain one could imagine the accompanying stochastic effects as an “earthquake”.\n• In our example, the expected dynamics of the genotype frequencies show convergence to the Hardy-Weinberg equilibrium (Additional file 5).\n\nNote: because of its dependence on a function or matrix specifying the distances between states, and on the triangular grid-like structure of the state space, this method is not included in the mamoth source code.\n\nOne major drawback of state-rich Markov chain models is that the transition matrix in its full form takes up a lot of memory (Table 1). Beside switching to one of the alternative model types mentioned in the introduction (diffusion approximation, coalescence process), there are multiple computational approaches to addressing this issue while keeping the original state and time discrete framework, including:\n• External memory: the whole matrix is stored on a (sufficiently large) hard drive, only parts are loaded into active storage when needed (analogous to [28])\n• Iterative/selective matrix creation: the whole matrix is never stored, only parts are created when needed (e.g. in combination with algorithms such as [29])\n• Lumping states based on model properties: if a group of states has the same (sum of) transition probabilities leading into it and out of it to any other (group of) states and the same analytical meaning (e.g. same value of \\(F_{IS}\\)) they can be combined into one ([30, 31]); other algorithms of state aggregation, such as [32], lead to an approximation of the original matrix\n• Sparse approximation: turning a dense matrix into a sparse matrix by approximating very small matrix elements to zero (e.g. as in [33, 34])\n\nWhich of the first two options is more appropriate depends both on the available hardware and the nature of the task: if the whole matrix is needed repeatedly, storing it will save the time to recalculate despite increased memory access times, but if calculating the matrix elements is fast, the matrix is needed only once or only some parts of the matrix (e.g. the most probable neighbor of each state) are needed, storing the matrix as a whole would be an unnecessary effort.\n\nBecause of the symmetry between the two allele frequencies in our model example, almost half of all states could be pairwise lumped, thus reducing matrix size to a little over a quarter of the original. The exception are the states on the symmetry axis of the de Finetti diagram (compare Figs. 1, 2), which do not have a “lumping partner”. Symmetry with respect to the allele frequencies is often found in population genetics models [3]. However, because of this dependency on model structure a size reduction algorithm based on lumping would not be applicable to non-symmetric extensions of the original model, e.g. with an asymmetric mutation rate or directional selection. Allele frequencies would have to be analyzed jointly, as the new states retain only the ratio of both; once lumped, “unpacking” the states becomes difficult.\n\nThe high number of very small values in the Markov chain transition matrix (Fig. 1) of our model example suggests that sparse approximation would be very effective. Moreover, as each column of the matrix corresponds to a probability distribution (constant sum of one) which becomes less uniform as the number of states/population size increases (the expected convergence to a multinormal distribution with variance proportional to 1/N is the underlying principle of the well-known diffusion approximation), the proportion of very small transition probabilities is likely to augment as the matrix size increases. While sparse approximation is independent of model-specific properties such as symmetry and does not change the states as such, it has the disadvantage of changing the actual content of the transition matrix, potentially leading to the loss of relevant properties such as left-stochasticity or irreducibility.\n\nThe sparse approximation algorithm we propose ensures that the resulting sparse matrix still has all the properties relevant to its function in the Markov chain model. Additionally, it can be executed iteratively so that the complete dense matrix need not be stored. The algorithm iterates over all columns of the transition matrix M and excludes (almost) all values which, in total, contribute less than a threshold value \\(s \\in [0,1]\\) to the column sum:\n\nfor all columns \\(C^i = M_{1\\ldots |S|,i}\\) with \\(i \\in [1, |S|]\\):\n• Create a permutation R of the row indices so that the corresponding entries are ranked according to size: \\(R \\leftarrow \\text {ordinalrank}(j\\, |\\, 1 \\ge C^i_{j} \\ge 0)\\)\n• Find the minimal rank (index of R) so the corresponding entries sum at least to the threshold value s: \\(r \\leftarrow \\text {min}(k)\\) for \\(\\sum _{R_1}^{R_k} C^i_{R_k} \\ge s\\)\n• Keep at least the two biggest values per column: \\(r \\leftarrow \\text {max}(2,r)\\)\n• Keep all values of equal rank: while \\(C^i_{R_{r+1}} = C^i_{R_{r}}\\) : \\(r \\leftarrow r+1\\)\n• Round all values with ranks greater then r to zero, but keep those on the main diagonal and the first lower and first upper diagonals: \\(C^i_{R_k} \\leftarrow 0\\) for all k with \\(k > r \\wedge R_k \n\notin \\{(i-1, i, i+1) \\, \\text {mod} \\, |S|\\}\\)\n• Rescale the column to sum to 1: \\(C^i \\leftarrow C^i/\\text {sum}(C^i)\\).\n\nThe first two steps, together with the rounding in step five, form the core of the algorithm (compare Fig. 3), steps three and four prevent distortions and steps five and six ensure the continued validity of essential Markov chain transition matrix properties: Irreducibility is assured by keeping at least one outgoing and one incoming transition probability per state in such a way that all states remain connected (step five, first lower and first upper diagonal), aperiodicity by keeping all probabilities to stay at the same state (step five, main diagonal), and the rescaling of each column ensures left-stochasticity of the matrix (step six). In contrast, the property that one-step transitions are possible between all states is deliberately given up. The sparse approximation algorithm is available as the appromatrix function in the mamoth module.\n\nBoth the efficiency, i.e. the density or memory use of the resulting matrix, and the bias vary according to the value of s and the distribution of values in the original matrix. If s is low or the probability distribution in the column is far from uniform, more values will be discarded (compare Fig. 3). An appropriate value for s has to be determined heuristically by testing successively increasing values, up to the point where the bias due to the approximation no longer interferes with the interpretability of the model results. The sum of the differences between the entries of the approximate and original matrices has a theoretical upper limit of \\((1-s) \\cdot |S|\\), but the effect of this perturbation on the model output may be more complex.\n\nIn our model example, we analysed the effect of sparse approximation on the equilibrium \\(F_{IS}\\) distribution derived from the dominant eigenvector of the transition matrix. The dominant eigenvector of either a sparse or dense matrix can be calculated with the eigenone function in mamoth, while a comparison between two vectors by a G-Test (correctly omitting infinity values from the test statistic) is implemented in the testvector function. A direct comparison between the “original” and “sparse approximate” equilibrium \\(F_{IS}\\) distributions (Fig. 4) shows a very close fit which does not obscure the biologically relevant changes due to different rates of asexual reproduction. To test if the method gives similarly good results over a wider range of parameters (population size, mutation rate, rate of asexuality and approximation threshold), we performed a Global Sensitivity Analysis (GSA) [37, 38] using different divergence statistics to compare the limiting distribution of \\(F_{IS}\\) derived from original and sparse approximate matrix [35, 39] and the density of the sparse matrix.\n\nThe results of the GSA show that all four model parameters may generally have non-linear/interacting effects on the quality of the approximation, but in the mean these effects are not very strong (Fig. 5). Memory reduction is highly efficient as the mean density of the sparse matrices was only \\(\\approx 0.11\\). Individual densities ranged from \\(\\approx 0.42\\) (small matrix, high threshold) to \\(\\approx 0.03\\) (big matrix, low threshold), varying most strongly with the population size, though all four parameters have a significant influence. On our reference system (Intel Core i7-3930K 3.2 GHz processor with 64 GB RAM), calculating the sparse approximation based on the original matrix took on average 1.7 s for \\(N=50\\) (14.6 s to construct the original), and 31.3 s for \\(N=100\\) (221.7 s to construct the original). Finding the dominant eigenvector of sparse approximate and original matrix took on average 0.1 s (sparse) versus 51.7 s (original) for \\(N=50\\) and 2.4 s (sparse) versus 7869.1 s (2 h, 11 min, 9.1 s, original) for \\(N=100\\), so that in both cases less than one percent of the original runtime was needed with the sparse approximate matrix.\n\nThe overall similarity of the original and approximate equilibrium \\(F_{IS}\\) distributions, measured with different divergence statistics (total distance, Kullback-Leibler divergence, power divergence statistics [40]; Fig. 5), is very high: e.g. the mean for the total distance \\(\\sum \\text {abs} (f_{orig}-f_{approx})\\) is \\(\\approx 0.06\\). It is largely independent of the rate of asexual reproduction and depends most strongly on the approximation threshold and the mutation rate. In contrast, the maximal difference (Kolmogorov-Smirnov two-sample test statistic) between classes of the original and approximate equilibrium \\(F_{IS}\\) distribution is hardly affected by the mutation rate, but rather by approximation threshold (high mean effect) and rate of asexual reproduction (strong non-linearity/interaction). Though on average not significant, the Kolmogorov-Smirnov test gave p-values below 0.05 in \\(20~\\%\\) of the parameter sets sampled. Consequently, the same approximation threshold can be used to compare the overall shape of the distributions across the whole range of rates of asexual reproduction, but it may have to be adapted if mutation rate and population size differ strongly between the modeled scenarios. Care must be taken when individual classes within the distribution (e.g. long-term fixation probability) shall be compared as the probabilities derived from a sparse approximate matrix may then be significantly different from the original.\n\nIn conclusion, sparse approximation using our algorithm has the advantage of being easily applicable to all transition matrices independently of the properties of the underlying model, and is well suited to provide an overview of the equilibrium \\(F_{IS}\\) distribution under different rates of asexual reproduction in our model example. However, it needs an initial effort to verify the model results derived from the approximate matrix and to estimate their final bias. For fine-scale analyses, lumping states may provide an approximation-free alternative, but is not always possible as it depends on the model structure."
    }
]