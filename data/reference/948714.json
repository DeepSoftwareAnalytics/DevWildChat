[
    {
        "link": "https://docs.pola.rs",
        "document": "Polars is a blazingly fast DataFrame library for manipulating structured data. The core is written in Rust, and available for Python, R and NodeJS.\n‚Ä¢ Fast: Written from scratch in Rust, designed close to the machine and without external dependencies.\n‚Ä¢ I/O: First class support for all common data storage layers: local, cloud storage & databases.\n‚Ä¢ Intuitive API: Write your queries the way they were intended. Polars, internally, will determine the most efficient way to execute using its query optimizer.\n‚Ä¢ Out of Core: The streaming API allows you to process your results without requiring all your data to be in memory at the same time.\n‚Ä¢ Parallel: Utilises the power of your machine by dividing the workload among the available CPU cores without any additional configuration.\n‚Ä¢ GPU Support: Optionally run queries on NVIDIA GPUs for maximum performance for in-memory workloads.\n‚Ä¢ Apache Arrow support: Polars can consume and produce Arrow data often with zero-copy operations. Note that Polars is not built on a Pyarrow/Arrow implementation. Instead, Polars has its own compute and buffer implementations.\n\nA DataFrame is a 2-dimensional data structure that is useful for data manipulation and analysis. With labeled axes for rows and columns, each column can contain different data types, making complex data operations such as merging and aggregation much easier. Due to their flexibility and intuitive way of storing and working with data, DataFrames have become increasingly popular in modern data analytics and engineering.\n\nThe goal of Polars is to provide a lightning fast DataFrame library that:\n‚Ä¢ Utilizes all available cores on your machine.\n‚Ä¢ Handles datasets much larger than your available RAM.\n‚Ä¢ Adheres to a strict schema (data-types should be known before running the query).\n\nPolars is written in Rust which gives it C/C++ performance and allows it to fully control performance-critical parts in a query engine.\n\nA more extensive introduction can be found in the next chapter.\n\nPolars has a very active community with frequent releases (approximately weekly). Below are some of the top contributors to the project:\n\nWe appreciate all contributions, from reporting bugs to implementing new features. Read our contributing guide to learn more.\n\nThis project is licensed under the terms of the MIT license."
    },
    {
        "link": "https://docs.pola.rs/user-guide/expressions/aggregation",
        "document": "The Polars context lets you apply expressions on subsets of columns, as defined by the unique values of the column over which the data is grouped. This is a very powerful capability that we explore in this section of the user guide.\n\nWe start by reading in a US congress :\n\nYou can easily apply multiple expressions to your aggregated values. Simply list all of the expressions you want inside the function . There is no upper bound on the number of aggregations you can do and you can make any combination you want. In the snippet below we will group the data based on the column ‚Äúfirst_name‚Äù and then we will apply the following aggregations:\n‚Ä¢ count the number of rows in the group (which means we count how many people in the data set have each unique first name);\n‚Ä¢ combine the values of the column ‚Äúgender‚Äù into a list by referring the column but omitting an aggregate function; and\n‚Ä¢ get the first value of the column ‚Äúlast_name‚Äù within the group.\n\nAfter computing the aggregations, we immediately sort the result and limit it to the top five rows so that we have a nice summary overview:\n\nIt's that easy! Let's turn it up a notch.\n\nLet's say we want to know how many delegates of a state are ‚ÄúPro‚Äù or ‚ÄúAnti‚Äù administration. We can query that directly in the aggregation without the need for a or grooming the dataframe:\n\nWe can also filter the groups. Let's say we want to compute a mean per group, but we don't want to include all values from that group, and we also don't want to actually filter the rows from the dataframe because we need those rows for another aggregation.\n\nIn the example below we show how this can be done.\n\nDo the average age values look nonsensical? That's because we are working with historical data that dates back to the 1800s and we are doing our computations assuming everyone represented in the dataset is still alive and kicking.\n\nThe two previous queries could have been done with a nested , but that wouldn't have let us show off some of these features. üòâ To do a nested , simply list the columns that will be used for grouping.\n\nFirst, we use a nested to figure out how many delegates of a state are ‚ÄúPro‚Äù or ‚ÄúAnti‚Äù administration:\n\nNext, we use a nested to compute the average age of delegates per state and per gender:\n\nNote that we get the same results but the format of the data is different. Depending on the situation, one format may be more suitable than the other.\n\nIt is common to see a dataframe being sorted for the sole purpose of managing the ordering during a grouping operation. Let's say that we want to get the names of the oldest and youngest politicians per state. We could start by sorting and then grouping:\n\nHowever, if we also want to sort the names alphabetically, we need to perform an extra sort operation. Luckily, we can sort in a context without changing the sorting of the underlying dataframe:\n\nWe can even sort a column with the order induced by another column, and this also works inside the context . This modification to the previous query lets us check if the delegate with the first name is male or female:\n\nPython is generally slower than Rust. Besides the overhead of running ‚Äúslow‚Äù bytecode, Python has to remain within the constraints of the Global Interpreter Lock (GIL). This means that if you were to use a or a custom Python function to apply during a parallelized phase, Polars' speed is capped running Python code, preventing any multiple threads from executing the function.\n\nPolars will try to parallelize the computation of the aggregating functions over the groups, so it is recommended that you avoid using s and custom Python functions as much as possible. Instead, try to stay within the realm of the Polars expression API. This is not always possible, though, so if you want to learn more about using s you can go the user guide section on using user-defined functions."
    },
    {
        "link": "https://realpython.com/polars-python",
        "document": "In the world of data analysis and manipulation, Python has long been the go-to language. With extensive and user-friendly libraries like NumPy, pandas, PySpark, and Dask, there‚Äôs a solution available for almost any data-driven task. Among these libraries, one name that‚Äôs been generating a significant amount of buzz lately is Polars.\n\nPolars is a high-performance DataFrame library, designed to provide fast and efficient data processing capabilities. Inspired by the reigning pandas library, Polars takes things to another level, offering a seamless experience for working with large datasets that might not fit into memory.\n‚Ä¢ Why Polars is so performant and attention-grabbing\n‚Ä¢ How to work with DataFrames, expressions, and contexts\n‚Ä¢ What the lazy API is and how to use it\n‚Ä¢ How to integrate Polars with external data sources and the broader Python ecosystem\n\nAfter reading, you‚Äôll be equipped with the knowledge and resources necessary to get started using Polars for your own data tasks. Before reading, you‚Äôll benefit from having a basic knowledge of Python and experience working with tabular datasets. You should also be comfortable with DataFrames from any of the popular DataFrame libraries.\n\nPolars has caught a lot of attention in a short amount of time, and for good reason. In this first section, you‚Äôll get an overview of Polars and a preview of the library‚Äôs powerful features. You‚Äôll also learn how to install Polars along with any dependencies that you might need for your data processing task. Getting to Know Polars Polars combines the flexibility and user-friendliness of Python with the speed and scalability of Rust, making it a compelling choice for a wide range of data processing tasks. So, what makes Polars stand out among the crowd? There are many reasons, one of the most prominent being that Polars is lightning fast. The core of Polars is written in Rust, a language that operates at a low level with no external dependencies. Rust is memory-efficient and gives you performance on par with C or C++, making it a great language to underpin a data analysis library. Polars also ensures that you can utilize all available CPU cores in parallel, and it supports large datasets without requiring all data to be in memory. Note: If you want to take a deeper dive into Polars‚Äô features, check out this Real Python Podcast episode with Liam Brannigan. Liam is a Polars contributor, and he offers a nice firsthand perspective on Polars‚Äô capabilities. Another standout feature of Polars is its intuitive API. If you‚Äôre already familiar with libraries like pandas, then you‚Äôll feel right at home with Polars. The library provides a familiar yet unique interface, making it easy to transition to Polars. This means you can leverage your existing knowledge and codebase while taking advantage of Polars‚Äô performance gains. Polars‚Äô query engine leverages Apache Arrow to execute vectorized queries. Exploiting the power of columnar data storage, Apache Arrow is a development platform designed for fast in-memory processing. This is yet another rich feature that gives Polars an outstanding performance boost. These are just a few key details that make Polars an attractive data processing library, and you‚Äôll get to see these in action throughout this tutorial. Up next, you‚Äôll get an overview of how to install Polars. Before installing Polars, make sure you have Python and installed on your system. Polars supports Python versions 3.7 and above. To check your Python version, open a terminal or command prompt and run the following command: If you have Python installed, then you‚Äôll see the version number displayed below the command. If you don‚Äôt have Python 3.7 or above installed, follow these instructions to get the correct version. Polars is available on PyPI, and you can install it with . Open a terminal or command prompt, create a new virtual environment, and then run the following command to install Polars: This command will install the latest version of Polars from PyPI onto your machine. To verify that the installation was successful, start a Python REPL and import Polars: If the import runs without error, then you‚Äôve successfully installed Polars. You now have the core of Polars installed on your system. This is a lightweight installation of Polars that allows you to get started without extra dependencies. Polars has other rich features that allow you to interact with the broader Python ecosystem and external data sources. To use these features, you need to install Polars with the feature flags that you‚Äôre interested in. For example, if you want to convert Polars DataFrames to pandas DataFrames and NumPy arrays, then run the following command when installing Polars: This command installs the Polars core and the functionality that you need to convert Polars DataFrames to pandas and NumPy objects. You can find the list of optional dependencies that you can install with Polars in the documentation. Alternatively, you can run the following command to install Polars with all the optional dependencies: This is the best way to go if you feel like you‚Äôll utilize a wide range of Polars features. Otherwise, if you‚Äôd like to keep your environment as lightweight as possible, you should only install the optional dependencies that you need. Note: Throughout this tutorial, you‚Äôll work with both NumPy arrays and pandas DataFrames, so you need to make sure you have those features. If don‚Äôt want to install Polars with , then you‚Äôll need to at least run at this point. With Polars installed, you‚Äôre now ready to dive in. In the next section, you‚Äôll get an overview of Polars‚Äô core functionalities with DataFrames, expressions, and contexts. You‚Äôll get a feel for Polars syntax and start to see why the library is so powerful.\n\nNow that you‚Äôve installed Polars and have a high-level understanding of why it‚Äôs so performant, it‚Äôs time to dive into some core concepts. In this section, you‚Äôll explore DataFrames, expressions, and contexts with examples. You‚Äôll get a first impression of Polars syntax. If you know other DataFrame libraries, then you‚Äôll notice some similarities but also some differences. Like most other data processing libraries, the core data structure used in Polars is the DataFrame. A DataFrame is a two-dimensional data structure composed of rows and columns. The columns of a DataFrame are made up of series, which are one-dimensional labeled arrays. You can create a Polars DataFrame in a few lines of code. In the following example, you‚Äôll create a Polars DataFrame from a dictionary of randomly generated data representing information about houses. Be sure you have NumPy installed before running this example: In this example, you first import and with aliases of and , respectively. Next, you define , which determines how many rows will be in the randomly generated data. To generate random numbers, you call from NumPy‚Äôs module. This returns a generator that can produce a variety of random numbers according to different probability distributions. You then define a dictionary with the entries , , and , which are randomly generated arrays of length . The array contains floats, contains integers, and the array contains strings. These will become the three columns of a Polars DataFrame. Note: If you‚Äôve used NumPy in the past, you might be wondering why this example uses the generator instead of directly calling functions from NumPy‚Äôs module. Since version 1.17, NumPy uses the permuted congruential generator-64 (PCG64) algorithm to generate random numbers. PCG64 is more efficient than the previously used Mersenne Twister algorithm, and it produces less predictable numbers. Check out Using the NumPy Random Number Generator if you‚Äôre interested in knowing more. To create the Polars DataFrame, you call . The class constructor for a Polars DataFrame accepts two-dimensional data in various forms, a dictionary in this example. You now have a Polars DataFrame that‚Äôs ready to use! When you display in the console, a nice string representation of the DataFrame is displayed. The string representation first prints the shape of the data as a tuple with the first entry telling you the number of rows and the second the number of columns in the DataFrame. You then see a tabular preview of the data that shows the column names and their data types. For instance, has type , and has type . Polars supports a variety of data types that are primarily based on the implementation from Arrow. Polars DataFrames are equipped with many useful methods and attributes for exploring the underlying data. If you‚Äôre already familiar with pandas, then you‚Äôll notice that Polars DataFrames use mostly the same naming conventions. You can see some of these methods and attributes in action on the DataFrame that you created in the previous example: You first look at the schema of the DataFrame with . Polars schemas are dictionaries that tell you the data type of each column in the DataFrame, and they‚Äôre necessary for the lazy API that you‚Äôll explore later. Next, you get a preview of the first five rows of the DataFrame with . You can pass any integer into , depending on how many of the top rows you want to see, and the default number of rows is five. Polars DataFrames also have a method that allows you to view the bottom rows. Lastly, you call to get summary statistics for each column in the DataFrame. This is one of the best ways to get a quick feel for the nature of the dataset that you‚Äôre working with. Here‚Äôs what each row returned from means:\n‚Ä¢ is the number of observations or rows in the dataset.\n‚Ä¢ is the number of missing values in the column.\n‚Ä¢ is the arithmetic mean, or average, of the column.\n‚Ä¢ is the standard deviation of the column.\n‚Ä¢ is the minimum value of the column.\n‚Ä¢ is the maximum value of the column.\n‚Ä¢ is the median value, or fiftieth percentile, of the column.\n‚Ä¢ is the twenty-fifth percentile, or first quartile, of the column.\n‚Ä¢ is the seventy-fifth percentile, or third quartile, of the column. As an example interpretation, the mean in the data is between 2008 and 2009, with a standard deviation of just above eight years. The column is missing most of the summary statistics because it consists of categorical values represented by strings. Now that you‚Äôve seen the basics of creating and interacting with Polars DataFrames, you can start trying more sophisticated queries and get a feel for the library‚Äôs power. To do this, you‚Äôll need to understand contexts and expressions, which are the topics of the next section. Contexts and expressions are the core components of Polars‚Äô unique data transformation syntax. Expressions refer to computations or transformations that are performed on data columns, and they allow you to apply various operations on the data to derive new results. Expressions include mathematical operations, aggregations, comparisons, string manipulations, and more. A context refers to the specific environment or situation in which an expression is evaluated. In other words, a context is the fundamental action that you want to perform on your data. Polars has three main contexts:\n‚Ä¢ Filtering: Reducing the DataFrame size by extracting rows that meet specified conditions\n‚Ä¢ Groupby/aggregation: Computing summary statistics within subgroups of the data You can think of contexts as verbs and expressions as nouns. Contexts determine how the expressions are evaluated and executed, just as verbs determine the actions performed by nouns in language. To get started working with expressions and contexts, you‚Äôll work with the same randomly generated data as before. Here‚Äôs the code to create the DataFrame again: With the DataFrame created, you‚Äôre ready to get started using expressions and contexts. Within Polars‚Äô three main contexts, there are many different types of expressions, and you can pipe multiple expressions together to run arbitrarily complex queries. To better understand these ideas, take a look at an example of the select context: With the same randomly generated data as before, you see two different contexts for selecting the column from the DataFrame. The first context, , extracts the column directly from its name. The second context, , accomplishes the same task in a more powerful way because you can perform further manipulations on the column. In this case, is the expression that‚Äôs passed into the context. By using the expression within the context, you can do further manipulations on the column. In fact, you can pipe as many expressions onto the column as you want, which allows you to carry out several operations. For instance, if you want to sort the column and then divide all of the values by 1000, you could do the following: As you can see, this select context returns the column sorted and scaled down by 1000. One context that you‚Äôll often use prior to is . As the name suggests, reduces the size of the data based on a given expression. For example, if you want to filter the data down to houses that were built after 2015, you could run the following: By passing the expression into , you get back a DataFrame that only contains houses that were built after 2015. You can see this because only has 1230 of the 5000 original rows, and the minimum in is 2016. Another commonly used context in Polars, and data analysis more broadly, is the groupby context, also known as aggregation. This is useful for computing summary statistics within subgroups of your data. In the building data example, suppose you want to know the average square footage, median building year, and number of buildings for each building type. The following query accomplishes this task: In this example, you first call , which creates a Polars object. The object has an aggregation method, , which accepts a list of expressions that are computed for each group. For instance, calculates the average square footage for each building type, and returns the number of buildings of each building type. You use to name the aggregated columns. While it‚Äôs not apparent with the high-level Python API, all Polars expressions are optimized and run in parallel under the hood. This means that Polars expressions don‚Äôt always run in the order you specify, and they don‚Äôt necessarily run on a single core. Instead, Polars optimizes the order in which expressions are evaluated in a query, and the work is spread across available cores. You‚Äôll see examples of optimized queries later in this tutorial. Now that you have an understanding of Polars contexts and expressions, as well as insight into why expressions are evaluated so quickly, you‚Äôre ready to take a deeper dive into another powerful Polars feature, the lazy API. With the lazy API, you‚Äôll see how Polars is able to evaluate sophisticated expressions on large datasets while keeping memory efficiency in mind.\n\nPolars‚Äô lazy API is one of the most powerful features of the library. With the lazy API, you can specify a sequence of operations without immediately running them. Instead, these operations are saved as a computational graph and only run when necessary. This allows Polars to optimize queries before execution, catch schema errors before the data is processed, and perform memory-efficient queries on datasets that don‚Äôt fit into memory. The core object within the lazy API is the LazyFrame, and you can create LazyFrames in a few different ways. To get started with LazyFrames and the lazy API, take a look at this example: You first create another toy dataset similar to the one that you worked with earlier, but this example includes a column named . You then call to create a LazyFrame from . Alternatively, you can convert an existing DataFrame to a LazyFrame with . To see how the lazy API works, you can create the following query: In this query, you compute the price per square foot of each building and assign it the name . You then filter the data on all buildings with a greater than 100 and less than 2010. You may have noticed that the lazy query returns another LazyFrame, rather than actually executing the query. This is the idea behind the lazy API. It only executes queries when you explicitly call them. Before you execute the query, you can inspect what‚Äôs known as the query plan. The query plan tells you the sequence of steps that the query will trigger. To get a nice visual of the LazyFrame query plan, you can run the following code: The method renders an image representation of the query plan. To see this image, be sure you have Matplotlib installed. If you‚Äôre working in a Jupyter Notebook, then the image should render in the output cell. Otherwise, a separate window should pop up with an image similar to this: You read query plan graphs from bottom to top in Polars, and each box corresponds to a stage in the query plan. Sigma (œÉ) and pi (œÄ) are symbols from relational algebra, and they tell you the operation that you‚Äôre performing on the data. In this example, œÄ */4 says that you‚Äôre working with all four columns of the DataFrame, and œÉ(col(‚Äúyear‚Äù)) < 2010 tells you that you‚Äôre only processing rows with a less than 2010. You can interpret the full query plan with these steps:\n‚Ä¢ Use the four columns of , and filter to rows where is less than 2010.\n‚Ä¢ Filter to all rows where is greater than 100. One important note is that Polars filters on before executing any other part of the query, despite this being the last filter that you specified in the code. This is known as predicate pushdown, a Polars optimization that makes queries more memory efficient by applying filters as early as possible and subsequently reducing the data size before further processing. You might have noticed that the query plan graph cuts off important details that don‚Äôt fit into a box. This is common, especially as your queries become more complex. If you need to see the full representation of your query plan, then you can use the method: When you print the output of on a LazyFrame, you get a string representation of the query plan in return. As with the graphical query plan, you read the string query plan from bottom to top, and each stage is on its own line. If you call without , then you‚Äôll see the string representation of the query plan. As usual, newlines show up as in strings, so the plan will be harder to read. Again, you should use when you need a more verbose explanation of the query plan that‚Äôs not contained in the graphical representation. With an understanding of what your lazy query is set to do, you‚Äôre ready to actually execute it. To do this, you call on your lazy query to evaluate it according to the query plan. Here‚Äôs what this looks like in action: When you run a lazy query with , you get a regular Polars DataFrame with the results. Because of the filtering criteria, you get only 1317 of the original 5000 rows. You might also notice that all of the and values displayed are greater than 100 and less than 2010, respectively. To further verify that the query properly filtered your data, you can take a look at the summary statistics: When you look at the summary statistics with , you see that the minimum is around 100, and the maximum is 2009. This is exactly what you asked for in the lazy query! Now you have some familiarity with the lazy API, but you might be wondering what the advantage of the lazy API is. If the entire dataset is already stored in memory, why do you need lazy queries to do your analysis? Continue reading to see where the lazy API really shines. In real-world applications, you‚Äôll most likely store your data externally in a static file or database before you do any processing in Python. One of the main superpowers of the lazy API is that it allows you to process large datasets stored in files without reading all the data into memory. When working with files like CSVs, you‚Äôd traditionally read all of the data into memory prior to analyzing it. With Polars‚Äô lazy API, you can minimize the amount of data read into memory by only processing what‚Äôs necessary. This allows Polars to optimize both memory usage and computation time. In the next example, you‚Äôll work with electric vehicle population data from Data.gov. This dataset contains information about electric and hybrid vehicles registered in the Washington State Department of Licensing. Each row in the data represents one car, and each column contains information about the car. You can manually download this data from the website, or you can use the following function to download the file programmatically. Make sure you have installed in your environment before trying this example: \"\"\"Download a file and save it with the specified file name.\"\"\" This function uses the requests library to download a file from a specified URL. You make a GET request to , and if the request is successful, you store the file locally at . You can run the following code to download the electric vehicle population data: In this code snippet, you first import from . You then call on the electric vehicle population data and store it as a CSV. You store the 140,000 rows as in the working directory of your Python instance. You‚Äôre now ready to interact with the data through the lazy API. The key to efficiently working with files through the lazy API is to use Polars‚Äô scan functionality. When you scan a file, rather than reading the entire file into memory, Polars creates a LazyFrame that references the file‚Äôs data. As before, no processing of the data occurs until you explicitly execute a query. With the following code, you scan : You create a LazyFrame, , by using . Crucially, the data from the CSV file isn‚Äôt stored in memory. Instead, the only thing stores from is the schema from . This allows you to see the file‚Äôs column names and their respective data types, and it also helps Polars optimize queries that you run on this data. In fact, Polars must know the schema before executing any step of a query plan. You can now run a query on the data contained in using the lazy API. Your queries can have arbitrary complexity, and Polars will only store and process the necessary data. For instance, you could run the following query: In this query, you filter the data on all cars where the model year is 2018 or later and the electric vehicle type is . You then compute the average electric range, the minimum model year, and the number of cars for each state and make. Lastly, you further filter the data where the average electric range is positive and where the number of cars for the state and make is greater than five. Because this is a lazy query, no computation is performed until you call . After the query is executed, only the data you asked for is stored and returned‚Äînothing more. Each row in the DataFrame returned from tells you the average electric range, oldest model year, and number of cars for each state and make. For example, the first row tells you there are 55,690 Teslas from 2018 or later in Washington State, and their average electric range is around 89.11 miles. With this example, you saw how Polars uses the lazy API to query data from files in a performant and memory-efficient manner. This powerful API gives Polars a huge leg up over other DataFrame libraries, and you should opt to use the lazy API whenever possible. In the next section, you‚Äôll get a look at how Polars integrates with external data sources and the broader Python ecosystem.\n\nPolars can read from most popular data sources, and it integrates well with other commonly used Python libraries. This means, for many use cases, Polars can replace whatever data processing library you‚Äôre currently using. In this section, you‚Äôll walk through examples of Polars‚Äô flexibility in working with different data sources and libraries. In the previous section, you saw how Polars performs lazy queries over CSV files with . Polars can also handle data sources like JSON, Parquet, Avro, Excel, and various databases. You can interact with most of these file types the same way you worked with the CSV file: In this example, you export the results of your work in Polars with various file formats. You first create a DataFrame with columns and . You then write the data to CSV, JSON, and Parquet files in the working path of your Python instance. These files are now ready for you to share and read, and Polars makes this quite straightforward: In this example, you read and scan each of the three files that you previously created, and you print their schemas to confirm that the column names and data types are correct. Polars‚Äô ability to scan these file types also means that the data can be quite large, and you can execute lazy queries to handle this. Note: Unlike Polars‚Äô scan functionality, Polars‚Äô read functions read the entire file into memory. To get the most out of Polars, you should prefer scans over reads whenever possible. Reading an entire file into memory should only occur when you‚Äôre working with a file type that isn‚Äôt scannable or performing an operation that you can‚Äôt execute with the lazy API. Polars supports other file types, and its file capabilities are constantly improving. Polars can also scan and read multiple files with the same schema as if they were a single file. Lastly, Polars can connect directly to a database and execute SQL queries. Overall, Polars provides a full suite of tools for interacting with commonly used data sources. Next, you‚Äôll see how Polars integrates with other Python libraries, making the library flexible enough to drop into existing code with minimal overhead. Polars integrates seamlessly with existing Python libraries. This is a crucial feature because it allows you to drop Polars into existing code without having to change your dependencies or do a big refactor. In the following example, you can see how Polars DataFrames seamlessly convert between NumPy arrays and pandas DataFrames: After importing , , and , you create identical datasets using the three libraries. You‚Äôll use the Polars DataFrame, pandas DataFrame, and NumPy array to see how interoperable these libraries are. For example, you can convert the pandas DataFrame and NumPy array to Polars DataFrames with the following functions: Here, converts your pandas DataFrame to a Polars DataFrame. Similarly, converts your NumPy array to a Polars DataFrame. If you want your columns to have the right data types and names, then you should specify the argument when calling . If you want to convert your Polars DataFrame back to pandas or NumPy, then you can do the following: You use and to convert your Polars DataFrame to a pandas DataFrame and NumPy array. Conveniently, and are methods of the Polars object. The creators of Polars anticipated that many users would want to integrate their Polars code with existing pandas and NumPy code, so they decided to make the conversion between Polars and pandas or NumPy a native action of the DataFrame object. Because of their widespread use in the Python community, libraries like pandas and NumPy are here to stay for a while. Polars‚Äô ability to integrate with these libraries means that you can introduce it as a way to improve the performance of an existing workflow. For instance, you could use Polars to do intensive data preprocessing for a machine learning model, and then convert the results to a NumPy array before feeding it to your model."
    },
    {
        "link": "https://pbpython.com/polars-intro.html",
        "document": "It‚Äôs been a while since I‚Äôve posted anything on the blog. One of the primary reasons for the hiatus is that I have been using python and pandas but not to do anything very new or different. In order to shake things up and hopefully get back into the blog a bit, I‚Äôm going to write about polars. This article assumes you know how to use pandas and are interested in determining if polars can fit into your workflow. I will cover some basic polars concepts that should get you started on your journey. Along the way I will point out some of the things I liked and some of the differences that that might limit your usage of polars if you‚Äôre coming from pandas. Ultimately, I do like polars and what it is trying to do. I‚Äôm not ready to throw out all my pandas code and move over to polars. However, I can see where polars could fit into my toolkit and provide some performance and capability that is missing from pandas. As you evaluate the choice for yourself, it is important to try other frameworks and tools and evaluate them on their merits as they apply to your needs. Even if you decide polars doesn‚Äôt meet your needs it is good to evaluate options and learn along the way. Hopefully this article will get you started down that path.\n\nAs mentioned above, pandas has been the data analysis tool for python for the past few years. Wes McKinney started the initial work on pandas in 2008 and the 1.0 release was in January 2020. Pandas has been around a long time and will continue to be. While pandas is great, it has it‚Äôs warts. Wes McKinney wrote about several of these challenges. There are many other criticisms online but most will boil down to two items: performance and awkward/complex API. Polars was initially developed by Richie Vink to solve these issues. His 2021 blog post does a thorough job of laying out metrics to back up his claims on the performance improvements and underlying design that leads to these benefit with polars. The user guide concisely lays out the polars philosophy: The goal of Polars is to provide a lightning fast DataFrame library that:\n‚Ä¢ Utilizes all available cores on your machine.\n‚Ä¢ Handles datasets much larger than your available .\n‚Ä¢ Has an that is consistent and predictable.\n‚Ä¢ Has a strict schema (data-types should be known before running the query). Polars is written in Rust which gives it C/C++ performance and allows it to fully control performance critical parts in a query engine. As such Polars goes to great lengths to: Clearly performance is an important goal in the development of polars and key reason why you might consider using polars. This article won‚Äôt discuss performance but will focus on the polars API. The main reason is that for the type of work I do, the data easily fits in RAM on a business-class laptop. The data will fit in Excel but it is slow and inefficient on a standard computer. I rarely find myself waiting on pandas once I have read in the data and have done basic data pre-processing. Of course performance matters but it‚Äôs not everything. If you‚Äôre trying to make a choice between pandas, polars or other tools don‚Äôt make a choice based on general notions of ‚Äúperformance improvement‚Äù but based on what works for your specific needs.\n\nFor this article, I‚Äôll be using data from an earlier post which you can find on github. I would recommend following the latest polar installation instructions in the user guide . I chose to install polars with all of the dependencies: Once installed, reading the downloaded Excel file is straightforward: When I read this specific file, I found that the date column did not come through as a type so I used the argument to make sure the data was properly typed. Since data typing is so important, here‚Äôs one quick way to check on it: A lot of the standard pandas commands such as , , work as expected with a little extra output sprinkled in: The polars output has a couple of notable features:\n‚Ä¢ The is included which is useful to make sure you‚Äôre not dropping rows or columns inadvertently\n‚Ä¢ Underneath each column name is a data type which is another useful reminder\n‚Ä¢ There are no index numbers\n‚Ä¢ The string columns include ‚Äù ‚Äù around the values Overall, I like this output and do find it useful for analyzing the data and making sure the data is stored in the way I expect.\n\nPolars introduces the concept of Expressions to help you work with your data. There are four main expressions you need to understand when working with data in polars:\n‚Ä¢ to choose the subset of columns you want to work with\n‚Ä¢ to choose the subset of rows you want to work with Choosing or reordering columns is straightforward with The code is used to create column expressions. You will want to use this any time you want to specify one or more columns for an action. There are shortcuts where you can use data without specifying but I‚Äôm choosing to show the recommended way. Filtering is a similar process (note the use of again): Coming from pandas, I found selecting columns and filtering rows to be intuitive.\n\nThe next expression, , takes a little more getting used to. The easiest way to think about it is that any time you want to add a new column to your data, you need to use . To illustrate, I will add a month name column which will also show how to work with date and strings. This command does a couple of things to create a new column:\n‚Ä¢ Access the underlying date with and convert it to the 3 character month name using\n‚Ä¢ Name the newly created column using the function As a brief aside, I like using to rename columns. As I played with polars, this made a lot of sense to me. Here‚Äôs another example to drive the point home. Let‚Äôs say we want to understand how much any one product order contributes to the total percentage unit volume for the year: In this example we divide the line item quantity by the total quantity and label it as . You may have noticed that the previous column is not there. That‚Äôs because none of the operations we have done are in-place. If we want to persist a new column, we need to assign it to a new variable. I will do so in a moment. I briefly mentioned working with strings but here‚Äôs another example. Let‚Äôs say that any of the sku data with an ‚ÄúS‚Äù at the front is a special product and we want to indicate that for each item. We use in a way very similar to the pandas accessor. Polars has a useful function which can replace pandas or Let‚Äôs say we want to create a column that indicates a special or includes the original sku if it‚Äôs not a special product. This is somewhat analogous to an if-then-else statement in python. I personally like this syntax because I alway struggle to use pandas equivalents. This example also introduces which we use to assign a literal value to the columns.\n\nThe pandas and polars functional similarly but the key difference is that polars does not have the concept of an index or multi-index. There are pros and cons to this approach which I will briefly touch on later in this article. Here‚Äôs a simple polars example to total the unit amount by sku by customer. The syntax is similar to pandas with dictionary approach I have mentioned before. You will notice that we continue to use to reference our column of data and then to assign a custom name. The other big change here is that the data does not have a multi-index, the result is roughly the same as using with a pandas groupby. The benefit of this approach is that it is easy to work with this data without flattening or resetting your data. The downside is that you can not use and to make the data wider or narrower as needed. When working with date/time data, you can group data similar to the pandas grouper function by using : There are a couple items to note:\n‚Ä¢ Polars asks that you sort the data by column before doing the\n‚Ä¢ The argument allows you to specify what date/time level to aggregate to To expand on this example, what if we wanted to show the month name and year, instead of the date time? We can chain together the and add a new column by using This example starts to show the API expressiveness of polars. Once you understand the basic concepts, you can chain them together in a way that is generally more straightforward than doing so with pandas. To summarize this example:\n‚Ä¢ Totaled the quantity and assigned the column name to\n‚Ä¢ Change the date label to be more readable and assigned the name\n‚Ä¢ Then down-selected to show the two columns I wanted to focus on\n\nWe have touched on chaining expressions but I wanted to give one full example below to act as a reference. Combining multiple expressions is available in pandas but it‚Äôs not required. This post from Tom Augspurger shows a nice example of how to use different pandas functions to chain operations together. This is also a common topic that Matt Harrison (@__mharrison__) discusses. Chaining expressions together is a first class citizen in polars so it is intuitive and an essential part of working with polars. Here is an example combining several concepts we showed earlier in the article: I made this graphic to show how the pieces of code interact with each other: The image is small on the blog but if you open it in a new window, it should be more legible. It may take a little time to wrap your head around this approach to programming. But the results should pay off in more maintainable and performant code.\n\nAs you work with pandas and polars there are convenience functions for moving back and forth between the two. Here‚Äôs an example of creating a pandas dataframe from polars: Having this capability means you can gradually start to use polars and go back to pandas if there are activities you need in polars that don‚Äôt quite work as expected. If you need to work the other way, you can convert a pandas dataframe to a polars one using Finally, one other item I noticed when working with polars is that there are some nice convenience features when saving your polars dataframe to Excel. By default the dataframe is stored in a table and you can make a lot of changes to the output by tweaking the parameters to the . I recommend reviewing the official API docs for the details. To give you a quick flavor, here is an example of some simple configuration: There are a lot of configuration options available but I generally find this default output easier to work with thank pandas.\n\nPandas has been the go-to data analysis tool in the python ecosystem for over a decade. Over that time it has grown and evolved and the surrounding ecosystem has changed. As a result some of the core parts of pandas might be showing their age. Polars brings a new approach to working with data. It is still in the early phases of its development but I am impressed with how far it has come in the first few years. As of this writing, polars is moving to a 1.0 release. This milestone means that the there will be fewer breaking changes going forward and the API will stabilize. It‚Äôs a good time to jump in and learn more for yourself. I‚Äôve only spent a few hours with polars so I‚Äôm still developing my long-term view on where it fits. Here are a few of my initial observations:\n‚Ä¢ Performant design from the ground up which maximizes modern hardware and minimizes memory usage\n‚Ä¢ Not having indices simplifies many cases\n‚Ä¢ Useful improvement in displaying output, saving excel files, etc. Regarding the plotting functionality, I think it‚Äôs better to use the available ones than try to include in polars. There is a namespace in polars but it defers to other libraries to do the plotting.\n‚Ä¢ Still newer code base with breaking changes\n‚Ä¢ Not as much third party documentation\n‚Ä¢ Not as seamlessly integrated with other libraries (although it is improving)\n‚Ä¢ Some pandas functions like stacking and unstacking are not as mature in polars\n‚Ä¢ Tried and tested code base that has been improved significantly over the years\n‚Ä¢ The multi-index support provides helpful shortcuts for re-shaping data\n‚Ä¢ Strong integrations with the rest of the python data ecosystem\n‚Ä¢ Good official documentation as well as lots of 3rd party sources for tips and tricks\n‚Ä¢ Some cruft in the design. There‚Äôs more than one way to do things in many cases.\n‚Ä¢ Performance for large data sets can get bogged down This is not necessarily exhaustive but I think hits the highlights. At the end of the day, diversity in tools and approaches is helpful. I intend to continue evaluating the integration of polars into my analysis - especially in cases where performance becomes an issue or the pandas code gets too be too messy. However, I don‚Äôt think pandas is going away any time soon and I continue to be excited about pandas evolution. I hope this article helps you get started. As always, if you have experiences, thoughts or comments on the article, let me know below."
    },
    {
        "link": "https://docs.pola.rs/py-polars/html/reference/dataframe/aggregation.html",
        "document": "Return the number of non-null elements for each column.\n\nAggregate the columns of this DataFrame to their maximum value.\n\nGet the maximum value horizontally across columns.\n\nAggregate the columns of this DataFrame to their mean value.\n\nTake the mean of all values horizontally across columns.\n\nAggregate the columns of this DataFrame to their median value.\n\nAggregate the columns of this DataFrame to their minimum value.\n\nGet the minimum value horizontally across columns.\n\nAggregate the columns of this DataFrame to their product values.\n\nAggregate the columns of this DataFrame to their quantile value."
    },
    {
        "link": "https://rhosignal.com/posts/pandas-to-polars-time-series",
        "document": "There are differences between some important time series concepts in Pandas and Polars that you should know. In this post, to help you make the Pandas to Polars switch I talk through some of these key differences.\n\nI‚Äôm working with Polars version 0.20.6 here, but most of these changes should be independant of the version of Polars you are using.\n\nIn Pandas we can use date strings when working with dates and times. In Polars, on the other hand, we use Python objects and we never use strings to do datetime operations.\n\nTo illustrate this we create a timeseries in Polars and then convert it to Pandas. To create a date column in Polars we use the confusingly-named class in Python.\n\nIn Pandas we can use datetime strings to filter datetimes like this:\n\nBut in Polars we use the class to filter dates:\n\nThe Polars developers chose not to support string datetime representations because they are ambiguous. For example, could be the 2nd of January or the 1st of February depending on the locale.\n\nOf course, we can still extract a string representation as a string column using the method:\n\nIn Pandas and Polars we can represent intervals using strings. In Pandas, for example, we use for 30 minutes. In Polars we use for 30 minutes. Here are some examples of interval strings in Polars:\n\nWe can compose these interval strings to create more complex intervals. For example, we can use for 1 hour and 30 minutes.\n\nIn both libraries the datetime, date and duration dtypes are all based on an underlying integer representation of time. For example, with the dtype, the integer represents a count since the start of the Unix epoch.\n\nIn Pandas the integer counts occur in nanoseconds by default but in Polars the integer counts occur in microseconds by default. The microseconds are denoted by in the schema below:\n\nHowever, Polars also supports nanosecond precision while Pandas also supports microsecond precision.\n\nIf we convert a Pandas to a Polars then the integer representations remain in nanoseconds. We can‚Äôt join two Polars on a datetime if one has nanosecond precision and the other has microsecond precision. So when I convert from Pandas to Polars I normally cast datetime columns to microseconds straight away using the expression:\n\nIn Pandas a missing datetime in a datetime column is represented by (not a time). In Polars a missing datetime is represented by the same value it is represented by in every column: .\n\nI find that having the same representation for missing values in every column makes it easier to work with missing values in Polars. This is because I don‚Äôt have to remember different approaches for missing values in different dtypes e.g . versus in Pandas.\n\nTemporal groupby in Polars has its own method\n\nIn Pandas you do temporal groupby by passing the method:\n\nIn Polars we have a special method for temporal groupby . In this example we get the mean value for each day:\n\nNote that we sort the by the datetime column before we do the groupby. This is because the method requires the data to be sorted by the column we are grouping by.\n\nAs in Pandas we have lots of flexibility in how the grouping windows are set. For example we if want to offset the start of the windows by 2 hours we can do this:\n\nResample in Pandas is or in Polars\n\nIn Pandas we use the method to change the frequency of a time series.\n\nIn Polars we can use the method change to a higher frequency than the data.\n\nFor example, to upsample a 1 hour frequency to 30 minutes we can do this:\n\nPolars can take advantage of sorted data to speed up operations using fast-path operations. These fast-path operations occur where Polars knows a column is sorted and can therefore use a faster algorithm to perform the operation. As time series data has a natural sort order it is particularly important to be aware of fast-paths for time series analysis.\n\nWe can adapt our code above for a simple example of a fast-path operation on time series data. This time we are looking for datetimes before the 2nd of January.\n\nIf Polars knows that the column is sorted then the fast-path operation is to stop scanning the column once it finds the first row that is greater than or equal to the filter value. This can be much faster than scanning the whole column.\n\nOther important time-series methods that support fast-path operations include and .\n\nCheck out these posts for more on fast-path operations in Polars:\n\nOr you can see my many other Polars posts here:https://www.rhosignal.com/tags/polars/\n\nIf you would like more detailed support on working with Polars then I provide consulting on optimising your data processing pipelines with Polars. You can also check out my online course to get you up-and-running with Polars by clicking on the bear below\n\nWant to know more about Polars for high performance data science? Then you can:\n‚Ä¢ get in touch to discuss your data processing challenges\n‚Ä¢ connect with me at linkedin"
    },
    {
        "link": "https://pandas.pydata.org/docs/getting_started/intro_tutorials/09_timeseries.html",
        "document": "Aggregate the current hourly time series values to the monthly maximum value in each of the stations.\n\nA very powerful method on time series data with a datetime index, is the ability to time series to another frequency (e.g., converting secondly data into 5-minutely data)."
    },
    {
        "link": "https://python.plainenglish.io/time-series-data-handling-dates-and-times-in-polars-and-pandas-f436309a0c30",
        "document": "Time series data is ubiquitous in fields like finance, healthcare, and IoT. Both Polars and Pandas provide robust support for handling time-based data. Below, I‚Äôll demonstrate a variety of examples with detailed explanations for each code block.\n\nBefore we begin, we need to import the required libraries.\n‚Ä¢ is the widely used Python library for data manipulation.\n‚Ä¢ is a high-performance alternative for working with tabular data.\n‚Ä¢ and are Python standard library modules to handle date and time objects."
    },
    {
        "link": "https://reddit.com/r/Python/comments/10gf1cr/replacing_pandas_with_polars_a_practical_guide",
        "document": "The official Python community for Reddit! Stay up to date with the latest news, packages, and meta information relating to the Python programming language. --- If you have questions or are new to Python use r/LearnPython"
    },
    {
        "link": "https://stackoverflow.com/questions/74844191/why-polars-date-time-subseting-is-slow",
        "document": "Could this just be an issue of benchmarking a relatively small query? My benchmarking shows the exact opposite.\n\nLet's start with a good amount of data, so that the subsetting process represents the largest share of the work. I'll also the data, so that issues of \"sortedness\" are not involved.\n\nI've put together the three cases from your code into separate tests.\n\nHere are the timings for three runs of the above on my system (a 32-core system).\n\nOn my system, the Pandas indexing runs the slowest - by far. I could run this more than three times, but I think the pattern is quite clear.\n\nCan you run a larger test on your system? Perhaps there is something else at play here...\n\nFor reference, version information on my test environment ..."
    }
]