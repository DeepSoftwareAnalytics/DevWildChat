[
    {
        "link": "https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Executors.html",
        "document": "\n• threads will be active processing tasks. If additional tasks are submitted when all threads are active, they will wait in the queue until a thread is available. If any thread terminates due to a failure during execution prior to shutdown, a new one will take its place if needed to execute subsequent tasks. The threads in the pool will exist until it is explicitly Creates a thread pool that reuses a fixed number of threads operating off a shared unbounded queue. At any point, at mostthreads will be active processing tasks. If additional tasks are submitted when all threads are active, they will wait in the queue until a thread is available. If any thread terminates due to a failure during execution prior to shutdown, a new one will take its place if needed to execute subsequent tasks. The threads in the pool will exist until it is explicitly - the number of threads in the pool\n• Creates a thread pool that maintains enough threads to support the given parallelism level, and may use multiple queues to reduce contention. The parallelism level corresponds to the maximum number of threads actively engaged in, or available to engage in, task processing. The actual number of threads may grow and shrink dynamically. A work-stealing pool makes no guarantees about the order in which submitted tasks are executed.\n• Creates a work-stealing thread pool using all as its target parallelism level.\n• threads will be active processing tasks. If additional tasks are submitted when all threads are active, they will wait in the queue until a thread is available. If any thread terminates due to a failure during execution prior to shutdown, a new one will take its place if needed to execute subsequent tasks. The threads in the pool will exist until it is explicitly Creates a thread pool that reuses a fixed number of threads operating off a shared unbounded queue, using the provided ThreadFactory to create new threads when needed. At any point, at mostthreads will be active processing tasks. If additional tasks are submitted when all threads are active, they will wait in the queue until a thread is available. If any thread terminates due to a failure during execution prior to shutdown, a new one will take its place if needed to execute subsequent tasks. The threads in the pool will exist until it is explicitly - the number of threads in the pool - the factory to use when creating new threads\n• Creates an Executor that uses a single worker thread operating off an unbounded queue. (Note however that if this single thread terminates due to a failure during execution prior to shutdown, a new one will take its place if needed to execute subsequent tasks.) Tasks are guaranteed to execute sequentially, and no more than one task will be active at any given time. Unlike the otherwise equivalent the returned executor is guaranteed not to be reconfigurable to use additional threads.\n• Creates an Executor that uses a single worker thread operating off an unbounded queue, and uses the provided ThreadFactory to create a new thread when needed. Unlike the otherwise equivalent the returned executor is guaranteed not to be reconfigurable to use additional threads. - the factory to use when creating new threads\n• will reuse previously constructed threads if available. If no existing thread is available, a new thread will be created and added to the pool. Threads that have not been used for sixty seconds are terminated and removed from the cache. Thus, a pool that remains idle for long enough will not consume any resources. Note that pools with similar properties but different details (for example, timeout parameters) may be created using Creates a thread pool that creates new threads as needed, but will reuse previously constructed threads when they are available. These pools will typically improve the performance of programs that execute many short-lived asynchronous tasks. Calls towill reuse previously constructed threads if available. If no existing thread is available, a new thread will be created and added to the pool. Threads that have not been used for sixty seconds are terminated and removed from the cache. Thus, a pool that remains idle for long enough will not consume any resources. Note that pools with similar properties but different details (for example, timeout parameters) may be created using constructors.\n• Creates a thread pool that creates new threads as needed, but will reuse previously constructed threads when they are available, and uses the provided ThreadFactory to create new threads when needed. - the factory to use when creating new threads\n• Creates a single-threaded executor that can schedule commands to run after a given delay, or to execute periodically. (Note however that if this single thread terminates due to a failure during execution prior to shutdown, a new one will take its place if needed to execute subsequent tasks.) Tasks are guaranteed to execute sequentially, and no more than one task will be active at any given time. Unlike the otherwise equivalent the returned executor is guaranteed not to be reconfigurable to use additional threads.\n• Creates a single-threaded executor that can schedule commands to run after a given delay, or to execute periodically. (Note however that if this single thread terminates due to a failure during execution prior to shutdown, a new one will take its place if needed to execute subsequent tasks.) Tasks are guaranteed to execute sequentially, and no more than one task will be active at any given time. Unlike the otherwise equivalent the returned executor is guaranteed not to be reconfigurable to use additional threads. - the factory to use when creating new threads\n• Creates a thread pool that can schedule commands to run after a given delay, or to execute periodically. - the number of threads to keep in the pool, even if they are idle\n• Creates a thread pool that can schedule commands to run after a given delay, or to execute periodically. - the number of threads to keep in the pool, even if they are idle - the factory to use when the executor creates a new thread\n• Returns an object that delegates all defined methods to the given executor, but not any other methods that might otherwise be accessible using casts. This provides a way to safely \"freeze\" configuration and disallow tuning of a given concrete implementation.\n• Returns an object that delegates all defined methods to the given executor, but not any other methods that might otherwise be accessible using casts. This provides a way to safely \"freeze\" configuration and disallow tuning of a given concrete implementation.\n• method. Each new thread is created as a non-daemon thread with priority set to the smaller of and the maximum priority permitted in the thread group. New threads have names accessible via Returns a default thread factory used to create new threads. This factory creates all new threads used by an Executor in the same . If there is a , it uses the group of , else the group of the thread invoking thismethod. Each new thread is created as a non-daemon thread with priority set to the smaller ofand the maximum priority permitted in the thread group. New threads have names accessible via of pool-N-thread-M, where N is the sequence number of this factory, and M is the sequence number of the thread created by this factory.\n• method. A new can be created within an Returns a thread factory used to create new threads that have the same permissions as the current thread. This factory creates threads with the same settings as , additionally setting the AccessControlContext and contextClassLoader of new threads to be the same as the thread invoking thismethod. A newcan be created within an action setting the current thread's access control context to create threads with the selected permission settings holding within that action. Note that while tasks running within such threads will have the same access control and class loader settings as the current thread, they need not have the same or values. If necessary, particular values of thread locals can be set or reset before any task runs in subclasses using . Also, if it is necessary to initialize worker threads to have the same InheritableThreadLocal settings as some other designated thread, you can create a custom ThreadFactory in which that thread waits for and services requests to create others that will inherit its values. - if the current access control context does not have permission to both get and set context class loader\n• to an otherwise resultless action. Returns a object that, when called, runs the given task and returns the given result. This can be useful when applying methods requiring ato an otherwise resultless action. - the type of the result\n• . Returns a object that, when called, runs the given task and returns\n• Returns a object that, when called, runs the given privileged action and returns its result.\n• Returns a object that, when called, runs the given privileged exception action and returns its result.\n• under the current access control context. This method should normally be invoked within an Returns a object that will, when called, execute the givenunder the current access control context. This method should normally be invoked within an action to create callables that will, if possible, execute under the selected permission settings holding within that action; or if not possible, throw an associated - the type of the callable's result\n• under the current access control context, with the current context class loader as the context class loader. This method should normally be invoked within an Returns a object that will, when called, execute the givenunder the current access control context, with the current context class loader as the context class loader. This method should normally be invoked within an action to create callables that will, if possible, execute under the selected permission settings holding within that action; or if not possible, throw an associated - the type of the callable's result - if the current access control context does not have permission to both set and get context class loader"
    },
    {
        "link": "https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/Executors.html",
        "document": "\n• threads will be active processing tasks. If additional tasks are submitted when all threads are active, they will wait in the queue until a thread is available. If any thread terminates due to a failure during execution prior to shutdown, a new one will take its place if needed to execute subsequent tasks. The threads in the pool will exist until it is explicitly Creates a thread pool that reuses a fixed number of threads operating off a shared unbounded queue. At any point, at mostthreads will be active processing tasks. If additional tasks are submitted when all threads are active, they will wait in the queue until a thread is available. If any thread terminates due to a failure during execution prior to shutdown, a new one will take its place if needed to execute subsequent tasks. The threads in the pool will exist until it is explicitly - the number of threads in the pool\n• threads will be active processing tasks. If additional tasks are submitted when all threads are active, they will wait in the queue until a thread is available. If any thread terminates due to a failure during execution prior to shutdown, a new one will take its place if needed to execute subsequent tasks. The threads in the pool will exist until it is explicitly Creates a thread pool that reuses a fixed number of threads operating off a shared unbounded queue, using the provided ThreadFactory to create new threads when needed. At any point, at mostthreads will be active processing tasks. If additional tasks are submitted when all threads are active, they will wait in the queue until a thread is available. If any thread terminates due to a failure during execution prior to shutdown, a new one will take its place if needed to execute subsequent tasks. The threads in the pool will exist until it is explicitly - the number of threads in the pool - the factory to use when creating new threads\n• Creates an Executor that uses a single worker thread operating off an unbounded queue. (Note however that if this single thread terminates due to a failure during execution prior to shutdown, a new one will take its place if needed to execute subsequent tasks.) Tasks are guaranteed to execute sequentially, and no more than one task will be active at any given time. Unlike the otherwise equivalent the returned executor is guaranteed not to be reconfigurable to use additional threads.\n• Creates an Executor that uses a single worker thread operating off an unbounded queue, and uses the provided ThreadFactory to create a new thread when needed. Unlike the otherwise equivalent the returned executor is guaranteed not to be reconfigurable to use additional threads. - the factory to use when creating new threads\n• will reuse previously constructed threads if available. If no existing thread is available, a new thread will be created and added to the pool. Threads that have not been used for sixty seconds are terminated and removed from the cache. Thus, a pool that remains idle for long enough will not consume any resources. Note that pools with similar properties but different details (for example, timeout parameters) may be created using Creates a thread pool that creates new threads as needed, but will reuse previously constructed threads when they are available. These pools will typically improve the performance of programs that execute many short-lived asynchronous tasks. Calls towill reuse previously constructed threads if available. If no existing thread is available, a new thread will be created and added to the pool. Threads that have not been used for sixty seconds are terminated and removed from the cache. Thus, a pool that remains idle for long enough will not consume any resources. Note that pools with similar properties but different details (for example, timeout parameters) may be created using constructors.\n• Creates a thread pool that creates new threads as needed, but will reuse previously constructed threads when they are available, and uses the provided ThreadFactory to create new threads when needed. - the factory to use when creating new threads\n• Creates a single-threaded executor that can schedule commands to run after a given delay, or to execute periodically. (Note however that if this single thread terminates due to a failure during execution prior to shutdown, a new one will take its place if needed to execute subsequent tasks.) Tasks are guaranteed to execute sequentially, and no more than one task will be active at any given time. Unlike the otherwise equivalent the returned executor is guaranteed not to be reconfigurable to use additional threads.\n• Creates a single-threaded executor that can schedule commands to run after a given delay, or to execute periodically. (Note however that if this single thread terminates due to a failure during execution prior to shutdown, a new one will take its place if needed to execute subsequent tasks.) Tasks are guaranteed to execute sequentially, and no more than one task will be active at any given time. Unlike the otherwise equivalent the returned executor is guaranteed not to be reconfigurable to use additional threads. - the factory to use when creating new threads\n• Creates a thread pool that can schedule commands to run after a given delay, or to execute periodically. - the number of threads to keep in the pool, even if they are idle.\n• Creates a thread pool that can schedule commands to run after a given delay, or to execute periodically. - the number of threads to keep in the pool, even if they are idle. - the factory to use when the executor creates a new thread.\n• Returns an object that delegates all defined methods to the given executor, but not any other methods that might otherwise be accessible using casts. This provides a way to safely \"freeze\" configuration and disallow tuning of a given concrete implementation.\n• Returns an object that delegates all defined methods to the given executor, but not any other methods that might otherwise be accessible using casts. This provides a way to safely \"freeze\" configuration and disallow tuning of a given concrete implementation.\n• method. Each new thread is created as a non-daemon thread with priority set to the smaller of and the maximum priority permitted in the thread group. New threads have names accessible via Returns a default thread factory used to create new threads. This factory creates all new threads used by an Executor in the same . If there is a , it uses the group of , else the group of the thread invoking thismethod. Each new thread is created as a non-daemon thread with priority set to the smaller ofand the maximum priority permitted in the thread group. New threads have names accessible via of pool-N-thread-M, where N is the sequence number of this factory, and M is the sequence number of the thread created by this factory.\n• method. A new can be created within an Returns a thread factory used to create new threads that have the same permissions as the current thread. This factory creates threads with the same settings as , additionally setting the AccessControlContext and contextClassLoader of new threads to be the same as the thread invoking thismethod. A newcan be created within an action setting the current thread's access control context to create threads with the selected permission settings holding within that action. Note that while tasks running within such threads will have the same access control and class loader settings as the current thread, they need not have the same or values. If necessary, particular values of thread locals can be set or reset before any task runs in subclasses using . Also, if it is necessary to initialize worker threads to have the same InheritableThreadLocal settings as some other designated thread, you can create a custom ThreadFactory in which that thread waits for and services requests to create others that will inherit its values. - if the current access control context does not have permission to both get and set context class loader.\n• to an otherwise resultless action. Returns a object that, when called, runs the given task and returns the given result. This can be useful when applying methods requiring ato an otherwise resultless action.\n• . Returns a object that, when called, runs the given task and returns\n• Returns a object that, when called, runs the given privileged action and returns its result.\n• Returns a object that, when called, runs the given privileged exception action and returns its result.\n• under the current access control context. This method should normally be invoked within an Returns a object that will, when called, execute the givenunder the current access control context. This method should normally be invoked within an action to create callables that will, if possible, execute under the selected permission settings holding within that action; or if not possible, throw an associated\n• under the current access control context, with the current context class loader as the context class loader. This method should normally be invoked within an Returns a object that will, when called, execute the givenunder the current access control context, with the current context class loader as the context class loader. This method should normally be invoked within an action to create callables that will, if possible, execute under the selected permission settings holding within that action; or if not possible, throw an associated - if the current access control context does not have permission to both set and get context class loader."
    },
    {
        "link": "https://baeldung.com/java-executor-service-tutorial",
        "document": "ExecutorService is a JDK API that simplifies running tasks in asynchronous mode. Generally speaking, ExecutorService automatically provides a pool of threads and an API for assigning tasks to it.\n\n Guide to the Fork/Join Framework in Java An intro to the fork/join framework presented in Java 7 and the tools to help speed up parallel processing by attempting to use all available processor cores. Discover the content of the java.util.concurrent package. In this article, we explore various implementations of the Lock interface and the newly introduced in Java 9 StampedLock class.\n\nThe easiest way to create ExecutorService is to use one of the factory methods of the Executors class.\n\nFor example, the following line of code will create a thread pool with 10 threads:\n\nThere are several other factory methods to create a predefined ExecutorService that meets specific use cases. To find the best method for your needs, consult Oracle’s official documentation.\n\nBecause ExecutorService is an interface, an instance of any its implementations can be used. There are several implementations to choose from in the java.util.concurrent package, or you can create your own.\n\nFor example, the ThreadPoolExecutor class has a few constructors that we can use to configure an executor service and its internal pool:\n\nYou may notice that the code above is very similar to the source code of the factory method newSingleThreadExecutor(). For most cases, a detailed manual configuration isn’t necessary.\n\nExecutorService can execute Runnable and Callable tasks. To keep things simple in this article, two primitive tasks will be used. Notice that we use lambda expressions here instead of anonymous inner classes:\n\nWe can assign tasks to the ExecutorService using several methods including execute(), which is inherited from the Executor interface, and also submit(), invokeAny() and invokeAll().\n\nThe execute() method is void and doesn’t give any possibility to get the result of a task’s execution or to check the task’s status (is it running):\n\nsubmit() submits a Callable or a Runnable task to an ExecutorService and returns a result of type Future:\n\ninvokeAny() assigns a collection of tasks to an ExecutorService, causing each to run, and returns the result of a successful execution of one task (if there was a successful execution):\n\ninvokeAll() assigns a collection of tasks to an ExecutorService, causing each to run, and returns the result of all task executions in the form of a list of objects of type Future:\n\nBefore going further, we need to discuss two more items: shutting down an ExecutorService and dealing with Future return types.\n\nIn general, the ExecutorService will not be automatically destroyed when there is no task to process. It will stay alive and wait for new work to do.\n\nIn some cases this is very helpful, such as when an app needs to process tasks that appear on an irregular basis or the task quantity is not known at compile time.\n\nOn the other hand, an app could reach its end but not be stopped because a waiting ExecutorService will cause the JVM to keep running.\n\nTo properly shut down an ExecutorService, we have the shutdown() and shutdownNow() APIs.\n\nThe shutdown() method doesn’t cause immediate destruction of the ExecutorService. It will make the ExecutorService stop accepting new tasks and shut down after all running threads finish their current work:\n\nThe shutdownNow() method tries to destroy the ExecutorService immediately, but it doesn’t guarantee that all the running threads will be stopped at the same time:\n\nThis method returns a list of tasks that are waiting to be processed. It is up to the developer to decide what to do with these tasks.\n\nOne good way to shut down the ExecutorService (which is also recommended by Oracle) is to use both of these methods combined with the awaitTermination() method:\n\nWith this approach, the ExecutorService will first stop taking new tasks and then wait up to a specified period of time for all tasks to be completed. If that time expires, the execution is stopped immediately.\n\nThe submit() and invokeAll() methods return an object or a collection of objects of type Future, which allows us to get the result of a task’s execution or to check the task’s status (is it running).\n\nThe Future interface provides a special blocking method get(), which returns an actual result of the Callable task’s execution or null in the case of a Runnable task:\n\nCalling the get() method while the task is still running will cause execution to block until the task properly executes and the result is available.\n\nWith very long blocking caused by the get() method, an application’s performance can degrade. If the resulting data is not crucial, it is possible to avoid such a problem by using timeouts:\n\nIf the execution period is longer than specified (in this case, 200 milliseconds), a TimeoutException will be thrown.\n\nWe can use the isDone() method to check if the assigned task already processed or not.\n\nThe Future interface also provides for canceling task execution with the cancel() method and checking the cancellation with the isCancelled() method:\n\nThe ScheduledExecutorService runs tasks after some predefined delay and/or periodically.\n\nOnce again, the best way to instantiate a ScheduledExecutorService is to use the factory methods of the Executors class.\n\nFor this section, we use a ScheduledExecutorService with one thread:\n\nTo schedule a single task’s execution after a fixed delay, use the scheduled() method of the ScheduledExecutorService.\n\nTwo scheduled() methods allow you to execute Runnable or Callable tasks:\n\nThe scheduleAtFixedRate() method lets us run a task periodically after a fixed delay. The code above delays for one second before executing callableTask.\n\nThe following block of code will run a task after an initial delay of 100 milliseconds. And after that, it will run the same task every 450 milliseconds:\n\nIf the processor needs more time to run an assigned task than the period parameter of the scheduleAtFixedRate() method, the ScheduledExecutorService will wait until the current task is completed before starting the next.\n\nIf it is necessary to have a fixed length delay between iterations of the task, scheduleWithFixedDelay() should be used.\n\nFor example, the following code will guarantee a 150-millisecond pause between the end of the current execution and the start of another one:\n\nAccording to the scheduleAtFixedRate() and scheduleWithFixedDelay() method contracts, period execution of the task will end at the termination of the ExecutorService or if an exception is thrown during task execution.\n\nAfter the release of Java 7, many developers decided to replace the ExecutorService framework with the fork/join framework.\n\nThis is not always the right decision, however. Despite the simplicity and frequent performance gains associated with fork/join, it reduces developer control over concurrent execution.\n\nExecutorService gives the developer the ability to control the number of generated threads and the granularity of tasks that should be run by separate threads. The best use case for ExecutorService is the processing of independent tasks, such as transactions or requests according to the scheme “one thread for one task.”\n\nIn contrast, according to Oracle’s documentation, fork/join was designed to speed up work that can be broken into smaller pieces recursively.\n\nDespite the relative simplicity of ExecutorService, there are a few common pitfalls.\n\nKeeping an unused ExecutorService alive: See the detailed explanation in Section 4 on how to shut down an ExecutorService.\n\nWrong thread-pool capacity while using fixed length thread pool: It is very important to determine how many threads the application will need to run tasks efficiently. A too-large thread pool will cause unnecessary overhead just to create threads that will mostly be in the waiting mode. Too few can make an application seem unresponsive because of long waiting periods for tasks in the queue.\n\nCalling a Future‘s get() method after task cancellation: Attempting to get the result of an already canceled task triggers a CancellationException.\n\nUnexpectedly long blocking with Future‘s get() method: We should use timeouts to avoid unexpected waits."
    },
    {
        "link": "https://stackoverflow.com/questions/36328492/executorservice-singlethreadexecutor",
        "document": "I have a list of objects, from which depending on user interaction some objects need to do work asynchronically. Something like this:\n\nThe class implements an (SingleThread!), which is used to do the work. Every object of type TheObject instantiates an . I don't want to make lasagna code. I don't have enough Objects at the same time, to make an extra extraction layer with thread pooling needed.\n\nI want to cite the Java Documentation about CachedThreadPools:\n\nThreads that have not been used for sixty seconds are terminated and removed from the cache. Thus, a pool that remains idle for long enough will not consume any resources.\n\nFirst question: Is this also true for a SingleThreadExecutor? Does the thread get terminated? JavaDoc doesn't say anything about SingleThreadExecutor. It wouldn't even matter in this application, as I have an amount of objects I can count on one hand. Just curiosity.\n\nFurthermore the method of needs to call the method to do the work async. Is it possible (I bet it is) to call the method implicitly? Is this a viable way of designing an async method?"
    },
    {
        "link": "https://geeksforgeeks.org/what-is-java-executor-framework",
        "document": "With the increase in the number of cores available in the processors nowadays, coupled with the ever-increasing need to achieve more throughput, multi-threading APIs are getting quite popular. Java provides its own multi-threading framework called the Java Executor Framework.\n\nJava executor framework (java.util.concurrent.Executor), released with the JDK 5 is used to run the Runnable objects without creating new threads every time and mostly re-using the already created threads. We all know that there are two ways to create a thread in Java. If you want to read more about their comparison, read how to create threads in Java.\n\nThe java.util.concurrent.Executors provide factory methods that are being used to create ThreadPools of worker threads. Thread pools overcome this issue by keeping the threads alive and reusing the threads. Any excess tasks flowing in, that the threads in the pool can’t handle are held in a Queue. Once any of the threads get free, they pick up the next task from this queue. This task queue is essentially unbounded for the out-of-box executors provided by the JDK.\n\nSome types of Java Executors are listed below:\n\nLet us discuss these popular java executors to some details what exactly they do to get a better idea prior to implementing the same.\n\nA thread pool of single thread can be obtained by calling the static newSingleThreadExecutor() method of the Executors class. It is used to execute tasks sequentially.\n\nAs the name indicates, it is a thread pool of a fixed number of threads. The tasks submitted to the executor are executed by the n threads and if there is more task they are stored on a LinkedBlockingQueue. It uses Blocking Queue.\n\nCreates a thread pool that creates new threads as needed, but will reuse previously constructed threads when they are available. Calls to execute will reuse previously constructed threads if available. If no existing thread is available, a new thread will be created and added to the pool. It uses a SynchronousQueue queue.\n\nScheduled executors are based on the interface ScheduledExecutorService which extends the ExecutorService interface. This executor is used when we have a task that needs to be run at regular intervals or if we wish to delay a certain task.\n• None The tasks can be scheduled using either of the two methods:\n• scheduleAtFixedRate : Executes the task with a fixed interval, irrespective of when the previous task ended.\n• scheduleWithFixedDelay : This will start the delay countdown only after the current task completes.`\n\nThe result of the task submitted for execution to an executor can be accessed using the java.util.concurrent.The future object returned by the executor. Future can be thought of as a promise made to the caller by the executor. The future interface is mainly used to get the results of Callable results. whenever the task execution is completed, it is set in this Future object by the executor.\n\nImplementation: Creating and Executing a Simple Executor in which we will create a task and execute it in a fixed pool\n• None The Task class implements Callable and is parameterized to String type. It is also declared to throw Exception.\n• None Now in order to execute task in class “Task” we have to instantiate the Task class and are passing it to the executor for execution.\n• None Print and display the result that is returned by the Future object"
    },
    {
        "link": "https://medium.com/@alxkm/concurrenthashmap-for-caching-05c48d1d6bf0",
        "document": "In a previous article, we explored the fundamentals of ConcurrentHashMap, a thread-safe implementation of the Map interface designed to handle high concurrency levels. ConcurrentHashMap provides efficient performance in multi-threaded environments by allowing concurrent access and updates without requiring explicit synchronization. This capability makes it a valuable tool for various applications where thread safety and performance are critical.\n\nOne of the most compelling use cases for ConcurrentHashMap is implementing caches. Caches are essential components in many systems, serving to store frequently accessed data in memory for quick retrieval, thus improving performance and reducing the load on underlying data sources. In this article, we will delve deeper into the use of ConcurrentHashMap for caching, examining different caching strategies and their implementation.\n\nWhy Use ConcurrentHashMap for Caching?\n\nCaching is a technique used to store data temporarily to reduce the time required to access that data in the future. In a multi-threaded application, implementing a cache that can be safely accessed and modified by multiple threads simultaneously is challenging. Traditional synchronization mechanisms can lead to performance bottlenecks due to contention among threads. ConcurrentHashMap addresses these challenges by providing:\n• Performance: Optimized for high concurrency, reducing contention through internal mechanisms like lock striping.\n\nBefore we dive into specific examples, let’s highlight some key features of ConcurrentHashMap that make it suitable for caching:\n• Atomic Operations: Methods like putIfAbsent, remove, replace, and computeIfAbsent allow atomic updates to the map, ensuring consistency without explicit locks.\n• Concurrency Level: Although the concurrencyLevel parameter is deprecated in Java 8 and beyond, the internal design still allows efficient concurrent updates.\n• Scalability: Designed to scale with the number of threads, making it ideal for high-concurrency environments.\n\nA basic in-memory cache stores key-value pairs and provides fast access to the data. This is suitable for scenarios where the data is frequently read and less frequently updated.\n\nIn many applications, it is important to ensure that cached data is not stale. An expiring cache automatically removes entries after a specified period.\n\nA loading cache automatically retrieves and stores values when they are missing. This is particularly useful for read-heavy applications where missing data can be fetched from a database or another data source.\n\nUsing ConcurrentHashMap for caching provides a powerful and efficient solution for managing concurrent access to cached data. Whether implementing a basic in-memory cache, an expiring cache, or a loading cache, ConcurrentHashMap offers the necessary thread-safety and performance characteristics. By leveraging its atomic operations and non-blocking reads, developers can build scalable and responsive applications that handle concurrent data access with ease."
    },
    {
        "link": "https://baeldung.com/java-concurrent-map",
        "document": "Maps are naturally one of the most widely style of Java collection.\n\nAnd, importantly, HashMap is not a thread-safe implementation, while Hashtable does provide thread-safety by synchronizing operations.\n\nEven though Hashtable is thread safe, it is not very efficient. Another fully synchronized Map, Collections.synchronizedMap, does not exhibit great efficiency either. If we want thread-safety with high throughput under high concurrency, these implementations aren’t the way to go.\n\nTo solve the problem, the Java Collections Framework introduced ConcurrentMap in Java 1.5.\n\nThe following discussions are based on Java 1.8.\n\nConcurrentMap is an extension of the Map interface. It aims to provides a structure and guidance to solving the problem of reconciling throughput with thread-safety.\n\nBy overriding several interface default methods, ConcurrentMap gives guidelines for valid implementations to provide thread-safety and memory-consistent atomic operations.\n\nSeveral default implementations are overridden, disabling the null key/value support:\n\nThe following APIs are also overridden to support atomicity, without a default interface implementation:\n\nThe rest of actions are directly inherited with basically consistent with Map.\n\nFor better performance, it consists of an array of nodes as table buckets (used to be table segments prior to Java 8) under the hood, and mainly uses CAS operations during updating.\n\nThe table buckets are initialized lazily, upon the first insertion. Each bucket can be independently locked by locking the very first node in the bucket. Read operations do not block, and update contentions are minimized.\n\nThe number of segments required is relative to the number of threads accessing the table so that the update in progress per segment would be no more than one most of time.\n\nBefore Java 8, the number of “segments” required was relative to the number of threads accessing the table so that the update in progress per segment would be no more than one most of time.\n\nThat’s why constructors, compared to HashMap, provides the extra concurrencyLevel argument to control the number of estimated threads to use:\n\nThe other two arguments: initialCapacity and loadFactor worked quite the same as HashMap.\n\nHowever, since Java 8, the constructors are only present for backward compatibility: the parameters can only affect the initial size of the map.\n\nActions in a thread prior to placing an object into a ConcurrentMap as a key or value happen-before actions subsequent to the access or removal of that object in another thread.\n\nTo confirm, let’s have a look at a memory inconsistent case:\n\nFor each map.computeIfPresent action in parallel, HashMap does not provide a consistent view of what should be the present integer value, leading to inconsistent and undesirable results.\n\nAs for ConcurrentHashMap, we can get a consistent and correct result:\n\nMost APIs provided by ConcurrentMap does not allow null key or value, for example:\n\nHowever, for compute* and merge actions, the computed value can be null, which indicates the key-value mapping is removed if present or remains absent if previously absent.\n\nJava 8 provides Stream support in the ConcurrentHashMap as well.\n\nUnlike most stream methods, the bulk (sequential and parallel) operations allow concurrent modification safely. ConcurrentModificationException won’t be thrown, which also applies to its iterators. Relevant to streams, several forEach*, search, and reduce* methods are also added to support richer traversal and map-reduce operations.\n\nUnder the hood, ConcurrentHashMap is somewhat similar to HashMap, with data access and update based on a hash table (though more complex).\n\nAnd of course, the ConcurrentHashMap should yield much better performance in most concurrent cases for data retrieval and update.\n\nLet’s write a quick micro-benchmark for get and put performance and compare that to Hashtable and Collections.synchronizedMap, running both operations for 500,000 times in 4 threads.\n\nKeep in mind micro-benchmarks are only looking at a single scenario and aren’t always a good reflection of real world performance.\n\nThat being said, on an OS X system with an average dev system, we’re seeing an average sample result for 100 consecutive runs (in nanoseconds):\n\nIn a multi-threading environment, where multiple threads are expected to access a common Map, the ConcurrentHashMap is clearly preferable.\n\nHowever, when the Map is only accessible to a single thread, HashMap can be a better choice for its simplicity and solid performance.\n\nRetrieval operations generally do not block in ConcurrentHashMap and could overlap with update operations. So for better performance, they only reflect the results of the most recently completed update operations, as stated in the official Javadoc.\n\nThere are several other facts to bear in mind:\n• results of aggregate status methods including size, isEmpty, and containsValue are typically useful only when a map is not undergoing concurrent updates in other threads:\n\nIf concurrent updates are under strict control, aggregate status would still be reliable.\n\nAlthough these aggregate status methods do not guarantee the real-time accuracy, they may be adequate for monitoring or estimation purposes.\n\nNote that usage of size() of ConcurrentHashMap should be replaced by mappingCount(), for the latter method returns a long count, although deep down they are based on the same estimation.\n• hashCode matters: note that using many keys with exactly the same hashCode() is a sure way to slow down a performance of any hash table.\n\nTo ameliorate impact when keys are Comparable, ConcurrentHashMap may use comparison order among keys to help break ties. Still, we should avoid using the same hashCode() as much as we can.\n• iterators are only designed to use in a single thread as they provide weak consistency rather than fast-fail traversal, and they will never throw ConcurrentModificationException.\n• the default initial table capacity is 16, and it’s adjusted by the specified concurrency level:\n• caution on remapping functions: though we can do remapping operations with provided compute and merge* methods, we should keep them fast, short and simple, and focus on the current mapping to avoid unexpected blocking.\n• keys in ConcurrentHashMap are not in sorted order, so for cases when ordering is required, ConcurrentSkipListMap is a suitable choice.\n\nFor cases when ordering of keys is required, we can use ConcurrentSkipListMap, a concurrent version of TreeMap.\n\nAs a supplement for ConcurrentMap, ConcurrentNavigableMap supports total ordering of its keys (in ascending order by default) and is concurrently navigable. Methods that return views of the map are overridden for concurrency compatibility:\n\nkeySet() views’ iterators and spliterators are enhanced with weak-memory-consistency:\n\nPreviously, we have covered NavigableMap interface and its implementation TreeMap. ConcurrentSkipListMap can be seen a scalable concurrent version of TreeMap.\n\nIn practice, there’s no concurrent implementation of the red-black tree in Java. A concurrent variant of SkipLists is implemented in ConcurrentSkipListMap, providing an expected average log(n) time cost for the containsKey, get, put and remove operations and their variants.\n\nIn addition to TreeMap‘s features, key insertion, removal, update and access operations are guaranteed with thread-safety. Here’s a comparison to TreeMap when navigating concurrently:\n\nA full explanation of the performance concerns behind the scenes is beyond the scope of this article. The details can be found in ConcurrentSkipListMap’s Javadoc, which is located under java/util/concurrent in the src.zip file.\n\nIn this article, we mainly introduced the ConcurrentMap interface and the features of ConcurrentHashMap and covered on ConcurrentNavigableMap being key-ordering required."
    },
    {
        "link": "https://stackoverflow.com/questions/1815646/how-to-implement-concurrenthashmap-with-features-similar-in-linkedhashmap",
        "document": "I did something similar recently with , where CacheEntry wraps the actual item and adds cache eviction statistics: expiration time, insertion time (for FIFO/LIFO eviction), last used time (for LRU/MRU eviction), number of hits (for LFU/MFU eviction), etc. The actual eviction is synchronized and creates an and does a Collections.sort() on it using the appropriate Comparator for the eviction strategy. Since this is expensive, each eviction then lops off the bottom 5% of the CacheEntries. I'm sure performance tuning would help though.\n\nIn your case, since you're doing FIFO, you could keep a separate ConcurrentLinkedQueue. When you add an object to the ConcurrentHashMap, do a ConcurrentLinkedQueue.add() of that object. When you want to evict an entry, do a ConcurrentLinkedQueue.poll() to remove the oldest object, then remove it from the ConcurrentHashMap as well.\n\nUpdate: Other possibilities in this area include a Java Collections synchronization wrapper and the Java 1.6 ConcurrentSkipListMap."
    },
    {
        "link": "https://medium.com/@alxkm/concurrency-in-java-best-practices-and-performance-optimization-0dfd990f413b",
        "document": "Welcome to an exploration of best practices and essential patterns in concurrent programming, where we delve into optimizing performance, navigating through the intricate challenges of handling exceptions and error conditions, and uncovering key strategies for writing robust and efficient concurrent code. Join us as we navigate the complex terrain of concurrent programming in Java and unlock the secrets to building scalable, responsive, and reliable multi-threaded applications.\n\nAlso maybe you are interested in multithreading in Java, here useful articles list:\n\nBut in this article we will cover next topics:\n• Best practices and common patterns for concurrent programming\n\nBest practices and common patterns for concurrent programming\n\nThe motivation for utilizing best practices and common patterns in concurrent programming lies in the pursuit of building reliable, scalable, and efficient software systems. By adhering to established guidelines and patterns, developers can:\n• Enhance Reliability: Implementing best practices ensures that concurrent code behaves predictably and consistently under varying conditions, reducing the likelihood of race conditions, deadlocks, and other concurrency-related bugs.\n• Improve Maintainability: By following common patterns and design principles, code becomes more structured and easier to understand, facilitating maintenance and future enhancements.\n• Enhance Performance: Optimized concurrency patterns and techniques can lead to improved performance and scalability, enabling applications to handle increasing workloads and concurrent user interactions more effectively.\n• Mitigate Risks: By leveraging proven patterns and practices, developers can mitigate the risks associated with concurrent programming, such as data corruption, resource contention, and synchronization overhead.\n• Foster Collaboration: Adopting common patterns fosters collaboration and knowledge sharing among developers, as it provides a common vocabulary and framework for discussing and reasoning about concurrent programming challenges. Overall, embracing best practices and common patterns in concurrent programming empowers developers to build robust, efficient, and maintainable software systems that meet the demands of modern multi-threaded applications.\n• Limit the use of shared mutable state between threads, as it can lead to race conditions, data corruption, and synchronization overhead.\n• Favor immutability and thread confinement wherever possible to ensure thread safety and simplify reasoning about concurrent code.\n• Prefer high-level concurrency abstractions provided by the java.util.concurrent package, such as Executors, ThreadPoolExecutor, and Concurrent collections, over low-level synchronization primitives.\n• These abstractions offer thread-safe data structures, thread pools, and synchronization mechanisms that simplify concurrent programming and reduce the likelihood of errors.\n• Synchronize access to shared resources using locks, conditions, and semaphores to prevent data races and ensure mutual exclusion.\n• Use fine-grained locking strategies to minimize contention and maximize concurrency, and prefer read-write locks for scenarios with more reads than writes.\n• Utilize common concurrency design patterns such as Producer-Consumer, Reader-Writer, and Thread Pool patterns to address recurring concurrency challenges in a structured and reusable manner.\n• These patterns provide well-established solutions for coordinating concurrent tasks, managing shared resources, and ensuring thread safety.\n• Implement error handling mechanisms to gracefully handle exceptions and error conditions in concurrent code, preventing thread termination and ensuring the stability of the application.\n• Use try-catch blocks, exception propagation, and logging mechanisms to capture and report errors effectively, ensuring visibility and traceability of exceptions in multi-threaded environments.\n• Thoroughly test concurrent code using stress testing, unit testing, and integration testing to uncover race conditions, deadlocks, and other concurrency bugs.\n• Employ debugging tools such as thread dumps, stack traces, and profiling tools to analyze and diagnose concurrency issues, identifying bottlenecks and hotspots in multi-threaded applications.\n• Document concurrency-related assumptions, invariants, and synchronization strategies in code comments and documentation to facilitate understanding and maintainability.\n• Conduct code reviews focusing on concurrency aspects to identify potential pitfalls, review synchronization strategies, and ensure adherence to best practices and coding standards.\n\nBy adhering to these best practices and adopting common patterns, developers can navigate the complexities of concurrent programming with confidence, building robust, scalable, and responsive multi-threaded applications in Java.\n\nUnderstanding performance considerations and implementing optimizations is crucial for several reasons:\n• User Experience: Improved performance leads to a better user experience by reducing latency, increasing responsiveness, and enhancing overall system throughput. This results in happier users who are more likely to engage with and return to the application.\n• Scalability: Optimized code scales more efficiently, allowing applications to handle larger workloads and accommodate growing user bases without sacrificing performance. This is essential for meeting the demands of expanding user populations and increasing data volumes.\n• Resource Efficiency: Optimizations help conserve system resources such as CPU cycles, memory, and network bandwidth, leading to more efficient resource utilization and reduced operational costs. This is particularly important in cloud-based environments where resource consumption directly impacts costs.\n• Competitive Advantage: Applications with superior performance often gain a competitive edge in the market, attracting more users and retaining existing ones. Faster response times and smoother user interactions can differentiate a product from its competitors and contribute to its success.\n• Reliability: Performance optimizations can also improve system reliability by reducing the likelihood of bottlenecks, resource exhaustion, and performance degradation under heavy loads. This leads to more stable and resilient applications that are less prone to failures and downtime.\n• Customer Satisfaction: Ultimately, performance directly impacts customer satisfaction and loyalty. Users expect fast, responsive, and reliable software experiences, and meeting or exceeding these expectations can enhance brand reputation and customer loyalty.\n\nPrioritizing performance considerations and implementing optimizations is essential for delivering high-quality software that meets user expectations, scales effectively, conserves resources, maintains competitiveness, ensures reliability, and ultimately, maximizes customer satisfaction.\n• Reduce lock contention by employing fine-grained locking strategies, where locks are acquired for smaller, more localized sections of code.\n• Consider using lock-free data structures or optimistic locking techniques to eliminate contention and maximize concurrency in highly contended scenarios.\n• Optimize throughput by batching and chunking operations, where multiple tasks are processed together in larger units to reduce overhead and maximize resource utilization.\n• Use batching techniques in concurrent data processing pipelines, I/O operations, and network communications to amortize costs and improve performance.\n• Employ lazy initialization and memoization techniques to defer expensive computations until they are actually needed and cache the results for subsequent use.\n• Lazy initialization reduces startup time and memory consumption, while memoization avoids redundant computations and improves overall efficiency.\n• Choose data structures and algorithms optimized for concurrent access and performance, such as ConcurrentHashMap for concurrent hash maps and ConcurrentSkipListMap for concurrent skip lists.\n\nExample of using ConcurrentHashMap:\n• Leverage parallel algorithms and stream processing frameworks introduced in Java 8 and later versions to harness multi-core processors and accelerate computation.\n• Utilize asynchronous and non-blocking I/O mechanisms, such as java.nio package and CompletableFuture, to maximize I/O throughput and responsiveness.\n• Asynchronous I/O allows multiple I/O operations to proceed concurrently without blocking, enabling efficient resource utilization and improved system responsiveness.\n\nExample of using Asynchronous File I/O using java.nio:\n\nExample of using Asynchronous HTTP Client using CompletableFuture:\n\nThese examples demonstrate how to perform asynchronous and non-blocking I/O operations in Java using java.nio package for file I/O and CompletableFuture for HTTP client requests. These techniques allow multiple I/O operations to proceed concurrently without blocking, maximizing I/O throughput and system responsiveness.\n• Identify opportunities for parallelism and parallelize computationally intensive tasks across multiple threads or cores to leverage the full processing power of modern hardware.\n• Use ForkJoinPool and parallel streams for parallel computation of CPU-bound tasks, and leverage parallelism in data processing pipelines and batch processing workflows.\n\nHere’s an example demonstrating the use of ForkJoinPool and parallel streams for parallel computation:\n\nIn this example:\n\nSumTask is a RecursiveTask that recursively computes the sum of an array using ForkJoinPool.\n\nThe main method initializes an array and demonstrates parallel computation using both ForkJoinPool and parallel streams. The array is filled with 1s, and the sum of its elements is computed in parallel.\n• Efficiently manage resources and minimize overhead by implementing resource pooling mechanisms for frequently used resources such as database connections, threads, and file handles.\n• Use connection pooling libraries, thread pools, and object pools to reuse resources and amortize creation costs, reducing latency and improving overall performance.\n\nBy incorporating these performance considerations and optimizations into Java applications, developers can achieve significant improvements in throughput, latency, and scalability, enabling their software systems to deliver better performance and responsiveness under varying workloads and concurrency levels.\n• Concurrent code introduces additional complexities when handling exceptions due to the presence of multiple threads executing simultaneously.\n• It’s essential to ensure that exception handling mechanisms are thread-safe and can handle exceptions thrown concurrently by multiple threads.\n• Java provides the UncaughtExceptionHandler interface, which allows you to define custom handlers for uncaught exceptions that occur within threads.\n• You can set an uncaught exception handler for individual threads using the setUncaughtExceptionHandler() method or globally for all threads using Thread.setDefaultUncaughtExceptionHandler().\n• Executors in java.util.concurrent package provide various methods for executing tasks asynchronously, such as execute(), submit(), and invokeAll().\n• These methods wrap the submitted tasks in Runnable or Callable instances and execute them in separate threads. It’s essential to handle exceptions thrown by these tasks properly.\n• When dealing with chained tasks or dependencies between tasks, it’s crucial to propagate exceptions from one task to another properly.\n• Ensure that exceptions thrown by one task are caught and handled appropriately to prevent them from propagating unchecked through the application.\n• When shutting down concurrent components such as ExecutorService or ForkJoinPool, ensure that resources are cleaned up properly, and any pending tasks are completed or canceled.\n• Handle exceptions thrown during shutdown operations to prevent resource leaks and ensure the orderly termination of concurrent components.\n\nHandling exceptions and error conditions in concurrent code requires careful consideration and implementation to ensure the stability, reliability, and proper functioning of multi-threaded applications. By following best practices and using appropriate exception handling mechanisms, developers can effectively manage exceptions and errors in concurrent code, leading to more robust and resilient software systems.\n\nIn conclusion, mastering best practices and common patterns for concurrent programming is essential for developing robust and efficient multi-threaded applications. By understanding and applying these patterns, developers can ensure thread safety, minimize contention, and maximize concurrency, leading to better performance and scalability.\n\nFurthermore, optimizing performance in concurrent applications involves careful consideration of resource utilization, minimizing overhead, and leveraging parallelism effectively. By implementing performance considerations and optimizations, developers can enhance the responsiveness and efficiency of their applications, resulting in improved user experience and resource efficiency.\n\nAdditionally, handling exceptions and error conditions in concurrent code is crucial for maintaining the stability and reliability of multi-threaded applications. By implementing proper exception handling mechanisms and error recovery strategies, developers can mitigate the risks associated with concurrent programming, ensuring graceful degradation and error resilience.\n\nIn summary, by adhering to best practices, optimizing performance, and handling exceptions effectively, developers can build highly resilient, scalable, and performant concurrent applications that meet the demands of modern computing environments.\n\nYou can find some examples at Github."
    },
    {
        "link": "https://blog.stackademic.com/understanding-concurrenthashmap-in-java-0b3b80be9c51",
        "document": "is widely used for key-value storage, but it’s not thread-safe. In highly concurrent environments, this can lead to unpredictable behavior and data corruption.\n\nA common workaround is using , which wraps a in a synchronized map to make it thread-safe. However, this comes with a trade-off: the entire map is locked for any operation, creating a bottleneck in high-concurrency scenarios.\n\nThis is where shines."
    },
    {
        "link": "https://softwareengineering.stackexchange.com/questions/368480/best-practice-for-exception-handling-in-java-threads",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://javanexus.com/blog/mastering-exception-handling-java-multithreading",
        "document": "In the realm of programming, exceptions are an inevitable part of the development process. In Java, managing exceptions in a multithreading context introduces a layer of complexity that demands careful consideration. In this post, we'll explore best practices for handling exceptions in Java threads, dive into relevant code snippets, and offer overarching guidelines that will take your understanding to new heights.\n\nIn Java, an exception is an event that disrupts the normal flow of a program’s execution. Exceptions in Java are classified into two main categories:\n• Checked Exceptions: These are exceptions that must be either caught or declared in the method where they might occur. Examples include and .\n• Unchecked Exceptions: These are not required to be mentioned in a method's clause, and they usually signify programming errors. Examples include and .\n\nIt is important to understand these distinctions, as they guide how you will handle exceptions in a multithreaded environment.\n\nWhen multiple threads are in operation, the likelihood of encountering exceptions rises. Each thread operates independently and any uncaught exception in a thread can terminate that thread without affecting others. However, tracking these exceptions can become complex.\n• Stability: An unhandled exception can lead to application crashes, undermining stability.\n• User Experience: Proper exception handling allows for graceful degradation of service.\n\nBest Practices for Exception Handling in Multithreading\n\nEach thread should handle its exceptions internally. By doing so, you can ensure that a single thread's failure does not derail the entire application.\n\nIn the code snippet above, the class implements . The method contains a try-catch block that captures any exceptions occurring during execution. This approach ensures that exceptions are managed at the thread level.\n\nFor uncaught exceptions in a thread, Java provides the interface. This allows you to define what happens when a thread exits due to an uncaught exception.\n\nIn the above example, the class has a method that throws a . By using the , we can specify a custom handler to deal with this exception, improving the clarity of error handling.\n\nWhen using the , returning a can be a valuable technique for propagating exceptions that occur within a thread.\n\nIn this example, the class implements and throws an exception. By getting the result from the object, we can handle the exception thrown during execution.\n\nLogging exceptions is crucial for future analysis. Use logging frameworks like SLF4J or Log4j to effectively manage logging across your application.\n\nThe above code leverages SLF4J to log exceptions in a structured and efficient manner. This practice can vastly improve the maintainability of your application.\n\nBe mindful of how your threads respond to interruptions; interruptions are a way to signal that a thread should stop what it is doing.\n\nIn this example, the class simulates a long-running task. We handle by catching it and, if required, restoring the thread's interrupt status.\n\nMastering exception handling in Java multithreading is essential for building robust, maintainable applications. With well-structured code to handle exceptions, utilize logging, and communicate errors appropriately, you can provide a smooth user experience and maintain application stability.\n\nBy implementing these best practices, you’ll ensure that your multithreaded applications can gracefully handle disruptions while still delivering the intended functionality. Remember that thorough testing and logging will help you catch edge cases that you might not initially foresee.\n\nFor further reading, check out the official Java documentation on Exception Handling and the Java Concurrency Tutorials.\n\nWith a sound grasp of exception handling, your Java development skills will certainly flourish. Happy coding!"
    },
    {
        "link": "https://stackoverflow.com/questions/64414504/best-practice-solution-for-exception-handling-using-threads-notifying-parent-thr",
        "document": "I got a question regarding what is the best-practice solution for exception handling in threads: My task is to implement a simplified version of SMTP on a TransferServer which receives a message and distributes it to the different mail servers associated with the recipient's domain.\n\nSo first I got a shared where each client connection (in a separate thread) adds messages to send, and another ConsumerThread that works through the , looks up domains and distributes each message to the right IP address.\n\nIn this consumer thread I have an which I submit the tasks of sending a message to a specified mail server. The problem now is the following: When message delivery to a mail server somehow fails, I need to send back a message to the sender informing them that the delivery failed. Additionally it can also happen in the consumer thread that a domain lookup fails and no mail server is found.\n\nSo I got two possible error cases:\n\nThe first case happens right in the consumer thread and therefore it is very easy to just try to send another message back to the sender containing the error. The second case on the other hand is the one that keeps me stuck.\n\nWhat would be best-practice to implement this error message sending in a separate message sending thread/task?\n\nI thought of three different possible solutions:\n• Make my SenderTask implement Callable, not Runnable and return an error code, then periodically check for success (i somehow need to keep track of all Futures in this scenario) and if an error occurs, the parent ConsumerThread will initiate a new SenderTask which sends the error message back to the sender.\n• Pass ExecutorService to each SenderTask and let SenderTask submit a new error message SenderTask to the executor itself (feels weird to me)\n• Use an UncaughtExceptionHandler which handles the error message sending and create a ThreadFactory to use the handler with the executor\n\nWhich approach is the best suitable or is there even an easier way to achieve this?"
    },
    {
        "link": "https://moldstud.com/articles/p-java-best-practices-for-exception-handling-in-multithreaded-applications",
        "document": "Incorporate specific types of exceptions over generic ones; this increases clarity and reduces the ambiguity of error handling. For instance, use IllegalArgumentException for invalid method arguments and NullPointerException when encountering null references. A study showed that systems using precise exception types had a 30% reduction in debugging time.\n\nImplement a logging framework that can handle concurrent writes, such as SLF4J or Log4j. This approach enables better traceability of errors across multiple threads. Statistics indicate that projects utilizing robust logging mechanisms see a 40% improvement in issue resolution rates.\n\nUtilize thread-safe collections like ConcurrentHashMap to store and manage error contexts, ensuring that data integrity is maintained even under heavy load. This practice not only minimizes the risk of data corruption but also promotes better data retrieval during error analysis. Research has shown that systems employing such collections report a 25% decrease in concurrency-related failures.\n\nAvoid catching low-level exceptions in higher-level components; doing so can obscure the root cause of issues. Instead, aim for a layered approach where exceptions are handled as close to the source as possible. Studies confirm that maintaining this separation leads to a 50% reduction in time spent identifying the source of failures.\n\nFinally, establish a consistent error handling policy across your application. This not only aids in maintaining uniformity but also facilitates smoother onboarding processes for new developers. A cohesive approach has been linked to a 35% reduction in onboarding time, creating a significant impact on overall productivity.\n\nUtilize Thread.setUncaughtExceptionHandler to manage uncaught issues. This approach allows you to define a handler that captures exceptions thrown by worker threads, preventing abrupt termination of the application. By centralizing error handling, you can improve maintainability and logging consistency across different threads.\n\nAccording to industry data, around 50% of unhandled exceptions in concurrent applications lead to system crashes. Implementing a global handler can significantly reduce this percentage, providing a fallback mechanism that ensures critical operations are logged and monitored.\n\nCreating specific exception types for different tasks is advisable. This practice aids in the identification of problems related to specific threads, assisting in debugging and error tracing. Statistics show that this level of granularity can reduce resolution time by 30%.\n\nUtilize synchronization techniques wisely to manage shared resources. Issues such as deadlocks and race conditions can lead to erratic behaviors and unresponsive threads. It is reported that 20% of concurrency-related bugs stem from improper synchronization. Use locks such as ReentrantLock for advanced control over thread access to critical sections of code.\n\nLog exceptions with appropriate context, including thread ID and operation details. Comprehensive logging practices can boost troubleshooting efficiency by 60%. Robust logging frameworks allow capturing exhaustive information without hindering performance, thus enhancing post-mortem analyses.\n\nConduct regular reviews of exception handling code. A systematic evaluation strategy can uncover potential pitfalls in concurrency logic. Organizations that implement such reviews reduce the likelihood of runtime errors by 40%, according to a survey conducted among leading technology firms.\n\nLastly, implement timeout strategies for long-running tasks to avoid indefinite waiting periods. The use of timeout values not only enhances the responsiveness of the application but also prevents resource blocking, with studies showing that timeouts can improve system stability by approximately 25%.\n\nAddressing various scenarios requires understanding distinct categories of errors that can emerge during concurrent execution.\n\nChecked Exceptions: These are anomalies that are anticipated by the programmer, such as IOException from file operations or SQLException when querying databases. Proper handling mechanisms should be in place, as 40% of exceptions in concurrent systems fall into this category.\n\nUnchecked Exceptions: These represent flaws that typically indicate programming bugs, such as NullPointerException or ArrayIndexOutOfBoundsException. Approximately 60% of runtime errors arise from these, where prevention through thorough testing and vigilant code reviews is essential.\n\nRuntime Exceptions: A subtype of unchecked errors, these may emerge from unpredictable situations during execution. Implementing strategic checks can help mitigate risks associated with these faults. Studies show that about 30% of issues in multi-threaded systems relate to improperly handled runtime anomalies.\n\nError: This category encompasses severe issues that are usually not recoverable, such as OutOfMemoryError. While these cannot be handled effectively within the thread’s execution, logging is critical for post-analysis to avoid future occurrences.\n\nThread-related Exceptions: Special cases include exceptions particular to threading, like InterruptedException, which occurs when a thread is in a wait state and is interrupted. Identifying and handling these exceptions can prevent significant thread management challenges, as failure to manage interruptions leads to a staggering 25% increase in system downtime.\n\nUtilizing a structured approach to categorize and manage exceptions not only enhances reliability but also improves maintainability in systems that utilize concurrent execution. Regular analysis and documentation of instance-specific exceptions can foster a deeper understanding and more robust error handling strategies across the codebase.\n\nThe Role of Thread Interruption in Exception Handling\n\nUtilizing thread interruption as a strategy for managing errors significantly streamlines the control flow in concurrent programming. By invoking the method, it's possible to signal that a thread should cease its ongoing task. This mechanism enables graceful termination, particularly in scenarios where prolonged execution is undesirable.\n\nIncorporating interruption within error management can prevent resource leaks and ensure that locks are promptly released. According to studies, around 75% of systems that implement dedicated interruption handling demonstrate enhanced responsiveness under failure conditions. This occurs because threads become readily aware of interruption signals, allowing for an immediate alternative action or cleanup.\n\nThreads should continuously monitor their interrupted status using or . Adopting this practice permits immediate detection of an interruption signal, which can trigger specific logic to handle such cases. For instance, when a thread recognizes an interrupt status, it can perform necessary cleanup tasks, thus maintaining optimal application performance.\n\nIt's crucial to cause threads to check their interrupted state frequently during lengthy operations. Industry reports indicate that systems with regular checks can recover from faulty states up to 50% faster than those that ignore interruption signals. Contrarily, threads that neglect to act on interruptions risk running indefinitely, leading to unresponsiveness or deadlocks, impacting overall system reliability.\n\nWhile exceptions provide a route to capture runtime errors, interruptions serve as a proactive measure, especially relevant in multi-threaded constructs. An effective hybrid model that integrates both methods can yield robust control over concurrent processes, significantly reducing the likelihood of cascading failures.\n\nFocusing on properly handling interruptions demonstrates a commitment to system resilience. In practice, this means not only responding to exceptions with logging and notifications but also preparing to halt thread execution safely. Placing these mechanisms at the core of development practices enables smoother user experiences and increased reliability across distributed systems.\n\nOne significant mistake in managing faults within concurrent executions is neglecting to log stack traces. Without comprehensive logging, debugging becomes nearly impossible. According to a survey conducted by the Eclipse Foundation, 58% of developers face challenges in analyzing issues due to insufficient logging practices. Always ensure stack traces are captured effectively, providing a crucial roadmap for troubleshooting.\n\nAn additional concern arises from ignoring uncaught exceptions. Default behaviors differ between environments, leading to unexpected outcomes. For instance, if a non-caught exception occurs in a child thread, it might terminate silently without impacting the parent thread. This can result in unnoticed errors affecting application stability. Utilize mechanisms like Thread.setDefaultUncaughtExceptionHandler() to manage uncaught exceptions gracefully.\n\nConsistently using shared resources can lead to a bottleneck if exceptions are improperly handled. A study by the ACM highlighted that 37% of performance issues in applications stem from inefficient resource management in concurrent tasks. Implement accurate exception propagation strategies to maintain performance integrity.\n\nAnother common error is duplicating exception handling logic across multiple threads. This can lead to code duplication and increased maintenance efforts. Instead, centralize your exception management by creating service classes or handler components. This promotes code reusability and simplifies updates.\n\nFinally, consider the ramifications of suppressing exceptions entirely. While it may seem like a quick solution, silencing faults can mask deeper issues, leading to long-term repercussions. A report on software quality by Microsoft indicates that 60% of bugs are a direct result of neglected error handling practices. Always ensure exceptions are either logged or rethrown with meaningful context.\n\nFor organizations looking to enhance their online presence, consider investing in corporate web design & development services. If you need expertise on handling application errors effectively, don’t hesitate to hire freelance developers who specialize in robust coding practices.\n\nHow to Preserve Thread State During Exceptions\n\nUtilize structured exception handling by employing try-catch blocks around critical sections of code. Capture exceptions as soon as they occur to maintain a clearer understanding of the thread's state at failure points.\n\nIncorporate a logging framework to document the state before throwing exceptions. This includes pertinent variables and resource statuses, enabling an easier recovery process. For instance, use a logging library like SLF4J or Logback, which allows for flexible logging levels and output formats.\n\nImplement lightweight, state-tracking wrappers around shared resources. By saving the pertinent state information within classes, threads can restore themselves to a known good state without losing context. Store necessary parameters with the method calls to enable reconstruction after an exception.\n\nLeverage thread-local variables for maintaining independent state data across concurrent threads. Using ThreadLocal enables each thread to have its own instance of a variable, ensuring data isolation and reducing the risk of state corruption. Around 30% of crashes in concurrent systems arise from unexpected thread interactions, emphasizing the importance of localizing state information.\n\nConsider employing a retry mechanism in your exception strategy. For transient errors, implement a back-off policy that allows the thread to attempt a task again after a delay, while preserving its current state. This is particularly useful in I/O operations where temporary failures are common.\n\nFinally, ensure proper cleanup through a finally block or similar structure, which guarantees execution of essential code regardless of success or failure. This includes releasing resources, resetting flags, and flushing buffers, which maintains the integrity of your threaded operations.\n\nEmploy a centralized exception management mechanism. This facilitates the logging and handling of errors from various threads in a consistent manner, enhancing fault tolerance across the system. A common approach is to use a global exception handler that captures uncaught exceptions from all threads. For instance, implementing the Thread.setDefaultUncaughtExceptionHandler method ensures that all unhandled exceptions are managed uniformly. This reduces the risk of application crashes and improves user experience.\n\nUtilize specific catch blocks instead of generic ones. This allows for precise responses to known issues while maintaining the ability to capture unexpected scenarios. Leverage custom exception types that extend the built-in exception classes. This practice facilitates clearer error categorization and streamlines debugging processes by providing contextual information about the source of the issue.\n\nIncorporate effective logging practices. Utilize frameworks such as Log4j or SLF4J to standardize logging. Capture error contexts, including thread IDs and timestamps, to aid troubleshooting efforts. Approximately 80% of debugging time is often spent on analyzing logs; hence, detailed records are invaluable.\n\nImplement retry mechanisms for transient failures, which are common in network calls or database interactions. A backoff strategy should be considered to prevent overwhelming resources during recovery attempts. Reports indicate that implementing retry logic can reduce system downtime by over 40% in environments prone to brief outages.\n\nEstablish alerting for critical failures. Use monitoring tools to notify developers of unhandled exceptions or application instabilities. Automatic alerting systems can significantly reduce the response time for addressing issues. According to studies, early detection of issues often prevents escalation, enabling more swift remediation.\n\nEncourage teamwork by ensuring developers share knowledge about handling specific exceptions related to shared components or services. This enhances code quality and reduces error recurrence. Collaborating on exception strategies contributes to more cohesive development efforts and leads to fewer deployment-related errors.\n\nFinally, consider external assistance for enhancing application reliability. Consulting experienced developers can bring unique insights and generate customized strategies tailored to your specific needs. Organizations often engage specialists to assess their existing systems. If you require such expertise, consider reaching out for hire lua developers or look into options for a website coder for hire.\n\nImplement try-catch statements selectively; avoid wrapping an entire method within a single try block. This practice enhances clarity and allows for more granular error management. Collect specific information related to the exception caught to facilitate debugging and enhance the application’s robustness. For instance, using can be more insightful compared to a generic .\n\nAlways log exceptions in the catch block to maintain a comprehensive record of errors that occur during execution. According to industry statistics, over 75% of application failures stem from uncaught exceptions, highlighting the need for proper logging mechanisms that include error types, stack traces, and contextual information.\n\nConsider re-throwing exceptions after logging them. This allows higher levels of the application to handle these errors appropriately. For example, wrapping the exception in a runtime-specific exception can help differentiate between error types and guide further corrective measures.\n\nLimit the use of multiple catch blocks. Excessive catch blocks may signal design flaws that promote complex error handling scenarios. Instead, consolidate similar exceptions to streamline the code. Statistical data indicates that the readability of code decreases by 30% with every additional catch block beyond three.\n\nIn a concurrent environment, dispatch exceptions to a central handler when appropriate. This approach ensures a uniform response to various failure scenarios across threads. Research shows that centralizing error handling can lead to a 40% reduction in maintenance time, as it simplifies the tracking and resolution of issues.\n\nUtilize custom exceptions to convey specific business logic failures. Incorporate a meaningful message and, if necessary, additional context to aid troubleshooting. This practice significantly enhances code maintainability. According to surveys, developers report a 50% improvement in issue resolution time when utilizing detailed custom exceptions.\n\nFor production environments, consider implementing fallback mechanisms or recovery procedures in your catch blocks. This could mean retrying an operation or switching to alternative workflows, thereby increasing the overall resilience of the application. Real-world application data illustrates that systems with defined recovery strategies experience 60% fewer critical downtime incidents.\n\nLastly, ensure thorough testing of all error paths. Unit tests should cover edge cases to verify that the try-catch blocks behave as expected. Coverage reports should reflect at least 85% on these scenarios to ensure robust error handling. Keeping a high testing standard correlates positively with reduced production bugs, with numbers showing a decrease of 70% when comprehensive tests are in place.\n\nFor those looking to enhance their projects or bring in specialized talent, consider options like hire lua developers' or find a reliable website coder for hire'.\n\nImplement a global exception handler that captures uncaught exceptions from all threads. This guarantees that application stability is maintained, and crucial error logging is performed consistently.\n• Utilize the method to register a handler that can process uncaught exceptions across threads.\n• Consider logging these exceptions to a centralized logging system or a file for future analysis.\n• Make decisions on recovery strategies that might include restarts of failed threads or notifying system administrators.\n\nResearch indicates that 75% of developers do not handle uncaught exceptions properly, leading to missed opportunities for stability enhancements.\n\nBy adopting this approach, track metrics on the frequency and nature of errors occurring across threads to identify patterns and areas for improvement.\n• Implement alerts for critical failures to ensure immediate attention.\n• Integrate monitoring tools that can provide real-time insights into exceptions thrown within threads.\n\nStatistically, about 30% of applications fail due to unhandled failures. Active monitoring and handling can drastically reduce downtime.\n\nIt's crucial to tailor the handler to fit the application's specific requirements, allowing for nuanced error processing based on the context of failures.\n\nConsider using frameworks like Akka which support resilient architectures that can manage failures directly, optimizing recovery efforts across threads.\n\nRegularly review and update the exception handling strategy as the application evolves to ensure alignment with current operational goals and challenges.\n\nDefine tailored exception classes that extend existing ones to enhance error reporting in complex scenarios. This allows for explicit identification of issues based on their context. For instance, consider creating a class named . It not only informs about the failure but can also include additional context such as the database name and the operation that failed.\n\nUse the following guidelines to develop useful custom exception classes:\n• Granularity: Specify exceptions at a granular level. For example, separate from . This granularity assists in precise troubleshooting.\n• Contextual Information: Include properties to capture relevant details. Properties like , , or can significantly aid in diagnosing issues rapidly.\n• Serialization: Implement interface if exceptions are passed across threads. This ensures that the complete state of the exception is preserved.\n\nIncorporate such classes within your error handling logic. For example:\n\ntry { connectToDatabase(); } catch (SQLException ex) { throw new DatabaseConnectionException('Failed to connect to database.', 'MyDB', 'CONNECT'); }\n\nStatistics indicate that custom exception hierarchies can reduce debugging time by 30% in large-scale systems. Consistency in error reporting helps maintain high code quality, with 45% fewer critical bugs reported in applications employing tailored exceptions.\n\nRegularly review and refine custom exceptions based on use cases. Maintain a documentation structure to ensure that all teams understand the purpose and expected use of each custom exception. An organized approach contributes to more robust and maintainable codebases.\n\nImplement structured logging to facilitate clear understanding and analysis of issues. Ensure that log messages contain the following components:\n• Thread ID: Identify which thread experienced the problem, especially critical in concurrent systems.\n• Error Level: Utilize levels such as DEBUG, INFO, WARN, and ERROR to classify severity.\n• Exception Details: Include stack traces and error messages to aid in debugging.\n• User Context: Log relevant user information for traceability when necessary.\n\nEmploy logging frameworks like Log4j, SLF4J, or java.util.logging to manage and format your logs efficiently. These tools provide enhanced functionalities such as:\n• Log Rotation: Manage log file sizes and archival for long-term storage of critical logs.\n• Filter Capabilities: Customize log output to focus on specific log levels or categories.\n\nAnalyze logged exceptions systematically. Integrating these logs with data analytics and monitoring solutions–like ELK Stack or Splunk– allows for:\n• Alerting Systems: Set triggers based on log patterns to notify developers of critical failures instantly.\n\nThese experts can optimize the analysis of logged data to provide insights and actionable recommendations.\n\nLastly, maintain discipline in your logging strategy by regularly reviewing and refining log configurations to align with evolving project needs. Use measured approaches when adjusting log levels to prevent overloading systems with excessive data.\n\nChoosing tailored crm development services can further streamline your error monitoring processes by embedding relevant logging capabilities directly into business applications, promoting seamless incident tracking and resolution.\n\nImplementing a robust mechanism for error handling in concurrent tasks is critical. Ensure that every thread captures exceptions using try-catch blocks, preventing unhandled exceptions from crashing the entire process. The Thread.UncaughtExceptionHandler interface provides a way to handle uncaught exceptions globally by defining a custom handler for threads. This practice allows the application to recover from exceptions without terminating the entire application.\n\nCentral aspects of building fail-safe mechanisms include logging and retrying strategies. Log error details at the time of exception to facilitate easier debugging. One way to manage transient errors is by employing an exponential backoff strategy for retrying operations, which progressively increases the wait time between attempts. Research suggests that around 70% of transient errors can be resolved with this method, significantly enhancing overall reliability.\n\nUtilizing a message queue for inter-thread communication can also improve error handling. This approach isolates thread execution from immediate failure responses, allowing for graceful degradation of service under failure conditions. The design should include a dead-letter queue for unprocessed messages, enabling inspection and reprocessing later.\n\nIn complex systems, consider leveraging circuit breaker patterns to intercept calls to services. This helps to prevent ongoing failures from overwhelming services and provides a fallback mechanism. It limits the number of attempts you make during an outage and switches to an alternative solution until the service is stable again.\n\nFor organizations looking to enhance their data processing capabilities, it may be beneficial to hire SQL CLR developers, ensuring smooth and resilient operations within the database layer, which is often critical in a concurrent execution environment."
    },
    {
        "link": "https://adtmag.com/articles/1999/12/18/multithreaded-exception-handling-in-java.aspx",
        "document": "Multithreaded programming has been with us for many years and is considered to be a feature that many robust applications utilize. Exception handling is another feature of many languages considered to be necessary for proper and complete error handling. Each of these technologies stands very well on their own. In fact, you cannot pick up a book about Java programming without finding a chapter devoted to each of these topics. Many of these books do a good job defining and describing how to use them properly in your programs. What is missing is the information on how to use these two technologies together effectively. After all, how effective is writing a multithreaded Java program if it is incapable of properly handling exceptions occurring on secondary threads? We will present a solution for effectively dealing with this problem. To solve this problem, we introduce two new classes and two new interfaces to be used when writing multithreaded Java programs. These classes are small, easy to use and understand, and effectively enable you to handle exceptions occurring on secondary threads. Writing robust code implies many things. One of them is the proper and effective way in which your program deals with error situations. The approach you take can vary from doing nothing to handling any and all problems. The approach you choose is more than likely dictated by the type of application you are writing. For example, if you are writing an application that must run uninterrupted for many hours, days, or months at a time, you will need to effectively employ an error handling strategy that will ensure your software can run uninterrupted when errors occur. Even if you are not writing such a program, effectively dealing with errors is just good programming practice. One of the main areas of importance for dealing with exceptions is what we call state management. This involves ensuring that when an exception occurs, the state of the object the exception occurs in remains valid such that if the code recovers from the exception, the object can be reliably used again. Doing so in single-threaded applications is challenging enough without introducing multiple threads of execution. The core problem that must be dealt with is how to manage concurrent threads of execution when one, or many, of those threads may terminate abnormally. We need a way to be notified of any errors occurring on secondary threads, and a solution that enables us to terminate gracefully while optionally preserving object state. Java is currently being considered as a platform for deploying mission-critical applications. Multithreaded exception handling is a reality in this environment. While much documentation exists on the virtues of using threads of execution and exceptions in Java, there is little documentation on how to integrate the two. One such solution to these problems is presented here. We designed our solution to be as generic as possible. Our design goals were to provide a solution that:\n• Does not tightly couple the objects and code running on secondary threads with the objects and code that need to know if an exception occurs. For example, we do not implement a simple callback interface.\n• Requires a minimum amount of maintenance when the code is changed to throw additional exceptions from the secondary threads.\n• Works even if the code to be executed on secondary threads is not owned by the developer calling it. This could occur if you are implementing a run() method that calls code you obtained from a third party, and you want to catch any exception thrown from it.\n• Works for all exceptions, both checked and unchecked, that may be thrown from within a secondary thread. Note: Throughout this article we will be using the notion of checked and unchecked exceptions. Checked exceptions are generally related to conditions that are specific to an operation being performed, such as trying to construct an invalid URL. The compiler requires that you take action on all checked exceptions that may occur in your method in one of two ways: either by handling them yourself with a try/catch block or by advertising to your callers that your method throws this exception by listing them in that method’s throws clause. In contrast, unchecked exceptions could occur anywhere in your program, such as an out-of-memory condition. Although it may be useful to be aware of these problems in certain situations, the compiler does not require you to address unchecked exceptions. The solution identified in this article satisfies all of the afforementioned design goals and is straightforward and generic enough to be used in production systems. All of the code presented here was compiled and run using the Sun JDK 1.1.4 on the Windows NT 4.0 Operating System. We present our solution to this problem in the following manner: Appendix A (posted at Java Report Online) contains a complete multithreaded program that attempts to open separate files on two threads. Listings 1-4 examine this code, point out the problems contained in it, and offer some initial attempts at solving them. Listings 5-8 introduce two new classes and two new interfaces to provide a solution to these problems. Appendix B (see Java Report Online) contains the program in Appendix A modified with the classes and interfaces introduced in Listings 5-8 such that it correctly deals with exceptions occurring on secondary threads. Listings 9-11 examine, more closely, Appendix B and offer commentary on how it was developed to solve these problems. The code in Appendix A contains a user interface that has two buttons and two listboxes. The listboxes are used to display the files and the buttons are used to fill the first listbox on the main thread and start the secondary thread to attempt to fill the second listbox. The code uses a FileOpener class to attempt to open the file on the secondary thread. The main thread will open the file and fill the first listbox without any errors. The second listbox will not fill up due to the exceptions occurring on the secondary thread. Listing 1 contains some relevant code fragments from this program. Pressing the first button will result in an invalid filename at //4, being sent to the FileOpener class causing the secondary thread to generate a checked exception, FileNotFoundException at //1. Pressing the other button will result in a null pointer at //5, being sent to the FileOpener class at making the secondary thread throw an unchecked, NullPointerException at //2. A key point to note is the primary thread must be able to determine the status of the secondary thread. This can be difficult particularly when the secondary thread may terminate due to an exception. What if you were writing a mission-critical application? How would you report failures in secondary threads to the calling code? After all, the calling code may be able to recover from the problem and try again. At a minimum, the calling code can inform the user that there is an unrecoverable problem and advise them to take some appropriate action. The worst thing that can happen is the calling code will continue as if the secondary thread completed successfully. This will result in errors occurring later, that will be much more difficult to track down. So, what do we do? You may notice at //3 we are catching the FileNotFoundException generated at //1. Why not catch it and let it pass through our run() method? The answer to this requires some explanation. Why Not Use The Traditional Try/Catch Technique? Our first attempt at solving the multithreaded exception handling problem was to devise a solution using traditional exception handling techniques. We simply placed a try/catch block around the start() method. After all, start() instantiates the secondary thread and calls its run() method and the use of try/catch is the natural way of dealing with exceptions. If the code in run() throws any exceptions we should be able to catch them. Let’s see what happens if we try to solve our problem this way. Listing 2 shows some of the code from Listing 1 modified with this dubious idea. Looking at this code we notice at //1 and //2 we are trying to catch exceptions thrown from the secondary thread by attempting to catch exceptions thrown from the call to start(). Because this code compiles cleanly, your next step may be to get the exception to propagate from the run() method so it can be caught at //2. Listing 3 shows our Runnable class modified with the changes you may make to accomplish this. Instead of catching the FileNotFoundException in run() as we did in Listing 1, we have removed the try/catch block to let the caller of the run() method handle it. Because the FileNotFoundException is a checked exception, we are required to advertise the fact that our run() method throws this exception by specifying it in the method’s throws clause at //3. On closer examination, Listings 2 and 3 are ill-fated for two reasons. First, the code in Listing 3 will not even compile because you are not allowed to throw checked exceptions from the run() method. The reason for this is because an override method can only throw exceptions of the same type of the method being overridden or specializations of those types. In other words, because the run() method of the Runnable class does not specify that it throws any checked exceptions, you cannot throw any checked exceptions from your overridden version of run(). The second problem is at //1 and //2 in Listing 2. Even though this code compiles cleanly, it is doomed to fail. Remember that start() instantiates a secondary thread and calls the run() method of the Runnable object asynchronously. Exceptions signal back only to some point on the stack of the affected thread. Because start() creates a new stack, the primary thread will never be notified of any exceptions that occur. What the code in Listing 2 is actually doing is catching exceptions that occur when calling start(), not exceptions that occur in the run() method, which is what you are trying to accomplish. One way to attempt to solve these problems is by creating a special class that acts like a thread, but also enables us to employ the try/catch model around the start() method for handling exceptions occurring on secondary threads. To accomplish this, we introduce a new class, shown in Listing 4, which will extend java.lang.ThreadGroup. ThreadGroup contains a key method, uncaughtException(), which is called on a thread when an exception occurs that is not handled, that is, caught. When the uncaughtException() method ends, the thread is terminated. To make our try/catch around start() scheme work, one may attempt to extend the ThreadGroup class at //1, provide a custom version of the start() method at //2, and override the uncaughtException() method at //4. We called this new class a ThreadWrapper. The steps outlined seem necessary so we can intercept the exception occurring on the secondary thread and then have our custom start() method throw it. This will enable the try/catch code from Listing 2 to actually catch the exception that occurred in the secondary thread. There is one major drawback to the code in Listing 4. This is the use of the join() method at //3. The call to join() is needed in order to support the try/catch technique around the call to start(). The big problem with this is the use of join() effectively makes your code single-threaded again. The join() method will block the main thread until the secondary thread has finished. This completely defeats the purpose of multithreaded programming but was necessary to make the try/catch around start() technique work. There does not exist a way in Java to use try/catch around your start() method to catch the exceptions thrown from a secondary thread and remain multithreaded. There does, however, exist an elegant way to handle exceptions thrown from secondary threads, that is derived from some of what we have seen so far. A new paradigm for dealing with exceptions is used which builds on the ideas of the JDK 1.1 event model. As we have seen, the try/catch model does not extend well into a multithreaded scenario. We need a generic mechanism that allows a main thread to have an arbitrary number of concurrently running secondary threads, each with the ability to communicate exceptions to objects that can deal with these exceptions. This mechanism must allow us to catch and propagate both checked and unchecked exceptions. Unchecked exceptions are relatively straightforward, as the compiler does not force us to either catch these or explicitly declare that we are passing them on to our clients. Checked exceptions are more challenging. Although we are required to handle these programmatically, it still may be desirable to pass this information on for possible recovery (or at least soft shutdown). The Java 1.1 Event Model introduces the notion of listener classes that can register with GUI components to be notified when events occur on those components. If we abstractly consider exceptions to be events, we can extend this paradigm to address our multithreaded exception handling issues. When an exception occurs in a secondary thread, notification could be sent to one or more other objects that are registered as listeners on that thread. Next, we discuss our approach and introduce the classes used to achieve our goal. For a complete solution, we have three fundamental requirements:\n• We need a type of thread that is capable of intercepting ALL of its exceptions, both checked and unchecked. This will allow us to consistently and comprehensively alert listeners of exceptions.\n• We need a conduit between the secondary thread and one or more listener objects through which we can pass exception information.\n• We need a mechanism that allows one or more listener objects to communicate back to the Runnable object on the thread where the exception occurred. This could be used to attempt recovery, preserve object state, or to perform some cleanup for soft termination. The SmartThread Class...A Better Thread Than Thread To address the first fundamental requirement, we introduce the SmartThread class in Listing 5. Like the ThreadWrapper class previously discussed, our SmartThread class extends ThreadGroup at //1. By overriding the ThreadGroup’s uncaughtException() method at //6, the SmartThread is able to intercept all unhandled, unchecked, and checked exceptions. The SmartThread can then notify all registered listener objects of the exception. In order for SmartThread to notify listener objects of exceptions, it needs to provide an interface for listener objects to register interest. This is done via addThreadExceptionListener() at //4. This method will support multiple listener objects because it is implemented with a Java Vector. When the uncaughtException() method is called, it will iterate over the Vector of listeners at //7 calling their exceptionOccurred() methods. (The astute reader may observe that addThreadExceptionListener() could also be implemented with a Multicaster. We have chosen to use a Vector here for simplicity of illustration. However, the final working program implements a Multicaster.) Each registered listener’s exceptionOccured() method will be called from //8 with the Runnable object the exception occurred in, the thread the exception occurred on, and the exception thrown that was not handled. It is important to pass the thread because one listener could be listening to multiple threads. Note that SmartThread is not truly a thread because it extends ThreadGroup by necessity. It does, however, compose a thread at //2 and //3. In order to avoid the pain of reimplementing the entire thread interface, but still give programmatic access to the composed thread, SmartThread provides a thread getter, getThread(), at //5. For example, the following would allow you to change the priority of the thread composed by the SmartThread to 1. Exceptions in secondary threads are communicated to listeners through the exceptionOccurred() method, which is defined in the ThreadExceptionListener interface in Listing 6. This addresses our second fundamental requirement. Classes that wish to be informed of exceptions occurring in other threads should implement this interface. In addition, they should utilize a SmartThread, rather than a regular Java Thread, because the SmartThread class knows how to notify the ThreadExceptionListener of exceptions. A ThreadExceptionListener that wishes to communicate back to the Runnable in which the exception occurred, can call its cleanupOnException() method, which is defined in the ThreadExceptionCleanup interface in Listing 7. This addresses our third fundamental requirement. Runnables should implement this interface to participate in cleanup requests originating from a ThreadExceptionListener. This enables reentry into the Runnable object. When an exception is thrown and not handled, and the uncaughtException() method called, you have left your Runnable object. This class gives you a way to get back into your Runnable object to perform any cleanup or state preservation. This class allows checked exceptions occurring in a secondary thread to be propagated to listeners in the same manner as unchecked exceptions. Recall that the occurrence of an unhandled unchecked exception results in a direct call to the SmartThread’s implementation of uncaughtException(). From here it is propagated to listeners via our exceptionOccurred() conduit. Because checked exceptions must be explicitly caught (we can’t rethrow them in the run() implementation for reasons discussed earlier), by definition they will never find their way directly to uncaughtException(). We need an indirect way to accomplish this. CheckedThreadException, shown in Listing 8, extends the unchecked RuntimeException class. CheckedThreadException is a special exception that allows us to throw checked exceptions from a method without requiring a throws clause. When we want to alert our listeners of the occurrence of a checked exception in a secondary thread, we wrapper this exception in a CheckedThreadException and rethrow it. This will result in a call to uncaught Exception(), allowing us to propagate both checked and unchecked exceptions through the same interface. The CheckedThreadException class also provides access to the original checked exception at //2 and the thread the exception occurred on at //1. This addresses our final fundamental requirement. Using the classes we have introduced, we have modified our initial multithreaded program from Appendix A to effectively deal with the exceptions generated on its secondary threads. The entire modified program is provided in Appendix B with the changes indicated in bold font. Let’s now examine the specific areas we modified utilizing our new classes. Listing 9 shows the modified FileOpener class. At //1 you will notice that our FileOpener class is implementing our ThreadExceptionCleanup interface. This also requires us to implement the cleanupOnException() method at //5. The cleanupOnException() method will be called if an exception is thrown from our Runnable object and not handled by that object. We are also utilizing the CheckedThreadException class at //4. As discussed, this enables us to throw checked exceptions out of our run() method. We must first catch the checked exception at //3, then wrapper it as an unchecked exception via the CheckedThreadException class. The \"decoding\" of this exception will occur later in the exceptionOccurred() method. The cleanupOnExeption() method proves to be very useful at //2. Depending on where the exception was thrown from in the run() method, a BufferedReader object may have been left open and need to be closed. If the exception was due to a file not found, then the BufferedReader would not yet have been opened, as this happens after it is determined that the file was found. However, what if the exception was thrown after //2? If an exception is thrown after //2, the BufferedReader is left open and we have created a resource leak. The cleanupOnException() method at //5 is called by exceptionOccurred() in this case to close the BufferedReader at //6, prior to this thread shutting down. This particular example could have been accomplished through the proper use of a finally clause, however, in other implementations you may not have a try/catch block to work with because you may not be dealing with checked exceptions, only unchecked exceptions. Therefore, you would not have a try block in which to attach a finally clause. Listing 10 shows the initial changes made to the PrimaryFrame class. PrimaryFrame now implements the ThreadExceptionListener interface at //1. This enables the PrimaryFrame class to register itself as a listener of uncaught exceptions occurring on its secondary threads via the addThreadExceptionListener() method shown at //3. However, for all of this to work, you must use the SmartThread class instead of the standard Java Thread class. This is accomplished at //2. The last area to examine is the exceptionOccurred() method. Listing 11 shows the implementation of this method and what we do when we are notified of an exception. This method is called after:\n• Overriding the exceptionOccurred() method in your ThreadExceptionListener object.\n• An exception is thrown from your Runnable object. We see at //1 the exceptionOccurred() method is provided the necessary information to properly deal with the problem being reported. This method knows the Runnable object and the thread the exception occurred on, along with the actual exception thrown. First, we want to know if the exception was really a checked or unchecked exception. At //2 we are checking if the exception is wrappered in a CheckedThreadException, indicating the exception we caught is a checked exception. We then \"decode\" our CheckedThreadException to see what type of checked exception it is at //3. If we determine the exception was a FileNotFoundException, we attempt to open another file at //4. Notice here that we don’t register another exception listener. You can register another one, and may want to do so depending on what you would like to happen if this second file is not found. However, be warned that if you register this same class as the listener, you can get the code into an infinite loop fairly easily. It is suggested that in cases like this you register another object as the listener, not the same object. Registering the same object would require some special case code to avoid the infinite loop scenario. If the exception caught here was not a checked exception, we know it was an unchecked exception. Therefore, we fall to the else clause and call back to our Runnable object via cleanupOnException() at //5. This is done to free the BufferedReader resource allocated in our Runnable object. This was discussed earlier in reference to Listing 9. Figure 1 represents the flow of control through the final version of our program. Note that when execution returns from the uncaughtException method, the thread will terminate immediately. In the course of our research for this article, we uncovered an alternate solution for multithreaded exception handling in Java published in CurrentProgramming with Java (Lea, D., Addison–Wesley, 1997) This method is called Completion Callbacks. Completion Callbacks involve a predefined interface which an object implements. Secondary threads then must call this interface to indicate success or failure. This allows the object implementing the interface to know whether the secondary thread completed successfully or with an exception. Completion Callbacks are an excellent approach to the problem addressed in this article but they do require a more tightly coupled relationship between the primary object(s) and the secondary thread(s). In cases where you don’t own the code to be executed in the secondary thread, this could be a problem. The Completion Callback solution also requires the developer to take explicit actions to address both checked and unchecked exceptions. By extending ThreadGroup and overriding the uncaughtException() method the developer is relieved of some of this work, and is also provided with a uniform and generic way of routing exception information to an object able to process it. We have shown a solution to enable Java programs to effectively deal with exceptions occurring in a multithreaded environment. We have done so, in part, by using the listener paradigm devised by the JDK 1.1 event model. We have met all of our design goals with our solution and solved all of the problems we have identified relating to completion callbacks. Our solution solves many of the problems of dealing with exceptions in a multithreaded environment. This solution allows you to throw checked and unchecked exceptions from run() methods at will, while ensuring these exceptions will be caught and communicated to predefined listener objects. This solution is more involved than a callback solution. The callback solution will work fine for certain cases but does not solve all of the problems associated with multithread exception handling. The solution we have provided is complete, robust, and follows the common listener paradigm familiar to Java developers. We have also provided a complete running code example of our final implementation utilizing our classes. The authors wish to thank Gary Craig, Art Jolin, and Bob Love for their efforts reviewing the first draft of this article and for their many useful comments and suggestions. (Appendices A and B are available at Java Report Online—www.javareport.com) n Joe DeRusso is a Senior Systems Developer with SAS Institute Inc. in Cary, NC. Joe applies existing and emerging Web technologies, such as Java, JavaScript, DHTML, and ActiveX, to prototype and develop Web clients for SAS Software products. He can be contacted at [email protected]. Peter Haggar is an Advisory Software Engineer with IBM in Research Triangle Park, NC. Peter works on emerging Java and Internet technology, focusing on embedded Java and real-time operating systems. He can be contacted at [email protected]."
    }
]