[
    {
        "link": "https://geeksforgeeks.org/linear-search",
        "document": ""
    },
    {
        "link": "https://digitalocean.com/community/tutorials/linear-search-algorithm-c",
        "document": ""
    },
    {
        "link": "https://geeksforgeeks.org/binary-search",
        "document": ""
    },
    {
        "link": "https://benhoyt.com/writings/hash-table-in-c",
        "document": "How to implement a hash table (in C)\n\nRecently I wrote an article that compared a simple program that counts word frequencies across various languages, and one of the things that came up was how C doesn’t have a hash table data structure in its standard library.\n\nThere are many things you can do when you realize this: use linear search, use binary search, grab someone else’s hash table implementation, or write your own hash table. Or switch to a richer language. We’re going to take a quick look at linear and binary search, and then learn how to write our own hash table. This is often necessary in C, but it can also be useful if you need a custom hash table when using another language.\n\nThe simplest option is to use linear search to scan through an array. This is actually not a bad strategy if you’ve only got a few items – in my simple comparison using strings, it’s faster than a hash table lookup up to about 7 items (but unless your program is very performance-sensitive, it’s probably fine up to 20 or 30 items). Linear search also allows you to append new items to the end of the array. With this type of search you’re comparing an average of num_keys/2 items.\n\nLet’s say you’re searching for the key in the following array (each item is a string key with an associated integer value):\n\nYou simply start at the beginning ( at index 0) and compare each key. If the key matches what you’re looking for, you’re done. If not, you move to the next slot. Searching for takes five steps (indexes 0 through 4).\n\nHere is the algorithm in C (assuming each array item is a string key and integer value):\n\nAnother simple approach is to put the items in an array which is sorted by key, and use binary search to reduce the number of comparisons. This is kind of how we might look something up in a (paper) dictionary.\n\nC even has a function in its standard library. Binary search is reasonably fast even for hundreds of items (though not as fast as a hash table), because you’re only comparing an average of log(num_keys) items. However, because the array needs to stay sorted, you can’t insert items without copying the rest down, so insertions still require an average of num_keys/2 operations.\n\nAssume we’re looking up again (in this pre-sorted array):\n\nWith binary search, we start in the middle ( ), and if the key there is greater than what we’re looking for, we repeat the process with the lower half. If it’s greater, we repeat the process with the higher half. In this case it results in three steps, at indexes 3, 1, 2, and then we have it. This is 3 steps instead of 5, and the improvement over linear search gets (exponentially) better the more items you have.\n\nHere’s how you’d do it in C (with and without ). The definition of the struct is the same as above.\n\nNote: in , it would be slightly better to avoid the up-front “half size overflow check” and allow the entire range of . This would mean changing the calculation to . However, I’m going to leave the code stand for educational purposes – with the initial overflow check, I don’t think there’s a bug, but it is non-ideal that I’m only allowing half the range of . Not that I’ll be searching a 16 exabyte array on my 64-bit system anytime soon! For further reading, see the article Nearly All Binary Searches and Mergesorts are Broken. Thanks Seth Arnold and Olaf Seibert for the feedback.\n\nHash tables can seem quite scary: there are a lot of different types, and a ton of different optimizations you can do. However, if you use a simple hash function together with what’s called “linear probing” you can create a decent hash table quite easily.\n\nIf you don’t know how a hash table works, here’s a quick refresher. A hash table is a container data structure that allows you to quickly look up a key (often a string) to find its corresponding value (any data type). Under the hood, they’re arrays that are indexed by a hash function of the key.\n\nA hash function turns a key into a random-looking number, and it must always return the same number given the same key. For example, with the hash function we’re going to use (64-bit FNV-1a), the hashes of the keys above are as follows:\n\nThe reason I’ve shown the hash modulo 16 is because we’re going to start with an array of 16 elements, so we need to limit the hash to the number of elements in the array – the modulo operation divides by 16 and gives the remainder, limiting the array index to the range 0 through 15.\n\nWhen we insert a value into the hash table, we calculate its hash, modulo by 16, and use that as the array index. So with an array of size 16, we’d insert at index 10, at 8, at 4, and so on. Let’s insert all the items into our hash table array (except for – we’ll get to that below):\n\nTo look up a value, we simply fetch . If the array size is a power of two, we can use . Note how the order of the elements is no longer meaningful.\n\nBut what if two keys hash to the same value (after the modulo 16)? Depending on the hash function and the size of the array, this is fairly common. For example, when we try to add to the array above, its hash modulo 16 is 7. But we already have at index 7, so we get a collision.\n\nThere are various ways of handling collisions. Traditionally you’d create a hash array of a certain size, and if there was a collision, you’d use a linked list to store the values that hashed to the same index. However, linked lists normally require an extra memory allocation when you add an item, and traversing them means following pointers scattered around in memory, which is relatively slow on modern CPUs.\n\nA simpler and faster way of dealing with collisions is linear probing: if we’re trying to insert an item but there’s one already there, simply move to the next slot. If the next slot is full too, move along again, until you find an empty one, wrapping around to the beginning if you hit the end of the array. (There are other ways of probing than just moving to the next slot, but that’s beyond the scope of this article.) This technique is a lot faster than linked lists, because your CPU’s cache has probably fetched the next items already.\n\nHere’s what the hash table array looks like after adding “collision” (with value 200). We try index 7 first, but that’s holding , so we move to index 8, but that’s holding , so we move again to index 9, and that’s empty, so we insert it there:\n\nWhen the hash table gets too full, we need to allocate a larger array and move the items over. This is absolutely required when the number of items in the hash table has reached the size of the array, but usually you want to do it when the table is half or three-quarters full. If you don’t resize it early enough, collisions will become more and more common, and lookups and inserts will get slower and slower. If you wait till it’s almost full, you’re essentially back to linear search.\n\nWith a good hash function, this kind of hash table requires an average of one operation per lookup, plus the time to hash the key (but often the keys are relatively short string).\n\nAnd that’s it! There’s a huge amount more you can do here, and this just scratches the surface. I’m not going to go into a scientific analysis of big O notation, optimal array sizes, different kinds of probing, and so on. Read Donald Knuth’s TAOCP if you want that level of detail!\n\nYou can find the code for this implementation in the benhoyt/ht repo on GitHub, in ht.h and ht.c. For what it’s worth, all the code is released under a permissive MIT license.\n\nI got some good feedback from Code Review Stack Exchange that helped clean up a few sharp edges, not the least of which was a memory leak due to how I was calling during the step (fixed here). I confirmed the leak using Valgrind, which I should have run earlier. Seth Arnold also gave me some helpful feedback on a draft of this article. Thanks, folks!\n\nFirst let’s consider what API we want: we need a way to create and destroy a hash table, get the value for a given key, set a value for a given key, get the number of items, and iterate over the items. I’m not aiming for a maximum-efficiency API, but one that is fairly simple to implement.\n\nAfter a couple of iterations, I settled on the following functions and structs (see ht.h):\n\nA few notes about this API design:\n• For simplicity, we use C-style NUL-terminated strings. I know there are more efficient approaches to string handling, but this fits with C’s standard library.\n• The function allocates and copies the key (if inserting for the first time). Usually you don’t want the caller to have to worry about this, or ensuring the key memory stays around. Note that returns a pointer to the duplicated key. This is mainly used as an “out of memory” error signal – it returns NULL on failure.\n• However, does not copy the value. It’s up to the caller to ensure that the value pointer is valid for the lifetime of the hash table.\n• Values can’t be NULL. This makes the signature of slightly simpler, as you don’t have to distinguish between a NULL value and one that hasn’t been set at all.\n• The function isn’t strictly necessary, as you can find the length by iterating the table. However, that’s a bit of a pain (and slow), so it’s useful to have .\n• There are various ways I could have done iteration. Using an explicit iterator type with a while loop seems simple and natural in C (see the example below). The value returned from is a value, not a pointer, both for efficiency and so the caller doesn’t have to free anything.\n• There’s no to remove an item from the hash table. Removal is the one thing that’s trickier with linear probing (due to the “holes” that are left), but I don’t often need to remove items when using hash tables, so I’ve left that as an exercise for the reader.\n\nBelow is a simple program (demo.c) that demonstrates using all the functions of the API. It counts the frequencies of unique, space-separated words from standard input, and prints the results (in an arbitrary order, because the iteration order of our hash table is undefined). It ends by printing the total number of unique words.\n\nNow let’s turn to the hash table implementation (ht.c).\n\nAllocating a new hash table is fairly straight-forward. We start with an initial array capacity of 16 (stored in ), meaning it can hold up to 8 items before expanding. There are two allocations, one for the hash table struct itself, and one for the entries array. Note that we use for the entries array, to ensure all the keys are NULL to start with, meaning all slots are empty.\n\nThe function frees this memory, but also frees memory from the duplicated keys that were allocated along the way (more on that below).\n\nNext we define our hash function, which is a straight-forward C implementation of the FNV-1a hash algorithm. Note that FNV is not a randomized or cryptographic hash function, so it’s possible for an attacker to create keys with a lot of collisions and cause lookups to slow way down – Python switched away from FNV for this reason. For our use case, however, FNV is simple and fast.\n\nAs far as the algorithm goes, FNV-1a simply starts the hash with an “offset” constant, and for each byte in the string, XORs the hash with the byte, and then multiplies it by a big prime number. The offset and prime are carefully chosen by people with PhDs.\n\nWe’re using the 64-bit variant, because, well, most computers are 64-bit these days and it seemed like a good idea. You can tell I don’t have one of those PhDs. :-) Seriously, though, it seemed better than using the 32-bit version in case we have a very large hash table.\n\nI won’t be doing a detailed analysis here, but I have included a little statistics program that prints the average probe length of the hash table created from the unique words in the input. The FNV-1a hash algorithm we’re using seems to work well on the list of half a million English words (average probe length 1.40), and also works well with a list of half a million very similar keys like , , and so on (average probe length 1.38).\n\nInterestingly, when I tried the FNV-1 algorithm (like FNV-1a but with the multiply done before the XOR), the English words still gave an average probe length of 1.43, but the similar keys performed very badly – an average probe length of 5.02. So FNV-1a was a clear winner in my quick tests.\n\nNext let’s look at the function. First it calculates the hash, modulo the (the size of the entries array), which is done by ANDing with . Using AND is only possible because, as we’ll see below, we’re ensuring our array size is always a power of two, for simplicity.\n\nThen we loop till we find an empty slot, in which case we didn’t find the key. For each non-empty slot, we use to check whether the key at this slot is the one we’re looking for (it’ll be the first one unless there had been a collision). If not, we move along one slot.\n\nThe function is slightly more complicated, because it has to expand the table if there are too many elements. In our implementation, we double the capacity whenever it gets to be half full. This is a little wasteful of memory, but it keeps things very simple.\n\nFirst, the function. It simply expands the table if necessary, and then inserts the item:\n\nThe guts of the operation is in the helper function (note how the loop is very similar to the one in ). If the argument is non-NULL, it’s being called from , so we allocate and copy the key and update the length:\n\nWhat about the helper function? It allocates a new entries array of double the current capacity, and uses with NULL to copy the entries over. Even though the hash value is the same, the indexes will be different because the capacity has changed (and the index is hash modulo capacity).\n\nThe function is trivial – we update the number of items in as we go, so just return that:\n\nIteration is the final piece. To create an iterator, a user will call , and to move to the next item, call in a loop while it returns . Here’s how they’re defined:\n\nThat’s it – the implementation in ht.c is only about 200 lines of code, including blank lines and comments.\n\nBeware: this is a teaching tool and not a library, so I encourage you to play with it and let me know about any bugs I haven’t found! I would advise against using it without a bunch of further testing, checking edge cases, etc. Remember, this is unsafe C we’re dealing with. Even while writing this I realized I’d used instead of to allocate the entries array, which meant the keys may not have been initialized to NULL.\n\nAs I mentioned, I wanted to keep the implementation simple, and wasn’t too worried about performance. However, a quick, non-scientific performance comparison with Go’s implementation shows that it compares pretty well – with half a million English words, this C version is about 50% slower for lookups and 40% faster for insertion.\n\nSpeaking of Go, it’s even easier to write custom hash tables in a language like Go, because you don’t have to worry about handling memory allocation errors or freeing allocated memory. I recently wrote a counter package in Go which implements a similar kind of hash table.\n\nThere’s obviously a lot more you could do with the C version. You could focus on safety and reliability by doing various kinds of testing. You could focus on performance, and reduce memory allocations, use a “bump allocator” for the duplicated keys, store short keys inside each item struct, and so on. You could improve the memory usage, and tune to not double in size every time. Or you could add features such as item removal.\n\nAfter I’d finished writing this, I remembered that Bob Nystrom’s excellent Crafting Interpreters book has a chapter on hash tables. He makes some similar design choices, though his chapter is significantly more in-depth than this article. If I’d remembered his chapter before I started, I probably wouldn’t have written this one!\n\nIn any case, I hope you’ve found this useful or interesting. If you spot any bugs or have any feedback, please let me know. You can also go to the discussions on Hacker News, programming Reddit, and Lobsters."
    },
    {
        "link": "https://reddit.com/r/C_Programming/comments/10fpd7w/is_there_any_official_documentation_of_c",
        "document": "maybe some of you will say that the C programming language book is the best but is there any documentation official ."
    },
    {
        "link": "https://geeksforgeeks.org/c-bubble-sort",
        "document": "Bubble Sort is a comparison based simple sorting algorithm that works by comparing the adjacent elements and swapping them if the elements are not in the correct order. It is an in-place and stable sorting algorithm that can sort items in data structures such as arrays and linked lists.\n\nIn this article, we will learn how to implement bubble sort algorithm in a C program.\n\nFollow the below approach to implement bubble sort algorithm in C. It is modified for increasing order, but you can change it easily for any desired order.\n• None Compare and swap the adjacent elements if they are in the wrong order starting from the first two elements.\n• None Do that for all elements moving from left to right. We will get the largest element at the right end.\n• None Start compare and swap again from the start but this time, skip the last element as its already at correct position.\n• None The second last element will be moved at the right end just before the last element.\n• None Repeat the above steps till all the elements are sorted.\n\nThe below C program sorts the given array into ascending order using bubble sort.\n\nTime Complexity: O(n2), where n is the number of items in the list.\n\nAuxiliary Space: O(1)\n\nAs we may notice, the above bubble sort algorithm still completed the third pass even when the array was sorted in the second pass. We can optimize it using a new variable swapped to signal if the swap operation is done in the inner loop iteration. If the swap doesn’t occur in the iteration, it means that the array is already sorted.\n\nWhen should I use Bubble Sort?"
    },
    {
        "link": "https://medium.com/@beyond_verse/sorting-algorithms-from-bubble-sort-to-quick-sort-2247cd9b67a5",
        "document": "Sorting algorithms are a fundamental part of computer science and programming. They are procedures or routines that organize a collection of data into a specific order, often in ascending or descending numerical or lexicographical sequence.\n\nThese algorithms play a crucial role in various applications, from organizing databases to optimizing search operations. Understanding and implementing different sorting techniques is essential for efficient data processing in software development.\n\nEfficient sorting is at the core of many computational tasks. It enables faster searching, facilitates easier data retrieval, and forms the basis for a wide range of algorithms and applications.\n\nFor instance, in a contact management application, efficient sorting allows users to quickly locate contacts by name or other criteria. Similarly, in databases and search engines, sorting algorithms are instrumental in delivering relevant results promptly.\n\nBubble sort is a simple and intuitive sorting algorithm that works by repeatedly swapping adjacent elements if they are in the wrong order. It gets its name from the way smaller elements “bubble up” to their correct positions as the algorithm iterates through the data.\n• Iterative Comparison: The algorithm iterates over the entire dataset, comparing adjacent elements.\n• Swapping: If two adjacent elements are in the wrong order, they are swapped.\n• Passes: After each pass, the largest element (in ascending order) is guaranteed to be in its correct position at the end of the array.\n• Multiple Passes: The algorithm continues with multiple passes until the entire array is sorted.\n• Best Case: O(n) — When the array is already sorted and no swaps are needed.\n• Worst Case: O(n²) — When the array is sorted in reverse order.\n• Bubble sort is an in-place sorting algorithm, meaning it requires only a constant amount of extra memory for temporary variables.\n• Bubble sort can be useful for small datasets or nearly sorted arrays where its simplicity can outweigh its inefficiency.\n• Inefficient for large datasets due to its quadratic time complexity.\n• Not suitable for real-time applications or situations where high performance is critical.\n\nBubble sorting is primarily used for educational purposes and as a stepping stone to understanding more efficient sorting algorithms.\n\nSelection sort is a simple and intuitive sorting algorithm that works by repeatedly selecting the smallest (or largest) element from the unsorted part of the array and swapping it with the first unsorted element. It is called the “selection” sort because, at each step, it selects the smallest (or largest) element and places it in its correct position.\n• Dividing the Array: The array is divided into two parts: the sorted part on the left and the unsorted part on the right.\n• Finding the Minimum: The algorithm iterates through the unsorted part to find the smallest element.\n• Swapping: The smallest element is then swapped with the first element in the unsorted part.\n• Expanding the Sorted Part: The boundary between the sorted and unsorted parts is moved one position to the right.\n• Repeating the Process: Steps 2–4 are repeated until the entire array is sorted.\n• Best Case: O(n²) — Since it doesn’t have any special cases, it performs at its worst even if the array is partially sorted.\n• Worst Case: O(n²) — Same as the best case.\n• Selection sort is an in-place sorting algorithm, meaning it requires only a constant amount of extra memory for temporary variables.\n• Selection sorting can be practical for small datasets or when the simplicity of the algorithm outweighs its inefficiency.\n• Inefficient for large datasets due to its quadratic time complexity.\n• Not suitable for real-time applications or situations where high performance is critical.\n• Not stable, meaning it may change the relative order of equal elements.\n\nSelection sort is often used for educational purposes or when simplicity and ease of implementation are more important than performance.\n\nInsertion sort is a simple and intuitive sorting algorithm that builds the sorted array one element at a time. It works by taking each element from the unsorted part of the array and inserting it into its correct position in the sorted part.\n• Dividing the Array: The array is divided into two parts: the sorted part on the left and the unsorted part on the right.\n• Iterative Insertion: The algorithm iterates through the unsorted part, taking one element at a time.\n• Inserting into the Sorted Part: For each element, it finds the correct position in the sorted part and inserts it.\n• Expanding the Sorted Part: The boundary between the sorted and unsorted parts is moved one position to the right.\n• Repeating the Process: Steps 2–4 are repeated until the entire array is sorted.\n• Best Case: O(n) — When the array is already sorted, and no or minimal insertions are needed.\n• Worst Case: O(n²) — When the array is sorted in reverse order.\n• Insertion sort is an in-place sorting algorithm, meaning it requires only a constant amount of extra memory for temporary variables.\n• Insertion sort can be efficient for small datasets or nearly sorted arrays where its simplicity can outweigh its inefficiency.\n• It’s often used in scenarios where elements are frequently added to an already sorted list.\n• Inefficient for large datasets due to its quadratic time complexity.\n• Not suitable for real-time applications or situations where high performance is critical.\n\nInsertion sort is frequently used in scenarios where the list is partially sorted, or where elements are continuously added to an already sorted list.\n\nMerge sort is a highly efficient and widely used sorting algorithm that employs a divide-and-conquer strategy. It divides the unsorted list into smaller sublists, sorts them independently, and then merges them back together.\n• Divide: The unsorted list is divided into smaller sublists, usually by halving it.\n• Conquer: Each sublist is recursively sorted using the merge sort algorithm.\n• Combine: The sorted sublists are then merged back together, creating a larger sorted list.\n• Best Case: O(n log n) — When the array is divided evenly, and the merging process is efficient.\n• Worst Case: O(n log n) — Same as the best case.\n• Average Case: O(n log n) — Regardless of the initial order of elements.\n• Merge sort typically requires additional space for the merging process, making it a stable but not in-place sorting algorithm.\n• Merge sort is highly efficient and is often used for sorting large datasets or in situations where stable and predictable performance is crucial.\n• Requires additional memory space for the merging process, which can be a limiting factor in resource-constrained environments.\n\nMerge sort’s efficiency and stability make it a popular choice for a wide range of applications, particularly in scenarios where a stable sorting algorithm with predictable performance is required.\n\nQuick sort is a highly efficient and widely used sorting algorithm known for its speed and effectiveness. It uses a divide-and-conquer strategy to partition the array into smaller segments, sort them independently, and then combine them.\n• Pivot Selection: A pivot element is chosen from the array. This element will be used to divide the array into two partitions.\n• Partitioning: Elements smaller than the pivot are moved to the left, while elements greater than the pivot are moved to the right.\n• Recursive Sorting: The subarrays on the left and right of the pivot are recursively sorted using the same process.\n• Combining the Sorted Subarrays: The sorted subarrays are combined to produce the final sorted array.\n• Best Case: O(n log n) — When the pivot is chosen consistently divides the array in a balanced manner.\n• Worst Case: O(n²) — Occurs when the pivot selection leads to highly unbalanced partitions.\n• Average Case: O(n log n) — On average, quick sort performs significantly faster than other quadratic time complexity sorting algorithms.\n• Quick sort is an in-place sorting algorithm, meaning it requires only a constant amount of extra memory for temporary variables.\n• Quick Sort’s efficiency makes it a popular choice for sorting large datasets or in situations where speed is critical.\n• The worst-case time complexity of O(n²) can occur if the pivot selection consistently leads to unbalanced partitions.\n\nDespite its worst-case time complexity, Quick Sort’s average-case performance and efficiency make it a widely used sorting algorithm in various applications.\n\nHeap sort is a highly efficient and comparison-based sorting algorithm that leverages the properties of a special data structure called a “heap.” It builds a max-heap or min-heap from the elements to be sorted and then efficiently extracts and places the extremum element to achieve sorting.\n• Heapification: The unsorted array is treated as a binary tree and transformed into a max-heap or min-heap (depending on ascending or descending order).\n• Extract and Heapify: The root element (max or min) is extracted and replaced with the last element of the heap. The heap is then heapified again to maintain the heap property.\n• Repeat: Steps 2 are repeated until the heap is empty and all elements are extracted in sorted order.\n• Best Case: O(n log n) — Due to the properties of the heap data structure, heap sort performs consistently well.\n• Worst Case: O(n log n) — Same as the best case.\n• Average Case: O(n log n) — Regardless of the initial order of elements.\n• Heap sort is an in-place sorting algorithm, meaning it requires only a constant amount of extra memory for temporary variables.\n• Heap sort is efficient for large datasets and is often used in scenarios where stable and predictable performance is crucial.\n• Although efficient, heap sort may be slightly slower in practice compared to quick sort or merge sort due to its higher constant factors.\n\nHeap sort’s consistent performance and in-place sorting nature make it a valuable choice for scenarios where stability and predictability are paramount, especially for large datasets.\n\nWhen evaluating sorting algorithms, it’s crucial to consider their performance under different scenarios. Here’s a comparison of various sorting algorithms based on their time complexity in different cases:\n• Bubble Sort, Selection Sort, and Insertion Sort are in-place sorting algorithms, meaning they require only a constant amount of extra memory.\n• Merge Sort, Quick Sort, and Heap Sort are not in place, and they require additional memory for recursive calls or heap data structures. The space complexity for these algorithms is O(n) or O(log n), depending on the specific implementation.\n\nWhen choosing a sorting algorithm for a specific task, practical considerations beyond theoretical performance must be taken into account:\n• Dataset Size: For small datasets, simpler algorithms like bubble sort or insertion sort may be sufficient. For large datasets, more efficient algorithms like merge sort, quick sort, or heap sort are preferred.\n• Data Characteristics: If the data is partially sorted or nearly in order, algorithms like insertion sort or bubble sort may perform well.\n• Stability: If the relative order of equal elements must be preserved, merge sort is a stable sorting algorithm.\n• Implementation Complexity: Some algorithms are easier to implement than others. Consider the complexity of implementation when choosing an algorithm for a specific application.\n• Memory Constraints: In situations with limited memory, in-place algorithms like bubble sort, selection sort, and insertion sort may be the only feasible options.\n\nIndustries and Scenarios Where Sorting Algorithms Are Crucial\n\nSorting algorithms play a pivotal role in numerous industries and scenarios where organized data is essential for efficient operations. Here are some key industries and scenarios where sorting algorithms are crucial:\n\n1. E-Commerce and Retail: Sorting products based on various attributes like price, popularity, or category helps users find what they’re looking for quickly.\n\n2. Finance and Banking: Sorting transactions, customer accounts, and financial data is crucial for accurate reporting and analysis.\n\n3. Healthcare: Sorting patient records, medical test results, and prescription histories aids in providing timely and accurate healthcare services.\n\n4. Logistics and Supply Chain: Optimizing routes, managing inventory, and scheduling shipments rely heavily on sorting algorithms.\n\n5. Search Engines: Sorting search results by relevance, date, or other criteria ensures users get the most relevant information.\n\n6. Social Media Platforms: Arranging posts, comments, and user interactions based on time or relevance enhances user experience.\n\n7. Gaming: Sorting high scores, leaderboards, and game states is fundamental for competitive gaming experiences.\n\n8. Data Analytics and Business Intelligence: Sorting large datasets for analysis, reporting, and generating insights is a fundamental operation.\n\n9. Genomics and Bioinformatics: Sorting genetic sequences and analyzing biological data is vital for research in genetics and medicine.\n\n10. Telecommunications: Sorting call logs, network data, and customer information is essential for effective service delivery.\n\n1. Email Inbox: Sorting emails by date, sender, or subject helps users quickly find and respond to messages.\n\n2. File Systems: Sorting files by name, size, type, or modification date enables efficient file management.\n\n3. Contact Lists: Sorting contacts by name, organization, or recent interaction simplifies communication.\n\n4. Music Libraries: Sorting songs by artist, album, genre, or play count enhances the music listening experience.\n\n5. E-Libraries and Document Management Systems: Sorting documents by title, author, or category facilitates easy access and retrieval.\n\n6. E-Learning Platforms: Sorting courses by category, popularity, or completion status helps learners navigate and choose courses.\n\n8. Social Networking Feeds: Sorting posts by relevance, time, or user engagement ensures users see the most relevant content.\n\n9. Online Marketplaces: Sorting products by price, rating, or category helps users find and compare items efficiently.\n\n10. Cloud Storage Services: Sorting files by name, date, or size simplifies file organization and retrieval.\n\nIn this comprehensive exploration of sorting algorithms, we’ve delved into a range of techniques, each with its strengths, weaknesses, and ideal use cases. Let’s recap what we’ve learned:\n\n· Selection Sort: Another straightforward algorithm with a quadratic time complexity, useful for small datasets.\n\n· Insertion Sort: A practical choice for partially sorted or small datasets, but less efficient for larger lists.\n\n· Merge Sort: An efficient algorithm that performs consistently well regardless of the initial order of elements.\n\n· Quick Sort: Highly efficient in most cases, but may have a higher constant factor in practice due to its partitioning process.\n\n· Heap Sort: Offers consistent performance and is suitable for large datasets, though may not be as widely used as other algorithms.\n\nEncouragement for Further Exploration and Practice\n\nSorting algorithms are a cornerstone of computer science and play a crucial role in a wide range of applications. To further your understanding and proficiency, consider the following steps:\n\n1. Hands-On Practice: Implement these algorithms in your projects to gain practical experience and deepen your understanding.\n\n2. Explore Variants and Optimizations: Each algorithm has variants and optimizations that can improve performance in specific scenarios. Dive deeper into these to enhance your skills.\n\n3. Participate in Coding Challenges: Engage in coding platforms and challenges to test your knowledge and explore different approaches to sorting.\n\n4. Study Advanced Topics: Delve into advanced topics like parallel sorting algorithms, external sorting, and hybrid approaches to gain a more nuanced understanding.\n\n5. Stay Updated: Keep an eye on emerging trends and techniques in sorting algorithms. Stay curious and open to adopting new approaches.\n\nRemember, becoming proficient in sorting algorithms is a journey that requires practice, exploration, and continuous learning. With dedication and curiosity, you’ll become adept at selecting and implementing the right sorting algorithm for any given task.\n\nThank you for joining us on this exploration of sorting algorithms. Keep sorting, keep coding, and keep innovating!"
    },
    {
        "link": "https://geeksforgeeks.org/quick-sort-algorithm",
        "document": "QuickSort is a sorting algorithm based on the Divide and Conquer that picks an element as a pivot and partitions the given array around the picked pivot by placing the pivot in its correct position in the sorted array.\n\nQuickSort works on the principle of divide and conquer, breaking down the problem into smaller sub-problems.\n\nThere are mainly three steps in the algorithm:\n• Choose a Pivot: Select an element from the array as the pivot. The choice of pivot can vary (e.g., first element, last element, random element, or median).\n• Partition the Array: Rearrange the array around the pivot. After partitioning, all elements smaller than the pivot will be on its left, and all elements greater than the pivot will be on its right. The pivot is then in its correct position, and we obtain the index of the pivot.\n• Recursively Call: Recursively apply the same process to the two partitioned sub-arrays (left and right of the pivot).\n• Base Case: The recursion stops when there is only one element left in the sub-array, as a single element is already sorted.\n\nHere’s a basic overview of how the QuickSort algorithm works.\n\nThere are many different choices for picking pivots.\n• None Always pick the first (or last) element as a pivot . The below implementation picks the last element as pivot. The problem with this approach is it ends up in the worst case when array is already sorted.\n• None . This is a preferred approach because it does not have a pattern for which the worst case happens.\n• None Pick the median element is pivot. This is an ideal approach in terms of time complexity as we can find median in linear time and the partition function will always divide the input array into two halves. But it takes more time on average as median finding has high constants.\n\nThe key process in quickSort is a partition(). There are three common algorithms to partition. All these algorithms have O(n) time complexity.\n• Naive Partition : Here we create copy of the array. First put all smaller elements and then all greater. Finally we copy the temporary array back to original array. This requires O(n) extra space.\n• Lomuto Partition : We have used this partition in this article. This is a simple algorithm, we keep track of index of smaller elements and keep swapping. We have used it here in this article because of its simplicity.\n• Hoare’s Partition : This is the fastest of all. Here we traverse array from both sides and keep swapping greater element on left with smaller on right while the array is not partitioned. Please refer\n\nLet us understand the working of partition algorithm with the help of the following example:\n\nIn the previous step, we looked at how the partitioning process rearranges the array based on the chosen pivot. Next, we apply the same method recursively to the smaller sub-arrays on the left and right of the pivot. Each time, we select new pivots and partition the arrays again. This process continues until only one element is left, which is always sorted. Once every element is in its correct position, the entire array is sorted.\n\nBelow image illustrates, how the recursive method calls for the smaller sub-arrays on the left and right of the pivot:\n\nQuick Sort is a crucial algorithm in the industry, but there are other sorting algorithms that may be more optimal in different cases.\n\n// Index of smaller element and indicates // the right position of pivot found so far // elements on left side. Elements from low to // i are smaller after every iteration // pi is the partition return index of pivot // Index of smaller element and indicates // the right position of pivot found so far // elements to the left side. Elements from low to // i are smaller after every iteration // pi is the partition return index of pivot // Index of smaller element and indicates // the right position of pivot found so far // elements to the left side. Elements from low to // i are smaller after every iteration // pi is the partition return index of pivot # Index of smaller element and indicates # the right position of pivot found so far # elements to the left side. Elements from low to # i are smaller after every iteration # pi is the partition return index of pivot // Index of smaller element and indicates // the right position of pivot found so far // elements to the left side. Elements from low to // i are smaller after every iteration // pi is the partition return index of pivot // Index of smaller element and indicates // the right position of pivot found so far // elements to the left side. Elements from low to // i are smaller after every iteration // pi is the partition return index of pivot // Index of smaller element and indicates // the right position of pivot found so far // elements to the left side. Elements from low to // i are smaller after every iteration // pi is the partition return index of pivot\n• Best Case: (Ω(n log n)), Occurs when the pivot element divides the array into two equal halves.\n• Average Case (θ(n log n)), On average, the pivot divides the array into two parts, but not necessarily equal.\n• Worst Case: (O(n²)), Occurs when the smallest or largest element is always chosen as the pivot (e.g., sorted arrays).\n\nPlease refer Time and Space Complexity Analysis of Quick Sort for more details.\n• None It is a divide-and-conquer algorithm that makes it easier to solve problems.\n• None It is efficient on large data sets.\n• None It has a low overhead, as it only requires a small amount of memory to function.\n• None It is Cache Friendly as we work on the same array to sort and do not copy data to any auxiliary array.\n• None Fastest general purpose algorithm for large data when stability is not required.\n• tail recursive and hence all the can be done.\n• None It has a worst-case time complexity of O(n ), which occurs when the pivot is chosen poorly.\n• None It is not a good choice for small data sets.\n• None It is not a stable sort, meaning that if two elements have the same key, their relative order will not be preserved in the sorted output in case of quick sort, because here we are swapping elements according to the pivot’s position (without considering their original positions).\n• None Efficient for sorting large datasets with O(n log n) average-case time complexity.\n• None Used in partitioning problems like finding the kth smallest element or dividing arrays by pivot.\n• None Integral to randomized algorithms, offering better performance than deterministic approaches.\n• None Applied in cryptography for generating random permutations and unpredictable encryption keys.\n• None Partitioning step can be parallelized for improved performance in multi-core or distributed systems.\n• None Important in theoretical computer science for analyzing average-case complexity and developing new techniques.\n\nPlease refer Application of Quicksort for more details."
    },
    {
        "link": "https://stackoverflow.com/questions/69709795/make-bubble-sorting-more-efficient",
        "document": "Bubble sort is an inefficiency sorting algorithm and there are much better sorting algorithm.\n\nYou can make a bubble sort more efficient, its called Optimized Bubble Sort (It is still quite inefficient)\n\nOptimized bubble sort in short is, - you pass times , but on the every iteration you 'bubble' the biggest (or smallest) element to the end of the array. Now that last item is sorted, so you don't have to compare it again.\n\nI wrote this code last year, not to sure if it still works:\n\nHere you can read on optimized bubble sort\n\nHere you can find a list of different sorting algorithms that could be more efficient depending on your scenario."
    },
    {
        "link": "https://stackoverflow.com/questions/1358383/best-case-for-bubble-sort",
        "document": "There are multiple ways to write the bubble sort algorithm, it seems like over time the algorithm has gotten better, and more efficient. The first bubble sort algorithm I learned is below. The algorithm below Best and Worst Case is O(n^2).\n\nThe algorithm that Wikipedia uses (below) seems to be an improvement using a flag to tell if the elements have been swapped or not, which allows the bubble sort algorithm best case to be O(n) instead of O(n^2)\n\nHere is a video that helps explain a little bit about the first algorithm in the C programming language: https://youtu.be/qOpNrqqTsk4"
    }
]