[
    {
        "link": "https://mongodb.com/resources/products/capabilities/performance-best-practices",
        "document": "MongoDB is the premier NoSQL document database for modern developers working on high-performance applications. With its JSON-like documents, MongoDB is notable for horizontal scaling and load balancing, which offers developers an excellent balance of customization and scalability.\n\nBut like any high-performance tool, MongoDB performs best in the hands of an expert who knows what they’re doing. Performance issues may indicate that the database is not working as hard as it could and that specific optimizations could lead to better performance.\n\nIn this post, we’ll focus on how to achieve performance at scale using MongoDB by looking at:\n\nPlease keep in mind that the best practices we are going to cover are not exhaustive (that would require a much longer post).\n\nWho should read this guide?\n\nWhile anyone interested in document database platforms could learn something from this post, you’ll probably find this information particularly useful if you’re:\n• None Starting your first project as a seasoned MongoDB developer.\n\nIf you only have a few minutes to spare, this quick performance FAQ might be useful: \n\n\n\nAd hoc queries, indexing, and real time aggregation provide powerful ways to access data. MongoDB is a distributed database by default, which allows for expansive horizontal scalability without any changes to application logic.\n\nPretty darn fast. Primary key or index queries should take just a few milliseconds. Queries without indexes depend on collection size and machine specs, etc.\n\nIt depends on what you are and aren’t doing already. Try adding indices. Don’t do joins (embedding is preferable). Upgrade your machine specs. And, if you haven’t, definitely try sharding for horizontal scaling.\n\n4. How much RAM does MongoDB need?\n\nMongoDB needs enough RAM to hold your working set in memory. Of course, the exact answer depends on your data size and your workloads. You can use MongoDB Atlas for auto-scaling.\n\nYes, it most certainly is. MongoDB is great for large datasets. MongoDB Atlas can handle federated queries across object storage (e.g., Amazon S3) and document storage.\n\nWithout further ado, let’s go over the top 5 best practices for getting peak performance out of your MongoDB databases.\n\nMost developers would agree that the first step in optimizing performance is to understand expected and actual query patterns. Once you know your application’s query patterns like the back of your hand, you can design your data model and select appropriate indices accordingly.\n\nWith MongoDB, developers have access to several powerful tools that can help them greatly improve performance—but that doesn’t mean that query patterns and profiles can be ignored, either.\n\nFor example, one easy way to beef up performance is by simply analyzing your query patterns and determining where you can embed data rather than doing in-application or in-database joins.\n\nOther ways to improve MongoDB performance after identifying your major query patterns include:\n• None Storing the results of frequent sub-queries on documents to reduce read load\n• None Making sure that you have indices on any fields you regularly query against\n• None Looking at your logs to identify slow queries, then check your indices\n\nMongoDB is noted for its flexible schema, but this doesn’t mean that you can ignore schema design best practices. You should always figure out your schema at the beginning of a project so that you won’t have to retool everything later on. This applies 100% to your data models.\n\nWhile designing your data model, you must decide how to model relationships between data. Deciding when to embed a document or create a reference between separate documents in different collections instead, for example, is an application-specific consideration.\n\nA major advantage of JSON documents is that they allow developers to model data however the application requires. Nesting arrays and subdocuments allow you to model complex relationships between data using simple text documents.\n\nBut using MongoDB, you can also model:\n• None nodes and edges of connected graph data structures, etc.\n\nData modeling is a vast and sprawling topic that you could spend months learning about. If you haven’t already, here are a few resources that might help:\n• None MongoDB documentation includes a great section on data modeling, starting from planning out your document data model and going into detail on specifics such as embedding and referencing.\n• None MongoDB University offers a free training course on data modeling. This is a great way for beginners to get started with schema design and document data models. \n\n\n\n\n\n\n\nBest Practice #3: Try embedding and referencing\n\nA natural extension of data modelling, embedding allows you to avoid application joins, which minimizes queries and updates.\n\nNotably, data with a 1:1 relationship should be embedded within a single document. Data with a 1:many relationship in which \"many\" objects appear with or are viewed alongside their parent documents are also great candidates for embedding. Because these types of data are always accessed together, storing them together in the same document just makes sense.\n\nEmbedding generally provides better performance for read operations due to this kind of data locality. Embedded data models also allow developers to update related data in a single write operation because single document writes are transactional.\n\nHowever, not all 1:1 and 1:many relationships are good candidates for embedding in a single document. That’s where referencing between documents in different collections comes in.\n\nReferencing makes much more sense when modeling many:many relationships. However, whenever referencing, your application must issue follow-up queries to resolve any references. This, in turn, requires more round-trips to the server.\n\nYou should consider referencing when:\n• None A document is frequently accessed but contains data that is rarely used. Embedding would only increase in-memory requirements, so referencing may make more sense.\n• None A part of a document is frequently updated and keeps getting longer, while the remainder of the document is relatively static.\n• None The document size exceeds MongoDB’s 16MB document limit. This could occur when modeling many:1 relationships, such as product reviews:product, for example.\n\nBest Practice #4: Determine memory use (easier in Atlas)\n\nAs with most databases, MongoDB performs best when an application’s working set (e.g., indices and frequently accessed data) fits in memory without issue. While other factors play a part in performance, RAM size is obviously the most important consideration for instance sizing.\n\nWhen an application’s working set fits in RAM, read activity from the disk should be low. But if your working set exceeds the RAM of the instance size or server, read activity will begin to shoot up. If you notice this happening, you may be able to solve the problem by moving over to a larger instance with more memory.\n\nOr you could partition (shard) your database across multiple servers (more on this later).\n\nProperly sizing your working set is crucial whether you’re running MongoDB Atlas or managing MongoDB yourself. If you’re using Atlas, remember to always review your Atlas sizing and tier selection documentation to properly calculate the size of your working set.\n\nIt’s worth pointing out that in MongoDB Atlas, scaling memory is automated and straightforward. You can opt into cluster tier auto-scaling, for example, which automatically adjusts compute capacity in response to real-time changes in application demands.\n\nNo discussion of increased performance or horizontal scaling is complete without mentioning replication, which increases data availability via horizontal scaling. Replication can lead to better performance and also offers redundancy, which provides more security.\n\nIn MongoDB, replication is accomplished via replica sets that allow developers to copy data from a primary server or node across multiple secondaries. This allows your application to run some queries on secondaries instead of the primary, avoiding contention and leading to improved load balancing.\n• None Redundancy and data availability: Replication can be very helpful in case of disastrous events like hardware failures or server crashes. Should a primary node fail, an election process will automatically elect a new primary node from the remaining secondary nodes.\n• None Load sharing: Replica sets offer better scalability for your applications. As an example, developers can configure their applications to read from multiple servers to help with load balancing among replica sets.\n• None Data locality: In terms of performance, replication also improves latency for read usage. If you have the same data spread out across multiple servers, that data can be accessed at the location closest to the end user.\n\nSharded clusters in MongoDB are another way to potentially improve performance. Like replication, sharding is a way to distribute large data sets across multiple servers. Using what’s called a shard key, developers can copy pieces of data (or “shards”) across multiple servers. These servers work together to utilize all of the data.\n\nSharding comes with several advantages, including horizontal scaling for reads/writes as well as increased storage capacity and higher availability.\n\nRemember that best practices are constantly improving\n\nOnce again, this is a non-comprehensive list of the top performance best practices for MongoDB. As any experienced database developer will tell you, there are a thousand and one things you can do to improve performance—and they all depend on your exact application.\n\nMore importantly, always remember that appropriate data modeling, indexing, embedding, and referencing are basic considerations. Assuming you know the query patterns of your application well, you’ll find that you can get solid performance and a lot of extra mileage out of the distributed and replicated nature of MongoDB.\n\nIf all else fails...did we mention that MongoDB Atlas also has a built-in Performance Advisor? It can make your life a whole lot easier if you’re not sure where to start.\n• How to Create a Database in MongoDB"
    },
    {
        "link": "https://mongodb.com/developer/products/mongodb/mongodb-schema-design-best-practices",
        "document": "Have you ever wondered, \"How do I model a schema for my application?\" It's one of the most common questions devs have pertaining to MongoDB. And the answer is, . This is because document databases have a rich vocabulary that is capable of expressing data relationships in more nuanced ways than SQL. There are many things to consider when picking a schema. Is your app read or write heavy? What data is frequently accessed together? What are your performance considerations? How will your data set grow and scale?\n\nNow, with a schema that could potentially be saving thousands of sub parts, we probably do not need to have all of the data for the parts on every single request, but it's still important that this relationship is maintained in our schema. So, we might have a Products collection with data about each product in our e-commerce store, and in order to keep that part data linked, we can keep an array of Object IDs that link to a document that has information about the part. These parts can be saved in the same collection or in a separate collection, if needed. Let's take a look at how this would look.\n\nAs you can see, there are a ton of different ways to express your schema design, by going beyond normalizing your data like you might be used to doing in SQL. By taking advantage of embedding data within a document or referencing documents using the $lookup operator, you can make some truly powerful, scalable, and efficient database queries that are completely unique to your application. In fact, we are only barely able to scratch the surface of all the ways that you could model your data in MongoDB. If you want to learn more about MongoDB schema design, be sure to check out our continued series on schema design in MongoDB:"
    },
    {
        "link": "https://blog.panoply.io/mongodb-best-practices",
        "document": "MongoDB database is really popular these days. Developers often use it instead of MySQL, but these two platforms aren’t in direct competition. While MySQL is a relational database, MongoDB is a NoSQL document-oriented database, so the two work quite differently. And for that reason, optimizing MongoDB is not the same as optimizing a traditional relational database, although some best practices are similar. Read on to learn what to do and what to avoid when using MongoDB.\n\nLet's start with the most important difference: the schema. Designing your database schema is a crucial task, and while making changes is possible and common, it can be expensive from an engineering perspective. When a database schema needs changes, your deployment process becomes much more complicated, so good design is critical. How do you design a good MongoDB database schema? Rule number one: don't design it as you would with relational databases. It sounds logical to split your schema into small table-like pieces, right?\n\nIn the case of MongoD, no. For relational databases, you usually construct a schema based on the data. You need to figure out how to split the data your application will use into tables so it’s logically organized and not duplicated. But when it comes to a MongoDB schema, you should look not at the data itself, but at the application. Specifically, how your application will use the data, what kind of queries it will likely execute, and so on. This means that two different applications using the exact same data might have very different schema designs in MongoDB, whereas for relational databases the schema would probably be the same or very similar across applications.\n\nAnother thing you need to know is that MongoDB has almost no rules or guidelines on how you should structure the data, because MongoDB operates on JSON-like documents. This gives you the ability to embed data into arrays and objects within one document. If you want to learn more about modelling data, take a look at this free course from MongoDB.\n\nEmbed Your Data Instead of Relying on Joins\n\nOne of the best practices when using MongoDB is to embed your data within one document instead of performing lookups or creating in-application joins. It may be a bit counterintuitive, but MongoDB performs better when you stuff all the data you need into one document. For example, instead of putting user details in one document and user order history in another, chuck them into the same one. Reading documents is extremely fast in MongoDB. Performing lookups or joins within the application is slower in most cases.\n\nKeep in mind that this is only a general rule and you should always start by understanding your application query pattern. Including the data in the document is preferred over lookup operations. But, of course, there is no point in dumping all possible data in one document.\n\nLet's talk about indexes. This next MongoDB best practice is similar to what you'd do with relational databases. In the previous best practice we mentioned that MongoDB prefers to embed data (instead of splitting it into smaller logical pieces). Therefore it's normal for MongoDB documents to become quite big. This will naturally impact performance, but indexes can solve that. Indexes in MongoDB work pretty much the same way as with relational databases. These special data structures store a small subset of the whole document in order to speed up the matching of data for frequently used queries.\n\nFor example, imagine that you have your user's data together with their order history in a single document and you want to find all users who ordered something in the last month. Normally (without indexes) MongoDB would have to scan the whole user collection, going one by one through the user document and checking the last order data for each user. It's not horrible; that's how the database performs a lot of operations. But if you frequently ask the database for this kind of matching, then indexing will help you a lot. Coming back to our example - with indexes, MongoDB stores a separate, small list containing pointers to the data (for example user id, email address, or last order date).\n\nIt may sound obvious, but server RAM sizing in MongoDB is crucial. There are two things to keep in mind: first, more memory won't increase the performance of your database. It's not just a matter of getting the server with the most RAM memory you can afford. Second, MongoDB performs best when its working set can fit an application's RAM.\n\nSizing your MongoDB machine is not dependent on the size of the database itself. It doesn't matter if you have 100MB or 2TB of data in your MongoDB instance. What matters is the size of indices and frequently accessed data. To size your MongoDB instance, you need to perform some tests to find out how much data your application normally uses. Then, make sure to use a server with slightly more memory than that. If your working set won't fit in the RAM, MongoDB will read the data from disk. And even if you use superfast SSD disks, the operation will be much slower than reading from RAM.\n\nSo, how do you know if your MongoDB working set fits in your RAM? The simplest way is to execute MongoDB's serverStatus command. From there, take a look at the pages-read-into-cache and unmodified-pages-evicted metrics. If you see high numbers in these two, it most likely means that your working set does not fit in your RAM memory.\n\nAs with relational databases, another MongoDB best practice is to use replication and/or sharding when your database becomes slow. MongoDB implements replication by use of replica sets, and works similarly to other database systems using primary and secondary nodes. You can instruct your application to run some queries on secondary servers (or use load balancers), relieving some pressure on your primary server.\n\nWhat's good about MongoDB replication is that it also serves as a great redundancy mechanism. Since it simply copies documents from primary to secondary nodes, electing one of the secondary nodes to be a primary in case your original primary server fails is simple. You won't run into any inconsistencies or complicated election processes with MongoDB. Therefore, replicating your MongoDB is good not only for better performance, but for redundancy.\n\nReplication helps the most for small and medium databases, so once your dataset gets really big, consider sharding. Although replication just copies all the data across multiple servers, sharding actually splits the data into smaller pieces and distributes them across servers. This brings great performance improvement for large data sets and allows you to horizontally scale both reads and writes. You can read more about how it works here.\n\nAs you can see, MongoDB best practices are a mix of typical database best practices and some specific to MongoDB. The nice thing about MongoDB is that you don’t need to start worrying about performance until you have a relatively big database - it’s fast and optimized by design. This doesn't mean you should ignore best practices when working with smaller databases. Some of the best practices we mentioned aren’t just for boosting performance, but can ensure good database design. They should always be top of mind no matter the size of the database.\n\nIf you want to learn more about the differences between SQL and NoSQL databases, take a look at our blog post here."
    },
    {
        "link": "https://edwinsiby.medium.com/mongodb-nosql-database-7317bf7a34a1",
        "document": "Mastering MongoDB: A Comprehensive Guide to Database Operations, Querying, Updates, Aggregation, Indexing, Transactions, Validation, Sharding, and More\n\nSQL (Structured Query Language) and NoSQL (Not Only SQL) are two different types of database management systems, each with its own characteristics and use cases.\n• SQL databases are based on a structured data model called the relational model.\n• Data in SQL databases is organized into tables with predefined schemas, consisting of rows and columns.\n• They use SQL as the standard query language for defining and manipulating data.\n• SQL databases are suitable for applications that require complex transactions, strict data integrity, and a fixed schema.\n• NoSQL databases are designed to handle unstructured, semi-structured, or polymorphic data.\n• They provide flexible and scalable data models that are not limited to the traditional tabular structure of SQL databases.\n• NoSQL databases can be categorized into different types, including document databases (like MongoDB), key-value stores, wide-column stores, and graph databases.\n• They are well-suited for use cases involving large-scale data storage, real-time analytics, content management systems, and scenarios where the data structure may evolve over time.\n• SQL databases are often used in applications that require complex querying, transactions, and strong data consistency. They are commonly employed in financial systems, e-commerce platforms, and applications with well-defined schemas.\n• NoSQL databases are a popular choice for applications with rapidly changing requirements, large amounts of unstructured data, and a need for horizontal scalability. They are commonly used in social media platforms, IoT (Internet of Things) applications, content management systems, and real-time analytics.\n\nIt’s important to note that the choice between SQL and NoSQL databases depends on various factors such as the nature of your data, scalability requirements, performance needs, development flexibility, and the specific use case of your application. Some projects may even combine both SQL and NoSQL databases, leveraging their strengths for different parts of the system.\n\nNow Let us jump into mongoDB\n\nMongoDB is a popular document-oriented NoSQL database that provides high performance, scalability, and flexibility for storing and managing data. It differs from traditional SQL databases in its data model and query language. Here are some key aspects of MongoDB:\n• MongoDB stores data in flexible, self-describing documents using a format called BSON (Binary JSON). BSON documents are similar to JSON objects and can have varying structures.\n• Documents in MongoDB are organized into collections, which are analogous to tables in SQL databases. However, unlike tables, collections do not enforce a fixed schema. Each document can have its own unique structure.\n• Schema Flexibility: MongoDB’s flexible schema allows for easy and dynamic changes to the data model without requiring migrations or downtime. This makes it well-suited for applications with evolving requirements.\n• High Performance: MongoDB is designed to handle large amounts of data and high throughput. It supports horizontal scaling by distributing data across multiple servers and provides built-in sharding capabilities.\n• Rich Query Language: MongoDB provides a powerful and expressive query language for retrieving data from collections. The query language supports a wide range of operators and can handle complex filtering, sorting, and aggregation operations.\n• Indexing: MongoDB supports various types of indexes to improve query performance. Indexes can be created on single fields, compound fields, arrays, and even text searches. They help optimize query execution and facilitate efficient data retrieval.\n• Replication and High Availability: MongoDB offers built-in replication features for ensuring data redundancy and high availability. Replication creates multiple copies of data across different servers, providing fault tolerance and automatic failover in case of server failures.\n• Geospatial and Text Search: MongoDB includes support for geospatial indexing and queries, allowing you to store and query location-based data efficiently. Additionally, MongoDB offers text search capabilities for performing full-text searches on textual data.\n• Aggregation Framework: MongoDB’s Aggregation Framework provides powerful data aggregation and transformation capabilities. It allows you to perform complex data manipulations, such as grouping, filtering, joining, and statistical operations, directly within the database.\n• Native Drivers and Ecosystem: MongoDB provides official drivers for a wide range of programming languages, making it easy to integrate MongoDB into your application stack. Additionally, there is a vibrant ecosystem around MongoDB with various tools, frameworks, and community support.\n\nThese are just some of the features that make MongoDB a popular choice among developers. It’s important to note that while MongoDB offers a lot of flexibility, it also requires careful design and consideration to ensure optimal performance and scalability for your specific use case.\n\nNow let us install MongoDB on Linux\n\nCreating a Database: MongoDB automatically creates a database when you first store data in it. To switch to a specific database or create a new one, you can use the command. For example, will switch to the \"mydatabase\" database or create it if it doesn't exist.\n• Creating a Collection: Collections in MongoDB are created implicitly when you insert data into them. For example, if you insert a document into a collection named “mycollection”, MongoDB will create the collection if it doesn’t already exist.\n• you can insert a document into a collection that doesn’t exist. For instance, let’s say you want to create a collection called “mycollection” and insert a document into it. Use the or method to insert data. Here's an example using :\n• Inserting Documents: To insert data into a collection, you can use the insertOne() or insertMany() methods. For example, db.mycollection.insertOne({ name: ‘John’, age: 30 }) will insert a single document into the “mycollection” collection.\n\nMongoDB provides a rich query language to retrieve documents from a collection. You can use the find() method to retrieve documents based on various query conditions. For example, db.mycollection.find({ name: ‘John’ }) will retrieve all documents from the “mycollection” collection where the name is “John”.\n\nThis will retrieve all documents in the “mycollection” collection.\n\nThis query will retrieve all documents where the “country” field is set to “India”.\n\nThis query will retrieve documents where the “age” field is greater than or equal to 25 and the “gender” field is “male”. use $lte for less than or equal to. also you can use $lt and $gt\n\n4. Find documents with an array field matching a specific value:\n\nThis query will retrieve documents where the “skills” field contains “javascript” in its array.\n\nThis query will retrieve documents where the nested field “language.english” is set to true.\n\nThis query will retrieve documents where either the “age” field is less than 30 or the “country” field is “USA”.\n\nThese are just a few examples of how you can query documents in MongoDB. You can combine various conditions, operators, and projection options to tailor your queries to specific requirements.\n\nRemember to refer to the MongoDB documentation for a comprehensive list of query operators and capabilities: MongoDB Query Operators\n\nUpdating documents in MongoDB allows you to modify existing data within a collection. MongoDB provides methods like and to update documents based on specific criteria. Here are a few examples of updating documents:\n\nThis query will update the first document that matches the condition and set the \"age\" field to 31 using the update operator.\n\nThis query will update all documents that match the condition and increment the \"age\" field by 1 using the update operator.\n\nThis query will update the document with the name “Alice” and set the nested field “language.french” to true using the update operator.\n\nThis query will update the document with the name “John” and add “MongoDB” to the “skills” array using the update operator.\n\nThis query will replace the document that matches the condition with a new document containing the specified fields. Note that the entire document will be replaced, so any fields not included in the new document will be removed. here you will lose the other fields like skills, and language. find why\n\nThese are just a few examples of how you can update documents in MongoDB. MongoDB provides various update operators such as , , , , , and more, allowing you to perform specific modifications to the documents.\n\nRemember to refer to the MongoDB documentation for a comprehensive list of update operators and their usage: MongoDB Update Operators\n\nDeleting documents in MongoDB allows you to remove specific data from a collection. MongoDB provides methods like and to delete documents based on specific criteria. Here are a few examples of deleting documents:\n\nThis query will delete the first document that matches the condition .\n\nThis query will delete all documents that match the condition .\n\nThis query will delete documents where the “age” field is less than 30 and the “gender” field is “male”.\n\nThis query will delete documents where the “skills” field contains “javascript” in its array.\n\nThis query will delete documents where the nested field “language.english” is set to false.\n\nThese are just a few examples of how you can delete documents in MongoDB. The method deletes a single document that matches the specified condition, while the method deletes multiple documents that match the condition.\n\nRemember to exercise caution when deleting documents, as it permanently removes the data from the collection.\n\nAggregation in MongoDB allows you to perform advanced data processing operations on the documents within a collection. It enables you to transform, filter, group, and analyze your data in a powerful and flexible way. MongoDB provides the method to perform aggregation operations using a pipeline of stages. Here's an overview of the aggregation pipeline stages and their usage:\n\nThis example filters documents to include only those with an “age” greater than 30.\n\nThis example groups documents by the “country” field and calculates the count of documents in each country.\n\nThis example includes only the “name” and “age” fields in the output documents.\n\n4. $sort: Sorts the documents based on a specified field or fields.\n\nThis example sorts documents in descending order based on the “age” field. Use age: 1 for ascending order.\n\n5. $limit: Limits the number of documents in the output.\n\nThis example limits the output to only the first 10 documents.\n\n6. $skip: Skips a specified number of documents in the output.\n\nThis example skips the first 5 documents in the output.\n\n7. $lookup: operation that allows you to perform a left outer join between two collections.\n\nThis code performs a lookup operation using the stage in MongoDB's aggregation framework. It joins the collection with the collection based on the matching fields. The matched documents from the collection are added as an array field named in the output documents of the aggregation.\n\nThese are just a few examples of the aggregation stages available in MongoDB. You can combine multiple stages in a pipeline to perform complex data transformations and analytics.\n\nRefer to the MongoDB documentation for a comprehensive list of aggregation pipeline stages and operators: MongoDB Aggregation\n\nIndexing in MongoDB is a technique used to optimize query performance by creating additional data structures that allow for efficient data retrieval. Indexes in MongoDB are similar to indexes in other database systems and help speed up query execution by reducing the number of documents that need to be scanned.\n• Index Types: MongoDB supports various index types, including the default B-tree index ( ), multikey indexes (for arrays and nested arrays), geospatial indexes, text indexes, hashed indexes, and more. Each index type has its own characteristics and is suitable for different use cases.\n\n2. Creating Indexes: You can create indexes using the method. For example, to create an index on the \"name\" field of a collection, you can use the following command:\n\n3. Indexing Compound Fields: MongoDB allows you to create indexes on multiple fields together. This is known as a compound index. For example, to create a compound index on the “name” and “age” fields, you can use the following command:\n\n3. Indexing Nested Fields: MongoDB also supports indexing on nested fields within documents. You can create indexes on specific nested fields to improve query performance on those fields.\n\n4. Query Optimization: Indexes help speed up queries by allowing MongoDB to quickly locate and retrieve relevant documents based on the specified search criteria. When a query matches an index, MongoDB can use the index to efficiently navigate to the desired data rather than scanning the entire collection.\n\n5. Index Selection: MongoDB automatically chooses the most suitable index to use based on the query predicates and available indexes. However, you can use the method to force MongoDB to use a specific index for a query.\n\n6. Index Management: You can view existing indexes in a collection using the method and drop an index using the method.\n\nSuppose you have a collection named with an index created on the field. To query for a specific user by their email, you can use the following command:\n\nIn this query, MongoDB will utilize the index on the field to quickly locate the document(s) with the matching email address. This results in faster query execution compared to scanning the entire collection.\n\nIt’s important to note that for MongoDB to utilize an index effectively, the query must include the indexed field(s) in the query predicates. Queries that do not include the indexed field(s) may not benefit from the index and may require scanning the entire collection.\n\nAdditionally, you can use the method to obtain information about the query execution plan, including whether an index is used, the number of documents examined, and other relevant details. This can help you analyze the query performance and index usage.\n\nThis will provide detailed information about the query plan, including the index usage.\n\nTransactions in MongoDB provide a way to perform multiple operations as a single atomic unit of work. Transactions ensure that either all the operations within the transaction are applied or none of them are, maintaining data consistency in the process. Here’s an overview of working with transactions in MongoDB:\n• Starting a Transaction: You can start a transaction using the method, which returns a session object to work with. Here's an example:\n\n2. Performing Operations: Once a transaction is started, you can perform operations within the session. Any read, write, or other database operations can be included in the transaction. For example:\n\n3. Committing a Transaction: If all operations within the transaction are successful and you want to persist the changes, you can commit the transaction using the method. Here's an example:\n\n4. Aborting a Transaction: If an error occurs or you want to discard the changes made in the transaction, you can abort the transaction using the method. Here's an example:\n\n5. Error Handling: Transactions can encounter errors during their execution. You can handle these errors using try-catch blocks and implement appropriate error-handling logic to ensure transactional integrity.\n\nIt’s important to note that transactions in MongoDB are only supported in replica sets and sharded clusters running with MongoDB 4.0 or later, using the WiredTiger storage engine.\n\nTransactions are useful in scenarios where multiple operations need to be executed together to maintain data consistency. For example, when transferring funds between two accounts, you can use a transaction to ensure that the debit and credit operations occur atomically.\n\nValidation in MongoDB allows you to enforce data integrity by defining rules that validate the structure and content of documents upon insertion or update. You can use validation rules to ensure that documents meet specific requirements, such as field presence, data types, value ranges, and more. Here’s an overview of working with document validation in MongoDB:\n• Enabling Document Validation: To enable document validation for a collection, you need to specify a validation rule during the collection creation or modification. You can define the validation rule using the option. For example:\n\n2. Validation Rules: MongoDB uses the JSON Schema validation format to define validation rules. You can specify the document structure, required fields, data types, value constraints, and more using JSON Schema syntax.\n\n3. Validation Actions: MongoDB provides three options to handle documents that fail validation:\n• “error”: The default option, which rejects any documents that violate the validation rule and returns an error.\n• “warn”: Allows the insertion or update of documents that violate the validation rule but logs a warning message.\n\n4. Updating Validation Rules: You can modify the validation rules for an existing collection using the command. For example, to add a new field to the validation rule:\n\n5. Validation Errors: When a document fails validation, an error is raised, and the document is rejected or logged based on the configured validation action. The error message provides details about the validation failure, such as the field name, value, and the specific validation rule that was violated.\n\nDocument validation helps maintain data integrity and consistency by ensuring that documents adhere to predefined rules. It’s particularly useful when working with applications that have complex data models and require strict data validation.\n\nSharding in MongoDB is a technique used for distributing data across multiple servers or machines to handle large amounts of data and high read/write workloads. Sharding allows you to horizontally scale your MongoDB infrastructure by partitioning data and distributing it among multiple shards (servers or machines), thereby improving performance and capacity.\n\nHere are the key concepts and steps involved in sharding:\n• Shard: A shard is a separate MongoDB server or replica set that stores a portion of the data. Each shard can be hosted on a separate machine or server. Shards collectively hold the entire dataset of the sharded collection.\n• Sharded Collection: A sharded collection is a MongoDB collection that is partitioned and distributed across multiple shards. Each document in the collection is assigned a shard key, which determines the shard on which the document is stored.\n• Shard Key: The shard key is a field or a set of fields in the document that MongoDB uses to determine the shard on which the document is stored. The shard key should be carefully chosen to evenly distribute data across shards and support the application’s query patterns.\n• Config Servers: Config servers are MongoDB servers that store the metadata and configuration information for the sharded cluster. They keep track of which shard stores which portion of the data and handle the coordination of the sharded cluster.\n• Router (mongos): The MongoDB router, also known as the process, is responsible for receiving client requests, routing them to the appropriate shard(s), and aggregating the results from multiple shards.\n\nTo enable sharding and distribute data across shards, you need to follow these general steps:\n• Set up the Config Servers: Deploy a set of config servers that will store the metadata and configuration information for the sharded cluster. Typically, a config server replica set is recommended for high availability.\n• Enable Sharding for a Database: Connect to a MongoDB instance, and use the command to enable sharding for a specific database. This step creates the necessary metadata and configures the cluster to support sharding for that database.\n• Choose a Shard Key: Select an appropriate shard key based on your data and query patterns. The shard key should distribute data evenly and support efficient querying.\n• Shard the Collection: Use the command to shard the desired collection within the database. This command specifies the database, collection, and the shard key. MongoDB will distribute the existing data across shards according to the shard key.\n• Scale the Cluster: As the data and workload grow, you can add more shards to the cluster to distribute the data further and increase capacity. MongoDB provides commands to add new shards and balance the data across the cluster.\n\nSharding allows you to horizontally scale your MongoDB deployment, handle large amounts of data, and support high write and read workloads. It provides scalability, fault tolerance, and improved performance by distributing data across multiple servers.\n\nCongratulations on learning about various aspects of MongoDB! You’ve covered many essential topics. However, there are still some additional areas you might consider exploring to deepen your understanding:\n• Replication: MongoDB replication involves creating multiple copies of your data across multiple servers. It provides redundancy and high availability by allowing automatic failover in case of primary node failure. Replication ensures data durability and minimizes downtime. You can learn about configuring replica sets, electing primary nodes, and handling replication-related scenarios.\n• Backup and Restore: Understanding MongoDB’s backup and restore mechanisms is crucial for data protection and disaster recovery. Explore different backup strategies, such as point-in-time backups and snapshots, and learn how to restore data from backups.\n• Security: MongoDB offers various security features to protect your data, including authentication, access control, encryption, and auditing. Dive into configuring user authentication, managing roles and privileges, enabling TLS/SSL encryption, and implementing auditing to ensure your MongoDB deployment is secure.\n• Schema Design: Designing effective schemas is essential for optimizing query performance and scalability. Learn about data modeling best practices, including denormalization, embedding vs. referencing data, schema versioning, and handling schema migrations.\n• Geospatial Queries: MongoDB provides robust support for geospatial data and allows you to perform spatial queries, such as finding points within a specific radius, calculating distances, and working with complex geometries. Explore the geospatial features and learn how to leverage them in your applications.\n• MongoDB Atlas: MongoDB Atlas is a fully managed cloud database service provided by MongoDB. It offers automated deployment, scaling, monitoring, and backup features, allowing you to focus on application development rather than managing infrastructure. Learn about setting up and using MongoDB Atlas for your projects.\n\nRemember that MongoDB is a versatile database system, and there is always more to explore based on your specific use cases and requirements. The MongoDB documentation and online resources provide in-depth information on these topics and more.\n\nHappy learning, and feel free to comment if you have any further questions!"
    },
    {
        "link": "https://mongodb.com/blog/post/performance-best-practices-hardware-and-os-configuration",
        "document": "Welcome to the sixth in a series of blog posts covering performance best practices for MongoDB.\n\nIn this series, we are covering key considerations for achieving performance at scale across a number of important dimensions, including:\n\nIf you are running MongoDB on Atlas, our fully-managed and global cloud database service, then many of the considerations in this section are taken care of for you. You should refer to the Atlas Sizing and Tier Selection documentation for guidance on sizing.\n\nIf you are running MongoDB yourself, then this post will be useful for you.\n\nBeyond Atlas, you can run MongoDB on a variety of operating systems and processor architectures – from 64 bit x86 and ARM CPUs, to IBM POWER and mainframe systems. Refer to the supported platforms section of the documentation for the latest hardware and OS support matrices.\n\nAs stated in the very first blog post of this series, MongoDB performs best when the application’s working set (indexes and most frequently accessed data) fits in memory. RAM size is the most important factor for instance sizing; other optimizations may not significantly improve the performance of the database if there is insufficient RAM.\n\nRefer to Part 1 of the series for more information.\n\nMongoDB’s WiredTiger storage engine architecture is capable of efficiently using multiple CPU cores. Typically a single client connection is represented by its own thread. In addition background worker threads perform tasks like checkpointing and cache eviction. You should provision an adequate number of CPU cores in proportion to concurrent client connections. Note that typically investing in more RAM and disk IOPS gives the highest benefit to database performance.\n\nIn MongoDB Atlas, the number of CPU cores and concurrent client connections is a function of your chosen cluster tier. Review the documentation to see the current limits.\n\nDedicate Each Server to a Single Role in the System\n\nWith appropriate sizing and resource allocation using virtualization or container technologies, multiple MongoDB processes can safely run on a single physical server without contending for resources.\n\nFor some use cases (multi-tenant, microsharding) users deploy multiple MongoDB processes on the same host. In this case you will have to make several configuration changes to make sure each process has sufficient resources.\n\nFor availability, multiple members of the same replica set should not be co-located on the same physical hardware or share any single point of failure such as a power supply or network switch.\n\nThe size of WiredTiger storage engine’s internal cache is tunable through the setting and should be large enough to hold your entire working set. If the cache does not have enough space to load additional data, WiredTiger evicts pages from the cache to free up space.\n\nBy default, is set to 50% of the available RAM, minus 1 GB. Caution should be taken if raising the value as it takes resources from the OS, and WiredTiger performance can actually degrade as the filesystem cache becomes less effective. Note that MongoDB itself will also allocate memory beyond the WiredTiger cache.\n\nAlso, as MongoDB supports variable sized records and WiredTiger creates variable sized pages, some memory fragmentation is expected and will consume memory above the configured cache size.\n\nMultiple processes [query routers] should be spread across multiple servers. You should use at least as many mongos processes as there are shards. MongoDB Atlas automatically provisions a query router for each shard in your cluster.\n\nRunning MongoDB on a system with Non-Uniform Memory Access (NUMA) can cause a number of operational problems, including slow performance for periods of time, inability to use all available RAM, and high system process usage.\n\nWhen running MongoDB servers and clients on NUMA hardware, you should configure a memory interleave policy using the command.\n\nAs a distributed database, MongoDB relies on efficient network transport during query routing and inter-node replication. Based on the snappy compression algorithm, network traffic across a MongoDB cluster can be compressed by up to 80%, providing major performance benefits in bandwidth-constrained environments, and reducing networking costs.\n\nAdd the parameter to the connection string to enable compression:\n\nWhile MongoDB performs all read and write operations through in-memory data structures, data is persisted to disk, and queries on data not already in RAM trigger a read from disk. As a result, the performance of the storage sub-system is a critical aspect of any system.\n\nMost disk access patterns in MongoDB do not have sequential properties, and as a result, you may experience substantial performance gains by using SSDs.\n\nGood results and strong price to performance have been observed with SATA, PCIe, and NVMe SSDs. Rather than spending more on expensive spinning drives, that money may be more effectively spent on more RAM or SSDs. SSDs should also be used for read heavy applications if the working set no longer fits in memory.\n\nMost MongoDB deployments should use RAID-10 storage configurations. RAID-5 and RAID-6 have limitations and may not provide sufficient performance. MongoDB's replica sets allow deployments to provide stronger availability for data, and should be considered with RAID and other factors to meet the desired availability SLA. You don't need to buy SAN disk arrays for high availability.\n\nUse MongoDB’s Default Compression for Storage and I/O-Intensive Workloads\n\nThe default snappy compression reduces storage footprint typically by 50% or more, and enables higher IOPs as fewer bits are read from disk. As with any compression algorithm, you trade storage efficiency for CPU overhead, and so it is important to test the impacts of compression in your own environment.\n\nMongoDB offers you a range of compression options for both documents and indexes. The snappy compression algorithm provides a balance between high document compression ratios (typically around 50%+, dependent on data types) with low CPU overhead, while the optional zStandard and zlib libraries will achieve higher compression, but incur additional CPU cycles as data is written to and read from disk. zStandard was introduced with the MongoDB 4.2 release and is recommended over the existing zLib library due to lower CPU overhead.\n\nIndexes use prefix compression by default, which serves to reduce the in-memory footprint of index storage, freeing up more of the RAM for frequently accessed documents. Testing has shown a typical 50% compression ratio using the prefix algorithm, though users are advised to test with their own data sets.\n\nYou can modify the default compression settings for all collections and indexes. Compression is also configurable on a per-collection and per-index basis during collection and index creation.\n\nSet the readahead setting between 8 and 32. Use the command to set the block size\n\nUse of XFS is strongly recommended to avoid performance issues that have been observed when using EXT4 with WiredTiger.\n\nSome file systems will maintain metadata for the last time a file was accessed. While this may be useful for some applications, in a database it means that the file system will issue a write every time the database accesses a page, which will negatively impact the performance and throughput of the system.\n\nTransparent hugepages can add additional memory pressure and CPU utilization and have negative performance implications for swapping.\n\nFor the latest guidance on hardware and OS configuration, review the MongoDB Production Notes and the Operations Checklist.\n\nThat wraps up this installment of the performance best practices series. In our final post we will cover benchmarking."
    },
    {
        "link": "https://dlcdn.apache.org/spark/docs/3.3.0/mllib-collaborative-filtering.html",
        "document": "Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix. currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. uses the alternating least squares (ALS) algorithm to learn these latent factors. The implementation in has the following parameters:\n• numBlocks is the number of blocks used to parallelize computation (set to -1 to auto-configure).\n• rank is the number of features to use (also referred to as the number of latent factors).\n• iterations is the number of iterations of ALS to run. ALS typically converges to a reasonable solution in 20 iterations or less.\n• implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data.\n• alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations.\n\nThe standard approach to matrix factorization-based collaborative filtering treats the entries in the user-item matrix as explicit preferences given by the user to the item, for example, users giving ratings to movies.\n\nIt is common in many real-world use cases to only have access to implicit feedback (e.g. views, clicks, purchases, likes, shares etc.). The approach used in to deal with such data is taken from Collaborative Filtering for Implicit Feedback Datasets. Essentially, instead of trying to model the matrix of ratings directly, this approach treats the data as numbers representing the strength in observations of user actions (such as the number of clicks, or the cumulative duration someone spent viewing a movie). Those numbers are then related to the level of confidence in observed user preferences, rather than explicit ratings given to items. The model then tries to find latent factors that can be used to predict the expected preference of a user for an item.\n\nSince v1.1, we scale the regularization parameter in solving each least squares problem by the number of ratings the user generated in updating user factors, or the number of ratings the product received in updating product factors. This approach is named “ALS-WR” and discussed in the paper “Large-Scale Parallel Collaborative Filtering for the Netflix Prize”. It makes less dependent on the scale of the dataset, so we can apply the best parameter learned from a sampled subset to the full dataset and expect similar performance.\n\nIn the following example, we load rating data. Each row consists of a user, a product and a rating. We use the default ALS.train() method which assumes ratings are explicit. We evaluate the recommendation model by measuring the Mean Squared Error of rating prediction. Refer to the Scala docs for more details on the API. Find full example code at \"examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\" in the Spark repo. If the rating matrix is derived from another source of information (i.e. it is inferred from other signals), you can use the method to get better results. All of MLlib’s methods use Java-friendly types, so you can import and call them there the same way you do in Scala. The only caveat is that the methods take Scala RDD objects, while the Spark Java API uses a separate class. You can convert a Java RDD to a Scala one by calling on your object. A self-contained application example that is equivalent to the provided example in Scala is given below: Refer to the Java docs for more details on the API. Find full example code at \"examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\" in the Spark repo. In the following example we load rating data. Each row consists of a user, a product and a rating. We use the default ALS.train() method which assumes ratings are explicit. We evaluate the recommendation by measuring the Mean Squared Error of rating prediction. Refer to the Python docs for more details on the API. \\ # Build the recommendation model using Alternating Least Squares Find full example code at \"examples/src/main/python/mllib/recommendation_example.py\" in the Spark repo. If the rating matrix is derived from other source of information (i.e. it is inferred from other signals), you can use the trainImplicit method to get better results. # Build the recommendation model using Alternating Least Squares based on implicit ratings\n\nIn order to run the above application, follow the instructions provided in the Self-Contained Applications section of the Spark Quick Start guide. Be sure to also include spark-mllib to your build file as a dependency.\n\nThe training exercises from the Spark Summit 2014 include a hands-on tutorial for personalized movie recommendation with ."
    },
    {
        "link": "https://downloads.apache.org/spark/docs/3.3.1/ml-collaborative-filtering.html",
        "document": ""
    },
    {
        "link": "https://github.com/recommenders-team/recommenders/blob/main/examples/02_model_collaborative_filtering/als_deep_dive.ipynb",
        "document": "To see all available qualifiers, see our documentation .\n\nSaved searches Use saved searches to filter your results more quickly\n\nWe read every piece of feedback, and take your input very seriously.\n\nYou signed in with another tab or window. Reload to refresh your session.\n\nYou signed out in another tab or window. Reload to refresh your session.\n\nYou switched accounts on another tab or window. Reload to refresh your session."
    },
    {
        "link": "https://bigdatawire.com/2018/11/02/movie-recommendations-with-spark-collaborative-filtering",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/26213573/apache-spark-als-collaborative-filtering-results-they-dont-make-sense",
        "document": "I wanted to try out Spark for collaborative filtering using MLlib as explained in this tutorial: https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html The algorithm is based on the paper \"Collaborative Filtering for Implicit Feedback Datasets\", doing matrix factorization.\n\nEverything is up and running using the 10 million Movielens data set. The data set it split into 80% training 10% test and 10% validation.\n• RMSE (validation) = 0.8057135933012889 for the model trained with rank = 24, lambda = 0.1, and Iterations = 10.\n• The best model improves the baseline by 23.94%.\n\nWhich are values similar to the tutorial, although with different training parameters.\n\nI tried running the algorithm several times and always got recommendations that don't make any sense to me. Even rating only kids movies I get the following results:\n• Who's Singin' Over There? (a.k.a. Who Sings Over There) (Ko to tamo peva) (1980)\n• Please Vote for Me (2007)\n\nWhich except for Only Yesterday doesn't seem to make any sense.\n\nIf there is anyone out there who knows how to interpret those results or get better ones I would really appreciate you sharing your knowledge.\n\nAs suggested I trained another model with more factors:\n• RMSE (validation) = 0.8070339258049574 for the model trained with rank = 100, lambda = 0.1, and numIter = 10.\n• For the Bible Tells Me So (2007)\n\nNot one useful result.\n\nEDIT2: With using the implicit feedback method, I get much better results! With the same action movies as above the recommendations are:\n• Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)\n\nThat's more what I expected! The question is why the explicit version is so-so-so bad"
    },
    {
        "link": "https://flume.apache.org/FlumeUserGuide.html",
        "document": "Flume agent configuration is stored in one or more configuration files that follow the Java properties file format. Configurations for one or more agents can be specified in these configuration files. The configuration includes properties of each source, sink and channel in an agent and how they are wired together to form data flows. Each component (source, sink or channel) in the flow has a name, type, and set of properties that are specific to the type and instantiation. For example, an Avro source needs a hostname (or IP address) and a port number to receive data from. A memory channel can have max queue size (“capacity”), and an HDFS sink needs to know the file system URI, path to create files, frequency of file rotation (“hdfs.rollInterval”) etc. All such attributes of a component needs to be set in the properties file of the hosting Flume agent. The agent needs to know what individual components to load and how they are connected in order to constitute the flow. This is done by listing the names of each of the sources, sinks and channels in the agent, and then specifying the connecting channel for each sink and source. For example, an agent flows events from an Avro source called avroWeb to HDFS sink hdfs-cluster1 via a file channel called file-channel. The configuration file will contain names of these components and file-channel as a shared channel for both avroWeb source and hdfs-cluster1 sink. An agent is started using a shell script called flume-ng which is located in the bin directory of the Flume distribution. You need to specify the agent name, the config directory, and the config file on the command line: Now the agent will start running source and sinks configured in the given properties file. # Bind the source and sink to the channel This configuration defines a single agent named a1. a1 has a source that listens for data on port 44444, a channel that buffers event data in memory, and a sink that logs event data to the console. The configuration file names the various components, then describes their types and configuration parameters. A given configuration file might define several named agents; when a given Flume process is launched a flag is passed telling it which named agent to manifest. Note that in a full deployment we would typically include one more option: . The directory would include a shell script flume-env.sh and potentially a log4j configuration file. The original Flume terminal will output the event in a log message. As of version 1.10.0 Flume supports being configured using URIs instead of just from local files. Direct support for HTTP(S), file, and classpath URIs is included. The HTTP support includes support for authentication using basic authorization but other authorization mechanisms may be supported by specifying the fully qualified name of the class that implements the AuthorizationProvider interface using the –auth-provider option. HTTP also supports reloading of configuration files using polling if the target server properly responds to the If-Modified-Since header. As of version 1.10.0 Flume supports being configured from multiple configuration files instead of just one. This more easily allows values to be overridden or added based on specific environments. Each file should be configured using its own –conf-file or –conf-uri option. However, all files should either be provided with –conf-file or with –conf-uri. If –conf-file and –conf-uri appear together as options all –conf-uri configurations will be processed before any of the –conf-file configurations are merged. will cause flume.conf to be read first, override.conf to be merged with it and finally example.conf would be merged last. If it is desirec to have example.conf be the base configuration it should be specified using the –conf-uri option either as: Flume has the ability to substitute environment variables in the configuration. For example: a1.sources = r1 a1.sources.r1.type = netcat a1.sources.r1.bind = 0.0.0.0 a1.sources.r1.port = ${env:NC_PORT} a1.sources.r1.channels = c1 NB: it currently works for values only, not for keys. (Ie. only on the “right side” of the mark of the config lines.) As of version 1.10.0 Flume resolves configuration values using Apache Commons Text’s StringSubstitutor class using the default set of Lookups along with a lookup that uses the configuration files as a source for replacement values. Note the above is just an example, environment variables can be configured in other ways, including being set in . As noted, system properties are also supported, so the configuration: a1.sources = r1 a1.sources.r1.type = netcat a1.sources.r1.bind = 0.0.0.0 a1.sources.r1.port = ${sys:NC_PORT} a1.sources.r1.channels = c1 a1.sources = r1 a1.sources.r1.type = netcat a1.sources.r1.bind = 0.0.0.0 a1.sources.r1.port = ${NC_PORT} a1.sources.r1.channels = c1 Note that the method for specifying environment variables as was done in prior versions will stil work but has been deprecated in favor of using ${env:varName}. Instead of specifying all the command options on the command line as of version 1.10.0 command options may be placed in either /etc/flume/flume.opts or flume.opts on the classpath. An example might be: conf-file = example.conf conf-file = override.conf name = a1 Logging the raw stream of data flowing through the ingest pipeline is not desired behavior in many production environments because this may result in leaking sensitive data or security related configurations, such as secret keys, to Flume log files. By default, Flume will not log such information. On the other hand, if the data pipeline is broken, Flume will attempt to provide clues for debugging the problem. One way to debug problems with event pipelines is to set up an additional Memory Channel connected to a Logger Sink, which will output all event data to the Flume logs. In some situations, however, this approach is insufficient. In order to enable logging of event- and configuration-related data, some Java system properties must be set in addition to log4j properties. To enable configuration-related logging, set the Java system property . This can either be passed on the command line or by setting this in the variable in flume-env.sh. To enable data logging, set the Java system property in the same way described above. For most components, the log4j logging level must also be set to DEBUG or TRACE to make event-specific logging appear in the Flume logs. Flume supports Agent configurations via Zookeeper. This is an experimental feature. The configuration file needs to be uploaded in the Zookeeper, under a configurable prefix. The configuration file is stored in Zookeeper Node data. Following is how the Zookeeper Node tree would look like for agents a1 and a2 - /flume |- /a1 [Agent config file] |- /a2 [Agent config file] While it has always been possible to include custom Flume components by adding their jars to the FLUME_CLASSPATH variable in the flume-env.sh file, Flume now supports a special directory called which automatically picks up plugins that are packaged in a specific format. This allows for easier management of plugin packaging issues as well as simpler debugging and troubleshooting of several classes of issues, especially library dependency conflicts. The directory is located at . At startup time, the start script looks in the directory for plugins that conform to the below format and includes them in proper paths when starting up . An Avro client included in the Flume distribution can send a given file to Flume Avro source using avro RPC mechanism: The above command will send the contents of /usr/logs/log.10 to to the Flume source listening on that ports. There’s an exec source that executes a given command and consumes the output. A single ‘line’ of output ie. text followed by carriage return (‘\\r’) or line feed (‘\n\n’) or both together. In order to flow the data across multiple agents or hops, the sink of the previous agent and source of the current hop need to be avro type with the sink pointing to the hostname (or IP address) and port of the source. A very common scenario in log collection is a large number of log producing clients sending data to a few consumer agents that are attached to the storage subsystem. For example, logs collected from hundreds of web servers sent to a dozen of agents that write to HDFS cluster. This can be achieved in Flume by configuring a number of first tier agents with an avro sink, all pointing to an avro source of single agent (Again you could use the thrift sources/sinks/clients in such a scenario). This source on the second tier agent consolidates the received events into a single channel which is consumed by a sink to its final destination. The above example shows a source from agent “foo” fanning out the flow to three different channels. This fan out can be replicating or multiplexing. In case of replicating flow, each event is sent to all three channels. For the multiplexing case, an event is delivered to a subset of available channels when an event’s attribute matches a preconfigured value. For example, if an event attribute called “txnType” is set to “customer”, then it should go to channel1 and channel3, if it’s “vendor” then it should go to channel2, otherwise channel3. The mapping can be set in the agent’s configuration file.\n\nTo define the flow within a single agent, you need to link the sources and sinks via a channel. You need to list the sources, sinks and channels for the given agent, and then point the source and sink to a channel. A source instance can specify multiple channels, but a sink instance can only specify one channel. The format is as follows: # list the sources, sinks and channels for the agent For example, an agent named agent_foo is reading data from an external avro client and sending it to HDFS via a memory channel. The config file weblog.config could look like: # list the sources, sinks and channels for the agent This will make the events flow from avro-AppSrv-source to hdfs-Cluster1-sink through the memory channel mem-channel-1. When the agent is started with the weblog.config as its config file, it will instantiate that flow. After defining the flow, you need to set properties of each source, sink and channel. This is done in the same hierarchical namespace fashion where you set the component type and other values for the properties specific to each component: The property “type” needs to be set for each component for Flume to understand what kind of object it needs to be. Each source, sink and channel type has its own set of properties required for it to function as intended. All those need to be set as needed. In the previous example, we have a flow from avro-AppSrv-source to hdfs-Cluster1-sink through the memory channel mem-channel-1. Here’s an example that shows configuration of each of those components: # list the sources, sinks and channels for the agent Then you can link the sources and sinks to their corresponding channels (for sources) or channel (for sinks) to setup two different flows. For example, if you need to setup two flows in an agent, one going from an external avro client to external HDFS and another from output of a tail to avro sink, then here’s a config to do that: # list the sources, sinks and channels in the agent To setup a multi-tier flow, you need to have an avro/thrift sink of first hop pointing to avro/thrift source of the next hop. This will result in the first Flume agent forwarding events to the next Flume agent. For example, if you are periodically sending files (1 file per event) using avro client to a local Flume agent, then this local agent can forward it to another agent that has the mounted for storage. # list sources, sinks and channels in the agent # list sources, sinks and channels in the agent Here we link the avro-forward-sink from the weblog agent to the avro-collection-source of the hdfs agent. This will result in the events coming from the external appserver source eventually getting stored in HDFS. As discussed in previous section, Flume supports fanning out the flow from one source to multiple channels. There are two modes of fan out, replicating and multiplexing. In the replicating flow, the event is sent to all the configured channels. In case of multiplexing, the event is sent to only a subset of qualifying channels. To fan out the flow, one needs to specify a list of channels for a source and the policy for the fanning it out. This is done by adding a channel “selector” that can be replicating or multiplexing. Then further specify the selection rules if it’s a multiplexer. If you don’t specify a selector, then by default it’s replicating: # List the sources, sinks and channels for the agent # set list of channels for source (separated by space) The multiplexing select has a further set of properties to bifurcate the flow. This requires specifying a mapping of an event attribute to a set for channel. The selector checks for each configured attribute in the event header. If it matches the specified value, then that event is sent to all the channels mapped to that value. If there’s no match, then the event is sent to set of channels configured as default: The mapping allows overlapping the channels for each value. # list the sources, sinks and channels in the agent The selector checks for a header called “State”. If the value is “CA” then its sent to mem-channel-1, if its “AZ” then it goes to file-channel-2 or if its “NY” then both. If the “State” header is not set or doesn’t match any of the three, then it goes to mem-channel-1 which is designated as ‘default’. The selector also supports optional channels. To specify optional channels for a header, the config parameter ‘optional’ is used in the following way: The selector will attempt to write to the required channels first and will fail the transaction if even one of these channels fails to consume the events. The transaction is reattempted on all of the channels. Once all required channels have consumed the events, then the selector will attempt to write to the optional channels. A failure by any of the optional channels to consume the event is simply ignored and not retried. If there is an overlap between the optional channels and required channels for a specific header, the channel is considered to be required, and a failure in the channel will cause the entire set of required channels to be retried. For instance, in the above example, for the header “CA” mem-channel-1 is considered to be a required channel even though it is marked both as required and optional, and a failure to write to this channel will cause that event to be retried on all channels configured for the selector. Note that if a header does not have any required channels, then the event will be written to the default channels and will be attempted to be written to the optional channels for that header. Specifying optional channels will still cause the event to be written to the default channels, if no required channels are specified. If no channels are designated as default and there are no required, the selector will attempt to write the events to the optional channels. Any failures are simply ignored in that case. Enabling SSL for a component is always specified at component level in the agent configuration file. So some components may be configured to use SSL while others not (even with the same component type). In case of the component level setup, the keystore / truststore is configured in the agent configuration file through component specific parameters. The advantage of this method is that the components can use different keystores (if this would be needed). The disadvantage is that the keystore parameters must be copied for each component in the agent configuration file. The component level setup is optional, but if it is defined, it has higher precedence than the global parameters. With the global setup, it is enough to define the keystore / truststore parameters once and use the same settings for all components, which means less and more centralized configuration. The SSL system properties can either be passed on the command line or by setting the environment variable in conf/flume-env.sh. (Although, using the command line is inadvisable because the commands including the passwords will be saved to the command history.) Flume uses the system properties defined in JSSE (Java Secure Socket Extension), so this is a standard way for setting up SSL. On the other hand, specifying passwords in system properties means that the passwords can be seen in the process list. For cases where it is not acceptable, it is also be possible to define the parameters in environment variables. Flume initializes the JSSE system properties from the corresponding environment variables internally in this case. The SSL environment variables can either be set in the shell environment before starting Flume or in conf/flume-env.sh. (Although, using the command line is inadvisable because the commands including the passwords will be saved to the command history.)\n• If the global SSL parameters are specified at multiple levels, the priority is the following (from higher to lower):\n• If SSL is enabled for a component, but the SSL parameters are not specified in any of the ways described above, then\n• in case of truststores: the default truststore will be used ( / in Oracle JDK)\n• The trustore password is optional in all cases. If not specified, then no integrity check will be performed on the truststore when it is opened by the JDK. Sources and sinks can have a batch size parameter that determines the maximum number of events they process in one batch. This happens within a channel transaction that has an upper limit called transaction capacity. Batch size must be smaller than the channel’s transaction capacity. There is an explicit check to prevent incompatible settings. This check happens whenever the configuration is read. This can be “none” or “deflate”. The compression-type must match the compression-type of matching AvroSource This is the path to a Java keystore file. If not specified here, then the global keystore will be used (if defined, otherwise configuration error). The password for the Java keystore. If not specified here, then the global keystore password will be used (if defined, otherwise configuration error). The type of the Java keystore. This can be “JKS” or “PKCS12”. If not specified here, then the global keystore type will be used (if defined, otherwise the default is JKS). Space-separated list of SSL/TLS protocols to exclude. SSLv3 will always be excluded in addition to the protocols specified. Space-separated list of SSL/TLS protocols to include. The enabled protocols will be the included protocols without the excluded protocols. If included-protocols is empty, it includes every supported protocols. Space-separated list of cipher suites to include. The enabled cipher suites will be the included cipher suites without the excluded cipher suites. If included-cipher-suites is empty, it includes every supported cipher suites. Note that the first rule to match will apply as the example below shows from a client on the localhost This will Allow the client on localhost be deny clients from any other ip “allow:name:localhost,deny:ip:” This will deny the client on localhost be allow clients from any other ip “deny:name:localhost,allow:ip:“ Listens on Thrift port and receives events from external Thrift client streams. When paired with the built-in ThriftSink on another (previous hop) Flume agent, it can create tiered collection topologies. Thrift source can be configured to start in secure mode by enabling kerberos authentication. agent-principal and agent-keytab are the properties used by the Thrift source to authenticate to the kerberos KDC. Required properties are in bold. This is the path to a Java keystore file. If not specified here, then the global keystore will be used (if defined, otherwise configuration error). The password for the Java keystore. If not specified here, then the global keystore password will be used (if defined, otherwise configuration error). The type of the Java keystore. This can be “JKS” or “PKCS12”. If not specified here, then the global keystore type will be used (if defined, otherwise the default is JKS). Space-separated list of SSL/TLS protocols to exclude. SSLv3 will always be excluded in addition to the protocols specified. Space-separated list of SSL/TLS protocols to include. The enabled protocols will be the included protocols without the excluded protocols. If included-protocols is empty, it includes every supported protocols. Space-separated list of cipher suites to include. The enabled cipher suites will be the included cipher suites without the excluded cipher suites. Set to true to enable kerberos authentication. In kerberos mode, agent-principal and agent-keytab are required for successful authentication. The Thrift source in secure mode, will accept connections only from Thrift clients that have kerberos enabled and are successfully authenticated to the kerberos KDC. The kerberos principal used by the Thrift Source to authenticate to the kerberos KDC. The keytab location used by the Thrift Source in combination with the agent-principal to authenticate to the kerberos KDC. Exec source runs a given Unix command on start-up and expects that process to continuously produce data on standard out (stderr is simply discarded, unless property logStdErr is set to true). If the process exits for any reason, the source also exits and will produce no further data. This means configurations such as or are going to produce the desired results where as will probably not - the former two commands produce streams of data where as the latter produces a single event and exits. A shell invocation used to run the command. e.g. /bin/sh -c. Required only for commands relying on shell features like wildcards, back ticks, pipes etc. The max number of lines to read and send to the channel at a time Amount of time (in milliseconds) to wait, if the buffer size was not reached, before data is pushed downstream The problem with ExecSource and other asynchronous sources is that the source can not guarantee that if there is a failure to put the event into the Channel the client knows about it. In such cases, the data will be lost. As a for instance, one of the most commonly requested features is the -like use case where an application writes to a log file on disk and Flume tails the file, sending each line as an event. While this is possible, there’s an obvious problem; what happens if the channel fills up and Flume can’t send an event? Flume has no way of indicating to the application writing the log file that it needs to retain the log or that the event hasn’t been sent, for some reason. If this doesn’t make sense, you need only know this: Your application can never guarantee data has been received when using a unidirectional asynchronous interface such as ExecSource! As an extension of this warning - and to be completely clear - there is absolutely zero guarantee of event delivery when using this source. For stronger reliability guarantees, consider the Spooling Directory Source, Taildir Source or direct integration with Flume via the SDK. The ‘shell’ config is used to invoke the ‘command’ through a command shell (such as Bash or Powershell). The ‘command’ is passed as an argument to ‘shell’ for execution. This allows the ‘command’ to use features from the shell such as wildcards, back ticks, pipes, loops, conditionals etc. In the absence of the ‘shell’ config, the ‘command’ will be invoked directly. Common values for ‘shell’ : ‘/bin/sh -c’, ‘/bin/ksh -c’, ‘cmd /c’, ‘powershell -Command’, etc. JMS Source reads messages from a JMS destination such as a queue or topic. Being a JMS application it should work with any JMS provider but has only been tested with ActiveMQ. The JMS source provides configurable batch size, message selector, user/pass, and message to flume event converter. Note that the vendor provided JMS jars should be included in the Flume classpath using plugins.d directory (preferred), –classpath on command line, or via FLUME_CLASSPATH variable in flume-env.sh. File containing the password for the destination/provider The JMS source allows pluggable converters, though it’s likely the default converter will work for most purposes. The default converter is able to convert Bytes, Text, and Object messages to FlumeEvents. In all cases, the properties in the message are added as headers to the FlumeEvent. Bytes of message are copied to body of the FlumeEvent. Cannot convert more than 2GB of data per message. Text of message is converted to a byte array and copied to the body of the FlumeEvent. The default converter uses UTF-8 by default but this is configurable. Object is written out to a ByteArrayOutputStream wrapped in an ObjectOutputStream and the resulting array is copied to the body of the FlumeEvent. JMS client implementations typically support to configure SSL/TLS via some Java system properties defined by JSSE (Java Secure Socket Extension). Specifying these system properties for Flume’s JVM, JMS Source (or more precisely the JMS client implementation used by the JMS Source) can connect to the JMS server through SSL (of course only when the JMS server has also been set up to use SSL). It should work with any JMS provider and has been tested with ActiveMQ, IBM MQ and Oracle WebLogic. The following sections describe the SSL configuration steps needed on the Flume side only. You can find more detailed descriptions about the server side setup of the different JMS providers and also full working configuration examples on Flume Wiki. If the JMS server uses self-signed certificate or its certificate is signed by a non-trusted CA (eg. the company’s own CA), then a truststore (containing the right certificate) needs to be set up and passed to Flume. It can be done via the global SSL parameters. For more details about the global SSL setup, see the SSL/TLS support section. Some JMS providers require SSL specific JNDI Initial Context Factory and/or Provider URL settings when using SSL (eg. ActiveMQ uses ssl:// URL prefix instead of tcp://). In this case the source properties ( and/or ) have to be adjusted in the agent config file. JMS Source can authenticate to the JMS server through client certificate authentication instead of the usual user/password login (when SSL is used and the JMS server is configured to accept this kind of authentication). The keystore containing Flume’s key used for the authentication needs to be configured via the global SSL parameters again. For more details about the global SSL setup, see the SSL/TLS support section. The keystore should contain only one key (if multiple keys are present, then the first one will be used). The key password must be the same as the keystore password. In case of client certificate authentication, it is not needed to specify the / properties for the JMS Source in the Flume agent config file. There are no component level configuration parameters for JMS Source unlike in case of other components. No enable SSL flag either. SSL setup is controlled by JNDI/Provider URL settings (ultimately the JMS server settings) and by the presence / absence of the truststore / keystore. This source lets you ingest data by placing files to be ingested into a “spooling” directory on disk. This source will watch the specified directory for new files, and will parse events out of new files as they appear. The event parsing logic is pluggable. After a given file has been fully read into the channel, completion by default is indicated by renaming the file or it can be deleted or the trackerDir is used to keep track of processed files. Unlike the Exec source, this source is reliable and will not miss data, even if Flume is restarted or killed. In exchange for this reliability, only immutable, uniquely-named files must be dropped into the spooling directory. Flume tries to detect these problem conditions and will fail loudly if they are violated:\n• If a file is written to after being placed into the spooling directory, Flume will print an error to its log file and stop processing. Despite the reliability guarantees of this source, there are still cases in which events may be duplicated if certain downstream failures occur. This is consistent with the guarantees offered by other Flume components. Whether to add a header storing the basename of the file. Directory to store metadata related to processing of files. If this path is not an absolute path, then it is interpreted as relative to the spoolDir. The tracking policy defines how file processing is tracked. It can be “rename” or “tracker_dir”. This parameter is only effective if the deletePolicy is “never”. “rename” - After processing files they get renamed according to the fileSuffix parameter. “tracker_dir” - Files are not renamed but a new empty file is created in the trackerDir. The new tracker file name is derived from the ingested one plus the fileSuffix. In which order files in the spooling directory will be consumed , and . In case of and , the last modified time of the files will be used to compare the files. In case of a tie, the file with smallest lexicographical order will be consumed first. In case of any file will be picked randomly. When using and the whole directory will be scanned to pick the oldest/youngest file, which might be slow if there are a large number of files, while using may cause old files to be consumed very late if new files keep coming in the spooling directory. The maximum time (in millis) to wait between consecutive attempts to write to the channel(s) if the channel is full. The source will start at a low backoff and increase it exponentially each time the channel throws a ChannelException, up to the value specified by this parameter. What to do when we see a non-decodable character in the input file. : Throw an exception and fail to parse the file. : Replace the unparseable character with the “replacement character” char, typically Unicode U+FFFD. : Drop the unparseable character sequence. Specify the deserializer used to parse the file into events. Defaults to parsing each line as an event. The class specified must implement . (Deprecated) Maximum length of a line in the commit buffer. Use deserializer.maxLineLength instead. Maximum number of characters to include in a single event. If a line exceeds this length, it is truncated, and the remaining characters on the line will appear in a subsequent event. This deserializer is able to read an Avro container file, and it generates one event per Avro record in the file. Each event is annotated with a header that indicates the schema used. The body of the event is the binary Avro record data, not including the schema or the rest of the container file elements. Note that if the spool directory source must retry putting one of these events onto a channel (for example, because the channel is full), then it will reset and retry from the most recent Avro container file sync point. To reduce potential event duplication in such a failure scenario, write sync markers more frequently in your Avro input files. How the schema is represented. By default, or when the value is specified, the Avro schema is hashed and the hash is stored in every event in the event header “flume.avro.schema.hash”. If is specified, the JSON-encoded schema itself is stored in every event in the event header “flume.avro.schema.literal”. Using mode is relatively inefficient compared to mode. This deserializer reads a Binary Large Object (BLOB) per event, typically one BLOB per file. For example a PDF or JPG file. Note that this approach is not suitable for very large objects because the entire BLOB is buffered in RAM. The maximum number of bytes to read and buffer for a given request Watch the specified files, and tail them in nearly real-time once detected new lines appended to the each files. If the new lines are being written, this source will retry reading them in wait for the completion of the write. This source is reliable and will not miss data even when the tailing files rotate. It periodically writes the last read position of each files on the given position file in JSON format. If Flume is stopped or down for some reason, it can restart tailing from the position written on the existing position file. In other use case, this source can also start tailing from the arbitrary position for each files using the given position file. When there is no position file on the specified path, it will start tailing from the first line of each files by default. Files will be consumed in order of their modification time. File with the oldest modification time will be consumed first. Absolute path of the file group. Regular expression (and not file system patterns) can be used for filename only. File in JSON format to record the inode, the absolute path and the last position of each tailing file. Whether to skip the position to EOF in the case of files not written on the position file. Interval time (ms) to write the last position of each file on the position file. Max number of lines to read and send to the channel at a time. Using the default is usually fine. Controls the number of batches being read consecutively from the same file. If the source is tailing multiple files and one of them is written at a fast rate, it can prevent other files to be processed, because the busy file would be read in an endless loop. In this case lower this value. The increment for time delay before reattempting to poll for new data, when the last attempt did not find any new data. The max time delay between each reattempt to poll for new data, when the last attempt did not find any new data. Listing directories and applying the filename regex pattern may be time consuming for directories containing thousands of files. Caching the list of matching files can improve performance. The order in which files are consumed will also be cached. Requires that the file system keeps track of modification times with at least a 1-second granularity. Experimental source that connects via Streaming API to the 1% sample twitter firehose, continuously downloads tweets, converts them to Avro format and sends Avro events to a downstream Flume sink. Requires the consumer and access tokens and secrets of a Twitter developer account. Required properties are in bold. Kafka Source is an Apache Kafka consumer that reads messages from Kafka topics. If you have multiple Kafka sources running, you can configure them with the same Consumer Group so each will read a unique set of partitions for the topics. This currently supports Kafka server releases 0.10.1.0 or higher. Testing was done up to 2.0.1 that was the highest avilable version at the time of the release. List of brokers in the Kafka cluster used by the source Unique identified of consumer group. Setting the same id in multiple sources or agents indicates that they are part of the same consumer group Comma-separated list of topics the Kafka consumer will read messages from. Maximum time (in ms) before a batch will be written to Channel The batch will be written whenever the first of size and time will be reached. Initial and incremental wait time that is triggered when a Kafka Topic appears to be empty. Wait period will reduce aggressive pinging of an empty Kafka Topic. One second is ideal for ingestion use cases but a lower value may be required for low latency operations with interceptors. By default events are taken as bytes from the Kafka topic directly into the event body. Set to true to read events as the Flume Avro binary format. Used in conjunction with the same property on the KafkaSink or with the parseAsFlumeEvent property on the Kafka Channel this will preserve any Flume headers sent on the producing side. When set to true, stores the topic of the retrieved message into a header, defined by the property. Defines the name of the header in which to store the name of the topic the message was received from, if the property is set to . Care should be taken if combining with the Kafka Sink property so as to avoid sending the message back to the same topic in a loop. If present the Kafka message timestamp value will be copied into the specified Flume header name. Used to identify which headers from the Kafka message should be added as a FLume header. The value of NAME should match the Flume header name and the value should be the name of the header to be used as the Kafka header name. These properties are used to configure the Kafka Consumer. Any consumer property supported by Kafka can be used. The only requirement is to prepend the property name with the prefix . For example: The Kafka Source overrides two Kafka consumer parameters: auto.commit.enable is set to “false” by the source and every batch is committed. Kafka source guarantees at least once strategy of messages retrieval. The duplicates can be present when the source starts. The Kafka Source also provides defaults for the key.deserializer(org.apache.kafka.common.serialization.StringSerializer) and value.deserializer(org.apache.kafka.common.serialization.ByteArraySerializer). Modification of these parameters is not recommended. When no Kafka stored offset is found, look up the offsets in Zookeeper and commit them to Kafka. This should be true to support seamless Kafka client migration from older versions of Flume. Once migrated this can be set to false, though that should generally not be required. If no Zookeeper offset is found, the Kafka configuration kafka.consumer.auto.offset.reset defines how offsets are handled. Check Kafka documentation for details Secure authentication as well as data encryption is supported on the communication channel between Flume and Kafka. For secure authentication SASL/GSSAPI (Kerberos V5) or SSL (even though the parameter is named SSL, the actual protocol is a TLS implementation) can be used from Kafka version 0.9.0. There is a performance degradation when SSL is enabled, the magnitude of which depends on the CPU type and the JVM implementation. Reference: Kafka security overview and the jira for tracking this issue: KAFKA-2561 Please read the steps described in Configuring Kafka Clients SSL to learn about additional configuration settings for fine tuning for example any of the following: security provider, cipher suites, enabled protocols, truststore or keystore types. Specifying the truststore is optional here, the global truststore can be used instead. For more details about the global SSL setup, see the SSL/TLS support section. Note: By default the property is not defined, so hostname verification is not performed. In order to enable hostname verification, set the following properties Once enabled, clients will verify the server’s fully qualified domain name (FQDN) against one of the following two fields: If client side authentication is also required then additionally the following needs to be added to Flume agent configuration or the global SSL setup can be used (see SSL/TLS support section). Each Flume agent has to have its client certificate which has to be trusted by Kafka brokers either individually or by their signature chain. Common example is to sign each client certificate by a single Root CA which in turn is trusted by Kafka brokers. If keystore and key use different password protection then property will provide the required additional secret for both consumer keystores: To use Kafka source with a Kafka cluster secured with Kerberos, set the properties noted above for consumer. The Kerberos keytab and principal to be used with Kafka brokers is specified in a JAAS file’s “KafkaClient” section. “Client” section describes the Zookeeper connection if needed. See Kafka doc for information on the JAAS file contents. The location of this JAAS file and optionally the system wide kerberos configuration can be specified via JAVA_OPTS in flume-env.sh: Sample JAAS file. For reference of its content please see client config sections of the desired authentication mechanism (GSSAPI/PLAIN) in Kafka documentation of SASL configuration. Since the Kafka Source may also connect to Zookeeper for offset migration, the “Client” section was also added to this example. This won’t be needed unless you require offset migration, or you require this section for other secure components. Also please make sure that the operating system user of the Flume processes has read privileges on the jaas and keytab files. A netcat-like source that listens on a given port and turns each line of text into an event. Acts like . In other words, it opens a specified port and listens for data. The expectation is that the supplied data is newline separated text. Each line of text is turned into a Flume event and sent via the connected channel. As per the original Netcat (TCP) source, this source that listens on a given port and turns each line of text into an event and sent via the connected channel. Acts like . A simple sequence generator that continuously generates events with a counter that starts from 0, increments by 1 and stops at totalEvents. Retries when it can’t send events to the channel. Useful mainly for testing. During retries it keeps the body of the retried messages the same as before so that the number of unique events - after de-duplication at destination - is expected to be equal to the specified . Required properties are in bold. Reads syslog data and generate Flume events. The UDP source treats an entire message as a single event. The TCP sources create a new event for each string of characters separated by a newline (‘n’). Setting this to ‘all’ will preserve the Priority, Timestamp and Hostname in the body of the event. A spaced separated list of fields to include is allowed as well. Currently, the following fields can be included: priority, version, timestamp, hostname. The values ‘true’ and ‘false’ have been deprecated in favor of ‘all’ and ‘none’. If specified, the IP address of the client will be stored in the header of each event using the header name specified here. This allows for interceptors and channel selectors to customize routing logic based on the IP address of the client. Do not use the standard Syslog header names here (like _host_) because the event header will be overridden in that case. If specified, the host name of the client will be stored in the header of each event using the header name specified here. This allows for interceptors and channel selectors to customize routing logic based on the host name of the client. Retrieving the host name may involve a name service reverse lookup which may affect the performance. Do not use the standard Syslog header names here (like _host_) because the event header will be overridden in that case. This is the path to a Java keystore file. If not specified here, then the global keystore will be used (if defined, otherwise configuration error). The password for the Java keystore. If not specified here, then the global keystore password will be used (if defined, otherwise configuration error). The type of the Java keystore. This can be “JKS” or “PKCS12”. If not specified here, then the global keystore type will be used (if defined, otherwise the default is JKS). Space-separated list of SSL/TLS protocols to exclude. SSLv3 will always be excluded in addition to the protocols specified. Space-separated list of SSL/TLS protocols to include. The enabled protocols will be the included protocols without the excluded protocols. If included-protocols is empty, it includes every supported protocols. Space-separated list of cipher suites to include. The enabled cipher suites will be the included cipher suites without the excluded cipher suites. If included-cipher-suites is empty, it includes every supported cipher suites. This is a newer, faster, multi-port capable version of the Syslog TCP source. Note that the configuration setting has replaced . Multi-port capability means that it can listen on many ports at once in an efficient manner. This source uses the Apache Mina library to do that. Provides support for RFC-3164 and many common RFC-5424 formatted messages. Also provides the capability to configure the character set used on a per-port basis. Setting this to ‘all’ will preserve the Priority, Timestamp and Hostname in the body of the event. A spaced separated list of fields to include is allowed as well. Currently, the following fields can be included: priority, version, timestamp, hostname. The values ‘true’ and ‘false’ have been deprecated in favor of ‘all’ and ‘none’. If specified, the port number will be stored in the header of each event using the header name specified here. This allows for interceptors and channel selectors to customize routing logic based on the incoming port. If specified, the IP address of the client will be stored in the header of each event using the header name specified here. This allows for interceptors and channel selectors to customize routing logic based on the IP address of the client. Do not use the standard Syslog header names here (like _host_) because the event header will be overridden in that case. If specified, the host name of the client will be stored in the header of each event using the header name specified here. This allows for interceptors and channel selectors to customize routing logic based on the host name of the client. Retrieving the host name may involve a name service reverse lookup which may affect the performance. Do not use the standard Syslog header names here (like _host_) because the event header will be overridden in that case. Maximum number of events to attempt to process per request loop. Using the default is usually fine. Size of the internal Mina read buffer. Provided for performance tuning. Using the default is usually fine. Number of processors available on the system for use while processing messages. Default is to auto-detect # of CPUs using the Java Runtime API. Mina will spawn 2 request-processing threads per detected CPU, which is often reasonable. This is the path to a Java keystore file. If not specified here, then the global keystore will be used (if defined, otherwise configuration error). The password for the Java keystore. If not specified here, then the global keystore password will be used (if defined, otherwise configuration error). The type of the Java keystore. This can be “JKS” or “PKCS12”. If not specified here, then the global keystore type will be used (if defined, otherwise the default is JKS). Space-separated list of SSL/TLS protocols to exclude. SSLv3 will always be excluded in addition to the protocols specified. Space-separated list of SSL/TLS protocols to include. The enabled protocols will be the included protocols without the excluded protocols. If included-protocols is empty, it includes every supported protocols. Space-separated list of cipher suites to include. The enabled cipher suites will be the included cipher suites without the excluded cipher suites. If included-cipher-suites is empty, it includes every supported cipher suites. Setting this to true will preserve the Priority, Timestamp and Hostname in the body of the event. If specified, the IP address of the client will be stored in the header of each event using the header name specified here. This allows for interceptors and channel selectors to customize routing logic based on the IP address of the client. Do not use the standard Syslog header names here (like _host_) because the event header will be overridden in that case. If specified, the host name of the client will be stored in the header of each event using the header name specified here. This allows for interceptors and channel selectors to customize routing logic based on the host name of the client. Retrieving the host name may involve a name service reverse lookup which may affect the performance. Do not use the standard Syslog header names here (like _host_) because the event header will be overridden in that case. A source which accepts Flume Events by HTTP POST and GET. GET should be used for experimentation only. HTTP requests are converted into flume events by a pluggable “handler” which must implement the HTTPSourceHandler interface. This handler takes a HttpServletRequest and returns a list of flume events. All events handled from one Http request are committed to the channel in one transaction, thus allowing for increased efficiency on channels like the file channel. If the handler throws an exception, this source will return a HTTP status of 400. If the channel is full, or the source is unable to append events to the channel, the source will return a HTTP 503 - Temporarily unavailable status. All events sent in one post request are considered to be one batch and inserted into the channel in one transaction. This source is based on Jetty 9.4 and offers the ability to set additional Jetty-specific parameters which will be passed directly to the Jetty components. The FQCN of the handler class. Space-separated list of SSL/TLS protocols to exclude. SSLv3 will always be excluded in addition to the protocols specified. Space-separated list of SSL/TLS protocols to include. The enabled protocols will be the included protocols without the excluded protocols. If included-protocols is empty, it includes every supported protocols. Space-separated list of cipher suites to include. The enabled cipher suites will be the included cipher suites without the excluded cipher suites. Location of the keystore including keystore file name. If SSL is enabled but the keystore is not specified here, then the global keystore will be used (if defined, otherwise configuration error). Keystore password. If SSL is enabled but the keystore password is not specified here, then the global keystore password will be used (if defined, otherwise configuration error). N.B. Jetty-specific settings are set using the setter-methods on the objects listed above. For full details see the Javadoc for these classes (QueuedThreadPool, HttpConfiguration, SslContextFactory and ServerConnector). When using Jetty-specific setings, named properites above will take precedence (for example excludeProtocols will take precedence over SslContextFactory.ExcludeProtocols). All properties will be inital lower case. A handler is provided out of the box which can handle events represented in JSON format, and supports UTF-8, UTF-16 and UTF-32 character sets. The handler accepts an array of events (even if there is only one event, the event has to be sent in an array) and converts them to a Flume event based on the encoding specified in the request. If no encoding is specified, UTF-8 is assumed. The JSON handler supports UTF-8, UTF-16 and UTF-32. Events are represented as follows. One way to create an event in the format expected by this handler is to use JSONEvent provided in the Flume SDK and use Google Gson to create the JSON string using the Gson#fromJson(Object, Type) method. The type token to pass as the 2nd argument of this method for list of events can be created by: By default HTTPSource splits JSON input into Flume events. As an alternative, BlobHandler is a handler for HTTPSource that returns an event that contains the request parameters as well as the Binary Large Object (BLOB) uploaded with this request. For example a PDF or JPG file. Note that this approach is not suitable for very large objects because it buffers up the entire BLOB in RAM. The maximum number of bytes to read and buffer for a given request StressSource is an internal load-generating source implementation which is very useful for stress tests. It allows User to configure the size of Event payload, with empty headers. User can configure total number of events to be sent as well maximum number of Successful Event to be delivered. The legacy sources allow a Flume 1.x agent to receive events from Flume 0.9.4 agents. It accepts events in the Flume 0.9.4 format, converts them to the Flume 1.0 format, and stores them in the connected channel. The 0.9.4 event properties like timestamp, pri, host, nanos, etc get converted to 1.x event header attributes. The legacy source supports both Avro and Thrift RPC connections. To use this bridge between two Flume versions, you need to start a Flume 1.x agent with the avroLegacy or thriftLegacy source. The 0.9.4 agent should have the agent Sink pointing to the host/port of the 1.x agent. The reliability semantics of Flume 1.x are different from that of Flume 0.9.x. The E2E or DFO mode of a Flume 0.9.x agent will not be supported by the legacy source. The only supported 0.9.x mode is the best effort, though the reliability setting of the 1.x flow will be applicable to the events once they are saved into the Flume 1.x channel by the legacy source. A custom source is your own implementation of the Source interface. A custom source’s class and its dependencies must be included in the agent’s classpath when starting the Flume agent. The type of the custom source is its FQCN. Scribe is another type of ingest system. To adopt existing Scribe ingest system, Flume should use ScribeSource based on Thrift with compatible transfering protocol. For deployment of Scribe please follow the guide from Facebook. Required properties are in bold. This sink writes events into the Hadoop Distributed File System (HDFS). It currently supports creating text and sequence files. It supports compression in both file types. The files can be rolled (close current file and create a new one) periodically based on the elapsed time or size of data or number of events. It also buckets/partitions data by attributes like timestamp or machine where the event originated. The HDFS directory path may contain formatting escape sequences that will replaced by the HDFS sink to generate a directory/file name to store the events. Using this sink requires hadoop to be installed so that Flume can use the Hadoop jars to communicate with the HDFS cluster. Note that a version of Hadoop that supports the sync() call is required. Substitute the hostname of the host where the agent is running Substitute the IP address of the host where the agent is running Substitute the canonical hostname of the host where the agent is running Note: The escape strings %[localhost], %[IP] and %[FQDN] all rely on Java’s ability to obtain the hostname, which may fail in some networking environments. The file in use will have the name mangled to include ”.tmp” at the end. Once the file is closed, this extension is removed. This allows excluding partially complete files in the directory. Required properties are in bold. For all of the time related escape sequences, a header with the key “timestamp” must exist among the headers of the event (unless is set to ). One way to add this automatically is to use the TimestampInterceptor. If an is used while writing the output. After closing the output is removed from the output file name. If the parameter is ignored an empty string is used instead. Number of events written to file before it rolled (0 = never roll based on number of events) File format: currently , or (1)DataStream will not compress output file and please don’t set codeC (2)CompressedStream requires set hdfs.codeC with an available codeC Specify minimum number of replicas per HDFS block. If not specified, it comes from the default Hadoop config in the classpath. Format for sequence file records. One of or . Set to before creating data files with Flume, otherwise those files cannot be read by either Apache Impala (incubating) or Apache Hive. Number of threads per HDFS sink for HDFS IO ops (open, write, etc.) Number of threads per HDFS sink for scheduling timed file rolling Rounded down to the highest multiple of this (in the unit configured using ), less than current time. The unit of the round down value - , or . Name of the timezone that should be used for resolving the directory path, e.g. America/Los_Angeles. Use the local time (instead of the timestamp from the event header) while replacing the escape sequences. Number of times the sink must try renaming a file, after initiating a close attempt. If set to 1, this sink will not re-try a failed rename (due to, for example, NameNode or DataNode failure), and may leave the file in an open state with a .tmp extension. If set to 0, the sink will try to rename the file until the file is eventually renamed (there is no limit on the number of times it would try). The file may still remain open if the close call fails but the data will be intact and in this case, the file will be closed only after a Flume restart. Time in seconds between consecutive attempts to close a file. Each close call costs multiple RPC round-trips to the Namenode, so setting this too low can cause a lot of load on the name node. If set to 0 or less, the sink will not attempt to close the file if the first attempt fails, and may leave the file open or with a ”.tmp” extension. Other possible options include or the fully-qualified class name of an implementation of the interface. The above configuration will round down the timestamp to the last 10th minute. For example, an event with timestamp 11:54:34 AM, June 12, 2012 will cause the hdfs path to become . This sink streams events containing delimited text or JSON data directly into a Hive table or partition. Events are written using Hive transactions. As soon as a set of events are committed to Hive, they become immediately visible to Hive queries. Partitions to which flume will stream to can either be pre-created or, optionally, Flume can create them if they are missing. Fields from incoming event data are mapped to corresponding columns in the Hive table. Comma separate list of partition values identifying the partition to write to. May contain escape sequences. E.g: If the table is partitioned by (continent: string, country :string, time : string) then ‘Asia,India,2014-02-26-01-21’ will indicate continent=Asia,country=India,time=2014-02-26-01-21 Hive grants a batch of transactions instead of single transactions to streaming clients like Flume. This setting configures the number of desired transactions per Transaction Batch. Data from all transactions in a single batch end up in a single file. Flume will write a maximum of batchSize events in each transaction in the batch. This setting in conjunction with batchSize provides control over the size of each file. Note that eventually Hive will transparently compact these files into larger files. (In milliseconds) Timeout for Hive & HDFS I/O operations, such as openTxn, write, commit, abort. Serializer is responsible for parsing out field from the event and mapping them to columns in the hive table. Choice of serializer depends upon the format of the data in the event. Supported serializers: DELIMITED and JSON The unit of the round down value - , or . Rounded down to the highest multiple of this (in the unit configured using hive.roundUnit), less than current time Name of the timezone that should be used for resolving the escape sequences in partition, e.g. America/Los_Angeles. Use the local time (instead of the timestamp from the event header) while replacing the escape sequences. JSON: Handles UTF8 encoded Json (strict syntax) events and requires no configration. Object names in the JSON are mapped directly to columns with the same name in the Hive table. Internally uses org.apache.hive.hcatalog.data.JsonSerDe but is independent of the Serde of the Hive table. This serializer requires HCatalog to be installed. DELIMITED: Handles simple delimited textual events. Internally uses LazySimpleSerde but is independent of the Serde of the Hive table. (Type: string) The field delimiter in the incoming data. To use special characters, surround them with double quotes like “\\t” The mapping from input fields to columns in hive table. Specified as a comma separated list (no spaces) of hive table columns names, identifying the input fields in order of their occurrence. To skip fields leave the column name unspecified. Eg. ‘time,,ip,message’ indicates the 1st, 3rd and 4th fields in input map to time, ip and message columns in the hive table. (Type: character) Customizes the separator used by underlying serde. There can be a gain in efficiency if the fields in serializer.fieldnames are in same order as table columns, the serializer.delimiter is same as the serializer.serdeSeparator and number of fields in serializer.fieldnames is less than or equal to number of table columns, as the fields in incoming event body do not need to be reordered to match order of table columns. Use single quotes for special characters like ‘\\t’. Ensure input fields do not contain this character. NOTE: If serializer.delimiter is a single character, preferably set this to the same character For all of the time related escape sequences, a header with the key “timestamp” must exist among the headers of the event (unless is set to ). One way to add this automatically is to use the TimestampInterceptor. create table weblogs ( id int , msg string ) partitioned by (continent string, country string, time string) clustered by (id) into 5 buckets stored as orc; The above configuration will round down the timestamp to the last 10th minute. For example, an event with timestamp header set to 11:54:34 AM, June 12, 2012 and ‘country’ header set to ‘india’ will evaluate to the partition (continent=’asia’,country=’india’,time=‘2012-06-12-11-50’. The serializer is configured to accept tab separated input containing three fields and to skip the second field. Logs event at INFO level. Typically useful for testing/debugging purpose. Required properties are in bold. This sink is the only exception which doesn’t require the extra configuration explained in the Logging raw data section. Maximum number of bytes of the Event body to log This sink forms one half of Flume’s tiered collection support. Flume events sent to this sink are turned into Avro events and sent to the configured hostname / port pair. The events are taken from the configured Channel in batches of the configured batch size. Required properties are in bold. Amount of time (ms) to allow for the first (handshake) request. Amount of time (ms) to allow for requests after the first. Amount of time (s) before the connection to the next hop is reset. This will force the Avro Sink to reconnect to the next hop. This will allow the sink to connect to hosts behind a hardware load-balancer when news hosts are added without having to restart the agent. This can be “none” or “deflate”. The compression-type must match the compression-type of matching AvroSource The level of compression to compress event. 0 = no compression and 1-9 is compression. The higher the number the more compression If this is set to true, SSL server certificates for remote servers (Avro Sources) will not be checked. This should NOT be used in production because it makes it easier for an attacker to execute a man-in-the-middle attack and “listen in” on the encrypted connection. The path to a custom Java truststore file. Flume uses the certificate authority information in this file to determine whether the remote Avro Source’s SSL authentication credentials should be trusted. If not specified, then the global keystore will be used. If the global keystore not specified either, then the default Java JSSE certificate authority files (typically “jssecacerts” or “cacerts” in the Oracle JRE) will be used. The password for the truststore. If not specified, then the global keystore password will be used (if defined). The type of the Java truststore. This can be “JKS” or other supported Java truststore type. If not specified, then the global keystore type will be used (if defined, otherwise the defautl is JKS). Space-separated list of SSL/TLS protocols to exclude. SSLv3 will always be excluded in addition to the protocols specified. 2 * the number of available processors in the machine The maximum number of I/O worker threads. This is configured on the NettyAvroRpcClient NioClientSocketChannelFactory. This sink forms one half of Flume’s tiered collection support. Flume events sent to this sink are turned into Thrift events and sent to the configured hostname / port pair. The events are taken from the configured Channel in batches of the configured batch size. Thrift sink can be configured to start in secure mode by enabling kerberos authentication. To communicate with a Thrift source started in secure mode, the Thrift sink should also operate in secure mode. client-principal and client-keytab are the properties used by the Thrift sink to authenticate to the kerberos KDC. The server-principal represents the principal of the Thrift source this sink is configured to connect to in secure mode. Required properties are in bold. Amount of time (ms) to allow for the first (handshake) request. Amount of time (ms) to allow for requests after the first. Amount of time (s) before the connection to the next hop is reset. This will force the Thrift Sink to reconnect to the next hop. This will allow the sink to connect to hosts behind a hardware load-balancer when news hosts are added without having to restart the agent. The path to a custom Java truststore file. Flume uses the certificate authority information in this file to determine whether the remote Thrift Source’s SSL authentication credentials should be trusted. If not specified, then the global keystore will be used. If the global keystore not specified either, then the default Java JSSE certificate authority files (typically “jssecacerts” or “cacerts” in the Oracle JRE) will be used. The password for the truststore. If not specified, then the global keystore password will be used (if defined). The type of the Java truststore. This can be “JKS” or other supported Java truststore type. If not specified, then the global keystore type will be used (if defined, otherwise the defautl is JKS). The kerberos principal used by the Thrift Sink to authenticate to the kerberos KDC. The keytab location used by the Thrift Sink in combination with the client-principal to authenticate to the kerberos KDC. The kerberos principal of the Thrift Source to which the Thrift Sink is configured to connect to. line separator (if you were to enter the default value into the config file, then you would need to escape the backslash, like this: “\n\n”) A character string to add to the beginning of the file name if the default PathManager is used Other possible options include or the FQCN of an implementation of EventSerializer.Builder interface. This sink writes data to HBase. The Hbase configuration is picked up from the first hbase-site.xml encountered in the classpath. A class implementing HbaseEventSerializer which is specified by the configuration is used to convert the events into HBase puts and/or increments. These puts and increments are then written to HBase. This sink provides the same consistency guarantees as HBase, which is currently row-wise atomicity. In the event of Hbase failing to write certain events, the sink will replay all events in that transaction. The HBaseSink supports writing data to secure HBase. To write to secure HBase, the user the agent is running as must have write permissions to the table the sink is configured to write to. The principal and keytab to use to authenticate against the KDC can be specified in the configuration. The hbase-site.xml in the Flume agent’s classpath must have authentication set to (For details on how to do this, please refer to HBase documentation). For convenience, two serializers are provided with Flume. The SimpleHbaseEventSerializer (org.apache.flume.sink.hbase.SimpleHbaseEventSerializer) writes the event body as-is to HBase, and optionally increments a column in Hbase. This is primarily an example implementation. The RegexHbaseEventSerializer (org.apache.flume.sink.hbase.RegexHbaseEventSerializer) breaks the event body based on the given regex and writes each part into different columns. The name of the table in Hbase to write to. The quorum spec. This is the value for the property in hbase-site.xml The base path for the znode for the -ROOT- region. Value of in hbase-site.xml Should the sink coalesce multiple increments to a cell per batch. This might give better performance if there are multiple increments to a limited number of cells. HBase2Sink is the equivalent of HBaseSink for HBase version 2. The provided functionality and the configuration parameters are the same as in case of HBaseSink (except the hbase2 tag in the sink type and the package/class names). The name of the table in HBase to write to. The quorum spec. This is the value for the property in hbase-site.xml The base path for the znode for the -ROOT- region. Value of in hbase-site.xml Should the sink coalesce multiple increments to a cell per batch. This might give better performance if there are multiple increments to a limited number of cells. This sink writes data to HBase using an asynchronous model. A class implementing AsyncHbaseEventSerializer which is specified by the configuration is used to convert the events into HBase puts and/or increments. These puts and increments are then written to HBase. This sink uses the Asynchbase API to write to HBase. This sink provides the same consistency guarantees as HBase, which is currently row-wise atomicity. In the event of Hbase failing to write certain events, the sink will replay all events in that transaction. AsyncHBaseSink can only be used with HBase 1.x. The async client library used by AsyncHBaseSink is not available for HBase 2. The type is the FQCN: org.apache.flume.sink.hbase.AsyncHBaseSink. Required properties are in bold. The name of the table in Hbase to write to. The quorum spec. This is the value for the property in hbase-site.xml The base path for the znode for the -ROOT- region. Value of in hbase-site.xml Should the sink coalesce multiple increments to a cell per batch. This might give better performance if there are multiple increments to a limited number of cells. The length of time (in milliseconds) the sink waits for acks from hbase for all events in a transaction. Properties to be passed to asyncHbase library. These properties have precedence over the old and values. You can find the list of the available properties at the documentation page of AsyncHBase. Note that this sink takes the Zookeeper Quorum and parent znode information in the configuration. Zookeeper Quorum and parent node configuration may be specified in the flume configuration file. Alternatively, these configuration values are taken from the first hbase-site.xml file in the classpath. If these are not provided in the configuration, then the sink will read this information from the first hbase-site.xml file in the classpath. This sink is well suited for use cases that stream raw data into HDFS (via the HdfsSink) and simultaneously extract, transform and load the same data into Solr (via MorphlineSolrSink). In particular, this sink can process arbitrary heterogeneous raw data from disparate data sources and turn it into a data model that is useful to Search applications. Morphlines can be seen as an evolution of Unix pipelines where the data model is generalized to work with streams of generic records, including arbitrary binary payloads. A morphline command is a bit like a Flume Interceptor. Morphlines can be embedded into Hadoop components such as Flume. Commands to parse and transform a set of standard data formats such as log files, Avro, CSV, Text, HTML, XML, PDF, Word, Excel, etc. are provided out of the box, and additional custom commands and parsers for additional data formats can be added as morphline plugins. Any kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, and any custom ETL logic can be registered and executed. Morphlines manipulate continuous streams of records. The data model can be described as follows: A record is a set of named fields where each field has an ordered list of one or more values. A value can be any Java Object. That is, a record is essentially a hash table where each hash table entry contains a String key and a list of Java Objects as values. (The implementation uses Guava’s , which is a ). Note that a field can have multiple values and any two records need not use common field names. This sink fills the body of the Flume event into the field of the morphline record, as well as copies the headers of the Flume event into record fields of the same name. The commands can then act on this data. Routing to a SolrCloud cluster is supported to improve scalability. Indexing load can be spread across a large number of MorphlineSolrSinks for improved scalability. Indexing load can be replicated across multiple MorphlineSolrSinks for high availability, for example using Flume features such as Load balancing Sink Processor. MorphlineInterceptor can also help to implement dynamic routing to multiple Solr collections (e.g. for multi-tenancy). The morphline and solr jars required for your environment must be placed in the lib directory of the Apache Flume installation. The relative or absolute path on the local file system to the morphline configuration file. Example: The maximum number of events to take per flume transaction. The maximum duration per flume transaction (ms). The transaction commits after this duration or when batchSize is exceeded, whichever comes first. Comma separated list of recoverable exceptions that tend to be transient, in which case the corresponding task can be retried. Examples include network connection errors, timeouts, etc. When the production mode flag is set to true, the recoverable exceptions configured using this parameter will not be ignored and hence will lead to retries. This flag should be enabled, if an unrecoverable exception is accidentally misclassified as recoverable. This enables the sink to make progress and avoid retrying an event forever. Experimental sink that writes events to a Kite Dataset. This sink will deserialize the body of each incoming event and store the resulting record in a Kite Dataset. It determines target Dataset by loading a dataset by URI. The only supported serialization is avro, and the record schema must be passed in the event headers, using either with the JSON schema representation or with a URL where the schema may be found ( URIs are supported). This is compatible with the Log4jAppender flume client and the spooling directory source’s Avro deserializer using . Note 1: The header is not supported. Note 2: In some cases, file rolling may occur slightly after the roll interval has been exceeded. However, this delay will not exceed 5 seconds. In most cases, the delay is neglegible. Namespace of the Dataset where records will be written (deprecated; use kite.dataset.uri instead) Name of the Dataset where records will be written (deprecated; use kite.dataset.uri instead) If , the Flume transaction will be commited and the writer will be flushed on each batch of records. This setting only applies to flushable datasets. When , it’s possible for temp files with commited data to be left in the dataset directory. These files need to be recovered by hand for the data to be visible to DatasetReaders. Controls whether the sink will also sync data when committing the transaction. This setting only applies to syncable datasets. Syncing gaurentees that data will be written on stable storage on the remote system while flushing only gaurentees that data has left Flume’s client buffers. When the property is set to , this property must also be set to . Parser that turns Flume into Kite entities. Valid values are and the fully-qualified class name of an implementation of the interface. Policy that handles non-recoverable errors such as a missing in the header. The default value, , will fail the current batch and try again which matches the old behavior. Other valid values are , which will write the raw to the dataset, and the fully-qualified class name of an implementation of the interface. URI of the dataset where failed events are saved when is set to . Required when the is set to . The effective user for HDFS actions, if different from the kerberos principal This is a Flume Sink implementation that can publish data to a Kafka topic. One of the objective is to integrate Flume with Kafka so that pull based processing systems can process the data coming through various Flume sources. This currently supports Kafka server releases 0.10.1.0 or higher. Testing was done up to 2.0.1 that was the highest avilable version at the time of the release. List of brokers Kafka-Sink will connect to, to get the list of topic partitions This can be a partial list of brokers, but we recommend at least two for HA. The format is comma separated list of hostname:port The topic in Kafka to which the messages will be published. If this parameter is configured, messages will be published to this topic. If the event header contains a “topic” field, the event will be published to that topic overriding the topic configured here. Arbitrary header substitution is supported, eg. %{header} is replaced with value of event header named “header”. (If using the substitution, it is recommended to set “auto.create.topics.enable” property of Kafka broker to true.) How many replicas must acknowledge a message before its considered successfully written. Accepted values are 0 (Never wait for acknowledgement), 1 (wait for leader only), -1 (wait for all replicas) Set this to -1 to avoid data loss in some cases of leader failure. By default events are put as bytes onto the Kafka topic directly from the event body. Set to true to store events as the Flume Avro binary format. Used in conjunction with the same property on the KafkaSource or with the parseAsFlumeEvent property on the Kafka Channel this will preserve any Flume headers for the producing side. Specifies a Kafka partition ID (integer) for all events in this channel to be sent to, unless overriden by . By default, if this property is not set, events will be distributed by the Kafka Producer’s partitioner - including by if specified (or by a partitioner specified by ). When set, the sink will take the value of the field named using the value of this property from the event header and send the message to the specified partition of the topic. If the value represents an invalid partition, an EventDeliveryException will be thrown. If the header value is present then this setting overrides . When set, the sink will allow a message to be produced into a topic specified by the property (if provided). When set in conjunction with will produce a message into the value of the header named using the value of this property. Care should be taken when using in conjunction with the Kafka Source property to avoid creating a loopback. The header containing the caller provided timestamp value to use. If not provided the current time will be used. Used to identify which headers from the Flume Event should be passed to Kafka. The value of NAME should match the Flume header name and the value should be the name of the header to be used as the Kafka header name. These properties are used to configure the Kafka Producer. Any producer property supported by Kafka can be used. The only requirement is to prepend the property name with the prefix . For example: kafka.producer.linger.ms Kafka Sink uses the and properties from the FlumeEvent headers to send events to Kafka. If exists in the headers, the event will be sent to that specific topic, overriding the topic configured for the Sink. If exists in the headers, the key will used by Kafka to partition the data between the topic partitions. Events with same key will be sent to the same partition. If the key is null, events will be sent to random partitions. The Kafka sink also provides defaults for the key.serializer(org.apache.kafka.common.serialization.StringSerializer) and value.serializer(org.apache.kafka.common.serialization.ByteArraySerializer). Modification of these parameters is not recommended. An example configuration of a Kafka sink is given below. Properties starting with the prefix the Kafka producer. The properties that are passed when creating the Kafka producer are not limited to the properties given in this example. Also it is possible to include your custom properties here and access them inside the preprocessor through the Flume Context object passed in as a method argument. Secure authentication as well as data encryption is supported on the communication channel between Flume and Kafka. For secure authentication SASL/GSSAPI (Kerberos V5) or SSL (even though the parameter is named SSL, the actual protocol is a TLS implementation) can be used from Kafka version 0.9.0. There is a performance degradation when SSL is enabled, the magnitude of which depends on the CPU type and the JVM implementation. Reference: Kafka security overview and the jira for tracking this issue: KAFKA-2561 Please read the steps described in Configuring Kafka Clients SSL to learn about additional configuration settings for fine tuning for example any of the following: security provider, cipher suites, enabled protocols, truststore or keystore types. Specyfing the truststore is optional here, the global truststore can be used instead. For more details about the global SSL setup, see the SSL/TLS support section. Note: By default the property is not defined, so hostname verification is not performed. In order to enable hostname verification, set the following properties Once enabled, clients will verify the server’s fully qualified domain name (FQDN) against one of the following two fields: If client side authentication is also required then additionally the following needs to be added to Flume agent configuration or the global SSL setup can be used (see SSL/TLS support section). Each Flume agent has to have its client certificate which has to be trusted by Kafka brokers either individually or by their signature chain. Common example is to sign each client certificate by a single Root CA which in turn is trusted by Kafka brokers. If keystore and key use different password protection then property will provide the required additional secret for producer keystore: To use Kafka sink with a Kafka cluster secured with Kerberos, set the property noted above for producer. The Kerberos keytab and principal to be used with Kafka brokers is specified in a JAAS file’s “KafkaClient” section. “Client” section describes the Zookeeper connection if needed. See Kafka doc for information on the JAAS file contents. The location of this JAAS file and optionally the system wide kerberos configuration can be specified via JAVA_OPTS in flume-env.sh: Sample JAAS file. For reference of its content please see client config sections of the desired authentication mechanism (GSSAPI/PLAIN) in Kafka documentation of SASL configuration. Unlike the Kafka Source or Kafka Channel a “Client” section is not required, unless it is needed by other connecting components. Also please make sure that the operating system user of the Flume processes has read privileges on the jaas and keytab files. Behaviour of this sink is that it will take events from the channel, and send those events to a remote service using an HTTP POST request. The event content is sent as the POST body. Error handling behaviour of this sink depends on the HTTP response returned by the target server. The sink backoff/ready status is configurable, as is the transaction commit/rollback result and whether the event contributes to the successful event drain count. Any malformed HTTP response returned by the server where the status code is not readable will result in a backoff signal and the event is not consumed from the channel. Note that the most specific HTTP status code match is used for the backoff, rollback and incrementMetrics configuration options. If there are configuration values for both 2XX and 200 status codes, then 200 HTTP codes will use the 200 value, and all other HTTP codes in the 201-299 range will use the 2XX value. A custom sink is your own implementation of the Sink interface. A custom sink’s class and its dependencies must be included in the agent’s classpath when starting the Flume agent. The type of the custom sink is its FQCN. Required properties are in bold. Channels are the repositories where the events are staged on a agent. Source adds the events and Sink removes it. The events are stored in an in-memory queue with configurable max size. It’s ideal for flows that need higher throughput and are prepared to lose the staged data in the event of a agent failures. Required properties are in bold. The maximum number of events stored in the channel The maximum number of events the channel will take from a source or give to a sink per transaction Timeout in seconds for adding or removing an event Defines the percent of buffer between byteCapacity and the estimated total size of all events in the channel, to account for data in headers. See below. Maximum total bytes of memory allowed as a sum of all events in this channel. The implementation only counts the Event , which is the reason for providing the configuration parameter as well. Defaults to a computed value equal to 80% of the maximum memory available to the JVM (i.e. 80% of the -Xmx value passed on the command line). Note that if you have multiple memory channels on a single JVM, and they happen to hold the same physical events (i.e. if you are using a replicating channel selector from a single source) then those event sizes may be double-counted for channel byteCapacity purposes. Setting this value to will cause this value to fall back to a hard internal limit of about 200 GB. The events are stored in a persistent storage that’s backed by a database. The JDBC channel currently supports embedded Derby. This is a durable channel that’s ideal for flows where recoverability is important. Required properties are in bold. Max number of events in the channel The events are stored in a Kafka cluster (must be installed separately). Kafka provides high availability and replication, so in case an agent or a kafka broker crashes, the events are immediately available to other sinks This currently supports Kafka server releases 0.10.1.0 or higher. Testing was done up to 2.0.1 that was the highest avilable version at the time of the release.\n• Configuration values related to the channel generically are applied at the channel config level, eg: a1.channel.k1.type = This version of flume is backwards-compatible with previous versions, however deprecated properties are indicated in the table below and a warning message is logged on startup when they are present in the configuration file. List of brokers in the Kafka cluster used by the channel This can be a partial list of brokers, but we recommend at least two for HA. The format is comma separated list of hostname:port Consumer group ID the channel uses to register with Kafka. Multiple channels must use the same topic and group to ensure that when one agent fails another can get the data Note that having non-channel consumers with the same ID can lead to data loss. Expecting Avro datums with FlumeEvent schema in the channel. This should be true if Flume source is writing to the channel and false if other producers are writing into the topic that the channel is using. Flume source messages to Kafka can be parsed outside of Flume by using org.apache.flume.source.avro.AvroFlumeEvent provided by the flume-ng-sdk artifact The amount of time(in milliseconds) to wait in the “poll()” call of the consumer. https://kafka.apache.org/090/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#poll(long) Specifies a Kafka partition ID (integer) for all events in this channel to be sent to, unless overriden by . By default, if this property is not set, events will be distributed by the Kafka Producer’s partitioner - including by if specified (or by a partitioner specified by ). When set, the producer will take the value of the field named using the value of this property from the event header and send the message to the specified partition of the topic. If the value represents an invalid partition the event will not be accepted into the channel. If the header value is present then this setting overrides . What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted): earliest: automatically reset the offset to the earliest offset latest: automatically reset the offset to the latest offset none: throw exception to the consumer if no previous offset is found for the consumer’s group anything else: throw exception to the consumer. List of brokers in the Kafka cluster used by the channel This can be a partial list of brokers, but we recommend at least two for HA. The format is comma separated list of hostname:port When no Kafka stored offset is found, look up the offsets in Zookeeper and commit them to Kafka. This should be true to support seamless Kafka client migration from older versions of Flume. Once migrated this can be set to false, though that should generally not be required. If no Zookeeper offset is found the kafka.consumer.auto.offset.reset configuration defines how offsets are handled. Due to the way the channel is load balanced, there may be duplicate events when the agent first starts up Secure authentication as well as data encryption is supported on the communication channel between Flume and Kafka. For secure authentication SASL/GSSAPI (Kerberos V5) or SSL (even though the parameter is named SSL, the actual protocol is a TLS implementation) can be used from Kafka version 0.9.0. There is a performance degradation when SSL is enabled, the magnitude of which depends on the CPU type and the JVM implementation. Reference: Kafka security overview and the jira for tracking this issue: KAFKA-2561 Please read the steps described in Configuring Kafka Clients SSL to learn about additional configuration settings for fine tuning for example any of the following: security provider, cipher suites, enabled protocols, truststore or keystore types. Specyfing the truststore is optional here, the global truststore can be used instead. For more details about the global SSL setup, see the SSL/TLS support section. Note: By default the property is not defined, so hostname verification is not performed. In order to enable hostname verification, set the following properties Once enabled, clients will verify the server’s fully qualified domain name (FQDN) against one of the following two fields: If client side authentication is also required then additionally the following needs to be added to Flume agent configuration or the global SSL setup can be used (see SSL/TLS support section). Each Flume agent has to have its client certificate which has to be trusted by Kafka brokers either individually or by their signature chain. Common example is to sign each client certificate by a single Root CA which in turn is trusted by Kafka brokers. If keystore and key use different password protection then property will provide the required additional secret for both consumer and producer keystores: To use Kafka channel with a Kafka cluster secured with Kerberos, set the properties noted above for producer and/or consumer. The Kerberos keytab and principal to be used with Kafka brokers is specified in a JAAS file’s “KafkaClient” section. “Client” section describes the Zookeeper connection if needed. See Kafka doc for information on the JAAS file contents. The location of this JAAS file and optionally the system wide kerberos configuration can be specified via JAVA_OPTS in flume-env.sh: Sample JAAS file. For reference of its content please see client config sections of the desired authentication mechanism (GSSAPI/PLAIN) in Kafka documentation of SASL configuration. Since the Kafka Source may also connect to Zookeeper for offset migration, the “Client” section was also added to this example. This won’t be needed unless you require offset migration, or you require this section for other secure components. Also please make sure that the operating system user of the Flume processes has read privileges on the jaas and keytab files. The directory where the checkpoint is backed up to. This directory must not be the same as the data directories or the checkpoint directory The maximum size of transaction supported by the channel Amount of time (in sec) to wait for a put operation Controls if a checkpoint is created when the channel is closed. Creating a checkpoint on close speeds up subsequent startup of the file channel by avoiding replay. List of all keys (e.g. history of the activeKey setting) By default the File Channel uses paths for checkpoint and data directories that are within the user home as specified above. As a result if you have more than one File Channel instances active within the agent, only one will be able to lock the directories and cause the other channel initialization to fail. It is therefore necessary that you provide explicit paths to all the configured channels, preferably on different disks. Furthermore, as file channel will sync to disk after every commit, coupling it with a sink/source that batches events together may be necessary to provide good performance where multiple disks are not available for checkpoint and data directories. Generating a key with the password the same as the key store password: The events are stored in an in-memory queue and on disk. The in-memory queue serves as the primary store and the disk as overflow. The disk store is managed using an embedded File channel. When the in-memory queue is full, additional incoming events are stored in the file channel. This channel is ideal for flows that need high throughput of memory channel during normal operation, but at the same time need the larger capacity of the file channel for better tolerance of intermittent sink side outages or drop in drain rates. The throughput will reduce approximately to file channel speeds during such abnormal situations. In case of an agent crash or restart, only the events stored on disk are recovered when the agent comes online. This channel is currently experimental and not recommended for use in production. Maximum number of events stored in memory queue. To disable use of in-memory queue, set this to zero. Maximum number of events stored in overflow disk (i.e File channel). To disable use of overflow, set this to zero. Defines the percent of buffer between byteCapacity and the estimated total size of all events in the channel, to account for data in headers. See below. Maximum bytes of memory allowed as a sum of all events in the memory queue. The implementation only counts the Event , which is the reason for providing the configuration parameter as well. Defaults to a computed value equal to 80% of the maximum memory available to the JVM (i.e. 80% of the -Xmx value passed on the command line). Note that if you have multiple memory channels on a single JVM, and they happen to hold the same physical events (i.e. if you are using a replicating channel selector from a single source) then those event sizes may be double-counted for channel byteCapacity purposes. Setting this value to will cause this value to fall back to a hard internal limit of about 200 GB. Estimated average size of events, in bytes, going into the channel Any file channel property with the exception of ‘keep-alive’ and ‘capacity’ can be used. The keep-alive of file channel is managed by Spillable Memory Channel. Use ‘overflowCapacity’ to set the File channel’s capacity. To disable the use of the in-memory queue and function like a file channel: The Pseudo Transaction Channel is only for unit testing purposes and is NOT meant for production use. The max number of events stored in the channel Timeout in seconds for adding or removing an event A custom channel is your own implementation of the Channel interface. A custom channel’s class and its dependencies must be included in the agent’s classpath when starting the Flume agent. The type of the custom channel is its FQCN. Required properties are in bold. In the above configuration, c3 is an optional channel. Failure to write to c3 is simply ignored. Since c1 and c2 are not marked optional, failure to write to those channels will cause the transaction to fail. Load balancing channel selector provides the ability to load-balance flow over multiple channels. This effectively allows the incoming data to be processed on multiple threads. It maintains an indexed list of active channels on which the load must be distributed. Implementation supports distributing load using either via round_robin or random selection mechanisms. The choice of selection mechanism defaults to round_robin type, but can be overridden via configuration. A custom channel selector is your own implementation of the ChannelSelector interface. A custom channel selector’s class and its dependencies must be included in the agent’s classpath when starting the Flume agent. The type of the custom channel selector is its FQCN. Sink groups allow users to group multiple sinks into one entity. Sink processors can be used to provide load balancing capabilities over all sinks inside the group or to achieve fail over from one sink to another in case of temporal failure. Space-separated list of sinks that are participating in the group Default sink processor accepts only a single sink. User is not forced to create processor (sink group) for single sinks. Instead user can follow the source - channel - sink pattern that was explained above in this user guide. Failover Sink Processor maintains a prioritized list of sinks, guaranteeing that so long as one is available events will be processed (delivered). The failover mechanism works by relegating failed sinks to a pool where they are assigned a cool down period, increasing with sequential failures before they are retried. Once a sink successfully sends an event, it is restored to the live pool. The Sinks have a priority associated with them, larger the number, higher the priority. If a Sink fails while sending a Event the next Sink with highest priority shall be tried next for sending Events. For example, a sink with priority 100 is activated before the Sink with priority 80. If no priority is specified, thr priority is determined based on the order in which the Sinks are specified in configuration. Space-separated list of sinks that are participating in the group Priority value. <sinkName> must be one of the sink instances associated with the current sink group A higher priority value Sink gets activated earlier. A larger absolute value indicates higher priority The maximum backoff period for the failed Sink (in millis) Load balancing sink processor provides the ability to load-balance flow over multiple sinks. It maintains an indexed list of active sinks on which the load must be distributed. Implementation supports distributing load using either via or selection mechanisms. The choice of selection mechanism defaults to type, but can be overridden via configuration. Custom selection mechanisms are supported via custom classes that inherits from . When invoked, this selector picks the next sink using its configured selection mechanism and invokes it. For and In case the selected sink fails to deliver the event, the processor picks the next available sink via its configured selection mechanism. This implementation does not blacklist the failing sink and instead continues to optimistically attempt every available sink. If all sinks invocations result in failure, the selector propagates the failure to the sink runner. If is enabled, the sink processor will blacklist sinks that fail, removing them for selection for a given timeout. When the timeout ends, if the sink is still unresponsive timeout is increased exponentially to avoid potentially getting stuck in long waits on unresponsive sinks. With this disabled, in round-robin all the failed sinks load will be passed to the next sink in line and thus not evenly balanced Space-separated list of sinks that are participating in the group The sink and the sink both support the interface. Details of the EventSerializers that ship with Flume are provided below. Alias: . This interceptor writes the body of the event to an output stream without any transformation or modification. The event headers are ignored. Configuration options are as follows: Whether a newline will be appended to each event at write time. The default of true assumes that events do not contain newlines, for legacy reasons. This interceptor serializes Flume events into an Avro container file. The schema used is the same schema used for Flume events in the Avro RPC mechanism. This serializes Flume events into an Avro container file like the “Flume Event” Avro Event Serializer, however the record schema is configurable. The record schema may be specified either as a Flume configuration property or passed in an event header. To pass the record schema as part of the Flume configuration, use the property as listed below. To pass the record schema in an event header, specify either the event header containing a JSON-format representation of the schema or with a URL where the schema may be found ( URIs are supported). Flume has the capability to modify/drop events in-flight. This is done with the help of interceptors. Interceptors are classes that implement interface. An interceptor can modify or even drop events based on any criteria chosen by the developer of the interceptor. Flume supports chaining of interceptors. This is made possible through by specifying the list of interceptor builder class names in the configuration. Interceptors are specified as a whitespace separated list in the source configuration. The order in which the interceptors are specified is the order in which they are invoked. The list of events returned by one interceptor is passed to the next interceptor in the chain. Interceptors can modify or drop events. If an interceptor needs to drop events, it just does not return that event in the list that it returns. If it is to drop all events, then it simply returns an empty list. Interceptors are named components, here is an example of how they are created through configuration: Note that the interceptor builders are passed to the type config parameter. The interceptors are themselves configurable and can be passed configuration values just like they are passed to any other configurable component. In the above example, events are passed to the HostInterceptor first and the events returned by the HostInterceptor are then passed along to the TimestampInterceptor. You can specify either the fully qualified class name (FQCN) or the alias . If you have multiple collectors writing to the same HDFS path, then you could also use the HostInterceptor. This interceptor inserts the hostname or IP address of the host that this agent is running on. It inserts a header with key or a configured key whose value is the hostname or IP address of the host, based on configuration. This interceptor manipulates Flume event headers, by removing one or many headers. It can remove a statically defined header, headers based on a regular expression or headers in a list. If none of these is defined, or if no header matches the criteria, the Flume events are not modified. Regular expression used to separate multiple header names in the list specified by . Default is a comma surrounded by any number of whitespace characters Consider using UUIDInterceptor to automatically assign a UUID to an event if no application level unique key for the event is available. It can be important to assign UUIDs to events as soon as they enter the Flume network; that is, in the first Flume Source of the flow. This enables subsequent deduplication of events in the face of replication and redelivery in a Flume network that is designed for high availability and high performance. If an application level key is available, this is preferable over an auto-generated UUID because it enables subsequent updates and deletes of event in data stores using said well known application level key. The name of the Flume header to modify This interceptor filters the events through a morphline configuration file that defines a chain of transformation commands that pipe records from one command to another. For example the morphline can ignore certain events or alter or insert certain event headers via regular expression based pattern matching, or it can auto-detect and set a MIME type via Apache Tika on events that are intercepted. For example, this kind of packet sniffing can be used for content based dynamic routing in a Flume topology. MorphlineInterceptor can also help to implement dynamic routing to multiple Apache Solr collections (e.g. for multi-tenancy). Currently, there is a restriction in that the morphline of an interceptor must not generate more than one output record for each input event. This interceptor is not intended for heavy duty ETL processing - if you need this consider moving ETL processing from the Flume Source to a Flume Sink, e.g. to a MorphlineSolrSink. The relative or absolute path on the local file system to the morphline configuration file. Example: This interceptor provides simple string-based search-and-replace functionality based on Java regular expressions. Backtracking / group capture is also available. This interceptor uses the same rules as in the Java Matcher.replaceAll() method. The charset of the event body. Assumed by default to be UTF-8. # Remove leading alphanumeric characters in an event body. This interceptor filters events selectively by interpreting the event body as text and matching the text against a configured regular expression. The supplied regular expression can be used to include events or exclude events. This interceptor extracts regex match groups using a specified regular expression and appends the match groups as headers on the event. It also supports pluggable serializers for formatting the match groups before adding them as event headers. Space-separated list of serializers for mapping matches to header names and serializing their values. (See example below) Flume provides built-in support for the following serializers: Must be (org.apache.flume.interceptor.RegexExtractorInterceptorPassThroughSerializer), , or the FQCN of a custom class that implements The serializers are used to map the matches to a header name and a formatted header value; by default, you only need to specify the header name and the default will be used. This serializer simply maps the matches to the specified header name and passes the value through as it was extracted by the regex. You can plug custom serializer implementations into the extractor using the fully qualified class name (FQCN) to format the matches in anyway you like. If the Flume event body contained and the following configuration was used The extracted event will contain the same body but the following headers will have been added If the Flume event body contained and the following configuration was used the extracted event will contain the same body but the following headers will have been added If this property is specified then the Flume agent will continue polling for the config file even if the config file is not found at the expected location. Otherwise, the Flume agent will terminate if the config doesn’t exist at the expected location. No property value is needed when setting this property (eg, just specifying -Dflume.called.from.service is enough) Flume periodically polls, every 30 seconds, for changes to the specified config file. A Flume agent loads a new configuration from the config file if either an existing file is polled for the first time, or if an existing file’s modification date has changed since the last time it was polled. Renaming or moving a file does not change its modification time. When a Flume agent polls a non-existent file then one of two things happens: 1. When the agent polls a non-existent config file for the first time, then the agent behaves according to the flume.called.from.service property. If the property is set, then the agent will continue polling (always at the same period – every 30 seconds). If the property is not set, then the agent immediately terminates. ...OR... 2. When the agent polls a non-existent config file and this is not the first time the file is polled, then the agent makes no config changes for this polling period. The agent continues polling rather than terminating."
    },
    {
        "link": "https://stackoverflow.com/questions/26734829/using-kafka-to-import-data-to-hadoop",
        "document": "Firstly I was thinking what to use to get events into Hadoop, where they will be stored and periodically analysis would be performed on them (possibly using Ooozie to schedule periodic analysis) Kafka or Flume, and decided that Kafka is probably a better solution, since we also have a component that does event processing, so in this way, both batch and event processing components get data in the same way.\n\nBut know I'm looking for suggestions concretely how to get data out of broker to Hadoop.\n\nI found here that Flume can be used in combination with Kafka\n\nAnd also found on the same page and in Kafka documentation that there is something called Camus\n• Camus - LinkedIn's Kafka=>HDFS pipeline. This one is used for all data at LinkedIn, and works great.\n\nI'm interested in what would be a better (and easier, better documented solution) to do that? Also, are there any examples or tutorials how to do it?\n\nWhen should I use this variants over simpler, High level consumer?\n\nI'm opened for suggestions if there is another/better solution than this two."
    },
    {
        "link": "https://kafka.apache.org/documentation",
        "document": "Here is a description of a few of the popular use cases for Apache Kafka®. For an overview of a number of these areas in action, see this blog post.\n\nKafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.\n\nIn our experience messaging uses are often comparatively low-throughput, but may require low end-to-end latency and often depend on the strong durability guarantees Kafka provides.\n\nIn this domain Kafka is comparable to traditional messaging systems such as ActiveMQ or RabbitMQ.\n\nThe original use case for Kafka was to be able to rebuild a user activity tracking pipeline as a set of real-time publish-subscribe feeds. This means site activity (page views, searches, or other actions users may take) is published to central topics with one topic per activity type. These feeds are available for subscription for a range of use cases including real-time processing, real-time monitoring, and loading into Hadoop or offline data warehousing systems for offline processing and reporting.\n\nActivity tracking is often very high volume as many activity messages are generated for each user page view.\n\nKafka is often used for operational monitoring data. This involves aggregating statistics from distributed applications to produce centralized feeds of operational data.Many people use Kafka as a replacement for a log aggregation solution. Log aggregation typically collects physical log files off servers and puts them in a central place (a file server or HDFS perhaps) for processing. Kafka abstracts away the details of files and gives a cleaner abstraction of log or event data as a stream of messages. This allows for lower-latency processing and easier support for multiple data sources and distributed data consumption. In comparison to log-centric systems like Scribe or Flume, Kafka offers equally good performance, stronger durability guarantees due to replication, and much lower end-to-end latency.Many users of Kafka process data in processing pipelines consisting of multiple stages, where raw input data is consumed from Kafka topics and then aggregated, enriched, or otherwise transformed into new topics for further consumption or follow-up processing. For example, a processing pipeline for recommending news articles might crawl article content from RSS feeds and publish it to an \"articles\" topic; further processing might normalize or deduplicate this content and publish the cleansed article content to a new topic; a final processing stage might attempt to recommend this content to users. Such processing pipelines create graphs of real-time data flows based on the individual topics. Starting in 0.10.0.0, a light-weight but powerful stream processing library called Kafka Streams is available in Apache Kafka to perform such data processing as described above. Apart from Kafka Streams, alternative open source stream processing tools include Apache Storm and Apache Samza Event sourcing is a style of application design where state changes are logged as a time-ordered sequence of records. Kafka's support for very large stored log data makes it an excellent backend for an application built in this style.Kafka can serve as a kind of external commit-log for a distributed system. The log helps replicate data between nodes and acts as a re-syncing mechanism for failed nodes to restore their data. The log compaction feature in Kafka helps support this usage. In this usage Kafka is similar to Apache BookKeeper project.There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.\n\nKafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, exactly-once processing semantics and simple yet efficient management of application state.\n\nKafka Streams has a low barrier to entry: You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads. Kafka Streams transparently handles the load balancing of multiple instances of the same application by leveraging Kafka's parallelism model.\n\nTo learn more about Kafka Streams, visit the Kafka Streams page."
    },
    {
        "link": "https://tencentcloud.com/document/product/1026/34565",
        "document": ""
    },
    {
        "link": "https://medium.com/@sanchit10gawde/dealing-with-unstructured-data-kafka-spark-integration-2b2074868336",
        "document": "Introduction:\n\nApache Kafka is a community distributed event streaming platform capable of handling trillions of events a day. Initially conceived as a messaging queue, Kafka is based on an abstraction of a distributed commit log. Since being created and open sourced by LinkedIn in 2011, Kafka has quickly evolved from messaging queue to a full-fledged event streaming platform.\n• The Producer API allows an application to publish a stream of records to one or more Kafka topics.\n• The Consumer API allows an application to subscribe to one or more topics and process the stream of records produced to them.\n• The Streams API allows an application to act as a stream processor, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.\n• The Connector API allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table.\n\nSpark Streaming is an extension of the core SparkAPI that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data.\n\nSpark Streaming is used for processing real-time data/near real-time data acquired from Flume, Kafka, Kinesis API’s.\n\nSpark Streaming is different from other systems that either has a processing engine designed only for streaming or have similar batch and streaming APIs but compile internally to different engines. Spark’s single execution engine and unified programming model for batch and streaming lead to some unique benefits over other traditional streaming systems.\n• Receive streaming data from data sources (e.g. live logs, system telemetry data, IoT device data, etc.) into some data ingestion system like Apache Kafka, Amazon Kinesis, etc.\n• Process the data in parallel on a cluster. This is what stream processing engines are designed to do, as we will discuss in detail next.\n• Output the results out to downstream systems like HBase, Cassandra, Kafka, etc.\n• Kafka installation is done on the Cloudera distribution of Hadoop (cdh-5.13). Cloudera Quickstart does not come up with pre-installed Kafka. The kafka -2.12 (Latest Release) can be downloaded from Apache Kafka site. Link\n• Java version needs to be updated to 1.8 as kafka requires 1.8 Java version\n• For spark-streaming, we need to download scala version 2.11 and sbt for creating an application to run Spark Submit jobs.\n• Scala-2.11/2.10(Download any of this) Download Link: Link\n• Once downloaded untar the file eg. As follows:\n• Update the environment variables in .bash_profile as follows:\n• To access the web-server logs through Flume, sink it into Kafka and process the unstructured data to gain insights from it using Spark Streaming analytics.\n\nThe reason for kafka flume integration is that when it comes to web-server logs which are already getting the messages. We can read these messages by deploying flume on it. As in the case of Kafka, we cannot directly ingest data from producers to consumers as in web-server logs.\n\nIf we are dealing with a new application then flume-kafka integration is preferable. Flume is not reliable because there will be more number of sinks required to publish targets which is unpreferable.\n\nKafka, on the other hand, proves more stability and reliability as compared to Flume. When we are dealing with a new application then Kafka is more preferable over Flume.\n\nTo work with web-server logs we will use gen_logs to load web logs into the flume. Cloudera comes with pre-installed gen_logs. To install gen_logs go through this video once- Installation-Link\n\nWe will work with access.logs and try to get the department insights from the log file\n• To start tracking log files: Execute the following start_logs.sh shell script.\n• To check whether we have started to get log files from the web server:\n\nConfiguration of flume .conf file for setting up sinkers, memory, channels.\n• Create a directory and .conf file inside it for Flume implementation.\n• Following is configuration setup for flume1.conf.\n• To initially start with kafka streaming and as well as kafka server initializing of zookeeper is the first step. To initialize zookeeper:\n• It might happen that zookeeper port address might be already in use to kill the PID following is the command.\n• We can see that PID-5292 with zookeeper instance is running we need to kill it this can be done by:\n• Once the zookeeper port address is cleared run the following command to start the zookeeper instance for kafka server initialization:\n• Open another terminal and now start Kafka server.\n• To verify the topic created by flume sink that if the consumer is receiving web server log files:\n• Check for the SBT and Scala is installed properly. Check for the version of both scala and sbt as follows.\n• Setup SBT project for the use of spark-submit. Create a directory and setup build.sbt for installing spark dependencies.\n• Now create .scala file for processing web server logs in spark-streaming. It has to be in the same location as build.sbt\n• Following is the link to the spark-streaming documentation-Link\n• The code is developed to access the department trace to get a count of different type of department pages that were accessed during per session of the user.\n• The each RDD created result from the kafka web logs is stored into HDFS at the directory location “Data”\n• ssc.start() starts away the spark streaming context which starts listening to the kafka weblogs\n• sss.awaitTermination() keeps on running the program until the user manually stops the program.\n• Once the file is saved do sbt package to compile the file. This will also download the spark dependencies which we have mentioned in build.sbt\n• sbt package also creates a jar file in the target directory which will be used by us in spark-submit job\n• Now we need to download jar files or provide jar file path for spark-submit job to avoid any kind of major-minor version errors. Jar file required: Link\n• Initiate spark-submit job to listen to kafka logs and process data according to the requirements.\n• — class: It is the class name of the program which we have mentioned.\n• — master local[2]: It depicts the spark job will run on localhost with 2 executors\n• — conf: This will create a tracking job at port no.12689"
    }
]