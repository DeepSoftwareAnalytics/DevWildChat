[
    {
        "link": "https://docs.juliaplots.org",
        "document": ""
    },
    {
        "link": "https://docs.juliaplots.org/latest/backends",
        "document": "Backends are the lifeblood of Plots, and the diversity between features, approaches, and strengths/weaknesses was one of the primary reasons that I started this package.\n\nFor those who haven't had the pleasure of hacking on 15 different plotting APIs: first, consider yourself lucky. However, you will probably have a hard time choosing the right backend for your task at hand. This document is meant to be a guide and introduction to make that choice.\n\nMy favorites: for speed, for interactivity, for REPL/SSH and otherwise.\n\nOf course this list is rather subjective and nothing in life is that simple. Likely there are subtle tradeoffs between backends, long hidden bugs, and more excitement. Don't be shy to try out something new !\n\nThe default backend. Very fast with lots of plot types. Still actively developed and improving daily.\n\nIt is possible to use more features of via the mechanism.\n\nThese are treated as separate backends, though they share much of the code and use the Plotly JavaScript API. is the only dependency-free plotting option, as the required JavaScript is bundled with Plots. It can create inline plots in IJulia, or open standalone browser windows when run from the Julia REPL.\n\nis the preferred option, and taps into the great functionality of Spencer Lyon's PlotlyJS.jl. Inline IJulia plots can be updated from any cell... something that makes this backend stand out. From the Julia REPL, it taps into Blink.jl and Electron to plot within a standalone GUI window... also very cool. Also, PlotlyJS supports saving the output to more formats than Plotly, such as EPS and PDF, and thus is the recommended version of Plotly for developing publication-quality figures.\n\nPlotly needs to load MathJax to render LaTeX strings, therefore passing extra keywords with is implemented. With that it is possible to pass a header to the extra keyword. It has the following options:\n• include the standard online version of the header\n\nThese can also be passed using the keyword.\n\nIt is possible to add additional arguments to the plotly series and layout dictionaries via the mechanism. Arbitrary arguments are supported but one needs to be careful since no checks are performed and thus it is possible to unintentionally overwrite existing entries.\n\nFor example adding customdata can be done the following way . One can also pass multiple extra arguments to plotly.\n\nA Julia wrapper around the popular python package . It uses to pass data with minimal overhead.\n\nIt is possible to use more features of via the mechanism. For example, for a 3D plot, the following example should generate a colorbar at a proper location; without the below, the colorbar is displayed too far right to see its ticks and numbers. The four coordinates in the example below, i.e., specify the colorbar location . Note that for 2D plots, this fine tuning is not necessary.\n\nHas more features and is still in development otherwise the same.\n• Lots of functionality (though the code is still WIP)\n• Plots <–> PGFPlotsX link code: Simon Christ (@BeastyBlacksmith), based on the code of Patrick Kofod Mogensen (@pkofod)\n\nTo use the native LaTeX output of the backend you can save your plot as a or file.\n\nSaving as file has the advantage, that you can use to rescale your plot without changing the size of the fonts. The default LaTeX output is intended to be included as a figure in another document and will not compile by itself. If you include these figures in another LaTeX document you need to have the correct preamble. The preamble of a plot can be shown using or copied from the standalone output.\n\nIt is possible to use more features of via the mechanism. By default it interprets every extra keyword as an option to the command. Setting will treat them as an option to the command and will be treated as an option to the environment.\n\nFor example changing the colormap to one that is native to pgfplots can be achieved with the following. Like this it is possible to keep the preamble of latex documents clean.\n\nFurther more additional commands or strings can be added via the special keyword. This adds a square to a normal line plot:\n\nSimple and lightweight. Plot directly in your terminal. You won't produce anything publication quality, but for a quick look at your data it is awesome. Allows plotting over a headless node (SSH).\n\nIt is possible to use more features of via the mechanism.\n\nis a direct interface to gnuplot, a cross platform command line driven plotting utility. The integration of in is recent (2021), but a lot of features are supported.\n\nFast plotting with a responsive GUI (optional). Target: quickly identify design/simulation issues & glitches in order to shorten design iterations.\n• Relatively short load times / time to first plot.\n• Interactive mouse/keybindings.\n• Fast & simple way to pan/zoom into data.\n• Designed with larger datasets in mind.\n• Confirmed to handle 2GB datsets with reasonable speed on older desktop running Windows 7 (drag+pan of data area highly discouraged).\n\nWrite plot + data to a single file using a human-readable structure that can easily be reverse-engineered.\n• (Re)-render plots at a later time using your favourite backend(s).\n• Currently missing support for & .\n• (Please open an \"issue\" if you have a need).\n• Not yet designed for backwards compatibility (no proper versioning).\n• Therefore not truly adequate for archival purposes at the moment.\n• Currently implemented as a \"backend\" to avoid adding dependencies to .\n\nbased backend, using and . Superseded by and . Whilst still supported in , users are advised to transition to the backend.\n• Lots of functionality (though the code is still WIP)\n\nA Julia implementation inspired by the \"Grammar of Graphics\".\n• Flexible when combined with Compose.jl (inset plots, etc.)\n\nBuilt on top of Gadfly, Immerse adds some interactivity and a standalone GUI window, including zoom/pan and a cool \"point lasso\" tool to save Julia vectors with the selected data points.\n\nMy package which wraps PyQwt. Similar to PyPlot, it uses PyCall to convert calls to python. Though Qwt.jl was the \"first draft\" of Plots, the functionality is superceded by other backends, and it's not worth my time to maintain.\n\nUnfinished, but very similar to PlotlyJS... use that instead.\n\nFunctionality incomplete... I never finished wrapping it, and I don't think it offers anything beyond other backends. However, the plots are clean looking and it's relatively fast."
    },
    {
        "link": "https://discourse.julialang.org/t/plots-or-otherwise/104162",
        "document": "After 1.5 years of Julia experience, I now write almost everything in Julia. I used to use Bourne shell, Ruby, Fortran, and a bit of Haskell, but I don’t need these any longer. Except I don’t know which plotting package is the right one for me. (And except for some things that Haskell does very well, like lazy recursion.) When I started Julia, I read one of the official Julia webpages, which said that is the unified interface and you can switch backends to suit your needs. I thought, that’s a great idea! So, since then, I’ve been exclusively using . It’s much more elegant and easier to use than “traditional” plotting methods common in other languages and environments. But I found it sometimes very hard to find information about . I search Google and Julia Discourse, but almost always the information I find is exclusive to a particular package, like Makie and Matplotlib. The syntax I find there is totally different from ’s. So, my question No.1 is: Is it possible to mix two ways? using Plots use_somebackend() p = plot( . . . ) function_specific_to_the_backend(p, . . .) # or something like this. Also, sometimes I ask questions about plotting in this forum and I often get an answer like “package X can do that” and I end up not knowing how to replicate it in with X as the backend. I don’t know whether it’s a problem of documentation of or whether the capability isn’t accessible form . I don’t think I understand the architecture of . I’m not talking about large things like contouring. I know contouring is available in . I’m talking about smaller things, like having three or more y axes for a single graph, specifying the absolute positions of the axes, controlling the the precision of numeric labels, specifying fonts of the letterings, plotting an arbitrary polygons and arrows on an existing plot, open-ended contouring levels, specifying exact colors for contour levels, extending plot elements outside the plotting rectangle, availability of geographical maps (country boundaries, elevation of land surface, etc.) without plotting maps myself, availability of map projections, etc., etc. These aren’t big things, but I need them and since I’m not able to find out how to do these with , I give up Julia for producing figures and go back to an old plotting program I’m familiar with and have almost total control of. So, my question No. 2 is, does do almost everything and is it just a documentation problem? Or is it that is still very young and its future is promising even if it doesn’t have those capabilities now? Or should I find a suitable package to me and learn its specific ways? Then, supposing I’ll be migrating to another package, I still find it hard to decide on which one I should explore. Because I need to plot geographical maps, I search for it and find that seems to be a great package. But when I search for other capabilities, sometimes and sometimes another package, etc. I’m a full-time researcher and I use programming languages and tools for my scientific work. I don’t have too much time to explore various packages before I decide on which one to go with. As an aside . . . I’m the only Julia user among my colleagues as far as I know. A large fraction of my colleagues use Python or are switching to Python. It seems all of them use one well known plotting package. Is it called matplotlib? I then look over what matplotlib looks like and am dismayed to see its . . . how to put it . . . complicated user interface. I’m attracted to Julia, Ruby, and Haskell because of their elegance. The package suits Julia very well. Python just works but it doesn’t look fun to program in it. As far as I can tell from the tutorials, matplotlib looks like it’s not much fun.\n\ndo almost everything and is it just a documentation problem? No definitely doesn’t do everything. Personally, I’ve found to be the perfect plotting package for exploration. That is, to quickly look at things while I’m doing research, e.g., in a Jupyter Notebook. The package syntax is just very short and flexible. The multiple backends also come in handy. One of my favorites is UnicodePlots, which I love to use as part of running package tests in the terminal (but then switch to a different backend if I really need to see details). However, is not very good when you need extreme customization, most likely for generating plots for published papers. Personally, I’m still finding Python’s matplotlib unsurpassed, although Makie looks interesting if you’re looking for something in pure Julia.\n\nPublication quality may mean many things (features or font control, etc), but for the most typical publication plots I find the default Plots + GR with some customization options quite satisfactory, I should’ve mentioned that I’ve already published a scientific paper that includes a few line graphs (similar to the one you show) which I plotted with the package When I talk about “publication quality”, I certainly agree that the quality of plots from the package is high, higher than those I’ve produced with other tools. But that kind of “qtuality” isn’t what I’m after. I’ve hit the wall of customizability with . That is, I need a different kind of “quality”. and I didn’t feel the need to move to something else for the moment: To me, the current show stoppers of are 1) the inability (or difficulty, I can’t tell which) of producing publication quality contour plots with open-ended contour intervals and 2) the inability of specifying the absolute positions of the axes. The margins can be precisely controlled but the axes shift depending on the letterings (labels) plotted outside the axes. I often need to align separately plotted figures and graphs but if the positions and lengths of the axes are different between the plots, the alignment requires a tedious trial-and-error. For problem (1), I’ve found the PlotLy backend kind of produces not-too-bad plots, except that I can’t save them to image files because of a bug (it’s reported but taking some time). Then, I switched to another backend (pyplot?), which plotted a fine contour plot with open ended levels, but its contouring was with continuous color shading and without contour lines . . . Then I didn’t know how to get discrete color levels and also I had to manually overlay contour lines . . . You see, there is so much work before you get what you need. I’ll likely discover other showstoppers as/if I keep using . As I said in my initial posting, whenever a problem of customizability occurs, I search the net and Discourse, and was defeated in many cases. Initially my skill grew rapidly, but for this half year, it has stopped growing, having not been able to solve my problems.\n\nIf there is an easy way to upload an image, please tell me. In words . . . suppose you have a 2D variable which ranges from -100 to 100, but you want to make a contour plot over -10 to 10 and you want to paint -100 to -10 with the lower extreme color and 10 to 100 with the higher extreme color. The colorbar must indicate the fact. The most common way is to have a little downward pointing triangle for the -100 to -10 range and an upward pointing triangle for the 10 to 100 range at the lower and upper ends of the colorbar. In other words, I need contour levels of [-Inf, -10, -8, -6, . . . , 6, 8, 10, Inf] and I need the colorbar to indicate the fact. In my field, the standard plotting programs automatically do this. I’ve submitted a feature request to the package github to implement this feature for the GR backend.\n\nHave you really look at it? Because it can do all the things you mentioned here. I believe you. No, I haven’t looked at it carefully. But, I don’t understand what you intend by that question. I didn’t dismiss . All I’m saying is, I haven’t been able to find out how to produce what I need with . And I showed a few examples of that (because one person said that `Plots.jl’ is good enough.) That’s all. Then, you folks indicate what I need can be accomplished by packages X, Y, Z, etc. So, there is probably misunderstanding. I’m not saying that what I want cannot be done by the existing packages. I expect what I need can be done by any graphics packages. That’s why I’m frustrated with in the first place."
    },
    {
        "link": "https://plotly.com/julia/getting-started",
        "document": "The Julia library is an interactive, open-source plotting library that supports over 40 unique chart types covering a wide range of statistical, financial, geographic, scientific, and 3-dimensional use-cases.\n\nBuilt on top of the Plotly JavaScript library (plotly.js), enables Julia users to create beautiful interactive web-based visualizations that can be displayed in Jupyter notebooks, saved to standalone HTML files, or served as part of pure Julia-built web applications using Dash.jl.\n\nThanks to deep integration with our Kaleido image export utility, also provides great support for non-web contexts including desktop editors (e.g. QtConsole, Spyder, PyCharm) and static document publishing (e.g. exporting notebooks to PDF with high-quality vector images).\n\nThis Getting Started guide explains how to install and related optional pages. Once you've installed, you can use our documentation in three main ways:\n\nFor information on using Julia to build web applications containing plotly figures, see the .\n\nWe also encourage you to join the Plotly Community Forum if you want help with anything related to plotly.\n\nTo install PlotlyJS.jl, open up a Julia REPL, press to enter package mode and type:\n\nFor existing users you can run from the package manager REPL mode to get the latest release. If after doing this plots do not show up in your chosen frontend, please run (again from pkg REPL mode) to tell Julia to download the latest release of the plotly.js javascript library.\n\nOnce installation is complete, you are ready to create beautiful plotly.js powered charts!\n\nIf you are working at the Julia REPL, a dedicated plotting window (powered by electron and driven by Blink.jl) will be used to display your plot. If you are working in a Jupyter notebook or Jupyter lab, the plot will appear inline.\n\nIf instead, you would like to save the chart for external viewing you can do so using the method. The type of file will be inferred from the extension on the filename (e.g. will produce a pdf, while will produce a standalone html file).\n\nmakes use of Requires.jl to build optional integrations with other parts of the Julia package ecosystem. These optional dependencies include:\n• None DataFrames.jl: A widely used implementation of the DataFrame standard for tabular data in Julia. The PlotlyJS.jl integration with DataFrames is deep, yet elegant. By implementing a method , PlotlyJS.jl obtains many advanced functionalities present in other libraries, such as from the python library. Many examples throughout the documentation here use this method for the function.\n• None CSV.jl: The de-facto standard for reading csv data files in Julia. After importing and , a function is defined that can load a handful of datasets commonly used in plotting examples. If you have also loaded , an additional method is defined that will automatically load the desired dataset into a DataFrame\n• None IJulia.jl: The Jupyter kernel implementation for Julia. If you use jupyter notebooks (or Jupyter lab!), PlotlyJS.jl will be ready to go right out of the box thanks to our integration with IJulia\n• None Distributions.jl: The canonical way to represent and work with probability distributions in Julia. The integration with Distributions.jl adds the method to quickly and easily view the pdf of the distribution . Appropriate ranges are automatically selected based on the quantiles of the distribution.\n• None Colors.jl: A very feature complete library for representing and working with colors in Julia. Anywhere plotly.js accepts a color (for example , or , , etc.) Julia users can also pass an instance of any type from the Colors.jl library. This\n• None ColorSchemes.jl: a collection of over 800 ready to use color schemes. All schemes from the ColorSchemes.jl package are ready to be used. They are all exposed under the object that is imported when you run . To access the colorscheme you would use . If you aren't sure which color scheme you'd like to use, you can use tab completion to inspect the many available color schemes. The object also has helpful categorizations of families of color schemes such as , , and . Most often, a color scheme is used to define the property.\n\nTo use any of these optional integrations, all you need to do is import both and the desired package in your Julia code.\n\nWhere to next?\n\nOnce you've installed, you can use our documentation in two main ways:\n• None Jump right in to and see examples of how to make basic charts, statistical charts, scientific charts, financial charts, and 3-dimensional charts.\n• None Read through our documentation on the fundamentals of using plotly from Julia.\n• None Check out our exhaustive reference guide, the Figure Reference\n\nFor information on using Julia to build web applications containing plotly figures, see the .\n\nWe also encourage you to join the Plotly Community Forum if you want help with anything related to ."
    },
    {
        "link": "https://discourse.julialang.org/t/comparison-of-plotting-packages/99860",
        "document": "Julia interface to gnuplot. Contribute to gcalderone/Gnuplot.jl development by creating an account on GitHub."
    },
    {
        "link": "https://enccs.github.io/julia-for-hpda/dataformats-dataframes",
        "document": "We will now explore a Julian approach to a use case common to many scientific disciplines: manipulating data and visualization. Julia is a good language to use for data science problems as it will perform well and alleviate the need to translate computationally demanding parts to another language. Here we will learn how to work with data using the DataFrames package, visualize it with the Plots and StatsPlots. In Julia, a DataFrame is a two-dimensional table-like data structure, similar to a Excel spreadsheet or a SQL table. It is part of the DataFrames.jl package, which provides a powerful and flexible way to manipulate and analyze data in Julia. The rows usually represent independent observations, while the columns represent the features (variables) for each observation. You can perform various operations on a DataFrame, such as filtering, sorting, grouping, joining, and aggregating data. The DataFrames.jl package offers similar functionality as the library in Python and the function in R. also provides a rich set of functions for data cleaning, transformation, and visualization, making it a popular choice for data science and machine learning tasks in Julia. Just like in Python and R, the package provides functionality for data manipulation and analysis. We start by downloading a dataset containing measurements of characteristic features of different penguin species. The dataset is bundled within the package, so we need to add that: We will use DataFrames here to analyze the penguins dataset, but first we need to install it: Here’s how you can create a new dataframe: The following code loads the dataset into a DataFrame. # the raw data can be loaded by Note that the variable is of type ; the PalmerPenguins package uses the CSV.jl package for fast loading of data. Note further that can accept a object and read it into a dataframe! Data can be saved in several common formats such as CSV, JSON, and Parquet using the , , and packages respectively. An overview of common data formats for different use cases can be found here. We can inspect the data using a few basic operations: # slicing and column name (can also use \"island\") # access column directly without copying (editing will change the dataframe) Summary statistics can be displayed with the function: 7×7 DataFrame Row │ variable mean min median max nmissing eltype │ Symbol Union… Any Union… Any Int64 Type ─────┼────────────────────────────────────────────────────────────────────────────────────────── 1 │ species Adelie Gentoo 0 String 2 │ island Biscoe Torgersen 0 String 3 │ bill_length_mm 43.9219 32.1 44.45 59.6 2 Union{Missing, Float64} 4 │ bill_depth_mm 17.1512 13.1 17.3 21.5 2 Union{Missing, Float64} 5 │ flipper_length_mm 200.915 172 197.0 231 2 Union{Missing, Int64} 6 │ body_mass_g 4201.75 2700 4050.0 6300 2 Union{Missing, Int64} 7 │ sex female male 11 Union{Missing, String} We can see in the output of that the element type of all the columns is a union of and a numeric type. This implies that our dataset contains missing values. More about concept can be found in the training by Aalto Scientific Computing: https://aaltoscicomp.github.io/python-for-scicomp/pandas/#tidy-data We can remove these missing values by the or functions (what is the difference between them?): Alternatively, we can use: The code shows how to handle missing data in the column by replacing missing values with a specific value using the function or by interpolating missing values using the package. : This line is creating a logical mask that is wherever there are missing values ( ) in the column and elsewhere. : This line is creating an interpolation object . It’s using only the non-missing values of the column (specified by ) and a linear B-spline interpolation method ( ). : This line is replacing the missing values in the column with the interpolated values. It’s finding the indices of the missing values with and then using the interpolation object to estimate values at these indices. So, in summary, this code is filling in missing values in the column by estimating their value based on a linear interpolation of the non-missing values. This can be a useful way to handle missing data when you don’t want to or can’t simply ignore those missing values. 😊 The data is in a so-called wide format. In data analysis, we often encounter two types of data formats: long format and wide format. https://www.statology.org/long-vs-wide-data/\n• None Long format: In this format, each row is a single observation, and each column is a variable. This format is also known as “tidy” data.\n• None Wide format: In this format, each row is a subject, and each column is an observation. This format is also known as “spread” data. The package provides functions to reshape data between long and wide formats. These functions are , , , and . Further examples can be found in the official documentation. # To convert from wide to long format #First we create an ID column # To convert from long to wide format\n• None Applying some function/modification to each group; This is commonly referred to as “split-apply-combine” workflow, which can be achieved in Julia with the function to stratify and the function to aggregate with some reduction operator. An example of this is provided below: In this example, groups the DataFrame by the and columns. Then, calculates the mean of the column for each group. The function is used for aggregation. The result is a new DataFrame where each unique - combination forms a row, and the mean body mass for each species-island combination fills the DataFrame. In Julia, you can create a DataFrame from scratch using the constructor from the package. This constructor allows you to create a DataFrame by passing column vectors as keyword arguments or pairs. For example, to create a DataFrame with two columns named and , the following works: A DataFrame can also be created from other data structures such as dictionaries, named tuples, vectors of vectors, matrices, and more. You can find more information about creating DataFrames in Julia in the official documentation Also, you can merge two or more DataFrames using the function from the package. This function allows you to perform various types of joins, such as inner join, left join, right join, outer join, semi join, and anti join. You can specify the columns used to determine which rows should be combined during a join by passing them as the argument to the function. For example, to perform an inner join on two DataFrames and using the column as the key, you can use the following code: . You can find more information about joining DataFrames in Julia in the official documentation. Let us now look at different ways to visualize this data. Many different plotting libraries exist for Julia and which one to use will depend on the specific use case as well as personal preference.\n• None Plots.jl: high-level API for working with several different plotting back-ends, including , , and .\n• None GadFly.jl: based largely on ggplot2 for R and the book The Grammar of Graphics. Well suited for statistics and machine learning.\n• None VegaLite.jl: based on Vega-Lite, a grammar of interactive graphics. Great for interactive graphics.\n• None Makie.jl data visualization ecosystem with backends (OpenCL), (Cairo) and (WebGL). Good for publication-quality plotting but can be a bit slow to load and use. We will be using and but we encourage to explore these other packages to find the one that best fits your use case. First we install and backend: In VSCode, the plot should appear in a new plot pane. We can add labels: To add a line to an existing plot, we mutate it with : Finally we can save to the plot to a file: Multiple subplots can be created by: # Four histograms each with 10 points? Why not! First load and set the backend to GR (precompilation of Plots might take some time): For the Penguin dataset it is more appropriate to use scatter plots, for example: We can adjust the markers by this list of named colors and this list of marker types: We can also change the plot theme according to this list of themes, for example: We can add a dimension to the plot by grouping by another column. Let’s see if the different penguin species can be distiguished based on their bill length and bill depth. We also set different marker shapes and colors based on the grouping, and adjust the markersize and transparency ( ). Note that it is also possible to prescribe a palette rather than every colour individually, with many common palettes available here: The function comes from the base package. provides many other types of plot types, for example . To use dataframes with we need to use the macro which allows passing columns as symbols (this can also be used for and other plot functions):\n\nConvert the final plot in the type-along section “Visualizing the Penguin dataset” and convert it into a function:\n• None The function should take as arguments a dataframe and two column symbols.\n• None Use the and functions to automatically set the x-range of the plot using the argument to .\n• None If you have time, try grouping the data by or instead of (keep in mind that you may need to adjust the number of marker symbols and colors).\n• None If you have more time, play around with the plot appearance using and the marker symbols and colors. # markers and colors to use for the groups # number of unique groups can't be larger than the number of colors/markers In this exercise, you will practice reading data from CSV files into DataFrames, manipulating data in DataFrames, and visualizing data using a plotting package.\n• None Install the and packages by running the following commands in the Julia REPL: 2. Set the relative path to the and files in the and variables. Assume that the path to your files is and you are currently in the directory in the Julia REPL. The data is available here: ENCCS/julia-for-hpda and ENCCS/julia-for-hpda This climate data set contains daily mean temperature, humidity, wind speed and mean pressure at a location in Dehli India over a period of several years. The data set was downloaded from here.\n• None Read the data from the CSV files into DataFrames named and using the function.\n• None Use the functions provided by the package to manipulate the data in the DataFrames. For example, you can select columns, filter rows, group data, compute summary statistics, and compute aggregate functions.\n• None Install a plotting package such as or by running the following command in the Julia REPL:\n• None Use the plotting package to create a line plot of the mean of the column for each group in a grouped DataFrame. Customize the appearance of the plot by changing its properties such as color, line style, marker style, etc. Here is one possible solution to this exercise: Once you have read the data from the CSV files into DataFrames, you can manipulate the data using the functions provided by the package. Here are some examples that show how to manipulate data in a DataFrame: This code shows how to select columns, filter rows, group data, compute summary statistics, and compute aggregate functions on a DataFrame named . You can use these and other functions provided by the package to manipulate the data in the DataFrame. The function is part of the standard library module in Julia. To use the function, you need to load the module by running . Here is an example that shows how to compute the mean of the column for each group in a grouped DataFrame: # Compute mean of meantemp column for each group This code loads the module and uses the function to compute the mean of the column for each group in a grouped DataFrame named . What can we do with this mean, maybe vizualize? Yes, you can visualize the mean of the column for each group in a grouped DataFrame using a plotting package such as or . Here is an example that shows how to create a bar plot of the mean values for each group using the package: # Compute mean of meantemp column for each group # Create bar plot of mean meantemp values for each group This code computes the mean of the column for each group in a grouped DataFrame named and stores the result in a new DataFrame named . It then uses the function from the package to create a bar plot of the mean values for each group. The x-axis shows the date and the y-axis shows the mean temperature. I hope this exercise helps you practice working with DataFrames in Julia! Working with the Fourier Transform in Julia In this exercise, you will practice computing the Fourier transform of climate data using the package in Julia.\n• None Install the package by running the following command in the Julia REPL: 2. Read the data from the and files into DataFrames named and using the function.\n• None Compute the Fourier transform of the column in the DataFrame using the function from the package.\n• None Compute the frequencies corresponding to each element of the Fourier transform using the function.\n• None Plot the magnitude of the Fourier transform against the frequencies to visualize the frequency spectrum of the signal. Here is one possible solution to this exercise: This code uses the function from the package to compute the discrete Fourier transform of the column in a DataFrame named . It also uses the function to compute the frequencies corresponding to each element of the Fourier transform. Once you have computed the Fourier transform of the data, you can use it to analyze the frequency content of the signal. For example, you can plot the magnitude of the Fourier transform to visualize the dominant frequencies in the signal. The Fourier transform is a mathematical tool that decomposes a signal into its constituent frequencies. It converts a function from the time domain into the frequency domain, where the output is a complex-valued function of frequency. The magnitude of the Fourier transform represents the contribution of each frequency component to the original signal. In other words, it shows how much of each frequency is present in the original signal. The magnitude is usually plotted against the frequencies to visualize the frequency spectrum of the signal. Fourier transform can be used to analyze climate data in many ways. For example, it can help identify patterns and periodicities in the data, such as seasonal cycles or other recurring phenomena. By decomposing the signal into its frequency components, the Fourier transform can highlight the dominant frequencies present in the data and help understand the underlying processes that drive climate variability. For example, a study used wavelet local multiple correlation (WLMC) to analyze relationships among several large-scale reconstructed climate variables characterizing North Atlantic: i.e. sea surface temperatures (SST) from the tropical cyclone main developmental region (MDR), the El Niño-Southern Oscillation (ENSO), the North Atlantic Multidecadal Oscillation (AMO), and tropical cyclone counts (TC). References: (1) Fourier transform - Wikipedia. https://en.wikipedia.org/wiki/Fourier_transform (2) Fourier Transform – from Wolfram MathWorld. https://mathworld.wolfram.com/FourierTransform.html (3) Lecture 8: Fourier transforms - Scholars at Harvard. https://scholar.harvard.edu/files/schwartz/files/lecture8-fouriertransforms.pdf (4) NCL: Simple Fourier Analysis of Climate Data - NCAR Command Language (NCL). https://www.ncl.ucar.edu/Applications/fouranal.shtml (5) Dynamic wavelet correlation analysis for multivariate climate time …. https://www.nature.com/articles/s41598-020-77767-8 (6) NASA Global Daily Downscaled Projections, CMIP6 | Scientific Data - Nature. https://www.nature.com/articles/s41597-022-01393-4 Here is an example that shows how to create a bar plot of the mean values for each group using the package: # Create plot of magnitude of Fourier transform against frequencies I hope this exercise helps you practice working with the Fourier transform in Julia!"
    },
    {
        "link": "https://juliadata.github.io/DataFrames.jl/stable/man/getting_started",
        "document": "The DataFrames package is available through the Julia package system and can be installed using the following commands:\n\nThroughout the rest of this tutorial, we will assume that you have installed the DataFrames package and have already typed to bring all of the relevant variables into your current namespace.\n\nBy default DataFrames.jl limits the number of rows and columns when displaying a data frame in a Jupyter Notebook to 25 and 100, respectively. You can override this behavior by changing the values of the and variables to hold the maximum number of columns and rows of the output. All columns or rows will be printed if those numbers are equal or lower than 0. Alternatively, you may want to set the maximum number of data frame rows to print to and the maximum number of columns to print to for every Julia session using some Jupyter kernel file (numbers and are only examples and can be adjusted). In such case add a entry to the variable in this Jupyter kernel file. See here for information about location and specification of Jupyter kernels. The package PrettyTables.jl renders the in the Jupyter notebook. Users can customize the output by passing keywords arguments to the function : , where is the . Any argument supported by PrettyTables.jl in the HTML backend can be used here. Hence, for example, if the user wants to change the color of all numbers smaller than 0 to red in Jupyter, they can execute: after . For more information about the available options, check PrettyTables.jl documentation.\n\nObjects of the type represent a data table as a series of vectors, each corresponding to a column or variable. The simplest way of constructing a is to pass column vectors using keyword arguments or pairs:\n\nHere are examples of other commonly used ways to construct a data frame:\n\nColumns can be directly (i.e. without copying) extracted using , , or (this rule applies to getting data from a data frame, not writing data to a data frame). The two latter syntaxes are more flexible as they allow passing a variable holding the name of the column, and not only a literal name. Note that column names can be either symbols (written as , or ) or strings (written as ). In the forms and variable interpolation into a string using does not work. Columns can also be extracted using an integer index specifying their position.\n\nSince does not make a copy, changing the elements of the column vector returned by this syntax will affect the values stored in the original . To get a copy of the column use : changing the vector returned by this syntax does not change .\n\nColumn names can be obtained as strings using the function:\n\nYou can also filter column names by passing a column selector condition as a second argument. See the docstring for a detailed list of available conditions. Here we give some selected examples:\n\nTo get column names as s use the function:\n\nIt is also possible to start with an empty and add columns to it one by one:\n\nThe we build in this way has 8 rows and 3 columns. This can be checked using the function:\n\nIn the above example notice that the expression created a new column in the data frame by broadcasting a scalar.\n\nWhen setting a column of a data frame the and syntaxes are equivalent and they would replace (or create) the column in . This is different from using to set a column in a data frame, which updates the contents of column in-place if it already exists.\n\nHere is an example showing this difference. Let us try changing the column to a binary variable.\n\nThe above operations did not work because when you use as row selector the column is updated in-place, and it only supports storing strings.\n\nOn the other hand the following works:\n\nAs you can see because we used on the right-hand side of the assignment the column was replaced. The same effect would be achieved if we used instead or if we used broadcasted assignment .\n\nIn the Indexing section of the manual you can find all details about all the available indexing options.\n\nIt is also possible to fill a row by row. Let us construct an empty data frame with two columns (note that the first column can only contain integers and the second one can only contain strings):\n\nRows can then be added as tuples or vectors, where the order of elements matches that of columns. To add new rows at the end of a data frame use :\n\nRows can also be added as s, where the dictionary keys match the column names:\n\nNote that constructing a row by row is significantly less performant than constructing it all at once, or column by column. For many use-cases this will not matter, but for very large s this may be a consideration.\n\nIf you want to add rows at the beginning of a data frame use and to insert a row in an arbitrary location use .\n\nYou can also add whole tables to a data frame using the and functions.\n\nDataFrames supports the Tables.jl interface for interacting with tabular data. This means that a can be used as a \"source\" to any package that expects a Tables.jl interface input, (file format packages, data manipulation packages, etc.). A can also be a sink for any Tables.jl interface input. Some example uses are:\n\nA particular common case of a collection that supports the Tables.jl interface is a vector of s:\n\nYou can also easily convert a data frame back to a vector of s:"
    },
    {
        "link": "https://juliadata.github.io/DataFrames.jl/stable",
        "document": "Welcome to the DataFrames.jl documentation!\n\nThis resource aims to teach you everything you need to know to get up and running with tabular data manipulation using the DataFrames.jl package.\n\nFor more illustrations of DataFrames.jl usage, in particular in conjunction with other packages you can check-out the following resources (they are kept up to date with the released version of DataFrames.jl):\n• DataFrames.jl: Flexible and Fast Tabular Data in Julia article published in the Journal of Statistical Software\n\nIf you prefer to learn DataFrames.jl from a book you can consider reading:\n\nDataFrames.jl provides a set of tools for working with tabular data in Julia. Its design and functionality are similar to those of pandas (in Python) and , and dplyr (in R), making it a great general purpose data science tool.\n\nDataFrames.jl plays a central role in the Julia Data ecosystem, and has tight integrations with a range of different libraries. DataFrames.jl isn't the only tool for working with tabular data in Julia – as noted below, there are some other great libraries for certain use-cases – but it provides great data wrangling functionality through a familiar interface.\n\nTo understand the toolchain in more detail, have a look at the tutorials in this manual. New users can start with the First Steps with DataFrames.jl section.\n\nYou may find the DataFramesMeta.jl package or one of the other convenience packages discussed in the Data manipulation frameworks section of this manual helpful when writing more advanced data transformations, especially if you do not have a significant programming experience. These packages provide convenience syntax similar to dplyr in R.\n\nIf you use metadata when working with DataFrames.jl you might find the TableMetadataTools.jl package useful. This package defines several convenience functions for performing typical metadata operations.\n\nThe Julia data ecosystem can be a difficult space for new users to navigate, in part because the Julia ecosystem tends to distribute functionality across different libraries more than some other languages. Because many people coming to DataFrames.jl are just starting to explore the Julia data ecosystem, below is a list of well-supported libraries that provide different data science tools, along with a few notes about what makes each library special, and how well integrated they are with DataFrames.jl.\n• Statistics\n• StatsKit.jl: A convenience meta-package which loads a set of essential packages for statistics, including those mentioned below in this section and DataFrames.jl itself.\n• Statistics: The Julia standard library comes with a wide range of statistics functionality, but to gain access to these functions you must call .\n• LinearAlgebra: Like , many linear algebra features (factorizations, inversions, etc.) live in a library you have to load to use.\n• SparseArrays are also in the standard library but must be loaded to be used.\n• GLM.jl: Tools for estimating linear and generalized linear models. Tightly integrated with DataFrames.jl.\n• StatsModels.jl: For converting heterogeneous into homogeneous matrices for use with linear algebra libraries or machine learning applications that don't directly support s. Will do things like convert categorical variables into indicators/one-hot-encodings, create interaction terms, etc.\n• MultivariateStats.jl: linear regression, ridge regression, PCA, component analyses tools. Not well integrated with DataFrames.jl, but easily used in combination with .\n• Machine Learning\n• MLJ.jl: if you're more of an applied user, there is a single package the pulls from all these different libraries and provides a single, scikit-learn inspired API: MLJ.jl. MLJ.jl provides a common interface for a wide range of machine learning algorithms.\n• ScikitLearn.jl: A Julia wrapper around the full Python scikit-learn machine learning library. Not well integrated with DataFrames.jl, but can be combined using StatsModels.jl.\n• AutoMLPipeline: A package that makes it trivial to create complex ML pipeline structures using simple expressions. It leverages on the built-in macro programming features of Julia to symbolically process, manipulate pipeline expressions, and makes it easy to discover optimal structures for machine learning regression and classification.\n• Plotting\n• Plots.jl: Powerful, modern plotting library with a syntax akin to that of matplotlib (in Python) or (in R). StatsPlots.jl provides Plots.jl with recipes for many standard statistical plots.\n• Gadfly.jl: High-level plotting library with a \"grammar of graphics\" syntax akin to that of ggplot (in R).\n• VegaLite.jl: High-level plotting library that uses a different \"grammar of graphics\" syntax and has an emphasis on interactive graphics.\n• Data Wrangling:\n• Impute.jl: various methods for handling missing data in vectors, matrices and tables.\n• DataFramesMeta.jl: A range of convenience functions for DataFrames.jl that augment and to provide a user experience similar to that provided by dplyr in R.\n• DataFrameMacros.jl: Provides macro versions of the common DataFrames.jl functions similar to DataFramesMeta.jl, with convenient syntax for the manipulation of multiple columns at once.\n• Query.jl: Query.jl provides a single framework for data wrangling that works with a range of libraries, including DataFrames.jl, other tabular data libraries (more on those below), and even non-tabular data. Provides many convenience functions analogous to those in dplyr in R or LINQ.\n• You can find more information on these packages in the Data manipulation frameworks section of this manual.\n• And More!\n• Graphs.jl: A pure-Julia, high performance network analysis library. Edgelists in s can be easily converted into graphs using the GraphDataFrameBridge.jl package.\n• IO:\n• DataFrames.jl work well with a range of formats, including:\n• reading Stata, SAS and SPSS files (using ReadStatTables.jl; alternatively Queryverse users can choose StatFiles.jl),\n\nWhile not all of these libraries are tightly integrated with DataFrames.jl, because s are essentially collections of aligned Julia vectors, so it is easy to (a) pull out a vector for use with a non-DataFrames-integrated library, or (b) convert your table into a homogeneously-typed matrix using the constructor or StatsModels.jl.\n\nDataFrames.jl is a great general purpose tool for data manipulation and wrangling, but it's not ideal for all applications. For users with more specialized needs, consider using:\n• TypedTables.jl: Type-stable heterogeneous tables. Useful for improved performance when the structure of your table is relatively stable and does not feature thousands of columns.\n• JuliaDB.jl: For users working with data that is too large to fit in memory, we suggest JuliaDB.jl, which offers better performance for large datasets, and can handle out-of-core data manipulations (Python users can think of JuliaDB.jl as the Julia version of dask).\n\nNote that most tabular data libraries in the Julia ecosystem (including DataFrames.jl) support a common interface (defined in the Tables.jl package). As a result, some libraries are capable or working with a range of tabular data structures, making it easy to move between tabular libraries as your needs change. A user of Query.jl, for example, can use the same code to manipulate data in a , a (defined by TypedTables.jl), or a JuliaDB table.\n\nIf there is something you expect DataFrames to be capable of, but cannot figure out how to do, please reach out with questions in Domains/Data on Discourse. Additionally you might want to listen to an introduction to DataFrames.jl on JuliaAcademy.\n\nPlease report bugs by opening an issue.\n\nYou can follow the source links throughout the documentation to jump right to the source files on GitHub to make pull requests for improving the documentation and function capabilities.\n\nPlease review DataFrames contributing guidelines before submitting your first PR!\n\nInformation on specific versions can be found on the Release page.\n• None\n• Getting and Setting Data in a Data Frame\n• None\n• Joining on key columns with different names\n• Specifying row order in the join result\n• None\n• Using as an iterable and indexable object\n• None\n• Comparison with Stata (version 8 and above)\n\nOnly exported (i.e. available for use without qualifier after loading the DataFrames.jl package with ) types and functions are considered a part of the public API of the DataFrames.jl package. In general all such objects are documented in this manual (in case some documentation is missing please kindly report an issue here).\n\nPlease be warned that while Julia allows you to access internal functions or types of DataFrames.jl these can change without warning between versions of DataFrames.jl. In particular it is not safe to directly access fields of types that are a part of public API of the DataFrames.jl package using e.g. the function. Whenever some operation on fields of defined types is considered allowed an appropriate exported function should be used instead.\n• None\n• The design of handling of columns of a"
    },
    {
        "link": "https://discourse.julialang.org/t/recipe-to-plot-simple-dataframes/64889",
        "document": "I’m not aware of any DataFrames recipes other than the ones in StatsBase. I think a recipe as simple as yours might not exist because it assumes a very specific structure of your data (all columns except the first numeric, first column holding x labels) which is probably not present in most situations.\n\nThere is one here: https://github.com/viraltux/Forecast.jl/blob/main/src/plot_DataFrame.jl In this case it expects the first column to be a time type; first it checks its format and if the first column is not a time type it adds a column to the DataFrame with a default time type. Then it proceeds to plot it. If instead a time type we want labels in the first column, we could just do the same and add a default label column and then plot it, or just restrict the plot to those DataFrame values with the proper format and ignore the others.\n\nThanks Nils.\n\n For the 2nd question, wanted to know if that is all one need to do to create a method to plot dataframes. It needs further bells and whistles, for sure. NB:\n\n There are no labels on the first column, the simple dataframe example provided looks like this:\n\n@viraltux, thanks for the comprehensive example.\n\n It seems to call an external function ?\n\n More importantly, once the has been defined, how do you use it, as simply as ?\n\nstands for “TO Time Series” you can find its details here: Forecast.jl/src/utils_datetime.jl at main · viraltux/Forecast.jl · GitHub However if you are just to plot numeric values you don’t need to add anything. More importantly, once the has been defined, how do you use it, as simply as ?\n\nFWIW this is actually reasonably well documented: One thing to note is that I believe recipes should generally use to set plot kwargs, not : as stated in the docs, forces a certain attribute, which means the user can’t overwrite this, while sets a default which can be changed if the user passes that kwarg. Imho should therefore only be used where a plot doesn’t make sense if a certain attribute is changed, or if you absolutely want to enforce a specific look (e.g. writing a recipe for an organisation with a corporate color scheme or something)"
    },
    {
        "link": "https://jump.dev/JuMP.jl/stable/tutorials/getting_started/getting_started_with_data_and_plotting",
        "document": "Getting started with data and plotting\n\nThis tutorial was generated using Literate.jl. Download the source as a file.\n\nIn this tutorial we will learn how to read tabular data into Julia, and some of the basics of plotting.\n\nIf you're new to Julia, start by reading Getting started with Julia and Getting started with JuMP first.\n\nBefore we get started, we need this constant to point to where the data files are.\n\nWhere to get help\n\nTo get started, we need to install some packages.\n\nThe package provides a set of tools for working with tabular data. It is available through the Julia package manager.\n\nThe package provides a set of tools for plotting. It is available through the Julia package manager.\n\nCSV and other delimited text files can be read by the CSV.jl package.\n\nTo read a CSV file into a DataFrame, we use the function.\n\nLet's try plotting some of this data\n\nThat doesn't look right. What happened? If you look at the dataframe above, it read in as a column because there are \"NA\" fields. Let's correct that, by telling CSV to consider \"NA\" as .\n\nThat looks better.\n\nDataFrames.jl supports manipulation using functions similar to pandas. For example, split the dataframe into groups based on eye-color:\n\nThen recombine into a single dataframe based on a function operating over the split dataframes:\n\nThen we can visualize the data:\n\nWe can also use the package to read any other delimited text file format.\n\nBy default, CSV.File will try to detect a file's delimiter from the first 10 lines of the file.\n\nCandidate delimiters include , , , , , and . If it can't auto-detect the delimiter, it will assume .\n\nLet's take the example of space separated data.\n\nWe can also specify the delimiter as follows:\n\nNow that we have read the required data into a DataFrame, let us look at some basic operations we can perform on it.\n\nThe function gets us the dimensions of the DataFrame:\n\nWe can also use the and functions to get the number of rows and columns respectively:\n\nThe function gives basic summary statistics of data in a DataFrame:\n\nNames of every column can be obtained by the function:\n\nCorresponding data types are obtained using the broadcasted function:\n\nSimilar to regular arrays, we use numerical indexing to access elements of a DataFrame:\n\nThe following are different ways to access a column:\n\nThe following are different ways to access a row:\n\nWe can change the values just as we normally assign values.\n\nLet's now apply what we have learned to solve a real problem.\n\nThe Passport Index Dataset lists travel visa requirements for 199 countries, in format. Our task is to find the minimum number of passports required to visit all countries.\n\nIn this dataset, the first column represents a passport (=from) and each remaining column represents a foreign country (=to).\n\nThe values in each cell are as follows:\n• 1 = visa can be obtained on arrival\n• -1 is for all instances where passport and destination are the same\n\nOur task is to find out the minimum number of passports needed to visit every country without requiring a visa.\n\nThe values we are interested in are -1 and 3. Let's modify the dataframe so that the -1 and 3 are (true), and all others are (false):\n\nThe values in the cells now represent:\n\nTo model the problem as a mixed-integer linear program, we need a binary decision variable $x_c$ for each country $c$. $x_c$ is $1$ if we select passport $c$ and $0$ otherwise. Our objective is to minimize the sum $\\sum x_c$ over all countries.\n\nSince we wish to visit all the countries, for every country, we must own at least one passport that lets us travel to that country visa free. For one destination, this can be mathematically represented as $\\sum_{c \\in C} a_{c,d} \\cdot x_{d} \\geq 1$, where $a$ is the dataframe.\n\nThus, we can represent this problem using the following model:\n\n\\[\\begin{aligned} \\min && \\sum_{c \\in C} x_c \\\\ \\text{s.t.} && \\sum_{c \\in C} a_{c,d} x_c \\geq 1 && \\forall d \\in C \\\\ && x_c \\in \\{0,1\\} && \\forall c \\in C. \\end{aligned}\\]\n\nWe'll now solve the problem using JuMP:\n\nFirst, create the set of countries:\n\nThen, create the model and initialize the decision variables:\n\nWe can use the function to get an overview of the solution:\n\nJust to be sure, check that the solver found an optimal solution:\n\nLet's have a look at the solution in more detail:\n\nWe need some passports, like New Zealand and the United States, which have widespread access to a large number of countries. However, we also need passports like North Korea which only have visa-free access to a very limited number of countries."
    }
]