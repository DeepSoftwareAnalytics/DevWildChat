[
    {
        "link": "https://docs.streamlit.io/develop/api-reference/cli",
        "document": "When you install Streamlit, a command-line (CLI) tool gets installed as well. The purpose of this tool is to run Streamlit apps, change Streamlit configuration options, and help you diagnose and fix issues.\n\nTo see all of the supported commands:\n\nRuns your app. At any time you can stop the server with Ctrl+c.\n\nTo see the Streamlit 'Hello, World!' example app, run .\n\nTo see what version of Streamlit is installed, just type:\n\nOpens the Streamlit documentation (i.e. this website) in a web browser.\n\nClears persisted files from the on-disk Streamlit cache, if present.\n\nAs described in Configuration, Streamlit has several configuration options. To view them all, including their current values, just type:"
    },
    {
        "link": "https://docs.streamlit.io/develop/concepts/architecture/run-your-app",
        "document": "Working with Streamlit is simple. First you sprinkle a few Streamlit commands into a normal Python script, and then you run it. We list few ways to run your script, depending on your use case.\n\nOnce you've created your script, say , the easiest way to run it is with :\n\nAs soon as you run the script as shown above, a local Streamlit server will spin up and your app will open in a new tab in your default web browser.\n\nWhen passing your script some custom arguments, they must be passed after two dashes. Otherwise the arguments get interpreted as arguments to Streamlit itself:\n\nYou can also pass a URL to ! This is great when your script is hosted remotely, such as a GitHub Gist. For example:\n\nAnother way of running Streamlit is to run it as a Python module. This is useful when configuring an IDE like PyCharm to work with Streamlit:"
    },
    {
        "link": "https://docs.streamlit.io",
        "document": "Streamlit is an open-source Python framework for data scientists and AI/ML engineers to deliver dynamic data apps with only a few lines of code. Build and deploy powerful data apps in minutes. Let's get started!\n\nHow to use our docs\n\nrocket_launch Get started with Streamlit! Set up your development environment and learn the fundamental concepts, and start coding! description Develop your Streamlit app! Our API reference explains each Streamlit function with examples. Dive deep into all of our features with conceptual guides. Try out our step-by-step tutorials. cloud Deploy your Streamlit app! Streamlit Community Cloud our free platform for deploying and sharing Streamlit apps. Streamlit in Snowflake is an enterprise-class solution where you can house your data and apps in one, unified, global system. Explore all your options! school Knowledge base is a self-serve library of tips, tricks, and articles that answer your questions about creating and deploying Streamlit apps.\n\nStreamlit is more than just a way to make data apps, it's also a community of creators that share their apps and ideas and help each other make their work better. Please come join us on the community forum. We love to hear your questions, ideas, and help you work through your bugs — stop by today! View forum"
    },
    {
        "link": "https://docs.streamlit.io/get-started/installation/command-line",
        "document": "This page will walk you through creating an environment with and installing Streamlit with . These are our recommended tools, but if you are familiar with others you can use your favorite ones too. At the end, you'll build a simple \"Hello world\" app and run it. If you prefer to have a graphical interface to manage your Python environments, check out how to Install Streamlit using Anaconda Distribution.\n\nAs with any programming tool, in order to install Streamlit you first need to make sure your computer is properly set up. More specifically, you’ll need:\n• We recommend using virtual environments because installing or upgrading a Python package may cause unintentional effects on another package. For a detailed introduction to Python environments, check out Python Virtual Environments: A Primer. For this guide, we'll be using , which comes with Python.\n• Package managers handle installing each of your Python packages, including Streamlit. For this guide, we'll be using , which comes with Python.\n• Download Xcode command line tools using these instructions in order to let the package manager install some of Streamlit's dependencies.\n• Our favorite editor is VS Code, which is also what we use in all our tutorials.\n• Open a terminal and navigate to your project folder.\n• A folder named \".venv\" will appear in your project. This directory is where your virtual environment and its dependencies are installed.\n• In your terminal, activate your environment with one of the following commands, depending on your operating system.\n• Once activated, you will see your environment name in parentheses before your prompt. \"(.venv)\"\n• In the terminal with your environment activated, type:\n• Test that the installation worked by launching the Streamlit Hello example app: If this doesn't work, use the long-form command:\n• Streamlit's Hello app should appear in a new tab in your web browser!\n• Close your terminal when you are done.\n\nCreate a \"Hello World\" app and run it\n• Any time you want to use your new environment, you first need to go to your project folder (where the directory lives) and run the command to activate it:\n• Once activated, you will see your environment's name in parentheses at the beginning of your terminal prompt. \"(.venv)\"\n\nIf this doesn't work, use the long-form command:\n• To stop the Streamlit server, press in the terminal.\n• When you're done using this environment, return to your normal shell by typing:\n\nRead about our Basic concepts to understand Streamlit's dataflow model."
    },
    {
        "link": "https://docs.streamlit.io/develop/api-reference/configuration/config.toml",
        "document": "is an optional file you can define for your working directory or global development environment. When is defined both globally and in your working directory, Streamlit combines the configuration options and gives precedence to the working-directory configuration. Additionally, you can use environment variables and command-line options to override additional configuration options. For more information, see Configuration options.\n\nTo define your configuration locally or per-project, add to your working directory. Your working directory is wherever you call . If you haven't previously created the directory, you will need to add it.\n\nTo define your configuration globally, you must first locate your global directory. Streamlit adds this hidden directory to your OS user profile during installation. For MacOS/Linux, this will be . For Windows, this will be .\n\nBelow are all the sections and options you can have in your file. To see all configurations, use the following command in your terminal or CLI:\n\n[client] # Controls whether uncaught app exceptions and deprecation warnings # are displayed in the browser. This can be one of the following: # - \"full\" : In the browser, Streamlit displays app deprecation # warnings and exceptions, including exception types, # exception messages, and associated tracebacks. # - \"stacktrace\" : In the browser, Streamlit displays exceptions, # including exception types, generic exception messages, # and associated tracebacks. Deprecation warnings and # full exception messages will only print to the # console. # - \"type\" : In the browser, Streamlit displays exception types and # generic exception messages. Deprecation warnings, full # exception messages, and associated tracebacks only # print to the console. # - \"none\" : In the browser, Streamlit displays generic exception # messages. Deprecation warnings, full exception # messages, associated tracebacks, and exception types # will only print to the console. # - True : This is deprecated. Streamlit displays \"full\" # error details. # - False : This is deprecated. Streamlit displays \"stacktrace\" # error details. # Default: \"full\" showErrorDetails = \"full\" # Change the visibility of items in the toolbar, options menu, # and settings dialog (top right of the app). # Allowed values: # - \"auto\" : Show the developer options if the app is accessed through # localhost or through Streamlit Community Cloud as a developer. # Hide them otherwise. # - \"developer\" : Show the developer options. # - \"viewer\" : Hide the developer options. # - \"minimal\" : Show only options set externally (e.g. through # Streamlit Community Cloud) or through st.set_page_config. # If there are no options left, hide the menu. # Default: \"auto\" toolbarMode = \"auto\" # Controls whether to display the default sidebar page navigation in a # multi-page app. This only applies when app's pages are defined by the # `pages/` directory. # Default: true showSidebarNavigation = true\n\n[runner] # Allows you to type a variable or string by itself in a single line of # Python code to write it to the app. # Default: true magicEnabled = true # Handle script rerun requests immediately, rather than waiting for script # execution to reach a yield point. This makes Streamlit much more # responsive to user interaction, but it can lead to race conditions in # apps that mutate session_state data outside of explicit session_state # assignment statements. # Default: true fastReruns = true # Raise an exception after adding unserializable data to Session State. # Some execution environments may require serializing all data in Session # State, so it may be useful to detect incompatibility during development, # or when the execution environment will stop supporting it in the future. # Default: false enforceSerializableSessionState = false # Adjust how certain 'options' widgets like radio, selectbox, and # multiselect coerce Enum members when the Enum class gets re-defined # during a script re-run. For more information, check out the docs: # https://docs.streamlit.io/develop/concepts/design/custom-classes#enums # Allowed values: # - \"off\" : Disables Enum coercion. # - \"nameOnly\" : Enum classes can be coerced if their member names match. # - \"nameAndValue\" : Enum classes can be coerced if their member names AND # member values match. # Default: \"nameOnly\" enumCoercion = \"nameOnly\"\n\n[server] # List of folders that should not be watched for changes. # Relative paths will be taken as relative to the current working directory. # Example: ['/home/user1/env', 'relative/path/to/folder'] # Default: [] folderWatchBlacklist = [] # Change the type of file watcher used by Streamlit, or turn it off # completely. # Allowed values: # - \"auto\" : Streamlit will attempt to use the watchdog module, and # falls back to polling if watchdog is not available. # - \"watchdog\" : Force Streamlit to use the watchdog module. # - \"poll\" : Force Streamlit to always use polling. # - \"none\" : Streamlit will not watch files. # Default: \"auto\" fileWatcherType = \"auto\" # Symmetric key used to produce signed cookies. If deploying on multiple # replicas, this should be set to the same value across all replicas to ensure # they all share the same secret. # Default: randomly generated secret key. cookieSecret = \"a-random-key-appears-here\" # If false, will attempt to open a browser window on start. # Default: false unless (1) we are on a Linux box where DISPLAY is unset, or # (2) we are running in the Streamlit Atom plugin. headless = false # Automatically rerun script when the file is modified on disk. # Default: false runOnSave = false # The address where the server will listen for client and browser # connections. Use this if you want to bind the server to a specific address. # If set, the server will only be accessible from this address, and not from # any aliases (like localhost). # Default: (unset) address = # The port where the server will listen for browser connections. # Don't use port 3000 which is reserved for internal development. # Default: 8501 port = 8501 # The base path for the URL where Streamlit should be served from. # Default: \"\" baseUrlPath = \"\" # Enables support for Cross-Origin Resource Sharing (CORS) protection, # for added security. # If XSRF protection is enabled and CORS protection is disabled at the # same time, Streamlit will enable them both instead. # Default: true enableCORS = true # Enables support for Cross-Site Request Forgery (XSRF) protection, for # added security. # If XSRF protection is enabled and CORS protection is disabled at the # same time, Streamlit will enable them both instead. # Default: true enableXsrfProtection = true # Max size, in megabytes, for files uploaded with the file_uploader. # Default: 200 maxUploadSize = 200 # Max size, in megabytes, of messages that can be sent via the WebSocket # connection. # Default: 200 maxMessageSize = 200 # Enables support for websocket compression. # Default: false enableWebsocketCompression = false # Enable serving files from a `static` directory in the running app's # directory. # Default: false enableStaticServing = false # TTL in seconds for sessions whose websockets have been disconnected. The server # may choose to clean up session state, uploaded files, etc for a given session # with no active websocket connection at any point after this time has passed. # Default: 120 disconnectedSessionTTL = 120 # Server certificate file for connecting via HTTPS. # Must be set at the same time as \"server.sslKeyFile\". # ['DO NOT USE THIS OPTION IN A PRODUCTION ENVIRONMENT. It has not gone through # security audits or performance tests. For the production environment, we # recommend performing SSL termination by the load balancer or the reverse # proxy.'] sslCertFile = # Cryptographic key file for connecting via HTTPS. # Must be set at the same time as \"server.sslCertFile\". # ['DO NOT USE THIS OPTION IN A PRODUCTION ENVIRONMENT. It has not gone through # security audits or performance tests. For the production environment, we # recommend performing SSL termination by the load balancer or the reverse # proxy.'] sslKeyFile =\n\n[browser] # Internet address where users should point their browsers in order to # connect to the app. Can be IP address or DNS name and path. # This is used to: # - Set the correct URL for CORS and XSRF protection purposes. # - Show the URL on the terminal # - Open the browser # Default: \"localhost\" serverAddress = \"localhost\" # Whether to send usage statistics to Streamlit. # Default: true gatherUsageStats = true # Port where users should point their browsers in order to connect to the # app. # This is used to: # - Set the correct URL for XSRF protection purposes. # - Show the URL on the terminal (part of `streamlit run`). # - Open the browser automatically (part of `streamlit run`). # This option is for advanced use cases. To change the port of your app, use # `server.Port` instead. Don't use port 3000 which is reserved for internal # development. # Default: whatever value is set in server.port. serverPort = 8501"
    },
    {
        "link": "https://pytorch.org/get-started/previous-versions",
        "document": "We’d prefer you install the latest version, but old binaries and installation instructions are provided below for your convenience.\n\nmacOS is currently not supported for LTS.\n\nmacOS is currently not supported in LTS.\n\nTo install a previous version of PyTorch via Anaconda or Miniconda, replace “0.4.1” in the following commands with the desired version (i.e., “0.2.0”).\n\nIt is possible to checkout an older version of PyTorch and build it. You can list tags in PyTorch git repository with and checkout a particular one (replace ‘0.1.9’ with the desired version) with\n\nFollow the install from source instructions in the README.md of the PyTorch checkout.\n\nDownload the file with the desired version from the following html pages:\n\nThen, install the file with\n\nNote: most pytorch versions are available only for specific CUDA versions. For example pytorch=1.0.1 is not available for CUDA 9.2\n\nThese predate the html page above and have to be manually installed by downloading the wheel file and"
    },
    {
        "link": "https://discuss.pytorch.org/t/cuda-versioning-and-pytorch-compatibility/189777",
        "document": "I am pretty new at using pytorch. Currently, I have been trying to understand the concepts of using CUDA for performing better loading data and increasing speed for training models. I took a look into my system, I currently have an NVIDIA GTX1650 that contains CUDA v-11, yet I see that hasn’t been installed. Normally, when I work in python, I use virtual environments to set all the libraries I use for a project. With pytorch, I saw you can run on the CPU or use CUDA. Currently, the latest version is pytorch 2.1.0 which goes until CUDA 11.8 or 12.1. I may have a couple of questions regarding how to properly set my graphics card for usage. 1.) Since the drivers say the latest version is CUDA 11. Does that mean I have to download the 11.0 CUDA from NVIDIA? Since other versions would not work? 2.) Pytorch versioning must also meet the highest available CUDA version? In other words, downgrade pytorch 2.1.0? 3.) Is there a better way than installing for local venvs? (Conda for example). Thank you so much for your time.\n\nI try to install pytorch on my local machine via conda command.\n\n I use the conda command from PyTorch website: using above command the conda command remain in a loop. When I remove pytroch-cuda=11.8, the command successfully run and all other lib. are installed. Next I enter the below command to install pytorch-cuda: Unfortunately, it is not installed and I receive the following message. Collecting package metadata (current_repodata.json): - WARNING conda.models.version:get_matcher(542): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1., but conda is ignoring the . and treating it as 1.7.1\n\n done\n\n Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n\n Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n\n Solving environment: / I was wondering whether you could know what would be issue.\n\n@ptrblck\n\n Thanks for your quick reply. The thing is that I have created several new conda env., even I tried removing the whole Anaconda folder and reinstall again Anaconda. However, it did not work. When I enter the command to install pytorch, torchvision, and torchaudio, everything is fine. Once I add pytorch-cuda to this command, or I want to install pytorch-cuda after others, such as pytorch and torchvision, the install command remains in loop and finally stopped with the error One more thing, I have a yml file including pytorch 1.13 and cuda 11.6. It is installed with conda successfully. However, the problem is with new version. The reason I want to install new pytorch version is because in pytorch 1.13 it seems the output of hook are not stable. And the hook outputs are slightly changed in different running. Thanks again for your inputs.\n\nHi,\n\n I did install cuda and pytorch using this command:\n\n \n\n and now I have the following packages on my conda env:\n\n cuda-cudart 11.7.99 0 nvidia\n\n cuda-cupti 11.7.101 0 nvidia\n\n cuda-libraries 11.7.1 0 nvidia\n\n cuda-nvrtc 11.7.99 0 nvidia\n\n cuda-nvtx 11.7.91 0 nvidia\n\n cuda-runtime 11.7.1 0 nvidia\n\n nvidia-cuda-cupti-cu11 11.7.101 pypi_0 pypi\n\n nvidia-cuda-nvrtc-cu11 11.7.99 pypi_0 pypi\n\n nvidia-cuda-runtime-cu11 11.7.99 pypi_0 pypi\n\n pytorch 2.0.1 py3.8_cuda11.7_cudnn8.5.0_0 pytorch\n\n pytorch-cuda 11.7 h778d358_5 pytorch\n\n pytorch-mutex 1.0 cuda pytorch\n\n $ conda list | grep ‘torch’\n\n ffmpeg 4.3 hf484d3e_0 pytorch\n\n pytorch 2.0.1 py3.8_cuda11.7_cudnn8.5.0_0 pytorch\n\n pytorch-cuda 11.7 h778d358_5 pytorch\n\n pytorch-mutex 1.0 cuda pytorch\n\n pytorch-triton 2.1.0+440fd1bf20 pypi_0 pypi\n\n torch-tb-profiler 0.4.1 pypi_0 pypi\n\n torchaudio 2.1.0.dev20230628+cu118 pypi_0 pypi\n\n torchtriton 2.0.0 py38 pytorch\n\n torchvision 0.16.0.dev20230628+cu118 pypi_0 pypi\n\n but I still get the following error: RuntimeError: CUDA error: no kernel image is available for execution on the device CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1. Please let me know if you have any suggestions."
    },
    {
        "link": "https://stackoverflow.com/questions/75550882/trying-to-install-the-latest-pytorch-1-13-1-instead-installs-1-11-0",
        "document": "I'm trying to install the latest Pytorch version, but it keeps trying to instead install 1.11.0. I'm on Windows 10 running Python 3.10.8, and I have CUDA 11.6 installed.\n\nEven if I give it the flag --no-cache-dir, it proceeds to download 1.11.0 anyways. Running\n\nInstalls the CPU version. Any way that I can download the specific module directly and install it manually?"
    },
    {
        "link": "https://gpu-mart.com/blog/Installing-pytorch-with-cuda-support-on-Windows",
        "document": "How to Install Pytorch with CUDA support on Windows 10 PyTorch benefits significantly from using CUDA (NVIDIA's GPU acceleration framework), here are the steps to install PyTorch with CUDA support on Windows.\n\nMake sure you have an NVIDIA GPU supported by CUDA and have the following requirements. 1. CUDA for GPU support\n• For CUDA 11.8 version, make sure you have Nvidia Driver version 452.39 or higher\n• For CUDA 12.1 version, make sure you have Nvidia Driver version 527.41 or higher\n\nOpen the NVIDIA Control Panel. Click System Information and check the driver version. It should be greater then 537.58, as this is the current driver version at the time of writing. If you have an older version, goto https://www.nvidia.com/en-us/geforce/drivers/ and update your driver. There is an automatic and manual driver update possible if you know the videocard type.\n\nOpen a Windows terminal or the command prompt (cmd) and type python. The Windows app store will open automatically where you can install it from!\n\nTo ensure that PyTorch was installed correctly, we can verify the installation by running sample PyTorch code. Here we will construct a randomly initialized tensor. From the command line, type python, then then enter the following code: The output should be something similar to: Additionally, to check if your GPU driver and CUDA is enabled and accessible by PyTorch, run the following commands to return whether or not the CUDA driver is enabled: C:\\Users\\Administrator>python Python 3.11.6 (tags/v3.11.6:8b6ee5b, Oct 2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)] on win32 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import torch >>> torch.cuda.is_available() True >>> >>> print(torch.cuda.device_count()) 1"
    },
    {
        "link": "https://discuss.pytorch.org/t/pytorch-cuda-11-6/149647",
        "document": "As a follow-up to this question PyTorch + CUDA 11.4\n\n I have installed these Nvidia drivers version 510.60.02 along with Cuda 11.6.\n\n I’d like to install Pytorch in a conda virtual environment, and I’ve found in the Pytorch website that we couldn’t choose a stable version that relies on the latest versions of Cuda (the older version is 11.3) Start Locally | PyTorch\n\nHow can I install/build Pytorch with the latest version of Cuda in my conda virtual env ?"
    },
    {
        "link": "https://docs.github.com/en/rest/repos/contents",
        "document": "Gets the contents of a file or directory in a repository. Specify the file path or directory with the parameter. If you omit the parameter, you will receive the contents of the repository's root directory.\n\nThis endpoint supports the following custom media types. For more information, see \"Media types.\"\n• : Returns the raw file contents for files and symlinks.\n• : Returns the file contents in HTML. Markup languages are rendered to HTML using GitHub's open-source Markup library.\n• : Returns the contents in a consistent object format regardless of the content type. For example, instead of an array of objects for a directory, the response will be an object with an attribute containing the array of objects.\n\nIf the content is a directory, the response will be an array of objects, one object for each item in the directory. When listing the contents of a directory, submodules have their \"type\" specified as \"file\". Logically, the value should be \"submodule\". This behavior exists for backwards compatibility purposes. In the next major version of the API, the type will be returned as \"submodule\".\n\nIf the content is a symlink and the symlink's target is a normal file in the repository, then the API responds with the content of the file. Otherwise, the API responds with an object describing the symlink itself.\n\nIf the content is a submodule, the field identifies the location of the submodule repository, and the identifies a specific commit within the submodule repository. Git uses the given URL when cloning the submodule repository, and checks out the submodule at that specific commit. If the submodule repository is not hosted on github.com, the Git URLs ( and ) and the github.com URLs ( and ) will have null values.\n• To get a repository's contents recursively, you can recursively get the tree.\n• This API has an upper limit of 1,000 files for a directory. If you need to retrieve more files, use the Git Trees API.\n• Download URLs expire and are meant to be used just once. To ensure the download URL does not expire, please use the contents API to obtain a fresh download URL for each download.\n• If the requested file's size is:\n• 1 MB or smaller: All features of this endpoint are supported.\n• Between 1-100 MB: Only the or custom media types are supported. Both will work as normal, except that when using the media type, the field will be an empty string and the field will be . To get the contents of these larger files, use the media type.\n• Greater than 100 MB: This endpoint is not supported."
    },
    {
        "link": "https://stackoverflow.com/questions/3796927/how-do-i-git-clone-a-repo-including-its-submodules",
        "document": "if you have a specific branch for your submodules then change it to:\n\nFor a full example that was testing:"
    },
    {
        "link": "https://docs.github.com/rest/repos/contents",
        "document": "Gets the contents of a file or directory in a repository. Specify the file path or directory with the parameter. If you omit the parameter, you will receive the contents of the repository's root directory.\n\nThis endpoint supports the following custom media types. For more information, see \"Media types.\"\n• : Returns the raw file contents for files and symlinks.\n• : Returns the file contents in HTML. Markup languages are rendered to HTML using GitHub's open-source Markup library.\n• : Returns the contents in a consistent object format regardless of the content type. For example, instead of an array of objects for a directory, the response will be an object with an attribute containing the array of objects.\n\nIf the content is a directory, the response will be an array of objects, one object for each item in the directory. When listing the contents of a directory, submodules have their \"type\" specified as \"file\". Logically, the value should be \"submodule\". This behavior exists for backwards compatibility purposes. In the next major version of the API, the type will be returned as \"submodule\".\n\nIf the content is a symlink and the symlink's target is a normal file in the repository, then the API responds with the content of the file. Otherwise, the API responds with an object describing the symlink itself.\n\nIf the content is a submodule, the field identifies the location of the submodule repository, and the identifies a specific commit within the submodule repository. Git uses the given URL when cloning the submodule repository, and checks out the submodule at that specific commit. If the submodule repository is not hosted on github.com, the Git URLs ( and ) and the github.com URLs ( and ) will have null values.\n• To get a repository's contents recursively, you can recursively get the tree.\n• This API has an upper limit of 1,000 files for a directory. If you need to retrieve more files, use the Git Trees API.\n• Download URLs expire and are meant to be used just once. To ensure the download URL does not expire, please use the contents API to obtain a fresh download URL for each download.\n• If the requested file's size is:\n• 1 MB or smaller: All features of this endpoint are supported.\n• Between 1-100 MB: Only the or custom media types are supported. Both will work as normal, except that when using the media type, the field will be an empty string and the field will be . To get the contents of these larger files, use the media type.\n• Greater than 100 MB: This endpoint is not supported."
    },
    {
        "link": "https://stackoverflow.com/questions/39880380/how-do-i-duplicate-a-git-repository-including-submodules",
        "document": "I want to duplicate a repository, that also has a submodle - which is it's own repository.\n\nWhen I duplicate the top-level repository (using the method described in GitHub's help pages), only the top repository is duplicated, and the submodule is still linked to the original repository.\n\nHowever, I want to (temporarily) make both repositories private and thus be able to edit both as full duplicates.\n\nHow can I do that?"
    },
    {
        "link": "https://git-scm.com/book/en/v2/Git-Tools-Submodules",
        "document": "We’ll walk through developing a simple project that has been split up into a main project and a few sub-projects. Let’s start by adding an existing Git repository as a submodule of the repository that we’re working on. To add a new submodule you use the command with the absolute or relative URL of the project you would like to start tracking. In this example, we’ll add a library called “DbConnector”. $ git submodule add https://github.com/chaconinc/DbConnector Cloning into 'DbConnector'... remote: Counting objects: 11, done. remote: Compressing objects: 100% (10/10), done. remote: Total 11 (delta 0), reused 11 (delta 0) Unpacking objects: 100% (11/11), done. Checking connectivity... done. By default, submodules will add the subproject into a directory named the same as the repository, in this case “DbConnector”. You can add a different path at the end of the command if you want it to go elsewhere. If you run at this point, you’ll notice a few things. $ git status On branch master Your branch is up-to-date with 'origin/master'. Changes to be committed: (use \"git reset HEAD <file>...\" to unstage) new file: .gitmodules new file: DbConnector First you should notice the new file. This is a configuration file that stores the mapping between the project’s URL and the local subdirectory you’ve pulled it into: If you have multiple submodules, you’ll have multiple entries in this file. It’s important to note that this file is version-controlled with your other files, like your file. It’s pushed and pulled with the rest of your project. This is how other people who clone this project know where to get the submodule projects from. Since the URL in the .gitmodules file is what other people will first try to clone/fetch from, make sure to use a URL that they can access if possible. For example, if you use a different URL to push to than others would to pull from, use the one that others have access to. You can overwrite this value locally with for your own use. When applicable, a relative URL can be helpful. The other listing in the output is the project folder entry. If you run on that, you see something interesting: Although is a subdirectory in your working directory, Git sees it as a submodule and doesn’t track its contents when you’re not in that directory. Instead, Git sees it as a particular commit from that repository. If you want a little nicer diff output, you can pass the option to . When you commit, you see something like this: Notice the mode for the entry. That is a special mode in Git that basically means you’re recording a commit as a directory entry rather than a subdirectory or a file.\n\nNow we have a copy of a project with submodules in it and will collaborate with our teammates on both the main project and the submodule project. Pulling in Upstream Changes from the Submodule Remote The simplest model of using submodules in a project would be if you were simply consuming a subproject and wanted to get updates from it from time to time but were not actually modifying anything in your checkout. Let’s walk through a simple example there. If you want to check for new work in a submodule, you can go into the directory and run and the upstream branch to update the local code. Now if you go back into the main project and run you can see that the submodule was updated and get a list of commits that were added to it. If you don’t want to type every time you run , you can set it as the default format by setting the config value to “log”. If you commit at this point then you will lock the submodule into having the new code when other people update. There is an easier way to do this as well, if you prefer to not manually fetch and merge in the subdirectory. If you run , Git will go into your submodules and fetch and update for you. $ git submodule update --remote DbConnector remote: Counting objects: 4, done. remote: Compressing objects: 100% (2/2), done. remote: Total 4 (delta 2), reused 4 (delta 2) Unpacking objects: 100% (4/4), done. From https://github.com/chaconinc/DbConnector 3f19983..d0354fc master -> origin/master Submodule path 'DbConnector': checked out 'd0354fc054692d3906c85c3af05ddce39a1c0644' This command will by default assume that you want to update the checkout to the default branch of the remote submodule repository (the one pointed to by on the remote). You can, however, set this to something different if you want. For example, if you want to have the submodule track that repository’s “stable” branch, you can set it in either your file (so everyone else also tracks it), or just in your local file. Let’s set it in the file: $ git config -f .gitmodules submodule.DbConnector.branch stable $ git submodule update --remote remote: Counting objects: 4, done. remote: Compressing objects: 100% (2/2), done. remote: Total 4 (delta 2), reused 4 (delta 2) Unpacking objects: 100% (4/4), done. From https://github.com/chaconinc/DbConnector 27cf5d3..c87d55d stable -> origin/stable Submodule path 'DbConnector': checked out 'c87d55d4c6d4b05ee34fbc8cb6f7bf4585ae6687' If you leave off the it will only make the change for you, but it probably makes more sense to track that information with the repository so everyone else does as well. When we run at this point, Git will show us that we have “new commits” on the submodule. $ git status On branch master Your branch is up-to-date with 'origin/master'. Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: .gitmodules modified: DbConnector (new commits) no changes added to commit (use \"git add\" and/or \"git commit -a\") If you set the configuration setting , Git will also show you a short summary of changes to your submodules: $ git config status.submodulesummary 1 $ git status On branch master Your branch is up-to-date with 'origin/master'. Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: .gitmodules modified: DbConnector (new commits) Submodules changed but not updated: * DbConnector c3f01dc...c87d55d (4): > catch non-null terminated lines At this point if you run we can see both that we have modified our file and also that there are a number of commits that we’ve pulled down and are ready to commit to our submodule project. $ git diff diff --git a/.gitmodules b/.gitmodules index 6fc0b3d..fd1cc29 100644 --- a/.gitmodules +++ b/.gitmodules @@ -1,3 +1,4 @@ [submodule \"DbConnector\"] path = DbConnector url = https://github.com/chaconinc/DbConnector + branch = stable Submodule DbConnector c3f01dc..c87d55d: > catch non-null terminated lines > more robust error handling > more efficient db routine > better connection routine This is pretty cool as we can actually see the log of commits that we’re about to commit to in our submodule. Once committed, you can see this information after the fact as well when you run . $ git log -p --submodule commit 0a24cfc121a8a3c118e0105ae4ae4c00281cf7ae Author: Scott Chacon <schacon@gmail.com> Date: Wed Sep 17 16:37:02 2014 +0200 updating DbConnector for bug fixes diff --git a/.gitmodules b/.gitmodules index 6fc0b3d..fd1cc29 100644 --- a/.gitmodules +++ b/.gitmodules @@ -1,3 +1,4 @@ [submodule \"DbConnector\"] path = DbConnector url = https://github.com/chaconinc/DbConnector + branch = stable Submodule DbConnector c3f01dc..c87d55d: > catch non-null terminated lines > more robust error handling > more efficient db routine > better connection routine Git will by default try to update all of your submodules when you run . If you have a lot of them, you may want to pass the name of just the submodule you want to try to update. Pulling Upstream Changes from the Project Remote Let’s now step into the shoes of your collaborator, who has their own local clone of the MainProject repository. Simply executing to get your newly committed changes is not enough: $ git pull From https://github.com/chaconinc/MainProject fb9093c..0a24cfc master -> origin/master Fetching submodule DbConnector From https://github.com/chaconinc/DbConnector c3f01dc..c87d55d stable -> origin/stable Updating fb9093c..0a24cfc Fast-forward .gitmodules | 2 +- DbConnector | 2 +- 2 files changed, 2 insertions(+), 2 deletions(-) $ git status On branch master Your branch is up-to-date with 'origin/master'. Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout -- <file>...\" to discard changes in working directory) modified: DbConnector (new commits) Submodules changed but not updated: * DbConnector c87d55d...c3f01dc (4): < catch non-null terminated lines < more robust error handling < more efficient db routine < better connection routine no changes added to commit (use \"git add\" and/or \"git commit -a\") By default, the command recursively fetches submodules changes, as we can see in the output of the first command above. However, it does not update the submodules. This is shown by the output of the command, which shows the submodule is “modified”, and has “new commits”. What’s more, the brackets showing the new commits point left (<), indicating that these commits are recorded in MainProject but are not present in the local checkout. To finalize the update, you need to run : $ git submodule update --init --recursive Submodule path 'vendor/plugins/demo': checked out '48679c6302815f6c76f1fe30625d795d9e55fc56' $ git status On branch master Your branch is up-to-date with 'origin/master'. nothing to commit, working tree clean Note that to be on the safe side, you should run with the flag in case the MainProject commits you just pulled added new submodules, and with the flag if any submodules have nested submodules. If you want to automate this process, you can add the flag to the command (since Git 2.14). This will make Git run right after the pull, putting the submodules in the correct state. Moreover, if you want to make Git always pull with , you can set the configuration option to (this works for since Git 2.15). This option will make Git use the flag for all commands that support it (except ). There is a special situation that can happen when pulling superproject updates: it could be that the upstream repository has changed the URL of the submodule in the file in one of the commits you pull. This can happen for example if the submodule project changes its hosting platform. In that case, it is possible for , or , to fail if the superproject references a submodule commit that is not found in the submodule remote locally configured in your repository. In order to remedy this situation, the command is required: # copy the new URL to your local config $ git submodule sync --recursive # update the submodule from the new URL $ git submodule update --init --recursive It’s quite likely that if you’re using submodules, you’re doing so because you really want to work on the code in the submodule at the same time as you’re working on the code in the main project (or across several submodules). Otherwise you would probably instead be using a simpler dependency management system (such as Maven or Rubygems). So now let’s go through an example of making changes to the submodule at the same time as the main project and committing and publishing those changes at the same time. So far, when we’ve run the command to fetch changes from the submodule repositories, Git would get the changes and update the files in the subdirectory but will leave the sub-repository in what’s called a “detached HEAD” state. This means that there is no local working branch (like , for example) tracking changes. With no working branch tracking changes, that means even if you commit changes to the submodule, those changes will quite possibly be lost the next time you run . You have to do some extra steps if you want changes in a submodule to be tracked. In order to set up your submodule to be easier to go in and hack on, you need to do two things. You need to go into each submodule and check out a branch to work on. Then you need to tell Git what to do if you have made changes and later pulls in new work from upstream. The options are that you can merge them into your local work, or you can try to rebase your local work on top of the new changes. First of all, let’s go into our submodule directory and check out a branch. Let’s try updating our submodule with the “merge” option. To specify it manually, we can just add the option to our call. Here we’ll see that there was a change on the server for this submodule and it gets merged in. $ cd .. $ git submodule update --remote --merge remote: Counting objects: 4, done. remote: Compressing objects: 100% (2/2), done. remote: Total 4 (delta 2), reused 4 (delta 2) Unpacking objects: 100% (4/4), done. From https://github.com/chaconinc/DbConnector c87d55d..92c7337 stable -> origin/stable Updating c87d55d..92c7337 Fast-forward src/main.c | 1 + 1 file changed, 1 insertion(+) Submodule path 'DbConnector': merged in '92c7337b30ef9e0893e758dac2459d07362ab5ea' If we go into the directory, we have the new changes already merged into our local branch. Now let’s see what happens when we make our own local change to the library and someone else pushes another change to the upstream at the same time. Now if we update our submodule we can see what happens when we have made a local change and upstream also has a change we need to incorporate. $ cd .. $ git submodule update --remote --rebase First, rewinding head to replay your work on top of it... Applying: Unicode support Submodule path 'DbConnector': rebased into '5d60ef9bbebf5a0c1c1050f242ceeb54ad58da94' If you forget the or , Git will just update the submodule to whatever is on the server and reset your project to a detached HEAD state. If this happens, don’t worry, you can simply go back into the directory and check out your branch again (which will still contain your work) and merge or rebase (or whatever remote branch you want) manually. If you haven’t committed your changes in your submodule and you run a that would cause issues, Git will fetch the changes but not overwrite unsaved work in your submodule directory. $ git submodule update --remote remote: Counting objects: 4, done. remote: Compressing objects: 100% (3/3), done. remote: Total 4 (delta 0), reused 4 (delta 0) Unpacking objects: 100% (4/4), done. From https://github.com/chaconinc/DbConnector 5d60ef9..c75e92a stable -> origin/stable error: Your local changes to the following files would be overwritten by checkout: scripts/setup.sh Please, commit your changes or stash them before you can switch branches. Aborting Unable to checkout 'c75e92a2b3855c9e5b66f915308390d9db204aca' in submodule path 'DbConnector' If you made changes that conflict with something changed upstream, Git will let you know when you run the update. $ git submodule update --remote --merge Auto-merging scripts/setup.sh CONFLICT (content): Merge conflict in scripts/setup.sh Recorded preimage for 'scripts/setup.sh' Automatic merge failed; fix conflicts and then commit the result. Unable to merge 'c75e92a2b3855c9e5b66f915308390d9db204aca' in submodule path 'DbConnector' You can go into the submodule directory and fix the conflict just as you normally would. Now we have some changes in our submodule directory. Some of these were brought in from upstream by our updates and others were made locally and aren’t available to anyone else yet as we haven’t pushed them yet. $ git diff Submodule DbConnector c87d55d..82d2ad3: > Merge from origin/stable > Update setup script > Unicode support > Remove unnecessary method > Add new option for conn pooling If we commit in the main project and push it up without pushing the submodule changes up as well, other people who try to check out our changes are going to be in trouble since they will have no way to get the submodule changes that are depended on. Those changes will only exist on our local copy. In order to make sure this doesn’t happen, you can ask Git to check that all your submodules have been pushed properly before pushing the main project. The command takes the argument which can be set to either “check” or “on-demand”. The “check” option will make simply fail if any of the committed submodule changes haven’t been pushed. $ git push --recurse-submodules=check The following submodule paths contain changes that can not be found on any remote: DbConnector Please try git push --recurse-submodules=on-demand or cd to the path and use git push to push them to a remote. As you can see, it also gives us some helpful advice on what we might want to do next. The simple option is to go into each submodule and manually push to the remotes to make sure they’re externally available and then try this push again. If you want the “check” behavior to happen for all pushes, you can make this behavior the default by doing . The other option is to use the “on-demand” value, which will try to do this for you. $ git push --recurse-submodules=on-demand Pushing submodule 'DbConnector' Counting objects: 9, done. Delta compression using up to 8 threads. Compressing objects: 100% (8/8), done. Writing objects: 100% (9/9), 917 bytes | 0 bytes/s, done. Total 9 (delta 3), reused 0 (delta 0) To https://github.com/chaconinc/DbConnector c75e92a..82d2ad3 stable -> stable Counting objects: 2, done. Delta compression using up to 8 threads. Compressing objects: 100% (2/2), done. Writing objects: 100% (2/2), 266 bytes | 0 bytes/s, done. Total 2 (delta 1), reused 0 (delta 0) To https://github.com/chaconinc/MainProject 3d6d338..9a377d1 master -> master As you can see there, Git went into the module and pushed it before pushing the main project. If that submodule push fails for some reason, the main project push will also fail. You can make this behavior the default by doing . If you change a submodule reference at the same time as someone else, you may run into some problems. That is, if the submodule histories have diverged and are committed to diverging branches in a superproject, it may take a bit of work for you to fix. If one of the commits is a direct ancestor of the other (a fast-forward merge), then Git will simply choose the latter for the merge, so that works fine. Git will not attempt even a trivial merge for you, however. If the submodule commits diverge and need to be merged, you will get something that looks like this: $ git pull remote: Counting objects: 2, done. remote: Compressing objects: 100% (1/1), done. remote: Total 2 (delta 1), reused 2 (delta 1) Unpacking objects: 100% (2/2), done. From https://github.com/chaconinc/MainProject 9a377d1..eb974f8 master -> origin/master Fetching submodule DbConnector warning: Failed to merge submodule DbConnector (merge following commits not found) Auto-merging DbConnector CONFLICT (submodule): Merge conflict in DbConnector Automatic merge failed; fix conflicts and then commit the result. So basically what has happened here is that Git has figured out that the two branches record points in the submodule’s history that are divergent and need to be merged. It explains it as “merge following commits not found”, which is confusing but we’ll explain why that is in a bit. To solve the problem, you need to figure out what state the submodule should be in. Strangely, Git doesn’t really give you much information to help out here, not even the SHA-1s of the commits of both sides of the history. Fortunately, it’s simple to figure out. If you run you can get the SHA-1s of the commits recorded in both branches you were trying to merge. So, in this case, is the commit in our submodule that we had and is the commit that upstream had. If we go into our submodule directory, it should already be on as the merge would not have touched it. If for whatever reason it’s not, you can simply create and checkout a branch pointing to it. What is important is the SHA-1 of the commit from the other side. This is what you’ll have to merge in and resolve. You can either just try the merge with the SHA-1 directly, or you can create a branch for it and then try to merge that in. We would suggest the latter, even if only to make a nicer merge commit message. So, we will go into our submodule directory, create a branch named “try-merge” based on that second SHA-1 from , and manually merge. $ cd DbConnector $ git rev-parse HEAD eb41d764bccf88be77aced643c13a7fa86714135 $ git branch try-merge c771610 $ git merge try-merge Auto-merging src/main.c CONFLICT (content): Merge conflict in src/main.c Recorded preimage for 'src/main.c' Automatic merge failed; fix conflicts and then commit the result. We got an actual merge conflict here, so if we resolve that and commit it, then we can simply update the main project with the result. $ vim src/main.c (1) $ git add src/main.c $ git commit -am 'merged our changes' Recorded resolution for 'src/main.c'. [master 9fd905e] merged our changes $ cd .. (2) $ git diff (3) diff --cc DbConnector index eb41d76,c771610..0000000 --- a/DbConnector +++ b/DbConnector @@@ -1,1 -1,1 +1,1 @@@ - Subproject commit eb41d764bccf88be77aced643c13a7fa86714135 -Subproject commit c77161012afbbe1f58b5053316ead08f4b7e6d1d ++Subproject commit 9fd905e5d7f45a0d4cbc43d1ee550f16a30e825a $ git add DbConnector (4) $ git commit -m \"Merge Tom's Changes\" (5) [master 10d2c60] Merge Tom's Changes\n• First we resolve the conflict.\n• Then we go back to the main project directory.\n• We can check the SHA-1s again. It can be a bit confusing, but it’s really not very hard. Interestingly, there is another case that Git handles. If a merge commit exists in the submodule directory that contains both commits in its history, Git will suggest it to you as a possible solution. It sees that at some point in the submodule project, someone merged branches containing these two commits, so maybe you’ll want that one. This is why the error message from before was “merge following commits not found”, because it could not do this. It’s confusing because who would expect it to try to do this? If it does find a single acceptable merge commit, you’ll see something like this: $ git merge origin/master warning: Failed to merge submodule DbConnector (not fast-forward) Found a possible merge resolution for the submodule: 9fd905e5d7f45a0d4cbc43d1ee550f16a30e825a: > merged our changes If this is correct simply add it to the index for example by using: git update-index --cacheinfo 160000 9fd905e5d7f45a0d4cbc43d1ee550f16a30e825a \"DbConnector\" which will accept this suggestion. Auto-merging DbConnector CONFLICT (submodule): Merge conflict in DbConnector Automatic merge failed; fix conflicts and then commit the result. The suggested command Git is providing will update the index as though you had run (which clears the conflict), then commit. You probably shouldn’t do this though. You can just as easily go into the submodule directory, see what the difference is, fast-forward to this commit, test it properly, and then commit it. This accomplishes the same thing, but at least this way you can verify that it works and you have the code in your submodule directory when you’re done.\n\nThere are a few things you can do to make working with submodules a little easier. There is a submodule command to run some arbitrary command in each submodule. This can be really helpful if you have a number of submodules in the same project. For example, let’s say we want to start a new feature or do a bugfix and we have work going on in several submodules. We can easily stash all the work in all our submodules. $ git submodule foreach 'git stash' Entering 'CryptoLibrary' No local changes to save Entering 'DbConnector' Saved working directory and index state WIP on stable: 82d2ad3 Merge from origin/stable HEAD is now at 82d2ad3 Merge from origin/stable Then we can create a new branch and switch to it in all our submodules. $ git submodule foreach 'git checkout -b featureA' Entering 'CryptoLibrary' Switched to a new branch 'featureA' Entering 'DbConnector' Switched to a new branch 'featureA' You get the idea. One really useful thing you can do is produce a nice unified diff of what is changed in your main project and all your subprojects as well. $ git diff; git submodule foreach 'git diff' Submodule DbConnector contains modified content diff --git a/src/main.c b/src/main.c index 210f1ae..1f0acdc 100644 --- a/src/main.c +++ b/src/main.c @@ -245,6 +245,8 @@ static int handle_alias(int *argcp, const char ***argv) commit_pager_choice(); + url = url_decode(url_orig); + /* build alias_argv */ alias_argv = xmalloc(sizeof(*alias_argv) * (argc + 1)); alias_argv[0] = alias_string + 1; Entering 'DbConnector' diff --git a/src/db.c b/src/db.c index 1aaefb6..5297645 100644 --- a/src/db.c +++ b/src/db.c @@ -93,6 +93,11 @@ char *url_decode_mem(const char *url, int len) return url_decode_internal(&url, len, NULL, &out, 0); } +char *url_decode(const char *url) +{ + return url_decode_mem(url, strlen(url)); +} + char *url_decode_parameter_name(const char **query) { struct strbuf out = STRBUF_INIT; Here we can see that we’re defining a function in a submodule and calling it in the main project. This is obviously a simplified example, but hopefully it gives you an idea of how this may be useful. You may want to set up some aliases for some of these commands as they can be quite long and you can’t set configuration options for most of them to make them defaults. We covered setting up Git aliases in Git Aliases, but here is an example of what you may want to set up if you plan on working with submodules in Git a lot. This way you can simply run when you want to update your submodules, or to push with submodule dependency checking.\n\nUsing submodules isn’t without hiccups, however. For instance, switching branches with submodules in them can also be tricky with Git versions older than Git 2.13. If you create a new branch, add a submodule there, and then switch back to a branch without that submodule, you still have the submodule directory as an untracked directory: $ git --version git version 2.12.2 $ git checkout -b add-crypto Switched to a new branch 'add-crypto' $ git submodule add https://github.com/chaconinc/CryptoLibrary Cloning into 'CryptoLibrary'... ... $ git commit -am 'Add crypto library' [add-crypto 4445836] Add crypto library 2 files changed, 4 insertions(+) create mode 160000 CryptoLibrary $ git checkout master warning: unable to rmdir CryptoLibrary: Directory not empty Switched to branch 'master' Your branch is up-to-date with 'origin/master'. $ git status On branch master Your branch is up-to-date with 'origin/master'. Untracked files: (use \"git add <file>...\" to include in what will be committed) CryptoLibrary/ nothing added to commit but untracked files present (use \"git add\" to track) Removing the directory isn’t difficult, but it can be a bit confusing to have that in there. If you do remove it and then switch back to the branch that has that submodule, you will need to run to repopulate it. Again, not really very difficult, but it can be a little confusing. Newer Git versions (Git >= 2.13) simplify all this by adding the flag to the command, which takes care of placing the submodules in the right state for the branch we are switching to. $ git --version git version 2.13.3 $ git checkout -b add-crypto Switched to a new branch 'add-crypto' $ git submodule add https://github.com/chaconinc/CryptoLibrary Cloning into 'CryptoLibrary'... ... $ git commit -am 'Add crypto library' [add-crypto 4445836] Add crypto library 2 files changed, 4 insertions(+) create mode 160000 CryptoLibrary $ git checkout --recurse-submodules master Switched to branch 'master' Your branch is up-to-date with 'origin/master'. $ git status On branch master Your branch is up-to-date with 'origin/master'. nothing to commit, working tree clean Using the flag of can also be useful when you work on several branches in the superproject, each having your submodule pointing at different commits. Indeed, if you switch between branches that record the submodule at different commits, upon executing the submodule will appear as “modified”, and indicate “new commits”. That is because the submodule state is by default not carried over when switching branches. This can be really confusing, so it’s a good idea to always when your project has submodules. For older Git versions that do not have the flag, after the checkout you can use to put the submodules in the right state. Luckily, you can tell Git (>=2.14) to always use the flag by setting the configuration option : . As noted above, this will also make Git recurse into submodules for every command that has a option (except ). The other main caveat that many people run into involves switching from subdirectories to submodules. If you’ve been tracking files in your project and you want to move them out into a submodule, you must be careful or Git will get angry at you. Assume that you have files in a subdirectory of your project, and you want to switch it to a submodule. If you delete the subdirectory and then run , Git yells at you: $ rm -Rf CryptoLibrary/ $ git submodule add https://github.com/chaconinc/CryptoLibrary 'CryptoLibrary' already exists in the index You have to unstage the directory first. Then you can add the submodule: $ git rm -r CryptoLibrary $ git submodule add https://github.com/chaconinc/CryptoLibrary Cloning into 'CryptoLibrary'... remote: Counting objects: 11, done. remote: Compressing objects: 100% (10/10), done. remote: Total 11 (delta 0), reused 11 (delta 0) Unpacking objects: 100% (11/11), done. Checking connectivity... done. Now suppose you did that in a branch. If you try to switch back to a branch where those files are still in the actual tree rather than a submodule — you get this error: $ git checkout master error: The following untracked working tree files would be overwritten by checkout: CryptoLibrary/Makefile CryptoLibrary/includes/crypto.h ... Please move or remove them before you can switch branches. Aborting You can force it to switch with , but be careful that you don’t have unsaved changes in there as they could be overwritten with that command. $ git checkout -f master warning: unable to rmdir CryptoLibrary: Directory not empty Switched to branch 'master' Then, when you switch back, you get an empty directory for some reason and may not fix it either. You may need to go into your submodule directory and run a to get all your files back. You could run this in a script to run it for multiple submodules. It’s important to note that submodules these days keep all their Git data in the top project’s directory, so unlike much older versions of Git, destroying a submodule directory won’t lose any commits or branches that you had. With these tools, submodules can be a fairly simple and effective method for developing on several related but still separate projects simultaneously."
    }
]