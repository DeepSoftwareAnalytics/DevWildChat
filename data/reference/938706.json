[
    {
        "link": "https://cs.cmu.edu/afs/cs/academic/class/15492-f07/www/pthreads.html",
        "document": "\n• A thread does not maintain a list of created threads, nor does it know the thread that created it.\n• All threads within a process share the same address space.\n• Threads in the same process share:\n• In this example the same function is used in each thread. The arguments are different. The functions need not be the same.\n• Threads terminate by explicitly calling , by letting the function return, or by a call to the function which will terminate the process including any threads.\n• Function call: Arguments:\n• - Set to NULL if default thread attributes are used. (else define members of the struct defined in bits/pthreadtypes.h) Attributes include:\n• scope (Kernel threads: PTHREAD_SCOPE_SYSTEM User threads: PTHREAD_SCOPE_PROCESS Pick one or the other not both.)\n• - pointer to the function to be threaded. Function has a single argument: pointer to void.\n• - pointer to argument of function. To pass multiple arguments, send a pointer to a structure.\n• Function call: Arguments: This routine kills the thread. The function never returns. If the thread is not detached, the thread id and return value may be examined from another thread by using pthread_join. \n\n Note: the return pointer , must not be of local scope otherwise it would cease to exist once the thread terminates.\n• : The above sample program will compile with the GNU C and C++ compiler . The following function pointer representation below will work for C but not C++. Note the subtle differences and avoid the pitfall below:\n\nThe threads library provides three synchronization mechanisms:\n• mutexes - Mutual exclusion lock: Block access to variables by other threads. This enforces exclusive access by a thread to a variable or set of variables.\n\nIf register load and store operations for the incrementing of variable occurs with unfortunate timing, it is theoretically possible to have each thread increment and overwrite the same variable with the same value. Another possibility is that thread two would first increment locking out thread one until complete and then thread one would increment it to 2. \n\n\n\nWhen a mutex lock is attempted against a mutex which is held by another thread, the thread is blocked until the mutex is unlocked. When a thread terminates, the mutex does not unless explicitly unlocked. Nothing happens by default.\n\nA condition variable is a variable of type and is used with the appropriate functions for waiting and later, process continuation. The condition variable mechanism allows threads to suspend execution and relinquish the processor until some condition is true. A condition variable must always be associated with a mutex to avoid a race condition created by one thread preparing to wait and another thread which may signal the condition before the first thread actually waits on it resulting in a deadlock. The thread will be perpetually waiting for a signal that is never sent. Any mutex can be used, there is no explicit link between the mutex and the condition variable.\n\nFunctions used in conjunction with the condition variable:\n• Waiting on condition:\n• pthread_cond_timedwait - place limit on how long it will block.\n• Waking thread based on condition:\n• pthread_cond_broadcast - wake up all threads blocked by the specified condition variable.\n\nNote that was halted while count was between the values COUNT_HALT1 and COUNT_HALT2. The only thing that has been ensures is that will increment the count between the values COUNT_HALT1 and COUNT_HALT2. Everything else is random.\n\nThe logic conditions (the \"if\" and \"while\" statements) must be chosen to insure that the \"signal\" is executed if the \"wait\" is ever processed. Poor software logic can also lead to a deadlock condition.\n\nNote: Race conditions abound with this example because count is used as the condition and can't be locked in the while statement without causing deadlock. I'll work on a cleaner example but it is an example of a condition variable.\n\nWhen this option is enabled, each thread may have its own scheduling properties. Scheduling attributes may be specified:\n• by dynamically by changing the attributes of a thread already created\n• by defining the effect of a mutex on the thread's scheduling when creating a mutex\n• by dynamically changing the scheduling of a thread during synchronization operations.\n• Race conditions: While the code may appear on the screen in the order you wish the code to execute, threads are scheduled by the operating system and are executed at random. It cannot be assumed that threads are executed in the order they are created. They may also execute at different speeds. When threads are executing (racing to complete) they may give unexpected results (race condition). Mutexes and joins must be utilized to achieve a predictable execution order and outcome.\n• Thread safe code: The threaded routines must call functions which are \"thread safe\". This means that there are no static or global variables which other threads may clobber or read assuming single threaded operation. If static or global variables are used then mutexes must be applied or the functions must be re-written to avoid the use of these variables. In C, local variables are dynamically allocated on the stack. Therefore, any function that does not use static data or other shared resources is thread-safe. Thread-unsafe functions may be used by only one thread at a time in a program and the uniqueness of the thread must be ensured. Many non-reentrant functions return a pointer to static data. This can be avoided by returning dynamically allocated data or using caller-provided storage. An example of a non-thread safe function is which is also not re-entrant. The \"thread safe\" version is the re-entrant version .\n• Mutex Deadlock: This condition occurs when a mutex is applied but then not \"unlocked\". This causes program execution to halt indefinitely. It can also be caused by poor application of mutexes or joins. Be careful when applying two or more mutexes to a section of code. If the first pthread_mutex_lock is applied and the second pthread_mutex_lock fails due to another thread applying a mutex, the first mutex may eventually lock all other threads from accessing data including the thread which holds the second mutex. The threads may wait indefinitely for the resource to become free causing a deadlock. It is best to test and if failure occurs, free the resources and stall before retrying. ...\n\n pthread_mutex_lock(&mutex_1);\n\n while ( pthread_mutex_trylock(&mutex_2) ) /* Test if already locked */\n\n {\n\n pthread_mutex_unlock(&mutex_1); /* Free resource to avoid deadlock */\n\n ...\n\n /* stall here */\n\n ...\n\n pthread_mutex_lock(&mutex_1);\n\n }\n\n count++;\n\n pthread_mutex_unlock(&mutex_1);\n\n pthread_mutex_unlock(&mutex_2);\n\n ...\n\n The order of applying the mutex is also important. The following code segment illustrates a potential for deadlock: If acquires the first mutex and acquires the second, all resources are tied up and locked.\n• Condition Variable Deadlock: The logic conditions (the \"if\" and \"while\" statements) must be chosen to insure that the \"signal\" is executed if the \"wait\" is ever processed.\n• pthread_atfork - register handlers to be called at fork(2) time\n• pthread_join - wait for termination of another thread\n• pthread_kill_other_threads_np - terminate all threads in program except calling thread\n• Introduction of threads for Solaris, Linux, and Windows\n• Pthreads tutorial and examples of thread problems - by Andrae Muys\n• Linux-mag.com: The Fibers of Threads - Discussion of how Linux threads work\n• C++ Thread classes:\n• GNU: Common C++ - support for threading, sockets, file access, daemons, persistence, serial I/O, XML parsing and system services"
    },
    {
        "link": "https://cse.buffalo.edu/~eblanton/course/cse220-2023-0s/materials/34-pthreads.pdf",
        "document": ""
    },
    {
        "link": "https://pages.cs.wisc.edu/~remzi/OSTEP/threads-cv.pdf",
        "document": ""
    },
    {
        "link": "https://cs.uml.edu/~fredm/courses/91.308-fall05/files/pthreads.pdf",
        "document": ""
    },
    {
        "link": "https://github.com/shanfl/pthread_guide",
        "document": "This tutorial is an attempt to help you become familiar with multi-threaded programming with the POSIX threads (pthreads) library, and attempts to show how its features can be used in \"real-life\" programs. It explains the different tools defined by the library, shows how to use them, and then gives an example of using them to solve programming problems. There is an implicit assumption that the user has some theoretical familiarity with paralell programming (or multi-processing) concepts. Users without such background might find the concepts harder to grasp. A seperate tutorial will be prepared to explain the theoreticl background and terms to those who are familiar only with normal \"serial\" programming.\n\nI would assume that users which are familiar with asynchronous programming models, such as those used in windowing environments (X, Motif), will find it easier to grasp the concepts of multi-threaded programming.\n\nWhen talking about POSIX threads, one cannot avoid the question \"Which draft of the POSIX threads standard shall be used?\". As this threads standard has been revised over a period of several years, one will find that implementations adhering to different drafts of the standard have a different set of functions, different default values, and different nuances. Since this tutorial was written using a Linux system with the kernel-level LinuxThreads library, v0.5, programmers with access to other systems, using different versions of pthreads, should refer to their system's manuals in case of incompatibilities. Also, since some of the example programs are using blocking system calls, they won't work with user-level threading libraries (refer to our parallel programming theory tutorial for more information). Having said that, i'd try to check the example programs on other systems as well (Solaris 2.5 comes to mind), to make it more \"cross-platform\".\n\nA thread is a semi-process, that has its own stack, and executes a given piece of code. Unlike a real process, the thread normally shares its memory with other threads (where as for processes we usually have a different memory area for each one of them). A Thread Group is a set of threads all executing inside the same process. They all share the same memory, and thus can access the same global variables, same heap memory, same set of file descriptors, etc. All these threads execute in parallel (i.e. using time slices, or if the system has several processors, then really in parallel).\n\nThe advantage of using a thread group instead of a normal serial program is that several operations may be carried out in parallel, and thus events can be handled immediately as they arrive (for example, if we have one thread handling a user interface, and another thread handling database queries, we can execute a heavy query requested by the user, and still respond to user input while the query is executed).\n\nThe advantage of using a thread group over using a process group is that context switching between threads is much faster then context switching between processes (context switching means that the system switches from running one thread or process, to running another thread or process). Also, communications between two threads is usually faster and easier to implement then communications between two processes.\n\nOn the other hand, because threads in a group all use the same memory space, if one of them corrupts the contents of its memory, other threads might suffer as well. With processes, the operating system normally protects processes from one another, and thus if one corrupts its own memory space, other processes won't suffer. Another advantage of using processes is that they can run on different machines, while all the threads have to run on the same machine (at least normally).\n\nWhen a multi-threaded program starts executing, it has one thread running, which executes the main() function of the program. This is already a full-fledged thread, with its own thread ID. In order to create a new thread, the program should use the pthread_create() function. Here is how to use it:\n\nA few notes should be mentioned about this program:\n• Note that the main program is also a thread, so it executes the do_loop() function in parallel to the thread it creates.\n• pthread_create() gets 4 parameters. The first parameter is used by pthread_create() to supply the program with information about the thread. The second parameter is used to set some attributes for the new thread. In our case we supplied a NULL pointer to tell pthread_create() to use the default values. The third parameter is the name of the function that the thread will start executing. The forth parameter is an argument to pass to this function. Note the cast to a 'void*'. It is not required by ANSI-C syntax, but is placed here for clarification.\n• The delay loop inside the function is used only to demonstrate that the threads are executing in parallel. Use a larger delay value if your CPU runs too fast, and you see all the printouts of one thread before the other.\n• The call to pthread_exit() Causes the current thread to exit and free any thread-specific resources it is taking. There is no need to use this call at the end of the thread's top function, since when it returns, the thread would exit automatically anyway. This function is useful if we want to exit a thread in the middle of its execution.\n\nIn order to compile a multi-threaded program using gcc, we need to link it with the pthreads library. Assuming you have this library already installed on your system, here is how to compile our first program:\n\nThe source code for this program may be found in the pthread_create.c file.\n\nOne of the basic problems when running several threads that use the same memory space, is making sure they don't \"step on each other's toes\". By this we refer to the problem of using a data structure from two different threads.\n\nFor instance, consider the case where two threads try to update two variables. One tries to set both to 0, and the other tries to set both to 1. If both threads would try to do that at the same time, we might get with a situation where one variable contains 1, and one contains 0. This is because a context-switch (we already know what this is by now, right?) might occur after the first tread zeroed out the first variable, then the second thread would set both variables to 1, and when the first thread resumes operation, it will zero out the second variable, thus getting the first variable set to '1', and the second set to '0'.\n\nA basic mechanism supplied by the pthreads library to solve this problem, is called a mutex. A mutex is a lock that guarantees three things:\n• Atomicity - Locking a mutex is an atomic operation, meaning that the operating system (or threads library) assures you that if you locked a mutex, no other thread succeeded in locking this mutex at the same time.\n• Singularity - If a thread managed to lock a mutex, it is assured that no other thread will be able to lock the thread until the original thread releases the lock.\n• Non-Busy Wait - If a thread attempts to lock a thread that was locked by a second thread, the first thread will be suspended (and will not consume any CPU resources) until the lock is freed by the second thread. At this time, the first thread will wake up and continue execution, having the mutex locked by it.\n\nFrom these three points we can see how a mutex can be used to assure exclusive access to variables (or in general critical code sections). Here is some pseudo-code that updates the two variables we were talking about in the previous section, and can be used by the first thread:\n\nMeanwhile, the second thread will do something like this:\n\nAssuming both threads use the same mutex, we are assured that after they both ran through this code, either both variables are set to '0', or both are set to '1'. You'd note this requires some work from the programmer - If a third thread was to access these variables via some code that does not use this mutex, it still might mess up the variable's contents. Thus, it is important to enclose all the code that accesses these variables in a small set of functions, and always use only these functions to access these variables.\n\nIn order to create a mutex, we first need to declare a variable of type pthread_mutex_t , and then initialize it. The simplest way it by assigning it the PTHREAD_MUTEX_INITIALIZER constant. So we'll use a code that looks something like this:\n\nOne note should be made here: This type of initialization creates a mutex called 'fast mutex'. This means that if a thread locks the mutex and then tries to lock it again, it'll get stuck - it will be in a deadlock.\n\nThere is another type of mutex, called 'recursive mutex', which allows the thread that locked it, to lock it several more times, without getting blocked (but other threads that try to lock the mutex now will get blocked). If the thread then unlocks the mutex, it'll still be locked, until it is unlocked the same amount of times as it was locked. This is similar to the way modern door locks work - if you turned it twice clockwise to lock it, you need to turn it twice counter-clockwise to unlock it. This kind of mutex can be created by assigning the constant PTHREAD_RECURSIVE_MUTEX_INITIALIZER_NP to a mutex variable.\n\nIn order to lock a mutex, we may use the function pthread_mutex_lock(). This function attempts to lock the mutex, or block the thread if the mutex is already locked by another thread. In this case, when the mutex is unlocked by the first process, the function will return with the mutex locked by our process. Here is how to lock a mutex (assuming it was initialized earlier):\n\nAfter the thread did what it had to (change variables or data structures, handle file, or whatever it intended to do), it should free the mutex, using the pthread_mutex_unlock() function, like this:\n\nAfter we finished using a mutex, we should destroy it. Finished using means no thread needs it at all. If only one thread finished with the mutex, it should leave it alive, for the other threads that might still need to use it. Once all finished using it, the last one can destroy it using the pthread_mutex_destroy() function:\n\nAfter this call, this variable (a_mutex) may not be used as a mutex any more, unless it is initialized again. Thus, if one destroys a mutex too early, and another thread tries to lock or unlock it, that thread will get a EINVAL error code from the lock or unlock function.\n\nAfter we have seen the full life cycle of a mutex, lets see an example program that uses a mutex. The program introduces two employees competing for the \"employee of the day\" title, and the glory that comes with it. To simulate that in a rapid pace, the program employs 3 threads: one that promotes Danny to \"employee of the day\", one that promotes Moshe to that situation, and a third thread that makes sure that the employee of the day's contents is consistent (i.e. contains exactly the data of one employee). Two copies of the program are supplied. One that uses a mutex, and one that does not. Try them both, to see the differences, and be convinced that mutexes are essential in a multi-threaded environment.\n\nThe programs themselves are in the files accompanying this tutorial. The one that uses a mutex is employee-with-mutex.c. The one that does not use a mutex is employee-without-mutex.c. Read the comments inside the source files to get a better understanding of how they work.\n\nAgain we should remember that pthread_mutex_lock() might block for a non-determined duration, in case of the mutex being already locked. If it remains locked forever, it is said that our poor thread is \"starved\" - it was trying to acquire a resource, but never got it. It is up to the programmer to ensure that such starvation won't occur. The pthread library does not help us with that.\n\nThe pthread library might, however, figure out a \"deadlock\". A deadlock is a situation in which a set of threads are all waiting for resources taken by other threads, all in the same set. Naturally, if all threads are blocked waiting for a mutex, none of them will ever come back to life again. The pthread library keeps track of such situations, and thus would fail the last thread trying to call pthread_mutex_lock(), with an error of type EDEADLK. The programmer should check for such a value, and take steps to solve the deadlock somehow.\n\nAs we've seen before with mutexes, they allow for simple coordination - exclusive access to a resource. However, we often need to be able to make real synchronization between threads:\n• In a server, one thread reads requests from clients, and dispatches them to several threads for handling. These threads need to be notified when there is data to process, otherwise they should wait without consuming CPU time.\n• In a GUI (Graphical User Interface) Application, one thread reads user input, another handles graphical output, and a third thread sends requests to a server and handles its replies. The server-handling thread needs to be able to notify the graphics-drawing thread when a reply from the server arrived, so it will immediately show it to the user. The user-input thread needs to be always responsive to the user, for example, to allow her to cancel long operations currently executed by the server-handling thread.\n\nAll these examples require the ability to send notifications between threads. This is where condition variables are brought into the picture.\n\nA condition variable is a mechanism that allows threads to wait (without wasting CPU cycles) for some even to occur. Several threads may wait on a condition variable, until some other thread signals this condition variable (thus sending a notification). At this time, one of the threads waiting on this condition variable wakes up, and can act on the event. It is possible to also wake up all threads waiting on this condition variable by using a broadcast method on this variable.\n\nNote that a condition variable does not provide locking. Thus, a mutex is used along with the condition variable, to provide the necessary locking when accessing this condition variable.\n\nCreation of a condition variable requires defining a variable of type pthread_cond_t, and initializing it properly. Initialization may be done with either a simple use of a macro named PTHREAD_COND_INITIALIZER or the usage of the pthread_cond_init() function. We will show the first form here:\n\nThis defines a condition variable named 'got_request', and initializes it.\n\nNote: since the PTHREAD_COND_INITIALIZER is actually a structure, it may be used to initialize a condition variable only when it is declared. In order to initialize it during runtime, one must use the pthread_cond_init() function.\n\nIn order to signal a condition variable, one should either the pthread_cond_signal() function (to wake up a only one thread waiting on this variable), or the pthread_cond_broadcast() function (to wake up all threads waiting on this variable). Here is an example using signal, assuming 'got_request' is a properly initialized condition variable:\n\nOr by using the broadcast function:\n\nWhen either function returns, 'rc' is set to 0 on success, and to a non-zero value on failure. In such a case (failure), the return value denotes the error that occured (EINVAL denotes that the given parameter is not a condition variable. ENOMEM denotes that the system has run out of memory.\n\nNote: success of a signaling operation does not mean any thread was awakened - it might be that no thread was waiting on the condition variable, and thus the signaling does nothing (i.e. the signal is lost).\n\nIt is also not remembered for future use - if after the signaling function returns another thread starts waiting on this condition variable, a further signal is required to wake it up.\n\nIf one thread signals the condition variable, other threads would probably want to wait for this signal. They may do so using one of two functions, pthread_cond_wait() or pthread_cond_timedwait(). Each of these functions takes a condition variable, and a mutex (which should be locked before calling the wait function), unlocks the mutex, and waits until the condition variable is signaled, suspending the thread's execution. If this signaling causes the thread to awake (see discussion of pthread_cond_signal() earlier), the mutex is automagically locked again by the wait funciton, and the wait function returns.\n\nThe only difference between these two functions is that pthread_cond_timedwait() allows the programmer to specify a timeout for the waiting, after which the function always returns, with a proper error value (ETIMEDOUT) to notify that condition variable was NOT signaled before the timeout passed. The pthread_cond_wait() would wait indefinitely if it was never signaled.\n\nHere is how to use these two functions. We make the assumption that 'got_request' is a properly initialized condition variable, and that 'request_mutex' is a properly initialized mutex. First, we try the pthread_cond_wait() function:\n\nNow an example using the pthread_cond_timedwait() function:\n\nAs you can see, the timed wait version is way more complex, and thus better be wrapped up by some function, rather then being re-coded in every necessary location.\n\nNote: it might be that a condition variable that has 2 or more threads waiting on it is signaled many times, and yet one of the threads waiting on it never awakened. This is because we are not guaranteed which of the waiting threads is awakened when the variable is signaled. It might be that the awakened thread quickly comes back to waiting on the condition variables, and gets awakened again when the variable is signaled again, and so on. The situation for the un-awakened thread is called 'starvation'. It is up to the programmer to make sure this situation does not occur if it implies bad behavior. Yet, in our server example from before, this situation might indicate requests are coming in a very slow pace, and thus perhaps we have too many threads waiting to service requests. In this case, this situation is actually good, as it means every request is handled immediately when it arrives.\n\nNote 2: when the mutex is being broadcast (using pthread_cond_broadcast), this does not mean all threads are running together. Each of them tries to lock the mutex again before returning from their wait function, and thus they'll start running one by one, each one locking the mutex, doing their work, and freeing the mutex before the next thread gets its chance to run.\n\nAfter we are done using a condition variable, we should destroy it, to free any system resources it might be using. This can be done using the pthread_cond_destroy(). In order for this to work, there should be no threads waiting on this condition variable. Here is how to use this function, again, assuming 'got_request' is a pre-initialized condition variable:\n\nWhat if some thread is still waiting on this variable? depending on the case, it might imply some flaw in the usage of this variable, or just lack of proper thread cleanup code. It is probably good to alert the programmer, at least during debug phase of the program, of such a case. It might mean nothing, but it might be significant.\n\nA note should be taken about condition variables - they are usually pointless without some real condition checking combined with them. To make this clear, lets consider the server example we introduced earlier. Assume that we use the 'got_request' condition variable to signal that a new request has arrived that needs handling, and is held in some requests queue. If we had threads waiting on the condition variable when this variable is signaled, we are assured that one of these threads will awake and handle this request.\n\nHowever, what if all threads are busy handling previous requests, when a new one arrives? the signaling of the condition variable will do nothing (since all threads are busy doing other things, NOT waiting on the condition variable now), and after all threads finish handling their current request, they come back to wait on the variable, which won't necessarily be signaled again (for example, if no new requests arrive). Thus, there is at least one request pending, while all handling threads are blocked, waiting for a signal.\n\nIn order to overcome this problem, we may set some integer variable to denote the number of pending requests, and have each thread check the value of this variable before waiting on the variable. If this variable's value is positive, some request is pending, and the thread should go and handle it, instead of going to sleep. Further more, a thread that handled a request, should reduce the value of this variable by one, to make the count correct. Lets see how this affects the waiting code we have seen above.\n\nAs an example for the actual usage of condition variables, we will show a program that simulates the server we have described earlier - one thread, the receiver, gets client requests. It inserts the requests to a linked list, and a hoard of threads, the handlers, are handling these requests. For simplicity, in our simulation, the receiver thread creates requests and does not read them from real clients.\n\nThe program source is available in the file thread-pool-server.c, and contains many comments. Please read the source file first, and then read the following clarifying notes.\n• The 'main' function first launches the handler threads, and then performs the chore of the receiver thread, via its main loop.\n• A single mutex is used both to protect the condition variable, and to protect the linked list of waiting requests. This simplifies the design. As an exercise, you may think how to divide these roles into two mutexes.\n• The mutex itself MUST be a recursive mutex. In order to see why, look at the code of the 'handle_requests_loop' function. You will notice that it first locks the mutex, and afterwards calls the 'get_request' function, which locks the mutex again. If we used a non-recursive mutex, we'd get locked indefinitely in the mutex locking operation of the 'get_request' function. You may argue that we could remove the mutex locking in the 'get_request' function, and thus remove the double-locking problem, but this is a flawed design - in a larger program, we might call the 'get_request' function from other places in the code, and we'll need to check for proper locking of the mutex in each of them.\n• As a rule, when using recursive mutexes, we should try to make sure that each lock operation is accompanied by a matching unlock operation in the same function. Otherwise, it will be very hard to make sure that after locking the mutex several times, it is being unlocked the same number of times, and deadlocks would occur.\n• The implicit unlocking and re-locking of the mutex on the call to the pthread_cond_wait() function is confusing at first. It is best to add a comment regarding this behavior in the code, or else someone that reads this code might accidentally add a further mutex lock.\n\n####\"Private\" thread data - Thread-Specific Data In \"normal\", single-thread programs, we sometimes find the need to use a global variable. It is frequently a bad practice to have global variables, but they sometimes do come handy. Especially if they are static variables - meaning, they are recognized only on the scope of a single file.\n\nIn multi-threaded programs, we also might find a need for such variables. We should note, however, that the same variable is accessible from all the threads, so we need to protect access to it using a mutex, which is extra overhead. Further more, we sometimes need to have a variable that is 'global', but only for a specific thread. Or the same 'global' variable should have different values in different threads. For example, consider a program that needs to have one globally accessible linked list in each thread, but not the same list. Further, we want the same code to be executed by all threads. In this case, the global pointer to the start of the list should be point to a different address in each thread.\n\nIn order to have such a pointer, we need a mechanism that enables the same global variable to have a different location in memory. This is what the thread-specific data mechanism is used for.\n\nIn the thread-specific data (TSD) mechanism, we have notions of keys and values. Each key has a name, and pointer to some memory area. Keys with the same name in two separate threads always point to different memory locations - this is handled by the library functions that allocate memory blocks to be accessed via these keys. We have a function to create a key (invoked once per key name for the whole process), a function to allocate memory (invoked separately in each thread), and functions to de-allocate this memory for a specific thread, and a function to destroy the key, again, process-wide. we also have functions to access the data pointed to by a key, either setting its value, or returning the value it points to.\n\nThe pthread_key_create() function is used to allocate a new key. This key now becomes valid for all threads in our process. When a key is created, the value it points to defaults to NULL. Later on each thread may change its copy of the value as it wishes. Here is how to use this function:\n• After pthread_key_create() returns, the variable 'list_key' points to the newly created key.\n• The function pointer passed as second parameter to pthread_key_create(), will be automatically invoked by the pthread library when our thread exits, with a pointer to the key's value as its parameter. We may supply a NULL pointer as the function pointer, and then no function will be invoked for key. Note that the function will be invoked once in each thread, even thought we created this key only once, in one thread. If we created several keys, their associated destructor functions will be called in an arbitrary order, regardless of the order of keys creation.\n• If the pthread_key_create() function succeeds, it returns 0. Otherwise, it returns some error code. There is a limit of PTHREAD_KEYS_MAX keys that may exist in our process at any given time. An attempt to create a key after PTHREAD_KEYS_MAX exits, will cause a return value of EAGAIN from the pthread_key_create() function.\n\nAfter we have created a key, we may access its value using two pthread functions: pthread_getspecific() and pthread_setspecific(). The first is used to get the value of a given key, and the second is used to set the data of a given key. A key's value is simply a void pointer (void*), so we can store in it anything that we want. Lets see how to use these functions. We assume that 'a_key' is a properly initialized variable of type pthread_key_t that contains a previously created key:\n\nNote that if we set the value of the key in one thread, and try to get it in another thread, we will get a NULL, since this value is distinct for each thread.\n\nNote also that there are two cases where pthread_getspecific() might return NULL:\n• The key supplied as a parameter is invalid (e.g. its key wasn't created).\n• The value of this key is NULL. This means it either wasn't initialized, or was set to NULL explicitly by a previous call to pthread_setspecific().\n\nThe pthread_key_delete() function may be used to delete keys. But do not be confused by this function's name: it does not delete memory associated with this key, nor does it call the destructor function defined during the key's creation. Thus, you still need to do memory cleanup on your own if you need to free this memory during runtime. However, since usage of global variables (and thus also thread-specific data), you usually don't need to free this memory until the thread terminate, in which case the pthread library will invoke your destructor functions anyway.\n\nUsing this function is simple. Assuming list_key is a pthread_key_t variable pointing to a properly created key, use this function like this:\n\nthe function will return 0 on success, or EINVAL if the supplied variable does not point to a valid TSD key.\n\nAs we create threads, we need to think about terminating them as well. There are several issues involved here. We need to be able to terminate threads cleanly. Unlike processes, where a very ugly method of using signals is used, the folks that designed the pthreads library were a little more thoughtful. So they supplied us with a whole system of canceling a thread, cleaning up after a thread, and so on. We will discuss these methods here.\n\nWhen we want to terminate a thread, we can use the pthread_cancel function. This function gets a thread ID as a parameter, and sends a cancellation request to this thread. What this thread does with this request depends on its state. It might act on it immediately, it might act on it when it gets to a cancellation point (discussed below), or it might completely ignore it. We'll see later how to set the state of a thread and define how it acts on cancellation requests. Lets first see how to use the cancel function. We assume that 'thr_id' is a variable of type pthread_id containing the ID of a running thread:\n\nThe pthread_cancel() function returns 0, so we cannot know if it succeeded or not.\n\nA thread's cancel state may be modified using several methods. The first is by using the pthread_setcancelstate() function. This function defines whether the thread will accept cancellation requests or not. The function takes two arguments. One that sets the new cancel state, and one into which the previous cancel state is stored by the function. Here is how it is used:\n\nThis will disable canceling this thread. We can also enable canceling the thread like this:\n\nNote that you may supply a NULL pointer as the second parameter, and then you won't get the old cancel state.\n\nA similar function, named pthread_setcanceltype() is used to define how a thread responds to a cancellation request, assuming it is in the 'ENABLED' cancel state. One option is to handle the request immediately (asynchronously). The other is to defer the request until a cancellation point. To set the first option (asynchronous cancellation), do something like:\n\nAnd to set the second option (deferred cancellation):\n\nNote that you may supply a NULL pointer as the second parameter, and then you won't get the old cancel type.\n\nYou might wonder - \"What if i never set the cancellation state or type of a thread?\". Well, in such a case, the pthread_create() function automatically sets the thread to enabled deferred cancellation, that is, PTHREAD_CANCEL_ENABLE for the cancel mode, and PTHREAD_CANCEL_DEFERRED for the cancel type.\n\nAs we've seen, a thread might be in a state where it does not handle cancel requests immediately, but rather defers them until it reaches a cancellation point. So what are these cancellation points?\n\nIn general, any function that might suspend the execution of a thread for a long time, should be a cancellation point. In practice, this depends on the specific implementation, and how conformant it is to the relevant POSIX standard (and which version of the standard it conforms to...). The following set of pthread functions serve as cancellation points:\n\nThis means that if a thread executes any of these functions, it'll check for deferred cancel requests. If there is one, it will execute the cancellation sequence, and terminate. Out of these functions, pthread_testcancel() is unique - it's only purpose is to test whether a cancellation request is pending for this thread. If there is, it executes the cancellation sequence. If not, it returns immediately. This function may be used in a thread that does a lot of processing without getting into a \"natural\" cancellation state.\n\nOne of the features the pthreads library supplies is the ability for a thread to clean up after itself, before it exits. This is done by specifying one or more functions that will be called automatically by the pthreads library when the thread exits, either due to its own will (e.g. calling pthread_exit()), or due to it being canceled.\n\nTwo functions are supplied for this purpose. The pthread_cleanup_push() function is used to add a cleanup function to the set of cleanup functions for the current thread. The pthread_cleanup_pop() function removes the last function added with pthread_cleanup_push(). When the thread terminates, its cleanup functions are called in the reverse order of their registration. So the the last one to be registered is the first one to be called.\n\nWhen the cleanup functions are called, each one is supplied with one parameter, that was supplied as the second parameter to the pthread_cleanup_push() function call. Lets see how these functions may be used. In our example we'll see how these functions may be used to clean up some memory that our thread allocates when it starts running.\n\nAs we can see, we allocated some memory here, and registered a cleanup handler that will free this memory when our thread exits. After the execution of the main loop of our thread, we unregistered the cleanup handler. This must be done in the same function that registered the cleanup handler, and in the same nesting level, since both pthread_cleanup_pop() and pthread_cleanup_pop() functions are actually macros that add a '{' symbol and a '}' symbol, respectively.\n\nAs to the reason that we used that complex piece of code to unregister the cleanup handler, this is done to assure that our thread won't get canceled in the middle of the execution of our cleanup handler. This could have happened if our thread was in asynchronous cancellation mode. Thus, we made sure it was in deferred cancellation mode, then unregistered the cleanup handler, and finally restored whatever cancellation mode our thread was in previously. Note that we still assume the thread cannot be canceled in the execution of pthread_cleanup_pop() itself - this is true, since pthread_cleanup_pop() is not a cancellation point.\n\nSometimes it is desired for a thread to wait for the end of execution of another thread. This can be done using the pthread_join() function. It receives two parameters: a variable of type pthread_t, denoting the thread to be joined, and an address of a void* variable, into which the exit code of the thread will be placed (or PTHREAD_CANCELED if the joined thread was canceled). The pthread_join() function suspends the execution of the calling thread until the joined thread is terminated.\n\nFor example, consider our earlier thread pool server. Looking back at the code, you'll see that we used an odd sleep() call before terminating the process. We did this since the main thread had no idea when the other threads finished processing all pending requests. We could have solved it by making the main thread run a loop of checking if no more requests are pending, but that would be a busy loop.\n\nA cleaner way of implementing this, is by adding three changes to the code:\n• Tell the handler threads when we are done creating requests, by setting some flag.\n• Make the threads check, whenever the requests queue is empty, whether or not new requests are supposed to be generated. If not, then the thread should exit.\n• Make the main thread wait for the end of execution of each of the threads it spawned.\n\nThe first 2 changes are rather easy. We create a global variable named 'done_creating_requests' and set it to '0' initially. Each thread checks the contents of this variable every time before it intends to go to wait on the condition variable (i.e. the requests queue is empty). The main thread is modified to set this variable to '1' after it finished generating all requests. Then the condition variable is being broadcast, in case any of the threads is waiting on it, to make sure all threads go and check the 'done_creating_requests' flag.\n\nThe last change is done using a pthread_join() loop: call pthread_join() once for each handler thread. This way, we know that only after all handler threads have exited, this loop is finished, and then we may safely terminate the process. If we didn't use this loop, we might terminate the process while one of the handler threads is still handling a request.\n\nThe modified program is available in the file named thread-pool-server-with-join.c. Look for the word 'CHANGE' (in capital letters) to see the locations of the three changes.\n\nWe have seen how threads can be joined using the pthread_join() function. In fact, threads that are in a 'join-able' state, must be joined by other threads, or else their memory resources will not be fully cleaned out. This is similar to what happens with processes whose parents didn't clean up after them (also called 'orphan' or 'zombie' processes).\n\nIf we have a thread that we wish would exit whenever it wants without the need to join it, we should put it in the detached state. This can be done either with appropriate flags to the pthread_create() function, or by using the pthread_detach() function. We'll consider the second option in our tutorial.\n\nThe pthread_detach() function gets one parameter, of type pthread_t, that denotes the thread we wish to put in the detached state. For example, we can create a thread and immediately detach it with a code similar to this:\n\nOf-course, if we wish to have a thread in the detached state immediately, using the first option (setting the detached state directly when calling pthread_create() is more efficient.\n\nOur next example is much larger then the previous examples. It demonstrates how one could write a multi-threaded program in C, in a more or less clean manner. We take our previous thread-pool server, and enhance it in two ways. First, we add the ability to tune the number of handler threads based on the requests load. New threads are created if the requests queue becomes too large, and after the queue becomes shorter again, extra threads are canceled.\n\nSecond, we fix up the termination of the server when there are no more new requests to handle. Instead of the ugly sleep we used in our first example, this time the main thread waits for all threads to finish handling their last requests, by joining each of them using pthread_join().\n\nThe code is now being split to 4 separate files, as follows:\n• requests_queue.c - This file contains functions to manipulate a requests queue. We took the add_request() and get_request() functions and put them here, along with a data structure that contains all the variables previously defined as globals - pointer to queue's head, counter of requests, and even pointers to the queue's mutex and condition variable. This way, all the manipulation of the data is done in a single file, and all its functions receive a pointer to a 'requests_queue' structure.\n• handler_thread.c - this contains the functions executed by each handler thread - a function that runs the main loop (an enhanced version of the 'handle_requests_loop()' function, and a few local functions explained below). We also define a data structure to collect all the data we want to pass to each thread. We pass a pointer to such a structure as a parameter to the thread's function in the pthread_create() call, instead of using a bunch of ugly globals: the thread's ID, a pointer to the requests queue structure, and pointers to the mutex and condition variable to be used.\n• handler_threads_pool.c - here we define an abstraction of a thread pool. We have a function to create a thread, a function to delete (cancel) a thread, and a function to delete all active handler threads, called during program termination. we define here a structure similar to that used to hold the requests queue, and thus the functions are similar. However, because we only access this pool from one thread, the main thread, we don't need to protect it using a mutex. This saves some overhead caused by mutexes. the overhead is small, but for a busy server, it might begin to become noticeable. - main.c - and finally, the main function to rule them all, and in the system bind them. This function creates a requests queue, creates a threads pool, creates few handler threads, and then starts generating requests. After adding a request to the queue, it checks the queue size and the number of active handler threads, and adjusts the number of threads to the size of the queue. We use a simple water-marks algorithm here, but as you can see from the code, it can be easily be replaced by a more sophisticated algorithm. In our water-marks algorithm implementation, when the high water-mark is reached, we start creating new handler threads, to empty the queue faster. Later, when the low water-mark is reached, we start canceling the extra threads, until we are left with the original number of handler threads.\n\nAfter rewriting the program in a more manageable manner, we added code that uses the newly learned pthreads functions, as follows:\n• Each handler thread created puts itself in the deferred cancellation mode. This makes sure that when it gets canceled, it can finish handling its current request, before terminating.\n• Each handler thread also registers a cleanup function, to unlock the mutex when it terminates. This is done, since a thread is most likely to get canceled when calling pthread_cond_wait(), which is a cancellation point. Since the function is called with the mutex locked, it might cause the thread to exit and cause all other threads to 'hang' on the mutex. Thus, unlocking the mutex in a cleanup handler (registered with the pthread_cleanup_push() function) is the proper solution.\n• Finally, the main thread is set to clean up properly, and not brutally, as we did before. When it wishes to terminate, it calls the 'delete_handler_threads_pool()' function, which calls pthread_join for each remaining handler thread. This way, the function returns only after all handler threads finished handling their last request.\n\nPlease refer to the source code for the full details. Reading the header files first will make it easier to understand the design. To compile the program, just switch to the thread-pool-server-changes directory, and type 'gmake'.\n\nOne area in which threads can be very helpful is in user-interface programs. These programs are usually centered around a loop of reading user input, processing it, and showing the results of the processing. The processing part may sometimes take a while to complete, and the user is made to wait during this operation. By placing such long operations in a seperate thread, while having another thread to read user input, the program can be more responsive. It may allow the user to cancel the operation in the middle.\n\nIn graphical programs the problem is more severe, since the application should always be ready for a message from the windowing system telling it to repaint part of its window. If it's too busy executing some other task, its window will remain blank, which is rather ugly. In such a case, it is a good idea to have one thread handle the message loop of the windowing systm and always ready to get such repaint requests (as well as user input). When ever this thread sees a need to do an operation that might take a long time to complete (say, more then 0.2 seconds in the worse case), it will delegate the job to a seperate thread.\n\nIn order to structure things better, we may use a third thread, to control and synchronize the user-input and task-performing threads. If the user-input thread gets any user input, it will ask the controlling thread to handle the operation. If the task-performing thread finishes its operation, it will ask the controlling thread to show the results to the user.\n\nAs an example, we will write a simple character-mode program that counts the number of lines in a file, while allowing the user to cancel the operation in the middle.\n\nOur main thread will launch one thread to perform the line counting, and a second thread to check for user input. After that, the main thread waits on a condition variable. When any of the threads finishes its operation, it signals this condition variable, in order to let the main thread check what happened. A global variable is used to flag whether or not a cancel request was made by the user. It is initialized to '0', but if the user-input thread receives a cancellation request (the user pressing 'e'), it sets this flag to '1', signals the condition variable, and terminates. The line-counting thread will signal the condition variable only after it finished its computation.\n\nBefore you go read the program, we should explain the use of the system() function and the 'stty' Unix command. The system() function spawns a shell in which it executes the Unix command given as a parameter. The stty Unix command is used to change terminal mode settings. We use it to switch the terminal from its default, line-buffered mode, to a character mode (also known as raw mode), so the call to getchar() in the user-input thread will return immediatly after the user presses any key. If we hadn't done so, the system will buffer all input to the program until the user presses the ENTER key. Finally, since this raw mode is not very useful (to say the least) once the program terminates and we get the shell prompt again, the user-input thread registers a cleanup function that restores the normal terminal mode, i.e. line-buffered. For more info, please refer to stty's manual page.\n\nThe program's source can be found in the file line-count.c. The name of the file whose lines it reads is hardcoded to 'very_large_data_file'. You should create a file with this name in the program's directory (large enough for the operation to take enough time). Alternatively, you may un-compress the file 'very_large_data_file.Z' found in this directory, using the command:\n\nnote that this will create a 5MB(!) file named 'very_large_data_file', so make sure you have enough free disk-space before performing this operation.\n\nThe condition is now TRUE, but Process A is stuck waiting on the condition variable - it missed the wakeup signal. If we alter Process B to lock the mutex:\n\n...then the above cannot occur; the wakeup will never be missed."
    },
    {
        "link": "https://geeksforgeeks.org/producer-consumer-problem-in-c",
        "document": "Concurrency is an important topic in concurrent programming since it allows us to completely understand how the systems work. Among the several challenges faced by practitioners working with these systems, there is a major synchronization issue which is the producer-consumer problem. In this article, we will discuss this problem and look at possible solutions based on C programming.\n\nWhat is the Producer-Consumer Problem?\n\nThe producer-consumer problem is an example of a multi-process synchronization problem. The problem describes two processes, the producer and the consumer that share a common fixed-size buffer and use it as a queue.\n• None The producer’s job is to generate data, put it into the buffer, and start again.\n• None At the same time, the consumer is consuming the data (i.e., removing it from the buffer), one piece at a time.\n\nWhat is the Actual Problem?\n\nGiven the common fixed-size buffer, the task is to make sure that the producer can’t add data into the buffer when it is full and the consumer can’t remove data from an empty buffer. Accessing memory buffers should not be allowed to producers and consumers at the same time.\n\n\n\nThe producer is to either go to sleep or discard data if the buffer is full. The next time the consumer removes an item from the buffer, it notifies the producer, who starts to fill the buffer again. In the same manner, the consumer can go to sleep if it finds the buffer to be empty. The next time the producer transfer data into the buffer, it wakes up the sleeping consumer.\n\nNote: An inadequate solution could result in a deadlock where both processes are waiting to be awakened.\n\nApproach: The idea is to use the concept of parallel programming and Critical Section to implement the Producer-Consumer problem in C language using OpenMP.\n\nBelow is the implementation of the above approach:\n\nQuestion 1: Processes P1 and P2 have a producer-consumer relationship, communicating by the use of a set of shared buffers.\n\nIncreasing the number of buffers is likely to do which of the following? [ISRO CS 2018]\n\nI. Increase the rate at which requests are satisfied (throughput).\n\nSolution : Increasing the size of the memory allocated to the process or increasing buffer requirement does not affect the likelihood of the \n\n deadlock and doesn't affect the implementation of the system. It can increase the rate at which the requests are satisfied(throughput) larger \n\n will be the size of the buffer, larger will be throughput. Therefore the only statement correct is I. Hence option (C) is correct.\n\nHow do we prevent a race condition in producer-consumer code?\n\nWhy is the producer-consumer problem relevant in operating systems?"
    },
    {
        "link": "https://stackoverflow.com/questions/40855584/c-producer-consumer-using-pthreads",
        "document": "I am working on a problem where I am implementing a program that mimics the producer-consumer paradigm. The code that I am using works when I only have one producer and one consumer but it does not work when I add another producer and another consumer.\n\nI have spent a while on this and can not seem to figure out why I am getting the error . I have followed the problem through various tests and the problem lies in the fact that a producer is not being blocked when it notices another producer is in its critical section.\n\nAs you can see, Producer 0 manages to put a 2 in slot 1. However, before Producer 0 can increment , Producer 2 attempts to write data into Slot 1 because was not incremented.\n\nFor some reason it seems that my are not working. Can anybody help me out?"
    },
    {
        "link": "https://stackoverflow.com/questions/22164552/producer-consumer-in-c-with-pthread-semaphores-and-multiple-threads",
        "document": "Lets say I have a buffer that has 3 producer threads and 5 consumer threads inserting and consuming to/from the buffer.\n\nI only want to allow 1 producer or up to 3 consumer threads access the buffer at any given time.\n\nUp to 3 consumers can peek at the top element in the buffer, only, if no producer is accessing it. If more than 1 consumer thread does access the buffer, the last thread to leave must delete the top element.\n\nNow this is part of a class assignment, and the assignment explicitly states to use semaphores. However, I can't think of a way to really implement this wording exactly using only semaphores.\n\nThe pseudo code -I think- should look like this: (I'm not worrying about an empty or full buffer, just this sub-part of the problem)\n\nI think that's the gist of the logic, problem is how to implement this with just semaphores. How do I allow only 1 producer in with a semaphore but allow 3 consumers in on the other side? It seems like I would need to use something besides a semaphore.\n\nAlso, please correct any of the logic if needed, this is meant to just be a general idea."
    },
    {
        "link": "https://docs.oracle.com/cd/E36784_01/html/E36868/sync-31.html",
        "document": "The producer and consumer problem is one of the small collection of standard, well-known problems in concurrent programming. A finite-size buffer and two classes of threads, producers and consumers, put items into the buffer (producers) and take items out of the buffer (consumers).\n\nA producer cannot put something in the buffer until the buffer has space available. A consumer cannot take something out of the buffer until the producer has written to the buffer.\n\nA condition variable represents a queue of threads that wait for some condition to be signaled.\n\nExample 4–11 has two such queues. One ( ) queue for producers waits for a slot in the buffer. The other ( ) queue for consumers waits for a buffer slot containing information. The example also has a mutex, as the data structure describing the buffer must be accessed by only one thread at a time.\n\nAs Example 4–12 shows, the producer thread acquires the mutex protecting the buffer data structure. The producer thread then makes certain that space is available for the item produced. If space is not available, the producer thread calls . causes the producer thread to join the queue of threads that are waiting for the condition to be signaled. represents available room in the buffer.\n\nAt the same time, as part of the call to , the thread releases its lock on the mutex. The waiting producer threads depend on consumer threads to signal when the condition is true, as shown in Example 4–12. When the condition is signaled, the first thread waiting on is awakened. However, before the thread can return from , the thread must acquire the lock on the mutex again.\n\nAcquire the mutex to ensure that the thread again has mutually exclusive access to the buffer data structure. The thread then must check that available room in the buffer actually exists. If room is available, the thread writes into the next available slot.\n\nAt the same time, consumer threads might be waiting for items to appear in the buffer. These threads are waiting on the condition variable . A producer thread, having just deposited something in the buffer, calls to wake up the next waiting consumer. If no consumers are waiting, this call has no effect.\n\nFinally, the producer thread unlocks the mutex, allowing other threads to operate on the buffer data structure.\n\nThe Producer and Consumer Problem: the Producer\n\nNote the use of the statement. Unless the code is compiled with defined, does nothing when its argument evaluates to true (nonzero). The program aborts if the argument evaluates to false (zero). Such assertions are especially useful in multithreaded programs. immediately points out runtime problems if the assertion fails. has the additional effect of providing useful comments.\n\nThe comment that begins could better be expressed as an assertion, but the statement is too complicated as a Boolean-valued expression and so is given in English.\n\nBoth assertions and comments are examples of invariants. These invariants are logical statements that should not be falsified by the execution of the program with the following exception. The exception occurs during brief moments when a thread is modifying some of the program variables mentioned in the invariant. An assertion, of course, should be true whenever any thread executes the statement.\n\nThe use of invariants is an extremely useful technique. Even if the invariants are not stated in the program text, think in terms of invariants when you analyze a program.\n\nThe invariant in the producer code that is expressed as a comment is always true whenever a thread executes the code where the comment appears. If you move this comment to just after the , the comment does not necessarily remain true. If you move this comment to just after the , the comment is still true.\n\nThis invariant therefore expresses a property that is true at all times with the following exception. The exception occurs when either a producer or a consumer is changing the state of the buffer. While a thread is operating on the buffer under the protection of a mutex, the thread might temporarily falsify the invariant. However, once the thread is finished, the invariant should be true again.\n\nExample 4–13 shows the code for the consumer. The logic flow is symmetric with the logic flow of the producer.\n\nThe Producer and Consumer Problem: the Consumer"
    },
    {
        "link": "https://medium.com/nerd-for-tech/hands-on-multithreading-with-c-04-producer-consumer-problem-26abdddc485d",
        "document": "16_producer_consumer_01 16_producer_consumer_01.cpp 17_producer_consumer_02 17_producer_consumer_02.cpp Makefile\n\nroot@DemoYuChengKuo:~/3_multithreading_cpp/code_04_producer_consumer# ./17_producer_consumer_02 \n\nProducing 1\n\nBuffer size after producing: 1\n\n\n\nProducing 2\n\nBuffer size after producing: 2\n\n\n\nProducing 3\n\nBuffer size after producing: 3\n\n\n\nProducing 4\n\nBuffer size after producing: 4\n\n\n\nProducing 5\n\nBuffer size after producing: 5\n\n\n\nProducing 6\n\nBuffer size after producing: 6\n\n\n\nProducing 7\n\nBuffer size after producing: 7\n\n\n\nProducing 8\n\nBuffer size after producing: 8\n\n\n\nProducing 9\n\nBuffer size after producing: 9\n\n\n\nProducing 10\n\nBuffer size after producing: 10\n\n\n\nConsuming 1\n\nBuffer size after consuming: 9\n\n\n\nProducing 11\n\nBuffer size after producing: 10\n\n\n\nConsuming 2\n\nBuffer size after consuming: 9\n\n\n\nProducing 12\n\nBuffer size after producing: 10\n\n\n\nConsuming 3\n\nBuffer size after consuming: 9\n\n\n\nProducing 13\n\nBuffer size after producing: 10\n\n\n\nConsuming 4\n\nBuffer size after consuming: 9\n\n\n\nProducing 14\n\nBuffer size after producing: 10\n\n\n\nConsuming 5\n\nBuffer size after consuming: 9\n\n\n\nProducing 15\n\nBuffer size after producing: 10\n\n\n\nConsuming 6\n\nBuffer size after consuming: 9\n\n\n\nProducing 16\n\nBuffer size after producing: 10\n\n\n\nConsuming 7\n\nBuffer size after consuming: 9\n\n\n\nProducing 17\n\nBuffer size after producing: 10\n\n\n\nConsuming 8\n\nBuffer size after consuming: 9\n\n\n\nProducing 18\n\nBuffer size after producing: 10\n\n\n\nConsuming 9\n\nBuffer size after consuming: 9\n\n\n\nProducing 19\n\nBuffer size after producing: 10\n\n\n\nConsuming 10\n\nBuffer size after consuming: 9\n\n\n\nProducing 20\n\nBuffer size after producing: 10\n\n\n\nConsuming 11\n\nBuffer size after consuming: 9\n\n\n\nConsuming 12\n\nBuffer size after consuming: 8\n\n\n\nConsuming 13\n\nBuffer size after consuming: 7\n\n\n\nConsuming 14\n\nBuffer size after consuming: 6\n\n\n\nConsuming 15\n\nBuffer size after consuming: 5\n\n\n\nConsuming 16\n\nBuffer size after consuming: 4\n\n\n\nConsuming 17\n\nBuffer size after consuming: 3\n\n\n\nConsuming 18\n\nBuffer size after consuming: 2\n\n\n\nConsuming 19\n\nBuffer size after consuming: 1\n\n\n\nConsuming 20\n\nBuffer size after consuming: 0\n\n// 17_producer_consumer_02.cpp\n\n#include <iostream>\n\n#include <queue>\n\n#include <thread>\n\n#include <mutex>\n\n#include <condition_variable>\n\nusing namespace std;\n\n\n\nmutex mtx; // Declares a mutex for critical section management\n\ncondition_variable cond_var; // Declares a condition variable for blocking and waking threads\n\nqueue<int> buffer; // Declares a queue to act as the buffer\n\nconst unsigned int MAX_BUFFER_SIZE = 10; // Sets the maximum size of the buffer\n\n\n\nvoid producer(int value) { // Defines the producer function\n\n unique_lock<mutex> lock(mtx); // Locks the mutex before accessing the buffer\n\n cond_var.wait(lock, [] { return buffer.size() < MAX_BUFFER_SIZE; }); // Waits if the buffer is full\n\n\n\n cout << \"Producing \" << value << endl; // Prints the produced value\n\n buffer.push(value); // Pushes the value into the buffer\n\n cout << \"Buffer size after producing: \" << buffer.size() << endl << endl; // Prints the buffer size after producing\n\n\n\n lock.unlock(); // Unlocks the mutex\n\n cond_var.notify_one(); // Notifies one waiting thread\n\n}\n\n\n\nvoid consumer() { // Defines the consumer function\n\n unique_lock<mutex> lock(mtx); // Locks the mutex before accessing the buffer\n\n cond_var.wait(lock, [] { return buffer.size() > 0; }); // Waits if the buffer is empty\n\n\n\n int value = buffer.front(); // Gets the value from the front of the buffer\n\n buffer.pop(); // Removes the value from the buffer\n\n cout << \"Consuming \" << value << endl; // Prints the consumed value\n\n cout << \"Buffer size after consuming: \" << buffer.size() << endl << endl; // Prints the buffer size after consuming\n\n\n\n lock.unlock(); // Unlocks the mutex\n\n cond_var.notify_one(); // Notifies one waiting thread\n\n}\n\n\n\nint main() { // The main function\n\n thread producerThread([] { // Creates a producer thread\n\n for (int i = 1; i <= 20; ++i) {\n\n producer(i); // Produces 20 items\n\n }\n\n });\n\n\n\n // Delay before starting consumer thread\n\n this_thread::sleep_for(chrono::seconds(3));\n\n\n\n thread consumerThread([] { // Creates a consumer thread\n\n for (int i = 1; i <= 20; ++i) {\n\n consumer(); // Consumes 20 items\n\n }\n\n });\n\n\n\n producerThread.join(); // Waits for producer thread to finish\n\n consumerThread.join(); // Waits for consumer thread to finish\n\n\n\n return 0; // Ends the program\n\n}\n\nIn this example, we introduced a delay with “std::this_thread::sleep_for(std::chrono::seconds(3));” to make sure the buffer would be full with data."
    }
]