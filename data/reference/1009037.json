[
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.transform.html",
        "document": "Call on self producing a DataFrame with the same axis shape as self.\n\nFunction to use for transforming the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. If func is both list-like and dict-like, dict-like behavior takes precedence.\n• None dict-like of axis labels -> functions, function names or list-like of such. If 0 or ‘index’: apply function to each column. If 1 or ‘columns’: apply function to each row. A DataFrame that must have the same length as self. ValueError If the returned DataFrame has a different length than self.\n\nFunctions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.\n\nEven though the resulting DataFrame must have the same length as the input DataFrame, it is possible to provide several input functions:\n\nYou can call transform on a GroupBy object:"
    },
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html",
        "document": ""
    },
    {
        "link": "https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.transform.html",
        "document": "Call on self producing a DataFrame with the same axis shape as self.\n\nFunction to use for transforming the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. If func is both list-like and dict-like, dict-like behavior takes precedence.\n• None dict-like of axis labels -> functions, function names or list-like of such. If 0 or ‘index’: apply function to each column. If 1 or ‘columns’: apply function to each row. A DataFrame that must have the same length as self. ValueError If the returned DataFrame has a different length than self.\n\nFunctions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.\n\nEven though the resulting DataFrame must have the same length as the input DataFrame, it is possible to provide several input functions:\n\nYou can call transform on a GroupBy object:"
    },
    {
        "link": "https://pbpython.com/pandas_transform.html",
        "document": "One of the compelling features of pandas is that it has a rich library of methods for manipulating data. However, there are times when it is not clear what the various functions do and how to use them. If you are approaching a problem from an Excel mindset, it can be difficult to translate the planned solution into the unfamiliar pandas command. One of those “unknown” functions is the method. Even after using pandas for a while, I have never had the chance to use this function so I recently took some time to figure out what it is and how it could be helpful for real world analysis. This article will walk through an example where can be used to efficiently summarize data.\n\nI have found the best coverage of this topic in Jake VanderPlas’ excellent Python Data Science Handbook. I plan to write a review on this book in the future but the short and sweet is that it is a great resource that I highly recommend. As described in the book, is an operation used in conjunction with (which is one of the most useful operations in pandas). I suspect most pandas users likely have used , or with to summarize data. However, is a little more difficult to understand - especially coming from an Excel world. Since Jake made all of his book available via jupyter notebooks it is a good place to start to understand how transform is unique: While aggregation must return a reduced version of the data, transformation can return some transformed version of the full data to recombine. For such a transformation, the output is the same shape as the input. A common example is to center the data by subtracting the group-wise mean. With that basic definition, I will go through another example that can explain how this is useful in other instances outside of centering data.\n\nFor this example, we will analyze some fictitious sales data. In order to keep the dataset small, here is a sample of 12 sales transactions for our company: You can see in the data that the file contains 3 different orders (10001, 10005 and 10006) and that each order consists has multiple products (aka skus). The question we would like to answer is: “What percentage of the order total does each sku represent?” For example, if we look at order 10001 with a total of $576.12, the break down would be: The tricky part in this calculation is that we need to get a total for each order and combine it back with the transaction level detail in order to get the percentages. In Excel, you could try to use some version of a subtotal to try to calculate the values.\n\nIf you are familiar with pandas, your first inclination is going to be trying to group the data into a new dataframe and combine it in a multi-step process. Here’s what that approach would look like. Import all the modules we need and read in our data: Now that the data is in a dataframe, determining the total by order is simple with the help of the standard aggregation. Here is a simple image showing what is happening with the standard The tricky part is figuring out how to combine this data back with the original dataframe. The first instinct is to create a new dataframe with the totals by order and merge it back with the original. We could do something like this: This certainly works but there are several steps needed to get the data combined in the manner we need.\n\nUsing the original data, let’s try using and and see what we get: You will notice how this returns a different size data set from our normal functions. Instead of only showing the totals for 3 orders, we retain the same number of items as the original data set. That is the unique feature of using . As an added bonus, you could combine into one statement if you did not want to show the individual order totals: Here is a diagram to show what is happening: After taking the time to understand , I think you will agree that this tool can be very powerful - even if it is a unique approach as compared to the standard Excel mindset.\n\nI am continually amazed at the power of pandas to make complex numerical manipulations very efficient. Despite working with pandas for a while, I never took the time to figure out how to use Now that I understand how it works, I am sure I will be able to use it in future analysis and hope that you will find this useful as well."
    },
    {
        "link": "https://pandas.pydata.org/docs/user_guide/basics.html",
        "document": "Here we discuss a lot of the essential functionality common to the pandas data structures. To begin, let’s create some example objects like we did in the 10 minutes to pandas section:\n\npandas objects have a number of attributes enabling you to access the metadata\n• None shape: gives the axis dimensions of the object, consistent with ndarray Note, these attributes can be safely assigned to! pandas objects ( , , ) can be thought of as containers for arrays, which hold the actual data and do the actual computation. For many types, the underlying array is a . However, pandas and 3rd party libraries may extend NumPy’s type system to add support for custom arrays (see dtypes). To get the actual data inside a or , use the property will always be an . The exact details of what an is and why pandas uses them are a bit beyond the scope of this introduction. See dtypes for more. If you know you need a NumPy array, use or . When the Series or Index is backed by an , may involve copying data and coercing values. See dtypes for more. gives some control over the of the resulting . For example, consider datetimes with timezones. NumPy doesn’t have a dtype to represent timezone-aware datetimes, so there are two possibly useful representations:\n• None An object-dtype with objects, each with the correct\n• None A -dtype , where the values have been converted to UTC and the timezone discarded Timezones may be preserved with Or thrown away with Getting the “raw data” inside a is possibly a bit more complex. When your only has a single data type for all the columns, will return the underlying data: If a DataFrame contains homogeneously-typed data, the ndarray can actually be modified in-place, and the changes will be reflected in the data structure. For heterogeneous data (e.g. some of the DataFrame’s columns are not all the same dtype), this will not be the case. The values attribute itself, unlike the axis labels, cannot be assigned to. When working with heterogeneous data, the dtype of the resulting ndarray will be chosen to accommodate all of the data involved. For example, if strings are involved, the result will be of object dtype. If there are only floats and integers, the resulting array will be of float dtype. In the past, pandas recommended or for extracting the data from a Series or DataFrame. You’ll still find references to these in old code bases and online. Going forward, we recommend avoiding and using or . has the following drawbacks:\n• None When your Series contains an extension type, it’s unclear whether returns a NumPy array or the extension array. will always return an , and will never copy data. will always return a NumPy array, potentially at the cost of copying / coercing values.\n• None When your DataFrame contains a mixture of data types, may involve copying data and coercing values to a common dtype, a relatively expensive operation. , being a method, makes it clearer that the returned NumPy array may not be a view on the same data in the DataFrame.\n\nWith binary operations between pandas data structures, there are two key points of interest: We will demonstrate how to manage these issues independently, though they can be handled simultaneously. DataFrame has the methods , , , and related functions , , … for carrying out binary operations. For broadcasting behavior, Series input is of primary interest. Using these functions, you can use to either match on the index or columns via the axis keyword: Furthermore you can align a level of a MultiIndexed DataFrame with a Series. Series and Index also support the builtin. This function takes the floor division and modulo operation at the same time returning a two-tuple of the same type as the left hand side. For example: We can also do elementwise : In Series and DataFrame, the arithmetic functions have the option of inputting a fill_value, namely a value to substitute when at most one of the values at a location are missing. For example, when adding two DataFrame objects, you may wish to treat NaN as 0 unless both DataFrames are missing that value, in which case the result will be NaN (you can later replace NaN with some other value using if you wish). Series and DataFrame have the binary comparison methods , , , , , and whose behavior is analogous to the binary arithmetic operations described above: These operations produce a pandas object of the same type as the left-hand-side input that is of dtype . These objects can be used in indexing operations, see the section on Boolean indexing. You can apply the reductions: , , , and to provide a way to summarize a boolean result. You can reduce to a final boolean value. You can test if a pandas object is empty, via the property. Asserting the truthiness of a pandas object will raise an error, as the testing of the emptiness or values is ambiguous. Traceback (most recent call last) in in : The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). Traceback (most recent call last) in in : The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). See gotchas for a more detailed discussion. Often you may find that there is more than one way to compute the same result. As a simple example, consider and . To test that these two computations produce the same result, given the tools shown above, you might imagine using . But in fact, this expression is False: Notice that the boolean DataFrame contains some False values! This is because NaNs do not compare as equals: So, NDFrames (such as Series and DataFrames) have an method for testing equality, with NaNs in corresponding locations treated as equal. Note that the Series or DataFrame index needs to be in the same order for equality to be True: You can conveniently perform element-wise comparisons when comparing a pandas data structure with a scalar value: pandas also handles element-wise comparisons between different array-like objects of the same length: Trying to compare or objects of different lengths will raise a ValueError: Traceback (most recent call last) in in in : Can only compare identically-labeled Series objects Traceback (most recent call last) in in in : Can only compare identically-labeled Series objects A problem occasionally arising is the combination of two similar data sets where values in one are preferred over the other. An example would be two data series representing a particular economic indicator where one is considered to be of “higher quality”. However, the lower quality series might extend further back in history or have more complete data coverage. As such, we would like to combine two DataFrame objects where missing values in one DataFrame are conditionally filled with like-labeled values from the other DataFrame. The function implementing this operation is , which we illustrate: The method above calls the more general . This method takes another DataFrame and a combiner function, aligns the input DataFrame and then passes the combiner function pairs of Series (i.e., columns whose names are the same). So, for instance, to reproduce as above:\n\nThere exists a large number of methods for computing descriptive statistics and other related operations on Series, DataFrame. Most of these are aggregations (hence producing a lower-dimensional result) like , , and , but some of them, like and , produce an object of the same size. Generally speaking, these methods take an axis argument, just like ndarray.{sum, std, …}, but the axis can be specified by name or integer: All such methods have a option signaling whether to exclude missing data ( by default): Combined with the broadcasting / arithmetic behavior, one can describe various statistical procedures, like standardization (rendering data zero mean and standard deviation of 1), very concisely: Note that methods like and preserve the location of values. This is somewhat different from and since behavior is furthermore dictated by a parameter. Here is a quick reference summary table of common functions. Each also takes an optional parameter which applies only if the object has a hierarchical index. Standard error of the mean Note that by chance some NumPy methods, like , , and , will exclude NAs on Series input by default: will return the number of unique non-NA values in a Series: There is a convenient function which computes a variety of summary statistics about a Series or the columns of a DataFrame (excluding NAs of course): You can select specific percentiles to include in the output: By default, the median is always included. For a non-numerical Series object, will give a simple summary of the number of unique values and most frequently occurring values: Note that on a mixed-type DataFrame object, will restrict the summary to include only numerical columns or, if none are, only categorical columns: This behavior can be controlled by providing a list of types as / arguments. The special value can also be used: That feature relies on select_dtypes. Refer to there for details about accepted inputs. The and functions on Series and DataFrame compute the index labels with the minimum and maximum corresponding values: When there are multiple rows (or columns) matching the minimum or maximum value, and return the first matching index: and are called and in NumPy. The Series method computes a histogram of a 1D array of values. It can also be used as a function on regular arrays: The method can be used to count combinations across multiple columns. By default all columns are used but a subset can be selected using the argument. Similarly, you can get the most frequently occurring value(s), i.e. the mode, of the values in a Series or DataFrame: Continuous values can be discretized using the (bins based on values) and (bins based on sample quantiles) functions: computes sample quantiles. For example, we could slice up some normally distributed data into equal-size quartiles like so: We can also pass infinite values to define the bins:\n\nTo apply your own or another library’s functions to pandas objects, you should be aware of the three methods below. The appropriate method to use depends on whether your function expects to operate on an entire or , row- or column-wise, or elementwise. and can be passed into functions. However, if the function needs to be called in a chain, consider using the method. and are functions taking and returning . Now compare the following: pandas encourages the second style, which is known as method chaining. makes it easy to use your own or another library’s functions in method chains, alongside pandas’ methods. In the example above, the functions and each expected a as the first positional argument. What if the function you wish to apply takes its data as, say, the second argument? In this case, provide with a tuple of . will route the to the argument specified in the tuple. For example, we can fit a regression using statsmodels. Their API expects a formula first and a as the second argument, . We pass in the function, keyword pair to : The pipe method is inspired by unix pipes and more recently dplyr and magrittr, which have introduced the popular (read pipe) operator for R. The implementation of here is quite clean and feels right at home in Python. We encourage you to view the source code of . Arbitrary functions can be applied along the axes of a DataFrame using the method, which, like the descriptive statistics methods, takes an optional argument: The method will also dispatch on a string method name. The return type of the function passed to affects the type of the final output from for the default behaviour:\n• None If the applied function returns a , the final output is a . The columns match the index of the returned by the applied function.\n• None If the applied function returns any other type, the final output is a . This default behaviour can be overridden using the , which accepts three options: , , and . These will determine how list-likes return values expand (or not) to a . combined with some cleverness can be used to answer many questions about a data set. For example, suppose we wanted to extract the date where the maximum value for each column occurred: You may also pass additional arguments and keyword arguments to the method. Another useful feature is the ability to pass Series methods to carry out some Series operation on each column or row: Finally, takes an argument which is False by default, which converts each row or column into a Series before applying the function. When set to True, the passed function will instead receive an ndarray object, which has positive performance implications if you do not need the indexing functionality. The aggregation API allows one to express possibly multiple aggregation operations in a single concise way. This API is similar across pandas objects, see groupby API, the window API, and the resample API. The entry point for aggregation is , or the alias . We will use a similar starting frame from above: Using a single function is equivalent to . You can also pass named methods as strings. These will return a of the aggregated output: # these are equivalent to a ``.sum()`` because we are aggregating Single aggregations on a this will return a scalar value: You can pass multiple aggregation arguments as a list. The results of each of the passed functions will be a row in the resulting . These are naturally named from the aggregation function. On a , multiple functions return a , indexed by the function names: Passing a named function will yield that name for the row: Passing a dictionary of column names to a scalar or a list of scalars, to allows you to customize which functions are applied to which columns. Note that the results are not in any particular order, you can use an instead to guarantee ordering. Passing a list-like will generate a output. You will get a matrix-like output of all of the aggregators. The output will consist of all unique functions. Those that are not noted for a particular column will be : With it is possible to easily create a custom describe function, similar to the built in describe function. The method returns an object that is indexed the same (same size) as the original. This API allows you to provide multiple operations at the same time rather than one-by-one. Its API is quite similar to the API. We create a frame similar to the one used in the above sections. Transform the entire frame. allows input functions as: a NumPy function, a string function name or a user defined function. Here received a single function; this is equivalent to a ufunc application. Passing a single function to with a will yield a single in return. Passing multiple functions will yield a column MultiIndexed DataFrame. The first level will be the original frame column names; the second level will be the names of the transforming functions. Passing multiple functions to a Series will yield a DataFrame. The resulting column names will be the transforming functions. Passing a dict of functions will allow selective transforming per column. Passing a dict of lists will generate a MultiIndexed DataFrame with these selective transforms. Since not all functions can be vectorized (accept NumPy arrays and return another array or value), the methods on DataFrame and analogously on Series accept any Python function taking a single value and returning a single value. For example: has an additional feature; it can be used to easily “link” or “map” values defined by a secondary series. This is closely related to merging/joining functionality:\n\nis the fundamental data alignment method in pandas. It is used to implement nearly all other features relying on label-alignment functionality. To reindex means to conform the data to match a given set of labels along a particular axis. This accomplishes several things:\n• None Reorders the existing data to match a new set of labels\n• None Inserts missing value (NA) markers in label locations where no data for that label existed\n• None If specified, fill data for missing labels using logic (highly relevant to working with time series data) Here is a simple example: Here, the label was not contained in the Series and hence appears as in the result. With a DataFrame, you can simultaneously reindex the index and columns: Note that the objects containing the actual axis labels can be shared between objects. So if we have a Series and a DataFrame, the following can be done: This means that the reindexed Series’s index is the same Python object as the DataFrame’s index. also supports an “axis-style” calling convention, where you specify a single argument and the it applies to. MultiIndex / Advanced Indexing is an even more concise way of doing reindexing. When writing performance-sensitive code, there is a good reason to spend some time becoming a reindexing ninja: many operations are faster on pre-aligned data. Adding two unaligned DataFrames internally triggers a reindexing step. For exploratory analysis you will hardly notice the difference (because has been heavily optimized), but when CPU cycles matter sprinkling a few explicit calls here and there can have an impact. Reindexing to align with another object# You may wish to take an object and reindex its axes to be labeled the same as another object. While the syntax for this is straightforward albeit verbose, it is a common enough operation that the method is available to make this simpler: Aligning objects with each other with # The method is the fastest way to simultaneously align two objects. It supports a argument (related to joining and merging):\n• None : take the union of the indexes (default) It returns a tuple with both of the reindexed Series: For DataFrames, the join method will be applied to both the index and the columns by default: You can also pass an option to only align on the specified axis: If you pass a Series to , you can choose to align both objects either on the DataFrame’s index or columns using the argument: takes an optional parameter which is a filling method chosen from the following table: Fill from the nearest index value We illustrate these fill methods on a simple Series: These methods require that the indexes are ordered increasing or decreasing. Note that the same result could have been achieved using ffill (except for ) or interpolate: will raise a ValueError if the index is not monotonically increasing or decreasing. and will not perform any checks on the order of the index. The and arguments provide additional control over filling while reindexing. Limit specifies the maximum count of consecutive matches: In contrast, tolerance specifies the maximum distance between the index and indexer values: Notice that when used on a , or , will coerced into a if possible. This allows you to specify tolerance with appropriate strings. A method closely related to is the function. It removes a set of labels from an axis: Note that the following also works, but is a bit less obvious / clean: The method allows you to relabel an axis based on some mapping (a dict or Series) or an arbitrary function. If you pass a function, it must return a value when called with any of the labels (and must produce a set of unique values). A dict or Series can also be used: If the mapping doesn’t include a column/index label, it isn’t renamed. Note that extra labels in the mapping don’t throw an error. also supports an “axis-style” calling convention, where you specify a single and the to apply that mapping to. Finally, also accepts a scalar or list-like for altering the attribute. The methods and allow specific names of a to be changed (as opposed to the labels).\n\nThe behavior of basic iteration over pandas objects depends on the type. When iterating over a Series, it is regarded as array-like, and basic iteration produces the values. DataFrames follow the dict-like convention of iterating over the “keys” of the objects. Thus, for example, iterating over a DataFrame gives you the column names: pandas objects also have the dict-like method to iterate over the (key, value) pairs. To iterate over the rows of a DataFrame, you can use the following methods:\n• None : Iterate over the rows of a DataFrame as (index, Series) pairs. This converts the rows to Series objects, which can change the dtypes and has some performance implications.\n• None : Iterate over the rows of a DataFrame as namedtuples of the values. This is a lot faster than , and is in most cases preferable to use to iterate over the values of a DataFrame. Iterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed and can be avoided with one of the following approaches:\n• None Look for a vectorized solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing, …\n• None When you have a function that cannot work on the full DataFrame/Series at once, it is better to use instead of iterating over the values. See the docs on function application.\n• None If you need to do iterative manipulations on the values but performance is important, consider writing the inner loop with cython or numba. See the enhancing performance section for some examples of this approach. You should never modify something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect! For example, in the following case setting the value has no effect: Consistent with the dict-like interface, iterates through key-value pairs: allows you to iterate through the rows of a DataFrame as Series objects. It returns an iterator yielding each index value along with a Series containing the data in each row: Because returns a Series for each row, it does not preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). For example, All values in , returned as a Series, are now upcasted to floats, also the original integer value in column : To preserve dtypes while iterating over the rows, it is better to use which returns namedtuples of the values and which is generally much faster than . For instance, a contrived way to transpose the DataFrame would be: The method will return an iterator yielding a namedtuple for each row in the DataFrame. The first element of the tuple will be the row’s corresponding index value, while the remaining values are the row values. This method does not convert the row to a Series object; it merely returns the values inside a namedtuple. Therefore, preserves the data type of the values and is generally faster as . The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. With a large number of columns (>255), regular tuples are returned.\n\npandas supports three kinds of sorting: sorting by index labels, sorting by column values, and sorting by a combination of both. The and methods are used to sort a pandas object by its index levels. Sorting by index also supports a parameter that takes a callable function to apply to the index being sorted. For objects, the key is applied per-level to the levels specified by . For information on key sorting by value, see value sorting. The method is used to sort a by its values. The method is used to sort a by its column or row values. The optional parameter to may used to specify one or more columns to use to determine the sorted order. The parameter can take a list of column names, e.g.: These methods have special treatment of NA values via the argument: Sorting also supports a parameter that takes a callable function to apply to the values being sorted. will be given the of values and should return a or array of the same shape with the transformed values. For objects, the key is applied per column, so the key should still expect a Series and return a Series, e.g. The name or type of each column can be used to apply different functions to different columns. Strings passed as the parameter to may refer to either columns or index level names. If a string matches both a column name and an index level name then a warning is issued and the column takes precedence. This will result in an ambiguity error in a future version. Series has the method, which works similarly to . has the and methods which return the smallest or largest \\(n\\) values. For a large this can be much faster than sorting the entire Series and calling on the result. also has the and methods. You must be explicit about sorting when the column is a MultiIndex, and fully specify all levels to .\n\nFor the most part, pandas uses NumPy arrays and dtypes for Series or individual columns of a DataFrame. NumPy provides support for , , , and (note that NumPy does not support timezone-aware datetimes). pandas and third-party libraries extend NumPy’s type system in a few places. This section describes the extensions pandas has made internally. See Extension types for how to write your own extension that works with pandas. See the ecosystem page for a list of third-party libraries that have implemented an extension. The following table lists all of pandas extension types. For methods requiring arguments, strings can be specified as indicated. See the respective documentation sections for more on each type. pandas has two ways to store strings.\n• None dtype, which can hold any Python object, including strings.\n• None , which is dedicated to strings. Generally, we recommend using . See Text data types for more. Finally, arbitrary objects may be stored using the dtype, but should be avoided to the extent possible (for performance and interoperability with other libraries and methods. See object conversion). A convenient attribute for DataFrame returns a Series with the data type of each column. On a object, use the attribute. If a pandas object contains data with multiple dtypes in a single column, the dtype of the column will be chosen to accommodate all of the data types ( is the most general). # these ints are coerced to floats The number of columns of each type in a can be found by calling . Numeric dtypes will propagate and can coexist in DataFrames. If a dtype is passed (either directly via the keyword, a passed , or a passed ), then it will be preserved in DataFrame operations. Furthermore, different numeric dtypes will NOT be combined. The following example will give you a taste. By default integer types are and float types are , regardless of platform (32-bit or 64-bit). The following will all result in dtypes. Note that Numpy will choose platform-dependent types when creating arrays. The following WILL result in on 32-bit platform. Types can potentially be upcasted when combined with other types, meaning they are promoted from the current type (e.g. to ). will return the lower-common-denominator of the dtypes, meaning the dtype that can accommodate ALL of the types in the resulting homogeneous dtyped NumPy array. This can force some upcasting. You can use the method to explicitly convert dtypes from one to another. These will by default return a copy, even if the dtype was unchanged (pass to change this behavior). In addition, they will raise an exception if the astype operation is invalid. Upcasting is always according to the NumPy rules. If two different dtypes are involved in an operation, then the more general one will be used as the result of the operation. Convert a subset of columns to a specified type using . Convert certain columns to a specific dtype by passing a dict to . When trying to convert a subset of columns to a specified type using and , upcasting occurs. tries to fit in what we are assigning to the current dtypes, while will overwrite them taking the dtype from the right hand side. Therefore the following piece of code produces the unintended result. pandas offers various functions to try to force conversion of types from the dtype to other types. In cases where the data is already of the correct type, but stored in an array, the and methods can be used to soft convert to the correct type. Because the data was transposed the original inference stored all columns as object, which will correct. The following functions are available for one dimensional object arrays or scalars to perform hard conversion of objects to a specified type: To force a conversion, we can pass in an argument, which specifies how pandas should deal with elements that cannot be converted to desired dtype or object. By default, , meaning that any errors encountered will be raised during the conversion process. However, if , these errors will be ignored and pandas will convert problematic elements to (for datetime and timedelta) or (for numeric). This might be useful if you are reading in data which is mostly of the desired dtype (e.g. numeric, datetime), but occasionally has non-conforming elements intermixed that you want to represent as missing: In addition to object conversion, provides another argument , which gives the option of downcasting the newly (or already) numeric data to a smaller dtype, which can conserve memory: As these methods apply only to one-dimensional arrays, lists or scalars; they cannot be used directly on multi-dimensional objects such as DataFrames. However, with , we can “apply” the function over each column efficiently: Performing selection operations on type data can easily upcast the data to . The dtype of the input data will be preserved in cases where are not introduced. See also Support for integer NA."
    },
    {
        "link": "https://tensorflow.org/tutorials/structured_data/time_series",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nThis tutorial is an introduction to time series forecasting using TensorFlow. It builds a few different styles of models including Convolutional and Recurrent Neural Networks (CNNs and RNNs).\n\nThis is covered in two main parts, with subsections:\n• Forecast multiple steps:\n• Single-shot: Make the predictions all at once.\n• Autoregressive: Make one prediction at a time and feed the output back to the model.\n\nThis tutorial uses a weather time series dataset recorded by the Max Planck Institute for Biogeochemistry.\n\nThis dataset contains 14 different features such as air temperature, atmospheric pressure, and humidity. These were collected every 10 minutes, beginning in 2003. For efficiency, you will use only the data collected between 2009 and 2016. This section of the dataset was prepared by François Chollet for his book Deep Learning with Python.\n\nThis tutorial will just deal with hourly predictions, so start by sub-sampling the data from 10-minute intervals to one-hour intervals:\n\nLet's take a glance at the data. Here are the first few rows:\n\nHere is the evolution of a few features over time:\n\nNext, look at the statistics of the dataset:\n\nOne thing that should stand out is the value of the wind velocity ( ) and the maximum value ( ) columns. This is likely erroneous.\n\nThere's a separate wind direction column, so the velocity should be greater than zero ( ). Replace it with zeros:\n\nBefore diving in to build a model, it's important to understand your data and be sure that you're passing the model appropriately formatted data.\n\nThe last column of the data, —gives the wind direction in units of degrees. Angles do not make good model inputs: 360° and 0° should be close to each other and wrap around smoothly. Direction shouldn't matter if the wind is not blowing.\n\nRight now the distribution of wind data looks like this:\n\nBut this will be easier for the model to interpret if you convert the wind direction and velocity columns to a wind vector:\n\nThe distribution of wind vectors is much simpler for the model to correctly interpret:\n\nSimilarly, the column is very useful, but not in this string form. Start by converting it to seconds:\n\nSimilar to the wind direction, the time in seconds is not a useful model input. Being weather data, it has clear daily and yearly periodicity. There are many ways you could deal with periodicity.\n\nYou can get usable signals by using sine and cosine transforms to clear \"Time of day\" and \"Time of year\" signals:\n\nThis gives the model access to the most important frequency features. In this case you knew ahead of time which frequencies were important.\n\nIf you don't have that information, you can determine which frequencies are important by extracting features with Fast Fourier Transform. To check the assumptions, here is the of the temperature over time. Note the obvious peaks at frequencies near and :\n\nYou'll use a split for the training, validation, and test sets. Note the data is not being randomly shuffled before splitting. This is for two reasons:\n• It ensures that chopping the data into windows of consecutive samples is still possible.\n• It ensures that the validation/test results are more realistic, being evaluated on the data collected after the model was trained.\n\nIt is important to scale features before training a neural network. Normalization is a common way of doing this scaling: subtract the mean and divide by the standard deviation of each feature.\n\nThe mean and standard deviation should only be computed using the training data so that the models have no access to the values in the validation and test sets.\n\nIt's also arguable that the model shouldn't have access to future values in the training set when training, and that this normalization should be done using moving averages. That's not the focus of this tutorial, and the validation and test sets ensure that you get (somewhat) honest metrics. So, in the interest of simplicity this tutorial uses a simple average.\n\nNow, peek at the distribution of the features. Some features do have long tails, but there are no obvious errors like the wind velocity value.\n\nThe models in this tutorial will make a set of predictions based on a window of consecutive samples from the data.\n\nThe main features of the input windows are:\n• The width (number of time steps) of the input and label windows.\n• The time offset between them.\n• Which features are used as inputs, labels, or both.\n\nThis tutorial builds a variety of models (including Linear, DNN, CNN and RNN models), and uses them for both:\n\nThis section focuses on implementing the data windowing so that it can be reused for all of those models.\n\nDepending on the task and type of model you may want to generate a variety of data windows. Here are some examples:\n• None For example, to make a single prediction 24 hours into the future, given 24 hours of history, you might define a window like this:\n• None A model that makes a prediction one hour into the future, given six hours of history, would need a window like this:\n\nThe rest of this section defines a class. This class can:\n• Handle the indexes and offsets as shown in the diagrams above.\n• Plot the content of the resulting windows.\n• Efficiently generate batches of these windows from the training, evaluation, and test data, using s.\n\nStart by creating the class. The method includes all the necessary logic for the input and label indices.\n\nIt also takes the training, evaluation, and test DataFrames as input. These will be converted to s of windows later.\n\nHere is code to create the 2 windows shown in the diagrams at the start of this section:\n\nGiven a list of consecutive inputs, the method will convert them to a window of inputs and a window of labels.\n\nThe example you define earlier will be split like this:\n\nThis diagram doesn't show the axis of the data, but this function also handles the so it can be used for both the single output and multi-output examples.\n\nTry it out:\n\nTypically, data in TensorFlow is packed into arrays where the outermost index is across examples (the \"batch\" dimension). The middle indices are the \"time\" or \"space\" (width, height) dimension(s). The innermost indices are the features.\n\nThe code above took a batch of three 7-time step windows with 19 features at each time step. It splits them into a batch of 6-time step 19-feature inputs, and a 1-time step 1-feature label. The label only has one feature because the was initialized with . Initially, this tutorial will build models that predict single output labels.\n\nHere is a plot method that allows a simple visualization of the split window:\n\nThis plot aligns inputs, labels, and (later) predictions based on the time that the item refers to:\n\nYou can plot the other columns, but the example window configuration only has labels for the column.\n\nFinally, this method will take a time series DataFrame and convert it to a of pairs using the function:\n\nAdd properties for accessing them as s using the method you defined earlier. Also, add a standard example batch for easy access and plotting:\n\nNow, the object gives you access to the objects, so you can easily iterate over the data.\n\nThe property tells you the structure, data types, and shapes of the dataset elements.\n\nThe simplest model you can build on this sort of data is one that predicts a single feature's value—1 time step (one hour) into the future based only on the current conditions.\n\nSo, start by building models to predict the value one hour into the future.\n\nThe object creates s from the training, validation, and test sets, allowing you to easily iterate over batches of data.\n\nBefore building a trainable model it would be good to have a performance baseline as a point for comparison with the later more complicated models.\n\nThis first task is to predict temperature one hour into the future, given the current value of all features. The current values include the current temperature.\n\nSo, start with a model that just returns the current temperature as the prediction, predicting \"No change\". This is a reasonable baseline since temperature changes slowly. Of course, this baseline will work less well if you make a prediction further in the future.\n\nThat printed some performance metrics, but those don't give you a feeling for how well the model is doing.\n\nThe has a plot method, but the plots won't be very interesting with only a single sample.\n\nSo, create a wider that generates windows 24 hours of consecutive inputs and labels at a time. The new variable doesn't change the way the model operates. The model still makes predictions one hour into the future based on a single input time step. Here, the axis acts like the axis: each prediction is made independently with no interaction between time steps:\n\nThis expanded window can be passed directly to the same model without any code changes. This is possible because the inputs and labels have the same number of time steps, and the baseline just forwards the input to the output:\n\nBy plotting the baseline model's predictions, notice that it is simply the labels shifted right by one hour:\n\nIn the above plots of three examples the single step model is run over the course of 24 hours. This deserves some explanation:\n• The blue line shows the input temperature at each time step. The model receives all features, this plot only shows the temperature.\n• The green dots show the target prediction value. These dots are shown at the prediction time, not the input time. That is why the range of labels is shifted 1 step relative to the inputs.\n• The orange crosses are the model's prediction's for each output time step. If the model were predicting perfectly the predictions would land directly on the .\n\nThe simplest trainable model you can apply to this task is to insert linear transformation between the input and output. In this case the output from a time step only depends on that step:\n\nA layer with no set is a linear model. The layer only transforms the last axis of the data from to ; it is applied independently to every item across the and axes.\n\nThis tutorial trains many models, so package the training procedure into a function:\n\nTrain the model and evaluate its performance:\n\nLike the model, the linear model can be called on batches of wide windows. Used this way the model makes a set of independent predictions on consecutive time steps. The axis acts like another axis. There are no interactions between the predictions at each time step.\n\nHere is the plot of its example predictions on the , note how in many cases the prediction is clearly better than just returning the input temperature, but in a few cases it's worse:\n\nOne advantage to linear models is that they're relatively simple to interpret. You can pull out the layer's weights and visualize the weight assigned to each input:\n\nSometimes the model doesn't even place the most weight on the input . This is one of the risks of random initialization.\n\nBefore applying models that actually operate on multiple time-steps, it's worth checking the performance of deeper, more powerful, single input step models.\n\nHere's a model similar to the model, except it stacks several a few layers between the input and the output:\n\nA single-time-step model has no context for the current values of its inputs. It can't see how the input features are changing over time. To address this issue the model needs access to multiple time steps when making predictions:\n\nThe , and models handled each time step independently. Here the model will take multiple time steps as input to produce a single output.\n\nCreate a that will produce batches of three-hour inputs and one-hour labels:\n\nNote that the 's parameter is relative to the end of the two windows.\n\nYou could train a model on a multiple-input-step window by adding a as the first layer of the model:\n\nThe main down-side of this approach is that the resulting model can only be executed on input windows of exactly this shape.\n\nThe convolutional models in the next section fix this problem.\n\nA convolution layer ( ) also takes multiple time steps as input to each prediction.\n\nBelow is the same model as , re-written with a convolution.\n• The and the first are replaced by a .\n• The is no longer necessary since the convolution keeps the time axis in its output.\n\nRun it on an example batch to check that the model produces outputs with the expected shape:\n\nTrain and evaluate it on the and it should give performance similar to the model.\n\nThe difference between this and the model is that the can be run on inputs of any length. The convolutional layer is applied to a sliding window of inputs:\n\nIf you run it on wider input, it produces wider output:\n\nNote that the output is shorter than the input. To make training or plotting work, you need the labels, and prediction to have the same length. So build a to produce wide windows with a few extra input time steps so the label and prediction lengths match:\n\nNow, you can plot the model's predictions on a wider window. Note the 3 input time steps before the first prediction. Every prediction here is based on the 3 preceding time steps:\n\nA Recurrent Neural Network (RNN) is a type of neural network well-suited to time series data. RNNs process a time series step-by-step, maintaining an internal state from time-step to time-step.\n\nYou can learn more in the Text generation with an RNN tutorial and the Recurrent Neural Networks (RNN) with Keras guide.\n\nIn this tutorial, you will use an RNN layer called Long Short-Term Memory ( ).\n\nAn important constructor argument for all Keras RNN layers, such as , is the argument. This setting can configure the layer in one of two ways:\n• If , the default, the layer only returns the output of the final time step, giving the model time to warm up its internal state before making a single prediction:\n• If , the layer returns an output for each input. This is useful for:\n\nWith , the model can be trained on 24 hours of data at a time.\n\nWith this dataset typically each of the models does slightly better than the one before it:\n\nThe models so far all predicted a single output feature, , for a single time step.\n\nAll of these models can be converted to predict multiple features just by changing the number of units in the output layer and adjusting the training windows to include all features in the ( ):\n\nNote above that the axis of the labels now has the same depth as the inputs, instead of .\n\nThe same baseline model ( ) can be used here, but this time repeating all features instead of selecting a specific :\n\nThe model from earlier took advantage of the fact that the sequence doesn't change drastically from time step to time step. Every model trained in this tutorial so far was randomly initialized, and then had to learn that the output is a a small change from the previous time step.\n\nWhile you can get around this issue with careful initialization, it's simpler to build this into the model structure.\n\nIt's common in time series analysis to build models that instead of predicting the next value, predict how the value will change in the next time step. Similarly, residual networks—or ResNets—in deep learning refer to architectures where each layer adds to the model's accumulating result.\n\nThat is how you take advantage of the knowledge that the change should be small.\n\nEssentially, this initializes the model to match the . For this task it helps models converge faster, with slightly better performance.\n\nThis approach can be used in conjunction with any model discussed in this tutorial.\n\nHere, it is being applied to the LSTM model, note the use of the to ensure that the initial predicted changes are small, and don't overpower the residual connection. There are no symmetry-breaking concerns for the gradients here, since the are only used on the last layer.\n\nHere is the overall performance for these multi-output models.\n\nThe above performances are averaged across all model outputs.\n\nBoth the single-output and multiple-output models in the previous sections made single time step predictions, one hour into the future.\n\nThis section looks at how to expand these models to make multiple time step predictions.\n\nIn a multi-step prediction, the model needs to learn to predict a range of future values. Thus, unlike a single step model, where only a single future point is predicted, a multi-step model predicts a sequence of the future values.\n\nThere are two rough approaches to this:\n• Single shot predictions where the entire time series is predicted at once.\n• Autoregressive predictions where the model only makes single step predictions and its output is fed back as its input.\n\nIn this section all the models will predict all the features across all output time steps.\n\nFor the multi-step model, the training data again consists of hourly samples. However, here, the models will learn to predict 24 hours into the future, given 24 hours of the past.\n\nHere is a object that generates these slices from the dataset:\n\nA simple baseline for this task is to repeat the last input time step for the required number of output time steps:\n\nSince this task is to predict 24 hours into the future, given 24 hours of the past, another simple approach is to repeat the previous day, assuming tomorrow will be similar:\n\nOne high-level approach to this problem is to use a \"single-shot\" model, where the model makes the entire sequence prediction in a single step.\n\nThis can be implemented efficiently as a with output units. The model just needs to reshape that output to the required .\n\nA simple linear model based on the last input time step does better than either baseline, but is underpowered. The model needs to predict time steps, from a single input time step with a linear projection. It can only capture a low-dimensional slice of the behavior, likely based mainly on the time of day and time of year.\n\nAdding a between the input and output gives the linear model more power, but is still only based on a single input time step.\n\nA convolutional model makes predictions based on a fixed-width history, which may lead to better performance than the dense model since it can see how things are changing over time:\n\nA recurrent model can learn to use a long history of inputs, if it's relevant to the predictions the model is making. Here the model will accumulate internal state for 24 hours, before making a single prediction for the next 24 hours.\n\nIn this single-shot format, the LSTM only needs to produce an output at the last time step, so set in .\n\nThe above models all predict the entire output sequence in a single step.\n\nIn some cases it may be helpful for the model to decompose this prediction into individual time steps. Then, each model's output can be fed back into itself at each step and predictions can be made conditioned on the previous one, like in the classic Generating Sequences With Recurrent Neural Networks.\n\nOne clear advantage to this style of model is that it can be set up to produce output with a varying length.\n\nYou could take any of the single-step multi-output models trained in the first half of this tutorial and run in an autoregressive feedback loop, but here you'll focus on building a model that's been explicitly trained to do that.\n\nThis tutorial only builds an autoregressive RNN model, but this pattern could be applied to any model that was designed to output a single time step.\n\nThe model will have the same basic form as the single-step LSTM models from earlier: a layer followed by a layer that converts the layer's outputs to model predictions.\n\nA is a wrapped in the higher level that manages the state and sequence results for you (Check out the Recurrent Neural Networks (RNN) with Keras guide for details).\n\nIn this case, the model has to manually manage the inputs for each step, so it uses directly for the lower level, single time step interface.\n\nThe first method this model needs is a method to initialize its internal state based on the inputs. Once trained, this state will capture the relevant parts of the input history. This is equivalent to the single-step model from earlier:\n\nThis method returns a single time-step prediction and the internal state of the :\n\nWith the 's state, and an initial prediction you can now continue iterating the model feeding the predictions at each step back as the input.\n\nThe simplest approach for collecting the output predictions is to use a Python list and a after the loop.\n\nTest run this model on the example inputs:\n\nThere are clearly diminishing returns as a function of model complexity on this problem:\n\nThe metrics for the multi-output models in the first half of this tutorial show the performance averaged across all output features. These performances are similar but also averaged across output time steps.\n\nThe gains achieved going from a dense model to convolutional and recurrent models are only a few percent (if any), and the autoregressive model performed clearly worse. So these more complex approaches may not be worth while on this problem, but there was no way to know without trying, and these models could be helpful for your problem.\n\nThis tutorial was a quick introduction to time series forecasting using TensorFlow.\n\nTo learn more, refer to:\n• Chapter 15 of Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition.\n• Lesson 8 of Udacity's intro to TensorFlow for deep learning, including the exercise notebooks.\n\nAlso, remember that you can implement any classical time series model in TensorFlow—this tutorial just focuses on TensorFlow's built-in functionality."
    },
    {
        "link": "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras",
        "document": "Unlike regression predictive modeling, time series also adds the complexity of a sequence dependence among the input variables.\n\nA powerful type of neural network designed to handle sequence dependence is called a recurrent neural network. The Long Short-Term Memory network or LSTM network is a type of recurrent neural network used in deep learning because very large architectures can be successfully trained.\n\nIn this post, you will discover how to develop LSTM networks in Python using the Keras deep learning library to address a demonstration time-series prediction problem.\n\nAfter completing this tutorial, you will know how to implement and develop LSTM networks for your own time series prediction problems and other more general sequence problems. You will know:\n• How to develop LSTM networks for regression, window, and time-step-based framing of time series prediction problems\n• How to develop and make predictions using LSTM networks that maintain state (memory) across very long sequences\n\nIn this tutorial, we will develop a number of LSTMs for a standard time series prediction problem. The problem and the chosen configuration for the LSTM networks are for demonstration purposes only; they are not optimized.\n\nThese examples will show exactly how you can develop your own differently structured LSTM networks for time series predictive modeling problems.\n\nKick-start your project with my new book Deep Learning for Time Series Forecasting, including step-by-step tutorials and the Python source code files for all examples.\n• Update Oct/2016: There was an error in how RMSE was calculated in each example. Reported RMSEs were just plain wrong. Now, RMSE is calculated directly from predictions, and both RMSE and graphs of predictions are in the units of the original dataset. Models were evaluated using Keras 1.1.0, TensorFlow 0.10.0, and scikit-learn v0.18. Thanks to all those that pointed out the issue and to Philip O’Brien for helping to point out the fix.\n• Update Mar/2017: Updated example for Keras 2.0.2, TensorFlow 1.0.1 and Theano 0.9.0\n• Update Apr/2017: For a more complete and better-explained tutorial of LSTMs for time series forecasting, see the post Time Series Forecasting with the Long Short-Term Memory Network in Python\n\nThe example in this post is quite dated. You can view some better examples using LSTMs on time series with:\n\nThe problem you will look at in this post is the International Airline Passengers prediction problem.\n\nThis is a problem where, given a year and a month, the task is to predict the number of international airline passengers in units of 1,000. The data ranges from January 1949 to December 1960, or 12 years, with 144 observations.\n\nBelow is a sample of the first few lines of the file.\n\nYou can load this dataset easily using the Pandas library. You are not interested in the date, given that each observation is separated by the same interval of one month. Therefore, when you load the dataset, you can exclude the first column.\n\nOnce loaded, you can easily plot the whole dataset. The code to load and plot the dataset is listed below.\n\nYou can see an upward trend in the dataset over time.\n\nYou can also see some periodicity in the dataset that probably corresponds to the Northern Hemisphere vacation period.\n\nLet’s keep things simple and work with the data as-is.\n\nNormally, it is a good idea to investigate various data preparation techniques to rescale the data and make it stationary.\n\nThe Long Short-Term Memory network, or LSTM network, is a recurrent neural network trained using Backpropagation Through Time that overcomes the vanishing gradient problem.\n\nAs such, it can be used to create large recurrent networks that, in turn, can be used to address difficult sequence problems in machine learning and achieve state-of-the-art results.\n\nInstead of neurons, LSTM networks have memory blocks connected through layers.\n\nA block has components that make it smarter than a classical neuron and a memory for recent sequences. A block contains gates that manage the block’s state and output. A block operates upon an input sequence, and each gate within a block uses the sigmoid activation units to control whether it is triggered or not, making the change of state and addition of information flowing through the block conditional.\n\nThere are three types of gates within a unit:\n• Forget Gate: conditionally decides what information to throw away from the block\n• Input Gate: conditionally decides which values from the input to update the memory state\n• Output Gate: conditionally decides what to output based on input and the memory of the block\n\nEach unit is like a mini-state machine where the gates of the units have weights that are learned during the training procedure.\n\nYou can see how you may achieve sophisticated learning and memory from a layer of LSTMs, and it is not hard to imagine how higher-order abstractions may be layered with multiple such layers.\n\nYou can phrase the problem as a regression problem.\n\nThat is, given the number of passengers (in units of thousands) this month, what is the number of passengers next month?\n\nYou can write a simple function to convert the single column of data into a two-column dataset: the first column containing this month’s (t) passenger count and the second column containing next month’s (t+1) passenger count to be predicted.\n\nBefore you start, let’s first import all the functions and classes you will use. This assumes a working SciPy environment with the Keras deep learning library installed.\n\nBefore you do anything, it is a good idea to fix the random number seed to ensure your results are reproducible.\n\nYou can also use the code from the previous section to load the dataset as a Pandas dataframe. You can then extract the NumPy array from the dataframe and convert the integer values to floating point values, which are more suitable for modeling with a neural network.\n\nLSTMs are sensitive to the scale of the input data, specifically when the sigmoid (default) or tanh activation functions are used. It can be a good practice to rescale the data to the range of 0-to-1, also called normalizing. You can easily normalize the dataset using the MinMaxScaler preprocessing class from the scikit-learn library.\n\nAfter you model the data and estimate the skill of your model on the training dataset, you need to get an idea of the skill of the model on new unseen data. For a normal classification or regression problem, you would do this using cross validation.\n\nWith time series data, the sequence of values is important. A simple method that you can use is to split the ordered dataset into train and test datasets. The code below calculates the index of the split point and separates the data into the training datasets, with 67% of the observations used to train the model, leaving the remaining 33% for testing the model.\n\nNow, you can define a function to create a new dataset, as described above.\n\nThe function takes two arguments: the dataset, which is a NumPy array you want to convert into a dataset, and the look_back, which is the number of previous time steps to use as input variables to predict the next time period—in this case, defaulted to 1.\n\nThis default will create a dataset where X is the number of passengers at a given time (t), and Y is the number of passengers at the next time (t + 1).\n\nIt can be configured by constructing a differently shaped dataset in the next section.\n\nLet’s take a look at the effect of this function on the first rows of the dataset (shown in the unnormalized form for clarity).\n\nIf you compare these first five rows to the original dataset sample listed in the previous section, you can see the X=t and Y=t+1 pattern in the numbers.\n\nLet’s use this function to prepare the train and test datasets for modeling.\n\nThe LSTM network expects the input data (X) to be provided with a specific array structure in the form of [samples, time steps, features].\n\nCurrently, the data is in the form of [samples, features], and you are framing the problem as one time step for each sample. You can transform the prepared train and test input data into the expected structure using numpy.reshape() as follows:\n\nYou are now ready to design and fit your LSTM network for this problem.\n\nThe network has a visible layer with 1 input, a hidden layer with 4 LSTM blocks or neurons, and an output layer that makes a single value prediction. The default sigmoid activation function is used for the LSTM blocks. The network is trained for 100 epochs, and a batch size of 1 is used.\n\nOnce the model is fit, you can estimate the performance of the model on the train and test datasets. This will give you a point of comparison for new models.\n\nNote that you will invert the predictions before calculating error scores to ensure that performance is reported in the same units as the original data (thousands of passengers per month).\n\nFinally, you can generate predictions using the model for both the train and test dataset to get a visual indication of the skill of the model.\n\nBecause of how the dataset was prepared, you must shift the predictions so that they align on the x-axis with the original dataset. Once prepared, the data is plotted, showing the original dataset in blue, the predictions for the training dataset in green, and the predictions on the unseen test dataset in red.\n\nYou can see that the model did an excellent job of fitting both the training and the test datasets.\n\nFor completeness, below is the entire code example.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example produces the following output.\n\nYou can see that the model has an average error of about 23 passengers (in thousands) on the training dataset and about 49 passengers (in thousands) on the test dataset. Not that bad.\n\nLSTM for Regression Using the Window Method\n\nYou can also phrase the problem so that multiple, recent time steps can be used to make the prediction for the next time step.\n\nThis is called a window, and the size of the window is a parameter that can be tuned for each problem.\n\nFor example, given the current time (t) to predict the value at the next time in the sequence (t+1), you can use the current time (t), as well as the two prior times (t-1 and t-2) as input variables.\n\nWhen phrased as a regression problem, the input variables are t-2, t-1, and t, and the output variable is t+1.\n\nThe create_dataset() function created in the previous section allows you to create this formulation of the time series problem by increasing the look_back argument from 1 to 3.\n\nA sample of the dataset with this formulation is as follows:\n\nYou can re-run the example in the previous section with the larger window size. The whole code listing with just the window size change is listed below for completeness.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example provides the following output:\n\nYou can see that the error was increased slightly compared to that of the previous section. The window size and the network architecture were not tuned: This is just a demonstration of how to frame a prediction problem.\n\nYou may have noticed that the data preparation for the LSTM network includes time steps.\n\nSome sequence problems may have a varied number of time steps per sample. For example, you may have measurements of a physical machine leading up to the point of failure or a point of surge. Each incident would be a sample of observations that lead up to the event, which would be the time steps, and the variables observed would be the features.\n\nTime steps provide another way to phrase your time series problem. Like above in the window example, you can take prior time steps in your time series as inputs to predict the output at the next time step.\n\nInstead of phrasing the past observations as separate input features, you can use them as time steps of the one input feature, which is indeed a more accurate framing of the problem.\n\nYou can do this using the same data representation as in the previous window-based example, except when you reshape the data, you set the columns to be the time steps dimension and change the features dimension back to 1. For example:\n\nThe entire code listing is provided below for completeness.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example provides the following output:\n\nYou can see that the results are slightly better than the previous example, although the structure of the input data makes a lot more sense.\n\nThe LSTM network has memory capable of remembering across long sequences.\n\nNormally, the state within the network is reset after each training batch when fitting the model, as well as each call to model.predict() or model.evaluate().\n\nYou can gain finer control over when the internal state of the LSTM network is cleared in Keras by making the LSTM layer “stateful.” This means it can build a state over the entire training sequence and even maintain that state if needed to make predictions.\n\nIt requires that the training data not be shuffled when fitting the network. It also requires explicit resetting of the network state after each exposure to the training data (epoch) by calls to model.reset_states(). This means that you must create your own outer loop of epochs and within each epoch call model.fit() and model.reset_states(). For example:\n\nFinally, when the LSTM layer is constructed, the stateful parameter must be set to True. Instead of specifying the input dimensions, you must hard code the number of samples in a batch, the number of time steps in a sample, and the number of features in a time step by setting the batch_input_shape parameter. For example:\n\nThis same batch size must then be used later when evaluating the model and making predictions. For example:\n\nYou can adapt the previous time step example to use a stateful LSTM. The full code listing is provided below.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example provides the following output:\n\nYou do see that results are better than some, worse than others. The model may need more modules and may need to be trained for more epochs to internalize the structure of the problem.\n\nFinally, let’s take a look at one of the big benefits of LSTMs: the fact that they can be successfully trained when stacked into deep network architectures.\n\nLSTM networks can be stacked in Keras in the same way that other layer types can be stacked. One addition to the configuration that is required is that an LSTM layer prior to each subsequent LSTM layer must return the sequence. This can be done by setting the return_sequences parameter on the layer to True.\n\nYou can extend the stateful LSTM in the previous section to have two layers, as follows:\n\nThe entire code listing is provided below for completeness.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example produces the following output.\n\nThe predictions on the test dataset are again worse. This is more evidence to suggest the need for additional training epochs.\n\nIn this post, you discovered how to develop LSTM recurrent neural networks for time series prediction in Python with the Keras deep learning network.\n• How to create an LSTM for a regression and a window formulation of the time series problem\n• How to create an LSTM with a time step formulation of the time series problem\n• How to create an LSTM with state and stacked LSTMs with state to learn long sequences\n\nDo you have any questions about LSTMs for time series prediction or about this post?\n\n Ask your questions in the comments below, and I will do my best to answer.\n\nThe example in this post is quite dated. See these better examples available for using LSTMs on time series:"
    },
    {
        "link": "https://justintodata.com/forecast-time-series-lstm-with-tensorflow-keras",
        "document": "In this tutorial, we present a deep learning time series analysis example with Python. You’ll see:\n• How to preprocess/transform the dataset for time series forecasting.\n• How to handle large time series datasets when we have limited computer memory.\n• How to fit Long Short-Term Memory (LSTM) with TensorFlow Keras neural networks model.\n\nIf you want to analyze large time series dataset with machine learning techniques, you’ll love this guide with practical tips.\n\nThe dataset we are using is the Household Electric Power Consumption from Kaggle. It provides measurements of electric power consumption in one household with a one-minute sampling rate.\n\nThere are 2,075,259 measurements gathered within 4 years. Different electrical quantities and some sub-metering values are available. But we’ll only focus on three features:\n\nIn this project, we will predict the amount of Global_active_power 10 minutes ahead.\n\nTo begin, let’s process the dataset to get ready for time series analysis.\n\nWe transform the dataset df by:\n• creating feature date_time in DateTime format by combining Date and Time.\n• ordering the features by time in the new dataset.\n\nNow we have a dataset df as below.\n\nNext, we split the dataset into training, validation, and test datasets.\n\ndf_test holds the data within the last 7 days in the original dataset. df_val has data 14 days before the test dataset. df_train has the rest of the data.\n\nRelated article: Time Series Analysis, Visualization & Forecasting with LSTM\n\nThis article forecasted the Global_active_power only 1 minute ahead of historical data. \n\nBut practically, we want to forecast over a more extended period, which we’ll do in this article.\n\nBefore we can fit the TensorFlow Keras LSTM, there are still other processes that need to be done.\n\nLet’s deal with them little by little!\n\nAs mentioned earlier, we want to forecast the Global_active_power that’s 10 minutes in the future.\n\nThe graph below visualizes the problem: using the lagged data (from t-n to t-1) to predict the target (t+10).\n\nIt is not efficient to loop through the dataset while training the model. So we want to transform the dataset with each row representing the historical data and the target.\n\nIn this way, we only need to train the model using each row of the above matrix.\n\nNow here comes the challenges:\n• How do we convert the dataset to the new structure?\n• How do we handle this larger new data structure when our computer memory is limited?\n\nAs a result, the function create_ts_files is defined:\n• to convert the original dataset to the new dataset above.\n• at the same time, to divide the new dataset into smaller files, which is easier to process.\n\nWithin this function, we define the following parameters:\n• start_index: the earliest time to be included in all the historical data for forecasting. \n\nIn this practice, we want to include history from the very beginning, so we set the default of it to be 0.\n• end_index: the latest time to be included in all the historical data for forecasting.\n\nIn this practice, we want to include all the history, so we set the default of it to be None.\n• history_length: this is n mentioned earlier, which is the number of timesteps to look back for each forecasting.\n• step_size: the stride of the history window. \n\nGlobal_active_power doesn’t change fast throughout time. So to be more efficient, we can let step_size = 10. In this way, we downsample to use every 10 minutes of data in the past to predict the future amount. We are only looking at t-1, t-11, t-21 until t-n to predict t+10.\n• target_step: the number of periods in the future to predict.\n\nAs mentioned earlier, we are trying to predict the global_active_power 10 minutes ahead. So this feature = 10.\n• num_rows_per_file: the number of records to put in each file.\n\nThis is necessary to divide the large new dataset into smaller files.\n• data_folder: the one single folder that will contain all the files.\n\nIn the end, just know that this function creates a folder with files. \n\nAnd each file contains a pandas dataframe that looks like the new dataset in the chart above. \n\nEach of these dataframes has columns:\n• y, which is the target to predict. This will be the value at t + target_step (t + 10).\n• x_lag{i}, the value at time t + target_step – i (t + 10 – 11, t + 10 – 21, and so on), i.e., the lagged value compared to y.\n\nAt the same time, the function also returns the number of lags (len(col_names)-1) in the dataframes. This number will be required when defining the shape for TensorFlow models later.\n\nBefore applying the function create_ts_files, we also need to:\n• scale the global_active_power to work with Neural Networks.\n• define step_size within historical data to be 10 minutes.\n• set the target_step to be 10, so that we are forecasting the global_active_power 10 minutes after the historical data.\n\nAfter these, we apply the create_ts_files to:\n• create 158 files (each including a pandas dataframe) within the folder ts_data.\n• return num_timesteps as the number of lags.\n\nAs the function runs, it prints the name of every 10 files.\n\nThe folder ts_data is around 16 GB, and we were only using the past 7 days of data to predict. Now you can see why it’s necessary to divide the dataset into smaller dataframes!\n\nIn this procedure, we create a class TimeSeriesLoader to transform and feed the dataframes into the model.\n\nThere are built-in functions from Keras such as Keras Sequence, tf.data API. But they are not very efficient for this purpose.\n\nWithin this class, we define:\n• __init__: the initial settings of the object, including: \n\n– ts_folder, which will be ts_data that we just created.\n\n– filename_format, which is the string format of the file names in the ts_folder. \n\nFor example, when the files are ts_file0.pkl, ts_file1.pkl, …, ts_file100.pkl, the format would be ‘ts_file{}.pkl’.\n• get_chunk: this method takes the dataframe from one of the files, processes it to be ready for training.\n• shuffle_chunks: this method shuffles the order of the chunks that are returned in get_chunk. This is a good practice for modeling.\n\nThe definitions might seem a little confusing. But keep reading, you’ll see this object in action within the next step.\n\nAfter defining, we apply this TimeSeriesLoader to the ts_data folder.\n\nNow with the object tss points to our dataset, we are finally ready for LSTM!\n\nAs mentioned before, we are going to build an LSTM model based on the TensorFlow Keras library.\n\nWe all know the importance of hyperparameter tuning based on our guide. But in this article, we are simply demonstrating the model fitting without tuning.\n\nThe procedures are below:\n• define the shape of the input dataset:\n\n– num_timesteps, the number of lags in the dataframes we set in Step #2.\n\n– the number of time series as 1. Since we are only using one feature of global_active_power.\n• define the number of units, 4*units*(units+2) is the number of parameters of the LSTM. \n\nThe higher the number, the more parameters in the model.\n• define the dropout rate, which is used to prevent overfitting.\n• specify the output layer to have a linear activation function.\n\nThen we also define the optimization function and the loss function. Again, tuning these hyperparameters to find the best option would be a better practice.\n\nTo take a look at the model we just defined before running, we can print out the summary.\n\nYou can see that the output shape looks good, which is n / step_size (7*24*60 / 10 = 1008). The number of parameters that need to be trained looks right as well (4*units*(units+2) = 480).\n\nWe train each chunk in batches, and only run for one epoch. Ideally, you would train for multiple epochs for neural networks.\n\nAfter fitting the model, we may also evaluate the model performance using the validation dataset.\n\nSame as the training dataset, we also create a folder of the validation data, which prepares the validation dataset for model fitting.\n\nBesides testing using the validation dataset, we also test against a baseline model using only the most recent history point (t + 10 – 11).\n\nThe detailed Python code is below.\n\nThe validation dataset using LSTM gives Mean Squared Error (MSE) of 0.418. While the baseline model has MSE of 0.428. The LSTM does slightly better than the baseline.\n\nWe could do better with hyperparameter tuning and more epochs. Plus, some other essential time series analysis tips such as seasonality would help too.\n\nThank you for reading!\n\nHope you found something useful in this guide. Leave a comment if you have any questions.\n\nBefore you leave, don’t forget to sign up for the Just into Data newsletter! Or connect with us on Twitter, Facebook.\n\nSo you won’t miss any new data science articles from us!"
    },
    {
        "link": "https://medium.com/data-science/time-series-forecasting-with-lstms-using-tensorflow-2-and-keras-in-python-6ceee9c6c651",
        "document": "Want to learn how to use Multivariate Time Series data? Read in the next part:\n\nOften you might have to deal with data that does have a time component. No matter how much you squint your eyes, it will be difficult to make your favorite data independence assumption. It seems like newer values in your data might depend on the historical values. How can you use that kind of data to build models?\n\nThis guide will help you better understand Time Series data and how to build models using Deep Learning (Recurrent Neural Networks). You’ll learn how to preprocess Time Series, build a simple LSTM model, train it, and use it to make predictions. Here are the steps:"
    },
    {
        "link": "https://kaggle.com/code/iamleonie/time-series-tips-tricks-for-training-lstms",
        "document": ""
    },
    {
        "link": "https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras",
        "document": "It can be difficult to understand how to prepare your sequence data for input to an LSTM model.\n\nOften there is confusion around how to define the input layer for the LSTM model.\n\nThere is also confusion about how to convert your sequence data that may be a 1D or 2D matrix of numbers to the required 3D format of the LSTM input layer.\n\nIn this tutorial, you will discover how to define the input layer to LSTM models and how to reshape your loaded input data for LSTM models.\n\nAfter completing this tutorial, you will know:\n• How to define an LSTM input layer.\n• How to reshape a one-dimensional sequence data for an LSTM model and define the input layer.\n• How to reshape multiple parallel series data for an LSTM model and define the input layer.\n\nKick-start your project with my new book Long Short-Term Memory Networks With Python, including step-by-step tutorials and the Python source code files for all examples.\n\nThis tutorial is divided into 4 parts; they are:\n• Example of LSTM with Single Input Sample\n• Example of LSTM with Multiple Input Features\n\nThe LSTM input layer is specified by the “input_shape” argument on the first hidden layer of the network.\n\nThis can make things confusing for beginners.\n\nFor example, below is an example of a network with one hidden LSTM layer and one Dense output layer.\n\nIn this example, the LSTM() layer must specify the shape of the input.\n\nThe input to every LSTM layer must be three-dimensional.\n\nThe three dimensions of this input are:\n• Samples. One sequence is one sample. A batch is comprised of one or more samples.\n• Time Steps. One time step is one point of observation in the sample.\n• Features. One feature is one observation at a time step.\n\nThis means that the input layer expects a 3D array of data when fitting the model and when making predictions, even if specific dimensions of the array contain a single value, e.g. one sample or one feature.\n\nWhen defining the input layer of your LSTM network, the network assumes you have 1 or more samples and requires that you specify the number of time steps and the number of features. You can do this by specifying a tuple to the “input_shape” argument.\n\nFor example, the model below defines an input layer that expects 1 or more samples, 50 time steps, and 2 features.\n\nNow that we know how to define an LSTM input layer and the expectations of 3D inputs, let’s look at some examples of how we can prepare our data for the LSTM.\n\nExample of LSTM With Single Input Sample\n\nConsider the case where you have one sequence of multiple time steps and one feature.\n\nFor example, this could be a sequence of 10 values:\n\nWe can define this sequence of numbers as a NumPy array.\n\nWe can then use the reshape() function on the NumPy array to reshape this one-dimensional array into a three-dimensional array with 1 sample, 10 time steps, and 1 feature at each time step.\n\nThe reshape() function when called on an array takes one argument which is a tuple defining the new shape of the array. We cannot pass in any tuple of numbers; the reshape must evenly reorganize the data in the array.\n\nOnce reshaped, we can print the new shape of the array.\n\nPutting all of this together, the complete example is listed below.\n\nRunning the example prints the new 3D shape of the single sample.\n\nThis data is now ready to be used as input (X) to the LSTM with an input_shape of (10, 1).\n\nExample of LSTM with Multiple Input Features\n\nConsider the case where you have multiple parallel series as input for your model.\n\nFor example, this could be two parallel series of 10 values:\n\nWe can define these data as a matrix of 2 columns with 10 rows:\n\nThis data can be framed as 1 sample with 10 time steps and 2 features.\n\nIt can be reshaped as a 3D array as follows:\n\nPutting all of this together, the complete example is listed below.\n\nRunning the example prints the new 3D shape of the single sample.\n\nThis data is now ready to be used as input (X) to the LSTM with an input_shape of (10, 2).\n\nFor a complete end-to-end worked example of preparing data, see this post:\n• How to Prepare Univariate Time Series Data for Long Short-Term Memory Networks\n\nThis section lists some tips to help you when preparing your input data for LSTMs.\n• The LSTM input layer must be 3D.\n• The meaning of the 3 input dimensions are: samples, time steps, and features.\n• The LSTM input layer is defined by the input_shape argument on the first hidden layer.\n• The input_shape argument takes a tuple of two values that define the number of time steps and features.\n• The number of samples is assumed to be 1 or more.\n• The reshape() function on NumPy arrays can be used to reshape your 1D or 2D data to be 3D.\n• The reshape() function takes a tuple as an argument that defines the new shape.\n\nThis section provides more resources on the topic if you are looking go deeper.\n• How to Convert a Time Series to a Supervised Learning Problem in Python\n\nIn this tutorial, you discovered how to define the input layer for LSTMs and how to reshape your sequence data for input to LSTMs.\n• How to define an LSTM input layer.\n• How to reshape a one-dimensional sequence data for an LSTM model and define the input layer.\n• How to reshape multiple parallel series data for an LSTM model and define the input layer.\n\nDo you have any questions?\n\n Ask your questions in the comments below and I will do my best to answer."
    },
    {
        "link": "https://pieriantraining.com/tensorflow-lstm-example-a-beginners-guide",
        "document": "LSTM (Long Short-Term Memory) is a type of Recurrent Neural Network (RNN) that is widely used in deep learning. It is particularly useful in processing and making predictions based on sequential data, such as time series, speech recognition, and natural language processing.\n\nTensorFlow is an open-source platform for machine learning developed by Google Brain Team. It provides a comprehensive set of tools and libraries for building and deploying machine learning models.\n\nIn this tutorial, we will walk through a step-by-step example of how to use TensorFlow to build an LSTM model for time series prediction. We will start by importing the necessary libraries and loading the dataset. Then we will preprocess the data and split it into training and testing sets.\n\nNext, we will define the LSTM model architecture using TensorFlow’s Sequential API. We will also specify the hyperparameters for the model, such as the number of epochs and batch size.\n\nAfter defining the model, we will train it on the training set and evaluate its performance on the testing set. We will visualize the results using matplotlib to see how well our model is able to predict future values in the time series.\n\nOverall, this tutorial aims to provide a beginner-friendly introduction to using TensorFlow and LSTM for time series prediction. By following along with this example, you should gain a better understanding of how to build and train your own deep learning models using TensorFlow.\n\nTensorFlow is an open-source machine learning library developed by Google Brain team. It is used to build and train machine learning models, including deep neural networks. TensorFlow is highly flexible and can be used for a wide range of applications, including image and speech recognition, natural language processing, and recommendation systems.\n\nOne of the key features of TensorFlow is its ability to handle large datasets efficiently. It uses data flow graphs to represent computations, which allows it to distribute computations across multiple CPUs or GPUs. This makes it possible to train complex models on large datasets in a reasonable amount of time.\n\nTensorFlow also provides a high-level API called Keras, which makes it easy to build and train deep learning models. Keras provides a simple interface for defining layers, specifying activation functions, and configuring optimization algorithms.\n\nIn this blog post, we will use TensorFlow to build an LSTM model for predicting stock prices. We will walk through each step of the process, from loading the data to evaluating the model’s performance. By the end of this tutorial, you should have a good understanding of how LSTM models work and how to implement them using TensorFlow.\n\nLSTM stands for Long Short-Term Memory, which is a type of Recurrent Neural Network (RNN) architecture. RNNs are designed to handle sequential data by processing each input based on the previous inputs. In other words, they have memory of the past inputs.\n\nLSTM takes this concept further by introducing a cell state that can keep information over long periods of time. This cell state is controlled by three gates: the input gate, the forget gate, and the output gate. These gates determine what information to keep or discard from the cell state.\n\nThe input gate decides what information to add to the cell state, while the forget gate decides what information to remove from the cell state. The output gate controls what information to output from the cell state.\n\nLSTM has become a popular choice in natural language processing tasks, such as language translation and sentiment analysis. This is because it can effectively handle long-term dependencies in sequential data, which is common in natural language.\n\nIn TensorFlow, you can implement LSTM using the `tf.keras.layers.LSTM` layer. This layer takes in a sequence of inputs and outputs a sequence of hidden states and a final cell state. You can then use these outputs for further processing or prediction tasks.\n\nLet’s take a look at an example implementation of LSTM in TensorFlow.\n\nIn this example, we define an LSTM model with an input shape of `(10, 1)`, meaning it takes in a sequence of 10 inputs with 1 feature each. We then compile the model with a mean squared error loss function and the Adam optimizer. Finally, we train the model on some input and target data for 10 epochs.\n\nOverall, LSTM is a powerful tool for handling sequential data in machine learning tasks, and TensorFlow provides easy-to-use tools for implementing it in your models.\n\nTo get started with the TensorFlow LSTM example, we first need to set up our environment. Here are the steps you need to follow:\n\n1. Install Python: You can download Python from the official website and install it on your machine. Make sure you install the latest version of Python.\n\n2. Install TensorFlow: Once you have installed Python, you can use pip (Python’s package manager) to install TensorFlow. Open your command prompt and run the following command:\n\nThis will install the latest version of TensorFlow on your machine.\n\n3. Install NumPy: NumPy is a popular Python library for numerical computing. You can install it using pip by running the following command:\n\n4. Install Pandas: Pandas is another popular Python library for data manipulation and analysis. You can install it using pip by running the following command:\n\n5. Install Matplotlib: Matplotlib is a plotting library for Python. You can install it using pip by running the following command:\n\nOnce you have installed all these libraries, you are ready to start working with the TensorFlow LSTM example. In the next section, we will dive into the code and see how we can implement an LSTM network using TensorFlow.\n\nIn order to train a TensorFlow LSTM model, we need to first load the data. In this example, we will be using the famous “Alice in Wonderland” book as our dataset. We will use the Natural Language Toolkit (NLTK) library to preprocess the text data.\n\nNext, we can import NLTK and download the necessary resources:\n\nNow we can load the text file and convert it into a list of sentences using NLTK’s `sent_tokenize()` function:\n\nWe can also preprocess the sentences by removing stop words and converting all words to lowercase:\n\nNow we have our preprocessed data ready to be used for training our TensorFlow LSTM model.\n\nBefore we can use the data for our LSTM model, we need to preprocess it. First, we will load the dataset using pandas and split it into training and testing sets. We will use 80% of the data for training and the remaining 20% for testing.\n\nNext, we need to normalize the data so that it falls within a certain range, typically between 0 and 1. This helps the model converge faster during training. We can use scikit-learn’s MinMaxScaler to do this.\n\nFinally, we need to reshape the data into the format expected by our LSTM model. The input to an LSTM model is a 3D array of shape (samples, timesteps, features). In our case, samples refer to the number of rows in our dataset, timesteps refer to the number of time steps in each sample sequence, and features refer to the number of variables in each time step.\n\nWith the data preprocessed and in the correct format, we can now move on to building our LSTM model.\n\nTo build an LSTM model using TensorFlow, we need to first import the necessary libraries. We will be using the Keras API of TensorFlow to build our model.\n\nNext, we define the architecture of our LSTM model. The first layer is the LSTM layer with 128 units and input shape of (X_train.shape[1], X_train.shape[2]). The return sequences parameter is set to True as we want to stack multiple LSTM layers.\n\nWe then add two more LSTM layers with 64 units each and return sequences set to True.\n\nFinally, we add a dense layer with a single output unit and compile the model with mean squared error loss and Adam optimizer.\n\nThat’s it! We have successfully built our LSTM model using TensorFlow.\n\nTo train the LSTM model, we need to define the loss function and optimizer. In this example, we will use the mean squared error as the loss function and the Adam optimizer.\n\nNext, we can train the model using the `fit()` method. We will train for 100 epochs with a batch size of 1.\n\nDuring training, we can monitor the loss and visualize it using a graph. This can help us determine if our model is overfitting or underfitting.\n\nAfter training, we can evaluate the model on the test data and calculate its accuracy.\n\nIt’s important to note that LSTM models can be computationally expensive to train. Depending on the size of your data and complexity of your model, training may take a significant amount of time.\n\nNow that we have trained our LSTM model, it’s time to evaluate its performance. In TensorFlow, we can do this by using the `evaluate()` method of the model object.\n\nFirst, we need to load the test data and preprocess it in the same way as we did for the training data. Once we have preprocessed the test data, we can evaluate the model using the `evaluate()` method. This method takes two arguments: the test data and its corresponding labels.\n\nThe `evaluate()` method returns two values: the loss and accuracy of the model on the test data. The loss is a measure of how well the model is able to predict the correct output, while the accuracy is a measure of how often the model is correct.\n\nIt’s important to note that we should only use the test data for evaluation purposes and not for training. Using the same data for both training and evaluation can lead to overfitting, where the model performs well on the training data but poorly on new, unseen data.\n\nIn addition to evaluating the overall performance of our model, we can also look at individual predictions using the `predict()` method. This method takes a single input example and returns its predicted output.\n\nBy examining individual predictions, we can gain insights into how our model is making decisions and identify areas where it may be making errors. This can help us improve our model and make it more accurate for future predictions.\n\nTo predict future values using TensorFlow LSTM, we can use the trained model to generate new sequences of data. These new sequences can then be used to predict future values.\n\nFirst, we need to create a seed sequence of data that the model will use to generate the new sequence. This seed sequence should be similar to the data used to train the model.\n\nOnce we have the seed sequence, we can use the trained model to generate a new sequence of data. To do this, we need to call the `model.predict()` method and pass in the seed sequence.\n\nThe `model.predict()` method will return a new sequence of data that we can use to predict future values. We can repeat this process multiple times to generate longer sequences of data.\n\nFinally, we can use the generated sequence of data to predict future values. The predicted values will be based on the patterns learned by the LSTM model during training.\n\nHere’s an example code snippet that demonstrates how to predict future values using TensorFlow LSTM:\n\nIn this example, we create a seed sequence with four values and then generate a new sequence of ten values using the trained LSTM model. The generated sequence is then printed out for inspection.\n\nIt’s important to note that predicting future values using LSTM models is not an exact science. The predictions are based on patterns learned during training and may not always be accurate. It’s always a good idea to validate the predictions using real-world data.\n\nIn conclusion, this TensorFlow LSTM example has provided a beginner’s guide to understanding the basics of LSTM neural networks and their implementation using TensorFlow. We have seen how LSTMs can be used for time series prediction tasks and how they can effectively model sequential data.\n\nWe started by discussing the architecture of an LSTM cell and its components, such as the forget gate, input gate, output gate, and cell state. We then moved on to implement an LSTM model using TensorFlow, which involved defining the model architecture, compiling it with an optimizer and loss function, and training it on our dataset.\n\nFinally, we evaluated our model’s performance using metrics such as mean squared error and visualized our predictions against the actual values. Overall, this example provides a solid foundation for anyone looking to dive deeper into the world of deep learning with LSTMs and TensorFlow.\n\nInterested in learning more? Check out our Introduction to Python course!"
    },
    {
        "link": "https://medium.com/@john.kosinski/preparing-and-shaping-timeseries-data-for-keras-lstm-input-part-two-ad17f6ab450",
        "document": "It’s often said that successful machine learning comes more from how the data is processed, than from the model architecture. Clearly both are important, but the importance of pre-processing the data might be sometimes underestimated.\n\nIn order to allow a model to learn as much as possible from a set of data, the important features of the data need to be extracted and arranged in such a way that the model can use them to generalize relationships. Often there is a long discovery process, in which the data is interrogated manually in order to determine or intuit what features to extract and how to present them. Training the model is usually one of the last steps in a lengthy process.\n\nIn this example I’m going to focus on price series data, for example a multi-year daily stock price time series, and I’m going to demonstrate an example of preprocessing the data to extract a few features, in order to prepare the data to be used to train a keras LSTM model.\n\nPart One gave a walkthrough of the following steps:\n\nI would read Part One before continuing. In this part, I will walk through:\n\n7. Extracting data about the trend, as a new column\n\n8. Shaping the data into the correct shape to be used as input for a keras LSTM model\n\nThis article is available in jupyter notebook form, for both Part One and Part Two, here:\n\nAnd in complete form with minimal comments:\n\nEarlier in the process, we ‘removed’ the trend from the data by extracting the daily percent change. The trend is of course still implicit in the daily percent change, but it’s no longer prominent. However, the trend can be an important feature of the data, and for some purposes one may want to extract it as well.\n\nThere are many ways to extract the trend from a time series. One might use arithmetic decomposition, or one might be inclined to use a simple MA or EMA, but the problem (it may be a problem, depending on what you’re doing) is that moving averages tend to lag. To accurately capture the exact high and low points based on a certain given granularity, I created a function to discern non-seasonal (or quasi-seasonal) highs and lows, and draw trendlines between them. For this example, I’m going to do that, then extract from it a range of values in which:\n\n0: indicates the trend turns downwards at this point (it’s a high point) \n\n1: indicates the trend turns upwards at this point (it’s a low point)\n\n0.5: indicates that the current trend continues\n\nThis technique may not be good for de-noisifying a series, but it’s good for extracting the pivot points. Below is a demonstration of it, with the trendlines overlaid on the original price series, over a few small segments of the series (the small segments are more convenient to view).\n\nNote that passing a smaller value for the period parameter will capture more of the price spikes, and passing a larger value will tend to capture fewer of them.\n\nThe extract_trend function and related code is too big to comfortably put into a code block, but can be found here:\n\nThe following code uses the extract_trend function to get and display the trend from the first 450 data points in the series, 150 data points at a time (that’s just to make what it’s doing more clear and visible)\n\nThen this data will be added as a new column to the DataFrame, called ‘Trend’. Note that the data is already normalized between 0 and 1, so it doesn’t require scaling. And there’s no need to handle outliers, because there are none.\n\nFinally, we have 3 columns (or features): Range, Change, and Trend.\n\nLet’s assume that Trend is what we want the model to predict.\n\nThe input for a keras LSTM requires a three dimensional array with the shape: \n\n(s, t, f)\n\ns = samples: the number of samples in the data set (i.e. the number of rows of data) \n\nt = timesteps: the number of timesteps to be input for each sample (also sometimes called the ‘lag’)\n\nf = features: the number of distinct features to be considered; in this case, 3 (Range, Change, Trend)\n\nAn LSTM can predict multiple output features, and can do so with a variable offset and width. But just to keep things simple, we’ll assume for this example that the output offset is 1, the LSTM will predict only one output feature (Change), and it will predict for only one timestep: the next day’s Change.\n\nNote also that the output feature need not be one of the input features as well. In this case, Change is present in both the input and the output.\n\nX represents the input values. \n\ny represents the predicted or expected values.\n\nSteps:\n\n1. Extract the ‘y’ values, or the values to be predicted. This is supervised learning, so these are all the ‘correct’ answers for training.\n\n2. Window the appropriate number of timesteps for each input\n\n3. Add one example of each feature, to each window\n\nBecause the LSTM keeps a memory of more recent inputs, data is fed into it in a forward walking window the size of a predetermined number of timesteps. Each discrete input contains multiple overlapping windows, and each window contains one example of each feature. It’s easier to explain with an example:\n\nThe raw input data has 10 rows of 2 features each: f1, f2. It looks like this:\n\nSo the outermost dimension of the 3-dimensional input array will have 10 elements. Each of those elements will be an array, so let’s create this to begin with:\n\nHow many timesteps of lag? Let’s say 3. So each of those empty arrays will have 3 arrays inside of them. Each of those innermost arrays will contain the 2 features.\n\nTo simplify, first create an array of 3-element arrays, where each element of the inner array represents one row. Since this is daily data, we’ll call row 0 d0, row 1 is d1, and so on.\n\nThe function to get the X values from the data set, shaped correctly as a 3-dimensional array in the form (samples, timesteps, features):\n\nThere are missing values, because in the beginning two records, there is a lack of previous data for t-2, and t-1, and at the end it’s impossible to make a prediction because we don’t have the future y value; this is expected. If we remove those missing-data rows then we are left with:\n\nTo explain it in another way, each sample will contain all features from multiple data points, and some of the data from sample n+1 is going to overlap that of sample n.\n\nSample s0 will contain data from timesteps t0–t2 (so t0, t1, and t2).\n\nSample s1 will contain data from timestep t1–t3 (t1, t2, t3)\n\nSample s2 will contain data from timestep t2–t4 (t2, t3, t4)\n\nAnd that’s assuming a lag of 3, which we’re using as a convenient example. With a lag of 4, s0 would contain t0-t3, s1 would contain t1–t4, and so on.\n\nThis function will do the necessary shaping and transforming for X, and will output X shifted and shaped correctly as a 3-dimensional array:\n\nAnd the function to get the y values from the dataset, shaped correctly (as a 1 dimensional array):\n\nReplace each day (row) with an array containing the two features of that day (row). So d0 becomes the two-element array [r0f1, r0f2].\n\nThe row numbers are sequential in each column going from top to bottom, and ordinal from left to right. That’s the input format. Since the first two rows contain nulls, we’d remove them. So we end up with the number of rows being r = (r — (timesteps — 1)).\n\nNow the y values are just a scalar array of feature 2 from each row, but shifted back 1.\n\nAnd the three lines to call these methods on the dataset, to get X and y values:\n\nFinally we can take all of the scaled, processed, shaped data as a whole and split it into training, validation, and testing sets with an approximately 70–20–10 split:\n\ntrain set has 2240 samples\n\nval set has 672 samples\n\ntest set has 288 samples\n\nAnd that’s the input shape for a tensorflow LSTM.\n\nSo we have had, from Part 1 to now,\n• The long-term trend extracted from the series\n• The final data set shaped correctly for input into a keras LSTM model\n• Finally, the finished data is split into training, validation, and testing sets\n\nAnd the purpose was to demonstrate just a few of infinite ways in which time/price series data can be pre-processed before being used in a prediction model.\n\nQ: Is it really practical to predict Trend in this way? Will a model trained with this data successfully predict something useful?\n\nA: No! Or probably not. This is not meant to be a practical example of anything except for different ways of preparing and pre-processing data for input to a model, particularly a keras LSTM model. I haven’t actually run this through a model and I don’t intend to.\n\nQ: What is the point of extracting the trend from the price data, and then re-introducing it in a different form? \n\nA: The trend was implicit in the price, and in the Change column, and still is even after the transformations. It’s implicit, but not prominent enough to be useful. It would be very very difficult for an model to extract that feature (the trend) by itself, and models don’t have infinite processing power. Part of the discovery process is trying different things, bringing different features to prominence and seeing how the model performs on them. One can’t expect the model to do all the work itself.\n\nQ: If the model can’t extract important features from the raw data by itself, what is the model for? \n\nA: The model can extract important features, but its power to do so is very much not unlimited, and it needs due amounts of help. Normally, the model’s just doing the last but very important steps in a process that could be done by non-ML human-powered statistical analysis, but would possibly take an inordinately long time or large amount of effort. Unless you have access to model networks that are under lock and key at the NSA (joking here), you need to have some idea of what you want the model to do, and help it to get started. \n\nAn analogy here would be teaching mathematics to a young child. If you put a kid in a library full of math textbooks, the kid is very unlikely to learn multiplication despite the fact that all of the necessary information is available. The information must be taken out, molded into examples and stories, and fed in. And it’s possible that one day, that kid will discover new techniques or proofs that advance the field of mathematics.\n\nThank you for reading. This article is available in jupyter notebook form, for both Part One and Part Two, here:\n\nAnd in complete form with minimal comments:"
    },
    {
        "link": "https://stackoverflow.com/questions/56730261/how-to-reshape-text-data-to-be-suitable-for-lstm-model-in-keras",
        "document": "The code Im referring is exactly the code in the book which you can find it here.\n\nThe only thing is that I don't want to have in the decoder part. That's why I think I don't need to have embedding layer at all because If I put embedding layer, I need to have in the decoder part(please correct me if Im wrong).\n\nOverall, Im trying to adopt the same code without using the embedding layer, because I need o have in the decoder part.\n\nI think the suggestion provided in the comment could be correct ( ) how ever I faced with this error:\n\nin check_num_samples you should specify the + steps_name + argument ValueError: If your data is in the form of symbolic tensors, you should specify the steps_per_epoch argument (instead of the batch_size argument, because symbolic tensors are expected to produce batches of input data)\n\nThe way that I have prepared data is like this:\n\nshape of is and I want to reshape it in a way I can feed it into LSTM. here stands for the and is number of samples I have.\n\nbelow is The code for :\n\nDo I still need to do anything extra, if No, why I can not get this works?\n\nPlease let me know which part is not clear I will explain.\n\nThanks for your help:)"
    },
    {
        "link": "https://tensorflow.org/api_docs/python/tf/keras/layers/LSTM",
        "document": "Used in the notebooks\n\nBased on available runtime hardware and constraints, this layer will choose different implementations (cuDNN-based or backend-native) to maximize the performance. If a GPU is available and all the arguments to the layer meet the requirement of the cuDNN kernel (see below for details), the layer will use a fast cuDNN implementation when using the TensorFlow backend. The requirements to use the cuDNN implementation are:\n• Inputs, if use masking, are strictly right-padded.\n• Eager execution is enabled in the outermost context.\n\nThis method is the reverse of , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by )."
    }
]