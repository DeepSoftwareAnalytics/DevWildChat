[
    {
        "link": "https://pytest-xdist.readthedocs.io",
        "document": "The pytest-xdist plugin extends pytest with new test execution modes, the most used being distributing tests across multiple CPUs to speed up test execution:\n\nWith this call, pytest will spawn a number of workers processes equal to the number of available CPUs, and distribute the tests randomly across them."
    },
    {
        "link": "https://github.com/pytest-dev/pytest-xdist/issues/18",
        "document": "xdist currently supports 2 ways of distributing tests across a number of workers:\n‚Ä¢ \"Each\" Scheduling: Given a set of tests, \"each\" scheduling sends each test in the set to each available worker.\n‚Ä¢ Load Scheduling: Given a set of tests, load scheduling distributes the tests among the available workers. Each test runs only once, and xdist tries to give each worker the same amount of work.\n\nThe current load scheduling implementation distributes tests naively across the workers. Often this means that two tests which depend on the same fixture get assigned to different runners, and the fixture has to be created on each runner.\n\nThis is a problem. Fixtures often capture expensive operations. When multiple tests depend on the same fixture, the author typically expects the expensive operation represented by that fixture to happen only once and be reused by all dependent tests. When tests that depend on the same fixture are sent to different workers, that expensive operation is executed multiple times. This is wasteful and can add significantly to the overall testing runtime.\n\nOur goal should be to reuse the existing pytest concept of the fixture to better distribute tests and reduce overall testing time, preferably without adding new options or APIs. This benefits the most users and strengthens pytest's declarative style.\n\nWe can solve this problem in 3 phases:\n‚Ä¢ Let's formalize the concept of a \"test chunk\" or \"test block\". A test chunk is a group of tests that always execute on the same worker. This is an internal xdist abstraction that the user normally doesn't have to know about. The master will only send complete test chunks to workers, not individual tests. Initially, each test will be assigned to its own chunk, so this won't change xdist's behavior at first. But it will pave the way for us to chunk tests by their attributes, like what fixtures they depend on. Once we have this internal abstraction, we can optionally also expose a hook that lets users define their own chunking algorithm to replace the initial default of \"1 test -> 1 chunk\". The hook won't be very useful until more information about each test is made available, which brings us to the next phase.\n‚Ä¢ We need to pass in additional information to the xdist master about the tests it's running so it can better distribute them. Specifically the master needs to be able to identify unique instances of every fixture that each test depends on. Tests that depend on distinct fixture instances can be assigned to different chunks and thus sent to different workers. To identify the distinct fixture instances that each test depends on, we need the following pieces of information for each test:\n‚Ä¢ For each fixture that the test depends on:\n‚Ä¢ Fixture scope and \"scope instance\".\n‚Ä¢ e.g. This fixture is module-scoped and this particular instance of the fixture is for module .\n‚Ä¢ Fixture parameter inputs, if the fixture is parameterized.\n‚Ä¢ e.g. We need to distinguish and as separate fixture instances. Initially this information won't be used for anything. It will just be made available to the master. At this point, we'll be ready for the final phase.\n‚Ä¢ Using the new information we have about tests, along with the new internal abstraction of a test chunk, we can now chunk tests up by the list of unique fixture instances they depend on. Tests that depend on the same instance of a fixture will now always be sent to the same worker. üéâ\n\nThis approach has two major advantages over other proposals.\n‚Ä¢ This approach adds no new configuration options or public APIs that users have to learn. Everyone using pytest automatically gets a better, arguably even more correct, distribution of their tests to workers without having to do any work.\n‚Ä¢ This approach promotes a declarative style of writing tests over an imperative one. The goal we should strive for is: Capture your ideas correctly, and pytest will figure out the appropriate execution details. In practice, this is probably not always feasible, and the user will want to exercise control in specific cases. But the closer we can stick to this goal the better the pytest user experience will be.\n\nHow this approach addresses common use cases\n\nThere are several use cases described in the original issue requesting some way to control parallel execution. This is how the approach described here addresses those use cases:\n‚Ä¢ Use case: \"I want all the tests in a module to go to the same worker.\" Solution: Make all the tests in that module depend on the same module-scoped fixture. If the tests don't need to depend on the same fixture, then why do they need to go to the same worker? (The most likely reason is that they are not correctly isolated.)\n‚Ä¢ Use case: \"I want all the tests in a class to go the same worker.\" Solution: The solution here is the same. If the tests need to go to the same worker, then they should depend on a common fixture. To cleanly address this case, we may need to implement the concept of a class-scoped fixture. (Fixtures can currently be scoped only by function, module, or session.) Otherwise, the tests can depend on a common, module-scoped fixture and achieve the same result.\n‚Ä¢ Use case: \"I want all the tests in X to go to the same worker.\" Solution: You know the drill. If tests belong on the same worker, we are betting that there is an implied, shared dependency. Express that shared dependency as a fixture and pytest will take care of the rest for you.\n\nCounter-proposal to #17.\n\n Addresses pytest-dev/pytest#175.\n\n Adapted from this comment by @hpk42."
    },
    {
        "link": "https://github.com/pytest-dev/pytest-xdist/issues/255",
        "document": "I have several projects where the distribution of tests runtime is quite scattered, eg:\n\nThe current load scheduler comes short in this case, as it often ends up sending a batch of slow tests to the same worker.\n\nAs a workaround, I use a forked LoadScheduler that uses a fixed queue size (which I use with the minimum value of 2 -> each worker only has one test in its queue at any time):\n\nIt would be nice to have at least one of these propositions implemented in xdist:\n‚Ä¢ Integrate this scheduler (or an even simpler version where queue_size=2)\n‚Ä¢ Make LoadScheduler configurable, so that users can provide initial_batch_size / items_per_node_min / items_per_node_max\n‚Ä¢ When sending a batch of jobs to a node, shuffle like for the initial batch\n‚Ä¢ Maybe improve/reduce a bit the defaults settings for initial_batch_size / items_per_node_min / items_per_node_max"
    },
    {
        "link": "https://pytest-xdist.readthedocs.io/en/latest/changelog.html",
        "document": "This release was YANKED due to a regression fixed in 3.6.1.\n‚Ä¢ None #1027: workers now always execute the tests in the main thread. Previously some tests might end up executing in a separate thread other than in the workers, due to some internal details. This can cause problems specially with async frameworks where the event loop is running in the thread (for example #620).\n‚Ä¢ None #1024: Added proper handling of (such as set by ) and conditions in workers. Previously, a worker might have continued executing further tests before the controller could terminate the session.\n‚Ä¢ None #620: Use the new ‚Äúexecmodel‚Äù so that code which expects to only run in the main thread will now work as expected.\n‚Ä¢ None #937: Fixed a bug where plugin would raise an incompatibility error with despite using .\n‚Ä¢ If you relied on this file, e.g. to install pytest using , please see Why you shouldn‚Äôt invoke setup.py directly for alternatives.\n‚Ä¢ None #1057: The internals of pytest-xdist are now fully typed. The typing is not exposed yet.\n‚Ä¢ None #996: Adjusted license file format and content to ensure security scanners will identity the license.\n‚Ä¢ None #825: The command line argument and config variable are deprecated. The rsync feature will be removed in pytest-xdist 4.0.\n‚Ä¢ None #826: The command line argument and config variable are deprecated. The loop-on-fail feature will be removed in pytest-xdist 4.0.\n‚Ä¢ None #792: The environment variable can now be used to specify the default for and .\n‚Ä¢ pytest orders tests for optimal sequential execution - i. e. avoiding unnecessary setup and teardown of fixtures. So executing tests in consecutive chunks is important for optimal performance. In v1.14, initial test distribution in was changed to round-robin, optimized for the corner case, when the number of tests is less than . At the same time, it became worse for all other cases. For example: if some tests use some ‚Äúheavy‚Äù fixture, and these tests fit into the initial batch, with round-robin distribution the fixture will be created times, no matter how many other tests there are. With the old algorithm (before v1.14), if there are enough tests not using the fixture, the fixture was created only once. So restore the old behavior for typical cases where the number of tests is much greater than the number of workers (or, strictly speaking, when there are at least 2 tests for every node).\n‚Ä¢ None #468: The command-line option has been removed. If you still need this functionality, install pytest-forked separately.\n‚Ä¢ None #468: The dependency has been dropped.\n‚Ä¢ None #822: Replace internal usage of with a custom solution (but with the same interface).\n‚Ä¢ None #823: Remove usage of as an rsync candidate.\n‚Ä¢ None Hot fix release reverting the change introduced by #124, unfortunately it broke a number of test suites so we are reversing this change while we investigate the problem. (#157)\n‚Ä¢ None fix #124: xdist would mark test as complete after ‚Äòcall‚Äô step. As a result, xdist could identify the wrong test as failing when test crashes at teardown. To address this issue, xdist now marks test as complete at teardown.\n‚Ä¢ None now requires pytest 2.7 or later.\n‚Ä¢ None new hook: , can return custom tests items distribution logic implementation. You can take a look at built-in and implementations. Note that required scheduler class public API may change in next versions.\n‚Ä¢ None new fixture, returns the id of the worker in a test or fixture. Thanks Jared Hellman for the PR.\n‚Ä¢ None display progress during collection only when in a terminal, similar to pytest #1397 issue. Thanks Bruno Oliveira for the PR.\n‚Ä¢ None fix internal error message when is used (#62, #65). Thanks Collin RM Stocks and Bryan A. Jones for reports and Bruno Oliveira for the PR.\n‚Ä¢ None new hook: , called when a worker has finished collection. Thanks Omer Katz for the request and Bruno Oliveira for the PR.\n‚Ä¢ None fix #22: xdist now works if the internal tmpdir plugin is disabled. Thanks Bruno Oliveira for the PR.\n‚Ä¢ None fix #32: xdist now works if looponfail or boxed are disabled. Thanks Bruno Oliveira for the PR.\n‚Ä¢ None extended the tox matrix with the supported py.test versions\n‚Ä¢ None split up the plugin into 3 plugin‚Äôs to prepare the departure of boxed and looponfail. looponfail will be a part of core and forked boxed will be replaced with a more reliable primitive based on xdist\n‚Ä¢ None conforming with new pytest-2.8 behavior of returning non-zero when all tests were skipped or deselected.\n‚Ä¢ None new ‚Äú‚Äìmax-slave-restart‚Äù option that can be used to control maximum number of times pytest-xdist can restart slaves due to crashes. Thanks to Anatoly Bubenkov for the report and Bruno Oliveira for the PR.\n‚Ä¢ None ‚Äú-n‚Äù option now can be set to ‚Äúauto‚Äù for automatic detection of number of cpus in the host system. Thanks Suloev Dmitry for the PR.\n‚Ä¢ None fix issue594: properly report errors when the test collection is random. Thanks Bruno Oliveira.\n‚Ä¢ None some internal test suite adaptation (to become forward compatible with the upcoming pytest-2.8)\n‚Ä¢ None fix pytest/xdist issue485 (also depends on py-1.4.22): attach stdout/stderr on ‚Äìboxed processes that die.\n‚Ä¢ None fix pytest/xdist issue503: make sure that a node has usually two items to execute to avoid scoped fixtures to be torn down pre-maturely (fixture teardown/setup is ‚Äúnextitem‚Äù sensitive). Thanks to Andreas Pelme for bug analysis and failing test.\n‚Ä¢ None restart crashed nodes by internally refactoring setup handling of nodes. Also includes better code documentation. Many thanks to Floris Bruynooghe for the complete PR.\n‚Ä¢ None add glob support for rsyncignores, add command line option to pass additional rsyncignores. Thanks Anatoly Bubenkov.\n‚Ä¢ None fix pytest issue382 - produce ‚Äúpytest_runtest_logstart‚Äù event again in master. Thanks Aron Curzon.\n‚Ä¢ None fix pytest issue419 by sending/receiving indices into the test collection instead of node ids (which are not necessarily unique for functions parametrized with duplicate values)\n‚Ä¢ None send multiple ‚Äúto test‚Äù indices in one network message to a slave and improve heuristics for sending chunks where the chunksize depends on the number of remaining tests rather than fixed numbers. This reduces the number of master -> node messages (but not the reverse direction)\n‚Ä¢ None fix pytest issue41: re-run tests on all file changes, not just randomly select ones like .py/.c.\n‚Ä¢ None fix pytest issue347: slaves running on top of Python3.2 will set PYTHONDONTWRITEYBTECODE to 1 to avoid import concurrency bugs.\n‚Ä¢ None fix pytest-issue93 - use the refined pytest-2.2.1 runtestprotocol interface to perform eager teardowns for test items.\n‚Ä¢ None fix incompatibilities with pytest-2.2.0 (allow multiple pytest_runtest_logreport reports for a test item)\n‚Ä¢ None fix race condition in looponfail mode where a concurrent file removal could cause a crash\n‚Ä¢ None adapt to and require pytest-2.0 changes, rsyncdirs and rsyncignore can now only be specified in [pytest] sections of ini files, see ‚Äúpy.test -h‚Äù for details.\n‚Ä¢ None major internal refactoring to match the pytest-2.0 event refactoring - perform test collection always at slave side instead of at the master - make python2/python3 bridging work, remove usage of pickling\n‚Ä¢ None perform distributed testing related reporting in the plugin rather than having dist-related code in the generic py.test distribution\n‚Ä¢ None depend on execnet-1.0.7 which adds ‚Äúenv1:NAME=value‚Äù keys to gateway specification strings.\n‚Ä¢ None show detailed gateway setup and platform information only when ‚Äú-v‚Äù or ‚Äú‚Äìverbose‚Äù is specified.\n‚Ä¢ None fix ‚Äìlooponfailing - it would not actually run against the fully changed source tree when initial conftest files load application state.\n‚Ä¢ None fix issue79: sessionfinish/teardown hooks are now called systematically on the slave side\n‚Ä¢ None introduce a new data input/output mechanism to allow the master side to send and receive data from a slave.\n‚Ä¢ None use and require new register hooks facility of py.test>=1.3.0\n‚Ä¢ None require improved execnet>=1.0.6 because of various race conditions that can arise in xdist testing modes.\n‚Ä¢ None fix an indefinite hang which would wait for events although no events are pending - this happened if items arrive very quickly while the ‚Äúreschedule-event‚Äù tried unconditionally avoiding a busy-loop and not schedule new work.\n‚Ä¢ None moved code out of py-1.1.1 into its own plugin\n‚Ä¢ None use a new, faster and more sensible model to do load-balancing of tests - now no magic ‚ÄúMAXITEMSPERHOST‚Äù is needed and load-testing works effectively even with very few tests.\n‚Ä¢ None make -x cause hard killing of test nodes to decrease wait time until the traceback shows up on first failure"
    },
    {
        "link": "https://docs.pytest.org/en/stable/reference/plugin_list.html",
        "document": ""
    },
    {
        "link": "https://github.com/pytest-dev/pytest-xdist/issues/18",
        "document": "xdist currently supports 2 ways of distributing tests across a number of workers:\n‚Ä¢ \"Each\" Scheduling: Given a set of tests, \"each\" scheduling sends each test in the set to each available worker.\n‚Ä¢ Load Scheduling: Given a set of tests, load scheduling distributes the tests among the available workers. Each test runs only once, and xdist tries to give each worker the same amount of work.\n\nThe current load scheduling implementation distributes tests naively across the workers. Often this means that two tests which depend on the same fixture get assigned to different runners, and the fixture has to be created on each runner.\n\nThis is a problem. Fixtures often capture expensive operations. When multiple tests depend on the same fixture, the author typically expects the expensive operation represented by that fixture to happen only once and be reused by all dependent tests. When tests that depend on the same fixture are sent to different workers, that expensive operation is executed multiple times. This is wasteful and can add significantly to the overall testing runtime.\n\nOur goal should be to reuse the existing pytest concept of the fixture to better distribute tests and reduce overall testing time, preferably without adding new options or APIs. This benefits the most users and strengthens pytest's declarative style.\n\nWe can solve this problem in 3 phases:\n‚Ä¢ Let's formalize the concept of a \"test chunk\" or \"test block\". A test chunk is a group of tests that always execute on the same worker. This is an internal xdist abstraction that the user normally doesn't have to know about. The master will only send complete test chunks to workers, not individual tests. Initially, each test will be assigned to its own chunk, so this won't change xdist's behavior at first. But it will pave the way for us to chunk tests by their attributes, like what fixtures they depend on. Once we have this internal abstraction, we can optionally also expose a hook that lets users define their own chunking algorithm to replace the initial default of \"1 test -> 1 chunk\". The hook won't be very useful until more information about each test is made available, which brings us to the next phase.\n‚Ä¢ We need to pass in additional information to the xdist master about the tests it's running so it can better distribute them. Specifically the master needs to be able to identify unique instances of every fixture that each test depends on. Tests that depend on distinct fixture instances can be assigned to different chunks and thus sent to different workers. To identify the distinct fixture instances that each test depends on, we need the following pieces of information for each test:\n‚Ä¢ For each fixture that the test depends on:\n‚Ä¢ Fixture scope and \"scope instance\".\n‚Ä¢ e.g. This fixture is module-scoped and this particular instance of the fixture is for module .\n‚Ä¢ Fixture parameter inputs, if the fixture is parameterized.\n‚Ä¢ e.g. We need to distinguish and as separate fixture instances. Initially this information won't be used for anything. It will just be made available to the master. At this point, we'll be ready for the final phase.\n‚Ä¢ Using the new information we have about tests, along with the new internal abstraction of a test chunk, we can now chunk tests up by the list of unique fixture instances they depend on. Tests that depend on the same instance of a fixture will now always be sent to the same worker. üéâ\n\nThis approach has two major advantages over other proposals.\n‚Ä¢ This approach adds no new configuration options or public APIs that users have to learn. Everyone using pytest automatically gets a better, arguably even more correct, distribution of their tests to workers without having to do any work.\n‚Ä¢ This approach promotes a declarative style of writing tests over an imperative one. The goal we should strive for is: Capture your ideas correctly, and pytest will figure out the appropriate execution details. In practice, this is probably not always feasible, and the user will want to exercise control in specific cases. But the closer we can stick to this goal the better the pytest user experience will be.\n\nHow this approach addresses common use cases\n\nThere are several use cases described in the original issue requesting some way to control parallel execution. This is how the approach described here addresses those use cases:\n‚Ä¢ Use case: \"I want all the tests in a module to go to the same worker.\" Solution: Make all the tests in that module depend on the same module-scoped fixture. If the tests don't need to depend on the same fixture, then why do they need to go to the same worker? (The most likely reason is that they are not correctly isolated.)\n‚Ä¢ Use case: \"I want all the tests in a class to go the same worker.\" Solution: The solution here is the same. If the tests need to go to the same worker, then they should depend on a common fixture. To cleanly address this case, we may need to implement the concept of a class-scoped fixture. (Fixtures can currently be scoped only by function, module, or session.) Otherwise, the tests can depend on a common, module-scoped fixture and achieve the same result.\n‚Ä¢ Use case: \"I want all the tests in X to go to the same worker.\" Solution: You know the drill. If tests belong on the same worker, we are betting that there is an implied, shared dependency. Express that shared dependency as a fixture and pytest will take care of the rest for you.\n\nCounter-proposal to #17.\n\n Addresses pytest-dev/pytest#175.\n\n Adapted from this comment by @hpk42."
    },
    {
        "link": "https://pytest-xdist.readthedocs.io/en/stable/distribution.html",
        "document": "To send tests to multiple CPUs, use the (or ) option:\n\nThis can lead to considerable speed ups, especially if your test suite takes a noticeable amount of time.\n\nWith , pytest-xdist will use as many processes as your computer has physical CPU cores.\n\nUse to use the number of logical CPU cores rather than physical ones. This currently requires the psutil package to be installed; if it is not or if it fails to determine the number of logical CPUs, fall back to behavior.\n\nPass a number, e.g. , to specify the number of processes explicitly.\n\nTo specify a different meaning for and for your tests, you can:\n‚Ä¢ None Set the environment variable to the desired number of processes.\n‚Ä¢ None Implement the pytest hook (a function in e.g. ) that returns the number of processes to use. The hook can use to determine if the user asked for or , and it can return to fall back to the default.\n\nIf both the hook and environment variable are specified, the hook takes priority.\n\nParallelization can be configured further with these options:\n‚Ä¢ None : limit the maximum number of workers to process the tests.\n‚Ä¢ None : maximum number of workers that can be restarted when crashed (set to zero to disable this feature).\n\nThe test distribution algorithm is configured with the command-line option:\n‚Ä¢ None (default): Sends pending tests to any worker that is available, without any guaranteed order. Scheduling can be fine-tuned with the option, see output of .\n‚Ä¢ None : Tests are grouped by module for test functions and by class for test methods. Groups are distributed to available workers as whole units. This guarantees that all tests in a group run in the same process. This can be useful if you have expensive module-level or class-level fixtures. Grouping by class takes priority over grouping by module.\n‚Ä¢ None : Tests are grouped by their containing file. Groups are distributed to available workers as whole units. This guarantees that all tests in a file run in the same worker.\n‚Ä¢ None : Tests are grouped by the mark. Groups are distributed to available workers as whole units. This guarantees that all tests with same name run in the same worker. This will make sure and will run in the same worker. Tests without the mark are distributed normally as in the mode.\n‚Ä¢ None : Initially, tests are distributed evenly among all available workers. When a worker completes most of its assigned tests and doesn‚Äôt have enough tests to continue (currently, every worker needs at least two tests in its queue), an attempt is made to reassign (‚Äústeal‚Äù) a portion of tests from some other worker‚Äôs queue. The results should be similar to the method, but should handle tests with significantly differing duration better, and, at the same time, it should provide similar or better reuse of fixtures.\n‚Ä¢ None : The normal pytest execution mode, runs one test at a time (no distribution at all)."
    },
    {
        "link": "https://stackoverflow.com/questions/51756594/pytest-xdist-indirect-fixtures-with-class-scope",
        "document": "I have some complicated and heavy logic to build a test object and the tests are very long running. They are integration tests and I wanted to try and parallelize them a bit. So i found the pytest-xdist library.\n\nBecause of the heavy nature of building the test object, I am using pytests indirection capability on fixtures to build them at test time rather than at collection. Some code I am using for testing can be found below.\n\nMy run command is currently\n\nWhen I do no use loadscope, all the tests are sent to their own worker. I do not want this because I would like to only build the device object once and use it for all related tests.\n\nWhen I use loadscope, all the tests are executed against gw0 and I am not getting any parallelism.\n\nI am wondering if there is any tweaks that I am missing or is this functionality not implemented currently."
    },
    {
        "link": "https://github.com/pytest-dev/pytest-xdist/issues/620",
        "document": "When -based software needs to register handlers, calling only works when the current thread is main and raises a otherwise.\n\nWhen I first tried integrating into 's test suite a few years ago, we've faced a problem that about 15% of pytest invocations would hit this and had to disable the plugin:\n‚Ä¢ None pytest-aiohttp + pytest-xdist cause RuntimeError: set_wakeup_fd only works in main thread\n\nThe reproducer is to clone aio-libs/aiohttp, add to somewhere in , and re-run until you see the traceback. On aiohttp side, it's coming from the call invoked from the pytest plugin @ https://github.com/aio-libs/aiohttp/blob/742a8b6/aiohttp/pytest_plugin.py#L161.\n\nNow that I've had some time to try to debug what's happening, I've stuck a debugger right before the line that raises and confirmed that the thread wasn't main:\n\n@asvetlov It should be possible to fix this on the side with (https://bugs.python.org/issue35621 / python/cpython#14344) but it only appeared in the stdlib since Python 3.8: https://stackoverflow.com/a/58614689/595220.\n\nIt's hard to pinpoint where that thread is coming from but it appears to be 's fault:\n\n@nicoddemus do you have any insight into this? I'm rather lost at this point. Is there any way to avoid this race condition?"
    },
    {
        "link": "https://stackoverflow.com/questions/4637036/is-there-a-way-to-control-how-pytest-xdist-runs-tests-in-parallel",
        "document": "I have the following directory layout:\n\nThe format of testsuite*.py modules is as follows:\n\nThe problem I have is that I would like to execute the 'testsuites' in parallel i.e. I want testsuite1, testsuite2, testsuite3 and testsuite4 to start execution in parallel but individual tests within the testsuites need to be executed serially.\n\nWhen I use the 'xdist' plugin from py.test and kick off the tests using 'py.test -n 4', py.test is gathering all the tests and randomly load balancing the tests among 4 workers. This leads to the 'setup_class' method to be executed every time of each test within a 'testsuitex.py' module (which defeats my purpose. I want setup_class to be executed only once per class and tests executed serially there after).\n\nEssentially what I want the execution to look like is:\n\nwhile are all executed in parallel.\n\nIs there a way to achieve this in 'pytest-xidst' framework?\n\nThe only option that I can think of is to kick off different processes to execute each test suite individually within runner.py:"
    },
    {
        "link": "https://stackoverflow.com/questions/59498774/how-to-tell-pytest-xdist-to-run-tests-from-one-folder-sequencially-and-the-rest",
        "document": "Imagine that I have which are safe to run in parallel and which cannot be run in parallel yet.\n\nIs there an easy way to convince pytest to run the ones sequentially? Consider that we are talking about a big number of tests so altering each test function/method would be very noisy.\n\nAt this moment we run tests with marker filters so mainly we run them separated. Still, I am looking for a solution for removing the need to run them separated."
    },
    {
        "link": "https://github.com/pytest-dev/pytest-xdist/issues/709",
        "document": "I have opened 8 processes to run my test cases, but there are some special test cases, I hope they can be distributed to all processes for execution"
    },
    {
        "link": "https://github.com/pytest-dev/pytest-xdist/issues/255",
        "document": "I have several projects where the distribution of tests runtime is quite scattered, eg:\n\nThe current load scheduler comes short in this case, as it often ends up sending a batch of slow tests to the same worker.\n\nAs a workaround, I use a forked LoadScheduler that uses a fixed queue size (which I use with the minimum value of 2 -> each worker only has one test in its queue at any time):\n\nIt would be nice to have at least one of these propositions implemented in xdist:\n‚Ä¢ Integrate this scheduler (or an even simpler version where queue_size=2)\n‚Ä¢ Make LoadScheduler configurable, so that users can provide initial_batch_size / items_per_node_min / items_per_node_max\n‚Ä¢ When sending a batch of jobs to a node, shuffle like for the initial batch\n‚Ä¢ Maybe improve/reduce a bit the defaults settings for initial_batch_size / items_per_node_min / items_per_node_max"
    },
    {
        "link": "https://betterstack.com/community/guides/testing/pytest-guide",
        "document": "Unit testing is a crucial aspect of the software development process that ensures that individual components of your code function as expected. Pytest , with its intuitive syntax, robust features, and extensive plugin ecosystem, has emerged as a leading choice for Python unit testing.\n\nIn this guide, we will explore the core principles of unit testing, delve into Pytest's powerful capabilities, and equip you with the knowledge and skills to write clean, maintainable, and effective tests.\n\nBy the end, you'll be well-versed in leveraging Pytest to improve your code quality, catch bugs early, and build more reliable software.\n\nBefore proceeding with this tutorial, ensure that you have a recent version of Python installed , and a basic understanding of writing Python programs.\n\nBefore you can start learning about Pytest, you need to have a program to test. In this section, you will create a small program that formats file sizes in bytes in a human-readable format.\n\nStart by creating and navigating to the project directory using the following commands:\n\nWithin this directory, set up a virtual environment to isolate project dependencies:\n\nOnce activated, the command prompt will be prefixed with the name of the virtual environment ( in this case):\n\nYou can now proceed to create the file formatting program in the directory:\n\nEnsure that the directory is recognized as a package by adding an file:\n\nNow, create a file within the directory and paste in the contents below:\n\nThe function converts sizes into human-readable formats such as Kilobytes, Megabytes, or Gigabytes.\n\nIn the root directory, create a file that imports the function and executes it:\n\nThis script serves as the entry point to our program. It reads the file size from the command line, calls the function, and prints the result.\n\nLet's quickly test our script to make sure it's working as expected:\n\nWith the demo program ready, you're now all set to dive into unit testing with Pytest!\n\nIn this section, you'll automate the testing process using Pytest. Instead of manually providing input to your program, you'll write tests that feed in various file sizes and verify if the output matches your expectations.\n\nBefore you can utilize Pytest in your project, you need to install it first with:\n\nOnce installed, create a directory where all your tests will be written in. The convention is to use a directory placed adjacent to your source code like this:\n\nGo ahead and create the directory and a file within this directory:\n\nPopulate this file with the following contents:\n\nThis simple test calls the function with 1024 cubed (which represents 1GB) and asserts that the returned value is exactly \"1.00 GB\". If the function behaves as expected, this test will pass.\n\nNow that you've written a test for the program, we'll look at how to execute the test next.\n\nTo execute the test you just created, you must invoke the command like this:\n\nUpon execution, you should see output similar to the following:\n\nThis output indicates that one test item was found in the file, and it passed within 0.01 seconds.\n\nIf your terminal supports color, you'll see a green line at the bottom, which further signifies successful execution, as depicted in the screenshot below:\n\nWhen you run the command without any arguments, it searches through the current directory and all its subdirectories for file names that begin with and executes the test functions within them.\n\nNow that you've experienced running tests that pass, let's explore how Pytest presents failing tests. Go ahead and modify the previous test function to fail intentionally:\n\nThen re-run the test as follows:\n\nIn case of a failure, Pytest will display a red line and a detailed error report:\n\nThe output explains why the test failed, with a traceback indicating a mismatch between the expected result (\"1.00 GB\") and the actual result (\"0B\"). The subsequent summary block provides a concise overview of the failure without the traceback, indicating why the test failed.\n\nIf you're only interested in the summary and don't need the traceback, use the option:\n\nThe output will then appear as follows:\n\nIn this output, you only see the summaries, which can help you quickly understand why a test failed without the clutter of the traceback.\n\nYou may now revert your changes to the test function to see it passing once again. You can also use the \"quiet\" reporting mode with the flag, which keeps the output brief:\n\nThis iterative process of writing tests, running them, and fixing issues is the core of the automated testing process, that helps you develop more reliable and maintainable software.\n\nIn this section, you'll explore conventions and best practices you should follow when designing tests with Pytest to ensure clarity and maintainability.\n\nAs you've already seen, test files (beginning with ) are generally placed in a dedicated directory. This naming convention is crucial because Pytest relies on it to discover and run your tests.\n\nIf you prefer, you can place test files alongside their corresponding source files. For example, and its test file can reside in the same directory:\n\nPytest also accepts filenames ending with , but this is less common.\n\nDuring testing, you can create functions or classes to organize your assertions. As demonstrated earlier, function-based tests should be prefixed with . While it's not mandatory to include an underscore, it's recommended for clarity:\n\nSimilarly, when using classes for testing, the class name should be prefixed with (capitalized), and its methods should also be prefixed with :\n\nDescriptive names for functions, methods, and classes are crucial for test readability and maintainability. Avoid generic names like and instead opt for descriptive names that convey what the test is validating.\n\nConsider the following examples of well-named test files and functions:\n\nBy adhering to these guidelines, you'll create a test suite that is not only functional but also easy to navigate, understand, and maintain.\n\nAs your test suite grows, running every test with each change can become time-consuming. Pytest provides several methods for selectively running the tests you're currently focused on.\n\nBefore proceeding, modify your test file with additional test cases as follows:\n\nIf you have multiple test files and want to run tests only within a specific file, you can simply provide the file path to Pytest:\n\nTo target a single test function within a file, append followed by the function name to the file path:\n\nPytest's runner will execute the specified test alone:\n\nIf your tests are organized in classes, you can execute tests from a specific class like this:\n\nTo execute only a specific method within that class:\n\nPytest's option allows you to filter tests based on substring matches or Python expressions. For example, to execute only tests with the \"mb\" substring, run:\n\nThis command will execute only the test in this case (since it's the only test that contains the string).\n\nYou can verify this by adding the (verbose) option:\n\nYou can also exclude tests by using the keyword:\n\nThis will execute the tests that don't contain \"gb\" and \"mb\" in their names:\n\nBy selectively running tests, you can save time during development and focus on testing the specific parts of your code that you're actively working on.\n\nIt's common to test the same function multiple times with different inputs. Instead of writing repetitive test functions, Pytest offers a streamlined way to handle this using parametrization.\n\nConsider the file from the previous section. It contains multiple test functions, each verifying a different formatting scenario for the function. While each test has a unique purpose, the structure becomes repetitive as only the input values and expected results change.\n\nPytest's decorator solves this problem by allowing you to concisely define multiple test cases within a single function.\n\nTo apply this approach, rewrite the contents of the as follows:\n\nThe function now incorporates parametrization through the decorator which lists the test cases as tuples: tuples: (sizebytes, expectedoutput)\n\nPytest will run this function multiple times, once for each tuple, effectively creating separate test cases. Execute the command below to see this in action:\n\nRather than seeing all six tests as a collective block passing, you can use the option to display each test individually, with Pytest assigning a unique test ID to each:\n\nBy default, Pytest generates test IDs based on input values. For more descriptive IDs, use within your test cases:\n\nNow, the test output will display your custom IDs, making it clearer what each test is checking.\n\nWith parametrization, your test suite becomes more concise, easier to maintain, and checks a broader range of scenarios without code duplication.\n\nYou used in the previous step to define the test cases. In this section, I'll show you an alternative way to parametrize test cases using data classes.\n\nData classes offer a more structured and organized way to define test cases in the following ways:\n‚Ä¢ They logically group related test data (input values, expected outputs, and IDs) into a single object.\n‚Ä¢ This grouping improves the readability of your test cases and makes it easier to understand what each test is doing.\n‚Ä¢ You can set default values for fields, reducing redundancy if many test cases share similar properties.\n\nLet's convert the parametrized test from the previous section to use the data class pattern. Here's the updated code:\n\nThis code defines a class using the decorator. It defines three attributes: , , and . The attribute is initialized in the method, which assigns it a unique value based on the attribute.\n\nThis class represents a blueprint for our test cases. Each test case will be an instance of this class.\n\nThe decorator tells Pytest to run the function multiple times ‚Äì once for each item in the list. In each run, the parameter will be an instance of . The argument in the decorator ensures that each test case in the output is clearly labelled with its generated ID.\n\nWhen you re-run the tests with:\n\nYou will see that all the tests pass as before:\n\nWhen your code includes exception handling, confirming that specific exceptions are raised under the right conditions is necessary. The function is designed for testing such scenarios.\n\nFor instance, the function raises a if the input is a negative integer:\n\nYou can test if the exception is raised using with:\n\nThis code passes a negative input (-1) to and the context manager verifies if a with the message \"Size cannot be negative\" is raised.\n\nYou can integrate this case into your parametrized test as follows:\n\nHere, two fields were added to the class: , and . These signal if an error is expected and its message.\n\nThe new test case is then supplied an input of to trigger the error, and the and fields are supplied accordingly.\n\nFinally, in the function, checks the exception type and the error message. If no error is expected, ensures that the formatted output matches the expected result.\n\nUpon saving and running the test, you'll see that it passes, confirming that the exception was raised:\n\nYou should see output similar to:\n\nFor error messages that may vary slightly, you can use regular expressions with :\n\nWith this in place, you can efficiently verify that your code raises the expected exceptions under various conditions.\n\nHaving gained familiarity with writing and executing tests, let's turn our attention to helper functions known as fixtures. Fixtures are special functions that Pytest executes before or after tests to assist with setup tasks or to provide necessary data. Using fixtures minimizes repetition and improves maintainability by centralizing common setup procedures.\n\nAlthough the topic of Pytest fixtures is extensive enough to warrant a dedicated article, this section aims to provide a concise introduction to their fundamental principles.\n\nTo get started with fixtures, create a file named in your editor and include the following code:\n\nThe decorator defines a fixture in Pytest. Such fixtures, like , can execute setup tasks and deliver data to test functions. When a test function lists a fixture by name as a parameter, Pytest automatically invokes the fixture function before running the test function.\n\nExecute these tests by running the following command:\n\nExecuting the command will yield the following results:\n\nAn example of a more practical use of fixtures involves setting up databases as shown in the following example with SQLite :\n\nThe fixture is set to the scope to ensure it runs once per test module. It sets up an in-memory SQLite database connection, establishes a table, and manages the connection.\n\nFunctions like and use this database to add records and update email addresses. The database connection is closed after each test module through the fixture.\n\nNow, add these tests to assess the functionality of creating users and updating their email addresses:\n\nThe function checks the process of creating new user entries by adding users with specific usernames and email addresses, then confirming their existence in the database with the correct email.\n\nOn the other hand, the function tests the ability to update user email addresses. It involves initially creating a user entry, changing the user's email, and verifying the update by querying the database for the new email.\n\nRun the tests using the command:\n\nThe output will look similar to the following:\n\nWith the basics of fixtures covered, let's close out this article by exploring some useful Pytest plugins.\n\nPytest offers a long list of plugins to enhance its capabilities, ranging from integrating with frameworks like Django and Flask to providing coverage reports.\n\nFor example, the plugin can enforce timeouts on tests, helping to help with identifying slow tests that could run indefinitely.\n\nConsider the following example:\n\nThe marker from the plugin sets the maximum allowable execution time for the test function. In this example, the first test should pass because it completes within the 5-second limit, while the second test should fail because it deliberately takes 2 seconds, exceeding the 1-second limit.\n\nThe calls are placeholders for the logic you want to test. Replace them with the functions or code segments you need to time-constrain.\n\nBefore running the tests, ensure to install the plugin first:\n\nAfterwards, execute the tests through the command below:\n\nThe output will show one test passing and the other failing due to the timeout. You'll see the \"Timeout >1.00s\" message in the captured stdout.\n\nIt's also possible to set global timeouts for all tests using the flag, a configuration option , or the environmental variable:\n\nThere are many other useful plugins for Pytest, including:\n‚Ä¢ : Modifies the default Pytest interface to a more visually appealing one.\n\nEnsure to check out the plugins page for the full list.\n\nThis article provided a comprehensive walkthrough of many Pytest features, including parameterization, fixtures, plugins, and more. A future article will delve into more advanced techniques to further elevate your testing skills.\n\nTo continue learning about Pytest, check out the official documentation . You'll find extensive resources to deepen your understanding and proficiency with Pytest.\n\nThanks for reading, and happy testing!"
    },
    {
        "link": "https://pytest-xdist.readthedocs.io",
        "document": "The pytest-xdist plugin extends pytest with new test execution modes, the most used being distributing tests across multiple CPUs to speed up test execution:\n\nWith this call, pytest will spawn a number of workers processes equal to the number of available CPUs, and distribute the tests randomly across them."
    }
]