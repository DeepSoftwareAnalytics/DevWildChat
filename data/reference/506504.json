[
    {
        "link": "https://medium.com/towards-data-science/chinese-natural-language-pre-processing-an-introduction-995d16c2705f",
        "document": "Text normalization is a method for standardizing text to prepare it for the tokenization, vectorization and classification steps. With english, the first step would be to convert all text to lowercase. Because Chinese characters are not capitalized to begin with, there’s no need for that data cleaning step.\n\nNext comes stemming or lemmatization. Compared to English, there is also no concept of a stem in Chinese. Therefore, there is no need to perform this step either! So far, it seems like that preprocessing Chinese text data requires less steps than English text data; making the process (surprisingly) a little easier.\n\nIt’s worth noting, however, that there’s a concept similar to stems in this language, and they’re called Radicals. Radicals are basically the building blocks of Chinese characters. All Chinese characters are made up of a finite number of components which are put together in different orders and combinations. Radicals are usually the leftmost part of the character. There are around 200 radicals in Chinese, and they are used to index and categorize characters.\n\nFor instance, the radicals for and come together for the character . This character uses the phonetic sound for but the gender indicator of .\n\nTherefore, procedures like stemming and lemmatization are not useful for Chinese text data because seperating the radicals would change the word’s meaning entirely.\n\nAnother difference is that Chinese handles plurality much differently. A basic example is that instead of a single word for , it’s split into two characters . We wouldn’t be able to get to the root of this word without completely changing the meaning.\n\nJust to drive this point even further, the final reason why we can’t preprocess Chinese in the same way is because it uses different grammatical tenses for past, present and future. In Chinese, characters are added onto the main verb. This could be considered adding to stem, but it’s a little more tricky because the additional character could appear at the front or end, depending on the context. Here is a quick, basic example.\n\nWith stemming and lemmatization in English, you can just remove the or to get to the root word and meaning. But with Chinese, it’s not that simple because like I mentioned before, there’s no concept of “root word” in Chinese.\n\nI could go on all day about the differences between English and Chinese. But the bottom line is that NLP is pretty different between the two languages, and these fundamental linguistic differences are very important for any computer scientist to understand.\n\nBefore I get into stop words, let’s tokenize some text first. Tokenizing breaks up text data into shorter pre-set strings, which help build context and meaning for the machine learning model. For this example, we will be looking a short excerpt from the textbook A New China.\n\nIt’s worth noting that Google Translate somewhat incorrectly translated the second sentence as “A Mr. Zhang from the school’s foreign affairs office picked us up.” This could be due the Chinese Room Theory, which argues that a computer cannot truly have an “understanding” of language, no matter how perfectly things are translated, and no matter how human-like the program behaves.\n\nFor the sake of simplicity, I’ll only tokenize the first sentence. We can use text segmentation module to tokenize Chinese. Fun fact, “结巴” in Chinese means “to stutter.” Here is a link to their github for more information.\n\nWith this output, the tokenization was somewhat successful. The translated words are as follows: , , , , , , , , , , and .\n\nThere are three things that you may immediately notice:\n• The output has tags next to each word\n• Some words were not fully separated\n\nThese “tags” label the part of speech. There are 24 part of speech tags and 4 proper name category labels in the package’s existing dictionary.\n\nand should have actually been considered a single phrase because that’s the full name of the airport. Also, could have been separated into individual characters; but that’s up for debate.\n\nTo fix the words that should and should not have been separated, we can manually create custom words within the package’s dictionary.\n\nThis line of code sets the name of the airport as a custom token, with the tag. We can repeat this code for each of the individual custom words we want to set, and then the tokenization should be able to pick that up if we run it again. Obviously this process can get tedious with a much larger text file, so it’s suggested that you find a well established dictionary beforehand, outside of what provides by default.\n\nIn NLP, stop words are “meaningless” words that make the data too noisy or ambiguous. In our example sentence, the stop words are , and . We could manually filter them out, but that’s also very tedious. Just like with English, there are pre-set lists of stop words out there. There are about 119 official stop words in Chinese, and they can be viewed on this website. Instead of manually removing them, could import the package for a full list of Chinese stop words. More information can be found here. And with this, we can easily create code to filter out any stop words in large text data.\n\nAfter all of this work, our hypothetical data is ready to be vectorized with TF-IDF Factorization and fed into a classification model! Obviously with real text data, we would be working with more than just one sentence. And this can present many other challenges.\n\nThis was just a rough introduction to preprocessing Chinese text data for NLP, and may not totally represent the process for more complex data.\n\nAnd finally, just like with English, further procedures can be done with NLP, such as sentiment analysis. Check out this blog about Chinese sentiment analysis using .\n• Blog Post on the Importance of Non-English NLP\n• Blog Post on Word Segmentation for Languages Without Spaces Between Words"
    },
    {
        "link": "https://ruolanlin.medium.com/comparing-pytorch-and-tensorflow-in-natural-language-processing-pipeline-part-1-9af01f012ff",
        "document": "The first part is about preprocessing text data in NLP.\n\n1 . When doing natural language processing, the first thing we need to do is read documents’ data from the file and clean the documents’ data. (such as stripping the unnecessary space, substituting the characters, or normalizing the punctuations by basic python code.) This step for PyTorch and TensorFlow is almost the same.\n\n2. The second thing to do is to tokenize the sentence ( for the English language is to use the split function but for the Chinese language it needs to use some specific library such as “Jieba” (https://github.com/fxsjy/jieba), build a vocabulary, which maps the words to the number and vise-verse, get the words’ frequency.\n\nIf use the module the library provided( ), consult the PyTorch document for preprocessing here:\n\nBut if we need to build the vocabulary by ourselves in PyTorch. We can do it in the following steps:\n\nWe need to write a Vocabulary class which has the following part:\n• Initialize function : initialize a list (store the index_to_token) and a dictionary(store the token_to_index), add the <unk> token to the token_to_index and index_to_token.\n\n2. Build the vocabulary from the text(by looping the sentences)\n\n3. Convert tokens to ids and convert ids to tokens and implement the methods __len__ and __getitem__ that the PyTorch dataset is required.\n\n4. Save the vocab and load the vocab method\n\nWe also need to customize a dataset class (inheriting from class ) and class (inheriting from DataLoder class ) that are fit to PyTorch framework.\n• A simple example of customizing a dataset. Sometimes the dataset customizing is more complex than the example here. It depends on the dataset and the task.\n\nA necessary function to implement to pass to is the collate_fn function. Sometimes the collate_fn function is more complex than the example here. It depends on the dataset and the task.\n\nThe completed code example in preprocessing text data in PyTorch can be referred to here. Thanks very much for the code authors.\n\nWe can consult the TensorFlow document for preprocessing here:\n\nUse the method of the tokenizer to build the vocab and use of the tokenizer to get the tokens sequence(word index sequence).\n\nUse pad_sequences to pad the sequences to a uniform length.\n\nThe completed code example in preprocessing text data in TensorFlow can be referred to here. Thanks very much for the code authors."
    },
    {
        "link": "https://labpresse.com/demystifying-pytorch-datasets-building-a-chinese-character-dataset",
        "document": "DataSets in PyTorch serve as a crucial bridge between raw data and the machine learning model during training. They encapsulate the logic needed to access, transform, and even augment the data, making the model’s training loop cleaner and more manageable. In this tutorial, we’ll shed light on what PyTorch DataSets are, how you can create one from the ground up, and how it can be seamlessly incorporated into your model’s training process.\n\nTo make this journey engaging and practical, we’ll embark on a unique project: constructing a DataSet that renders Chinese characters. The choice of Chinese characters is strategic: these characters encompass both simple and intricate features, mirroring the complexity often found in real-world data. Furthermore, the vast number of unique Chinese characters, compared to Latin letters, presents a greater challenge for classification models, making this an excellent toy problem. This simple, yet powerful example will illustrate the versatility of PyTorch DataSets and showcase their potential to handle even complex data types.\n\nWhat is a DataSet in PyTorch?\n\nA in PyTorch is an abstraction that represents a collection of data. It provides a standardized way to load, preprocess, and access the data in a unified manner, regardless of the type or structure of the original data source. This can include anything from images and text files to CSVs and databases. PyTorch Datasets are designed around two main methods: and . The method returns the number of items in the dataset, while the method allows indexed access to individual data items, returning both input data and the corresponding target. This design allows the PyTorch DataLoader to efficiently and conveniently fetch batches of data, facilitating the implementation of large-scale machine learning models that learn from examples in an iterative manner.\n\nIn this tutorial, we aim to construct a DataSet that generates random Chinese characters on-demand. To bring our Chinese Character DataSet to life, we’ll break down the process into several manageable steps. First, we’ll identify the Unicode range corresponding to Chinese characters. Next, we’ll calculate the appropriate font size to ensure our character fits within a specified image size. We’ll then render each character onto an image canvas using Python’s Imaging Library (PIL). Following this, we’ll convert our image into a PyTorch tensor, a format suitable for machine learning models. Finally, to add a layer of realism and improve the robustness of our training, we’ll post-process our tensor by introducing random distortions, noise, and normalization.\n\nTo bring our Chinese Character DataSet to life, we’ll be harnessing the power of several Python libraries. Let’s start by importing the necessary modules:\n• torch: This is the PyTorch library, which we’ll use to convert our images into tensors – the standard data format for deep learning models.\n• unicodedata: This will help us identify Chinese characters within the Unicode range.\n• matplotlib: We’ll use this popular plotting library to visualize our images.\n• PIL: Python’s Imaging Library (PIL) will be instrumental in rendering the characters onto an image canvas.\n• torchvision: Finally, torchvision’s transformation functions will enable us to introduce random distortions and noise to our images, helping simulate real-world imperfections.\n\nWith our libraries imported, we can now define our Chinese Character DataSet class. Named , this class inherits from PyTorch’s Dataset class. It is initiated with an optional image_size parameter, dictating the size of the image canvas for rendering our characters. By default, we set the image size to 64×64 pixels.\n\nThe initializer begins by generating a list of all Chinese characters. We use the unicodedata library to identify these characters within the Unicode range 4E00-9FFF. Following this, we establish several parameters for the class: the image size, the list and count of characters, and the font file’s path. Given that many fonts cannot render Chinese characters, we opt for the versatile Noto Sans font, which supports a broad spectrum of languages, including Chinese.\n\nThe len method is a crucial part of any DataSet in PyTorch. It’s responsible for reporting the number of items in the dataset. For our Chinese Character DataSet, the len method returns the total count of unique Chinese characters we’ve identified:\n\nBy implementing this method, we enable PyTorch’s data loader to correctly iterate over our dataset during the training process, handling data fetching and batching.\n\nThe heart of the DataSet class, the method, is responsible for loading or creating data. It should return both the input data (in this case, an image of a Chinese character) and the corresponding target (the character itself).\n\nTo accomplish this, we need to calculate the appropriate font size to render the character onto an image canvas properly. We define an internal helper function, , to handle this task. In Python, it’s common to prefix an underscore to methods that are intended for internal use within a class.\n\nThe function calculates the maximum possible font size given the image size, then uses the library to measure the rendered text’s size. Finally, it finds an optimal font size that ensures the text fits within the image dimensions.\n\nWith the font size available, we can then render the character onto an image. We’ll define another internal helper function, , for this purpose:\n\nThe function creates a new image canvas, calculates the appropriate font size and the text’s position, and then renders the text onto the canvas. The image is then converted to a PyTorch tensor.\n\nNow we are equipped to define the main method. This function will not only return our created image and the target character but also incorporate an optional transformation parameter. This can be used for applying data augmentation to the image, enhancing the robustness of our model.\n\nHere, retrieves the character to be rendered and creates an image of it. If the transform parameter is set to True, the method will apply data augmentation techniques to the image. Finally, it adds random noise and normalizes the image before returning it.\n\nNow that we have our dataset class defined, it’s essential to show how it can be utilized. In this section, we demonstrate how to create a dataset instance and display a series of images from it.\n\nNote: The code block below is wrapped in an construct. This standard Python practice ensures the code is only executed when the script is run directly, not when it is imported as a module by another script.\n\nThis code creates an instance of the ChineseCharacters dataset. It then uses matplotlib to display a series of 100 images drawn from this dataset in real-time. The loop accesses the images, clears the current plot (if any), displays the new image, and then briefly pauses before moving on to the next one.\n\nBelow is an example of the output:\n\nThis demonstration shows that our dataset correctly generates and visualizes Chinese characters, confirming that our code functions as expected.\n\nUsing the DataSet with a DataLoader\n\nOften times, DataSets are paired with DataLoaders. The DataLoader class is responsible for managing various aspects of the data loading process, including batching, shuffling, and multiprocessing. In this section, we’ll briefly demonstrate how to use our dataset with a DataLoader.\n\nHere is an example script that creates a DataLoader instance and uses it to iterate through the dataset:\n\nIn this loop, each batch will contain a batch of 32 images from the ChineseCharacters DataSet. This allows you to load and process data in manageable chunks, making the DataLoader an invaluable tool in deep learning workflows.\n\nThroughout this tutorial, we have delved into the concept of a DataSet, elucidating its importance and functionality in the realm of deep learning. We didn’t stop at theoretical understanding; instead, we rolled up our sleeves and developed a hands-on ‘toy’ dataset from scratch. This DataSet, with its emphasis on generating Chinese characters, can serve as an insightful starting point for harnessing these tools in your deep learning endeavors.\n\nBut, the knowledge gained from this exercise transcends this specific application. The principles and techniques acquired here equip you to build tailored DataSets for a wide array of applications, enabling you to pave your unique path in the diverse world of deep learning.\n\nWe genuinely hope this tutorial was helpful, and we invite you to share your experiences, thoughts, and questions in the comments below. Your feedback not only helps us improve, but also contributes to a vibrant learning community. Happy experimenting!"
    },
    {
        "link": "https://kaggle.com/code/mashimo/chinese-mnist-with-simple-pytorch-cnn-step-by-step/notebook",
        "document": ""
    },
    {
        "link": "https://linkedin.com/advice/1/how-can-you-use-pytorch-natural-language-processing-dl21e",
        "document": ""
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/ddp_series_multigpu.html",
        "document": "Follow along with the video below or on youtube.\n\nIn the previous tutorial, we got a high-level overview of how DDP works; now we see how to use DDP in code. In this tutorial, we start with a single-GPU training script and migrate that to running it on 4 GPUs on a single node. Along the way, we will talk through important concepts in distributed training while implementing them in our code.\n\nIf your model contains any layers, it needs to be converted to to sync the running stats of layers across replicas. Use the helper function torch.nn.SyncBatchNorm.convert_sync_batchnorm(model) to convert all layers in the model to .\n\nThese are the changes you typically make to a single-GPU training script to enable DDP.\n• None DistributedSampler chunks the input data across all distributed processes.\n• None sampler, and provides an iterable over the given dataset.\n• None Each process will receive an input batch of 32 samples; the effective batch size is , or 128 when using 4 GPUs. # Use the Distributed Sampler here.\n• None Calling the method on the at the beginning of each epoch is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be used in each epoch. # call this additional line at every epoch\n• None We only need to save model checkpoints from one process. Without this condition, each process would save its copy of the identical mode. Read more on saving and loading models with DDP here Collective calls are functions that run on all the distributed processes, and they are used to gather certain states or values to a specific process. Collective calls require all ranks to run the collective code. In this example, should not have any collective calls because it is only run on the process. If you need to make any collective calls, it should be before the check."
    },
    {
        "link": "https://pytorch.org/tutorials/intermediate/ddp_tutorial.html",
        "document": "DistributedDataParallel (DDP) is a powerful module in PyTorch that allows you to parallelize your model across multiple machines, making it perfect for large-scale deep learning applications. To use DDP, you’ll need to spawn multiple processes and create a single instance of DDP per process.\n\nBut how does it work? DDP uses collective communications from the torch.distributed package to synchronize gradients and buffers across all processes. This means that each process will have its own copy of the model, but they’ll all work together to train the model as if it were on a single machine.\n\nTo make this happen, DDP registers an autograd hook for each parameter in the model. When the backward pass is run, this hook fires and triggers gradient synchronization across all processes. This ensures that each process has the same gradients, which are then used to update the model.\n\nFor more information on how DDP works and how to use it effectively, be sure to check out the DDP design note. With DDP, you can train your models faster and more efficiently than ever before!\n\nThe recommended way to use DDP is to spawn one process for each model replica. The model replica can span multiple devices. DDP processes can be placed on the same machine or across machines. Note that GPU devices cannot be shared across DDP processes (i.e. one GPU for one DDP process).\n\nIn this tutorial, we’ll start with a basic DDP use case and then demonstrate more advanced use cases, including checkpointing models and combining DDP with model parallel.\n\nTo create a DDP module, you must first set up process groups properly. More details can be found in Writing Distributed Applications with PyTorch. # On Windows platform, the torch.distributed package only # to a local file. Example as follow: # For TcpStore, same way as on Linux. Now, let’s create a toy module, wrap it with DDP, and feed it some dummy input data. Please note, as DDP broadcasts model states from rank 0 process to all other processes in the DDP constructor, you do not need to worry about different DDP processes starting from different initial model parameter values. # create model and move it to GPU with id rank As you can see, DDP wraps lower-level distributed communication details and provides a clean API as if it were a local model. Gradient synchronization communications take place during the backward pass and overlap with the backward computation. When the returns, already contains the synchronized gradient tensor. For basic use cases, DDP only requires a few more lines of code to set up the process group. When applying DDP to more advanced use cases, some caveats require caution.\n\nIt’s common to use and to checkpoint modules during training and recover from checkpoints. See SAVING AND LOADING MODELS for more details. When using DDP, one optimization is to save the model in only one process and then load it on all processes, reducing write overhead. This works because all processes start from the same parameters and gradients are synchronized in backward passes, and hence optimizers should keep setting parameters to the same values. If you use this optimization (i.e. save on one process but restore on all), make sure no process starts loading before the saving is finished. Additionally, when loading the module, you need to provide an appropriate argument to prevent processes from stepping into others’ devices. If is missing, will first load the module to CPU and then copy each parameter to where it was saved, which would result in all processes on the same machine using the same set of devices. For more advanced failure recovery and elasticity support, please refer to TorchElastic. # All processes should see same parameters as they all start from same # random parameters and gradients are synchronized in backward passes. # Therefore, saving it in one process is sufficient. # Use a barrier() to make sure that process 1 loads the model after process # Not necessary to use a dist.barrier() to guard the file deletion below # as the AllReduce ops in the backward pass of DDP already served as\n\nDDP also works with multi-GPU models. DDP wrapping multi-GPU models is especially helpful when training large models with a huge amount of data. When passing a multi-GPU model to DDP, and must NOT be set. Input and output data will be placed in proper devices by either the application or the model method. \"Running DDP with model parallel example on rank # setup mp_model and devices for this process # outputs will be on dev1 \"Finished running DDP with model parallel example on rank \"Requires at least 2 GPUs to run, but got\n\nWe can leverage PyTorch Elastic to simplify the DDP code and initialize the job more easily. Let’s still use the Toymodel example and create a file named . # create model and move it to GPU with id rank One can then run a torch elastic/torchrun command on all nodes to initialize the DDP job created above: In the example above, we are running the DDP script on two hosts and we run with 8 processes on each host. That is, we are running this job on 16 GPUs. Note that must be the same across all nodes. Here will launch 8 processes and invoke on each process on the node it is launched on, but user also needs to apply cluster management tools like slurm to actually run this command on 2 nodes. For example, on a SLURM enabled cluster, we can write a script to run the command above and set as: Then we can just run this script using the SLURM command: . This is just an example; you can choose your own cluster scheduling tools to initiate the job. For more information about Elastic run, please see the quick start document."
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/ddp_series_theory.html",
        "document": "Follow along with the video below or on youtube.\n\nThis tutorial is a gentle introduction to PyTorch DistributedDataParallel (DDP) which enables data parallel training in PyTorch. Data parallelism is a way to process multiple data batches across multiple devices simultaneously to achieve better performance. In PyTorch, the DistributedSampler ensures each device gets a non-overlapping input batch. The model is replicated on all the devices; each replica calculates gradients and simultaneously synchronizes with the others using the ring all-reduce algorithm.\n\nThis illustrative tutorial provides a more in-depth python view of the mechanics of DDP.\n\nWhy you should prefer DDP over (DP)¶ DataParallel is an older approach to data parallelism. DP is trivially simple (with just one extra line of code) but it is much less performant. DDP improves upon the architecture in a few ways: More overhead; model is replicated and destroyed at each forward pass Model is replicated only once Slower; uses multithreading on a single process and runs into Global Interpreter Lock (GIL) contention Faster (no GIL contention) because it uses multiprocessing"
    },
    {
        "link": "https://medium.com/codex/a-comprehensive-tutorial-to-pytorch-distributeddataparallel-1f4b42bb1b51",
        "document": "First we must understand several terms used in distributed training:\n• process group: if you want to train/test the model over K gpus, then the K process forms a group, which is supported by a backend (pytorch managed that for you, according to the documentation, nccl is the most recommended backend);\n• rank: within the process group, each process is identified by its rank, from 0 to K-1;\n• world size: the number of processes in the group i.e. gpu number——K.\n\nPytorch provides two settings for distributed training: torch.nn.DataParallel (DP) and torch.nn.parallel.DistributedDataParallel (DDP), where the latter is officially recommended. In short, DDP is faster, more flexible than DP. The fundamental thing DDP does is to copy the model to multiple gpus, gather the gradients from them, average the gradients to update the model, then synchronize the model over all K processes. We can also gather/scatter tensors/objects other than gradients by .\n\nIn case the model can fit on one gpu (it can be trained on one gpu with ) and we want to train/test it on K gpus, the best practice of DDP is to copy the model onto the K gpus (the DDP class automatically does this for you) and split the dataloader to K non-overlapping groups to feed into K models respectively.\n\nNow, things are clear to us. We have to do the following things:\n• setup the process group, which is three lines of code and needs no modification;\n• split the dataloader to each process in the group, which can be easily achieved by torch.utils.data.DistributedSampler or any customized sampler;\n• wrap our model with DDP, which is one line of code and barely needs modification;\n• train/test our model, which is the same as is on 1 gpu;\n• clean up the process groups (like free in C), which is one line of code.\n• optional: gather extra data among processes (possibly needed for distributed testing), which is basically one line of code;\n\nVery easy, right? In fact it is. Let’s do it step by step.\n\nHere it is, no extra steps.\n\nWe can easily split our dataloader by torch.utils.data.distributed.DistributedSampler. The sampler returns a iterator over indices, which are fed into dataloader to bachify.\n\nThe DistributedSampler split the total indices of the dataset into world_size parts, and evenly distributes them to the dataloader in each process without duplication.\n\nSuppose K=3, and the length of dataset is 10. We must understand that DistributedSampler imposes even partition of indices.\n• If we set when defining DistributedSampler, it will automatically pad. For example, it splits indices [0,1,2,3,4,5,6,7,8,9] to [0,3,6,9] when rank=1, [0,4,7,0] when rank=2, and [2,5,8,0] when rank=3. As you can see, such padding may cause issues because the padded 0 is a data record.\n• Otherwise, it will strip off the trailing elements. For example, it splits the indices to [0,3,6] at rank=1, [1,4,7] at rank=2, and [2,5,8] at rank=3. In this case, it tailored 9 to make the indice number divisible by world_size.\n\nIt is very simple to customize our Sampler. We only need to create a class, then define its and function. Refer to the official documentation for more details.\n\nBTW, you’d better set the when distributed training, because creating extra threads in the children processes may be problemistic. I also found avoids many horrible bugs, maybe such things are machine-specific, please email me if you readers explored more details.\n\nWe should first move our model to the specific gpu (recall that one model replica resides in one gpu), then we wrap it with DDP class. The following function takes in an argument rank, which we will introduce soon. For now, we just keep in mind rank equals the gpu id.\n\nThere are a few tricky things here:\n• When we want to access some customized attributes of the DDP wrapped model, we must reference . That is to say, our model instance is saved as a attribute of the DDP model. If we assign some attributes other than built-in properties or functions, we must access them by .\n• When we save the DDP model, our would add a module prefix to all parameters.\n• Consequently, if we want to load a DDP saved model to a non-DDP model, we have to manually strip the extra prefix. I provide my code below:\n\nThis part is the key to implementing DDP. First we need to know the basis of multi-processing: all children processes together with the parent process run the same code.\n\nIn PyTorch, torch.multiprocessing provides convenient ways to create parallel processes. As the official documentation says,\n\nSo, using is a good choice.\n\nIn our script, we should define a train/test function before spawning it to parallel processes:\n\n# prepare the dataloader\n\n dataloader = prepare(rank, world_size)\n\n \n\n # instantiate the model(it's your own model) and move it to the right device\n\n model = Your_Model().to(rank)\n\n \n\n # wrap the model with DDP\n\n # device_ids tell DDP where is your model\n\n # output_device tells DDP where to output, in our case, it is rank\n\n # find_unused_parameters=True instructs DDP to find unused output of the forward() function of any module in the model #################### The above is defined previously\n\n \n\n optimizer = Your_Optimizer()\n\n loss_fn = Your_Loss() for epoch in epochs:\n\n # if we are using DistributedSampler, we have to tell it which epoch this is\n\n dataloader.sampler.set_epoch(epoch) \n\n \n\n for step, x in enumerate(dataloader):\n\n optimizer.zero_grad(set_to_none=True)\n\n \n\n pred = model(x)\n\n label = x['label']\n\n \n\n loss = loss_fn(pred, label)\n\n loss.backward()\n\n optimizer.step()\n\nThis function is run in every parallel process. We now need to call it by method. In our script, we write:\n\nRemember the first argument of is rank? It is automatically passed to each process by , we don’t need to pass it explicitly. is the master node by default. The rank ranges from 0 to K-1 (2 in our case).\n\nThe last line of function is the clean up function, which is:\n\nBravo! We have completed the basic workflow of Distributed training/tesing!\n\nSometimes we need to collect some data from all processes, such as the testing result. We can easily gather tensors by and objects by .\n\nWithout loss of generality, I assume we want to collect python objects. The only constraint of the object is it must be serializable, which is basically everything in python. One should always assign before using . And, if we want to store a tensor in the object, it must locate at the .\n\nThe most confusing thing to me is when to use . As the documentation says, it synchronizes processes. In other words, it blocks processes until all of them reaches the same line of code: . I summarize its usage as follows:\n• we do not need it when training, since DDP automatically does it for us (in );\n• we do not need it when gathering data, since does it for us;\n• we need it when enforcing execution order of codes, say one process loads the model that another process saves (I can hardly imagine this scenario is needed)."
    },
    {
        "link": "https://cerfacs.fr/coop/pytorch-multi-gpu",
        "document": "Illustration of Distributed Data Parallel (DDP) all-reduce operation in PyTorch. By NVIDIA\n\nTraining deep learning models on multiple GPUs can significantly speed up your training process, especially for large-scale datasets or complex architectures. PyTorch offers flexible and powerful support for multi-GPU training through both data parallelism and distributed data parallelism. This guide will walk you through setting up multi-GPU training in PyTorch and provide useful tips to enhance performance.\n\nThe simplest way to train a model across multiple GPUs is to use . In this approach, the model is replicated across all available GPUs, and all processes are managed by the first GPU (also called the master process). This method splits the input across the GPUs, computes gradients in parallel, and averages them before updating the model parameters on the master process. After the update, the master process broadcasts the updated parameters to all other GPUs.\n\nTip: is not recommended for the following reasons: - Overhead: While is easy to use, it has some communication overhead due to waiting for all GPUs to finish the backpropagation, gathering gradients, and broadcasting the updated parameters. For better performance, especially when scaling to multiple nodes, use (DDP) instead. - Memory: The memory usage on the master GPU is higher than on the other GPUs, as it gathers all the gradients from the other GPUs. Thus if the you already have memory issues on a single GPU, will make it worse.\n• Scaling: Keep in mind that averages the gradients across GPUs after backpropagation. Make sure to scale the learning rate accordingly (multiply by the number of GPUs) to maintain the same effective learning rate. The same applies for the batch size, the provided batch size to data loader is divided over the GPUs.\n\nFor better performance, PyTorch provides (DDP), which is more efficient for multi-GPU training, especially for multi-node setups. Indeed, when using DDP, the training code is executed on each GPU separately, and each GPU communicates directly with the other, and only when necessary, reducing communication overhead. In the DDP approach, the role of the master process is heavily reduced, and each GPU is responsible for its own forward and backward passes, as well as parameter updates. After the forward pass, starting the backward pass, each GPU starts to send its gradients to all other GPUs, and each GPU receives the sum of all gradients from all other GPUs. This is called all-reduce operation *. After that, each GPU have exactly the same gradients and updates the parameters of its own copy of the model. - * Reduce: A common operation in distributed computing where the results of a computation are aggregated across multiple processes. All-reduce means that all processes calls for a reduce operation to receive the result from all other processes.\n• Important: This code should be executed on each separately.\n• Backend: For managing communication between GPUs during multi- training, always prefer the ( Collective Communication Library) backend over (a library developed by Facebook) for -based operations. is optimized for GPUs and ensures better performance and scalability, and Gloo is more suitable for -based operations. For more details about the compatible operations with each backend, refer to the PyTorch documentation.\n\nTo launch multi-GPU training with DDP, you can use the following command (on each GPU separately):\n• : Specifies the number of GPUs to use per node.\n\nThis way is the old way, but still used, to launch multi-GPU training. The new way is to use :\n\nHere you don’t need to specify the as it is automatically set by . And the script is changed to:\n\nYou can also use to launch multi-GPU training on a Slurm cluster. Inside a bash script replace with :\n\nHowever, the environment variables set by have different names than those set by . You can use the following code to get the and :\n\nTip: Use for Data Loaders when training with multiple GPUs using DDP, ensure that each GPU receives a unique mini-batch. This is achieved by using the :\n• Important: Call at the beginning of each epoch to ensure proper shuffling.\n\nTip: Pin memory and pre-fetch data for faster data loading. Use and increase the in your DataLoader:\n• : Copies data to pinned memory, which is faster to transfer to the .\n• : Sets the number of subprocesses to load data in parallel, avoiding I/O bottlenecks. It’s recommended to set this to the number of cores available, but you can experiment with different values. Generally, a value between 2 and 4 is a good starting point. A high number of workers will increase memory usage.\n• : Sets the number of batches that each worker will prepare in advance.\n\nIf a given mini-batch of data is too large to fit into the GPU memory, you can reduce the batch size and update model parameters only after processing a set of smaller mini-batches and summing the gradients across them. This is called gradient accumulation.\n• : Automatic Mixed Precision ( ) can be combined with gradient accumulation to further optimize memory usage and training speed.\n• Normalization: Ensure loss normalization by dividing by to maintain gradient scale.\n• Zero Gradients: Always reset gradients after the specified number of accumulation steps to prevent gradient accumulation across updates. You can also use for less memory usage.\n• Memory Efficiency: When working with large models or datasets, use to monitor and optimize memory usage. Tools like for mixed precision training can also help reduce memory footprint.\n\nMixed precision training allows models to use lower precision (e.g., float16) for forward and backward passes, reducing memory consumption and speeding up training.\n\nTip: Use AMP for Faster Training - AMP: Automatic Mixed Precision (AMP) can significantly speed up training by using lower precision (e.g., float16) for forward and backward passes, reducing memory consumption and improving throughput. - GradScaler: Automatically scales gradients to prevent underflow during backpropagation, especially useful when using gradient accumulation. - Zero Gradients: Always reset gradients after each update to avoid gradient accumulation across updates.\n\nPyTorch’s built-in profiler can help identify performance bottlenecks in your training pipeline:\n• Use TensorBoard to visualize profiling results and optimize accordingly.\n\nTip: Use NVIDIA Nsight Systems for GPU Profiling For more detailed GPU profiling, consider using NVIDIA Nsight Systems to analyze GPU utilization, memory usage, and kernel performance. Refer to the NVIDIA Nsight Systems documentation and to this post for more information.\n\nPyTorch Lightning is a lightweight PyTorch wrapper that simplifies the training process and provides built-in support for multi-GPU training, mixed precision, and distributed training. Here is an example of how to train a model using PyTorch Lightning:\n\nPyTorch Lightning enables single/multi-GPU as well as multi-node training using a single codebase. The user only needs to set the configuration accordingly:\n• : Specify the number of GPUs to use. It can be set to for automatic detection.\n• : Use Distributed Data Parallel ( ) for multi- training (more strategies).\n• : Specify the number of nodes for multi-node training.\n\nIf you are using Slurm to manage your training jobs, you can use the following script to launch multi-GPU training:\n• Important: Your main script should be able to handle the and arguments (using argparse, refer to this post).\n\nKeep it clean and efficient!\n\nLike this post? Share on: Twitter ❄ Facebook ❄ Email"
    }
]