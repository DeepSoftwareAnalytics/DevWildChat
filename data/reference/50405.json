[
    {
        "link": "https://documentation.sas.com/doc/en/statug/latest/statug_surveyselect_syntax01.htm",
        "document": ""
    },
    {
        "link": "http://documentation.sas.com/doc/ko/pgmsascdc/v_051/statug/statug_surveyselect_syntax01.htm",
        "document": ""
    },
    {
        "link": "https://stats.oarc.ucla.edu/sas/seminars/sas-survey",
        "document": "The purpose of this workshop is to explore some issues in the analysis of survey data using SAS 9.44 and SAS/Stat 15.2. Most of code shown in this seminar will work in earlier versions of SAS and SAS/Stat. To find out what version of SAS and SAS/Stat you are running, open SAS and look at the information in the log file.\n\nThere are seven survey procedures.\n\nproc surveyselect: This procedure can be used to select a sample from a dataset.\n\nproc surveyimpute: This procedure can be used to do single imputations on a survey dataset.\n\nproc surveymeans: This procedure can be used to obtain weighted descriptive statistics for continuous variables. This procedure can produce graphs.\n\nproc surveyfreq: This procedure can be used to run weighted one-way and multi-way crosstabulations. This procedure can produce graphs.\n\nproc surveyregress: This procedure can be used to run weighted OLS regressions.\n\nproc surveylogistic: This procedure can be used to run weighted logistic, ordinal, multinomial and probit regressions.\n\nproc surveyphreg: This procedure can be used to run weighted proportional hazards regression.\n\nWe will also briefly discuss proc glimmix.\n\nproc glimmix: This procedure will allow for sampling weights, so it can be used to run weighted multilevel models. This procedure does not have a strata, cluster or a domain statement, and it does not allow for replicate weights. It requires that a sampling weight be specified at each level of the model.\n\nWhy do we need survey data analysis software?\n\nRegular statistical software (that is not designed for survey data) analyzes data as if the data were collected using simple random sampling. For experimental and quasi-experimental designs, this is exactly what we want. However, very few surveys use a simple random sample to collect data. Not only is it nearly impossible to do so, but it is not as efficient (either financially and statistically) as other sampling methods. When any sampling method other than simple random sampling is used, we usually need to use survey data analysis software to take into account the differences between the design that was used to collect the data and simple random sampling. This is because the sampling design affects both the calculation of the point estimates and the standard errors of those estimates. If you ignore the sampling design, e.g., if you assume simple random sampling when another type of sampling design was used, both the point estimates and their standard errors will likely be calculated incorrectly. The sampling weight will affect the calculation of the point estimate, and the stratification and/or clustering will affect the calculation of the standard errors. Ignoring the clustering will likely lead to standard errors that are underestimated, possibly leading to results that seem to be statistically significant, when in fact, they are not. The difference in point estimates and standard errors obtained using non-survey software and survey software with the design properly specified will vary from data set to data set, and even between analyses using the same data set. While it may be possible to get reasonably accurate results using non-survey software, there is no practical way to know beforehand how far off the results from non-survey software will be.\n\nMost people do not conduct their own surveys. Rather, they use survey data that some agency or company collected and made available to the public. The documentation must be read carefully to find out what kind of sampling design was used to collect the data. This is very important because many of the estimates and standard errors are calculated differently for the different sampling designs. Hence, if you mis-specify the sampling design, the point estimates and standard errors will likely be wrong.\n\nBelow are some common features of many sampling designs.\n\n: There are several types of weights that can be associated with a survey. Perhaps the most common is the sampling weight. A sampling weight is a probability weight that has had one or more adjustments made to it. Both a sampling weight and a probability weight are used to weight the sample back to the population from which the sample was drawn. By definition, a probability weight is the inverse of the probability of being included in the sample due to the sampling design (except for a certainty PSU, see below). The probability weight, called a pweight in Stata, is calculated as N/n, where N = the number of elements in the population and n = the number of elements in the sample. For example, if a population has 10 elements and 3 are sampled at random with replacement, then the probability weight would be 10/3 = 3.33. In a two-stage design, the probability weight is calculated as f f , which means that the inverse of the sampling fraction for the first stage is multiplied by the inverse of the sampling fraction for the second stage. Under many sampling plans, the sum of the probability weights will equal the population total.\n\nWhile many textbooks will end their discussion of probability weights here, this definition does not fully describe the sampling weights that are included with actual survey data sets. Rather, the sampling weight, which is sometimes called a “final weight,” starts with the inverse of the sampling fraction, but then incorporates several other values, such as corrections for unit non-response, errors in the sampling frame (sometimes called non-coverage), and poststratification. Because these other values are included in the probability weight that is included with the data set, it is often inadvisable to modify the sampling weights, such as trying to standardize them for a particular variable, e.g., age.\n\n: This is the primary sampling unit. This is the first unit that is sampled in the design. For example, school districts from California may be sampled and then schools within districts may be sampled. The school district would be the PSU. If states from the US were sampled, and then school districts from within each state, and then schools from within each district, then states would be the PSU. One does not need to use the same sampling method at all levels of sampling. For example, probability-proportional-to-size sampling may be used at level 1 (to select states), while cluster sampling is used at level 2 (to select school districts). In the case of a simple random sample, the PSUs and the elementary units are the same. In general, accounting for the clustering in the data (i.e., using the PSUs), will increase the standard errors of the point estimates. Conversely, ignoring the PSUs will tend to yield standard errors that are too small, leading to false positives when doing significance tests.\n\n: Stratification is a method of breaking up the population into different groups, often by demographic variables such as gender, race or SES. Each element in the population must belong to one, and only one, strata. Once the strata have been defined, samples are taken from each stratum as if it were independent of all of the other strata. For example, if a sample is to be stratified on gender, men and women would be sampled independently of one another. This means that the probability weights for men will likely be different from the probability weights for the women. In most cases, you need to have two or more PSUs in each stratum. The purpose of stratification is to reduce the standard error of the estimates, and stratification works most effectively when the variance of the dependent variable is smaller within the strata than in the sample as a whole.\n\n: This is the finite population correction. This is used when the sampling fraction (the number of elements or respondents sampled relative to the population) becomes large. The FPC is used in the calculation of the standard error of the estimate. If the value of the FPC is close to 1, it will have little impact and can be safely ignored. In some survey data analysis programs, such as SUDAAN, this information will be needed if you specify that the data were collected without replacement (see below for a definition of “without replacement”). The formula for calculating the FPC is ((N-n)/(N-1))1/2, where N is the number of elements in the population and n is the number of elements in the sample. To see the impact of the FPC for samples of various proportions, suppose that you had a population of 10,000 elements.\n\n: Replicate weights are a series of weight variables that are used to correct the standard errors for the sampling plan. They serve the same function as the PSU and strata variables (which are used a Taylor series linearization) to correct the standard errors of the estimates for the sampling design. Many public use data sets are now being released with replicate weights instead of PSUs and strata in an effort to more securely protect the identity of the respondents. In theory, the same standard errors will be obtained using either the PSU and strata or the replicate weights. There are different ways of creating replicate weights; the method used is determined by the sampling plan. The most common are balanced repeated and jackknife replicate weights. You will need to read the documentation for the survey data set carefully to learn what type of replicate weight is included in the data set; specifying the wrong type of replicate weight will likely lead to incorrect standard errors. For more information on replicate weights, please see Stata Library: Replicate Weights and Appendix D of the WesVar Manual by Westat, Inc. Several statistical packages, including Stata, SAS, SUDAAN, WesVar and R, allow the use of replicate weights.\n\nConsequences of not using the design elements\n\nSampling design elements include the sampling weights, post-stratification weights (if provided), PSUs, strata, and replicate weights. Rarely are all of these elements included in a single public-use data set. However, ignoring the design elements that are included can often lead to inaccurate point estimates and/or inaccurate standard errors.\n\nSampling with and without replacement\n\nMost samples collected in the real world are collected “without replacement”. This means that once a respondent has been selected to be in the sample and has participated in the survey, that particular respondent cannot be selected again to be in the sample. Many of the calculations change depending on if a sample is collected with or without replacement. Hence, programs like SUDAAN request that you specify if a survey sampling design was implemented with our without replacement, and an FPC is used if sampling without replacement is used, even if the value of the FPC is very close to one.\n\nFor the examples in this workshop, we will use the data set from NHANES 2011-2012. The data set and documentation can be downloaded from the NHANES web site. The data files can be downloaded as SAS.xpt files.\n\nThe first step in analyzing any survey data set is to read the documentation. With many of the public use data sets, the documentation can be quite extensive and sometimes even intimidating. Instead of trying to read the documentation “cover to cover”, there are some parts you will want to focus on. First, read the Introduction. This is usually an “easy read” and will orient you to the survey. There is usually a section or chapter called something like “Sample Design and Analysis Guidelines”, “Variance Estimation”, etc. This is the part that tells you about the design elements included with the survey and how to use them. Some even give example code. If multiple sampling weights have been included in the data set, there will be some instruction about when to use which one. If there is a section or chapter on missing data or imputation, please read that. This will tell you how missing data were handled. You should also read any documentation regarding the specific variables that you intend to use. As we will see little later on, we will need to look at the documentation to get the value labels for the variables. This is especially important because some of the values are actually missing data codes, and you need to do something so that SAS doesn’t treat those as valid values (or you will get some very “interesting” means, totals, etc.).\n\nWe will use about a dozen different variables in the examples in this workshop. Below is a brief summary of them. Some of the variables have been recoded to be binary variables (values of 2 recoded to a value of 0). The count of missing observations includes values truly missing as well as refused and don’t know.\n\nridageyr – Age in years at exam – recoded; range of values: 0 – 79 are actual values, 80 = 80+ years of age\n\npad630 – How much time do you spend doing moderate-intensity activities on a type work day?; range of values: 10-960 (minutes), 7053 missing observations\n\nhsq496 – During the past 30 days, for about how many days have you felt worried, tense or anxious?; range of values: 0-30; 3073 missing observations\n\nfemale – Recode of the variable riagendr; 0 = male, 1 = female; no missing observations\n\ndmdborn4 – Country of birth; 1 = born in the United States, 0 = otherwise; 5 missing observations\n\ndmdeduc2 – Education level of adults aged 20+ years; 1 = less than 9th grade, 2 = 9-11th grade, 3 = high school graduate, GED or equivalent, 4 = some college or AA degree, 5 = college graduate or above; 4201 missing observations\n\npad675 – How much time do you spend doing moderate-intensity sports, fitness, or recreation activities on a typical day?; range of values: 10-600 (minutes); 6220 missing observations\n\nhsq571 – During the past 12 months, have you donated blood?; 0 = no, 1 = yes; 3673 missing observations\n\npad680 – How much time do you usually spend sitting on a typical day?; range of values: 0-1380 (minutes); 2365 missing observations\n\npaq665 – Do you do any moderate-intensity sports, fitness or recreational activities that cause a small increase in breathing or heart rate at least 10 minutes continually?; 0 = no, 1 = yes; 2329 missing observations\n\nhsd010 – Would you say that your general health is…; 1 = excellent, 2 = very good, 3 = good, 4 = fair, 5 = poor; 3064 missing observations\n\nhsq470 – number of days in the last 30 days that physical health is not good; range of values: 0 – 30 (days), 3075 missing observations\n\nhsq480 – number of days in the last 30 days that mental health is not good; range of values: 0 – 30 (days), 3073 missing observations\n\nThere are three other variables that we should identify. One is the sampling weight variable. It is wtint2yr. The cluster variable is sdmvpsu and the stratification variable is sdmvstra. There are 14 strata and 31 clusters in this dataset. Let’s briefly look at each of these variables.\n\nWe see that there are 9756 observations in the dataset. The average weight is 31425.86, with a minimum of 3320.89 and a maximum of 220233.32. What does this mean? Each row of data in this dataset has a value for the sampling weight. The person who contributed that row of data represents that many people in the population. What is “the population?” Quoting from the NHANES documentation ( NHANES 2011-2012 Overview ). The NHANES target population is the noninstitutionalized civilian resident population of the United States. The sum of the weights, 306,590,681, is the estimated number of people in the population. However, if you look at other sources for the population of the United States in 2012, you will see something like 314.1 million.\n\nNow let’s look at the cluster and strata variables.\n\nThis tells us is that there are two clusters (AKA PSUs) per strata. This is pretty typical for a survey dataset. The numbering of the clusters and strata does not matter in most statistical software packages.\n\nWe will start by calculating some descriptive statistics of some of the continuous variables. We will use proc surveymeans to get some basic information regarding the continuous variable ridageyr.\n\nWe see some familiar numbers in this output. We see the 14 strata, 31 clusters, 9756 observations, and the estimated population total of 306,590,681.\n\nThere are many options that you can use. The options are usually included on the proc statement. The range option gives the range, which is the maximum minus the minimum.\n\nNotice that the output includes only the statistics requested on the proc surveymeans statement.\n\nIn the example below, five options are specified. The nmiss option shows the number of missing values for the variable pad630 (How much time do you spend doing moderate-intensity activities on a type work day?). The df option shows the degrees of freedom used. The degrees of freedom are equal to the number of clusters (PSUs) minus the number of strata. In this example, 31 – 14 = 17. The cv option gives the coefficient of variation, which is the standard deviation divided by the mean. The geomean option gives the geometric mean, which is the nth root of n numbers. It is sometimes used when combining items that have different ranges. The gmstderr option gives the standard error of the geometric mean.\n\nNotice that SAS does not do a listwise deletion of missing values across all of the variables listed on the var statement. (Notice that the N is different for each of the three variables listed in the output.)\n\nThe ODS graphics must be turned on for SAS to produce the graphs. If you do not submit the ods graphics on; statement, SAS will give you all of the output from proc surveymeans except for the graphs. There will be a warning in the log file indicating that ODS graphics must be turned on in order to get the graphs.\n\nThe diamond is the mean (37.19), and the line is the median (36.21).\n\nThere are a few different ways to get descriptive statistics with categorical variables. You can use proc surveymeans if your variable is binary (i.e., coded 0/1).\n\nProbably the most common procedure for getting descriptive statistics for categorical variables is proc surveyfreq. The tables statement in proc surveyfreq works the same way that the tables statement in proc freq works.\n\nYou may want to use formats to help label the output.\n\nIn the next example, several options were used. The expected option gives the expected frequencies for each cell in the table. The row option gives the row percentages. The col option gives the column percentages. The chisq option gives the Rao-Scott chi-square test; lrchisq option gives the likelihood ratio chi-square test; the wchisq option gives the Wald chi-square test; the wllchisq option gives the Wald log-linear chi-square test.\n\nIn the next example, we will use three options. The cv option displays coefficients of variation for percentages. The definition of “coefficient of variation” is that it is the standard deviation / mean, or, in our case, the standard error divided by the point estimate. For example, for males born elsewhere for the percentage, .8655/7.3247 = .1182. The cvwt option displays coefficients of variation for weighted frequencies. For example, for males born elsewhere: 2368069/22449131 = .1055. The deff option displays design effects for percentages. This attempts to quantify the extent to which the observed sampling error differs from what would be expected if SRS had been used. It is defined as variance(observed) / variance(SRS). It can be thought of as a measure of efficiency. If the design effect is 1, then the current analysis with the current sampling plan is as efficient as the same analysis using a SRS. If the design effect is less than 1, then the current analysis with the current sample is more efficient than the same analysis with SRS. If the design effect is greater than 1, then the current analysis with the current sample is less efficient than the same analysis with a SRS. In general, clustering increases the design effect.\n\nThis is related to the idea of an “effective sample size”. For example, males born elsewhere: 1027/10.7602 = 95.44 total born elsewhere: 2083/18.9775 = 109.76.\n\nLet’s look at some graphs. Using the format will put the labels on the x-axis on the graph.\n\nIf you are requesting more than one plot, you need to enclose the plots in parentheses. The or option will give the odds ratios, and the risk option will give risks.\n\nBefore we continue, we should pause to discuss the analysis of subpopulations. The analysis of subpopulations is one place where survey data and experimental data are quite different. If you have data from an experiment (or quasi-experiment), and you want to analyze the responses from, say, just the women, or just people over age 50, you can just delete the unwanted cases from the data set or use a by statement. Survey data are different. With survey data, you (almost) never get to delete any cases from the data set, even if you will never use them in any of your analyses. Because of the way the by statement works, you usually don’t use it with survey data either. Instead, SAS has provided a domain statement in most survey procedures that allows you to correctly analyze subpopulations of your survey data. A domain and a subpopulation are the same thing. The domain statement is very similar to using a by statement in that you will get output for each level of the variable (or variables) listed on the statement. This means that you will often times get more output that you want; you simply ignore the output for domains that are not of interest to you. Please note that there is no domain statement in proc surveyfreq; you are expected to include the variables that you would have put on the domain statement on the tables statement.\n\nFirst, however, let’s take a second to see why deleting cases from a survey data set can be so problematic. If the data set is subset (meaning that observations not to be included in the subpopulation are deleted from the data set), two problems arise. First, the estimated number of elements in the population cannot be correctly calculated because some numbers are missing as you sum down the column of sampling weights. Secondly, the standard errors of the estimates cannot be calculated correctly. When the subpopulation option(s) is used, only the cases defined by the subpopulation are used in the calculation of the estimate, but all cases are used in the calculation of the standard errors. For more information on this issue, please see Sampling Techniques, Third Edition by William G. Cochran (1977) and Small Area Estimation by J. N. K. Rao (2003).\n\nWe will begin with an analysis that we have seen before.\n\nNow let’s add the domain statement. The format statement is not technically needed, but it is a nice way to more clearly label the output. If you were interested only in the mean for females, you would ignore the output for males.\n\nIn this example, we include two variables on the domain statement. Notice that this is the same as running proc surveymeans twice with each of the variables on the domain statement in turn.\n\nIn this example, we cross the domain variables, giving us each combination of the two variables.\n\nIn this example, we cross three variables on the domain statement.\n\nBy using proc print, we can see that there are only two cases that have a valid value for pad630 in subpopulation females who are living with a partner and have less than nine years of education, and both of those values are 120. This is why no standard error can be estimated.\n\nNow let’s say that you want to compare the means from two domains. In this example, we get the mean of pad630 for females and males.\n\nThere are a few different ways that you could compare 155.63 and 121.68. In this example, we will use proc surveyreg and the contrast statement. Notice that the output in the section titled Estimated Regression Coefficients is almost identical to the output of the proc surveymeans above.\n\nIn this example, we do the same analysis using proc surveymeans and the lsmeans statement. This example is adapted from code on the SAS website here.\n\nIf you square the t-value from this analysis, you will get the F-value given in the previous proc surveyreg analysis. The estimate of -33.94 is simply the difference of the means, 121.68 – 155.63 (with a little rounding error).\n\nNow we will look at a few examples of regression analyses. We will use proc surveyreg and proc surveylogistic. The variables in these examples were chosen because they were either continuous or categorical, not because of data that they contain. In other words, the models shown here were not constructed to make substantive sense; rather, they were constructed to illustrate how certain things can be done. The variable pad630 is the number of minutes spent doing moderate-intensity activities on a typical work day.\n\nThere is a class statement in proc surveyreg (there isn’t one in proc reg), and, depending on the version of SAS/Stat that you are running, it does have many of the options that are found on the class statements in most other SAS procedures. The default in SAS is to use the highest-numbered category as the reference group. Hence, in previous example, the reference group is 1 (females), not 0 (males), for the variable female. In the example below, the category coded 0 is used as the reference group for both predictor variables, female and hsq571 (have you donated blood in the last year?). Besides using the options on the class statement, you can change the reference group by using a format such that the group that you want to be the reference group has the value label that comes first alphabetically. Notice on the model statement that the “|” symbol was used. This tells SAS to include both the main effects and the interaction in the model.\n\nBelow are a few examples of binary logistic regression. The variable paq665 asks if you do any moderate-intensity sports. A little data management is needed before we can run the logistic regression. Notice the options on the class statement and the model statement.\n\nWe can use the expb option on the model statement to get the odds ratios. We can use the clodds option to get the confidence limits around the odds ratios. SAS will provide a generalized R-square, but not all statisticians agree that this is appropriate. The variable hsq470 is the number of days in the last 30 days that physical health was not good, and hsq480 is the number of days in the past 30 days that mental health was not good.\n\nThe following example is copied directly from the SAS website because I don’t have any good data to use for an example. Please see this page for this example and more information. Besides the comments that SAS makes below, there are a few things that I would like to point out. First, notice that you MUST supply two weight variables: a weight for level 1 and a weight for level 2. This is not an inconvenience of using SAS; rather, this is true of running any type of multilevel model in any statistical package. You need to do this because the level 1 sampling weights and the level 2 sampling weights enter into the multilevel model equation in different places. Having the two sampling weights is often a problem with public-use survey data sets, because the data are often not released with level 1 and level 2 weights. The other issue that you need to know about is the scaling of the level 1 sampling weights. This is not an issue in single-level models, but it is an issue in multilevel models. At this time, SAS does not have an option to scale the weights for you; rather, you need to do it yourself in a data step before you run proc glimmix. Please see Pfeffermann, et. al. (1998) and Rabe-Hesketh and Skrondal (2006) for more information.\n\nTo fit a weighted multilevel model, you should use METHOD=QUAD. The EMPIRICAL=CLASSICAL option in the PROC GLIMMIX statement instructs PROC GLIMMIX to compute the empirical (sandwich) variance estimators for the fixed effect and the variance. The empirical variance estimators are recommended for the inference about fixed effects and variance estimated by pseudo-likelihood.\n\nCarle (2009) provides the SAS and Stata code for the two most common methods of scaling the level 1 weights in Appendix B of his paper Fitting Multilevel Models in Complex Survey Data with Design Weights: Recommendations. One method scales the level 1 weight to the sample size within each cluster; the other method scales the level 1 weight to the effective sample size. There is currently no recommendation about when to use either type of scaling; rather, the recommendation is to do a sensitivity analysis comparing both methods.\n\nThe following is quoted from the SAS documentation: http://support.sas.com/documentation/cdl/en/statug/68162/HTML/default/viewer.htm#statug_surveyimpute_overview.htm\n\nThe SURVEYIMPUTE procedure imputes missing values of an item in a data set by replacing them with observed values from the same item. The principles by which the imputation is performed are particularly useful for survey data. PROC SURVEYIMPUTE also computes replicate weights (such as jackknife weights) that account for the imputation and that can be used for replication-based variance estimation for complex surveys. The procedure implements a fractional hot-deck imputation technique (Kim and Fuller 2004; Fuller 2009; Kim and Shao 2014) in addition to some traditional hot-deck imputation techniques (Andridge and Little 2010).\n\nNonresponse is a common problem in almost all surveys of human populations. Estimators that are based on survey data that include nonresponse can suffer from nonresponse bias if the nonrespondents are different from the respondents. Estimators that use complete cases (only the observed units) might also be less precise. Imputation techniques are important tools for reducing nonresponse bias and producing efficient estimators.\n\nThe main objectives of any imputation technique are to eliminate the nonresponse bias and to provide an imputed data set that results in consistent analyses conducted with the imputed data. In addition, a variance estimator must be available that accounts for both the sampling variance and the imputation variance. Imputation techniques use implicit or explicit models. Some model-based imputation techniques include multiple imputation, mean imputation, and regression imputation. For more information about multiple imputation in SAS/STAT, see Chapter 75: The MI Procedure, and Chapter 76: The MIANALYZE Procedure.\n\nImputation techniques that do not use explicit models include hot-deck imputation, cold-deck imputation, and fractional imputation. PROC SURVEYIMPUTE implements imputation techniques that do not use explicit models. It also produces replicate weights that can be used with any survey analysis procedure in SAS/STAT to estimate both the sampling variability and the imputation variability.\n\nHot-deck imputation is the most commonly used imputation technique for survey data. A donor is selected for a recipient unit, and the observed values of the donor are imputed for the missing items of the recipient. Although the imputation method is straightforward, the variance estimator that accounts for imputation variance might not be simple and is often ignored in practice. PROC SURVEYIMPUTE does not create imputation-adjusted replicate weights for hot-deck imputation.\n\nFractional hot-deck imputation (Kalton and Kish 1984; Fay 1996; Kim and Fuller 2004; Fuller and Kim 2005), also known as fractional imputation (FI), is a variation of hot-deck imputation in which one missing item for a recipient is imputed from multiple donors. Each donor donates a fraction of the original weight of the recipient such that the sum of the fractional weights from all the donors is equal to the original weight of the recipient. For fully efficient fractional imputation (FEFI), all observed values in an imputation cell are used as donors for a recipient unit in that cell (Kim and Fuller 2004).\n\nThe SURVEYIMPUTE procedure implements single and multiple hot-deck imputation and FEFI. Available donor selection techniques include simple random selection with or without replacement, probability proportional to weights selection (Rao and Shao 1992), and approximate Bayesian bootstrap selection (Rubin and Schenker 1986).\n\nA great deal of work has been done with respect to imputation methods for complex survey data. While this topic is beyond the scope of this workshop, interested readers may want to see\n\nAndridge Rebecca R. and Roderick J. Little. (2009). The Use of Sample Weights in Hot Deck Imputation. Journal of Official Statistics; 25(1): 21-36.\n\nFor more information on using the NHANES data sets\n\nThere are helpful resources for learning how to analyze the NHANES data sets correctly. One is a listserv at http://www.cdc.gov/nchs/nhanes/nhanes_listserv.htm . There are also online tutorials at http://www.cdc.gov/nchs/tutorials/index.htm .\n\nAnalysis of Health Surveys by Edward L. Korn and Barry I. Graubard\n\nSampling of Populations: Methods and Applications, Fourth Edition by Paul Levy and Stanley Lemeshow\n\nAnalysis of Survey Data Edited by R. L. Chambers and C. J. Skinner\n\nPfeffermann, D., Skinner, C. J., Holmes, D. J., Goldstein, H., and Rasbash, J. (1998), Weighting for Unequal Selection Probabilities in Multilevel Models, Journal of the Royal Statistical Society, Series B, 60, 23-40.\n\nRabe-Hesketh, S. and Skrondal, A. (2006), Multilevel Modelling of Complex Survey Data, Journal of the Royal Statistical Society, Series A, 169, 805-827.\n\nCarle, Adam C. (2009). Fitting Multilevel Models in Complex Survey Data with Design Weights: Recommendations. BMC Medical Research Methodology; 9(49).\n\nQuartagno, M., Carpenter, R., and Goldstein, H.. (2019). Multiple Imputation with Survey Weights: A Multilevel Approach. Journal of Survey Statistics and Methodology, Volume 8, Issue 5, November 2020, Pages 965–989, https://doi.org/10.1093/jssam/smz036"
    },
    {
        "link": "https://documentation.sas.com/doc/en/statug/latest/statug_surveyselect_overview.htm",
        "document": ""
    },
    {
        "link": "https://documentation.sas.com/doc/en/statug/15.2/statug_surveyselect_syntax01.htm",
        "document": ""
    },
    {
        "link": "https://communities.sas.com/t5/SAS-Procedures/Proc-Survey-Select-Stratified-Random-Sampling/td-p/437481",
        "document": "PROC SURVEYSELECT treats missing values of STRATA and SAMPLINGUNIT variables like any other STRATA or SAMPLINGUNIT .\n\nWhich means that your missing D strata has one more level than values which is likely causing issues with the A B C combinations\n\nConsider a strata that only has 7 members and you request a samprate of 80. How many would you expect in the output? (Hint: 7* .8= 5.6 rounds to 6) (or 80 percent of 23 or practically anything you'll have rounding issues.).\n\nYou may be having multiple round up issues due to the sizes of your strata.\n\nand see how many records per combination of the strata you have.\n\nYou don't mention how many levels any of your strata have but if there are more than 5 each and are roughly evenly distributed you don't have many records per combination of strata variables, about 25 per combination. With more levels the numbers per strata combination can go way down increasing the issue of rounding to 80 percent per.\n\nYou might be better served by summarizing the input data by the strata variables, getting an explicit count of available (proc means or summary don't forget missing option), using a data step to do your rounding per combination and use that as a SAMPSIZE data set."
    },
    {
        "link": "https://documentation.sas.com/doc/en/statug/latest/statug_surveyselect_gettingstarted02.htm",
        "document": ""
    },
    {
        "link": "https://documentation.sas.com/doc/en/statug/latest/statug_surveyselect_syntax01.htm",
        "document": ""
    },
    {
        "link": "https://stats.oarc.ucla.edu/sas/faq/how-can-i-take-a-stratified-random-sample-of-my-data",
        "document": "Sometimes you may want to take a random sample of your data, but you want to respect the stratification that was used when the data set was created. Other times you want to maintain certain proportions in the sampled data set; for example, drawing a sample from a data set, but having proportions of males and females that correspond to the current census figures. To draw these types of samples from your data set, you can use proc surveyselect. We will use the hsb2.sas7bdat dataset for our examples. Notice that the code on this page works with SAS 8.x. For the examples below, assume that you’ve imported this dataset into the work folder.\n\nExample 1: Taking a 50% sample from each strata using simple random sampling (srs)\n\nBefore we take our sample, let’s look at the data set using proc means. Because we will use a by statement, we need to sort the data first. We will use the variable female as our stratification variable. Also, we will use an options statement to suppress the showing of the variable labels in the output.\n\nIn the command below we have used several options. We have used the data = option to specify the data set from which we wish to draw the sample. The method option indicates the method by which we would like the sample drawn. SAS offers a wide range of options for this, including probability-proportional-to-size and systematic sampling. The samprate option is used to specify the sampling rate. Here, we have indicated .5, which means 50%. We have used the seed option to set the seed so that our results will be replicable. On the strata statement we specify the variable (or variables) that define the strata.\n\nIf you want to know which cases were not selected, or if you want to use the two samples for validation purposes, you have to merge the sampled data set back with the original data set. An example is given below. Note that we need to sort both the original data set and the sampled data set on the same variable. This variable must uniquely identify each case in the data set. You can tell which cases were selected into the sample because they have values for Selection Prob and Sampling Weight. These variables were created by proc surveyselect, and hence are not in the original data file. If you want to create three or more data sets from your original data set, you can use Enterprise Miner.\n\nExample 2: Using more than one strata variable\n\nIn this example, we will use three strata variables. The variable female has two values, and the variable ses has three levels. As before, we will sort the original data set on the strata variables, and then we will do a proc means to see what the variables look like before we draw the sample.\n\nThe same options are used as above.\n\nExample 3: Using different sampling rates from each strata\n\nIn the examples above, we sampled from each strata at the same rate. However, sometimes you want to sample more from one strata than another. You can specify different sampling rates for each strata by enclosing the proportions in parentheses for the samprate option. Let’s first take a look at the cell counts for the strata variables female and ses.\n\nThe table below gives sampling rates we will use for each of the cells above.\n\nExample 4: Specifying the number of observations to be sampled\n\nYou can specify the number of observations to be sampled from each strata if you prefer. Instead of using the samprate option, you would use the n = option and list the numbers in parentheses."
    },
    {
        "link": "https://lexjansen.com/sesug/2015/190_Final_PDF.pdf",
        "document": ""
    }
]