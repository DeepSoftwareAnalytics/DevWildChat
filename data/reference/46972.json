[
    {
        "link": "https://docs.python.org/3/library/threading.html",
        "document": "This module constructs higher-level threading interfaces on top of the lower level module.\n\noffers a higher level interface to push tasks to a background thread without blocking execution of the calling thread, while still being able to retrieve their results when needed. provides a thread-safe interface for exchanging data between running threads. offers an alternative approach to achieving task level concurrency without requiring the use of multiple operating system threads.\n\nThis module defines the following functions:\n\nThis module also defines the following constant:\n\nThis module defines a number of classes, which are detailed in the sections below.\n\nThe design of this module is loosely based on Java’s threading model. However, where Java makes locks and condition variables basic behavior of every object, they are separate objects in Python. Python’s class supports a subset of the behavior of Java’s Thread class; currently, there are no priorities, no thread groups, and threads cannot be destroyed, stopped, suspended, resumed, or interrupted. The static methods of Java’s Thread class, when implemented, are mapped to module-level functions.\n\nAll of the methods described below are executed atomically.\n\nThe class represents an activity that is run in a separate thread of control. There are two ways to specify the activity: by passing a callable object to the constructor, or by overriding the method in a subclass. No other methods (except for the constructor) should be overridden in a subclass. In other words, only override the and methods of this class. Once a thread object is created, its activity must be started by calling the thread’s method. This invokes the method in a separate thread of control. Once the thread’s activity is started, the thread is considered ‘alive’. It stops being alive when its method terminates – either normally, or by raising an unhandled exception. The method tests whether the thread is alive. Other threads can call a thread’s method. This blocks the calling thread until the thread whose method is called is terminated. A thread has a name. The name can be passed to the constructor, and read or changed through the attribute. If the method raises an exception, is called to handle it. By default, ignores silently . A thread can be flagged as a “daemon thread”. The significance of this flag is that the entire Python program exits when only daemon threads are left. The initial value is inherited from the creating thread. The flag can be set through the property or the daemon constructor argument. Daemon threads are abruptly stopped at shutdown. Their resources (such as open files, database transactions, etc.) may not be released properly. If you want your threads to stop gracefully, make them non-daemonic and use a suitable signalling mechanism such as an . There is a “main thread” object; this corresponds to the initial thread of control in the Python program. It is not a daemon thread. There is the possibility that “dummy thread objects” are created. These are thread objects corresponding to “alien threads”, which are threads of control started outside the threading module, such as directly from C code. Dummy thread objects have limited functionality; they are always considered alive and daemonic, and cannot be joined. They are never deleted, since it is impossible to detect the termination of alien threads. This constructor should always be called with keyword arguments. Arguments are: group should be ; reserved for future extension when a class is implemented. target is the callable object to be invoked by the method. Defaults to , meaning nothing is called. name is the thread name. By default, a unique name is constructed of the form “Thread-N” where N is a small decimal number, or “Thread-N (target)” where “target” is if the target argument is specified. args is a list or tuple of arguments for the target invocation. Defaults to . kwargs is a dictionary of keyword arguments for the target invocation. Defaults to . If not , daemon explicitly sets whether the thread is daemonic. If (the default), the daemonic property is inherited from the current thread. If the subclass overrides the constructor, it must make sure to invoke the base class constructor ( ) before doing anything else to the thread. Changed in version 3.10: Use the target name if name argument is omitted. It must be called at most once per thread object. It arranges for the object’s method to be invoked in a separate thread of control. This method will raise a if called more than once on the same thread object. You may override this method in a subclass. The standard method invokes the callable object passed to the object’s constructor as the target argument, if any, with positional and keyword arguments taken from the args and kwargs arguments, respectively. Using list or tuple as the args argument which passed to the could achieve the same effect. Wait until the thread terminates. This blocks the calling thread until the thread whose method is called terminates – either normally or through an unhandled exception – or until the optional timeout occurs. When the timeout argument is present and not , it should be a floating-point number specifying a timeout for the operation in seconds (or fractions thereof). As always returns , you must call after to decide whether a timeout happened – if the thread is still alive, the call timed out. When the timeout argument is not present or , the operation will block until the thread terminates. A thread can be joined many times. raises a if an attempt is made to join the current thread as that would cause a deadlock. It is also an error to a thread before it has been started and attempts to do so raise the same exception. A string used for identification purposes only. It has no semantics. Multiple threads may be given the same name. The initial name is set by the constructor. Deprecated getter/setter API for ; use it directly as a property instead. The ‘thread identifier’ of this thread or if the thread has not been started. This is a nonzero integer. See the function. Thread identifiers may be recycled when a thread exits and another thread is created. The identifier is available even after the thread has exited. The Thread ID ( ) of this thread, as assigned by the OS (kernel). This is a non-negative integer, or if the thread has not been started. See the function. This value may be used to uniquely identify this particular thread system-wide (until the thread terminates, after which the value may be recycled by the OS). Similar to Process IDs, Thread IDs are only valid (guaranteed unique system-wide) from the time the thread is created until the thread has been terminated. Return whether the thread is alive. This method returns just before the method starts until just after the method terminates. The module function returns a list of all alive threads. A boolean value indicating whether this thread is a daemon thread ( ) or not ( ). This must be set before is called, otherwise is raised. Its initial value is inherited from the creating thread; the main thread is not a daemon thread and therefore all threads created in the main thread default to = . The entire Python program exits when no alive non-daemon threads are left. Deprecated getter/setter API for ; use it directly as a property instead.\n\nA primitive lock is a synchronization primitive that is not owned by a particular thread when locked. In Python, it is currently the lowest level synchronization primitive available, implemented directly by the extension module. A primitive lock is in one of two states, “locked” or “unlocked”. It is created in the unlocked state. It has two basic methods, and . When the state is unlocked, changes the state to locked and returns immediately. When the state is locked, blocks until a call to in another thread changes it to unlocked, then the call resets it to locked and returns. The method should only be called in the locked state; it changes the state to unlocked and returns immediately. If an attempt is made to release an unlocked lock, a will be raised. When more than one thread is blocked in waiting for the state to turn to unlocked, only one thread proceeds when a call resets the state to unlocked; which one of the waiting threads proceeds is not defined, and may vary across implementations. The class implementing primitive lock objects. Once a thread has acquired a lock, subsequent attempts to acquire it block, until it is released; any thread may release it. Changed in version 3.13: is now a class. In earlier Pythons, was a factory function which returned an instance of the underlying private lock type. When invoked with the blocking argument set to (the default), block until the lock is unlocked, then set it to locked and return . When invoked with the blocking argument set to , do not block. If a call with blocking set to would block, return immediately; otherwise, set the lock to locked and return . When invoked with the floating-point timeout argument set to a positive value, block for at most the number of seconds specified by timeout and as long as the lock cannot be acquired. A timeout argument of specifies an unbounded wait. It is forbidden to specify a timeout when blocking is . The return value is if the lock is acquired successfully, if not (for example if the timeout expired). Changed in version 3.2: The timeout parameter is new. Changed in version 3.2: Lock acquisition can now be interrupted by signals on POSIX if the underlying threading implementation supports it. Release a lock. This can be called from any thread, not only the thread which has acquired the lock. When the lock is locked, reset it to unlocked, and return. If any other threads are blocked waiting for the lock to become unlocked, allow exactly one of them to proceed. When invoked on an unlocked lock, a is raised. There is no return value. Return if the lock is acquired.\n\nA reentrant lock is a synchronization primitive that may be acquired multiple times by the same thread. Internally, it uses the concepts of “owning thread” and “recursion level” in addition to the locked/unlocked state used by primitive locks. In the locked state, some thread owns the lock; in the unlocked state, no thread owns it. Threads call a lock’s method to lock it, and its method to unlock it. Reentrant locks support the context management protocol, so it is recommended to use instead of manually calling and to handle acquiring and releasing the lock for a block of code. RLock’s / call pairs may be nested, unlike Lock’s / . Only the final (the of the outermost pair) resets the lock to an unlocked state and allows another thread blocked in to proceed. / must be used in pairs: each acquire must have a release in the thread that has acquired the lock. Failing to call release as many times the lock has been acquired can lead to deadlock. This class implements reentrant lock objects. A reentrant lock must be released by the thread that acquired it. Once a thread has acquired a reentrant lock, the same thread may acquire it again without blocking; the thread must release it once for each time it has acquired it. Note that is actually a factory function which returns an instance of the most efficient version of the concrete RLock class that is supported by the platform. Recommended over manual and calls whenever practical. When invoked with the blocking argument set to (the default):\n• None If no thread owns the lock, acquire the lock and return immediately.\n• None If another thread owns the lock, block until we are able to acquire lock, or timeout, if set to a positive float value.\n• None If the same thread owns the lock, acquire the lock again, and return immediately. This is the difference between and ; handles this case the same as the previous, blocking until the lock can be acquired. When invoked with the blocking argument set to :\n• None If no thread owns the lock, acquire the lock and return immediately.\n• None If another thread owns the lock, return immediately.\n• None If the same thread owns the lock, acquire the lock again and return immediately. In all cases, if the thread was able to acquire the lock, return . If the thread was unable to acquire the lock (i.e. if not blocking or the timeout was reached) return . If called multiple times, failing to call as many times may lead to deadlock. Consider using as a context manager rather than calling acquire/release directly. Changed in version 3.2: The timeout parameter is new. Release a lock, decrementing the recursion level. If after the decrement it is zero, reset the lock to unlocked (not owned by any thread), and if any other threads are blocked waiting for the lock to become unlocked, allow exactly one of them to proceed. If after the decrement the recursion level is still nonzero, the lock remains locked and owned by the calling thread. Only call this method when the calling thread owns the lock. A is raised if this method is called when the lock is not acquired. There is no return value.\n\nA condition variable is always associated with some kind of lock; this can be passed in or one will be created by default. Passing one in is useful when several condition variables must share the same lock. The lock is part of the condition object: you don’t have to track it separately. A condition variable obeys the context management protocol: using the statement acquires the associated lock for the duration of the enclosed block. The and methods also call the corresponding methods of the associated lock. Other methods must be called with the associated lock held. The method releases the lock, and then blocks until another thread awakens it by calling or . Once awakened, re-acquires the lock and returns. It is also possible to specify a timeout. The method wakes up one of the threads waiting for the condition variable, if any are waiting. The method wakes up all threads waiting for the condition variable. Note: the and methods don’t release the lock; this means that the thread or threads awakened will not return from their call immediately, but only when the thread that called or finally relinquishes ownership of the lock. The typical programming style using condition variables uses the lock to synchronize access to some shared state; threads that are interested in a particular change of state call repeatedly until they see the desired state, while threads that modify the state call or when they change the state in such a way that it could possibly be a desired state for one of the waiters. For example, the following code is a generic producer-consumer situation with unlimited buffer capacity: The loop checking for the application’s condition is necessary because can return after an arbitrary long time, and the condition which prompted the call may no longer hold true. This is inherent to multi-threaded programming. The method can be used to automate the condition checking, and eases the computation of timeouts: To choose between and , consider whether one state change can be interesting for only one or several waiting threads. E.g. in a typical producer-consumer situation, adding one item to the buffer only needs to wake up one consumer thread. This class implements condition variable objects. A condition variable allows one or more threads to wait until they are notified by another thread. If the lock argument is given and not , it must be a or object, and it is used as the underlying lock. Otherwise, a new object is created and used as the underlying lock. Changed in version 3.3: changed from a factory function to a class. Acquire the underlying lock. This method calls the corresponding method on the underlying lock; the return value is whatever that method returns. Release the underlying lock. This method calls the corresponding method on the underlying lock; there is no return value. Wait until notified or until a timeout occurs. If the calling thread has not acquired the lock when this method is called, a is raised. This method releases the underlying lock, and then blocks until it is awakened by a or call for the same condition variable in another thread, or until the optional timeout occurs. Once awakened or timed out, it re-acquires the lock and returns. When the timeout argument is present and not , it should be a floating-point number specifying a timeout for the operation in seconds (or fractions thereof). When the underlying lock is an , it is not released using its method, since this may not actually unlock the lock when it was acquired multiple times recursively. Instead, an internal interface of the class is used, which really unlocks it even when it has been recursively acquired several times. Another internal interface is then used to restore the recursion level when the lock is reacquired. The return value is unless a given timeout expired, in which case it is . Changed in version 3.2: Previously, the method always returned . Wait until a condition evaluates to true. predicate should be a callable which result will be interpreted as a boolean value. A timeout may be provided giving the maximum time to wait. This utility method may call repeatedly until the predicate is satisfied, or until a timeout occurs. The return value is the last return value of the predicate and will evaluate to if the method timed out. Ignoring the timeout feature, calling this method is roughly equivalent to writing: Therefore, the same rules apply as with : The lock must be held when called and is re-acquired on return. The predicate is evaluated with the lock held. By default, wake up one thread waiting on this condition, if any. If the calling thread has not acquired the lock when this method is called, a is raised. This method wakes up at most n of the threads waiting for the condition variable; it is a no-op if no threads are waiting. The current implementation wakes up exactly n threads, if at least n threads are waiting. However, it’s not safe to rely on this behavior. A future, optimized implementation may occasionally wake up more than n threads. Note: an awakened thread does not actually return from its call until it can reacquire the lock. Since does not release the lock, its caller should. Wake up all threads waiting on this condition. This method acts like , but wakes up all waiting threads instead of one. If the calling thread has not acquired the lock when this method is called, a is raised. The method is a deprecated alias for this method.\n\nThis class provides a simple synchronization primitive for use by a fixed number of threads that need to wait for each other. Each of the threads tries to pass the barrier by calling the method and will block until all of the threads have made their calls. At this point, the threads are released simultaneously. The barrier can be reused any number of times for the same number of threads. As an example, here is a simple way to synchronize a client and server thread: Create a barrier object for parties number of threads. An action, when provided, is a callable to be called by one of the threads when they are released. timeout is the default timeout value if none is specified for the method. Pass the barrier. When all the threads party to the barrier have called this function, they are all released simultaneously. If a timeout is provided, it is used in preference to any that was supplied to the class constructor. The return value is an integer in the range 0 to parties – 1, different for each thread. This can be used to select a thread to do some special housekeeping, e.g.: # Only one thread needs to print this If an action was provided to the constructor, one of the threads will have called it prior to being released. Should this call raise an error, the barrier is put into the broken state. If the call times out, the barrier is put into the broken state. This method may raise a exception if the barrier is broken or reset while a thread is waiting. Return the barrier to the default, empty state. Any threads waiting on it will receive the exception. Note that using this function may require some external synchronization if there are other threads whose state is unknown. If a barrier is broken it may be better to just leave it and create a new one. Put the barrier into a broken state. This causes any active or future calls to to fail with the . Use this for example if one of the threads needs to abort, to avoid deadlocking the application. It may be preferable to simply create the barrier with a sensible timeout value to automatically guard against one of the threads going awry. The number of threads required to pass the barrier. The number of threads currently waiting in the barrier. A boolean that is if the barrier is in the broken state. This exception, a subclass of , is raised when the object is reset or broken."
    },
    {
        "link": "https://realpython.com/intro-to-python-threading",
        "document": "Python threading allows you to have different parts of your program run concurrently and can simplify your design. If you’ve got some experience in Python and want to speed up your program using threads, then this tutorial is for you!\n• How to create threads and wait for them to finish\n• How to use a\n• How to use the common tools that Python provides\n\nThis article assumes you’ve got the Python basics down pat and that you’re using at least version 3.6 to run the examples. If you need a refresher, you can start with the Python Learning Paths and get up to speed.\n\nIf you’re not sure if you want to use Python , , or , then you can check out Speed Up Your Python Program With Concurrency.\n\nAll of the sources used in this tutorial are available to you in the Real Python GitHub repo.\n\nA thread is a separate flow of execution. This means that your program will have two things happening at once. But for most Python 3 implementations the different threads do not actually execute at the same time: they merely appear to. It’s tempting to think of threading as having two (or more) different processors running on your program, each one doing an independent task at the same time. That’s almost right. The threads may be running on different processors, but they will only be running one at a time. Getting multiple tasks running simultaneously requires a non-standard implementation of Python, writing some of your code in a different language, or using which comes with some extra overhead. Because of the way CPython implementation of Python works, threading may not speed up all tasks. This is due to interactions with the GIL that essentially limit one Python thread to run at a time. Tasks that spend much of their time waiting for external events are generally good candidates for threading. Problems that require heavy CPU computation and spend little time waiting for external events might not run faster at all. This is true for code written in Python and running on the standard CPython implementation. If your threads are written in C they have the ability to release the GIL and run concurrently. If you are running on a different Python implementation, check with the documentation too see how it handles threads. If you are running a standard Python implementation, writing in only Python, and have a CPU-bound problem, you should check out the module instead. Architecting your program to use threading can also provide gains in design clarity. Most of the examples you’ll learn about in this tutorial are not necessarily going to run faster because they use threads. Using threading in them helps to make the design cleaner and easier to reason about. So, let’s stop talking about threading and start using it!\n\nNow that you’ve got an idea of what a thread is, let’s learn how to make one. The Python standard library provides , which contains most of the primitives you’ll see in this article. , in this module, nicely encapsulates threads, providing a clean interface to work with them. To start a separate thread, you create a instance and then tell it to : \"Main : wait for the thread to finish\" If you look around the logging statements, you can see that the section is creating and starting the thread: When you create a , you pass it a function and a list containing the arguments to that function. In this case, you’re telling the to run and to pass it as an argument. For this article, you’ll use sequential integers as names for your threads. There is , which returns a unique name for each thread, but these are usually neither short nor easily readable. itself doesn’t do much. It simply logs some messages with a in between them. When you run this program as it is (with line twenty commented out), the output will look like this: ./single_thread.py Main : wait for the thread to finish You’ll notice that the finished after the section of your code did. You’ll come back to why that is and talk about the mysterious line twenty in the next section. In computer science, a is a process that runs in the background. Python has a more specific meaning for . A thread will shut down immediately when the program exits. One way to think about these definitions is to consider the thread a thread that runs in the background without worrying about shutting it down. If a program is running that are not , then the program will wait for those threads to complete before it terminates. that are daemons, however, are just killed wherever they are when the program is exiting. Let’s look a little more closely at the output of your program above. The last two lines are the interesting bit. When you run the program, you’ll notice that there is a pause (of about 2 seconds) after has printed its message and before the thread is finished. This pause is Python waiting for the non-daemonic thread to complete. When your Python program ends, part of the shutdown process is to clean up the threading routine. If you look at the source for Python , you’ll see that walks through all of the running threads and calls on every one that does not have the flag set. So your program waits to exit because the thread itself is waiting in a sleep. As soon as it has completed and printed the message, will return and the program can exit. Frequently, this behavior is what you want, but there are other options available to us. Let’s first repeat the program with a thread. You do that by changing how you construct the , adding the flag: When you run the program now, you should see this output: ./daemon_thread.py Main : wait for the thread to finish The difference here is that the final line of the output is missing. did not get a chance to complete. It was a thread, so when reached the end of its code and the program wanted to finish, the daemon was killed. Daemon threads are handy, but what about when you want to wait for a thread to stop? What about when you want to do that and not exit your program? Now let’s go back to your original program and look at that commented out line twenty: To tell one thread to wait for another thread to finish, you call . If you uncomment that line, the main thread will pause and wait for the thread to complete running. Did you test this on the code with the daemon thread or the regular thread? It turns out that it doesn’t matter. If you a thread, that statement will wait until either kind of thread is finished.\n\nThe example code so far has only been working with two threads: the main thread and one you started with the object. Frequently, you’ll want to start a number of threads and have them do interesting work. Let’s start by looking at the harder way of doing that, and then you’ll move on to an easier method. The harder way of starting multiple threads is the one you already know: This code uses the same mechanism you saw above to start a thread, create a object, and then call . The program keeps a list of objects so that it can then wait for them later using . Running this code multiple times will likely produce some interesting results. Here’s an example output from my machine: If you walk through the output carefully, you’ll see all three threads getting started in the order you might expect, but in this case they finish in the opposite order! Multiple runs will produce different orderings. Look for the message to tell you when each thread is done. The order in which threads are run is determined by the operating system and can be quite hard to predict. It may (and likely will) vary from run to run, so you need to be aware of that when you design algorithms that use threading. Fortunately, Python gives you several primitives that you’ll look at later to help coordinate threads and get them running together. Before that, let’s look at how to make managing a group of threads a bit easier.\n\nBefore you move on to some of the other features tucked away in Python , let’s talk a bit about one of the more difficult issues you’ll run into when writing threaded programs: race conditions. Once you’ve seen what a race condition is and looked at one happening, you’ll move on to some of the primitives provided by the standard library to prevent race conditions from happening. Race conditions can occur when two or more threads access a shared piece of data or resource. In this example, you’re going to create a large race condition that happens every time, but be aware that most race conditions are not this obvious. Frequently, they only occur rarely, and they can produce confusing results. As you can imagine, this makes them quite difficult to debug. Fortunately, this race condition will happen every time, and you’ll walk through it in detail to explain what is happening. For this example, you’re going to write a class that updates a database. Okay, you’re not really going to have a database: you’re just going to fake it, because that’s not the point of this article. Your will have and methods: is keeping track of a single number: . This is going to be the shared data on which you’ll see the race condition. simply initializes to zero. So far, so good. looks a little strange. It’s simulating reading a value from a database, doing some computation on it, and then writing a new value back to the database. In this case, reading from the database just means copying to a local variable. The computation is just to add one to the value and then for a little bit. Finally, it writes the value back by copying the local value back to . Here’s how you’ll use this : The program creates a with two threads and then calls on each of them, telling them to run . has a signature that allows both positional and named arguments to be passed to the function running in the thread: In the usage above, is passed as the first and only positional argument to . You’ll see later in this article where you can pass multiple arguments in a similar manner. Since each thread runs , and adds one to , you might expect to be when it’s printed out at the end. But you wouldn’t be looking at this example if that was the case. If you run the above code, the output looks like this: You might have expected that to happen, but let’s look at the details of what’s really going on here, as that will make the solution to this problem easier to understand. Before you dive into this issue with two threads, let’s step back and talk a bit about some details of how threads work. You won’t be diving into all of the details here, as that’s not important at this level. We’ll also be simplifying a few things in a way that won’t be technically accurate but will give you the right idea of what is happening. When you tell your to run each thread, you tell it which function to run and what parameters to pass to it: . The result of this is that each of the threads in the pool will call . Note that is a reference to the one object created in . Calling on that object calls an instance method on that object. Each thread is going to have a reference to the same object, . Each thread will also have a unique value, , to make the logging statements a bit easier to read: When the thread starts running , it has its own version of all of the data local to the function. In the case of , this is . This is definitely a good thing. Otherwise, two threads running the same function would always confuse each other. It means that all variables that are scoped (or local) to a function are thread-safe. Now you can start walking through what happens if you run the program above with a single thread and a single call to . The image below steps through the execution of if only a single thread is run. The statement is shown on the left followed by a diagram showing the values in the thread’s and the shared : The diagram is laid out so that time increases as you move from top to bottom. It begins when is created and ends when it is terminated. When starts, is zero. The first line of code in the method, , copies the value zero to the local variable. Next it increments the value of with the statement. You can see in getting set to one. Next is called, which makes the current thread pause and allows other threads to run. Since there is only one thread in this example, this has no effect. When wakes up and continues, it copies the new value from to , and then the thread is complete. You can see that is set to one. So far, so good. You ran once and was incremented to one. Getting back to the race condition, the two threads will be running concurrently but not at the same time. They will each have their own version of and will each point to the same . It is this shared object that is going to cause the problems. When calls , it allows the other thread to start running. This is where things get interesting. starts up and does the same operations. It’s also copying into its private , and this shared has not yet been updated: When finally goes to sleep, the shared is still unmodified at zero, and both private versions of have the value one. now wakes up and saves its version of and then terminates, giving a final chance to run. has no idea that ran and updated while it was sleeping. It stores its version of into , also setting it to one: The two threads have interleaving access to a single shared object, overwriting each other’s results. Similar race conditions can arise when one thread frees memory or closes a file handle before the other thread is finished accessing it. Why This Isn’t a Silly Example The example above is contrived to make sure that the race condition happens every time you run your program. Because the operating system can swap out a thread at any time, it is possible to interrupt a statement like after it has read the value of but before it has written back the incremented value. The details of how this happens are quite interesting, but not needed for the rest of this article, so feel free to skip over this hidden section. How Does This Really WorkShow/Hide The code above isn’t quite as out there as you might originally have thought. It was designed to force a race condition every time you run it, but that makes it much easier to solve than most race conditions. There are two things to keep in mind when thinking about race conditions:\n• Even an operation like takes the processor many steps. Each of these steps is a separate instruction to the processor.\n• The operating system can swap which thread is running at any time. A thread can be swapped out after any of these small instructions. This means that a thread can be put to sleep to let another thread run in the middle of a Python statement. Let’s look at this in detail. The REPL below shows a function that takes a parameter and increments it: The REPL example uses from the Python standard library to show the smaller steps that the processor does to implement your function. It does a of the data value , it does a , and then it uses the to add those values together. We’re stopping here for a specific reason. This is the point in above where forced the threads to switch. It is entirely possible that, every once in while, the operating system would switch threads at that exact point even without , but the call to makes it happen every time. As you learned above, the operating system can swap threads at any time. You’ve walked down this listing to the statement marked . If the operating system swaps out this thread and runs a different thread that also modifies , then when this thread resumes, it will overwrite with an incorrect value. Technically, this example won’t have a race condition because is local to . It does illustrate how a thread can be interrupted during a single Python operation, however. The same LOAD, MODIFY, STORE set of operations also happens on global and shared values. You can explore with the module and prove that yourself. It’s rare to get a race condition like this to occur, but remember that an infrequent event taken over millions of iterations becomes likely to happen. The rarity of these race conditions makes them much, much harder to debug than regular bugs. Now back to your regularly scheduled tutorial! Now that you’ve seen a race condition in action, let’s find out how to solve them!\n\nThere are a number of ways to avoid or solve race conditions. You won’t look at all of them here, but there are a couple that are used frequently. Let’s start with . To solve your race condition above, you need to find a way to allow only one thread at a time into the read-modify-write section of your code. The most common way to do this is called in Python. In some other languages this same idea is called a . Mutex comes from MUTual EXclusion, which is exactly what a does. A is an object that acts like a hall pass. Only one thread at a time can have the . Any other thread that wants the must wait until the owner of the gives it up. The basic functions to do this are and . A thread will call to get the lock. If the lock is already held, the calling thread will wait until it is released. There’s an important point here. If one thread gets the lock but never gives it back, your program will be stuck. You’ll read more about this later. Fortunately, Python’s will also operate as a context manager, so you can use it in a statement, and it gets released automatically when the block exits for any reason. Let’s look at the with a added to it. The calling function stays the same: Other than adding a bunch of debug logging so you can see the locking more clearly, the big change here is to add a member called , which is a object. This is initialized in the unlocked state and locked and released by the statement. It’s worth noting here that the thread running this function will hold on to that until it is completely finished updating the database. In this case, that means it will hold the while it copies, updates, sleeps, and then writes the value back to the database. If you run this version with logging set to warning level, you’ll see this: Look at that. Your program finally works! You can turn on full logging by setting the level to by adding this statement after you configure the logging output in : Running this program with logging turned on looks like this: In this output you can see acquires the lock and is still holding it when it goes to sleep. then starts and attempts to acquire the same lock. Because is still holding it, has to wait. This is the mutual exclusion that a provides. Many of the examples in the rest of this article will have and level logging. We’ll generally only show the level output, as the logs can be quite lengthy. Try out the programs with the logging turned up and see what they do.\n\nThe Producer-Consumer Problem is a standard computer science problem used to look at threading or process synchronization issues. You’re going to look at a variant of it to get some ideas of what primitives the Python module provides. For this example, you’re going to imagine a program that needs to read messages from a network and write them to disk. The program does not request a message when it wants. It must be listening and accept messages as they come in. The messages will not come in at a regular pace, but will be coming in bursts. This part of the program is called the producer. On the other side, once you have a message, you need to write it to a database. The database access is slow, but fast enough to keep up to the average pace of messages. It is not fast enough to keep up when a burst of messages comes in. This part is the consumer. In between the producer and the consumer, you will create a that will be the part that changes as you learn about different synchronization objects. That’s the basic layout. Let’s look at a solution using . It doesn’t work perfectly, but it uses tools you already know, so it’s a good place to start. Since this is an article about Python , and since you just read about the primitive, let’s try to solve this problem with two threads using a or two. The general design is that there is a thread that reads from the fake network and puts the message into a : \"\"\"Pretend we're getting a message from the network.\"\"\" # Send a sentinel message to tell consumer we're done To generate a fake message, the gets a random number between one and one hundred. It calls on the to send it to the . The also uses a value to signal the consumer to stop after it has sent ten values. This is a little awkward, but don’t worry, you’ll see ways to get rid of this value after you work through this example. On the other side of the is the consumer: \"\"\"Pretend we're saving a number in the database.\"\"\" The reads a message from the and writes it to a fake database, which in this case is just printing it to the display. If it gets the value, it returns from the function, which will terminate the thread. Before you look at the really interesting part, the , here’s the section, which spawns these threads: This should look fairly familiar as it’s close to the code in the previous examples. Remember that you can turn on logging to see all of the logging messages by uncommenting this line: It can be worthwhile to walk through the logging messages to see exactly where each thread acquires and releases the locks. Now let’s take a look at the that passes messages from the to the : Class to allow a single element pipeline between producer and consumer. Woah! That’s a lot of code. A pretty high percentage of that is just logging statements to make it easier to see what’s happening when you run it. Here’s the same code with all of the logging statements removed: Class to allow a single element pipeline between producer and consumer. That seems a bit more manageable. The in this version of your code has three members:\n• is a object that restricts access to the message by the thread.\n• is also a that restricts access to the message by the thread. initializes these three members and then calls on the . This is the state you want to start in. The is allowed to add a new message, but the needs to wait until a message is present. and are nearly opposites. calls on the . This is the call that will make the wait until a message is ready. Once the has acquired the , it copies out the value in and then calls on the . Releasing this lock is what allows the to insert the next message into the . Before you go on to , there’s something subtle going on in that’s pretty easy to miss. It might seem tempting to get rid of and just have the function end with . See if you can figure out why you don’t want to do that before moving on. Here’s the answer. As soon as the calls , it can be swapped out, and the can start running. That could happen before returns! This means that there is a slight possibility that when the function returns , that could actually be the next message generated, so you would lose the first message. This is another example of a race condition. Moving on to , you can see the opposite side of the transaction. The will call this with a message. It will acquire the , set the , and the call on then , which will allow the to read that value. Let’s run the code that has logging set to and see what it looks like: At first, you might find it odd that the producer gets two messages before the consumer even runs. If you look back at the and , you will notice that the only place it will wait for a is when it attempts to put the message into the pipeline. This is done after the gets the message and logs that it has it. When the attempts to send this second message, it will call the second time and it will block. The operating system can swap threads at any time, but it generally lets each thread have a reasonable amount of time to run before swapping it out. That’s why the usually runs until it blocks in the second call to . Once a thread is blocked, however, the operating system will always swap it out and find a different thread to run. In this case, the only other thread with anything to do is the . The calls , which reads the message and calls on the , thus allowing the to run again the next time threads are swapped. Notice that the first message was , and that is exactly what the read, even though the had already generated the message. While it works for this limited test, it is not a great solution to the producer-consumer problem in general because it only allows a single value in the pipeline at a time. When the gets a burst of messages, it will have nowhere to put them. Let’s move on to a better way to solve this problem, using a . If you want to be able to handle more than one value in the pipeline at a time, you’ll need a data structure for the pipeline that allows the number to grow and shrink as data backs up from the . Python’s standard library has a module which, in turn, has a class. Let’s change the to use a instead of just a variable protected by a . You’ll also use a different way to stop the worker threads by using a different primitive from Python , an . Let’s start with the . The object allows one thread to signal an while many other threads can be waiting for that to happen. The key usage in this code is that the threads that are waiting for the event do not necessarily need to stop what they are doing, they can just check the status of the every once in a while. The triggering of the event can be many things. In this example, the main thread will simply sleep for a while and then it: The only changes here are the creation of the object on line 8, passing the as a parameter on lines 10 and 11, and the final section on lines 13 to 15, which sleep for a second, log a message, and then call on the event. The also did not have to change too much: \"\"\"Pretend we're getting a number from the network.\"\"\" It now will loop until it sees that the event was set on line 3. It also no longer puts the value into the . had to change a little more: \"\"\"Pretend we're saving a number in the database.\"\"\" While you got to take out the code related to the value, you did have to do a slightly more complicated condition. Not only does it loop until the is set, but it also needs to keep looping until the has been emptied. Making sure the queue is empty before the consumer finishes prevents another fun issue. If the does exit while the has messages in it, there are two bad things that can happen. The first is that you lose those final messages, but the more serious one is that the can get caught attempting to add a message to a full queue and never return. This happens if the gets triggered after the has checked the condition but before it calls . If that happens, it’s possible for the consumer to wake up and exit with the queue still completely full. The will then call which will wait until there is space on the queue for the new message. The has already exited, so this will not happen and the will not exit. The rest of the should look familiar. The has changed dramatically, however: :about to get from queue\" You can see that is a subclass of . has an optional parameter when initializing to specify a maximum size of the queue. If you give a positive number for , it will limit the queue to that number of elements, causing to block until there are fewer than elements. If you don’t specify , then the queue will grow to the limits of your computer’s memory. and got much smaller. They basically wrap and on the . You might be wondering where all of the locking code that prevents the threads from causing race conditions went. The core devs who wrote the standard library knew that a is frequently used in multi-threading environments and incorporated all of that locking code inside the itself. is thread-safe. Running this program looks like the following: If you read through the output in my example, you can see some interesting things happening. Right at the top, you can see the got to create five messages and place four of them on the queue. It got swapped out by the operating system before it could place the fifth one. The then ran and pulled off the first message. It printed out that message as well as how deep the queue was at that point: This is how you know that the fifth message hasn’t made it into the yet. The queue is down to size three after a single message was removed. You also know that the can hold ten messages, so the thread didn’t get blocked by the . It was swapped out by the OS. Note: Your output will be different. Your output will change from run to run. That’s the fun part of working with threads! As the program starts to wrap up, can you see the main thread generating the which causes the to exit immediately. The still has a bunch of work do to, so it keeps running until it has cleaned out the . Try playing with different queue sizes and calls to in the or the to simulate longer network or disk access times respectively. Even slight changes to these elements of the program will make large differences in your results. This is a much better solution to the producer-consumer problem, but you can simplify it even more. The really isn’t needed for this problem. Once you take away the logging, it just becomes a . Here’s what the final code looks like using directly: \"\"\"Pretend we're getting a number from the network.\"\"\" \"\"\"Pretend we're saving a number in the database.\"\"\" That’s easier to read and shows how using Python’s built-in primitives can simplify a complex problem. and are handy classes to solve concurrency issues, but there are others provided by the standard library. Before you wrap up this tutorial, let’s do a quick survey of some of them.\n\nThere are a few more primitives offered by the Python module. While you didn’t need these for the examples above, they can come in handy in different use cases, so it’s good to be familiar with them. The first Python object to look at is . A is a counter with a few special properties. The first one is that the counting is atomic. This means that there is a guarantee that the operating system will not swap out the thread in the middle of incrementing or decrementing the counter. The internal counter is incremented when you call and decremented when you call . The next special property is that if a thread calls when the counter is zero, that thread will block until a different thread calls and increments the counter to one. Semaphores are frequently used to protect a resource that has a limited capacity. An example would be if you have a pool of connections and want to limit the size of that pool to a specific number. A is a way to schedule a function to be called after a certain amount of time has passed. You create a by passing in a number of seconds to wait and a function to call: You start the by calling . The function will be called on a new thread at some point after the specified time, but be aware that there is no promise that it will be called exactly at the time you want. If you want to stop a that you’ve already started, you can cancel it by calling . Calling after the has triggered does nothing and does not produce an exception. A can be used to prompt a user for action after a specific amount of time. If the user does the action before the expires, can be called. A can be used to keep a fixed number of threads in sync. When creating a , the caller must specify how many threads will be synchronizing on it. Each thread calls on the . They all will remain blocked until the specified number of threads are waiting, and then the are all released at the same time. Remember that threads are scheduled by the operating system so, even though all of the threads are released simultaneously, they will be scheduled to run one at a time. One use for a is to allow a pool of threads to initialize themselves. Having the threads wait on a after they are initialized will ensure that none of the threads start running before all of the threads are finished with their initialization."
    },
    {
        "link": "https://codecademy.com/resources/docs/python/threading",
        "document": "The module allows multiple threads of execution to take place in a Python program.\n\nWhile threads may appear to run simultaneously, only one thread can be executed at a time. This is enforced by Python’s global interpreter lock.\n\nThreading is helpful when working with tasks that are I/O bound. This includes web-oriented tasks like scraping or downloading files.\n\nThe module must first be imported before thread constants can be created and their methods can be used.\n\nThe following example features five threads that are created, started, and end at different points before the program finishes:\n\nThis results in output like the following:"
    },
    {
        "link": "https://stratascratch.com/blog/python-threading-like-a-pro",
        "document": "What is threading in Python, and what is it useful for? We’ll explain everything: what it is, how it works, and give you some code examples to get you started.\n\nThe concurrent execution of more than one sequential set is commonly known as \"thread of execution”. Or simply \"threading\", if you want to use its pet name.\n\nIn the context of Python, threading is a built-in module that allows various threads to execute concurrently.\n\nThreads run in the same unique memory heap. That way, each thread can access every variable and data structure of the program. While they do share the same heap space, each thread has its own stack.\n\nWith Python threading, you can increase the efficiency of I/O-bound programs, where the majority of time is spent waiting for input or output operations to complete.\n\nYour process can be single-threaded and multi-threaded. Python multi-threading means there are two or more threads started concurrently.\n\nHere’s a simple visualization that shows these two approaches of Python threading.\n\nAdvantages and Use Cases of Python Threading\n\nThe main advantage of Python threading is that it allows programs to run simultaneously. Also, when a program is performing multiple tasks independently, this can be made easier by threading. It also uses the system resources more efficiently, which increases efficiency, and provides a smoother user experience.\n\n\n\nThe most typical use cases of Python threading are:\n• I/O-bound programs: The I/O-bound program is any application that reads and writes data from the input-output system and waits for data from it. If tasks wait too much from external resources, they are perfect candidates for threading. Examples of these tasks might be reading/writing files, network interactions, or user input.\n• GUI applications: When the user is using Graphical User Interface (GUI), it must remain responsive (GUI, not the user) despite the tasks being performed in the background. This is achieved by threading in Python.\n• Simulations and modeling: Python threading is typically used in simulations where several entities are acting independently.\n\nCreating threads in Python is made simple by its threading module.\n\nThe syntax for doing that is given below. It involves three steps:\n\nFunctions play a crucial role in Python threading. As you already saw, when creating a new thread, we specify a function the thread will execute. The function includes the code that the thread will run.\n\n\n\nSo how do we utilize this function? It’s simple: just use the start() function.\n\nContinuing the steps of the generic example above means you have to write this code:\n\nLet’s now see how this syntax works in a real example.\n\nCreating a single thread in Python is straightforward. We basically already covered that in the previous section. Now, let’s see how the syntax works when creating useful single threads in Python.\n\nThis is the simplest possible example of how the Python threading logic works.\n\nWe define the function hello_world and print the message. Then we create a thread t, whose target function is hello_world.\n\n\n\nIn the end, we use the start() method to run the thread.\n\nHere’s a more complex example. It demonstrates the background task. This can be considered a single-thread example since we define only one thread.\n\nHowever, depending on the complexity of the tasks, this can also be considered a multithreading example because the time-consuming task will be split up by the threading module.\n\n\n\nWe import the necessary modules. You know what the Python threading module is for. We use the time module to introduce a delay in the execution.\n\n\n\nWe then define the function worker that we want our thread to execute. This function simply prints a message, waits for 2 seconds (simulating a time-consuming task), and then prints another message. The waiting time is determined by the sleep() function.\n\n\n\nNow comes creating a new Thread object, t. We pass the worker function as the target argument to the Thread constructor. This tells the thread to execute the worker function when it starts.\n\nWe start the thread by using the start() function. Python begins executing the worker function in the new thread. Note that calling start() doesn't mean the thread will immediately execute. It's up to the operating system's scheduler to determine when the thread will actually start running.\n\nFinally, we print a message from the main thread indicating that the main thread execution has ended.\n\n\n\nWorking with multiple threads in Python involves creating several threads and executing them concurrently.\n\nIt is not much different than working with a single thread. Actually, the process is the same, only there is more of the same.\n\nYou define at least two functions and then create at least two thread instances. Finally, you call the functions from the threads using the start() function and do that at least two times.\n\n\n\nHere’s the syntax that shows these steps.\n\n\n\nLet’s now see how this works in practice.\n\nLet's say we want to monitor the system's CPU and memory usage every 5 seconds, and we want to do this concurrently. We can use Python's built-in psutil library to get system information and threading to run the monitoring tasks concurrently.\n\n\n\nThe print_cpu_usage and print_memory_usage functions are defined. Both functions run indefinitely due to their while loops.\n\n\n\nThe print_cpu_usage function retrieves the CPU usage every second using psutil.cpu_percent(interval=1) and prints it. Then, it sleeps for 5 seconds before repeating the process.\n\nThe print_memory_usage function retrieves the system's memory usage using psutil.virtual_memory().percent and prints it. It, too, sleeps for 5 seconds before repeating the process.\n\n\n\nNow, we create threads t1 and t2. The target function for t1 is print_cpu_usage, and for t2, it's print_memory_usage.\n\n\n\nIt’s time to execute the threads. We do that by calling the start() method on both thread objects.\n\nThe result of this code is two threads running concurrently, each repeatedly printing the current CPU usage and memory usage of the system every 5 seconds. Because these tasks are running in separate threads, they can operate independently and concurrently without blocking each other.\n\nYou can see the output below.\n\nIt’s not necessary that the two threads have two different target functions. You can also assign one function to multiple threads. This becomes easy when you use the for loop. Utilizing the loop also makes it possible to start multiple threads with only one start() method.\n\n\n\nThe below code shows you a web scraper that does exactly that.\n\n\n\nThe only function we define is download_file. This function takes two arguments, url and filename. The function fetches the files from the provided URLs using the requests.get() method and saves it with the given filename.\n\nThe file is written in binary mode ('wb'), which is required for non-text files like images, executables, etc. It can also be used for text files without any problems. After the file is downloaded and saved, a message is printed to the console indicating the download is complete.\n\nWe now define lists of URLs and filenames. The urls list contains the URLs of two CSV files to download. We use the files from data.gov. The filenames list contains the corresponding filenames to save the downloaded files under.\n\nThen, we create and start threads. A for loop is used to create and start a new thread for each URL/filename pair. The threading.Thread() function creates a new thread, with the target parameter specifying the function to run in the new thread (in this case, download_file). The args parameter is a tuple specifying the arguments to pass to the target function. The start() method then starts the newly created thread, which begins its execution. This results in the files being downloaded concurrently rather than one at a time.\n\nExample 3: Downloading a File From the Internet\n\nHere’s one interesting example where we’ll have only one thread, but this can be considered multithreading.\n\n\n\nIn this example, a thread is used to perform an I/O-bound task. More specifically, we’re downloading a file from the internet. In this case, it will be a ‘Better Call Saul’ Wikipedia article.\n\nIf we download only one file, like in our example, it’s a single thread. But if there are multiple files to be downloaded, this becomes a multi-threading example.\n\n\n\nWe define the download_file function. It prints a message to the console indicating that the file download is starting.\n\n\n\nThen we use the get request to the specified URL and store the server's response in the response variable. After that, the function opens a file in write-binary mode at the specified path (here, it's 'F:\\BetterCallSaul.json'). If the file does not exist, it is created. The file is referred to as f within the block.\n\nWe use the write() function to write the content of the server's response to the file. The content is retrieved from the response object using the content attribute, and we print a message that the file was downloaded.\n\nNext, we again use the print() function to print a message and create a new thread object that will execute the download_file function when started.\n\n\n\nAfter that, there’s the start() function that starts the thread, which begins executing the download_file function.\n\nThe remaining lines of the code represent operations happening in the main thread while the file is being downloaded by the separate thread. The sleep() function makes the main thread wait for 2 seconds.\n\nHere’s the code output. And, trust me, the downloaded file really is on my F: disc.\n\nHow to Stop a Thread in Python\n\nWe learned how to create and run threads in Python. It seems logical it’s also possible to stop the thread once it starts.\n\nIn Python, threads are not directly stoppable. If you think there’s some kind of Thread.stop() method that can stop a thread's execution, we’ll have to disappoint you; there isn’t.\n\nThis is largely because abruptly stopping a thread is a bad idea. Why? The thread could be holding resources, e.g., file handles or database connections. Force-stopping the thread without properly releasing these resources can lead to various problems.\n\n\n\nHowever, there are a few techniques available to stop a thread effectively:\n\nUsing a Flag to Stop the Thread in Python\n\nUsing a flag to signal the thread to stop executing involves periodically checking the flag within the thread's target function and returning from the function if the flag is set:\n\nThe approach is shown below.\n\nIn this code, an Event object is used as the flag. The stop method sets the flag, and the run method checks the flag in each iteration of its loop.\n\nExample: Monitoring the Disk Usage of the System\n\nThreading here is used to monitor the disk usage of the system. If the disk usage crosses a certain threshold, it will print a warning message. This monitoring will continue until the thread is stopped using the stop method.\n\nApart from already familiar Python modules, we also import shutil. It contains the disk_usage method we’ll use to get the disk space information.\n\nWe create the class DiskMonitorThread which inherits from threading.Thread, which means it represents a separate thread of execution in the program.\n\n\n\nThen we initialize the thread. The __init__ method is the constructor for the class. We set the default threshold for disk usage warning to 80%. We create an event object which can be set and checked (used to stop the thread gracefully) by: self._stop_event = threading.Event().\n\nThe stop method sets the _stop_event. This will signal the thread to stop its operation.\n\nThe run method is what the thread executes when it starts. Every 10 seconds, it continually checks the disk usage using the disk_usage() method. If the disk usage exceeds the specified threshold, it prints a warning message. The loop will continue until _stop_event is set, meaning the thread will keep monitoring until it's explicitly stopped.\n\n\n\nThen we create an instance of the DiskMonitorThread with a threshold of 50% and starts it.\n\nWe allow for the thread to run for 60 seconds. After 60 seconds, regardless of any exceptions or errors, the finally block will execute, stopping the thread using the stop() method and then waiting for the thread to finish with monitor.join(). With this, we also use another method for stopping the thread: join(). We’ll cover it in the next section.\n\nBut let’s first have a look at the output of our code here.\n\nUsing a Join With Timeout to Stop the Thread in Python\n\nYou can use the join() method with a timeout to stop waiting for a thread after a certain amount of time. This doesn't actually stop the thread's execution; it just allows the main thread (or the thread that called join) to continue executing:\n\nThe generic code for using this method is here.\n\nIt makes the main thread to stop waiting for t and continue executing after 1.0 seconds.\n\nWe’ll create a simple threaded program that simulates downloading some data over a period of time.\n\nThe download_data function simulates a downloading process. In it, a message indicating the start of the download is printed. A for loop simulates the download in 5 steps. With each iteration, it prints a message indicating the percentage downloaded and then sleeps for 0.5 seconds to simulate the time taken to download that part. After all iterations are completed, the message is printed.\n\n\n\nWe create a thread object t with download_data as its target function. Then, we use the start() method to run the target function in a separate thread.\n\nNow comes stopping the Python thread. The join() method makes the main thread wait for the t thread to complete its execution, but only up to 1 second, which is defined in the timeout parameter. If the thread t hasn't finished within this time, the main thread will stop waiting and continue its execution. In this case, since our simulated download takes 2.5 seconds in total, the main thread will move on after 1 second of waiting.\n\nAfter the join or timeout, the main thread will print a message we specified.\n\nUsing Daemon Threads to Stop the Thread\n\nA daemon thread is a thread that doesn’t prevent the program from exiting. If the program ends or all non-daemon threads finish execution, any remaining daemon threads are stopped.\n\nIn this code, t is a daemon thread. If the rest of the program finishes while t is still running, t will be stopped.\n\nHere’s an example that uses a simulated server status function.\n\nThe server_status function simulates a background task that constantly prints the server status. It runs indefinitely because of the while True loop. Every two seconds, it prints the server's status.\n\nThen, we create a new thread, t, whose target function is server_status.\n\nBy setting t.daemon = True, the t thread is marked as a daemon. Daemon threads are background threads that automatically exit as soon as all non-daemon threads (typically the main program) have completed. This means that when the main server function (run_server) completes, the daemon thread (t) will automatically stop, and you won't have to manually stop it. We then use the start() method to start this thread.\n\nThe run_server function simulates the main work of the server. It \"processes data\" ten times, with a delay of 1 second between each iteration. This simulates the server's main functionality.\n\n\n\nAfter this function completes, the program prints \"Server shutdown.\" to indicate the end of the server's operation. Since the daemon thread will automatically stop when the main program completes, there's no need to manually stop or join the background thread.\n\n\n\nWe’ve been using Python’s threading module in our examples so far. This is now a good place to take a look at its commonly used tools and some other modules for threading.\n\nPython's threading module provides some tools that can help you manage and coordinate threads.\n\nLock: This is a synchronization tool, and it’s the simplest one in Python. It ensures that only one thread executes a particular part of code at a time. When a thread acquires a Lock, no other thread can acquire it until the original thread releases it.\n\nEvent: This is another synchronization tool. Event allows one thread to signal one or more other threads that a particular event has occurred.\n\nCondition: This is a more complex synchronization tool that allows one thread to wait for a particular condition to be met while other threads can signal that the condition has been met. Here’s how it’s explained in the official Python documentation.\n\nSemaphore: Semaphore is a synchronization tool that controls access to a common resource by multiple processes in a concurrent system and helps in avoiding the critical section problem.\n\nAsyncio is a library introduced in Python 3.3. It is used for writing single-threaded concurrent code using coroutines, multiplexing I/O access over sockets and other resources, running network clients and servers, and other related primitives.\n\nHere are some of its beneficial features.\n\n\n\nCoroutines: These are special functions that can give up control to their caller without losing their state. Coroutines are prefixed with the async keyword. They can use the await keyword to call other coroutines.\n\n\n\nEvent Loop: At the core of every asyncio application is the event loop. It schedules asynchronous tasks and callbacks, handles I/O, and manages subprocesses.\n\nTasks: These are a type of coroutine that can run concurrently with other tasks. Tasks are equivalent to threads but without the need for traditional synchronization tools.\n\nThe concurrent.futures library is not directly related to Python threading but to concurrency. It provides a high-level interface for asynchronously executing functions using threads or processes. The two primary classes of interest are ThreadPoolExecutor and ProcessPoolExecutor.\n\nThreadPoolExecutor: This executor class is used for parallelizing the execution of tasks using threads. It is suitable for I/O-bound tasks since threads in Python, especially with the standard CPython interpreter, are hampered by the Global Interpreter Lock (GIL) when it comes to CPU-bound tasks.\n\n\n\nProcessPoolExecutor: This executor class uses processes to parallelize the execution of tasks. It is particularly suitable for CPU-bound tasks because each process runs in its own Python interpreter with its own independent GIL. This allows you to truly exploit multiple CPU cores, bypassing the GIL limitation.\n• Suitable for CPU-intensive tasks, as it bypasses the GIL.\n\n\n\nThe key differences between ThreadPoolExecutor and ProcessPoolExecutor are:\n\n\n\n1. Concurrency Model: ThreadPoolExecutor uses threads (lightweight, same memory space) while ProcessPoolExecutor uses processes (heavier, separate memory space).\n• ThreadPoolExecutor is typically used for I/O-bound tasks where you spend more time waiting for something (like network or disk operations) than doing CPU-intensive computations.\n• ProcessPoolExecutor is best suited for CPU-bound tasks that require significant computation.\n\n3. Memory: Threads in a ThreadPoolExecutor share the same memory space. This makes it easy for them to share data but also makes them susceptible to race conditions. Meanwhile, each process in a ProcessPoolExecutor has its own memory space. Sharing data between processes is slower because it requires serialization and inter-process communication.\n\n\n\n4. GIL (Global Interpreter Lock): This is a mutex in CPython that ensures only one thread can execute Python bytecode at a time. ThreadPoolExecutor is affected by the GIL, making it less suitable for CPU-bound tasks in CPython. ProcessPoolExecutor, using separate processes, bypasses the GIL entirely.\n\n\n\n5. Overhead: Threads have a lower overhead compared to processes. Starting a process is slower and requires more resources. However, the isolation of processes can be beneficial for stability and data security\n\nApart from the three mentioned above, there are also some other Python libraries you may find useful when dealing with threading.\n\ngreenlet and gevent: Third-party libraries that provide lightweight, \"green\" threading via coroutines. They don't offer true parallel execution – this is similar to asyncio – but can achieve high levels of concurrency. Here’s the official documentation for greenlet and gevent.\n\nTwisted: This is an older, event-driven network programming framework that's been used to build many networked apps. Twisted is not as user-friendly as asyncio, but you might like its robustness.\n\nQuart and FastAPI: These are modern asynchronous web frameworks that allow for handling large numbers of simultaneous connections, useful in building scalable web APIs.\n\n\n\nTo sum up, traditional threading (like the threading module) is about multiple threads of execution and is subject to the Global Interpreter Lock (GIL) in CPython, which means only one thread executes Python bytecode at a time. However, I/O-bound tasks can benefit from this model.\n\n\n\nOn the other hand, asyncio, greenlet, gevent, and similar tools use an asynchronous, event-driven model where context switches are made in user-space (cooperative multitasking). They excel in scenarios with high I/O wait times, like many network operations.\n\nSpeaking of libraries, there are plenty more threading non-related libraries that data scientist should know. We talked about them already in the Python Libraries article.\n\nUnderstanding the distinctions between threading and asynchronous programming can provide you with the flexibility to choose the right tool for the job.\n\nAs previously mentioned, Python's threading module allows for concurrent programming. But due to Python's Global Interpreter Lock (GIL), threads are limited to running on a single processor core. This means that while threads can be useful for I/O-bound tasks, they don't provide true parallelism for CPU-bound tasks.\n\nFor real parallel computing, we could utilize Python's multiprocessing module, which allows the creation of multiple processes, each having its own Python interpreter and memory space. While this could lead to performance improvements for CPU-bound tasks, it also introduces additional complexity and resource usage.\n\nWe can visualize multiprocessing like this.\n\nMultiprocessing can be synchronous and asynchronous. Asynchronous programming in Python, often associated with the asyncio library, is a different approach to concurrency. Asynchronous programming is single-threaded, but it uses non-blocking I/O operations and coroutines to perform multiple tasks concurrently without the need for threads.\n\nHere’s the visualization of how synchronous and asynchronous multiprocessing works.\n\nFrom the above image, the main difference is obvious. The synchronous tasks are executed in a sequence. One task starts only after the previous one has been completed. It waits for an operation to complete before moving on.\n\nOn the other hand, asynchronous tasks are executed out of sequence. They don't wait for one task to complete before moving on to the next one. An operation can be started, set aside, and another operation can be initiated.\n\nHere’s an overview of all the main differences between synchronous and asynchronous multiprocessing.\n\nBest Practices for Managing Threads in Python\n\nDespite Python's ease of use for threading, there are still some best practices we advise you to follow when working with threads.\n\nAvoid sharing state: Try to keep data confined to individual threads as much as possible. A shared state can lead to complicated race conditions and bugs that are hard to track down.\n\n\n\nDon't forget to join: If a program ends before a thread has completed its task, the thread will be killed, potentially leaving resources in an uncertain state. Always use join() to ensure all threads have been completed before the program ends.\n\nUse the appropriate synchronization tool: Different synchronization tools are appropriate for different situations. Use the simplest tool that fulfills your requirements.\n\nDon't use too many threads: Each thread requires resources. Using too many threads can actually decrease performance due to the overhead of creating and destroying threads.\n\n\n\nTest thoroughly: Threading bugs can be elusive and are often timing-dependent. Make sure to thoroughly test your threaded programs under various conditions.\n\nIn conclusion, Python's threading module is a powerful tool for adding concurrency to your programs. While it requires careful management and testing, it can greatly enhance the performance and responsiveness of your Python applications.\n\nIn this article, we talked about how you can use Python threading. We showed you how to create and use single and multiple threads in examples such as I/O bound, GUI applications, and simulations, and modeling.\n\nWe leveraged the threading Python module, but it’s not the only option for threading libraries in Python – there are also libraries such as asyncio, concurrent.futures, greenlet, gevent, Twisted, Quart, and Fast API.\n\nPython threading is a complex topic, and we could go on and on about it. This article should give you a good start in exploring threading in Python. To use it properly, you need to have a solid knowledge of other Python programming concepts, which you can learn and practice in our coding questions. Some of the examples are given in the Python Interview Questions article."
    },
    {
        "link": "https://docs.python.org/2/library/threading.html",
        "document": "This module constructs higher-level threading interfaces on top of the lower level module. See also the and modules.\n\nThe module is provided for situations where cannot be used because is missing.\n\nThis module defines the following functions and objects:\n\nDetailed interfaces for the objects are documented below.\n\nThe design of this module is loosely based on Java’s threading model. However, where Java makes locks and condition variables basic behavior of every object, they are separate objects in Python. Python’s class supports a subset of the behavior of Java’s Thread class; currently, there are no priorities, no thread groups, and threads cannot be destroyed, stopped, suspended, resumed, or interrupted. The static methods of Java’s Thread class, when implemented, are mapped to module-level functions.\n\nAll of the methods described below are executed atomically.\n\nThis class represents an activity that is run in a separate thread of control. There are two ways to specify the activity: by passing a callable object to the constructor, or by overriding the method in a subclass. No other methods (except for the constructor) should be overridden in a subclass. In other words, only override the and methods of this class. Once a thread object is created, its activity must be started by calling the thread’s method. This invokes the method in a separate thread of control. Once the thread’s activity is started, the thread is considered ‘alive’. It stops being alive when its method terminates – either normally, or by raising an unhandled exception. The method tests whether the thread is alive. Other threads can call a thread’s method. This blocks the calling thread until the thread whose method is called is terminated. A thread has a name. The name can be passed to the constructor, and read or changed through the attribute. A thread can be flagged as a “daemon thread”. The significance of this flag is that the entire Python program exits when only daemon threads are left. The initial value is inherited from the creating thread. The flag can be set through the property. Daemon threads are abruptly stopped at shutdown. Their resources (such as open files, database transactions, etc.) may not be released properly. If you want your threads to stop gracefully, make them non-daemonic and use a suitable signalling mechanism such as an . There is a “main thread” object; this corresponds to the initial thread of control in the Python program. It is not a daemon thread. There is the possibility that “dummy thread objects” are created. These are thread objects corresponding to “alien threads”, which are threads of control started outside the threading module, such as directly from C code. Dummy thread objects have limited functionality; they are always considered alive and daemonic, and cannot be ed. They are never deleted, since it is impossible to detect the termination of alien threads. This constructor should always be called with keyword arguments. Arguments are: group should be ; reserved for future extension when a class is implemented. target is the callable object to be invoked by the method. Defaults to , meaning nothing is called. name is the thread name. By default, a unique name is constructed of the form “Thread-N” where N is a small decimal number. args is the argument tuple for the target invocation. Defaults to . kwargs is a dictionary of keyword arguments for the target invocation. Defaults to . If the subclass overrides the constructor, it must make sure to invoke the base class constructor ( ) before doing anything else to the thread. It must be called at most once per thread object. It arranges for the object’s method to be invoked in a separate thread of control. This method will raise a if called more than once on the same thread object. You may override this method in a subclass. The standard method invokes the callable object passed to the object’s constructor as the target argument, if any, with sequential and keyword arguments taken from the args and kwargs arguments, respectively. Wait until the thread terminates. This blocks the calling thread until the thread whose method is called terminates – either normally or through an unhandled exception – or until the optional timeout occurs. When the timeout argument is present and not , it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof). As always returns , you must call after to decide whether a timeout happened – if the thread is still alive, the call timed out. When the timeout argument is not present or , the operation will block until the thread terminates. A thread can be ed many times. raises a if an attempt is made to join the current thread as that would cause a deadlock. It is also an error to a thread before it has been started and attempts to do so raises the same exception. A string used for identification purposes only. It has no semantics. Multiple threads may be given the same name. The initial name is set by the constructor. The ‘thread identifier’ of this thread or if the thread has not been started. This is a nonzero integer. See the function. Thread identifiers may be recycled when a thread exits and another thread is created. The identifier is available even after the thread has exited. Return whether the thread is alive. This method returns just before the method starts until just after the method terminates. The module function returns a list of all alive threads. A boolean value indicating whether this thread is a daemon thread (True) or not (False). This must be set before is called, otherwise is raised. Its initial value is inherited from the creating thread; the main thread is not a daemon thread and therefore all threads created in the main thread default to = . The entire Python program exits when no alive non-daemon threads are left.\n\nA primitive lock is a synchronization primitive that is not owned by a particular thread when locked. In Python, it is currently the lowest level synchronization primitive available, implemented directly by the extension module. A primitive lock is in one of two states, “locked” or “unlocked”. It is created in the unlocked state. It has two basic methods, and . When the state is unlocked, changes the state to locked and returns immediately. When the state is locked, blocks until a call to in another thread changes it to unlocked, then the call resets it to locked and returns. The method should only be called in the locked state; it changes the state to unlocked and returns immediately. If an attempt is made to release an unlocked lock, a will be raised. When more than one thread is blocked in waiting for the state to turn to unlocked, only one thread proceeds when a call resets the state to unlocked; which one of the waiting threads proceeds is not defined, and may vary across implementations. When invoked with the blocking argument set to (the default), block until the lock is unlocked, then set it to locked and return . When invoked with the blocking argument set to , do not block. If a call with blocking set to would block, return immediately; otherwise, set the lock to locked and return . When the lock is locked, reset it to unlocked, and return. If any other threads are blocked waiting for the lock to become unlocked, allow exactly one of them to proceed. When invoked on an unlocked lock, a is raised. There is no return value. Return true if the lock is acquired.\n\nA reentrant lock is a synchronization primitive that may be acquired multiple times by the same thread. Internally, it uses the concepts of “owning thread” and “recursion level” in addition to the locked/unlocked state used by primitive locks. In the locked state, some thread owns the lock; in the unlocked state, no thread owns it. To lock the lock, a thread calls its method; this returns once the thread owns the lock. To unlock the lock, a thread calls its method. / call pairs may be nested; only the final (the of the outermost pair) resets the lock to unlocked and allows another thread blocked in to proceed. When invoked without arguments: if this thread already owns the lock, increment the recursion level by one, and return immediately. Otherwise, if another thread owns the lock, block until the lock is unlocked. Once the lock is unlocked (not owned by any thread), then grab ownership, set the recursion level to one, and return. If more than one thread is blocked waiting until the lock is unlocked, only one at a time will be able to grab ownership of the lock. There is no return value in this case. When invoked with the blocking argument set to true, do the same thing as when called without arguments, and return true. When invoked with the blocking argument set to false, do not block. If a call without an argument would block, return false immediately; otherwise, do the same thing as when called without arguments, and return true. Release a lock, decrementing the recursion level. If after the decrement it is zero, reset the lock to unlocked (not owned by any thread), and if any other threads are blocked waiting for the lock to become unlocked, allow exactly one of them to proceed. If after the decrement the recursion level is still nonzero, the lock remains locked and owned by the calling thread. Only call this method when the calling thread owns the lock. A is raised if this method is called when the lock is unlocked. There is no return value.\n\nA condition variable is always associated with some kind of lock; this can be passed in or one will be created by default. (Passing one in is useful when several condition variables must share the same lock.) A condition variable has and methods that call the corresponding methods of the associated lock. It also has a method, and and methods. These three must only be called when the calling thread has acquired the lock, otherwise a is raised. The method releases the lock, and then blocks until it is awakened by a or call for the same condition variable in another thread. Once awakened, it re-acquires the lock and returns. It is also possible to specify a timeout. The method wakes up one of the threads waiting for the condition variable, if any are waiting. The method wakes up all threads waiting for the condition variable. Note: the and methods don’t release the lock; this means that the thread or threads awakened will not return from their call immediately, but only when the thread that called or finally relinquishes ownership of the lock. Tip: the typical programming style using condition variables uses the lock to synchronize access to some shared state; threads that are interested in a particular change of state call repeatedly until they see the desired state, while threads that modify the state call or when they change the state in such a way that it could possibly be a desired state for one of the waiters. For example, the following code is a generic producer-consumer situation with unlimited buffer capacity: To choose between and , consider whether one state change can be interesting for only one or several waiting threads. E.g. in a typical producer-consumer situation, adding one item to the buffer only needs to wake up one consumer thread. If the lock argument is given and not , it must be a or object, and it is used as the underlying lock. Otherwise, a new object is created and used as the underlying lock. Acquire the underlying lock. This method calls the corresponding method on the underlying lock; the return value is whatever that method returns. Release the underlying lock. This method calls the corresponding method on the underlying lock; there is no return value. Wait until notified or until a timeout occurs. If the calling thread has not acquired the lock when this method is called, a is raised. This method releases the underlying lock, and then blocks until it is awakened by a or call for the same condition variable in another thread, or until the optional timeout occurs. Once awakened or timed out, it re-acquires the lock and returns. When the timeout argument is present and not , it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof). When the underlying lock is an , it is not released using its method, since this may not actually unlock the lock when it was acquired multiple times recursively. Instead, an internal interface of the class is used, which really unlocks it even when it has been recursively acquired several times. Another internal interface is then used to restore the recursion level when the lock is reacquired. By default, wake up one thread waiting on this condition, if any. If the calling thread has not acquired the lock when this method is called, a is raised. This method wakes up at most n of the threads waiting for the condition variable; it is a no-op if no threads are waiting. The current implementation wakes up exactly n threads, if at least n threads are waiting. However, it’s not safe to rely on this behavior. A future, optimized implementation may occasionally wake up more than n threads. Note: an awakened thread does not actually return from its call until it can reacquire the lock. Since does not release the lock, its caller should. Wake up all threads waiting on this condition. This method acts like , but wakes up all waiting threads instead of one. If the calling thread has not acquired the lock when this method is called, a is raised."
    },
    {
        "link": "https://docs.python.org/3/library/queue.html",
        "document": "The module implements multi-producer, multi-consumer queues. It is especially useful in threaded programming when information must be exchanged safely between multiple threads. The class in this module implements all the required locking semantics.\n\nThe module implements three types of queue, which differ only in the order in which the entries are retrieved. In a queue, the first tasks added are the first retrieved. In a queue, the most recently added entry is the first retrieved (operating like a stack). With a priority queue, the entries are kept sorted (using the module) and the lowest valued entry is retrieved first.\n\nInternally, those three types of queues use locks to temporarily block competing threads; however, they are not designed to handle reentrancy within a thread.\n\nIn addition, the module implements a “simple” queue type, , whose specific implementation provides additional guarantees in exchange for the smaller functionality.\n\nThe module defines the following classes and exceptions:\n\nConstructor for a priority queue. maxsize is an integer that sets the upperbound limit on the number of items that can be placed in the queue. Insertion will block once this size has been reached, until queue items are consumed. If maxsize is less than or equal to zero, the queue size is infinite. The lowest valued entries are retrieved first (the lowest valued entry is the one that would be returned by ). A typical pattern for entries is a tuple in the form: . If the data elements are not comparable, the data can be wrapped in a class that ignores the data item and only compares the priority number:\n\nQueue objects ( , , or ) provide the public methods described below. Return the approximate size of the queue. Note, qsize() > 0 doesn’t guarantee that a subsequent get() will not block, nor will qsize() < maxsize guarantee that put() will not block. Return if the queue is empty, otherwise. If empty() returns it doesn’t guarantee that a subsequent call to put() will not block. Similarly, if empty() returns it doesn’t guarantee that a subsequent call to get() will not block. Return if the queue is full, otherwise. If full() returns it doesn’t guarantee that a subsequent call to get() will not block. Similarly, if full() returns it doesn’t guarantee that a subsequent call to put() will not block. Put item into the queue. If optional args block is true and timeout is (the default), block if necessary until a free slot is available. If timeout is a positive number, it blocks at most timeout seconds and raises the exception if no free slot was available within that time. Otherwise (block is false), put an item on the queue if a free slot is immediately available, else raise the exception (timeout is ignored in that case). Raises if the queue has been shut down. Remove and return an item from the queue. If optional args block is true and timeout is (the default), block if necessary until an item is available. If timeout is a positive number, it blocks at most timeout seconds and raises the exception if no item was available within that time. Otherwise (block is false), return an item if one is immediately available, else raise the exception (timeout is ignored in that case). Prior to 3.0 on POSIX systems, and for all versions on Windows, if block is true and timeout is , this operation goes into an uninterruptible wait on an underlying lock. This means that no exceptions can occur, and in particular a SIGINT will not trigger a . Raises if the queue has been shut down and is empty, or if the queue has been shut down immediately. Two methods are offered to support tracking whether enqueued tasks have been fully processed by daemon consumer threads. Indicate that a formerly enqueued task is complete. Used by queue consumer threads. For each used to fetch a task, a subsequent call to tells the queue that the processing on the task is complete. If a is currently blocking, it will resume when all items have been processed (meaning that a call was received for every item that had been into the queue). calls for each remaining item in the queue. Raises a if called more times than there were items placed in the queue. Blocks until all items in the queue have been gotten and processed. The count of unfinished tasks goes up whenever an item is added to the queue. The count goes down whenever a consumer thread calls to indicate that the item was retrieved and all work on it is complete. When the count of unfinished tasks drops to zero, unblocks. Example of how to wait for enqueued tasks to be completed: # Block until all tasks are done. objects can be made to prevent further interaction by shutting them down. Shut down the queue, making and raise . By default, on a shut down queue will only raise once the queue is empty. Set immediate to true to make raise immediately instead. All blocked callers of and will be unblocked. If immediate is true, a task will be marked as done for each remaining item in the queue, which may unblock callers of ."
    },
    {
        "link": "https://docs.python.org/3/library/threading.html",
        "document": "This module constructs higher-level threading interfaces on top of the lower level module.\n\noffers a higher level interface to push tasks to a background thread without blocking execution of the calling thread, while still being able to retrieve their results when needed. provides a thread-safe interface for exchanging data between running threads. offers an alternative approach to achieving task level concurrency without requiring the use of multiple operating system threads.\n\nThis module defines the following functions:\n\nThis module also defines the following constant:\n\nThis module defines a number of classes, which are detailed in the sections below.\n\nThe design of this module is loosely based on Java’s threading model. However, where Java makes locks and condition variables basic behavior of every object, they are separate objects in Python. Python’s class supports a subset of the behavior of Java’s Thread class; currently, there are no priorities, no thread groups, and threads cannot be destroyed, stopped, suspended, resumed, or interrupted. The static methods of Java’s Thread class, when implemented, are mapped to module-level functions.\n\nAll of the methods described below are executed atomically.\n\nThe class represents an activity that is run in a separate thread of control. There are two ways to specify the activity: by passing a callable object to the constructor, or by overriding the method in a subclass. No other methods (except for the constructor) should be overridden in a subclass. In other words, only override the and methods of this class. Once a thread object is created, its activity must be started by calling the thread’s method. This invokes the method in a separate thread of control. Once the thread’s activity is started, the thread is considered ‘alive’. It stops being alive when its method terminates – either normally, or by raising an unhandled exception. The method tests whether the thread is alive. Other threads can call a thread’s method. This blocks the calling thread until the thread whose method is called is terminated. A thread has a name. The name can be passed to the constructor, and read or changed through the attribute. If the method raises an exception, is called to handle it. By default, ignores silently . A thread can be flagged as a “daemon thread”. The significance of this flag is that the entire Python program exits when only daemon threads are left. The initial value is inherited from the creating thread. The flag can be set through the property or the daemon constructor argument. Daemon threads are abruptly stopped at shutdown. Their resources (such as open files, database transactions, etc.) may not be released properly. If you want your threads to stop gracefully, make them non-daemonic and use a suitable signalling mechanism such as an . There is a “main thread” object; this corresponds to the initial thread of control in the Python program. It is not a daemon thread. There is the possibility that “dummy thread objects” are created. These are thread objects corresponding to “alien threads”, which are threads of control started outside the threading module, such as directly from C code. Dummy thread objects have limited functionality; they are always considered alive and daemonic, and cannot be joined. They are never deleted, since it is impossible to detect the termination of alien threads. This constructor should always be called with keyword arguments. Arguments are: group should be ; reserved for future extension when a class is implemented. target is the callable object to be invoked by the method. Defaults to , meaning nothing is called. name is the thread name. By default, a unique name is constructed of the form “Thread-N” where N is a small decimal number, or “Thread-N (target)” where “target” is if the target argument is specified. args is a list or tuple of arguments for the target invocation. Defaults to . kwargs is a dictionary of keyword arguments for the target invocation. Defaults to . If not , daemon explicitly sets whether the thread is daemonic. If (the default), the daemonic property is inherited from the current thread. If the subclass overrides the constructor, it must make sure to invoke the base class constructor ( ) before doing anything else to the thread. Changed in version 3.10: Use the target name if name argument is omitted. It must be called at most once per thread object. It arranges for the object’s method to be invoked in a separate thread of control. This method will raise a if called more than once on the same thread object. You may override this method in a subclass. The standard method invokes the callable object passed to the object’s constructor as the target argument, if any, with positional and keyword arguments taken from the args and kwargs arguments, respectively. Using list or tuple as the args argument which passed to the could achieve the same effect. Wait until the thread terminates. This blocks the calling thread until the thread whose method is called terminates – either normally or through an unhandled exception – or until the optional timeout occurs. When the timeout argument is present and not , it should be a floating-point number specifying a timeout for the operation in seconds (or fractions thereof). As always returns , you must call after to decide whether a timeout happened – if the thread is still alive, the call timed out. When the timeout argument is not present or , the operation will block until the thread terminates. A thread can be joined many times. raises a if an attempt is made to join the current thread as that would cause a deadlock. It is also an error to a thread before it has been started and attempts to do so raise the same exception. A string used for identification purposes only. It has no semantics. Multiple threads may be given the same name. The initial name is set by the constructor. Deprecated getter/setter API for ; use it directly as a property instead. The ‘thread identifier’ of this thread or if the thread has not been started. This is a nonzero integer. See the function. Thread identifiers may be recycled when a thread exits and another thread is created. The identifier is available even after the thread has exited. The Thread ID ( ) of this thread, as assigned by the OS (kernel). This is a non-negative integer, or if the thread has not been started. See the function. This value may be used to uniquely identify this particular thread system-wide (until the thread terminates, after which the value may be recycled by the OS). Similar to Process IDs, Thread IDs are only valid (guaranteed unique system-wide) from the time the thread is created until the thread has been terminated. Return whether the thread is alive. This method returns just before the method starts until just after the method terminates. The module function returns a list of all alive threads. A boolean value indicating whether this thread is a daemon thread ( ) or not ( ). This must be set before is called, otherwise is raised. Its initial value is inherited from the creating thread; the main thread is not a daemon thread and therefore all threads created in the main thread default to = . The entire Python program exits when no alive non-daemon threads are left. Deprecated getter/setter API for ; use it directly as a property instead.\n\nA primitive lock is a synchronization primitive that is not owned by a particular thread when locked. In Python, it is currently the lowest level synchronization primitive available, implemented directly by the extension module. A primitive lock is in one of two states, “locked” or “unlocked”. It is created in the unlocked state. It has two basic methods, and . When the state is unlocked, changes the state to locked and returns immediately. When the state is locked, blocks until a call to in another thread changes it to unlocked, then the call resets it to locked and returns. The method should only be called in the locked state; it changes the state to unlocked and returns immediately. If an attempt is made to release an unlocked lock, a will be raised. When more than one thread is blocked in waiting for the state to turn to unlocked, only one thread proceeds when a call resets the state to unlocked; which one of the waiting threads proceeds is not defined, and may vary across implementations. The class implementing primitive lock objects. Once a thread has acquired a lock, subsequent attempts to acquire it block, until it is released; any thread may release it. Changed in version 3.13: is now a class. In earlier Pythons, was a factory function which returned an instance of the underlying private lock type. When invoked with the blocking argument set to (the default), block until the lock is unlocked, then set it to locked and return . When invoked with the blocking argument set to , do not block. If a call with blocking set to would block, return immediately; otherwise, set the lock to locked and return . When invoked with the floating-point timeout argument set to a positive value, block for at most the number of seconds specified by timeout and as long as the lock cannot be acquired. A timeout argument of specifies an unbounded wait. It is forbidden to specify a timeout when blocking is . The return value is if the lock is acquired successfully, if not (for example if the timeout expired). Changed in version 3.2: The timeout parameter is new. Changed in version 3.2: Lock acquisition can now be interrupted by signals on POSIX if the underlying threading implementation supports it. Release a lock. This can be called from any thread, not only the thread which has acquired the lock. When the lock is locked, reset it to unlocked, and return. If any other threads are blocked waiting for the lock to become unlocked, allow exactly one of them to proceed. When invoked on an unlocked lock, a is raised. There is no return value. Return if the lock is acquired.\n\nA reentrant lock is a synchronization primitive that may be acquired multiple times by the same thread. Internally, it uses the concepts of “owning thread” and “recursion level” in addition to the locked/unlocked state used by primitive locks. In the locked state, some thread owns the lock; in the unlocked state, no thread owns it. Threads call a lock’s method to lock it, and its method to unlock it. Reentrant locks support the context management protocol, so it is recommended to use instead of manually calling and to handle acquiring and releasing the lock for a block of code. RLock’s / call pairs may be nested, unlike Lock’s / . Only the final (the of the outermost pair) resets the lock to an unlocked state and allows another thread blocked in to proceed. / must be used in pairs: each acquire must have a release in the thread that has acquired the lock. Failing to call release as many times the lock has been acquired can lead to deadlock. This class implements reentrant lock objects. A reentrant lock must be released by the thread that acquired it. Once a thread has acquired a reentrant lock, the same thread may acquire it again without blocking; the thread must release it once for each time it has acquired it. Note that is actually a factory function which returns an instance of the most efficient version of the concrete RLock class that is supported by the platform. Recommended over manual and calls whenever practical. When invoked with the blocking argument set to (the default):\n• None If no thread owns the lock, acquire the lock and return immediately.\n• None If another thread owns the lock, block until we are able to acquire lock, or timeout, if set to a positive float value.\n• None If the same thread owns the lock, acquire the lock again, and return immediately. This is the difference between and ; handles this case the same as the previous, blocking until the lock can be acquired. When invoked with the blocking argument set to :\n• None If no thread owns the lock, acquire the lock and return immediately.\n• None If another thread owns the lock, return immediately.\n• None If the same thread owns the lock, acquire the lock again and return immediately. In all cases, if the thread was able to acquire the lock, return . If the thread was unable to acquire the lock (i.e. if not blocking or the timeout was reached) return . If called multiple times, failing to call as many times may lead to deadlock. Consider using as a context manager rather than calling acquire/release directly. Changed in version 3.2: The timeout parameter is new. Release a lock, decrementing the recursion level. If after the decrement it is zero, reset the lock to unlocked (not owned by any thread), and if any other threads are blocked waiting for the lock to become unlocked, allow exactly one of them to proceed. If after the decrement the recursion level is still nonzero, the lock remains locked and owned by the calling thread. Only call this method when the calling thread owns the lock. A is raised if this method is called when the lock is not acquired. There is no return value.\n\nA condition variable is always associated with some kind of lock; this can be passed in or one will be created by default. Passing one in is useful when several condition variables must share the same lock. The lock is part of the condition object: you don’t have to track it separately. A condition variable obeys the context management protocol: using the statement acquires the associated lock for the duration of the enclosed block. The and methods also call the corresponding methods of the associated lock. Other methods must be called with the associated lock held. The method releases the lock, and then blocks until another thread awakens it by calling or . Once awakened, re-acquires the lock and returns. It is also possible to specify a timeout. The method wakes up one of the threads waiting for the condition variable, if any are waiting. The method wakes up all threads waiting for the condition variable. Note: the and methods don’t release the lock; this means that the thread or threads awakened will not return from their call immediately, but only when the thread that called or finally relinquishes ownership of the lock. The typical programming style using condition variables uses the lock to synchronize access to some shared state; threads that are interested in a particular change of state call repeatedly until they see the desired state, while threads that modify the state call or when they change the state in such a way that it could possibly be a desired state for one of the waiters. For example, the following code is a generic producer-consumer situation with unlimited buffer capacity: The loop checking for the application’s condition is necessary because can return after an arbitrary long time, and the condition which prompted the call may no longer hold true. This is inherent to multi-threaded programming. The method can be used to automate the condition checking, and eases the computation of timeouts: To choose between and , consider whether one state change can be interesting for only one or several waiting threads. E.g. in a typical producer-consumer situation, adding one item to the buffer only needs to wake up one consumer thread. This class implements condition variable objects. A condition variable allows one or more threads to wait until they are notified by another thread. If the lock argument is given and not , it must be a or object, and it is used as the underlying lock. Otherwise, a new object is created and used as the underlying lock. Changed in version 3.3: changed from a factory function to a class. Acquire the underlying lock. This method calls the corresponding method on the underlying lock; the return value is whatever that method returns. Release the underlying lock. This method calls the corresponding method on the underlying lock; there is no return value. Wait until notified or until a timeout occurs. If the calling thread has not acquired the lock when this method is called, a is raised. This method releases the underlying lock, and then blocks until it is awakened by a or call for the same condition variable in another thread, or until the optional timeout occurs. Once awakened or timed out, it re-acquires the lock and returns. When the timeout argument is present and not , it should be a floating-point number specifying a timeout for the operation in seconds (or fractions thereof). When the underlying lock is an , it is not released using its method, since this may not actually unlock the lock when it was acquired multiple times recursively. Instead, an internal interface of the class is used, which really unlocks it even when it has been recursively acquired several times. Another internal interface is then used to restore the recursion level when the lock is reacquired. The return value is unless a given timeout expired, in which case it is . Changed in version 3.2: Previously, the method always returned . Wait until a condition evaluates to true. predicate should be a callable which result will be interpreted as a boolean value. A timeout may be provided giving the maximum time to wait. This utility method may call repeatedly until the predicate is satisfied, or until a timeout occurs. The return value is the last return value of the predicate and will evaluate to if the method timed out. Ignoring the timeout feature, calling this method is roughly equivalent to writing: Therefore, the same rules apply as with : The lock must be held when called and is re-acquired on return. The predicate is evaluated with the lock held. By default, wake up one thread waiting on this condition, if any. If the calling thread has not acquired the lock when this method is called, a is raised. This method wakes up at most n of the threads waiting for the condition variable; it is a no-op if no threads are waiting. The current implementation wakes up exactly n threads, if at least n threads are waiting. However, it’s not safe to rely on this behavior. A future, optimized implementation may occasionally wake up more than n threads. Note: an awakened thread does not actually return from its call until it can reacquire the lock. Since does not release the lock, its caller should. Wake up all threads waiting on this condition. This method acts like , but wakes up all waiting threads instead of one. If the calling thread has not acquired the lock when this method is called, a is raised. The method is a deprecated alias for this method.\n\nThis class provides a simple synchronization primitive for use by a fixed number of threads that need to wait for each other. Each of the threads tries to pass the barrier by calling the method and will block until all of the threads have made their calls. At this point, the threads are released simultaneously. The barrier can be reused any number of times for the same number of threads. As an example, here is a simple way to synchronize a client and server thread: Create a barrier object for parties number of threads. An action, when provided, is a callable to be called by one of the threads when they are released. timeout is the default timeout value if none is specified for the method. Pass the barrier. When all the threads party to the barrier have called this function, they are all released simultaneously. If a timeout is provided, it is used in preference to any that was supplied to the class constructor. The return value is an integer in the range 0 to parties – 1, different for each thread. This can be used to select a thread to do some special housekeeping, e.g.: # Only one thread needs to print this If an action was provided to the constructor, one of the threads will have called it prior to being released. Should this call raise an error, the barrier is put into the broken state. If the call times out, the barrier is put into the broken state. This method may raise a exception if the barrier is broken or reset while a thread is waiting. Return the barrier to the default, empty state. Any threads waiting on it will receive the exception. Note that using this function may require some external synchronization if there are other threads whose state is unknown. If a barrier is broken it may be better to just leave it and create a new one. Put the barrier into a broken state. This causes any active or future calls to to fail with the . Use this for example if one of the threads needs to abort, to avoid deadlocking the application. It may be preferable to simply create the barrier with a sensible timeout value to automatically guard against one of the threads going awry. The number of threads required to pass the barrier. The number of threads currently waiting in the barrier. A boolean that is if the barrier is in the broken state. This exception, a subclass of , is raised when the object is reset or broken."
    },
    {
        "link": "https://geeksforgeeks.org/queue-in-python",
        "document": "Like a stack, the queue is a linear data structure that stores items in a First In First Out (FIFO) manner. With a queue, the least recently added item is removed first. A good example of a queue is any queue of consumers for a resource where the consumer that came first is served first.\n\n\n\n\n\nOperations associated with queue are:\n• Enqueue: Adds an item to the queue. If the queue is full, then it is said to be an Overflow condition – Time Complexity : O(1)\n• Dequeue: Removes an item from the queue. The items are popped in the same order in which they are pushed. If the queue is empty, then it is said to be an Underflow condition – Time Complexity : O(1)\n• Front: Get the front item from queue – Time Complexity : O(1)\n• Rear: Get the last item from queue – Time Complexity : O(1)\n\nThere are various ways to implement a queue in Python. This article covers the implementation of queue using data structures and modules from Python library. Python Queue can be implemented by the following ways:\n\nList is a Python’s built-in data structure that can be used as a queue. Instead of enqueue() and dequeue(), append() and pop() function is used. However, lists are quite slow for this purpose because inserting or deleting an element at the beginning requires shifting all of the other elements by one, requiring O(n) time.\n\nThe code simulates a queue using a Python list. It adds elements ‘a’, ‘b’, and ‘c’ to the queue and then dequeues them, resulting in an empty queue at the end. The output shows the initial queue, elements dequeued (‘a’, ‘b’, ‘c’), and the queue’s empty state.\n\nQueue in Python can be implemented using deque class from the collections module. Deque is preferred over list in the cases where we need quicker append and pop operations from both the ends of container, as deque provides an O(1) time complexity for append and pop operations as compared to list which provides O(n) time complexity. Instead of enqueue and deque, append() and popleft() functions are used.\n\nThe code uses a from the module to represent a queue. It appends ‘a’, ‘b’, and ‘c’ to the queue and dequeues them with , resulting in an empty queue. Uncommenting after the queue is empty would raise an . The code demonstrates queue operations and handles an empty queue scenario.\n\nQueue is built-in module of Python which is used to implement a queue. queue.Queue(maxsize) initializes a variable to a maximum size of maxsize. A maxsize of zero ‘0’ means a infinite queue. This Queue follows FIFO rule. \n\nThere are various functions available in this module:\n• maxsize – Number of items allowed in the queue.\n• empty() – Return True if the queue is empty, False otherwise.\n• full() – Return True if there are maxsize items in the queue. If the queue was initialized with maxsize=0 (the default), then full() never returns True.\n• get() – Remove and return an item from the queue. If queue is empty, wait until an item is available.\n• get_nowait() – Return an item if one is immediately available, else raise QueueEmpty.\n• put(item) – Put an item into the queue. If the queue is full, wait until a free slot is available before adding the item.\n• put_nowait(item) – Put an item into the queue without blocking. If no free slot is immediately available, raise QueueFull.\n• qsize() – Return the number of items in the queue.\n\nExample: This code utilizes the class from the module. It starts with an empty queue and fills it with ‘a’, ‘b’, and ‘c’. After dequeuing, the queue becomes empty, and ‘1’ is added. Despite being empty earlier, it remains full, as the maximum size is set to 3. The code demonstrates queue operations, including checking for fullness and emptiness.\n\nWhat is a queue in Python?\n\nIs the queue FIFO or LIFO?\n\nWhat is front and rear queue in Python?\n\nWhat is the difference between front and rear in queue?\n\nWhat is the difference between queue and lock in Python?\n\nWhat is the difference between queue and stack in Python?"
    },
    {
        "link": "https://basillica.medium.com/working-with-queues-in-python-a-complete-guide-aa112d310542",
        "document": "A queue is a linear data structure that follows the FIFO principle — the first element added to the queue is the first one to be removed. This is analogous to a physical queue of people waiting in line — the first person in line is the first to be served.\n\nThe main operations of a queue are:\n• Enqueue — Add an element to the end of the queue\n• Dequeue — Remove an element from the front of the queue\n• IsEmpty — Check if the queue is empty\n• IsFull — Check if the queue is full\n• Peek — Get the value of the front element without removing it\n\nQueues maintain a logical order of elements and provide efficient insertion and deletion. They can be implemented using arrays, linked lists, stacks, or dequeues.\n\nSome common uses of queues are:\n\n- Job scheduling — Tasks get added to a queue and processed in order\n\n- Print spoolers — Print jobs get spooled in a queue before being printed\n\n- Keyboard buffer — Keystrokes get added to a queue before being processed\n\n- Web server request queue — Requests get queued before being handled\n\n- Queues provide first-in, first-out behavior that is very useful in programming for certain situations.\n\nPython provides a queue module in the standard library that has different queue implementations for programmers to use in their applications.\n\nThe key classes implemented in the queue module are:\n\n- deque — A double-ended queue that allows adding/removing from both ends\n\nLet’s explore how to use these classes to implement queues in Python:"
    },
    {
        "link": "https://alibabacloud.com/en/developer/a/python/inter-thread-communication-multi-process?_p_lc=1",
        "document": "inter-thread communication\n\nThreads sometimes need to communicate, and the operating system provides many mechanisms to achieve inter-process communication, among which we use the most queue Queue. Classic case: producer and consumer.\n\n\n\nThe principle of Queue\n\nQueue is a first-in-first-out (First In First Out) queue. A Queue object is created in the main process and passed to the child process as a parameter. Data is put in between the two through put( ), and data is taken out through get( ). After the get() function is executed, the data in the queue will be deleted at the same time, and the Queue of the multiprocessing module can be used to transfer data between multiple processes.\n\nimage.png\n\n\n\nimport threading, queue\n\nimport time\n\n\n\n\n\ndef produce():\n\n for i in range(10):\n\n time. sleep(0.5)\n\n print('production++++++bread {} {}'.format(threading.current_thread().name, i))\n\n q.put('{}{}'.format(threading.current_thread().name, i))\n\n\n\n\n\ndef consumer():\n\n while True:\n\n time. sleep(1)\n\n # The q.get() method is a blocking method\n\n print('{} bought------bread{}'.format(threading.current_thread().name, q.get()))\n\n\n\n\n\nq = queue.Queue() # create a q\n\n\n\n# a production line\n\npa = threading.Thread(target=produce, name='pa')\n\npb = threading.Thread(target=produce, name='pb')\n\npc = threading.Thread(target=produce, name='pc')\n\n\n\n# A consumption line\n\nca = threading.Thread(target=consumer, name='ca')\n\ncb = threading.Thread(target=consumer, name='cb')\n\ncc = threading.Thread(target=consumer, name='cc')\n\n\n\npa. start()\n\npb. start()\n\npc. start()\n\n\n\nca. start()\n\ncb. start()\n\ncc. start()\n\n\n\nUse of multiple processes\n\nprocess\n\nProgram: For example, xxx.py is a program, which is a static one.\n\n\n\nProcess: After a program runs, the code + the resources used are called processes, which are the basic unit for resource allocation by the operating system.\n\n\n\nNot only can multitasking be done through threads, but processes are also possible.\n\n\n\nstate of the process\n\nDuring work, the number of tasks is often greater than the number of CPU cores, that is, some tasks must be executing, while other tasks are waiting for the CPU to execute, thus resulting in different states.\n\n\n\nReady state: The running conditions have been met and are waiting to be executed on the CPU.\n\nExecuting state: CPU is executing its function.\n\nWaiting state: Waiting for certain conditions to be met, for example, a program sleeps, and it is in a waiting state at this time.\n\ncreate process\n\nThe multiprocessing module is a cross-platform version of the multi-process module. It provides a Process class to represent a process object. This object can be understood as an independent process that can perform other things.\n\nWhen creating a child process, you only need to pass in an execution function and function parameters, create a Process instance, and start it with the start() method.\n\n\n\nimport multiprocessing, time, os\n\n\n\n\n\ndef dance(n):\n\n for i in range(n):\n\n time. sleep(0.5)\n\n print('Dancing {}, pid={}'.format(i, os.getpid()))\n\n\n\n\n\ndef sing(m):\n\n for i in range(m):\n\n time. sleep(0.5)\n\n print('Singing {}, pid={}'.format(i, os.getpid()))\n\n\n\n\n\nif __name__ == '__main__':\n\n print('The pid of the main process={}'.format(os.getpid()))\n\n # create two processes\n\n # target is used to indicate the task to execute\n\n # args is used to pass parameters, the type is a tuple\n\n p1 = multiprocessing.Process(target=dance, args=(100,))\n\n p2 = multiprocessing.Process(target=sing, args=(100,))\n\n\n\n p1. start()\n\n p2. start()\n\nMethod Description\n\n\n\nProcess( target [, name [, args [, kwargs]]])\n\ntarget: If a function reference is passed, the child process can be tasked to execute the code here\n\nargs: the parameters passed to the function specified by target, passed as a tuple\n\nkwargs: Pass named parameters to the function specified by target\n\nname: Set a name for the process, you can not set it\n\nCommon methods of instance objects created by Process:\n\n\n\nstart(): Start a child process instance (create a child process)\n\nis_alive(): Determine whether the process child process is still alive\n\njoin([timeout]): Whether to wait for the child process to end, or how many seconds to wait\n\nterminate(): Regardless of whether the task is completed, the child process is terminated immediately\n\nCommon properties of instance objects created by Process:\n\n\n\nname: the alias of the current process, the default is Process-N, N is an integer incrementing from 1\n\npid: the pid (process number) of the current process"
    }
]