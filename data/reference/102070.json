[
    {
        "link": "https://pypi.org/project/SpeechRecognition",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://realpython.com/python-speech-recognition",
        "document": "Have you ever wondered how to add speech recognition to your Python project? If so, then keep reading! It’s easier than you might think.\n\nFar from a being a fad, the overwhelming success of speech-enabled products like Amazon Alexa has proven that some degree of speech support will be an essential aspect of household tech for the foreseeable future. If you think about it, the reasons why are pretty obvious. Incorporating speech recognition into your Python application offers a level of interactivity and accessibility that few technologies can match.\n\nThe accessibility improvements alone are worth considering. Speech recognition allows the elderly and the physically and visually impaired to interact with state-of-the-art products and services quickly and naturally—no GUI needed!\n\nBest of all, including speech recognition in a Python project is really simple. In this guide, you’ll find out how. You’ll learn:\n• What packages are available on PyPI; and\n• How to install and use the SpeechRecognition package—a full-featured and easy-to-use Python speech recognition library.\n\nIn the end, you’ll apply what you’ve learned to a simple “Guess the Word” game and see how it all comes together.\n\nBefore we get to the nitty-gritty of doing speech recognition in Python, let’s take a moment to talk about how speech recognition works. A full discussion would fill a book, so I won’t bore you with all of the technical details here. In fact, this section is not pre-requisite to the rest of the tutorial. If you’d like to get straight to the point, then feel free to skip ahead. Speech recognition has its roots in research done at Bell Labs in the early 1950s. Early systems were limited to a single speaker and had limited vocabularies of about a dozen words. Modern speech recognition systems have come a long way since their ancient counterparts. They can recognize speech from multiple speakers and have enormous vocabularies in numerous languages. The first component of speech recognition is, of course, speech. Speech must be converted from physical sound to an electrical signal with a microphone, and then to digital data with an analog-to-digital converter. Once digitized, several models can be used to transcribe the audio to text. Most modern speech recognition systems rely on what is known as a Hidden Markov Model (HMM). This approach works on the assumption that a speech signal, when viewed on a short enough timescale (say, ten milliseconds), can be reasonably approximated as a stationary process—that is, a process in which statistical properties do not change over time. In a typical HMM, the speech signal is divided into 10-millisecond fragments. The power spectrum of each fragment, which is essentially a plot of the signal’s power as a function of frequency, is mapped to a vector of real numbers known as cepstral coefficients. The dimension of this vector is usually small—sometimes as low as 10, although more accurate systems may have dimension 32 or more. The final output of the HMM is a sequence of these vectors. To decode the speech into text, groups of vectors are matched to one or more phonemes—a fundamental unit of speech. This calculation requires training, since the sound of a phoneme varies from speaker to speaker, and even varies from one utterance to another by the same speaker. A special algorithm is then applied to determine the most likely word (or words) that produce the given sequence of phonemes. One can imagine that this whole process may be computationally expensive. In many modern speech recognition systems, neural networks are used to simplify the speech signal using techniques for feature transformation and dimensionality reduction before HMM recognition. Voice activity detectors (VADs) are also used to reduce an audio signal to only the portions that are likely to contain speech. This prevents the recognizer from wasting time analyzing unnecessary parts of the signal. Fortunately, as a Python programmer, you don’t have to worry about any of this. A number of speech recognition services are available for use online through an API, and many of these services offer Python SDKs.\n\nA handful of packages for speech recognition exist on PyPI. A few of them include: Some of these packages—such as wit and apiai—offer built-in features, like natural language processing for identifying a speaker’s intent, which go beyond basic speech recognition. Others, like google-cloud-speech, focus solely on speech-to-text conversion. There is one package that stands out in terms of ease-of-use: SpeechRecognition. Recognizing speech requires audio input, and SpeechRecognition makes retrieving this input really easy. Instead of having to build scripts for accessing microphones and processing audio files from scratch, SpeechRecognition will have you up and running in just a few minutes. The SpeechRecognition library acts as a wrapper for several popular speech APIs and is thus extremely flexible. One of these—the Google Web Speech API—supports a default API key that is hard-coded into the SpeechRecognition library. That means you can get off your feet without having to sign up for a service. The flexibility and ease-of-use of the SpeechRecognition package make it an excellent choice for any Python project. However, support for every feature of each API it wraps is not guaranteed. You will need to spend some time researching the available options to find out if SpeechRecognition will work in your particular case. So, now that you’re convinced you should try out SpeechRecognition, the next step is getting it installed in your environment.\n\nAll of the magic in SpeechRecognition happens with the class. The primary purpose of a instance is, of course, to recognize speech. Each instance comes with a variety of settings and functionality for recognizing speech from an audio source. Creating a instance is easy. In your current interpreter session, just type: Each instance has seven methods for recognizing speech from an audio source using various APIs. These are: Of the seven, only works offline with the CMU Sphinx engine. The other six all require an internet connection. A full discussion of the features and benefits of each API is beyond the scope of this tutorial. Since SpeechRecognition ships with a default API key for the Google Web Speech API, you can get started with it right away. For this reason, we’ll use the Web Speech API in this guide. The other six APIs all require authentication with either an API key or a username/password combination. For more information, consult the SpeechRecognition docs. Caution: The default key provided by SpeechRecognition is for testing purposes only, and Google may revoke it at any time. It is not a good idea to use the Google Web Speech API in production. Even with a valid API key, you’ll be limited to only 50 requests per day, and there is no way to raise this quota. Fortunately, SpeechRecognition’s interface is nearly identical for each API, so what you learn today will be easy to translate to a real-world project. Each method will throw a exception if the API is unreachable. For , this could happen as the result of a missing, corrupt or incompatible Sphinx installation. For the other six methods, may be thrown if quota limits are met, the server is unavailable, or there is no internet connection. Ok, enough chit-chat. Let’s get our hands dirty. Go ahead and try to call in your interpreter session. You probably got something that looks like this: You might have guessed this would happen. How could something be recognized from nothing? All seven methods of the class require an argument. In each case, must be an instance of SpeechRecognition’s class. There are two ways to create an instance: from an audio file or audio recorded by a microphone. Audio files are a little easier to get started with, so let’s take a look at that first.\n\nBefore you continue, you’ll need to download an audio file. The one I used to get started, “harvard.wav,” can be found here. Make sure you save it to the same directory in which your Python interpreter session is running. SpeechRecognition makes working with audio files easy thanks to its handy class. This class can be initialized with the path to an audio file and provides a context manager interface for reading and working with the file’s contents. Currently, SpeechRecognition supports the following file formats:\n• WAV: must be in PCM/LPCM format\n• FLAC: must be native FLAC format; OGG-FLAC is not supported If you are working on x-86 based Linux, macOS or Windows, you should be able to work with FLAC files without a problem. On other platforms, you will need to install a FLAC encoder and ensure you have access to the command line tool. You can find more information here if this applies to you. Using to Capture Data From a File Type the following into your interpreter session to process the contents of the “harvard.wav” file: The context manager opens the file and reads its contents, storing the data in an instance called Then the method records the data from the entire file into an instance. You can confirm this by checking the type of : You can now invoke to attempt to recognize any speech in the audio. Depending on your internet connection speed, you may have to wait several seconds before seeing the result. 'the stale smell of old beer lingers it takes heat to bring out the odor a cold dip restores health and Pastore are my favorite a zestful food is the hot Congratulations! You’ve just transcribed your first audio file! If you’re wondering where the phrases in the “harvard.wav” file come from, they are examples of Harvard Sentences. These phrases were published by the IEEE in 1965 for use in speech intelligibility testing of telephone lines. They are still used in VoIP and cellular testing today. The Harvard Sentences are comprised of 72 lists of ten phrases. You can find freely available recordings of these phrases on the Open Speech Repository website. Recordings are available in English, Mandarin Chinese, French, and Hindi. They provide an excellent source of free material for testing your code. What if you only want to capture a portion of the speech in a file? The method accepts a keyword argument that stops the recording after a specified number of seconds. For example, the following captures any speech in the first four seconds of the file: 'the stale smell of old beer lingers' The method, when used inside a block, always moves ahead in the file stream. This means that if you record once for four seconds and then record again for four seconds, the second time returns the four seconds of audio after the first four seconds. 'the stale smell of old beer lingers' 'it takes heat to bring out the odor a cold dip' Notice that contains a portion of the third phrase in the file. When specifying a duration, the recording might stop mid-phrase—or even mid-word—which can hurt the accuracy of the transcription. More on this in a bit. In addition to specifying a recording duration, the method can be given a specific starting point using the keyword argument. This value represents the number of seconds from the beginning of the file to ignore before starting to record. To capture only the second phrase in the file, you could start with an offset of four seconds and record for, say, three seconds. 'it takes heat to bring out the odor' The and keyword arguments are useful for segmenting an audio file if you have prior knowledge of the structure of the speech in the file. However, using them hastily can result in poor transcriptions. To see this effect, try the following in your interpreter: 'Mesquite to bring out the odor Aiko' By starting the recording at 4.7 seconds, you miss the “it t” portion a the beginning of the phrase “it takes heat to bring out the odor,” so the API only got “akes heat,” which it matched to “Mesquite.” Similarly, at the end of the recording, you captured “a co,” which is the beginning of the third phrase “a cold dip restores health and zest.” This was matched to “Aiko” by the API. There is another reason you may get inaccurate transcriptions. Noise! The above examples worked well because the audio file is reasonably clean. In the real world, unless you have the opportunity to process audio files beforehand, you can not expect the audio to be noise-free. The Effect of Noise on Speech Recognition Noise is a fact of life. All audio recordings have some degree of noise in them, and un-handled noise can wreck the accuracy of speech recognition apps. To get a feel for how noise can affect speech recognition, download the “jackhammer.wav” file here. As always, make sure you save this to your interpreter session’s working directory. This file has the phrase “the stale smell of old beer lingers” spoken with a loud jackhammer in the background. What happens when you try to transcribe this file? 'the snail smell of old gear vendors' So how do you deal with this? One thing you can try is using the method of the class. 'still smell of old beer vendors' That got you a little closer to the actual phrase, but it still isn’t perfect. Also, “the” is missing from the beginning of the phrase. Why is that? The method reads the first second of the file stream and calibrates the recognizer to the noise level of the audio. Hence, that portion of the stream is consumed before you call to capture the data. You can adjust the time-frame that uses for analysis with the keyword argument. This argument takes a numerical value in seconds and is set to 1 by default. Try lowering this value to 0.5. 'the snail smell like old Beer Mongers' Well, that got you “the” at the beginning of the phrase, but now you have some new issues! Sometimes it isn’t possible to remove the effect of the noise—the signal is just too noisy to be dealt with successfully. That’s the case with this file. If you find yourself running up against these issues frequently, you may have to resort to some pre-processing of the audio. This can be done with audio editing software or a Python package (such as SciPy) that can apply filters to the files. A detailed discussion of this is beyond the scope of this tutorial—check out Allen Downey’s Think DSP book if you are interested. For now, just be aware that ambient noise in an audio file can cause problems and must be addressed in order to maximize the accuracy of speech recognition. When working with noisy files, it can be helpful to see the actual API response. Most APIs return a JSON string containing many possible transcriptions. The method will always return the most likely transcription unless you force it to give you the full response. You can do this by setting the keyword argument of the method to {'transcript': 'the snail smell like old Beer Mongers'}, {'transcript': 'the still smell of old beer vendors'}, {'transcript': 'the snail smell like old beer vendors'}, {'transcript': 'the stale smell of old beer vendors'}, {'transcript': 'the snail smell like old beermongers'}, {'transcript': 'the still smell like old beer vendors'}, {'transcript': 'the still smell like old beermongers'}, {'transcript': 'the still smell of old beer venders'}, {'transcript': 'the still smelling old beer vendors'}, {'transcript': 'the still smell of old beer vendor'} As you can see, returns a dictionary with the key that points to a list of possible transcripts. The structure of this response may vary from API to API and is mainly useful for debugging. By now, you have a pretty good idea of the basics of the SpeechRecognition package. You’ve seen how to create an instance from an audio file and use the method to capture data from the file. You learned how to record segments of a file using the and keyword arguments of , and you experienced the detrimental effect noise can have on transcription accuracy. Now for the fun part. Let’s transition from transcribing static audio files to making your project interactive by accepting input from a microphone.\n\nTo access your microphone with SpeechRecognizer, you’ll have to install the PyAudio package. Go ahead and close your current interpreter session, and let’s do that. The process for installing PyAudio will vary depending on your operating system. If you’re on Debian-based Linux (like Ubuntu) you can install PyAudio with : Once installed, you may still need to run , especially if you are working in a virtual environment. For macOS, first you will need to install PortAudio with Homebrew, and then install PyAudio with : On Windows, you can install PyAudio with : Once you’ve got PyAudio installed, you can test the installation from the console. Make sure your default microphone is on and unmuted. If the installation worked, you should see something like this: Go ahead and play around with it a little bit by speaking into your microphone and seeing how well SpeechRecognition transcribes your speech. Note: If you are on Ubuntu and get some funky output like ‘ALSA lib … Unknown PCM’, refer to this page for tips on suppressing these messages. This output comes from the ALSA package installed with Ubuntu—not SpeechRecognition or PyAudio. In all reality, these messages may indicate a problem with your ALSA configuration, but in my experience, they do not impact the functionality of your code. They are mostly a nuisance. Open up another interpreter session and create an instance of the recognizer class. Now, instead of using an audio file as the source, you will use the default system microphone. You can access this by creating an instance of the class. If your system has no default microphone (such as on a Raspberry Pi), or you want to use a microphone other than the default, you will need to specify which one to use by supplying a device index. You can get a list of microphone names by calling the static method of the class. Note that your output may differ from the above example. The device index of the microphone is the index of its name in the list returned by For example, given the above output, if you want to use the microphone called “front,” which has index 3 in the list, you would create a microphone instance like this: # This is just an example; do not run For most projects, though, you’ll probably want to use the default system microphone. Now that you’ve got a instance ready to go, it’s time to capture some input. Just like the class, is a context manager. You can capture input from the microphone using the method of the class inside of the block. This method takes an audio source as its first argument and records input from the source until silence is detected. Once you execute the block, try speaking “hello” into your microphone. Wait a moment for the interpreter prompt to display again. Once the “>>>” prompt returns, you’re ready to recognize the speech. If the prompt never returns, your microphone is most likely picking up too much ambient noise. You can interrupt the process with + to get your prompt back. To handle ambient noise, you’ll need to use the method of the class, just like you did when trying to make sense of the noisy audio file. Since input from a microphone is far less predictable than input from an audio file, it is a good idea to do this anytime you listen for microphone input. After running the above code, wait a second for to do its thing, then try speaking “hello” into the microphone. Again, you will have to wait a moment for the interpreter prompt to return before trying to recognize the speech. Recall that analyzes the audio source for one second. If this seems too long to you, feel free to adjust this with the keyword argument. The SpeechRecognition documentation recommends using a duration no less than 0.5 seconds. In some cases, you may find that durations longer than the default of one second generate better results. The minimum value you need depends on the microphone’s ambient environment. Unfortunately, this information is typically unknown during development. In my experience, the default duration of one second is adequate for most applications. Try typing the previous code example in to the interpeter and making some unintelligible noises into the microphone. You should get something like this in response: Audio that cannot be matched to text by the API raises an exception. You should always wrap calls to the API with and blocks to handle this exception. Note: You may have to try harder than you expect to get the exception thrown. The API works very hard to transcribe any vocal sounds. Even short grunts were transcribed as words like “how” for me. Coughing, hand claps, and tongue clicks would consistently raise the exception.\n\nPutting It All Together: A “Guess the Word” Game Now that you’ve seen the basics of recognizing speech with the SpeechRecognition package let’s put your newfound knowledge to use and write a small game that picks a random word from a list and gives the user three attempts to guess the word. \"success\": a boolean indicating whether or not the API request was \"error\": `None` if no error occured, otherwise a string containing an error message if the API could not be reached or \"transcription\": `None` if speech could not be transcribed, otherwise a string containing the transcribed text # check that recognizer and microphone arguments are appropriate type # adjust the recognizer sensitivity to ambient noise and record audio # try recognizing the speech in the recording # if a RequestError or UnknownValueError exception is caught, # set the list of words, maxnumber of guesses, and prompt limit # get a random word from the list \"I'm thinking of one of these words: tries to guess which one. # show instructions and wait 3 seconds before starting the game # get the guess from the user # if a transcription is returned, break out of the loop and # if no transcription returned and API request failed, break # if API request succeeded but no transcription was returned, # re-prompt the user to say their guess again. Do this up \"I didn't catch that. What did you say? # if there was an error, stop the game # determine if guess is correct and if any attempts remain # determine if the user has won the game # if not, repeat the loop if user has more attempts # if no attempts left, the user loses the game Let’s break that down a little bit. The function takes a and instance as arguments and returns a dictionary with three keys. The first key, , is a boolean that indicates whether or not the API request was successful. The second key, , is either or an error message indicating that the API is unavailable or the speech was unintelligible. Finally, the key contains the transcription of the audio recorded by the microphone. The function first checks that the and arguments are of the correct type, and raises a if either is invalid: The method is then used to record microphone input: The method is used to calibrate the recognizer for changing noise conditions each time the function is called. Next, is called to transcribe any speech in the recording. A block is used to catch the and exceptions and handle them accordingly. The success of the API request, any error messages, and the transcribed speech are stored in the , and keys of the dictionary, which is returned by the function. You can test the function by saving the above script to a file called “guessing_game.py” and running the following in an interpreter session: # Your output will vary depending on what you say The game itself is pretty simple. First, a list of words, a maximum number of allowed guesses and a prompt limit are declared: Next, a and instance is created and a random word is chosen from : After printing some instructions and waiting for 3 three seconds, a loop is used to manage each user attempt at guessing the chosen word. The first thing inside the loop is another loop that prompts the user at most times for a guess, attempting to recognize the input each time with the function and storing the dictionary returned to the local variable . If the key of is not , then the user’s speech was transcribed and the inner loop is terminated with . If the speech was not transcribed and the key is set to , then an API error occurred and the loop is again terminated with . Otherwise, the API request was successful but the speech was unrecognizable. The user is warned and the loop repeats, giving the user another chance at the current attempt. \"I didn't catch that. What did you say? Once the inner loop terminates, the dictionary is checked for errors. If any occurred, the error message is displayed and the outer loop is terminated with , which will end the program execution. If there weren’t any errors, the transcription is compared to the randomly selected word. The method for string objects is used to ensure better matching of the guess to the chosen word. The API may return speech matched to the word “apple” as “Apple” or “apple,” and either response should count as a correct answer. If the guess was correct, the user wins and the game is terminated. If the user was incorrect and has any remaining attempts, the outer loop repeats and a new guess is retrieved. Otherwise, the user loses the game. When run, the output will look something like this: I'm thinking of one of these words: You have 3 tries to guess which one."
    },
    {
        "link": "https://projector-video-pdf-converter.datacamp.com/17718/chapter2.pdf",
        "document": ""
    },
    {
        "link": "https://github.com/Uberi/speech_recognition",
        "document": "Library for performing speech recognition, with support for several engines and APIs, online and offline.\n\nUPDATE 2022-02-09: Hey everyone! This project started as a tech demo, but these days it needs more time than I have to keep up with all the PRs and issues. Therefore, I'd like to put out an open invite for collaborators - just reach out at me@anthonyz.ca if you're interested!\n\nQuickstart: . See the \"Installing\" section for more details.\n\nTo quickly try it out, run after installing.\n\nThe library reference documents every publicly accessible object in the library. This document is also included under .\n\nSee Notes on using PocketSphinx for information about installing languages, compiling PocketSphinx, and building language packs from online resources. This document is also included under .\n\nYou have to install Vosk models for using Vosk. Here are models avaiable. You have to place them in models folder of your project, like \"your-project-folder/models/your-vosk-model\"\n\nSee the directory in the repository root for usage examples:\n• Calibrate the recognizer energy threshold for ambient noise levels (see for details)\n• Listening to a microphone in the background\n• Various other useful recognizer features\n\nFirst, make sure you have all the requirements listed in the \"Requirements\" section.\n\nThe easiest way to install this is using .\n\nOtherwise, download the source distribution from PyPI, and extract the archive.\n\nTo use all of the functionality of the library, you should have:\n• PyAudio 0.2.11+ (required only if you need to use microphone input, )\n• PocketSphinx (required only if you need to use the Sphinx recognizer, )\n• Google API Client Library for Python (required only if you need to use the Google Cloud Speech API, )\n• FLAC encoder (required only if the system is not x86-based Windows/Linux/OS X)\n• Vosk (required only if you need to use Vosk API speech recognition )\n• Whisper (required only if you need to use Whisper )\n• Faster Whisper (required only if you need to use Faster Whisper )\n• openai (required only if you need to use OpenAI Whisper API speech recognition )\n• groq (required only if you need to use Groq Whisper API speech recognition )\n\nThe following requirements are optional, but can improve or extend functionality in some situations:\n• If using CMU Sphinx, you may want to install additional language packs to support languages like International French or Mandarin Chinese.\n\nThe following sections go over the details of each requirement.\n\nThe first software requirement is Python 3.9+. This is required to use the library.\n\nPyAudio is required if and only if you want to use microphone input ( ). PyAudio version 0.2.11+ is required, as earlier versions have known memory management bugs when recording from microphones in certain situations.\n\nIf not installed, everything in the library will still work, except attempting to instantiate a object will raise an .\n\nThe installation instructions on the PyAudio website are quite good - for convenience, they are summarized below:\n• On Windows, install with PyAudio using Pip: execute in a terminal.\n• None On Debian-derived Linux distributions (like Ubuntu and Mint), install PyAudio using APT: execute in a terminal.\n• If the version in the repositories is too old, install the latest release using Pip: execute (replace with if using Python 3).\n• On OS X, install PortAudio using Homebrew: . Then, install with PyAudio using Pip: .\n• On other POSIX-based systems, install the and (or if using Python 3) packages (or their closest equivalents) using a package manager of your choice, and then install with PyAudio using Pip: (replace with if using Python 3).\n\nPocketSphinx is required if and only if you want to use the Sphinx recognizer ( ).\n\nOn Linux and other POSIX systems (such as OS X), run . Follow the instructions under \"Building PocketSphinx-Python from source\" in Notes on using PocketSphinx for installation instructions.\n\nNote that the versions available in most package repositories are outdated and will not work with the bundled language data. Using the bundled wheel packages or building from source is recommended.\n\nSee Notes on using PocketSphinx for information about installing languages, compiling PocketSphinx, and building language packs from online resources. This document is also included under .\n\nVosk API is required if and only if you want to use Vosk recognizer ( ).\n\nYou can install it with .\n\nYou also have to install Vosk Models:\n\nHere are models avaiable for download. You have to place them in models folder of your project, like \"your-project-folder/models/your-vosk-model\"\n\nThe library google-cloud-speech is required if and only if you want to use Google Cloud Speech-to-Text API ( ). You can install it with . (ref: official installation instructions)\n• Digest: Before you begin (Transcribe speech to text by using client libraries)\n\nCurrently only V1 is supported. (V2 is not supported)\n\nA FLAC encoder is required to encode the audio data to send to the API. If using Windows (x86 or x86-64), OS X (Intel Macs only, OS X 10.6 or higher), or Linux (x86 or x86-64), this is already bundled with this library - you do not need to install anything.\n\nOtherwise, ensure that you have the command line tool, which is often available through the system package manager. For example, this would usually be on Debian-derivatives, or on OS X with Homebrew.\n\nWhisper is required if and only if you want to use whisper ( ).\n\nYou can install it with .\n\nThe library faster-whisper is required if and only if you want to use Faster Whisper ( ).\n\nYou can install it with .\n\nThe library openai is required if and only if you want to use OpenAI Whisper API ( ).\n\nYou can install it with .\n\nPlease set the environment variable before calling .\n\nThe library groq is required if and only if you want to use Groq Whisper API ( ).\n\nYou can install it with .\n\nPlease set the environment variable before calling .\n\nTry increasing the property. This is basically how sensitive the recognizer is to when recognition should start. Higher values mean that it will be less sensitive, which is useful if you are in a loud room.\n\nThis value depends entirely on your microphone or audio data. There is no one-size-fits-all value, but good values typically range from 50 to 4000.\n\nAlso, check on your microphone volume settings. If it is too sensitive, the microphone may be picking up a lot of ambient noise. If it is too insensitive, the microphone may be rejecting speech as just noise.\n\nThe property is probably set to a value that is too high to start off with, and then being adjusted lower automatically by dynamic energy threshold adjustment. Before it is at a good level, the energy threshold is so high that speech is just considered ambient noise.\n\nThe solution is to decrease this threshold, or call beforehand, which will set the threshold to a good value automatically.\n\nTry setting the recognition language to your language/dialect. To do this, see the documentation for , , , , , , and .\n\nFor example, if your language/dialect is British English, it is better to use as the language rather than .\n\nThis usually happens when you're using a Raspberry Pi board, which doesn't have audio input capabilities by itself. This causes the default microphone used by PyAudio to simply block when we try to read it. If you happen to be using a Raspberry Pi, you'll need a USB sound card (or USB microphone).\n\nOnce you do this, change all instances of to , where is the hardware-specific index of the microphone.\n\nTo figure out what the value of should be, run the following code:\n\nThis will print out something like the following:\n\nNow, to use the Snowball microphone, you would change to .\n\nAs the error says, the program doesn't know which microphone to use.\n\nTo proceed, either use instead of , or set a default microphone in your OS. You can obtain possible values of using the code in the troubleshooting entry right above this one.\n\nAs of PyInstaller version 3.0, SpeechRecognition is supported out of the box. If you're getting weird issues when compiling your program using PyInstaller, simply update PyInstaller.\n\nYou can easily do this by running .\n\nThe \"bt_audio_service_open\" error means that you have a Bluetooth audio device, but as a physical device is not currently connected, we can't actually use it - if you're not using a Bluetooth microphone, then this can be safely ignored. If you are, and audio isn't working, then double check to make sure your microphone is actually connected. There does not seem to be a simple way to disable these messages.\n\nFor errors of the form \"ALSA lib [...] Unknown PCM\", see this StackOverflow answer. Basically, to get rid of an error of the form \"Unknown PCM cards.pcm.rear\", simply comment out in , , and .\n\nFor \"jack server is not running or cannot be started\" or \"connect(2) call to /dev/shm/jack-1000/default/jack_0 failed (err=No such file or directory)\" or \"attempt to connect to server failed\", these are caused by ALSA trying to connect to JACK, and can be safely ignored. I'm not aware of any simple way to turn those messages off at this time, besides entirely disabling printing while starting the microphone.\n\nInstalling FLAC for OS X directly from the source code will not work, since it doesn't correctly add the executables to the search path.\n\nInstalling FLAC using Homebrew ensures that the search path is correctly updated. First, ensure you have Homebrew, then run to install the necessary files.\n\nTo hack on this library, first make sure you have all the requirements listed in the \"Requirements\" section.\n• Most of the library code lives in .\n• Examples live under the directory, and the demo script lives in .\n• The FLAC encoder binaries are in the directory.\n• Documentation can be found in the directory.\n• Third-party libraries, utilities, and reference material are in the directory.\n\nTo install/reinstall the library locally, run in the project root directory.\n\nBefore a release, the version number is bumped in and . Version tags are then created using .\n\nReleases are done by running to build the Python source packages, sign them, and upload them to PyPI.\n\nTo run all the tests:\n\nTesting is also done automatically by GitHub Actions, upon every push.\n\nThe included executable is the official FLAC 1.3.2 32-bit Windows binary.\n\nThe included and executables are built from the FLAC 1.3.2 source code with Manylinux to ensure that it's compatible with a wide variety of distributions.\n\nThe built FLAC executables should be bit-for-bit reproducible. To rebuild them, run the following inside the project directory on a Debian-like system:\n\nThe included executable is extracted from xACT 2.39, which is a frontend for FLAC 1.3.2 that conveniently includes binaries for all of its encoders. Specifically, it is a copy of in .\n\nPlease report bugs and suggestions at the issue tracker!\n\nHow to cite this library (APA style):\n\nHow to cite this library (Chicago style):\n\nAlso check out the Python Baidu Yuyin API, which is based on an older version of this project, and adds support for Baidu Yuyin. Note that Baidu Yuyin is only available inside China.\n\nCopyright 2014- Anthony Zhang (Uberi). The source code for this library is available online at GitHub.\n\nSpeechRecognition is made available under the 3-clause BSD license. See in the project's root directory for more information.\n\nFor convenience, all the official distributions of SpeechRecognition already include a copy of the necessary copyright notices and licenses. In your project, you can simply say that licensing information for SpeechRecognition can be found within the SpeechRecognition README, and make sure SpeechRecognition is visible to users if they wish to see it.\n\nSpeechRecognition distributes language files from CMU Sphinx. These files are BSD-licensed and redistributable as long as copyright notices are correctly retained. See for license details for individual parts.\n\nSpeechRecognition distributes binaries from FLAC - , , and . These files are GPLv2-licensed and redistributable, as long as the terms of the GPL are satisfied. The FLAC binaries are an aggregate of separate programs, so these GPL restrictions do not apply to the library or your programs that use the library, only to FLAC itself. See for license details."
    },
    {
        "link": "https://robot-intelligence-lab.gitlab.io/fezzik-docs/speech_recog.html",
        "document": "Python library to connect to several speech recognition APIs, including CMU Sphinx, Google Cloud, Wit.ai, … . It can be used for stream processing (in “real-time” with automatic detection of end according to the API) or for batch processing, both with synchronous speech recognition (response as soon as it is processed) for short audio sequences and asynchronous speech recognition (for larger files). Below, we explain how to make it work with two particular speech recognition APIs, namely CMU Sphinx and the Google API. Let us quickly compare their pros and cons: We found the google API to be very precise straight out of the box. Sphinx performance very poor and was close to non-usuable without a grammar definition (explained in detail below) and good with a grammar.\n• First, install the speech recognition package itself:\n• The library requires PyAudio as a dependency (and PyAudio’s dependencies) in order to use real-time microphone input. The pip installation procedure should install PyAudio automatically, but the pip full pip installation is as of the date of this writing broken and might still be when used in the future. Note that a simple does not work. To resolve this, please follow the steps of the setup test below. If the output is correct, PyAudio has been installed correctly and you can jump to the next step. However, if the output is an error message indicating something similar to then PyAudio has not been installed correctly and you have to follow the following steps: a) Install several further dependencies of PyAudio manually by executing . b) Now, install PyAudio manually by executing . c) As a final step, re-run the setup test below and give Fezzik a hand for documenting all this struggle. Now let’s install further dependencies for the underlying APIs. Note that you can install either the Google API or CMU Sphinx; they are independent. Some useful links to start: * Documentation: https://cloud.google.com/speech/docs/ * Overview of Google speech recognition API: https://cloud.google.com/speech/\n• Follow the steps under https://cloud.google.com/iam/docs/creating-managing-service-accounts#creating_a_service_account to create a service account. To do so, you need a Google account and have to provide billing information (a working credit card).\n• At the end of this process, you should get a so-called service account key which is a file. You will need the content of this key for the speech recognition package. Store it in a location on your local machine (don’t share it via git, since it can be used to access the API). You only have to setup an API key and activate the service once. You can use it later on other machines, thus, you can skip this first step in such a case.\n• Install the python API client (assuming you are using python) by executing How to setup the CMU Sphinx package¶\n\nThis is a setup test for the package only. This should start a demo program which, if correct, outputs something similar to: Say something! Got it! Now to recognize it... You said hello hello Say something! Got it! Now to recognize it... Oops! Didn't catch that If not, consider the usage of this demo program in the installation instructions above."
    },
    {
        "link": "https://pypi.org/project/pyttsx3",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://geeksforgeeks.org/python-text-to-speech-by-using-pyttsx3",
        "document": "pyttsx3 is a text-to-speech conversion library in Python. Unlike alternative libraries, it works offline and is compatible with both Python 2 and 3. An application invokes the pyttsx3.init() factory function to get a reference to a pyttsx3. Engine instance. it is a very easy to use tool which converts the entered text into speech. The pyttsx3 module supports two voices first is female and the second is male which is provided by “sapi5” for windows. It supports three TTS engines :\n• espeak – eSpeak on every other platform\n\nInstallation To install the pyttsx3 module, first of all, you have to open the terminal and write\n\nIf you receive errors such as No module named win32com.client, No module named win32, or No module named win32api, you will need to additionally install pypiwin32. It can work on any platform. Now we are all set to write a program for conversion of text to speech. Code : Python program to convert text to speech\n\nOutput : The output of the above program would be a voice saying,"
    },
    {
        "link": "https://pyttsx3.readthedocs.io",
        "document": "This documentation describes the pyttsx3 Python package v 2.6 and was rendered on Jul 14, 2021."
    },
    {
        "link": "https://medium.com/@xinweiiiii/how-to-create-a-text-to-speech-api-using-pyttsx3-library-44017dad9ef2",
        "document": "Taking in a text and convert it to an audio file\n\nThe second method is slightly more complex as we will be returning an audio file back as a response instead of playing the text real time.\n\nWe will be using AWS S3 to store the audio file.\n\nAn AWS Account with your Access Key information. Refer to this guide to set up your Access Key.\n\nWe will be adding this set of code to the above file.\n\nLet’s break down the code into 2 portion.\n\nWe will create a endpoint . When this file is run you can post a request to access and a response with a S3 url that will access your audio file.\n\nThis endpoint will take in a JSON request body as shown above. The text processing is the same as what we did earlier in the real-time conversion setup. The only difference is the method use here, we will be using the method instead of the method.\n\nYou can see from the code that the filename is hardcoded with . When running the method, it will save an audio file in your current directory. But to further enhance on this implementation, we want store this audio file somewhere secure and easily accessible just by an URL. By hardcoding the audio name, S3 upload function can easily reference the file. Next, we will go into detail on how we use AWS SDK for python Boto3 to upload and retrieve the audio file.\n\nBefore uploading the audio file, we will have to set up the SDK boto3 configuration. This is where we will create the boto3.client with our access_key and secret_key.\n\nNext we will create the function that takes in the following parameters file_name (audio.mps), object_name (filename in the request body) and bucket_name. Within the function, we will call the s3 upload file api. You will be able to see the audio file uploaded to your s3 bucket in the directory.\n\nAfter uploading the audio file, you will want to retrieve and access the file. We have a function that takes in bucket_name and file_name as parameters. This function will return an URL to the audio file which expires in 60 minutes (Configurable)."
    },
    {
        "link": "https://github.com/nateshmbhat/pyttsx3",
        "document": "is a text-to-speech conversion library in Python. Unlike alternative libraries, it works offline.\n\nIf you get installation errors , make sure you first upgrade your wheel version using :\n• 🎈 Choose among different voices installed in your system\n• If you are on a Linux system and if the voice output is not working, then : Install espeak-ng and libespeak1 as shown below:\n\n. () # For Mac, If you face error related to \"pyobjc\" when running the `init()` method : . ( ) . ()\n\n. () . ( ) ( ) . ( , ) . ( ) # getting to know current volume level (min=0 and max=1) ( ) . ( , ) # setting up volume level between 0 and 1 . ( ) . ( , [ ]. ) . ( ) . ( ( )) . () . () # On Linux, make sure that 'espeak-ng' is installed . ( , ) . ()\n\nFeel free to wrap another text-to-speech engine for use with ."
    }
]