[
    {
        "link": "https://nayuki.io/page/primitive-recursive-functions",
        "document": "This page explains what primitive recursive functions are, provides runnable code (Python, Haskell, Java) for constructing and evaluating them, and includes a large library of familiar arithmetic functions implemented as .\n\nA primitive recursive function maps each vector of natural numbers to a natural number. In other words, a PRF \\(f\\) with \\(n\\) arguments has the type \\(f : \\mathbb{N}^n → \\mathbb{N}\\). The set of all possible PRFs is constructed in a special way, using only these 5 building blocks:\n• None (Zero) \\(Z : \\mathbb{N} → \\mathbb{N}\\) is a PRF.\n\n For each \\(x ∈ \\mathbb{N}\\), \\(\\:Z(x) = 0\\).\n• None (Successor) \\(S : \\mathbb{N} → \\mathbb{N}\\) is a PRF.\n\n For each \\(x ∈ \\mathbb{N}\\), \\(\\:S(x) = x + 1\\).\n• None (Composition) \\(C_{f,(g_0,...,g_{k-1})} : \\mathbb{N}^n → \\mathbb{N}\\) is a PRF.\n\n It is based on a PRF \\(f : \\mathbb{N}^k → \\mathbb{N}\\) and \\(k\\) PRFs \\(g_0, \\ldots, g_{k-1}\\) each of type \\(\\mathbb{N}^n → \\mathbb{N}\\).\n\n For each \\(\\vec{x} ∈ \\mathbb{N}^n\\), \\(\\: C_{f,(g_0,...,g_{k-1})}(\\vec{x}) = f(g_0(\\vec{x}), \\ldots, g_{k-1}(\\vec{x}))\\).\n• None (Primitive recursion) \\(R_{f,g} : \\mathbb{N}^{n+1} → \\mathbb{N}\\) is a PRF.\n\n It is based on a PRF \\(f : \\mathbb{N}^n → \\mathbb{N}\\) and a PRF \\(g : \\mathbb{N}^{n+2} → \\mathbb{N}\\).\n\n For each \\((y, \\vec{x}) ∈ \\mathbb{N}^{n+1}\\), \\(\\: R_{f,g}(y, \\vec{x}) = \\begin{cases} f(\\vec{x}), & \\text{if } y = 0 \\\\ g(R_{f,g}(y-1, \\vec{x}), y-1, \\vec{x}), & \\text{if } y ≥ 1 \\end{cases}\\).\n• None Every primitive recursive function terminates on every input, thus these functions are computable.\n• None The set of all PRFs has countable cardinality. Thus it is a strict subset of the set of all natural number functions, which has uncountable cardinality.\n• None A non-PRF can be constructed by diagonalizing on the set of PRFs. Also, the Ackermann function is specifically a non-PRF.\n\nPrimRecFuncTest contains an extensive test suite for each function, and it can be run as a main program. PrimRecFunc contains data structures that you can use to build your own computations. PrimRecFunc also contains a library of useful PRFs for arithmetic, Boolean logic, and comparisons. This library of PRFs is described near the bottom of the page.\n\nis a class that represents a primitive recursive function. To get a PRF, use one of these subclass objects or constructors:\n• None is a constructor that takes two integers: is the size of the input vector, and is the index to extract.\n\n Zero-based indexing is used.\n• None is a constructor that takes a PRF followed by a list of PRFs .\n\n These PRFs must have the correct number of input arguments, but this is not checked when the is constructed.\n• None is a constructor that takes a PRF followed by a PRF .\n\n These PRFs must have the correct number of input arguments, but this is not checked when the is constructed.\n\nTo evaluate a PRF with a vector (list) of natural numbers , use the expression , which yields a natural number.\n• None The subclass allows you to smuggle a native Python function as a , which can let you speed up function evaluation. For example: . For example, the function uses the PRF , which can be substituted for a faster native function. Generally, if you are replacing a PRF with a native function, you should first ensure that the PRF has been mathematically proven to be correctly implemented.\n\nis a data type representing a primitive recursive function. To get a PRF, use one of these data constructors:\n• None takes two integers: the first is the size of the input vector, and the second is the index to extract.\n\n Zero-based indexing is used.\n• None takes a PRF \\(f\\) followed by a list of PRFs \\(g_0, \\ldots, g_{k-1}\\).\n\n These PRFs must have the correct number of input arguments, but this is not checked when the is constructed.\n• None takes a PRF \\(f\\) followed by a PRF \\(g\\).\n\n These PRFs must have the correct number of input arguments, but this is not checked when the is constructed.\n• None C (R (const 1) (C mul [C S [I 3 1], I 3 0])) [I 1 0, Z] (1 input argument) computes \\(x!\\).\n\nTo evaluate a PRF with a vector (list) of natural numbers , use the expression , which yields a natural number.\n• None The data constructor allows you to smuggle a native Haskell function as a , which can let you speed up function evaluation. For example: . For example, the function uses the PRF , which can be substituted for a faster native function. Generally, if you are replacing a PRF with a native function, you should first ensure that the PRF has been mathematically proven to be correctly implemented.\n• None The function calculates the number of input arguments that the given takes, and it also checks that the entire function structure is internally consistent.\n• None The function is an extension of that returns a tuple of numbers: (result, number of evaluation steps, maximum recursion depth). This can help in understanding PRFs evaluations that run slowly and PRFs evaluations that overflow the stack.\n• None If you are using the Glasgow Haskell Compiler (GHC), using compiled mode instead of interpreted mode will make the code run much faster.\n\nis a class that represents a primitive recursive function. To get a PRF, use one of these subclass objects or constructors:\n• None is a factory method that takes two integers: is the size of the input vector, and is the index to extract.\n\n Zero-based indexing is used.\n• None is a factory method that takes a PRF followed by a list of PRFs .\n\n These PRFs must have the correct number of input arguments, but this is not checked when the is constructed.\n• None is a factory method that takes a PRF followed by a PRF .\n\n These PRFs must have the correct number of input arguments, but this is not checked when the is constructed.\n\nTo evaluate a PRF with an array or varargs list of natural numbers , use the expression , which yields a natural number.\n• None You can smuggle a native Java function as a by subclassing and implementing and . This can let you speed up function evaluation for large arguments or very complicated functions. For example, the function uses the PRF , which can be substituted for a faster native function. Generally, if you are replacing a PRF with a native function, you should first ensure that the PRF has been mathematically proven to be correctly implemented.\n\nThese included PRFs serve as excellent examples of how to implement familiar, useful functions in terms of PRFs.\n\nBoolean functions (0 means false, 1 means true, and all other input values yield arbitrary output values):\n• : NOT (1 argument) (named in the Python version)\n• : AND (2 arguments) (named in the Python version)\n• : OR (2 arguments) (named in the Python version)\n• : Less than or equal (2 arguments)\n\nTo see examples of input/output values for each library function, please read the test suites: primrecfunc-test.py, PrimRecFuncTest.hs, PrfTest.java"
    },
    {
        "link": "https://matklad.github.io/2024/08/01/primitive-recursive-functions.html",
        "document": "Programmers on the internet often use “Turing-completeness” terminology. Typically, not being Turing-complete is extolled as a virtue or even a requirement in specific domains. I claim that most such discussions are misinformed — that not being Turing complete doesn’t actually mean what folks want it to mean, and is instead a stand-in for a bunch of different practically useful properties, which are mostly orthogonal to actual Turing completeness.\n\nWhile I am generally descriptivist in nature and am ok with words losing their original meaning as long as the new meaning is sufficiently commonly understood, Turing completeness is a hill I will die on. It is a term from math, it has a very specific meaning, and you are not allowed to re-purpose it for anything else, sorry!\n\nI understand why this happens: to really understand what Turing completeness is and is not you need to know one (simple!) theoretical result about so-called primitive recursive functions. And, although this result is simple, I was only made aware of it in a fairly advanced course during my masters. That’s the CS education deficiency I want to rectify — you can’t teach students the halting problem without also teaching them about primitive recursion!\n\nThe post is going to be rather meaty, and will be split in three parts:\n\nIn Part I, I give a TL;DR for the theoretical result and some of its consequences. Part II is going to be a whirlwind tour of Turing Machines, Finite State Automata and Primitive Recursive Functions. And then Part III will circle back to practical matters.\n\nIf math makes you slightly nauseous, you might to skip Part II. But maybe give it a try? The math we’ll need will be baby math from first principles, without reference to any advanced results.\n\nare simple! An FSM takes a string as input, and returns a binary answer, “yes” or “no”. Unsurprisingly an FSM has a finite number of states: Q0, Q1, …, Qn. A subset of states are designated as “yes” states, the rest are “no” states. There’s also one specific starting state. The behavior of the state machine is guided by a transition (step) function, . This function takes the current state of FSM, the next symbol of input, and returns a new state. The semantics of FSM is determined by repeatably applying the single step function for all symbols of the input, and noting whether the final state is a “yes” state or a “no” state. Here’s an FSM which accepts only strings of zeros and ones of even length: This machine ping-pongs between states Q0 and Q1 ends up in Q0 only for inputs of even length (including an empty input). What can FSMs do? As they give a binary answer, they are recognizers — they don’t compute functions, but rather just characterize certain sets of strings. A famous result is that the expressive power of FSMs is equivalent to the expressive power of regular expressions. If you can write a regular expression for it, you could also do an FSM! There are also certain things that state machines can’t do. For example they can’t enter an infinite loop. Any FSM is linear in the input size and always terminates. But there are much more specific sets of strings that couldn’t be recognized by an FSM. Consider this set: That is, an infinite set which contains ‘1’s surrounded by the equal number of ‘0’s on the both sides. Let’s prove that there isn’t a state machine that recognizes this set! As usually, suppose there is such a state machine. It has a certain number of states — maybe a dozen, maybe a hundred, maybe a thousand, maybe even more. But let’s say fewer than a million. Then, let’s take a string which looks like a million zeros, followed by one, followed by million zeros. And let’s observe our FSM eating this particular string. First of all, because the string is in fact a one surrounded by the equal number of zeros on both sides, the FSM ends up in a “yes” state. Moreover, because the length of the string is much greater than the number of states in the state machine, the state machine necessarily visits some state twice. There is a cycle, where the machine goes from A to B to C to D and back to A. This cycle might be pretty long, but it’s definitely shorter than the total number of states we have. And now we can fool the state machine. Let’s make it eat our string again, but this time, once it completes the ABCDA cycle, we’ll force it to traverse this cycle again. That is, the original cycle corresponds to some portion of our giant string: If we duplicate this portion, our string will no longer look like one surrounded by equal number of twos, but the state machine will still in the “yes” state. Which is a contradiction that completes the proof.\n\nThere are a bunch of fiddly details to Turing Machines! The tape is conceptually infinite, so beyond the input, everything is just zeros. This creates a problem: it might be hard to say where the input (or the output) ends! There are a couple of technical solutions here. One is to say that there are three different symbols on the tape — zeros, ones, and blanks, and require that the tape is initialized with blanks. A different solution is to invent some encoding scheme. For example, we can say that the input is a sequence of 8-bit bytes, without interior null bytes. So, eight consecutive zeros at a byte boundary designate the end of input/output. It’s useful to think about how this byte-oriented TM could be implemented. We could have one large state for each byte of input. So, Q142 would mean that the head is on the byte with value 142. And then we’ll have a bunch of small states to read out the current byte. Eg, we start reading a byte in state . Depending on the next bit we move to S0 or S1, then to S00, or S01, etc. Once we reached something like S01111001, we move back 8 positions and enter state Q121. This is one of the patterns of Turing Machine programming — while your main memory is the tape, you can represent some constant amount of memory directly in the states. What we’ve done here is essentially lowering a byte-oriented Turing Machine to a bit-oriented machine. So, we could think only in terms of big states operating on bytes, as we know the general pattern for converting that to direct bit-twiddling. With this encoding scheme in place, we now can feed arbitrary files to a Turing Machine! Which will be handy to the next observation: You can’t actually program a Turing Machine. What I mean is that, counter-intuitively, there isn’t some user-supplied program that a Turing Machine executes. Rather, the program is hard-wired into the machine. The transition function is the program. But with some ingenuity we can regain our ability to write programs. Recall that we’ve just learned to feed arbitrary files to a TM. So what we could do is to write a text file that specifies a TM and its input, and then feed that entire file as an input to an “interpreter” Turing Machine which would read the file, and act as the machine specified there. A Turing Machine can have an function. Is such an “interpreter” Turing Machine possible? Yes! And it is not hard: if you spend a couple of hours programming Turing Machines by hand, you’ll see that you pretty much can do anything — you can do numbers, arithmetic, loops, control flow. It’s just very very tedious. So let’s just declare that we’ve actually coded up this Universal Turing Machine which simulates a TM given to it as an input in a particular encoding. This sort of construct also gives rise to the Church-Turing thesis. We have a TM which can run other TMs. And you can implement a TM interpreter in something like Python. And, with a bit of legwork, you could also implement a Python interpreter as a TM (you likely want to avoid doing that directly, and instead do a simpler interpreter for WASM, and then use a Python interpreter compiled to WASM). This sort of bidirectional interpretation shows that Python and TMs have equivalent computing power. Moreover, it’s quite hard to come up with a reasonable computational device which is more powerful than a Turing Machine. There are computational devices that are strictly weaker than TMs though. Recall FSMs. By this point, it should be obvious that a TM can simulate an FSM. Everything a Finite State Machine can do, a Turing Machine can do as well. And it should be intuitively clear that a TM is more powerful than an FSM. An FSM gets to use only a finite number of states. A TM has these same states, but it also posses a tape which serves like an infinitely sized external memory. Directly proving that you can’t encode a Universal Turing Machine as an FSM sounds complicated, so let’s prove something simpler. Recall that we have established that there’s no FSM that accepts only ones surrounded by an equal number of zeros on both sides (because a sufficiently large word of this form would necessary enter a cycle in a state machine, which could then be further pumped). But it’s actually easy to write a Turing Machine that does this:\n• Erase zero (at the left side of the tape)\n• Go to the right end of the tape\n• Go to the left side of the tape We found a specific problem that can be solved by a TM, but is out of reach of any FSM. So it necessarily follows that there isn’t an FSM that can simulate an arbitrary TM. It is also useful to take a closer look at the tape. It is a convenient skeuomorphic abstraction which makes the behavior of the machine intuitive, but it is inconvenient to implement in a normal programming language. There isn’t a standard data structure that behaves just like a tape. One cool practical trick is to simulate the tape as a pair of stacks. Take this: And transform it to something like this: That is, everything to the left of the head is one stack, everything to the right, reversed, is the other. Here, moving the reading head left or right corresponds to popping a value off one stack and pushing it onto another. So, an equivalent-in-power definition would be to say that a TM is an FSM endowed with two stacks. This of course creates an obvious question: is an FSM with just one stack a thing? Yes! It would be called a pushdown automaton, and it would correspond to context-free languages. But that’s beyond the scope of this post! There’s yet another way to look at the tape, or the pair of stacks, if the set of symbols is 0 and 1. You could say that a stack is just a number! So, something like will be . Looking at the top of the stack is , removing an item from the stack is and pushing x onto the stack is . We won’t need this right now, so just hold onto this for a brief moment.\n\nOk, so we have some idea about the lower bound for the power of a Turing Machine — FSMs are strictly less expressive. What about the opposite direction? Is there some computation that a Turing Machine is incapable of doing? Yes! Let’s construct a function which maps natural numbers to natural numbers, which can’t be implemented by a Turing Machine. Recall that we can encode an arbitrary Turing Machine as text. That means that we can actually enumerate all possible Turing Machines, and write them in a giant line, from the most simple Turing Machine to more complex ones: This is of course going to be an infinite list. Now, let’s see how TM0 behaves on input : it either prints something, or doesn’t terminate. Then, note how TM1 behaves on input , and generalizing, create function that behaves as the nth TM on input . It might look something like this: Now, let’s construct function which is maximally diffed from : where gives , will return , and it will return in all other cases: There isn’t a Turing machine that computes . For suppose there is. Then, it exists in our list of all Turing Machines somewhere. Let’s say it is TM1000064. So, if we feed to it, it will return , which is , which is different from . And the same holds for , and , and . But once we get to , we are in trouble, because, by the definition of , is different from what is computed by TM1000064! So such a machine is impossible. Those math savvy might express this more succinctly — there’s a countably-infinite number of Turing Machines, and an uncountably-infinite number of functions. So there must be some functions which do not have a corresponding Turing Machine. It is the same proof — the diagonalization argument is hiding in the claim that the set of all functions is an uncountable set. But this is super weird and abstract. Let’s rather come up with some very specific problem which isn’t solvable by a Turing Machine. The halting problem: given source code for a Turing Machine and its input, determine if the machine halts on this input eventually. As we have waved our hands sufficiently vigorously to establish that Python and Turing Machines have equivalent computational power, I am going to try to solve this in Python: Now, I will do a weird thing and start asking whether a program terminates, if it is fed its own source code, in a reverse-quine of sorts: and finally I construct this weird beast of a program: To make this even worse, I’ll feed the text of this program to itself. Does it terminate with this input? Well, if it terminates, and if our function is implemented correctly, then the invocation above returns . But then we enter the infinite loop and don’t actually terminate. Hence, it must be the case that does not terminate when self-applied. But then returns , and it should terminate. So we get a contradiction both ways. Which necessarily means that either our sometimes returns a straight-up incorrect answer, or that it sometimes does not terminate. So this is the flip side of a Turing Machine’s power — it is so powerful that it becomes impossible to tell whether it’ll terminate or not! It actually gets much worse, because this result can be generalized to an unreasonable degree! In general, there’s very little we can say about arbitrary programs. We can easily check syntactic properties (is the program text shorter than 4 kilobytes?), but they are, in some sense, not very interesting, as they depend a lot on how exactly one writes a program. It would be much more interesting to check some refactoring-invariant properties, which hold when you change the text of the program, but leave the behavior intact. Indeed, “does this change preserve behavior?” would be one very useful property to check! So let’s define two TMs to be equivalent, if they have identical behavior. That is, for each specific input, either both machines don’t terminate, or they both halt, and give identical results. Then, our refactoring-invariant properties are, by definition, properties that hold (or do not hold) for the entire classes of equivalence of TMs. And a somewhat depressing result here is that there are no non-trivial refactoring-invariant properties that you can algorithmically check. Suppose we have some magic TM, called P, which checks such a property. Let’s show that, using P, we can solve the problem we know we can not solve — the halting problem. Consider a Turing Machine that is just an infinite loop and never terminates, M1. P might or might not hold for it. But, because P is non-trivial (it holds for some machines and doesn’t hold for some machines), there’s some different machine M2 which differs from M1 with respect to P. That is, holds. Let’s use these M1 and M2 to figure out whether a given machine M halts on input I. Using Universal Turing Machine (interpreter), we can construct a new machine, M12 that just runs M on input I, then erases the contents of the tape and runs M2. Now, if M halts on I, then the resulting machine M12 is behaviorally-equivalent to M2. If M doesn’t halt on I, then the result is equivalent to the infinite loop program, M1. Or, in pseudo-code: This is pretty bad and depressing — we can’t learn anything meaningful about an arbitrary Turing Machine! So let’s finally get to the actual topic of today’s post:\n\nThis is going to be another computational device, like FSMs and TMs. Like an FSM, it’s going to be a nice, always terminating, non-Turing complete device. But it will turn out to have quite a bit of the power of a full Turing Machine! However, unlike both TMs and FSMs, are defined directly as functions which take a tuple of natural numbers and return a natural number. The two simplest ones are (that is, zero-arity function that returns ) and — a unary function that just adds 1. Everything else is going to get constructed out of these two: One way we are allowed to combine these functions is by composition. So we can get all the constants right off the bat: We aren’t going to be allowed to use general recursion (because it can trivially non-terminate), but we do get to use a restricted form of C-style loop. It is a bit fiddly to define formally! The overall shape is . Here, and are numbers — the initial value of the accumulator and the total number of iterations. The is a unary function that specifies the loop body – it takes the current value of the accumulator and returns the new value. So While this is similar to a C-style loop, the crucial difference here is that the total number of iterations is fixed up-front. There’s no way to mutate the loop counter in the loop body. This allows us to define addition: Multiplication is trickier. Conceptually, to multiply and , we want to from zero, and repeat “add ” times. The problem here is that we can’t write an “add ” function yet # Doesn't work either, no x in scope! One way around this is to define as a family of operators, which can pass extra arguments to the iteration function: That is, takes an extra arguments, and passes them through to any invocation of the body function. To express this idea a little bit more succinctly, let’s just allow to partially apply the second argument of . That is:\n• All our functions are going to be first order. All arguments are numbers, the result is a number. ’ ’\n• is not a function in our language — ’ convenience, we allow passing partially applied functions to it. But semantically this is equivalent to just passing in extra arguments on each iteration. Which finally allows us to write Ok, so that’s progress — we made something as complicated as multiplication, and we still are in the guaranteed-to-terminate land. Because each loop has a fixed number of iterations, everything eventually finishes. We can go on and define xy: And this in turn allows us to define a couple of concerning fast growing functions: That’s fun, but to do some programming, we’ll need an . We’ll get to it, but first we’ll need some boolean operations. We can encode as and as . Then Defining is tricky, due to two problems: First, we only have natural numbers, no negatives. This one is easy to solve — we’ll just define subtraction to saturate. The second problem is more severe — I think we actually can’t express subtraction given the set of allowable operations so far. That is because all our operations are monotonic — the result is never less than the arguments. One way to solve this problem is to define the LOOP in such a way that the body function also gets passed a second argument — the current iteration. So, if you iterate up to , the last iteration will observe , and that would be the non-monotonic operation that creates subtraction. But that seems somewhat inelegant to me, so instead I will just add a function to the basis, and use that to add loop counters to our iterations. And now we can do a bunch of comparison operators: With that we could implement modulus. To compute we will start with , and will be subtracting until we get a number smaller than . We’ll need at most iterations for that. That’s a curious structure — rather than computing the modulo directly, we essentially search for it using trial and error, and relying on the fact that the search has a clear upper bound. Division can be done similarly: to divide x by y, start with 0, and then repeatedly add one to the accumulator until the product of the accumulator and y exceeds x: This really starts to look like programming! One thing we are currently missing are data structures. While our functions take multiple arguments, they only return one number. But it’s easy enough to pack two numbers into one: to represent an pair, we’ll use 2a 3b number: To deconstruct such a pair into its first and second components, we need to find the maximum power of 2 or 3 that divides our number. Which is exactly the same shape we used to implement : Here again we use the fact that the maximal power of two that divides is not larger than itself, so we can over-estimate the number of iterations we’ll need as . Using this pair construction, we can finally add a loop counter to our construct. To track the counter, we pack it as a pair with the accumulator: And then inside f, we first unpack that pair into accumulator and counter, pass them to actual loop iteration, and then pack the result again, incrementing the counter: Ok, so we have achieved something remarkable: while we are writing terminating-by-construction programs, which are definitely not Turing complete, we have constructed basic programming staples, like boolean logic and data structures, and we have also built some rather complicated mathematical functions, like 22N. We could try to further enrich our little primitive recursive kingdom by adding more and more functions on an ad hoc basis, but let’s try to be really ambitious and go for the main prize — simulating Turing Machines. We know that we will fail: Turing machines can enter an infinite loop, but PRFs necessarily terminate. That means, that, if a PRF were able to simulate an arbitrary TM, it would have to say after a certain finite amount of steps that “this TM doesn’t terminate”. And, while we didn’t do this, it’s easy to see that you could simulate the other way around and implement PRFs in a TM. But that would give us a TM algorithm to decide if an arbitrary TM halts, which we know doesn’t exist. So, this is hopeless! But we might still be able to learn something from failing. Ok! So let’s start with a configuration of a TM which we somehow need to encode into a single number. First, we need the state variable proper (Q0, Q1, etc), which seems easy enough to represent with a number. Then, we need a tape and a position of the reading head. Recall how we used a pair of stacks to represent exactly the tape and the position. And recall that we can look at a stack of zeros and ones as a number in binary form, where push and pop operations are implemented using , , and — exactly the operations we already can do. So, our configuration is just three numbers: . And, using the 2a3b5c trick, we can pack this triple into just a single number. But that means we could directly encode a single step of a Turing Machine: # and the symbol at the top of left stack is 0 div(snd(config), 2), # pop value from the left stack mul(trd(config), 2), # push zero onto the right stack And now we could plug that into our to simulate a Turing Machine running for N steps: The catch of course is that we can’t know the that’s going to be enough. But we can have a very good guess! We could do something like this: That is, run for some large tower of exponents of the initial state. Which would be plenty for normal algorithms, which are usually 2N at worst! If a TM has a runtime which is bounded by some primitive-recursive function, then the entire TM can be replaced with a PRF. Be advised that PRFs can grow really fast. Which is the headline result we have set out to prove!\n\nIt might seem that non-termination is the only principle obstacle. That anything that terminates at all has to be implementable as a PRF. Alas, that’s not so. Let’s go and construct a function that is surmountable by a TM, but is out of reach of PRFs. We will combine the ideas of the impossibility proofs for FSMs (noting that if a function is computed by some machine, that machine has a specific finite size) and TMs (diagonalization). So, suppose we have some function that can’t be computed by a PRF. How would we go about proving that? Well, we’d start with “suppose that we have a PRF P that computes ”. And then we could notice that P would have some finite size. If you look at it abstractly, the P is its syntax tree, with lots of constructs, but it always boils down to some s and s at the leaves. Let’s say that the depth of P is . And, actually, if you look at it, there are only a finite number of PRFs with depth at most . Some of them describe pretty fast growing functions. But probably there’s a limit to how fast a function can grow, given that it is computed by a PRF of size . Or, to use a concrete example: we have constructed a PRF of depth 5 that computes two to the power of two to the power of N. Probably if we were smarter, we could have squeezed a couple more levels into that tower of exponents. But intuitively it seems that if you build a tower of, say, 10 exponents, that would grow faster than any PRF of depth . And that this generalizes — for any fixed depth, there’s a high-enough tower of exponents that grows faster than any PRF with that depth. So we could conceivably build an that defeats our -deep P. But that’s not quite a victory yet: maybe that is feasible for -deep PRFs! So here we’ll additionally apply diagonalization: for each depth, we’ll build it’s own depth-specific nemesis . And then we’ll define our overall function as So, for large enough it’ll grow faster than a PRF with any fixed depth. So that’s the general plan, the rest of the own is basically just calculating the upper bound on the growth of a PRF of depth . One technical difficulty here is that PRFs tend to have different arities: Ideally, we’d use just one upper bound of them all. So we’ll be looking for an upper bound of the following form:\n• Compute the largest of its arguments.\n• And plug that into unary function for depth Let’s start with . We have only primitive functions on this level, , , and , so we could say that Now, let’s handle an arbitrary other depth . In that case, our function is non-primitive, so at the root of the syntax tree we have either a composition or a . Composition would look like this: where and are deep and the resulting is deep. We can immediately estimate the then: In this somewhat loose notation, stands for a tuple of arguments, and stands for the largest one. And then we could use the same estimate for : This is super high-order, so let’s do a concrete example for a depth-2 two-argument function which starts with a composition: This sounds legit: if we don’t use LOOP, then is either or so indeed is the bound! Ok, now the fun case! If the top-level node is a , then we have This sounds complicated to estimate, especially due to that last argument, which is the number of iterations. So we’ll be cowards and won’t actually try to estimate this case. Instead, we will require that our PRF is written in a simplified form, where the first and the last arguments to are simple. So, if your PRF looks like you are required to re-write it first as So now we only have to deal with this: On the first iteration, we’ll call , which we can estimate as . That is, does get an extra argument, but it is one of the original arguments of , and we are looking at the maximum argument anyway, so it doesn’t matter. On the second iteration, we are going to call which we can estimate as . Now we plug our estimation for the first iteration: That is, the estimate for the first iteration is . The estimation for the second iteration adds one more layer: . For the third iteration we’ll get . So the overall thing is going to be smaller than iteratively applied to itself some number of times, where “some number” is one of the original arguments. But no harm’s done if we iterate up to . As a sanity check, the worst depth-2 function constructed with iteration is probably which is . And our estimate gives applied times to , which is , which is indeed the correct upper bound! Combining everything together, we have: That there is significant — although it seems like the second line, with applications, is always going to be longer, , in fact, could be as small as zero. But we can take repetitions to fix this: So let’s just define to make that inequality work: We define a family of unary functions , such that each “grows faster” than any n-ary PRF of depth . If is a ternary PRF of depth 3, then . To evaluate at point , we use the following recursive procedure: We can simplify this a bit if we stop treating as a kind of function index, and instead say that our is just a function of two arguments. Then we have the following equations: The last equation can re-formatted as And for non-zero x that is just So we get the following recursive definition for A(d, x): It’s easy to see that computing on a Turing Machine using this definition terminates — this is a function with two arguments, and every recursive call uses a lexicographically smaller pair of arguments. And we constructed A in such a way that as a function of is larger than any PRF with a single argument of depth d. But that means that the following function with one argument grows faster than any PRF. And that’s an example of a function which a Turing Machine has no trouble computing (given sufficient time), but which is beyond the capabilities of PRFs.\n\nRemember, this is a three-part post! And are finally at the part 3! So let’s circle back to the practical matters. We have learned that:\n• While other computational devices, like FSMs and PRFs, can be made to always terminate, there ’ ’ ll terminate fast. PRFs in particular can compute quite large functions!\n• And non-Turing complete devices can be quite expressive. For example, any real-world algorithm that works on a TM can be adapted to run as a PRF.\n• ’ t even have to contort the algorithm much to make it fit. There ’ recipe for how to take something Turing complete and make it a primitive recursive function — just add an iteration counter to the device, and forcibly halt it if the counter grows Or, more succinctly: there’s no practical difference between a program that doesn’t terminate, and the one that terminates after a billion years. As a practitioner, if you think you need to solve the first problem, you need to solve the second problem as well. And making your programming language non-Turing complete doesn’t really help with this. And yet, there are a lot of configuration languages out there that use non-Turing completeness as one of their key design goals. Why is that? I would say that we are never interested in Turing-completeness per-se. We usually want some much stronger properties. And yet there’s no convenient catchy name for that bag of features of a good configuration language. So, “non-Turing-complete” gets used as a sort of rallying cry to signal that something is a good configuration language, and maybe sometimes even to justify to others inventing a new language instead of taking something like Lua. That is, the real reason why you want at least a different implementation is all those properties you really need, but they are kinda hard to explain, or at least much harder than “we can’t use Python/Lua/JavaScript because they are Turing-complete”. So what are the properties of a good configuration language? First, we need the language to be deterministic. If you launch Python and type , you’ll see some number. If you hit , and than do this again, you’ll see a different number. This is OK for “normal” programming, but is usually anathema for configuration. Configuration is often used as a key in some incremental, caching system, and letting in non-determinism there wreaks absolute chaos! Second, you need the language to be well-defined. You can compile Python with ASLR disabled, and use some specific allocator, such that always returns the same result. But that result would be hard to predict! And if someone tries to do an alternative implementation, even if they disable ASLR as well, they are likely to get a different deterministic number! Or the same could happen if you just update the version of Python. So, the semantics of the language should be clearly pinned-down by some sort of a reference, such that it is possible to guarantee not only deterministic behavior, but fully identical behavior across different implementations. Third, you need the language to be pure. If your configuration can access environment variables or read files on disk, than the meaning of the configuration would depend on the environment where the configuration is evaluated, and you again don’t want that, to make caching work. Fourth, a thing that is closely related to purity is security and sandboxing. The mechanism to achieve both purity and security is the same — you don’t expose general IO to your language. But the purpose is different: purity is about not letting the results be non-deterministic, while security is about not exposing access tokens to the attacker. And now this gets tricky. One particular possible attack is a denial of service — sending some bad config which makes our system just spin there burning the CPU. Even if you control all IO, you are generally still open to these kinds of attacks. It might be OK to say this is outside of the threat model — that no one would find it valuable enough to just burn your CPU, if they can’t also do IO, and that, even in the event that this happens, there’s going to be some easy mitigation in the form of a higher-level timeout. But you also might choose to provide some sort of guarantees about execution time, and that’s really hard. Two approaches work. One is to make sure that processing is obviously linear. Not just terminates, but is actually proportional to the size of inputs, and in a very direct way. If the correspondence is not direct, than it’s highly likely that it is in fact non linear. The second approach is to ensure metered execution — during processing, decrement a counter for every simple atomic step and terminate processing when the counter reaches zero. Finally one more vague property you’d want from a configuration language is for it to be simple. That is, to ensure that, when people use your language, they write simple programs. It seems to me that this might actually be the case where banning recursion and unbounded loops could help, though I am not sure. As we know from the PRF exercise, this won’t actually prevent people from writing arbitrary recursive programs. It’ll just require some roundabout code to do that. But maybe that’ll be enough of a speedbump to make someone invent a simple solution, instead of brute-forcing the most obvious one? That’s all for today! Have a great weekend, and remember: Any algorithm that can be implemented by a Turing Machine such that its runtime is bounded by some primitive recursive function of input can also be implemented by a primitive recursive function!"
    },
    {
        "link": "https://cs.umd.edu/~gasarch/COURSES/452/S14/dec.pdf",
        "document": ""
    },
    {
        "link": "https://softwareengineering.stackexchange.com/questions/308978/enumerating-the-primitive-recursive-functions",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://sjorv.github.io/CSC363%20W2023/Tutorials/2/CSC363_Tutorial_2.pdf",
        "document": ""
    },
    {
        "link": "https://rosettacode.org/wiki/Ackermann_function",
        "document": "The Ackermann function is a classic example of a recursive function, notable especially because it is not a primitive recursive function. It grows very quickly in value, as does the size of its call tree.\n\n\n\n The Ackermann function is usually defined as follows:\n\n\n\n Its arguments are never negative and it always terminates.\n\nWrite a function which returns the value of . Arbitrary precision is preferred (since the function grows so quickly), but not required.\n\nThe OS/360 linkage is a bit tricky with the S/360 basic instruction set. To simplify, the program is recursive not reentrant.\n\nThis implementation is based on the code shown in the computerphile episode in the youtube link at the top of this page (time index 5:00).\n\nThis function does 16-bit math. The test code prints a table of for and , on a real 8080 this takes a little over two minutes.\n\nThis code does 16-bit math just like the 8080 version.\n\nAction! language does not support recursion. Therefore an iterative approach with a stack has been proposed.\n\nThe implementation does not care about arbitrary precision numbers because the Ackermann function does not only grow, but also slow quickly, when computed recursively.\n\nThe Unicode characters can be entered in Emacs Agda as follows:\n\nA version that takes the arguments together in a single array:\n\nThis version works way faster than the classical one: Ackermann(3, 5) runs in 12,7 ms, while the classical version takes 402,2 ms.\n\nBASIC runs out of stack space very quickly. The call gives a stack error.\n\nHad trouble with this, so called in the gurus at StackOverflow. Thanks to Patrick Cuff for pointing out where I was going wrong.\n\nBecause of the statements, running this bare closes one's shell, so this test routine handles the calling of Ackermann.cmd\n\nRequires a that supports long names and the statement.\n\nA functional and recursive realization of the version above. Functions are realized by direct calls of functions via jumps (instruction ) to the entry points of two distinct functions:\n\nEach block of or in the code is a call of the Ackermann function itself.\n\nHigher values than A(4,1)/(5,0) lead to UInt64 wraparound, support for numbers bigger than 2^64-1 is not implemented in these solutions.\n\nSince Befunge-93 doesn't have recursive capabilities we need to use an iterative algorithm.\n\nThe program reads two integers (first m, then n) from command line, idles around funge space, then outputs the result of the Ackerman function. Since the latter is calculated truly recursively, the execution time becomes unwieldy for most m>3.\n\nThe Ackermann function on Church numerals (arbitrary precision), as shown in https://github.com/tromp/AIT/blob/master/fast_growing_and_conjectures/ackermann.lam is the 63 bit BLC program\n\nThree solutions are presented here. The first one is a purely recursive version, only using the formulas at the top of the page. The value of A(4,1) cannot be computed due to stack overflow. It can compute A(3,9) (4093), but probably not A(3,10)\n\nThe second version is a purely non-recursive solution that easily can compute A(4,1). The program uses a stack for Ackermann function calls that are to be evaluated, but that cannot be computed given the currently known function values - the \"known unknowns\". The currently known values are stored in a hash table. The Hash table also contains incomplete Ackermann function calls, namely those for which the second argument is not known yet - \"the unknown unknowns\". These function calls are associated with \"known unknowns\" that are going to provide the value of the second argument. As soon as such an associated known unknown becomes known, the unknown unknown becomes a known unknown and is pushed onto the stack.\n\nAlthough all known values are stored in the hash table, the converse is not true: an element in the hash table is either a \"known known\" or an \"unknown unknown\" associated with an \"known unknown\".\n\nThe last solution is a recursive solution that employs some extra formulas, inspired by the Common Lisp solution further down.\n\nThe last computation costs about 0,03 seconds.\n\nAckermann function makes a lot of recursive calls, so the above program is a bit naive. We need to be slightly less naive, by doing some simple caching:\n\nWhee. Well, with some extra work, we calculated one more n value, big deal, right? But see, you can see how fast it blows up. In fact, no amount of caching will help you calculate large m values; on the machine I use A(4, 2) segfaults because the recursions run out of stack space--not a whole lot I can do about it. At least it runs out of stack space quickly, unlike the first solution...\n\nA few further tweaks/optimizations may be possible.\n\nPossibly the most efficient implementation of Ackermann in C#. It successfully runs Ack(4,2) when executed in Visual Studio. Don't forget to add a reference to System.Numerics.\n\nWhen invoked, each required A(m,n) needed to solve the requested (solve ?m ?n) facts gets generated as its own fact. Below shows the invocation of the above, as well as an excerpt of the final facts list. Regardless of how many input (solve ?m ?n) requests are made, each possible A(m,n) is only solved once.\n\nno caching, the implementation takes ages even for A(4,1)\n\nThis needs a modern Dc with (swap) and (comment). It easily can be adapted to an older Dc, but it will impact readability a lot.\n\nIterative version, using a stack. First version used various GOTOs statement, now removed and substituted with the new ERRE control statements.\n\nPrints a list of Ackermann function values: from A(0,0) to A(3,9). Uses a stack to avoid overflow. Formating options to make this pretty are available, but for this example only basic output is used.\n\nThis is based on the VBScript example.\n\nThe following program implements the Ackermann function in F# but is not tail-recursive and so runs out of stack space quite fast.\n\nTransforming this into continuation passing style avoids limited stack space by permitting tail-recursion.\n\nThe above will output the below. Formating options to make this pretty are available, but for this example only basic output is used.\n\nMixed-language solution using pure FBSL, Dynamic Assembler, and Dynamic C layers of FBSL v3.5 concurrently. The following is a single script; the breaks are caused by switching between RC's different syntax highlighting schemes:\n\nFōrmulæ programs are not textual, visualization/edition of programs is done showing/manipulating structures but not text. Moreover, there can be multiple visual representations of the same program. Even though it is possible to have textual representation —i.e. XML, JSON— they are intended for storage and transfer purposes more than visualization and edition.\n\nPrograms in Fōrmulæ are created/edited online in its website.\n\nIn this page you can see and run the program(s) related to this task and their results. You can also change either the programs or the parameters they are called with, for experimentation, but remember that these programs were created with the main purpose of showing a clear solution of the task, and they generally lack any kind of validation.\n\nNote: In the default groovyConsole configuration for WinXP, \"ack(4,1)\" caused a stack overflow error!\n\nTaken from the public domain Icon Programming Library's acker in memrfncs, written by Ralph E. Griswold.\n\nThe different cases can be split into different lines:\n\nJ's stack was too small for me to compute .\n\nThis version works by first generating verbs (functions) and then applying them to compute the rows of the related Buck function; then the Ackermann function is obtained in terms of the Buck function. It uses extended precision to be able to compute 4 Ack 2.\n\nThe Ackermann function derived in this fashion is primitive recursive. This is possible because in J (as in some other languages) functions, or representations of them, are first-class values.\n\nAlso works with gojq, the Go implementation of jq.\n\nExcept for a minor tweak to the line using string interpolation, the following have also been tested using jaq, the Rust implementation of jq, as of April 13, 2023.\n\nFor infinite-precision integer arithmetic, use gojq or fq.\n\nWhile MAD supports function calls, it does not handle recursion automatically. There is support for a stack, but the programmer has to set it up himself (by defining an array to reserve memory, then making it the stack using the ) command. Values have to be pushed and popped from it by hand (using and ), and for a function to be reentrant, even the return address has to be kept.\n\nOn top of this, all variables are global throughout the program (there is no scope), and argument passing is done by reference, meaning that even once the stack is set up, arguments cannot be passed in the normal way. To define a function that takes arguments, one would have to declare a helper function that then passes the arguments to the recursive function via the stack or the global variables. The following program demonstrates this.\n\nStrictly by the definition given above, we can code this as follows.\n\nrefers to the currently executing procedure (closure) and is used when writing recursive procedures. (You could also use the name of the procedure, Ackermann in this case, but then a concurrently executing task or thread could re-assign that name while the recursive procedure is executing, resulting in an incorrect result.)\n\nTo make this faster, you can use known expansions for small values of . (See Wikipedia:Ackermann function)\n\nThis makes it possible to compute and essentially instantly, though is still out of reach.\n\nTo compute Ackermann( 1, i ) for i from 1 to 10 use\n\nTo get the first 10 values for m = 2 use\n\nFor Ackermann( 4, 2 ) we get a very long number with\n\nMathcad is a non-text-based programming environment. The equation below is an approximation of the way that it is entered (and) displayed on a Mathcad worksheet. The worksheet is available at https://community.ptc.com/t5/PTC-Mathcad/Rosetta-Code-Ackermann-Function/m-p/750117#M197410\n\nThis particular version of Ackermann's function was created in Mathcad Prime Express 7.0, a free version of Mathcad Prime 7.0 with restrictions (such as no programming or symbolics). All Prime Express numbers are complex. There is a recursion depth limit of about 4,500.\n\nThe worksheet also contains an explictly-calculated version of Ackermann's function that calls the tetration function n .\n\nTwo possible implementations would be:\n\nNote that the second implementation is quite a bit faster, as doing 'if' comparisons is slower than the built-in pattern matching algorithms. Examples:\n\nIf we would like to calculate Ackermann[4,1] or Ackermann[4,2] we have to optimize a little bit:\n\nNow computing Ackermann[4,1] and Ackermann[4,2] can be done quickly (<0.01 sec): Examples 2:\n\nAckermann[4,2] has 19729 digits, several thousands of digits omitted in the result above for obvious reasons. Ackermann[5,0] can be computed also quite fast, and is equal to 65533. Summarizing Ackermann[0,n_], Ackermann[1,n_], Ackermann[2,n_], and Ackermann[3,n_] can all be calculated for n>>1000. Ackermann[4,0], Ackermann[4,1], Ackermann[4,2] and Ackermann[5,0] are only possible now. Maybe in the future we can calculate higher Ackermann numbers efficiently and fast. Although showing the results will always be a problem.\n\nUse with caution. Will cause a stack overflow for m > 3.\n\nThis is the Ackermann function with some (obvious) elements elided. The predicate is implemented in terms of the function. The function is implemented in terms of the predicate. This makes the code both more concise and easier to follow than would otherwise be the case. The type is used instead of because the problem statement stipulates the use of bignum integers if possible.\n\nML/I loves recursion, but runs out of its default amount of storage with larger numbers than those tested here!\n\nThe type CARDINAL is defined in Modula-3 as [0..LAST(INTEGER)], in other words, it can hold all positive integers.\n\nIn Nemerle, we can state the Ackermann function as a lambda. By using pattern-matching, our definition strongly resembles the mathematical notation.\n\nA terser version using implicit (which doesn't use the alias internally):\n\nOr, if we were set on using the notation, we could do this:\n\nwith memoization using an hash-table:\n\ntaking advantage of the memoization we start calling small values of m and n in order to reduce the recursion call stack:\n\nThis one uses the arbitrary precision, the tail-recursion, and the optimisation explain on the Wikipedia page about .\n\nWe memoize calls to A to make A(2, n) and A(3, n) feasible for larger values of n.\n\nAn implementation using the conditional statements 'if', 'elsif' and 'else':\n\nAn optimized version, which uses as a stack, instead of recursion. Very fast.\n\nack(4,2) and above fail with power function overflow. ack(3,100) will get you an answer, but only accurate to 16 or so digits.\n\nThis is a naive implementation that does not use any optimization. Find the explanation at [[1]]. Computing the Ackermann function for (4,1) is possible, but takes quite a while because the stack grows very fast to large dimensions.\n\nBuilding an example table (takes a while to compute, though, especially for the last three numbers; also it fails with the last line in Powershell v1 since the maximum recursion depth is only 100 there):\n\nSave the result to an array (for possible future use?), then display it using the cmdlet:\n\nPython is not very adequate for deep recursion, so even setting sys.setrecursionlimit(1000000000) if m = 5 it throws 'maximum recursion depth exceeded'\n\nProcessing.R may exceed its stack depth at ~n==6 and returns null.\n\n\"Pure\" Prolog Version (Uses Peano arithmetic instead of is/2):\n\nFrom the Mathematica ack3 example:\n\nResults confirm those of Mathematica for ack(4,1) and ack(4,2)\n\nThe heading is more correct than saying the following is iterative as an explicit stack is used to replace explicit recursive function calls. I don't think this is what Comp. Sci. professors mean by iterative.\n\nNote that in either case, Int is defined to be arbitrary precision in Raku.\n\nHere's a caching version of that, written in the sigilless style, with liberal use of Unicode, and the extra optimizing terms to make A(4,2) possible:\n\nThis output is from Regina and takes about 4 seconds. Running under ooRexx, the last line displayed is 'Ackermann(3, 6)...' and then the program just stops at (3,7). Looks like very deep recursion is more limited in ooRexx.\n\nRegina reaches somewhat higher: (3,9) = 4094 in 12s and 11m calls, (3,10) = 8189 in 55s and 45m calls, (3,11) = 16381 in 250s and 180m calls. But (4,1) is also out of reach for Regina...\n\nThis REXX version takes advantage that some of the lower numbers for the Ackermann function have direct formulas. \n\n\n\nIf the numeric digits 100 were to be increased to 20000, then the value of Ackermann(4,2) \n\n(the last line of output) would be presented with the full 19,729 decimal digits.\n\nOutput note: of the numbers shown below use recursion to compute.\n\nThe last value is correct in magnitude, but not in value (only last digits plus exponent). Some other entries and Wikipedia give 2.0035...E+19728.\n\n But leaving out the formatting and running with 20000 digits, the last value is correct shown in its full 19728 digits. \n\n By the way, this version does not illustrate recursion anymore, because all workable values are captured as special values. Ackermann(4,2) = 2^65536-3 (19768 digits) and Ackerman(4,3) = 2^(2^65536)-3, far beyond REXX' (and other languages) capabilities in expressing numbers.\n\nthe basic recursive function, because memorization and other improvements would blow the clarity.\n\nRuns in 7 min 13 secs on a HP-50g. Speed could be increased by replacing every by , which would force calculations to be made with floating-point numbers, but we would then lose the arbitrary precision.\n\nInstead of , the class could be used, even though we need to use a workaround since in the GNU Sather v1.2.3 compiler the INTI literals are not implemented yet.\n\nAn improved solution that uses a lazy data structure, streams, and defines Knuth up-arrows to calculate iterative exponentiation:\n\nas modified by R. Péter and R. Robinson:\n\nBoth Snobol4+ and CSnobol stack overflow, at ack(3,3) and ack(3,4), respectively.\n\nOne could employ tail recursion elimination by replacing \"@/#\" with \"/\" in two places above.\n\nThe maximum levels of cascade calls in Db2 are 16, and in some cases when executing the Ackermann function, it arrives to this limit (SQL0724N). Thus, the code catches the exception and continues with the next try.\n\nWith Tcl 8.6, this version is preferred (though the language supports tailcall optimization, it does not apply it automatically in order to preserve stack frame semantics):\n\nTo Infinity… and Beyond!\n\nIf we want to explore the higher reaches of the world of Ackermann's function, we need techniques to really cut the amount of computation being done.\n\nBut even with all this, you still run into problems calculating as that's kind-of large…\n\nThis program assumes the variables N and M are the arguments of the function, and that the list L1 is empty. It stores the result in the system variable ANS. (Program names can be no longer than 8 characters, so I had to truncate the function's name.)\n\nHere is a handler function that makes the previous function easier to use. (You can name it whatever you want.)\n\nAnonymous recursion is the usual way of doing things like this.\n\ntest program for the first 4 by 7 numbers:\n\nBased on BASIC version. Uncomment all the lines referring to and see just how deep the recursion goes.\n\nThat worked well. Test it again:\n\nRecursion overflows the stack if either M or N is extended by a single count.\n\nThe following named template calculates the Ackermann function:\n\nHere it is as part of a template\n\nWhich will transform this input\n\nWhat smart code can get. Fast as lightning!"
    },
    {
        "link": "https://reddit.com/r/compsci/comments/bpckhp/the_y_combinator_or_how_to_implement_recursion_in",
        "document": "Computer Science Theory and Application. We share and discuss any content that computer scientists find interesting. People from all walks of life welcome, including hackers, hobbyists, professionals, and academics."
    },
    {
        "link": "https://geeksforgeeks.org/ackermann-function",
        "document": "In computability theory, the Ackermann function, named after Wilhelm Ackermann, is one of the simplest and earliest-discovered examples of a total computable function that is not primitive recursive. All primitive recursive functions are total and computable, but the Ackermann function illustrates that not all total computable functions are primitive recursive. Refer this for more.\n\nIt’s a function with two arguments each of which can be assigned any non-negative integer.\n\nAckermann function is defined as:\n\n\n\nHere’s the explanation of the given Algorithm: \n\nLet me explain the algorithm by taking the example A(1, 2) where m = 1 and n = 2 \n\nSo according to the algorithm initially the value of next, goal, value and current are:\n\n\n\nThough next[current] != goal[current], so else statement will execute and transferring become false. \n\nSo now, the value of next, goal, value and current are:\n\n\n\nSimilarly by tracing the algorithm until next[m] = 3 the value of next, goal, value and current are changing accordingly. Here’s the explanation how the values are changing,\n\n\n\nFinally returning the value e.g 4\n\nAnalysis of this algorithm:\n• The time complexity of this algorithm is: O(mA(m, n)) to compute A(m, n)\n• The space complexity of this algorithm is: O(m) to compute A(m, n) \n\n\n\nSolve A(1, 2)?\n\nAnswer:\n\nGiven problem is A(1, 2) \n\nHere m = 1, n = 2 e.g m > 0 and n > 0 \n\nHence applying third condition of Ackermann function \n\nA(1, 2) = A(0, A(1, 1)) ———- (1) \n\nNow, Let’s find A(1, 1) by applying third condition of Ackermann function \n\nA(1, 1) = A(0, A(1, 0)) ———- (2) \n\nNow, Let’s find A(1, 0) by applying second condition of Ackermann function \n\nA(1, 0) = A(0, 1) ———- (3) \n\nNow, Let’s find A(0, 1) by applying first condition of Ackermann function \n\nA(0, 1) = 1 + 1 = 2 \n\nNow put this value in equation 3 \n\nHence A(1, 0) = 2 \n\nNow put this value in equation 2 \n\nA(1, 1) = A(0, 2) ———- (4) \n\nNow, Let’s find A(0, 2) by applying first condition of Ackermann function \n\nA(0, 2) = 2 + 1 = 3 \n\nNow put this value in equation 4 \n\nHence A(1, 1) = 3 \n\nNow put this value in equation 1 \n\nA(1, 2) = A(0, 3) ———- (5) \n\nNow, Let’s find A(0, 3) by applying first condition of Ackermann function \n\nA(0, 3) = 3 + 1 = 4 \n\nNow put this value in equation 5 \n\nHence A(1, 2) = 4\n\nSo, A (1, 2) = 4\n\n\n\nHere is the simplest c and python recursion function code for generating Ackermann function\n\n\n\nIf you still wish to visualize how this result is arrived at, you can take a look at this page, which animates the calculation of every recursion step."
    },
    {
        "link": "https://reddit.com/r/programming/comments/2ztyog/brilliant_presentation_on_the_ackermann_function",
        "document": "Create your account and connect with a world of communities.\n\nBy continuing, you agree to our\n\nand acknowledge that you understand the"
    },
    {
        "link": "https://stackoverflow.com/questions/5605258/simple-loop-ackermann-function",
        "document": "Reach devs & technologists worldwide about your product, service or employer brand"
    }
]