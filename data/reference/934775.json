[
    {
        "link": "https://playwright.dev/python/docs/api/class-playwright",
        "document": "Playwright module provides a method to launch a browser instance. The following is a typical example of using Playwright to drive automation:\n\nTerminates this instance of Playwright in case it was created bypassing the Python context manager. This is useful in REPL applications.\n\nThis object can be used to launch or connect to Chromium, returning instances of Browser.\n\nReturns a dictionary of devices to be used with browser.new_context() or browser.new_page().\n\nThis object can be used to launch or connect to Firefox, returning instances of Browser.\n\nExposes API that can be used for the Web API testing.\n\nSelectors can be used to install custom selector engines. See extensibility for more information.\n\nThis object can be used to launch or connect to WebKit, returning instances of Browser."
    },
    {
        "link": "https://playwright.dev/docs/release-notes",
        "document": "\n• New option indexedDB for browserContext.storageState() allows to save and restore IndexedDB contents. Useful when your application uses IndexedDB API to store authentication tokens, like Firebase Authentication. Here is an example following the authentication guide:\n\nNew \"Copy prompt\" button on errors in the HTML report, trace viewer and UI mode. Click to copy a pre-filled LLM prompt that contains the error message and useful context for fixing the error.\n\nNew option visible for locator.filter() allows matching only visible elements.\n\nHTML report will show this information when available:\n\nA new TestStepInfo object is now available in test steps. You can add step attachments or skip the step under some conditions.\n• New option for methods page.emulateMedia() and browser.newContext() allows to emulate the media feature.\n• New option failOnStatusCode makes all fetch requests made through the APIRequestContext throw on response codes other than 2xx and 3xx.\n\nThis version was also tested against the following stable channels:\n• New option timeout allows specifying a maximum run time for an individual test step. A timed-out step will fail the execution of the test.\n• New method test.step.skip() to disable execution of a test step.\n• Expanded expect(locator).toMatchAriaSnapshot() to allow storing of aria snapshots in separate YAML files.\n• Added method expect(locator).toHaveAccessibleErrorMessage() to assert the Locator points to an element with a given aria errormessage.\n• Option testConfig.updateSnapshots added the configuration enum . updates only the snapshots that have changed, whereas now updates all snapshots, regardless of whether there are any differences.\n• New option testConfig.updateSourceMethod defines the way source code is updated when testConfig.updateSnapshots is configured. Added and modes that write the changes into source code, on top of existing mode that creates a patch file.\n• Option testConfig.webServer added a field for specifying a process kill signal other than the default .\n• Exposed testStep.attachments from the reporter API to allow retrieval of all attachments created by that step.\n• New option for and assertions in the testConfig.expect configuration.\n• New button in Codegen for picking elements to produce aria snapshots.\n• Additional details (such as keys pressed) are now displayed alongside action API calls in traces.\n• Display of content in traces is error-prone. Display is now disabled by default, and can be enabled via the UI setting.\n• expect(locator).toBeEditable() and locator.isEditable() now throw if the target element is not , , or a number of other editable elements.\n• Option testConfig.updateSnapshots now updates all snapshots when set to , rather than only the failed/changed snapshots. Use the new enum to keep the old functionality of only updating the changed snapshots.\n\nThis version was also tested against the following stable channels:\n\nNew assertion expect(locator).toMatchAriaSnapshot() verifies page structure by comparing to an expected accessibility tree, represented as YAML.\n\nYou can generate this assertion with Test Generator and update the expected snapshot with command line flag.\n\nLearn more in the aria snapshots guide.\n• New option testConfig.tsconfig allows to specify a single to be used for all tests.\n• New method test.fail.only() to focus on a failing test.\n• New value for testOptions.screenshot.\n• Added \"previous\" and \"next\" buttons to the HTML report to quickly switch between test cases.\n\nThis change affects you if you're using one of the following channels in your :\n\nAfter updating to Playwright v1.49, run your test suite. If it still passes, you're good to go. If not, you will probably need to update your snapshots, and adapt some of your test code around PDF viewers and extensions. See issue #33566 for more details.\n• There will be no more updates for WebKit on Ubuntu 20.04 and Debian 11. We recommend updating your OS to a later version.\n• Package will no longer be updated.\n• Package will no longer be updated.\n\nYou can opt into the new headless mode by using channel. As official Chrome documentation puts it:\n\nSee issue #33566 for the list of possible breakages you could encounter and more details on Chromium headless. Please file an issue if you see any problems after opting in.\n• New method tracing.group() to visually group actions in the trace.\n\nThis version was also tested against the following stable channels:\n\nNew methods page.routeWebSocket() and browserContext.routeWebSocket() allow to intercept, modify and mock WebSocket connections initiated in the page. Below is a simple example that mocks WebSocket communication by responding to a with a .\n\nSee WebSocketRoute for more details.\n• New \"copy\" buttons for annotations and test location in the HTML report.\n• Route method calls like route.fulfill() are not shown in the report and trace viewer anymore. You can see which network requests were routed in the network tab instead.\n• New \"Copy as cURL\" and \"Copy as fetch\" buttons for requests in the network tab.\n• Option form and similar ones now accept FormData.\n• New method page.requestGC() may help detect memory leaks.\n• Requests made by APIRequestContext now record detailed timing and security information in the HAR.\n\nThis version was also tested against the following stable channels:\n\nThe Network tab in the UI mode and trace viewer has several nice improvements:\n\nBy default, Playwright will look up the closest tsconfig for each imported file using a heuristic. You can now specify a single tsconfig file in the command line, and Playwright will use it for all imported files, not only test files:\n\nYou can now pass and as query parameters to APIRequestContext:\n• The now serves a Playwright image based on Ubuntu 24.04 Noble. To use the 22.04 jammy-based image, please use instead.\n• New options behavior, behavior and behavior to wait for ongoing listeners to complete.\n• TLS client certificates can now be passed from memory by passing clientCertificates.cert and clientCertificates.key as buffers instead of file paths.\n• Attachments with a content type can now be opened in a new tab in the HTML report. This is useful for including third-party reports or other HTML content in the Playwright test report and distributing it to your team.\n• We've seen reports of WebGL in Webkit misbehaving on GitHub Actions . We recommend upgrading GitHub Actions to .\n\nThis version was also tested against the following stable channels:\n\nPlaywright now allows you to supply client-side certificates, so that server can verify them, as specified by TLS Client Authentication.\n\nThe following snippet sets up a client certificate for :\n\nYou can also provide client certificates to a particular test project or as a parameter of browser.newContext() and apiRequest.newContext().\n\nNew CLI option will only run test files that have been changed since the last git commit or from a specific git \"ref\". This will also run all test files that import any changed files.\n\nThis release introduces an experimental fixture to intercept and handle network requests in component testing. There are two ways to use the router fixture:\n• Call and pass MSW library request handlers to it.\n\nHere is an example of reusing your existing MSW handlers in the test.\n\nThis fixture is only available in component tests.\n• Test annotations are now shown in UI mode.\n• Content of text attachments is now rendered inline in the attachments pane.\n• New setting to show/hide routing actions like route.continue().\n• Request method and status are shown in the network details tab.\n• New button to copy source file location to clipboard.\n• New option in apiRequestContext.fetch() which retries on the network error.\n• New option to box a fixture to minimize the fixture exposure in test reports and error messages.\n• New option to provide a custom fixture title to be used in test reports and error messages.\n\nThis version was also tested against the following stable channels:\n\nUtilizing the new Clock API allows to manipulate and control time within tests to verify time-related behavior. This API covers many common scenarios, including:\n\nSee the clock guide for more details.\n• New CLI option that sets exit code to upon any flaky tests. Note that by default, the test runner exits with code when all failed tests recovered upon a retry. With this option, the test run will fail in such case.\n• New environment variable controls whether built-in , and reporters assume a live terminal. For example, this could be useful to disable tty behavior when your CI environment does not handle ANSI control sequences well. Alternatively, you can enable tty behavior even when to live terminal is present, if you plan to post-process the output and handle control sequences.\n• New options testConfig.respectGitIgnore and testProject.respectGitIgnore control whether files matching patterns are excluded when searching for tests.\n• New property is now available for custom expect matchers. This property takes into account and .\n• Multiple methods like locator.click() or locator.press() now support a modifier key. This key maps to on macOS and maps to on Windows and Linux.\n• New property in apiRequest.newContext() that allows to either always send the header or only send it in response to .\n• New option in apiRequestContext.dispose() that will be included in the error message of ongoing operations interrupted by the context disposal.\n• New option in browserType.launchServer() allows to accept websocket connections on a specific address instead of unspecified .\n• Playwright now supports Chromium, Firefox and WebKit on Ubuntu 24.04.\n• v1.45 is the last release to receive WebKit update for macOS 12 Monterey. Please update macOS to keep using the latest WebKit.\n\nThis version was also tested against the following stable channels:\n• expect(locator).toHaveAccessibleName() checks if the element has the specified accessible name:\n• expect(locator).toHaveAccessibleDescription() checks if the element has the specified accessible description:\n• expect(locator).toHaveRole() checks if the element has the specified ARIA role:\n• After executing the handler added with page.addLocatorHandler(), Playwright will now wait until the overlay that triggered the handler is not visible anymore. You can opt-out of this behavior with the new option.\n• You can use new option in page.addLocatorHandler() to specify maximum number of times the handler should be run.\n• The handler in page.addLocatorHandler() now accepts the locator as argument.\n• option in now accepts and supports repeating fields with the same name.\n• can now be configured by option globally in testConfig.expect or per project in testProject.expect.\n• testProject.ignoreSnapshots allows to configure per project whether to skip screenshot expectations.\n• New method suite.entries() returns child test suites and test cases in their declaration order. suite.type and testCase.type can be used to tell apart test cases and suites in the list.\n• Blob reporter now allows overriding report file path with a single option . The same option can also be specified as environment variable that might be more convenient on CI/CD.\n• CLI option to for running only tests that failed in the previous run. Now fix the failing tests and run Playwright again with option:\n\nThis version was also tested against the following stable channels:\n• Method browserContext.clearCookies() now supports filters to remove only some cookies.\n• New mode for testOptions.trace. In this mode, trace is recorded for the first run of each test, but not for retires. When test run fails, the trace file is retained, otherwise it is removed.\n• New method locator.contentFrame() converts a Locator object to a FrameLocator. This can be useful when you have a Locator object obtained somewhere, and later on would like to interact with the content inside the frame.\n• New method frameLocator.owner() converts a FrameLocator object to a Locator. This can be useful when you have a FrameLocator object obtained somewhere, and later on would like to interact with the element.\n• See tags in the test list.\n• Filter by tags by typing or clicking on the tag itself.\n\nThis version was also tested against the following stable channels:\n• New method page.addLocatorHandler() registers a callback that will be invoked when specified element becomes visible and may block Playwright actions. The callback can get rid of the overlay. Here is an example that closes a cookie dialog when it appears:\n• timeout can now be configured by option globally or in project config\n• New syntax for adding tags to the tests (@-tokens in the test title are still supported):\n\nUse command line option to run only tests with certain tags.\n• page.pdf() accepts two new options and .\n\nThis version was also tested against the following stable channels:\n• New method page.unrouteAll() removes all routes registered by page.route() and page.routeFromHAR(). Optionally allows to wait for ongoing routes to finish, or ignore any errors from them.\n• New method browserContext.unrouteAll() removes all routes registered by browserContext.route() and browserContext.routeFromHAR(). Optionally allows to wait for ongoing routes to finish, or ignore any errors from them.\n• New options style in page.screenshot() and style in locator.screenshot() to add custom CSS to the page before taking a screenshot.\n• New option for methods expect(page).toHaveScreenshot() and expect(locator).toHaveScreenshot() to apply a custom stylesheet while making the screenshot.\n• New option for Blob reporter, to specify the name of the report to be created.\n\nThis version was also tested against the following stable channels:\n\nHere is an example of a generated test with assertions:\n• Options reason in page.close(), reason in browserContext.close() and reason in browser.close(). Close reason is reported for all operations interrupted by the closure.\n• Methods download.path() and download.createReadStream() throw an error for failed and cancelled downloads.\n• Playwright docker image now comes with Node.js v20.\n\nThis version was also tested against the following stable channels:\n\nYou can extend Playwright assertions by providing custom matchers. These matchers will be available on the expect object.\n\nSee the documentation for a full example.\n\nYou can now merge test fixtures from multiple files or modules:\n\nYou can now merge custom expect matchers from multiple files or modules:\n\nYou can mark a test.step() as \"boxed\" so that errors inside it point to the step call site.\n\nSee test.step() documentation for a full example.\n\nThis version was also tested against the following stable channels:\n• The reporter.onEnd() now reports and total run .\n• The following methods were deprecated: page.type(), frame.type(), locator.type() and elementHandle.type(). Please use locator.fill() instead which is much faster. Use locator.pressSequentially() only if there is a special keyboard handling on the page, and you need to press keys one-by-one.\n\nPlaywright recommends to use package and download browsers via command. If you are following this recommendation, nothing has changed for you.\n\nHowever, up to v1.38, installing the package instead of did automatically download browsers. This is no longer the case, and we recommend to explicitly download browsers via command.\n\npackage was downloading browsers during , while was not.\n\nand packages do not download browsers during .\n\nRun to download browsers after . For example, in your CI configuration:\n\nAdd , and as a dependency. These packages download respective browsers during . Make sure you keep the version of all playwright packages in sync:\n\nThis version was also tested against the following stable channels:\n\nIf you run tests on multiple shards, you can now merge all reports in a single HTML report (or any other report) using the new CLI tool.\n\nUsing tool requires the following steps:\n• Adding a new \"blob\" reporter to the config when running on CI: The \"blob\" reporter will produce \".zip\" files that contain all the information about the test run.\n• Copying all \"blob\" reports in a single shared location and running :\n\nRead more in our documentation.\n\nPlaywright now supports Debian 12 Bookworm on both x86_64 and arm64 for Chromium, Firefox and WebKit. Let us know if you encounter any issues!\n\nLinux support looks like this:\n• UI Mode now respects project dependencies. You can control which dependencies to respect by checking/unchecking them in a projects list.\n• Console logs from the test are now displayed in the Console tab.\n\nThis version was also tested against the following stable channels:\n\nThis version was also tested against the following stable channels:\n• UI mode is now available in VSCode Playwright extension via a new \"Show trace viewer\" button:\n• UI mode and trace viewer mark network requests handled with page.route() and browserContext.route() handlers, as well as those issued via the API testing:\n• New option for methods page.screenshot(), locator.screenshot(), expect(page).toHaveScreenshot() and expect(locator).toHaveScreenshot() to change default masking color:\n• Both UI mode and trace viewer now could be opened in a browser tab:\n• binary got renamed from to . So if you use CLI, make sure to update the name: This change does not affect and package users.\n\nThis version was also tested against the following stable channels:\n• New property testProject.teardown to specify a project that needs to run after this and all dependent projects have finished. Teardown is useful to cleanup any resources acquired by this project. A common pattern would be a dependency with a corresponding :\n• New method to create pre-configured expect instance with its own defaults such as and .\n• New options and in testConfig.webServer to configure output handling:\n• New locator.and() to create a locator that matches both locators.\n• New events browserContext.on('console') and browserContext.on('dialog') to subscribe to any dialogs and console messages from any page from the given browser context. Use the new methods consoleMessage.page() and dialog.page() to pin-point event source.\n• no longer works if you install both and . There's no need to install both, since you can always import browser automation APIs from directly:\n• Node.js 14 is no longer supported since it reached its end-of-life on April 30, 2023.\n\nThis version was also tested against the following stable channels:\n• Use locator.or() to create a locator that matches either of the two locators. Consider a scenario where you'd like to click on a \"New email\" button, but sometimes a security settings dialog shows up instead. In this case, you can wait for either a \"New email\" button, or a dialog and act accordingly:\n• Use new options hasNot and hasNotText in locator.filter() to find elements that do not match certain conditions.\n• Use new web-first assertion expect(locator).toBeAttached() to ensure that the element is present in the page's DOM. Do not confuse with the expect(locator).toBeVisible() that ensures that element is both attached & visible.\n• The now serves a Playwright image based on Ubuntu Jammy. To use the focal-based image, please use instead.\n\nThis version was also tested against the following stable channels:\n\nNew UI Mode lets you explore, run and debug tests. Comes with a built-in watch mode.\n• New options updateMode and updateContent in page.routeFromHAR() and browserContext.routeFromHAR().\n• New option name in method tracing.startChunk().\n\nNote: component tests only, does not affect end-to-end tests.\n• If you're running component tests with React 16 or 17, please replace with .\n\nThis version was also tested against the following stable channels:\n• New property testProject.dependencies to configure dependencies between projects. Using dependencies allows global setup to produce traces and other artifacts, see the setup steps in the test report and more.\n• New assertion expect(locator).toBeInViewport() ensures that locator points to an element that intersects viewport, according to the intersection observer API.\n• DOM snapshots in trace viewer can be now opened in a separate window.\n• New method to be used in .\n• Official docker images now include Node 18 instead of Node 16.\n\nNote: component tests only, does not affect end-to-end tests.\n\nThis version was also tested against the following stable channels:\n\nThis version was also tested against the following stable channels:\n• New method route.fetch() and new option for route.fulfill():\n• New method locator.all() to iterate over all matching elements:\n• locator.selectOption() matches now by value or label:\n• Retry blocks of code until all assertions pass: Read more in our documentation.\n• New options and for androidDevice.launchBrowser().\n\nThis version was also tested against the following stable channels:\n• Record at Cursor in VSCode. You can run the test, position the cursor at the end of the test and continue generating the test.\n• Live Locators in VSCode. You can hover and edit locators in VSCode to get them highlighted in the opened browser.\n• Live Locators in CodeGen. Generate a locator for any element on the page using \"Explore\" tool.\n• Codegen and Trace Viewer Dark Theme. Automatically picked up from operating system settings.\n• Configure retries and test timeout for a file or a test with test.describe.configure().\n• Use testProject.snapshotPathTemplate and testConfig.snapshotPathTemplate to configure a template controlling location of snapshots generated by expect(page).toHaveScreenshot() and expect(value).toMatchSnapshot().\n\nThis version was also tested against the following stable channels:\n\nWith these new APIs writing locators is a joy:\n• page.getByRole() to locate by ARIA role, ARIA attributes and accessible name.\n• page.getByLabel() to locate a form control by associated label's text.\n• page.getByTestId() to locate an element based on its attribute (other attribute can be configured).\n• page.getByPlaceholder() to locate an input by placeholder.\n• page.getByAltText() to locate an element, usually image, by its text alternative.\n• page.getByTitle() to locate an element by its title.\n\nAll the same methods are also available on Locator, FrameLocator and Frame classes.\n• option in the now accepts a percentage string to use some of the available CPUs. You can also pass it in the command line:\n• New options and for the html reporter.\n• New field is available to test reporters, specifying the path to the config file if any.\n• As announced in v1.25, Ubuntu 18 will not be supported as of Dec 2022. In addition to that, there will be no WebKit updates on Ubuntu 18 starting from the next Playwright release.\n• expect(locator).toHaveAttribute() with an empty value does not match missing attribute anymore. For example, the following snippet will succeed when does not have a attribute.\n• Command line options and previously incorrectly ignored and options specified in the config. Now all of them are applied together.\n\nThis version was also tested against the following stable channels:\n• New option for apiRequestContext.get() and others to limit redirect count.\n• New command-line flag that allows the test suite to pass when no files are found.\n• New command-line flag to skip snapshot expectations, such as and .\n\nA bunch of Playwright APIs already support the option. For example:\n\nPrior to 1.26, this would wait for all iframes to fire the event.\n\nTo align with web specification, the value only waits for the target frame to fire the event. Use to wait for all iframes.\n\nThis version was also tested against the following stable channels:\n• test.step() now returns the value of the step function:\n• 🪦 This is the last release with macOS 10.15 support (deprecated as of 1.21).\n• 🪦 This is the last release with Node.js 12 support, we recommend upgrading to Node.js LTS (16).\n• ⚠️ Ubuntu 18 is now deprecated and will not be supported as of Dec 2022.\n\nThis version was also tested against the following stable channels:\n\nLaunch multiple web servers, databases, or other processes by passing an array of configurations:\n\nPlaywright now supports Debian 11 Bullseye on x86_64 for Chromium, Firefox and WebKit. Let us know if you encounter any issues!\n\nLinux support looks like this:\n\nIt is now possible to call test.describe() to create suites without a title. This is useful for giving a group of tests a common option with test.use().\n\nPlaywright 1.24 Component Tests introduce and hooks. Use these to configure your app for tests.\n\nFor example, this could be used to setup App router in Vue.js:\n\nA similar configuration in Next.js would look like this:\n\nNow you can record network traffic into a HAR file and re-use this traffic in your tests.\n\nUse the new methods page.routeFromHAR() or browserContext.routeFromHAR() to serve matching responses from the HAR file:\n\nRead more in our documentation.\n\nYou can now use route.fallback() to defer routing to other handlers.\n\nConsider the following example:\n\nNote that the new methods page.routeFromHAR() and browserContext.routeFromHAR() also participate in routing and could be deferred to.\n• New method expect(locator).toHaveValues() that asserts all selected values of element.\n• Support for Vue2 via the package.\n• Support for component tests for create-react-app with components in files.\n\nRead more about component testing with Playwright.\n• If there's a service worker that's in your way, you can now easily disable it with a new context option :\n• Using path for context option automatically zips the resulting HAR:\n• If you intend to edit HAR by hand, consider using the HAR recording mode that only records information that is essential for replaying:\n• Playwright now runs on Ubuntu 22 amd64 and Ubuntu 22 arm64. We also publish new docker image .\n\nWebServer is now considered \"ready\" if request to the specified url has any of the following HTTP status codes:\n• Playwright Test can now test your React, Vue.js or Svelte components. You can use all the features of Playwright Test (such as parallelization, emulation & debugging) while running components in real browsers. Here is what a typical component test looks like: Read more in our documentation.\n• Role selectors that allow selecting elements by their ARIA role, ARIA attributes and accessible name. Read more in our documentation.\n• New locator.filter() API to filter an existing locator\n• New web-first assertions expect(page).toHaveScreenshot() and expect(locator).toHaveScreenshot() that wait for screenshot stabilization and enhances test reliability. The new assertions has screenshot-specific defaults, such as: The new expect(page).toHaveScreenshot() saves screenshots at the same location as expect(value).toMatchSnapshot().\n• New role selectors that allow selecting elements by their ARIA role, ARIA attributes and accessible name. Read more in our documentation.\n• New option in page.screenshot() for smaller sized screenshots.\n• New option in page.screenshot() to control text caret. Defaults to .\n• New method to wait for an arbitrary condition: supports most synchronous matchers, like , , etc. Read more in our documentation.\n• ESM support when running TypeScript tests is now enabled by default. The env variable is no longer required.\n• The docker image no longer contains Python. Please use as a Playwright-ready docker image with pre-installed Python.\n• Playwright now supports large file uploads (100s of MBs) via locator.setInputFiles() API.\n\nThis version was also tested against the following stable channels:\n• New options for methods page.screenshot(), locator.screenshot() and elementHandle.screenshot():\n• Option rewinds all CSS animations and transitions to a consistent state\n• Option masks given elements, overlaying them with pink boxes.\n• now supports anonymous snapshots: when snapshot name is missing, Playwright Test will generate one automatically:\n• New and options for fine-grained screenshot comparison using : It is most convenient to specify or once in testConfig.expect.\n• Playwright Test now adds testConfig.fullyParallel mode. By default, Playwright Test parallelizes between files. In fully parallel mode, tests inside a single file are also run in parallel. You can also use command line flag.\n• testProject.grep and testProject.grepInvert are now configurable per project. For example, you can now configure smoke tests project using :\n• We now ship a designated Python docker image . Please switch over to it if you use Python. This is the last release that includes Python inside our javascript docker image.\n• v1.20 is the last release to receive WebKit update for macOS 10.15 Catalina. Please update macOS to keep using latest & greatest WebKit!\n\nThis version was also tested against the following stable channels:\n• do not terminate test execution, but mark the test as failed. Read more in our documentation\n• You can now specify a custom expect message as a second argument to the and functions, for example: 'should be logged in' \n\n The error would look like this: 'should be logged in' \n\n Read more in our documentation\n• By default, tests in a single file are run in order. If you have many independent tests in a single file, you can now run them in parallel with test.describe.configure().\n• Locator now supports a option that makes sure it contains another locator inside:\n• New option in testConfig.webServer to ensure your web server is ready before running the tests\n• New testInfo.errors and testResult.errors that contain all failed assertions and soft assertions.\n\nIt is unlikely that this change will affect you, no action is required if your tests keep running as they did.\n\nWe've noticed that in rare cases, the set of tests to be executed was configured in the global setup by means of the environment variables. We also noticed some applications that were post processing the reporters' output in the global teardown. If you are doing one of the two, learn more\n\nThis version was also tested against the following stable channels:\n• Each locator can now be optionally filtered by the text it contains:\n• Playwright Test now respects 's and , so you can use aliases\n• There is a new environment variable that allows importing ESM modules in your TS code, without the need for the compile step. Don't forget the suffix when you are importing your esm modules. Run your tests as follows:\n\nThe command is now generally available for your use:\n\nThis will create a Playwright Test configuration file, optionally add examples, a GitHub Action workflow and a first test .\n\nCustom config options are a convenient way to parametrize projects with different values. Learn more in this guide.\n\nPreviously, any fixture introduced through test.extend() could be overridden in the testProject.use config section. For example,\n\nThe proper way to make a fixture parametrized in the config file is to specify when defining the fixture. For example,\n\nThis version was also tested against the following stable channels:\n\nPlaywright 1.17 introduces frame locators - a locator to the iframe on the page. Frame locators capture the logic sufficient to retrieve the and then locate elements in that iframe. Frame locators are strict by default, will wait for to appear and can be used in Web-First assertions.\n\nFrame locators can be created with either page.frameLocator() or locator.frameLocator() method.\n\nRead more at our documentation.\n\nPlaywright Trace Viewer is now available online at https://trace.playwright.dev! Just drag-and-drop your file to inspect its contents.\n• Playwright Test traces now include sources by default (these could be turned off with tracing option)\n• Report is now a single static HTML file that could be sent by e-mail or as a slack attachment.\n• Playwright now supports Ubuntu 20.04 ARM64. You can now run Playwright tests inside Docker on Apple M1 and on Raspberry Pi.\n• You can now use Playwright to install stable version of Edge on Linux:\n\nPlaywright 1.16 introduces new API Testing that lets you send requests to the server directly from Node.js! Now you can:\n• prepare server side state before visiting the web application in a test\n• validate server side post-conditions after running some actions in the browser\n\nTo do a request on behalf of Playwright's Page, use new page.request API:\n\nTo do a stand-alone request from node.js to an API endpoint, use new fixture:\n\nRead more about it in our API testing guide.\n\nIt is now possible to do response interception by combining API Testing with request interception.\n\nFor example, we can blur all the images on the page:\n\nTry it out new HTML reporter with either or a entry in file:\n\nThe HTML reporter has all the information about tests and their failures, including surfacing trace and image artifacts.\n\nRead more about our reporters.\n\nWait for a locator to resolve to a single element with a given state. Defaults to the .\n\nComes especially handy when working with lists:\n\nPlaywright Docker image is now published for Arm64 so it can be used on Apple Silicon.\n• run trace viewer with and drop trace files to the trace viewer PWA\n\nThis version of Playwright was also tested against the following stable channels:\n\nBy using mouse.wheel() you are now able to scroll vertically or horizontally.\n\nPreviously it was not possible to get multiple header values of a response. This is now possible and additional helper functions are available:\n\nIts now possible to emulate the CSS media feature by passing it in the browser.newContext() or calling page.emulateMedia().\n• page.route() accepts new option to specify how many times this route should be matched.\n• page.setChecked() and locator.setChecked() were introduced to set the checked state of a checkbox.\n\nBy default, tests in a single file are run in order. If you have many independent tests in a single file, you can now run them in parallel with test.describe.parallel(title, callback).\n\nBy using it will enable the Playwright Inspector for you to debug your tests.\n\nSelector ambiguity is a common problem in automation testing. \"strict\" mode ensures that your selector points to a single element and throws otherwise.\n\nPass into your action calls to opt in.\n\nLocator represents a view to the element(s) on the page. It captures the logic sufficient to retrieve the element at any given moment.\n\nThe difference between the Locator and ElementHandle is that the latter points to a particular element, while Locator captures the logic of how to retrieve that element.\n\nAlso, locators are \"strict\" by default!\n\nLearn more in the documentation.\n\nReact and Vue selectors allow selecting elements by its component name and/or property values. The syntax is very similar to attribute selectors and supports all attribute selector operators.\n\nLearn more in the react selectors documentation and the vue selectors documentation.\n• selector engine is equivalent to the pseudo class, but could be combined with other selector engines.\n• selector engine is equivalent to the pseudo class, but could be combined with other selector engines.\n\nnow supports lots of new web-first assertions.\n\nConsider the following example:\n\nPlaywright Test will be re-testing the node with the selector until fetched Node has the text. It will be re-fetching the node and checking it over and over, until the condition is met or until the timeout is reached. You can either pass this timeout or configure it once via the value in test config.\n\nBy default, the timeout for assertions is not set, so it'll wait forever, until the whole test times out.\n\nList of all new assertions:\n\nDeclares a group of tests that should always be run serially. If one of the tests fails, all subsequent tests are skipped. All tests in a group are retried together.\n\nLearn more in the documentation.\n\nTo launch a server during the tests, use the option in the configuration file. The server will wait for a given url to be available before running the tests, and the url will be passed over to Playwright as a when creating a context.\n\nLearn more in the documentation.\n• ⚡️ Introducing Reporter API which is already used to create an Allure Playwright reporter.\n• ⛺️ New fixture to support relative paths in tests.\n• 🔎 Enhanced HAR with body sizes for requests and responses. Use via option in browser.newContext().\n• new option in browser.newContext() and browser.newPage()\n• new option in page.fill(), frame.fill(), and elementHandle.fill()\n• new option in page.selectOption(), frame.selectOption(), and elementHandle.selectOption()\n\nPlaywright Test is a new test runner built from scratch by Playwright team specifically to accommodate end-to-end testing needs:\n• Enjoy context isolation and sensible defaults out of the box.\n• Capture videos, screenshots and other artifacts on failure.\n\nPlaywright Trace Viewer is a new GUI tool that helps exploring recorded Playwright traces after the script ran. Playwright traces let you examine:\n• page DOM before and after each Playwright action\n• page rendering before and after each Playwright action\n\nTraces are recorded using the new browserContext.tracing API:\n\nTraces are examined later with the Playwright CLI:\n\nThat will open the following GUI:\n\nThis version of Playwright was also tested against the following stable channels:\n\n🎥 New video: Playwright: A New Test Automation Framework for the Modern Web (slides)\n• Did live demos with new features ✨\n• Special thanks to applitools for hosting the event and inviting us!\n• support for async predicates across the API in methods such as page.waitForRequest() and others\n• new options:\n• option in the browser.newContext() method to emulate dimensions\n• option to dry-run actions in page.check(), page.uncheck(), page.click(), page.dblclick(), page.hover() and page.tap()\n• Playwright for Java v1.10 is now stable!\n• Run Playwright against Google Chrome and Microsoft Edge stable channels with the new channels API.\n\nThis version of Playwright was also tested against the following stable channels:\n• browserType.launch() now accepts the new option. Read more in our documentation.\n• Playwright Inspector is a new GUI tool to author and debug your tests.\n• Line-by-line debugging of your Playwright scripts, with play, pause and step-through.\n• Generate element selectors for your script by hovering over elements.\n• Set the environment variable to launch the Inspector\n• Pause script execution with page.pause() in headed mode. Pausing the page launches Playwright Inspector for debugging.\n• New has-text pseudo-class for CSS selectors. matches any element containing somewhere inside, possibly in a child or a descendant element. See more examples.\n• Page dialogs are now auto-dismissed during execution, unless a listener for event is configured. Learn more about this.\n• Playwright for Python is now stable with an idiomatic snake case API and pre-built Docker image to run tests in CI/CD.\n• Selecting elements based on layout with , , and .\n• page.selectOption() now waits for the options to be present.\n• New methods to assert element state like page.isEditable().\n• New Java SDK: Playwright for Java is now on par with JavaScript, Python and .NET bindings.\n• Browser storage API: New convenience APIs to save and load browser storage state (cookies, local storage) to simplify automation scenarios with authentication.\n• New CSS selectors: We heard your feedback for more flexible selectors and have revamped the selectors implementation. Playwright 1.7 introduces new CSS extensions and there's more coming soon.\n• New website: The docs website at playwright.dev has been updated and is now built with Docusaurus.\n• Support for Apple Silicon: Playwright browser binaries for WebKit and Chromium are now built for Apple Silicon.\n• browserContext.storageState() to get current state for later reuse.\n• option in browser.newContext() and browser.newPage() to setup browser context state."
    },
    {
        "link": "https://stackoverflow.com/questions/72790769/sync-playwright-start-seems-to-hang",
        "document": "And the output I get:\n\nI am a complete noob in using playwright library, and the program just hangs on the 'sync_playwright().start()' part. I've tried running other example scripts from the internet, and all of them froze on this line. Also I've tried changing browsers and using headless mode, all of them didn't work."
    },
    {
        "link": "https://playwright.dev/python/docs/library",
        "document": "These commands download the Playwright package and install browser binaries for Chromium, Firefox and WebKit. To modify this behavior see installation parameters.\n\nOnce installed, you can Playwright in a Python script, and launch any of the 3 browsers ( , and ).\n\nPlaywright supports two variations of the API: synchronous and asynchronous. If your modern project uses asyncio, you should use async API:\n\nIn our first script, we will navigate to and take a screenshot in WebKit.\n\nBy default, Playwright runs the browsers in headless mode. To see the browser UI, set headless option to . You can also use slow_mo to slow down execution. Learn more in the debugging tools section.\n\nYou can launch the interactive python REPL:\n\nand then launch Playwright within it for quick experimentation:\n\nYou can use Playwright with Pyinstaller to create standalone executables.\n\nIf you want to bundle browsers with the executables:\n\nMost likely you don't need to wait manually, since Playwright has auto-waiting. If you still rely on it, you should use instead of and it is better to not wait for a timeout at all, but sometimes it is useful for debugging. In these cases, use our wait ( ) method instead of the module. This is because we internally rely on asynchronous operations and when using they can't get processed correctly.\n\nPlaywright runs the driver in a subprocess, so it requires of on Windows because does not supports async subprocesses.\n\nOn Windows Python 3.7, Playwright sets the default event loop to as it is default on Python 3.8+.\n\nPlaywright's API is not thread-safe. If you are using Playwright in a multi-threaded environment, you should create a playwright instance per thread. See threading issue for more details."
    },
    {
        "link": "https://restack.io/p/playwright-answer-pytest-playwright-latest-version",
        "document": "Discover the latest features and updates in pytest-playwright for seamless browser automation with Playwright."
    },
    {
        "link": "https://docs.python.org/3/library/concurrent.futures.html",
        "document": "The module provides a high-level interface for asynchronously executing callables.\n\nThe asynchronous execution can be performed with threads, using , or separate processes, using . Both implement the same interface, which is defined by the abstract class.\n\nAn abstract class that provides methods to execute calls asynchronously. It should not be used directly, but through its concrete subclasses. Schedules the callable, fn, to be executed as and returns a object representing the execution of the callable.\n• None the iterables are collected immediately rather than lazily;\n• None fn is executed asynchronously and several calls to fn may be made concurrently. The returned iterator raises a if is called and the result isn’t available after timeout seconds from the original call to . timeout can be an int or a float. If timeout is not specified or , there is no limit to the wait time. If a fn call raises an exception, then that exception will be raised when its value is retrieved from the iterator. When using , this method chops iterables into a number of chunks which it submits to the pool as separate tasks. The (approximate) size of these chunks can be specified by setting chunksize to a positive integer. For very long iterables, using a large value for chunksize can significantly improve performance compared to the default size of 1. With , chunksize has no effect. Signal the executor that it should free any resources that it is using when the currently pending futures are done executing. Calls to and made after shutdown will raise . If wait is then this method will not return until all the pending futures are done executing and the resources associated with the executor have been freed. If wait is then this method will return immediately and the resources associated with the executor will be freed when all pending futures are done executing. Regardless of the value of wait, the entire Python program will not exit until all pending futures are done executing. If cancel_futures is , this method will cancel all pending futures that the executor has not started running. Any futures that are completed or running won’t be cancelled, regardless of the value of cancel_futures. If both cancel_futures and wait are , all futures that the executor has started running will be completed prior to this method returning. The remaining futures are cancelled. You can avoid having to call this method explicitly if you use the statement, which will shutdown the (waiting as if were called with wait set to ):\n\nis an subclass that uses a pool of threads to execute calls asynchronously. Deadlocks can occur when the callable associated with a waits on the results of another . For example: # b will never complete because it is waiting on a. # a will never complete because it is waiting on b. # This will never complete because there is only one worker thread and # it is executing this function. An subclass that uses a pool of at most max_workers threads to execute calls asynchronously. All threads enqueued to will be joined before the interpreter can exit. Note that the exit handler which does this is executed before any exit handlers added using . This means exceptions in the main thread must be caught and handled in order to signal threads to exit gracefully. For this reason, it is recommended that not be used for long-running tasks. initializer is an optional callable that is called at the start of each worker thread; initargs is a tuple of arguments passed to the initializer. Should initializer raise an exception, all currently pending jobs will raise a , as well as any attempt to submit more jobs to the pool. Changed in version 3.5: If max_workers is or not given, it will default to the number of processors on the machine, multiplied by , assuming that is often used to overlap I/O instead of CPU work and the number of workers should be higher than the number of workers for . Changed in version 3.6: Added the thread_name_prefix parameter to allow users to control the names for worker threads created by the pool for easier debugging. Changed in version 3.7: Added the initializer and initargs arguments. Changed in version 3.8: Default value of max_workers is changed to . This default value preserves at least 5 workers for I/O bound tasks. It utilizes at most 32 CPU cores for CPU bound tasks which release the GIL. And it avoids using very large resources implicitly on many-core machines. ThreadPoolExecutor now reuses idle worker threads before starting max_workers worker threads too. Changed in version 3.13: Default value of max_workers is changed to . # Retrieve a single page and report the URL and contents # We can use a with statement to ensure threads are cleaned up promptly # Start the load operations and mark each future with its URL\n\nThe class is an subclass that uses a pool of processes to execute calls asynchronously. uses the module, which allows it to side-step the Global Interpreter Lock but also means that only picklable objects can be executed and returned. The module must be importable by worker subprocesses. This means that will not work in the interactive interpreter. Calling or methods from a callable submitted to a will result in deadlock. An subclass that executes calls asynchronously using a pool of at most max_workers processes. If max_workers is or not given, it will default to . If max_workers is less than or equal to , then a will be raised. On Windows, max_workers must be less than or equal to . If it is not then will be raised. If max_workers is , then the default chosen will be at most , even if more processors are available. mp_context can be a context or . It will be used to launch the workers. If mp_context is or not given, the default context is used. See Contexts and start methods. initializer is an optional callable that is called at the start of each worker process; initargs is a tuple of arguments passed to the initializer. Should initializer raise an exception, all currently pending jobs will raise a , as well as any attempt to submit more jobs to the pool. max_tasks_per_child is an optional argument that specifies the maximum number of tasks a single process can execute before it will exit and be replaced with a fresh worker process. By default max_tasks_per_child is which means worker processes will live as long as the pool. When a max is specified, the “spawn” multiprocessing start method will be used by default in absence of a mp_context parameter. This feature is incompatible with the “fork” start method. Changed in version 3.3: When one of the worker processes terminates abruptly, a error is now raised. Previously, behaviour was undefined but operations on the executor or its futures would often freeze or deadlock. Changed in version 3.7: The mp_context argument was added to allow users to control the start_method for worker processes created by the pool. The default start method (see Contexts and start methods) will change away from fork in Python 3.14. Code that requires fork be used for their should explicitly specify that by passing a parameter. Changed in version 3.11: The max_tasks_per_child argument was added to allow users to control the lifetime of workers in the pool. Changed in version 3.12: On POSIX systems, if your application has multiple threads and the context uses the start method: The function called internally to spawn workers may raise a . Pass a mp_context configured to use a different start method. See the documentation for further explanation. Changed in version 3.13: max_workers uses by default, instead of .\n\nThe class encapsulates the asynchronous execution of a callable. instances are created by . Encapsulates the asynchronous execution of a callable. instances are created by and should not be created directly except for testing. Attempt to cancel the call. If the call is currently being executed or finished running and cannot be cancelled then the method will return , otherwise the call will be cancelled and the method will return . Return if the call was successfully cancelled. Return if the call is currently being executed and cannot be cancelled. Return if the call was successfully cancelled or finished running. Return the value returned by the call. If the call hasn’t yet completed then this method will wait up to timeout seconds. If the call hasn’t completed in timeout seconds, then a will be raised. timeout can be an int or float. If timeout is not specified or , there is no limit to the wait time. If the future is cancelled before completing then will be raised. If the call raised an exception, this method will raise the same exception. Return the exception raised by the call. If the call hasn’t yet completed then this method will wait up to timeout seconds. If the call hasn’t completed in timeout seconds, then a will be raised. timeout can be an int or float. If timeout is not specified or , there is no limit to the wait time. If the future is cancelled before completing then will be raised. If the call completed without raising, is returned. Attaches the callable fn to the future. fn will be called, with the future as its only argument, when the future is cancelled or finishes running. Added callables are called in the order that they were added and are always called in a thread belonging to the process that added them. If the callable raises an subclass, it will be logged and ignored. If the callable raises a subclass, the behavior is undefined. If the future has already completed or been cancelled, fn will be called immediately. The following methods are meant for use in unit tests and implementations. This method should only be called by implementations before executing the work associated with the and by unit tests. If the method returns then the was cancelled, i.e. was called and returned . Any threads waiting on the completing (i.e. through or ) will be woken up. If the method returns then the was not cancelled and has been put in the running state, i.e. calls to will return . This method can only be called once and cannot be called after or have been called. Sets the result of the work associated with the to result. This method should only be used by implementations and unit tests. Changed in version 3.8: This method raises if the is already done. Sets the result of the work associated with the to the exception. This method should only be used by implementations and unit tests. Changed in version 3.8: This method raises if the is already done.\n\nWait for the instances (possibly created by different instances) given by fs to complete. Duplicate futures given to fs are removed and will be returned only once. Returns a named 2-tuple of sets. The first set, named , contains the futures that completed (finished or cancelled futures) before the wait completed. The second set, named , contains the futures that did not complete (pending or running futures). timeout can be used to control the maximum number of seconds to wait before returning. timeout can be an int or float. If timeout is not specified or , there is no limit to the wait time. return_when indicates when this function should return. It must be one of the following constants: The function will return when any future finishes or is cancelled. The function will return when any future finishes by raising an exception. If no future raises an exception then it is equivalent to . The function will return when all futures finish or are cancelled. Returns an iterator over the instances (possibly created by different instances) given by fs that yields futures as they complete (finished or cancelled futures). Any futures given by fs that are duplicated will be returned once. Any futures that completed before is called will be yielded first. The returned iterator raises a if is called and the result isn’t available after timeout seconds from the original call to . timeout can be an int or float. If timeout is not specified or , there is no limit to the wait time. The proposal which described this feature for inclusion in the Python standard library.\n\nA deprecated alias of , raised when a future operation exceeds the given timeout. Changed in version 3.11: This class was made an alias of . Derived from , this exception class is raised when an executor is broken for some reason, and cannot be used to submit or execute new tasks. Raised when an operation is performed on a future that is not allowed in the current state. Derived from , this exception class is raised when one of the workers of a has failed initializing. Derived from (formerly ), this exception class is raised when one of the workers of a has terminated in a non-clean fashion (for example, if it was killed from the outside)."
    },
    {
        "link": "https://medium.com/@smrati.katiyar/introduction-to-concurrent-futures-in-python-009fe1d4592c",
        "document": "Concurrency in Python can be efficiently handled using the module. This module provides a high-level interface for asynchronously executing function calls using either threads or processes. It allows you to run tasks in parallel, whether they are I/O-bound or CPU-bound, without worrying too much about the underlying details of thread and process management.\n\nIn this article, we’ll explore how to use , explain the differences between threads and processes, and provide examples to illustrate the concepts.\n\nThe module provides two primary classes for parallel execution:\n• ThreadPoolExecutor: Uses threads to perform tasks. Best suited for I/O-bound operations where tasks spend time waiting (e.g., file I/O or network requests).\n• ProcessPoolExecutor: Uses separate processes to perform tasks. Ideal for CPU-bound tasks, such as computation-heavy operations, since Python’s Global Interpreter Lock (GIL) does not apply to separate processes.\n\nBoth classes work similarly and allow you to submit tasks for concurrent execution, providing a way to execute them in parallel or asynchronously while managing their results.\n• Executor: An executor is an object that manages worker threads or processes.\n• Future: A object represents a result that may not have been computed yet. It allows you to check if the task is complete, retrieve the result, or cancel the task if needed.\n• submit: The method is used to schedule a function for execution and returns a object.\n• map: Similar to Python’s built-in , but tasks are executed concurrently.\n\nLet’s dive into examples to see how each component works.\n\nThe is perfect for I/O-bound tasks like reading files, making network requests, or handling user input concurrently.\n• The function fetches a webpage and returns the size of the page content.\n• Using , we submit the tasks to download each URL concurrently.\n• The function returns each as it completes, allowing us to process results as soon as they are ready.\n\nIn this example, tasks that could take time to wait for network responses are handled in separate threads, speeding up the overall execution time.\n\nFor CPU-intensive tasks like mathematical computations, is more suitable. Python’s GIL (Global Interpreter Lock) limits true parallelism in threads, but since processes have separate memory spaces, multiple processes can run in parallel on different CPU cores.\n• The function performs a CPU-bound operation of calculating the factorial of a number.\n• Using , we submit each factorial computation task to be executed in a separate process.\n• The function is used to retrieve results as they become available.\n\nThis example demonstrates how CPU-bound tasks can be handled in parallel across multiple CPU cores, improving performance.\n\nBoth and provide a convenient method called to apply a function to multiple inputs concurrently. It works similarly to Python's built-in , but with concurrent execution.\n\nExample: Using with ThreadPoolExecutor\n\nHere, applies the function to each element in concurrently. The function returns results in the same order as the input data, making it a convenient choice for cases where task order matters.\n\nWhen working with concurrent tasks, some tasks may fail or raise exceptions. allows you to handle these exceptions gracefully.\n\nIn this example, the function raises an exception for input . The method either returns the result of the task or raises an exception if the task failed.\n\nThe module in Python offers a powerful and simple interface for writing concurrent and parallel code. Whether you're dealing with I/O-bound tasks using or CPU-bound tasks with , this module provides a high-level way to manage tasks and retrieve results asynchronously.\n\nBy combining concepts like , , and , you can easily implement concurrency into your Python programs and achieve significant performance improvements for both I/O-bound and CPU-bound operations."
    },
    {
        "link": "https://realpython.com/python-concurrency",
        "document": "Concurrency refers to the ability of a program to manage multiple tasks at once, improving performance and responsiveness. It encompasses different models like threading, asynchronous tasks, and multiprocessing, each offering unique benefits and trade-offs. In Python, threads and asynchronous tasks facilitate concurrency on a single processor, while multiprocessing allows for true parallelism by utilizing multiple CPU cores.\n\nUnderstanding concurrency is crucial for optimizing programs, especially those that are I/O-bound or CPU-bound. Efficient concurrency management can significantly enhance a program’s performance by reducing wait times and better utilizing system resources.\n\nIn this tutorial, you’ll learn how to:\n• Understand the different forms of concurrency in Python\n• Choose the appropriate concurrency model based on your program’s needs\n\nTo get the most out of this tutorial, you should be familiar with Python basics, including functions and loops. A rudimentary understanding of system processes and CPU operations will also be helpful. You can download the sample code for this tutorial by clicking the link below:\n\nIn this section, you’ll get familiar with the terminology surrounding concurrency. You’ll also learn that concurrency can take different forms depending on the problem it aims to solve. Finally, you’ll discover how the different concurrency models translate to Python. The dictionary definition of concurrency is simultaneous occurrence. In Python, the things that are occurring simultaneously are called by different names, including these: At a high level, they all refer to a sequence of instructions that run in order. You can think of them as different trains of thought. Each one can be stopped at certain points, and the CPU or brain that’s processing them can switch to a different one. The state of each train of thought is saved so it can be restored right where it was interrupted. You might wonder why Python uses different words for the same concept. It turns out that threads, tasks, and processes are only the same if you view them from a high-level perspective. Once you start digging into the details, you’ll find that they all represent slightly different things. You’ll see more of how they’re different as you progress through the examples. Now, you’ll consider the simultaneous part of that definition. You have to be a little careful because, when you get down to the details, you’ll discover that only multiple system processes can enable Python to run these trains of thought at literally the same time. In contrast, threads and asynchronous tasks always run on a single processor, which means they can only run one at a time. They just cleverly find ways to take turns to speed up the overall process. Even though they don’t run different trains of thought simultaneously, they still fall under the concept of concurrency. Note: Threads in most other programming languages often run in parallel. To learn why Python threads can’t, check out What Is the Python Global Interpreter Lock (GIL)? If you’re curious about even more details, then you can also read about Bypassing the GIL for Parallel Processing in Python or check out the experimental free threading introduced in Python 3.13. The way the threads, tasks, or processes take turns differs. In a multi-threaded approach, the operating system actually knows about each thread and can interrupt it at any time to start running a different thread. This mechanism is also true for processes. It’s called preemptive multitasking since the operating system can preempt your thread or process to make the switch. Preemptive multitasking is handy in that the code in the thread doesn’t need to do anything special to make the switch. It can also be difficult because of that at any time phrase. The context switch can happen in the middle of a single Python statement, even a trivial one like . This is because Python statements typically consist of several low-level bytecode instructions. On the other hand, asynchronous tasks use cooperative multitasking. The tasks must cooperate with each other by announcing when they’re ready to be switched out without the operating system’s involvement. This means that the code in the task has to change slightly to make it happen. The benefit of doing this extra work upfront is that you always know where your task will be swapped out, making it easier to reason about the flow of execution. A task won’t be swapped out in the middle of a Python statement unless that statement is appropriately marked. You’ll see later how this can simplify parts of your design. So far, you’ve looked at concurrency that happens on a single processor. What about all of those CPU cores your cool, new laptop has? How can you make use of them in Python? The answer is to execute separate processes! A process can be thought of as almost a completely different program, though technically, it’s usually defined as a collection of resources including memory, file handles, and things like that. One way to think about it is that each process runs in its own Python interpreter. Because they’re different processes, each of your trains of thought in a program leveraging multiprocessing can run on a different CPU core. Running on a different core means that they can actually run at the same time, which is fabulous. There are some complications that arise from doing this, but Python does a pretty good job of smoothing them over most of the time. Now that you have an idea of what concurrency and parallelism are, you can review their differences and then determine which Python modules support them: The tasks decide when to give up control. The operating system decides when to switch tasks external to Python. The processes all run at the same time on different processors. You’ll explore these modules as you make your way through the tutorial. Note: Both and represent fairly low-level building blocks in concurrent programs. In practice, you can often replace them with , which provides a higher-level interface for both modules. On the other hand, offers a bit of a different approach to concurrency, which you’ll dive into later. Each of the corresponding types of concurrency can be useful in its own way. You’ll now take a look at what types of programs they can help you speed up. When Is Concurrency Useful? Concurrency can make a big difference for two types of problems: I/O-bound problems cause your program to slow down because it frequently must wait for input or output (I/O) from some external resource. They arise when your program is working with things that are much slower than your CPU. Examples of things that are slower than your CPU are legion, but your program thankfully doesn’t interact with most of them. The slow things your program will interact with the most are the file system and network connections. The blue boxes show the time when your program is doing work, and the red boxes are time spent waiting for an I/O operation to complete. This diagram is not to scale because requests on the internet can take several orders of magnitude longer than CPU instructions, so your program can end up spending most of its time waiting. That’s what your web browser is doing most of the time. On the flip side, there are classes of programs that do significant computation without talking to the network or accessing a file. These are CPU-bound programs because the resource limiting the speed of your program is the CPU, not the network or the file system. As you work through the examples in the following section, you’ll see that different forms of concurrency work better or worse with I/O-bound and CPU-bound programs. Adding concurrency to your program introduces extra code and complications, so you’ll need to decide if the potential speedup is worth the additional effort. By the end of this tutorial, you should have enough information to start making that decision. Your program spends most of its time talking to a slow device, like a network adapter, a hard drive, or a printer. Your program spends most of its time doing CPU operations. Speeding it up involves overlapping the times spent waiting for these devices. Speeding it up involves finding ways to do more computations in the same amount of time. You’ll look at I/O-bound programs first. Then, you’ll get to see some code dealing with CPU-bound programs.\n\nIn this section, you’ll focus on I/O-bound programs and a common problem: downloading content over the network. For this example, you’ll be downloading web pages from a few sites, but it really could be any network traffic. It’s just more convenient to visualize and set up with web pages. You’ll start with a non-concurrent version of this task. Note that this program requires the third-party Requests library. So, you should first run the following command in an activated virtual environment: This version of your program doesn’t use concurrency at all: As you can see, this is a fairly short program. It just downloads the site contents from a list of addresses and prints their sizes. One small thing to point out is that you’re using a session object from . It’s possible to call directly, but creating a object allows the library to retain state across requests and reuse the connection to speed things up. You create the session in and then walk through the list of sites, downloading each one in turn. Finally, you print out how long this process took so you can have the satisfaction of seeing how much concurrency has helped you in the following examples. The processing diagram for this program will look much like the I/O-bound diagram in the last section. Note: Network traffic is dependent on many factors that can vary from second to second. You may see the times of these tests double from one run to another due to network issues. The great thing about this version of code is that, well, it’s simple. It was comparatively quick to write and debug. It’s also more straightforward to think about. There’s only one train of thought running through it, so you can predict what the next step is and how it’ll behave. The big problem here is that it’s relatively slow compared to the other solutions that you’re about to see. Here’s an example of what the final output might look like: Note that these results may vary significantly depending on the speed of your internet connection, network congestion, and other factors. To account for them, you should repeat each benchmark a few times and take the fastest of the runs. That way, the differences between your program’s versions will still be clear. Being slower isn’t always a big issue. If the program you’re running takes only two seconds with a synchronous version and is only run rarely, then it’s probably not worth adding concurrency. You can stop here. What if your program is run frequently? What if it takes hours to run? You’ll move on to concurrency by rewriting this program using Python threads. As you probably guessed, writing a program leveraging multithreading takes more effort. However, you might be surprised at how little extra effort it takes for basic cases. Here’s what the same program looks like when you take advantage of the and modules mentioned earlier: The overall structure of your program is the same, but the highlighted lines indicate the changes you needed to make. On line 20, you created an instance of the to manage the threads for you. In this case, you explicitly requested five workers or threads. Note: How do you pick the number of threads in your pool? The difficult answer here is that the correct number of threads is not a constant from one task to another. In general, with IO-bound problems, you’re not limited to the number of CPU cores. In fact, it’s not uncommon to create hundreds or even thousands of threads as long as they wait for data instead of doing real work. But, at some point, you’ll eventually start experiencing diminishing returns due to the extra overhead of switching threads. Some experimentation is always recommended. Feel free to play around with this number to see how it affects the overall execution time. Creating a seems like a complicated thing. But, when you break it down, you’ll end up with these three components: You already know about the thread part. That’s just the train of thought mentioned earlier. The pool portion is where it starts to get interesting. This object is going to create a pool of threads, each of which can run concurrently. Finally, the executor is the part that’s going to control how and when each of the threads in the pool will run. It’ll execute the request in the pool. Note: Using a thread pool can be beneficial when you have limited system resources but still want to handle many tasks. By creating the threads upfront and reusing them for the subsequent tasks, a pool reduces the overhead of repeatedly creating and destroying threads. The standard library implements as a context manager, so you can use the syntax to manage creating and freeing the pool of instances. In this multi-threaded version of the program, you let the executor call on your behalf instead of doing it manually in a loop. The method on line 21 takes care of distributing the workload across the available threads, allowing each one to handle a different site concurrently. This method takes two arguments:\n• A function to be executed on each data item, like a site address\n• A collection of data items to be processed by that function Since the function that you passed to the executor’s method must take exactly one argument, you modified on line 23 to only accept a URL. But how do you obtain the session object now? This is one of the interesting and difficult issues with threading. Because the operating system controls when your task gets interrupted and another task starts, any data shared between the threads needs to be protected or thread-safe to avoid unexpected behavior or potential data corruption. Unfortunately, isn’t thread-safe, meaning that one thread may interfere with the session while another thread is still using it. There are several strategies for making data access thread-safe. One of them is to use a thread-safe data structure, such as a , , or an . These objects use low-level primitives like lock objects to ensure that only one thread can access a block of code or a bit of memory at the same time. You’re using this strategy indirectly by way of the object. Another strategy to use here is something called thread-local storage. When you call on line 7, you create an object that resembles a global variable but is specific to each individual thread. It looks a little odd, but you only want to create one of these objects, not one for each thread. The object itself takes care of separating accesses from different threads to its attributes. When is called, the session it looks up is specific to the particular thread on which it’s running. So each thread will create a single session the first time it calls and then will use that session on each subsequent call throughout its lifetime. Okay. It’s time to put your multi-threaded program to the ultimate test: It’s fast! Remember that the non-concurrent version took more than fourteen seconds in the best case. Here’s what its execution timing diagram looks like: The program uses multiple threads to have many open requests out to web sites at the same time. This allows your program to overlap the waiting times and get the final result faster. Yippee! That was the goal. Are there any problems with the multi-threaded version? Well, as you can see from the example, it takes a little more code to make this happen, and you really have to give some thought to what data is shared between threads. Threads can interact in ways that are subtle and hard to detect. These interactions can cause race conditions that frequently result in random, intermittent bugs that can be quite difficult to find. If you’re unfamiliar with this concept, then you might want to check out a section on race conditions in another tutorial on thread safety. Running threads concurrently allowed you to cut down the total execution time of your original synchronous code by an order of magnitude. That’s already pretty remarkable, but you can do even better than that by taking advantage of Python’s module, which enables asynchronous I/O. Asynchronous processing is a concurrency model that’s well-suited for I/O-bound tasks—hence the name, . It avoids the overhead of context switching between threads by employing the event loop, non-blocking operations, and coroutines, among other things. Perhaps somewhat surprisingly, the asynchronous code needs only one thread of execution to run concurrently. Note: If these concepts sound unfamiliar to you, or you need a quick refresher, then check out Getting Started With Async Features in Python and Async IO in Python: A Complete Walkthrough to learn more. In a nutshell, the event loop controls how and when each asynchronous task gets to execute. As the name suggests, it continuously loops through your tasks while monitoring their state. As soon as the current task starts waiting for an I/O operation to finish, the loop suspends it and immediately switches to another task. Conversely, once the expected event occurs, the loop will eventually resume the suspended task in the next iteration. A coroutine is similar to a thread but much more lightweight and cheaper to suspend or resume. That’s what makes it possible to spawn many more coroutines than threads without a significant memory or performance overhead. This capability helps address the C10k problem, which involves handling ten thousand concurrent connections efficiently. But there’s a catch. You can’t have blocking function calls in your coroutines if you want to reap the full benefits of asynchronous programming. A blocking call is a synchronous one, meaning that it prevents other code from running while it’s waiting for data to arrive. In contrast, a non-blocking call can voluntarily give up control and wait to be notified when the data is ready. In Python, you create a coroutine object by calling an asynchronous function, also known as a coroutine function. Those are defined with the statement instead of the usual . Only within the body of an asynchronous function are you allowed to use the keyword, which pauses the execution of the coroutine until the awaited task is completed: In this case, you defined as an asynchronous function that implicitly returns a coroutine object when called. Thanks to the keyword, your coroutine makes a non-blocking call to , simulating a delay of three and a half seconds. While your function awaits the wake-up event, other tasks could potentially run concurrently. Note: To run the sample code above, you’ll need to either wrap the call to in or await in Python’s asyncio REPL. Now that you’ve got a basic understanding of what asynchronous I/O is, you can walk through the asynchronous version of the example code and figure out how it works. However, because the Requests library that you’ve been using in this tutorial is blocking, you must now switch to a non-blocking counterpart, such as , which was designed for Python’s : After installing this library in your virtual environment, you can use it in the asynchronous version of the code: This version looks strikingly similar to the synchronous one, which is yet another advantage of . It’s a double-edged sword, though. While it arguably makes your concurrent code easier to reason about than the multi-threaded version, is far from easy when you get into more complex scenarios. Here are the most important differences when compared to the non-concurrent version:\n• Line 1 imports from Python’s standard library. This is necessary to run your asynchronous function on line 26.\n• Line 4 imports the third-party library, which you’ve installed into the virtual environment. This library replaces Requests from earlier examples.\n• Lines 6, 16, and 21 redefine your regular functions as asynchronous ones by qualifying their signatures with the keyword.\n• Line 12 prepends the keyword to so that the returned coroutine object can be awaited. This effectively suspends your function until all sites have been downloaded.\n• Lines 17 and 22 leverage the statement to create asynchronous context managers for the session object and the response, respectively.\n• Line 18 creates a list of tasks using a list comprehension, where each task is a coroutine object returned by . Notice that you don’t await the individual coroutine objects, as doing so would lead to executing them sequentially.\n• Line 19 uses to run all the tasks concurrently, allowing for efficient downloading of multiple sites at the same time.\n• Line 23 awaits the completion of the session’s HTTP GET request before printing the number of bytes read. You can share the session across all tasks, so the session is created here as a context manager. The tasks can share the session because they’re all running on the same thread. There’s no way one task could interrupt another while the session is in a bad state. There’s one small but important change buried in the details here. Remember the mention about the optimal number of threads to create? It wasn’t obvious in the multi-threaded example what the optimal number of threads was. One of the cool advantages of is that it scales far better than or . Each task takes far fewer resources and less time to create than a thread, so creating and running more of them works well. This example just creates a separate task for each site to download, which works out quite well. And, it’s really fast. The asynchronous version is the fastest of them all by a good margin: It took less than a half a second to complete, making this code seven times quicker than the multi-threaded version and over thirty times faster than the non-concurrent version! Note: In the synchronous version, you cycled through a list of sites and kept downloading their content in a deterministic order. With the multi-threaded version, you ceded control over task scheduling to the operating system, so the final order seemed random. While the asynchronous version may show some clustering of completions, it’s generally non-deterministic due to changing network conditions. The execution timing diagram looks quite similar to what’s happening in the multi-threaded example. It’s just that the I/O requests are all done by the same thread: There’s a common argument that having to add and in the proper locations is an extra complication. To a small extent, that’s true. The flip side of this argument is that it forces you to think about when a given task will get swapped out, which can help you create a better design. The scaling issue also looms large here. Running the multi-threaded example with a thread for each site is noticeably slower than running it with a handful of threads. Running the example with hundreds of tasks doesn’t slow it down at all. There are a couple of issues with at this point. You need special asynchronous versions of libraries to gain the full advantage of . Had you just used Requests for downloading the sites, it would’ve been much slower because Requests isn’t designed to notify the event loop that it’s blocked. This issue is becoming less significant as time goes on and more libraries embrace . Another more subtle issue is that all the advantages of cooperative multitasking get thrown away if one of the tasks doesn’t cooperate. A minor mistake in code can cause a task to run off and hold the processor for a long time, starving other tasks that need running. There’s no way for the event loop to break in if a task doesn’t hand control back to it. With that in mind, you can step up to a radically different approach to concurrency using multiple processes. Up to this point, all of the examples of concurrency in this tutorial ran only on a single CPU or core in your computer. The reasons for this have to do with the current design of CPython and something called the Global Interpreter Lock, or GIL. This tutorial won’t dive into the hows and whys of the GIL. It’s enough for now to know that the synchronous, multi-threaded, and asynchronous versions of this example all run on a single CPU. The module, along with the corresponding wrappers in , was designed to break down that barrier and run your code across multiple CPUs. At a high level, it does this by creating a new instance of the Python interpreter to run on each CPU and then farming out part of your program to run on it. As you can imagine, bringing up a separate Python interpreter is not as fast as starting a new thread in the current Python interpreter. It’s a heavyweight operation and comes with some restrictions and difficulties, but for the correct problem, it can make a huge difference. Unlike the previous approaches, using multiprocessing allows you to take full advantage of the all CPUs that your cool, new computer has. Here’s the sample code: This actually looks quite similar to the multi-threaded example, as you leverage the familiar abstraction instead of relying on directly. Go ahead and take a quick tour of what this code does for you:\n• Line 8 uses type hints to declare a global variable that will hold the session object. Note that this doesn’t actually define the value of the variable.\n• Line 21 replaces with from and passes , which is defined further down.\n• Lines 29 to 32 define a custom initializer function that each process will call shortly after starting. It ensures that each process initializes its own session.\n• Line 32 registers a cleanup function with , which ensures that the session is properly closed when the process stops. This helps prevent potential memory leaks. What happens here is that the pool creates a number of separate Python interpreter processes and has each one run the specified function on some of the items in the iterable, which in your case is the list of sites. The communication between the main process and the other processes is handled for you. The line that creates a pool instance is worth your attention. First off, it doesn’t specify how many processes to create in the pool, although that’s an optional parameter. By default, it’ll determine the number of CPUs in your computer and match that. This is frequently the best answer, and it is in your case. For an I/O-bound problem, increasing the number of processes won’t make things faster. It’ll actually slow things down because the cost of setting up and tearing down all those processes is larger than the benefit of doing the I/O requests in parallel. Note: If you need to exchange data between your processes, then it’ll require expensive inter-process communication (IPC) and data serialization, which increases the overall cost even further. Besides this, serialization isn’t always possible because Python uses the module under the surface, which supports only a few data types. Next, you have the initializer part of that call. Remember that each process in our pool has its own memory space. That means they can’t easily share things like a session object. You don’t want to create a new instance each time the function is called—you want to create one for each process. The function parameter is built for just this case. There’s no way to pass a return value back from the to , but you can initialize a global variable to hold the single session for each process. Because each process has its own memory space, the global for each one will be different. That’s really all there is to it. The rest of the code is quite similar to what you’ve seen before. The process-based version does require some extra setup, and the global session object is strange. You have to spend some time thinking about which variables will be accessed in each process. While this version takes full advantage of the CPU power in your computer, the resulting performance is surprisingly underwhelming: On a computer equipped with four CPU cores, it runs about four times faster than the synchronous version. Still, it’s a bit slower than the multi-threaded version and much slower than the asynchronous version. The execution timing diagram for this code looks like this: There are a few separate processes executing in parallel. The corresponding diagrams of each one of them resemble the non-concurrent version you saw at the beginning of this tutorial. I/O-bound problems aren’t really why multiprocessing exists. You’ll see more as you step into the next section and look at CPU-bound examples.\n\nIt’s time to shift gears here a little bit. The examples so far have all dealt with an I/O-bound problem. Now, you’ll look into a CPU-bound problem. As you learned earlier, an I/O-bound problem spends most of its time waiting for external operations to complete, such as network calls. In contrast, a CPU-bound problem performs fewer I/O operations, and its total execution time depends on how quickly it can process the required data. For the purposes of this example, you’ll use a somewhat silly function to create a piece of code that takes a long time to run on the CPU. This function computes the n-th Fibonacci number using the recursive approach: Notice how quickly the resulting values grow as the function computes higher Fibonacci numbers. The recursive nature of this implementation leads to many repeated calculations of the same numbers, which requires substantial processing time. That’s what makes this such a convenient example of a CPU-bound task. Remember, this is just a placeholder for your code that actually does something useful and requires lengthy processing, like computing the roots of equations or sorting a large data structure. First off, you can look at the non-concurrent version of the example: This code calls twenty times in a loop. Due to the recursive nature of its implementation, the function calls itself hundreds of millions of times! It does all of this on a single thread in a single process on a single CPU. The execution timing diagram looks like this: Unlike the I/O-bound examples, the CPU-bound examples are usually fairly consistent in their run times. This one takes about thirty-five seconds on the same machine as before: Clearly, you can do better than this. After all, it’s all running on a single CPU with no concurrency. Next, you’ll see what you can do to improve it. How much do you think rewriting this code using threads—or asynchronous tasks—will speed this up? If you answered “Not at all,” then give yourself a cookie. If you answered, “It will slow it down,” then give yourself two cookies. Here’s why: In your earlier I/O-bound example, much of the overall time was spent waiting for slow operations to finish. Threads and asynchronous tasks sped this up by allowing you to overlap the waiting times instead of performing them sequentially. With a CPU-bound problem, there’s no waiting. The CPU is cranking away as fast as it can to finish the problem. In Python, both threads and asynchronous tasks run on the same CPU in the same process. This means that the one CPU is doing all of the work of the non-concurrent code plus the extra work of setting up threads or tasks. Here’s the code of the multi-threaded version of your CPU-bound problem: Little of this code had to change from the non-concurrent version. After importing , you just changed from looping through the numbers to creating a thread pool and using its method to send individual numbers to worker threads as they become free. This was just what you did for the I/O-bound multi-threaded code, but here, you didn’t need to worry about the object. Below is the output you might see when running this code: Unsurprisingly, it takes a few seconds longer than the synchronous version. Okay. At this point, you should know what to expect from the asynchronous version of a CPU-bound problem. But for completeness, you’ll now test how it stacks up against the others. Implementing the asynchronous version of this CPU-bound problem involves rewriting your functions into coroutine functions with and awaiting their return values: You create twenty tasks and pass them to to let the corresponding coroutines run concurrently. However, they actually run in sequence, as each blocks execution until the previous one is finished. When run, this code takes over twice as long to execute as your original synchronous version and also takes longer than the multi-threaded version: Ironically, the asynchronous approach is the slowest for a CPU-bound problem, yet it was the fastest for an I/O-bound one. Because there are no I/O operations involved here, there’s nothing to wait for. The overhead of the event loop and context switching at every single statement slows down the total execution substantially. In Python, to improve the performance of a CPU-bound task like this one, you must use an alternative concurrency model. You’ll take a closer look at that now. You’ve finally reached the part where multiprocessing really shines. Unlike the other concurrency models, process-based parallelism is explicitly designed to share heavy CPU workloads across multiple CPUs. Here’s what the corresponding code looks like: It’s almost identical to the multi-threaded version of the Fibonacci problem. You literally changed just two lines of code! Instead of using , you replaced it with . As mentioned before, the optional parameter to the pool’s constructor deserves some attention. You can use it to specify how many processes you want to be created and managed in the pool. By default, it’ll determine how many CPUs are in your machine and create a process for each one. While this works great for your simple example, you might want to have a little more control in a production environment. This version takes about ten seconds, which is less than one-third of the non-concurrent implementation you started with: This is much better than what you saw with the other options, making it by far the best choice for this kind of task. Here’s what the execution timing diagram looks like: The individual tasks run alongside each other on separate CPU cores, making parallel execution possible. There are some drawbacks to using multiprocessing that don’t really show up in a simple example like this one. For example, dividing your problem into segments so each processor can operate independently can sometimes be difficult. Also, many solutions require more communication between the processes. This can add some complexity to your solution that a non-concurrent program just wouldn’t need to deal with."
    },
    {
        "link": "https://superfastpython.com/processpoolexecutor-in-python",
        "document": "The Python ProcessPoolExecutor provides reusable worker processes in Python.\n\nThe ProcessPoolExecutor class is part of the Python standard library. It offers easy-to-use pools of child worker processes via the modern executor design pattern. It is ideal for parallelizing loops of CPU-bound tasks and for issuing tasks asynchronously.\n\nThis book-length guide provides a detailed and comprehensive walkthrough of the Python ProcessPoolExecutor API.\n• You may want to bookmark this guide and read it over a few sittings.\n• You can download a zip of all code used in this guide.\n• You can get help, ask a question in the comments or email me.\n• You can jump to the topics that interest you via the table of contents (below).\n\nPython Processes and the Need for Process Pools\n\nSo, what are processes and why do we care about process pools?\n\nEvery Python program is a process and has one thread called the main thread used to execute your program instructions. Each process is, in fact, one instance of the Python interpreter that executes Python instructions (Python byte-code), which is a slightly lower level than the code you type into your Python program.\n\nSometimes we may need to create new processes to run additional tasks concurrently.\n\nPython provides real system-level processes via the Process class in the multiprocessing module.\n\nYou can learn more about multiprocessing in the tutorial:\n\nThe underlying operating system controls how new processes are created. On some systems, that may require spawning a new process, and on others, it may require that the process is forked. The operating-specific method used for creating new processes in Python is not something we need to worry about as it is managed by your installed Python interpreter.\n\nA task can be run in a new process by creating an instance of the Process class and specifying the function to run in the new process via the “target” argument.\n\nOnce the process is created, it must be started by calling the start() function.\n\nWe can then wait around for the task to complete by joining the process; for example:\n\nWhenever we create new processes, we must protect the entry point of the program.\n\nTying this together, the complete example of creating a Process to run an ad hoc task function is listed below.\n\nThis is useful for running one-off ad hoc tasks in a separate process, although it becomes cumbersome when you have many tasks to run.\n\nEach process that is created requires the application of resources (e.g. an instance of the Python interpreter and a memory for the process’s main thread’s stack space). The computational costs for setting up processes can become expensive if we are creating and destroying many processes over and over for ad hoc tasks.\n\nInstead, we would prefer to keep worker processes around for reuse if we expect to run many ad hoc tasks throughout our program.\n\nThis can be achieved using a process pool.\n\nA process pool is a programming pattern for automatically managing a pool of worker processes.\n\nThe pool is responsible for a fixed number of processes.\n• It controls when they are created, such as when they are needed.\n• It also controls what they should do when they are not being used, such as making them wait without consuming computational resources.\n\nThe pool can provide a generic interface for executing ad hoc tasks with a variable number of arguments, much like the target property on the Process object, but does not require that we choose a process to run the task, start the process, or wait for the task to complete.\n\nPython provides a process pool via the ProcessPoolExecutor class.\n\nThe ProcessPoolExecutor Python class is used to create and manage process pools and is provided in the concurrent.futures module.\n\nThe concurrent.futures module was introduced in Python 3.2 written by Brian Quinlan and provides both thread pools and process pools, although we will focus our attention on process pools in this guide.\n\nIf you’re interested, you can access the Python source code for the ProcessPoolExecutor class directly via process.py. It may be interesting to dig into how the class works internally, perhaps after you are familiar with how it works from the outside.\n\nThe ProcessPoolExecutor extends the Executor class and will return Future objects when it is called.\n• Executor: Parent class for the ProcessPoolExecutor that defines basic life-cycle operations for the pool.\n• Future: Object returned when submitting tasks to the process pool that may complete later.\n\nLet’s take a closer look at Executors, Futures, and the life-cycle of using the ProcessPoolExecutor class.\n\nThe Executor class defines three methods used to control our process pool; they are: submit(), map(), and shutdown().\n• submit(): Dispatch a function to be executed and return a future object.\n• map(): Apply a function to an iterable of elements.\n\nThe Executor is started when the class is created and must be shut down explicitly by calling shutdown(), which will release any resources held by the Executor. We can also shut down automatically, but we will look at that a little later.\n\nThe submit() and map() functions are used to submit tasks to the Executor for asynchronous execution.\n\nThe map() function operates just like the built-in map() function and is used to apply a function to each element in an iterable object, like a list. Unlike the built-in map() function, each application of the function to an element will happen asynchronously instead of sequentially.\n\nThe submit() function takes a function as well as any arguments and will execute it asynchronously, although the call returns immediately and provides a Future object.\n\nWe will take a closer look at each of these three functions in a moment. Firstly, what is a Future?\n\nA future is an object that represents a delayed result for an asynchronous task.\n\nIt is also sometimes called a promise or a delay. It provides a context for the result of a task that may or may not be executing and a way of getting a result once it is available.\n\nIn Python, the Future object is returned from an Executor, such as a ProcessPoolExecutor, when calling the submit() function to dispatch a task to be executed asynchronously.\n\nIn general, we do not create Future objects; we only receive them and we may need to call functions on them.\n\nThere is always one Future object for each task sent into the ProcessPoolExecutor via a call to submit().\n\nThe Future object provides a number of helpful functions for inspecting the status of the task such as: cancelled(), running(), and done() to determine if the task was cancelled, is currently running, or has finished execution.\n• cancelled(): Returns True if the task was cancelled before being executed.\n• running(): Returns True if the task is currently running.\n• done(): Returns True if the task has completed or was cancelled.\n\nA running task cannot be cancelled and a done task could have been cancelled.\n\nA Future object also provides access to the result of the task via the result() function. If an exception was raised while executing the task, it will be re-raised when calling the result() function, or can be accessed via the exception() function.\n• result(): Access the result from running the task.\n• exception(): Access any exception raised while running the task.\n\nBoth the result() and exception() functions allow a timeout to be specified as an argument, which is the number of seconds to wait for a return value if the task is not yet complete. If the timeout expires, then a TimeoutError will be raised.\n\nFinally, we may want to have the process pool automatically call a function once the task is completed.\n\nThis can be achieved by attaching a callback to the Future object for the task via the add_done_callback() function.\n• add_done_callback(): Add a callback function to the task to be executed by the process pool once the task is completed.\n\nWe can add more than one callback to each task, and they will be executed in the order they were added. If the task has already completed before we add the callback, then the callback is executed immediately.\n\nAny exceptions raised in the callback function will not impact the task or process pool.\n\nWe will take a closer look at the Future object in a later section.\n\nNow that we are familiar with the functionality of a ProcessPoolExecutor provided by the Executor class and of Future objects returned by calling submit(), let’s take a closer look at the life-cycle of the ProcessPoolExecutor class.\n\nThe ProcessPoolExecutor provides a pool of generic worker processes.\n\nThe ProcessPoolExecutor was designed to be easy and straightforward to use.\n\nIf multiprocessing was like the transmission for changing gears in a car, then using multiprocessing.Process is a manual transmission (e.g. hard to learn and and use) whereas concurrency.futures.ProcessPoolExecutor is an automatic transmission (e.g. easy to learn and use).\n• concurrency.futures.ProcessPoolExecutor: Automatic or “just work” mode for multiprocessing in Python.\n\nThere are four main steps in the life-cycle of using the ProcessPoolExecutor class; they are: create, submit, wait, and shut down.\n• Step 1. Create: Create the process pool by calling the constructor ProcessPoolExecutor().\n• Step 2. Submit: Submit tasks and get futures by calling submit() or map().\n• Step 3. Wait: Wait and get results as tasks complete (optional).\n• Step 4. Shut down: Shut down the process pool by calling shutdown().\n\nThe following figure helps to picture the life-cycle of the ProcessPoolExecutor class.\n\nLet’s take a closer look at each life-cycle step in turn.\n\nFirst, a ProcessPoolExecutor instance must be created.\n\nWhen an instance of a ProcessPoolExecutor is created, it must be configured with the fixed number of processes in the pool, method used for creating new processes (e.g. spawn or fork), and the name of a function to call when initializing each process along with any arguments for the function.\n\nThe pool is created with one process for each CPU in your system. This is good for most purposes.\n\nFor example, if you have 4 CPUs, each with hyperthreading (most modern CPUs have this), then Python will see 8 CPUs and will allocate 6 processes to the pool by default.\n\nIt is a good idea to test your application in order to determine the number of processes that results in the best performance.\n\nFor example, for some computationally intensive tasks, you may achieve the best performance by setting the number of processes to be equal to the number of physical CPU cores (before hyperthreading), instead of the logical number of CPU cores (after hyperthreading).\n\nWe’ll discuss tuning the number of processes for your pool more later on.\n\nYou can specify the number of process to create in the pool via the max_workers argument; for example:\n\nRecall, whenever we use processes, we must protect the entry point of the program.\n\nThis can be achieved using an if-statement; for example:\n\nOnce the process pool has been created, you can submit tasks for asynchronous execution.\n\nAs discussed, there are two main approaches for submitting tasks defined on the Executor parent class. They are: map() and submit().\n\nThe map() function is an asynchronous version of the built-in map() function for applying a function to each element in an iterable, like a list.\n\nYou can call the map() function on the pool and pass it the name of your function and the iterable.\n\nYou are most likely to use map() when converting a for loop to run using one process per loop iteration.\n\nWhere “my_task” is your function that you wish to execute and “my_items” is your iterable of objects, each to be executed by your “my_task” function.\n\nThe tasks will be queued up in the process pool and executed by worker processes in the pool as they become available.\n\nThe map() function will return an iterable immediately. This iterable can be used to access the results from the target task function as they are available in the order that the tasks were submitted (e.g. order of the iterable you provided).\n\nYou can also set a timeout when calling map() via the “timeout” argument in seconds if you wish to impose a limit on how long you’re willing to wait for each task to complete as you’re iterating, after which a TimeOut error will be raised.\n\nThe submit() function submits one task to the process pool for execution.\n\nThe function takes the name of the function to call and all arguments to the function, then returns a Future object immediately.\n\nThe Future object is a promise to return the results from the task (if any) and provides a way to determine if a specific task has been completed or not.\n\nWhere “my_task” is the function you wish to execute and “arg1” and “arg2” are the first and second arguments to pass to the “my_task” function.\n\nYou can use the submit() function to submit tasks that do not take any arguments; for example:\n\nYou can access the result of the task via the result() function on the returned Future object. This call will block until the task is completed.\n\nYou can also set a timeout when calling result() via the “timeout” argument in seconds if you wish to impose a limit on how long you’re willing to wait for each task to complete, after which a TimeOut error will be raised.\n\nThe concurrent.futures module provides two module utility functions for waiting for tasks via their Future objects.\n\nRecall that Future objects are only created when we call submit() to push tasks into the process pool.\n\nThese wait functions are optional to use, as you can wait for results directly after calling map() or submit() or wait for all tasks in the process pool to finish.\n\nThese two module functions are wait() for waiting for Future objects to complete and as_completed() for getting Future objects as their tasks complete.\n• wait(): Wait on one or more Future objects until they are completed.\n• as_completed(): Returns Future objects from a collection as they complete their execution.\n\nYou can use both functions with Future objects created by one or more process pools; they are not specific to any given process pool in your application. This is helpful if you want to perform waiting operations across multiple process pools that are executing different types of tasks.\n\nBoth functions are useful to use with an idiom of dispatching multiple tasks into the process pool via submit in a list compression; for example:\n\nHere, my_task is our custom target task function, “my_data” is one element of data passed as an argument to “my_task“, and “my_datalist” is our source of my_data objects.\n\nWe can then pass the “futures” to wait() or as_completed().\n\nCreating a list of futures in this way is not required, just a common pattern when converting for loops into tasks submitted to a process pool.\n\nThe wait() function can take one or more futures and will return when a specified action occurs, such as all tasks completing, one task completing, or one task throwing an exception.\n\nThe function will return one set of future objects that match the condition set via the “return_when“. The second set will contain all of the futures for tasks that did not meet the condition. These are called the “done” and the “not_done” sets of futures.\n\nIt is useful for waiting on a large batch of work and to stop waiting when we get the first result.\n\nThis can be achieved via the FIRST_COMPLETED constant passed to the “return_when” argument.\n\nAlternatively, we can wait for all tasks to complete via the ALL_COMPLETED constant.\n\nThis can be helpful if you are using submit() to dispatch tasks and are looking for an easy way to wait for all work to be completed.\n\nThere is also an option to wait for the first exception via the FIRST_EXCEPTION constant.\n\nThe beauty of performing tasks concurrently is that we can get results as they become available, rather than waiting for all tasks to be completed.\n\nThe as_completed() function will return Future objects for tasks as they are completed in the process pool.\n\nWe can call the function and provide it a list of Future objects created by calling submit() and it will return Future objects as they are completed in whatever order.\n\nIt is common to use the as_completed() function in a loop over the list of Futures created when calling submit(); for example:\n\nNote: this is different from iterating over the results from calling map() in two ways.\n\nFirstly, map() returns an iterator over objects, not over Futures.\n\nSecondly, map() returns results in the order that the tasks were submitted, not in the order that they are completed.\n\nOnce all tasks are completed, we can close down the process pool, which will release each process and any resources it may hold.\n\nFor example, each process is running an instance of the Python interpreter and at least one thread (the main thread) that has its own stack space.\n\nThe shutdown() function will wait for all tasks in the process pool to complete before returning by default.\n\nThis behavior can be changed by setting the “wait” argument to False when calling shutdown(), in which case the function will return immediately. The resources used by the process pool will not be released until all current and queued tasks are completed.\n\nWe can also instruct the pool to cancel all queued tasks to prevent their execution. This can be achieved by setting the “cancel_futures” argument to True. By default, queued tasks are not cancelled when calling shutdown().\n\nIf we forget to close the process pool, the process pool will be closed automatically when we exit the main thread. If we forget to close the pool and there are still tasks executing, the main process will not exit until all tasks in the pool and all queued tasks have executed.\n\nA preferred way to work with the ProcessPoolExecutor class is to use a context manager.\n\nThis matches the preferred way to work with other resources, such as files and sockets.\n\nUsing the ProcessPoolExecutor with a context manager involves using the “with” keyword to create a block in which you can use the process pool to execute tasks and get results.\n\nOnce the block has completed, the process pool is automatically shut down. Internally, the context manager will call the shutdown() function with the default arguments, waiting for all queued and executing tasks to complete before returning and carrying on.\n\nBelow is a code snippet to demonstrate creating a process pool using the context manager.\n\nThis is a very handy idiom if you are converting a for loop to be executed asynchronously.\n\nIt is less useful if you want the process pool to operate in the background while you perform other work in the main thread of your program, or if you wish to reuse the process pool multiple times throughout your program.\n\nNow that we are familiar with how to use the ProcessoolExecutor, let’s look at some worked examples.\n\nIn this section, we will look at a more complete example of using the ProcessPoolExecutor.\n\nConsider a situation where we might want to check if a word is known to the program or not, e.g. whether it is in a dictionary of known words.\n\nIf the word is known, that is fine, but if not, we might want to take action for the user, perhaps underline it in read like an automatic spell check.\n\nOne approach to implementing this feature would be to load a dictionary of known words and create a hash of each word. We can then hash new words and check if they exist in the set of known hashed words or not.\n\nThis is a good problem to explore with the ProcessPoolExecutor as hashing words can be relatively slow, especially for large dictionaries of hundreds of thousands or millions of known words.\n\nFirst, let’s develop a serial (non-concurrent) version of the program.\n\nThe first step is to select a dictionary of words to use.\n\nOn Unix systems, like MacOS and Linux, we have a dictionary already installed, called Unix Words.\n\nIt is located in one of the following locations:\n\nOn my system, it is located in ‘/usr/share/dict/words‘ and contains 235,886 words calculated using the command:\n\nWe can use this dictionary of words.\n\nAlternatively, if we are on Windows or wish to have a larger dictionary, we can download one of many free lists of words online.\n\nFor example, you can download a list of one million English words from here:\n\nDownload this file and save it to your current working directory with the filename “1m_words.txt“.\n\nLooking in the file, we can see that indeed we have a long list of words, one per line.\n\nFirst, we need to load the list of words into memory.\n\nThis can be achieved by first opening the file, then calling the readlines() function that will automatically read ASCII lines of text into a list.\n\nThe load_words() function below takes a path to the text file and returns a list of words loaded from the file.\n\nNext, we need to hash each word.\n\nWe will intentionally select a slow hash function in this example, specifically the SHA512 algorithm.\n\nThis is available in Python via the hashlib.ha512() function.\n\nFirst, we can create an instance of the hashing object by calling the sha512() function.\n\nNext, we can convert a given word to bytes and then hash it using the hash function.\n\nFinally, we can get a HEX string representation of the hash for the word by calling the hexdigest() function.\n\nTying this together, the hash_word() function below takes a word and returns a HEX hash code of the word.\n\nThat’s about all there is to it.\n\nWe can define a function that will drive the program, first loading the list of words by calling our load_words(), then creating a set of hashes of known words by calling our hash_word() for each loaded word.\n\nThe main() function below implements this.\n\nTying this all together, the complete example of loading a dictionary of words and creating a set of known word hashes is listed below.\n\nRunning the example first loads the file and reports that a total of 1,049,938 words were loaded.\n\nThe list of words is then hashed and the hashes are stored in a set.\n\nThe program reports that a total of 979,250 hashes were stored, suggesting thousands of duplicates in the dictionary.\n\nThe program takes about 1.4 seconds to run on a modern system.\n\nHow long does the example take to run on your system?\n\nLet me know in the comments below.\n\nNext, we can update the program to hash the words concurrently.\n\nHashing words is relatively slow, but even so, hashing nearly one million words takes under two seconds.\n\nNevertheless, we can accelerate the process by making use of all CPUs in the system and hashing the words concurrently.\n\nThis can be achieved using the ProcessPoolExecutor.\n\nFirstly, we can create the process pool and specify the number of concurrent processes to run. I recommend configuring the pool to match the number of physical CPU cores in your system.\n\nI have four cores, so the example will use four cores, but update it for the number of cores you have available.\n\nNext, we need to submit the tasks to the process pool, that is, the hashing of each word.\n\nBecause the task is simply applying a function for each item in a list, we can use the map() function directly.\n\nFor example, the updated version of the main() function to hash words concurrently is listed below.\n\nWell, not so fast.\n\nThis would execute, but it would take a very long time to complete.\n\nThe reason is that we would be adding nearly one million tasks to the pool to be executed by four processes, and each task would need to be pickled and queued internally. Repeating these operations so many times results in an overhead that far surpasses the execution time of the task.\n\nWe must reduce the overhead by reducing the number of internal tasks within the process pool.\n\nThis can be achieved by setting the “chunksize” parameter when calling map().\n\nThis controls how many items in the iterable map to one task in the process pool. By default, one item is mapped to one task, meaning we have nearly one million tasks.\n\nPerhaps a good first approach would be to split the number items by the number of processes available, in this case four. This would create four tasks, e.g. four large chunks of words, each to be processed by one process, likely on one CPU core.\n\nThis can be achieved by calculating the length of the list of words and dividing it by the number of worker processes. The division might not be clean, therefore we can use the math.ceil() math function to round the number of items per task up to the nearest integer.\n\nWe can estimate that this would be (1049938 / 4) or about 262484.5 words per task, e.g. just over half a million.\n\nWe can then use this chunksize when calling the map() function.\n\nTying this together, the complete example of hashing a dictionary of words concurrently using the ProcessPoolExecutor is listed below.\n\nRunning the example loads the words as before, then creates the set of hashed words concurrently by splitting it into four tasks, one for each process in the pool.\n\nThis concurrent version does offer a very minor speedup, taking about 1.2 seconds on my system, offering a small speedup.\n\nNext, let’s see if we can get a further improvement by tuning the chunksize argument.\n\nTesting chunksize Values When Hashing a Dictionary of Words With map()\n\nSplitting items into tasks for the process pool is more art than science.\n\nGetting it wrong, like setting it to one when we have a large number of tasks, can result in much worse performance than the serial case. Setting it naively can result in equivalent or slightly better performance than the serial case.\n\nAs such, we can tune the performance of the application by testing different values of the “chunksize” argument.\n\nIn the previous section, we saw that a chunksize of 262485 resulted in similar performance to the serial case.\n\nI recommend testing different chunk sizes in order to discover what works well on your specific system; for example, some numbers you could try include:\n\nIt is common to perform this type of tuning when working with distributed systems and multi-process systems as the specific cost of serializing and transmitting data between workers depends on the hardware and specific data.\n\nIf the tasks involved were long running or sensitive in some way, you could design a test harness with mock tasks.\n\nWe can define a function to test a given chunksize argument that also calculates how long the task takes to complete, including the fixed cost of setting up the process pool.\n\nThe test_chunksize() function below implements this, taking the loaded dictionary of words and chunksize to test, and reports how long it took to execute the task for the given chunksize.\n\nWe can call this function from our main() function with a list of different chunk size values to test; for example:\n\nTying this together, the complete example of testing different chunksize values is listed below.\n\nRunning the example, we can see that a chunksize of about 10,000 or 5,000 would work well, performing the task in about 0.8 seconds as opposed to about 1.4 in the serial case and 1.2 for the naive configuration of chunksize, at least on my system.\n\nThis highlights the importance of tuning the chunksize for your specific task and computer hardware.\n\nWhat worked well on your system?\n\nLet me know in the comments below.\n\nThe ProcessPoolExecutor provides a lot of flexibility for executing concurrent tasks in Python.\n\nNevertheless, there are a handful of common usage patterns that will fit most program scenarios.\n\nThis section lists the common usage patterns with worked examples that you can copy-and-paste into your own project and adapt as needed.\n\nThe patterns we will look at are as follows:\n• Submit and Use as Completed Pattern\n• Submit and Wait for All Pattern\n• Submit and Wait for First Pattern\n\nWe will use a contrived task in each example that will sleep for a random amount of time less than one second. You can easily replace this example task with your own task in each pattern.\n\nAlso, recall that each Python program is a process and has one thread by default called the main thread where we do our work. We will create the process pool in the main thread in each example and may reference actions in the main thread in some of the patterns, as opposed to actions in processes in the process pool.\n\nPerhaps the most common pattern when using the ProcessPoolExecutor is to convert a for loop that executes a function on each item in a collection to use multiprocessing.\n\nIt assumes that the function has no side effects, meaning it does not access any data outside of the function and does not change the data provided to it. It takes data and produces a result.\n\nThese types of for loops can be written explicitly in Python; for example:\n\nA better practice is to use the built-in map() function that applies the function to each item in the iterable for you.\n\nThis does not perform the task() function to each item until we iterate the results, so-called lazy evaluation:\n\nTherefore, it is common to see this operation consolidated to the following:\n\nWe can perform this same operation using the process pool, except each application of the function to an item in the list is a task that is executed asynchronously. For example:\n\nAlthough the tasks are executed asynchronously, the results are iterated in the order of the iterable provided to the map() function, the same as the built-in map() function.\n\nIn this way, we can think of the process pool version of map() as a concurrent version of the map() function and is ideal if you are looking to update your for loop to use processes.\n\nThe example below demonstrates using the map and wait pattern with a task that will sleep a random amount of time less than one second and return the provided value.\n\nRunning the example, we can see that the results are reported in the order that the tasks were created and sent into the process pool.\n\nThe map() function supports target functions that take more than one argument by providing more than iterable as arguments to the call to map().\n\nFor example, we can define a target function for map that takes two arguments, then provide two iterables of the same length to the call to map.\n\nThe complete example is listed below.\n\nRunning the example executes the tasks as expected, providing two arguments to map and reporting a result that combines both arguments.\n\nA call to the map function will issue all tasks to the process pool immediately, even if you do not iterate the iterable of results.\n\nThis is unlike the built-in map() function that is lazy and does not compute each call until you ask for the result during iteration.\n\nThe example below confirms this by issuing all tasks with a map and not iterating the results.\n\nRunning the example, we can see that the tasks are sent into the process pool and executed without having to explicitly pass over the iterable of results that was returned.\n\nThe use of the context manager ensured that the process pool did not shut down until all tasks were complete.\n\nSubmit and Use as Completed\n\nPerhaps the second most common pattern when using the ProcessPoolExecutor is to submit tasks and use the results as they become available.\n\nThis can be achieved using the submit() function to push tasks into the process pool that returns Future objects, then calling the module method as_completed() on the list of Future objects that will return each Future object as it’s task is completed.\n\nThe example below demonstrates this pattern, submitting the tasks in order from 0 to 9 and showing results in the order that they were completed.\n\nRunning the example, we can see that the results are retrieved and printed in the order that the tasks completed, not the order that the tasks were submitted to the process pool.\n\nWe may require the results from tasks in the order that the tasks were submitted.\n\nThis may be because the tasks have a natural ordering.\n\nWe can implement this pattern by calling submit() for each task to get a list of Future objects then iterating over the Future objects in the order that the tasks were submitted and retrieving the results.\n\nThe main difference from the “as completed” pattern is that we enumerate the list of futures directly, instead of calling the as_completed() function.\n\nThe example below demonstrates this pattern, submitting the tasks in order from 0 to 9 and showing the results in the order that they were submitted.\n\nRunning the example, we can see that the results are retrieved and printed in the order that the tasks were submitted, not the order that the tasks were completed.\n\nWe may not want to explicitly process the results once they are available; instead, we want to call a function on the result.\n\nInstead of doing this manually, such as in the as completed pattern above, we can have the process pool call the function for us with the result automatically.\n\nThis can be achieved by setting a callback on each future object by calling the add_done_callback() function and passing the name of the function.\n\nThe process pool will then call the callback function as each task completes, passing in Future objects for the task.\n\nThe example below demonstrates this pattern, registering a custom callback function to be applied to each task as it is completed.\n\nRunning the example, we can see that results are retrieved and printed in the order they are completed, not the order that tasks were completed.\n\nWe can register multiple callbacks on each Future object; it is not limited to a single callback.\n\nThe callback functions are called in the order in which they were registered on each Future object.\n\nThe following example demonstrates having two callbacks on each Future.\n\nRunning the example, we can see that results are reported in the order that tasks were completed and that the two callback functions are called for each task in the order that we registered them with each Future object.\n\nSubmit and Wait for All\n\nIt is common to submit all tasks and then wait for all tasks in the process pool to complete.\n\nThis pattern may be useful when tasks do not return a result directly, such as if each task stores the result in a resource directly like a file.\n\nThere are two ways that we can wait for tasks to complete: by calling the wait() module function or by calling shutdown().\n\nThe most likely case is you want to explicitly wait for a set or subset of tasks in the process pool to complete.\n\nYou can achieve this by passing the list of tasks to the wait() function, which by default will wait for all tasks to complete.\n\nWe can explicitly specify to wait for all tasks by setting the “return_when” argument to the ALL_COMPLETED constant; for example:\n\nThe example below demonstrates this pattern. Note that we are intentionally ignoring the return from calling wait() as we have no need to inspect it in this case.\n\nRunning the example, we can see that results are handled by each task as the tasks complete. Importantly, we can see that the main process waits until all tasks are completed before carrying on and printing a message.\n\nAn alternative approach would be to shut down the process pool and wait for all executing and queued tasks to complete before moving on.\n\nThis might be preferred when we don’t have a list of Future objects or when we only intend to use the process pool once for a set of tasks.\n\nWe can implement this pattern using the context manager; for example:\n\nRunning the example, we can see that the main process does not move on and print the message until all tasks are completed, after the process pool has been automatically shut down by the context manager.\n\nThe context manager automatic shutdown pattern might be confusing to developers not used to how process pools work, hence the comment at the end of the context manager block in the previous example.\n\nWe can achieve the same effect without the context manager and an explicit call to shutdown.\n\nRecall that the shutdown() function will wait for all tasks to complete by default and will not cancel any queued tasks, but we can make this explicit by setting the “wait” argument to True and the “cancel_futures” argument to False; for example:\n\nThe example below demonstrates the pattern of waiting for all tasks in the process pool to complete by calling shutdown() before moving on.\n\nRunning the example, we can see that all tasks report their result as they complete and that the main thread does not move on until all tasks have completed and the process pool has been shut down.\n\nSubmit and Wait for First\n\nIt is common to issue many tasks and only be concerned with the first result returned.\n\nThat is, not the result of the first task, but a result from any task that happens to be the first to complete its execution.\n\nThis may be the case if you are trying to access the same resource from multiple locations, like a file or some data.\n\nThis pattern can be achieved using the wait() module function and setting the “return_when” argument to the FIRST_COMPLETED constant.\n\nWe must also manage the process pool manually by constructing it and calling shutdown() manually so that we can continue on with the execution of the main process without waiting for all of the other tasks to complete.\n\nThe example below demonstrates this pattern and will stop waiting as soon as the first task is completed.\n\nRunning the example will wait for any of the tasks to complete, then retrieve the result of the first completed task and shut down the process pool.\n\nImportantly, the tasks will continue to execute in the process pool in the background and the main thread will not close until all tasks have completed.\n\nNow that we have seen some common usage patterns for the ProcessPoolExecutor, let’s look at how we might customize the configuration of the process pool.\n\nWe can customize the configuration of the process pool when constructing a ProcessPoolExecutor instance.\n\nThere are three aspects of the process pool we may wish to customize for our application; they are the number of workers, the names of processes in the pool, and the initialization of each process in the pool.\n\nLet’s take a closer look at each in turn.\n\nThe number of processes in the process pool can be configured by the “max_workers” argument.\n\nIt takes a positive integer and defaults to the number of CPUs in your system.\n\nFor example, if you had 2 physical CPUs in your system and each CPU has hyperthreading (common in modern CPUs), then you would have 2 physical and 4 logical CPUs. Python would see 4 CPUs. The default number of worker processes on your system would then be 4.\n\nThe number of workers must be less than or equal to 61 if Windows is your operating system.\n\nIt is common to have more processes than CPUs (physical or logical) in your system, if the target task function is performing IO operations.\n\nThe reason for this is that IO-bound tasks spend most of their time waiting rather than using the CPU. Examples include reading or writing from hard drives, DVD drives, printers, and network connections, and much more. We will discuss the best application of processes in a later section.\n\nIf you require hundreds or processes for IO-bound tasks, you might want to consider using threads instead and the ThreadPoolExecutor. If you require thousands of processes for IO-bound tasks, you might want to consider using the AsyncIO module.\n\nFirst, let’s check how many processes are created for process pools on your system.\n\nLooking at the source code for the ProcessPoolExecutor, we can see that the number of worker processes chosen by default is stored in the _max_workers property, which we can access and report after a process pool is created.\n\nNote: “_max_workers” is a protected member and may change in the future.\n\nThe example below reports the number of default processes in a process pool on your system.\n\nRunning the example reports the number of worker processes used by default on your system.\n\nI have four physical CPU cores, eight logical cores; therefore, the default is 8 processes.\n\nHow many worker processes are allocated by default on your system?\n\nLet me know in the comments below.\n\nWe can specify the number of worker processes directly, and this is a good idea in most applications.\n\nThe example below demonstrates how to configure 60 worker processes.\n\nRunning the example configures the process pool to use 60 processes and confirms that it will create 60 processes.\n\nHow Many Processes Should You Use?\n\nThis is a tough question and depends on the specifics of your program.\n\nPerhaps if you have fewer than 100 IO-bound tasks (or 60 on Windows), then you might want to set the number of worker processes to the number of tasks.\n\nIf you are working with IO-bound tasks, then you might want to cap the number of workers that number of logical CPUs in your system, e.g. the default for the ProcessPoolExecutor.\n\nIf your application is intended to be executed multiple times in the future, you can test different numbers of processes and compare overall execution time, then choose a number of processes that gives approximately the best performance. You may want to mock the task in these tests with a random sleep or compute operation.\n\nDifferent operating systems provide different ways to create new processes.\n\nPerhaps the two most common ways to create new processes are spawn and fork.\n• spawn: Creates a new instance of the Python interpreter as a process. Available on Windows, Unix, and MacOS.\n• fork: Creates a fork of an existing Python interpreter process. Available on Unix.\n• forkserver: Creates a server Python interpreter process to be used to create all all forked processes for the life of the program. Available on Unix.\n\nYour Python installation will select the most appropriate method for creating a new process for your operating system. Nevertheless, you can specify how new processes are created and this is called a “process context.”\n\nYou can get a new process context for a specific method for creating new processes (e.g. fork or spawn) and pass this context to the ProcessPoolExecutor. This will allow all new processes created by the process pool to be created using the provided context and use your preferred method for starting processes.\n\nFirstly, let’s see what process start methods are supported by your operating system and discover the default method that you are using.\n\nWe can call the get_all_start_methods() function to get a list of all supported methods and the get_start_method() function to get the currently configured (default) process start method.\n\nThe program below will report the process start methods and default start method on your system.\n\nRunning the example first reports all of the process start methods supported by your system.\n\nNext, the default process start method is supported.\n\nIn this case, running the program on MacOS, we can see that the operating system supports all three process start methods and the default is the “spawn” method.\n\nNext, we can check the default context used to start processes in the process pool.\n\nThe ProcessPoolExecutor will use the default context unless it is configured to use a different context.\n\nWe can check the start process context used by the ProcessPoolExecutor via the “_mp_context” protected property.\n\nThe example below creates a process pool and reports the default context used by the process pool\n\nRunning the example creates a process pool and reports the default start process context used by the pool.\n\nIn this case, we can see that it is the ‘spawn‘ context, denoted by the “SpawnContext” object.\n\nNext, we can create a context and pass it to the process pool.\n\nThe ProcessPoolExecutor takes an argument named “mp_context” that defines the context used for creating processes in the pool.\n\nBy default, it is set to None, in which case the default context is used.\n\nWe can set the context by first calling the get_context() function and specifying the preferred method as a string that matches a string returned from calling the get_all_start_methods() function, e.g. ‘fork‘ or ‘spawn‘.\n\nPerhaps we wanted to force all processes to be created using the ‘fork‘ method, regardless of the default.\n\nNote: using ‘fork‘ will not work on windows. You might want to change it to use ‘spawn‘ or report the error message you see in the comments below.\n\nFirst, we would create a context, then pass this context to the process pool. We can then access and report the context manager used by the process pool; for example.\n\nTying this together, the complete example of setting the context manager for the process pool and then confirming it was changed is listed below.\n\nRunning the example first creates a new start process context, then passes it to the new ProcessPoolExecutor.\n\nAfter the pool is created, the context manager used by the pool is reported, which in this case is ‘fork‘ denoted by the ‘ForkContext‘ object.\n\nWorker processes can call a function before they start executing tasks.\n\nThis is called an initializer function and can be specified via the “initializer” argument when creating a process pool. If the initializer function takes arguments, they can be passed in via the “initargs” argument to the process pool which is a tuple of arguments to pass to the initializer function.\n\nBy default, there is no initializer function.\n\nWe might choose to set an initializer function for worker processes if we would like each process to set up resources specific to the process.\n\nExamples might include a process-specific log file or a process-specific connection to a remote resource like a server or database. The resource would then be available to all tasks executed by the process, rather than being created and discarded or opened and closed for each task.\n\nThese process-specific resources can then be stored somewhere where the worker process can reference, like a global variable, or in a process-local variable. Care must be taken to correctly close these resources once you are finished with the process pool.\n\nThe example below will create a process pool with two workers and use a custom initialization function. In this case, the function does nothing other than print a message. We then complete ten tasks with the process pool.\n\nRunning the example, we can see that the two processes are initialized before running any tasks, then all ten tasks are completed successfully.\n\nNow that we are familiar with how to configure the process pools, let’s learn more about how to check and manipulate tasks via Future objects.\n\nHow to Use Future Objects in Detail\n\nFuture objects are created when we call submit() to send tasks into the ProcessPoolExecutor to be executed asynchronously.\n\nFuture objects provide the capability to check the status of a task (e.g. is it running?) and to control the execution of the task (e.g. cancel).\n\nIn this section, we will look at some examples of checking and manipulating Future objects created by our process pool.\n\nSpecifically, we will look at the following:\n• How to Check the Status of Futures\n• How to Get Results From Futures\n• How to Add a Callback to Futures\n• How to Get Exceptions from Futures\n\nFirst, let’s take a closer look at the life-cycle of a Future object.\n\nA Future object is created when we call submit() for a task on a ProcessPoolExecutor.\n\nWhile the task is executing, the Future object has the status “running“.\n\nWhen the task completes, it has the status “done” and if the target function returns a value, it can be retrieved.\n\nBefore a task is running, it will be inserted into a queue of tasks for a worker process to take and start running. In this “pre-running” state, the task can be cancelled and has the “cancelled” state. A task in the “running” state cannot be cancelled.\n\nA “cancelled” task is always also in the “done” state.\n\nWhile a task is running, it can raise an uncaught exception, causing the execution of the task to stop. The exception will be stored and can be retrieved directly or will be re-raised if the result is attempted to be retrieved.\n\nThe figure below summarizes the life-cycle of a Future object.\n\nNow that we are familiar with the life-cycle of a Future object, let’s look at how we might use check and manipulate it.\n\nHow to Check the Status of Futures\n\nThere are two types of normal status of a Future object that we might want to check: running and done.\n\nEach has its own function that returns a True if the Future object is in that state or False otherwise; for example:\n• running(): Returns True if the task is currently running.\n• done(): Returns True if the task has completed or was cancelled.\n\nWe can develop simple examples to demonstrate how to check the status of a Future object.\n\nIn this example, we can start a task and then check that it’s running and not done, wait for it to complete, then check that it is done and not running.\n\nRunning the example, we can see that immediately after the task is submitted that it is marked as running, and that after the task is completed, we can confirm that it is done.\n\nHow to Get Results From Futures\n\nWhen a task is completed, we can retrieve the result from the task by calling the result() function on the Future.\n\nThis returns the result from the return function of the task we executed or None if the function did not return a value.\n\nThe function will block until the task completes and a result can be retrieved. If the task has already been completed, it will return a result immediately.\n\nThe example below demonstrates how to retrieve a result from a Future object\n\nRunning the example submits the task, then attempts to retrieve the result, blocking until the result is available then reports the result that was received.\n\nWe can also set a timeout for how long we wish to wait for a result in seconds.\n\nIf the timeout elapses before we get a result, a TimeoutError is raised.\n\nThe example below demonstrates the timeout, showing how to give up waiting before the task has completed.\n\nRunning the example shows that we gave up waiting for a result after half a second.\n\nWe can also cancel a task that has not yet started running.\n\nRecall that when we put tasks into the pool with submit() or map() that the tasks are added to an internal queue of work from which worker processes can remove the tasks and execute them.\n\nWhile a task is on the queue and before it has been started, we can cancel it by calling cancel() on the Future object associated with the task. The cancel() function will return True if the task was cancelled, False otherwise.\n\nLet’s demonstrate this with a worked example.\n\nWe can create a process pool with one process then start a long running task, then submit a second task, request that it is cancelled, then confirm that it was indeed cancelled.\n\nRunning the example, we can see that the first task is started and is running normally.\n\nThe second task is scheduled and is not yet running because the process pool is occupied with the first task. We then cancel the second task and confirm that it is indeed not running; it was cancelled and is done.\n\nNext, let’s try to cancel a task that has already completed running.\n\nThe complete example is listed below.\n\nRunning the example, we can see that the task was started as per normal.\n\nWe then tried to cancel the task, but this was not successful, as we expected since the task was already running.\n\nWe then wait for the task to complete and then check it’s status. We can see that the task is no longer running and was not cancelled, as we expect, and it was marked as done.\n\nConsider what would happen if we tried to cancel a task that was already done.\n\nWe might expect that canceling a task that is already done has no effect, and this happens to be the case.\n\nThis can be demonstrated with a short example.\n\nWe start and run a task as per normal, then wait for it to complete and report its status. We then attempt to cancel the task.\n\nRunning the example confirms that the task runs and is marked done, as per normal.\n\nThe attempt to cancel the task fails and checking the status after the attempt to cancel, confirms that the task was not impacted by the attempt.\n\nHow to Add a Callback to Futures\n\nWe have already seen above how to add a callback to a Future; nevertheless, let’s look at some more examples for completeness, including some edge cases.\n\nWe can register one or more callback functions on a Future object by calling the add_done_callback() function and specifying the name of the function to call.\n\nThe callbacks functions will be called with the Future object as an argument immediately after the completion of the task. If more than one callback function is registered, then they will be called in the order they were registered and any exceptions within each callback function will be caught, logged, and ignored.\n\nThe callback will be called by the worker process that executed the task.\n\nThe example below demonstrates how to add a callback function to a Future object.\n\nRunning the example, we can see that the task is completed first, then the callback is executed as we expected.\n\nA common error is to forget to add the Future object as an argument to the custom callback.\n\nIf you register this function and try to run the code, you will get a TypeError as follows:\n\nThe message in the TypeError makes it clear how to fix the issue: add a single argument to the function for the Future object, even if you don’t intend on using it in your callback.\n\nWe can also see the effect of callbacks on Future objects for tasks that are cancelled.\n\nThe effect does not appear to be documented in the API, but we might expect for the callback to always be executed, whether the task is run normally or whether it is cancelled. And this happens to be the case.\n\nThe example below demonstrates this.\n\nFirst, a process pool is created with a single process. A long running task is issued that occupies the entire pool, then we send in a second task, add a callback to the second task, cancel it, and wait for all tasks to finish.\n\nRunning the example, we can see that the first task is started as we expect.\n\nThe second task is scheduled but does not get a chance to run before we cancel it.\n\nThe callback is run immediately after we cancel the task, then we report in the main thread that indeed the task was cancelled correctly.\n\nHow to Get Exceptions From Futures\n\nA task may raise an exception during execution.\n\nIf we can anticipate the exception, we can wrap parts of our task function in a try-except block and handle the exception within the task.\n\nIf an unexpected exception occurs within our task, the task will stop executing.\n\nWe cannot know based on the task status whether an exception was raised, but we can check for an exception directly.\n\nWe can then access the exception via the exception() function. Alternately, the exception will be re-raised when calling the result() function when trying to get a result.\n\nWe can demonstrate this with an example.\n\nThe example below will raise a ValueError within the task that will not be caught but instead will be caught by the process pool for us to access later.\n\nRunning the example starts the task normally, which sleeps for one second.\n\nThe task then throws an exception that is caught by the process pool. The process pool stores the exception and the task is completed.\n\nWe can see that after the task is completed, it is marked as not running, not cancelled, and done.\n\nWe then access the exception from the task, which matches the exception we intentionally throw.\n\nAttempting to access the result via the result() function fails and we catch the same exception raised in the task.\n\nCallbacks Are Still Called if a Task Raises an Exception\n\nWe might wonder if we register a callback function with a Future, whether it will still execute if the task raises an exception.\n\nAs we might expect, the callback is executed even if the task raises an exception.\n\nWe can test this by updating the previous example to register a callback function before the task fails with an exception.\n\nRunning the example starts the task as before, but this time registers a callback function.\n\nWhen the task fails with an exception, the callback is called immediately. The main process then reports the status of the failed task and the details of the exception.\n\nWhen to Use the ProcessPoolExecutor\n\nThe ProcessPoolExecutor is powerful and flexible, although is not suited for all situations where you need to run a background task.\n\nIn this section, we will look at some general cases where it is a good fit, and where it isn’t, then we’ll look at broad classes of tasks and why they are or are not appropriate for the ProcessPoolExecutor.\n• Your tasks can be defined by a pure function that has no state or side effects.\n• Your task can fit within a single Python function, likely making it simple and easy to understand.\n• You need to perform the same task many times, e.g. homogeneous tasks.\n• You need to apply the same function to each object in a collection in a for-loop.\n\nProcess pools work best when applying the same pure function on a set of different data (e.g. homogeneous tasks, heterogeneous data). This makes code easier to read and debug. This is not a rule, just a gentle suggestion.\n• You need to perform groups of different types of tasks; one process pool could be used for each task type.\n• You need to perform a pipeline of tasks or operations; one process pool can be used for each step.\n\nProcess pools can operate on tasks of different types (e.g. heterogeneous tasks), although it may make the organization of your program and debugging easy if a separate process pool is responsible for each task type. This is not a rule, just a gentle suggestion.\n• You have a single task; consider using the Process class with the “target” argument.\n• You have long running tasks, such as monitoring or scheduling; consider extending the Process class.\n• Your task functions require state; consider extending the Process class.\n• Your tasks require coordination; consider using a Process and patterns like a Barrier or Semaphore.\n• Your tasks require synchronization; consider using a Process and Locks.\n• You require a process trigger on an event; consider using the Process class.\n\nThe sweet spot for process pools is in dispatching many similar tasks, the results of which may be used later in the program. Tasks that don’t fit neatly into this summary are probably not a good fit for process pools. This is not a rule, just a gentle suggestion.\n\nDo you know any other good or bad cases where using a ProcessPoolExecutor?\n\nLet me know in the comments below.\n\nYou can use processes for IO-bound tasks, although threads may be a better fit.\n\nAn IO-bound task is a type of task that involves reading from or writing to a device, file, or socket connection.\n\nThe operations involve input and output (IO) and the speed of these operations is bound by the device, hard drive, or network connection. This is why these tasks are referred to as IO-bound.\n\nCPUs are really fast. Modern CPUs like a 4GHz can execute 4 billion instructions per second, and you likely have more than one CPU in your system.\n\nDoing IO is very slow compared to the speed of CPUs.\n\nInteracting with devices, reading and writing files, and socket connections involves calling instructions in your operating system (the kernel), which will wait for the operation to complete. If this operation is the main focus for your CPU, such as executing in the main thread of your Python program, then your CPU is going to wait many milliseconds or even many seconds doing nothing.\n\nThat is potentially billions of operations that it is prevented from executing.\n\nWe can free-up the CPU from IO-bound operations by performing IO-bound operations on another process of execution. This allows the CPU to start the task and pass it off to the operating system (kernel) to do the waiting, and free it up to execute in another application process.\n\nThere’s more to it under the covers, but this is the gist.\n\nTherefore, the tasks we execute with a ProcessPoolExecutor can be tasks that involve IO operations.\n• Reading or writing a file from the hard drive.\n• Reading or writing to standard output, input, or error (stdin, stdout, stderr).\n• And so much more.\n\nYou should probably use processes for CPU-bound tasks.\n\nA CPU-bound task is a type of task that involves performing a computation and does not involve IO.\n\nThe operations only involve data in main memory (RAM) or cache (CPU cache) and performing computations on or with that data. As such, the limit on these operations is the speed of the CPU. This is why we call them CPU-bound tasks.\n\nCPUs are very fast and we often have more than one CPU. We would like to perform our tasks and make full use of multiple CPU cores in modern hardware.\n\nUsing processes and process pools via the ProcessPoolExecutor class in Python is probably the best path toward achieving this end.\n\nException handling is an important consideration when using processes.\n\nCode will raise an exception when something unexpected happens and the exception should be dealt with by your application explicitly, even if it means logging it and moving on.\n\nThere are three points you may need to consider exception handling when using the ProcessPoolExecutor; they are:\n\nLet’s take a closer look at each point in turn.\n\nYou can specify a custom initialization function when configuring your ProcessPoolExecutor.\n\nThis can be set via the “initializer” argument to specify the function name and “initargs” to specify a tuple of arguments to the function.\n\nEach process started by the process pool will call your initialization function before starting the process.\n\nIf your initialization function throws an exception, it will break your process pool.\n\nAll current tasks and any future tasks executed by the process pool will not run and will raise a BrokenProcessPool exception.\n\nWe can demonstrate this with an example of a contrived initializer function that throws an exception.\n\nRunning the example fails with an exception, as we expected.\n\nThe process pool is created as per normal, but as soon as we try to execute tasks, new worker processes are created, the custom worker process initialization function is called and throws an exception.\n\nMultiple processes attempted to start, and in turn, multiple processes failed with an Exception. Finally, the process pool itself logged a message that the pool is broken and cannot be used any longer.\n\nThis highlights that if you use a custom initializer function, that you must carefully consider the exceptions that may be raised and perhaps handle them, otherwise risk all tasks that depend on the process pool.\n\nAn exception may occur while executing your task.\n\nThis will cause the task to stop executing, but will not break the process pool. Instead, the exception will be caught by the process pool and will be available via the Future object associated with the task via the exception() function.\n\nAlternately, the exception will be re-raised if you call result() in the future in order to get the result. This will impact both calls to submit() and map() when adding tasks to the process pool.\n\nIt means that you have two options for handling exceptions in tasks; they are:\n• 2. Handle exceptions when getting results from tasks.\n\nHandling the exception within the task means that you need some mechanism to let the recipient of the result know that something unexpected happened.\n\nThis could be via the return value from the function, e.g. None.\n\nAlternatively, you can re-raise an exception and have the recipient handle it directly. A third option might be to use some broader state or global state, perhaps passed by reference into the call to the function.\n\nThe example below defines a work task that will raise an exception, but will catch the exception and return a result indicating a failure case.\n\nRunning the example starts the process pool as per normal, issues the task, then blocks, waiting for the result.\n\nThe task raises an exception and the result received is an error message.\n\nThis approach is reasonably clean for the recipient code and would be appropriate for tasks issued by both submit() and map(). It may require special handling of a custom return value for the failure case.\n\nHandle Exception by the Recipient of the Task Result\n\nAn alternative to handling the exception in the task is to leave the responsibility to the recipient of the result.\n\nThis may feel like a more natural solution, as it matches the synchronous version of the same operation, e.g. if we were performing the function call in a for-loop.\n\nIt means that the recipient must be aware of the types of errors that the task may raise and handle them explicitly.\n\nThe example below defines a simple task that raises an Exception, which is then handled by the recipient when attempting to get the result from the function call.\n\nRunning the example creates the process pool and submits the work as per normal. The task fails with an error; the process pool catches the exception, stores it, then re-throws it when we call the result() function in the future.\n\nThe recipient of the result accepts the exception and catches it, reporting a failure case.\n\nWe can also check for the exception directly via a call to the exception() function on the Future object. This function blocks until an exception occurs and takes a timeout, just like a call to result().\n\nIf an exception never occurs and the task is cancelled or completes successfully, then exception() will return a value of None.\n\nWe can demonstrate the explicit checking for an exceptional case in the task in the example below.\n\nRunning the example creates and submits the work per normal.\n\nThe recipient checks for the exceptional case, which blocks until an exception is raised or the task is completed. An exception is received and is handled by reporting it.\n\nWe cannot check the exception() function of the Future object for each task, as map() does not provide access to Future objects.\n\nWorse still, the approach of handling the exception in the recipient cannot be used when using map() to submit tasks, unless you wrap the entire iteration.\n\nWe can demonstrate this by submitting one task with map() that happens to raise an Exception.\n\nThe complete example is listed below.\n\nRunning the example submits the single task (a bad use for map()) and waits for the first result.\n\nThe task throws an exception and the main thread and process exits, as we expected.\n\nThis highlights that if map() is used to submit tasks to the process pool, then the tasks should handle their own exceptions or be simple enough that exceptions are not expected.\n\nA final case we must consider for exception handling when using the ProcessPoolExecutor is in callback functions.\n\nWhen issuing tasks to the process pool with a call to submit(), we receive a Future object in return on which we can register callback functions to call when the task completes via the add_done_callback() function.\n\nThis allows one or more callback functions to be registered that will be executed in the order in which they are registered.\n\nThese callbacks are always called, even if the task is cancelled or fails itself with an exception.\n\nA callback can fail with an exception and it will not impact other callback functions that have been registered or the task.\n\nThe exception is caught by the process pool, logged as an exception type message and the procedure moves on. In a sense, callbacks are able to fail silently.\n\nWe can demonstrate this with a worked example with multiple callback functions, the first of which will raise an exception.\n\nRunning the example starts the process pool as per normal and executes the task.\n\nWhen the task completes, the first callback is called, which fails with an exception. The exception is logged and reported on the console (the default behavior for logging).\n\nThe process pool is not broken and carries on.\n\nThe second callback is called successfully, and finally the main thread gets the result of the task.\n\nThis highlights that if callbacks are expected to raise an exception, that it must be handled explicitly and checked for if you wish to have the failure impact the task itself.\n\nIt is important to pause for a moment and look at how the ProcessPoolExecutor works internally.\n\nThe internal workings of the class impact how we use the process pool and the behavior we can expect, specifically around cancelling tasks.\n\nWithout this knowledge, some of the behavior of the process pool may appear confusing or even buggy from the outside.\n\nYou can see the source code for the ProcessPoolExecutor and the base class here:\n\nThere is a lot we could learn about how the process pool works internally, but we will limit ourselves to the most critical aspects.\n\nThere are two aspects that you need to consider about the internal operation of the ProcessPoolExecutor class: how tasks are sent into the pool and how worker processes are created.\n\nThe source code for the ProcessPoolExecutor provides a good summary for how the pool works internally.\n\nThere is an ASCII diagram in the source that we can reproduce here that will help:\n\nThe data flow for submitting new tasks to the pool is as follows.\n\nFirst, we call submit() on the pool to add a new task. Note that calling map() internally will call the submit() function.\n\nThis will add a work item to an internal dictionary, specifically an instance of an internal class named _WorkItem. Each work item also has a unique identifier that is tracked in an internal Queue.\n\nThe process pool has an internal worker thread that is responsible for preparing work for the worker processes. It will wake-up when new work is added to the queue of work ids and will retrieve the new task and submit it to a second queue.\n\nThis second queue contains the tasks to be executed by worker processes. Results of the target task functions are then placed in a result queue to be read by the worker thread and made available to any associated Future objects.\n\nThis decoupling between the queue that the user interacts with and the queue of tasks that the processes interact with is intentional and provides some measure of control and safety.\n\nThe important aspect about these internals from a user perspective is in cancelling tasks via their Future objects.\n\nTasks in the internal work queue can be canceled. Otherwise tasks cannot be cancelled, such as running tasks. Additionally, tasks in the internal call queue cannot be cancelled, but may not yet be running.\n\nThis means we may query the status of a Future object and see that it is not running and is not done. Then attempt to cancel it, fail to cancel it, and see that the task begins running and is then done.\n\nThis happens because some number of tasks will sit in the call queue waiting to be consumed by the processes.\n\nSpecifically, the call queue will match the number of worker processes, plus one.\n\nWorker processes are not created when the process pool is created.\n\nInstead, worker processes are created on demand or just-in-time.\n\nEach time a task is added to the internal queues, the process pool will check if the number of active processes is less than the upper limit of processes supported by the pool. If so, an additional process is created to handle the new work.\n\nOnce a process has completed a task, it will wait on the call queue for new work to arrive. As new work arrives, all processes waiting on the queue will be notified and one will attempt to consume the unit of work and start executing it.\n\nThese two points show how the pool will only ever create new processes until the limit is reached and how processes will be reused, waiting for new tasks without consuming computational resources.\n\nIt also shows that the process pool will not release worker processes after a fixed number of units of work. Perhaps this would be a nice addition to the API in the future.\n\nNow that we understand how work is injected into the process pool and how the pool manages processes, let’s look at some best practices to consider when using the ProcessPoolExecutor.\n\nNow that we know how the ProcessPoolExecutor works and how to use it, let’s review some best practices to consider when bringing process pools into our Python programs.\n\nTo keep things simple, there are five best practices; they are:\n• 5. Use for CPU-Bound Tasks (probably)\n\nUse the context manager when using process pools and handle all task dispatching to the process pool and processing results within the manager.\n\nRemember to configure your process pool when creating it in the context manager, specifically by setting the number of processes to use in the pool.\n\nUsing the context manager avoids the situation where you have explicitly instantiated the process pool and forget to shut it down manually by calling shutdown().\n\nIt is also less code and better grouped than managing instantiation and shutdown manually; for example:\n\nDon’t use the context manager when you need to dispatch tasks and get results over a broader context (e.g. multiple functions) and/or when you have more control over the shutdown of the pool.\n\nIf you have a for-loop that applies a function to each item in a list, then use the map() function to dispatch the tasks asynchronously.\n\nFor example, you may have a for-loop over a list that calls myfunc() for each item:\n\nOr, you may already be using the built-in map() function:\n\nBoth of these cases can be made asynchronous using the map() function on the process pool.\n\nProbably do not use the map() function if your target task function has side effects.\n\nDo not use the map() function if your target task function has no arguments or more than one argument, unless all arguments can come from parallel iterables (i.e. map() can take multiple iterables).\n\nDo not use the map() function if you need control over exception handling for each task, or if you would like to get results to tasks in the order that tasks are completed.\n\nIf you would like to process results in the order that tasks are completed rather than the order that tasks are submitted, then use submit() and as_completed().\n\nThe submit() function is in the process pool and is used to push tasks into the pool for execution and returns immediately with a Future object for the task. The as_completed() function is a module method that will take an iterable of Future objects, like a list, and will return Future objects as the tasks are completed.\n\nDo not use the submit() and as_completed() combination if you need to process the results in the order that the tasks were submitted to the process pool; instead, use map() or use submit() and iterate the Future objects directly.\n\nDo not use the submit() and as_completed() combination if you need results from all tasks to continue; instead, you may be better off using the wait() module function.\n\nDo not use the submit() and as_completed() combination for a simple asynchronous for-loop; instead, you may be better off using map().\n\nUse the ProcessPoolExecutor if your tasks are independent.\n\nThis means that each task is not dependent on other tasks that could execute at the same time. It also may mean tasks that are not dependent on any data other than data provided via function arguments to the task.\n\nThe ProcessPoolExecutor is ideal for tasks that do not change any data, e.g. have no side effects, so-called pure functions.\n\nProcess pools can be organized into data flows and pipelines for linear dependence between tasks, perhaps with one process pool per task type.\n\nThe process pool is not designed for tasks that require coordination; you should consider using the Process class and coordination patterns like the Barrier and Semaphore.\n\nProcess pools are not designed for tasks that require synchronization, you should consider using the Process class and locking patterns like Lock and RLock via a Manager.\n\nUse for CPU-Bound Tasks (probably)\n\nThe ProcessPoolExecutor can be used for IO-bound tasks and CPU-bound tasks.\n\nNevertheless, it is probably best suited for CPU-bound tasks, whereas the ThreadPoolExecutor is probably best suited for IO-bound tasks.\n\nCPU-bound tasks are those tasks that involve direct computation, e.g. executing instructions on data in the CPU. They are bound by the speed of execution of the CPU, hence the name CPU-bound.\n\nThis is unlike IO-bound tasks that must wait on external resources such as reading or writing to or from network connections and files.\n\nExamples of common CPU-bound tasks that may be well suited to the ProcessPoolExecutor include:\n• Search, e.g. searching for a keyword in a large number of documents.\n\nThe ProcessPoolExecutor can be used for IO bound tasks, but it is probably a less suited fit compared to using threads and the ThreadPoolExecutor.\n\nThis is because of two reasons:\n• You can have more threads than processes.\n\nThe number of processes you can create and manage is often quite limited, such as tens or less than 100.\n\nWhereas, when you are using threads, you can have hundreds of threads or even thousands of threads within one process. This is helpful for IO operations that many need to access or manage a large number of connections or resources concurrently.\n\nThis can be pushed to tens of thousands of connections or resources or even higher when using AsyncIO.\n\nThis may be data read or written from or to remote connections, databases, servers, files, external devices, and so on.\n\nAs such, if the data needs to be shared between processes, such as in a pipeline, it may require that the data be serialized (called pickled, the built-in Python serialization process) in order to pass from process to process. This can be slow and very memory intensive, especially for large amounts of data.\n\nThis is not the case when using threads that can share and access the same resource in memory without data serialization.\n\nThere are a number of common errors when using the ProcessPoolExecutor.\n\nThese errors are typically made because of bugs introduced by copy-and-pasting code, or from a slight misunderstanding in how the ProcessPoolExecutor works.\n\nWe will take a closer look at some of the more common errors made when using the ProcessPoolExecutor, such as:\n• Arguments or Shared Data that Does Not Pickle\n\nDo you have an error using the ProcessPoolExecutor?\n\nLet me know in the comments so I can recommend a fix and add the case to this section.\n\nBy far, the biggest error when using the ProcessPoolExecutor is forgetting to check for the __main__ module.\n\nRecall that when using processes in Python, such as the Process class or the ProcessPoolExecutor, we must include a check for the top-level environment. This is achieved by checking if the module name __name__ is equal to the string ‘__main__‘.\n\nThis indicates that the code is running at the top-level code environment, rather than being imported by a program or script.\n\nYou can learn more about __main__ more generally here:\n\nForgetting the main function will result in an error that can be quite confusing.\n\nA complete example of using the ProcessPoolExecutor without a check for the __main__ module is listed below.\n\nRunning this example will fail with a RuntimeError.\n\nThe error message does include information about the need to import an entry point to the program, but also comments on freeze_support, which can be confusing for beginners as well as a BrokenProcessPool.\n\nThis error can be fixed by protecting the entry point of the program with an if-statement:\n\nA common error is to call your function when using the submit() function.\n\nA complete example with this error is listed below.\n\nRunning this example will fail with an error.\n\nYou can fix the error by updating the call to submit() to take the name of your function and any arguments, instead of calling the function in the call to submit.\n\nA common error is to call your function when using the map() function.\n\nA complete example with this error is listed below.\n\nRunning the example results in a TypeError.\n\nThis error can be fixed by changing the call to map() to pass the name of the target task function instead of a call to the function.\n\nAnother common error when using map() is to provide no second argument to function, e.g. the iterable.\n\nA complete example with this error is listed below.\n\nRunning the example does not issue any tasks to the process pool as there was no iterable for the map() function to iterate over.\n\nIn this case, no output is displayed.\n\nThe fix involves providing an iterable in the call to map() along with your function name.\n\nAnother common error is to forget to include the Future in the signature for the callback function registered with a Future object.\n\nA complete example with this error is listed below.\n\nRunning this example will result in an error when the callback is called by the thread pool.\n\nFixing this error involves updating the signature of your callback function to include the future object.\n\nArguments or Shared Data that Does Not Pickle\n\nA common error is sharing data between processes that cannot be serialized.\n\nPython has a built-in object serialization process called pickle, where objects are pickled or unpickled when serialized and unserialized.\n\nWhen sharing data between processes, the data will be pickled automatically.\n\nThis includes arguments passed to target task functions, data returned from target task functions, and data accessed directly, such as global variables.\n\nIf data shared between processes cannot be automatically pickled, a PicklingError will be raised.\n\nMost normal Python objects can be pickled.\n\nExamples of objects that cannot pickle are those that might have an open connection, such as to a file, database, server, or similar.\n\nWe can demonstrate this with an example below that attempts to pass a file handle as an argument to a target task function.\n\nRunning the example, we can see that it fails with an error indicating that the argument cannot be pickled for transmission to the worker process.\n\nThis was a contrived example, where the file could be opened within the target task function or the ProcessPoolExecutor could be changed to a ThreadPoolExecutor.\n\nIn general, if you experience this error and you are attempting to pass around a connection or open file, perhaps try to open the connection within the task or use threads instead of processes.\n\nIf you experience this type of error with custom data types that are being passed around, you may need to implement code to manually serialize and deserialize your types. I recommend reading the documentation for the pickle module.\n\nA common error is to not flush standard out (stdout) when calling the built-in print() statement from target task functions.\n\nBy default, the built-in print() statement in Python does not flush output.\n\nThe standard output stream (stout) will flush automatically in the main process, often when the internal buffer is full or a new line is detected. This means you see your print statements reported almost immediately after the print function is called in code.\n\nThere is a problem when calling the print() function from spawned or forked processes because standard out (stdout) will buffer output by default.\n\nThis means if you call print() from target task functions in the ProcessPoolExecutor, you probably will not see the print statements on standard out until the worker processes are closed.\n\nThis will be confusing because it will look like your program is not working correctly, e.g. buggy.\n\nThe example below demonstrates this with a target task function that will call print() to report some status.\n\nRunning the example will wait until all tasks in the process pool have completed before printing all messages on standard out.\n\nThis can be fixed by updating all calls to the print() statement called from target task functions to flush output after each call.\n\nThis can be achieved by setting the “flush” argument to True; for example:\n\nCommon Questions When Using the ProcessPoolExecutor\n\nThis section answers common questions asked by developers when using the ProcessPoolExecutor.\n\nDo you have a question about the ProcessPoolExecutor?\n\nAsk your question in the comments below and I will do my best to answer it and perhaps add it to this list of questions.\n\nHow Do You Stop a Running Task?\n\nA task in the ProcessPoolExecutor can be cancelled before it has started running.\n\nIn this case, the task must have been sent into the pool by calling submit(), which returns a Future object. You can then call the cancel() function in the Future.\n\nIf the task is already running, it cannot be canceled, stopped, or terminated by the process pool.\n\nInstead, you must add this functionality to your task.\n\nOne approach might be to use a flag, like a multiprocessing.Event object, that if set will indicate that all tasks must stop running as soon as they can. You can then update your target task function or functions to check the state of this flag frequently.\n\nWe cannot use an Event directly; instead, we must use a Manager to create the Event instance, then share the instance with each task so that the worker processes can access it correctly. The Manager will create a server process to manage the inter-process coordination of shared objects, like an Event.\n\nThis can be achieved by first creating the Manager, then creating the event from the manager using the context manager technique so that the Manager is correctly closed once we are finished with it.\n\nWe can then pass the event to the target task function and check it frequently.\n\nIt may require that you change the structure of your task.\n\nFor example, if your task reads data from a file or a socket, you may need to change the read operation to be performed in blocks of data in a loop so that each iteration of the loop you can check the status of the flag.\n\nThe example below provides a template you can use for adding an event flag to your target task function to check for a stop condition to shutdown all currently running tasks.\n\nThe example below demonstrates this with a worked example.\n\nRunning the example first creates the Manager and Event followed by the process pool with the default number of worker processes and schedules 50 tasks.\n\nThe Event object is passed to each task, where it is checked each iteration to see if it has been set and if so to bail out of the task.\n\nThe first 9 tasks start executing for a few seconds, then we decide to shut everything down.\n\nNote: in this case, we have 8 CPUs and 1 extract task is added to the internal call Queue and cannot be cancelled. See the section on how the ProcessPoolExecutor works internally to see why this is the case.\n\nFirst, we cancel all scheduled tasks that are not yet running so that if they make it off the queue into a worker process, they will not start running.\n\nWe then set the event to trigger all running tasks to stop.\n\nThe process pool is then shut down and we wait for all running processes to complete their execution.\n\nThe running processes check the status of the event in their next loop iteration and bail out, printing a message.\n\nHow Do You Wait for All Tasks to Complete?\n\nThere are a few ways to wait for all tasks to complete in a ProcessPoolExecutor.\n\nFirstly, if you have a Future object for all tasks in the process pool because you called submit(), then you can provide the collection of tasks to the wait() module function. By default, this function will return when all provided Future objects have completed.\n\nAlternatively, you can enumerate the list of Future objects and attempt to get the result from each. This iteration will complete when all results are available, meaning that all tasks were completed.\n\nAnother approach is to shut down the process pool. We can set “cancel_futures” to True, which will cancel all scheduled tasks and wait for all currently running tasks to complete.\n\nYou can also shut down the pool and not cancel the scheduled tasks, yet still wait for all tasks to complete. This will ensure all running and scheduled tasks are completed before the function returns. This is the default behavior of the shutdown function, but is a good idea to specify explicitly.\n\nHow Do You Dynamically Change the Number of Processes\n\nYou cannot dynamically increase or decrease the number of processes in a ProcessPoolExecutor.\n\nThe number of processes is fixed when the ProcessPoolExecutor is configured in the call to the object constructor. For example:\n\nHow Do You Unit Tasks and Process Pools?\n\nYou can unit test your target task functions directly, perhaps mocking any external resources required.\n\nYou can unit test your usage of the process pool with mock tasks that do not interact with external resources.\n\nUnit testing of tasks and the process pool itself must be considered as part of your design and may require that connection to the IO resource be configurable so that it can be mocked, and that the target task function called by your process pool is configurable so that it can be mocked.\n\nHow Do You Compare Serial to Parallel Performance?\n\nYou can compare the performance of your program with and without the process pool.\n\nThis can be a useful exercise to confirm that making use of the ProcessPoolExecutor in your program has resulted in a speed-up.\n\nPerhaps the simplest approach is to manually record the start and end time of your code and subtract the end from the start time to report the total execution time. Then record the time with and without the use of the process pool.\n\nUsing an average program execution time might give a more stable program timing than a one-off run.\n\nYou can record the execution time 3 or more times for your program without the process pool then calculate the average as the sum of times divided by the total runs. Then repeat this exercise to calculate the average time with the process pool.\n\nThis would probably only be appropriate if the running time of your program is minutes rather than hours.\n\nYou can then compare the serial vs. parallel version by calculating the speed up multiple as:\n\nFor example, if the serial run of a program took 15 minutes (900 seconds) and the parallel version with a ProcessPoolExecutor took 5 minutes (300 seconds), then the percentage multiple up would be calculated as:\n\nThat is, the parallel version of the program with the ProcessPoolExecutor is 3 times faster or 3x faster.\n\nYou can multiply the speed-up multiple by 100 to give a percentage\n\nIn this example, the parallel version is 300% faster than the serial version.\n\nHow Do You Set chunksize in map()?\n\nThe map() function on the ProcessPoolExecutor takes a parameter called “chunksize“, which defaults to 1.\n\nThe “chunksize” argument controls the mapping of items in the iterable passed to map to tasks used in the ProcessPoolExecutor executor.\n\nA value of one means that one item is mapped to one task.\n\nRecall that the data for each task in terms of arguments sent to the target task function and values that are returned must be serialized by pickle. This happens automatically, but incurs some computational and memory cost, adding overhead to each task processed by the process pool.\n\nWhen there are a vast number of tasks or tasks are relatively quick to compute, then the chunksize should be tuned to maximize the grouping of items to process pool tasks in order to minimize the overhead per task and in turn reduce the overall compute time.\n\nThis will likely require some tuning of the chunksize that you may be able to perform with real task data, or perhaps a test harness with mock data and task processes.\n\nSome values to try might include:\n• 1: The default mapping of one item per task.\n• items / max_workers: Splits all items into max_workers groups, e.g. one batch of items per process.\n\nNote: the (items / max_workers) division may need to be rounded as the “chunksize” argument must be a positive integer.\n\nCompare the performance to see if one configuration is better than another, and if so, use it as a starting point for similar values to evaluate.\n\nHow Do You Submit a Follow-up Task?\n\nSome tasks require that a second task be executed that makes use of the result from the first task in some way.\n\nWe might call this the need to execute a follow-up task for each task that is submitted, which might be conditional on the result in some way.\n\nThere are a few ways to submit a follow-up task.\n\nOne approach would be to submit the follow-up task as we are processing the results from the first task.\n\nFor example, we can process the result from each of the first tasks as they complete, then manually call submit() for each follow-up task when needed and store the new future object in a second list for later use.\n\nWe can make this example of submitting follow-up tasks concrete with a full example.\n\nRunning the example starts a process pool with 5 worker threads and submits 10 tasks.\n\nWe then process the results for the tasks as they are completed. If a result from the first round of tasks requires a follow-up task, we submit the follow-up task and keep track of the Future object in a second list.\n\nThese follow-up tasks are submitted as needed, rather than waiting until all first round tasks are completed, which is a nice benefit of using the as_completed() function with a list of Future objects.\n\nWe can see that in this case, five first round tasks resulted in follow-up tasks.\n\nYou might like to use a separate process pool for follow-up tasks, to keep things separate.\n\nI would not recommend submitting new tasks from within a task.\n\nThis would require access to the process pool either as a global variable or by being passed in and would break the idea of tasks being pure functions that don’t have side effects, a good practice when using process pools.\n\nAdditionally, interacting with tasks within tasks, e.g. waiting on the results from Future objects, can lead to deadlocks, which will halt the execution of your program.\n\nHow Do You Show Progress of All Tasks?\n\nThere are many ways to show progress for tasks that are being executed by the ProcessPoolExecutor.\n\nPerhaps the simplest is to use a callback function that updates a progress indicator. This can be achieved by defining the progress indicator function and registering it with the Future object for each task via the add_done_callback() function.\n\nThe simplest progress indicator is to print a dot to the screen for each task that completes.\n\nThe example below demonstrates this simple progress indicator.\n\nRunning the example starts a process pool with two worker processes and dispatches 20 tasks.\n\nA progress indicator callback function is registered with each Future object that prints one dot as each task is completed, ensuring that the standard output is flushed with each call to print() and that no new line is printed.\n\nThis ensures that each we see the dot immediately regardless of the process that prints and that all dots appear on one line.\n\nEach task will work for a variable amount of time less than one second and a dot is printed once the task is completed.\n\nA more elaborate progress indicator must know the total number of tasks and will use a process-safe counter to update the status of the number of tasks completed out of all tasks to be completed.\n\nDo We Need to Protect __main__?\n\nWhen using processes in general, and the ProcessPoolExecutor in particular, you need to explicitly protect the entry point with the if-statement:\n\nDo I Need to Call freeze_support()?\n\nPython programs can be converted into a Windows executable.\n\nIn the conversion process, the Python programs are “frozen.” If these programs attempt to start new processes, it will result in a RuntimeError.\n\nAs such, if you intend to “freeze” your program (e.g. convert it to be a Windows executable), you must add freeze support.\n\nThis can be achieved by calling the freeze_support() function as the first line of your program, such the first line after checking for the protected entry point; for example:\n\nHow Do You Get a Future Object for Tasks Added With map()?\n\nWhen you call map(), it does create a Future object for each task.\n\nInternally, submit() is called for each item in the iterable provided to the call to map().\n\nNevertheless, there is no clean way to access the Future object for tasks sent into the process pool via map().\n\nEach task on the ProcessPoolExecutor object’s internal work queue is an instance of a _WorkItem that does have a reference to the Future object for the task. You can access the ProcessPoolExecutor object’s internal queue, but you have no safe way of enumerating items in the queue without removing them.\n\nIf you need a Future object for a task, call submit().\n\nHow to Check How Many Tasks Remain in the ProcessPoolExecutor\n\nThere are two ways you can estimate the number of tasks that remain in the process pool.\n• 1. Keep track of how many tasks were submitted and how many have completed.\n• 2. Query the process pool directly for the number of tasks that remain.\n\nIn the first case, you can keep track of the number of tasks submitted with a counter from the main thread, e.g. if all tasks are submitted from the main thread.\n\nYou could then use a process-safe counter in a Future callback to keep track of the number of tasks that are done. This could be used as a crude progress indicator.\n\nMore simply, you can query into the protected members of the ProcessPoolExecutor directly in order to find out how many tasks remain to be processed.\n\nPerhaps a simple structure to check internal to the process pool is the dictionary that maps work item identifiers to work items, e.g. the tasks that have been submitted but not yet dispatched to processes for execution. This is in the _pending_work_items protected property.\n\nWe can report the size of this dictionary directly; for example:\n\nWe could report the number of remaining tasks as each submitted task is completed, e.g. via the as_completed() function for a list of Future objects; for example:\n\nTying this together, the complete example below submits 50 tasks into a process pool with four worker processes and reports the number of tasks that remain as each task finishes.\n\nRunning the example executes the tasks quickly and provides an updated report on the number of tasks that remain as each task is completed.\n\nThe ProcessPoolExecutor may not be the best solution for all concurrency problems in your program.\n\nThat being said, there may also be some misunderstandings that are preventing you from making full and best use of the capabilities of the ProcessPoolExecutor in your program.\n\nIn this section, we review some of the common objections seen by developers when considering using the ProcessPoolExecutor in their code\n\nWhat About The Global Interpreter Lock (GIL)?\n\nThe GIL is generally not relevant when using processes such as the Process class or the ProcessPoolExecutor class.\n\nThe Global Interpreter Lock, or GIL for short, is a design decision with the reference Python interpreter.\n\nIt refers to the fact that the implementation of the Python interpreter makes use of a master lock that prevents more than one Python instruction executing at the same time.\n\nThis prevents more than one thread of execution within Python programs, specifically within each Python process, that is each instance of the Python interpreter.\n\nThe implementation of the GIL means that Python threads may be concurrent, but cannot run in parallel. Recall that concurrent means that more than one task can be in progress at the same time, parallel means more than one task actually executing at the same time. Parallel tasks are concurrent;, concurrent tasks may or may not execute in parallel.\n\nIt is the reason behind the heuristic that Python threads should only be used for IO-bound tasks, and not CPU-bound tasks, as IO-bound tasks will wait in the operating system kernel for remote resources to respond (not executing Python instructions), allowing other Python threads to run and execute Python instructions.\n\nAs such, the GIL is a consideration when using threads in Python such as the threading.Thread class and the ThreadPoolExecutor. It is not a consideration when using the ProcessPoolExecutor (unless you use additional threads within each task).\n\nPython makes use of real system-level processes, also called spawning processes or forking processes, a capability provided by modern operating systems like Windows, Linux, and MacOS.\n\nPython processes are a first-class capability of the Python platform and have been for a very long time.\n\nDevelopers love python for many reasons, most commonly because it is easy to use and fast for development.\n\nPython is commonly used for glue code, one-off scripts, but more and more for large scale software systems.\n\nIf you are using Python and then you need concurrency, then you work with what you have. The question is moot.\n\nIf you need concurrency and you have not chosen a language, perhaps another language would be more appropriate, or perhaps not. Consider the full scope of functional and non-functional requirements (or user needs, wants, and desires) for your project and the capabilities of different development platforms.\n\nWhy Not Use The ThreadPoolExecutor Instead?\n\nThe ThreadPoolExecutor supports pools of threads, unlike the ProcessPoolExecutor that supports pools of processes, where each process will have one thread.\n\nThreads and processes are quite different and choosing one over the other must be quite intentional.\n\nA Python program is a process that has a main thread. You can create many additional threads in a Python process. You can also fork or spawn many Python processes, each of which will have one main thread, and may spawn additional threads.\n\nMore broadly, threads are lightweight and can share memory (data and variables) within a process whereas processes are heavyweight and require more overhead and impose more limits on sharing memory (data and variables).\n\nTypically in Python, processes are used for CPU-bound tasks and threads are used for IO-bound tasks, and this is a good heuristic, but this does not have to be the case.\n\nPerhaps ThreadPoolExecutor is a better fit for your specific problem. Perhaps try it and see.\n\nWhy Not Use multiprocessing.Process Instead?\n\nThe ProcessPoolExecutor is like the “auto mode” for Python processes.\n\nIf you have a more sophisticated use case, you may need to use the multiprocessing.Process class directly.\n\nThis may be because you require more synchronization between processes with locking mechanisms, shared memory between processes such as a manager, and/or more coordination between processes with barriers and semaphores.\n\nIt may be that you have a simpler use case, such as a single task, in which case perhaps a process pool would be too heavy a solution.\n\nThat being said, if you find yourself using the Process class with the “target” keyword for pure functions (functions that don’t have side effects), perhaps you would be better suited to using the ProcessPoolExecutor.\n\nYou can learn more about multiprocessing in the tutorial:\n\nWhy Not Use AsyncIO?\n\nAsyncIO can be an alternative to using a ThreadPoolExecutor, but is probably not a good alternative for the ProcessPoolExecutor.\n\nAsyncIO is designed to support large numbers of IO operations, perhaps thousands to tens of thousands, all within a single Thread.\n\nIt requires an alternate programming paradigm, called reactive programming, which can be challenging for beginners.\n\nWhen using the ProcessPoolExecutor, you are typically executing CPU-bound tasks, which are not appropriate when using the AsyncIO module.\n\nThis section provides additional resources that you may find helpful.\n\nI also recommend specific chapters from the following books:\n\nThis is a large guide and you have discovered in great detail how the ProcessPoolExecutor works and how to best use it on your project.\n\nDid you find this guide useful?\n\nI’d love to know. Please share a kind word in the comments below.\n\nHave you used the ProcessPoolExecutor on a project?\n\nI’d love to hear about it; please let me know in the comments.\n\nDo you have any questions?\n\nLeave your question in a comment below and I will reply fast with my best advice."
    },
    {
        "link": "https://stackoverflow.com/questions/75263023/how-can-i-use-pythons-concurrent-futures-to-queue-tasks-across-multiple-process",
        "document": "If I understand what you are trying to do you basically have a lot of jobs that are suitable for multithreading except that there is some CPU-intensive work. So your idea is to create multiple threading pools in multiple child processes so that there is less GIL contention. Of course, in a any given child process the CPU-intensive code will only be executed serially (assuming it is Python byte code), so it's not a perfect solution.\n\nOne approach is to just create a very large multiprocessing pool (larger than the number of cores you have). There is a limit to how may processes you can create and their creation is expensive. But since most of the time they will be waiting for I/O to complete the I/O portion will execute concurrently.\n\nA better approach would be to create a multiprocessing pool whose executor can be passed to a multithreading pool worker function along with the other required arguments. This is an inversion of what you were planning to do. When the worker function has a CPU-intensive work to perform, it can submit that work to the passed multiprocessing pool executor and block for the returned result. In that way you get the optimal parallelism you can achieve given the number of cores you have. This would be my recommendation.. For example:\n\nBut if you wanted to go along with your original idea or for some reason the above framework does not fit your actual situation, perhaps something like the following might work:"
    }
]