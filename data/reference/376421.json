[
    {
        "link": "https://tutorialspoint.com/computer_graphics/computer_graphics_orthographic_projection.htm",
        "document": "In the previous chapter, we explained the concept of parallel projections. There are multiple such projections available in parallel projections; we can broadly divide them into orthographic and oblique projections. There are still some variations in orthographic projections.\n\nIn this chapter, we will see what orthographic projection is, how it works, and its different types. We will also cover the mathematical concepts behind orthographic projection, with examples for a better understanding.\n\nAs we know from the overview, the orthographic projection is a method of representing three-dimensional objects in two-dimensional space or screen. Unlike perspective projection, which mimics how the human eye sees objects (with far objects appearing smaller). The orthographic projection preserves the true size of the objects, regardless of their distance from the viewer. This makes it useful in applications where accuracy and scale are crucial.\n\nIn an orthographic projection, parallel lines in the 3D space remain parallel in the 2D representation, without any distortion. The objects are projected along lines that are perpendicular to the projection plane.\n\nImagine we have a complex shape in 3D space. The 3D shape can be seen differently in different sides. The following figure will give you the insight of it.\n\nThere are three main types of orthographic projection, multi-view projection, axonometric projection, and oblique projection. Each type is used for different purposes and provides a unique way of representing 3D objects in 2D space. Here we will see orthographic Multiview and axonometric projections. In the next article we will cover oblique in detail.\n\nMulti-view projection is the most commonly used type of orthographic projection. This is used in technical drawings and engineering designs. In this method, multiple views of the object are projected onto different planes. These views are typically −\n\nThe above figure is showing these in action, in three panels.\n\nEach of these views represents the object as seen from a different angle. The object is projected onto the projection plane along lines perpendicular to the plane. This ensures the true size and shape of each part of the object are preserved.\n\nOn the other hand the axonometric projection is another type of orthographic projection, where the object is viewed from a skewed angle. Here it is allowing multiple faces of the object to be seen in a single image. The object is rotated along one or more axes. This is providing a view that shows the object's depth as well as its height and width. Axonometric projection is useful for visualizing 3D objects without the complexity of perspective distortion.\n\nThere are three main types of axonometric projection −\n\nLet us see these projections with examples a little\n\nIn isometric projection, the object is rotated so that the angles between the projection of the x, y, and z axes are all equal (typically 120 degrees). This results in a view where all three axes are equally foreshortened, giving a balanced view of the object. See the following example −\n\nIn isometric projection, all three dimensions (height, width, and depth) are represented equally, which gives a clear and simple visualization of the object. The scale remains consistent, and no perspective distortion occurs.\n\nIn dimetric projection, two of the three axes are equally foreshortened, while the third axis is foreshortened by a different amount. This creates a view where two dimensions appear more balanced, while the third dimension is scaled differently. See the same example but in dimetric projection.\n\nIn trimetric projection, all three axes are foreshortened by different amounts. This allows for a more flexible representation of the object, though it may be more challenging to interpret compared to isometric or dimetric projections. Let us see the example below −\n\nLet us see the mathematical details of orthographic projection. Here only we see the basics of them, for each projections, angles will be different. We need matrices to represent the projections. These matrices are used to transform 3D coordinates into 2D coordinates on the projection plane.\n\nThe general form of an orthographic projection matrix is as follows, with homogeneous coordinates.\n\nThis matrix projects the 3D object onto the 2D plane by ignoring the z-coordinate, which represents depth.\n\nIn addition to the basic projection matrix, scaling factors may be applied to adjust the view. The scaling factors S , S , and S can be used to scale the object along the x, y, and z axes, respectively. The projection matrix with scaling becomes −\n\nBy applying this matrix, we can scale the object while projecting it onto the 2D plane.\n\nIn axonometric projection, where the object is rotated before projection, we use rotation matrices to rotate the object along the x, y, or z axes before applying the orthographic projection matrix. For example, the rotation matrix around the z-axis is given by −\n\nBy multiplying the projection matrix by the rotation matrix, we can create rotated views of the object in axonometric projection.\n\nOrthographic projection is widely used in various fields of computer graphics, engineering, and design. Some common applications include −\n• Technical Drawings − Orthographic projection is essential for creating accurate technical drawings and blueprints where precise measurements and dimensions are needed.\n• Computer-Aided Design (CAD) − CAD software often uses orthographic projection to represent 3D models in 2D views.\n• Game Development− In 2D and isometric games, orthographic projection is used to display objects without distortion.\n• Architecture− Architects use orthographic projection to create accurate floor plans and building designs.\n\nIn this chapter, we covered the concept of orthographic projection in computer graphics. We explained how it works and its importance in representing 3D objects in a 2D space. We discussed the main two types of orthographic projection: multi-view projection and axonometric projection. Each of these projections serve different purposes and provide unique ways to represent 3D objects.\n\nWe also covered the mathematics behind orthographic projection, using projection matrices and transformations like scaling and rotation to accurately represent objects. Orthographic projection is widely used in fields like CAD, technical drawing, game development, and architecture, where accurate, distortion-free representation is crucial."
    },
    {
        "link": "https://geeksforgeeks.org/parallel-othographic-oblique-projection-in-computer-graphics",
        "document": "Projection is a kind of phenomena that are used in computer graphics to map the view of a 3D object onto the projecting display panel where the viewing volume is specified by the world coordinate and then map these world coordinate over the view port.\n\nProjection is of the following kind :\n\n\n\nParallel Projection : Parallel projection is a kind of projection where the projecting lines emerge parallelly from the polygon surface and then incident parallelly on the plane. In parallel projection, the centre of the projection lies at infinity. In parallel projection, the view of the object obtained at the plane is less-realistic as there is no for-shortcoming. and the relative dimension of the object remains preserves.\n\nParallel projection is further divided into two categories :\n\n\n\n(a) Orthographic Projection : It is a kind of parallel projection where the projecting lines emerge parallelly from the object surface and incident perpendicularly at the projecting plane.\n\nOrthographic Projection is of two categories :\n\n(a).1. Multiview Projection : It is further divided into three categories –\n\n(1) Top-View : In this projection, the rays that emerge from the top of the polygon surface are observed.\n\n2) : It is another type of projection orthographic projection where the side view of the polygon surface is observed.\n\n3) : In this orthographic projection front face view of the object is observed.\n\n\n\na.2) : Axonometric projection is an orthographic projection, where the projection lines are perpendicular to the plane of projection, and the object is rotated around one or more of its axes to show multiple sides.\n\nIt is further divided into three categories :\n\n(1) Isometric Projection : It is a method for visually representing three-dimensional objects in two-dimensional display in technical and engineering drawings. Here in this projection, the three coordinate axes appear equally foreshortened and the angle between any two of them is 120 degrees.\n\n(2) Dimetric Projection : It is a kind of orthographic projection where the visualized object appears to have only two adjacent sides and angles are equal.\n\n(3) Trimetric Projection : It is a kind of orthographic projection where the visualized object appears to have all the adjacent sides and angles unequal.\n\n(b) Oblique Projection : It is a kind of parallel projection where projecting rays emerges parallelly from the surface of the polygon and incident at an angle other than 90 degrees on the plane.\n\nIt is of two kinds :\n\n(b).1. Cavalier Projection : It is a kind of oblique projection where the projecting lines emerge parallelly from the object surface and incident at 45‘ rather than 90′ at the projecting plane. In this projection, the length of the reading axis is larger than the cabinet projection.\n\n(b). 2. Cabinet Projection : It is similar to that cavalier projection but here the length of reading axes just half than the cavalier projection and the incident angle at the projecting plane is 63.4′ rather 45′."
    },
    {
        "link": "https://courses.grainger.illinois.edu/cs418/sp2018/slides/CS418-Lecture9-OrthoProjection.pdf",
        "document": ""
    },
    {
        "link": "https://en.wikipedia.org/wiki/Orthographic_projection",
        "document": "Means of projecting three-dimensional objects in two dimensions\n\nOrthographic projection (also orthogonal projection and analemma)[a] is a means of representing three-dimensional objects in two dimensions. Orthographic projection is a form of parallel projection in which all the projection lines are orthogonal to the projection plane,[2] resulting in every plane of the scene appearing in affine transformation on the viewing surface. The obverse of an orthographic projection is an oblique projection, which is a parallel projection in which the projection lines are not orthogonal to the projection plane.\n\nThe term orthographic sometimes means a technique in multiview projection in which principal axes or the planes of the subject are also parallel with the projection plane to create the primary views.[2] If the principal planes or axes of an object in an orthographic projection are not parallel with the projection plane, the depiction is called axonometric or an auxiliary views. (Axonometric projection is synonymous with parallel projection.) Sub-types of primary views include plans, elevations, and sections; sub-types of auxiliary views include isometric, dimetric, and trimetric projections.\n\nA lens that provides an orthographic projection is an object-space telecentric lens.\n\nA simple orthographic projection onto the plane z = 0 can be defined by the following matrix:\n\nFor each point v = (v , v , v ), the transformed point Pv would be\n\nOften, it is more useful to use homogeneous coordinates. The transformation above can be represented for homogeneous coordinates as\n\nFor each homogeneous vector v = (v , v , v , 1), the transformed vector Pv would be\n\nIn computer graphics, one of the most common matrices used for orthographic projection can be defined by a 6-tuple, (left, right, bottom, top, near, far), which defines the clipping planes. These planes form a box with the minimum corner at (left, bottom, -near) and the maximum corner at (right, top, -far).[3]\n\nThe box is translated so that its center is at the origin, then it is scaled to the unit cube which is defined by having a minimum corner at (−1,−1,−1) and a maximum corner at (1,1,1).\n\nThe orthographic transform can be given by the following matrix:\n\nwhich can be given as a scaling S followed by a translation T of the form\n\nThe inversion of the projection matrix P−1, which can be used as the unprojection matrix is defined:\n\nThree sub-types of orthographic projection are isometric projection, dimetric projection, and trimetric projection, depending on the exact angle at which the view deviates from the orthogonal.[2][4] Typically in axonometric drawing, as in other types of pictorials, one axis of space is shown to be vertical.\n\nIn isometric projection, the most commonly used form of axonometric projection in engineering drawing,[5] the direction of viewing is such that the three axes of space appear equally foreshortened, and there is a common angle of 120° between them. As the distortion caused by foreshortening is uniform, the proportionality between lengths is preserved, and the axes share a common scale; this eases one's ability to take measurements directly from the drawing. Another advantage is that 120° angles are easily constructed using only a compass and straightedge.\n\nIn dimetric projection, the direction of viewing is such that two of the three axes of space appear equally foreshortened, of which the attendant scale and angles of presentation are determined according to the angle of viewing; the scale of the third direction is determined separately.\n\nIn trimetric projection, the direction of viewing is such that all of the three axes of space appear unequally foreshortened. The scale along each of the three axes and the angles among them are determined separately as dictated by the angle of viewing. Trimetric perspective is seldom used in technical drawings.[4]\n\nIn multiview projection, up to six pictures of an object are produced, called primary views, with each projection plane parallel to one of the coordinate axes of the object. The views are positioned relative to each other according to either of two schemes: first-angle or third-angle projection. In each, the appearances of views may be thought of as being projected onto planes that form a six-sided box around the object. Although six different sides can be drawn, usually three views of a drawing give enough information to make a three-dimensional object. These views are known as front view (also elevation), top view (also plan) and end view (also section). When the plane or axis of the object depicted is not parallel to the projection plane, and where multiple sides of an object are visible in the same image, it is called an auxiliary view. Thus isometric projection, dimetric projection and trimetric projection would be considered auxiliary views in multiview projection. A typical characteristic of multiview projection is that one axis of space is usually displayed as vertical.\n\nAn orthographic projection map is a map projection of cartography. Like the stereographic projection and gnomonic projection, orthographic projection is a perspective (or azimuthal) projection, in which the sphere is projected onto a tangent plane or secant plane. The point of perspective for the orthographic projection is at infinite distance. It depicts a hemisphere of the globe as it appears from outer space, where the horizon is a great circle. The shapes and areas are distorted, particularly near the edges.[6][7]\n\nThe orthographic projection has been known since antiquity, with its cartographic uses being well documented. Hipparchus used the projection in the 2nd century BC to determine the places of star-rise and star-set. In about 14 BC, Roman engineer Marcus Vitruvius Pollio used the projection to construct sundials and to compute sun positions.[7]\n\nVitruvius also seems to have devised the term orthographic – from the Greek orthos (\"straight\") and graphē (\"drawing\") – for the projection. However, the name analemma, which also meant a sundial showing latitude and longitude, was the common name until François d'Aguilon of Antwerp promoted its present name in 1613.[7]\n\nThe earliest surviving maps on the projection appear as woodcut drawings of terrestrial globes of 1509 (anonymous), 1533 and 1551 (Johannes Schöner), and 1524 and 1551 (Apian).[7]"
    },
    {
        "link": "https://scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/projection-matrix-introduction.html",
        "document": "What Are Projection Matrices and Where/Why Are They Used?\n\nHeads Up Before You Dive In\n\nGrasping the material in this lesson requires prior knowledge in several key areas. Make sure you're comfortable with matrices, the process of transforming points between different spaces, understanding perspective projection (including the calculation of 3D point coordinates on a canvas), and the fundamentals of the rasterization algorithm. If any of these topics sound unfamiliar or you need a refresher, we strongly recommend revisiting the preceding lessons and the dedicated Geometry lesson. This foundational understanding will equip you with the necessary tools to fully engage with and benefit from the upcoming content.\n\nProjection matrices are specialized 4x4 matrices designed to transform a 3D point in camera space into its projected counterpart on the canvas. Essentially, when you multiply a 3D point by a projection matrix, you determine its 2D coordinates on the canvas within NDC (Normalized Device Coordinates) space. Recall from earlier discussions that points in NDC space fall within the range [-1, 1], adhering to the conventions of graphics APIs like Direct3D or OpenGL.\n\nIn simpler terms, the act of multiplying a 3D point by a projection matrix achieves what previously required a series of operations, including the perspective divide and remapping from screen space to NDC space. This complex process:\n\nCan now be streamlined into a single point-matrix multiplication, given a projection matrix \\(M_{proj}\\):\n\nThis approach consolidates the previously separate variables—\\(near\\), \\(t\\), \\(b\\), \\(l\\), and \\(r\\)—representing the near clipping plane and the screen's boundaries, into a concise matrix operation. This lesson's goal is to demystify \\(M_{proj}\\), elucidating its construction and how it encapsulates:\n• The remapping from screen space to NDC space.\n\nWe'll delve into how this matrix incorporates various transformation steps, including the near clipping plane and screen dimensions.\n\nIt's crucial to remember that projection matrices are intended for transforming vertices or 3D points, not vectors. Their primary purpose is to project 3D objects' vertices onto the screen, adhering to perspective rules, creating images of these objects. As discussed in the geometry lesson, a point can also be regarded as a matrix—a [1x3] row vector matrix in row-major order, as utilized on Scratchapixel. For matrix multiplication to occur, the number of columns in the left matrix must match the number of rows in the right matrix. This requirement implies that multiplying a 3D point ([1x3]) by a 4x4 matrix directly is not feasible.\n\nThe workaround involves treating points as [1x4] vectors, enabling their multiplication by a 4x4 matrix. The result is another [1x4] matrix, or 4D points with homogeneous coordinates. These coordinates are only directly applicable as 3D points if their fourth component is 1, allowing the first three components to represent a standard 3D Cartesian point. The transition from Homogeneous to Cartesian coordinates, particularly when involving projection matrices, will be discussed in the following chapter. Homogeneous coordinates are generally only considered when dealing with projection matrices, necessitating explicit handling of the fourth coordinate, unlike with standard transformation matrices where it's implicitly managed.\n\nThis differentiation means that 4D points and, by extension, homogeneous coordinates, are primarily encountered in rendering contexts involving projection matrices. Other times, the explicit handling of these coordinates is unnecessary. However, projection matrices introduce complexity due to the clipping process—a critical rendering step—occurring during the point's transformation by the projection matrix. This intermediate process, discussed in subsequent chapters, is often conflated with projection matrices in literature without clear explanation, adding to the confusion. This lesson aims to clarify the function and application of projection matrices, separating them from the intertwined clipping process.\n\nProjection matrices are widely discussed across this site and various specialized forums, yet they remain a source of confusion for many. Their widespread discussion hints at their importance, even though they are often found to be inadequately explained. This lack of comprehensive documentation is puzzling, given the critical role these matrices play. They are integral to the functionality of real-time graphics APIs like OpenGL, Direct3D, WebGL, Vulkan, Metal, and WebGPU, which enjoy widespread use in video gaming and general desktop graphics applications. These APIs serve as a bridge between the software you develop and the graphics processing unit (GPU). It's worth noting, as previously mentioned, that GPUs are designed to execute the rasterization algorithm. Historically, in what's known as the fixed function pipeline, GPUs would utilize a projection matrix to map points from camera space to normalized device coordinates (NDC). However, the GPU itself lacked the capability to construct this matrix, placing the onus on developers to do so.\n\nHowever, this code is now obsolete and provided only for context and to understand the evolution of graphics programming. Modern practices have moved away from such direct use of OpenGL, as these features have been deprecated and OpenGL's prevalence has declined. In contrast, the contemporary \"programmable\" rendering pipeline—found in DirectX, Vulkan, Metal, and newer versions of OpenGL—employs projection matrices differently. Rather than directly mapping points from camera to NDC space, they now transform points to an intermediary \"clip space.\" While the details of clip space can be complex, it's enough to understand that it involves points represented in homogeneous coordinates, a concept referring to the 4-dimensional points discussed previously.\n\nIn the contemporary GPU rendering process within the programmable pipeline, transforming vertices through projection matrix multiplication is carried out in the vertex shader. Essentially, a vertex shader is a concise program designed to convert the vertices of 3D objects from camera space to clip space. It operates by accepting a vertex and a projection matrix (both associated with the shader) and then computes the product of the vertex and the projection matrix. This product is assigned to a predefined global variable, , within OpenGL, signifying the transformed vertex location in clip space. It's important to note that both and the input vertex are defined as , meaning they are treated as points with homogeneous coordinates. Here's a simplified example of how a vertex shader might look:\n\nThe execution of this vertex shader by the GPU is a key step in processing each vertex within the scene. When using graphics APIs like OpenGL or Direct3D, the vertex information and how these vertices interconnect to form the geometry of meshes are stored in the GPU's memory. The GPU then transforms this vertex data from its initial space to clip space. The initial space of these vertices at the time of processing by the vertex shader is determined by how you prepare them:\n• You may opt to pre-transform the vertices from world space to camera space before uploading them to the GPU's memory. This approach ensures that vertices are already in camera space when processed by the vertex shader.\n• Alternatively, you can upload the vertices in world space to the GPU's memory. In this case, alongside the projection matrix (commonly denoted as P or Proj), the shader also needs the world-to-camera transformation matrix (often labeled M or MV, where MV stands for \"model-view\" matrix, indicating a combination of the object-to-world and world-to-camera transformations). Object space refers to the original position of an object before any transformation, while world space is the coordinate system after applying an object-to-world matrix. To properly transform vertices in this scenario, they must first be converted from world to camera space, followed by the projection matrix application. If vertices are in world space upon entering the vertex shader, only the view component (the world-to-camera matrix) is required. The process could be represented in pseudocode as follows:\n\nMany developers have a preference for the latter method, and there are practical reasons behind this choice. While it's possible to combine the world-to-camera matrix with the projection matrix into a single matrix, it's generally advisable to pass them as two distinct matrices to the vertex shader for reasons beyond the scope of this discussion.\n\nIf you're not well-versed in graphics API usage, or if your experience is more with Direct3D than OpenGL, don't fret. The underlying principles are consistent across both, allowing for easy identification of parallels. The essence of the code is to configure the projection and model-view matrices (merging object-to-world and world-to-camera transformations) and to combine these matrices. This approach simplifies the process, requiring only a single matrix to be passed to the vertex shader. In OpenGL, identifying the shader variable's location necessitates using the function.\n\nAfter locating the shader variable with , we use to set its value. During rendering, the vertex shader processes each vertex, converting them from object space to clip space. This transformation involves multiplying the vertex's position, which is initially in object space, by the PM matrix. The PM matrix is a composite of the projection and model-view matrices, integrating their effects. This step is pivotal in the GPU rendering process for accurately rendering objects.\n\nAlthough this discussion does not serve as an in-depth introduction to the GPU rendering pipeline, the goal is to provide a sufficient understanding of the concept. It's important to note that, as of 2024, the focus should be on utilizing programmable pipelines, which still necessitate the creation of projection matrices by the programmer. This requirement underscores the significance of projection matrices and explains their popularity in computer graphics discussions. However, in the modern rendering pipeline, employing a projection matrix is not strictly necessary. The essential task is to transform vertices to clip space, which can be achieved either through a matrix or equivalent code. For instance:\n\nThis flexibility in the programmable pipeline allows for alternative methods of vertex transformation, offering greater control over the process. While most scenarios will likely use standard projection techniques, understanding how to construct a projection matrix remains essential for those opting to directly manipulate vertex positions through custom code.\n• The GPU rendering pipeline fundamentally relies on the rasterization algorithm, where projection matrices are instrumental in transitioning vertices from camera space to clip space. Given the prevalence of GPUs and the central role of projection matrices in image generation, they are a focal point of interest in the field.\n• In the programmable pipeline, the vertex shader is responsible for moving vertices into clip space, with the projection matrix typically being a shader variable. This matrix is defined on the CPU before shader execution. Understanding how to construct this matrix and grasp the concept of clip space is crucial for effective graphics programming.\n\nTo wrap up this chapter, you might be curious about the title \"The Perspective and Orthographic Projection Matrix\" and the differences between these two types of projection matrices. Fundamentally, both serve the purpose of projecting 3D points onto a 2D plane. We've already delved into perspective projection, where as the viewing angle narrows (theoretically approaching zero), the sides of the frustum's pyramid become parallel. In such a scenario, objects maintain consistent size in the rendered image regardless of their distance from the camera, a characteristic known as orthographic projection. Despite orthographic projection appearing less natural compared to perspective projection, it finds its utility in certain contexts. A notable example is the game Sim City, which utilizes orthographic projection to achieve its distinctive visual style.\n\nIt's important to reiterate that in ray tracing, the concept of projecting points onto a screen is not applicable. Instead, the technique involves casting rays from the image plane and detecting their intersections with scene objects, bypassing the need for projection matrices.\n\nBeginning with the basics of constructing a simple projection matrix is an effective approach to understanding the broader concepts discussed here. Familiarity with homogeneous coordinates paves the way to grasp further concepts like clipping, which are intricately linked to projection matrices. The initial perspective projection matrix we'll explore in Chapter Three may not exhibit the complexity of those utilized in OpenGL or Direct3D, but it will serve to elucidate the fundamental workings of projection matrices — specifically, their role in transforming vertices from any given space to clip space. For the ensuing two chapters, our focus will revert to the foundational aspect of projection matrices: their function in transitioning points from camera space to NDC space. The transition to discussing the transformation of points to clip space will be reserved for Chapter Four, where we'll dissect the intricacies of the OpenGL matrix. For now, it's sufficient to understand that projection matrices are instrumental in converting vertices from camera space to NDC space, bearing in mind that this definition is provisional."
    },
    {
        "link": "https://reddit.com/r/opengl/comments/s28a5g/convert_normal_coordinates_to_screen_coordinates",
        "document": "So I'm following the tutorials at learnopengl.com so I've stumbled upon perspective matrices, but the issue with perspective matrix is that i cannot have screen coordinates like orthographic projection, so is there a way to convert normal coordinates ([-1 ,-1, -1] to [1,1,1]) to screen coordinates like ([0, 0, 0] to [1280, 720, 400]) ."
    },
    {
        "link": "https://stackoverflow.com/questions/20985369/how-to-convert-world-coordinates-to-screen-coordinates",
        "document": "I am creating a game that will have 2d pictures inside a 3d world.\n\nI originally started off by not caring about my images been stretched to a square while I learnt more about how game mechanics work... but it's now time to get my textures to display in the correct ratio and... size.\n\nJust a side note, I have played with orthographic left hand projections but I noticed that you cannot do 3d in that... (I guess that makes sense... but I could be wrong, I tried it and when I rotated my image, it went all stretchy and weirdosss).\n\nthe nature of my game is as follows:\n\nIn the image it says -1.0 to 1.0... i'm not fussed if the coordinates are:\n\nBut if that's the solution, then whatever... (p.s the game is not currently set up so that -1.0 and 1.0 is left and right of screen. infact i'm not sure how i'm going to make the screen edges the boundaries (but that's a question for another day)\n\nThe issue I am having is that my image for my player (2d) is 128 x 64 pixels. After world matrix multiplication (I think that's what it is) the vertices I put in scale my texture hugely... which makes sense but it looks butt ugly and I don't want to just whack a massive scaling matrix into the mix because it'll be difficult to work out how to make the texture 1:1 to my screen pixels (although maybe you will tell me it's actually how you do it but you need to do a clever formula to work out what the scaling should be).\n\nBut basically, I want the vertices to hold a 1:1 pixel size of my image, unstretched...\n\nSo I assume I need to convert my world coords to screen coords before outputting my textures and vertices??? I'm not sure how it works...\n\nAnyways, here are my vertices.. you may notice what I've done:\n\n(128 pixels / 2 = 64 ), ( 64 / 2 = 32 ) because the centre is 0.0... but what do I need to do to projections, world transdoobifications and what nots to get the worlds to screens?\n\nMy current setups look like this:\n\nand here is a sneaky look at my update and render methods:\n\nJust to clarify, if I make my vertices use 2 and 1 respective of the fact my image is 128 x 64.. I get a normal looking size image.. and yet at 0,0,0 it's not at 1:1 size... wadduuuppp buddyyyy"
    },
    {
        "link": "https://reddit.com/r/opengl/comments/z3qt6h/whats_the_best_method_for_converting_3d_global",
        "document": "Basically I currently have a working program where there a bunch of points in 3D space which you can see with a camera. What I want to do is draw some circles above the vertices which are 2D. I'm able to get the projection and view matrices from the camera but I don't know how to use\n\nI am using OpenGL, Glad and GLM all of which I'm very new to.\n\nLet me know if there's anything else you need to know"
    },
    {
        "link": "https://learnopengl.com/Getting-started/Coordinate-Systems",
        "document": "In the last chapter we learned how we can use matrices to our advantage by transforming all vertices with transformation matrices. OpenGL expects all the vertices, that we want to become visible, to be in normalized device coordinates after each vertex shader run. That is, the , and coordinates of each vertex should be between and ; coordinates outside this range will not be visible. What we usually do, is specify the coordinates in a range (or space) we determine ourselves and in the vertex shader transform these coordinates to normalized device coordinates (NDC). These NDC are then given to the rasterizer to transform them to 2D coordinates/pixels on your screen.\n\nTransforming coordinates to NDC is usually accomplished in a step-by-step fashion where we transform an object's vertices to several coordinate systems before finally transforming them to NDC. The advantage of transforming them to several intermediate coordinate systems is that some operations/calculations are easier in certain coordinate systems as will soon become apparent. There are a total of 5 different coordinate systems that are of importance to us:\n\nThose are all a different state at which our vertices will be transformed in before finally ending up as fragments.\n\nYou're probably quite confused by now by what a space or coordinate system actually is so we'll explain them in a more high-level fashion first by showing the total picture and what each specific space represents.\n\nTo transform the coordinates from one space to the next coordinate space we'll use several transformation matrices of which the most important are the , and matrix. Our vertex coordinates first start in as and are then further processed to , , and eventually end up as . The following image displays the process and shows what each transformation does:\n• Local coordinates are the coordinates of your object relative to its local origin; they're the coordinates your object begins in.\n• The next step is to transform the local coordinates to world-space coordinates which are coordinates in respect of a larger world. These coordinates are relative to some global origin of the world, together with many other objects also placed relative to this world's origin.\n• Next we transform the world coordinates to view-space coordinates in such a way that each coordinate is as seen from the camera or viewer's point of view.\n• After the coordinates are in view space we want to project them to clip coordinates. Clip coordinates are processed to the and range and determine which vertices will end up on the screen. Projection to clip-space coordinates can add perspective if using perspective projection.\n• And lastly we transform the clip coordinates to screen coordinates in a process we call that transforms the coordinates from and to the coordinate range defined by . The resulting coordinates are then sent to the rasterizer to turn them into fragments.\n\nYou probably got a slight idea what each individual space is used for. The reason we're transforming our vertices into all these different spaces is that some operations make more sense or are easier to use in certain coordinate systems. For example, when modifying your object it makes most sense to do this in local space, while calculating certain operations on the object with respect to the position of other objects makes most sense in world coordinates and so on. If we want, we could define one transformation matrix that goes from local space to clip space all in one go, but that leaves us with less flexibility.\n\nWe'll discuss each coordinate system in more detail below.\n\nLocal space is the coordinate space that is local to your object, i.e. where your object begins in. Imagine that you've created your cube in a modeling software package (like Blender). The origin of your cube is probably at even though your cube may end up at a different location in your final application. Probably all the models you've created all have as their initial position. All the vertices of your model are therefore in local space: they are all local to your object.\n\nThe vertices of the container we've been using were specified as coordinates between and with as its origin. These are local coordinates.\n\nIf we would import all our objects directly in the application they would probably all be somewhere positioned inside each other at the world's origin of which is not what we want. We want to define a position for each object to position them inside a larger world. The coordinates in world space are exactly what they sound like: the coordinates of all your vertices relative to a (game) world. This is the coordinate space where you want your objects transformed to in such a way that they're all scattered around the place (preferably in a realistic fashion). The coordinates of your object are transformed from local to world space; this is accomplished with the matrix.\n\nThe model matrix is a transformation matrix that translates, scales and/or rotates your object to place it in the world at a location/orientation they belong to. Think of it as transforming a house by scaling it down (it was a bit too large in local space), translating it to a suburbia town and rotating it a bit to the left on the y-axis so that it neatly fits with the neighboring houses. You could think of the matrix in the previous chapter to position the container all over the scene as a sort of model matrix as well; we transformed the local coordinates of the container to some different place in the scene/world.\n\nThe view space is what people usually refer to as the of OpenGL (it is sometimes also known as or ). The view space is the result of transforming your world-space coordinates to coordinates that are in front of the user's view. The view space is thus the space as seen from the camera's point of view. This is usually accomplished with a combination of translations and rotations to translate/rotate the scene so that certain items are transformed to the front of the camera. These combined transformations are generally stored inside a that transforms world coordinates to view space. In the next chapter we'll extensively discuss how to create such a view matrix to simulate a camera.\n\nAt the end of each vertex shader run, OpenGL expects the coordinates to be within a specific range and any coordinate that falls outside this range is . Coordinates that are clipped are discarded, so the remaining coordinates will end up as fragments visible on your screen. This is also where gets its name from.\n\nBecause specifying all the visible coordinates to be within the range and isn't really intuitive, we specify our own coordinate set to work in and convert those back to NDC as OpenGL expects them.\n\nTo transform vertex coordinates from view to clip-space we define a so called that specifies a range of coordinates e.g. and in each dimension. The projection matrix then converts coordinates within this specified range to normalized device coordinates ( , ) (not directly, a step called Perspective Division sits in between). All coordinates outside this range will not be mapped between and and therefore be clipped. With this range we specified in the projection matrix, a coordinate of ( , , ) would not be visible, since the coordinate is out of range and thus gets converted to a coordinate higher than in NDC and is therefore clipped.\n\nThis viewing box a projection matrix creates is called a and each coordinate that ends up inside this frustum will end up on the user's screen. The total process to convert coordinates within a specified range to NDC that can easily be mapped to 2D view-space coordinates is called since the projection matrix 3D coordinates to the easy-to-map-to-2D normalized device coordinates.\n\nOnce all the vertices are transformed to clip space a final operation called is performed where we divide the , and components of the position vectors by the vector's homogeneous component; perspective division is what transforms the 4D clip space coordinates to 3D normalized device coordinates. This step is performed automatically at the end of the vertex shader step.\n\nIt is after this stage where the resulting coordinates are mapped to screen coordinates (using the settings of ) and turned into fragments.\n\nThe projection matrix to transform view coordinates to clip coordinates usually takes two different forms, where each form defines its own unique frustum. We can either create an projection matrix or a projection matrix.\n\nAn orthographic projection matrix defines a cube-like frustum box that defines the clipping space where each vertex outside this box is clipped. When creating an orthographic projection matrix we specify the width, height and length of the visible frustum. All the coordinates inside this frustum will end up within the NDC range after transformed by its matrix and thus won't be clipped. The frustum looks a bit like a container:\n\nThe frustum defines the visible coordinates and is specified by a width, a height and a and plane. Any coordinate in front of the near plane is clipped and the same applies to coordinates behind the far plane. The orthographic frustum directly maps all coordinates inside the frustum to normalized device coordinates without any special side effects since it won't touch the component of the transformed vector; if the component remains equal to perspective division won't change the coordinates.\n\nTo create an orthographic projection matrix we make use of GLM's built-in function :\n\nThe first two parameters specify the left and right coordinate of the frustum and the third and fourth parameter specify the bottom and top part of the frustum. With those 4 points we've defined the size of the near and far planes and the 5th and 6th parameter then define the distances between the near and far plane. This specific projection matrix transforms all coordinates between these , and range values to normalized device coordinates.\n\nAn orthographic projection matrix directly maps coordinates to the 2D plane that is your screen, but in reality a direct projection produces unrealistic results since the projection doesn't take into account. That is something the matrix fixes for us.\n\nIf you ever were to enjoy the graphics the real life has to offer you'll notice that objects that are farther away appear much smaller. This weird effect is something we call . Perspective is especially noticeable when looking down the end of an infinite motorway or railway as seen in the following image:\n\nAs you can see, due to perspective the lines seem to coincide at a far enough distance. This is exactly the effect perspective projection tries to mimic and it does so using a . The projection matrix maps a given frustum range to clip space, but also manipulates the value of each vertex coordinate in such a way that the further away a vertex coordinate is from the viewer, the higher this component becomes. Once the coordinates are transformed to clip space they are in the range to (anything outside this range is clipped). OpenGL requires that the visible coordinates fall between the range and as the final vertex shader output, thus once the coordinates are in clip space, perspective division is applied to the clip space coordinates: \\[ out = \\begin{pmatrix} x /w \\\\ y / w \\\\ z / w \\end{pmatrix} \\] Each component of the vertex coordinate is divided by its component giving smaller vertex coordinates the further away a vertex is from the viewer. This is another reason why the component is important, since it helps us with perspective projection. The resulting coordinates are then in normalized device space. If you're interested to figure out how the orthographic and perspective projection matrices are actually calculated (and aren't too scared of the mathematics) I can recommend this excellent article by Songho.\n\nA perspective projection matrix can be created in GLM as follows:\n\nWhat does is again create a large frustum that defines the visible space, anything outside the frustum will not end up in the clip space volume and will thus become clipped. A perspective frustum can be visualized as a non-uniformly shaped box from where each coordinate inside this box will be mapped to a point in clip space. An image of a perspective frustum is seen below:\n\nIts first parameter defines the value, that stands for and sets how large the viewspace is. For a realistic view it is usually set to 45 degrees, but for more doom-style results you could set it to a higher value. The second parameter sets the aspect ratio which is calculated by dividing the viewport's width by its height. The third and fourth parameter set the near and far plane of the frustum. We usually set the near distance to and the far distance to . All the vertices between the near and far plane and inside the frustum will be rendered.\n\nWhen using orthographic projection, each of the vertex coordinates are directly mapped to clip space without any fancy perspective division (it still does perspective division, but the component is not manipulated (it stays ) and thus has no effect). Because the orthographic projection doesn't use perspective projection, objects farther away do not seem smaller, which produces a weird visual output. For this reason the orthographic projection is mainly used for 2D renderings and for some architectural or engineering applications where we'd rather not have vertices distorted by perspective. Applications like Blender that are used for 3D modeling sometimes use orthographic projection for modeling, because it more accurately depicts each object's dimensions. Below you'll see a comparison of both projection methods in Blender:\n\nYou can see that with perspective projection, the vertices farther away appear much smaller, while in orthographic projection each vertex has the same distance to the user.\n\nPutting it all together\n\nWe create a transformation matrix for each of the aforementioned steps: model, view and projection matrix. A vertex coordinate is then transformed to clip coordinates as follows: \\[ V_{clip} = M_{projection} \\cdot M_{view} \\cdot M_{model} \\cdot V_{local} \\] Note that the order of matrix multiplication is reversed (remember that we need to read matrix multiplication from right to left). The resulting vertex should then be assigned to in the vertex shader and OpenGL will then automatically perform perspective division and clipping.\n\nThis is a difficult topic to understand so if you're still not exactly sure about what each space is used for you don't have to worry. Below you'll see how we can actually put these coordinate spaces to good use and enough examples will follow in the upcoming chapters.\n\nNow that we know how to transform 3D coordinates to 2D coordinates we can start rendering real 3D objects instead of the lame 2D plane we've been showing so far.\n\nTo start drawing in 3D we'll first create a model matrix. The model matrix consists of translations, scaling and/or rotations we'd like to apply to transform all object's vertices to the global world space. Let's transform our plane a bit by rotating it on the x-axis so it looks like it's laying on the floor. The model matrix then looks like this:\n\nBy multiplying the vertex coordinates with this model matrix we're transforming the vertex coordinates to world coordinates. Our plane that is slightly on the floor thus represents the plane in the global world.\n\nNext we need to create a view matrix. We want to move slightly backwards in the scene so the object becomes visible (when in world space we're located at the origin ). To move around the scene, think about the following:\n• To move a camera backwards, is the same as moving the entire scene forward.\n\nThat is exactly what a view matrix does, we move the entire scene around inversed to where we want the camera to move.Because we want to move backwards and since OpenGL is a right-handed system we have to move in the positive z-axis. We do this by translating the scene towards the negative z-axis. This gives the impression that we are moving backwards.\n\nWe'll discuss how to move around the scene in more detail in the next chapter. For now the view matrix looks like this:\n\nThe last thing we need to define is the projection matrix. We want to use perspective projection for our scene so we'll declare the projection matrix like this:\n\nNow that we created the transformation matrices we should pass them to our shaders. First let's declare the transformation matrices as uniforms in the vertex shader and multiply them with the vertex coordinates:\n\nWe should also send the matrices to the shader (this is usually done each frame since transformation matrices tend to change a lot):\n\nNow that our vertex coordinates are transformed via the model, view and projection matrix the final object should be:\n• A bit farther away from us.\n• Be displayed with perspective (it should get smaller, the further its vertices are).\n\nLet's check if the result actually does fulfill these requirements:\n\nIt does indeed look like the plane is a 3D plane that's resting at some imaginary floor. If you're not getting the same result, compare your code with the complete source code.\n\nSo far we've been working with a 2D plane, even in 3D space, so let's take the adventurous route and extend our 2D plane to a 3D cube. To render a cube we need a total of 36 vertices (6 faces * 2 triangles * 3 vertices each). 36 vertices are a lot to sum up so you can retrieve them from here.\n\nFor fun, we'll let the cube rotate over time:\n\nAnd then we'll draw the cube using (as we didn't specify indices), but this time with a count of 36 vertices.\n\nYou should get something similar to the following:\n\nIt does resemble a cube slightly but something's off. Some sides of the cubes are being drawn over other sides of the cube. This happens because when OpenGL draws your cube triangle-by-triangle, fragment by fragment, it will overwrite any pixel color that may have already been drawn there before. Since OpenGL gives no guarantee on the order of triangles rendered (within the same draw call), some triangles are drawn on top of each other even though one should clearly be in front of the other.\n\nLuckily, OpenGL stores depth information in a buffer called the that allows OpenGL to decide when to draw over a pixel and when not to. Using the z-buffer we can configure OpenGL to do depth-testing.\n\nOpenGL stores all its depth information in a z-buffer, also known as a . GLFW automatically creates such a buffer for you (just like it has a color-buffer that stores the colors of the output image). The depth is stored within each fragment (as the fragment's value) and whenever the fragment wants to output its color, OpenGL compares its depth values with the z-buffer. If the current fragment is behind the other fragment it is discarded, otherwise overwritten. This process is called and is done automatically by OpenGL.\n\nHowever, if we want to make sure OpenGL actually performs the depth testing we first need to tell OpenGL we want to enable depth testing; it is disabled by default. We can enable depth testing using . The and functions allow us to enable/disable certain functionality in OpenGL. That functionality is then enabled/disabled until another call is made to disable/enable it. Right now we want to enable depth testing by enabling :\n\nSince we're using a depth buffer we also want to clear the depth buffer before each render iteration (otherwise the depth information of the previous frame stays in the buffer). Just like clearing the color buffer, we can clear the depth buffer by specifying the bit in the function:\n\nLet's re-run our program and see if OpenGL now performs depth testing:\n\nThere we go! A fully textured cube with proper depth testing that rotates over time. Check the source code here.\n\nSay we wanted to display 10 of our cubes on screen. Each cube will look the same but will only differ in where it's located in the world with each a different rotation. The graphical layout of the cube is already defined so we don't have to change our buffers or attribute arrays when rendering more objects. The only thing we have to change for each object is its model matrix where we transform the cubes into the world.\n\nFirst, let's define a translation vector for each cube that specifies its position in world space. We'll define 10 cube positions in a array:\n\nNow, within the render loop we want to call 10 times, but this time send a different model matrix to the vertex shader each time before we send out the draw call. We will create a small loop within the render loop that renders our object 10 times with a different model matrix each time. Note that we also add a small unique rotation to each container.\n\nThis snippet of code will update the model matrix each time a new cube is drawn and do this 10 times in total. Right now we should be looking into a world filled with 10 oddly rotated cubes:\n\nPerfect! It looks like our container found some like-minded friends. If you're stuck see if you can compare your code with the source code.\n• Try experimenting with the and parameters of GLM's function. See if you can figure out how those affect the perspective frustum.\n• Play with the view matrix by translating in several directions and see how the scene changes. Think of the view matrix as a camera object.\n• Try to make every 3rd container (including the 1st) rotate over time, while leaving the other containers static using just the model matrix: solution."
    },
    {
        "link": "https://stackoverflow.com/questions/16440717/3d-graphics-changing-objects-world-position-relative-to-its-screen-position",
        "document": "I am implementing a game with a camera view like in Diablo 3 (3d objects on a 3d surface, top-down view), but I have one limitation:\n\nI need my game to disregard distance in any respect.\n\nThat includes having all the 3d models face the camera from the same angle in ANY place on the screen, having the same size in any place on the screen and moving with the same speed (screenspace-wise).\n\nI need this due to the specific mechanics I have in mind and I need it to be in a 3d world, because I what to have accurate shadows in the game.\n\nI know how to implement every feature (mainly by using orthogonal projection and some shader magic) but the last one, that is I can't understand how to move a 3d object, positioned on a 3d plane with the very same speed screenspace-wise.\n\nIf the object is in the upper part of the screen, it will move slower, and vise versa, so I do need some code to compensate for that.\n\nDo you guys have any idea how this can be done?\n\nP.S. Thanks for editing btw, it does look better."
    }
]