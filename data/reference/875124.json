[
    {
        "link": "https://stackoverflow.com/questions/906233/logging-in-java-and-in-general-best-practices",
        "document": "For anylogger API, we have at leaset these Log Levels: ERROR > WARN > INFO > DEBUG > TRACE\n\nAnd we can use each log level to write different types of logs to achieve better understanding of our collected traces:\n\nTrace – It would be better if we write a trace in every method at entry point with method name and method argument as well as at exit point with return value/object,\n\nNote – It is better to follow our coding guidelines and write the methods modular, then in that case we don’t need to write multiple logs line in between the method to print the data.\n\nDebug – Debug log we will add in middle of method to show which if/else/switch condition got satisfied, also the data which we get it from DB and using it in the method and so on. Note –don’t add those data in debug which is being send as an argument or return as a value, since those data already getting printed by Trace level (try not to print same logs multiple times).\n\nInfo – Imagine client has log level info, so what message and all you want to show him if they see the log, so add those things in info. Example – Blabla connection created/deleted/modified successfully or Blabla link locked/Unlocked or blabla sync triggered for blabla node/nodes.\n\nWarn – It is rare condition but while writing the code we come across some condition which is not possible in normal case, it only come due to any stale entry or any breakage happens, normally we ignore this condition, but it would be better if we add such condition and add warring log there. Example – I was querying from one table with condition on column which is not primary key or unique but it was told that it will always return only one row so do , so in such case we should write one condition like if resultSet.size > 1 add some warning log with better messages.\n\nError – Error log should be present in every catch block which is not expected, and it should print complete stack trace properly (and not only the error message). Also in catch block people are throwing a new Exception without logging existing Exception trace, in such scenario we do not get the actual point of exception. So, writing Error log in every catch block with complete stack trace is very much mandatory."
    },
    {
        "link": "https://foojay.io/today/effective-java-logging",
        "document": "Effective Logging is an essential aspect of any Java application, providing insights into its operational state. It is especially crucial in production environments, where it aids in debugging, monitoring, and incident response. In this comprehensive guide, we will explore the effective practices for using SLF4J with Logback, ensuring a reliable and maintainable logging strategy.\n\nBy following these best practices, developers and operations teams can leverage SLF4J and Logback to turn logs into strategic resources for application management and incident resolution. Embracing these guidelines will lead to improved observability, quicker troubleshooting, and a deeper understanding of system behavior, establishing a solid foundation for application reliability and performance.\n• Improved observability: Logs provide a detailed record of application behavior, making it easier to understand how the system is operating and identify potential issues.\n• Faster troubleshooting: Well-structured and informative logs enable developers to quickly pinpoint the root cause of problems and resolve them efficiently.\n• Enhanced incident response: Logs are invaluable during incident response, providing a chronological account of events leading up to and during an issue.\n• Compliance and security: Logs can serve as evidence of compliance with regulations and help identify security breaches or suspicious activities.\n\nSLF4J (Simple Logging Facade for Java) is a popular logging facade that provides a consistent API for logging across different logging frameworks. Logback is a widely used logging framework that offers a rich set of features and customization options. By combining SLF4J with Logback, you can benefit from the flexibility and power of both tools.\n\nIn this guide, we will cover 14 essential best practices for using SLF4J and Logback effectively in your Java applications. These practices will help you achieve reliable, maintainable, and informative logging that supports your application's operational needs.\n\n1. Use SLF4J as the Logging Facade\n\n\n\n 🟢 Good Practice:\n\n \n\n Choose SLF4J as your application's logging facade to decouple your logging architecture from the underlying logging library implementation. This abstraction allows you to switch between different logging frameworks without major code changes.\n\n🔴 Avoid Practice:\n\n \n\n Hardcoding a specific logging framework implementation in your application code can lead to difficulties when needing to switch libraries.\n\n\n\n 🟢 Good Practice:\n\n \n\n Externalize your Logback configuration and use for improved performance and flexibility. Define different configurations for development, staging, and production environments to better manage the verbosity and detail of logs.\n\n🔴 Avoid Practice:\n\n \n\n Using an outdated or non-performant layout class and hardcoding configuration settings in the code can make it difficult to adapt to different environments.\n\n\n\n 🟢 Good Practice:\n\n \n\n Log at the correct level to convey the importance and intention of the message. Use for general events, for detailed information during development, and for serious issues that need attention.\n\n🔴 Avoid Practice:\n\n \n\n Logging everything at the same level, can overwhelm the log files with noise and make it difficult to spot critical issues.\n\n\n\n 🟢 Good Practice:\n\n \n\n Include relevant information such as transaction or correlation IDs in your log messages to provide context. This is especially helpful in distributed systems for tracing requests across services.\n\n🔴 Avoid Practice:\n\n \n\n Vague or generic log messages that do not provide sufficient context to understand the event or issue.\n\nUtilize placeholders to 🔴 Avoid Practice: unnecessary string concatenation when the log level is disabled, saving memory and CPU cycles.\n\n🔴 Avoid Practice:\n\n \n\n Concatenating strings within log statements is less efficient.\n\n\n\n 🟢 Good Practice:\n\n \n\n Always log the full exception, including the stack trace, to provide maximum context for diagnosing issues.\n\n🔴 Avoid Practice:\n\n \n\n Logging only the exception message without the stack trace can omit critical diagnostic information.\n\n\n\n 🟢 Good Practice:\n\n \n\n Implement asynchronous logging to improve application performance by offloading logging activities to a separate thread.\n\n🔴 Avoid Practice:\n\n \n\n Synchronous logging in performance-critical paths without considering the potential for log-related latency.\n\n8. Log at the Appropriate Granularity\n\n\n\n 🟢 Good Practice:\n\n \n\n You should balance between logging too much and too little. Log at the appropriate granularity based on the specific requirements of your application. Avoid excessive logging that clutters the logs and makes it difficult to identify important information.\n\n🔴 Avoid Practice:\n\n \n\n Excessive logging at a high granularity in production, can lead to performance issues and log flooding.\n\nConfigure log file rotation based on size or time to prevent logs from consuming excessive disk space. Set up monitoring for log files to trigger alerts when nearing capacity.\n\n🔴 Avoid Practice:\n\n Letting log files grow indefinitely, can lead to disk space exhaustion and potential system failure.\n\nImplement filters or custom converters in your logging framework to redact or hash sensitive data before it's written to the logs.\n\n\n\n 🔴 Avoid Practice:\n\n Logging sensitive information such as passwords, API keys, Credit Cards, or personally identifiable information (PII).\n\nAdopt structured logging to output logs in a machine-readable format like JSON, facilitating better searching and indexing in log management systems.\n\nLet's take a look at an example log message that is printed in JSON format:\n\nThe output of the above log message will be printed as below:\n\n\n\n 🔴 Avoid Practice:\n\n Using unstructured log formats that are difficult to parse and analyze programmatically.\n\nLink your logging with monitoring and alerting tools to automatically detect anomalies and notify the concerned teams.\n\n🔴 Avoid Practice:\n\n Ignoring the integration of logs with monitoring systems can delay the detection of issues.\n\nIn distributed environments, use centralized log aggregation to collect logs from multiple services, simplifying analysis and correlation of events.\n\nAllowing logs to remain scattered across various systems, complicates the troubleshooting process.\n\nWe have great content here for implementing Smart Logging using AOP.\n\nEffective logging is not just about capturing data; it's about capturing the right data at the right time and in the right format. By implementing these best practices, developers and operations teams can leverage SLF4J and Logback to turn logs into strategic resources for application management and incident resolution.\n\nEmbracing these guidelines will lead to improved observability, quicker troubleshooting, and a deeper understanding of system behavior, establishing a solid foundation for application reliability and performance."
    },
    {
        "link": "https://sematext.com/blog/java-logging-best-practices",
        "document": "Having visibility into your Java application is crucial for understanding how it works right now, how it worked some time in the past and increasing your understanding of how it might work in the future. More often than not, analyzing logs is the fastest way to detect what went wrong, thus making logging in Java critical to ensuring the performance and health of your app, as well as minimizing and reducing any downtime. Having a centralized logging and monitoring solution helps reduce the Mean Time To Repair by improving the effectiveness of your Ops or DevOps team.\n\nBy following good practices you will get more value out of your logs and make it easier to use them. You will be able to more easily pinpoint the root cause of errors and poor performance and solve problems before they impact end-users. So today, let me share some of the best practices you should swear by when working with Java applications. Let’s dig in.\n\nLogging in Java can be done in a few different ways. You can use a dedicated logging library, a common API, or even just write logs to file or directly to a dedicated logging system. However, when choosing the logging library for your system think ahead. Things to consider and evaluate are performance, flexibility, appenders for new log centralization solutions, and so on. If you tie yourself directly to a single framework the switch to a newer library can take a substantial amount of work and time. Keep that in mind and go for the API that will give you the flexibility to swap logging libraries in the future. Just like with the switch from Log4j to Logback and to Log4j 2, when using the SLF4J API the only thing you need to do is change the dependency, not the code.\n\nIf you’re new to Java logging libraries, check out our beginner’s guides:\n• JUL vs Log4j vs Log4j2 vs Logback vs SLF4J\n\nAppenders define where your log events will be delivered. The most common appenders are the Console and File Appenders. While useful and widely known, they may not fulfill your requirements. For example, you may want to write your logs in an asynchronous way or you may want to ship your logs over the network using appenders like the one for Syslog, like this:\n\nHowever, remember that using appenders like the one shown above makes your logging pipeline susceptible to network errors and communication disruptions. That may result in logs not being shipped to their destination, which may not be acceptable. You also want to avoid logging affecting your system if the appender is designed in a blocking way. To learn more, check our Logging libraries vs Log shippers blog post.\n\nOne of the crucial things when it comes to creating logs, yet one of the not-so-easy ones is using meaningful messages. Your log events should include messages that are unique to the given situation, clearly describe them and inform the person reading them. Imagine a communication error occurred in your application. You might do it like this:\n\nBut you could also create a message like this:\n\nYou can easily see that the first message will inform the person looking at the logs about communication issues. That person will probably have the context, the name of the logger, and the line number where the warning happened, but that is all. To get more context, that person would have to look at the code, know which version of the code the error is related to, and so on. This is not fun and often not easy, and certainly not something one wants to do while trying to troubleshoot a production issue as quickly as possible.\n\nThe second message is better. It provides exact information about what kind of communication error happened, what the application was doing at the time, what error code it got, and what the response from the remote server was. Finally, it also informs that sending the message will be retried. Working with such messages is definitely easier and more pleasant.\n\nFinally, think about the size and verbosity of the message. Don’t log information that is too verbose. This data needs to be stored somewhere in order to be useful. One very long message will not be a problem, but if that line is repeating hundreds of times in a minute and you have lots of verbose logs, keeping longer retention of such data may be problematic and, at the end of the day, will also cost more.\n\nOne of the very important parts of Java logging are the Java stack traces. Have a look at the following code:\n\nThe above code will result in an exception being thrown and a log message that will be printed to the console with our default configuration will look as follows:\n\nAs you can see there is not a lot of information there. We only know that the problem occurred, but we don’t know where it happened, or what the problem was, etc. Not very informative.\n\nNow, look at the same code with a slightly modified logging statement:\n\nAs you can see, this time we’ve included the exception object itself in our log message:\n\nThat would result in the following error log in the console with our default configuration:\n\nIt contains relevant information – i.e. the name of the class, the method where the problem occurred, and finally the line number where the problem happened. Of course, in real-life situations, the stack traces will be longer, but you should include them to give you enough information for proper debugging.\n\nTo learn more about handling Java stack traces with Logstash see Handling Multiline Stack Traces with Logstash or check out Logagent which can do that for you out of the box.\n\nWhen dealing with Java exceptions and stack traces you shouldn’t only think about the whole stack trace, the lines where the problem appeared, and so on. You should also think about how not to deal with exceptions.\n\nAvoid silently ignoring exceptions. You don’t want to ignore something important. For example, do not do this:\n\nAlso, don’t just log an exception and throw it further. That means that you just pushed the problem up the execution stack. Avoid things like this as well:\n\nIf you’re interested to learn more about exceptions, read our guide about Java exception handling where we cover everything from what they are to how to catch and fix them.\n\nWhen writing your application code think twice about a given log message. Not every bit of information is equally important and not every unexpected situation is an error or a critical message. Also, using the logging levels consistently – information of a similar type should be on a similar severity level.\n\nBoth SLF4J facade and each Java logging framework that you will be using provide methods that can be used to provide a proper log level. For example:\n\nIf we plan to log and look at the data manually in a file or the standard output then the planned logging will be more than fine. It is more user friendly – we are used to it. But that is only viable for very small applications and even then it is suggested to use something that will allow you to correlate JVM metrics data with Java logs. Doing such operations in a terminal window ain’t fun and sometimes it is simply not possible. If you want to store logs in the log management and centralization system you should log in JSON. That’s because parsing doesn’t come for free – it usually means using regular expressions. Of course, you can pay that price in the log shipper, but why do that if you can easily log in JSON. Logging in JSON also means easy handling of stack traces, so yet another advantage. Well, you can also just log to a Syslog compatible destination, but that is a different story.\n\nIn most cases, to enable logging in JSON in your Java logging framework it is enough to include the proper configuration. For example, let’s assume that we have the following log message included in our code:\n\nTo configure Log4J 2 to write log messages in JSON we would include the following configuration:\n\nThe result would look as follows:\n\nThe structure of your log events should be consistent. This is not only true within a single application or set of microservices, but should be applied across your whole application stack. With similarly structured logs it will be easier to look into them, compare them, correlate them, or simply store them in a dedicated data store. It is easier to look into data coming from your systems when you know they have common fields like severity and hostname, so you can easily slice and dice the data based on that information. For inspiration, have a look at Sematext Common Schema even if you are not a Sematext user.\n\nOf course, keeping the structure is not always possible because your full stack consists of externally developed servers, databases, search engines, queues, etc., each with its own set of logs and log formats. However, to keep your and your team’s sanity, minimize the number of different log message structures that you can control.\n\nOne way of keeping a common structure is to use the same pattern for your logs, at least the ones that use the same logging framework. For example, if your applications and microservices use Log4J 2 you could use a pattern like this:\n\nBy using a single or a very limited set of patterns you can be sure that the number of log formats will remain small and manageable.\n\nInformation context is important and for us developers and DevOps a log message is information. Look at the following log entry:\n\nWe know that an error appeared somewhere in the application. We don’t know where it happened, we don’t know what kind of error it was, we only know when it happened. Now look at a message with slightly more contextual information:\n\nThe same log record, but a lot more contextual information. We know the thread in which it happened, we know what class the error was generated at. We modified the message as well to include the user that the error happened for, so we can get back to the user if needed. We could also include additional information like diagnostic contexts. Think about what you need and include it.\n\nTo include context information you don’t have to do much when it comes to the code that is responsible for generating the log message. For example, the PatternLayout in Log4J 2 gives you all that you need to include context information. You can go with a very simple pattern like this:\n\nThat will result in a log message similar to the following one:\n\nBut you can also include a pattern that will include way more information:\n\nThat will result in a log message like this:\n\nThink about the environment your application is going to be running in. There is a difference in logging configuration when you are running your Java code in a VM or on a bare-metal machine, it is different when running it in a containerized environment, and of course, it is different when you run your Java or Kotlin code on an Android device.\n\nTo set up logging in a containerized environment you need to choose the approach you want to take. You can use one of the provided logging drivers – like the journald, logagent, Syslog, or JSON file. To do that, remember that your application shouldn’t write the log file to the container ephemeral storage, but to the standard output. That can be easily done by configuring your logging framework to write the log to the console. For example, with Log4J 2 you would just use the following appender configuration:\n\nYou can also completely omit the logging drivers and ship logs directly to your centralized logs solution like our Sematext Cloud:\n\n11. Don’t Log Too Much or Too Little\n\nAs developers we tend to think that everything might be important – we tend to mark each step of our algorithm or business code as important. On the other hand, we sometimes do the opposite – we don’t add logging where we should or we log only FATAL and ERROR log levels. Both approaches will not do very well. When writing your code and adding logging, think about what will be important to see if the application is working properly and what will be important to be able to diagnose a wrong application state and fix it. Use this as your guiding light to decide what and where to log. Keep in mind that adding too many logs will end up in information fatigue and not having enough information will result in the inability to troubleshoot.\n\n12. Keep the Audience in Mind\n\nIn most cases, you will not be the only person looking at the logs. Always remember that. There are multiple actors that may be looking at the logs.\n\nThe developer may be looking at the logs for troubleshooting or during debugging sessions. For such people, logs can be detailed, technical, and include very deep information related to how the system is running. Such a person will also have access to the code or will even know the code and you can assume that.\n\nThen there are DevOps. For them, log events will be needed for troubleshooting and should include information helpful in diagnostics. You can assume the knowledge of the system, its architecture, its components, and the configuration of the components, but you should not assume the knowledge about the code of the platform.\n\nFinally, your application logs may be read by your users themselves. In such a case, the logs should be descriptive enough to help fix the issue if that is even possible or give enough information to the support team helping the user. For example, using Sematext for monitoring involves installing and running a monitoring agent. If you are behind a very restrictive firewall and the agent cannot ship metrics to Sematext, it logs errors aimed that Sematext users themselves can look at, too.\n\nWe could go further and identify even more actors who might be looking into logs, but this shortlist should give you a glimpse into what you should think about when writing your log messages.\n\nSensitive information shouldn’t be present in logs or should be masked. Passwords, credit card numbers, social security numbers, access tokens, and so on – all of that may be dangerous if leaked or accessed by those who shouldn’t see that. There are two things you ought to consider.\n\nThink whether sensitive information is truly essential for troubleshooting. Maybe instead of a credit card number, it is enough to keep the information about the transaction identifier and the date of the transaction? Maybe it is not necessary to keep the social security number in the logs when you can easily store the user identifier. Think about such situations, think about the data that you store, and only write sensitive data when it is really necessary.\n\nThe second thing is shipping logs with sensitive information to a hosted logs service. There are very few exceptions where the following advice should not be followed. If your logs have and need to have sensitive information stored, mask or remove them before sending them to your centralized logs store. Most popular log shippers, like our own Logagent, include functionality that allows removal or masking of sensitive data.\n\nFinally, the masking of sensitive information can be done in the logging framework itself. Let’s look at how it can be done by extending Log4j 2. Our code that produces log events looks as follows (full example can be found at Sematext Github):\n\nIf you were to run the whole example from Github the output would be as follows:\n\nYou can see that the credit card number was masked. This was done because we added a custom Converter that checks if the given Marker is passed along the log event and tries to replace a defined pattern. The implementation of such Converter looks as follows:\n\nIt is very simple and could be written in a more optimized way and should also handle all the possible credit cards number formats, but it is enough for this purpose.\n\nBefore jumping into the code explanation I would also like to show you the log4j2.xml configuration file for this example:\n\nAs you can see, we’ve added the packages attribute in our Configuration to tell the framework where to look for our converter. Then we’ve used the %sc pattern to provide the log message. We do that because we can’t overwrite the default %m pattern. Once Log4j2 finds our %sc pattern it will use our converter which takes the formatted message of the log event and uses a simple regex and replaces the data if it was found. As simple as that.\n\nOne thing to notice here is that we are using the Marker functionality. Regex matching is expensive and we don’t want to do that for every log message. That’s why we mark the log events that should be processed with the created Marker, so only the marked ones are checked.\n\nWith the complexity of the applications, the volume of your logs will grow, too. You may get away with logging to a file and only using logs when troubleshooting is needed, but when the amount of data grows it quickly becomes difficult and slow to troubleshoot this way When this happens, consider using a log management solution to centralize and monitor your logs. You can either go for an in-house solution based on the open-source software, like Elastic Stack or use one of the log management tools available on the market like Sematext Logs.\n\nA fully managed log aggregation tool will give you the freedom of not needing to manage yet another, usually quite complex, part of your infrastructure. Instead, you will be able to focus on your application and will need to set up only log shipping. You may want to include logs like JVM garbage collection logs in your managed log solution. After turning them on for your applications and systems working on the JVM you will want to monitor logs and centralize them in a single place for log correlation, log analysis, and to help you tune the garbage collection in the JVM instances. Such logs correlated with metrics are an invaluable source of information for troubleshooting garbage collection-related problems.\n\nIf you’re interested to see how Sematext Logs stacks up against similar solutions, we wrote some in-depth reviews of the best log management software, log analysis tools, and cloud logging services but we recommend you jump right into the 14-day free trial to fully explore its features. Try it and see for yourself!\n\nOr, if you’d like a little more information on how Sematext Logs works, check out this short video below:\n\nIncorporating each and every good practice may not be easy to implement right away, especially for applications that are already live and working in production. But if you take the time and roll the suggestions out one after another you will start seeing an increase in the usefulness of your logs. For more tips on how to get the most out of your logs, we recommend you also go through our other article on logging best practices where we explain and ins and outs you should follow regardless of the type of app you’re working with. And remember that at Sematext we do help organizations with their logging setups by offering logging consulting, so reach out if you are having trouble and we will be happy to help."
    },
    {
        "link": "https://papertrail.com/solution/tips/logging-in-java-best-practices-and-tips",
        "document": ""
    },
    {
        "link": "https://romanglushach.medium.com/java-rest-api-logging-best-practices-and-guidelines-bf5982ee4180",
        "document": "Logging is essential for understanding how an application works, both in the present and the past. Analyzing logs is the fastest way to detect what went wrong, making logging is critical to ensuring the performance and health of your app, as well as minimizing and reducing any downtime. By following good practices, you will get more value out of your logs and make it easier to use them. You will be able to more easily pinpoint the root cause of errors and poor performance and solve problems before they impact end-users.\n\nLogging in Java can be done in a few different ways. You can use a dedicated logging library, a common API, or even just write logs to file or directly to a dedicated logging service. However, it is recommended to use a standard logging library, such as Log4j or Logback, as it provides a consistent and well-documented API for logging. This makes it easier to maintain and troubleshoot your logging code.\n\nAppenders are responsible for writing log messages to various destinations, such as files, databases, or remote logging services. It is important to select your appenders wisely, based on your specific use case. For example, if you need to store logs for a long time, you might want to use a database appender. If you need to analyze logs in real-time, you might want to use a remote logging service.\n\nWhen logging messages, it is important to use meaningful messages that provide context and information about what is happening in the application. This makes it easier to understand what is happening when analyzing logs. For example, instead of logging “Error occurred,” log “Failed to connect to database.”\n\nThe format and structure of log messages can have a significant impact on the readability and usability of log data. You should follow some best practices and guidelines when formatting and structuring log messages, such as:\n• Use a consistent and clear pattern for each log message that includes relevant information (such as timestamp, level, logger name, thread name, message text, exception stack trace, etc\n• Use a standard date and time format (ISO 8601) that is easy to parse and compare\n• Use a common delimiter (such as space or semicolon) to separate different fields in a log message\n• Use a structured format (JSON) to represent complex or nested data in a log message\n• Use parameterized messages (such as “{}”) instead of string concatenation (such as “+”) to improve performance and readability\n• Use markers (such as Audit or Performance) to tag log messages with specific attributes or categories\n• Use MDC (Mapped Diagnostic Context) to add contextual information (such as request ID, user ID, session ID, etc.) to log messages\n\nLogging filters and interceptors are components that can intercept and modify the requests and responses of a REST API. You can use logging filters and interceptors to implement various logging functionalities, such as:\n• Logging the request and response details (such as method, URL, headers, body, status, etc.)\n• Logging the execution time and performance metrics of a REST API\n• Logging the exceptions and errors that occur during the processing of a REST API\n• Logging the authentication and authorization information of a REST API\n• Logging the business logic and validation rules of a REST API\n\nThere are different ways to implement logging filters and interceptors in Java, depending on the framework or library you are using. For example, if you are using Spring Boot, you can use the following options:\n• HandlerInterceptor: An interface that allows you to intercept the incoming and outgoing HTTP messages at the level of the DispatcherServlet\n• Filter: A component that allows you to intercept the incoming and outgoing HTTP messages at the level of the servlet container\n• AOP (Aspect-Oriented Programming): A technique that allows you to inject cross-cutting concerns (such as logging) into your code without modifying it\n• ControllerAdvice attribute: An annotation that allows you to handle global exceptions and errors in your REST API\n\nLogging sensitive data (such as passwords, credit card numbers, personal information, etc.) can pose a serious security risk for your REST API. You should avoid logging sensitive data or mask or encrypt it before logging it. You can use various techniques to mask or encrypt sensitive data, such as:\n• Using regular expressions or patterns to replace sensitive data with asterisks (*) or other symbols\n• Using hashing or encryption algorithms (such as MD5 or AES) to transform sensitive data into unreadable strings\n• Using annotations or attributes (such as Sensitive or Loggable attributes) to mark sensitive data in your code or configuration\n• Using custom converters or serializers (such as MaskingConverter or EncryptingSerializer) to mask or encrypt sensitive data before writing it to appenders\n\nLog levels are labels you can attach to each log entry to indicate their severity. Adding adequate log levels to messages is essential if you want to be able to make sense of them. Log4j 2, besides supporting custom log levels, offers the following standard ones: TRACE, DEBUG, INFO, WARN, ERROR, and FATAL. Use them appropriately to categorize your log messages.\n\nLog to always have relevant, contextual logs that don’t add overhead. Work smarter, not harder. Logging is often one of the few lifelines you have in production environments where you can’t physically attach and debug. You want to log as much relevant, contextual data as you can.\n\nA best practice here would be to ensure all of our logged outputs are in JSON form. This makes searching through log files, categorizing data, and analyzing logs much easier.\n\nLog files can grow very large over time and consume a lot of disk space. This can affect the performance and availability of your REST API. You should rotate and archive log files periodically to manage their size and number. You can use various strategies to rotate and archive log files, such as:\n• Time-based rotation: Rotating log files based on a fixed time interval (such as daily, weekly, monthly, etc.)\n• Size-based rotation: Rotating log files based on a fixed size limit (such as 10 MB, 100 MB, 1 GB, etc.)\n• Hybrid rotation: Rotating log files based on a combination of time and size criteria (such as daily or 10 MB, whichever comes first)\n• Compression: Compressing log files after rotation to reduce their size\n• Deletion: Deleting old log files after a certain period of time or when a certain number of files is reached\n\nLogging is a critical aspect of application development, providing visibility into how the application is working and helping detect issues before they impact end-users."
    },
    {
        "link": "https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/List.html",
        "document": "An ordered collection (also known as a). The user of this interface has precise control over where in the list each element is inserted. The user can access elements by their integer index (position in the list), and search for elements in the list.\n\nUnlike sets, lists typically allow duplicate elements. More formally, lists typically allow pairs of elements and such that , and they typically allow multiple null elements if they allow null elements at all. It is not inconceivable that someone might wish to implement a list that prohibits duplicates, by throwing runtime exceptions when the user attempts to insert them, but we expect this usage to be rare.\n\nThe interface places additional stipulations, beyond those specified in the interface, on the contracts of the , , , , and methods. Declarations for other inherited methods are also included here for convenience.\n\nThe interface provides four methods for positional (indexed) access to list elements. Lists (like Java arrays) are zero based. Note that these operations may execute in time proportional to the index value for some implementations (the class, for example). Thus, iterating over the elements in a list is typically preferable to indexing through it if the caller does not know the implementation.\n\nThe interface provides a special iterator, called a , that allows element insertion and replacement, and bidirectional access in addition to the normal operations that the interface provides. A method is provided to obtain a list iterator that starts at a specified position in the list.\n\nThe interface provides two methods to search for a specified object. From a performance standpoint, these methods should be used with caution. In many implementations they will perform costly linear searches.\n\nThe interface provides two methods to efficiently insert and remove multiple elements at an arbitrary point in the list.\n\nNote: While it is permissible for lists to contain themselves as elements, extreme caution is advised: the and methods are no longer well defined on such a list.\n\nSome list implementations have restrictions on the elements that they may contain. For example, some implementations prohibit null elements, and some have restrictions on the types of their elements. Attempting to add an ineligible element throws an unchecked exception, typically or . Attempting to query the presence of an ineligible element may throw an exception, or it may simply return false; some implementations will exhibit the former behavior and some will exhibit the latter. More generally, attempting an operation on an ineligible element whose completion would not result in the insertion of an ineligible element into the list may throw an exception or it may succeed, at the option of the implementation. Such exceptions are marked as \"optional\" in the specification for this interface.\n\nThe and static factory methods provide a convenient way to create unmodifiable lists. The instances created by these methods have the following characteristics:\n• They are unmodifiable. Elements cannot be added, removed, or replaced. Calling any mutator method on the List will always cause to be thrown. However, if the contained elements are themselves mutable, this may cause the List's contents to appear to change.\n• They disallow elements. Attempts to create them with elements result in .\n• They are serializable if all elements are serializable.\n• The order of elements in the list is the same as the order of the provided arguments, or of the elements in the provided array.\n• They are value-based. Callers should make no assumptions about the identity of the returned instances. Factories are free to create new instances or reuse existing ones. Therefore, identity-sensitive operations on these instances (reference equality ( ), identity hash code, and synchronization) are unreliable and should be avoided.\n• They are serialized as specified on the Serialized Form page.\n\nThis interface is a member of the Java Collections Framework."
    },
    {
        "link": "https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Collection.html",
        "document": "The root interface in the. A collection represents a group of objects, known as its. Some collections allow duplicate elements and others do not. Some are ordered and others unordered. The JDK does not provide anyimplementations of this interface: it provides implementations of more specific subinterfaces likeand. This interface is typically used to pass collections around and manipulate them where maximum generality is desired.\n\nBags or multisets (unordered collections that may contain duplicate elements) should implement this interface directly.\n\nAll general-purpose implementation classes (which typically implement indirectly through one of its subinterfaces) should provide two \"standard\" constructors: a void (no arguments) constructor, which creates an empty collection, and a constructor with a single argument of type , which creates a new collection with the same elements as its argument. In effect, the latter constructor allows the user to copy any collection, producing an equivalent collection of the desired implementation type. There is no way to enforce this convention (as interfaces cannot contain constructors) but all of the general-purpose implementations in the Java platform libraries comply.\n\nCertain methods are specified to be optional. If a collection implementation doesn't implement a particular operation, it should define the corresponding method to throw . Such methods are marked \"optional operation\" in method specifications of the collections interfaces.\n\nSome collection implementations have restrictions on the elements that they may contain. For example, some implementations prohibit null elements, and some have restrictions on the types of their elements. Attempting to add an ineligible element throws an unchecked exception, typically or . Attempting to query the presence of an ineligible element may throw an exception, or it may simply return false; some implementations will exhibit the former behavior and some will exhibit the latter. More generally, attempting an operation on an ineligible element whose completion would not result in the insertion of an ineligible element into the collection may throw an exception or it may succeed, at the option of the implementation. Such exceptions are marked as \"optional\" in the specification for this interface.\n\nIt is up to each collection to determine its own synchronization policy. In the absence of a stronger guarantee by the implementation, undefined behavior may result from the invocation of any method on a collection that is being mutated by another thread; this includes direct invocations, passing the collection to a method that might perform invocations, and using an existing iterator to examine the collection.\n\nMany methods in Collections Framework interfaces are defined in terms of the method. For example, the specification for the method says: \"returns if and only if this collection contains at least one element such that .\" This specification should not be construed to imply that invoking with a non-null argument will cause to be invoked for any element . Implementations are free to implement optimizations whereby the invocation is avoided, for example, by first comparing the hash codes of the two elements. (The specification guarantees that two objects with unequal hash codes cannot be equal.) More generally, implementations of the various Collections Framework interfaces are free to take advantage of the specified behavior of underlying methods wherever the implementor deems it appropriate.\n\nSome collection operations which perform recursive traversal of the collection may fail with an exception for self-referential instances where the collection directly or indirectly contains itself. This includes the , , and methods. Implementations may optionally handle the self-referential scenario, however most current implementations do not do so.\n\nMost collections manage storage for elements they contain. By contrast, view collections themselves do not store elements, but instead they rely on a backing collection to store the actual elements. Operations that are not handled by the view collection itself are delegated to the backing collection. Examples of view collections include the wrapper collections returned by methods such as , , and . Other examples of view collections include collections that provide a different representation of the same elements, for example, as provided by , , or . Any changes made to the backing collection are visible in the view collection. Correspondingly, any changes made to the view collection — if changes are permitted — are written through to the backing collection. Although they technically aren't collections, instances of and can also allow modifications to be written through to the backing collection, and in some cases, modifications to the backing collection will be visible to the Iterator during iteration.\n\nCertain methods of this interface are considered \"destructive\" and are called \"mutator\" methods in that they modify the group of objects contained within the collection on which they operate. They can be specified to throw if this collection implementation does not support the operation. Such methods should (but are not required to) throw an if the invocation would have no effect on the collection. For example, consider a collection that does not support the operation. What will happen if the method is invoked on this collection, with an empty collection as the argument? The addition of zero elements has no effect, so it is permissible for this collection simply to do nothing and not to throw an exception. However, it is recommended that such cases throw an exception unconditionally, as throwing only in certain cases can lead to programming errors.\n\nAn unmodifiable collection is a collection, all of whose mutator methods (as defined above) are specified to throw . Such a collection thus cannot be modified by calling any methods on it. For a collection to be properly unmodifiable, any view collections derived from it must also be unmodifiable. For example, if a List is unmodifiable, the List returned by is also unmodifiable.\n\nAn unmodifiable collection is not necessarily immutable. If the contained elements are mutable, the entire collection is clearly mutable, even though it might be unmodifiable. For example, consider two unmodifiable lists containing mutable elements. The result of calling might differ from one call to the next if the elements had been mutated, even though both lists are unmodifiable. However, if an unmodifiable collection contains all immutable elements, it can be considered effectively immutable.\n\nAn unmodifiable view collection is a collection that is unmodifiable and that is also a view onto a backing collection. Its mutator methods throw , as described above, while reading and querying methods are delegated to the backing collection. The effect is to provide read-only access to the backing collection. This is useful for a component to provide users with read access to an internal collection, while preventing them from modifying such collections unexpectedly. Examples of unmodifiable view collections are those returned by the , , and related methods.\n\nNote that changes to the backing collection might still be possible, and if they occur, they are visible through the unmodifiable view. Thus, an unmodifiable view collection is not necessarily immutable. However, if the backing collection of an unmodifiable view is effectively immutable, or if the only reference to the backing collection is through an unmodifiable view, the view can be considered effectively immutable.\n\nThis interface is a member of the Java Collections Framework."
    },
    {
        "link": "https://geeksforgeeks.org/list-interface-java-examples",
        "document": "The List Interface in Java extends the Collection Interface and is a part of java.util package. It is used to store the ordered collections of elements. So in a Java List, you can organize and manage the data sequentially.\n• None Maintained the order of elements in which they are added.\n• None The implementation classes of the List interface are ArrayList LinkedList Stack Vector\n• None Can add Null values that depend on the implementation.\n• None The List interface offers methods to access elements by their index and includes the listIterator() method ListIterator\n• None Using ListIterator, we can traverse the list in both forward and backward directions.\n\nLet us elaborate on creating objects or instances in a List. Since List is an interface, objects cannot be created of the type list. We always need a class that implements this List in order to create an object. And also, after the introduction of Generics in Java 1.5, it is possible to restrict the type of object that can be stored in the List. Just like several other user-defined ‘interfaces’ implemented by user-defined ‘classes’, List is an ‘interface’, implemented by the ArrayList class, pre-defined in java.util package.\n\nObj is the type of the object to be stored in List.\n\nThe common implementation classes of the interface are ArrayList, LinkedList, Stack, and Vector:\n• ArrayList LinkedList are the most widely used due to their dynamic resizing and efficient performance for specific operations.\n• Vector is considered a legacy class and is rarely used in modern Java programming. It is replaced by ArrayList and java.util.concurrent package.\n\nNow let us perform various operations using List Interface to have a better understanding of the same. We will be discussing the following operations listed below and later on implementing them via clean Java codes.\n\nSince List is an interface, it can be used only with a class that implements this interface. Now, let’s see how to perform a few frequently used operations on the List.\n• Operation 5: Accessing Elements in List using get() method\n• Operation 6: Checking if an element is present in the List using contains() method\n\nNow let us discuss the operations individually and implement the same in the code to grasp a better grip over it.\n\nIn order to add an element to the list, we can use the add() method. This method is overloaded to perform multiple operations based on different parameters.\n• add(Object o): This method is used to add an element at the end of the List.\n• add(int index, Object o): This method is used to add an element at a specific index in the List\n\nNote: If we do not specify the length of the array in the ArrayList constructor while creating the List object, using add(int index, Object) for any index i will throw an Exception if we have not specified the values for 0 to i-1 index already.\n\nNote: If we try to add element at index 1 before adding elements at index 0 it will throw an error. It is always recommended to add elements in a particular index only when the size is defined or to add them sequentially.\n\nAfter adding the elements, if we wish to change the element, it can be done using the set() method. Since List is indexed, the element which we wish to change is referenced by the index of the element. Therefore, this method takes an index and the updated element which needs to be inserted at that index.\n\nSearching for elements in the List interface is a common operation in Java programming. The List interface provides several methods to search for elements, such as the indexOf(), lastIndexOf() methods.\n\nThe indexOf() method returns the index of the first occurrence of a specified element in the list, while the lastIndexOf() method returns the index of the last occurrence of a specified element.\n• indexOf(Object o): Returns the index of the first occurrence of the specified element in the list, or -1 if the element is not found\n• lastIndexOf(Object o): Returns the index of the last occurrence of the specified element in the list, or -1 if the element is not found\n\nIn order to remove an element from a list, we can use the remove() method. This method is overloaded to perform multiple operations based on different parameters. They are:\n• remove(Object o): This method is used to simply remove an object from the List. If there are multiple such objects, then the first occurrence of the object is removed.\n• remove(int index): Since a List is indexed, this method takes an integer value which simply removes the element present at that specific index in the List. After removing the element, all the elements are moved to the left to fill the space and the indices of the objects are updated.\n\nIn order to access an element in the list, we can use the get() method, which returns the element at the specified index\n• None get(int index): This method returns the element at the specified index in the list.\n\n6. Checking if an element is present or not\n\nIn order to check if an element is present in the list, we can use the contains() method. This method returns true if the specified element is present in the list, otherwise, it returns false.\n• None contains(Object o): This method takes a single parameter, the object to be checked if it is present in the list.\n\nTill now we are having a very small input size and we are doing operations manually for every entity. Now let us discuss various ways by which we can iterate over the list to get them working for a larger sample set.\n\nMethods: There are multiple ways to iterate through the List. The most famous ways are by using the basic for loop in combination with a get() method to get the element at a specific index and the advanced for a loop.\n\nSince the main concept behind the different types of lists is the same, the list interface contains the following methods:\n\nThis method is used with Java List Interface to add an element at a particular index in the list. When a single parameter is passed, it simply adds the element at the end of the list. This method is used with List Interface in Java to add all the elements in the given collection to the list. When a single parameter is passed, it adds all the elements of the given collection at the end of the list. This method is used with Java List Interface to return the size of the list. This method is used to remove all the elements in the list. However, the reference of the list created is still stored. This method removes an element from the specified index. It shifts subsequent elements(if any) to left and decreases their indexes by 1. This method is used with Java List Interface to remove the first occurrence of the given element in the list. This method returns elements at the specified index. This method replaces elements at a given index with the new element. This function returns the element which was just replaced by a new element. This method returns the first occurrence of the given element or -1 if the element is not present in the list. This method returns the last occurrence of the given element or -1 if the element is not present in the list. This method is used with Java List Interface to compare the equality of the given element with the elements of the list. This method is used with List Interface in Java to return the hashcode value of the given list. This method is used with Java List Interface to check if the list is empty or not. It returns true if the list is empty, else false. This method is used with List Interface in Java to check if the list contains the given element or not. It returns true if the list contains the element. This method is used with Java List Interface to check if the list contains all the collection of elements. This method is used with List Interface in Java to sort the elements of the list on the basis of the given\n\nBoth the List interface and the Set interface inherits the Collection interface. However, there exists some differences between them.\n\nNow let us discuss the classes that implement the List Interface for which first do refer to the pictorial representation below to have a better understanding of the List interface. It is as follows:\n\nAbstractList, CopyOnWriteArrayList, and the AbstractSequentialList are the classes that implement the List interface. A separate functionality is implemented in each of the mentioned classes. They are as follows:\n• AbstractList: It is a skeletal implementation for the List interface but does not necessarily imply the list is unmodifiable. It is used for both mutable and immutable lists depending on the subclass implementation.\n• CopyOnWriteArrayList: This class implements the list interface. It is an enhanced version of in which all the modifications (add, set, remove, etc.) are implemented by making a fresh copy of the list.\n• AbstractSequentialList: This class extends AbstractList. This class is used to provide the skeletal implementation for lists that are accessed sequiencially (i.e iterators) to create a concrete class. It can implement the get(int index) and size() methods.\n\nWe will proceed in this manner.\n\nLet us discuss them sequentially and implement the same to figure out the working of the classes with the List interface.\n\nAn ArrayList class which is implemented in the collection framework provides us with dynamic arrays in Java. Though, it may be slower than standard arrays but can be helpful in programs where lots of manipulation in the array is needed. Let’s see how to create a list object using this class.\n\nVector is a class that is implemented in the collection framework implements a growable array of objects. Vector implements a dynamic array that means it can grow or shrink as required. Like an array, it contains components that can be accessed using an integer index. Vectors basically fall in legacy classes but now it is fully compatible with collections. Let’s see how to create a list object using this class.\n\nStack is a class that is implemented in the collection framework and extends the vector class models and implements the Stack data structure. The class is based on the basic principle of last-in-first-out. In addition to the basic push and pop operations, the class provides three more functions of empty, search and peek. Let’s see how to create a list object using this class.\n\nLinkedList is a class that is implemented in the collection framework which inherently implements the linked list data structure. It is a linear data structure where the elements are not stored in contiguous locations and every element is a separate object with a data part and address part. The elements are linked using pointers and addresses. Each element is known as a node. Due to the dynamicity and ease of insertions and deletions, they are preferred over the arrays. Let’s see how to create a list object using this class."
    },
    {
        "link": "https://geeksforgeeks.org/collection-interface-in-java-with-examples",
        "document": "The Collection interface in Java is a core member of the Java Collections Framework located in the java.util package. It is one of the root interfaces of the Java Collection Hierarchy. The Collection interface is not directly implemented by any class. Instead, it is implemented indirectly through its sub-interfaces like List, Queue, and Set.\n\nFor Example, the ArrayList class implements the List interface, a sub-interface of the Collection interface.\n\nHere, E represents the type of elements stored in the collection.\n\nNote: In the above syntax, we can replace any class with ArrayList if that class implements the Collection interface.\n\nThe Collection interface is part of a hierarchy that extends Iterable, means collections can be traversed.\n\nThe hierarchy also includes several key sub-interfaces:\n\nThe subInterfaces are sometimes called as Collection Types or SubTypes of Collection. These include the following:\n\nThe List interface represents an ordered collection that allows duplicates. It is implemented by classes like ArrayList, LinkedList, and Vector. Lists allow elements to be accessed by their index position.\n\nA set is an unordered collection of objects in which duplicate values cannot be stored. This set interface is implemented by various classes like HashSet, TreeSet, LinkedHashSet, etc.\n\nThis interface is very similar to the set interface. The only difference is that this interface has extra methods that maintain the ordering of the elements. The sorted set interface extends the set interface and is used to handle the data which needs to be sorted. The class which implements this interface is TreeSet.\n\nThe Queue interface represents a collection that follows FIFO (First-In-First-Out) order. It is implemented by classes like PriorityQueue, Deque, ArrayDeque, etc.\n\nThe Deque interface extends Queue and allows elements to be added or removed from both ends of the queue. It is implemented by ArrayDeque and LinkedList.\n\nThe NavigableSet interface extends SortedSet and provides additional methods for navigation such as finding the closest element.\n\nNote: The Collection Interface is not limited to the above classes, there are many more classes.\n\nEnsures that this collection contains the specified element (optional operation). Adds all the elements in the specified collection to this collection (optional operation). Removes all the elements from this collection (optional operation). Returns true if this collection contains the specified element. Returns true if this collection contains all the elements in the specified collection. Compares the specified object with this collection for equality. Returns the hash code value for this collection. Returns true if this collection contains no elements. Returns an iterator over the elements in this collection. Returns a possibly parallel Stream with this collection as its source. Removes a single instance of the specified element from this collection, if it is present (optional operation). Removes all of this collection’s elements that are also contained in the specified collection (optional operation). Removes all the elements of this collection that satisfy the given predicate. Retains only the elements in this collection that are contained in the specified collection (optional operation). Returns the number of elements in this collection. Creates a Spliterator over the elements in this collection. Returns a sequential Stream with this collection as its source. Returns an array containing all the elements in this collection. Returns an array containing all the elements in this collection, using the provided generator function to allocate the returned array. Returns an array containing all the elements in this collection; the runtime type of the returned array is that of the specified array.\n\nThe add(E e) and addAll(Collection c) methods provided by Collection can be used to add elements.\n\nThe remove(E e) and removeAll(Collection c) methods can be used to remove a particular element or a Collection of elements from a collection.\n\nTo iterate over the elements of Collection we can use iterator() method."
    },
    {
        "link": "https://medium.com/@reetesh043/java-collection-list-interface-7b8cc69f2a5a",
        "document": "In this article, we will discuss the List interface and its characteristics, methods, and implementation classes.\n\nThe List interface in Java is a fundamental component of the Java Collections Framework, designed to represent an ordered collection of elements. Lists allow duplicate elements and maintain the insertion order. Elements in a list can be accessed and manipulated using their indexes, starting from 0 for the first element.\n• Ordered Collection: Lists preserve the order of elements, ensuring that elements are stored in the sequence in which they were added.\n• Duplicates Allowed: Lists permit duplicate elements. Multiple occurrences of the same element can coexist in a list.\n• Indexed Access: Elements can be retrieved and modified using their index positions.\n• Dynamic Size: Lists can dynamically grow or shrink as elements are added or removed.\n\nNote: I have yet to include abstract classes like AbstractList, AbstractSequentialList, etc, and Interfaces like RandomAccess, Queue, etc for simplicity.\n\nThe List interface inherits several methods from the Collection interface, such as add(), remove(), contains(), isEmpty(), size(), and more.\n\nThe List interface provides additional methods to perform operations specific to lists, including:\n• get(int index): Returns the element at the specified index in the list.\n• set(int index, E element): Replaces the element at the specified index with the given element.\n• indexOf(Object o): Returns the index of the first occurrence of the specified element in the list, or -1 if not found.\n• lastIndexOf(Object o): Returns the index of the last occurrence of the specified element in the list, or -1 if not found.\n• addAll(Collection<? extends E> c): Appends all elements from the specified collection to the end of the list.\n• addAll(int index, Collection<? extends E> c): Inserts all elements from the specified collection into the list at the specified position.\n• remove(int index): Removes the element at the specified index from the list.\n• subList(int fromIndex, int toIndex): Returns a view of the portion of the list between the specified fromIndex (inclusive) and toIndex (exclusive).\n\nThe AbstractList class is an abstract class that provides a skeletal implementation of the List interface. The ArrayList, LinkedList, Vector, and CopyOnWriteArrayList classes are concrete classes that implement the List interface.\n\nThe ArrayList class is a dynamic array-based list. It is the most commonly used implementation of the List interface. The LinkedList class is a doubly linked list-based list. It is a good choice for lists that are frequently modified. The Vector class is a synchronized list. It is a good choice for lists that are accessed from multiple threads. The CopyOnWriteArrayList class is a thread-safe list. It is a good choice for lists that are frequently modified by multiple threads.\n\nHere’s a basic example using a LinkedList to illustrate the implementation and usage of its methods:\n\nOverall, the List interface is a powerful and flexible component for representing ordered collections of elements in Java. It is a good choice for a wide variety of applications."
    },
    {
        "link": "https://kafka.apache.org/documentation",
        "document": "Here is a description of a few of the popular use cases for Apache Kafka®. For an overview of a number of these areas in action, see this blog post.\n\nKafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.\n\nIn our experience messaging uses are often comparatively low-throughput, but may require low end-to-end latency and often depend on the strong durability guarantees Kafka provides.\n\nIn this domain Kafka is comparable to traditional messaging systems such as ActiveMQ or RabbitMQ.\n\nThe original use case for Kafka was to be able to rebuild a user activity tracking pipeline as a set of real-time publish-subscribe feeds. This means site activity (page views, searches, or other actions users may take) is published to central topics with one topic per activity type. These feeds are available for subscription for a range of use cases including real-time processing, real-time monitoring, and loading into Hadoop or offline data warehousing systems for offline processing and reporting.\n\nActivity tracking is often very high volume as many activity messages are generated for each user page view.\n\nKafka is often used for operational monitoring data. This involves aggregating statistics from distributed applications to produce centralized feeds of operational data.Many people use Kafka as a replacement for a log aggregation solution. Log aggregation typically collects physical log files off servers and puts them in a central place (a file server or HDFS perhaps) for processing. Kafka abstracts away the details of files and gives a cleaner abstraction of log or event data as a stream of messages. This allows for lower-latency processing and easier support for multiple data sources and distributed data consumption. In comparison to log-centric systems like Scribe or Flume, Kafka offers equally good performance, stronger durability guarantees due to replication, and much lower end-to-end latency.Many users of Kafka process data in processing pipelines consisting of multiple stages, where raw input data is consumed from Kafka topics and then aggregated, enriched, or otherwise transformed into new topics for further consumption or follow-up processing. For example, a processing pipeline for recommending news articles might crawl article content from RSS feeds and publish it to an \"articles\" topic; further processing might normalize or deduplicate this content and publish the cleansed article content to a new topic; a final processing stage might attempt to recommend this content to users. Such processing pipelines create graphs of real-time data flows based on the individual topics. Starting in 0.10.0.0, a light-weight but powerful stream processing library called Kafka Streams is available in Apache Kafka to perform such data processing as described above. Apart from Kafka Streams, alternative open source stream processing tools include Apache Storm and Apache Samza Event sourcing is a style of application design where state changes are logged as a time-ordered sequence of records. Kafka's support for very large stored log data makes it an excellent backend for an application built in this style.Kafka can serve as a kind of external commit-log for a distributed system. The log helps replicate data between nodes and acts as a re-syncing mechanism for failed nodes to restore their data. The log compaction feature in Kafka helps support this usage. In this usage Kafka is similar to Apache BookKeeper project.There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.\n\nKafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, exactly-once processing semantics and simple yet efficient management of application state.\n\nKafka Streams has a low barrier to entry: You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads. Kafka Streams transparently handles the load balancing of multiple instances of the same application by leveraging Kafka's parallelism model.\n\nTo learn more about Kafka Streams, visit the Kafka Streams page."
    },
    {
        "link": "https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html",
        "document": "This topic provides Apache Kafka® consumer configuration parameters. The configuration parameters are organized by order of importance, ranked from high to low.\n\nTo learn more about consumers in Kafka, see this free Apache Kafka 101 course. You can find code samples for the consumer in different languages in these guides.\n\nEnter a string to search and filter by configuration property name."
    },
    {
        "link": "https://redpanda.com/guides/kafka-tutorial-kafka-consumer-config",
        "document": "Apache Kafka® is a distributed streaming platform for building real-time data pipelines and streaming applications. It’s a high-throughput, low-latency platform that can handle millions of messages per second. Kafka is often used as a message broker, allowing different software systems to communicate by sending (producing) and receiving (consuming) messages.\n\nThis article will focus on configuring Kafka consumers. It’s important to properly set up and manage consumers to ensure the efficient and reliable processing of messages. We will cover essential configuration parameters, tips for optimizing consumers and avoiding pitfalls, and security and engineering best practices.\n\nThere are many different configurations that you can provide to a Kafka consumer, but the default values work for most use cases. We’ll highlight a few that you’ll want to make sure to set properly.\n\nThere are client libraries for Kafka in most major programming languages. You can look here for the list of the official clients. Though there might be some language-specific syntactical differences, the process is the same among them. A common practice is to set the configuration properties using a config file rather than using environment variables, as the properties are in dot case and aren’t valid environment variable names (dots aren’t allowed in environment variable names).\n\nUsing that config file to create a Kafka consumer using the Java client library might look something like this:\n\nHere’s an example using Python:\n\nThis is a comma-separated list of host:port pairs that the Kafka client will use to establish initial connections to the Kafka cluster. To allow for failover and high availability, it’s important to provide at least two Kafka brokers.\n\nThe group.id is a unique identifier that specifies the consumer group to which a particular consumer belongs. Each consumer group has a unique consumer offset representing the point in the topic where the consumer group is currently reading. When a consumer starts consuming messages from a topic, it’ll use the group.id to determine the consumer offset from which to start consuming.\n\nThe group.id is used to ensure that all consumers within the same group are reading from the same consumer offset and to allow consumers to automatically recover from failures by picking up where they left off. Accordingly, it’s essential to choose a unique group.id for each consumer group to avoid conflicts with other consumer groups.\n\nThe enable.auto.commit configuration option determines whether the consumer should automatically commit offsets to the Kafka broker at a set interval.\n\nIf enable.auto.commit is true, the consumer will commit offsets automatically at the configured interval. If enable.auto.commit is false, the consumer won’t commit offsets automatically and must manually commit offsets using the consumer’s commitSync() method.\n\nAuto-commit can be useful when the consumer doesn’t need to perform any additional processing on the messages after they’ve been consumed. This allows the consumer to automatically commit offsets and move on to the next batch of messages, improving performance and reducing the risk of message duplication.\n\nDisabling auto-commit can be useful when the consumer needs to perform additional processing on the messages after they have been consumed, such as storing them in a database or forwarding them to another system. Disabling auto-commit allows the consumer to commit offsets only after the additional processing has been completed, ensuring that messages aren’t lost in the event of failure.\n\nThe auto.offset.reset configuration option determines what action the consumer should take if there are no committed offsets for the consumer group or if the consumer is unable to connect to the last committed offset. The options for this setting are “latest,” which means the consumer will start consuming messages from the most recent message in the topic; and “earliest,” which means the consumer will start consuming messages from the beginning of the topic.\n\nThe auto.offset.reset setting is useful for ensuring that the consumer can start consuming messages from a known point in the topic. This is true even if it hasn’t previously consumed from the topic or has been unable to connect to the last committed offset.\n\nThe heartbeat.interval.ms configuration option specifies the interval at which the consumer will send heartbeats to the Kafka broker to indicate that it’s still active. Heartbeats are used to prevent the consumer group coordinator from marking the consumer as failed due to inactivity.\n\nSuppose the consumer doesn’t send a heartbeat within the configured interval. In that case, the consumer group coordinator will assume that the consumer has failed and will trigger a rebalance of the consumer group.\n\nThe heartbeat.interval.ms setting is important because it determines how quickly the consumer group coordinator will detect a failure and trigger a rebalance. A shorter interval will result in faster detection of failures but may also increase the load on the consumer group coordinator. Worse, it may result in an unwanted rebalancing of partition assignments among the consumers in the group, and consumption is blocked during rebalancing. Conversely, a longer interval may result in slower detection of failures but might reduce the load on the consumer group coordinator.\n\nThe max.poll.records configuration option specifies the maximum number of records the consumer will retrieve in a single call to the poll() method. This method is used to retrieve messages from the Kafka broker, and the max.poll.records setting determines the maximum number of messages returned in a single call.\n\nThe max.poll.records setting is important because it determines the maximum number of messages the consumer will process in a single batch. A larger value for max.poll.records may result in higher throughput but may also increase the risk of message duplication in the event of a failure. Conversely, a smaller value may result in lower throughput but could also reduce the risk of message duplication.\n\nThis configuration sets the maximum time, in milliseconds, that a Kafka consumer can go without polling the Kafka cluster for new messages. If a consumer goes longer than the specified time without polling, it will be considered as failed by the Kafka cluster, and a rebalance will be triggered.\n\nWhen setting the value of max.poll.interval.ms, it’s important to consider the expected rate of message consumption for your use case. If messages are consumed quickly, a lower value for max.poll.interval.ms can be set. However, if messages are consumed more slowly, or if the consumer may need to perform additional processing on messages before polling again, a higher value for max.poll.interval.ms may be necessary to prevent unwanted rebalances.\n\nIt’s also important to consider how often the consumer polls with relation to the rate of message production. Having a value that is too high may increase the latency of messages being consumed.\n\nWe recommend testing different values for max.poll.interval.ms and monitoring the performance of your Kafka consumer to determine the best value for your use case.\n\nNow that we’ve covered the theory side of various configurations, let’s briefly look at some common situations where configurations might be particularly important.\n\nIn an event-driven architecture, consumers are microservices that need to be highly responsive to each event. If you start them up and Kafka topic lag has built up, it may take a while before recent events are processed, and you will see this lag percolate out through your system. You may want to set auto.offset.reset to “latest” to ensure that the consumers are immediately available for new messages upon startup. Conversely, if you want the full event log to be processed even if the events were emitted while consumers were down, you need to be sure that this value is set to “earliest.”\n\nWhen your system needs nearly instantaneous insights or action from streaming data, a lower heartbeat.interval.ms value ensures that consumers are responsive and unhealthy consumers are caught quickly. Additionally, increasing the number of partitions in the topic will allow you to scale the consumer much higher to ensure that consumers don’t build up lag and latency.\n\nBatch data processing is often used to collect streams of data into batches of an optimized size and put them into files that can be uploaded and queried later. In these scenarios, many files with few records is undesirable because of the overhead of opening each file. To account for this, you should prefer larger max.poll.records and max.poll.interval.ms values to ensure that large batches can be processed without triggering rebalances.\n\nOnce you’ve started your consumers, you will likely begin looking for the optimal number of consumers so that they can keep up with producers. If consumers are falling behind, it may seem sensible to scale the number of consumers up. If you try this, initially, you will see increased consumption, but soon you will see it taper off. Why?\n\nKafka divides topics into partitions. Each partition is an ordered, immutable sequence of messages stored on a broker. The number of partitions in a topic can be configured when the topic is created and can be increased or decreased over time as needed.\n\nWhen a consumer group consumes from a topic, the topic’s partitions are assigned to the consumers in the group. When the group membership changes due to a consumer failure or a new consumer joining the group, the group coordinator sends an assignment request to each consumer. This assignment request includes a list of the partitions that the consumer will read from. The consumer then begins consuming from the assigned partitions. By default, partition assignments are distributed using a round-robin algorithm to evenly distribute the workload among the consumers and ensure that each consumer has a unique partition to consume from.\n\nBy dividing topics into partitions and assigning those partitions to consumers, Kafka can distribute the workload of consuming and processing messages among multiple consumers and scale the processing of messages horizontally. This allows Kafka to support high levels of message throughput and enables real-time processing of large volumes of data.\n\nHowever, if you scale your consumers beyond the number of partitions in the topics from which they’re consuming, your consumers will just sit idle, wasting resources.\n\nFor example, if you have four partitions in your topic, and you scale the number of consumers to five, one consumer will be left without an assigned partition and will be sitting idle, as shown in the following illustration.\n\nIf you need to continue scaling consumers beyond the number of partitions, increase the partition count for the topic on the Kafka broker and then scale your consumers. It’s good practice to give yourself some headroom to scale up when needed without increasing the number of partitions, but keep in mind that the maximum number of partitions you can create is limited by the resources the broker has (CPU cores, memory, and disk space). Another optimization is to have the number of consumers be a factor of the number of partitions so that the work is evenly distributed. See below for an example of an optimized partition and consumer count configuration.\n\nConfiguring a Kafka consumer involves setting some key properties that control the behavior and performance of the consumer. These properties determine the consumer group to which the consumer belongs, control whether the consumer automatically commits offsets, determine how the consumer handles starting from an unknown offset, control the interval at which the consumer sends heartbeats to the group coordinator, and dictate the maximum number of records that the consumer will retrieve in a single poll.\n\nWe hope this guide has helped provide a clear and concise overview of the key concepts and considerations for configuring and running Kafka consumers. We wish you the best of luck in following this guide and configuring and running your own Kafka consumers."
    },
    {
        "link": "https://docs.confluent.io/platform/current/clients/consumer.html",
        "document": "An Apache Kafka® Consumer is a client application that subscribes to (reads and processes) events. This section provides an overview of the Kafka consumer and an introduction to the configuration settings for tuning.\n\nThe Kafka consumer works by issuing “fetch” requests to the brokers leading the partitions it wants to consume. The consumer offset is specified in the log with each request. The consumer receives back a chunk of log that contains all of the messages in that topic beginning from the offset position. The consumer has significant control over this position and can rewind it to re-consume data if desired.\n\nA consumer group is a set of consumers that cooperate to consume data from some topics. You set the group for a consumer by setting its in the properties file for the consumer. To use the or methods provided by the KafkaConsumer API, you must assign the consumer to a consumer group by setting the property. If you don’t, an exception occurs when these methods are called. When assigned to a group, the partitions of all the topics are divided among the consumers in the group. As new group members arrive and old members leave, the partitions are re-assigned so that each member receives a proportional share of the partitions. This is known as rebalancing the group. One of the brokers is designated as the group’s coordinator and is responsible for managing the members of the group as well as their partition assignments. The coordinator of each group is chosen from the leaders of the internal offsets topic, , which is used to store committed offsets. Basically, the group’s ID is hashed to one of the partitions for this topic, and the leader of that partition is selected as the coordinator. In this way, management of consumer groups is divided roughly equally across all the brokers in the cluster, which allows the number of groups to scale by increasing the number of brokers. When the consumer starts up, it finds the coordinator for its group and sends a request to join the group. The coordinator then begins a group rebalance so that the new member is assigned its fair share of the group’s partitions. Every rebalance results in a new generation of the group. Each member in the group must send heartbeats to the coordinator in order to remain a member of the group. If no heartbeat is received before expiration of the configured session timeout, then the coordinator will kick the member out of the group and reassign its partitions to another member. For a short video that describes consumer groups, group leaders, and group coordinators, watch:\n\nAfter the consumer receives its assignment from the coordinator, it must determine the initial position for each assigned partition. When the group is first created, before any messages have been consumed, the position is set according to a configurable offset reset policy ( ). Typically, consumption starts either at the earliest offset or the latest offset. As a consumer in the group reads messages from the partitions assigned by the coordinator, it must commit the offsets corresponding to the messages it has read. If the consumer crashes or is shut down, its partitions will be re-assigned to another member, which will begin consumption from the last committed offset of each partition. If the consumer crashes before any offset has been committed, then the consumer which takes over its partitions will use the reset policy. The offset commit policy is crucial to providing the message delivery guarantees needed by your application. By default, the consumer is configured to use an automatic commit policy, which triggers a commit on a periodic interval. The consumer also supports a commit API which can be used for manual offset management. Correct offset management is crucial because it affects delivery semantics. By default, the consumer is configured to auto-commit offsets. The property sets the upper time bound of the commit interval. Using auto-commit offsets can give you “at-least-once” delivery, but you must consume all data returned from a call before any subsequent calls, or before closing the consumer. To explain further; when auto-commit is enabled, every time the method is called and data is fetched, the consumer is ready to automatically commit the offsets of messages that have been returned by the poll. If the processing of these messages is not completed before the next auto-commit interval, there’s a risk of losing the message’s progress if the consumer crashes or is otherwise restarted. In this case, when the consumer restarts, it will begin consuming from the last committed offset. When this happens, the last committed position can be as old as the auto-commit interval. Any messages that have arrived since the last commit are read again. If you want to reduce the window for duplicates, you can reduce the auto-commit interval, but some users may want even finer control over offsets. The consumer therefore supports a commit API which gives you full control over offsets. Note that when you use the commit API directly, you should first disable auto-commit in the configuration by setting the property to . Each call to the commit API results in an offset commit request being sent to the broker. Using the synchronous API, the consumer is blocked until that request returns successfully. This may reduce overall throughput since the consumer might otherwise be able to process records while that commit is pending. One way to deal with this is to increase the amount of data that is returned when polling. The consumer has a configuration setting which controls how much data is returned in each fetch. The broker will hold on to the fetch until enough data is available (or expires). The tradeoff, however, is that this also increases the amount of duplicates that have to be dealt with in a worst-case failure. A second option is to use asynchronous commits. Instead of waiting for the request to complete, the consumer can send the request and return immediately by using asynchronous commits. So if it helps performance, why not always use async commits? The main reason is that the consumer does not retry the request if the commit fails. This is something that committing synchronously gives you for free; it will retry indefinitely until the commit succeeds or an unrecoverable error is encountered. The problem with asynchronous commits is dealing with commit ordering. By the time the consumer finds out that a commit has failed, you may already have processed the next batch of messages and even sent the next commit. In this case, a retry of the old commit could cause duplicate consumption. Instead of complicating the consumer internals to try and handle this problem in a sane way, the API gives you a callback which is invoked when the commit either succeeds or fails. If you like, you can use this callback to retry the commit, but you will have to deal with the same reordering problem. Offset commit failures are merely annoying if the following commits succeed since they won’t actually result in duplicate reads. However, if the last commit fails before a rebalance occurs or before the consumer is shut down, then offsets will be reset to the last commit and you will likely see duplicates. A common pattern is therefore to combine async commits in the poll loop with sync commits on rebalances or shut down. Committing on close is straightforward, but you need a way to hook into rebalances. Each rebalance has two phases: partition revocation and partition assignment. The revocation method is always called before a rebalance and is the last chance to commit offsets before the partitions are re-assigned. The assignment method is always called after the rebalance and can be used to set the initial position of the assigned partitions. In this case, the revocation hook is used to commit the current offsets synchronously. In general, asynchronous commits should be considered less safe than synchronous commits. Consecutive commit failures before a crash will result in increased duplicate processing. You can mitigate this danger by adding logic to handle commit failures in the callback or by mixing occasional synchronous commits, but you shouldn’t add too much complexity unless testing shows it is necessary. If you need more reliability, synchronous commits are there for you, and you can still scale up by increasing the number of topic partitions and the number of consumers in the group. But if you just want to maximize throughput and you’re willing to accept some increase in the number of duplicates, then asynchronous commits may be a good option. A somewhat obvious point, but one that’s worth making is that asynchronous commits only make sense for “at least once” message delivery. To get “at most once,” you need to know if the commit succeeded before consuming the message. This implies a synchronous commit unless you have the ability to “unread” a message after you find that the commit failed. In the examples, we show several detailed examples of the commit API and discuss the tradeoffs in terms of performance and reliability. When writing to an external system, the consumer’s position must be coordinated with what is stored as output. That is why the consumer stores its offset in the same place as its output. For example, a connector populates data in HDFS along with the offsets of the data it reads so that it is guaranteed that either data and offsets are both updated, or neither is. A similar pattern is followed for many other data systems that require these stronger semantics, and for which the messages do not have a primary key to allow for deduplication. This is how Kafka supports exactly-once processing in Kafka Streams, and the transactional producer or consumer can be used generally to provide exactly-once delivery when transferring and processing data between Kafka topics. Otherwise, Kafka guarantees at-least-once delivery by default, and you can implement at-most-once delivery by disabling retries on the producer and committing offsets in the consumer prior to processing a batch of messages. Consumers can fetch/consume from out-of-sync follower replicas if using a fetch-from-follower configuration. See Multi-Region Clusters to learn more.\n\nWhile the Java consumer does all IO and processing in the foreground thread, librdkafka-based clients (C/C++, Python, Go and C#) use a background thread. The main consequence of this is that polling is totally safe when used from multiple threads. You can use this to parallelize message handling in multiple threads. From a high level, poll is taking messages off of a queue which is filled in the background. Another consequence of using a background thread is that all heartbeats and rebalancing are executed in the background. The benefit of this is that you don’t need to worry about message handling causing the consumer to “miss” a rebalance. The drawback, however, is that the background thread will continue heart beating even if your message processor dies. If this happens, then the consumer will continue to hold on to its partitions and the read lag will continue to build until the process is shut down. Although the clients have taken different approaches internally, they are not as far apart as they seem. To provide the same abstraction in the Java client, you could place a queue in between the poll loop and the message processors. The poll loop would fill the queue and the processors would pull messages off of it."
    },
    {
        "link": "https://kafka.apache.org/20/documentation.html",
        "document": "Here is a description of a few of the popular use cases for Apache Kafka®. For an overview of a number of these areas in action, see this blog post.\n\nKafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput, built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications.\n\nIn our experience messaging uses are often comparatively low-throughput, but may require low end-to-end latency and often depend on the strong durability guarantees Kafka provides.\n\nIn this domain Kafka is comparable to traditional messaging systems such as ActiveMQ or RabbitMQ.\n\nThe original use case for Kafka was to be able to rebuild a user activity tracking pipeline as a set of real-time publish-subscribe feeds. This means site activity (page views, searches, or other actions users may take) is published to central topics with one topic per activity type. These feeds are available for subscription for a range of use cases including real-time processing, real-time monitoring, and loading into Hadoop or offline data warehousing systems for offline processing and reporting.\n\nActivity tracking is often very high volume as many activity messages are generated for each user page view.\n\nKafka is often used for operational monitoring data. This involves aggregating statistics from distributed applications to produce centralized feeds of operational data.Many people use Kafka as a replacement for a log aggregation solution. Log aggregation typically collects physical log files off servers and puts them in a central place (a file server or HDFS perhaps) for processing. Kafka abstracts away the details of files and gives a cleaner abstraction of log or event data as a stream of messages. This allows for lower-latency processing and easier support for multiple data sources and distributed data consumption. In comparison to log-centric systems like Scribe or Flume, Kafka offers equally good performance, stronger durability guarantees due to replication, and much lower end-to-end latency.Many users of Kafka process data in processing pipelines consisting of multiple stages, where raw input data is consumed from Kafka topics and then aggregated, enriched, or otherwise transformed into new topics for further consumption or follow-up processing. For example, a processing pipeline for recommending news articles might crawl article content from RSS feeds and publish it to an \"articles\" topic; further processing might normalize or deduplicate this content and publish the cleansed article content to a new topic; a final processing stage might attempt to recommend this content to users. Such processing pipelines create graphs of real-time data flows based on the individual topics. Starting in 0.10.0.0, a light-weight but powerful stream processing library called Kafka Streams is available in Apache Kafka to perform such data processing as described above. Apart from Kafka Streams, alternative open source stream processing tools include Apache Storm and Apache Samza Event sourcing is a style of application design where state changes are logged as a time-ordered sequence of records. Kafka's support for very large stored log data makes it an excellent backend for an application built in this style.Kafka can serve as a kind of external commit-log for a distributed system. The log helps replicate data between nodes and acts as a re-syncing mechanism for failed nodes to restore their data. The log compaction feature in Kafka helps support this usage. In this usage Kafka is similar to Apache BookKeeper project.There are a plethora of tools that integrate with Kafka outside the main distribution. The ecosystem page lists many of these, including stream processing systems, Hadoop integration, monitoring, and deployment tools.\n\nKafka Streams is a client library for processing and analyzing data stored in Kafka. It builds upon important stream processing concepts such as properly distinguishing between event time and processing time, windowing support, exactly-once processing semantics and simple yet efficient management of application state.\n\nKafka Streams has a low barrier to entry: You can quickly write and run a small-scale proof-of-concept on a single machine; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads. Kafka Streams transparently handles the load balancing of multiple instances of the same application by leveraging Kafka's parallelism model.\n\nLearn More about Kafka Streams read this Section."
    }
]