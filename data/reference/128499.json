[
    {
        "link": "https://docs.ros.org",
        "document": ""
    },
    {
        "link": "http://wiki.ros.org/rospy",
        "document": ""
    },
    {
        "link": "http://alvarestech.com/temp/capp/GDT_Forma3D/Programming%20Robots%20with%20ROS%20by%20Morgan%20Quigley,%20Brian%20Gerkey,%20William%20D.%20Smart%20(z-lib.org).pdf",
        "document": ""
    },
    {
        "link": "http://repo.darmajaya.ac.id/5558/1/Programming%20Robots%20with%20ROS_%20A%20Practical%20Introduction%20to%20the%20Robot%20Operating%20System%20%28%20PDFDrive%20%29.pdf",
        "document": ""
    },
    {
        "link": "https://ucr-robotics.readthedocs.io/en/latest/intro_ros.html",
        "document": "Robot Operating System (ROS) is a collection of software frameworks for robot software developments. Although ROS is not an operating system, it provides services designed for a heterogeneous computer cluster and a majority of the packages are open source. One of its main advantages is that the client libraries (C++ and Python) allow nodes written in different programming languages to communicate. It is a powerful tool embedded with hardware abstraction, low-level device control, implementation of commonly used functionality, message-passing between processes, and package management. While here we will only list the basic (key) components that might be used.\n\nIn ROS, all resources (e.g., data from different sensors) are “Messages” of . These “Messages”” could be accessed and transmitted among as (as well as and ).\n• : An executable file, can publish or subscribe to a .\n• or : Broadcast or receive the “Message” Their relation could be expressed in the following figure.\n\n\"welcome to the Robotics Lab \" Now we are going to explain each sentence of the sample script. Please read carefully and try to write your own code.\n• This first line makes sure your code is executed as a python script.\n• As mentioned, “rospy” is the python client library that needs to be imported if you are writting a ROS Node.\n• This line imports a well-defined message type “String” that will be later used in “rospy.Publisher”. You could find all information about a type of message by typing on google. Most of the message types could be found at std_msgs or common_msgs.\n• Initialize the node with name “talker”.\n• Declare a that your “talker” will publish messages to the “chatter”. The format of the message is defined as “String”, i.e. the topic using the message type “String”. The “queue_size” limits the amount of queued messages if any subscriber is not receiving them fast enough.\n• This loop is a fairly standard rospy construct: checking the “rospy.is_shutdown()” flag and then doing work. In this case, the “work” is a call to that publishes a string to our “chatter” . Keep in mind that the “content” has format “String” (consistent with what we declared in “pub”). Along with the string information we concat also the corresponding timestamp data, by storing it first in a “Header” variable. The loop calls “rate.sleep()”, which sleeps just long enough to maintain the desired rate (10 hz in that script) through the loop. \"welcome to the Robotics Lab\n• is a good tool to clarify the relations among topics and nodes by providing a ROS communication graph. You could check whether your communication algorithm works or not. To use it, just open a new terminal and type , an example is shown as follows.\n\nGazebo is an open-source 3D robotics simulator. It offers the ability to accurately and efficiently simulate populations of robots in complex indoor and outdoor environments. You could use it for rapidly and safely testing your algorithms, designing robots and collecting or training a large amount of data using realistic scenarios. Generally speaking, if you use an existing robot, you could find its model in their documents. If you are building your own robot and want to simulate it in Gazebo, please look at the tutorial."
    },
    {
        "link": "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html",
        "document": "Click here to download the full example code\n\nThis tutorial shows how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v1 task from Gymnasium.\n\nYou might find it helpful to read the original Deep Q Learning (DQN) paper\n\nThe agent has to decide between two actions - moving the cart left or right - so that the pole attached to it stays upright. You can find more information about the environment and other more challenging environments at Gymnasium’s website.\n\nAs the agent observes the current state of the environment and chooses an action, the environment transitions to a new state, and also returns a reward that indicates the consequences of the action. In this task, rewards are +1 for every incremental timestep and the environment terminates if the pole falls over too far or the cart moves more than 2.4 units away from center. This means better performing scenarios will run for longer duration, accumulating larger return.\n\nThe CartPole task is designed so that the inputs to the agent are 4 real values representing the environment state (position, velocity, etc.). We take these 4 inputs without any scaling and pass them through a small fully-connected network with 2 outputs, one for each action. The network is trained to predict the expected value for each action, given the input state. The action with the highest expected value is then chosen.\n\nFirst, let’s import needed packages. Firstly, we need gymnasium for the environment, installed by using . This is a fork of the original OpenAI Gym project and maintained by the same team since Gym v0.19. If you are running this in Google Colab, run:\n\nWe’ll also use the following from PyTorch:\n\nWe’ll be using experience replay memory for training our DQN. It stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure. For this, we’re going to need two classes:\n• None - a named tuple representing a single transition in our environment. It essentially maps (state, action) pairs to their (next_state, reward) result, with the state being the screen difference image as described later on.\n• None - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements a method for selecting a random batch of transitions for training. Now, let’s define our model. But first, let’s quickly recap what a DQN is.\n\nOur environment is deterministic, so all equations presented here are also formulated deterministically for the sake of simplicity. In the reinforcement learning literature, they would also contain expectations over stochastic transitions in the environment. Our aim will be to train a policy that tries to maximize the discounted, cumulative reward \\(R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t\\), where \\(R_{t_0}\\) is also known as the return. The discount, \\(\\gamma\\), should be a constant between \\(0\\) and \\(1\\) that ensures the sum converges. A lower \\(\\gamma\\) makes rewards from the uncertain far future less important for our agent than the ones in the near future that it can be fairly confident about. It also encourages agents to collect reward closer in time than equivalent rewards that are temporally far away in the future. The main idea behind Q-learning is that if we had a function \\(Q^*: State \\times Action \\rightarrow \\mathbb{R}\\), that could tell us what our return would be, if we were to take an action in a given state, then we could easily construct a policy that maximizes our rewards: However, we don’t know everything about the world, so we don’t have access to \\(Q^*\\). But, since neural networks are universal function approximators, we can simply create one and train it to resemble \\(Q^*\\). For our training update rule, we’ll use a fact that every \\(Q\\) function for some policy obeys the Bellman equation: The difference between the two sides of the equality is known as the temporal difference error, \\(\\delta\\): To minimize this error, we will use the Huber loss. The Huber loss acts like the mean squared error when the error is small, but like the mean absolute error when the error is large - this makes it more robust to outliers when the estimates of \\(Q\\) are very noisy. We calculate this over a batch of transitions, \\(B\\), sampled from the replay memory: Our model will be a feed forward neural network that takes in the difference between the current and previous screen patches. It has two outputs, representing \\(Q(s, \\mathrm{left})\\) and \\(Q(s, \\mathrm{right})\\) (where \\(s\\) is the input to the network). In effect, the network is trying to predict the expected return of taking each action given the current input. # Called with either one element to determine next action, or a batch"
    },
    {
        "link": "https://medium.com/@hkabhi916/mastering-deep-q-learning-with-pytorch-a-comprehensive-guide-a7e690d644fc",
        "document": "In the realm of reinforcement learning, Deep Q-Learning (DQN) has emerged as a powerful technique for training agents to make optimal decisions in complex environments. By combining deep neural networks with Q-learning, DQN enables agents to learn effective strategies through trial and error. In this blog post, we’ll delve into the world of Deep Q-Learning using PyTorch, providing a step-by-step guide, detailed explanations, and practical examples to help you master this cutting-edge technique.\n\nAt its core, Deep Q-Learning extends traditional Q-learning by utilizing deep neural networks to approximate the Q-function, which represents the expected cumulative reward of taking a particular action in a given state. The key idea is to train a neural network to predict Q-values for all possible actions in a state, enabling the agent to select the action with the highest predicted Q-value to maximize its long-term rewards.\n\nLet’s dive into the implementation of Deep Q-Learning using PyTorch. We’ll build a DQN agent to play the classic Atari game, Breakout. Here’s a step-by-step guide:\n\nWe’ll start by defining a deep neural network architecture to approximate the Q-function. This network will take the game screen (state) as input and output Q-values for each possible action.\n\nImplement an experience replay buffer to store and sample experiences (state, action, reward, next state) for training the DQN. This buffer helps stabilize training by decorrelating the training samples and breaking the sequential nature of experiences.\n\nTrain the DQN agent by iteratively collecting experiences from interactions with the environment, updating the Q-network parameters using the Bellman equation, and periodically updating the target network to improve stability.\n\nIn this specific case, refers to the Breakout environment, which is a classic Atari game where the agent controls a paddle to bounce a ball and break bricks at the top of the screen. The goal of the game is to clear as many bricks as possible while preventing the ball from falling to the bottom of the screen.\n\nImplement an exploration strategy, such as ε-greedy exploration, to balance exploration (trying new actions) and exploitation (selecting the best-known actions). This ensures the agent explores the environment sufficiently to discover optimal strategies.\n\nIn this blog post, we’ve embarked on a journey to master Deep Q-Learning using PyTorch, a powerful technique for training agents to make optimal decisions in complex environments. We’ve explored the foundational concepts of Deep Q-Learning, its implementation in PyTorch, and practical considerations for training DQN agents effectively. By following the step-by-step guide and understanding the underlying principles, you’re now equipped to apply Deep Q-Learning to a wide range of reinforcement learning tasks and unleash the potential of intelligent agents in various domains."
    },
    {
        "link": "https://github.com/Corbelli/dqn-pytorch",
        "document": "This project is a Pytorch implementation of several variants of the Deep Q Learning (DQN) model. It is based on the material provided by Udacity's Deep Reinforcement Learning Nanodegree. The objective is to use one of the Unity ML-Agents libraries to demonstrate how different DQN implementations can be coded, trained and evaluation.\n\nThe code structure builds from the Nature DQN, and incrementally implements 3 modifications, in order: Double Q Learning, Dueling Networks and Prioritized Experience Replay. The articles for each one of these implementations can be found at\n\nAlthough the code can be used in any operating system, the compiled versions of the Unity ML-Agents environment used are only available to MAC (with graphics) and Linux (headless version, for faster training). To download the Linux version with graphics or the Windows versions, please use the links below (provided by Udacitys Nanodeegre):\n• It is recommended to use mini-conda to manage python environments. In order to install the dependencies the initial step would be:\n• The necessary packages to run the code can be obtained by cloning and installing Udacity's Nanodegrees repository (plus, the repo is lots of fun to anyone wanting to explore more projects related to reinforcement learning)\n• To use jupyter notebooks or jupyter lab properly, it is important to create an ipython kernel.\n• Before running code in a notebook, change the kernel to match the environment by using the drop-down menu.\n\nThe environment consists of a robot surround by a boxed enclosure filled with yellow and blue bananas At each time step, it has four actions at its disposal:\n\nThe state-space has dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction. A reward of is provided for collecting a yellow banana, and a reward of is provided for collecting a blue banana.\n\nThe environment is considered solved when an average score above 13.0 is obtained for the last 100 episodes.\n\nTo get started with the code, the first step is to load the Unity-ML agent's environment. It is important to note that the path must be adjusted to the location of the environment file in your system. The environment is organized around brains that represent each controllable agent. In the banana environment, it suffices to use the first brain. The initial code would be:\n\nThe next step is to load one of the implemented agents and corresponding training class. For the banana environment, the state size must be 37, and action size 4. The training setup must include the number of episodes, and the values for the epsilon and beta parameters evolution. An example with the values used in the trained models and the Prioritized Replay model is:\n\nTo train the agent and get the scores during training, use the train function of the training class.\n• track_every - the number of steps between the tracking of the training\n• plot - wether or not the tracking is visual (with an evolution plot) or only informative (with prints)\n• success_thresh - The threshold for the moving average of the last 100 runs. When it is conquered, the training stops and the weights are saved in the models folder\n• weights - The name of the weights file where the model will be saved\n\nOnce the scores is saved, you can save the training with a name and description using the Benchmark class. To do so, just do as the code bellow.\n\nTo check all available saved trainings, check the Benchmarks section. To see a trained model play, just load the weights for the agent with the load_weights function, and use the play function of the training class.\n\nBelow is a comparison with the Prioritized Replay model, of an untrained agent, with an agent trained for 2000 steps. Check how the trained model is able to search for yellow bananas while avoiding blue ones\n\nThe folder system in the code is structured as:\n• benchmarks - Training scores and description of each model already trained\n• dqn - Main library, with different implementations of the DQN model\n\nThe DQN library is organized in classes as follows\n• Model Modules - Modules to train and use each one of the implementations\n• Benchmarks - Class to load and display the saved training scores\n\nEach model module is organized as\n• Agent - The agent implementation, responsible to interact and learn with the environment\n• Model - Neural Net implementation in PyTorchh of the DQN architecture\n• Training - Convenience class to handle training and tracking of the agent\n\nFor a description of the implementation of the most complex variant, see the Report file.\n\nTraining using the other implemented models is very similar to the instructions in the Training and Playing. The available models, corresponding classes and code examples are listed bellow\n• Dueling DQN \n\n DQN with modification to implement double q learning and a dueling network architecture\n• Prioritized Replay \n\n DQN with modification to implement double q learning, a dueling network architecture, and Prioritized replay\n\nThe 4 models implemented have trained versions saved in the models folder. Those models are named as:\n\nAlso, the scores for every training along with a description of the model used are saved in the benchmarks folder. The available scores are:\n\nTo load a specific model, just use the function load_bench from the Benchmarks class. The load class receives the name of the saved scores. To plot the scores, use the plot_bench function. This function receives the scores vector, the title of the plot\n\nThe plot function receives the scores vector, the title of the plot, the number of runs to use in the moving mean calculation (or None for not displaying the mean) and the opacity to use for the plotting of the scores.\n\nTo see a comparison of all the trainings, you can load a dictionary of { 'model name': [scores vector] } with the load_benchmarks function. To plot the dictionary, use the plot_benchs function\n\nFor further details of the implementation of the reinforcement learning agent, the Prioritized Replay model architecture is describe with details in the Report file."
    },
    {
        "link": "https://github.com/hungtuchen/pytorch-dqn",
        "document": "This project is pytorch implementation of Human-level control through deep reinforcement learning and I also plan to implement the following ones:\n\nThis project reuses most of the code in https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3\n\nThe model is defined in\n\nThe algorithm is defined in\n\nThe running script and hyper-parameters are defined in"
    },
    {
        "link": "https://discuss.pytorch.org/t/dqn-example-from-pytorch-diverged/4123",
        "document": "I just implemented my DQN by following the example from PyTorch. I found nothing weird about it, but it diverged. I run the original code again and it also diverged. The behaviors are like this. It often reaches a high average (around 200, 300) within 100 episodes. Then it starts to perform worse and worse, and stops around an average around 20, just like some random behaviors. I tried a lot of changes, the original version was surprisingly the best one, as described.\n\nI tried after I saw your reply. The nature DQN paper didn’t mention how frequently they updated their target network. But I tried every 2, 10, and 100 iterations. None of them solved my divergence problem. But they made my averages stay at 200 for a very long time. I guess it’s working, but not enough. BTW I’m using exactly the same implementation given by PyTorch tutorials, which takes screens as inputs.\n\nHow do you keep the average over 200 for a period? In my experiments, it cannot keep. There is only bursts into high durations and the following duration will fall to very low values. I also tried the the update frequency but it does not work. Can you show me your duration-episode curve? I am also using the difference image as the state.\n\nHey xuehy. Sorry since it was not very successful so I didn’t keep a lot of records. What I was doing was just adjusting hyperparameters. What I do remember is, memory capacity matters a lot. So far I haven’t found a solution to make DQN converge. The potential reasons in my mind are 1) NN models are prone to divergence in theory; 2) simple image recognition model doesn’t learn CartPole very well. I don’t know how far did you get since then, but I would suggest both include a LSTM cell, and try your model on other games. I haven’t done them yet though.\n\nI found that the usage of smooth l1 loss (Huber) always led to divergence on the cart pole environment (somebody else also had that problem I’ll add the link later) It worked for me when using l2 (mse) loss Further investigation showed that the loss explodedn with the Huber loss, (I ll need to check what happened to the gradient) I found this very strange considering that the Huber loss is specifically designed to be stable If anyone has any idea why this happens please let me know Tldr if you usw smoothl1 loss, just try mse\n\nDefinitely, it’s way harder to train than training your model with the observations, which in that particular case of return the underlying physical model of the game. However, it is also way cooler, as it demonstrates the power of this algorithm However, I’d like to mention that the original Deep Q Learning algorithm described by Mnih et al. didn’t use the difference of previous and current frame as a state representation, but rather a stack of the 4 last seen and processed frames (Resulting in a 1x4x84x84 input tensor as they were training on gray scale images). Also, they leveraged a technique called frame skipping: The agent selects an action on every kth frame (I think it was 4th in the paper) and this action is applied to the environment for k-1 frames. This helps the agent to see more frames in the same time, as computing an action requires significantly more computing power than stepping the environment. Also, the paper mentioned priorly deployed a second ‘target network’, which would be updated every 10.000 frames, which additionally stabilizes the algorithm.\n\nI wanted to add my two cents. One of the problems is related to what is known as the “vanishing gradient” problem in supervised learning. Bottom line is, your agent learns a good policy and then stops exploring the areas that are sub-optimal. At one point, all your agent knows about is the best states, and actions because there is nothing other than the samples coming from a near-optimal policy in your replay buffer. Then, all updates to your network come from the same couple of near-optimal states, actions. So, due to the vanishing gradient problem, your agent forgets how to get to the best, straight-up pole, position. It knows how to stay there, but not how to get there. You have no more samples of that in your replay buffer, guess what. As soon as the initial state is minimally different, chaos… BTW, this happens in virtually any DQN/Cart Pole example I’ve tested, even the ones using the continuous variables as opposed to images. Yes, this includes OpenAI Baselines! Just change the code so it keeps training indefinitely and you’ll find the same divergence issues. The way I got it to perform better is to increase the Replay Buffer size to 100,000 or 1,000,000 (you may need a solid implementation - see OpenAI Baselines: https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py), and increase the Batch Size to ~64 or ~128. I think reducing the learning rate would help as well, but also slow down learning, of course. Though, I suppose this will only postpone the issues, but at least I ran 1,000 episodes of perfect performance which works for me. Finally, I found it interesting to review the basics described by Sutton in his book: Sutton & Barto Book: Reinforcement Learning: An Introduction From the book, take a look at Example 3.4: Pole-Balancing. Example 3.4: Pole-Balancing The objective in this task is to apply forces to a cart moving along\n\n a track so as to keep a pole hinged to the cart from falling over: A failure is said to occur if the pole\n\n falls past a given angle from vertical or if the cart runs off the track. The pole is reset to vertical\n\n after each failure. This task could be treated as episodic, where the natural episodes are the repeated\n\n attempts to balance the pole. The reward in this case could be +1 for every time step on which failure\n\n did not occur, so that the return at each time would be the number of steps until failure. In this case,\n\n successful balancing forever would mean a return of infinity. Alternatively, we could treat pole-balancing\n\n as a continuing task, using discounting. In this case the reward would be −1 on each failure and zero\n\n at all other times. The return at each time would then be related to −γ\n\n K, where K is the number of\n\n time steps before failure. In either case, the return is maximized by keeping the pole balanced for as\n\n long as possible. As the Cart-Pole example is setup as an episodic task in OpenAI Gym; +1 every time step on which failure did not occur, gamma should then be set to 1."
    }
]