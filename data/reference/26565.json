[
    {
        "link": "https://numpy.org/doc/2.2/reference/routines.array-manipulation.html",
        "document": "Return a new array with sub-arrays along an axis deleted.\n\nInsert values along the given axis before the given indices.\n\nAppend values to the end of an array.\n\nReturn a new array with the specified shape.\n\nRemove values along a dimension which are zero along all other.\n\nFind the unique elements of an array."
    },
    {
        "link": "https://numpy.org/doc/2.1/reference/routines.array-manipulation.html",
        "document": "Return a new array with sub-arrays along an axis deleted.\n\nInsert values along the given axis before the given indices.\n\nAppend values to the end of an array.\n\nReturn a new array with the specified shape.\n\nTrim the leading and/or trailing zeros from a 1-D array or sequence.\n\nFind the unique elements of an array."
    },
    {
        "link": "https://numpy.org/doc/stable/reference/index.html",
        "document": "This reference manual details functions, modules, and objects included in NumPy, describing what they are and what they do. For learning how to use NumPy, see the complete documentation.\n\nLarge parts of this manual originate from Travis E. Oliphant’s book Guide to NumPy (which generously entered Public Domain in August 2008). The reference documentation for many of the functions are written by numerous contributors and developers of NumPy."
    },
    {
        "link": "https://numpy.org/doc/stable/reference/arrays.ndarray.html",
        "document": "An is a (usually fixed-size) multidimensional container of items of the same type and size. The number of dimensions and items in an array is defined by its , which is a of N non-negative integers that specify the sizes of each dimension. The type of items in the array is specified by a separate data-type object (dtype), one of which is associated with each ndarray.\n\nAs with other container objects in Python, the contents of an can be accessed and modified by indexing or slicing the array (using, for example, N integers), and via the methods and attributes of the .\n\nDifferent can share the same data, so that changes made in one may be visible in another. That is, an ndarray can be a “view” to another ndarray, and the data it is referring to is taken care of by the “base” ndarray. ndarrays can also be views to memory owned by Python or objects implementing the or array interfaces.\n\nAn instance of class consists of a contiguous one-dimensional segment of computer memory (owned by the array, or by some other object), combined with an indexing scheme that maps N integers into the location of an item in the block. The ranges in which the indices can vary is specified by the of the array. How many bytes each item takes and how the bytes are interpreted is defined by the data-type object associated with the array. A segment of memory is inherently 1-dimensional, and there are many different schemes for arranging the items of an N-dimensional array in a 1-dimensional block. NumPy is flexible, and objects can accommodate any strided indexing scheme. In a strided scheme, the N-dimensional index \\((n_0, n_1, ..., n_{N-1})\\) corresponds to the offset (in bytes): from the beginning of the memory block associated with the array. Here, \\(s_k\\) are integers which specify the of the array. The column-major order (used, for example, in the Fortran language and in Matlab) and row-major order (used in C) schemes are just specific kinds of strided scheme, and correspond to memory that can be addressed by the strides: Both the C and Fortran orders are contiguous, i.e., single-segment, memory layouts, in which every part of the memory block can be accessed by some combination of the indices. Contiguous arrays and single-segment arrays are synonymous and are used interchangeably throughout the documentation. While a C-style and Fortran-style contiguous array, which has the corresponding flags set, can be addressed with the above strides, the actual strides may be different. This can happen in two cases:\n• None If then for any legal index . This means that in the formula for the offset \\(n_k = 0\\) and thus \\(s_k n_k = 0\\) and the value of \\(s_k\\) = self.strides[k] is arbitrary.\n• None If an array has no elements ( ) there is no legal index and the strides are never used. Any array with no elements may be considered C-style and Fortran-style contiguous. Point 1. means that and always have the same contiguity and flags value. This also means that even a high dimensional array could be C-style and Fortran-style contiguous at the same time. An array is considered aligned if the memory offsets for all elements and the base offset itself is a multiple of . Understanding memory-alignment leads to better performance on most hardware. It does not generally hold that for C-style contiguous arrays or for Fortran-style contiguous arrays is true. Data in new is in the row-major (C) order, unless otherwise specified, but, for example, basic array slicing often produces views in a different scheme. Several algorithms in NumPy work on arbitrarily strided arrays. However, some algorithms require single-segment arrays. When an irregularly strided array is passed in to such algorithms, a copy is automatically made.\n\nArray attributes reflect information that is intrinsic to the array itself. Generally, accessing an array through its attributes allows you to get and sometimes set intrinsic properties of the array without creating a new array. The exposed attributes are the core parts of an array and only some of them can be reset meaningfully without creating a new array. Information on each attribute is given below. The following attributes contain information about the memory layout of the array: Information about the memory layout of the array. Tuple of bytes to step in each dimension when traversing an array. Python buffer object pointing to the start of the array's data. Number of elements in the array. Length of one array element in bytes. Total bytes consumed by the elements of the array. Base object if memory is from some other object. The data type object associated with the array can be found in the attribute: The real part of the array. The imaginary part of the array. An object to simplify the interaction of the array with the ctypes module.\n\nAn object has many methods which operate on or with the array in some fashion, typically returning an array result. These methods are briefly explained below. (Each method’s docstring has a more complete description.) For the following methods there are also corresponding functions in : , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , . Copy an element of an array to a standard Python scalar and return it. Return the array as an -levels deep nested list of Python scalars. A compatibility alias for , with exactly the same behavior. Construct Python bytes containing the raw data bytes in the array. Write array to a file as text or binary (default). Dump a pickle of the array to the specified file. Returns the pickle of the array as a string. Copy of the array, cast to a specified type. Swap the bytes of the array elements New view of array with the same data. Returns a field of the given array as a certain type. Fill the array with a scalar value. For reshape, resize, and transpose, the single tuple argument may be replaced with integers which will be interpreted as an n-tuple. Returns an array containing the same data with a new shape. Returns a view of the array with axes transposed. Return a view of the array with axis1 and axis2 interchanged. Return a copy of the array collapsed into one dimension. Remove axes of length one from a. For array methods that take an axis keyword, it defaults to None. If axis is None, then the array is treated as a 1-D array. Any other value for axis represents the dimension along which the operation should proceed. Return an array formed from the elements of a at the given indices. Set for all n in indices. Use an index array to construct a new array from a set of choices. Returns the indices that would sort this array. Partially sorts the elements in the array in such a way that the value of the element in k-th position is in the position it would be in a sorted array. Returns the indices that would partition this array. Find indices where elements of v should be inserted in a to maintain order. Return the indices of the elements that are non-zero. Return selected slices of this array along given axis. Many of these methods take an argument named axis. In such cases,\n• None If axis is None (the default), the array is treated as a 1-D array and the operation is performed over the entire array. This behavior is also the default if self is a 0-dimensional array or array scalar. (An array scalar is an instance of the types/classes float32, float64, etc., whereas a 0-dimensional array is an ndarray instance containing precisely one array scalar.)\n• None If axis is an integer, then the operation is done over the given axis (for each 1-D subarray that can be created along the given axis). Example of the axis argument A 3-dimensional array of size 3 x 3 x 3, summed over each of its three axes: # for sum, axis is the first keyword, so we may omit it, # specifying only its value The parameter dtype specifies the data type over which a reduction operation (like summing) should take place. The default reduce data type is the same as the data type of self. To avoid overflow, it can be useful to perform the reduction using a larger data type. For several methods, an optional out argument can also be provided and the result will be placed into the output array given. The out argument must be an and have the same number of elements. It can have a different data type in which case casting will be performed. Return the maximum along a given axis. Return indices of the maximum values along the given axis. Return the minimum along a given axis. Return indices of the minimum values along the given axis. Return an array whose values are limited to . Return a with each element rounded to the given number of decimals. Return the sum along diagonals of the array. Return the sum of the array elements over the given axis. Return the cumulative sum of the elements along the given axis. Returns the average of the array elements along given axis. Returns the variance of the array elements, along given axis. Returns the standard deviation of the array elements along given axis. Return the product of the array elements over the given axis Return the cumulative product of the elements along the given axis. Returns True if all elements evaluate to True. Returns True if any of the elements of a evaluate to True.\n\nArithmetic and comparison operations on are defined as element-wise operations, and generally yield objects as results. Each of the arithmetic operations ( , , , , , , , or , , , , , , ) and the comparisons ( , , , , , ) is equivalent to the corresponding universal function (or ufunc for short) in NumPy. For more information, see the section on Universal Functions. Truth value of an array ( ): Truth-value testing of an array invokes , which raises an error if the number of elements in the array is not 1, because the truth value of such arrays is ambiguous. Use and instead to be clear about what is meant in such cases. (If you wish to check for whether an array is empty, use for example .)\n• None Any third argument to is silently ignored, as the underlying takes only two arguments.\n• None Because is a built-in type (written in C), the special methods are not directly defined.\n• None The functions called to implement many arithmetic special methods for arrays can be modified using . In place operations will perform the calculation using the precision decided by the data type of the two operands, but will silently downcast the result (if necessary) so it can fit back into the array. Therefore, for mixed precision calculations, can be different than . For example, suppose . Then, is different than : while they both perform the same computation, casts the result to fit back in , whereas re-binds the name to the result. Matrix operators and were introduced in Python 3.5 following PEP 465, and the operator has been introduced in NumPy 1.10.0. Further information can be found in the documentation."
    },
    {
        "link": "https://numpy.org/doc/stable/reference",
        "document": "This reference manual details functions, modules, and objects included in NumPy, describing what they are and what they do. For learning how to use NumPy, see the complete documentation.\n\nLarge parts of this manual originate from Travis E. Oliphant’s book Guide to NumPy (which generously entered Public Domain in August 2008). The reference documentation for many of the functions are written by numerous contributors and developers of NumPy."
    },
    {
        "link": "https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contour.html",
        "document": "The colors of the levels, i.e. the lines for and the areas for . The sequence is cycled for the levels in ascending order. If the sequence is shorter than the number of levels, it's repeated. As a shortcut, a single color may be used in place of one-element lists, i.e. instead of to color all levels with the same color. Changed in version 3.10: Previously a single color had to be expressed as a string, but now any valid color format may be passed. By default (value None), the colormap specified by cmap will be used.\n\nThe normalization method used to scale scalar data to the [0, 1] range before mapping to colors using cmap. By default, a linear scaling is used, mapping the lowest value to 0 and the highest to 1. If given, this can be one of the following:\n• None An instance of or one of its subclasses (see Colormap normalization).\n• None A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc. For a list of available scales, call . In that case, a suitable subclass is dynamically generated and instantiated. This parameter is ignored if colors is set.\n\nWhen using scalar data and no explicit norm, vmin and vmax define the data range that the colormap covers. By default, the colormap covers the complete value range of the supplied data. It is an error to use vmin/vmax when a norm instance is given (but using a norm name together with vmin/vmax is acceptable). If vmin or vmax are not given, the default color scaling is based on levels. This parameter is ignored if colors is set.\n\nIf origin is not None, then extent is interpreted as in : it gives the outer pixel boundaries. In this case, the position of Z[0, 0] is the center of the pixel, not a corner. If origin is None, then (x0, y0) is the position of Z[0, 0], and (x1, y1) is the position of Z[-1, -1]. This argument is ignored if X and Y are specified in the call to contour.\n\nDetermines the -coloring of values that are outside the levels range. If 'neither', values outside the levels range are not colored. If 'min', 'max' or 'both', color the values below, above or below and above the levels range. Values below and above are mapped to the under/over values of the . Note that most colormaps do not have dedicated colors for these by default, so that the over and under values are the edge values of the colormap. You may want to set these values explicitly using and . An existing does not get notified if properties of its colormap are changed. Therefore, an explicit call is needed after modifying the colormap. The explicit call can be left out, if a colorbar is assigned to the because it internally calls .\n\nIf 0, no subdivision of the domain. Specify a positive integer to divide the domain into subdomains of nchunk by nchunk quads. Chunking reduces the maximum length of polygons generated by the contouring algorithm which reduces the rendering workload passed on to the backend and also requires slightly less RAM. It can however introduce rendering artifacts at chunk boundaries depending on the backend, the antialiased flag and value of alpha.\n\nThe line width of the contour lines. If a number, all levels will be plotted with this linewidth. If a sequence, the levels in ascending order will be plotted with the linewidths in the order specified. If None, this falls back to (default: ).\n\nIf linestyles is None, the default is 'solid' unless the lines are monochrome. In that case, negative contours will instead take their linestyle from the negative_linestyles argument. linestyles can also be an iterable of the above strings specifying a set of linestyles to be used. If this iterable is shorter than the number of contour levels it will be repeated as necessary.\n\nWhich contouring algorithm to use to calculate the contour lines and polygons. The algorithms are implemented in ContourPy, consult the ContourPy documentation for further information. The default is taken from (default: )."
    },
    {
        "link": "https://geeksforgeeks.org/contour-plot-using-matplotlib-python",
        "document": "Contour plots also called level plots are a tool for doing multivariate analysis and visualizing 3-D plots in 2-D space. If we consider X and Y as our variables we want to plot then the response Z will be plotted as slices on the X-Y plane due to which contours are sometimes referred as Z-slices or iso-response.\n\nContour plots are widely used to visualize density, altitudes or heights of the mountain as well as in the meteorological department. Due to such wide usage provides a method to make it easy for us to draw contour plots.\n\nThe matplotlib.pyplot.contour() are usually useful when Z = f(X, Y) i.e Z changes as a function of input X and Y. A is also available which allows us to draw filled contours.\n\nParameters:\n\n X, Y: 2-D numpy arrays with same shape as Z or 1-D arrays such that len(X)==M and len(Y)==N (where M and N are rows and columns of Z)\n\n Z: The height values over which the contour is drawn. Shape is (M, N)\n\n levels: Determines the number and positions of the contour lines / regions.\n\nBelow examples illustrate the function in matplotlib.pyplot:\n\nExample #1: Plotting of Contour using which only plots contour lines."
    },
    {
        "link": "https://matplotlib.org/stable/gallery/images_contours_and_fields/contour_demo.html",
        "document": "Go to the end to download the full example code.\n\nIllustrate simple contour plotting, contours on an image with a colorbar for the contours, and labelled contours.\n\nSee also the contour image example.\n\nCreate a simple contour plot with labels using default colors. The inline argument to clabel will control whether the labels are draw over the line segments of the contour, removing the lines beneath the label.\n\nContour labels can be placed manually by providing list of positions (in data coordinate). See Interactive functions for interactive placement.\n\nYou can force all the contours to be the same color.\n\nYou can set negative contours to be solid instead of dashed:\n\nAnd you can manually specify the colors of the contour\n\nOr you can use a colormap to specify the colors; the default colormap will be used for the contour lines"
    },
    {
        "link": "https://oreilly.com/library/view/python-data-science/9781491912126/ch04.html",
        "document": "We’ll now take an in-depth look at the Matplotlib tool for visualization in Python. Matplotlib is a multiplatform data visualization library built on NumPy arrays, and designed to work with the broader SciPy stack. It was conceived by John Hunter in 2002, originally as a patch to IPython for enabling interactive MATLAB-style plotting via gnuplot from the IPython command line. IPython’s creator, Fernando Perez, was at the time scrambling to finish his PhD, and let John know he wouldn’t have time to review the patch for several months. John took this as a cue to set out on his own, and the Matplotlib package was born, with version 0.1 released in 2003. It received an early boost when it was adopted as the plotting package of choice of the Space Telescope Science Institute (the folks behind the Hubble Telescope), which financially supported Matplotlib’s development and greatly expanded its capabilities.\n\nOne of Matplotlib’s most important features is its ability to play well with many operating systems and graphics backends. Matplotlib supports dozens of backends and output types, which means you can count on it to work regardless of which operating system you are using or which output format you wish. This cross-platform, everything-to-everyone approach has been one of the great strengths of Matplotlib. It has led to a large userbase, which in turn has led to an active developer base and Matplotlib’s powerful tools and ubiquity within the scientific Python world.\n\nIn recent years, however, the interface and style of Matplotlib have begun to show their age. Newer tools like ggplot and ggvis in the R language, along with web visualization toolkits based on D3js and HTML5 canvas, often make Matplotlib feel clunky and old-fashioned. Still, I’m of the opinion that we cannot ignore Matplotlib’s strength as a well-tested, cross-platform graphics engine. Recent Matplotlib versions make it relatively easy to set new global plotting styles (see “Customizing Matplotlib: Configurations and Stylesheets”), and people have been developing new packages that build on its powerful internals to drive Matplotlib via cleaner, more modern APIs—for example, Seaborn (discussed in “Visualization with Seaborn”), ggplot, HoloViews, Altair, and even Pandas itself can be used as wrappers around Matplotlib’s API. Even with wrappers like these, it is still often useful to dive into Matplotlib’s syntax to adjust the final plot output. For this reason, I believe that Matplotlib itself will remain a vital piece of the data visualization stack, even if new tools mean the community gradually moves away from using the Matplotlib API directly.\n\nPerhaps the simplest of all plots is the visualization of a single function . Here we will take a first look at creating a simple plot of this type. As with all the following sections, we’ll start by setting up the notebook for plotting and importing the functions we will use: For all Matplotlib plots, we start by creating a figure and an axes. In their simplest form, a figure and axes can be created as follows (Figure 4-5): In Matplotlib, the figure (an instance of the class ) can be thought of as a single container that contains all the objects representing axes, graphics, text, and labels. The axes (an instance of the class ) is what we see above: a bounding box with ticks and labels, which will eventually contain the plot elements that make up our visualization. Throughout this book, we’ll commonly use the variable name to refer to a figure instance, and to refer to an axes instance or group of axes instances. Once we have created an axes, we can use the function to plot some data. Let’s start with a simple sinusoid (Figure 4-6): Alternatively, we can use the pylab interface and let the figure and axes be created for us in the background (Figure 4-7; see “Two Interfaces for the Price of One” for a discussion of these two interfaces): If we want to create a single figure with multiple lines, we can simply call the function multiple times (Figure 4-8): That’s all there is to plotting simple functions in Matplotlib! We’ll now dive into some more details about how to control the appearance of the axes and lines. The first adjustment you might wish to make to a plot is to control the line colors and styles. The function takes additional arguments that can be used to specify these. To adjust the color, you can use the keyword, which accepts a string argument representing virtually any imaginable color. The color can be specified in a variety of ways (Figure 4-9): # specify color by name If no color is specified, Matplotlib will automatically cycle through a set of default colors for multiple lines. Similarly, you can adjust the line style using the keyword (Figure 4-10): # For short, you can use the following codes: Example of various line styles If you would like to be extremely terse, these and codes can be combined into a single nonkeyword argument to the function (Figure 4-11): Controlling colors and styles with the shorthand syntax These single-character color codes reflect the standard abbreviations in the RGB (Red/Green/Blue) and CMYK (Cyan/Magenta/Yellow/blacK) color systems, commonly used for digital color graphics. There are many other keyword arguments that can be used to fine-tune the appearance of the plot; for more details, I’d suggest viewing the docstring of the function using IPython’s help tools (see “Help and Documentation in IPython”). Matplotlib does a decent job of choosing default axes limits for your plot, but sometimes it’s nice to have finer control. The most basic way to adjust axis limits is to use the and methods (Figure 4-12): If for some reason you’d like either axis to be displayed in reverse, you can simply reverse the order of the arguments (Figure 4-13): Example of reversing the y-axis A useful related method is (note here the potential confusion between axes with an e, and axis with an i). The method allows you to set the and limits with a single call, by passing a list that specifies (Figure 4-14): The method goes even beyond this, allowing you to do things like automatically tighten the bounds around the current plot (Figure 4-15): It allows even higher-level specifications, such as ensuring an equal aspect ratio so that on your screen, one unit in is equal to one unit in (Figure 4-16): Example of an “equal” layout, with units matched to the output resolution For more information on axis limits and the other capabilities of the method, refer to the docstring. As the last piece of this section, we’ll briefly look at the labeling of plots: titles, axis labels, and simple legends. Titles and axis labels are the simplest such labels—there are methods that can be used to quickly set them (Figure 4-17): You can adjust the position, size, and style of these labels using optional arguments to the function. For more information, see the Matplotlib documentation and the docstrings of each of these functions. When multiple lines are being shown within a single axes, it can be useful to create a plot legend that labels each line type. Again, Matplotlib has a built-in way of quickly creating such a legend. It is done via the (you guessed it) method. Though there are several valid ways of using this, I find it easiest to specify the label of each line using the keyword of the plot function (Figure 4-18): As you can see, the function keeps track of the line style and color, and matches these with the correct label. More information on specifying and formatting plot legends can be found in the docstring; additionally, we will cover some more advanced legend options in “Customizing Plot Legends”. While most functions translate directly to methods (such as → , → , etc.), this is not the case for all commands. In particular, functions to set limits, labels, and titles are slightly modified. For transitioning between MATLAB-style functions and object-oriented methods, make the following changes: In the object-oriented interface to plotting, rather than calling these functions individually, it is often more convenient to use the method to set all these properties at once (Figure 4-19): Example of using ax.set to set multiple properties at once\n\nMatplotlib’s default tick locators and formatters are designed to be generally sufficient in many common situations, but are in no way optimal for every plot. This section will give several examples of adjusting the tick locations and formatting for the particular plot type you’re interested in. Before we go into examples, it will be best for us to understand further the object hierarchy of Matplotlib plots. Matplotlib aims to have a Python object representing everything that appears on the plot: for example, recall that the is the bounding box within which plot elements appear. Each Matplotlib object can also act as a container of sub-objects; for example, each can contain one or more objects, each of which in turn contain other objects representing plot contents. The tick marks are no exception. Each has attributes and , which in turn have attributes that contain all the properties of the lines, ticks, and labels that make up the axes. Within each axis, there is the concept of a major tick mark and a minor tick mark. As the names would imply, major ticks are usually bigger or more pronounced, while minor ticks are usually smaller. By default, Matplotlib rarely makes use of minor ticks, but one place you can see them is within logarithmic plots (Figure 4-73): Example of logarithmic scales and labels We see here that each major tick shows a large tick mark and a label, while each minor tick shows a smaller tick mark with no label. We can customize these tick properties—that is, locations and labels—by setting the and objects of each axis. Let’s examine these for the x axis of the plot just shown: We see that both major and minor tick labels have their locations specified by a (which makes sense for a logarithmic plot). Minor ticks, though, have their labels formatted by a ; this says that no labels will be shown. We’ll now show a few examples of setting these locators and formatters for various plots. Perhaps the most common tick/label formatting operation is the act of hiding ticks or labels. We can do this using and , as shown here (Figure 4-74): Notice that we’ve removed the labels (but kept the ticks/gridlines) from the x axis, and removed the ticks (and thus the labels as well) from the y axis. Having no ticks at all can be useful in many situations—for example, when you want to show a grid of images. For instance, consider Figure 4-75, which includes images of different faces, an example often used in supervised machine learning problems (for more information, see “In-Depth: Support Vector Machines”): # Get some face data from scikit-learn Notice that each image has its own axes, and we’ve set the locators to null because the tick values (pixel number in this case) do not convey relevant information for this particular visualization. Reducing or Increasing the Number of Ticks One common problem with the default settings is that smaller subplots can end up with crowded labels. We can see this in the plot grid shown in Figure 4-76: Particularly for the x ticks, the numbers nearly overlap, making them quite difficult to decipher. We can fix this with the , which allows us to specify the maximum number of ticks that will be displayed. Given this maximum number, Matplotlib will use internal logic to choose the particular tick locations (Figure 4-77): # For every axis, set the x and y major locator This makes things much cleaner. If you want even more control over the locations of regularly spaced ticks, you might also use , which we’ll discuss in the following section. Matplotlib’s default tick formatting can leave a lot to be desired; it works well as a broad default, but sometimes you’d like to do something more. Consider the plot shown in Figure 4-78, a sine and a cosine: There are a couple changes we might like to make. First, it’s more natural for this data to space the ticks and grid lines in multiples of . We can do this by setting a , which locates ticks at a multiple of the number you provide. For good measure, we’ll add both major and minor ticks in multiples of (Figure 4-79): But now these tick labels look a little bit silly: we can see that they are multiples of , but the decimal representation does not immediately convey this. To fix this, we can change the tick formatter. There’s no built-in formatter for what we want to do, so we’ll instead use , which accepts a user-defined function giving fine-grained control over the tick outputs (Figure 4-80): This is much better! Notice that we’ve made use of Matplotlib’s LaTeX support, specified by enclosing the string within dollar signs. This is very convenient for display of mathematical symbols and formulae; in this case, is rendered as the Greek character . The offers extremely fine-grained control over the appearance of your plot ticks, and comes in very handy when you’re preparing plots for presentation or publication. We’ve mentioned a couple of the available formatters and locators. We’ll conclude this section by briefly listing all the built-in locator and formatter options. For more information on any of these, refer to the docstrings or to the Matplotlib online documentation. Each of the following is available in the namespace: Ticks and range are a multiple of base Finds up to a max number of ticks at nice locations Set the strings from a list of labels Set the strings manually for the labels Use a format string for each value We’ll see additional examples of these throughout the remainder of the book.\n\nOne common type of visualization in data science is that of geographic data. Matplotlib’s main tool for this type of visualization is the Basemap toolkit, which is one of several Matplotlib toolkits that live under the namespace. Admittedly, Basemap feels a bit clunky to use, and often even simple visualizations take much longer to render than you might hope. More modern solutions, such as leaflet or the Google Maps API, may be a better choice for more intensive map visualizations. Still, Basemap is a useful tool for Python users to have in their virtual toolbelts. In this section, we’ll show several examples of the type of map visualization that is possible with this toolkit. Installation of Basemap is straightforward; if you’re using conda you can type this and the package will be downloaded: We add just a single new import to our standard boilerplate: Once you have the Basemap toolkit installed and imported, geographic plots are just a few lines away (the graphics in Figure 4-102 also require the package in Python 2, or the package in Python 3): The meaning of the arguments to Basemap will be discussed momentarily. The useful thing is that the globe shown here is not a mere image; it is a fully functioning Matplotlib axes that understands spherical coordinates and allows us to easily over-plot data on the map! For example, we can use a different map projection, zoom in to North America, and plot the location of Seattle. We’ll use an etopo image (which shows topographical features both on land and under the ocean) as the map background (Figure 4-103): Plotting data and labels on the map This gives you a brief glimpse into the sort of geographic visualizations that are possible with just a few lines of Python. We’ll now discuss the features of Basemap in more depth, and provide several examples of visualizing map data. Using these brief examples as building blocks, you should be able to create nearly any map visualization that you desire. The first thing to decide when you are using maps is which projection to use. You’re probably familiar with the fact that it is impossible to project a spherical map, such as that of the Earth, onto a flat surface without somehow distorting it or breaking its continuity. These projections have been developed over the course of human history, and there are a lot of choices! Depending on the intended use of the map projection, there are certain map features (e.g., direction, area, distance, shape, or other considerations) that are useful to maintain. The Basemap package implements several dozen such projections, all referenced by a short format code. Here we’ll briefly demonstrate some of the more common ones. We’ll start by defining a convenience routine to draw our world map along with the longitude and latitude lines: # lats and longs are returned as a dictionary # cycle through these lines and set the desired style The simplest of map projections are cylindrical projections, in which lines of constant latitude and longitude are mapped to horizontal and vertical lines, respectively. This type of mapping represents equatorial regions quite well, but results in extreme distortions near the poles. The spacing of latitude lines varies between different cylindrical projections, leading to different conservation properties, and different distortion near the poles. In Figure 4-104, we show an example of the equidistant cylindrical projection, which chooses a latitude scaling that preserves distances along meridians. Other cylindrical projections are the Mercator ( ) and the cylindrical equal-area ( ) projections. The additional arguments to Basemap for this view specify the latitude ( ) and longitude ( ) of the lower-left corner ( ) and upper-right corner ( ) for the desired map, in units of degrees. Pseudo-cylindrical projections relax the requirement that meridians (lines of constant longitude) remain vertical; this can give better properties near the poles of the projection. The Mollweide projection ( ) is one common example of this, in which all meridians are elliptical arcs (Figure 4-105). It is constructed so as to preserve area across the map: though there are distortions near the poles, the area of small patches reflects the true area. Other pseudo-cylindrical projections are the sinusoidal ( ) and Robinson ( ) projections. The extra arguments to here refer to the central latitude ( ) and longitude ( ) for the desired map. Perspective projections are constructed using a particular choice of perspective point, similar to if you photographed the Earth from a particular point in space (a point which, for some projections, technically lies within the Earth!). One common example is the orthographic projection ( ), which shows one side of the globe as seen from a viewer at a very long distance. Thus, it can show only half the globe at a time. Other perspective-based projections include the gnomonic projection ( ) and stereographic projection ( ). These are often the most useful for showing small portions of the map. Here is an example of the orthographic projection (Figure 4-106): A conic projection projects the map onto a single cone, which is then unrolled. This can lead to very good local properties, but regions far from the focus point of the cone may become very distorted. One example of this is the Lambert conformal conic projection ( ), which we saw earlier in the map of North America. It projects the map onto a cone arranged in such a way that two standard parallels (specified in by and ) have well-represented distances, with scale decreasing between them and increasing outside of them. Other useful conic projections are the equidistant conic ( ) and the Albers equal-area ( ) projection (Figure 4-107). Conic projections, like perspective projections, tend to be good choices for representing small to medium patches of the globe. If you’re going to do much with map-based visualizations, I encourage you to read up on other available projections, along with their properties, advantages, and disadvantages. Most likely, they are available in the Basemap package. If you dig deep enough into this topic, you’ll find an incredible subculture of geo-viz geeks who will be ready to argue fervently in support of their favorite projection for any given application! Earlier we saw the and methods for projecting global images on the map, as well as the and methods for drawing lines of constant latitude and longitude. The Basemap package contains a range of useful functions for drawing borders of physical features like continents, oceans, lakes, and rivers, as well as political boundaries such as countries and US states and counties. The following are some of the available drawing functions that you may wish to explore using IPython’s help features:\n• Draw a mask between the land and sea, for use with projecting images on one or the other Draw the map boundary, including the fill color for oceans Fill the continents with a given color; optionally fill lakes with another color\n• Draw an etopo relief image onto the map For the boundary-based features, you must set the desired resolution when creating a Basemap image. The argument of the class sets the level of detail in boundaries, either (crude), (low), (intermediate), (high), (full), or if no boundaries will be used. This choice is important: setting high-resolution boundaries on a global map, for example, can be very slow. Here’s an example of drawing land/sea boundaries, and the effect of the resolution parameter. We’ll create both a low- and high-resolution map of Scotland’s beautiful Isle of Skye. It’s located at 57.3°N, 6.2°W, and a map of 90,000×120,000 kilometers shows it well (Figure 4-108): Notice that the low-resolution coastlines are not suitable for this level of zoom, while high-resolution works just fine. The low level would work just fine for a global view, however, and would be much faster than loading the high-resolution border data for the entire globe! It might require some experimentation to find the correct resolution parameter for a given view; the best route is to start with a fast, low-resolution plot and increase the resolution as needed. Perhaps the most useful piece of the Basemap toolkit is the ability to over-plot a variety of data onto a map background. For simple plotting and text, any function works on the map; you can use the instance to project latitude and longitude coordinates to coordinates for plotting with , as we saw earlier in the Seattle example. In addition to this, there are many map-specific functions available as methods of the instance. These work very similarly to their standard Matplotlib counterparts, but have an additional Boolean argument , which if set to allows you to pass raw latitudes and longitudes to the method, rather than projected coordinates. Some of these map-specific methods are: We’ll see examples of a few of these as we continue. For more information on these functions, including several example plots, see the online Basemap documentation. Recall that in “Customizing Plot Legends”, we demonstrated the use of size and color in a scatter plot to convey information about the location, size, and population of California cities. Here, we’ll create this plot again, but using Basemap to put the data in context. We start with loading the data, as we did before: # Extract the data we're interested in Next, we set up the map projection, scatter the data, and then create a colorbar and legend (Figure 4-109): This shows us roughly where larger populations of people have settled in California: they are clustered near the coast in the Los Angeles and San Francisco areas, stretched along the highways in the flat central valley, and avoiding almost completely the mountainous regions along the borders of the state. As an example of visualizing some more continuous geographic data, let’s consider the “polar vortex” that hit the eastern half of the United States in January 2014. A great source for any sort of climatic data is NASA’s Goddard Institute for Space Studies. Here we’ll use the GIS 250 temperature data, which we can download using shell commands (these commands may have to be modified on Windows machines). The data used here was downloaded on 6/12/2016, and the file size is approximately 9 MB: The data comes in NetCDF format, which can be read in Python by the library. You can install this library as shown here: We read the data as follows: The file contains many global temperature readings on a variety of dates; we need to select the index of the date we’re interested in—in this case, January 15, 2014: Now we can load the latitude and longitude data, as well as the temperature anomaly for this index: Finally, we’ll use the method to draw a color mesh of the data. We’ll look at North America, and use a shaded relief map in the background. Note that for this data we specifically chose a divergent colormap, which has a neutral color at zero and two contrasting colors at negative and positive values (Figure 4-110). We’ll also lightly draw the coastlines over the colors for reference: The data paints a picture of the localized, extreme temperature anomalies that happened during that month. The eastern half of the United States was much colder than normal, while the western half and Alaska were much warmer. Regions with no recorded temperature show the map background.\n\nMatplotlib has proven to be an incredibly useful and popular visualization tool, but even avid users will admit it often leaves much to be desired. There are several valid complaints about Matplotlib that often come up:\n• Prior to version 2.0, Matplotlib’s defaults are not exactly the best choices. It was based off of MATLAB circa 1999, and this often shows.\n• Matplotlib’s API is relatively low level. Doing sophisticated statistical visualization is possible, but often requires a lot of boilerplate code.\n• Matplotlib predated Pandas by more than a decade, and thus is not designed for use with Pandas s. In order to visualize data from a Pandas , you must extract each and often concatenate them together into the right format. It would be nicer to have a plotting library that can intelligently use the labels in a plot. An answer to these problems is Seaborn. Seaborn provides an API on top of Matplotlib that offers sane choices for plot style and color defaults, defines simple high-level functions for common statistical plot types, and integrates with the functionality provided by Pandas s. To be fair, the Matplotlib team is addressing this: it has recently added the tools (discussed in “Customizing Matplotlib: Configurations and Stylesheets”), and is starting to handle Pandas data more seamlessly. The 2.0 release of the library will include a new default stylesheet that will improve on the current status quo. But for all the reasons just discussed, Seaborn remains an extremely useful add-on. Here is an example of a simple random-walk plot in Matplotlib, using its classic plot formatting and colors. We start with the typical imports: Now we create some random walk data: Although the result contains all the information we’d like it to convey, it does so in a way that is not all that aesthetically pleasing, and even looks a bit old-fashioned in the context of 21st-century data visualization. Now let’s take a look at how it works with Seaborn. As we will see, Seaborn has many of its own high-level plotting routines, but it can also overwrite Matplotlib’s default parameters and in turn get even simple Matplotlib scripts to produce vastly superior output. We can set the style by calling Seaborn’s method. By convention, Seaborn is imported as : Now let’s rerun the same two lines as before (Figure 4-112): # same plotting code as above! The main idea of Seaborn is that it provides high-level commands to create a variety of plot types useful for statistical data exploration, and even some statistical model fitting. Let’s take a look at a few of the datasets and plot types available in Seaborn. Note that all of the following could be done using raw Matplotlib commands (this is, in fact, what Seaborn does under the hood), but the Seaborn API is much more convenient. Often in statistical data visualization, all you want is to plot histograms and joint distributions of variables. We have seen that this is relatively straightforward in Matplotlib (Figure 4-113): Rather than a histogram, we can get a smooth estimate of the distribution using a kernel density estimation, which Seaborn does with (Figure 4-114): Histograms and KDE can be combined using (Figure 4-115): If we pass the full two-dimensional dataset to , we will get a two-dimensional visualization of the data (Figure 4-116): We can see the joint distribution and the marginal distributions together using . For this plot, we’ll set the style to a white background (Figure 4-117): There are other parameters that can be passed to —for example, we can use a hexagonally based histogram instead (Figure 4-118): When you generalize joint plots to datasets of larger dimensions, you end up with pair plots. This is very useful for exploring correlations between multidimensional data, when you’d like to plot all pairs of values against each other. We’ll demo this with the well-known Iris dataset, which lists measurements of petals and sepals of three iris species: Visualizing the multidimensional relationships among the samples is as easy as calling (Figure 4-119): A pair plot showing the relationships between four variables Sometimes the best way to view data is via histograms of subsets. Seaborn’s makes this extremely simple. We’ll take a look at some data that shows the amount that restaurant staff receive in tips based on various indicator data (Figure 4-120): Out[14]: total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 An example of a faceted histogram Factor plots can be useful for this kind of visualization as well. This allows you to view the distribution of a parameter within bins defined by any other parameter (Figure 4-121): An example of a factor plot, comparing distributions given various discrete factors Similar to the pair plot we saw earlier, we can use to show the joint distribution between different datasets, along with the associated marginal distributions (Figure 4-122): The joint plot can even do some automatic kernel density estimation and regression (Figure 4-123): Time series can be plotted with . In the following example (visualized in Figure 4-124), we’ll use the Planets data that we first saw in “Aggregation and Grouping”: We can learn more by looking at the method of discovery of each of these planets, as illustrated in Figure 4-125: Number of planets discovered by year and type (see the online appendix for a full-scale figure) For more information on plotting with Seaborn, see the Seaborn documentation, a tutorial, and the Seaborn gallery. Here we’ll look at using Seaborn to help visualize and understand finishing results from a marathon. I’ve scraped the data from sources on the Web, aggregated it and removed any identifying information, and put it on GitHub where it can be downloaded (if you are interested in using Python for web scraping, I would recommend Web Scraping with Python by Ryan Mitchell). We will start by downloading the data from the Web, and loading it into Pandas: By default, Pandas loaded the time columns as Python strings (type ); we can see this by looking at the attribute of the : Let’s fix this by providing a converter for the times: That looks much better. For the purpose of our Seaborn plotting utilities, let’s next add columns that give the times in seconds: To get an idea of what the data looks like, we can plot a over the data (Figure 4-126): The relationship between the split for the first half-marathon and the finishing time for the full marathon The dotted line shows where someone’s time would lie if they ran the marathon at a perfectly steady pace. The fact that the distribution lies above this indicates (as you might expect) that most people slow down over the course of the marathon. If you have run competitively, you’ll know that those who do the opposite—run faster during the second half of the race—are said to have “negative-split” the race. Let’s create another column in the data, the split fraction, which measures the degree to which each runner negative-splits or positive-splits the race: Where this split difference is less than zero, the person negative-split the race by that fraction. Let’s do a distribution plot of this split fraction (Figure 4-127): The distribution of split fractions; 0.0 indicates a runner who completed the first and second halves in identical times Out of nearly 40,000 participants, there were only 250 people who negative-split their marathon. Let’s see whether there is any correlation between this split fraction and other variables. We’ll do this using a , which draws plots of all these correlations (Figure 4-128): The relationship between quantities within the marathon dataset It looks like the split fraction does not correlate particularly with age, but does correlate with the final time: faster runners tend to have closer to even splits on their marathon time. (We see here that Seaborn is no panacea for Matplotlib’s ills when it comes to plot styles: in particular, the x-axis labels overlap. Because the output is a simple Matplotlib plot, however, the methods in “Customizing Ticks” can be used to adjust such things if desired.) The difference between men and women here is interesting. Let’s look at the histogram of split fractions for these two groups (Figure 4-129): The distribution of split fractions by gender The interesting thing here is that there are many more men than women who are running close to an even split! This almost looks like some kind of bimodal distribution among the men and women. Let’s see if we can suss out what’s going on by looking at the distributions as a function of age. A nice way to compare distributions is to use a violin plot (Figure 4-130): This is yet another way to compare the distributions between men and women. Let’s look a little deeper, and compare these violin plots as a function of age. We’ll start by creating a new column in the array that specifies the decade of age that each person is in (Figure 4-131): A violin plot showing the split fraction by gender and age Looking at this, we can see where the distributions of men and women differ: the split distributions of men in their 20s to 50s show a pronounced over-density toward lower splits when compared to women of the same age (or of any age, for that matter). Also surprisingly, the 80-year-old women seem to outperform everyone in terms of their split time. This is probably due to the fact that we’re estimating the distribution from small numbers, as there are only a handful of runners in that range: Back to the men with negative splits: who are these runners? Does this split fraction correlate with finishing quickly? We can plot this very easily. We’ll use , which will automatically fit a linear regression to the data (Figure 4-132): Apparently the people with fast splits are the elite runners who are finishing within ~15,000 seconds, or about 4 hours. People slower than that are much less likely to have a fast second split."
    },
    {
        "link": "https://tutorialspoint.com/matplotlib/matplotlib_contour_plot.htm",
        "document": "A contour plot, also known as a contour map or a level plot, is a graphical representation of a three-dimensional surface on a two-dimensional plane.\n\nIn a contour plot, the surface is represented by a series of contour lines. Each contour line connects points of equal value on the surface, showing regions where the function has the same value. These contour lines are drawn at constant intervals or \"levels\", hence the name \"level plot\".\n\nImagine you have a contour plot of temperature across a map. Each contour line represents areas with the same temperature, like 50F, 60F, and so on. By looking at the plot, you can easily see where it is hotter or cooler across the map −\n\nYou can create contour plots in Matplotlib using the contour() function in the \"matplotlib.pyplot\" module. This function accepts X and Y coordinates as either 1D or 2D arrays, representing the grid on which the function \"Z\" is evaluated. \"Z\" is a 2D array containing the function values corresponding to the grid points defined by X and Y.\n\nA basic 3D contour in Matplotlib shows contour lines that connect points of equal value, representing the levels or \"heights\" of the data. Each contour line corresponds to a specific value, forming a map-like representation of the dataset.\n\nIn the following example, we are create a basic contour plot. We define the x and y coordinates for the grid, and then use a mathematical function to generate the z values. With these x, y, and z values, we create a contour plot using the contour() function −\n\nFollowing is the output of the above code −\n\nIn a filled contour plot in Matplotlib, instead of just showing contour lines, it fills in the areas between the lines with colors, creating a shaded representation of the data surface. Each color represents a different level or \"height\" of the data, allowing you to easily see the distribution within the dataset.\n\nIn here, we create a filled contour plot using the contourf() function, which colors the regions between contour lines −\n\nOn executing the above code we will get the following output −\n\nIn a contour plot with specific levels, you specify the levels at which you want the contours to be drawn. Each contour line connects points of equal value, representing different levels or \"heights\" of the data. This allows you to customize the visualization to highlight specific features or intervals within the dataset.\n\nIn this example, we customize the contour plot by specifying specific contour levels using Matplotlib. After generating the data and creating the contour plot, we use the levels parameter in the contour() function to define the contour levels −\n\nAfter executing the above code, we get the following output −\n\nIn Matplotlib, a contour plot with a colorbar displays contour lines to show points of equal value in the dataset, and a colorbar alongside the plot to indicate the correspondence between colors and data values. The colorbar acts as a visual guide, helping you to understand the range and distribution of data values represented by different colors in the plot.\n\nHere, we create a contour plot with a colorbar using Matplotlib. After generating the data and creating the contour plot, we add a colorbar to the plot using the colorbar() function. This colorbar provides a visual representation of the z values corresponding to the contour plot −\n\nOn executing the above code we will get the following output −"
    },
    {
        "link": "https://realpython.com/gradient-descent-algorithm-python",
        "document": "Stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs. It’s an inexact but powerful technique.\n\nStochastic gradient descent is widely used in machine learning applications. Combined with backpropagation, it’s dominant in neural network training applications.\n• How to apply gradient descent and stochastic gradient descent to minimize the loss function in machine learning\n• What the learning rate is, why it’s important, and how it impacts results\n• How to write your own function for stochastic gradient descent\n\nIn this section, you’ll see two short examples of using gradient descent. You’ll also learn that it can be used in real-life machine learning problems like linear regression. In the second case, you’ll need to modify the code of because you need the data from the observations to calculate the gradient. First, you’ll apply to another one-dimensional problem. Take the function 𝑣 − log(𝑣). The gradient of this function is 1 − 1/𝑣. With this information, you can find its minimum: With the provided set of arguments, correctly calculates that this function has the minimum in 𝑣 = 1. You can try it with other values for the learning rate and starting point. You can also use with functions of more than one variable. The application is the same, but you need to provide the gradient and starting points as vectors or arrays. For example, you can find the minimum of the function 𝑣₁² + 𝑣₂⁴ that has the gradient vector (2𝑣₁, 4𝑣₂³): In this case, your gradient function returns an array, and the start value is an array, so you get an array as the result. The resulting values are almost equal to zero, so you can say that correctly found that the minimum of this function is at 𝑣₁ = 𝑣₂ = 0. As you’ve already learned, linear regression and the ordinary least squares method start with the observed values of the inputs 𝐱 = (𝑥₁, …, 𝑥ᵣ) and outputs 𝑦. They define a linear function 𝑓(𝐱) = 𝑏₀ + 𝑏₁𝑥₁ + ⋯ + 𝑏ᵣ𝑥ᵣ, which is as close as possible to 𝑦. This is an optimization problem. It finds the values of weights 𝑏₀, 𝑏₁, …, 𝑏ᵣ that minimize the sum of squared residuals SSR = Σᵢ(𝑦ᵢ − 𝑓(𝐱ᵢ))² or the mean squared error MSE = SSR / 𝑛. Here, 𝑛 is the total number of observations and 𝑖 = 1, …, 𝑛. You can also use the cost function 𝐶 = SSR / (2𝑛), which is mathematically more convenient than SSR or MSE. The most basic form of linear regression is simple linear regression. It has only one set of inputs 𝑥 and two weights: 𝑏₀ and 𝑏₁. The equation of the regression line is 𝑓(𝑥) = 𝑏₀ + 𝑏₁𝑥. Although the optimal values of 𝑏₀ and 𝑏₁ can be calculated analytically, you’ll use gradient descent to determine them. First, you need calculus to find the gradient of the cost function 𝐶 = Σᵢ(𝑦ᵢ − 𝑏₀ − 𝑏₁𝑥ᵢ)² / (2𝑛). Since you have two decision variables, 𝑏₀ and 𝑏₁, the gradient ∇𝐶 is a vector with two components: You need the values of 𝑥 and 𝑦 to calculate the gradient of this cost function. Your gradient function will have as inputs not only 𝑏₀ and 𝑏₁ but also 𝑥 and 𝑦. This is how it might look: # .mean() is a method of np.ndarray takes the arrays and , which contain the observation inputs and outputs, and the array that holds the current values of the decision variables 𝑏₀ and 𝑏₁. This function first calculates the array of the residuals for each observation ( ) and then returns the pair of values of ∂𝐶/∂𝑏₀ and ∂𝐶/∂𝑏₁. In this example, you can use the convenient NumPy method since you pass NumPy arrays as the arguments.\n• Add and as the parameters of on line 4.\n• Provide and to the gradient function and make sure you convert your gradient tuple to a NumPy array on line 8. Here’s how looks after these changes: now accepts the observation inputs and outputs and can use them to calculate the gradient. Converting the output of to a NumPy array enables elementwise multiplication of the gradient elements by the learning rate, which isn’t necessary in the case of a single-variable function. Now apply your new version of to find the regression line for some arbitrary values of and : The result is an array with two values that correspond to the decision variables: 𝑏₀ = 5.63 and 𝑏₁ = 0.54. The best regression line is 𝑓(𝑥) = 5.63 + 0.54𝑥. As in the previous examples, this result heavily depends on the learning rate. You might not get such a good result with too low or too high of a learning rate. This example isn’t entirely random–it’s taken from the tutorial Linear Regression in Python. The good news is that you’ve obtained almost the same result as the linear regressor from scikit-learn. The data and regression results are visualized in the section Simple Linear Regression. You can make more robust, comprehensive, and better-looking without modifying its core functionality: # Checking if the gradient is callable # Setting up the data type for NumPy arrays \"'x' and 'y' lengths do not match\" # Initializing the values of the variables # Setting up and checking the learning rate \"'learn_rate' must be greater than zero\" # Setting up and checking the maximal number of iterations \"'n_iter' must be greater than zero\" # Setting up and checking the tolerance \"'tolerance' must be greater than zero\" # Checking if the absolute difference is small enough # Updating the values of the variables now accepts an additional parameter that defines the data type of NumPy arrays inside the function. For more information about NumPy types, see the official documentation on data types. In most applications, you won’t notice a difference between 32-bit and 64-bit floating-point numbers, but when you work with big datasets, this might significantly affect memory use and maybe even processing speed. For example, although NumPy uses 64-bit floats by default, TensorFlow often uses 32-bit decimal numbers. In addition to considering data types, the code above introduces a few modifications related to type checking and ensuring the use of NumPy capabilities:\n• Lines 8 and 9 check if is a Python callable object and whether it can be used as a function. If not, then the function will raise a .\n• Line 12 sets an instance of , which will be used as the data type for all arrays throughout the function.\n• Line 15 takes the arguments and and produces NumPy arrays with the desired data type. The arguments and can be lists, tuples, arrays, or other sequences.\n• Lines 16 and 17 compare the sizes of and . This is useful because you want to be sure that both arrays have the same number of observations. If they don’t, then the function will raise a .\n• Line 20 converts the argument to a NumPy array. This is an interesting trick: if is a Python scalar, then it’ll be transformed into a corresponding NumPy object (an array with one item and zero dimensions). If you pass a sequence, then it’ll become a regular NumPy array with the same number of elements.\n• Line 23 does the same thing with the learning rate. This can be very useful because it enables you to specify different learning rates for each decision variable by passing a list, tuple, or NumPy array to .\n• Lines 24 and 25 check if the learning rate value (or values for all variables) is greater than zero.\n• Lines 28 to 35 similarly set and and check that they are greater than zero.\n• Lines 38 to 47 are almost the same as before. The only difference is the type of the gradient array on line 40.\n• Line 49 conveniently returns the resulting array if you have several decision variables or a Python scalar if you have a single variable. Your is now finished. Feel free to add some additional capabilities or polishing. The next step of this tutorial is to use what you’ve learned so far to implement the stochastic version of gradient descent.\n\nStochastic gradient descent algorithms are a modification of gradient descent. In stochastic gradient descent, you calculate the gradient using just a random small part of the observations instead of all of them. In some cases, this approach can reduce computation time. Online stochastic gradient descent is a variant of stochastic gradient descent in which you estimate the gradient of the cost function for each observation and update the decision variables accordingly. This can help you find the global minimum, especially if the objective function is convex. Batch stochastic gradient descent is somewhere between ordinary gradient descent and the online method. The gradients are calculated and the decision variables are updated iteratively with subsets of all observations, called minibatches. This variant is very popular for training neural networks. You can imagine the online algorithm as a special kind of batch algorithm in which each minibatch has only one observation. Classical gradient descent is another special case in which there’s only one batch containing all observations. As in the case of the ordinary gradient descent, stochastic gradient descent starts with an initial vector of decision variables and updates it through several iterations. The difference between the two is in what happens inside the iterations:\n• Stochastic gradient descent randomly divides the set of observations into minibatches.\n• For each minibatch, the gradient is computed and the vector is moved.\n• Once all minibatches are used, you say that the iteration, or epoch, is finished and start the next one. This algorithm randomly selects observations for minibatches, so you need to simulate this random (or pseudorandom) behavior. You can do that with random number generation. Python has the built-in module, and NumPy has its own random generator. The latter is more convenient when you work with arrays. You’ll create a new function called that is very similar to but uses randomly selected minibatches to move along the search space: # Checking if the gradient is callable # Setting up the data type for NumPy arrays \"'x' and 'y' lengths do not match\" # Initializing the values of the variables # Setting up and checking the learning rate \"'learn_rate' must be greater than zero\" # Setting up and checking the size of minibatches \"'batch_size' must be greater than zero and less than \" \"or equal to the number of observations\" # Setting up and checking the maximal number of iterations \"'n_iter' must be greater than zero\" # Setting up and checking the tolerance \"'tolerance' must be greater than zero\" # Checking if the absolute difference is small enough # Updating the values of the variables You have a new parameter here. With , you specify the number of observations in each minibatch. This is an essential parameter for stochastic gradient descent that can significantly affect performance. Lines 34 to 39 ensure that is a positive integer no larger than the total number of observations. Another new parameter is . It defines the seed of the random number generator on line 22. The seed is used on line 23 as an argument to , which creates an instance of . If you pass the argument for , then the random number generator will return different numbers each time it’s instantiated. If you want each instance of the generator to behave exactly the same way, then you need to specify . The easiest way is to provide an arbitrary integer. Line 16 deduces the number of observations with . If is a one-dimensional array, then this is its size. If has two dimensions, then is the number of rows. On line 19, you use to make sure that both and become two-dimensional arrays with rows and that has exactly one column. conveniently concatenates the columns of and into a single array, . This is one way to make data suitable for random selection. Finally, on lines 52 to 70, you implement the loop for the stochastic gradient descent. It differs from . On line 54, you use the random number generator and its method to shuffle the observations. This is one of the ways to choose minibatches randomly. The inner loop is repeated for each minibatch. The main difference from the ordinary gradient descent is that, on line 62, the gradient is calculated for the observations from a minibatch ( and ) instead of for all observations ( and ). On line 59, becomes a part of that contains the rows of the current minibatch (from to ) and the columns that correspond to . holds the same rows from but only the last column (the outputs). For more information about how indices work in NumPy, see the official documentation on indexing. Now you can test your implementation of stochastic gradient descent: The result is almost the same as you got with . If you omit or use , then you’ll get somewhat different results each time you run because the random number generator will shuffle differently. As you’ve already seen, the learning rate can have a significant impact on the result of gradient descent. You can use several different strategies for adapting the learning rate during the algorithm execution. You can also apply momentum to your algorithm. You can use momentum to correct the effect of the learning rate. The idea is to remember the previous update of the vector and apply it when calculating the next one. You don’t move the vector exactly in the direction of the negative gradient, but you also tend to keep the direction and magnitude from the previous move. The parameter called the decay rate or decay factor defines how strong the contribution of the previous update is. To include the momentum and the decay rate, you can modify by adding the parameter and use it to calculate the direction and magnitude of the vector update ( ): # Checking if the gradient is callable # Setting up the data type for NumPy arrays \"'x' and 'y' lengths do not match\" # Initializing the values of the variables # Setting up and checking the learning rate \"'learn_rate' must be greater than zero\" # Setting up and checking the decay rate \"'decay_rate' must be between zero and one\" # Setting up and checking the size of minibatches \"'batch_size' must be greater than zero and less than \" \"or equal to the number of observations\" # Setting up and checking the maximal number of iterations \"'n_iter' must be greater than zero\" # Setting up and checking the tolerance \"'tolerance' must be greater than zero\" # Setting the difference to zero for the first iteration # Checking if the absolute difference is small enough # Updating the values of the variables In this implementation, you add the parameter on line 4, convert it to a NumPy array of the desired type on line 34, and check if it’s between zero and one on lines 35 and 36. On line 57, you initialize before the iterations start to ensure that it’s available in the first iteration. The most important change happens on line 71. You recalculate with the learning rate and gradient but also add the product of the decay rate and the old value of . Now has two components:\n• is the momentum, or impact of the previous move.\n• is the impact of the current gradient. The decay and learning rates serve as the weights that define the contributions of the two. As opposed to ordinary gradient descent, the starting point is often not so important for stochastic gradient descent. It may also be an unnecessary difficulty for a user, especially when you have many decision variables. To get an idea, just imagine if you needed to manually initialize the values for a neural network with thousands of biases and weights! In practice, you can start with some small arbitrary values. You’ll use the random number generator to get them: # Checking if the gradient is callable # Setting up the data type for NumPy arrays \"'x' and 'y' lengths do not match\" # Initializing the values of the variables # Setting up and checking the learning rate \"'learn_rate' must be greater than zero\" # Setting up and checking the decay rate \"'decay_rate' must be between zero and one\" # Setting up and checking the size of minibatches \"'batch_size' must be greater than zero and less than \" \"or equal to the number of observations\" # Setting up and checking the maximal number of iterations \"'n_iter' must be greater than zero\" # Setting up and checking the tolerance \"'tolerance' must be greater than zero\" # Setting the difference to zero for the first iteration # Checking if the absolute difference is small enough # Updating the values of the variables You now have the new parameter that defines the number of decision variables in your problem. The parameter is optional and has the default value . Lines 27 to 31 initialize the starting values of the decision variables:\n• If you provide a value other than , then it’s used for the starting values.\n• If is , then your random number generator creates the starting values using the standard normal distribution and the NumPy method . You get similar results again. You’ve learned how to write the functions that implement gradient descent and stochastic gradient descent. The code above can be made more robust and polished. You can also find different implementations of these methods in well-known machine learning libraries.\n\nStochastic gradient descent is widely used to train neural networks. The libraries for neural networks often have different variants of optimization algorithms based on stochastic gradient descent, such as: These optimization libraries are usually called internally when neural network software is trained. However, you can use them independently as well: In this example, you first import and then create the object needed for optimization:\n• is an instance of the stochastic gradient descent optimizer with a learning rate of and a momentum of .\n• is an instance of the decision variable with an initial value of .\n• is the cost function, which is a square function in this case. The main part of the code is a loop that iteratively calls and modifies and . Once the loop is exhausted, you can get the values of the decision variable and the cost function with . You can find more information on these algorithms in the Keras and TensorFlow documentation. The article An overview of gradient descent optimization algorithms offers a comprehensive list with explanations of gradient descent variants."
    },
    {
        "link": "https://linkedin.com/pulse/understanding-gradient-descent-python-rany-elhousieny-phd%E1%B4%AC%E1%B4%AE%E1%B4%B0",
        "document": "If you're new to the world of machine learning and optimization, the term \"Gradient Descent\" might sound intimidating. However, don't let the name scare you away. Gradient Descent is a fundamental optimization technique used not only in machine learning but also in various fields like physics, engineering, and economics. In this article, we'll break down Gradient Descent in simple terms and provide step-by-step examples in Python to help absolute beginners understand how it works.\n• Start at a random point: Imagine you're randomly placed on the hill.\n• Compute the slope (gradient): Determine the direction in which the hill is steepest. In math, this is the derivative of the function at your current location.\n• Take a step downhill: Move a small distance in the direction of the steepest slope. The size of this step is controlled by a parameter called the \"learning rate.\"\n• Repeat: Keep repeating steps 2 and 3 until you're close enough to the lowest point.\n• Import the necessary libraries:import numpy as np: This imports the NumPy library and gives it the alias \"np\" for convenience.import matplotlib.pyplot as plt: This imports the Pyplot module from the Matplotlib library and gives it the alias \"plt\" for convenience.\n• Define a quadratic cost function cost_function(x):def cost_function(x):: This defines a Python function called \"cost_function\" that takes a single argument \"x.\"return x**2 - 4*x + 3: The function computes a quadratic cost function of \"x,\" specifically x^2 - 4x + 3, and returns the result.\n• Create an array of x values:x = np.linspace(-10, 20, 1000): This creates an array of 1000 evenly spaced values of \"x\" ranging from -10 to 20 using NumPy's linspace function. These values will be used as input for the cost function \"cost_function(x).\"\n• Evaluate the cost function for each value of x:y = cost_function(x): This calculates the corresponding values of \"y\" by applying the cost function \"cost_function(x)\" to each value in the array \"x.\"\n• Plot the cost function f(x) versus x:plt.plot(x, y): This creates a line plot of \"y\" (the result of the cost function evaluations) against \"x\" values. This is essentially a visualization of the cost function J(x).\n• Add labels and a title to the plot:plt.xlabel('x'): Adds a label to the x-axis with the text \"x.\"plt.ylabel('cost_function(x)'): Adds a label to the y-axis with the text \"cost_function(x),\". plt.title('Plot of cost_function(x) versus x'): Adds a title to the plot with the text \"Plot of cost_function(x) versus x.\"\n• Display the plot:plt.show(): This command displays the plot on your screen, allowing you to visualize the cost function J(x) as a graph.\n\n4. Here is the final Program\n\n# Import necessary libraries import numpy as np # Define the cost function f(x) def cost_function(x): return x**2 - 4*x + 3 # Define the derivative of the cost function f'(x) def gradient(x): return 2*x - 4 # Gradient Descent parameters learning_rate = 0.1 # Step size iterations = 20 # Number of iterations # Initial guess for x (starting point) x = 0.0 # Gradient Descent optimization for i in range(iterations): # Compute the gradient at the current point grad = gradient(x) # Update x using the Gradient Descent formula x = x - learning_rate * grad # The value of x after optimization represents the minimum of the cost function minimum_x = x minimum_cost = cost_function(minimum_x) # Print the result print(f\"Minimum value of x: {minimum_x}\") print(f\"Minimum cost: {minimum_cost}\")\n• We define the cost function cost_function(x) representing our function f(x), and its derivative gradient(x) representing f′(x).\n• We set the Gradient Descent parameters, including the learning rate and the number of iterations.\n• We initialize the variable x with a starting point (in this case, 0).\n• We run the Gradient Descent optimization loop for the specified number of iterations. In each iteration, we compute the gradient and update x using the Gradient Descent formula.\n• After optimization, minimum_x contains the value of x that minimizes the cost function, and minimum_cost contains the minimum cost.\n\nimport numpy as np import matplotlib.pyplot as plt # Define the cost function f(x) def cost_function(x): return x**2 - 4*x + 3 # Define the derivative of the cost function f'(x) def gradient(x): return 2*x - 4 # Initialize the model parameter with a random value x = np.random.randn() # Set the learning rate learning_rate = 0.1 # Set the maximum number of iterations max_iterations = 100 # Initialize the list to store the cost values cost_values = [] # Iterate until convergence or max iterations for i in range(max_iterations): # Calculate the cost and gradient at the current parameter cost = cost_function(x) grad = gradient(x) # Update the parameter x = x - learning_rate * grad # Store the cost value cost_values.append(cost) # Print the progress if i % 10 == 0: print(\"Iteration {}: x = {}, cost = {}\".format(i, x, cost)) # Plot the cost values plt.plot(cost_values) plt.xlabel(\"Iterations\") plt.ylabel(\"Cost\") plt.show()\n\nIn the above code, we first define the cost function and its gradient. We then initialize the model parameter with a random value and set the learning rate and maximum number of iterations. We iterate until max iterations and calculate the cost and gradient at each iteration. We update the parameter by subtracting the product of the learning rate and gradient from the current parameter. We also store the cost value at each iteration and print the progress. Finally, we plot the cost values to visualize the convergence.\n\nimport numpy as np import matplotlib.pyplot as plt # Define the cost function f(x) def cost_function(x): return x**2 - 4*x + 3 # Define the derivative of the cost function f'(x) def gradient(x): return 2*x - 4 # Define the gradient descent algorithm def gradient_descent(cost_function, gradient, x_init, alpha, num_iterations): x = x_init x_list = [x] for i in range(num_iterations): x = x - alpha * gradient(x) x_list.append(x) # Print the progress print(\"Iteration {}: x = {}, cost = {}\".format(i, x, cost)) return x_list # Set the hyperparameters alpha = 0.1 num_iterations = 5 # Run gradient descent on f(x) x_init = 15 x_list = gradient_descent(cost_function, gradient, x_init, alpha, num_iterations) # Create an array of x values x = np.linspace(-2, 15, 100) # Evaluate f(x) for each value of x y = cost_function(x) # Plot f(x) versus x plt.plot(x, y) # Plot the gradient descent iterations for i in range(len(x_list) - 1): x1 = x_list[i] y1 = cost_function(x1) x2 = x_list[i + 1] y2 = cost_function(x2) plt.plot([x1, x2], [y1, y2], 'ro-') plt.text(x1, y1 + 0.5, round(y1, 2)) # Label the final cost value x_final = x_list[-1] y_final = cost_function(x_final) plt.text(x_final, y_final + 0.5, round(y_final, 2)) # Add labels and a title to the plot plt.xlabel('x') plt.ylabel('f(x)') plt.title('Gradient descent iterations on f(x)') # Display the plot plt.show()"
    },
    {
        "link": "https://stackoverflow.com/questions/17784587/gradient-descent-using-python-and-numpy",
        "document": "I think your code is a bit too complicated and it needs more structure, because otherwise you'll be lost in all equations and operations. In the end this regression boils down to four operations:\n• Calculate the loss = h - y and maybe the squared cost (loss^2)/2m\n\nIn your case, I guess you have confused with . Here denotes the number of examples in your training set, not the number of features.\n\nLet's have a look at my variation of your code:\n\nAt first I create a small random dataset which should look like this:\n\nAs you can see I also added the generated regression line and formula that was calculated by excel.\n\nYou need to take care about the intuition of the regression using gradient descent. As you do a complete batch pass over your data X, you need to reduce the m-losses of every example to a single weight update. In this case, this is the average of the sum over the gradients, thus the division by .\n\nThe next thing you need to take care about is to track the convergence and adjust the learning rate. For that matter you should always track your cost every iteration, maybe even plot it.\n\nIf you run my example, the theta returned will look like this:\n\nWhich is actually quite close to the equation that was calculated by excel (y = x + 30). Note that as we passed the bias into the first column, the first theta value denotes the bias weight."
    },
    {
        "link": "https://induraj2020.medium.com/implementing-gradient-descent-in-python-d1c6aeb9a448",
        "document": "Gradient descent is a popular optimization algorithm used in machine learning and deep learning to find the optimal parameters or weights for a given model. The goal of gradient descent is to minimize the cost function, which measures the difference between the predicted output of the model and the actual output.\n\nThe algorithm works by iteratively adjusting the parameters of the model in the direction of the steepest descent of the cost function gradient until a minimum is reached. The gradient is computed by taking the partial derivatives of the cost function with respect to each parameter.\n\nThere are three main variants of gradient descent:\n• Batch Gradient Descent: In this variant, the gradient is computed over the entire dataset, and the parameters are updated after each epoch.\n• Stochastic Gradient Descent: In this variant, the gradient is computed on a single training example, and the parameters are updated after each example.\n• Mini-Batch Gradient Descent: In this variant, the gradient is computed on a small subset of the training data, and the parameters are updated after each mini-batch.\n\nGradient descent is used in various machine learning applications, such as linear regression, logistic regression, and neural networks, to optimize the model’s parameters and improve its accuracy. It is a fundamental algorithm in machine learning and is essential for training complex models with large amounts of data.\n\nDuring each iteration of gradient descent, the parameters θ are updated according to the above formula, where ∇J(θ) is evaluated using the current values of θ. This means that in each iteration, the algorithm takes a step in the direction of the steepest descent of the cost function, with a step size determined by the learning rate. The learning rate determines the size of the step taken at each iteration and needs to be carefully chosen to ensure that the algorithm converges to the optimal solution.\n\nGradient descent is a fundamental optimization algorithm in machine learning and has numerous practical use cases. Here are some examples:\n• Linear Regression: In linear regression, gradient descent is used to find the optimal coefficients that minimize the sum of squared errors between the predicted and actual values.\n• Logistic Regression: In logistic regression, gradient descent is used to find the optimal parameters that minimize the cross-entropy loss function, which measures the difference between the predicted probabilities and actual labels.\n• Neural Networks: In deep learning, gradient descent is used to optimize the weights and biases of the neural network by minimizing the loss function, which measures the difference between the predicted and actual outputs.\n• Support Vector Machines (SVMs): In SVMs, gradient descent is used to find the optimal hyperplane that separates the data points into different classes with maximum margin.\n• Dimensionality Reduction: In techniques such as Principal Component Analysis (PCA), gradient descent is used to find the optimal eigenvectors that capture the maximum variance in the data.\n• Clustering: In clustering algorithms such as k-means, gradient descent is used to optimize the centroids of the clusters by minimizing the sum of squared distances between the data points and their assigned cluster centroids.\n\nOverall, gradient descent is a versatile optimization algorithm that is widely used in various machine learning applications to find the optimal parameters of the model and improve its accuracy.\n• Select a model that you want to optimize, such as linear regression, logistic regression, or a neural network.\n• Choose a cost function that measures the difference between the predicted output and the actual output, such as mean squared error, cross-entropy loss, or binary log loss.\n• Set the initial values for the parameters that you want to optimize, such as the weights and biases of the model.\n• Calculate the gradient of the cost function with respect to each parameter by taking the partial derivative of the cost function with respect to each parameter.\n• Adjust the parameters in the direction of the negative gradient by multiplying it with a learning rate, which controls the size of the update.\n• Iterate the above three steps until the cost function converges to a minimum or a satisfactory threshold, such as a small change in the cost function between iterations.\n• Test the trained model on a separate set of data to evaluate its performance, such as the accuracy, precision, recall, or F1 score.\n\nNote that there are different variants of gradient descent, such as batch gradient descent, stochastic gradient descent, and mini-batch gradient descent, which have different computational and convergence properties. The implementation details may also vary depending on the specific model and library used.\n\nThis implementation defines a simple quadratic function and its partial derivatives and . It then defines the function, which takes the starting point , learning rate , and the number of iterations as inputs and returns the optimal point and the minimum value of the function, as well as the history of the parameter values during the iterations. The mesh grid is defined for plotting the function and the results of the gradient descent algorithm are plotted in a 3D graph using\n• Flexibility: Gradient descent can be used with different types of models and loss functions, making it a versatile optimization algorithm.\n• Efficiency: Gradient descent is computationally efficient and can handle large datasets with numerous features.\n• Convergence: Gradient descent guarantees convergence to a minimum, given a sufficiently small learning rate and enough iterations.\n• Scalability: Gradient descent can be parallelized across multiple processors or nodes, enabling faster training times.\n• Sensitivity to Learning Rate: The performance of gradient descent is highly sensitive to the choice of the learning rate, which can be difficult to tune.\n• Local Minima: Gradient descent can get trapped in local minima, which may not be the global minimum.\n• Overfitting: Gradient descent can overfit the training data if the regularization is not applied or if the model is too complex.\n• Scaling: Gradient descent may require feature scaling to ensure that each feature contributes equally to the gradient, which can be a time-consuming preprocessing step.\n\nOverall, gradient descent is a powerful optimization algorithm with many advantages, but its performance can be impacted by various factors, including the learning rate, the choice of optimization algorithm, and the complexity of the model."
    },
    {
        "link": "https://geeksforgeeks.org/how-to-implement-a-gradient-descent-in-python-to-find-a-local-minimum",
        "document": "Gradient Descent is an iterative algorithm that is used to minimize a function by finding the optimal parameters. Gradient Descent can be applied to any dimension function i.e. 1-D, 2-D, 3-D. In this article, we will be working on finding global minima for parabolic function (2-D) and will be implementing gradient descent in python to find the optimal parameters for the linear regression equation (1-D). Before diving into the implementation part, let us make sure the set of parameters required to implement the gradient descent algorithm. To implement a gradient descent algorithm, we require a cost function that needs to be minimized, the number of iterations, a learning rate to determine the step size at each iteration while moving towards the minimum, partial derivatives for weight & bias to update the parameters at each iteration, and a prediction function.\n\nTill now we have seen the parameters required for gradient descent. Now let us map the parameters with the gradient descent algorithm and work on an example to better understand gradient descent. Let us consider a parabolic equation y=4x2. By looking at the equation we can identify that the parabolic function is minimum at x = 0 i.e. at x=0, y=0. Therefore x=0 is the local minima of the parabolic function y=4x2. Now let us see the algorithm for gradient descent and how we can obtain the local minima by applying gradient descent:\n\nSteps should be made in proportion to the negative of the function gradient (move away from the gradient) at the current point to find local minima. Gradient Ascent is the procedure for approaching a local maximum of a function by taking steps proportional to the positive of the gradient (moving towards the gradient).\n\nStep 1: Initializing all the necessary parameters and deriving the gradient function for the parabolic equation 4x2. The derivative of x2 is 2x, so the derivative of the parabolic equation 4x2 will be 8x.\n\nStep 2: Let us perform 3 iterations of gradient descent:\n\nFor each iteration keep on updating the value of x based on the gradient descent formula.\n\nFrom the above three iterations of gradient descent, we can notice that the value of x is decreasing iteration by iteration and will slowly converge to 0 (local minima) by running the gradient descent for more iterations. Now you might have a question, for how many iterations we should run gradient descent?\n\nWe can set a stopping threshold i.e. when the difference between the previous and the present value of x becomes less than the stopping threshold we stop the iterations. When it comes to the implementation of gradient descent for machine learning algorithms and deep learning algorithms we try to minimize the cost function in the algorithms using gradient descent. Now that we are clear with the gradient descent’s internal working, let us look into the python implementation of gradient descent where we will be minimizing the cost function of the linear regression algorithm and finding the best fit line. In our case the parameters are below mentioned:\n\nThe prediction function for the linear regression algorithm is a linear equation given by y=wx+b.\n\nThe cost function is used to calculate the loss based on the predictions made. In linear regression, we use mean squared error to calculate the loss. Mean Squared Error is the sum of the squared differences between the actual and predicted values.\n\nHere, n is the number of samples\n\nCalculating the partial derivatives for weight and bias using the cost function. We get:\n\nUpdating the weight and bias by subtracting the multiplication of learning rates and their respective gradients.\n\nIn the implementation part, we will be writing two functions, one will be the cost functions that take the actual output and the predicted output as input and returns the loss, the second will be the actual gradient descent function which takes the independent variable, target variable as input and finds the best fit line using gradient descent algorithm. The iterations, learning_rate, and stopping threshold are the tuning parameters for the gradient descent algorithm and can be tuned by the user. In the main function, we will be initializing linearly related random data and applying the gradient descent algorithm on the data to find the best fit line. The optimal weight and bias found by using the gradient descent algorithm are later used to plot the best fit line in the main function. The iterations specify the number of times the update of parameters must be done, the stopping threshold is the minimum change of loss between two successive iterations to stop the gradient descent algorithm."
    }
]