[
    {
        "link": "https://github.com/Taeyoung96/Yolo-to-COCO-format-converter",
        "document": "When you use Yolo-model, you might create annotation labels with Yolo-mark.\n\n For example,\nâ€¢ - example of list with object names\nâ€¢ - example with list of image filenames for training Yolo model\nâ€¢ - example of folder that contain images and labels\n\nBut, when you want to use another model(ex. efficientdet), you need another annotation format! ðŸ˜¥\nâ€¢ Oct 13th, 2021 - We could support not only outputs, but also outputs!\n\n Also, We could make segmentation mask polygons information in json file.\n\n Thanks to @NauchtanRobotics!\n\nYou can make same environment with anaconda environment.\n\nWhen you have your own Yolo annotation format, just change a little bit!\n\nIn , there is a code that declare the classes. You will change this with your .\n\nNext, follow step 2 if you have your annotations in separate text files, one for each image. Alternatively, follow step 3 if you wish to work from YOLO annotations which are concatenated into a single file.\n\nUse this approach if your training data file structure looks like this:\n\nYou don't need to specify argument.\nâ€¢ None python main.py --path <Absolute path to dataset_root_dir> --output <Name of the json file>\n\nUse this approach if your annotations are in nested a level below the image files like this:\nâ€¢ None python main.py --yolo-subdir --path <Absolute path to dataset_root_dir> --output <Name of the json file>\nâ€¢ None python main.py --yolo-subdir --box2seg --path <Absolute path to dataset_root_dir> --output <Name of the json file>\n\nThe arg initializes segmentation mask polygons that have box shapes. This is useful for when changing your modeling from object detection to image segmentation. These masks can then be reshaped using software such as the interface provided by makesense.ai\n\nMake sure that it points to the absolute path to the folder where the image and text files are located.\n\n You can easily change the path with (Ubuntu 18.04) or (Window 10).\n\nIf you want to quickly create a train.txt file in Ubuntu, you can use path_replacer.py.\nâ€¢ path_image_folder: File path where the images are located.\nâ€¢ path_txt: File path of the 'txt' file you want to create.\n\nWhen you want to use\nâ€¢ None python path_replacer.py --path_image_folder <File path where the images are located> --path_txt <File path of the 'txt' file you want to create>\n\nYou need to provide 2 argments(essential) & 3 argments(optional).\n\n essential\nâ€¢ output : Name of the json file\nâ€¢ yolo-subdir : If your annotation label have OpenLabeling output.\nâ€¢ box2seg : If you want to make segmentation mask polygons that have box shapes.\nâ€¢ debug : If you want to check the bounding boxes or annotation information.\n\nWhen you want to make json file,\nâ€¢ None python main.py --path <Absolute Path of train.txt> --output <Name of the json file>\n\nOr when you want to check the bounding boxes,\nâ€¢ None python main.py --path <Absolute Path of train.txt> --output <Name of the json file> --debug\n\nIf you want to read json files more clearly, you should use !\n\nOn debug mode, you can check bounding boxes\n\nOn debug mode, you can check annotation information on terminal"
    },
    {
        "link": "https://docs.ultralytics.com/reference/data/converter",
        "document": "Path to directory to save results to. Whether to include segmentation masks in the output. Whether to include keypoint annotations in the output. Whether to map 91 COCO class IDs to the corresponding 80 COCO class IDs. Whether to convert data in lvis dataset way. Generates output files in the specified output directory. save_dir (str, optional): Path to directory to save results to. use_segments (bool, optional): Whether to include segmentation masks in the output. use_keypoints (bool, optional): Whether to include keypoint annotations in the output. cls91to80 (bool, optional): Whether to map 91 COCO class IDs to the corresponding 80 COCO class IDs. lvis (bool, optional): Whether to convert data in lvis dataset way. Generates output files in the specified output directory. # NOTE: create folders for both train and val in advance, # since LVIS val set contains images from COCO 2017 train in addition to the COCO 2017 val split.\n\nConverts a dataset of segmentation mask images to the YOLO segmentation format. This function takes the directory containing the binary format mask images and converts them into YOLO segmentation format. The converted masks are saved in the specified output directory. The path to the directory where all mask images (png, jpg) are stored. The path to the directory where the converted YOLO segmentation masks will be stored. Total classes in the dataset i.e. for COCO classes=80 The classes here is the total classes in the dataset, for COCO dataset we have 80 classes The expected directory structure for the masks is: - masks â”œâ”€ mask_image_01.png or mask_image_01.jpg â”œâ”€ mask_image_02.png or mask_image_02.jpg â”œâ”€ mask_image_03.png or mask_image_03.jpg â””â”€ mask_image_04.png or mask_image_04.jpg After execution, the labels will be organized in the following structure: Converts a dataset of segmentation mask images to the YOLO segmentation format. This function takes the directory containing the binary format mask images and converts them into YOLO segmentation format. The converted masks are saved in the specified output directory. masks_dir (str): The path to the directory where all mask images (png, jpg) are stored. output_dir (str): The path to the directory where the converted YOLO segmentation masks will be stored. classes (int): Total classes in the dataset i.e. for COCO classes=80 The classes here is the total classes in the dataset, for COCO dataset we have 80 classes The expected directory structure for the masks is: After execution, the labels will be organized in the following structure: # Create a binary mask for the current class and find contours # YOLO requires at least 3 points for a valid segmentation\n\nThe function processes images in the 'train' and 'val' folders of the DOTA dataset. For each image, it reads the associated label from the original labels directory and writes new labels in YOLO OBB format to a new directory. The root directory path of the DOTA dataset. The directory structure assumed for the DOTA dataset: After execution, the function will organize the labels into: The function processes images in the 'train' and 'val' folders of the DOTA dataset. For each image, it reads the associated label from the original labels directory and writes new labels in YOLO OBB format to a new directory. dota_root_path (str): The root directory path of the DOTA dataset. The directory structure assumed for the DOTA dataset: After execution, the function will organize the labels into: \"\"\"Converts a single image's DOTA annotation to YOLO OBB format and saves it to a specified directory.\"\"\"\n\nMerge multiple segments into one list by connecting the coordinates with the minimum distance between each segment. This function connects these coordinates with a thin line to merge all segments into one. Original segmentations in COCO's JSON file. Each element is a list of coordinates, like [segmentation1, segmentation2,...]. Merge multiple segments into one list by connecting the coordinates with the minimum distance between each segment. This function connects these coordinates with a thin line to merge all segments into one. Each element is a list of coordinates, like [segmentation1, segmentation2,...]. # Record the indexes with min distance between each segment # Use two round to connect all the segments # Middle segments have two indexes, reverse the index of middle segments # Deal with the first segment and the last one\n\nConverts existing object detection dataset (bounding boxes) to segmentation dataset or oriented bounding box (OBB) in YOLO format. Generates segmentation data using SAM auto-annotator as needed. Path to save the generated labels, labels will be saved into in the same directory level of if save_dir is None. Segmentation model to use for intermediate segmentation data. in YOLO format. Generates segmentation data using SAM auto-annotator as needed. save_dir (str | Path): Path to save the generated labels, labels will be saved into `labels-segment` in the same directory level of `im_dir` if save_dir is None. sam_model (str): Segmentation model to use for intermediate segmentation data. \"Segmentation labels detected, no need to generate new ones!\"\n\nCreates a synthetic COCO dataset with random images based on filenames from label lists. This function downloads COCO labels, reads image filenames from label list files, creates synthetic images for train2017 and val2017 subsets, and organizes them in the COCO dataset structure. It uses multithreading to generate images efficiently.\nâ€¢ Existing test2017 directory is removed as it's not needed. Creates a synthetic COCO dataset with random images based on filenames from label lists. creates synthetic images for train2017 and val2017 subsets, and organizes them in the COCO dataset structure. It uses multithreading to generate images efficiently. - Existing test2017 directory is removed as it's not needed. \"\"\"Generates synthetic images with random sizes and colors for dataset augmentation or testing purposes.\"\"\" # The actual work is done in the background does not exist. Skipping image creation for"
    },
    {
        "link": "https://stackoverflow.com/questions/64096953/how-to-convert-yolo-format-bounding-box-coordinates-into-opencv-format",
        "document": "I have format bounding box annotations of objects saved in a files. Now I want to load those coordinates and draw it on the image using , but I donâ€™t know how to convert those float values into format coordinates values\n\nI tried this post but it didnâ€™t help, below is a sample example of what I am trying to do"
    },
    {
        "link": "https://github.com/ultralytics/ultralytics/issues/7398",
        "document": "\nâ€¢ I have searched the YOLOv8 issues and discussions and found no similar questions.\n\nI am currently utilizing predict.py to obtain predictions and corresponding labels in YOLO format for human pose estimation. However, I'm interested in finding out if there's a method to transform these labels from YOLO format into COCO format (.json file). This is because I aim to train Yolox-pose models using the annotations derived from the output of Yolov8-pose models."
    },
    {
        "link": "https://stackoverflow.com/questions/56115874/how-to-convert-bounding-box-x1-y1-x2-y2-to-yolo-style-x-y-w-h",
        "document": "I'm training a YOLO model, I have the bounding boxes in this format:-\n\nI need to convert it to YOLO format to be something like:-\n\nI already calculated the center point X, Y, the height H, and the weight W. But still need a away to convert them to floating numbers as mentioned."
    },
    {
        "link": "https://medium.com/@mikolaj.buchwald/yolo-and-coco-object-recognition-basics-in-python-65d06f42a6f8",
        "document": "I quote this explanation from: OpenCV YOLO tutorial\n\nThe class names are stored in the `coco.names` text file:\n\nNote, in order to use the code from this notebook with larger resolution images, you will have to adjust border width and font size in the cells below, else the borders will be very thin, almost unvisible on the final rendered picture, and the font size will be extremely small.\n\nIt seems that, depending on the (OpenCV?) implementation, the can be either a vector or a matrix. This problem is dealt with in this Notebook by - clause, but pay attention to that in your own use cases.\n\nAs a matter of fact, we will need OpenCV for Python in this example (i.e., ). It is usually (by convention) imported like that: .\n\nMatplotlib interprets images in RGB format, but OpenCV uses BGR format\n\nSo to convert the image so that itâ€™s properly loaded, convert it before loading (for details, see: https://www.codegrepper.com/code-examples/python/imshow+wrong+colors).\n\nIn order to run the network you will have to download the pre-trained YOLO weight file (237 MB). https://pjreddie.com/media/files/yolov3.weights Also download the the YOLO configuration file:"
    },
    {
        "link": "https://docs.ultralytics.com/datasets/detect",
        "document": "Training a robust and accurate object detection model requires a comprehensive dataset. This guide introduces various formats of datasets that are compatible with the Ultralytics YOLO model and provides insights into their structure, usage, and how to convert between different formats.\n\nThe Ultralytics YOLO format is a dataset configuration format that allows you to define the dataset root directory, the relative paths to training/validation/testing image directories or files containing image paths, and a dictionary of class names. Here is an example:\n\nLabels for this format should be exported to YOLO format with one file per image. If there are no objects in an image, no file is required. The file should be formatted with one row per object in format. Box coordinates must be in normalized xywh format (from 0 to 1). If your boxes are in pixels, you should divide and by image width, and and by image height. Class numbers should be zero-indexed (start with 0).\n\nThe label file corresponding to the above image contains 2 persons (class ) and a tie (class ):\n\nWhen using the Ultralytics YOLO format, organize your training and validation images and labels as shown in the COCO8 dataset example below.\n\nHere's how you can use these formats to train your model:\n\nHere is a list of the supported datasets and a brief description for each:\nâ€¢ Argoverse: A dataset containing 3D tracking and motion forecasting data from urban environments with rich annotations.\nâ€¢ COCO: Common Objects in Context (COCO) is a large-scale object detection, segmentation, and captioning dataset with 80 object categories.\nâ€¢ COCO8: A smaller subset of the first 4 images from COCO train and COCO val, suitable for quick tests.\nâ€¢ COCO128: A smaller subset of the first 128 images from COCO train and COCO val, suitable for tests.\nâ€¢ Global Wheat 2020: A dataset containing images of wheat heads for the Global Wheat Challenge 2020.\nâ€¢ Objects365: A high-quality, large-scale dataset for object detection with 365 object categories and over 600K annotated images.\nâ€¢ OpenImagesV7: A comprehensive dataset by Google with 1.7M train images and 42k validation images.\nâ€¢ SKU-110K: A dataset featuring dense object detection in retail environments with over 11K images and 1.7 million bounding boxes.\nâ€¢ VisDrone: A dataset containing object detection and multi-object tracking data from drone-captured imagery with over 10K images and video sequences.\nâ€¢ VOC: The Pascal Visual Object Classes (VOC) dataset for object detection and segmentation with 20 object classes and over 11K images.\nâ€¢ xView: A dataset for object detection in overhead imagery with 60 object categories and over 1 million annotated objects.\nâ€¢ Roboflow 100: A diverse object detection benchmark with 100 datasets spanning seven imagery domains for comprehensive model evaluation.\nâ€¢ Brain-tumor: A dataset for detecting brain tumors includes MRI or CT scan images with details on tumor presence, location, and characteristics.\nâ€¢ Signature: A dataset featuring images of various documents with annotated signatures, supporting document verification and fraud detection research.\nâ€¢ Medical-pills: A dataset featuring images of medical-pills, annotated for applications such as pharmaceutical quality assurance, pill sorting, and regulatory compliance.\n\nIf you have your own dataset and would like to use it for training detection models with Ultralytics YOLO format, ensure that it follows the format specified above under \"Ultralytics YOLO format\". Convert your annotations to the required format and specify the paths, number of classes, and class names in the YAML configuration file.\n\nYou can easily convert labels from the popular COCO dataset format to the YOLO format using the following code snippet:\n\nThis conversion tool can be used to convert the COCO dataset or any dataset in the COCO format to the Ultralytics YOLO format. The process transforms the JSON-based COCO annotations into the simpler text-based YOLO format, making it compatible with Ultralytics YOLO models.\n\nRemember to double-check if the dataset you want to use is compatible with your model and follows the necessary format conventions. Properly formatted datasets are crucial for training successful object detection models.\n\nWhat is the Ultralytics YOLO dataset format and how to structure it?\n\nThe Ultralytics YOLO format is a structured configuration for defining datasets in your training projects. It involves setting paths to your training, validation, and testing images and corresponding labels. For example:\n\nLabels are saved in files with one file per image, formatted as with normalized coordinates. For a detailed guide, see the COCO8 dataset example.\n\nHow do I convert a COCO dataset to the YOLO format?\n\nYou can convert a COCO dataset to the YOLO format using the Ultralytics conversion tools. Here's a quick method:\n\nThis code will convert your COCO annotations to YOLO format, enabling seamless integration with Ultralytics YOLO models. For additional details, visit the Port or Convert Label Formats section.\n\nWhich datasets are supported by Ultralytics YOLO for object detection?\n\nEach dataset page provides detailed information on the structure and usage tailored for efficient YOLO11 training. Explore the full list in the Supported Datasets section.\n\nHow do I start training a YOLO11 model using my dataset?\n\nTo start training a YOLO11 model, ensure your dataset is formatted correctly and the paths are defined in a YAML file. Use the following script to begin training:\n\nRefer to the Usage section for more details on utilizing different modes, including CLI commands.\n\nWhere can I find practical examples of using Ultralytics YOLO for object detection?\n\nUltralytics provides numerous examples and practical guides for using YOLO11 in diverse applications. For a comprehensive overview, visit the Ultralytics Blog where you can find case studies, detailed tutorials, and community stories showcasing object detection, segmentation, and more with YOLO11. For specific examples, check the Usage section in the documentation."
    },
    {
        "link": "https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation",
        "document": "Bounding boxes are rectangles that mark objects on an image. There are multiple formats of bounding boxes annotations. Each format uses its specific representation of bounding boxes coordinates. Albumentations supports four formats: , , , and .\n\nLet's take a look at each of those formats and how they represent coordinates of bounding boxes.\n\nAs an example, we will use an image from the dataset named Common Objects in Context. It contains one bounding box that marks a cat. The image width is 640 pixels, and its height is 480 pixels. The width of the bounding box is 322 pixels, and its height is 117 pixels.\n\nThe bounding box has the following coordinates of its corners: top-left is or , top-right is or , bottom-left is or , bottom-right is or . As you see, coordinates of the bounding box's corners are calculated with respect to the top-left corner of the image which has coordinates .\n\nAn example image with a bounding box from the COCO dataset\n\nis a format used by the Pascal VOC dataset. Coordinates of a bounding box are encoded with four values in pixels: . and are coordinates of the top-left corner of the bounding box. and are coordinates of bottom-right corner of the bounding box.\n\nCoordinates of the example bounding box in this format are .\n\nis similar to , because it also uses four values to represent a bounding box. But unlike , uses normalized values. To normalize values, we divide coordinates in pixels for the x- and y-axis by the width and the height of the image.\n\nCoordinates of the example bounding box in this format are which are .\n\nAlbumentations uses this format internally to work with bounding boxes and augment them.\n\nis a format used by the Common Objects in Context COCO dataset.\n\nIn , a bounding box is defined by four values in pixels . They are coordinates of the top-left corner along with the width and height of the bounding box.\n\nCoordinates of the example bounding box in this format are .\n\nIn , a bounding box is represented by four values . and are the normalized coordinates of the center of the bounding box. To make coordinates normalized, we take pixel values of x and y, which marks the center of the bounding box on the x- and y-axis. Then we divide the value of x by the width of the image and value of y by the height of the image. and represent the width and the height of the bounding box. They are normalized as well.\n\nCoordinates of the example bounding box in this format are which are .\n\nHow different formats represent coordinates of a bounding box\n\nJust like with images and masks augmentation, the process of augmenting bounding boxes consists of 4 steps.\nâ€¢ You read images and bounding boxes from the disk.\nâ€¢ You pass an image and bounding boxes to the augmentation pipeline and receive augmented images and boxes.\n\nHere an example of a minimal declaration of an augmentation pipeline that works with bounding boxes.\n\nNote that unlike image and masks augmentation, now has an additional parameter . You need to pass an instance of to that argument. specifies settings for working with bounding boxes. sets the format for bounding boxes coordinates.\n\nIt can either be , , or . This value is required because Albumentation needs to know the coordinates' source format for bounding boxes to apply augmentations correctly.\n\nBesides , supports a few more settings.\n\nHere is an example of that shows all available settings with :\n\nand parameters control what Albumentations should do to the augmented bounding boxes if their size has changed after augmentation. The size of bounding boxes could change if you apply spatial augmentations, for example, when you crop a part of an image or when you resize an image.\n\nis a value in pixels. If the area of a bounding box after augmentation becomes smaller than , Albumentations will drop that box. So the returned list of augmented bounding boxes won't contain that bounding box.\n\nis a value between 0 and 1. If the ratio of the bounding box area after augmentation to the area of the bounding box before augmentation becomes smaller than , Albumentations will drop that box. So if the augmentation process cuts the most of the bounding box, that box won't be present in the returned list of the augmented bounding boxes.\n\nHere is an example image that contains two bounding boxes. Bounding boxes coordinates are declared using the format.\n\nAn example image with two bounding boxes\n\nFirst, we apply the augmentation without declaring parameters and . The augmented image contains two bounding boxes.\n\nAn example image with two bounding boxes after applying augmentation\n\nNext, we apply the same augmentation, but now we also use the parameter. Now, the augmented image contains only one bounding box, because the other bounding box's area after augmentation became smaller than , so Albumentations dropped that bounding box.\n\nAn example image with one bounding box after applying augmentation with 'min_area'\n\nFinally, we apply the augmentation with the . After that augmentation, the resulting image doesn't contain any bounding box, because visibility of all bounding boxes after augmentation are below threshold set by .\n\nAn example image with zero bounding boxes after applying augmentation with 'min_visibility'\n\nBesides coordinates, each bounding box should have an associated class label that tells which object lies inside the bounding box. There are two ways to pass a label for a bounding box.\n\nLet's say you have an example image with three objects: , , and . Bounding boxes coordinates in the format for those objects are , , and .\n\nAn example image with 3 bounding boxes from the COCO dataset\n\n1. You can pass labels along with bounding boxes coordinates by adding them as additional values to the list of coordinates.Â¶\n\nFor the image above, bounding boxes with class labels will become , , and .\n\nAlso, you can use multiple class values for each bounding box, for example , , and .\n\n2.You can pass labels for bounding boxes as a separate list (the preferred way).Â¶\n\nFor example, if you have three bounding boxes like , , and you can create a separate list with values like , or that contains class labels for those bounding boxes. Next, you pass that list with class labels as a separate argument to the function. Albumentations needs to know the names of all those lists with class labels to join them with augmented bounding boxes correctly. Then, if a bounding box is dropped after augmentation because it is no longer visible, Albumentations will drop the class label for that box as well. Use parameter to set names for all arguments in that will contain label descriptions for bounding boxes (more on that in Step 4).\n\nStep 3. Read images and bounding boxes from the disk.Â¶\n\nRead an image from the disk.\n\nBounding boxes can be stored on the disk in different serialization formats: JSON, XML, YAML, CSV, etc. So the code to read bounding boxes depends on the actual format of data on the disk.\n\nAfter you read the data from the disk, you need to prepare bounding boxes for Albumentations.\n\nAlbumentations expects that bounding boxes will be represented as a list of lists. Each list contains information about a single bounding box. A bounding box definition should have at list four elements that represent the coordinates of that bounding box. The actual meaning of those four values depends on the format of bounding boxes (either , , , or ). Besides four coordinates, each definition of a bounding box may contain one or more extra values. You can use those extra values to store additional information about the bounding box, such as a class label of the object inside the box. During augmentation, Albumentations will not process those extra values. The library will return them as is along with the updated coordinates of the augmented bounding box.\n\nStep 4. Pass an image and bounding boxes to the augmentation pipeline and receive augmented images and boxes.Â¶\n\nAs discussed in Step 2, there are two ways of passing class labels along with bounding boxes coordinates:\n\nSo, if you have coordinates of three bounding boxes that look like this:\n\nyou can add a class label for each bounding box as an additional element of the list along with four coordinates. So now a list with bounding boxes and their coordinates will look the following:\n\nor with multiple labels per each bounding box:\n\nNext, you pass an image and bounding boxes for it to the function and receive the augmented image and bounding boxes.\n\nExample input and output data for bounding boxes augmentation\n\n2. Pass class labels in a separate argument to (the preferred way).Â¶\n\nLet's say you have coordinates of three bounding boxes\n\nYou can create a separate list that contains class labels for those bounding boxes:\n\nThen you pass both bounding boxes and class labels to . Note that to pass class labels, you need to use the name of the argument that you declared in when creating an instance of Compose in step 2. In our case, we set the name of the argument to .\n\nExample input and output data for bounding boxes augmentation with a separate argument for class labels\n\nNote that expects a list, so you can set multiple fields that contain labels for your bounding boxes. So if you declare Compose like\n\nyou can use those multiple arguments to pass info about class labels, like\nâ€¢ Using Albumentations to augment bounding boxes for object detection tasks\nâ€¢ How to use Albumentations for detection tasks if you need to keep all bounding boxes\nâ€¢ Showcase. Cool augmentation examples on diverse set of images from various real-world tasks."
    },
    {
        "link": "https://christianbernecker.medium.com/convert-bounding-boxes-from-coco-to-pascal-voc-to-yolo-and-back-660dc6178742",
        "document": "This article describes the differences between the bounding boxes format Coco, Pascal_VOC and Yolo Style and provides Code-Snippets about converting from one to another.\n\nBounding boxes are rectangles used to surround objects in images. They are often used in object detection. There are multiple formats of bounding boxes (Coco, Yolo, Pascal). Each format uses its specific representation of bounding boxes coordinates. The following picture illustrates a bounding box.\n\nIn the next chapter I will explain the specific representations of each bounding box standard. Afterwards you will find code snippets to convert from one format to another."
    },
    {
        "link": "https://docs.ultralytics.com/datasets/detect/coco",
        "document": "The COCO (Common Objects in Context) dataset is a large-scale object detection, segmentation, and captioning dataset. It is designed to encourage research on a wide variety of object categories and is commonly used for benchmarking computer vision models. It is an essential dataset for researchers and developers working on object detection, segmentation, and pose estimation tasks.\nâ€¢ COCO contains 330K images, with 200K images having annotations for object detection, segmentation, and captioning tasks.\nâ€¢ The dataset comprises 80 object categories, including common objects like cars, bicycles, and animals, as well as more specific categories such as umbrellas, handbags, and sports equipment.\nâ€¢ Annotations include object bounding boxes, segmentation masks, and captions for each image.\nâ€¢ COCO provides standardized evaluation metrics like mean Average Precision (mAP) for object detection, and mean Average Recall (mAR) for segmentation tasks, making it suitable for comparing model performance.\n\nThe COCO dataset is split into three subsets:\nâ€¢ Train2017: This subset contains 118K images for training object detection, segmentation, and captioning models.\nâ€¢ Val2017: This subset has 5K images used for validation purposes during model training.\nâ€¢ Test2017: This subset consists of 20K images used for testing and benchmarking the trained models. Ground truth annotations for this subset are not publicly available, and the results are submitted to the COCO evaluation server for performance evaluation.\n\nThe COCO dataset is widely used for training and evaluating deep learning models in object detection (such as Ultralytics YOLO, Faster R-CNN, and SSD), instance segmentation (such as Mask R-CNN), and keypoint detection (such as OpenPose). The dataset's diverse set of object categories, large number of annotated images, and standardized evaluation metrics make it an essential resource for computer vision researchers and practitioners.\n\nA YAML (Yet Another Markup Language) file is used to define the dataset configuration. It contains information about the dataset's paths, classes, and other relevant information. In the case of the COCO dataset, the file is maintained at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml.\n\nTo train a YOLO11n model on the COCO dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.\n\nThe COCO dataset contains a diverse set of images with various object categories and complex scenes. Here are some examples of images from the dataset, along with their corresponding annotations:\nâ€¢ Mosaiced Image: This image demonstrates a training batch composed of mosaiced dataset images. Mosaicing is a technique used during training that combines multiple images into a single image to increase the variety of objects and scenes within each training batch. This helps improve the model's ability to generalize to different object sizes, aspect ratios, and contexts.\n\nThe example showcases the variety and complexity of the images in the COCO dataset and the benefits of using mosaicing during the training process.\n\nIf you use the COCO dataset in your research or development work, please cite the following paper:\n\nWe would like to acknowledge the COCO Consortium for creating and maintaining this valuable resource for the computer vision community. For more information about the COCO dataset and its creators, visit the COCO dataset website.\n\nWhat is the COCO dataset and why is it important for computer vision?\n\nThe COCO dataset (Common Objects in Context) is a large-scale dataset used for object detection, segmentation, and captioning. It contains 330K images with detailed annotations for 80 object categories, making it essential for benchmarking and training computer vision models. Researchers use COCO due to its diverse categories and standardized evaluation metrics like mean Average Precision (mAP).\n\nHow can I train a YOLO model using the COCO dataset?\n\nTo train a YOLO11 model using the COCO dataset, you can use the following code snippets:\n\nRefer to the Training page for more details on available arguments.\n\nWhat are the key features of the COCO dataset?\nâ€¢ 330K images, with 200K annotated for object detection, segmentation, and captioning.\nâ€¢ 80 object categories ranging from common items like cars and animals to specific ones like handbags and sports equipment.\nâ€¢ Standardized evaluation metrics for object detection (mAP) and segmentation (mean Average Recall, mAR).\nâ€¢ Mosaicing technique in training batches to enhance model generalization across various object sizes and contexts.\n\nWhere can I find pretrained YOLO11 models trained on the COCO dataset?\n\nPretrained YOLO11 models on the COCO dataset can be downloaded from the links provided in the documentation. Examples include:\n\nThese models vary in size, mAP, and inference speed, providing options for different performance and resource requirements.\n\nHow is the COCO dataset structured and how do I use it?\n\nThe COCO dataset is split into three subsets:\nâ€¢ Test2017: 20K images for benchmarking trained models. Results need to be submitted to the COCO evaluation server for performance evaluation.\n\nThe dataset's YAML configuration file is available at coco.yaml, which defines paths, classes, and dataset details."
    }
]