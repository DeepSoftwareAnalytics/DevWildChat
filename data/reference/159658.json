[
    {
        "link": "https://vkguide.dev/docs/chapter-2/pipeline_walkthrough",
        "document": "Now that we can load the shaders we needed for the triangle, we have to build the VkPipeline to render it.\n\nThe VkPipeline is a huge object in Vulkan that encompasses the configuration of the entire GPU for the draw. Building them can be very expensive, as it will fully convert the shader module into the GPU instructions, and will validate the setup for it.\n\nOnce a pipeline is built, it can be bound inside a command buffer, and then when you draw anything it will use the bound pipeline.\n\nVulkan pipelines are a massive object with many different configuration structs, some of them even running pointers and being arrays. For this reason, we are going to create a class specifically for building pipelines, that will simplify the process.\n\nOver the tutorial, we are going to create more pipelines, so having a relatively easy way to create pipelines will be of great use.\n\nLet’s begin by declaring the class. We are going to add the class into the vk_engine.h header, alongside the VulkanEngine class.\n\nThe pipeline builder is a class with all the needed Vulkan structs stored inside (this is a basic set, there are more, but for now these are the ones we will need to fill). And a build_pipeline function that will finalize it and build it. If you want, you can put the builder in its own file (recommended vk_pipeline.h) but we aren’t doing it to keep number of files low.\n\nWe will now go to vk_initializers.h and start writing an initializer for each of those structs.\n\nWe are going to start with the initializer for This CreateInfo will hold information about a single shader stage for the pipeline. We build it from a shader stage and a shader module.\n\nWe are hardcoding the entry point to “main”. Remember from the last article that the entry point for the shaders was the function. This allows us to control it, but main() is fairly standard so let’s just keep it like that.\n\ncontains the information for vertex buffers and vertex formats. This is equivalent to the VAO configuration on opengl, but at the time we are not using it, so we will initialize it with an empty state. On the next tutorial chapter we will learn how to set this one up correctly.\n\ncontains the configuration for what kind of topology will be drawn. This is where you set it to draw triangles, lines, points, or others like triangle-list.\n\nOn the info we just have to set boilerplate plus what kind of topology we want. Example topologies:\n\n. Configuration for the fixed-function rasterization. In here is where we enable or disable backface culling, and set line width or wireframe drawing.\n\nWe are just going to leave polygonMode as editable input, to be able to toggle between wireframe and solid drawing.\n\ncullMode is used to cull backfaces or frontfaces, but in here we are going to leave it with no cull by default. We are also not using any depth bias here, so we are going to set all of that to 0.\n\nIf is enabled, primitives (triangles in our case) are discarded before even making it to the rasterization stage which means the triangles would never get drawn to the screen. You might enable this, for example, if you’re only interested in the side effects of the vertex processing stages, such as writing to a buffer which you later read from. But in our case we’re interested in drawing the triangle, so we leave it disabled.\n\nallows us to configure MSAA for this pipeline. We are not going to use MSAA on the entire tutorial, so we are going to default it to 1 sample and MSAA disabled. If you wanted to enable MSAA, you would need to set to more than 1, and enable sampleShading. Keep in mind that for MSAA to work, your renderpass also has to support it, which complicates things significantly.\n\nControls how this pipeline blends into a given attachment. We are rendering to only 1 attachment, so we will just need one of them, and defaulted to “not blend” and just override. In here it’s possible to make objects that will blend with the image. This one also doesn’t have sType + pNext\n\nNow that have all of the structs, we need to fill the build_pipeline function of the PipelineBuilder, that assembles all of the above into the final Info struct to create the pipeline.\n\nLet’s begin by connecting the viewport and scissor into ViewportState, and setting the ColorBlenderStateCreateInfo\n\ncontains the information about the attachments and how they are used. This one has to match the fragment shader outputs.\n\nTime to connect everything to the main VkGraphicsPipelineCreateInfo and create it\n\nAlongside of all the State structs, we will need a VkPipelineLayout object for our pipeline. Unlike the other state structs, this one is an actual full Vulkan object, and needs to be created separately from the pipeline.\n\nPipeline layouts contain the information about shader inputs of a given pipeline. It’s here where you would configure your push-constants and descriptor sets, but at the time we won’t need it, so we are going to create an empty pipeline layout for our Pipeline\n\nWe need yet another info struct, so let’s add it.\n\nWe are setting pSetLayouts and pPushConstantRanges both to null because our shader has no inputs, but we will soon add something to here.\n\nIt is needed that we store the pipeline layout somewhere, as there are a lot of Vulkan commands that need it, so let’s add a member to our VulkanEngine class for it\n\nNow we have to create it from our init_pipelines() function.\n\nIt’s now time to assemble everything together and build the pipeline for rendering the triangle.\n\nAdd _trianglePipeline as a new variable on the VulkanEngine class\n\nWe have finally created the pipeline we needed to draw the triangle, so we can finally do it.\n\nLet’s go to our main function, and execute the draw.\n\nWe need to add the draw commands between VkCmdBeginRenderPass and vkCmdEndRenderPass\n\nvkCmdBingPipeline sets the pipeline to be used on the further rendering commands, we bind the triangle pipeline here.\n\nvkCmdDraw executes a draw, in this case we are drawing 1 object with 3 vertices.\n\nRun it, and you should see a red triangle with a flashing blue background.\n\nCongratulations on your first triangle! We can now proceed to do interesting things with the shaders to make it more interesting."
    },
    {
        "link": "https://docs.vulkan.org/spec/latest/chapters/pipelines.html",
        "document": "When a pipeline is created, its state and shaders are compiled into zero or more device-specific executables, which are used when executing commands against that pipeline. To query the properties of these pipeline executables, call:\n• is the device that created the pipeline.\n• is a pointer to an integer related to the number of pipeline executables available or queried, as described below.\n• is either or a pointer to an array of VkPipelineExecutablePropertiesKHR structures. If is , then the number of pipeline executables associated with the pipeline is returned in . Otherwise, must point to a variable set by the application to the number of elements in the array, and on return the variable is overwritten with the number of structures actually written to . If is less than the number of pipeline executables associated with the pipeline, at most structures will be written, and will be returned instead of , to indicate that not all the available properties were returned.\n• VUID-vkGetPipelineExecutablePropertiesKHR-pipelineExecutableInfo-03270\n\n The feature must be enabled\n• VUID-vkGetPipelineExecutablePropertiesKHR-pipeline-03271\n\n The member of must have been created with\n• VUID-vkGetPipelineExecutablePropertiesKHR-pPipelineInfo-parameter\n\n must be a valid pointer to a valid VkPipelineInfoKHR structure\n• VUID-vkGetPipelineExecutablePropertiesKHR-pExecutableCount-parameter\n\n must be a valid pointer to a value\n• VUID-vkGetPipelineExecutablePropertiesKHR-pProperties-parameter\n\n If the value referenced by is not , and is not , must be a valid pointer to an array of VkPipelineExecutablePropertiesKHR structures The structure is defined as:\n• is a VkStructureType value identifying this structure.\n• is or a pointer to a structure extending this structure.\n• is a bitmask of zero or more VkShaderStageFlagBits indicating which shader stages (if any) were principally used as inputs to compile this pipeline executable.\n• is an array of containing a null-terminated UTF-8 string which is a short human readable name for this pipeline executable.\n• is an array of containing a null-terminated UTF-8 string which is a human readable description for this pipeline executable.\n• is the subgroup size with which this pipeline executable is dispatched. Not all implementations have a 1:1 mapping between shader stages and pipeline executables and some implementations may reduce a given shader stage to fixed function hardware programming such that no pipeline executable is available. No guarantees are provided about the mapping between shader stages and pipeline executables and should be considered a best effort hint. Because the application cannot rely on the field to provide an exact description, and provide a human readable name and description which more accurately describes the given pipeline executable.\n• is the logical device that created the pipeline.\n• is a pointer to a VkPipelineInfoEXT structure which describes the pipeline being queried.\n• is a pointer to a VkBaseOutStructure structure in which the pipeline properties will be written. To query a pipeline’s pass a VkPipelinePropertiesIdentifierEXT structure in . Each pipeline is associated with a and the identifier is implementation specific.\n• VUID-vkGetPipelinePropertiesEXT-pipeline-06738\n\n The member of must have been created with\n• VUID-vkGetPipelinePropertiesEXT-pPipelineProperties-06739\n\n must be a valid pointer to a VkPipelinePropertiesIdentifierEXT structure\n• VUID-vkGetPipelinePropertiesEXT-None-06766\n\n The feature must be enabled\n• VUID-vkGetPipelinePropertiesEXT-pPipelineInfo-parameter\n\n must be a valid pointer to a valid VkPipelineInfoEXT structure The structure is defined as:\n• is a VkStructureType value identifying this structure.\n• is or a pointer to a structure extending this structure.\n• is an array of values into which the pipeline identifier will be written. The structure is defined as:\n• is a VkStructureType value identifying this structure.\n• is or a pointer to a structure extending this structure. Each pipeline executable may have a set of statistics associated with it that are generated by the pipeline compilation process. These statistics may include things such as instruction counts, amount of spilling (if any), maximum number of simultaneous threads, or anything else which may aid developers in evaluating the expected performance of a shader. To query the compile time statistics associated with a pipeline executable, call:\n• is the device that created the pipeline.\n• is a pointer to an integer related to the number of statistics available or queried, as described below.\n• is either or a pointer to an array of VkPipelineExecutableStatisticKHR structures. If is , then the number of statistics associated with the pipeline executable is returned in . Otherwise, must point to a variable set by the application to the number of elements in the array, and on return the variable is overwritten with the number of structures actually written to . If is less than the number of statistics associated with the pipeline executable, at most structures will be written, and will be returned instead of , to indicate that not all the available statistics were returned.\n• VUID-vkGetPipelineExecutableStatisticsKHR-pipelineExecutableInfo-03272\n\n The feature must be enabled\n• VUID-vkGetPipelineExecutableStatisticsKHR-pipeline-03273\n\n The member of must have been created with\n• VUID-vkGetPipelineExecutableStatisticsKHR-pipeline-03274\n\n The member of must have been created with\n• VUID-vkGetPipelineExecutableStatisticsKHR-pExecutableInfo-parameter\n\n must be a valid pointer to a valid VkPipelineExecutableInfoKHR structure\n• VUID-vkGetPipelineExecutableStatisticsKHR-pStatisticCount-parameter\n\n must be a valid pointer to a value\n• VUID-vkGetPipelineExecutableStatisticsKHR-pStatistics-parameter\n\n If the value referenced by is not , and is not , must be a valid pointer to an array of VkPipelineExecutableStatisticKHR structures The structure is defined as:\n• is a VkStructureType value identifying this structure.\n• is or a pointer to a structure extending this structure.\n• is the pipeline to query.\n• is the index of the pipeline executable to query in the array of executable properties returned by vkGetPipelineExecutablePropertiesKHR.\n• VUID-VkPipelineExecutableInfoKHR-executableIndex-03275\n\n must be less than the number of pipeline executables associated with as returned in the parameter of The structure is defined as:\n• is a VkStructureType value identifying this structure.\n• is or a pointer to a structure extending this structure.\n• is an array of containing a null-terminated UTF-8 string which is a short human readable name for this statistic.\n• is an array of containing a null-terminated UTF-8 string which is a human readable description for this statistic.\n• is a VkPipelineExecutableStatisticFormatKHR value specifying the format of the data found in .\n• is the value of this statistic. The enum is defined as:\n• specifies that the statistic is returned as a 32-bit boolean value which must be either or and should be read from the field of .\n• specifies that the statistic is returned as a signed 64-bit integer and should be read from the field of .\n• specifies that the statistic is returned as an unsigned 64-bit integer and should be read from the field of .\n• specifies that the statistic is returned as a 64-bit floating-point value and should be read from the field of . The union is defined as:\n• is the 32-bit boolean value if the is .\n• is the signed 64-bit integer value if the is .\n• is the unsigned 64-bit integer value if the is .\n• is the 64-bit floating-point value if the is . Each pipeline executable may have one or more text or binary internal representations associated with it which are generated as part of the compile process. These may include the final shader assembly, a binary form of the compiled shader, or the shader compiler’s internal representation at any number of intermediate compile steps. To query the internal representations associated with a pipeline executable, call:\n• is the device that created the pipeline.\n• is a pointer to an integer related to the number of internal representations available or queried, as described below.\n• is either or a pointer to an array of VkPipelineExecutableInternalRepresentationKHR structures. If is , then the number of internal representations associated with the pipeline executable is returned in . Otherwise, must point to a variable set by the application to the number of elements in the array, and on return the variable is overwritten with the number of structures actually written to . If is less than the number of internal representations associated with the pipeline executable, at most structures will be written, and will be returned instead of , to indicate that not all the available representations were returned. While the details of the internal representations remain implementation-dependent, the implementation should order the internal representations in the order in which they occur in the compiled pipeline with the final shader assembly (if any) last.\n• VUID-vkGetPipelineExecutableInternalRepresentationsKHR-pipelineExecutableInfo-03276\n\n The feature must be enabled\n• VUID-vkGetPipelineExecutableInternalRepresentationsKHR-pipeline-03277\n\n The member of must have been created with\n• VUID-vkGetPipelineExecutableInternalRepresentationsKHR-pipeline-03278\n\n The member of must have been created with\n• VUID-vkGetPipelineExecutableInternalRepresentationsKHR-pExecutableInfo-parameter\n\n must be a valid pointer to a valid VkPipelineExecutableInfoKHR structure\n• VUID-vkGetPipelineExecutableInternalRepresentationsKHR-pInternalRepresentationCount-parameter\n\n must be a valid pointer to a value\n• VUID-vkGetPipelineExecutableInternalRepresentationsKHR-pInternalRepresentations-parameter\n\n If the value referenced by is not , and is not , must be a valid pointer to an array of VkPipelineExecutableInternalRepresentationKHR structures The structure is defined as:\n• is a VkStructureType value identifying this structure.\n• is or a pointer to a structure extending this structure.\n• is an array of containing a null-terminated UTF-8 string which is a short human readable name for this internal representation.\n• is an array of containing a null-terminated UTF-8 string which is a human readable description for this internal representation.\n• specifies whether the returned data is text or opaque data. If is then the data returned in is text and is guaranteed to be a null-terminated UTF-8 string.\n• is an integer related to the size, in bytes, of the internal representation’s data, as described below.\n• is either or a pointer to a block of data into which the implementation will write the internal representation. If is , then the size, in bytes, of the internal representation data is returned in . Otherwise, must be the size of the buffer, in bytes, pointed to by and on return is overwritten with the number of bytes of data actually written to including any trailing null character. If is less than the size, in bytes, of the internal representation’s data, at most bytes of data will be written to , and will be returned instead of , to indicate that not all the available representation was returned. If is and is not and is not zero, the last byte written to will be a null character. Information about a particular shader that has been compiled as part of a pipeline object can be extracted by calling:\n• is the device that created .\n• is the target of the query.\n• is a VkShaderStageFlagBits specifying the particular shader within the pipeline about which information is being queried.\n• describes what kind of information is being queried.\n• is a pointer to a value related to the amount of data the query returns, as described below.\n• is either or a pointer to a buffer. If is , then the maximum size of the information that can be retrieved about the shader, in bytes, is returned in . Otherwise, must point to a variable set by the application to the size of the buffer, in bytes, pointed to by , and on return the variable is overwritten with the amount of data actually written to . If is less than the maximum size that can be retrieved by the pipeline cache, then at most bytes will be written to , and will be returned, instead of , to indicate that not all required of the pipeline cache was returned. Not all information is available for every shader and implementations may not support all kinds of information for any shader. When a certain type of information is unavailable, the function returns . If information is successfully and fully queried, the function will return . For , a structure will be written to the buffer pointed to by . This structure will be populated with statistics regarding the physical device resources used by that shader along with other miscellaneous information and is described in further detail below. For , is a pointer to a null-terminated UTF-8 string containing human-readable disassembly. The exact formatting and contents of the disassembly string are vendor-specific. The formatting and contents of all other types of information, including , are left to the vendor and are not further specified by this extension. This query does not behave consistently with the behavior described in Opaque Binary Data Results, for historical reasons. If the amount of data available is larger than the passed , the query returns up to the size of the passed buffer, and signals overflow with a success status instead of returning a error status.\n• VUID-vkGetShaderInfoAMD-shaderStage-parameter\n\n must be a valid VkShaderStageFlagBits value\n• VUID-vkGetShaderInfoAMD-infoType-parameter\n\n must be a valid VkShaderInfoTypeAMD value\n• VUID-vkGetShaderInfoAMD-pInfoSize-parameter\n\n must be a valid pointer to a value\n• VUID-vkGetShaderInfoAMD-pInfo-parameter\n\n If the value referenced by is not , and is not , must be a valid pointer to an array of bytes\n• VUID-vkGetShaderInfoAMD-pipeline-parent\n\n must have been created, allocated, or retrieved from Possible values of vkGetShaderInfoAMD:: , specifying the information being queried from a shader, are:\n• specifies that device resources used by a shader will be queried.\n• specifies that implementation-specific information will be queried. The structure is defined as:\n• are the combination of logical shader stages contained within this shader.\n• is a VkShaderResourceUsageAMD structure describing internal physical device resources used by this shader.\n• is the maximum number of vector instruction general-purpose registers (VGPRs) available to the physical device.\n• is the maximum number of scalar instruction general-purpose registers (SGPRs) available to the physical device.\n• is the maximum limit of VGPRs made available to the shader compiler.\n• is the maximum limit of SGPRs made available to the shader compiler.\n• is the local workgroup size of this shader in { X, Y, Z } dimensions. Some implementations may merge multiple logical shader stages together in a single shader. In such cases, will contain a bitmask of all of the stages that are active within that shader. Consequently, if specifying those stages as input to vkGetShaderInfoAMD, the same output information may be returned for all such shader stage queries. The number of available VGPRs and SGPRs ( and respectively) are the shader-addressable subset of physical registers that is given as a limit to the compiler for register assignment. These values may further be limited by implementations due to performance optimizations where register pressure is a bottleneck. The structure is defined as:\n• is the number of vector instruction general-purpose registers used by this shader.\n• is the number of scalar instruction general-purpose registers used by this shader.\n• is the maximum local data store size per work group in bytes.\n• is the LDS usage size in bytes per work group by this shader.\n• is the scratch memory usage in bytes by this shader."
    },
    {
        "link": "https://vulkan-tutorial.com/Drawing_a_triangle/Graphics_pipeline_basics/Introduction",
        "document": "Over the course of the next few chapters we'll be setting up a graphics pipeline that is configured to draw our first triangle. The graphics pipeline is the sequence of operations that take the vertices and textures of your meshes all the way to the pixels in the render targets. A simplified overview is displayed below:\n\nThe input assembler collects the raw vertex data from the buffers you specify and may also use an index buffer to repeat certain elements without having to duplicate the vertex data itself.\n\nThe vertex shader is run for every vertex and generally applies transformations to turn vertex positions from model space to screen space. It also passes per-vertex data down the pipeline.\n\nThe tessellation shaders allow you to subdivide geometry based on certain rules to increase the mesh quality. This is often used to make surfaces like brick walls and staircases look less flat when they are nearby.\n\nThe geometry shader is run on every primitive (triangle, line, point) and can discard it or output more primitives than came in. This is similar to the tessellation shader, but much more flexible. However, it is not used much in today's applications because the performance is not that good on most graphics cards except for Intel's integrated GPUs.\n\nThe rasterization stage discretizes the primitives into fragments. These are the pixel elements that they fill on the framebuffer. Any fragments that fall outside the screen are discarded and the attributes outputted by the vertex shader are interpolated across the fragments, as shown in the figure. Usually the fragments that are behind other primitive fragments are also discarded here because of depth testing.\n\nThe fragment shader is invoked for every fragment that survives and determines which framebuffer(s) the fragments are written to and with which color and depth values. It can do this using the interpolated data from the vertex shader, which can include things like texture coordinates and normals for lighting.\n\nThe color blending stage applies operations to mix different fragments that map to the same pixel in the framebuffer. Fragments can simply overwrite each other, add up or be mixed based upon transparency.\n\nStages with a green color are known as fixed-function stages. These stages allow you to tweak their operations using parameters, but the way they work is predefined.\n\nStages with an orange color on the other hand are , which means that you can upload your own code to the graphics card to apply exactly the operations you want. This allows you to use fragment shaders, for example, to implement anything from texturing and lighting to ray tracers. These programs run on many GPU cores simultaneously to process many objects, like vertices and fragments in parallel.\n\nIf you've used older APIs like OpenGL and Direct3D before, then you'll be used to being able to change any pipeline settings at will with calls like and . The graphics pipeline in Vulkan is almost completely immutable, so you must recreate the pipeline from scratch if you want to change shaders, bind different framebuffers or change the blend function. The disadvantage is that you'll have to create a number of pipelines that represent all of the different combinations of states you want to use in your rendering operations. However, because all of the operations you'll be doing in the pipeline are known in advance, the driver can optimize for it much better.\n\nSome of the programmable stages are optional based on what you intend to do. For example, the tessellation and geometry stages can be disabled if you are just drawing simple geometry. If you are only interested in depth values then you can disable the fragment shader stage, which is useful for shadow map generation.\n\nIn the next chapter we'll first create the two programmable stages required to put a triangle onto the screen: the vertex shader and fragment shader. The fixed-function configuration like blending mode, viewport, rasterization will be set up in the chapter after that. The final part of setting up the graphics pipeline in Vulkan involves the specification of input and output framebuffers.\n\nCreate a function that is called right after in . We'll work on this function throughout the following chapters."
    },
    {
        "link": "https://vkguide.dev/docs/new_chapter_3/building_pipeline",
        "document": "Building a graphics pipeline is a far more involved task than building a compute pipeline. With the compute pipeline, we only needed a single shader module and pipeline layout, so there was no need of an abstraction layer. But graphics pipelines contain a considerable amount of options, and without a way to simplify it, creating them can be considerably complicated.\n\nFor that reason, we will be creating a PipelineBuilder structure, that keeps track of all those options, and will offer some simpler functions to enable/disable features we want, keeping as much defaulted as possible. A lot of those options are things that we wont be using on the tutorial, so trying to reduce the area will be useful.\n\nSome of the options on a pipeline can be set to be dynamic, which means we will set those options when binding the pipeline and recording draw commands. For example we will put viewport as dynamic, as if we had it “baked in”, we would need to create new pipelines if we wanted to change the resolution of our rendering.\n\nBefore writing the builder, lets look at what we will need to fill. In the same way creating a compute pipeline required a , a graphics one is a structure.\n\nSpec page for graphics pipeline can be found here, which can be used to check things in detail.\n\nand contains the ShaderStageCreateInfo that will contain the shader modules for the different stages on the pipeline. We will be sending here our fragment shader and vertex shader.\n\ncontains the configuration for vertex attribute input with vertex buffers. If we configure this correctly, our vertex shader will get vertex properties as input in an optimal way. But we will not be using this, as we are just going to send a data array to the shader and index it ourselves, which allows techniques that improve performance and allows more complicated vertex formats that compress data. This is generally known as “vertex pulling”, and even if you are doing equivalent thing as the fixed-hardware vertex input, on modern gpus it will perform about the same.\n\ncontains the configuration for triangle topology. We use this to set the pipeline to draw triangles, points, or lines.\n\nis configuration for fixed tesellation. We will not be using this and will leave it as null.\n\ncontains information about the viewport the pixels will be rendered into. This lets you set what region of pixels will the pipeline draw. We will default it, because we will be using dynamic state for this.\n\nhas the information on how exactly do the triangles get rasterized between the vertex shader and the fragment shader. It has options for depth bias (used when rendering shadows), toggling between wireframe and solid rendering, and the configuration for drawing or skipping backfaces.\n\nlets us configure Multi Sample antialiasing. Thats a way of improving the antialiasing of our rendering by rasterizing the fragments more times at triangle edges. We will default it to no antialiasing, but we will look into using it later.\n\ncontains the depth-testing and stencil configuration.\n\nhas the color blending and attachment write information. Its used to make triangles transparent or other blending configurations.\n\nconfigures dynamic state. One great downside that vulkan pipelines have is that their configuration is “hardcoded” at creation. So if we want to do things like toggle depth-testing on and off, we will need 2 pipelines. It even hardcodes viewport, so if we want to change the size of our render targets, we will also need to rebuild all pipelines. Building pipelines is a very expensive operation, and we want to minimize the number of pipelines used as its critical for performance. For that reason, some of the state of a vulkan pipeline can be set as dynamic, and then the configuration option can be modified at runtime when recording commands. What dynamic state is supported by a given gpu depends on gpu vendor, driver version, and other variables. We will be using dynamic state for our viewport and scissor configuration, as almost all GPUs support that one, and it removes the need to hardcode the draw image resolution when building the pipelines.\n\nThe VkGraphicsPipelineCreateInfo takes a VkPipelineLayout that is the same one used when building compute pipelines.\n\nIt also takes a VkRenderPass and subpass index. We will not be using that because we use dynamic rendering, so all systems related to VkRenderPass will be completely skipped. Instead, we need to extend the VkGraphicsPipelineCreateInfo with a added into its pNext chain. This structure holds a list of the attachment formats the pipeline will use.\n\nLets begin writing the builder. All pipeline code will be on vk_pipelines.h/cpp. You can find it on the shared folder if you are checking the chapter code.\n\nThe pipeline builder will hold most of the state we need to track of. and an array of color attachment formats and shader stages. The actual CreateInfo structure will be fully filled from the build_pipeline() function. We have a clear() function that will set everything into empty/default properties. The constructor for the pipeline builder will call it, but its useful to have the clear function so we can call it manually when wanted.\n\nWe will set the .sType of every structure here, and leave everything else as 0. This is using cpp20 initializers, so the parameters we dont write from within the brackets will be defaulted to 0. Most of the Info structures in vulkan are designed so that 0 is valid clear/default option, so this works great here.\n\nLets begin writing the build_pipeline function. first we will begin by setting some of the Info structures we are missing because they wont be configured.\n\nWe first fill with just viewport count and nothing else. With dynamic viewport state we dont need to fill the viewport or stencil options here.\n\nThen we fill with some default options for logic blending (we wont use it), and hook the for the blending options for a single attachment. We only support rendering to one attachment here, so this is fine. It can be made into an array of if drawing to multiple attachments is needed.\n\nLets continue with the function, and begin filling the VkGraphicsPipelineCreateInfo\n\nWe connect all of the configuration structures we have on the builder, and add _renderInfo into the pNext of the graphics pipeline info itself.\n\nnext is setting up dynamic state\n\nSetting up dynamic state is just filling a with an array of VkDynamicState enums. We will use these 2 for now.\n\nThis is all we needed for the pipeline, so we can finally call the create function.\n\nAnd thats it with the main creation function. We now need to actually set the options properly, as right now the entire pipeline is essentially null, which will error as-is due to missing options.\n\nWe begin by adding a function to set the vertex and fragment shaders. We add them into the _shaderStages array with the proper info creation, which we already had from building the compute pipeline.\n\nNext we add a function to set input topology\n\nVkPrimitiveTopology has the options for VK_PRIMITIVE_TOPOLOGY_TRIANGLE_LIST, VK_PRIMITIVE_TOPOLOGY_POINT_LIST, and so on. PrimitiveRestart is used for triangle strips and line strips, but we dont use it.\n\nThe rasterizer state is a big one so we will split it on a few options.\n\nWe need to have lineWidth as 1.f as default, then we set the polygon mode, which controls wireframe vs solid rendering and point rendering.\n\nCull mode will set the front face and the cull mode for backface culling.\n\nNext is setting the multisample state. We will default the structure to multisampling disabled. Later we can add other functions for enabling different multisampling levels for antialiasing\n\nNext we will add a function for blending mode\n\nWe will have our disable_blending() function that sets blendEnable to false but sets the correct write mask. We will add functions for more blending modes later. We need to setup a proper colorWriteMask here so that our pixel output will write to the attachment correctly.\n\nNow we hook our formats, lets add the functions for both depth testing and color attachment.\n\nOn the color attachment, the pipeline needs it by pointer because it wants an array of color attachments. This is useful for things like deferred rendering where you draw to multiple images at once, but we dont need this yet so we can default it to just 1 color format.\n\nThe last one we need is a function to disable the depth testing logic.\n\nWith all the basic features for the pipeline builder filled, we can now draw a triangle. For our triangle, we are going to use hardcoded vertex positions in the vertex shader, and the output will be a pure color.\n\nThese are the shaders:\n\nIn our vertex shader, we have a hardcoded array of positions, and we index into it from . This works in a similar way to LocalThreadID on compute shaders worked. For every invocation of the vertex shader, this will be a different index, and we can use it to process out vertex, which will write into the fixed function gl_Position variable. As the array is only of lenght 3, if we tried to render more than 3 vertices (1 triangle) this will error.\n\nIn our fragment shader, we will declare an output at layout = 0 (this connects to the render attachments of the render pass), and we have a simple hardcoded red output.\n\nLets now create the pipeline and layout we need to draw this triangle. We are adding new shader files, so make sure you rebuild the CMake project and build the Shaders target.\n\nOn VulkanEngine class, we will add a function, and a couple of members to hold the pipeline and its layout\n\nWe will call this from function.\n\nLets write that function We will start by loading the 2 shaders into VkShaderModules, like we did with the compute shader, but this time more shaders.\n\nWe also create the pipeline layout. Unlike with the compute shader before, this time we have no push constants and no descriptor bindings on here, so its really just a completely empty layout.\n\nNow we create the pipeline, using the Pipeline Builder created before.\n\nWith the pipeline built, we can draw our triangle as part of the command buffer we create every frame.\n\nThe compute shader we run for the background needed to draw into GENERAL image layout, but when doing geometry rendering, we need to use COLOR_ATTACHMENT_OPTIMAL. It is possible to draw into GENERAL layout with graphics pipelines, but its lower performance and the validation layers will complain. We will create a new function, , to hold these graphics commands. Lets update the draw loop first.\n\nTo draw our triangle we need to begin a renderpass with cmdBeginRendering. This is the same we were doing for imgui last chapter, but this time we are pointing it into our _drawImage instead of the swapchain image.\n\nWe do a CmdBindPipeline, but instead of using the BIND_POINT_COMPUTE, we now use . Then, we have to set our viewport and scissor. This is required before we left them undefined when creating the pipeline as we were using dynamic pipeline state. With that set, we can do a vkCmdDraw() to draw the triangle. With that done, we can finish the render pass to end our drawing.\n\nIf you run the program at this point, you should see a triangle being rendered on top of the compute based background"
    },
    {
        "link": "https://docs.vulkan.org/tutorial/latest/03_Drawing_a_triangle/02_Graphics_pipeline_basics/00_Introduction.html",
        "document": "Over the course of the next few chapters we’ll be setting up a graphics pipeline that is configured to draw our first triangle. The graphics pipeline is the sequence of operations that take the vertices and textures of your meshes all the way to the pixels in the render targets. A simplified overview is displayed below:\n\nThe tessellation shaders allow you to subdivide geometry based on certain rules to increase the mesh quality. This is often used to make surfaces like brick walls and staircases look less flat when they are nearby.\n\nThe geometry shader is run on every primitive (triangle, line, point) and can discard it or output more primitives than came in. This is similar to the tessellation shader, but much more flexible. However, it is not used much in today’s applications because the performance is not that good on most graphics cards except for Intel’s integrated GPUs.\n\nThe rasterization stage discretizes the primitives into fragments. These are the pixel elements that they fill on the framebuffer. Any fragments that fall outside the screen are discarded and the attributes outputted by the vertex shader are interpolated across the fragments, as shown in the figure. Usually the fragments that are behind other primitive fragments are also discarded here because of depth testing.\n\nThe fragment shader is invoked for every fragment that survives and determines which framebuffer(s) the fragments are written to and with which color and depth values. It can do this using the interpolated data from the vertex shader, which can include things like texture coordinates and normals for lighting.\n\nStages with an orange color on the other hand are , which means that you can upload your own code to the graphics card to apply exactly the operations you want. This allows you to use fragment shaders, for example, to implement anything from texturing and lighting to ray tracers. These programs run on many GPU cores simultaneously to process many objects, like vertices and fragments in parallel.\n\nIf you’ve used older APIs like OpenGL and Direct3D before, then you’ll be used to being able to change any pipeline settings at will with calls like and . The graphics pipeline in Vulkan is almost completely immutable, so you must recreate the pipeline from scratch if you want to change shaders, bind different framebuffers or change the blend function. The disadvantage is that you’ll have to create a number of pipelines that represent all of the different combinations of states you want to use in your rendering operations. However, because all of the operations you’ll be doing in the pipeline are known in advance, the driver can optimize for it much better.\n\nSome of the programmable stages are optional based on what you intend to do. For example, the tessellation and geometry stages can be disabled if you are just drawing simple geometry. If you are only interested in depth values then you can disable the fragment shader stage, which is useful for shadow map generation.\n\nIn the next chapter we’ll first create the two programmable stages required to put a triangle onto the screen: the vertex shader and fragment shader. The fixed-function configuration like blending mode, viewport, rasterization will be set up in the chapter after that. The final part of setting up the graphics pipeline in Vulkan involves the specification of input and output framebuffers."
    },
    {
        "link": "https://glfw.org/docs/3.3/vulkan_guide.html",
        "document": "This guide is intended to fill the gaps between the official Vulkan resources and the rest of the GLFW documentation and is not a replacement for either. It assumes some familiarity with Vulkan concepts like loaders, devices, queues and surfaces and leaves it to the Vulkan documentation to explain the details of Vulkan functions.\n\nTo develop for Vulkan you should download the LunarG Vulkan SDK for your platform. Apart from headers and link libraries, they also provide the validation layers necessary for development.\n\nThe Vulkan Tutorial has more information on how to use GLFW and Vulkan. The Khronos Vulkan Samples also use GLFW, although with a small framework in between.\n\nFor details on a specific Vulkan support function, see the Vulkan support reference. There are also guides for the other areas of the GLFW API.\n\nBy default, GLFW will look for the Vulkan loader on demand at runtime via its standard name ( on Windows, on Linux and other Unix-like systems and on macOS). This means that GLFW does not need to be linked against the loader. However, it also means that if you are using the static library form of the Vulkan loader GLFW will either fail to find it or (worse) use the wrong one.\n\nThe GLFW_VULKAN_STATIC CMake option makes GLFW call the Vulkan loader directly instead of dynamically loading it at runtime. Not linking against the Vulkan loader will then be a compile-time error.\n\nmacOS: To make your application be redistributable you will need to set up the application bundle according to the LunarG SDK documentation. This is explained in more detail in the SDK documentation for macOS.\n\nTo include the Vulkan header, define GLFW_INCLUDE_VULKAN before including the GLFW header.\n\nIf you instead want to include the Vulkan header from a custom location or use your own custom Vulkan header then do this before the GLFW header.\n\nUnless a Vulkan header is included, either by the GLFW header or above it, any GLFW functions that take or return Vulkan types will not be declared.\n\nThe macros do not need to be defined for the Vulkan part of GLFW to work. Define them only if you are using these extensions directly.\n\nIf you are linking directly against the Vulkan loader then you can skip this section. The canonical desktop loader library exports all Vulkan core and Khronos extension functions, allowing them to be called directly.\n\nIf you are loading the Vulkan loader dynamically instead of linking directly against it, you can check for the availability of a loader and ICD with glfwVulkanSupported.\n\nThis function returns if the Vulkan loader and any minimally functional ICD was found.\n\nIf one or both were not found, calling any other Vulkan related GLFW function will generate a GLFW_API_UNAVAILABLE error.\n\nTo load any Vulkan core or extension function from the found loader, call glfwGetInstanceProcAddress. To load functions needed for instance creation, pass as the instance.\n\nOnce you have created an instance, you can load from it all other Vulkan core functions and functions from any instance extensions you enabled.\n\nThis function in turn calls . If that fails, the function falls back to a platform-specific query of the Vulkan loader (i.e. or ). If that also fails, the function returns . For more information about , see the Vulkan documentation.\n\nVulkan also provides for loading device-specific versions of Vulkan function. This function can be retrieved from an instance with glfwGetInstanceProcAddress.\n\nDevice-specific functions may execute a little faster, due to not having to dispatch internally based on the device passed to them. For more information about , see the Vulkan documentation.\n\nTo do anything useful with Vulkan you need to create an instance. If you want to use Vulkan to render to a window, you must enable the instance extensions GLFW requires to create Vulkan surfaces.\n\nThese extensions must all be enabled when creating instances that are going to be passed to glfwGetPhysicalDevicePresentationSupport and glfwCreateWindowSurface. The set of extensions will vary depending on platform and may also vary depending on graphics drivers and other factors.\n\nIf it fails it will return and GLFW will not be able to create Vulkan window surfaces. You can still use Vulkan for off-screen rendering and compute work.\n\nIf successful the returned array will always include , so if you don't require any additional extensions you can pass this list directly to the struct.\n\nAdditional extensions may be required by future versions of GLFW. You should check whether any extensions you wish to enable are already in the returned array, as it is an error to specify an extension more than once in the struct.\n\nmacOS: MoltenVK is (as of July 2022) not yet a fully conformant implementation of Vulkan. As of Vulkan SDK 1.3.216.0, this means you must also enable the instance extension and set the bit in the instance creation info flags for MoltenVK to show up in the list of physical devices. For more information, see the Vulkan and MoltenVK documentation.\n\nNot every queue family of every Vulkan device can present images to surfaces. To check whether a specific queue family of a physical device supports image presentation without first having to create a window and surface, call glfwGetPhysicalDevicePresentationSupport.\n\nThe extension additionally provides the function, which performs the same test on an existing Vulkan surface.\n\nUnless you will be using OpenGL or OpenGL ES with the same window as Vulkan, there is no need to create a context. You can disable context creation with the GLFW_CLIENT_API hint.\n\nSee Windows without contexts for more information.\n\nYou can create a Vulkan surface (as defined by the extension) for a GLFW window with glfwCreateWindowSurface.\n\nIf an OpenGL or OpenGL ES context was created on the window, the context has ownership of the presentation on the window and a Vulkan surface cannot be created.\n\nIt is your responsibility to destroy the surface. GLFW does not destroy it for you. Call function from the same extension to destroy it."
    },
    {
        "link": "https://stackoverflow.com/questions/59551859/about-vulkan-and-using-glfw-to-abstract-away-surface-creation",
        "document": "I'm trying learn Vulkan API by writing a simple test renderer using Vulkan + GLFW in C++\n\nSo far I have\n\nand right now I am trying to create a window. So far I have two pieces of code that look very similar\n\nAnd I am trying to define a function that'd return a object.\n\nGoing through some of the tutorials online, I see most of them define a object to pass to function.\n\nThat looks like it is platform dependent and it would only work on stuff.\n\nWhich is fine in my case but I want to let deal with all that so I could make my application a bit more crossplatform.\n\nThe problem is I don't know how I would pass to .\n\nI also may be confusing Vulkan surface for something else because of this piece of code of vulkan-tutorial.com\n\nI'm not too familiar with Windows but it looks like it's using platform dependent info structure and ."
    },
    {
        "link": "https://glfw.org/docs/latest/vulkan_guide.html",
        "document": "This guide is intended to fill the gaps between the official Vulkan resources and the rest of the GLFW documentation and is not a replacement for either. It assumes some familiarity with Vulkan concepts like loaders, devices, queues and surfaces and leaves it to the Vulkan documentation to explain the details of Vulkan functions.\n\nTo develop for Vulkan you should download the LunarG Vulkan SDK for your platform. Apart from headers and link libraries, they also provide the validation layers necessary for development.\n\nThe Vulkan Tutorial has more information on how to use GLFW and Vulkan. The Khronos Vulkan Samples also use GLFW, although with a small framework in between.\n\nFor details on a specific Vulkan support function, see the Vulkan support reference. There are also guides for the other areas of the GLFW API.\n\nGLFW itself does not ever need to be linked against the Vulkan loader.\n\nBy default, GLFW will load the Vulkan loader dynamically at runtime via its standard name: on Windows, on Linux and other Unix-like systems and on macOS.\n\nmacOS: GLFW will also look up and search the subdirectory of your application bundle.\n\nIf your code is using a Vulkan loader with a different name or in a non-standard location you will need to direct GLFW to it. Pass your version of to glfwInitVulkanLoader before initializing GLFW and it will use that function for all Vulkan entry point retrieval. This prevents GLFW from dynamically loading the Vulkan loader.\n\nmacOS: To make your application be redistributable you will need to set up the application bundle according to the LunarG SDK documentation. This is explained in more detail in the SDK documentation for macOS.\n\nTo have GLFW include the Vulkan header, define GLFW_INCLUDE_VULKAN before including the GLFW header.\n\nIf you instead want to include the Vulkan header from a custom location or use your own custom Vulkan header then do this before the GLFW header.\n\nUnless a Vulkan header is included, either by the GLFW header or above it, the following GLFW functions will not be declared, as depend on Vulkan types.\n\nThe macros do not need to be defined for the Vulkan part of GLFW to work. Define them only if you are using these extensions directly.\n\nIf you are linking directly against the Vulkan loader then you can skip this section. The canonical desktop loader library exports all Vulkan core and Khronos extension functions, allowing them to be called directly.\n\nIf you are loading the Vulkan loader dynamically instead of linking directly against it, you can check for the availability of a loader and ICD with glfwVulkanSupported.\n\nThis function returns if the Vulkan loader and any minimally functional ICD was found.\n\nIf one or both were not found, calling any other Vulkan related GLFW function will generate a GLFW_API_UNAVAILABLE error.\n\nTo load any Vulkan core or extension function from the found loader, call glfwGetInstanceProcAddress. To load functions needed for instance creation, pass as the instance.\n\nOnce you have created an instance, you can load from it all other Vulkan core functions and functions from any instance extensions you enabled.\n\nThis function in turn calls . If that fails, the function falls back to a platform-specific query of the Vulkan loader (i.e. or ). If that also fails, the function returns . For more information about , see the Vulkan documentation.\n\nVulkan also provides for loading device-specific versions of Vulkan function. This function can be retrieved from an instance with glfwGetInstanceProcAddress.\n\nDevice-specific functions may execute a little faster, due to not having to dispatch internally based on the device passed to them. For more information about , see the Vulkan documentation.\n\nTo do anything useful with Vulkan you need to create an instance. If you want to use Vulkan to render to a window, you must enable the instance extensions GLFW requires to create Vulkan surfaces.\n\nThese extensions must all be enabled when creating instances that are going to be passed to glfwGetPhysicalDevicePresentationSupport and glfwCreateWindowSurface. The set of extensions will vary depending on platform and may also vary depending on graphics drivers and other factors.\n\nIf it fails it will return and GLFW will not be able to create Vulkan window surfaces. You can still use Vulkan for off-screen rendering and compute work.\n\nIf successful the returned array will always include , so if you don't require any additional extensions you can pass this list directly to the struct.\n\nAdditional extensions may be required by future versions of GLFW. You should check whether any extensions you wish to enable are already in the returned array, as it is an error to specify an extension more than once in the struct.\n\nmacOS: MoltenVK is (as of July 2022) not yet a fully conformant implementation of Vulkan. As of Vulkan SDK 1.3.216.0, this means you must also enable the instance extension and set the bit in the instance creation info flags for MoltenVK to show up in the list of physical devices. For more information, see the Vulkan and MoltenVK documentation.\n\nNot every queue family of every Vulkan device can present images to surfaces. To check whether a specific queue family of a physical device supports image presentation without first having to create a window and surface, call glfwGetPhysicalDevicePresentationSupport.\n\nThe extension additionally provides the function, which performs the same test on an existing Vulkan surface.\n\nUnless you will be using OpenGL or OpenGL ES with the same window as Vulkan, there is no need to create a context. You can disable context creation with the GLFW_CLIENT_API hint.\n\nSee Windows without contexts for more information.\n\nYou can create a Vulkan surface (as defined by the extension) for a GLFW window with glfwCreateWindowSurface.\n\nIf an OpenGL or OpenGL ES context was created on the window, the context has ownership of the presentation on the window and a Vulkan surface cannot be created.\n\nIt is your responsibility to destroy the surface. GLFW does not destroy it for you. Call function from the same extension to destroy it."
    },
    {
        "link": "https://vulkan-tutorial.com/Drawing_a_triangle/Presentation/Window_surface",
        "document": "Since Vulkan is a platform agnostic API, it can not interface directly with the window system on its own. To establish the connection between Vulkan and the window system to present results to the screen, we need to use the WSI (Window System Integration) extensions. In this chapter we'll discuss the first one, which is . It exposes a object that represents an abstract type of surface to present rendered images to. The surface in our program will be backed by the window that we've already opened with GLFW.\n\nThe extension is an instance level extension and we've actually already enabled it, because it's included in the list returned by . The list also includes some other WSI extensions that we'll use in the next couple of chapters.\n\nThe window surface needs to be created right after the instance creation, because it can actually influence the physical device selection. The reason we postponed this is because window surfaces are part of the larger topic of render targets and presentation for which the explanation would have cluttered the basic setup. It should also be noted that window surfaces are an entirely optional component in Vulkan, if you just need off-screen rendering. Vulkan allows you to do that without hacks like creating an invisible window (necessary for OpenGL).\n\nStart by adding a class member right below the debug callback.\n\nAlthough the object and its usage is platform agnostic, its creation isn't because it depends on window system details. For example, it needs the and handles on Windows. Therefore there is a platform-specific addition to the extension, which on Windows is called and is also automatically included in the list from .\n\nI will demonstrate how this platform specific extension can be used to create a surface on Windows, but we won't actually use it in this tutorial. It doesn't make any sense to use a library like GLFW and then proceed to use platform-specific code anyway. GLFW actually has that handles the platform differences for us. Still, it's good to see what it does behind the scenes before we start relying on it.\n\nTo access native platform functions, you need to update the includes at the top:\n\nBecause a window surface is a Vulkan object, it comes with a struct that needs to be filled in. It has two important parameters: and . These are the handles to the window and the process.\n\nThe function is used to get the raw from the GLFW window object. The call returns the handle of the current process.\n\nAfter that the surface can be created with , which includes a parameter for the instance, surface creation details, custom allocators and the variable for the surface handle to be stored in. Technically this is a WSI extension function, but it is so commonly used that the standard Vulkan loader includes it, so unlike other extensions you don't need to explicitly load it.\n\nThe process is similar for other platforms like Linux, where takes an XCB connection and window as creation details with X11.\n\nThe function performs exactly this operation with a different implementation for each platform. We'll now integrate it into our program. Add a function to be called from right after instance creation and .\n\nThe GLFW call takes simple parameters instead of a struct which makes the implementation of the function very straightforward:\n\nThe parameters are the , GLFW window pointer, custom allocators and pointer to variable. It simply passes through the from the relevant platform call. GLFW doesn't offer a special function for destroying a surface, but that can easily be done through the original API:\n\nMake sure that the surface is destroyed before the instance.\n\nAlthough the Vulkan implementation may support window system integration, that does not mean that every device in the system supports it. Therefore we need to extend to ensure that a device can present images to the surface we created. Since the presentation is a queue-specific feature, the problem is actually about finding a queue family that supports presenting to the surface we created.\n\nIt's actually possible that the queue families supporting drawing commands and the ones supporting presentation do not overlap. Therefore we have to take into account that there could be a distinct presentation queue by modifying the structure:\n\nNext, we'll modify the function to look for a queue family that has the capability of presenting to our window surface. The function to check for that is , which takes the physical device, queue family index and surface as parameters. Add a call to it in the same loop as the :\n\nThen simply check the value of the boolean and store the presentation family queue index:\n\nNote that it's very likely that these end up being the same queue family after all, but throughout the program we will treat them as if they were separate queues for a uniform approach. Nevertheless, you could add logic to explicitly prefer a physical device that supports drawing and presentation in the same queue for improved performance.\n\nThe one thing that remains is modifying the logical device creation procedure to create the presentation queue and retrieve the handle. Add a member variable for the handle:\n\nNext, we need to have multiple structs to create a queue from both families. An elegant way to do that is to create a set of all unique queue families that are necessary for the required queues:\n\nAnd modify to point to the vector:\n\nIf the queue families are the same, then we only need to pass its index once. Finally, add a call to retrieve the queue handle:\n\nIn case the queue families are the same, the two handles will most likely have the same value now. In the next chapter we're going to look at swap chains and how they give us the ability to present images to the surface."
    },
    {
        "link": "https://reddit.com/r/vulkan/comments/46g7dq/creating_a_glfw_window_with_a_vulkan_contextdevice",
        "document": "Hello, I am trying to create a window with glfw and vulkan but I can't get it to work. Could some one have a look at my code and point me in the right direction?"
    },
    {
        "link": "https://glm.g-truc.net/0.9.8/index.html",
        "document": "OpenGL Mathematics (GLM) is a header only C++ mathematics library for graphics software based on the OpenGL Shading Language (GLSL) specifications. GLM provides classes and functions designed and implemented with the same naming conventions and functionalities than GLSL so that anyone who knows GLSL, can use GLM as well in C++. This project isn't limited to GLSL features. An extension system, based on the GLSL extension conventions, provides extended capabilities: matrix transformations, quaternions, data packing, random numbers, noise, etc... This library works perfectly with OpenGL but it also ensures interoperability with other third party libraries and SDK. It is a good candidate for software rendering (raytracing / rasterisation), image processing, physic simulations and any development context that requires a simple and convenient mathematics library.\n• GLM is written in C++98 but can take advantage of C++11 when supported by the compiler. It is a platform independent library with no dependence and it officially supports the following compilers: For more information about GLM, please have a look at the manual and the API reference documentation. The source code and the documentation, including this manual, are licensed under the Happy Bunny License (Modified MIT) or the MIT License. Thanks for contributing to the project by submitting issues for bug reports and feature requests. Any feedback is welcome at glm@g-truc.net."
    },
    {
        "link": "https://github.com/g-truc/glm",
        "document": "OpenGL Mathematics (GLM) is a header only C++ mathematics library for graphics software based on the OpenGL Shading Language (GLSL) specifications.\n\nGLM provides classes and functions designed and implemented with the same naming conventions and functionality than GLSL so that anyone who knows GLSL, can use GLM as well in C++.\n\nThis project isn't limited to GLSL features. An extension system, based on the GLSL extension conventions, provides extended capabilities: matrix transformations, quaternions, data packing, random numbers, noise, etc...\n\nThis library works perfectly with OpenGL but it also ensures interoperability with other third party libraries and SDK. It is a good candidate for software rendering (raytracing / rasterisation), image processing, physics simulations and any development context that requires a simple and convenient mathematics library.\n\nGLM is written in C++98 but can take advantage of C++11 when supported by the compiler. It is a platform independent library with no dependence and it officially supports the following compilers:\n\nFor more information about GLM, please have a look at the manual and the API reference documentation. The source code and the documentation are licensed under either the Happy Bunny License (Modified MIT) or the MIT License.\n\nThanks for contributing to the project by submitting pull requests.\n\nAnd then in your :\n\nIf your prefer to use header-only version of GLM\n\nYou can add glm to your CMake project to be built together.\n• Unit tests are not build by default, set to required.\n• Enables only warnings as errors while building unit tests\n• Added and to GLM_EXT_scalar_common and GLM_EXT_vector_common\n• Added GLM_FORCE_UNRESTRICTED_FLOAT to prevent static asserts when using other scalar types with function expecting floats.\n• Fixed discards the sign of result for angles in range (2pi-1, 2pi) #1038\n• Removed ban on using with CUDA host code #1041\n• Added , , and function to and extensions with tests\n• Added to store quat data as w,x,y,z instead of x,y,z,w #983\n• Added GLM_EXT_scalar_integer extension with power of two and multiple scalar functions\n• Added GLM_EXT_vector_integer extension with power of two and multiple vector functions\n• Fixed for g++6 where -std=c++1z sets __cplusplus to 201500 instead of 201402 #921\n• Added GLM_FORCE_INTRINSICS to enable SIMD instruction code path. By default, it's disabled allowing constexpr support by default. #865\n• Fixed being defined as unsigned char with some compiler #839\n• Added and overload with max ULPs parameters for scalar numbers #121\n• Added to silent GLM warnings when using language extensions but using W4 or Wpedantic warnings #814 #775\n• Added to enable aligned types and SIMD instruction are not enabled. This disable #816\n• Fixed default initialization with vector and quaternion types using #812\n• Added missing and with epsilon for quaternion types to GLM_GTC_quaternion\n• Added GLM_EXT_matrix_relational: and with epsilon for matrix types\n• Added a section to the manual for contributing to GLM\n• Redesigned constexpr support which excludes both SIMD and #783\n• Clarified refract valid range of the indices of refraction, between -1 and 1 inclusively #806\n• Fixed invalid conversion from int scalar with vec4 constructor when using SSE instruction\n• Fixed infinite loop in random functions when using negative radius values using an assert #739\n• Added GLM_GTX_matrix_factorisation to factor matrices in various forms #654\n• Added GLM_EXT_vector_relational: extend and to take an epsilon argument\n• Added separate functions to use both negative one and zero near clip plans #680\n• Added to use GLM on platforms that don't support double #627\n• No more default initialization of vector, matrix and quaternion types\n• Added error for including of different versions of GLM #619\n• Added GLM_FORCE_IGNORE_VERSION to ignore error caused by including different version of GLM #619\n• Reduced warnings when using very strict compilation flags #646\n• Removed doxygen references to GLM_GTC_half_float which was removed in 0.9.4\n• Fixed references to which was removed #642\n• Fixed when OpenMP is not enabled\n• Fixed Visual C++ internal error when declaring a global vec type with siwzzle expression enabled #594\n• Fixed with Clang and libstlc++ which wasn't using C++11 STL features. #604\n• Added warning messages when using but the compiler is known to not fully support the requested C++ version #555\n• Added right and left handed projection and clip control support #447 #415 #119\n• Added and to GLM_GTC_packing for RGB9E5 #416\n• Added and to GLM_GTC_integer, fast round on positive values\n• Improved SIMD and swizzle operators interactions with GCC and Clang #474\n• Use Cuda built-in function for abs function implementation with Cuda compiler\n• No more warnings for use of long long\n• Fixed to not do any unintentional backface culling\n• Fixed long long warnings when using C++98 on GCC and Clang #482\n• Fixed long long warnings when using C++98 on GCC and Clang #482\n• Fixed to_string when used with GLM_FORCE_INLINE #506\n• Fixed intersectRayTriangle to not do any unintentional backface culling\n• Fixed outerProduct definitions and operator signatures for mat2x4 and vec4 #475\n• Fixed various 'X is not defined' warnings #468\n• Added to_string for quat and dual_quat in GTX_string_cast #375\n• Fixed builtin bitscan never being used #392\n• Added static components and precision members to all vector and quat types #350\n• Added support of defaulted functions to GLM types, to use them in unions #366\n• Don't show status message in 'FindGLM' if 'QUIET' option is set. #317\n• Fixed use of libstdc++ with Clang #351\n• Added display of GLM version with other GLM_MESSAGES\n• Clean up GLM_MESSAGES compilation log to report only detected capabilities\n• Fixed missing explicit conversion when using integer log2 with *vec1 types\n• Fixed Android build issue, STL C++11 is not supported by the NDK #284\n• Added GTX_scalar_multiplication for C++ 11 compiler only #242\n• Added GTX_range for C++ 11 compiler only #240\n• Added support of precision and integers to linearRand #230\n• Rely on C++11 to implement isinf and isnan\n• Undetected C++ compiler automatically compile with GLM_FORCE_CXX98 and GLM_FORCE_PURE\n• Added not function (from GLSL specification) on VC12\n• Used std features within GLM without redeclaring\n• Added explicit cast from quat to mat3 and mat4 #275\n• Fixed std::nextafter not supported with C++11 on Android #217\n• Fixed implicit conversion from another tvec2 type to another tvec2 #241\n• Fixed lack of consistency of quat and dualquat constructors\n• Fixed glm::isinf and glm::isnan for with Android NDK 9d #191\n• Fixed lerp when cosTheta is close to 1 in quaternion slerp #210\n• Fixed std::nextafter not supported with C++11 on Android #213\n• Fixed corner cases in exp and log functions for quaternions #199\n• Added instruction set auto detection with Visual C++ using _M_IX86_FP - /arch compiler argument\n• Added support for all extensions but GTX_string_cast to CUDA\n• Fixed non power of two matrix products\n• Fixed angle and orientedAngle that sometimes return NaN values (#145)\n• Fixed error 'inverse' is not a member of 'glm' from glm::unProject (#146)\n• Fixed mismatch between some declarations and definitions\n• Replaced C cast by C++ casts\n• Fixed .length() that should return a int and not a size_t\n• Removed the normalization of the up argument of lookAt function (#114)\n• Replaced GLM traits by STL traits when possible\n• Added creating of a quaternion from two vectors\n• Fixed detection to select the last known compiler if newer version #106\n• Fixed is_int and is_uint code duplication with GCC and C++11 #107\n• Fixed test suite build while using Clang in C++11 mode\n• Removed ms extension mode to CMake when no using Visual C++\n• Added pedantic mode to CMake test suite for Clang and GCC\n• Added use of GCC frontend on Unix for ICC and Visual C++ fronted on Windows for ICC\n• Fixed language detection on GCC when the C++0x mode isn't enabled #95\n• Fixed slerp when costheta is close to 1 #65\n• Added assert in inversesqrt to detect division by zero #61\n• Fixed glm::perspective when zNear is zero #71\n• Fixed C++11 mode for GCC, couldn't be enabled without MS extensions\n• Clarify the license applying on the manual\n• Fixed isnan and isinf on Android with Clang\n• Autodetected C++ version using __cplusplus value\n• Fixed mix for bool and bvec* third parameter\n• Fixed 0x2013 dash character in comments that cause issue in Windows Japanese mode\n• Fixed quat slerp using mix function when cosTheta close to 1\n• Added GLM_FORCE_RADIANS so that all functions takes radians for arguments\n• Fixed detection of Clang and LLVM GCC on MacOS X\n• Removed VIRTREV_xstream and the incompatibility generated with GCC\n• Fixed many warnings across platforms and compilers\n• Fixed errors and warnings in VC with C++ extensions disabled\n• Clarify that GLM is a header only library.\n• Added == and != operators for every types.\n• New method to use extension."
    },
    {
        "link": "https://learnopengl.com/Getting-started/Transformations",
        "document": "We now know how to create objects, color them and/or give them a detailed appearance using textures, but they're still not that interesting since they're all static objects. We could try and make them move by changing their vertices and re-configuring their buffers each frame, but that's cumbersome and costs quite some processing power. There are much better ways to an object and that's by using (multiple) objects. This doesn't mean we're going to talk about Kung Fu and a large digital artificial world.\n\nMatrices are very powerful mathematical constructs that seem scary at first, but once you'll grow accustomed to them they'll prove extremely useful. When discussing matrices, we'll have to make a small dive into some mathematics and for the more mathematically inclined readers I'll post additional resources for further reading.\n\nHowever, to fully understand transformations we first have to delve a bit deeper into vectors before discussing matrices. The focus of this chapter is to give you a basic mathematical background in topics we will require later on. If the subjects are difficult, try to understand them as much as you can and come back to this chapter later to review the concepts whenever you need them.\n\nIn its most basic definition, vectors are directions and nothing more. A vector has a and a (also known as its strength or length). You can think of vectors like directions on a treasure map: 'go left 10 steps, now go north 3 steps and go right 5 steps'; here 'left' is the direction and '10 steps' is the magnitude of the vector. The directions for the treasure map thus contains 3 vectors. Vectors can have any dimension, but we usually work with dimensions of 2 to 4. If a vector has 2 dimensions it represents a direction on a plane (think of 2D graphs) and when it has 3 dimensions it can represent any direction in a 3D world.\n\nBelow you'll see 3 vectors where each vector is represented with as arrows in a 2D graph. Because it is more intuitive to display vectors in 2D (rather than 3D) you can think of the 2D vectors as 3D vectors with a coordinate of . Since vectors represent directions, the origin of the vector does not change its value. In the graph below we can see that the vectors \\(\\color{red}{\\bar{v}}\\) and \\(\\color{blue}{\\bar{w}}\\) are equal even though their origin is different:\n\nWhen describing vectors mathematicians generally prefer to describe vectors as character symbols with a little bar over their head like \\(\\bar{v}\\). Also, when displaying vectors in formulas they are generally displayed as follows: \\[\\bar{v} = \\begin{pmatrix} \\color{red}x \\\\ \\color{green}y \\\\ \\color{blue}z \\end{pmatrix} \\]\n\nBecause vectors are specified as directions it is sometimes hard to visualize them as positions. If we want to visualize vectors as positions we can imagine the origin of the direction vector to be and then point towards a certain direction that specifies the point, making it a (we could also specify a different origin and then say: 'this vector points to that point in space from this origin'). The position vector would then point to on the graph with an origin of . Using vectors we can thus describe directions and positions in 2D and 3D space.\n\nJust like with normal numbers we can also define several operations on vectors (some of which you've already seen).\n\nA is a single digit. When adding/subtracting/multiplying or dividing a vector with a scalar we simply add/subtract/multiply or divide each element of the vector by the scalar. For addition it would look like this: \\[ \\begin{pmatrix} \\color{red}1 \\\\ \\color{green}2 \\\\ \\color{blue}3 \\end{pmatrix} + x \\rightarrow \\begin{pmatrix} \\color{red}1 \\\\ \\color{green}2 \\\\ \\color{blue}3 \\end{pmatrix} + \\begin{pmatrix} x \\\\ x \\\\ x \\end{pmatrix} = \\begin{pmatrix} \\color{red}1 + x \\\\ \\color{green}2 + x \\\\ \\color{blue}3 + x \\end{pmatrix} \\] Where \\(+\\) can be \\(+\\),\\(-\\),\\(\\cdot\\) or \\(\\div\\) where \\(\\cdot\\) is the multiplication operator.\n\nNegating a vector results in a vector in the reversed direction. A vector pointing north-east would point south-west after negation. To negate a vector we add a minus-sign to each component (you can also represent it as a scalar-vector multiplication with a scalar value of ): \\[-\\bar{v} = -\\begin{pmatrix} \\color{red}{v_x} \\\\ \\color{blue}{v_y} \\\\ \\color{green}{v_z} \\end{pmatrix} = \\begin{pmatrix} -\\color{red}{v_x} \\\\ -\\color{blue}{v_y} \\\\ -\\color{green}{v_z} \\end{pmatrix} \\]\n\nAddition of two vectors is defined as addition, that is each component of one vector is added to the same component of the other vector like so: \\[\\bar{v} = \\begin{pmatrix} \\color{red}1 \\\\ \\color{green}2 \\\\ \\color{blue}3 \\end{pmatrix}, \\bar{k} = \\begin{pmatrix} \\color{red}4 \\\\ \\color{green}5 \\\\ \\color{blue}6 \\end{pmatrix} \\rightarrow \\bar{v} + \\bar{k} = \\begin{pmatrix} \\color{red}1 + \\color{red}4 \\\\ \\color{green}2 + \\color{green}5 \\\\ \\color{blue}3 + \\color{blue}6 \\end{pmatrix} = \\begin{pmatrix} \\color{red}5 \\\\ \\color{green}7 \\\\ \\color{blue}9 \\end{pmatrix} \\] Visually, it looks like this on vectors and , where the second vector is added on top of the first vector's end to find the end point of the resulting vector (head-to-tail method):\n\nJust like normal addition and subtraction, vector subtraction is the same as addition with a negated second vector: \\[\\bar{v} = \\begin{pmatrix} \\color{red}{1} \\\\ \\color{green}{2} \\\\ \\color{blue}{3} \\end{pmatrix}, \\bar{k} = \\begin{pmatrix} \\color{red}{4} \\\\ \\color{green}{5} \\\\ \\color{blue}{6} \\end{pmatrix} \\rightarrow \\bar{v} + -\\bar{k} = \\begin{pmatrix} \\color{red}{1} + (-\\color{red}{4}) \\\\ \\color{green}{2} + (-\\color{green}{5}) \\\\ \\color{blue}{3} + (-\\color{blue}{6}) \\end{pmatrix} = \\begin{pmatrix} -\\color{red}{3} \\\\ -\\color{green}{3} \\\\ -\\color{blue}{3} \\end{pmatrix} \\]\n\nSubtracting two vectors from each other results in a vector that's the difference of the positions both vectors are pointing at. This proves useful in certain cases where we need to retrieve a vector that's the difference between two points.\n\nTo retrieve the length/magnitude of a vector we use the that you may remember from your math classes. A vector forms a triangle when you visualize its individual and component as two sides of a triangle:\n\nSince the length of the two sides are known and we want to know the length of the tilted side \\(\\color{red}{\\bar{v}}\\) we can calculate it using the Pythagoras theorem as: \\[||\\color{red}{\\bar{v}}|| = \\sqrt{\\color{green}x^2 + \\color{blue}y^2} \\] Where \\(||\\color{red}{\\bar{v}}||\\) is denoted as the length of vector \\(\\color{red}{\\bar{v}}\\). This is easily extended to 3D by adding \\(z^2\\) to the equation.\n\nIn this case the length of vector equals: \\[||\\color{red}{\\bar{v}}|| = \\sqrt{\\color{green}4^2 + \\color{blue}2^2} = \\sqrt{\\color{green}16 + \\color{blue}4} = \\sqrt{20} = 4.47 \\] Which is .\n\nThere is also a special type of vector that we call a . A unit vector has one extra property and that is that its length is exactly 1. We can calculate a unit vector \\(\\hat{n}\\) from any vector by dividing each of the vector's components by its length: \\[\\hat{n} = \\frac{\\bar{v}}{||\\bar{v}||}\\] We call this a vector. Unit vectors are displayed with a little roof over their head and are generally easier to work with, especially when we only care about their directions (the direction does not change if we change a vector's length).\n\nMultiplying two vectors is a bit of a weird case. Normal multiplication isn't really defined on vectors since it has no visual meaning, but we have two specific cases that we could choose from when multiplying: one is the denoted as \\(\\bar{v} \\cdot \\bar{k}\\) and the other is the denoted as \\(\\bar{v} \\times \\bar{k}\\).\n\nThe dot product of two vectors is equal to the scalar product of their lengths times the cosine of the angle between them. If this sounds confusing take a look at its formula: \\[\\bar{v} \\cdot \\bar{k} = ||\\bar{v}|| \\cdot ||\\bar{k}|| \\cdot \\cos \\theta \\] Where the angle between them is represented as theta (\\(\\theta\\)). Why is this interesting? Well, imagine if \\(\\bar{v}\\) and \\(\\bar{k}\\) are unit vectors then their length would be equal to 1. This would effectively reduce the formula to: \\[\\hat{v} \\cdot \\hat{k} = 1 \\cdot 1 \\cdot \\cos \\theta = \\cos \\theta\\] Now the dot product only defines the angle between both vectors. You may remember that the cosine or cos function becomes when the angle is 90 degrees or when the angle is 0. This allows us to easily test if the two vectors are or to each other using the dot product (orthogonal means the vectors are at a to each other). In case you want to know more about the or the functions I'd suggest the following Khan Academy videos about basic trigonometry.\n\nSo how do we calculate the dot product? The dot product is a component-wise multiplication where we add the results together. It looks like this with two unit vectors (you can verify that both their lengths are exactly ): \\[ \\begin{pmatrix} \\color{red}{0.6} \\\\ -\\color{green}{0.8} \\\\ \\color{blue}0 \\end{pmatrix} \\cdot \\begin{pmatrix} \\color{red}0 \\\\ \\color{green}1 \\\\ \\color{blue}0 \\end{pmatrix} = (\\color{red}{0.6} * \\color{red}0) + (-\\color{green}{0.8} * \\color{green}1) + (\\color{blue}0 * \\color{blue}0) = -0.8 \\] To calculate the degree between both these unit vectors we use the inverse of the cosine function \\(cos^{-1}\\) and this results in degrees. We now effectively calculated the angle between these two vectors. The dot product proves very useful when doing lighting calculations later on.\n\nThe cross product is only defined in 3D space and takes two non-parallel vectors as input and produces a third vector that is orthogonal to both the input vectors. If both the input vectors are orthogonal to each other as well, a cross product would result in 3 orthogonal vectors; this will prove useful in the upcoming chapters. The following image shows what this looks like in 3D space:\n\nUnlike the other operations, the cross product isn't really intuitive without delving into linear algebra so it's best to just memorize the formula and you'll be fine (or don't, you'll probably be fine as well). Below you'll see the cross product between two orthogonal vectors A and B: \\[\\begin{pmatrix} \\color{red}{A_{x}} \\\\ \\color{green}{A_{y}} \\\\ \\color{blue}{A_{z}} \\end{pmatrix} \\times \\begin{pmatrix} \\color{red}{B_{x}} \\\\ \\color{green}{B_{y}} \\\\ \\color{blue}{B_{z}} \\end{pmatrix} = \\begin{pmatrix} \\color{green}{A_{y}} \\cdot \\color{blue}{B_{z}} - \\color{blue}{A_{z}} \\cdot \\color{green}{B_{y}} \\\\ \\color{blue}{A_{z}} \\cdot \\color{red}{B_{x}} - \\color{red}{A_{x}} \\cdot \\color{blue}{B_{z}} \\\\ \\color{red}{A_{x}} \\cdot \\color{green}{B_{y}} - \\color{green}{A_{y}} \\cdot \\color{red}{B_{x}} \\end{pmatrix} \\] As you can see, it doesn't really seem to make sense. However, if you just follow these steps you'll get another vector that is orthogonal to your input vectors.\n\nNow that we've discussed almost all there is to vectors it is time to enter the matrix! A matrix is a rectangular array of numbers, symbols and/or mathematical expressions. Each individual item in a matrix is called an of the matrix. An example of a 2x3 matrix is shown below: \\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\] Matrices are indexed by where is the row and is the column, that is why the above matrix is called a 2x3 matrix (3 columns and 2 rows, also known as the of the matrix). This is the opposite of what you're used to when indexing 2D graphs as . To retrieve the value 4 we would index it as (second row, first column).\n\nMatrices are basically nothing more than that, just rectangular arrays of mathematical expressions. They do have a very nice set of mathematical properties and just like vectors we can define several operations on matrices, namely: addition, subtraction and multiplication.\n\nMatrix addition and subtraction between two matrices is done on a per-element basis. So the same general rules apply that we're familiar with for normal numbers, but done on the elements of both matrices with the same index. This does mean that addition and subtraction is only defined for matrices of the same dimensions. A 3x2 matrix and a 2x3 matrix (or a 3x3 matrix and a 4x4 matrix) cannot be added or subtracted together. Let's see how matrix addition works on two 2x2 matrices: \\[\\begin{bmatrix} \\color{red}1 & \\color{red}2 \\\\ \\color{green}3 & \\color{green}4 \\end{bmatrix} + \\begin{bmatrix} \\color{red}5 & \\color{red}6 \\\\ \\color{green}7 & \\color{green}8 \\end{bmatrix} = \\begin{bmatrix} \\color{red}1 + \\color{red}5 & \\color{red}2 + \\color{red}6 \\\\ \\color{green}3 + \\color{green}7 & \\color{green}4 + \\color{green}8 \\end{bmatrix} = \\begin{bmatrix} \\color{red}6 & \\color{red}8 \\\\ \\color{green}{10} & \\color{green}{12} \\end{bmatrix} \\] The same rules apply for matrix subtraction: \\[\\begin{bmatrix} \\color{red}4 & \\color{red}2 \\\\ \\color{green}1 & \\color{green}6 \\end{bmatrix} - \\begin{bmatrix} \\color{red}2 & \\color{red}4 \\\\ \\color{green}0 & \\color{green}1 \\end{bmatrix} = \\begin{bmatrix} \\color{red}4 - \\color{red}2 & \\color{red}2 - \\color{red}4 \\\\ \\color{green}1 - \\color{green}0 & \\color{green}6 - \\color{green}1 \\end{bmatrix} = \\begin{bmatrix} \\color{red}2 & -\\color{red}2 \\\\ \\color{green}1 & \\color{green}5 \\end{bmatrix} \\]\n\nA matrix-scalar product multiples each element of the matrix by a scalar. The following example illustrates the multiplication: \\[\\color{green}2 \\cdot \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} \\color{green}2 \\cdot 1 & \\color{green}2 \\cdot 2 \\\\ \\color{green}2 \\cdot 3 & \\color{green}2 \\cdot 4 \\end{bmatrix} = \\begin{bmatrix} 2 & 4 \\\\ 6 & 8 \\end{bmatrix}\\] Now it also makes sense as to why those single numbers are called scalars. A scalar basically scales all the elements of the matrix by its value. In the previous example, all elements were scaled by .\n\nSo far so good, all of our cases weren't really too complicated. That is, until we start on matrix-matrix multiplication.\n\nMultiplying matrices is not necessarily complex, but rather difficult to get comfortable with. Matrix multiplication basically means to follow a set of pre-defined rules when multiplying. There are a few restrictions though:\n• You can only multiply two matrices if the number of columns on the left-hand side matrix is equal to the number of rows on the right-hand side matrix.\n• Matrix multiplication is not that is \\(A \\cdot B \n\neq B \\cdot A\\).\n\nLet's get started with an example of a matrix multiplication of 2 matrices: \\[ \\begin{bmatrix} \\color{red}1 & \\color{red}2 \\\\ \\color{green}3 & \\color{green}4 \\end{bmatrix} \\cdot \\begin{bmatrix} \\color{blue}5 & \\color{purple}6 \\\\ \\color{blue}7 & \\color{purple}8 \\end{bmatrix} = \\begin{bmatrix} \\color{red}1 \\cdot \\color{blue}5 + \\color{red}2 \\cdot \\color{blue}7 & \\color{red}1 \\cdot \\color{purple}6 + \\color{red}2 \\cdot \\color{purple}8 \\\\ \\color{green}3 \\cdot \\color{blue}5 + \\color{green}4 \\cdot \\color{blue}7 & \\color{green}3 \\cdot \\color{purple}6 + \\color{green}4 \\cdot \\color{purple}8 \\end{bmatrix} = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix} \\] Right now you're probably trying to figure out what the hell just happened? Matrix multiplication is a combination of normal multiplication and addition using the left-matrix's rows with the right-matrix's columns. Let's try discussing this with the following image:\n\nWe first take the upper row of the left matrix and then take a column from the right matrix. The row and column that we picked decides which output value of the resulting matrix we're going to calculate. If we take the first row of the left matrix the resulting value will end up in the first row of the result matrix, then we pick a column and if it's the first column the result value will end up in the first column of the result matrix. This is exactly the case of the red pathway. To calculate the bottom-right result we take the bottom row of the first matrix and the rightmost column of the second matrix.\n\nTo calculate the resulting value we multiply the first element of the row and column together using normal multiplication, we do the same for the second elements, third, fourth etc. The results of the individual multiplications are then summed up and we have our result. Now it also makes sense that one of the requirements is that the size of the left-matrix's columns and the right-matrix's rows are equal, otherwise we can't finish the operations!\n\nThe result is then a matrix that has dimensions of ( ) where is equal to the number of rows of the left-hand side matrix and is equal to the columns of the right-hand side matrix.\n\nDon't worry if you have difficulties imagining the multiplications inside your head. Just keep trying to do the calculations by hand and return to this page whenever you have difficulties. Over time, matrix multiplication becomes second nature to you.\n\nLet's finish the discussion of matrix-matrix multiplication with a larger example. Try to visualize the pattern using the colors. As a useful exercise, see if you can come up with your own answer of the multiplication and then compare them with the resulting matrix (once you try to do a matrix multiplication by hand you'll quickly get the grasp of them). \\[ \\begin{bmatrix} \\color{red}4 & \\color{red}2 & \\color{red}0 \\\\ \\color{green}0 & \\color{green}8 & \\color{green}1 \\\\ \\color{blue}0 & \\color{blue}1 & \\color{blue}0 \\end{bmatrix} \\cdot \\begin{bmatrix} \\color{red}4 & \\color{green}2 & \\color{blue}1 \\\\ \\color{red}2 & \\color{green}0 & \\color{blue}4 \\\\ \\color{red}9 & \\color{green}4 & \\color{blue}2 \\end{bmatrix} = \\begin{bmatrix} \\color{red}4 \\cdot \\color{red}4 + \\color{red}2 \\cdot \\color{red}2 + \\color{red}0 \\cdot \\color{red}9 & \\color{red}4 \\cdot \\color{green}2 + \\color{red}2 \\cdot \\color{green}0 + \\color{red}0 \\cdot \\color{green}4 & \\color{red}4 \\cdot \\color{blue}1 + \\color{red}2 \\cdot \\color{blue}4 + \\color{red}0 \\cdot \\color{blue}2 \\\\ \\color{green}0 \\cdot \\color{red}4 + \\color{green}8 \\cdot \\color{red}2 + \\color{green}1 \\cdot \\color{red}9 & \\color{green}0 \\cdot \\color{green}2 + \\color{green}8 \\cdot \\color{green}0 + \\color{green}1 \\cdot \\color{green}4 & \\color{green}0 \\cdot \\color{blue}1 + \\color{green}8 \\cdot \\color{blue}4 + \\color{green}1 \\cdot \\color{blue}2 \\\\ \\color{blue}0 \\cdot \\color{red}4 + \\color{blue}1 \\cdot \\color{red}2 + \\color{blue}0 \\cdot \\color{red}9 & \\color{blue}0 \\cdot \\color{green}2 + \\color{blue}1 \\cdot \\color{green}0 + \\color{blue}0 \\cdot \\color{green}4 & \\color{blue}0 \\cdot \\color{blue}1 + \\color{blue}1 \\cdot \\color{blue}4 + \\color{blue}0 \\cdot \\color{blue}2 \\end{bmatrix} \\\\ = \\begin{bmatrix} 20 & 8 & 12 \\\\ 25 & 4 & 34 \\\\ 2 & 0 & 4 \\end{bmatrix}\\]\n\nAs you can see, matrix-matrix multiplication is quite a cumbersome process and very prone to errors (which is why we usually let computers do this) and this gets problematic real quick when the matrices become larger. If you're still thirsty for more and you're curious about some more of the mathematical properties of matrices I strongly suggest you take a look at these Khan Academy videos about matrices.\n\nAnyways, now that we know how to multiply matrices together, we can start getting to the good stuff.\n\nUp until now we've had our fair share of vectors. We used them to represent positions, colors and even texture coordinates. Let's move a bit further down the rabbit hole and tell you that a vector is basically a matrix where is the vector's number of components (also known as an vector). If you think about it, it makes a lot of sense. Vectors are just like matrices an array of numbers, but with only 1 column. So, how does this new piece of information help us? Well, if we have a matrix we can multiply this matrix with our vector, since the columns of the matrix are equal to the number of rows of the vector, thus matrix multiplication is defined.\n\nBut why do we care if we can multiply matrices with a vector? Well, it just so happens that there are lots of interesting 2D/3D transformations we can place inside a matrix, and multiplying that matrix with a vector then transforms that vector. In case you're still a bit confused, let's start with a few examples and you'll soon see what we mean.\n\nIn OpenGL we usually work with transformation matrices for several reasons and one of them is that most of the vectors are of size 4. The most simple transformation matrix that we can think of is the . The identity matrix is an matrix with only 0s except on its diagonal. As you'll see, this transformation matrix leaves a vector completely unharmed: \\[ \\begin{bmatrix} \\color{red}1 & \\color{red}0 & \\color{red}0 & \\color{red}0 \\\\ \\color{green}0 & \\color{green}1 & \\color{green}0 & \\color{green}0 \\\\ \\color{blue}0 & \\color{blue}0 & \\color{blue}1 & \\color{blue}0 \\\\ \\color{purple}0 & \\color{purple}0 & \\color{purple}0 & \\color{purple}1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} \\color{red}1 \\cdot 1 \\\\ \\color{green}1 \\cdot 2 \\\\ \\color{blue}1 \\cdot 3 \\\\ \\color{purple}1 \\cdot 4 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix} \\] The vector is completely untouched. This becomes obvious from the rules of multiplication: the first result element is each individual element of the first row of the matrix multiplied with each element of the vector. Since each of the row's elements are 0 except the first one, we get: \\(\\color{red}1\\cdot1 + \\color{red}0\\cdot2 + \\color{red}0\\cdot3 + \\color{red}0\\cdot4 = 1\\) and the same applies for the other 3 elements of the vector.\n\nWhen we're scaling a vector we are increasing the length of the arrow by the amount we'd like to scale, keeping its direction the same. Since we're working in either 2 or 3 dimensions we can define scaling by a vector of 2 or 3 scaling variables, each scaling one axis ( , or ).\n\nLet's try scaling the vector \\(\\color{red}{\\bar{v}} = (3,2)\\). We will scale the vector along the x-axis by , thus making it twice as narrow; and we'll scale the vector by along the y-axis, making it twice as high. Let's see what it looks like if we scale the vector by as \\(\\color{blue}{\\bar{s}}\\):\n\nKeep in mind that OpenGL usually operates in 3D space so for this 2D case we could set the z-axis scale to , leaving it unharmed. The scaling operation we just performed is a scale, because the scaling factor is not the same for each axis. If the scalar would be equal on all axes it would be called a .\n\nLet's start building a transformation matrix that does the scaling for us. We saw from the identity matrix that each of the diagonal elements were multiplied with its corresponding vector element. What if we were to change the s in the identity matrix to s? In that case, we would be multiplying each of the vector elements by a value of and thus effectively uniformly scale the vector by 3. If we represent the scaling variables as \\( (\\color{red}{S_1}, \\color{green}{S_2}, \\color{blue}{S_3}) \\) we can define a scaling matrix on any vector \\((x,y,z)\\) as: \\[\\begin{bmatrix} \\color{red}{S_1} & \\color{red}0 & \\color{red}0 & \\color{red}0 \\\\ \\color{green}0 & \\color{green}{S_2} & \\color{green}0 & \\color{green}0 \\\\ \\color{blue}0 & \\color{blue}0 & \\color{blue}{S_3} & \\color{blue}0 \\\\ \\color{purple}0 & \\color{purple}0 & \\color{purple}0 & \\color{purple}1 \\end{bmatrix} \\cdot \\begin{pmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\color{red}{S_1} \\cdot x \\\\ \\color{green}{S_2} \\cdot y \\\\ \\color{blue}{S_3} \\cdot z \\\\ 1 \\end{pmatrix} \\] Note that we keep the 4th scaling value . The component is used for other purposes as we'll see later on.\n\nis the process of adding another vector on top of the original vector to return a new vector with a different position, thus moving the vector based on a translation vector. We've already discussed vector addition so this shouldn't be too new.\n\nJust like the scaling matrix there are several locations on a 4-by-4 matrix that we can use to perform certain operations and for translation those are the top-3 values of the 4th column. If we represent the translation vector as \\((\\color{red}{T_x},\\color{green}{T_y},\\color{blue}{T_z})\\) we can define the translation matrix by: \\[\\begin{bmatrix} \\color{red}1 & \\color{red}0 & \\color{red}0 & \\color{red}{T_x} \\\\ \\color{green}0 & \\color{green}1 & \\color{green}0 & \\color{green}{T_y} \\\\ \\color{blue}0 & \\color{blue}0 & \\color{blue}1 & \\color{blue}{T_z} \\\\ \\color{purple}0 & \\color{purple}0 & \\color{purple}0 & \\color{purple}1 \\end{bmatrix} \\cdot \\begin{pmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} x + \\color{red}{T_x} \\\\ y + \\color{green}{T_y} \\\\ z + \\color{blue}{T_z} \\\\ 1 \\end{pmatrix} \\] This works because all of the translation values are multiplied by the vector's column and added to the vector's original values (remember the matrix-multiplication rules). This wouldn't have been possible with a 3-by-3 matrix.\n\nWith a translation matrix we can move objects in any of the 3 axis directions ( , , ), making it a very useful transformation matrix for our transformation toolkit.\n\nThe last few transformations were relatively easy to understand and visualize in 2D or 3D space, but rotations are a bit trickier. If you want to know exactly how these matrices are constructed I'd recommend that you watch the rotation items of Khan Academy's linear algebra videos.\n\nFirst let's define what a rotation of a vector actually is. A rotation in 2D or 3D is represented with an . An angle could be in degrees or radians where a whole circle has 360 degrees or 2 PI radians. I prefer explaining rotations using degrees as we're generally more accustomed to them. Most rotation functions require an angle in radians, but luckily degrees are easily converted to radians: \n\n \n\n \n\n Where equals (rounded) . Rotating half a circle rotates us 360/2 = 180 degrees and rotating 1/5th to the right means we rotate 360/5 = 72 degrees to the right. This is demonstrated for a basic 2D vector where \\(\\color{red}{\\bar{v}}\\) is rotated 72 degrees to the right, or clockwise, from \\(\\color{green}{\\bar{k}}\\):\n\nRotations in 3D are specified with an angle and a . The angle specified will rotate the object along the rotation axis given. Try to visualize this by spinning your head a certain degree while continually looking down a single rotation axis. When rotating 2D vectors in a 3D world for example, we set the rotation axis to the z-axis (try to visualize this).\n\nUsing trigonometry it is possible to transform vectors to newly rotated vectors given an angle. This is usually done via a smart combination of the and functions (commonly abbreviated to and ). A discussion of how the rotation matrices are generated is out of the scope of this chapter.\n\nA rotation matrix is defined for each unit axis in 3D space where the angle is represented as the theta symbol \\(\\theta\\).\n\nUsing the rotation matrices we can transform our position vectors around one of the three unit axes. To rotate around an arbitrary 3D axis we can combine all 3 them by first rotating around the X-axis, then Y and then Z for example. However, this quickly introduces a problem called . We won't discuss the details, but a better solution is to rotate around an arbitrary unit axis e.g. (note that this is a unit vector) right away instead of combining the rotation matrices. Such a (verbose) matrix exists and is given below with \\((\\color{red}{R_x}, \\color{green}{R_y}, \\color{blue}{R_z})\\) as the arbitrary rotation axis: \\[\\begin{bmatrix} \\cos \\theta + \\color{red}{R_x}^2(1 - \\cos \\theta) & \\color{red}{R_x}\\color{green}{R_y}(1 - \\cos \\theta) - \\color{blue}{R_z} \\sin \\theta & \\color{red}{R_x}\\color{blue}{R_z}(1 - \\cos \\theta) + \\color{green}{R_y} \\sin \\theta & 0 \\\\ \\color{green}{R_y}\\color{red}{R_x} (1 - \\cos \\theta) + \\color{blue}{R_z} \\sin \\theta & \\cos \\theta + \\color{green}{R_y}^2(1 - \\cos \\theta) & \\color{green}{R_y}\\color{blue}{R_z}(1 - \\cos \\theta) - \\color{red}{R_x} \\sin \\theta & 0 \\\\ \\color{blue}{R_z}\\color{red}{R_x}(1 - \\cos \\theta) - \\color{green}{R_y} \\sin \\theta & \\color{blue}{R_z}\\color{green}{R_y}(1 - \\cos \\theta) + \\color{red}{R_x} \\sin \\theta & \\cos \\theta + \\color{blue}{R_z}^2(1 - \\cos \\theta) & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}\\] A mathematical discussion of generating such a matrix is out of the scope of this chapter. Keep in mind that even this matrix does not completely prevent gimbal lock (although it gets a lot harder). To truly prevent Gimbal locks we have to represent rotations using , that are not only safer, but also more computationally friendly. However, a discussion of quaternions is out of this chapter's scope.\n\nThe true power from using matrices for transformations is that we can combine multiple transformations in a single matrix thanks to matrix-matrix multiplication. Let's see if we can generate a transformation matrix that combines several transformations. Say we have a vector and we want to scale it by 2 and then translate it by . We need a translation and a scaling matrix for our required steps. The resulting transformation matrix would then look like: \\[Trans . Scale = \\begin{bmatrix} \\color{red}1 & \\color{red}0 & \\color{red}0 & \\color{red}1 \\\\ \\color{green}0 & \\color{green}1 & \\color{green}0 & \\color{green}2 \\\\ \\color{blue}0 & \\color{blue}0 & \\color{blue}1 & \\color{blue}3 \\\\ \\color{purple}0 & \\color{purple}0 & \\color{purple}0 & \\color{purple}1 \\end{bmatrix} . \\begin{bmatrix} \\color{red}2 & \\color{red}0 & \\color{red}0 & \\color{red}0 \\\\ \\color{green}0 & \\color{green}2 & \\color{green}0 & \\color{green}0 \\\\ \\color{blue}0 & \\color{blue}0 & \\color{blue}2 & \\color{blue}0 \\\\ \\color{purple}0 & \\color{purple}0 & \\color{purple}0 & \\color{purple}1 \\end{bmatrix} = \\begin{bmatrix} \\color{red}2 & \\color{red}0 & \\color{red}0 & \\color{red}1 \\\\ \\color{green}0 & \\color{green}2 & \\color{green}0 & \\color{green}2 \\\\ \\color{blue}0 & \\color{blue}0 & \\color{blue}2 & \\color{blue}3 \\\\ \\color{purple}0 & \\color{purple}0 & \\color{purple}0 & \\color{purple}1 \\end{bmatrix} \\] Note that we first do a translation and then a scale transformation when multiplying matrices. Matrix multiplication is not commutative, which means their order is important. When multiplying matrices the right-most matrix is first multiplied with the vector so you should read the multiplications from right to left. It is advised to first do scaling operations, then rotations and lastly translations when combining matrices otherwise they may (negatively) affect each other. For example, if you would first do a translation and then scale, the translation vector would also scale!\n\nRunning the final transformation matrix on our vector results in the following vector: \\[\\begin{bmatrix} \\color{red}2 & \\color{red}0 & \\color{red}0 & \\color{red}1 \\\\ \\color{green}0 & \\color{green}2 & \\color{green}0 & \\color{green}2 \\\\ \\color{blue}0 & \\color{blue}0 & \\color{blue}2 & \\color{blue}3 \\\\ \\color{purple}0 & \\color{purple}0 & \\color{purple}0 & \\color{purple}1 \\end{bmatrix} . \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\color{red}2x + \\color{red}1 \\\\ \\color{green}2y + \\color{green}2 \\\\ \\color{blue}2z + \\color{blue}3 \\\\ 1 \\end{bmatrix} \\] Great! The vector is first scaled by two and then translated by .\n\nNow that we've explained all the theory behind transformations, it's time to see how we can actually use this knowledge to our advantage. OpenGL does not have any form of matrix or vector knowledge built in, so we have to define our own mathematics classes and functions. In this book we'd rather abstract from all the tiny mathematical details and simply use pre-made mathematics libraries. Luckily, there is an easy-to-use and tailored-for-OpenGL mathematics library called GLM.\n\nGLM stands for OpenGL Mathematics and is a header-only library, which means that we only have to include the proper header files and we're done; no linking and compiling necessary. GLM can be downloaded from their website. Copy the root directory of the header files into your includes folder and let's get rolling.\n\nMost of GLM's functionality that we need can be found in 3 headers files that we'll include as follows:\n\nLet's see if we can put our transformation knowledge to good use by translating a vector of by (note that we define it as a with its homogeneous coordinate set to :\n\nWe first define a vector named using GLM's built-in vector class. Next we define a and explicitly initialize it to the identity matrix by initializing the matrix's diagonals to ; if we do not initialize it to the identity matrix the matrix would be a null matrix (all elements ) and all subsequent matrix operations would end up a null matrix as well.\n\nThe next step is to create a transformation matrix by passing our identity matrix to the function, together with a translation vector (the given matrix is then multiplied with a translation matrix and the resulting matrix is returned). \n\n Then we multiply our vector by the transformation matrix and output the result. If we still remember how matrix translation works then the resulting vector should be which is . This snippet of code outputs so the translation matrix did its job.\n\nLet's do something more interesting and scale and rotate the container object from the previous chapter:\n\nFirst we scale the container by on each axis and then rotate the container degrees around the Z-axis. GLM expects its angles in radians so we convert the degrees to radians using . Note that the textured rectangle is on the XY plane so we want to rotate around the Z-axis. Keep in mind that the axis that we rotate around should be a unit vector, so be sure to normalize the vector first if you're not rotating around the X, Y, or Z axis. Because we pass the matrix to each of GLM's functions, GLM automatically multiples the matrices together, resulting in a transformation matrix that combines all the transformations.\n\nThe next big question is: how do we get the transformation matrix to the shaders? We shortly mentioned before that GLSL also has a type. So we'll adapt the vertex shader to accept a uniform variable and multiply the position vector by the matrix uniform:\n\nWe added the uniform and multiplied the position vector with the transformation matrix before passing it to . Our container should now be twice as small and rotated degrees (tilted to the left). We still need to pass the transformation matrix to the shader though:\n\nWe first query the location of the uniform variable and then send the matrix data to the shaders using with as its postfix. The first argument should be familiar by now which is the uniform's location. The second argument tells OpenGL how many matrices we'd like to send, which is . The third argument asks us if we want to transpose our matrix, that is to swap the columns and rows. OpenGL developers often use an internal matrix layout called which is the default matrix layout in GLM so there is no need to transpose the matrices; we can keep it at . The last parameter is the actual matrix data, but GLM stores their matrices' data in a way that doesn't always match OpenGL's expectations so we first convert the data with GLM's built-in function .\n\nWe created a transformation matrix, declared a uniform in the vertex shader and sent the matrix to the shaders where we transform our vertex coordinates. The result should look something like this:\n\nPerfect! Our container is indeed tilted to the left and twice as small so the transformation was successful. Let's get a little more funky and see if we can rotate the container over time, and for fun we'll also reposition the container at the bottom-right side of the window. To rotate the container over time we have to update the transformation matrix in the render loop because it needs to update each frame. We use GLFW's time function to get an angle over time:\n\nKeep in mind that in the previous case we could declare the transformation matrix anywhere, but now we have to create it every iteration to continuously update the rotation. This means we have to re-create the transformation matrix in each iteration of the render loop. Usually when rendering scenes we have several transformation matrices that are re-created with new values each frame.\n\nHere we first rotate the container around the origin and once it's rotated, we translate its rotated version to the bottom-right corner of the screen. Remember that the actual transformation order should be read in reverse: even though in code we first translate and then later rotate, the actual transformations first apply a rotation and then a translation. Understanding all these combinations of transformations and how they apply to objects is difficult to understand. Try and experiment with transformations like these and you'll quickly get a grasp of it.\n\nIf you did things right you should get the following result:\n\nAnd there you have it. A translated container that's rotated over time, all done by a single transformation matrix! Now you can see why matrices are such a powerful construct in graphics land. We can define an infinite amount of transformations and combine them all in a single matrix that we can re-use as often as we'd like. Using transformations like this in the vertex shader saves us the effort of re-defining the vertex data and saves us some processing time as well, since we don't have to re-send our data all the time (which is quite slow); all we need to do is update the transformation uniform.\n\nIf you didn't get the right result or you're stuck somewhere else, take a look at the source code and the updated shader class.\n\nIn the next chapter we'll discuss how we can use matrices to define different coordinate spaces for our vertices. This will be our first step into 3D graphics!\n• Essence of Linear Algebra: great video tutorial series by Grant Sanderson about the underlying mathematics of transformations and linear algebra.\n• Matrix Multiplication XYZ: check out this amazing interactive visual tool for showcasing matrix multiplication. Trying a few of these should help solidify your understanding.\n• Using the last transformation on the container, try switching the order around by first rotating and then translating. See what happens and try to reason why this happens: solution.\n• Try drawing a second container with another call to but place it at a different position using transformations only. Make sure this second container is placed at the top-left of the window and instead of rotating, scale it over time (using the function is useful here; note that using will cause the object to invert as soon as a negative scale is applied): solution."
    },
    {
        "link": "https://eecs.oregonstate.edu/~mjb/cs557/Handouts/GLM.1pp.pdf",
        "document": ""
    },
    {
        "link": "http://glm.g-truc.net/glm.pdf",
        "document": ""
    }
]