[
    {
        "link": "https://towardsdatascience.com/how-to-use-bayesian-inference-for-predictions-in-python-4de5d0bc84f3",
        "document": "The beauty of Bayesian Statistics is, at the same time, one of its most annoying features: we often get answers in the form of \"well, the number is somewhere between…\"\n\nIf on one hand, this might be frustrating to business strategists or policymakers looking for straightforward guidance, it is also a fantastic way to know how wrong you can be.\n\nTo start with a simple example, let us say that we are given the task of finding the distribution of heights of students in a high school classroom. Maybe this school requires a uniform, and the uniform company wants to have some sense of how many uniforms of each size to produce, and knowing the distribution of heights will allow us to know what sizes to make.\n\nSay we have a variable, h, that denotes the height of these students. What if we want to find out how h is distributed? That is, we might already have an idea of what the distribution looks like (height is usually normally distributed), but what are the parameters of said distribution? Why do we even care? Well, we care because knowing how a variable is distributed helps us predict what new data will look like. If we know height is normally distributed, for example, we know that a new data point will likely be close to the mean (if the mean is 1.75 m, then we expect a new student who joins the class to have a height close to that mean). Knowing the parameters of distribution also allows us to sample from it (so we can create a sample of ‘fake’ students and thus have an idea of how many students of each height we might expect). Therefore, finding the distribution of a variable helps us with prediction problems.\n\nBayesian inference is a method to figure out what the distribution of variables is (like the distribution of the heights h). The interesting feature of Bayesian inference is that it is up to the statistician (or data scientist) to use their prior knowledge as a means to improve our guess of how the distribution looks like.\n\nBayesian inference depends on the principal formula of Bayesian statistics: Bayes’ theorem. Bayes’ theorem takes in our assumptions about how the distribution looks like, a new piece of data, and outputs an updated distribution. For data science, Bayes’ theorem is usually presented as such:\n\nStatisticians also gave each component of this theorem names:\n\nLet’s go over them to understand them a bit better.\n\nWe can see from Bayes’ theorem that the prior is a probability: P(θ). First, let’s dig into what ‘θ’ means. θ is often expressed as our hypothesis for the model that best describes the variable we are trying to investigate. Let’s go back to the example of heights. We infer, from background knowledge and common sense, that height is normally distributed in a class. Formally:\n\nWhere N denotes the normal distribution, μ denotes the mean and σ denotes the standard deviation.\n\nNow, our prior isn’t exactly the expression above. Instead, it is our assumptions for how each of the parameters, μ, and σ, is distributed. Note that this is where the defining characteristic of Bayesian statistics shows up: how do we find the distribution of these parameters? Well, amusingly, we \"make them up\" based on our prior knowledge. If we have very little prior knowledge, we can choose a very uninformative prior so as not to bias the process at all. For example, we can define that μ, the mean height, is somewhere between 1.65m and 1.8m. If we want to go for an uninformative prior, we can say that μ is uniformly distributed along that interval. If instead, we believe that the mean height is somewhat skewed to a value closer to 1.65m than to 1.8m, we can define that μ is distributed with beta distribution, defined by \"hyper\" parameters α and β. We can take a look at these options below:\n\nNote how the y-axis gives us the ‘probability density’, i.e. how likely we think it is that the true μ is the one on the x-axis. Also, note that the beta and uniform distributions lead to different conclusions on what we think the value of μ may be. If we choose the uniform distribution, we are saying that we have no inclination as to whether μ is close to any value within our range, we just think it is somewhere in there. If we choose the beta distribution, we are fairly sure that the ‘real’ value of μ is somewhere between 1.68m and 1.72m, as shown by the peak along the blue line.\n\nNote that we are discussing the prior for μ, but that our model actually has two parameters: N(μ,σ). Generally, we can define a prior over σ as well. However, if we are feeling lucky about our guess for σ, or if we want to simplify the process for the sake of an example, we can just set σ to be a fixed value, like 0.1m.\n\nThe likelihood is expressed as P(Data|θ). The ‘data’ in this case would be an observed value for the height. Say we get to measure one student, picked at random, and their height is 1.7m. Consider that with this datum we can now have a sense of how good each option for θ is. We do this by asking: if a particular option for θ, call it θ1, were the true one, how ‘likely’ would it be for us to observe a height of 1.7m? How about θ2: how likely would it be to observe a height of 1.7m if θ2 were the ‘correct’ model?\n\nIndeed what we want at this stage is a function that will systematically look at every possibility for the model θ and find the probability that this model would have ‘produced’ or ‘predicted’ the observed value. Remember that in this case, our model θ is defined as N(μ,σ), i.e. a normal distribution with mean μ and standard deviation σ. We are also keeping σ constant to simplify the process. Therefore our function will take in possible values for μ as our ‘independent’ variable, our observed data point of 1.7m, and it will output the probabilities that each model is the correct model as our ‘dependent’ variable. Note that we run into a slight hiccup here: the ‘probability’ that a particular model is the correct one would technically be zero because, in theory, the possibilities for θ are endless. For this reason, I discretized the possibilities for θ, making it such that there are 50 options for θ between 1.65m and 1.8m. We then use the probability density functions for each proposed model to evaluate the probability density of the observed datum for a particular model. The probability density isn’t the same as the probability, it just gives us a relative measure of how likely it is that the point was observed given each of the model options. To get true probabilities, we would have to ‘normalize’ the probability densities, making it such that adding all of their values would give us 1.\n\nThis turns out to not be a huge deal, however, because we can still compare how likely each model is. Its as if we are asking: constraining the possibilities to this discrete set of models that somewhat comprehensively cover plausible possibilities, which model is the best? Nevertheless, we are also going to normalize the probability densities as well, as you will see below.\n\nThis function is called the ‘likelihood’ function, and we typically define it by the probability ‘density’ function (PDF) of the model we are proposing, evaluated at the new data point. Thus, we want the \"PDF\" of the distribution N(μ, 0.1m) for the data point 1.7m.\n\nNote that for those who have used PDFs before this seems a bit backward. With PDFs, we usually have a fixed model, e.g. N(1.8,0.1), and we use it to evaluate the probability of different values for our variable h. This would mean that we would have the variable h on the x-axis, and the probability density on the y-axis.\n\nFor our current purposes, however, we are varying the distribution/model itself. This means that our x-axis will actually have the different possibilities for our variable μ, and our y-axis will have the probability density for each of these possibilities. Have a look at the code below, which represents our likelihood function and visualization for it:\n\nSee how our function simply shows us which values of μ are most likely? See how this gives us a relative understanding of the probability, which statisticians prefer to call the ‘likelihood’ for each possible μ? With this evaluation of the likelihood, we get a preemptive inclination as to what the ‘best’ value for μ might be. We then have to combine this with the prior, however, to get our final ‘guess’ on what the best value of μ is.\n\nSome statisticians call P(Data) the ‘evidence’. The meaning of this variable is pretty straightforward: it is the probability that value Data is produced. However, this is tough to calculate directly. Thankfully, we have a good trick up our sleeves.\n\nConsider the following expression:\n\nCan you see why it makes sense? Intuitively, what we are saying is that if we were to sum the probability evaluations for every hypothesis θ, we’d exhaust all the hypothesis probabilities, and we should be left with just P(Data). Note that P(θ) is a probability distribution, and becomes equal to 1 if we add together all of its outputs for every θ. Note that ∫P(Data|θ)∗P(θ)dθ is equivalent to finding the area under the curve of the graph with P(Data|θ)∗P(θ) on the y-axis and θ on the x-axis, we will do exactly this for the next step.\n\nThe right-hand side of Bayes’ theorem, P(θ|Data), is called the ‘posterior’. It is our posterior understanding of how the data is distributed given that we witnessed the data, and that we had a prior about it.\n\nHow do we get the posterior? Going back to the equation:\n\nThe first step, then, is to multiply the likelihood (P(Data|θ)) and the prior (P(θ)):\n\nWe can see how, if we were to sum the probability densities in the unnormalized posterior, we would have a value that is different than 1:\n\nNote that we still have ‘P(Data)’ to contend with, hence why I call this the ‘unnormalized_posterior’.\n\nRemember from above that:\n\nWe can simply compute that integral and make that final division and there we go: we get the posterior. That being said, it’s nice to think a bit more about why we divide by P(Data). Note that calculating P(Data) is the same as finding the area between the unnormalized posterior and the x-axis on the graph above. Again, because the posterior is a probability distribution, it must be the case that the area bounded by the posterior pdf sums to 1. To make sure that this is the case, we have to find out what the current area under the curve is, and then we divide each data point by that number. This then gives us the normalized version!\n\nAlso, note that it would be very challenging to compute the integral analytically. Instead, we can rely on an approximation using scipy and the ‘trapezium’ method that needs only the values for the x-axis and y-axis to make an estimate of the area. This is the implementation I used below:\n\nCan we verify that this new graph represents a valid PDF? Remember that for that to be the case, the area under the graph must sum to 1. We can quickly compute that value to check:\n\nNote that so far, our Bayesian inference went through the following steps:\n• Using Bayes’ theorem to find the posterior distribution (and using the trapezium rule to normalize the posterior).\n\nIn practice, we don’t stop there: observing one datum is probably not enough for us to have a high degree of confidence in our model! The idea behind Bayesian inference is that it is iterative and that every new observation makes us better informed about the ‘true’ distribution of our variable of interest.\n\nHow do we move forward, then? The next steps are to:\n• Take the posterior we just found, and have that become our new prior!\n• Using the same old likelihood function, evaluate the likelihood of our different hypotheses given this newly observed data point.\n• Multiply our new prior and likelihood values to get the unnormalized posterior. Use the ‘trapezium’ rule to normalize the posterior.\n\nWe then repeat steps 1 through 4 for however many data points we have! To showcase this process, let me first generate 1000 new data points:\n\nSee how, as we get more and more data, our model gets closer and closer to the ‘truth’? In other words, our model’s mean value converges to the real value of μ, whose true value is 1.7m, and the uncertainty about our guess (i.e. the standard deviation of our distribution) shrinks! This means that more data = more accurate AND precise guesses.\n\nBelow, we can see how the increased number of data leads to a predicted mean μ that is ever closer to the ‘true’ value of μ, 1.7m.\n\nNotice from the code that I used a cumulative trapezium function to find the mean of the distribution output by our posterior. I’ll leave it to the reader to re-create the code and investigate why and how this makes sense!\n\nAt this stage, all that is left for us to do is to get the final predicted mean of our model (the one with the most data) and take that to be our ‘true’ model. After observing data from 1000 students, we take the final mean and have that as the mean of our model. Furthermore, we can get the 99% confidence interval as well, which helps us understand how ‘wrong’ our predicted mean can be.\n\nNow that we have our improved model, we can use it to make Predictions! Based on the final model we arrived at, our model is specified as:\n\nWe can now use this model to answer potentially interesting business-related questions! For example:\n\nHow many students can we expect to have more than 1.75m in a class of 100 students?\n\nUsing our distribution, we can answer the question in two ways: the first is the analytical way. We can use the cumulative density function for the normal distribution to find how much of the density is below 1.75m, and then subtract that value from 1 to obtain the density that is above 1.75m:\n\nThis indicates that there is about a 30% chance that a student will be taller than 1.75m. In a class of 100 students, this means that we expect 30 students to be taller than 1.75m. The other way we might answer this question is through a simulation. We can use our model to sample 100 students, and count how many are taller than 1.75m:\n\nTrue Bayesian statisticians rarely simulate anything only once, however. We want to be able to capture our uncertainty and variance. Therefore, we can perform the above simulation one hundred times and plot that distribution:\n\nWe can also find the 99% confidence interval for our assertions given the simulation, which is an important advantage of this method:\n\nWith our simulated mean as well as our bounds, we can now have a really good idea of how many students we might expect that have more than 1.75m. If we needed to know that information in order to produce school uniforms for sale, for instance, we could take a conservative approach and produce 40 large-sized uniforms, and we would expect that 99% of the time we would have enough uniforms to sell. This helps us know how much redundancy we might want to have. Whereas producing 30 large uniforms might leave no room for error, a more interesting question is: how much room for error should we leave? With the help of Bayesian Statistics and Bayesian inference, we can find a very compelling answer to questions of that type!"
    },
    {
        "link": "https://pymc.io/projects/examples/en/latest/causal_inference/bayesian_ab_testing_introduction.html",
        "document": ""
    },
    {
        "link": "http://varianceexplained.org/r/empirical_bayes_baseball",
        "document": "Which of these two proportions is higher: 4 out of 10, or 300 out of 1000? This sounds like a silly question. Obviously \\(4/10=.4\\), which is greater than \\(300/1000=.3\\).\n\nBut suppose you were a baseball recruiter, trying to decide which of two potential players is a better batter based on how many hits they get. One has achieved 4 hits in 10 chances, the other 300 hits in 1000 chances. While the first player has a higher proportion of hits, it’s not a lot of evidence: a typical player tends to achieve a hit around 27% of the time, and this player’s \\(4/10\\) could be due to luck. The second player, on the other hand, has a lot of evidence that he’s an above-average batter.\n\nThis post isn’t really about baseball, I’m just using it as an illustrative example. (I actually know very little about sabermetrics. If you want a more technical version of this post, check out this great paper). This post is, rather, about a very useful statistical method for estimating a large number of proportions, called empirical Bayes estimation. It’s to help you with data that looks like this:\n\nA lot of data takes the form of these success/total counts, where you want to estimate a “proportion of success” for each instance. Each row might represent:\n• An ad you’re running: Which of your ads have higher clickthrough rates, and which have lower? (Note that I’m not talking about running an A/B test comparing two options, but rather about ranking and analyzing a large list of choices.)\n• A user on your site: In my work at Stack Overflow, I might look at what fraction of a user’s visits are to Javascript questions, to guess whether they are a web developer. In another application, you might consider how often a user decides to read an article they browse over, or to purchase a product they’ve clicked on.\n\nWhen you work with pairs of successes/totals like this, you tend to get tripped up by the uncertainty in low counts. \\(1/2\\) does not mean the same thing as \\(50/100\\); nor does \\(0/1\\) mean the same thing as \\(0/1000\\). One approach is to filter out all cases that don’t meet some minimum, but this isn’t always an option: you’re throwing away useful information.\n\nI previously wrote a post about one approach to this problem, using the same analogy: Understanding the beta distribution (using baseball statistics). Using the beta distribution to represent your prior expectations, and updating based on the new evidence, can help make your estimate more accurate and practical. Now I’ll demonstrate the related method of empirical Bayes estimation, where the beta distribution is used to improve a large set of estimates. What’s great about this method is that as long as you have a lot of examples, you don’t need to bring in prior expectations.\n\nHere I’ll apply empirical Bayes estimation to a baseball dataset, with the goal of improving our estimate of each player’s batting average. I’ll focus on the intuition of this approach, but will also show the R code for running this analysis yourself. (So that the post doesn’t get cluttered, I don’t show the code for the graphs and tables, only the estimation itself. But you can find all this post’s code here).\n\nIn my original post about the beta distribution, I made some vague guesses about the distribution of batting averages across history, but here we’ll work with real data. We’ll use the dataset from the excellent Lahman package. We’ll prepare and clean the data a little first, using dplyr and tidyr:\n\nAbove, we filtered out pitchers (generally the weakest batters, who should be analyzed separately). We summarized each player across multiple years to get their career Hits (H) and At Bats (AB), and batting average. Finally, we added first and last names to the dataset, so we could work with them rather than an identifier:\n\nI wonder who the best batters in history were. Well, here are the ones with the highest batting average:\n\nErr, that’s not really what I was looking for. These aren’t the best batters, they’re just the batters who went up once or twice and got lucky. How about the worst batters?\n\nAlso not what I was looking for. That “average” is a really crummy estimate. Let’s make a better one.\n\nStep 1: Estimate a prior from all your data\n\nLet’s look at the distribution of batting averages across players.\n\n(For the sake of estimating the prior distribution, I’ve filtered out all players that have fewer than 500 at-bats, since we’ll get a better estimate from the less noisy cases. I show a more principled approach in the Appendix).\n\nThe first step of empirical Bayes estimation is to estimate a beta prior using this data. Estimating priors from the data you’re currently analyzing is not the typical Bayesian approach- usually you decide on your priors ahead of time. There’s a lot of debate and discussion about when and where it’s appropriate to use empirical Bayesian methods, but it basically comes down to how many observations we have: if we have a lot, we can get a good estimate that doesn’t depend much on any one individual. Empirical Bayes is an approximation to more exact Bayesian methods- and with the amount of data we have, it’s a very good approximation.\n\nSo far, a beta distribution looks like a pretty appropriate choice based on the above histogram. (What would make it a bad choice? Well, suppose the histogram had two peaks, or three, instead of one. Then we might need a mixture of Betas, or an even more complicated model). So we know we want to fit the following model:\n\nWe just need to pick \\(\\alpha_0\\) and \\(\\beta_0\\), which we call “hyper-parameters” of our model. There are many methods in R for fitting a probability distribution to data ( , , , etc). You don’t even have to use maximum likelihood: you could use the mean and variance, called the “method of moments”. But we’ll use the fitdistr function from MASS.\n\nThis comes up with \\(\\alpha_0=78.661\\) and \\(\\beta_0=224.875\\). How well does this fit the data?\n\nNot bad! Not perfect, but something we can work with.\n\nStep 2: Use that distribution as a prior for each individual estimate\n\nNow when we look at any individual to estimate their batting average, we’ll start with our overall prior, and update based on the individual evidence. I went over this process in detail in the original Beta distribution post: it’s as simple as adding \\(\\alpha_0\\) to the number of hits, and \\(\\alpha_0 + \\beta_0\\) to the total number of at-bats.\n\nFor example, consider our hypothetical batter from the introduction that went up 1000 times, and got 300 hits. We would estimate his batting average as:\n\nHow about the batter who went up only 10 times, and got 4 hits. We would estimate his batting average as:\n\nThus, even though \\(\\frac{4}{10}>\\frac{300}{1000}\\), we would guess that the \\(\\frac{300}{1000}\\) batter is better than the \\(\\frac{4}{10}\\) batter!\n\nPerforming this calculation for all the batters is simple enough:\n\nNow we can ask: who are the best batters by this improved estimate?\n\nWho are the worst batters?\n\nNotice that in each of these cases, empirical Bayes didn’t simply pick the players who had 1 or 2 at-bats. It found players who batted well, or poorly, across a long career. What a load off our minds: we can start using these empirical Bayes estimates in downstream analyses and algorithms, and not worry that we’re accidentally letting \\(0/1\\) or \\(1/1\\) cases ruin everything.\n\nOverall, let’s see how empirical Bayes changed all of the batting average estimates:\n\nThe horizontal dashed red line marks \\(y=\\frac{\\alpha_0}{\\alpha_0 + \\beta_0}=0.259\\)- that’s what we would guess someone’s batting average was if we had no evidence at all. Notice that points above that line tend to move down towards it, while points below it move up.\n\nThe diagonal red line marks \\(x=y\\). Points that lie close to it are the ones that didn’t get shrunk at all by empirical Bayes. Notice that they’re the ones with the highest number of at-bats (the brightest blue): they have enough evidence that we’re willing to believe the naive batting average estimate.\n\nThis is why this process is sometimes called shrinkage: we’ve moved all our estimates towards the average. How much it moves these estimates depends on how much evidence we have: if we have very little evidence (4 hits out of 10) we move it a lot, if we have a lot of evidence (300 hits out of 1000) we move it only a little. That’s shrinkage in a nutshell: Extraordinary outliers require extraordinary evidence.\n\nConclusion: So easy it feels like cheating\n\nRecall that there were two steps in empirical Bayes estimation:\n• Estimate the overall distribution of your data.\n• Use that distribution as your prior for estimating each average.\n\nStep 1 can be done once, “offline”- analyze all your data and come up with some estimates of your overall distribution. Step 2 is done for each new observation you’re considering. You might be estimating the success of a post or an ad, or classifying the behavior of a user in terms of how often they make a particular choice.\n\nAnd because we’re using the beta and the binomial, consider how easy that second step is. All we did was add one number to the successes, and add another number to the total. You can build that into your production system with a single line of code that takes nanoseconds to run.\n\nThat really is so simple that it feels like cheating- like the kind of “fudge factor” you might throw into your code, with the intention of coming back to it later to do some real Machine Learning.\n\nI bring this up to disprove the notion that statistical sophistication necessarily means dealing with complicated, burdensome algorithms. This Bayesian approach is based on sound principles, but it’s still easy to implement. Conversely, next time you think “I only have time to implement a dumb hack,” remember that you can use methods like these: it’s a way to choose your fudge factor. Some dumb hacks are better than others!\n\nBut when anyone asks what you did, remember to call it “empirical Bayesian shrinkage towards a Beta prior.” We statisticians have to keep up appearances.\n\nAppendix: How could we make this more complicated?\n\nWe’ve made some enormous simplifications in this post. For one thing, we assumed all batting averages are drawn from a single distribution. In reality, we’d expect that it depends on some known factors. For instance, the distribution of batting averages has changed over time:\n\nIdeally, we’d want to estimate a different Beta prior for each decade. Similarly, we could estimate separate priors for each team, a separate prior for pitchers, and so on. One useful approach to this is Bayesian hierarchical modeling (as used in, for example, this study of SAT scores across different schools).\n\nAlso, as alluded to above, we shouldn’t be estimating the distribution of batting averages using only the ones with more than 500 at-bats. Really, we should use all of our data to estimate the distribution, but give more consideration to the players with a higher number of at-bats. This can be done by fitting a beta-binomial distribution. For instance, we can use the dbetabinom.ab function from VGAM, and the function:\n\nWe end up getting almost the same prior, which is reassuring!"
    },
    {
        "link": "https://docs.scipy.org/doc/scipy/reference/stats.html",
        "document": "This module contains a large number of probability distributions, summary and frequency statistics, correlation functions and statistical tests, masked statistics, kernel density estimation, quasi-Monte Carlo functionality, and more.\n\nStatistics is a very large area, and there are topics that are out of scope for SciPy and are covered by other packages. Some of the most important ones are:\n• None statsmodels: regression, linear models, time series analysis, extensions to topics also covered by .\n\nCompute several descriptive statistics of the passed array. Compute the weighted geometric mean along the specified axis. Calculate the weighted harmonic mean along the specified axis. Calculate the weighted power mean along the specified axis. Compute the kurtosis (Fisher or Pearson) of a dataset. Return an array of the modal (most common) value in the passed array. Calculate the nth moment about the mean for a sample. Compute the expectile at the specified level. Return the n th k-statistic ( so far). Return an unbiased estimator of the variance of the k-statistic. Compute the trimmed standard error of the mean. Return mean of array after trimming a specified fraction of extreme values Calculate the geometric standard deviation of an array. Compute the interquartile range of the data along the specified axis. Compute standard error of the mean. Bayesian confidence intervals for the mean, var, and std. 'Frozen' distributions for mean, variance, and standard deviation of data. Calculate the Shannon entropy/relative entropy of given distribution(s). Given a sample of a distribution, estimate the differential entropy. Compute the median absolute deviation of the data along the given axis.\n\nSciPy has many functions for performing hypothesis tests that return a test statistic and a p-value, and several of them return confidence intervals and/or other related information. The headings below are based on common uses of the functions within, but due to the wide variety of statistical procedures, any attempt at coarse-grained categorization will be imperfect. Also, note that tests within the same heading are not interchangeable in general (e.g. many have different distributional assumptions). One sample tests are typically used to assess whether a single sample was drawn from a specified distribution or a distribution with specified properties (e.g. zero mean). Calculate the T-test for the mean of ONE group of scores. Perform a test that the probability of success is p. Perform a quantile test and compute a confidence interval of the quantile. Test whether the skew is different from the normal distribution. Perform the Jarque-Bera goodness of fit test on sample data. Anderson-Darling test for data coming from a particular distribution. Perform the one-sample Cramér-von Mises test for goodness of fit. Performs the one-sample Kolmogorov-Smirnov test for goodness of fit. Paired sample tests are often used to assess whether two samples were drawn from the same distribution; they differ from the independent sample tests below in that each observation in one sample is treated as paired with a closely-related observation in the other sample (e.g. when environmental factors are controlled between observations within a pair but not among pairs). They can also be interpreted or used as one-sample tests (e.g. tests on the mean or median of differences between paired observations). Calculate the t-test on TWO RELATED samples of scores, a and b. These tests are often used to assess whether there is a relationship (e.g. linear) between paired observations in multiple samples or among the coordinates of multivariate observations. Calculate a linear least-squares regression for two sets of measurements. Compute the xi correlation and perform a test of independence Computes the Siegel estimator for a set of points (x, y). Computes the Theil-Sen estimator for a set of points (x, y). Perform Page's Test, a measure of trend in observations between treatments. These association tests and are to work with samples in the form of contingency tables. Supporting functions are available in . Chi-square test of independence of variables in a contingency table. Independent sample tests are typically used to assess whether multiple samples were independently drawn from the same distribution or different distributions with a shared property (e.g. equal means). Some tests are specifically for comparing two samples. T-test for means of two independent samples from descriptive statistics. Calculate the T-test for the means of two independent samples of scores. Perform the Mann-Whitney U rank test on two independent samples. Perform the Baumgartner-Weiss-Schindler test on two independent samples. Compute the Wilcoxon rank-sum statistic for two samples. Compute the Brunner-Munzel test on samples x and y. Perform the two-sample Cramér-von Mises test for goodness of fit. Performs the two-sample Kolmogorov-Smirnov test for goodness of fit. Performs the (one-sample or two-sample) Kolmogorov-Smirnov test for goodness of fit. Others are generalized to multiple samples. Perform Tukey's HSD test for equality of means over multiple treatments. The following functions can reproduce the p-value and confidence interval results of most of the functions above, and often produce accurate results in a wider variety of conditions. They can also be used to perform hypothesis tests and generate confidence intervals for custom statistics. This flexibility comes at the cost of greater computational requirements and stochastic results. Performs a permutation test of a given statistic on provided data. Simulate the power of a hypothesis test under an alternative hypothesis. Instances of the following object can be passed into some hypothesis test functions to perform a resampling or Monte Carlo version of the hypothesis test. These functions are for assessing the results of individual tests as a whole. Functions for performing specific multiple hypothesis tests (e.g. post hoc tests) are listed above. Combine p-values from independent tests that bear upon the same hypothesis. The following functions are related to the tests above but do not belong in the above categories."
    },
    {
        "link": "https://analyticsvidhya.com/blog/2022/04/bayesian-approach-to-regression-analysis-with-python",
        "document": "This article was published as a part of the Data Science Blogathon.\n\nThe entire world of statistical rigour is split into two parts one is a Bayesian approach and the other is a frequentist. And it has been a moot point for statisticians throughout the century which one is better than the other. It is safe to say that the century has been dominated by frequentist statistics. Many algorithms that we use now namely least square regression, and logistic regression are a part of the frequentist approach to statistics. Where the parameters are always constant and predictor variables are random. While in a Bayesian world the predictors are treated as constants while the parameter estimates are random and each of them follows a distribution with some mean and variance. It gives us an extra layer of interpretability as the output is not any more a single point estimate but rather a distribution. This fundamentally separates Bayesian statistical inference from frequentist inference for statistical analysis. But there is a big trade-off as nothing comes free in this vain world which is one of the major reasons for its limited practical use.\n\nSo, in this article, we are going to explore the Bayesian method of regression analysis and see how it compares and contrasts with our good old Ordinary Least Square (OLS) regression. So, before delving into Bayesian rigour let’s have a brief primer on frequentist Least Square Regression.\n\nThis is the most basic and most popular form of linear regression that you are already accustomed to and yes this uses a frequentist approach for parameter estimation. The model assumes the predictor variables are random samples and with a linear combination of them we finally predict the response variable as a single point estimate. And to account for random sampling we have a residual term that explains the unexplained variance of the model. And this residual term follows a normal distribution with a mean of 0 and a standard deviation of sigma. The hypothesis equation for OLS is given by,\n\nHere the beta is the intercept term and beta are the model parameters, x1, x2 and so on are predictor variables and epsilon is the residual error term.\n\nThese coefficients show the effect on the response variable for a unit change in predictor variables. For example, if beta1 is m then the Y will increase by m for every unit increase in x1 provided the rest of the coefficients are zero. And the intercept term is the value of response f(x) when the rest of the coefficients are zero.\n\nWe can also generalize the above equation for multiple variables using a matrix of feature vectors. In this way, we will be finding the best fitting multidimensional hyperplane instead of a single line as is the case with most real-world data.\n\nThe entire goal of Least Square Regression is to find out the optimal value of the coefficients i.e. beta that minimises measurement of error. And makes reasonably good predictions on unseen data. And we can achieve this by minimising the residual sum of squares. It is the sum of the squares of the difference between the output and linear regression estimates. Given by\n\nThis is our cost function. The Maximum Likelihood Estimates for the beta that minimises the residual sum of squares (RSS) is given by\n\nThe focal point of everything till now is that in frequentist linear regression beta^ is a point estimate as opposed to the Bayesian approach where the outcomes are distributions.\n\nBefore getting to the Bayesian Regression part let’s get familiar with the Bayes principle and know-how it does what it does? I suppose we are all a little familiar with the Bayes theorem. Let’s have a look at it.\n\nThe equation is given as\n\nPrior probability, in Bayesian statistical inference, is the probability of an event occurring before new data is collected. In other words, it represents the best rational assessment of the probability of a particular outcome based on current knowledge before an experiment is performed.\n\nThe posterior probability is the revised probability of an event occurring after taking into consideration the new information. The posterior probability is calculated by updating the prior probability using Bayes’ theorem. In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred.\n\nTo sum it up the Bayesian framework has three basic tenets. These basic tenets outline the entire structure of Bayesian Frameworks.\n• Belief: Belief about the world includes Prior and Likelihood\n• Model: Collect data and use probability to update the model.\n• Update: Update your belief as per your model\n\nLet’s try to understand this through a simple example.\n\nWe are in a college a measure the average height of male students. Now to make the Bayesian work we need a prior assumption regarding the process at hand. This is probably one of the best things in Bayesian Frameworks that it leaves enough room for one’s own belief, making it more intuitive in general. So to find the prior we must have some knowledge regarding the experiment. In our example, we will use let’s say the average height of males in that country. Let it be 67.73 inches with a standard deviation of 7. The distribution will look something like this\n\nWe centred the distribution at 67.73 with a standard deviation of 7. Now, we asked around and some good guys volunteered us and gave us their input. The first guy’s height was 76.11 inches. Now that we have new data we will update the posterior distribution.\n\nYou can see the plot has shifted to the right as was obvious. We will now move on and collect some more data and update our posterior distribution accordingly.\n\nWith more data, the posterior distribution starts to shrink in size as the variance of the distributions reduces. And it is quite similar to the way we experience things with more information at our disposal regarding a particular event we tend to make fewer mistakes or the likelihood of getting it right improves. Our brain works just as Bayesian Updating.\n\nIn a frequentist setting, the same problem could have been approached differently or we can say rather straightforward as we will only need to calculate the mean or median and desired confidence interval of our estimate. So, the question is why would someone go the extra mile to calculate these prior, posterior distributions and not just calculate the direct estimates? The sole reason for this is the increased interpretability of the model we now have a whole distribution of the quantity of interest rather than an estimate. So, now we can directly say about the probability of our estimates. But in a frequentist approach, the best we could do is to find a confidence interval of our estimates and these two terms, however similar they sound, are fundamentally different from each other.\n\nA Confidence Interval is a measure of uncertainty around the true estimate. It is the combination of values above and below the true parameter which suggests that for a 95% confidence interval if we draw n samples from the population then 95% of the time the true (unknown) parameter will be captured by the said interval. But we cannot say that there is a 95% probability that the true parameter lies in that particular interval.\n\nBut with a Bayesian posterior distribution, we can very well predict the probability of true parameter within an interval and this is called Credible Interval. And it can do that because the parameters are considered random in the Bayesian framework. And this thing is very powerful for any kind of research analysis.\n\nWe just learned how the entire thought process that goes behind the Bayesian approach is fundamentally different from that of the Frequentist approach. As we saw in the previous section OLS estimates a single value for a parameter that minimises the cost function, but in a Bayesian world instead of single values, we will estimate entire distributions for each parameter individually. That means every parameter is a random value sampled from a distribution with a mean and variance. The standard syntax for Bayesian Linear Regression is given by\n\nHere, as you can see the response variable is not anymore a point estimate but a normal distribution with a mean 𝛽TX and variance sigma2I, where 𝛽TX is the general linear equation in X and I is the identity matrix to account for the multivariate nature of the distribution.\n\nBayesian calculations more often than not are tough, and cumbersome. It takes far more resources to do a Bayesian regression than a Linear one. Thankfully we have libraries that take care of this complexity. For this article, we will be using the PyMC3 library for calculation and Arviz for visualizations. So, let’s get started.\n\nSo, far we learned the workings of Bayesian updating. Now. in this section we are going to work out a simple Bayesian analysis of a dataset. For this article, to keep things straight forward we are going to use the height-weight dataset. This is a fairly simple dataset and here we will be using weight as the response variable and height as a predictor variable.\n\nSo, before going full throttle at it let’s get familiar with the PyMC3 library. This powerful Probabilistic Programming Framework was designed to incorporate Bayesian techniques in data analysis processes. PyMC3 provides Generalized Linear Modules(GLM) to extend the functionalities of OLS to other regression techniques such as Logistic Regression, Poisson Regression etc. The response or outcome variable is related to predictor variables via a link function. And its usability is not limited to normal distribution but can be extended to any distribution from the ”exponential family“.\n\nNow we are all set for the real deal.\n\nFor dealing with data we will be using Pandas and Numpy, Bayesian modelling will be aided by PyMC3 and for visualizations, we will be using seaborn, matplotlib and arviz. Arviz is a dedicated library for Bayesian Exploratory Data Analysis. Which has a lot of tools for many statistical visualizations.\n\nAs I said earlier we will be using a simple Height-Weight dataset.\n\nScaling data is always deemed a good practice. We do not want our data to wander, scaling data will help contain data within a small boundary while not losing its original properties. More often than not variation in the outcome and predictors are quite high and samplers used in the GLM module might not perform as intended so it’s good practice to scale the data and it does no harm anyway.\n\nWe will use these estimates later on.\n\nPyMC3 uses GLM (Generalized Linear Models) to operate on data. It is required to specify a model with a “with” context manager. We will use MAP (Maximum A Posteriori) as a starting point for MCMC and finally will use the No-U-Turn sampler to calculate the trace. The idea behind MCMC is that Bayesian Models become intractable in high dimensional space an efficient search method is necessary to carry out sampling that is how we got MCMC. It has a collection of algorithms that are used for sampling variables.\n\nThis will return a trace object. A tracing object is nothing but a collection of dictionaries consisting of posterior predicted values. The values that are used to construct the ultimate posterior distributions.\n\nThe start gives the starting point for MCMC sampling. The MAP estimates the most common value as the point estimate which is also the mean for a normal distribution. The interesting part is the value is the same as the Maximum Likelihood Estimate.\n\nNext up we calculate the posterior predictive estimates. And this is not the same as the posterior we saw earlier. The posterior distribution “is the distribution of an unknown quantity, treated as a random variable, conditional on the evidence obtained” (Wikipedia). It’s the distribution that explains your unknown, random, parameter. While the posterior predictive distribution is the distribution of possible unobserved values conditional on the observed values (Wikipedia).\n\nA lot of arviz methods work fine with trace objects while a number of them do not. In future releases PyMC3 most likely will return inference objects. We will also include our posterior predictive estimates in our final inference object.\n\nOn the left side of the plot, we can observe the posterior plots of our parameters and distribution for standard deviation. Interestingly the mean of the parameters is almost the same as that of the Linear Regression we saw earlier.\n\nNow if you observe the means of intercept and variable x are almost the same as what we estimated from frequentist Linear Regression. Here, HDI stands for Highest Probability Density, which means any point within that boundary will have more probability than points outside.\n\nWe will now visualize all the linear plots for the posterior parameter and frequentist linear regression line. And if you were wondering why we renamed our variables to x and y is because of this plot as post_predictive_glm() has it hardcoded that any variable named other than x when passed will throw a keyword error. So, this was a workaround.\n\nThe OLS line is pretty much at the middle of all the line plots from the posterior distribution.\n\nWhere Do We Use It?\n\nSo, far so good. We learnt the basics of Bayesian Linear Regression. But where in the real world this can be implemented? Well, there could be numerous such instances where we can use Bayesian Linear Regression. In any such situation where we have limited historical data regarding some events, we can incorporate the prior data into the model to get better results.\n\nFor example, a supermarket wishes to launch a new product line this holiday season and the management intends to know the overall sales forecast of the particular product line. In a situation like this where we do not have enough historical data to efficiently forecast sales, we can always incorporate the average effect of a similar product line as the prior for the new products’ holiday season effects. And the prior will get updated upon seeing new data. So, we will get a reasonable Holiday sale forecast for the new product line.\n\nIn hierarchical modelling Bayesian regression is used where we need to account for the individual as well as group effect of the variables. For example, modelling the SAT scores of students from different schools. Here, the outcomes depend both on individual variables (students) as well as the school level variables(environment, socio-economics etc). In cases like this, we can use the concept of hierarchical priors to account for both individual and group effects. More reading on this can be found here.\n\nAnd one of the added advantages of Bayesian is that it does not require regularization. The prior itself work as a regularizer. When we take a prior with a tight distribution or a small standard deviation it signals we have a strong belief in the prior. And it takes a lot of data to shift the distribution away from prior parameters. It does the work of a regularizer in the frequentists approach though they are implemented differently (one through optimisation and the other through sampling). This reduces the hassle of using extra regularization parameters for over parameterized models.\n\nBefore winding up we must discuss the pros and cons of the Bayesian Approach and when should we use it and when should not.\n• Being able to incorporate prior beliefs\n• Being able to say the percentage probability of any point in the posterior distribution\n• Can be used for Hierarchical or multilevel models\n• Slower compared to frequentist. where performance and speed are essential Bayesian does a pretty not good job.\n• May not perform as desired in high dimensional space\n\nThroughout the article, we explored the . The key thing to notice here is that in OLS we will only be able to find the point estimate, a single value for each coefficient while the only term random is the residual term. While in Bayesian Approach we are getting individual distributions for each predictor variable. Which subsequently enables us to make better judgements. We can predict the percentage probability of an estimate which is very powerful and not there in the frequentist statistics.\n\nThe media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion."
    },
    {
        "link": "https://stackoverflow.com/questions/54894038/exception-handling-over-multiple-calling-levels",
        "document": "If you don't catch an exception, it bubbles up the call stack until someone does. If no one catches it, the runtime will get it and die with the exception error message and a full traceback. IOW, you don't have to explicitely catch and reraise your exception everywhere - which would actually defeat the whole point of having exceptions. Actually, despite being primarily used for errors / unexpected conditions, exceptions are first and foremost a control flow tool allowing to break out of the normal execution flow and pass control (and some informations) to any arbitrary place up in the call stack.\n\nFrom this POV your code seems mostlt correct (caveat: I didn't bother reading the whole thing, just had a quick look), except (no pun indented) for a couple points:\n\nFirst, you should define your own specific exception class(es) instead of using the builtin ValueError (you can inherit from it if it makes sense to you) so you're sure you only catch the exact exceptions you expect (quite a few layers \"under\" your own code could raise a ValueError that you didn't expect).\n\nThen, you may (or not, depending on how your code is used) also want to add a catch-all top-level handler in your function so you can properly log (using the module) all errors and eventually free resources, do some cleanup etc before your process dies.\n\nAs a side note, you may also want to learn and use proper string formatting, and - if perfs are an issue at least -, avoid duplicate constant calls like this:\n\nGiven Python's very dynamic nature, neither the compiler nor runtime can safely optimize those repeated calls (the method could have been dynamically redefined between calls), so you have to do it yourself.\n\nYou can easily check that it works correctly with a simple MCVE:\n\nUhu... Even embedded, the mechanism expection should still work as expected - at least for the part of the call stack that depends on your function (can't tell what happens upper in the call stack). But given how MySQL treats errors (what about having your data silently truncated ?), I wouldn't be specially suprised if they hacked the runtime to silently pass any error in plugins code xD"
    },
    {
        "link": "https://stackoverflow.com/questions/44718757/in-python-3-4-how-to-hand-over-multiple-arguments-to-constraint-function-in-mini",
        "document": "I would like to minimize the following function\n\nwhere and are some simple helper function for certain calculations. I have the following constraints:\n\nwhere the dictionary of additional arguments\n\nHowever, when I try to run the following\n\nI get the error (in pdb): . But everything is defined. If you print the dictionaries and variables all have a certain value. What is going wrong here?"
    },
    {
        "link": "https://reddit.com/r/Python/comments/mdotpg/exceptions_are_a_common_way_of_dealing_with",
        "document": "The official Python community for Reddit! Stay up to date with the latest news, packages, and meta information relating to the Python programming language. --- If you have questions or are new to Python use r/LearnPython"
    },
    {
        "link": "https://blog.miguelgrinberg.com/post/the-ultimate-guide-to-error-handling-in-python",
        "document": "I often come across developers who know the mechanics of Python error handling well, yet when I review their code I find it to be far from good. Exceptions in Python is one of those areas that have a surface layer that most people know, and a deeper, almost secret one that a lot of developers don't even know exists. If you want to test yourself on this topic, see if you can answer the following questions:\n• When should you catch exceptions raised by functions you call, and when should you not?\n• How can you know what exception classes to catch?\n• When you catch an exception, what should you do to \"handle\" it?\n• Why is catching all exceptions considered a bad practice, and when is it okay to do it?\n\nAre you ready to learn the secrets of error handling in Python? Let's go!\n\nThe Basics: Two Paths to Error Handling in Python\n\nI'm going to start with something that I believe many of my readers already know or have seen discussed elsewhere. In Python, there are two main styles of writing error handling code, often called by their unpronounceable acronyms of \"LBYL\" and \"EAFP\". Are you familiar with these? In case you are not, below is a quick introduction to them.\n\nLook Before You Leap (LBYL)\n\nThe \"look before you leap\" pattern for error handling says that you should check that the conditions for performing an action that can fail are proper before triggering the action itself.\n\nConsider as an example the task of deleting a file from disk. Using, LBYL this could be coded as follows:\n\nWhile as a first impression it may appear that this code is fairly robust, in practice it isn't.\n\nThe main problem here is that we need to know all the possible things that can go wrong with deleting a file so that we can check for them before we make the call. It is obvious that the file must exist, but a missing file isn't the only reason why a deletion can fail. Here are just a few other reasons why a file may fail to delete:\n• The path could be of a directory instead of a file\n• The file could be owned by a different user than the one attempting the deletion\n• The file could have read-only permissions\n• The disk on which the file is stored could be mounted as a read-only volume\n• The file could be locked by another process, a common annoyance on Microsoft Windows\n\nHow would the delete file example above look if we had to add checks for all these as well?\n\nAs you see, it is quite difficult to write robust logic using LBYL, because you have to know all the possible ways in which the functions that you call can fail, and sometimes there are just too many.\n\nAnother problem when using the LBYL pattern is that of race conditions. If you check for the failure conditions, and then execute the action, it is always possible for the conditions to change in the small window of time between when the checks were made and when the action was executed.\n\nEasier to Ask Forgiveness than Permission (EAFP)\n\nI'm sure you realize that I don't have a very high opinion of the LBYL pattern (but in fact it is useful in some situations, as you will see later). The competing pattern says that it is \"easier to ask forgiveness than permission\". What does this mean? It means you should perform the action, and deal with any errors afterwards.\n\nIn Python, EAFP is best implemented using exceptions:\n\nHere is how to delete a file using EAFP:\n\nI hope you agree that in most cases EAFP is preferable to LBYL.\n\nIt is a big improvement that with this pattern the target function is tasked with checking and reporting errors, so we as callers can make the call and trust that the function will let us know if the action failed.\n\nOn the other side, we need to know what exceptions to write down in the clause, because any exception classes that we miss are going to bubble up and potentially cause the Python application to crash. For a file deletion it is safe to assume that any errors that are raised are going to be or one of its subclasses, but in other cases knowing what exceptions a function could raise requires looking at documentation or source code.\n\nYou may ask why not catch all possible exceptions to make sure none are missed. This is a bad pattern that causes more problems than it solves, so I do not recommend it except in a few very specific cases that I will discuss later. The problem is that usually bugs in your own code manifest themselves as unexpected exceptions. If you are catching and silencing all exceptions every time you call a function, you are likely to miss the exceptions that shouldn't have occurred, the ones that were caused by bugs that need to be fixed.\n\nTo avoid the risk of missing application bugs that manifest as unexpected exceptions, you should always catch the smallest possible list of exception classes, and when it makes sense, don't catch any exceptions at all. Hold on to the thought of not catching exceptions as an error handling strategy. It may sound like a contradiction, but it isn't. I will come back to this.\n\nUnfortunately the traditional error handling knowledge doesn't go very far. You can have a complete understanding of LBYL and EAFP and know how and work by heart, and still, many times you may not know what to do or feel that the way you write error handling code could be better.\n\nSo now we are going to look at errors in a completely different way that is centered around the errors themselves, and not so much on the techniques to deal with them. I hope this is going to make it much easier for you to know what to do.\n\nFirst, we need to classify the error based on its origin. There are two types to consider:\n• Your code found a problem and needs to generate an error. I'll call this type a \"new error\".\n• Your code received an error from a function it called. I'll call this one a \"bubbled-up error\".\n\nWhen it comes down to it, these are really the two situations in which errors may come to exist, right? You either need to introduce a new error yourself and put it in the system for some other part of the application to handle, or you received an error from somewhere else and need to decide what to do with it.\n\nIn case you are not familiar with the expression \"bubbled-up\", this is an attribute of exceptions. When a piece of code raises an exception, the caller of the errored function gets a chance to catch the exception in a / block. If the caller doesn't catch it, then the exception is offered to the next caller up the call stack, and this continues until some code decides to catch the exception and handle it. When the exception travels towards the top of the call stack it is said to be \"bubbling up\". If the exception isn't caught and bubbles up all the way to the top, then Python will interrupt the application, and this is when you see a stack trace with all the levels through which the error traveled, a very useful debugging aid.\n\nAside from the error being new or bubbled-up, you need to decide if it is recoverable or not. A recoverable error is an error that the code dealing with it can correct before continuing. For example, if a piece of code tries to delete a file and finds that the file does not exist, it's not a big deal, it can just ignore the error and continue.\n\nA non-recoverable error is an error that the code in question cannot correct, or in other words, an error that makes it impossible for the code at this level to continue running. As an example, consider a function that needs to read some data from the database, modify it and save it back. If the reading fails, the function has to abort early, since it cannot do the rest of the work.\n\nNow you have an easy way to categorize an error based on its origin and its recoverable status, resulting in just four different error configurations that you need to know how to handle. In the following sections I will tell you exactly what you need to do for each of these four error types!\n\nThis is an easy case. You have a piece of code in your own application that found an error condition. Luckily this code is able to recover from this error itself and continue.\n\nWhat do you think is the best way to handle this case? Well, recover from the error and continue, without bothering anyone else!\n\nLet's look at an example:\n\nHere we have a function that writes a song to a database. Let's say that in the database schema the song's year cannot be null.\n\nUsing ideas from the LBYL pattern we can check if the year attribute of the song is not set, to prevent a database write to fail. How do we recover from the error? In this case we set the year to unknown and we keep going, knowing that the database write is not going to fail (from this one reason, at least).\n\nOf course, how to recover from an error is going to be very specific to each application and error. In the example above I'm assuming that the song's year is stored as a string in the database. If it is stored as a number then maybe setting the year to is an acceptable way to handle songs with an unknown year. In another application the year may be required, in which case this wouldn't be a recoverable error for that application.\n\nMakes sense? If you find a mistake or inconsistency in the current state of the application, and have a way to correct the state without raising an error, then no need to raise an error, just correct the state and keep going.\n\nThe second case is a variation of the first. Here the error is not a new error, it is an error that bubbles up from a function that was called. As in the previous case, the nature of the error is such that the code that receives the error knows how to recover from it and continue.\n\nHow do we handle this case? We use EAFP to catch the error, and then we do whatever needs to be done to recover from it and continue.\n\nHere is another part of the function that demonstrates this case:\n\nThe function wants to retrieve the artist given with the song from the database, but this is something that may fail from time to time, for example when adding the first song of a given artist. The function uses EAFP to catch the error from the database, and then corrects the error by adding the unknown artist to the database before continuing.\n\nAs with the first case, here the code that needs to handle the error knows how to adjust the state of the application to continue running, so it can consume the error and continue. None of the layers in the call stack above this code need to know that there was an error, so the bubbling up of this error ends at this point.\n\nThe third case is a bit more interesting. Now we have a new error of such severity that the code does not know what to do and cannot continue. The only reasonable action that can be taken is to stop the current function and alert one level up the call stack of the error, with the hope that the caller knows what to do. As discussed above, in Python the preferred mechanism to notify the caller of an error is to raise an exception, so this is what we'll do.\n\nThis strategy works well because of an interesting property of non-recoverable errors. In most cases, a non-recoverable error will eventually become recoverable when it reaches a high enough position in the call stack. So the error can bubble up the call stack until it becomes recoverable, at which point it'll be a type 2 error, which we know how to handle.\n\nLet's revisit the function. We've seen that if the year of the song was missing, we decided that can recover and prevent a database error by setting the year to . If the song does not have a name, however, it is much harder to know what's the right thing to do at this level, so we can say that a missing name is a non-recoverable error for this function. Here is how we handle this error:\n\nThe choice of what exception class to use really depends on the application and your personal taste. For many errors the exceptions that come with Python can be used, but if none of the built-in exceptions fit, then you can always create your own exception subclasses. Here is the same example implemented with a custom exception:\n\nThe important thing to note here is that the keyword interrupts the function. This is necessary because we said that this error cannot be recovered, so the rest of the function after the error will not be able to do what it needs to do and should not run. Raising the exception interrupts the current function and starts the bubbling up of the error starting from the closest caller and continuing up the call stack until some code decides to catch the exception.\n\nOkay, we have one last error type to review, and this is actually the most interesting of all and also my favorite.\n\nNow we have a piece of code that called some function, the function raised an error, and we in our function have no idea how to fix things up so that we can continue, so we have to consider this error as non-recoverable. What do we do now?\n\nThe answer is going to surprise you. In this case we do absolutely nothing!\n\nI've mentioned earlier that not handling errors can be a great error handling strategy, and this is exactly what I meant.\n\n Let me show you an example of how it looks to handle an error by doing nothing:\n\nLet's say that both functions called in can fail and raise exceptions. Here are a couple of examples of things that can go wrong with these functions:\n• The user could press Ctrl-C while the application is waiting for input inside , or in the case of a GUI application, the user could click a Close or Cancel button.\n• While inside either one of the functions, the database can go offline due to a cloud issue, causing all queries and commits to fail for some time.\n\nIf we have no way to recover from these errors, then there is no point in catching them. Doing nothing is actually the most useful thing you can do, as it allows the exceptions to bubble up. Eventually the exceptions will reach a level at which the code knows how to do recovery, and at that point they will be considered type 2 errors, which are easily caught and handled.\n\nYou may think that this is an exceptionally rare situation to be in. I think you are wrong. In fact, you should design your applications so that as much code as possible is in functions that do not need to concern themselves with error handling. Moving error handling code to higher-level functions is a very good strategy that helps you have clean and maintainable code.\n\nI expect some of you may disagree. Maybe you think that the function above should at least print an error message to inform the user that there was a failure. I don't disagree, but let's think about that for a bit. Can we be sure that we have a console to print on? Or is this a GUI application? GUIs do not have , they present errors to users visually through some sort of alert or message box. Maybe this is a web application instead? In web apps you present errors by returning an HTTP error response to the user. Should this function know which type of application this is and how errors are to be presented to the user? The separation of concerns principle says that it should not.\n\nOnce again, I'll reiterate that doing nothing in this function does not mean that the error is being ignored, it means that we are allowing the error to bubble up so that some other part of the application with more context can deal with it appropriately.\n\nOne of the reasons you may be doubting that type 4 errors should be the most common in your application is that by letting exceptions bubble up freely they may go all the way to the top without being caught anywhere else, causing the application to crash. This is a valid concern that has an easy solution.\n\nYou should design your applications so that it is impossible for an exception to ever reach the Python layer. And you do this by adding a / block at the highest level that catches the runaway exceptions.\n\nIf you were writing a command line application, you could do this as follows:\n\nHere the top-level of this application is in the conditional, and it considers any errors that reach this level as recoverable. The recovery mechanism is to show the error to the user and to exit the application with a exit code of , which will inform the shell or the parent process that the application failed. With this logic the application knows how to exit with failure, so now there is no need to reimplement this anywhere else. The application can simply let errors bubble up, and they'll eventually be caught here, where the error message will be shown and the application will then exit with an error code.\n\nYou may remember that I've mentioned above that catching all exceptions is a bad practice. Yet, that is exactly what I'm doing here! The reason is that at this level we really cannot let any exceptions reach Python because we do not want this program to ever crash, so this is the one situation in which it makes sense to catch all exceptions. This is the exception (pun intended) that proves the rule.\n\nHaving a high-level catch-all exception block is actually a common pattern that is implemented by most application frameworks. Here are two examples:\n• The Flask web framework: Flask considers each request as a separate run of the application, with the method as the top layer. The code that catches all exceptions is here.\n• The Tkinter GUI toolkit (part of the Python standard library): Tkinter considers each application event handler as separate little run of the application, and adds a generic catch-all exception block each time it calls a handler, to prevent faulty application handlers from ever crashing the GUI. See the code here. In this snippet note how Tkinter allows the exception (indicating the application is exiting) to bubble up, but catches every other one to prevent a crash.\n\nI want to show you an example of how you can improve your code when using a smart design for error handling. For this I'm going to use Flask, but this applies to most other frameworks or application types as well.\n\nLet's say this is a database application that uses the Flask-SQLAlchemy extension. Through my consulting and code review work I see lots of developers coding database operations in Flask endpoints as follows:\n\nHere this route attempts to save a song to the database, and catches database errors, which are all subclasses of the exception class. If the error occurs, it writes an explanatory message to the log, and then rolls back the database session. But of course, the rollback operation can also fail sometimes, so there is a second exception catching block to catch rollback errors and also log them. After all this, a 500 error is returned to the user so that they know that there was a server error. This pattern is repeated in every endpoint that writes to the database.\n\nThis is a very bad solution. First of all, there is nothing that this function can do to recover a rollback error. If a rollback error occurs that means the database is in big trouble, so you will likely continue to see errors, and logging that there was a rollback error is not going to help you in any way. Second, logging an error message when a commit fails appears useful at first, but this particular log lacks information, especially the stack trace of the error, which is the most important debugging tool you will need later when figuring out what happened. At the very least, this code should use instead of , since that will log an error message plus a stack trace. But we can do even better.\n\nThis endpoint falls in the type 4 category, so it can be coded using the \"doing nothing\" approach, resulting in a much better implementation:\n\nWhy does this work? As you've seen before, Flask catches all errors, so your application will never crash due to missing to catch an error. As part of its handling, Flask will log the error message and the stack trace to the Flask log for you, which is exactly what we want, so no need to do this ourselves. Flask will also return a 500 error to the client, to indicate that an unexpected server error has occurred. In addition, the Flask-SQLAlchemy extension attaches to the exception handling mechanism in Flask and rolls back the session for you when a database error occurs, the last important thing that we need. There is really nothing left for us to do in the route!\n\nThe recovery process for database errors is the same in most applications, so you should let the framework do the dirty work for you, while you benefit from much simpler logic in your own application code.\n\nErrors in Production vs. Errors in Development\n\nI mentioned that one of the benefits of moving as much of the error handling logic as possible to the higher layers of the application call stack is that your application code can let those errors bubble up without having to catch them, resulting in much easier to maintain and readable code.\n\nAnother benefit of moving the bulk of error handling code to a separate part of the application is that with the error handling code in a single place you have better control of how the application reacts to errors. The best example of this is how easy it becomes to change the error behavior on the production and development configurations of your application.\n\nDuring development, there is actually nothing wrong with the application crashing and showing a stack trace. In fact, this is a good thing, since you want errors and bugs to be noticed and fixed. But of course, the same application must be rock solid during production, with errors being logged and developers notified if feasible, without leaking any internal or private details of the error to the end user.\n\nThis becomes much easier to implement when the error handling is in one place and separate from the application logic. Let's go back to the command line example I shared earlier, but now let's add development and production modes:\n\nIsn't this wonderful? When we are running in development mode we now re-raise the exceptions to cause the application to crash, so that we can see the errors and the stack traces while working. But we do this without compromising the robustness of the production version, which continues to catch all errors and prevent crashes. More importantly, the application logic does not need to know of these configuration differences.\n\nDoes this remind of you of anything Flask, Django and other web frameworks do? Many web frameworks have a development or debug mode, which shows you crashes in your console and sometimes even in your web browser. Exactly the same solution I'm showing you on a made-up CLI application, but applied to a web application!\n\nI hope you've learned a thing or two from this article and as a result you are able to write better error handling code for your projects! If there are any questions that you aren't clear on, feel free to write them below and I'll do my best to address them."
    },
    {
        "link": "https://wiki.python.org/moin/HandlingExceptions",
        "document": "The simplest way to handle exceptions is with a \"try-except\" block:\n\nIf you wanted to examine the exception from code, you could have:\n\nSometimes, you want to catch all errors that could possibly be generated, but usually you don't. In most cases, you want to be as specific as possible. In the first example above, if you were using a catch-all exception clause and a user presses Ctrl-C, generating a KeyboardInterrupt, you don't want the program to print \"divide by zero\".\n\nHowever, there are some situations where it's best to catch all errors.\n\nFor example, suppose you are writing an extension module to a web service. You want the error information to output the output web page, and the server to continue to run, if at all possible. But you have no idea what kind of errors you might have put in your code.\n\nIn situations like these, you may want to code something like this:\n\nMoinMoin software is a good example of where general error catching is good. If you write MoinMoin extension macros, and trigger an error, MoinMoin will give you a detailed report of your error and the chain of events leading up to it. Python software needs to be able to catch all errors, and deliver them to the recipient of the web page.\n\nAnother case is when you want to do something when code fails:\n\nBy using with no arguments, you will re-raise the last exception. A common place to use this would be to roll back a transaction, or undo operations. If it's a matter of cleanup that should be run regardless of success or failure, then you would do:\n\nStandard exceptions that can be raised are detailed at:\n\nLook to class documentation to find out what exceptions a given class can raise.\n• Give example of IOError, and interpreting the IOError code.\n• Give example of multiple excepts. Handling multiple excepts in one line.\n\nIn the \"general error handling\" section above, it says to catch all exceptions, you use the following code:\n\nHowever, it originally was:\n\nSomeone pointed out that \"except\" catches more than just\n\nWhy is that the case? What is the difference?-- LionKimbro\n\nFor now (version <= 2.4) exception doesn't have to be inherited from Exception. Thus plain 'except:' catches all exceptions, not only system. String exceptions are one example of an exception that doesn't inherit from Exception. -- MikeRovner\n\nI believe that as of 2.7, exceptions still don't have to be inherited from Exception or even BaseException. However, as of Python 3, exceptions must subclass . -- ElephantJim\n\nGetting Useful Information from an Exception\n\nSo, I've got something like:\n\n...and so, you naturally wonder, \"Well, what was in ?\"\n\nYou know- you can put a in there, and that works. But is there a better, more interesting way to get at that information that people know of?\n\nYou can do something like:\n\nThe attribute of exceptions is a tuple of all the arguments that were passed in (typically the one and only argument is the error message). This way you can modify the arguments and re-raise, and the extra information will be displayed. You could also put a print statement or logging in the block.\n\nNote that not all exceptions subclass Exception (though almost all do), so this might not catch some exceptions; also, exceptions aren't required to have an attribute (though it will if the exception subclasses Exception and doesn't override without calling its superclass), so the code as written might fail But in practice it almost never does (and if it does, you should fix the non-conformant exception!)\n\nIsn't it better to prevent then to remediate?\n\nJoel Spolsky might be a great C++ programmer, and his advice on user interface design is invaluable, but Python is not C++ or Java, and his arguments about exceptions do not hold in Python.\n\n\"They are invisible in the source code. Looking at a block of code, including functions which may or may not throw exceptions, there is no way to see which exceptions might be thrown and from where. This means that even careful code inspection doesn't reveal potential bugs.\"\n\nI don't quite get this argument. In a random piece of source code, there is no way to tell whether or not it will fail just by inspection. If you look at:\n\nyou can't tell whether or not myfunction will fail at runtime just by inspection, so why should it matter whether it fails by crashing at runtime or fails by raising an exception?\n\nJoel's argument that raising exceptions is just a goto in disguise is partly correct. But so are for loops, while loops, functions and methods! Like those other constructs, exceptions are gotos tamed and put to work for you, instead of wild and dangerous. You can't jump *anywhere*, only highly constrained places.\n\n\"They create too many possible exit points for a function. To write correct code, you really have to think about every possible code path through your function. Every time you call a function that can raise an exception and don't catch it on the spot, you create opportunities for surprise bugs caused by functions that terminated abruptly, leaving data in an inconsistent state, or other code paths that you didn't think about.\"\n\nThis is a better argument for *careful* use of exceptions, not an argument to avoid them. Or better still, it is an argument for writing code which doesn't has side-effects and implements data transactions. That's a good idea regardless of whether you use exceptions or not. (In python, \"transactions\" are small enough that it is usually difficult to interrupt an operation inside one without writing C code. You *can* do it, say, with recursive generators, but it is difficult.)\n\nJoel's concern about multiple exit points is good advice, but it can be taken too far. Consider the following code snippet:\n\nThere is no benefit in deferring returning value as myfunc does, just for the sake of having a single exit point. \"Have a single exit point\" is a good heuristic for many functions, but it is pointless make-work for this one. (In fact, it increases, not decreases, the chances of a bug. If you look carefully, myfunc above has such a bug in the \"0 < x <= 3\" clause.)\n\nUsed correctly, exceptions in Python have more advantages than disadvantages. They aren't just for errors either: exceptions can be triggered for exceptional cases (hence the name) without needing to track (and debug) multiple special cases.\n\nLastly, let me argue against one of Joel's comments:\n\n\"A better alternative is to have your functions return error values when things go wrong, and to deal with these explicitly, no matter how verbose it might be. It is true that what should be a simple 3 line program often blossoms to 48 lines when you put in good error checking, but that's life, and papering it over with exceptions does not make your program more robust.\"\n\nMaybe that holds true for C++. I don't know the language, and wouldn't like to guess. But it doesn't hold true for Python.\n\nThis is how Joel might write a function as a C programmer:\n\nand then call it with:\n\nThis is how I would write it in Python:\n\nand call it with:\n\nIn the case of Python, calling a function that may raise an exception is no more difficult or unsafe than calling a function that returns a status flag and a result, but writing the function itself is much easier, with fewer places for the programmer to make a mistake. (The one difference is that if you don't handle the error, your program will stop and complain, instead of continuing and corrupting the data.)\n\nIn effect, exceptions allow the Python programmer to concentrate on his actual program, rather than be responsible for building error-handling infrastructure into every function. Python supplies that infrastructure for you, in the form of exceptions.\n\nSee also: Italian translation at ManutenereLeEccezioni."
    }
]