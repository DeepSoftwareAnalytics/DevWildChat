[
    {
        "link": "https://auth0.com/blog/developing-restful-apis-with-python-and-flask",
        "document": "TL;DR: Throughout this article, we will use Flask and Python to develop a RESTful API. We will create an endpoint that returns static data (dictionaries). Afterward, we will create a class with two specializations and a few endpoints to insert and retrieve instances of these classes. Finally, we will look at how to run the API on a Docker container. The final code developed throughout this article is available in this GitHub repository. I hope you enjoy it!\n\nThis article is divided into the following sections:\n\nNowadays, choosing Python to develop applications is becoming a very popular choice. As StackOverflow recently analyzed, Python is one of the fastest-growing programming languages, having surpassed even Java in the number of questions asked on the platform. On GitHub, the language also shows signs of mass adoption, occupying the second position among the top programming languages in 2021.\n\nThe huge community forming around Python is improving every aspect of the language. More and more open source libraries are being released to address many different subjects, like Artificial Intelligence, Machine Learning, and web development. Besides the tremendous support provided by the overall community, the Python Software Foundation also provides excellent documentation, where new adopters can learn its essence fast.\n\nWhen it comes to web development on Python, there are three predominant frameworks: Django, Flask, and a relatively new player FastAPI. Django is older, more mature, and a little bit more popular. On GitHub, this framework has around 66k stars, 2.2k contributors, ~350 releases, and more than 25k forks.\n\nFastAPI is growing at high speed, with 48k stars on Github, 370 contributors, and more than 3.9k forks. This elegant framework built for high-performance and fast-to-code APIs is not one to miss.\n\nFlask, although less popular, is not far behind. On GitHub, Flask has almost 60k stars, ~650 contributors, ~23 releases, and nearly 15k forks.\n\nEven though Django is older and has a slightly more extensive community, Flask has its strengths. From the ground up, Flask was built with scalability and simplicity. Flask applications are known for being lightweight, mainly compared to their Django counterparts. Flask developers call it a microframework, where micro (as explained here) means that the goal is to keep the core simple but extensible. Flask won't make many decisions for us, such as what database to use or what template engine to choose. Lastly, Flask has extensive documentation that addresses everything developers need to start. FastAPI follows a similar \"micro\" approach to Flask, though it provides more tools like automatic Swagger UI and is an excellent choice for APIs. However, as it is a newer framework, many more resources and libraries are compatible with frameworks like Django and Flask but not with FastAPI.\n\nBeing lightweight, easy to adopt, well-documented, and popular, Flask is a good option for developing RESTful APIs.\n\nFirst and foremost, we will need to install some dependencies on our development machine. We will need to install Python 3, Pip (Python Package Index), and Flask.\n\nIf we are using some recent version of a popular Linux distribution (like Ubuntu) or macOS, we might already have Python 3 installed on our computer. If we are running Windows, we will probably need to install Python 3, as this operating system does not ship with any version.\n\nAfter installing Python 3 on our machine, we can check that we have everything set up as expected by running the following command:\n\nNote that the command above might produce a different output when we have a different Python version. What is important is that you are running at least\n\nor newer. If we get \"Python 2\" instead, we can try issuing. If this command produces the correct output, we must replace all commands throughout the article to useinstead of just\n\nPip is the recommended tool for installing Python packages. While the official installation page states that\n\ncomes installed if we're using Python 2 >=or Python 3 >=, installing Python throughon Ubuntu doesn't install. Therefore, let's check if we need to installseparately or already have it.\n\nIf the command above produces an output similar to\n\n, then we are good to go. If we get, we can try replacingwith. If we cannot find Pip for Python 3 on our machine, we can follow the instructions here to install Pip\n\nWe already know what Flask is and its capabilities. Therefore, let's focus on installing it on our machine and testing to see if we can get a basic Flask application running. The first step is to use\n\nAfter installing the package, we will create a file called\n\nand add five lines of code to it. As we will use this file to check if Flask was correctly installed, we don't need to nest it in a new directory.\n\nThese 5 lines of code are everything we need to handle HTTP requests and return a \"Hello, World!\" message. To run it, we execute the following command:\n\nAfter executing these commands, we can reach our application by opening a browser and navigating to\n• installs packages globally, making it hard to manage multiple versions of the same package on the same machine.\n• need all dependencies and sub-dependencies listed explicitly, a manual process that is tedious and error-prone.\n\nas the tool for installing Python packages, we will need to use another package to manage our project's dependencies. It's true thatsupports package management through the file , but the tool lacks some features required on serious projects running on different production and development machines. Among its issues, the ones that cause the most problems are:\n\nTo solve these issues, we are going to use Pipenv. Pipenv is a dependency manager that isolates projects in private environments, allowing packages to be installed per project. If you're familiar with NPM or Ruby's bundler, it's similar in spirit to those tools.\n\nNow, to start creating a serious Flask application, let's create a new directory that will hold our source code. In this article, we will create Cashman, a small RESTful API that allows users to manage incomes and expenses. Therefore, we will create a directory called\n\n. After that, we will useto start our project and manage our dependencies.\n\nThe second command creates our virtual environment, where all our dependencies get installed, and the third will add Flask as our first dependency. If we check our project's directory, we will see two new files:\n• contains details about our project, such as the Python version and the packages needed.\n• contains precisely what version of each package our project depends on and its transitive dependencies.\n\nLike other mainstream programming languages, Python also has the concept of packages to enable developers to organize source code according to subjects/functionalities. Similar to Java packages and C# namespaces, packages in Python are files organized in directories that other Python scripts can import. To create a package in a Python application, we need to create a folder and add an empty file called\n\nLet's create our first package in our application, the main package, with all our RESTful endpoints. Inside the application's directory, let's create another one with the same name,\n\n. The rootdirectory created before will hold metadata about our project, like what dependencies it has, while this new one will be our package with our Python scripts.\n\n. In this script, we will define the first endpoint of our application.\n\nAs in the previous example, our application returns a \"Hello, world!\" message. We will start improving it in a second, but first, let's create an executable file called\n\nin the root directory of our application.\n\nThe goal of this file is to facilitate the start-up of our application. Its source code will be the following:\n\nThe first command defines the main script to be executed by Flask. The second command runs our Flask application in the context of the virtual environment listening to all interfaces on the computer (\n\nTo check that this script is working correctly, we run\n\nto get similar results as when executing the \"Hello, world!\" application.\n\nNow that our application is structured, we can start coding some relevant endpoints. As mentioned before, the goal of our application is to help users to manage incomes and expenses. We will begin by defining two endpoints to handle incomes. Let's replace the contents of the\n\nfile with the following:\n\nSince improving our application, we have removed the endpoint that returned \"Hello, world!\" to users. In its place, we defined an endpoint to handle HTTP\n\nrequests to return incomes and another endpoint to handle HTTPrequests to add new ones. These endpoints are annotated withto define routes listening to requests on theendpoint. Flask provides great documentation on what exactly this does\n\nTo facilitate the process, we currently manipulate incomes as dictionaries. However, we will soon create classes to represent incomes and expenses.\n\nTo interact with both endpoints that we have created, we can start our application and issue some HTTP requests:\n\nUsing dictionaries in a simple use case like the one above is enough. However, for more complex applications that deal with different entities and have multiple business rules and validations, we might need to encapsulate our data into Python classes.\n\nWe will refactor our application to learn the process of mapping entities (like incomes) as classes. The first thing that we will do is create a subpackage to hold all our entities. Let's create a\n\ndirectory inside thepackage and add an empty file calledon it.\n\nWe will create three classes in this new directory:\n\n, and. The first class will be the base for the two others, and we will call it. Let's create a file calledin thedirectory with the following code:\n\nclass, we also defined a. We will use the latter to deserialize and serialize instances offrom and to JSON objects. This class inherits from another superclass calledthat belongs on a package not yet installed.\n\nMarshmallow is a popular Python package for converting complex datatypes, such as objects, to and from built-in Python datatypes. We can use this package to validate, serialize, and deserialize data. We won't dive into validation in this article, as it will be the subject of another one. Though, as mentioned, we will use\n\nto serialize and deserialize entities through our endpoints.\n\nTo keep things more organized and meaningful, we won't expose the\n\nclass on our endpoints. We will create two specializations to handle the requests:and. Let's make a module calledinside thepackage with the following code:\n\nThe only value that this class adds for our application is that it hardcodes the type of transaction. This type is a Python enumerator, which we still have to create, that will help us filter transactions in the future. Let's create another file, called\n\nThe code of the enumerator is quite simple. It just defines a class called\n\nthat inherits fromand that defines two types:and\n\nLastly, let's create the class that represents expenses. To do that, let's add a new file called\n\n, this class hardcodes the type of the transaction, but now it passesto the superclass. The difference is that it transforms the givento be negative. Therefore, no matter if the user sends a positive or a negative value, we will always store it as negative to facilitate calculations.\n\nsuperclass and its specializations adequately implemented, we can now enhance our endpoints to deal with these classes. Let's replacecontents to:\n\nThe new version that we just implemented starts by redefining the\n\nvariable into a list ofand, now called. Besides that, we have also changed the implementation of both methods that deal with incomes. For the endpoint used to retrieve incomes, we defined an instance ofto produce a JSON representation of incomes. We also used to extract incomes only from thelist. In the end we send the array of JSON incomes back to users.\n\nThe endpoint responsible for accepting new incomes was also refactored. The change on this endpoint was the addition of\n\nto load an instance ofbased on the JSON data sent by the user. As thelist deals with instances ofand its subclasses, we just added the newin that list.\n\nThe other two endpoints responsible for dealing with expenses,\n• instead of dealing with instances of , we deal with instances of to accept new expenses,\n• and instead of filtering by , we filter by to send expenses back to the user.\n\nand, are almost copies of theircounterparts. The differences are:\n\nThis finishes the implementation of our API. If we run our Flask application now, we will be able to interact with the endpoints, as shown here:\n\nAs we are planning to eventually release our API in the cloud, we are going to create a\n\nto describe what is needed to run the application on a Docker container. We need to install Docker on our development machine to test and run dockerized instances of our project. Defining a Docker recipe () will help us run the API in different environments. That is, in the future, we will also install Docker and run our program on environments like production and staging\n\nin the root directory of our project with the following code:\n\nThe first item in the recipe defines that we will create our Docker container based on the default Python 3 Docker image. After that, we update APK and install\n\n. Having, we define the working directory we will use in the image and copy the code needed to bootstrap and run the application. In the fourth step, we useto install all our Python dependencies. Lastly, we define that our image will communicate through portand that this image, when executed, needs to run thescript to start Flask.\n\nTo create and run a Docker container based on the\n\nthat we created, we can execute the following commands:\n\nis simple but effective, and using it is similarly easy. With these commands and this, we can run as many instances of our API as we need with no trouble. It's just a matter of defining another port on the host or even another host.\n\nIn this article, we learned about the basic components needed to develop a well-structured Flask application. We looked at how to use\n\nto manage the dependencies of our API. After that, we installed and used Flask and Marshmallow to create endpoints capable of receiving and sending JSON responses. In the end, we also looked at how to dockerize the API, which will facilitate the release of the application to the cloud.\n\nAlthough well structured, our API is not that useful yet. Among the things that we can improve, we are going to cover the following topics in the following article:\n• How to handle JWTs in Python"
    },
    {
        "link": "https://geeksforgeeks.org/python-build-a-rest-api-using-flask",
        "document": "REST stands for REpresentational State Transfer and is an architectural style used in modern web development. It defines a set or rules/constraints for a web application to send and receive data.\n\nIn this article, we will build a REST API in Python using the Flask framework. Flask is a popular micro framework for building web applications. Since it is a micro-framework, it is very easy to use and lacks most of the advanced functionality which is found in a full-fledged framework. Therefore, building a REST API in Flask is very simple.\n\nThere are two ways of creating a REST API in Flask:\n• Using Flask without any external libraries\n\ncan be installed via the pip command:\n\nHere, there are two functions: One function to just return or print the data sent through GET or POST and another function to calculate the square of a number sent through GET request and print it."
    },
    {
        "link": "https://imaginarycloud.com/blog/flask-python",
        "document": "How to make a REST API using Python Flask?\n\nThis article will guide you through the first steps to create a REST API using Flask(🌶️).\n\nBelow you can see the endpoints you’ll have by the end of the tutorial. The documentation presented is also generated by the application you will create!\n\nYou must have Python installed on the current machine. The code presented will consider Python3. If you want to use Python2 and/or are following this procedure in a Windows machine, please follow the instructions presented in the Flask installation guide.\n\nLet’s start by creating a directory to store the project. In the directory you want to have your project, run the following commands on the shell:\n\nWe’ve created the directory and moved it inside. Before starting to install dependencies, let’s create a virtual environment by running the following command:\n\nThis will create a folder into your project with the name . After that, we need to activate the respective environment by running:\n\nThis means we are now considering the venv virtual environment when running any Python code. It might be important to specify the environment you are considering when running it in your IDE.\n\nMake sure you have the environment active before following the next steps. You can check if you are inside the environment by looking to the left side of the console. If there’s the virtual environment name inside parentheses, you’re good to go.\n\nIf you want to deactivate the environment, just run the following command:\n\n‍\n\nAt the point of writing, the Flask stable version is 1.1.2. If you want to have the project specifications updated and with the same version as me, you can use a file instead of installing Flask. You can copy the file below to the root directory.\n\nAfter that, you just need to run the following command:\n\nNow we are ready to start developing our REST API. The usage of the file is widespread and useful in Python projects since it easily allows you to share and have the project with the same packages installed.\n\nWhat?! Is it that simple? Yes, it is! The code is done to have an application with an implemented endpoint. Now we simply have to define the Flask environment variables and see your application ?. Note that you need to be in the folder with the file to run this commands.\n\nAccess the link to see the classic message we all love.\n\nNow that we have created our first endpoint, let’s get back to this application blog post's main purpose: Have the REST API with the basic CRUD structure. Import request from Flask and add the lines below to the main file to have the CRUD endpoints created for a specific entity:\n\nNow, for each of the routes created different methods are allowed. The following endpoints are available:\n\n‍\n• GET /entities - get list of entities\n\nWith this structure, you are ready to create an API with all the complexity you need. This is not the most scalable structure since you consider multiple validations for each route, and the structure is not rigid. In the next sections, we’ll cover a more scalable solution that allows easy documentation creation as you develop your endpoints.\n\nTo test how these endpoints are working, you can use a curl request like the one presented below:\n\nThis is the result you should get if you run this command on your machine. Make sure the port your application is running on is the same:\n\nAnother tool that is very useful and common to use in the development of APIs is Postman. If you’ve never used it, we recommend you try it and explore it! It is very powerful and will increase your development to the moon and beyond. 🚀\n\nBefore we present other Flask strengths, let’s talk about blueprints. A blueprint is an object very similar to a flask application object, but instead of creating a new one, it allows the extension of the current application. This might be useful if you want to create multiple versions of an API or simply divide services within the same application.\n\nWe will use this class type to present different use case scenarios of a Flask application. Let’s convert the code above to be inserted into a blueprint and load into the main application.\n\nCreate a new folder to start inserting blueprints models as we progress in the blog post. Inside create a folder named and create a file in it named :\n\nNow the file just needs to load the created blueprint and register it to the application object:\n\nNow you should have exactly the same endpoints but with the usage of Blueprints.\n\nAs already stated, Flask is a very minimal framework; however, it relies on a handy tool: the Jinja template engine. This allows for rendering dynamic HTML templates. Although this is out of this blog post's scope, we will just give a small example to demonstrate the concept.\n\nThe first thing you have to do is create the folder and inside this folder, insert the file.\n\nNote there are two variables in use in the template inside {{ }}. This is a special format to include Python code inside the template, allowing for dynamic content to be rendered. In this case, we have the top and bottom of the variable, which will be converted to text inside the tag. Now that the template is created let’s load it using Flask. Let’s create a new blueprint in a different file to demonstrate this example:\n\nDo not forget to register this blueprint in the main.py file:\n\nHere we can see the definition of the top and bottom variables based on the query params sent in the URL. For example, if you go to you will get the following result:\n\nFor the giggles, let’s add some CSS to the page we just created. Create the folder with the file inside.\n\nAfter that, add the corresponding reference to the CSS file we just created by adding this head tag to the HTML file:\n\nGo to and you should see something like this:\n\nThere! You have your jinja crash course and a meme generator endpoint! Do not touch the Business Cat! You might be surprised with what you'll find..."
    },
    {
        "link": "https://flask.palletsprojects.com/en/stable/api",
        "document": "This part of the documentation covers all the interfaces of Flask. For parts where Flask depends on external libraries, we document the most important right here and provide links to the canonical documentation.\n\nThe Click command group for registering CLI commands for this object. The commands are available from the command once the application has been discovered and blueprints have been registered. Used by to determine the cache value for a given file path if it wasn’t passed. By default, this returns from the configuration of . This defaults to , which tells the browser to use conditional requests instead of a timed cache, which is usually preferable. Note this is a duplicate of the same method in the Flask class. Changed in version 2.0: The default configuration is instead of 12 hours. The view function used to serve files from . A route is automatically registered for this view at if is set. Note this is a duplicate of the same method in the Flask class. Open a resource file relative to for reading. The blueprint-relative equivalent of the app’s method.\n• None resource (str) – Path to the resource relative to .\n• None mode (str) – Open the file in this mode. Only reading is supported, valid values are (or ) and .\n• None encoding (str | None) – Open the file with this encoding when opening in text mode. This is ignored when opening in binary mode. Register a template filter, available in any template rendered by the application. Works like the decorator. Equivalent to .\n• None name (str | None) – the optional name of the filter, otherwise the function name will be used. Register a template global, available in any template rendered by the application. Works like the decorator. Equivalent to .\n• None name (str | None) – the optional name of the global, otherwise the function name will be used. Register a template test, available in any template rendered by the application. Works like the decorator. Equivalent to .\n• None name (str | None) – the optional name of the test, otherwise the function name will be used. Register a URL rule with the blueprint. See for full documentation. The URL rule is prefixed with the blueprint’s URL prefix. The endpoint name, used with , is prefixed with the blueprint’s name. Like , but after every request, not only those handled by the blueprint. Equivalent to . Register a function to run after each request to this object. The function is called with the response object, and must return a response object. This allows the functions to modify or replace the response before it is sent. If a function raises an exception, any remaining functions will not be called. Therefore, this should not be used for actions that must execute, such as to close resources. Use for that. This is available on both app and blueprint objects. When used on an app, this executes after every request. When used on a blueprint, this executes after every request that the blueprint handles. To register with a blueprint and execute after every request, use . Like , but for templates rendered by every view, not only by the blueprint. Equivalent to . Like , but for every request, not only those handled by the blueprint. Equivalent to . Register a template filter, available in any template rendered by the application. Equivalent to . name (str | None) – the optional name of the filter, otherwise the function name will be used. Register a template global, available in any template rendered by the application. Equivalent to . name (str | None) – the optional name of the global, otherwise the function name will be used. Register a template test, available in any template rendered by the application. Equivalent to . name (str | None) – the optional name of the test, otherwise the function name will be used. Like , but for every request, not only those handled by the blueprint. Equivalent to . Like , but for every request, not only those handled by the blueprint. Equivalent to . Like , but before every request, not only those handled by the blueprint. Equivalent to . Register a function to run before each request. For example, this can be used to open a database connection, or to load the logged in user from the session. The function will be called without any arguments. If it returns a non- value, the value is handled as if it was the return value from the view, and further request handling is stopped. This is available on both app and blueprint objects. When used on an app, this executes before every request. When used on a blueprint, this executes before every request that the blueprint handles. To register with a blueprint and execute before every request, use . Registers a template context processor function. These functions run before rendering a template. The keys of the returned dict are added as variables available in the template. This is available on both app and blueprint objects. When used on an app, this is called for every rendered template. When used on a blueprint, this is called for templates rendered from the blueprint’s views. To register with a blueprint and affect every template, use . Decorate a view function to register it for the given endpoint. Used if a rule is added without a with . endpoint (str) – The endpoint name to associate with the view function. Register a function to handle errors by code or exception class. A decorator that is used to register a function given an error code. Example: 'This page does not exist' You can also register handlers for arbitrary exceptions: This is available on both app and blueprint objects. When used on an app, this can handle errors from every request. When used on a blueprint, this can handle errors from requests that the blueprint handles. To register with a blueprint and affect every request, use . Added in version 0.7: Use instead of modifying directly, for application wide error handlers. Added in version 0.7: One can now additionally also register custom exception types that do not necessarily have to be a subclass of the class. code_or_exception (type[Exception] | int) – the code as integer for the handler, or an arbitrary exception The Jinja loader for this object’s templates. By default this is a class to if it is set. Creates an instance of object that is later passed to the register callback functions. Subclasses can override this to return a subclass of the setup state. Registers a function that is called when the blueprint is registered on the application. This function is called with the state as argument as returned by the method. Works like but wraps the function in another function that will ensure the function is only called once. If the blueprint is registered a second time on the application, the function passed is not called. Called by to register all views and callbacks registered on the blueprint with the application. Creates a and calls each callback with it.\n• None app (App) – The application this blueprint is being registered with. Changed in version 2.1: Registering the same blueprint with the same name multiple times is an error. Changed in version 2.0.1: Nested blueprints are registered with their dotted name. This allows different blueprints with the same name to be nested at different locations. Changed in version 2.0.1: The option can be used to change the (pre-dotted) name the blueprint is registered with. This allows the same blueprint to be registered multiple times with unique names for . Register a on this blueprint. Keyword arguments passed to this method will override the defaults set on the blueprint. Changed in version 2.0.1: The option can be used to change the (pre-dotted) name the blueprint is registered with. This allows the same blueprint to be registered multiple times with unique names for . Alternative error attach function to the decorator that is more straightforward to use for non decorator usage. Decorate a view function to register it with the given URL rule and options. Calls , which has more details about the implementation. The endpoint name for the route defaults to the name of the view function if the parameter isn’t passed. The parameter defaults to . and are added automatically.\n• None options (Any) – Extra options passed to the object. The absolute path to the configured static folder. if no static folder is set. The URL prefix that the static route will be accessible from. If it was not configured during init, it is derived from . Like , but after every request, not only those handled by the blueprint. Equivalent to . Register a function to be called when the request context is popped. Typically this happens at the end of each request, but contexts may be pushed manually as well during testing. When the block exits (or is called), the teardown functions are called just before the request context is made inactive. When a teardown function was called because of an unhandled exception it will be passed an error object. If an is registered, it will handle the exception and the teardown will not receive it. Teardown functions must avoid raising exceptions. If they execute code that might fail they must surround that code with a / block and log any errors. The return values of teardown functions are ignored. This is available on both app and blueprint objects. When used on an app, this executes after every request. When used on a blueprint, this executes after every request that the blueprint handles. To register with a blueprint and execute after every request, use . Callback function for URL defaults for all view functions of the application. It’s called with the endpoint and values and should update the values passed in place. This is available on both app and blueprint objects. When used on an app, this is called for every request. When used on a blueprint, this is called for requests that the blueprint handles. To register with a blueprint and affect every request, use . Register a URL value preprocessor function for all view functions in the application. These functions will be called before the functions. The function can modify the values captured from the matched url before they are passed to the view. For example, this can be used to pop a common language code value and place it in rather than pass it to every view. The function is passed the endpoint name and values dict. The return value is ignored. This is available on both app and blueprint objects. When used on an app, this is called for every request. When used on a blueprint, this is called for requests that the blueprint handles. To register with a blueprint and affect every request, use . The name of the package or module that this object belongs to. Do not change this once it is set by the constructor. The path to the templates folder, relative to , to add to the template loader. if templates should not be added. Absolute path to the package on the filesystem. Used to look up resources contained in the package. To register a view function, use the decorator. This data structure is internal. It should not be modified directly and its format may change at any time. A data structure of registered error handlers, in the format . The key is the name of a blueprint the handlers are active for, or for all requests. The key is the HTTP status code for , or for other exceptions. The innermost dictionary maps exception classes to handler functions. To register an error handler, use the decorator. This data structure is internal. It should not be modified directly and its format may change at any time. A data structure of functions to call at the beginning of each request, in the format . The key is the name of a blueprint the functions are active for, or for all requests. To register a function, use the decorator. This data structure is internal. It should not be modified directly and its format may change at any time. A data structure of functions to call at the end of each request, in the format . The key is the name of a blueprint the functions are active for, or for all requests. To register a function, use the decorator. This data structure is internal. It should not be modified directly and its format may change at any time. A data structure of functions to call at the end of each request even if an exception is raised, in the format . The key is the name of a blueprint the functions are active for, or for all requests. To register a function, use the decorator. This data structure is internal. It should not be modified directly and its format may change at any time. A data structure of functions to call to pass extra context values when rendering templates, in the format . The key is the name of a blueprint the functions are active for, or for all requests. To register a function, use the decorator. This data structure is internal. It should not be modified directly and its format may change at any time. A data structure of functions to call to modify the keyword arguments passed to the view function, in the format . The key is the name of a blueprint the functions are active for, or for all requests. To register a function, use the decorator. This data structure is internal. It should not be modified directly and its format may change at any time. A data structure of functions to call to modify the keyword arguments when generating URLs, in the format . The key is the name of a blueprint the functions are active for, or for all requests. To register a function, use the decorator. This data structure is internal. It should not be modified directly and its format may change at any time.\n\nThe request object used by default in Flask. Remembers the matched endpoint and view arguments. It is what ends up as . If you want to replace the request object used you can subclass this and set to your subclass. The request object is a subclass and provides all of the attributes Werkzeug defines plus a few Flask specific ones. The internal URL rule that matched the request. This can be useful to inspect which methods are allowed for the URL from a before/after handler ( ) etc. Though if the request’s method was invalid for the URL rule, the valid list is available in instead (an attribute of the Werkzeug exception ) because the request was never internally bound. A dict of view arguments that matched the request. If an exception happened when matching, this will be . If matching the URL failed, this is the exception that will be raised / was raised as part of the request handling. This is usually a exception or something similar. The maximum number of bytes that will be read during this request. If this limit is exceeded, a 413 error is raised. If it is set to , no limit is enforced at the Flask application level. However, if it is and the request has no header and the WSGI server does not indicate that it terminates the stream, then no data is read to avoid an infinite stream. Each request defaults to the config, which defaults to . It can be set on a specific to apply the limit to that specific view. This should be set appropriately based on an application’s or view’s specific needs. Changed in version 3.1: This can be set per-request. Changed in version 0.6: This is configurable through Flask config. The maximum size in bytes any non-file form field may be in a body. If this limit is exceeded, a 413 error is raised. If it is set to , no limit is enforced at the Flask application level. Each request defaults to the config, which defaults to . It can be set on a specific to apply the limit to that specific view. This should be set appropriately based on an application’s or view’s specific needs. Changed in version 3.1: This is configurable through Flask config. The maximum number of fields that may be present in a body. If this limit is exceeded, a 413 error is raised. If it is set to , no limit is enforced at the Flask application level. Each request defaults to the config, which defaults to . It can be set on a specific to apply the limit to that specific view. This should be set appropriately based on an application’s or view’s specific needs. Changed in version 3.1: This is configurable through Flask config. The endpoint that matched the request URL. This will be if matching failed or has not been performed yet. This in combination with can be used to reconstruct the same URL or a modified URL. The registered name of the current blueprint. This will be if the endpoint is not part of a blueprint, or if URL matching failed or has not been performed yet. This does not necessarily match the name the blueprint was created with. It may have been nested, or registered with a different name. The registered names of the current blueprint upwards through parent blueprints. This will be an empty list if there is no current blueprint, or if URL matching failed. If this method returns a value, it is used as the return value for . The default implementation raises . e (ValueError | None) – If parsing failed, this is the exception. It will be if the content type wasn’t . Changed in version 2.3: Raise a 415 error instead of 400. List of charsets this client supports as object. List of encodings this client accepts. Encodings in a HTTP term are compression encodings such as gzip. For charsets have a look at . List of languages this client accepts as object. List of mimetypes this client supports as object. Sent with a preflight request to indicate which headers will be sent with the cross origin request. Set on the response to indicate which headers are allowed. Sent with a preflight request to indicate which method will be used for the cross origin request. Set on the response to indicate which methods are allowed. If a forwarded header exists this is a list of all ip addresses from the client ip to the last proxy server. Decorate a function as responder that accepts the request as the last argument. This works like the decorator but the function is passed the request object as the last argument and the request object will be closed automatically: As of Werkzeug 0.14 HTTP exceptions are automatically caught and converted to responses instead of failing. The parsed URL parameters (the part in the URL after the question mark). By default an is returned from this function. This can be changed by setting to a different type. This might be necessary if the order of the form data is important. The header parsed into an object. if the header is not present. Changed in version 2.3: is no longer a . The attribute was added for auth schemes that use a token instead of parameters. Like but without the query string. Closes associated resources of this request object. This closes all file handles explicitly. You can also use the request object in a with statement which will automatically close it. The Content-Encoding entity-header field is used as a modifier to the media-type. When present, its value indicates what additional content codings have been applied to the entity-body, and thus what decoding mechanisms must be applied in order to obtain the media-type referenced by the Content-Type header field. The Content-Length entity-header field indicates the size of the entity-body in bytes or, in the case of the HEAD method, the size of the entity-body that would have been sent had the request been a GET. The Content-MD5 entity-header field, as defined in RFC 1864, is an MD5 digest of the entity-body for the purpose of providing an end-to-end message integrity check (MIC) of the entity-body. (Note: a MIC is good for detecting accidental modification of the entity-body in transit, but is not proof against malicious attacks.) The Content-Type entity-header field indicates the media type of the entity-body sent to the recipient or, in the case of the HEAD method, the media type that would have been sent had the request been a GET. A with the contents of all cookies transmitted with the request. The raw data read from . Will be empty if the request represents form data. To get the raw data even if it represents form data, use . The Date general-header field represents the date and time at which the message was originated, having the same semantics as orig-date in RFC 822. Changed in version 2.0: The datetime object is timezone-aware. object containing all uploaded files. Each key in is the name from the . Each value in is a Werkzeug object. It basically behaves like a standard file object you know from Python, with the difference that it also has a function that can store the file on the filesystem. Note that will only contain data if the request method was POST, PUT or PATCH and the that posted to the request had . It will be empty otherwise. See the / documentation for more details about the used data structure. The form parameters. By default an is returned from this function. This can be changed by setting to a different type. This might be necessary if the order of the form data is important. Please keep in mind that file uploads will not end up here, but instead in the attribute. Changed in version 0.9: Previous to Werkzeug 0.9 this would only contain form data for POST and PUT requests. Create a new request object based on the values provided. If environ is given missing values are filled from there. This method is useful for small scripts when you need to simulate a request from an URL. Do not use this method for unittesting, there is a full featured client object ( ) that allows to create multipart requests, support for cookies etc. This accepts the same options as the . Changed in version 0.5: This method now accepts the same arguments as . Because of this the parameter is now called . This reads the buffered incoming data from the client into one bytes object. By default this is cached but that behavior can be changed by setting to . Usually it’s a bad idea to call this method without checking the content length first as a client could send dozens of megabytes or more to cause memory problems on the server. Note that if the form data was already parsed this method will not return anything as form data parsing does not cache the data like this method does. To implicitly invoke form data parsing function set to . When this is done the return value of this method will be an empty string if the form parser handles the data. This generally is not necessary as if the whole data is cached (which is the default) the form parser will used the cached data to parse the form data. Please be generally aware of checking the content length first in any case before calling this method to avoid exhausting server memory. If is set to the return value will be a decoded string. If the mimetype does not indicate JSON (application/json, see ), or parsing fails, is called and its return value is used as the return value. By default this raises a 415 Unsupported Media Type resp.\n• None force (bool) – Ignore the mimetype and always try to parse JSON.\n• None silent (bool) – Silence mimetype and parsing errors, and return instead.\n• None cache (bool) – Store the parsed JSON to return for subsequent calls. Changed in version 2.3: Raise a 415 error instead of 400. Changed in version 2.1: Raise a 400 error if the content type is incorrect. The host name the request was made to, including the port if it’s non-standard. Validated with . The request URL scheme and host only. An object containing all the etags in the header. Changed in version 2.0: The datetime object is timezone-aware. An object containing all the etags in the header. Changed in version 2.0: The datetime object is timezone-aware. The raw WSGI input stream, without any safety checks. This is dangerous to use. It does not guard against infinite streams or reading past or . Check if the mimetype indicates JSON data, either application/json or application/*+json. boolean that is if the application is served by a WSGI server that spawns multiple processes. boolean that is if the application is served by a multithreaded WSGI server. boolean that is if the application will be executed only once in a process lifetime. This is the case for CGI for example, but it’s not guaranteed that the execution only happens one time. if the request was made with a secure protocol (HTTPS or WSS). The parsed JSON data if indicates JSON (application/json, see ). If the request content type is not , this will raise a 415 Unsupported Media Type error. Changed in version 2.3: Raise a 415 error instead of 400. Changed in version 2.1: Raise a 400 error if the content type is incorrect. Creates the form data parser. Instantiates the with some parameters. The Max-Forwards request-header field provides a mechanism with the TRACE and OPTIONS methods to limit the number of proxies or gateways that can forward the request to the next inbound server. Like , but without parameters (eg, without charset, type etc.) and always lowercase. For example if the content type is the mimetype would be . The mimetype parameters as dict. For example if the content type is the params would be . The host that the request originated from. Set on the response to indicate which origins are allowed. The Pragma general-header field is used to include implementation-specific directives that might apply to any recipient along the request/response chain. All pragma directives specify optional behavior from the viewpoint of the protocol; however, some systems MAY require that behavior be consistent with the directives. The Referer[sic] request-header field allows the client to specify, for the server’s benefit, the address (URI) of the resource from which the Request-URI was obtained (the “referrer”, although the header field is misspelled). If the server supports user authentication, and the script is protected, this attribute contains the username the user has authenticated as. The request URL scheme, host, and root path. This is the root that the application is accessed from. The WSGI input stream, with safety checks. This stream can only be consumed once. Use to get the full data as bytes or text. The attribute will contain the full bytes only if they do not represent form data. The attribute will contain the parsed form data in that case. Unlike , this stream guards against infinite streams or reading past or . If is set, it can be enforced on streams if is set. Otherwise, an empty stream is returned. If the limit is reached before the underlying stream is exhausted (such as a file that is too large, or an infinite stream), the remaining contents of the stream cannot be read safely. Depending on how the server handles this, clients may show a “connection reset” failure instead of seeing the 413 response. Changed in version 2.3: Check preemptively and while reading. Changed in version 0.9: The stream is always set (but may be consumed) even if form parsing was accessed first. Valid host names when handling requests. By default all hosts are trusted, which means that whatever the client says the host is will be accepted. Because and headers can be set to any value by a malicious client, it is recommended to either set this property or implement similar validation in the proxy (if the application is being run behind one). The full request URL with the scheme, host, root path, path, and query string. Alias for . The URL with scheme, host, and root path. For example, . The user agent. Use to get the header value. Set to a subclass of to provide parsing for the other properties or other extended data. Changed in version 2.1: The built-in parser was removed. Set to a subclass to parse data from the string. For GET requests, only are present, not . Changed in version 2.0: For GET requests, only are present, not . if the request method carries content. By default this is true if a is sent. The WSGI environment containing HTTP headers and information from the WSGI server. Set when creating the request object. If , reading from the request body will cause a . Useful to prevent modifying the stream from middleware. The method the request was made with, such as . The URL scheme of the protocol the request used, such as or . The address of the server. , for unix sockets, or if not known. The prefix that the application is mounted under, without a trailing slash. comes after this. The path part of the URL after . This is the path used for routing within the application. The part of the URL after the “?”. This is the raw value, use for the parsed values. The headers received with the request. The address of the client sending the request. To access incoming request data, you can use the global object. Flask parses incoming request data for you and gives you access to it through that global object. Internally Flask makes sure that you always get the correct data for the active thread if you are in a multithreaded environment. This is a proxy. See Notes On Proxies for more information. The request object is an instance of a .\n\nThe response object that is used by default in Flask. Works like the response object from Werkzeug but is set to have an HTML mimetype by default. Quite often you don’t have to create this object yourself because will take care of that for you. If you want to replace the response object used you can subclass this and set to your subclass. Changed in version 1.0: JSON support is added to the response, like the request. This is useful when testing to get the test client response data as JSON. the default mimetype if none is provided. The header. Even though the name would indicate that multiple values are supported, it must be one string token only. The values and are common. Whether credentials can be shared by the browser to JavaScript code. As part of the preflight request it indicates whether credentials can be used on the cross origin request. Which headers can be sent with the cross origin request. Which methods can be used for the cross origin request. The origin or ‘*’ for any origin that may make cross origin requests. Which headers can be shared by the browser to JavaScript code. The maximum age in seconds the access control settings can be cached for. Add an etag for the current response if there is none yet. Changed in version 2.0: SHA-1 is used to generate the value. MD5 may not be available in some environments. The Age response-header field conveys the sender’s estimate of the amount of time since the response (or its revalidation) was generated at the origin server. The Allow entity-header field lists the set of methods supported by the resource identified by the Request-URI. The purpose of this field is strictly to inform the recipient of valid methods associated with the resource. An Allow header field MUST be present in a 405 (Method Not Allowed) response. Should this response object automatically set the content-length header if possible? This is true by default. The Cache-Control general-header field is used to specify directives that MUST be obeyed by all caching mechanisms along the request/response chain. Returns the content length if available or otherwise. Adds a function to the internal list of functions that should be called as part of closing down the response. Since 0.7 this function also returns the function that was passed so that this can be used as a decorator. Close the wrapped response if possible. You can also use the object in a with statement which will automatically close it. Added in version 0.9: Can now be used in a with statement. The Content-Encoding entity-header field is used as a modifier to the media-type. When present, its value indicates what additional content codings have been applied to the entity-body, and thus what decoding mechanisms must be applied in order to obtain the media-type referenced by the Content-Type header field. The Content-Language entity-header field describes the natural language(s) of the intended audience for the enclosed entity. Note that this might not be equivalent to all the languages used within the entity-body. The Content-Length entity-header field indicates the size of the entity-body, in decimal number of OCTETs, sent to the recipient or, in the case of the HEAD method, the size of the entity-body that would have been sent had the request been a GET. The Content-Location entity-header field MAY be used to supply the resource location for the entity enclosed in the message when that entity is accessible from a location separate from the requested resource’s URI. The Content-MD5 entity-header field, as defined in RFC 1864, is an MD5 digest of the entity-body for the purpose of providing an end-to-end message integrity check (MIC) of the entity-body. (Note: a MIC is good for detecting accidental modification of the entity-body in transit, but is not proof against malicious attacks.) The header as a object. Available even if the header is not set. The header as a object. Available even if the header is not set. The Content-Security-Policy header adds an additional layer of security to help detect and mitigate certain types of attacks. The header as a object. Available even if the header is not set. The Content-Security-Policy-Report-Only header adds a csp policy that is not enforced but is reported thereby helping detect certain types of attacks. The Content-Type entity-header field indicates the media type of the entity-body sent to the recipient or, in the case of the HEAD method, the media type that would have been sent had the request been a GET. Prevents a document from loading any cross-origin resources that do not explicitly grant the document permission. Values must be a member of the enum. Allows control over sharing of browsing context group with cross-origin documents. Values must be a member of the enum. The Date general-header field represents the date and time at which the message was originated, having the same semantics as orig-date in RFC 822. Changed in version 2.0: The datetime object is timezone-aware. the default status if none is provided.\n• None key (str) – the key (name) of the cookie to be deleted.\n• None path (str | None) – if the cookie that should be deleted was limited to a path, the path has to be defined here.\n• None domain (str | None) – if the cookie that should be deleted was limited to a domain, that domain has to be defined here.\n• None secure (bool) – If , the cookie will only be available via HTTPS.\n• None samesite (str | None) – Limit the scope of the cookie to only be attached to requests that are “same-site”.\n• None partitioned (bool) – If , the cookie will be partitioned. The Expires entity-header field gives the date/time after which the response is considered stale. A stale cache entry may not normally be returned by a cache. Changed in version 2.0: The datetime object is timezone-aware. Enforce that the WSGI response is a response object of the current type. Werkzeug will use the internally in many situations like the exceptions. If you call on an exception you will get back a regular object, even if you are using a custom subclass. This method can enforce a given response type, and it will also convert arbitrary WSGI callables into response objects if an environ is provided: # convert a Werkzeug response object into an instance of the This is especially useful if you want to post-process responses in the main dispatcher and use functionality provided by your subclass. Keep in mind that this will modify response objects in place if possible! Make the response object ready to be pickled. Does the following:\n• None Buffer the response into a list, ignoring and .\n• None Generate an header if one is not already set. Changed in version 2.0: An header is always added. Changed in version 0.6: The header is set. Create a new response object from an application output. This works best if you pass it an application that returns a generator all the time. Sometimes applications may use the callable returned by the function. This tries to resolve such edge cases automatically. But if you don’t get the expected output you should set to which enforces buffering.\n• None environ (WSGIEnvironment) – the WSGI environment to execute against. Returns the application iterator for the given environ. Depending on the request method and the current status code the return value might be an empty response rather than the one from the response. If the request method is or the status code is in a range where the HTTP specification requires an empty response, an empty iterable is returned. environ (WSGIEnvironment) – the WSGI environment of the request. The string representation of the response body. Whenever you call this property the response iterable is encoded and flattened. This can lead to unwanted behavior if you stream big data. This behavior can be disabled by setting to . If is set to the return value will be a decoded string. Return a tuple in the form . If there is no ETag the return value is . Parse as JSON. Useful during testing. If the mimetype does not indicate JSON (application/json, see ), this returns . Unlike , the result is not cached.\n• None force (bool) – Ignore the mimetype and always try to parse JSON. This is automatically called right before the response is started and returns headers modified for the given environment. It returns a copy of the headers from the response with some modifications applied if necessary. For example the location header (if present) is joined with the root URL of the environment. Also the content length is automatically set to zero here for certain status codes. Changed in version 0.6: Previously that function was called and modified the response object in place. Also since 0.6, IRIs in location and content-location headers are handled properly. Also starting with 0.6, Werkzeug will attempt to set the content length if it is able to figure it out on its own. This is the case if all the strings in the response iterable are already encoded and the iterable is buffered. environ (WSGIEnvironment) – the WSGI environment of the request. Returns the final WSGI response as tuple. The first item in the tuple is the application iterator, the second the status and the third the list of headers. The response returned is created specially for the given environment. For example if the request method in the WSGI environment is the response will be empty and only the headers and status code will be present. environ (WSGIEnvironment) – the WSGI environment of the request. if set to accessing properties on the response object will not try to consume the response iterator and convert it into a list. Added in version 0.6.2: That attribute was previously called . (Notice the typo). If you did use this feature, you have to adapt your code to the name change. Check if the mimetype indicates JSON data, either application/json or application/*+json. If the iterator is buffered, this property will be . A response object will consider an iterator to be buffered if the response attribute is a list or tuple. If the response is streamed (the response is not an iterable with a length information) this property is . In this case streamed means that there is no information about the number of iterations. This is usually if a generator is passed to the response object. This is useful for checking before applying some sort of post filtering that should not take place for streamed responses. Iter the response encoded with the encoding of the response. If the response object is invoked as WSGI application the return value of this method is used as application iterator unless was activated. The parsed JSON data if indicates JSON (application/json, see ). The Last-Modified entity-header field indicates the date and time at which the origin server believes the variant was last modified. Changed in version 2.0: The datetime object is timezone-aware. The Location response-header field is used to redirect the recipient to a location other than the Request-URI for completion of the request or identification of a new resource. Make the response conditional to the request. This method works best if an etag was defined for the response already. The method can be used to do that. If called without etag just the date header is set. This does nothing if the request method in the request or environ is anything but GET or HEAD. For optimal performance when handling range requests, it’s recommended that your response data object implements , and methods as described by . Objects returned by automatically implement those methods. It does not remove the body of the response because that’s something the function does for us automatically. Returns self so that you can do but modifies the object in-place.\n• None request_or_environ (WSGIEnvironment | Request) – a request object or WSGI environment to be used to make the response conditional against.\n• None accept_ranges (bool | str) – This parameter dictates the value of header. If (default), the header is not set. If , it will be set to . If it’s a string, it will use this value.\n• None complete_length (int | None) – Will be used only in valid Range Requests. It will set complete length value and compute real value. This parameter is mandatory for successful Range Requests completion. if header could not be parsed or satisfied. Changed in version 2.0: Range processing is skipped if length is 0 instead of raising a 416 Range Not Satisfiable error. Converts the response iterator in a list. By default this happens automatically if required. If is disabled, this method is not automatically called and some properties might raise exceptions. This also encodes all the items. The mimetype (content type without charset etc.) The mimetype parameters as dict. For example if the content type is the params would be . The Retry-After response-header field can be used with a 503 (Service Unavailable) response to indicate how long the service is expected to be unavailable to the requesting client. Time in seconds until expiration or date. Changed in version 2.0: The datetime object is timezone-aware. A warning is raised if the size of the cookie header exceeds , but the header will still be set.\n• None key (str) – the key (name) of the cookie to be set.\n• None value (str) – the value of the cookie.\n• None max_age (timedelta | int | None) – should be a number of seconds, or (default) if the cookie should last only as long as the client’s browser session.\n• None expires (str | datetime | int | float | None) – should be a object or UNIX timestamp.\n• None path (str | None) – limits the cookie to a given path, per default it will span the whole domain.\n• None domain (str | None) – if you want to set a cross-domain cookie. For example, will set a cookie that is readable by the domain , etc. Otherwise, a cookie will only be readable by the domain that set it.\n• None secure (bool) – If , the cookie will only be available via HTTPS.\n• None samesite (str | None) – Limit the scope of the cookie to only be attached to requests that are “same-site”.\n• None partitioned (bool) – If , the cookie will be partitioned. Changed in version 3.1: The parameter was added. Sets a new string as response. The value must be a string or bytes. If a string is set it’s encoded to the charset of the response (utf-8 by default). Set the etag, and override the old one if there was one. The Vary field value indicates the set of request-header fields that fully determines, while the response is fresh, whether a cache is permitted to use the response to reply to a subsequent request without revalidation. The header parsed into a object. Modifying the object will modify the header value. This header is not set by default. To set this header, assign an instance of to this attribute. Multiple values for this header can be sent to give the client multiple options. Assign a list to set multiple headers. However, modifying the items in the list will not automatically update the header values, and accessing this attribute will only ever return the first value. To unset this header, assign or use . Changed in version 2.3: This attribute can be assigned to to set the header. A list can be assigned to set multiple header values. Use to unset the header. Changed in version 2.3: is no longer a . The attribute was added for auth challenges that use a token instead of parameters. The response body to send as the WSGI iterable. A list of strings or bytes represents a fixed-length response, any other iterable is a streaming response. Strings are encoded to bytes as UTF-8. Do not set to a plain string or bytes, that will cause sending the response to be very inefficient as it will iterate one byte at a time. Pass the response body directly through as the WSGI iterable. This can be used when the body is a binary file or other iterator of bytes, to skip some unnecessary checks. Use instead of setting this manually. If a redirect header is a relative URL, make it an absolute URL, including scheme and domain. Changed in version 2.1: This is disabled by default, so responses will send relative redirects.\n\nGenerally there are three ways to define rules for the routing system:\n• None You can use the decorator.\n• None You can use the function.\n• None You can directly access the underlying Werkzeug routing system which is exposed as . Variable parts in the route can be specified with angular brackets ( ). By default a variable part in the URL accepts any string without a slash however a different converter can be specified as well by using . Variable parts are passed to the view function as keyword arguments. The following converters are available: accepts any text without a slash (the default) like but for floating point values like the default but also accepts slashes matches one of the items provided Custom converters can be defined using . An important detail to keep in mind is how Flask deals with trailing slashes. The idea is to keep each URL unique so the following rules apply:\n• None If a rule ends with a slash and is requested without a slash by the user, the user is automatically redirected to the same page with a trailing slash attached.\n• None If a rule does not end with a trailing slash and the user requests the page with a trailing slash, a 404 not found is raised. This is consistent with how web servers deal with static files. This also makes it possible to use relative link targets safely. You can also define multiple rules for the same function. They have to be unique however. Defaults can also be specified. Here for example is a definition for a URL that accepts an optional page: This specifies that will be the URL for page one and will be the URL for page . If a URL contains a default value, it will be redirected to its simpler form with a 301 redirect. In the above example, will be redirected to . If your route handles and requests, make sure the default route only handles , as redirects can’t preserve form data. Here are the parameters that and accept. The only difference is that with the route parameter the view function is defined with the decorator instead of the parameter. the endpoint for the registered URL rule. Flask itself assumes that the name of the view function is the name of the endpoint if not explicitly stated. the function to call when serving a request to the provided endpoint. If this is not provided one can specify the function later by storing it in the dictionary with the endpoint as key. A dictionary with defaults for this rule. See the example above for how defaults work. specifies the rule for the subdomain in case subdomain matching is in use. If not specified the default subdomain is assumed. the options to be forwarded to the underlying object. A change to Werkzeug is handling of method options. methods is a list of methods this rule should be limited to ( , etc.). By default a rule just listens for (and implicitly ). Starting with Flask 0.6, is implicitly added and handled by the standard request handling. They have to be specified as keyword arguments."
    },
    {
        "link": "https://realpython.com/flask-connexion-rest-api",
        "document": "Most modern web applications are powered by a REST API under the hood. That way, developers can separate the front-end code from the back-end logic, and users can interact with the interface dynamically. In this three-part tutorial series, you’ll build a REST API with the Flask web framework.\n\nYou’ll create a foundation with a basic Flask project then add endpoints and connect them to a SQLite database. You’ll test your API with Swagger UI API documentation that you’ll build along the way.\n\nIn the first part of this tutorial series, you’ll learn how to:\n• Interact with your API to manage data\n\nAfter finishing the first part of this series, you’ll move on to the second part, where you’ll learn to use a proper database to store your data permanently instead of relying on in-memory storage.\n\nThis tutorial series is a hand-on guide on how to create a REST API with Flask and interact with it using CRUD operations. If you want to refresh your knowledge on working with APIs, then you can give Python and REST APIs: Interacting With Web Services a read.\n\nYou can download the code for the first part of this project by clicking the link below:\n\nIn this three-part tutorial series, you’ll build a REST API to keep track of notes for people that may visit you throughout the year. In this tutorial, you’ll create people like the Tooth Fairy, the Easter Bunny, and Knecht Ruprecht. Ideally, you want to be on good terms with all three of them. That’s why you’ll send them notes, to increase the chance of getting valuable gifts from them. You can interact with your application by leveraging the API documentation. Along the way, you’ll build a basic front end that reflects the contents of your database: In the first part of this series, you’ll create a base Flask project and plug in your first API endpoints. At the end of this part, you’ll be able to see a list of people in the front end and manage each person in the back end: By leveraging Swagger UI, you’ll create handy documentation for your API along the way. That way, you’ll have the opportunity to test how your API works at each stage of this tutorial and get a useful overview of all your endpoints.\n\nBesides building the Flask project foundation, you’re going to create a REST API that provides access to a collection of people and to the individuals within that collection. Here’s the API design for the people collection: The REST API that you’ll be building will serve a simple people data structure where the people are keyed to the last name, and any updates are marked with a new timestamp. The dataset that you’ll be working with looks like this: One of the purposes of an API is to decouple the data from the application that uses it, thereby hiding the data implementation details. Later in this tutorial series, you’ll save your data in a database. But for the start, an in-memory data structure works fine.\n\nIn this section, you’ll prepare the development environment for your Flask REST API project. First, you’ll create a virtual environment and install all the dependencies that you need for your project. In this section, you’ll build your project structure. You can name the root folder of your project any way you like. For example, you could name it . Create the folder and navigate into it: In this case, you name the root folder of your project . The files and folders that you create over the course of this series will be located in either this folder or its subfolders. After you navigate to the project folder, it’s a good idea to create and activate a virtual environment. That way, you’re installing any project dependencies not system-wide but only in your project’s virtual environment. Select your operating system below and use your platform-specific command to set up a virtual environment: With the commands shown above, you create and activate a virtual environment named by using Python’s built-in module. The parenthesized in front of the prompt indicate that you’ve successfully activated the virtual environment. After you’ve created and activated your virtual environment, it’s time to install Flask with : The Flask micro web framework is the the main dependency that your project requires. On top of Flask, install Connexion to handle the HTTP requests: To also make use of auto-generated API documentation, you install Connexion with the added support for Swagger UI. Later in this tutorial, you’ll learn more about the Python packages that you just installed. The main file of your Flask project will be . Create in and add the following content: You import the module, giving the application access to the Flask functionality. You then create a Flask application instance named . Next, you connect the URL route to the function by decorating it with . This function calls the Flask function to get the file from the templates directory and return it to the browser. In short, this code gets a basic web server up and running and makes it respond with a template, which will be served to a browser when navigating to the URL . Note: Flask’s development server defaults to port . On newer macOS versions, this port is already in use by the macOS AirPlay receiver. Above, you’ve changed the port of your Flask app with . If you want, you can change the AirPlay receiver preferences on your Mac instead. Flask expects in a template directory named . Create the directory and add : Flask comes with the Jinja Templating Engine, which enables you to enhance your templates. But your template is a basic HTML file without any Jinja features. That’s okay for now, because the purpose of is to verify that your Flask project responds as intended. With the Python virtual environment active, you can run your application with this command line in the directory containing the file: When you run , a web server will start on port 8000. If you open a browser and navigate to , you should see Hello, World! displayed: Congratulations, your web server is running! You’ll extend the file later to work with the REST API that you’re developing. By now, your Flask project structure should look like this: This is a great structure for starting any Flask project. You may find that the source code will come in handy when you’re working on future projects. You can download it here: Source Code: Click here to download the free source code that you’ll use to build a REST API with the Flask web framework. In the next sections, you’ll expand the project and add your first REST API endpoints.\n\nNow that you’ve got a working web server, you can add your first REST API endpoint. To do this, you’ll use Connexion, which you installed in the previous section. The Connexion module allows a Python program to use the OpenAPI specification with Swagger. The OpenAPI Specification is an API description format for REST APIs and provides a lot of functionality, including:\n• Validation of input and output data to and from your API\n• Configuration of the API URL endpoints and the expected parameters When you use OpenAPI with Swagger, you can create a user interface (UI) to explore the API. All of this can happen when you create a configuration file that your Flask application can access. The Swagger configuration file is a YAML or JSON file containing your OpenAPI definitions. This file contains all of the information necessary to configure your server to provide input parameter validation, output response data validation, and URL endpoint definition. Create a file named and begin adding metadata to it: When you define an API, you must include the version of your OpenAPI definition. You use the keyword for this. The version string is important because some parts of the OpenAPI structure may change over time. Also, just like each new Python version includes new features, there may be keywords added or deprecated in the OpenAPI specification. The keyword begins the scope of the API information block:\n• Description of what the API provides or is about\n• Version value for the API Next, add and , which define the root path of your API: By providing as the value of , you’ll be able to access all of your API paths relative to . You define your API endpoints in a block: The block begins the configuration of the API URL endpoint paths:\n• The relative URL of your API endpoint\n• The HTTP method that this URL endpoint will respond to Together with the definition in , this creates the URL endpoint that you can access at . The block begins the configuration of the single URL endpoint:\n• The Python function that’ll respond to the request\n• The tags assigned to this endpoint, which allow you to group the operations in the UI\n• : The UI display text for this endpoint\n• : The status codes that the endpoint responds with must contain a string. Connexion will use to find a Python function named in a module of your project. You’ll create the corresponding Python code later in this tutorial. The block defines the configuration of the possible status codes. Here, you define a successful response for the status code , containing some text. You can find the complete content of the file in the collapsible below: Below, you’ll find the full source code of your OpenAPI definition: You’ve organized this file in a hierarchical manner. Each indentation level represents a level of ownership, or scope. For example, marks the beginning of where all the API URL endpoints are defined. The value indented under that represents the start of where all the URL endpoints will be defined. The scope indented under holds the definitions associated with an HTTP GET request to the URL endpoint. This pattern goes on for the entire configuration. The file is like a blueprint for your API. With the specifications that you include in , you define what data your web server can expect and how your server should respond to requests. But so far, your Flask project doesn’t know about your file. Read on to use Connexion to connect your OpenAPI specification with your Flask app. There are two steps to adding a REST API URL endpoint to your Flask application with Connexion:\n• Add an API configuration file to your project.\n• Connect your Flask app with the configuration file. You already added a configuration file named in the last section. To connect the API configuration file with your Flask app, you must reference in your file: The statement adds the module to the program. The next step is creating the application instance using Connexion rather than Flask. Internally, the Flask app is still created, but it now has additional functionality added to it. Part of the app instance creation includes the parameter in line 6. This tells Connexion which directory to look in for its configuration file. In this case, it’s the same directory that you run from. In line 7, you tell the app instance to read the file from the specification directory and configure the system to provide the Connexion functionality. In the file, you configured Connexion with the value . So, when the API gets an HTTP request for , your Flask app calls a function within a module. To make this work, create a file with a function: In line 5, you create a helper function named that generates a string representation of the current timestamp. You then define the dictionary data structure in line 8, which is the data you’ll work with in this part of the tutorial series. The dictionary stands in for a proper database. As is a module variable, its state persists between REST API calls. However, any data that you change will be lost when you restart your web application. This is not ideal, but it’s fine for now. Then you create the function in line 26. Your server will run when it receives an HTTP request to . The return value of is a list of dictionaries with information about a person. Running your server code and navigating your browser to will display the list of people on-screen: Congratulations, you’ve created your first API endpoint! Before continuing on your way to building out your REST API with multiple endpoints, take a moment and explore the API a bit more in the next section. Currently you have a REST API running with a single URL endpoint. Your Flask app knows what to serve based on your API specification in . Additionally, Connexion uses to create API documentation for you. Navigate to to see your API documentation in action: This is the initial Swagger interface. It shows the list of URL endpoints supported at your endpoint. Connexion builds this automatically when it parses the file. If you click on the endpoint in the interface, then the interface will expand to show more information about your API: This displays the structure of the expected response, the of that response, and the description text that you entered about the endpoint in the file. Any time the configuration file changes, the Swagger UI changes as well. You can even try the endpoint out by clicking the Try it out button. This feature can be extremely useful when your API grows. The Swagger UI API documentation gives you a way to explore and experiment with the API without having to write any code to do so. Using OpenAPI with the Swagger UI offers a nice, clean way to create the API URL endpoints. So far, you’ve only created one endpoint to serve all people. In the next section, you’ll add additional endpoints to create, update, and delete people in your collection.\n\nSo far, your Flask REST API has one endpoint. Now it’s time to build out an API providing full CRUD access to your people structure. As you recall, the definition of your API looks like this: To achieve this, you’ll extend both the and files to fully support the API defined above. Before you define new API paths in , you’ll add a new block for components. Components are building blocks in your OpenAPI specification that you can reference from other parts of your specification. To avoid code duplication, you create a block. For now, you save only the data model in the block:\n• The data type of the schema The dash ( ) in front of indicates that can contain a list of properties. Any property that you define as must also exist in , which includes the following:\n• The first name of a person\n• The last name of a person The key defines the value associated with its parent key. For , all properties are strings. You’ll represent this schema in your Python code as a dictionary later in this tutorial. Extend your API endpoints by adding a new block for the request in the block: The structure for looks similar to the existing schema. One difference is that you also send to the server. After all, you need to tell Flask the information that it needs to create a new person. Another difference is , which you set to . Inside of , you define as the data exchange format of your API. You can serve different media types in your API requests and API responses. Nowadays APIs commonly use JSON as the data exchange format. This is good news for you as a Python developer, because JSON objects look very much like Python dictionaries. For example: This JSON object resembles the component that you were defining earlier in and that you’re referencing with in . You’re also using a 201 HTTP status code, which is a success response that indicates the creation of a new resource. Note: If you want to learn more about HTTP status codes, then you can check out Mozilla’s documentation about HTTP response status codes. With , you’re telling your server to look for a function in the module. Open and add to the file: In line 4, you’re importing Flask’s function. Using helps you send an error message in line 20. You raise the error response when the request body doesn’t contain a last name or when a person with this last name already exists. Note: A person’s last name must be unique, because you’re using as a dictionary key of . That means you can’t have two people with the same last name in your project for now. If the data in the request body is valid, you update in line 13 and respond with the new object and a 201 HTTP code in line 18. So far, you’re able to create a new person and get a list with all your people. In this section, you’ll update and to work with a new path that handles a single existing person. Open and add the code below: Similar to your path, you start with the operation for the path. The substring is a placeholder for the last name, which you have to pass in as a URL parameter. So, for example, the URL path contains as . Note: The URL parameters are case sensitive. That means you must type a last name like Ruprecht with an uppercase R. You’ll use the parameter in other operations, too. So it makes sense to create a component for it and reference it where needed. points to a function in , so head over to that file again and create the missing function: When your Flask app finds the provided last name in , then it returns the data for this particular person. Otherwise, the server will return a 404 HTTP error. To update an existing person, update with this code: With this definition of the operation, your server expects in : The function expects the arguments and . When a person with the provided last name exists, then you update the corresponding values in with the data. To get rid of a person in your dataset, you need to work with a operation: Add the corresponding function to : If the person you want to delete exists in your dataset, then you remove the item from . Both and are complete for this part of the tutorial. You can download the complete files by clicking the link below: Source Code: Click here to download the free source code that you’ll use to build a REST API with the Flask web framework. With all the endpoints to manage people in place, it’s time to try out your API. Since you used Connexion to connect your Flask project with Swagger, your API documentation is ready for you when you restart your server. Once you’ve updated the and files to complete the people API functionality, the Swagger UI system will update accordingly and look something like this: This UI allows you to see all of the documentation that you’ve included in the file and to interact with all of the URL endpoints making up the CRUD functionality of the people interface. Unfortunately, any changes that you make won’t persist when you restart your Flask application. That’s why you’ll plug a proper database in to your project in the next part of this tutorial series."
    },
    {
        "link": "https://github.com/modelscope/modelscope/blob/master/modelscope/preprocessors/multi_modal.py",
        "document": "# specify the input keys, compatible with training and inference whose key names may be different\n\nThe context length to use; all baseline models use 24 as the context length\n\nf'text should be str or List[str], but got a List containing one '\n\nThe 'role' should be choose from ['system', 'user', 'assistant'].\n\nThe 'content' can be either str or List[Union[str, Dict]]"
    },
    {
        "link": "https://stackoverflow.com/questions/67595500/how-to-download-a-model-from-huggingface",
        "document": "works fine with getting models from huggingface. Here is an example:\n\nNote that you need to have installed if you want to actually download the large files in the repo, rather than just references to them. If you run and get a \"Command not recognized\" message, then you haven't got it installed. You can get the latest version from the official git-lfs website, or install an older version using a package manager - for example, on Ubuntu:\n\nRelated: How do I clone a repository that includes Git LFS files?"
    },
    {
        "link": "https://huggingface.co/docs/transformers/model_doc/auto",
        "document": "and get access to the augmented documentation experience\n\nIn many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the method. AutoClasses are here to do this job for you so that you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.\n\nInstantiating one of AutoConfig, AutoModel, and AutoTokenizer will directly create a class of the relevant architecture. For instance\n\nwill create a model that is an instance of BertModel.\n\nThere is one class of for each task, and for each backend (PyTorch, TensorFlow, or Flax).\n\nEach of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a custom class of model , make sure you have a then you can add those to the auto classes like this:\n\nYou will then be able to use the auto classes like you would usually do!\n\nThe following auto classes are available for instantiating a base model class without a specific head.\n\nThe following auto classes are available for instantiating a model with a pretraining head.\n\nThe following auto classes are available for the following natural language processing tasks.\n\nThe following auto classes are available for the following computer vision tasks.\n\nThe following auto classes are available for the following audio tasks.\n\nThe following auto classes are available for the following multimodal tasks."
    },
    {
        "link": "https://github.com/modelscope/modelscope-classroom/blob/main/LLM-tutorial/RAG%2BRerank%2BLlamaindex.ipynb",
        "document": ""
    },
    {
        "link": "https://huggingface.co/docs/huggingface_hub/v0.17.3/en/package_reference/file_download",
        "document": "and get access to the augmented documentation experience\n• None repo_id ( ) — A user or an organization name and a repo name separated by a .\n• None filename ( ) — The name of the file in the repo.\n• None subfolder ( , optional) — An optional value corresponding to a folder inside the model repo.\n• None repo_type ( , optional) — Set to or if downloading from a dataset or space, or if downloading from a model. Default is .\n• None revision ( , optional) — An optional Git revision id which can be a branch name, a tag, or a commit hash.\n• None endpoint ( , optional) — Hugging Face Hub base url. Will default to environment variable. , optional) — Hugging Face Hub base url. Will default to https://huggingface.co/ . Otherwise, one can set theenvironment variable.\n• None library_name ( , optional) — The name of the library to which the object corresponds.\n• None library_version ( , optional) — The version of the library.\n• None cache_dir ( , , optional) — Path to the folder where cached files are stored.\n• None local_dir ( or , optional) — If provided, the downloaded file will be placed under this directory, either as a symlink (default) or a regular file (see description for more details).\n• None local_dir_use_symlinks ( or , defaults to ) — To be used with . If set to “auto”, the cache directory will be used and the file will be either duplicated or symlinked to the local directory depending on its size. It set to , a symlink will be created, no matter the file size. If set to , the file will either be duplicated from cache (if already exists) or downloaded from the Hub and not cached. See description for more details.\n• None user_agent ( , , optional) — The user-agent info in the form of a dictionary or a string.\n• None force_download ( , optional, defaults to ) — Whether the file should be downloaded even if it already exists in the local cache.\n• None proxies ( , optional) — Dictionary mapping protocol to the URL of the proxy passed to .\n• None etag_timeout ( , optional, defaults to ) — When fetching ETag, how many seconds to wait for the server to send data before giving up which is passed to .\n• None token ( , , optional) — A token to be used for the download.\n• If , the token is read from the HuggingFace config folder.\n• If a string, it’s used as the authentication token. , optional) — A token to be used for the download.\n• None local_files_only ( , optional, defaults to ) — If , avoid downloading the file and return the path to the local cached file if it exists.\n• None legacy_cache_layout ( , optional, defaults to ) — If , uses the legacy file cache layout i.e. just call . This is deprecated as the new cache layout is more powerful. , optional, defaults to) — If, uses the legacy file cache layout i.e. just call hf_hub_url() then. This is deprecated as the new cache layout is more powerful. Download a given file if it’s not already present in the local cache. The new cache file layout looks like this:\n• The cache directory contains one subfolder per repo_id (namespaced by repo type)\n• inside each repo folder:\n• refs is a list of the latest known revision => commit_hash pairs\n• blobs contains the actual file blobs (identified by their git-sha or sha256, depending on whether they’re LFS files or not)\n• snapshots contains one subfolder per commit, each “commit” contains the subset of the files that have been resolved at that particular commit. Each filename is a symlink to the blob at that particular commit. If is provided, the file structure from the repo will be replicated in this location. You can configure how you want to move those files:\n• If (default), files are downloaded and stored in the cache directory as blob files. Small files (<5MB) are duplicated in while a symlink is created for bigger files. The goal is to be able to manually edit and save small files without corrupting the cache while saving disk space for binary files. The 5MB threshold can be configured with the environment variable.\n• If , files are downloaded, stored in the cache directory and symlinked in . This is optimal in term of disk usage but files must not be manually edited.\n• If and the blob files exist in the cache directory, they are duplicated in the local dir. This means disk usage is not optimized.\n• Finally, if and the blob files do not exist in the cache directory, then the files are downloaded and directly placed under . This means if you need to download them again later, they will be re-downloaded entirely.\n• if and the token cannot be found.\n• if ETag cannot be determined.\n• if some parameter value is invalid\n• RepositoryNotFoundError If the repository to download from cannot be found. This may be because it doesn’t exist, or because it is set to and you do not have access.\n• RevisionNotFoundError If the revision to download from cannot be found.\n• EntryNotFoundError If the file to download cannot be found.\n• LocalEntryNotFoundError If network is disabled or unavailable and file is not found in cache.\n• None repo_id ( ) — A namespace (user or an organization) name and a repo name separated by a .\n• None filename ( ) — The name of the file in the repo.\n• None subfolder ( , optional) — An optional value corresponding to a folder inside the repo.\n• None repo_type ( , optional) — Set to or if downloading from a dataset or space, or if downloading from a model. Default is .\n• None revision ( , optional) — An optional Git revision id which can be a branch name, a tag, or a commit hash.\n• None endpoint ( , optional) — Hugging Face Hub base url. Will default to environment variable. , optional) — Hugging Face Hub base url. Will default to https://huggingface.co/ . Otherwise, one can set theenvironment variable. Construct the URL of a file from the given information. The resolved address can either be a huggingface.co-hosted url, or a link to Cloudfront (a Content Delivery Network, or CDN) for large files which are more than a few MBs. Cloudfront is replicated over the globe so downloads are way faster for the end user (and it also lowers our bandwidth costs). Cloudfront aggressively caches files by default (default TTL is 24 hours), however this is not an issue here because we implement a git-based versioning system on huggingface.co, which means that we store the files on S3/Cloudfront in a content-addressable way (i.e., the file name is its hash). Using content-addressable filenames means cache can’t ever be stale. In terms of client-side caching from this library, we base our caching on the objects’ entity tag ( ), which is an identifier of a specific version of a resource [1]_. An object’s ETag is: its git-sha1 if stored in git, or its sha256 if stored in git-lfs.\n• None repo_id ( ) — A user or an organization name and a repo name separated by a .\n• None repo_type ( , optional) — Set to or if downloading from a dataset or space, or if downloading from a model. Default is .\n• None revision ( , optional) — An optional Git revision id which can be a branch name, a tag, or a commit hash.\n• None endpoint ( , optional) — Hugging Face Hub base url. Will default to environment variable. , optional) — Hugging Face Hub base url. Will default to https://huggingface.co/ . Otherwise, one can set theenvironment variable.\n• None cache_dir ( , , optional) — Path to the folder where cached files are stored.\n• None local_dir ( or , optional) — If provided, the downloaded files will be placed under this directory, either as symlinks (default) or regular files (see description for more details).\n• None local_dir_use_symlinks ( or , defaults to ) — To be used with . If set to “auto”, the cache directory will be used and the file will be either duplicated or symlinked to the local directory depending on its size. It set to , a symlink will be created, no matter the file size. If set to , the file will either be duplicated from cache (if already exists) or downloaded from the Hub and not cached. See description for more details.\n• None library_name ( , optional) — The name of the library to which the object corresponds.\n• None library_version ( , optional) — The version of the library.\n• None user_agent ( , , optional) — The user-agent info in the form of a dictionary or a string.\n• None proxies ( , optional) — Dictionary mapping protocol to the URL of the proxy passed to .\n• None etag_timeout ( , optional, defaults to ) — When fetching ETag, how many seconds to wait for the server to send data before giving up which is passed to .\n• None force_download ( , optional, defaults to ) — Whether the file should be downloaded even if it already exists in the local cache.\n• None token ( , , optional) — A token to be used for the download.\n• If , the token is read from the HuggingFace config folder.\n• If a string, it’s used as the authentication token. , optional) — A token to be used for the download.\n• None local_files_only ( , optional, defaults to ) — If , avoid downloading the file and return the path to the local cached file if it exists.\n• None allow_patterns ( or , optional) — If provided, only files matching at least one pattern are downloaded.\n• None ignore_patterns ( or , optional) — If provided, files matching any of the patterns are not downloaded.\n• None max_workers ( , optional) — Number of concurrent threads to download files (1 thread = 1 file download). Defaults to 8.\n• None tqdm_class ( , optional) — If provided, overwrites the default behavior for the progress bar. Passed argument must inherit from or at least mimic its behavior. Note that the is not passed to each individual download. Defaults to the custom HF progress bar that can be disabled by setting environment variable. Download a whole snapshot of a repo’s files at the specified revision. This is useful when you want all files from a repo, because you don’t know which ones you will need a priori. All files are nested inside a folder in order to keep their actual filename relative to that folder. You can also filter which files to download using and . If is provided, the file structure from the repo will be replicated in this location. You can configure how you want to move those files:\n• If (default), files are downloaded and stored in the cache directory as blob files. Small files (<5MB) are duplicated in while a symlink is created for bigger files. The goal is to be able to manually edit and save small files without corrupting the cache while saving disk space for binary files. The 5MB threshold can be configured with the environment variable.\n• If , files are downloaded, stored in the cache directory and symlinked in . This is optimal in term of disk usage but files must not be manually edited.\n• If and the blob files exist in the cache directory, they are duplicated in the local dir. This means disk usage is not optimized.\n• Finally, if and the blob files do not exist in the cache directory, then the files are downloaded and directly placed under . This means if you need to download them again later, they will be re-downloaded entirely. An alternative would be to clone the repo but this requires git and git-lfs to be installed and properly configured. It is also not possible to filter which files to download when cloning a repository using git.\n• if and the token cannot be found.\n• if ETag cannot be determined.\n• if some parameter value is invalid\n\nThe methods displayed above are designed to work with a caching system that prevents re-downloading files. The caching system was updated in v0.8.0 to become the central cache-system shared across libraries that depend on the Hub.\n\nRead the cache-system guide for a detailed presentation of caching at at HF."
    },
    {
        "link": "https://pytorch.org/docs/stable/notes/randomness.html",
        "document": "Completely reproducible results are not guaranteed across PyTorch releases, individual commits, or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds.\n\nHowever, there are some steps you can take to limit the number of sources of nondeterministic behavior for a specific platform, device, and PyTorch release. First, you can control sources of randomness that can cause multiple executions of your application to behave differently. Second, you can configure PyTorch to avoid using nondeterministic algorithms for some operations, so that multiple calls to those operations, given the same inputs, will produce the same result.\n\nYou can use to seed the RNG for all devices (both CPU and CUDA): Some PyTorch operations may use random numbers internally. does this, for instance. Consequently, calling it multiple times back-to-back with the same input arguments may give different results. However, as long as is set to a constant at the beginning of an application and all other sources of nondeterminism have been eliminated, the same series of random numbers will be generated each time the application is run in the same environment. It is also possible to obtain identical results from an operation that uses random numbers by setting to the same value between subsequent calls. For custom operators, you might need to set python seed as well: If you or any of the libraries you are using rely on NumPy, you can seed the global NumPy RNG with: However, some applications and libraries may use NumPy Random Generator objects, not the global RNG (https://numpy.org/doc/stable/reference/random/generator.html), and those will need to be seeded consistently as well. If you are using any other libraries that use random number generators, refer to the documentation for those libraries to see how to set consistent seeds for them. The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism across multiple executions of an application. When a cuDNN convolution is called with a new set of size parameters, an optional feature can run multiple convolution algorithms, benchmarking them to find the fastest one. Then, the fastest algorithm will be used consistently during the rest of the process for the corresponding set of size parameters. Due to benchmarking noise and different hardware, the benchmark may select different algorithms on subsequent runs, even on the same machine. Disabling the benchmarking feature with causes cuDNN to deterministically select an algorithm, possibly at the cost of reduced performance. However, if you do not need reproducibility across multiple executions of your application, then performance might improve if the benchmarking feature is enabled with . Note that this setting is different from the setting discussed below.\n\nlets you configure PyTorch to use deterministic algorithms instead of nondeterministic ones where available, and to throw an error if an operation is known to be nondeterministic (and without a deterministic alternative). Please check the documentation for for a full list of affected operations. If an operation does not act correctly according to the documentation, or if you need a deterministic implementation of an operation that does not have one, please submit an issue: https://github.com/pytorch/pytorch/issues?q=label:%22module:%20determinism%22 For example, running the nondeterministic CUDA implementation of will throw an error: >>> import torch >>> torch.use_deterministic_algorithms(True) >>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2)) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> RuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. ... When is called with sparse-dense CUDA tensors it typically uses a nondeterministic algorithm, but when the deterministic flag is turned on, its alternate deterministic implementation will be used: Furthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, you should set the environment variable according to CUDA documentation: https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility While disabling CUDA convolution benchmarking (discussed above) ensures that CUDA selects the same algorithm each time an application is run, that algorithm itself may be nondeterministic, unless either or is set. The latter setting controls only this behavior, unlike which will make other PyTorch operations behave deterministically, too. In some versions of CUDA, RNNs and LSTM networks may have non-deterministic behavior. See and for details and workarounds. Operations like and can return tensors with uninitialized memory that contain undefined values. Using such a tensor as an input to another operation is invalid if determinism is required, because the output will be nondeterministic. But there is nothing to actually prevent such invalid code from being run. So for safety, is set to by default, which will fill the uninitialized memory with a known value if is set. This will prevent the possibility of this kind of nondeterministic behavior. However, filling uninitialized memory is detrimental to performance. So if your program is valid and does not use uninitialized memory as the input to an operation, then this setting can be turned off for better performance."
    },
    {
        "link": "https://pytorch.org/docs/stable/data.html",
        "document": "At the heart of PyTorch data loading utility is the class. It represents a Python iterable over a dataset, with support for\n\nThese options are configured by the constructor arguments of a , which has signature:\n\nThe sections below describe in details the effects and usages of these options.\n\nThe most important argument of constructor is , which indicates a dataset object to load data from. PyTorch supports two different types of datasets: A map-style dataset is one that implements the and protocols, and represents a map from (possibly non-integral) indices/keys to data samples. For example, such a dataset, when accessed with , could read the -th image and its corresponding label from a folder on the disk. See for more details. An iterable-style dataset is an instance of a subclass of that implements the protocol, and represents an iterable over data samples. This type of datasets is particularly suitable for cases where random reads are expensive or even improbable, and where the batch size depends on the fetched data. For example, such a dataset, when called , could return a stream of data reading from a database, a remote server, or even logs generated in real time. See for more details. When using a with multi-process data loading. The same dataset object is replicated on each worker process, and thus the replicas must be configured differently to avoid duplicated data. See documentations for how to achieve this.\n\nFor iterable-style datasets, data loading order is entirely controlled by the user-defined iterable. This allows easier implementations of chunk-reading and dynamic batch size (e.g., by yielding a batched sample at each time). The rest of this section concerns the case with map-style datasets. classes are used to specify the sequence of indices/keys used in data loading. They represent iterable objects over the indices to datasets. E.g., in the common case with stochastic gradient decent (SGD), a could randomly permute a list of indices and yield each one at a time, or yield a small number of them for mini-batch SGD. A sequential or shuffled sampler will be automatically constructed based on the argument to a . Alternatively, users may use the argument to specify a custom object that at each time yields the next index/key to fetch. A custom that yields a list of batch indices at a time can be passed as the argument. Automatic batching can also be enabled via and arguments. See the next section for more details on this. Neither nor is compatible with iterable-style datasets, since such datasets have no notion of a key or an index.\n\nsupports automatically collating individual fetched data samples into batches via arguments , , , and (which has a default function). This is the most common case, and corresponds to fetching a minibatch of data and collating them into batched samples, i.e., containing Tensors with one dimension being the batch dimension (usually the first). When (default ) is not , the data loader yields batched samples instead of individual samples. and arguments are used to specify how the data loader obtains batches of dataset keys. For map-style datasets, users can alternatively specify , which yields a list of keys at a time. The and arguments essentially are used to construct a from . For map-style datasets, the is either provided by user or constructed based on the argument. For iterable-style datasets, the is a dummy infinite one. See this section on more details on samplers. When fetching from iterable-style datasets with multi-processing, the argument drops the last non-full batch of each worker’s dataset replica. After fetching a list of samples using the indices from sampler, the function passed as the argument is used to collate lists of samples into batches. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: A custom can be used to customize collation, e.g., padding sequential data to max length of a batch. See this section on more about . In certain cases, users may want to handle batching manually in dataset code, or simply load individual samples. For example, it could be cheaper to directly load batched data (e.g., bulk reads from a database or reading continuous chunks of memory), or the batch size is data dependent, or the program is designed to work on individual samples. Under these scenarios, it’s likely better to not use automatic batching (where is used to collate the samples), but let the data loader directly return each member of the object. When both and are (default value for is already ), automatic batching is disabled. Each sample obtained from the is processed with the function passed as the argument. When automatic batching is disabled, the default simply converts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: and loading from an iterable-style dataset is roughly equivalent with: See this section on more about . The use of is slightly different when automatic batching is enabled or disabled. When automatic batching is disabled, is called with each individual data sample, and the output is yielded from the data loader iterator. In this case, the default simply converts NumPy arrays in PyTorch tensors. When automatic batching is enabled, is called with a list of data samples at each time. It is expected to collate the input samples into a batch for yielding from the data loader iterator. The rest of this section describes the behavior of the default ( ). For instance, if each data sample consists of a 3-channel image and an integral class label, i.e., each element of the dataset returns a tuple , the default collates a list of such tuples into a single tuple of a batched image tensor and a batched class label Tensor. In particular, the default has the following properties:\n• None It always prepends a new dimension as the batch dimension.\n• None It automatically converts NumPy arrays and Python numerical values into PyTorch Tensors.\n• None It preserves the data structure, e.g., if each sample is a dictionary, it outputs a dictionary with the same set of keys but batched Tensors as values (or lists if the values can not be converted into Tensors). Same for s, s, s, etc. Users may use customized to achieve custom batching, e.g., collating along a dimension other than the first, padding sequences of various lengths, or adding support for custom data types. If you run into a situation where the outputs of have dimensions or type that is different from your expectation, you may want to check your .\n\nWithin a Python process, the Global Interpreter Lock (GIL) prevents true fully parallelizing Python code across threads. To avoid blocking computation code with data loading, PyTorch provides an easy switch to perform multi-process data loading by simply setting the argument to a positive integer. In this mode, data fetching is done in the same process a is initialized. Therefore, data loading may block computing. However, this mode may be preferred when resource(s) used for sharing data among processes (e.g., shared memory, file descriptors) is limited, or when the entire dataset is small and can be loaded entirely in memory. Additionally, single-process loading often shows more readable error traces and thus is useful for debugging. Setting the argument as a positive integer will turn on multi-process data loading with the specified number of loader worker processes. After several iterations, the loader worker processes will consume the same amount of CPU memory as the parent process for all Python objects in the parent process which are accessed from the worker processes. This can be problematic if the Dataset contains a lot of data (e.g., you are loading a very large list of filenames at Dataset construction time) and/or you are using a lot of workers (overall memory usage is ). The simplest workaround is to replace Python objects with non-refcounted representations such as Pandas, Numpy or PyArrow objects. Check out issue #13246 for more details on why this occurs and example code for how to workaround these problems. In this mode, each time an iterator of a is created (e.g., when you call ), worker processes are created. At this point, the , , and are passed to each worker, where they are used to initialize, and fetch data. This means that dataset access together with its internal IO, transforms (including ) runs in the worker process. returns various useful information in a worker process (including the worker id, dataset replica, initial seed, etc.), and returns in main process. Users may use this function in dataset code and/or to individually configure each dataset replica, and to determine whether the code is running in a worker process. For example, this can be particularly helpful in sharding the dataset. For map-style datasets, the main process generates the indices using and sends them to the workers. So any shuffle randomization is done in the main process which guides loading by assigning indices to load. For iterable-style datasets, since each worker process gets a replica of the object, naive multi-process loading will often result in duplicated data. Using and/or , users may configure each replica independently. (See documentations for how to achieve this. ) For similar reasons, in multi-process loading, the argument drops the last non-full batch of each worker’s iterable-style dataset replica. Workers are shut down once the end of the iteration is reached, or when the iterator becomes garbage collected. It is generally not recommended to return CUDA tensors in multi-process loading because of many subtleties in using CUDA and sharing CUDA tensors in multiprocessing (see CUDA in multiprocessing). Instead, we recommend using automatic memory pinning (i.e., setting ), which enables fast data transfer to CUDA-enabled GPUs. Since workers rely on Python , worker launch behavior is different on Windows compared to Unix.\n• None On Unix, is the default start method. Using , child workers typically can access the and Python argument functions directly through the cloned address space.\n• None On Windows or MacOS, is the default start method. Using , another interpreter is launched which runs your main script, followed by the internal worker function that receives the , and other arguments through serialization. This separate serialization means that you should take two steps to ensure you are compatible with Windows while using multi-process data loading:\n• None Wrap most of you main script’s code within block, to make sure it doesn’t run again (most likely generating error) when each worker process is launched. You can place your dataset and instance creation logic here, as it doesn’t need to be re-executed in workers.\n• None Make sure that any custom , or code is declared as top level definitions, outside of the check. This ensures that they are available in worker processes. (this is needed since functions are pickled as references only, not .) By default, each worker will have its PyTorch seed set to , where is a long generated by main process using its RNG (thereby, consuming a RNG state mandatorily) or a specified . However, seeds for other libraries may be duplicated upon initializing workers, causing each worker to return identical random numbers. (See this section in FAQ.). In , you may access the PyTorch seed set for each worker with either or , and use it to seed other libraries before data loading.\n\nHost to GPU copies are much faster when they originate from pinned (page-locked) memory. See Use pinned memory buffers for more details on when and how to use pinned memory generally. For data loading, passing to a will automatically put the fetched data Tensors in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs. The default memory pinning logic only recognizes Tensors and maps and iterables containing Tensors. By default, if the pinning logic sees a batch that is a custom type (which will occur if you have a that returns a custom batch type), or if each element of your batch is a custom type, the pinning logic will not recognize them, and it will return that batch (or those elements) without pinning the memory. To enable memory pinning for custom batch or data type(s), define a method on your custom type(s). Data loader combines a dataset and a sampler, and provides an iterable over the given dataset. The supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning. See documentation page for more details.\n• None dataset (Dataset) – dataset from which to load the data.\n• None batch_size (int, optional) – how many samples per batch to load (default: ).\n• None shuffle (bool, optional) – set to to have the data reshuffled at every epoch (default: ).\n• None sampler (Sampler or Iterable, optional) – defines the strategy to draw samples from the dataset. Can be any with implemented. If specified, must not be specified.\n• None batch_sampler (Sampler or Iterable, optional) – like , but returns a batch of indices at a time. Mutually exclusive with , , , and .\n• None num_workers (int, optional) – how many subprocesses to use for data loading. means that the data will be loaded in the main process. (default: )\n• None collate_fn (Callable, optional) – merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset.\n• None pin_memory (bool, optional) – If , the data loader will copy Tensors into device/CUDA pinned memory before returning them. If your data elements are a custom type, or your returns a batch that is a custom type, see the example below.\n• None drop_last (bool, optional) – set to to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: )\n• None timeout (numeric, optional) – if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: )\n• None worker_init_fn (Callable, optional) – If not , this will be called on each worker subprocess with the worker id (an int in ) as input, after seeding and before data loading. (default: )\n• None multiprocessing_context (str or multiprocessing.context.BaseContext, optional) – If , the default multiprocessing context of your operating system will be used. (default: )\n• None generator (torch.Generator, optional) – If not , this RNG will be used by RandomSampler to generate random indexes and multiprocessing to generate for workers. (default: )\n• None prefetch_factor (int, optional, keyword-only arg) – Number of batches loaded in advance by each worker. means there will be a total of 2 * num_workers batches prefetched across all workers. (default value depends on the set value for num_workers. If value of num_workers=0 default is . Otherwise, if value of default is ).\n• None persistent_workers (bool, optional) – If , the data loader will not shut down the worker processes after a dataset has been consumed once. This allows to maintain the workers instances alive. (default: )\n• None pin_memory_device (str, optional) – the device to to if is .\n• None in_order (bool, optional) – If , the data loader will not enforce that batches are returned in a first-in, first-out order. Only applies when . (default: ) If the start method is used, cannot be an unpicklable object, e.g., a lambda function. See Multiprocessing best practices on more details related to multiprocessing in PyTorch. heuristic is based on the length of the sampler used. When is an , it instead returns an estimate based on , with proper rounding depending on , regardless of multi-process loading configurations. This represents the best guess PyTorch can make because PyTorch trusts user code in correctly handling multi-process loading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches, this estimate can still be inaccurate, because (1) an otherwise complete batch can be broken into multiple ones and (2) more than one batch worth of samples can be dropped when is set. Unfortunately, PyTorch can not detect such cases in general. See Dataset Types for more details on these two types of datasets and how interacts with Multi-process data loading. See Reproducibility, and My data loader workers return identical random numbers, and Randomness in multi-process data loading notes for random seed related questions. Setting to can harm reproducibility and may lead to a skewed data distribution being fed to the trainer in cases with imbalanced data. All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite , supporting fetching a data sample for a given key. Subclasses could also optionally overwrite , which is expected to return the size of the dataset by many implementations and the default options of . Subclasses could also optionally implement , for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples. by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided. All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream. All subclasses should overwrite , which would return an iterator of samples in this dataset. When a subclass is used with , each item in the dataset will be yielded from the iterator. When , each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. , when called in a worker process, returns information about the worker. It can be used in either the dataset’s method or the ‘s option to modify each copy’s behavior. Example 1: splitting workload across all workers in : \"this example code only works with end >= start\" # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. Example 2: splitting workload across all workers using : \"this example code only works with end >= start\" # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. # the dataset copy in this worker process # configure the dataset to only process the split workload Each sample will be retrieved by indexing tensors along the first dimension. *tensors (Tensor) – tensors that have the same size of the first dimension. This class is useful to assemble different parts of complex input data, given as datasets. This class is useful to assemble different existing datasets. datasets (sequence) – List of datasets to be concatenated This class is useful to assemble different existing dataset streams. The chaining operation is done on-the-fly, so concatenating large-scale datasets with this class will be efficient. datasets (iterable of IterableDataset) – datasets to be chained together Subset of a dataset at specified indices.\n• None indices (sequence) – Indices in the whole set selected for subset General collate function that handles collection type of element within each batch. The function also opens function registry to deal with specific element types. provides default collate functions for tensors, numpy arrays, numbers and strings.\n• None collate_fn_map (Optional[Dict[Union[Type, Tuple[Type, ...]], Callable]]) – Optional dictionary mapping from element type to the corresponding collate function. If the element type isn’t present in this dictionary, this function will go through each key of the dictionary in the insertion order to invoke the corresponding collate function if the element type is a subclass of the key. # Extend this function to handle batch of tensors Each collate function requires a positional argument for batch and a keyword argument for the dictionary of collate functions as . Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size. The exact output type can be a , a of , a Collection of , or left unchanged, depending on the input type. This is used as the default function for collation when or is defined in . Here is the general input type (based on the type of the element within the batch) to output type mapping: # Example with a batch of `int`s: # Example with a batch of `str`s: # Example with `Map` inside the batch: # Example with `NamedTuple` inside the batch: # Example with `Tuple` inside the batch: # Example with `List` inside the batch: # Two options to extend `default_collate` to handle specific type If the input is a , , or , it tries to convert each element inside to a . If the input is not an NumPy array, it is left unchanged. This is used as the default function for collation when both and are NOT defined in . The general input type to output type mapping is similar to that of . See the description there for more details. Returns the information about the current iterator worker process. When called in a worker, this returns an object guaranteed to have the following attributes:\n• None : the random seed set for the current worker. This value is determined by main process RNG and the worker id. See ’s documentation for more details.\n• None : the copy of the dataset object in this process. Note that this will be a different object in a different process than the one in the main process. When called in the main process, this returns . When used in a passed over to , this method can be useful to set up each worker process differently, for instance, using to configure the object to only read a specific fraction of a sharded dataset, or use to seed other libraries used in dataset code. Randomly split a dataset into non-overlapping new datasets of given lengths. If a list of fractions that sum up to 1 is given, the lengths will be computed automatically as floor(frac * len(dataset)) for each fraction provided. After computing the lengths, if there are any remainders, 1 count will be distributed in round-robin fashion to the lengths until there are no remainders left.\n• None lengths (sequence) – lengths or fractions of splits to be produced\n• None generator (Generator) – Generator used for the random permutation. Every Sampler subclass has to provide an method, providing a way to iterate over indices or lists of indices (batches) of dataset elements, and may provide a method that returns the length of the returned iterators. data_source (Dataset) – This argument is not used and will be removed in 2.2.0. You may still have custom implementation that utilizes it. The method isn’t strictly required by , but is expected in any calculation involving the length of a . Samples elements sequentially, always in the same order. Samples elements randomly. If without replacement, then sample from a shuffled dataset. If with replacement, then user can specify to draw.\n• None replacement (bool) – samples are drawn on-demand with replacement if , default=``False`` Samples elements randomly from a given list of indices, without replacement. Samples elements from with given probabilities (weights).\n• None weights (sequence) – a sequence of weights, not necessary summing up to one\n• None replacement (bool) – if , samples are drawn with replacement. If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row. Wraps another sampler to yield a mini-batch of indices.\n• None sampler (Sampler or Iterable) – Base sampler. Can be any iterable object\n• None drop_last (bool) – If , the sampler will drop the last batch if its size would be less than Sampler that restricts data loading to a subset of the dataset. It is especially useful in conjunction with . In such a case, each process can pass a instance as a sampler, and load a subset of the original dataset that is exclusive to it. Dataset is assumed to be of constant size and that any instance of it always returns the same elements in the same order.\n• None num_replicas (int, optional) – Number of processes participating in distributed training. By default, is retrieved from the current distributed group.\n• None rank (int, optional) – Rank of the current process within . By default, is retrieved from the current distributed group.\n• None shuffle (bool, optional) – If (default), sampler will shuffle the indices.\n• None seed (int, optional) – random seed used to shuffle the sampler if . This number should be identical across all processes in the distributed group. Default: .\n• None drop_last (bool, optional) – if , then the sampler will drop the tail of the data to make it evenly divisible across the number of replicas. If , the sampler will add extra indices to make the data evenly divisible across the replicas. Default: . In distributed mode, calling the method at the beginning of each epoch before creating the iterator is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be always used."
    },
    {
        "link": "https://discuss.pytorch.org/t/what-is-manual-seed/5939",
        "document": "hi, i saw this from a dcgan codes . it seems it relates to cuda\n\n here are parts of the code if opt.manualSeed is None:\n\n opt.manualSeed = random.randint(1, 10000)\n\n print(\"Random Seed: \", opt.manualSeed)\n\n random.seed(opt.manualSeed)\n\n torch.manual_seed(opt.manualSeed)\n\n if opt.cuda:\n\n torch.cuda.manual_seed_all(opt.manualSeed)\n\nBut I still cannot understand why there should exist such code. Where are these generated random numbers used? It seems that “opt.manualSeed” has never been called later on.\n\nYou just need to call , and it will set the seed of the random number generator to a fixed value, so that when you call for example , the results will be reproducible.\n\n An example Try now without the , and you’ll see that it changes over time.\n\nIt’s probably defined in the DCGAN example. In this line the arguments from the are assigned to . However, this is just an example code. You can use whatever number you like.\n\nNo.\n\n The random values will be still “random” but in a defined order.\n\n I.e. if you restart your script, the same random numbers will be created.\n\n Have a look at PRNG for more information.\n\nThus, to seed everything, on the assumption one is using PyTorch and Numpy: Anything else is missing?"
    },
    {
        "link": "https://pytorch.org/xla/release/2.3/index.html",
        "document": "PyTorch runs on XLA devices, like TPUs, with the torch_xla package. This document describes how to run your models on these devices. PyTorch/XLA adds a new device type to PyTorch. This device type works just like other PyTorch device types. For example, here’s how to create and print an XLA tensor: This code should look familiar. PyTorch/XLA uses the same interface as regular PyTorch with a few additions. Importing initializes PyTorch/XLA, and returns the current XLA device. This may be a CPU or TPU depending on your environment. PyTorch operations can be performed on XLA tensors just like CPU or CUDA tensors. For example, XLA tensors can be added together: Or used with neural network modules: Like other device types, XLA tensors only work with other XLA tensors on the same device. So code like # Input tensor is not an XLA tensor: torch.FloatTensor will throw an error since the module is on the CPU. Building a new PyTorch network or converting an existing one to run on XLA devices requires only a few lines of XLA-specific code. The following snippets highlight these lines when running on a single device and multiple devices with XLA multi-processing. The following snippet shows a network training on a single XLA device: This snippet highlights how easy it is to switch your model to run on XLA. The model definition, dataloader, optimizer and training loop can work on any device. The only XLA-specific code is a couple lines that acquire the XLA device and mark the step. Calling at the end of each training iteration causes XLA to execute its current graph and update the model’s parameters. See XLA Tensor Deep Dive for more on how XLA creates graphs and runs operations. PyTorch/XLA makes it easy to accelerate training by running on multiple XLA devices. The following snippet shows how: There are three differences between this multi-device snippet and the previous single device snippet. Let’s go over then one by one.\n• \n• None Creates the processes that each run an XLA device.\n• None Each process will only be able to access the device assigned to the current process. For example on a TPU v4-8, there will be 4 processes being spawn up and each process will own a TPU device.\n• None Note that if you print the on each process you will see on all devices. This is because each process can only see one device. This does not mean multi-process is not functioning. The only execution is with PJRT runtime on TPU v2 and TPU v3 since there will be processes and each process will have 2 threads(check this doc for more details).\n• \n• None Loads the training data onto each device.\n• None can wrap on a torch dataloader. It can preload the data to the device and overlap the dataloading with device execution to improve the performance.\n• None also call for you every (default to 1) batch being yield.\n• \n• None Consolidates the gradients between devices and issues the XLA device step computation.\n• None It is pretty much a + + and returns the loss being reduced. The model definition, optimizer definition and training loop remain the same. NOTE: It is important to note that, when using multi-processing, the user can start retrieving and accessing XLA devices only from within the target function of (or any function which has as parent in the call stack). See the full multiprocessing example for more on training a network on multiple XLA devices with multi-processing. Multi-host setup for different accelerators can be very different. This doc will talk about the device independent bits of multi-host training and will use the TPU + PJRT runtime(currently available on 1.13 and 2.x releases) as an example. Before you being, please take a look at our user guide at here which will explain some Google Cloud basis like how to use command and how to setup your project. You can also check here for all Cloud TPU Howto. This doc will focus on the PyTorch/XLA perspective of the Setup. Let’s assume you have the above mnist example from above section in a . If it is a single host multi device training, you would ssh to the TPUVM and run command like Now in order to run the same models on a TPU v4-16 (which has 2 host, each with 4 TPU devices), you will need to\n• None Make sure each host can access the training script and training data. This is usually done by using the command or command to copy the training scripts to all hosts.\n• None Run the same training command on all hosts at the same time. Above command will ssh to all hosts in TPUVM Pod and run the same command at the same time.. NOTE: You need to run run above command outside of the TPUVM vm. The model code and training script is the same for the multi-process training and the multi-host training. PyTorch/XLA and the underlying infrastructure will make sure each device is aware of the global topology and each device’s local and global ordinal. Cross-device communication will happen across all devices instead of local devices. For more details regarding PJRT runtime and how to run it on pod, please refer to this doc. For more information about PyTorch/XLA and TPU pod and a complete guide to run a resnet50 with fakedata on TPU pod, please refer to this guide. Using XLA tensors and devices requires changing only a few lines of code. But even though XLA tensors act a lot like CPU and CUDA tensors, their internals are different. This section describes what makes XLA tensors unique. CPU and CUDA tensors launch operations immediately or eagerly. XLA tensors, on the other hand, are lazy. They record operations in a graph until the results are needed. Deferring execution like this lets XLA optimize it. A graph of multiple separate operations might be fused into a single optimized operation, for example. Lazy execution is generally invisible to the caller. PyTorch/XLA automatically constructs the graphs, sends them to XLA devices, and synchronizes when copying data between an XLA device and the CPU. Inserting a barrier when taking an optimizer step explicitly synchronizes the CPU and the XLA device. For more information about our lazy tensor design, you can read this paper. PyTorch/XLA can use the bfloat16 datatype when running on TPUs. In fact, PyTorch/XLA handles float types ( and ) differently on TPUs. This behavior is controlled by the and environment variable:\n• None By default both and are on TPUs.\n• None If is set, then and are both on TPUs.\n• None If is set, then is on TPUs and is on TPUs.\n• None If a PyTorch tensor has data type, this will be directly mapped to the TPU (XLA primitive type). Developers should note that XLA tensors on TPUs will always report their PyTorch datatype regardless of the actual datatype they’re using. This conversion is automatic and opaque. If an XLA tensor on a TPU is moved back to the CPU it will be converted from its actual datatype to its PyTorch datatype. Depending on how your code operates, this conversion triggered by the type of processing unit can be important. The internal data representation of XLA tensors is opaque to the user. They do not expose their storage and they always appear to be contiguous, unlike CPU and CUDA tensors. This allows XLA to adjust a tensor’s memory layout for better performance. Moving XLA Tensors to and from the CPU¶ XLA tensors can be moved from the CPU to an XLA device and from an XLA device to the CPU. If a view is moved then the data its viewing is also copied to the other device and the view relationship is not preserved. Put another way, once data is copied to another device it has no relationship with its previous device or any tensors on it. Again, depending on how your code operates, appreciating and accommodating this transition can be important. XLA tensors should be moved to the CPU before saving, as in the following snippet: This lets you put the loaded tensors on any available device, not just the one on which they were initialized. Per the above note on moving XLA tensors to the CPU, care must be taken when working with views. Instead of saving views it is recommended that you recreate them after the tensors have been loaded and moved to their destination device(s). A utility API is provided to save data by taking care of previously moving it to CPU: In case of multiple devices, the above API will only save the data for the master device ordinal (0). In case where memory is limited compared to the size of the model parameters, an API is provided that reduces the memory footprint on the host: This API streams XLA tensors to CPU one at a time, reducing the amount of host memory used, but it requires a matching load API to restore: Directly saving XLA tensors is possible but not recommended. XLA tensors are always loaded back to the device they were saved from, and if that device is unavailable the load will fail. PyTorch/XLA, like all of PyTorch, is under active development and this behavior may change in the future. The XLA compiler converts the traced HLO into an executable which runs on the devices. Compilation can be time consuming, and in cases where the HLO doesn’t change across executions, the compilation result can be persisted to disk for reuse, significantly reducing development iteration time. Note that if the HLO changes between executions, a recompilation will still occur. This is currently an experimental opt-in API, which must be activated before any computations are executed. Initialization is done through the API: This will initialize a persistent compilation cache at the specified path. The parameter can be used to control whether the worker will be able to write to the cache, which can be useful when a shared cache mount is used for an SPMD workload. Additional documentation is available at the PyTorch/XLA repo. More examples of running networks on TPUs are available here.\n\nReturns a given instance of an XLA device.\n• None n (python:int, optional) – The specific instance (ordinal) to be returned. If specified, the specific XLA device instance will be returned. Otherwise the first device of will be returned.\n• None devkind (string..., optional) – If specified, device type such as , , , or custom PJRT device. Deprecated. Returns a list of supported devices of a given kind.\n• None devkind (string..., optional) – If specified, a device type such as , , , or name of custom PJRT device.\n• None max_devices (python:int, optional) – The maximum number of devices to be returned of that kind. The list of device strings such as [‘xla Returns the hardware type of the given device. device (string or torch.device) – The xla device that will be mapped to the real device. A string representation of the hardware type of the given device. Retrieves the replication ordinal of the current thread. The ordinals range from 0 to minus 1. defval (python:int, optional) – The default value to be returned in case there is no replication information available. Ignored for runtime. Default: 0 The replication ordinal of the current thread. Retrieves the replication local ordinal of the current thread. The local ordinals range from 0 to the number of local devices minus 1. defval (python:int, optional) – The default value to be returned in case there is no replication information available. Ignored for runtime. Default: 0 The replication local ordinal of the current thread. Checks whether the current process is the master ordinal (0). local (bool) – Whether the local or global master ordinal should be checked. In case of multi-host replication, there is only one global master ordinal (host 0, device 0), while there are NUM_HOSTS local master ordinals. Default: True A boolean indicating whether the current process is the master ordinal. Retrieves the number of devices which is taking part of the replication. defval (python:int, optional) – The default value to be returned in case there is no replication information available. Default: 1 The number of devices which is taking part of the replication. Performs an inplace reduce operation on the input tensor(s).\n• None reduce_type (string) – One of , , , , and .\n• None inputs – Either a single or a list of to perform the all reduce op to.\n• None scale (python:float) – A default scaling value to be applied after the reduce. Default: 1.0\n• None A list of list, representing the replica groups for the operation. Example: defines two groups, one with the replicas and one with the replicas. If there will be only one group with all the replicas in it.\n• None pin_layout (bool, optional) – whether to pin the layout for this communication op. Layout pining can prevent potential data corruption when each process that participate in the communication has slightly different program, but it might cause some xla compilation to fail. Unpin the layout when you see error message like “HloModule has a mix of layout constrained”. If a single is passed, the return value is a holding the reduced value (across the replicas). If a list/tuple is passed, this function performs an inplace all-reduce op on the input tensors, and returns the list/tuple itself. Performs an all-gather operation along a given dimension.\n• None A list of list, representing the replica groups for the operation. Example: defines two groups, one with the replicas and one with the replicas. If there will be only one group with all the replicas in it.\n• None pin_layout (bool, optional) – whether to pin the layout for this communication op. Layout pining can prevent potential data corruption when each process that participate in the communication has slightly different program, but it might cause some xla compilation to fail. Unpin the layout when you see error message like “HloModule has a mix of layout constrained”. A tensor which has, in the dimension, all the values from the participating replicas. Performs an XLA operation on the input tensor.\n• None split_dimension (python:int) – The dimension upon which the split should happen.\n• None concat_dimension (python:int) – The dimension upon which the concat should happen.\n• None A list of list, representing the replica groups for the operation. Example: defines two groups, one with the replicas and one with the replicas. If there will be only one group with all the replicas in it.\n• None pin_layout (bool, optional) – whether to pin the layout for this communication op. Layout pining can prevent potential data corruption when each process that participate in the communication has slightly different program, but it might cause some xla compilation to fail. Unpin the layout when you see error message like “HloModule has a mix of layout constrained”. The result of the operation. Adds a closure to the list of the ones to be run at the end of the step. Many times during model training there is the need to print/report (print to console, post to tensorboard, etc…) information which require the content of intermediary tensors to be inspected. Inspecting different tensors content in different points of the model code requires many executions and typically causes performance issues. Adding a step closure will ensure that it will be run after the barrier, when all the live tensors will be already materialized to device data. Live tensors which will include the ones captured by the closure arguments. So using will ensure a single execution will be performed, even when multiple closures are queued, requiring multiple tensors to be inspected. Step closures will be run sequentially in the order they have been queued. Note that even though using this API the execution will be optimized, it is advised to throttle the printing/reporting events once every N steps.\n• None closure (callable) – The function to be called.\n• None args (tuple) – The arguments to be passed to the closure. Waits for all the async operations on the given devices to complete. devices (string..., optional) – The devices whose async ops need to be waited for. If empty, all the local devices will be waited for. Run the provided optimizer step and issue the XLA device step computation.\n• None optimizer ( ) – The instance whose function needs to be called. The function will be called with the named arguments.\n• None barrier (bool, optional) – Whether the XLA tensor barrier should be issued in this API. If using the PyTorch XLA or support, this is not necessary as the barrier will be issued by the XLA data loader iterator call. Default: False\n• None A list of list, representing the replica groups for the operation. Example: defines two groups, one with the replicas and one with the replicas. If there will be only one group with all the replicas in it.\n• None pin_layout (bool, optional) – whether to pin the layout when reducing gradients. See for details. The same value returned by the call. The saved data is transferred to PyTorch CPU device before being saved, so a following will load CPU data. Care must be taken when working with views. Instead of saving views it’s recommended that you recreate them after the tensors have been loaded and moved to their destination device(s).\n• None data – The input data to be saved. Any nested combination of Python objects (list, tuples, sets, dicts, …).\n• None file_or_path – The destination for the data saving operation. Either a file path or a Python file object. If is the path or file objects must point to different destinations as otherwise all the writes from the same host will override each other.\n• None master_only (bool, optional) – Whether only the master device should save the data. If False, the argument should be a different file or path for each of the ordinals taking part to the replication, otherwise all the replicas on the same host will be writing to the same location. Default: True\n• None global_master (bool, optional) – When is this flag controls whether every host’s master (if is ) saves the content, or only the global master (ordinal 0). Default: False\n• None sync (bool, optional) – Whether to synchronize all replicas after saving tensors. If True, all replicas must call or the main process will hang. Waits for all the mesh clients to reach the named rendezvous. Note: PJRT does not support the XRT mesh server, so this is effectively an alias to .\n• None tag (string) – The name of the rendezvous to join.\n• None payload (bytes, optional) – The payload to be sent to the rendezvous.\n• None replicas (list, python:int) – The replica ordinals taking part of the rendezvous. Empty means all replicas in the mesh. Default: [] The payloads exchanged by all the other cores, with the payload of core ordinal at position in the returned tuple. Runs a function only on a given set of ordinals.\n• None target (callable) – The function to be run on .\n• None data – Any input data for the function which contains tensors. All the XLA tensors used by the function must be passed in this argument. Every other data used by the function can be captured by the Python interpreter as usual. Default: ()\n• None ordinals (list, python:int) – The list/set of ordinals where the function should run. Default: (0,) In the ordinals that ran the function, the function return value, otherwise .\n• None tag (string) – The name of the rendezvous to join.\n• None data – The data to be reduced. The callable will receive a list with the copies of the same data coming from all the mesh client processes (one per core).\n• None reduce_fn (callable) – A function which receives a list of -like objects and returns the reduced result.\n• None seed (python:integer) – The state to be set.\n• None device (string, optional) – The device where the RNG state needs to be set. If missing the default device seed will be set. device (string, optional) – The device whose RNG state needs to be retrieved. If missing the default device seed will be set. device (string) – The device whose memory information are requested. A dictionary with (free memory in KB) and (total memory in KB) keys. Get StableHLO for the computation graph in string format. If is not empty, the graph with as outputs will be dump. If is empty, the whole computation graph will be dump. TODO(lsy323): When is empty, the some intermediate tensors will also be dump as outputs. Need further investigation. For inference graph, it is recommended to pass the model outputs to . For training graph, it is not straightforward to identify the “outputs”. Using empty is recommended. To enable source line info in StableHLO, please set env var XLA_HLO_DEBUG=1. tensors (list[torch.Tensor], optional) – Tensors that represent the output/root of the StableHLO graph. Get StableHLO for the computation graph in bytecode format. If is not empty, the graph with as outputs will be dump. If is empty, the whole computation graph will be dump. TODO(lsy323): When is empty, the some intermediate tensors will also be dump as outputs. Need further investigation. For inference graph, it is recommended to pass the model outputs to . For training graph, it is not straightforward to identify the “outputs”. Using empty is recommended. tensors (list[torch.Tensor], optional) – Tensors that represent the output/root of the StableHLO graph. Performs an inplace reduce operation on the input tensor. This is the same as but supports autograd differentiation.\n• None reduce_type (string) – One of , , , , and .\n• None value (torch.Tensor) – The to perform the all reduce op to.\n• None scale (python:float) – A default scaling value to be applied after the reduce. Default: 1.0\n• None A list of list, representing the replica groups for the operation. Example: defines two groups, one with the replicas and one with the replicas. If there will be only one group with all the replicas in it. The reduced value across the selected replicas. Performs an all-gather operation along a given dimension. This is the same as but supports autograd differentiation. A tensor which has, in the dimension, all the values from the participating replicas.\n• None boxes (torch.Tensor) – A of shape listing the boxes coordinates in form.\n• None scores (torch.Tensor) – A of shape listing the scores of each box.\n• None score_threshold (torch.Tensor) – The minimum score for a box to qualify as valid.\n• None iou_threshold (torch.Tensor) – The minimum IOU (Intersection Over Union) score to trigger overlap logic.\n• None output_size (python:int) – The maximum number of returned indices (must be lower or equal to N). A tuple of with the first element being the selected box indices, and the second element being the number of valid boxes.\n• None loader ( ) – The PyTorch DataLoader to be wrapped.\n• None devices ( …) – The list of devices where the data has to be sent. The i-th sample returned by the will be sent to .\n• None batchdim (python:int, optional) – The dimension which is holding the batch size. Default: 0\n• None loader_prefetch_size (python:int, optional) – The max capacity of the queue used by the thread which is reading samples from the , to be processed by the worker threads which upload data to the devices. Default: 8\n• None device_prefetch_size (python:int, optional) – The max size of the per-device queues, where the worker threads deposit tensors which have already been sent to devices. Default: 4\n• None host_to_device_transfer_threads (python:int, optional) – The number of threads that work in parallel to transfer data from loader queue to device queue. Default: 1\n• None input_sharding (ShardingSpec, optional) – Sharding spec to apply to compatible input tensors after loading. Default: None Retrieves the loader iterator object for the given device. device ( ) – The device whole loader is being requested. The loader iterator object for the . This is not a interface, but a Python iterator which returns the same tensor data structure as returned by the wrapped , but residing on XLA devices.\n• None fn (callable) – The function to be called for each device which takes part of the replication. The function will be called with a first argument being the global index of the process within the replication, followed by the arguments passed in .\n• None nprocs (python:int) – The number of processes/devices for the replication. At the moment, if specified, can be either 1 or the maximum number of devices.\n• None join (bool) – Whether the call should block waiting for the completion of the processes which have being spawned. Default: True\n• None daemon (bool) – Whether the processes being spawned should have the flag set (see Python multi-processing API). Default: False The same object returned by the API. If is 1 the function will be called directly, and the API will return None. Wraps a model to minimize host memory usage when method is used. This class should be used together with the API to minimize the use of host memory. Instead of creating models on each multiprocessing process, hence replicating the model’s initial host memory, the model is created once at global scope, and then moved into each device inside the target function. Example: This method has two advantages. First it uses only one copy of the memory pages to host the original model weights, and second it serializes the move of the wrapped model into each device, by lowering the load onto the system memory during the process. Retrieves the model moved onto the specified device. device (torch.device) – The device where the model should be moved onto. The model on the specified device. Utility to run a function in a serialized fashion among multi-core processes. # Avoid all cores downloading the same data with the serial executor. fn (callable) – The function to run in a serialized fashion. Iterator which returns multiple samples of a given input data. Can be used in place of a PyTorch to generate synthetic data.\n• None data – The data which should be returned at each iterator step.\n• None sample_count – The maximum number of samples to be returned. Utility class to wrap data structures to be sent to device. The saved data is transferred to PyTorch CPU device before being saved, so a following will load CPU data. Care must be taken when working with views. Instead of saving views it’s recommended that you recreate them after the tensors have been loaded and moved to their destination device(s).\n• None data – The input data to be saved. Any nested combination of Python objects (list, tuples, sets, dicts, …).\n• None path – The destination file for the data saving operation. If is the path must point to different destinations as otherwise all the writes from the same host will override each other.\n• None master_only (bool, optional) – Whether only the master device should save the data. If False, the argument should be a different path for each of the ordinals taking part to the replication, otherwise all the replicas on the same host will be writing to the same location. Default: True\n• None global_master (bool, optional) – When is this flag controls whether every host’s master (if is ) saves the content, or only the global master (ordinal 0). Default: False path (str) – The path passed to the API.\n\nThis document provides a high-level overview of PyTorch XLA and illustrates a few examples how PyTorch code is converted to run on XLA devices (e.g. TPUs). This is not a complete solution, and additional changes may be required depending on the specific code. However, this document should serve as a starting point for the conversion process. This section provides a brief overview of the basic details of PyTorch XLA, which should help readers better understand the required modifications and optimizations of code. It is supplement to the API guide described here. Unlike regular PyTorch, which executes code line by line and does not block execution until the value of a is fetched, PyTorch XLA works differently. It iterates through the python code and records the operations on in an intermediate representation (IR) graph until it encounters a barrier (discussed below). This process of generating the IR graph is referred to as tracing (LazyTensor tracing or code tracing). PyTorch XLA then converts the IR graph to a lower-level machine-readable format called HLO (High-Level Opcodes). HLO is a representation of a computation that is specific to the XLA compiler and allows it to generate efficient code for the hardware that it is running on. HLO is fed to the XLA compiler for compilation and optimization. Compilation is then cached by PyTorch XLA to be reused later if/when needed. The compilation of the graph is done on the host (CPU), which is the machine that runs the Python code. If there are multiple XLA devices, the host compiles the code for each of the devices separately except when using SPMD (single-program, multiple-data). For example, v4-8 has one host machine and four devices. In this case the host compiles the code for each of the four devices separately. In case of pod slices, when there are multiple hosts, each host does the compilation for XLA devices it is attached to. If SPMD is used, then the code is compiled only once (for given shapes and computations) on each host for all the devices. For more details and examples, please refer to the LazyTensor guide. The operations in the IR graph are executed only when values of tensors are needed. This is referred to as evaluation or materialization of tensors. Sometimes this is also called lazy evaluation and it can lead to significant performance improvements. The synchronous operations in Pytorch XLA, like printing, logging, checkpointing or callbacks block tracing and result in slower execution. In the case when an operation requires a specific value of an XLA tensor, e.g. , tracing is blocked until the value of that tensor is available to the host. Note that only the part of the graph responsible for computing that tensor value is executed. These operations do not cut the IR graph, but they trigger host-device communication through , which results in slower performance. A barrier is a special instruction that tells XLA to execute the IR graph and materialize the tensors. This means that the PyTorch XLA tensors will be evaluated, and the results will be available to the host. The user-exposed barrier in Pytorch XLA is xm.mark_step(), which breaks the IR graph and results in code execution on the XLA devices. One of the key properties of is that unlike synchronous operations it does not block the further tracing while the device is executing the graph. However, it does block access to the values of the tensors that are being materialized. The example in the LazyTensor guide illustrates what happens in a simple case of adding two tensors. Now, suppose we have a for loop that adds XLA tensors and uses the value later: Without a barrier, the Python tracing will result in a single graph that wraps the addition of tensors times. This is because the loop is not captured by the tracing, so each iteration of the loop will create a new subgraph corresponding to the computation of and add it to the graph. Here is an example when . However, introducing a barrier at the end of the loop will result in a smaller graph that will be compiled once during the first pass inside the loop and will be reused for the next iterations. The barrier will signal to the tracing that the graph traced so far can be submitted for execution, and if that graph has been seen before, a cached compiled program will be reused. In this case there will be a small graph that is used times. It is important to highlight that in PyTorch XLA Python code inside for loops is traced and a new graph is constructed for each iteration if there is a barrier at the end. This can be a significant performance bottleneck. The XLA graphs can be reused when the same computation happens on the same shapes of tensors. If the shapes of the inputs or intermediate tensors change, then the XLA compiler will recompile a new graph with the new tensor shapes. This means that if you have dynamic shapes or if your code does not reuse tensor graphs, running your model on XLA will not be suitable for that use case. Padding the input into a fixed shape can be an option to help avoid dynamic shapes. Otherwise, a significant amount of time will be spent by the compiler on optimizing and fusing operations which will not be used again. The trade-off between graph size and compilation time is also important to consider. If there is one large IR graph, the XLA compiler can spend a lot of time on optimization and fusion of the ops. This can result in a very long compilation time. However, the later execution may be much faster, due to the optimizations that were performed during compilation. Sometimes it is worth breaking the IR graph with . As explained above, this will result in a smaller graph that can be reused later. However making graphs smaller can reduce optimizations that otherwise could be done by the XLA compiler. Another important point to consider is MPDeviceLoader. Once your code is running on an XLA device, consider wrapping the torch dataloader with XLA which preloads data to the device to improve performance and includes in it. The latter automatically breaks the iterations over batches of data and sends them for execution. Note, if you are not using MPDeviceLoader, you might need to set in the to enable if running a training job or explicitly adding . Create TPU with base image to use nightly wheels or from the stable release by specifying the . If you have a single host VM (e.g. v4-8), you can ssh to your vm and run the following commands from the vm directly. Otherwise, in case of TPU pods, you can use similar to Next, if you are using base image, install nightly packages and required libraries\n• None Remove progress bar, printing that would access the XLA tensor values\n• None Reduce logging and callbacks that would access the XLA tensor values\n• None Profile to further optimize the code Remember: each case is unique so you might need to do something different for each case. Example 1. Stable Diffusion inference in PyTorch Lightning on a Single TPU Device¶ As a first example consider the inference code of the stable diffusion model in PyTorch Lightning which can be run from command line as For your reference, the diff of modifications described below can be found here. Let’s go over them step by step. As in the general guideline above, start with changes related to device. This inference code is written to run on GPUs and can be found in multiple places. Start making changes by removing from this line, and from here. Additionally, replace the device in this line with the device similar to the code below: Next, this particular configuration of the model is using , therefore we will modify this line as well. For simplicity we will directly define the in this tutorial, but you can pass the value to the function as well. Another place in the code that has cuda specific code is DDIM scheduler. Add on top of the file then replace these lines Next, you can reduce device (TPU) and host (CPU) communication by removing print statements, disabling progress bars, and reducing or removing callbacks and logging. These operations require the device to stop executing, falling back to the CPU, executing the logging/callbacks, and then returning to the device. This can be a significant performance bottleneck, especially on large models. After making these changes, the code will run on TPUs. However, the performance will be very slow. This is because the XLA compiler tries to build a single (huge) graph that wraps the number of inference steps (in this case, 50) as there is no barrier inside the for loop. It is difficult for the compiler to optimize the graph, and this leads to significant performance degradation. As discussed above, breaking the for loop with the barrier (xm.mark_step()) will result in a smaller graph that is easier for the compiler to optimize. This will also allow the compiler to reuse the graph from the previous step, which can improve performance. Now the code is ready to run on TPUs in a reasonable time. More optimization and analysis can be done by capturing a profile and investigating further. However, this is not covered here. Note: if you are running on v4-8 TPU, then you have 4 available XLA (TPU) devices. Running the code as above will only use one XLA device. In order to run on all 4 devices you need to use function to spawn the code on all the devices. We will discuss an in the next example. Now, consider using Stable Diffusion Inference in the HuggingFace diffusers library for both the SD-XL and 2.1 versions of the model. For your reference, the changes described below can be found in this repo. You can clone the repo and run the inference using the following command on your TPU VM: Since there is no bf16 version of the SD-XL model available, you can use the flag to convert all values to bf16 and speed up training. (already includes in the 2.1 version of the model). Warning: watch out for caveats highlighted here. This section describes the changes that need to be made to the text_to_image inference example code to run it on TPUs. The original code uses Lora for inference, but this tutorial will not use it. Instead, we will set the argument to when initializing the pipeline. We will also use the default scheduler (DPMSolverMultistepScheduler). However, similar changes can be made to the other schedulers as well. Log in to HF and agree to the sd-xl 0.9 license on the model card. Next, go to account→settings→access token and generate a new token. Copy the token and run the following command with that specific token value on your vm The HuggingFace readme provides PyTorch code that is written to run on GPUs. To run it on TPUs, the first step is to change the CUDA device to an XLA device. This can be done by replacing the line with the following lines: Additionally, it is important to note that the first time you run inference with XLA, it will take a long time to compile. For example, compilation time for stable diffusion XL model inference from HuggingFace can take about an hour to compile, whereas the actual inference may take only 5 seconds, depending on the batch size. Likewise, a GPT-2 model can take about 10-15 mins to compile, after which the training epoch time becomes much faster. This is because XLA builds a graph of the computation that will be performed, and then optimizes this graph for the specific hardware that it is running on. However, once the graph has been compiled, it can be reused for subsequent inferences, which will be much faster. Therefore, if you are only running inference once, you may not benefit from using XLA. However, if you are running inference multiple times, or if you are running inference on a list of prompts, you will start to see the advantages of XLA after the first few inferences. For example, if you run inference on a list of 10 prompts, the first inference (maybe two ) may take a long time to compile, but the remaining inference steps will be much faster. This is because XLA will reuse the graph that it compiled for the first inference. If you try to run the code without making any additional changes, you will notice that the compilation time is very long (>6 hours). This is because the XLA compiler tries to build a single graph for all of the scheduler steps at once similar to what we have discussed in the previous example. To make the code run faster, we need to break the graph up into smaller pieces with and reuse them in the next steps. This happens inside the function in these lines. Disabling the progress bar, removing callbacks and adding at the end of the for loop speeds up the code significantly. Changes are provided in this commit. Additionally, the function, which by default uses the DPMSolverMultistepScheduler scheduler, has a few issues that are described in the PyTorch XLA caveats. The and calls in this function send requests to the CPU for tensor evaluation, which trigger device-host communication. This is not desirable, as it can slow down the code. In this particular case, we can avoid these calls by passing the index to the function directly. This will prevent the function from sending requests to the CPU, and will improve the performance of the code. Changes are available in this commit. The code now is ready to be run on TPUs. To further investigate the performance of the model, we can profile it using the profiling guide. As a rule of thumb, the profiling script should be run with the maximum batch size that fits into the memory for optimal memory usage. It also helps to overlap tracing of the code with device execution which leads to more optimal device usage. The duration of profiling should be long enough to capture at least one step. Good performance of the model on TPUs means that device-host communication is minimized and the device is constantly running processes with no idle time. Starting a server in the file and running script as described in the guide will give us information on processes that run on the devices. Currently, only one XLA device is profiled. To better understand the TPU idle time (gaps in the profile), profiling traces ( ) should be added to the code. The measures the time it takes to trace the python code on the host machine wrapped with the trace. For this example, traces were added inside the pipeline and the U-net model to measure the time to run specific sections of the code on the host (CPU). If the gaps in the profile are due to Python code tracing that happens on the host, then this might be a bottleneck and there is no further straightforward optimization that can be done. Otherwise, the code should be analyzed further to understand the caveats and improve the performance further. Note that you cannot wrap portions of the code where is called. To illustrate this we can look at already captured profiles that were uploaded to tensorboard following the profiling guide. If we capture a profile without inserting any traces, we will see the following: The single TPU device on v4-8, which has two cores, appears to be busy. There are no significant gaps in their usage, except for a small one in the middle. If we scroll up to try to find which process is occupying the host machine, we will not find any information. Therefore, we will add to the pipeline file as well as the U-net function. The latter may not be useful for this particular use case, but it does demonstrate how traces can be added in different places and how their information is displayed in TensorBoard. If we add traces and re-capture the profile with the largest batch size that can fit on the device (32 in this case), we will see that the gap in the device is caused by a Python process that is running on the host machine. We can use the appropriate tool to zoom in on the timeline and see which process is running during that period. This is when the Python code tracing happens on the host, and we cannot improve the tracing further at this point. Now, let’s examine the XL version of the model and do the same thing. We will add traces to the pipeline file in the same way that we did for the 2.1 version and capture a profile. This time, in addition to the large gap in the middle, which is caused by the tracing, there are many small gaps between the inference steps within this loop. First look closer into the large gap that is caused by . The gap is preceded with which indicates that something is happening on the host machine that is waiting for computation to finish before proceeding. Looking into watermark code, we can see that tensors are transferred to cpu and converted to numpy arrays in order to be processed with and libraries later. Since this part is not straightforward to optimize, we will leave this as is. Now if we zoom in on the loop, we can see that the graph within the loop is broken into smaller parts because the operation happens. If we investigate the U-Net function and the scheduler, we can see that the U-Net code does not contain any optimization targets for PyTorch/XLA. However, there are and calls inside the scheduler.step. We can rewrite the function to avoid those calls. If we fix this issue and rerun a profile, we will not see much difference. However, since we have reduced the device-host communication that was introducing smaller graphs, we allowed the compiler to optimize the code better. The function scale_model_input has similar issues, and we can fix these by making the changes we made above to the function. Overall, since many of the gaps are caused from python level code tracing and graph building, these gaps are not possible to optimize with the current version of PyTorch XLA, but we may see improvements in the future when dynamo is enabled in PyTorch XLA. To use multiple TPU devices, you can use the function to spawn the function you ran on a single device to multiple devices. The function will start processes on multiple TPU devices and sync them when needed. This can be done by passing the argument to the function that runs on a single device. For example, In this example, the function will be spawned on 4 TPU devices on v4-8, with each device being assigned an index from 0 to 3. This file illustrates how xmp.spawn can be used to run stable diffusion 2.1 version on multiple TPU devices. For this version similar to the above changes were made to the pipeline file. Once you have the code for running on a single host device, there is no further change needed. You can create the TPU pod, for example, by following these instructions. Then run your script with\n\nNote that the information in this section is subject to be removed in future releases of the PyTorch/XLA software, since many of them are peculiar to a given internal implementation which might change. Before performing any in depth debugging, we want to do a sanity check on the installed PyTorch/XLA. PyTorch and PyTorch/XLA version should match. Check out our README for more detials on versions available. For release version , you want to use the branch . For example if you installed 2.1 release, you should do If you can get the resnet to run we can conclude that torch_xla is installed correctly. To diagnose performance issues, we can use the execution metrics and counters provided by PyTorch/XLA The first thing to check when model is slow is to generate a metrics report. Metrics report is extremely helpful in diagnosing issues. Please try to include it in your bug report sent to us if you have it. You can enable the PyTorch/XLA debugging tool by setting , which provides a couple useful debugging features. You can enable the PyTorch/XLA + Dynamo debugging tool by setting . The debugging tool will analyze the metrics report and provide a summary. Some example output would be The debugging tool will analyze every compilation and execution for your model. Some example output would be Some common causes of Compilation/Executation are\n• None User trying to access(often due to logging) the value of a tensor before the . The executation caused by 1-4 are expected, and we want to avoid 5 by either reduce the frequency of accessing tensor values or manually add a before accessing. Users should expect to see this + pairs for first couple steps. After the model stabilize users should expect to only see . To use PyTorch/XLA efficiently, we expect the same models code to be run for every step and compilation only happen once for every graph. If you keep seeing , you should try to dump the IR/HLO following this section and compare the graphs for each step and understand the source of the differences. Following section will explain how to get and understand a more detail metrics report. Put the following line in your program to generate a report: # For short report that only contains a few key metrics. # For full report that includes all metrics.\n• None how many time we issue XLA compilations and time spent on issuing.\n• None how many times we execute and time spent on execution\n• None how many device data handles we create/destroy etc. This information is reported in terms of percentiles of the samples. An example is: We also provide counters, which are named integer variables which track internal software status. For example: In this report, any counter that starts with indicates a context switch between the XLA device and CPU, which can be a potential performance optimization area in the model code. Counters are useful to understand which operations are routed back to the CPU engine of PyTorch. They are fully qualified with their C++ namespace: If you see ops other than and , that usually means a missing lowering in PyTorch/XLA. Feel free to open a feature request for it on GitHub issues. If you want to clear the metrics between steps/epochs, you can use To profile your workload in depth to understand bottlenecks please check the following resources: PyTorch/XLA behaves semantically like regular PyTorch and XLA tensors share the full tensor interface with CPU & GPU tensors. However, constraints in XLA/hardware and the lazy evaluation model suggest certain patterns might result in bad performance. If your model shows bad performance, keep in mind the following caveats:\n• None XLA/TPU yield degraded performance with too many recompilations. XLA compilation is expensive. PyTorch/XLA automatically recompiles the graph every time new shapes are encountered. Usually models should stabilize within a few steps and you can see huge speedup for the rest of training. In order to avoid recompilations, not only must shapes be constant, but computations across XLA devices in all hosts should also be constant.\n• None Direct or indirect uses of introduce dynamic shapes; for example, masked indexing where is a mask tensor.\n• None Loops with a different number of iterations between steps can result in different execution graphs, thus require recompilations.\n• None Tensor shapes should be the same between iterations, or a low number of shape variations should be used.\n• None Pad tensors to fixed sizes when possible.\n• None Certain operations don’t have native translations to XLA. For these operations PyTorch/XLA automatically transfers to the CPU memory, evaluates on CPU, and transfers the result back to the XLA device. Doing too many such operations during the training step can lead to significant slowdowns.\n• None The operation explicitly asks to evaluate the result. Don’t use it unless it’s necessary.\n• None For most ops we can lower them to XLA to fix it. Checkout metrics report section to find out the missing ops and open a feature request on GitHub.\n• None Even when a PyTorch tensor is known as a scalar, avoid using . Keep it as a tensor and use tensor operations on it.\n• None Use to substitute control flow when applicable. E.g. The control flow with used in clip_grad*norm* is problematic and impacts performance, so we have patched by calling instead, which gives us a dramatic performance improvement. .. code-block:: python device = parameters[0].device total_norm = torch.zeros([], device=device if parameters else None) for p in parameters:\n• None Iterators in ``torch_xla.distributed.data_parallel`` may drop the last few batches in the input iterator. This is to make sure we do the same amount of work on all XLA devices.\n• None When dataset is small, and there are too few steps, this may result in a no-op epoch. Therefore, it is better to use small batch sizes in those cases.\n• None XLA tensor internals are opaque. XLA tensors always appear to be contiguous and without storage. Networks should not try to check the strides of XLA tensors.\n• None XLA tensors should be moved to the CPU before saving them. Saving XLA tensors directly causes them to be loaded back on the device(s) they were saved from. If a device is unavailable at load time then the load will fail. Moving XLA tensors to the CPU before saving them lets you decide which device(s) to put the loaded tensors on. This is necessary if you want to load the tensors on a machine without XLA devices. Care should be taken moving the XLA tensors to the CPU before saving them, however, as moving tensors across device types does not preserve view relationships. Instead, views should be reconstructed as necessary after the tensors are loaded.\n• None Copying an XLA Tensor with Python’s copy.copy returns a deep copy, not a shallow copy. Use a view of an XLA tensor to get a shallow copy of it.\n• None Handling shared weights. Modules can share weights by setting the Parameters of one module to another. This “tying” of module weights should be done AFTER the modules are moved to an XLA device. Otherwise two independent copies of the shared tensor will be made on the XLA device.\n\nPyTorch/XLA has migrated from the TensorFlow-based XRT runtime to the PJRT runtime used by JAX. If you encounter a bug with PJRT, please file an issue on GitHub with the tag.\n• None Public runtime APIs have moved from to .\n• None The init method has been renamed to , and it is registered by .\n• None The previous names are still available in this release for compatibility.\n• None is now supported when using .\n• None New plugins for XPU and Neuron via the PJRT C API.\n• None PJRT will be configured by default if you don’t pass in any other runtime configuration. If you continue to set XRT configuration ( ), this change has no impact\n• None New TPU runtime implementation in improves performance by up to 30%.\n• None New implementation that scales to thousands of TPU cores\n• None To use the PJRT preview runtime, set the environment variable to , , or\n• None In XRT, all distributed workloads are multiprocess, with one process per device. On TPU v2 and v3 in PJRT, workloads are multiprocess and multithreaded (4 processes with 2 threads each), so your workload should be thread-safe. See Multithreading on TPU v2/v3 and the Multiprocessing section of the API guide for more information. Key differences to keep in mind:\n• None To initialize a model in a thread-safe way, either broadcast the parameters across replicas after initialization ( ) or load each replica’s parameters from a common checkpoint.\n• None For other random number generation, use where possible. The global RNG is not thread-safe, even if you set the same across replicas.\n• None To use , import and use the .\n• None These steps are optional for GPU and TPU v4. import os import torch import torch.nn as nn from torch.nn.parallel import DistributedDataParallel as DDP import torch.optim as optim import torch.distributed as dist import torch_xla.core.xla_model as xm import torch_xla.distributed.parallel_loader as pl import torch_xla.distributed.xla_backend import torch_xla.distributed.xla_multiprocessing as xmp def _mp_fn(index): device = xm.xla_device() torch.manual_seed(42) model = nn.Linear(128, 10).to(device) model = DDP(model, gradient_as_bucket_view=True) loss_fn = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=.001) for i in range(10): data, target = torch.randn((128, 128), device=device), torch.randn((128, 10), device=device) optimizer.zero_grad() output = model(data) loss = loss_fn(output, target) loss.backward() optimizer.step() xm.mark_step() # Print mean parameters so we can confirm they're the same across replicas print([p.mean() for p in model.parameters()]) if __name__ == '__main__': xmp.spawn(_mp_fn)\n• None Simple runtime configuration: just set to , , or and start using XLA! Or, let PJRT select a device automatically based on your environment.\n• None Improved performance: reduced overhead from gRPC means faster end-to-end execution. On TorchBench 2.0, we observed a >35% improvement in training time on TPU v4.\n• None Easy pod execution: just copy your code to each TPU worker, and execute them all at the same time with .\n• None Better scaling: removes XRT’s limitation on parameter sizes and supports up to 2048 TPU chips. To start using PJRT with PyTorch/XLA, all you need to do is set the environment variable. If you’re working on a TPU v2 or v3, keep reading to learn about the differences between TPU v2 and v3 and v4. On any machine with PyTorch/XLA installed, you can run our MNIST example on CPU like this: To create a new TPU with PyTorch/XLA r2.0 installed: On a v4-8, you can run our ResNet50 example like this: By default, PJRT will use all TPU chips. To use only one TPU chip, configure and : On TPU Pods, use to run your command on each TPU in parallel: You can also use Docker to run your workload in a container with PyTorch/XLA preinstalled: export DOCKER_IMAGE=gcr.io/... # Optional: authenticate docker if your image is in a private GCP repository gcloud compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command \"sudo gcloud auth configure-docker\" # Run your workload gcloud compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command \"sudo docker run --rm --privileged --net=host -e PJRT_DEVICE=TPU $DOCKER_IMAGE python pytorch/xla/test/test_train_mp_imagenet.py --fake_data\" Note that requires privileged access to the host ( ) to expose the TPU device to the container. Docker on TPU pods is only supported with host networking at this time. See the Cloud TPU documentation for more information. To use GPUs with PJRT, simply set and configure to the number of devices on the host. For example: You can also use to initiate the single-node multi-GPU training. For example, In the above example, means how many machines (physical machines or VMs) to be used (it is 1 since we do single-node training). means how many GPU devices to be used. Note that this feature only works for cuda 12+. Similar to how PyTorch uses multi-node training, you can run the command as below:\n• None : how many GPU machines to be used.\n• None : the index of the current GPU machines. The value can be 0, 1, …, ${NUMBER_GPU_VM}-1.\n• None : the number of GPU devices to be used on the current machine.\n• None : the endpoint of the GPU machine with node_rank==0, in the form . The``host will be the internal IP address. The port` can be any available port on the machine. For single-node training/inference, this parameter can be omitted. For example, if you want to train on 2 GPU machines: machine_0 and machine_1, on the first GPU machine machine_0, run On the second GPU machine, run the difference between the 2 commands above are and potentially if you want to use different number of GPU devices on each machine. All the rest are identical. For more information about , please refer to this page. Although in most cases we expect PJRT and XRT to work mostly interchangeably from the end-user’s perspective (especially on TPU v4), there are some subtle differences that are important to keep in mind. Importantly, XRT was designed around the TPU Node architecture, so it will always spawn a client and a server process, even on TPU VMs. Thus, every batch of inputs has additional latency from serializing and deserializing data to send it over the network. PJRT uses the local device directly with no intermediate server process. In the default configuration, PJRT will create one process per TPU chip, or 4 processes per TPU host. See the Cloud TPU documentation for more information about TPU architecture.\n• None Performance gains are possible for workloads constrained overhead from .\n• None Under XRT, the server process is the only process that interacts with the TPU devices, and client processes don’t have direct access to the TPU devices. When profiling a single-host TPU (e.g. v3-8 or v4-8), you would normally see 8 device traces (one for each TPU core). With PJRT, each process has one chip, and a profile from that process will show only 2 TPU cores.\n• None For the same reason, profiling does not work on TPU Pods with XRT, because the server process runs independently from the user’s model code. PJRT does not have that constraint, so it is possible to profile 2 TPU cores per process in a TPU Pod.\n• None PJRT only supports the TPU VM architecture and we have no plans to support the TPU Node architecture with PJRT.\n• None Runtime configuration is significantly simpler with PJRT. is not required to run TPU Pod workloads. Instead, copy your code to each TPU host ( ) and run the code on each host in parallel (e.g. )\n• None has been reimplemented using XLA-native collective communication to enhance stability on large TPU pods. See below for more details. On TPU v2 and v3, distributed workloads always run multithreaded, since each TPU core exposes two TPU cores as devices and only one process may open a TPU chip at a time. In its default configuration, automatically spawns as many processes as possible (4 per TPU host) and creates two threads per process (one per TPU core). Note: on TPU v4, each TPU chip is represented as one PyTorch device, so distributed workloads will run across 4 processes, each with only one thread. This is identical to XRT’s behavior. In most cases, this will not require substantial changes to your existing code. The main change you will have to make in most cases is to model initialization. Because ‘s global RNG is shared between threads, results will vary between threads and runs even if you set to the same value in every replica. To get consistent parameters between replicas, either use to broadcast one replica’s parameters to all other replicas, or load each replica’s parameters from a common checkpoint. With XRT, worker 0 runs a mesh master service, and all processes on all workers connect to that service over gRPC. In practice, we found that running a single mesh master process was unreliable on TPU pods with thousands of chips due to the number of inbound connections to worker 0. A single client process timing out could cause a failure and force the entire workload to restart. Thus, we have reimplemented with native XLA collective communication, which is much more stable and well-tested on large TPU pods. This imposes two new constraints compared to the XRT implementation:\n• None Because the payload has to become part of the XLA graph, is called both before and after the data is transferred. Calling in the middle of model code may force an unwanted compilation.\n• None Because XLA does not permit collective operations to run on a subset of workers, all workers must participate in the . If you require the old behavior of (i.e. communicating data without altering the XLA graph and/or synchronizing a subset of workers), consider using <https://pytorch.org/docs/stable/distributed.html#torch.distributed.barrier>`_ or <https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather_object>`_ with a process group. If you are also using the backend, you can use to create a subgroup. See this example from the PyTorch documentation. Keep in mind these constraints:\n• None is not fully supported on TPU v2/v3. Only a subset of operations with the backend are implemented, and will likely not work as expected in a multithreaded context.\n• None In our experiments, does not scale well to thousands of TPU chips, so expect this alternative to be less reliable than using with PJRT at large scales. When using PJRT with and we strongly recommend using the new , which automatically finds the replica IDs, world size, and master IP by querying the runtime. For example: # No need to pass in `rank` or `world_size` Note: Although the init_method is not required on TPU v4, it is still recommended. If you use , must be set to IP host that has device 0, which is not always worker 0. The init_method finds this IP automatically. Note: For TPU v2/v3, you still need to import , as TPU v2/v3 support in is still experimental. For more information about using on PyTorch/XLA, see <./ddp.md>`_ on TPU V4. For an example that uses DDP and PJRT together, run the following example script on a TPU: TorchBench shows improvements in average training time across tasks with PJRT compared to XRT, with an average improvement of over 35% on TPU v4-8. The benefits vary significantly by task and model type, ranging from 0% to 175%. The following chart shows the breakdown by task: The PyTorch/XLA r2.0 release introduces support for the PJRT Plugin API, used to access the new TFRT-based TPU runtime in . This is now the default runtime when is set. The legacy StreamExecutor-based TPU runtime used in 1.13 will still be available with in the 2.0 release, but it will be removed in a future version. If you encounter an issue that only happens on and not , please file an issue on GitHub. In most cases, we expect performance to be similar between the two runtimes, but in some cases, the new runtime may be up to 30% faster. The following chart shows the breakdown by task: Note: the improvements shown in this chart are also included in the PJRT vs XRT comparison. TorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster. It provides a clean API for compiler backends to hook in and its biggest feature is to dynamically modify Python bytecode right before it is executed. In the pytorch/xla 2.0 release, PyTorch/XLA provided an experimental backend for the TorchDynamo for both inference and training. The way that XLA bridge works is that Dynamo will provide a TorchFX graph when it recognizes a model pattern and PyTorch/XLA will use existing Lazy Tensor technology to compile the FX graph and return the compiled function. Support for PyTorch/XLA and Dynamo currently exists by adding the argument to . For example: Here is a small code example of running resnet18 with With the you will see that PyTorch/XLA only traces the resent18 model once during the init time and executes the compiled binary every time is invoked, instead of tracing the model every time. Here is a inference speed analysis to compare Dynamo and Lazy using torch bench on Cloud TPU v4-8 PyTorch/XLA also supports Dynamo for training, but it is experimental and we are working with the PyTorch Compiler team to iterate on the implementation. Here is an example of training a resnet18 with We expect to extract and execute 3 graphs per training step instead of 1 graph per training step if you use the Lazy tensor. Here is a training speed analysis to compare Dynamo and Lazy using a torch bench on Cloud TPU v4-8. NOTE: We run each model’s fwd and bwd for a single step and then collect the e2e time. In the real world we will run multiple steps at each training job which can easily hide the tracing cost from execution(since it is async). Lazy Tensor will have much better performance in that scenario. There is one gap we want to call out that are preventing us from using the TorchDynamo on larger scale models.\n• None TorchDynamo will trace forward and backward into separate graphs. For PyTorch/XLA it is important to let the XLA compiler see the whole step as one graph to best optimize the speed. There is also a fixed overhead to launch every device execution which make executing multiple graphs per training step less ideal. This gap compared to Lazy Tensor makes it less efficient in real world training use cases, especially the tracing cost can be overlapped with the execution in training. TorchDynamo provides a really promising way for the compiler backend to hide the complexity from the user and easily retrieve the modeling code in a graph format. Compared with PyTorch/XLA’s traditional Lazy Tensor way of extracting the graph, TorchDynamo can skip the graph tracing for every iteration, hence providing a much better inference response time. Most models supported by PyTorch/XLA, have seen significant speedup when running inference with the new dynamo-xla bridge. Our community is working hard to expand the set of supported models. Regarding the training feature gaps mentioned above, the PyTorch/XLA community is super excited to improve the training gap in our upcoming development work. The team continues to heavily invest in TorchDynamo and work with the upstream to mature the training story. Fully Sharded Data Parallel (FSDP) in PyTorch XLA is a utility for sharding Module parameters across data-parallel workers. It is also possible to shard individual layers separately and have an outer wrapper handle any leftover parameters.\n• None The class supports both the ZeRO-2 optimizer (sharding gradients and optimizer states) and the ZeRO-3 optimizer (sharding parameters, gradients, and optimizer states) in https://arxiv.org/abs/1910.02054.\n• None The ZeRO-3 optimizer should be implemented via nested FSDP with . See and for an example.\n• None For large models that cannot fit into a single TPU memory or the host CPU memory, one should interleave submodule construction with inner FSDP wrapping. See <https://github.com/ronghanghu/vit_10b_fsdp_example/blob/master/run_vit_training.py>`_ for an example.\n• None a simple wrapper is provided (based on from https://github.com/pytorch/xla/pull/3524) to perform gradient checkpointing over a given instance. See and for an example.\n• None Auto-wrapping submodules: instead of manually nested FSDP wrapping, one can also specify an argument to automatically wrap the submodules with inner FSDP. in is an example of callable, this policy wraps layers with the number of parameters larger than 100M. in is an example of callable for transformer-like model architectures. For example, to automatically wrap all submodules with inner FSDP, one can use: Additionally, one can also specify an argument to use a custom callable wrapper for the submodules (the default wrapper is just the class itself). For example, one can use the following to apply gradient checkpointing (i.e. activation checkpointing/rematerialization) to each auto-wrapped submodule.\n• None When stepping the optimizer, directly call and do not call . The latter reduces the gradient across ranks, which is not needed for FSDP (where the parameters are already sharded).\n• None When saving model and optimizer checkpoints during training, each training process needs to save its own checkpoint of the (sharded) model and optimizer state dicts (use and set different paths for each rank in ). When resuming, it needs to load the checkpoint for the corresponding rank.\n• None Please also save along with as follows and use to stitch the sharded model checkpoints together into a full model state dict. See for an example. .. code-block:: python3\n• None The checkpoint consolidation script can also be launched from the command line as follows. .. code-block:: bash The implementation of this class is largely inspired by and mostly follows the structure of in https://fairscale.readthedocs.io/en/stable/api/nn/fsdp.html. One of the biggest differences from is that in XLA we don’t have explicit parameter storage, so here we resort to a different approach to free full parameters for ZeRO-3. Example training scripts on MNIST and ImageNet¶ FSDP is available on PyTorch/XLA 1.12 release and newer nightly. Please refer to https://github.com/pytorch/xla#-available-images-and-wheels for installation guide. It gets around 98.9 accuracy for 2 epochs: This script automatically tests checkpoint consolidation at the end. You can also manually consolidate the sharded checkpoints via It gets around 75.9 accuracy for 100 epochs; download ImageNet-1k to : You can also add (which needs to be used along with or ) to apply gradient checkpointing on the residual blocks. Example training scripts on TPU pod (with 10 billion parameters)¶ To train large models that cannot fit into a single TPU, one should apply auto-wrap or manually wrap the submodules with inner FSDP when building the entire model to implement the ZeRO-3 algorithm. Please see https://github.com/ronghanghu/vit_10b_fsdp_example for an example of sharded training of a Vision Transformer (ViT) model using this XLA FSDP PR.\n\nIn this user guide, we discuss how GSPMD is integrated in PyTorch/XLA, and provide a design overview to illustrate how the SPMD sharding annotation API and its constructs work. And then, we provide a list of reference examples for users to try. GSPMD is an automatic parallelization system for common ML workloads. The XLA compiler will transform the single device program into a partitioned one with proper collectives, based on the user provided sharding hints. This feature allows developers to write PyTorch programs as if they are on a single large device without any custom sharded computation ops and/or collective communications to scale. *Figure 1. Comparison of two different execution strategies, (a) for non-SPMD and (b) for SPMD.* To support GSPMD in PyTorch/XLA, we are introducing a new execution mode. Before GSPMD, the execution mode in PyTorch/XLA assumed multiple model replicas, each with a single core (Figure 1.a). This mode of execution, as illustrated in the above suits data parallelism frameworks, like the popular PyTorch Distributed Data Parallel (DDP) or Fully Sharded Data Parallel (FSDP), but is also limited in that a replica can only reside on one device core for execution. PyTorch/XLA SPMD introduces a new execution mode that assumes a single replica with multiple cores (Figure 1.b), allowing a replica to run across multiple device cores. This shift unlocks more advanced parallelism strategies for better large model training performance. PyTorch/XLA SPMD is available on the new PJRT runtime. To enable PyTorch/XLA SPMD execution mode, the user must call . It is important to note that SPMD is a replacement for any existing parallel mechanisms, including DDP and FSDP. Users can not mix two different execution modes (SPMD and non-SPMD), and later in this guide we will go over how to use SPMD annotation to perform DDP and FSDP. Also, this version of the SPMD is currently only tested.optimized on Google Cloud TPU. GPU support and optimization will come in the 2.2 release. Users can annotate native PyTorch tensors using the API (src). This takes as input and returns a as output. Invoking API takes a user defined logical mesh and partition_spec and generates a sharding annotation for the XLA compiler. The sharding spec is attached to the XLATensor. Here is a simple usage example from the [RFC, to illustrate how the sharding annotation API works: # Device mesh, this and partition spec as well as the input tensor shape define the individual shard shape. # Mesh partitioning, each device holds 1/8-th of the input We can annotate different tensors in the PyTorch program to enable different parallelism techniques, as described in the comment below: # Sharding annotate input data, we can shard any input More complete unit test cases and integration test examples are available in the PyTorch/XLA repo. For a given cluster of devices, a physical mesh is a representation of the interconnect topology. We derive a logical mesh based on this topology to create sub-groups of devices which can be used for partitioning different axes of tensors in a model. We abstract logical mesh with Mesh API. The axes of the logical Mesh can be named. Here is an example: # Assuming you are running on a TPU host that has 8 devices attached # mesh shape will be (4,2) in this example In general, SPMD programs should create a single mesh and reuse it for all sharding to ensure that the tiling assignment is consistent with the intended sharding strategy. The same mesh can be reused for tensors of different shapes and shardings by manipulating the partition spec, described further below. Mesh nicely abstracts how the physical device mesh is constructed. Users can arrange devices in any shape and order using the logical mesh. However, one can define a more performant mesh based on the physical topology, especially when it involves Data Center Network (DCN) cross slice connections. HybridMesh creates a mesh which gives good performance out of the box for such multislice environments. It accepts ici_mesh_shape and dcn_mesh_shape which denote logical mesh shapes of inner and outer network. # This example is assuming 2 slices of v4-8. # - ici_mesh_shape: shape of the logical mesh for inner connected devices. partition_spec has the same rank as the input tensor. Each dimension describes how the corresponding input tensor dimension is sharded across the device mesh (logically defined by mesh_shape). is a tuple of dimension or None. The index can be an or , if the corresponding mesh dimension is named. This specifies how each input rank is sharded ( to ) or replicated ( ). # Provide optional mesh axis names and use them in the partition spec We support all three types of sharding, described in the original GSPMD paper. For instance, one can specify partial replication like this: # Provide optional mesh axis names and use them in the partition spec # evenly shard across x and z and replicate among y The partition spec enables reuse of the same mesh for different tensor shapes and desired sharding strategies. The following example demonstrates this using a 3D mesh: # Create a 3-D mesh of 8 devices with logical dimensions replica, fsdp, and # A 2D tensor can be sharded along the fsdp and tensor axes and replicated # along the replica axis by omitting `replica` from the partition spec. # A 2D tensor can be sharded across all dimensions by combining, for example, # the replica and fsdp mesh axes using a tuple # A 4D tensor can be sharded along up to three of its axes using the 3D mesh The main use case for [RFC] is to annotate a native (on a single device) with a sharding spec. The annotation takes place immediately, but the actual sharding of the tensor is delayed as the computation is carried out lazily, except for the input tensors which are sharded without delay. Once a tensor is annotated and wrapped inside a , it can be passed to existing PyTorch ops and layers as . This is important to ensure that the same PyTorch layers and tensor ops can be stacked together with . This means that the user does not need to rewrite the existing ops and model codes for sharded computation. Namely, will satisfy the following requirements:\n• None is a subclass and works directly with native torch ops and . We use to send to the XLA backend. PyTorch/XLA retrieves attached sharding annotations to trace the graph and invokes XLA SPMDPartitioner.\n• None Internally, (and its global_tensor input) is backed by with a special data structure holding references to the sharded device data.\n• None The sharded tensor after lazy execution may be gathered and materialized back to the host as global_tensor when requested on the host (e.g., printing the value of the global tensor.\n• None The handles to the local shards are materialized strictly after the lazy execution. exposes local_shards to return the local shards on addressable devices as . There is also an ongoing effort to integrate into API to support XLA backend [RFC]. PyTorch has prototype-released DTensor in 2.1. We are integrating PyTorch/XLA SPMD into DTensor API RFC. We have a proof-of-concept integration for , which calls annotation API to shard a tensor and its computation using XLA: # distribute_tensor now works with `xla` backend using PyTorch/XLA SPMD. This feature is experimental and stay tuned for more updates, examples and tutorials in the upcoming releases. PyTorch/XLA SPMD takes a single-device program, shards and executes it in parallel. The SPMD execution requires using the native PyTorch DataLoader, which transfers data synchronously from the host to XLA devices. This blocks the training during the input data transfer every step. To improve the native data loading performance, we made PyTorch/XLA ParallelLoader support input sharding directly (src), when passed the optional kwarg _input_sharding_: PyTorch/XLA SPMD is compatible with the torch.distributed.checkpoint library through a dedicated instance. Users are able to synchronously save and load checkpoints through this common interface. The SPMDSavePlanner and SPMDLoadPlanner (src) classes enable the and functions to operate directly on the shards of an , enabling all of the benefits of distributed checkpointing in SPMD training. Here is a demonstration of the synchronous distributed checkpointing API: # Loading the model's state_dict from the checkpoint. The model should # already be on the XLA device and have the desired sharding applied. The experimental CheckpointManager interface provides a higher-level API over the functions to enable a few key features:\n• None Managed checkpoints: Each checkpoint taken by the is identified by the step at which it was taken. All steps tracked are accessible through the method, and any tracked steps can be restored using .\n• None Asynchronous checkpointing: Checkpoints taken through the API are written to persistent storage asynchronously to unblock training for the duration of the checkpoint. The input sharded state_dict is first moved to CPU before the checkpoint is dispatched to a background thread.\n• None Auto-checkpointing on preemption: On Cloud TPU, preemptions can be detected and a checkpoint taken before the process is terminated. To use, ensure your TPU is provisioned through a QueuedResource with Autocheckpointing enabled, and ensure the parameter is set when constructing the CheckpointManager (this option is enabled by default).\n• None FSSpec Support: uses an fsspec storage backend to enable checkpointing directly to any fsspec-compatible filesystem, including GCS. Example usage of the CheckpointManager is below: # Create a CheckpointManager to checkpoint every 10 steps into GCS. # Select a checkpoint to restore from, and restore if applicable # Call `save` or `save_async` every step within the train loop. These methods # return True when a checkpoint is taken. To use APIs such as distributed checkpointing, a process group is required. In SPMD mode, the backend is not supported since the compiler is responsible for all collectives. Instead, a CPU process group such as must be used. On TPUs, the init_method is still supported to discover the master IP, global world size, and host rank. An example initialization is below: # and global world size without requiring environment configuration on TPUs. PyTorch/XLA normally transfers tensor data asynchronously from host to device once the tensor is defined. This is to overlap the data transfer with the graph tracing time. However, because GSPMD allows the user to modify the tensor sharding _after _the tensor has been defined, we need an optimization to prevent unnecessary transfer of tensor data back and forth between host and device. We introduce Virtual Device Optimization, a technique to place the tensor data on a virtual device SPMD:0 first, before uploading to the physical devices when all the sharding decisions are finalized. Every tensor data in SPMD mode is placed on a virtual device, SPMD:0. The virtual device is exposed to the user as an XLA device XLA:0 with the actual shards on physical devices, like TPU:0, TPU:1, etc. Unlike existing DDP and FSDP, under the SPMD mode, there is always a single process running on each accelerator host. This provides the benefit that PyTorch/XLA only need to compile each graph once which can be reused for all accelerators attached to this host. There is no code change required to go from single TPU host to TPU Pod if you construct your mesh and partition spec based on the number of devices instead of some hardcode constant. To run the PyTorch/XLA workload on TPU Pod, please refer to the Pods section of our PJRT guide. PyTorch/XLA supports SPMD on NVIDIA GPU (single-node or multi-nodes). The training/inference script remains the same as the one used for TPU, such as this ResNet script. To execute the script using SPMD, we leverage :\n• None : how many GPU machines to be used.\n• None : the index of the current GPU machines. The value can be 0, 1, …, ${NUMBER_GPU_VM}-1.\n• None : the value must be 1 due to the SPMD requirement.\n• None : the endpoint of the GPU machine with node_rank==0, in the form . The host will be the internal IP address. The``port` can be any available port on the machine. For single-node training/inference, this parameter can be omitted. For example, if you want to train a ResNet model on 2 GPU machines using SPMD, you can run the script below on the first machine: and run the following on the second machine: For more information, please refer to the SPMD support on GPU RFC. The SPMD API is general enough to express both data parallelism and model parallelism. One can implement data parallelism simply by annotating the input batch dimension for sharding. Here, we have shard the batch dimension across all available devices (N-way):There are 2 ways of using SPMD to express data parallel or batch sharding: # Assume data is 4d and 0th dimension is the batch dimension PyTorch/XLA’s MpDeviceLoader supports input batch sharding, which also loads the batches to the devices in the background: # Assume data is 4d and 0th dimension is the batch dimension # Use MpDeviceLoader to load data in background We highly recommend the second approach as it should yield a better training performance. PyTorch’s FSDP is data parallel + sharded model parameters at 0th dimension. Users first need to use SPMD to express Data Parallels as suggested in the previous section. We provided a quick example of resnet50 with a couple different SPMD sharding strategies for you to play around with. You can first run it without SPMD using and check the throughput. After that you can enable the batch sharding with Note that I used a batch size 4 times as large since I am running it on a TPU v4 which has 4 TPU devices attached to it. You should see the throughput becomes roughly 4x the non-spmd run. We provide a for PyTorch/XLA SPMD user on TPU/GPU/CPU with single-host/multi-host: you could use to visualize sharded tensor, or you could use to visualize sharing string. Here are two code examples on TPU single-host(v4-8) with or : # Here, mesh is a 2x2 mesh with axes 'x' and 'y' # A tensor's sharding can be visualized using the `visualize_tensor_sharding` method You could use these examples on TPU/GPU/CPU single-host and modify it to run on multi-host. And you could modify it to sharding-style , and . We are introducing a new PyTorch/XLA SPMD feature, called , RFC. This is an experimental feature in and , that supports and a single TPUVM host. PyTorch/XLA auto-sharding can be enabled by one of the following:\n• None Calling the SPMD API in the beginning of your code: # Currently, model should be loaded to xla device via distribute_module. Optionally, one can set the following options/env-vars to control the behvaior of the XLA-based auto-sharding pass:\n• None : group resharding of the parameters. Set by default.\n• None : logical mesh shape to be used for auto-sharding. For example, corresponds to a 2-by-2 mesh with 4 global devices. If unset, a default device mesh shape of will be used."
    },
    {
        "link": "https://medium.com/we-talk-data/how-to-set-random-seeds-in-pytorch-and-tensorflow-89c5f8e80ce4",
        "document": "““The foundation of all great achievements is reproducibility.” While this may not be a quote from the past, it’s a truth that holds firm in the fast-evolving world of machine learning.” When you’re working on a machine learning project, achieving reproducibility is not just a best practice — it’s a necessity. Why? Because you can’t improve what you can’t replicate. From debugging models to publishing results, setting random seeds ensures your experiments yield consistent and predictable outcomes. Let me give you a real-world example: imagine training a deep learning model and seeing promising results on one run, only for it to perform poorly on the next. A lack of reproducibility stemming from random initialization and inherent stochasticity in the training process. Reproducibility doesn’t just make your work look polished; it’s a core requirement for scientific rigor. You want to debug effectively, benchmark fairly, or share your findings with confidence, right? Without setting proper seeds, these goals become a gamble. However, here’s the deal: reproducibility across frameworks and devices isn’t as straightforward as it sounds. Different libraries have different mechanisms for generating random numbers. Add GPUs, distributed training, or even slight variations in hardware into the mix, and things can spiral out of control. In this guide, I’m here to cut through the noise and give you the practical, actionable code examples you need to master reproducibility in PyTorch and TensorFlow. Let’s dive in and make reproducibility one less thing you need to worry about in your workflow.\n\nBefore we go deeper, let’s address the basics. In PyTorch, you can set a random seed with the manual_seed function. This initializes PyTorch’s random number generator to ensure reproducible results for operations involving randomness. This might look simple, but here’s what it does under the hood: it ensures that every call to PyTorch’s random number generator produces the same results. However, it only affects PyTorch’s generator. This means randomness introduced by other libraries, such as NumPy or Python’s random, won’t be controlled. Now let’s get serious. Reproducibility in PyTorch isn’t just about one seed. You’ve got to consider multiple aspects — CPU operations, GPU computations, and even multi-GPU scenarios. Here’s a complete example: import torch\n\nimport random\n\nimport numpy as np\n\n\n\n# Define the seed value\n\nseed = 42\n\n\n\n# Set seed for PyTorch\n\ntorch.manual_seed(seed)\n\n\n\n# Set seed for CUDA (if using GPUs)\n\ntorch.cuda.manual_seed(seed)\n\ntorch.cuda.manual_seed_all(seed) # For multi-GPU setups\n\n\n\n# Set seed for Python's random module\n\nrandom.seed(seed)\n\n\n\n# Set seed for NumPy\n\nnp.random.seed(seed)\n\n\n\n# Ensure deterministic behavior for PyTorch operations\n\ntorch.backends.cudnn.deterministic = True\n\ntorch.backends.cudnn.benchmark = False\n• CPU and GPU Randomness: While torch.manual_seed handles PyTorch randomness, you also need to set seeds for CUDA to ensure GPU operations are consistent. This is crucial if you’re working on GPU-enabled deep learning models.\n• External Libraries: Libraries like NumPy and Python’s random need their seeds set, as they might contribute to randomness in your data preprocessing or model initialization.\n• Deterministic Behavior: Enabling cudnn.deterministic ensures that PyTorch uses deterministic algorithms where possible. Setting benchmark to False prevents the framework from tuning for faster, non-deterministic kernels. But here’s the catch: while deterministic settings are great for reproducibility, they might slow down training. If speed is a priority and reproducibility is less critical, you can skip these settings. You might be wondering, “Why are my results still inconsistent?” Here are a few common pitfalls and solutions:\n• Forgetting GPU Seeds: If you’re using GPUs but forget torch.cuda.manual_seed, you’ll still encounter variability.\n• Non-Deterministic Operations: Some PyTorch operations are inherently non-deterministic. For example, torch.nn.Conv2d on GPUs can produce slightly different results due to floating-point precision. Always consult PyTorch’s Reproducibility documentation for a list of such operations.\n• Stochastic Layers: Layers like dropout introduce randomness during training. To verify reproducibility, set the model to evaluation mode using model.eval() or control dropout rates explicitly.\n\nTensorFlow has its own mechanism for managing randomness. To set a seed for TensorFlow, use the tf.random.set_seed function: What does this do? It initializes the random number generator for TensorFlow operations, ensuring consistent results across training runs. Reproducibility in TensorFlow often involves more than just TensorFlow’s RNG. Like PyTorch, TensorFlow workflows can involve external libraries like NumPy or Python’s random. Let’s handle them all: import tensorflow as tf\n\nimport numpy as np\n\nimport random\n\n\n\n# Set a common seed\n\nseed = 42\n\n\n\n# Set seed for TensorFlow\n\ntf.random.set_seed(seed)\n\n\n\n# Set seed for NumPy\n\nnp.random.seed(seed)\n\n\n\n# Set seed for Python's random module\n\nrandom.seed(seed) For multi-GPU or TPU setups, TensorFlow’s distributed strategies automatically inherit the tf.random.set_seed. However, always validate reproducibility by running test cases on your target hardware. Did you know the Python interpreter itself can introduce variability? Setting the PYTHONHASHSEED environment variable ensures consistent hash-based operations, like data shuffling: Combine this with your TensorFlow workflow for an additional layer of reproducibility. For example: import os\n\nos.environ['PYTHONHASHSEED'] = '42'\n\n\n\n# Now proceed with your TensorFlow workflow Reproducibility is like the hidden backbone of machine learning projects. Master it, and you’ll not only debug faster but also ensure your work withstands the test of peer review or production deployment. Let’s move on to the next section and tackle cross-framework reproducibility challenges. Shall we?\n\n“When two powerful frameworks walk into a room, you need to make sure they’re speaking the same language — random seeds.” Here’s the challenge: you’re juggling both PyTorch and TensorFlow in the same project, whether for comparison, ensemble methods, or pipeline integration. Without a consistent approach to setting seeds, you’re setting yourself up for unpredictability. Luckily, with the right setup, you can synchronize randomness across frameworks. Here’s a single script to ensure reproducibility across TensorFlow, PyTorch, and other critical libraries like NumPy and Python’s random: import tensorflow as tf\n\nimport torch\n\nimport numpy as np\n\nimport random\n\nimport os\n\n\n\n# Set a common seed value\n\nseed = 42\n\n\n\n# Set environment-level seed (affects Python's hash functions)\n\nos.environ['PYTHONHASHSEED'] = str(seed)\n\n\n\n# Set seeds for Python libraries\n\nrandom.seed(seed)\n\nnp.random.seed(seed)\n\n\n\n# Set seed for TensorFlow\n\ntf.random.set_seed(seed)\n\n\n\n# Set seeds for PyTorch (CPU and GPU)\n\ntorch.manual_seed(seed)\n\ntorch.cuda.manual_seed(seed)\n\ntorch.cuda.manual_seed_all(seed) # For multi-GPU setups\n\n\n\n# Ensure deterministic behavior in PyTorch\n\ntorch.backends.cudnn.deterministic = True\n\ntorch.backends.cudnn.benchmark = False\n• Environment-Level Seeds: By setting PYTHONHASHSEED, you ensure Python’s built-in hash functions (used in data shuffling) remain consistent.\n• Cross-Framework Synchronization: This code aligns random number generation in TensorFlow, PyTorch, and NumPy — ensuring that your data splits, weight initialization, and augmentations behave predictably across runs.\n• GPU Consistency: GPU computations often introduce non-determinism. By setting PyTorch’s cudnn.deterministic, you enforce deterministic algorithms, though this may come at the cost of reduced performance. You might be wondering: Why does this level of detail matter? Imagine a scenario where your TensorFlow preprocessing pipeline feeds directly into a PyTorch model. If the seeds aren’t synchronized, even slight variations in preprocessing randomness can cascade into drastically different model behaviors.\n• Benchmarking Frameworks: Comparing TensorFlow and PyTorch models on the same dataset.\n• Hybrid Pipelines: Using TensorFlow for data preprocessing and PyTorch for model training.\n• Ensemble Learning: Combining models trained in both frameworks for better predictions. Here’s the deal: setting seeds is only part of the story. Verifying reproducibility is critical, especially in cross-framework workflows. Save the outputs (e.g., predictions, intermediate variables) from two runs of the same script. Compare them for consistency: If the outputs differ, investigate further. Some operations (e.g., GPU-based convolutions) are inherently non-deterministic, even with seeds set. In PyTorch, you can audit your code for such operations using their official guidance. Log key checkpoints in your script, like random splits or weight initializations. Compare these logs across runs to pinpoint sources of randomness.\n\n“Reproducibility isn’t just about repeating what you did; it’s about building confidence in your work.” Here are the key takeaways:\n• Always set random seeds at multiple levels — framework, libraries, and environment.\n• Validate reproducibility with controlled experiments, especially in complex workflows.\n• Understand when to relax reproducibility for exploration or ensemble methods. I’ve shown you how to master reproducibility across frameworks like PyTorch and TensorFlow, even in advanced scenarios like distributed training. Now, it’s your turn: what challenges have you faced in making your experiments reproducible? Drop your insights in the comments — I’d love to hear from you! Let’s keep the conversation going."
    }
]