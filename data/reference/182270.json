[
    {
        "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC5248982",
        "document": "Despite the fact that numerous antibiotic drugs are available and have been routinely used for a much longer time than most other drugs, the fight between humans and the surrounding bacteria responsible for infections are ongoing and will be so for the foreseeable future. Contributing to this is the steady rise of antibiotics drug resistance leading to the need for new antibiotics (1, 2). Toward the design of new antibiotics, computer-aided drug design (CADD) can be combined with wet-lab techniques to elucidate the mechanism of drug resistance, to search for new antibiotic targets and to design novel antibiotics for both known and new targets. Notably CADD methods can produce an atomic level structure-activity relationship (SAR) used to facilitate the drug design process thereby minimizing time and costs (3, 4). Understanding the atomic-detailed mechanism behind the antibiotics resistance helps to reveal limitations in current antibiotics and shed light on the design of new drugs. For examples, Trylska et al. studied the effects of mutations at the bacterial ribosomal A-site using molecular dynamics (MD) simulations to reveal the origins of bacterial resistance to aminoglycosidic antibiotics (5). Our lab studied the impact of ribosomal modification on the binding of the antibiotic telithromycin using a combined Grand Canonical Monte Carlo (GCMC)/Molecular Dynamics (MD) simulation methodology (6, 7) and revealed atom-level details of how those modifications lead to resistance that will be of utility to improve the activity and spectrum of macrolide analogs thereby minimizing resistance (8). An important alternative to solve the antibiotic resistance issue is the identification of new antibiotic targets that may represent novel mechanisms essential for bacterial survival. For example, researchers used bioinformatics approaches to screen various databases computationally and identified seven enzymes involved in bacterial metabolic pathways as well as 15 non-homologous proteins located on membranes in the gram positive bacterium Staphylococcus aureus (SA), thereby indicating them as potential targets (9). Such findings may help to overcome the resistance of this bacterium to common antibiotics such as methicillin, fluoroquinolones and oxazolidinones. An example of a recently identified novel antibiotic target is the protein heme oxygenase, involved in the metabolism of heme by bacteria as required to access iron (10–12). In collaborative studies with the Wilks lab, we have successfully applied CADD techniques to identify inhibitors of the bacterial heme oxygenases from Pseudomonas aeruginosa and Neisseria meningitides, thereby confirming the potential role of heme oxygenases as a novel antimicrobial targets (13, 14). Researchers are also continuing to look for new antibiotics against existing targets and computational approaches have been successfully used in a number of studies. Using in silico database screening, Chang et al. found a new series of non-β-lactam antibiotics, the oxadiazoles, which can inhibit penicillin-binding protein 2a (PBP2a) of methicillin-resistant SA (MRSA), the cause of most infections in hospitals (15). Using ligand-based drug design (LBDD), our lab with Andrade and coworkers investigated analogs of the third-generation ketolide antibiotic telithromycin as a possible means to address the bacterial resistance problem associated with that class of antibiotics (16–18). In another study, based on the 3D structure of the complex of human defensin peptide HNP1 with Lipid II, which serves as precursor for bacterial cell wall biosynthesis and is a validated target for antibiotics, our lab designed a simple pharmacophore model and used it in a database screen to search for low weight defensin mimetics (19). From that effort, a lead compound was identified that targets Lipid II with high specificity and affinity. Notably, this is the first example of a small molecular weight compound that shows promising activity against Lipid II. Lead compound derivatives were subsequently identified again using CADD in combination with medicinal chemistry (20) and the accumulated SAR information will facilitate the development of next generation antibiotics targeting gram positive pathogenic bacteria. Figure 1 illustrates the basic CADD workflow that can be interactively used with experimental techniques to identify novel lead compounds as well as direct iterative ligand optimization (3, 4, 21, 22). The process starts with the biological identification of a putative target to which ligand binding should lead to antimicrobial activity. In SDBB, the 3D structure of the target can be identified by X-ray crystallography or NMR or using homology modeling. This lays the foundation for CADD SBDD screening using the methods described below. LBDD is used in the absence of the target 3D structure with the central theme being the development of an SAR from which information on modification of the lead compound to improve activity can be obtained. Information from the CADD methods is then used to design compounds that are subjected to chemical synthesis and biological assay, with the information from those experiments used to further develop the SAR, yielding further improvements in the compounds with respect to activity as well as absorption, disposition, metabolism and excretion (ADME) considerations (23). Notably, CADD methods are evolving with researchers continually updating and implementing new CADD techniques with higher levels of accuracy and speed (24–26). In this chapter, we will present commonly used CADD approaches, including those used in our lab for the design of next-generation antibiotics. Basic CADD workflow in drug discovery. Wet-lab, SBDD and LBDD CADD techniques are outlined in solid lines, dashed lines or dotted lines, respectively. Double headed arrows indicate the two techniques can be used interactively in several iterative rounds of ligand design.\n\nCADD methods are mathematical tools to manipulate and quantify the properties of potential drug candidates as implemented in a number of programs. These include a range of publicly and commercially available software packages; the subset described below represents examples of fundamental tools for CADD with emphasis on those commonly used in our laboratory.\n• None Commonly used MD simulation codes include CHARMM (27), AMBER (28), NAMD, (29) GROMACS (30) and OpenMM (31). These programs run on a variety of computer architectures including running in parallel on multicore central processing units (CPU) and, more recently, optimized for graphics processing units (GPU), such as those commonly used in video games.\n• None For SBDD, the 3D structure of the protein, RNA or other macromolecule may be obtained from the Protein Data Bank (PDB) (32) if it was solved by X-ray crystallography or nuclear magnetic resonance (NMR) experiments. Alternatively, a 3D structure may be constructed using homology modeling methods with a program such as MODELLER (33) or an on-line web server such as SWISS-MODEL (34).\n• None In order to perform MD simulations, homology modeling, database screening or other CADD techniques empirical force fields for the molecules of interest are needed. These force fields are used by the respective programs to estimate the energy and forces associated with, for example, a drug-protein complex. Force fields such as those from the CHARMM (35–38) or AMBER (39, 40) families are used to describe the internal and external energetic properties of a molecular system during an energy minimization or a MD simulation. When parameters are missing in the existing force field, which is common for small drug-like molecules, automated parameter generation programs such as the CGenFF program (41, 42) or Antechamber (43) can be used to complete the force field. It is important to note that when using a force field the parameters for different parts of the system (e.g. the protein and the ligand) need to be compatible, such that CGenFF should be used with CHARMM or Antechamber with AMBER. In addition, when parameters are estimated it is suggested that the user check the parameters with respect to their accuracy in treating the energy as a function of conformation, as described for CGenFF (37, 44, 45). To facilitate this process when generating parameters using the CGenFF program (see https://cgenff.paramchem.org), penalties are assigned to parameters estimated based on analogy, guiding the user with respect to parameters that require checking.\n• None When no information on the binding site of a target is available, putative binding sites can be identified by various CADD methods. An example, is the binding response program (46) developed in our lab. The program identifies potential binding sites by considering both the geometrical match and the binding energy of a set of diverse drug-like compounds to the sites being queried on the protein. Other programs for binding site identification include FINDSITE (47) and ConCavity (48).\n• None Virtual database screening (VS) techniques are generally used to screen large in silico compound databases to identify potential binders for a query target. Examples of docking software commonly used for this purpose are DOCK (49) and AutoDock (50) as well as AutoDock Vina (51), all of which are well-known freeware programs. Another example is the program Pharmer (52), which uses 3D pharmacophores for database screening.\n• None The in silico database of drug-like compounds is an essential component of CADD ligand identification based on VS. A publically accessible database of compounds for VS is ZINC (53) which currently has about 90 million compounds that can be purchased from various chemical vendors. In-house databases can also be constructed for particular VS needs and chemical vendors such as ChemBridge and ChemDiv (54) supply their chemical catalogs in SDF format for download. However, conversion of these to 3D structures can be challenging and all physiologically accessible protonation and tautomeric states of the ligands in the database should be included.\n• None Commercially available CADD software packages include Discovery Studio (55), OpenEye (56), Schrödinger (57) and MOE (58). These programs, which can often be obtained at a discount for academic users, cover most of the capabilities required for CADD including both SBDD and LBDD methods.\n\nCADD can be separated into ligand or hit identification and ligand or hit optimization, with both SBDD and LBDD methods useful in the appropriate context. Database screening methods are often used for hit identification (59) while a number of methods may be used for hit optimization (4, 24, 60). These include the Site-identification by ligand competitive saturation (SILCS) methodology. Below we present a collection of methods that may be used for both ligand identification and optimization. MD simulations can be used to study target-ligand interactions at an atomic level of detail (61), to generate conformational ensembles for the target or for the ligand to take flexibility into account for both SBDD and LBDD studies (see Note 1) and, in combination with other methods, used to estimate relative free energies of binding. Following are the steps required to perform a standard MD simulation (see Note 2 for additional MD techniques). A convenient web-based tool to perform a number of the steps below is the CHARMM-GUI at www.charmm-gui.org (62).\n• None Download the 3D structure of the bacterial target structure of interest from the PDB or use homology modelling to generate a structure.\n• None Refine the target structure including adjusting the side chain orientations, add hydrogens, and determine the appropriate protonation states for titratable residues. Software such as Reduce (63) can be used for this purpose. Remove or retain cofactors, ions and crystal waters depending on the study needs.\n• None Choose a force field, such as CHARMM36 (http://mackerell.umaryland.edu/charmm_ff.shtml) to describe the system and a MD code to carry out the simulation. Prepare the input files according to the program formats. If force field parameters are missing, develop parameters using an automated program such as the CGenFF program or following a standard parametrization protocol for the chosen force field (37, 38).\n• None For explicit solvent MD, solvate the system in a water box with periodic boundary conditions (PBC) (61), a process that can be performed automatically using the CHARMM-GUI mentioned above. Minimize and equilibrate the whole system step by step to allow bad atomic contacts to relax and attain relaxed geometries. Usually harmonic restraints are first put on non-water components of the system and gradually reduced through the minimization and MD equilibration. This avoids large changes in the target structure due to bad atomic contacts in the initial model. NVT canonical ensemble MD is usually used for first step equilibration and followed by NPT ensemble MD to allow the PBC box size to adjust corresponding to the temperature and pressure, typically 298 K and 1 atm, respectively, of interest.\n• None Run the MD simulation in the NPT ensemble for the time scale corresponding to the phenomena being studied. This usually involves nano- to microsecond timescales, although some phenomena can occur on shorter timescales. The user is advised to check that the event of interest (e.g. conformational change of the protein binding site) has occurred multiple times during the simulation or the phenomenon being monitored does not change significantly with increasing simulation time. However, no MD simulation is ever truly converged such that changes in the properties being monitored may occur after it appears that they are no longer changing.\n• None Do a basic quality check on the MD trajectories such as analyzing the root-mean-square deviation (RMSD) of the target with respect to the starting conformation along the simulation time. Typically, there is an increase in the RMSD followed by a stable, fluctuating value. However, as stated in the preceding section, though a simulation appears stable, additional changes can occur upon additional simulation time.\n• None When studying target-ligand interactions, different properties along the trajectory can be calculated for analyses such as interaction energy and hydrogen bonding profiles. In addition structural clustering algorithms can be used to extract representative conformations from MD trajectories (64) to understand different interaction patterns between the ligand and the protein that contribute to binding. SILCS is a novel CADD protocol developed in our lab to facilitate ligand design (65). It uses all-atom explicit-solvent MD simulations that include small organic solutes, such as propane, methanol and others, to identify 3D functional-group binding patterns on the target. These patterns can be used qualitatively to direct ligand design and, when converted to free energies, termed grid free energy (GFE) FragMaps (66, 67), used to quantitatively estimate the relative binding affinities of ligands. The detailed protocol based on full MD simulations was described previously in this same book series (68). Here we present an updated protocol based on the use of oscillating μ Grand Canonical Monte Carlo/MD (GCMC/MD) simulations for SILCS (69). The GCMC/MD approach allows for the application of the SILCS method to target systems with deep or occluded pockets such as nuclear receptors and GPCRs (70).\n• None Prepare the system in a similar way as described in section 3.1 for MD simulations. In addition to water, add solute molecules such as benzene, propane, methanol, formamide, acetaldehyde, imidazole, methylammonium and acetate at a concentration of about 0.25 M.\n• None Place weak restraints only on the backbone Cα carbon atoms with a force constant (k in 1/2 kδx2) of 0.12 kcal/mol/Å for all residues or only on core region residues in the target if additional flexibility of selected regions of the protein is desired. The use of Cα restraints prevents the rotation and translation of the protein in the simulation box and prevents potential denaturation due to the presence of small solutes in the aqueous solution surrounding the target (71).\n• None This system is minimized for 5000 steps with the steepest descent (SD) algorithm (72) in the presence of PBC followed by a 250 picosecond (ps) MD equilibration during which temperature is adjusted by velocity rescaling.\n• None During GCMC, solutes and water are exchanged between their gas-phase reservoirs and the simulation system. The excess chemical potential (μ ) supplied to drive solute and water exchange is periodically oscillated over every 3 cycles for each solute or water, based on their target concentration (eg. 0.25 M for the solutes and 55 M for water). From these calculations, which are performed over 100 or more cycles, the average μ is close to the respective experimental hydration free energy values of the solutes and water. As described in detail elsewhere (69), there are four possible GCMC moves: insertion, deletion, translation and rotation, with the probabilities for acceptance of these moves governed by the Metropolis criteria.\n• None The configuration at the end of each GCMC cycle is used as the starting configuration for a 0.5 to 1 nanosecond (ns) MD simulation during which the protein can undergo conformational changes as well as to obtain additional sampling of the water and solutes in and around the target molecule. Before the production MD, a 500 step SD minimization and a 100 ps equilibration is run. The last conformation from the production MD is used as the starting conformation of the next GCMC cycle.\n• None Ten independent 100 cycle GCMC-MD runs are recommended. For each cycle, 200,000 steps of GCMC and 0.5 ns MD are conducted yielding a cumulative 200 million steps of GCMC and 500 ns of MD over all 10 independent simulations.\n• None 3D probability distributions of selected atoms from the solutes, called “FragMaps”, from the GCMC/MD simulations are constructed. These are converted to GFE FragMaps based on a Boltzmann transformation, which allow for quantitative evaluation of ligand affinities, including the contribution of individual atoms. The GFE FragMaps can be used to guide ligand docking using the MC-SILCS approach (67) or for the calculation of target pharmacophore models using SILCS-Pharm (73, 74). VS against a database containing commercially available compounds, is an efficient way to find potential low-molecular weight binders to the target protein (59). While the ZINC database is available, researchers may want to prepare an in-house database for specific use.\n• None Download the commercial database(s) from chemical vendors such as Chembridge, Chemdiv, Maybridge, Specs, etc. These databases are most often in 2D SDF format and need further refinement.\n• None Convert 2D SDF files into 3D structure files such as MOL2 format files using a chemical data tool such as Open Babel (75) or RDKit (76). During the conversion, preliminary geometry optimization can be conducted to refine the 3D geometry to avoid bad contacts that may be transferred from the 2D structure. Missing hydrogens are added and appropriate protonation states are determined usually for pH 7.2 (see Note 3). Various tautomers can also be generated and if subsequent screening studies will use rigid ligand docking, multiple rotamers, typically 100 to 200, can also be generated for consideration of the conformations accessible to each molecule.\n• None All 3D structures can be further optimized using a force field based minimization to obtain more chemically-accurate structures and assign atomic charges for subsequent screening studies if required. Organic molecule force fields such as CGenFF (37, 38), GAFF (40) or MMFF94 (77) can be used for this purpose.\n• None When a database is prepared based on compounds from various vendors, in-house consistent identifiers are often needed to tag all the compounds for easy data management. For each compound, various entries such as physical properties and vendor information can be added for convenient use in subsequent analyses. The database, if extremely large, can be divided into several pieces for more efficient use. Finally, the database needs to be saved in the format required by the software to be used in following studies, for example, MOE (58) uses the binary MDB format while Dock uses the readable MOL2 format. Docking involves posing a compound in the putative binding site on the target in an optimal way defined by a scoring function in combination with a conformational sampling method (78). Various docking programs are available that differ based on the scoring function used to describe the interaction between small molecule and the target and the conformational sampling method used to generate the binding poses of the ligand on the protein. Here we present a docking protocol using the DOCK program (49) to illustrate the typical docking VS workflow.\n• None Prepare the target structure in the required DOCK input format. Define the desired binding pocket on the protein surface either using experimental information or by using a binding pocket prediction program as described in the Materials section. As docking typically is based on a single conformation of the target, MD simulations of the target can be used to generate multiple conformations for individual docking runs. In this scenario, each compound in the database is docked to each target conformation and the most favorable score for that compound is used for ranking as described below.\n• None Choose a sampling method and scoring scheme for docking. The DOCK program adopts an incremental ligand construction and conformational sampling scheme which divides ligands into fragments and reassembles these fragments in the binding site in a number of different conformational poses. Scoring the binding poses uses a physical force field based scoring function that includes both van der Waals (vdW) and electrostatic terms (see Note 4 also).\n• None Dock the entire compound database using a single crystal structure of the target or multiple conformations from MD mentioned above. Compounds are then ranked based on their interactions energies and selected for further analyses. It is suggested that multiple step VS can be used to balance the efficiency and reliability of docking results (79, 80). This approach applies a more approximate, computationally faster approach for the full database of typically > 1 million compounds from which a subset of compounds are selected for a secondary, more accurate dock screen.\n• None When using multiple step VS with DOCK in our laboratory, the first round of docking involves a coarse but fast optimization for each compound in the database targeting one or a few target structures. 50,000 compounds are selected from this round based on the vdW attractive energy normalized for the compound molecular weight (81). In this way, compounds with maximal steric complementarity with the target are selected rather than compounds with very favorable electrostatic interaction that do not complement the shape of the binding pocket. The molecular weight normalization accounts for the tendency of ranking based on interaction energies to favor larger compounds.\n• None The 50,000 compounds selected from the first round of VS are subject to a second round of docking using a more rigorous optimization that includes more steps of minimization and multiple protein conformations (~10) are used to take target flexibility into account. The top 1000 hits based on MW normalized total interaction energies, including both vdW and electrostatic terms are selected for further consideration. We emphasize that each compound is docked against each target conformation with the most favorable score over all the target conformations assigned to each compounds, with that score used to select the top 1000 compounds.\n• None The final selection step is to obtain ~100 compounds for biological assays that are diverse as well as having properties that will likely have favorable ADME properties (see Note 6). Diversity is important as it will maximize the potential of selecting biologically active compounds and having diverse lead compounds will improve the probability of ultimately identifying compounds that have a high probability of success in clinical trials. The top 1000 compounds can be clustered based on chemical structure and/or physiochemical properties to maximize the chemical diversity of the selected compounds (80). Other descriptors such as Lipinski’s rule of 5 (RO5) (82) or the 4D Bioavailability (4D-BA) ranking (83) can be used as metrics of ADME to filter the final list for testing, although using rigorous cutoffs based on these metric is not advised as there are many therapeutic agents on the market that “break the rules”. An alternative to docking based VS is target-based pharmacophore VS (84). This approach can quickly filter a database for potential binders to a specific bacterial target. A pharmacophore model is defined as spatially distributed chemical features that are essential for specific ligand-target binding. It represents a simplification of the detailed energetic information used by docking methods and so its computational requirements are much lower. While multiple methods can be used to generate pharmacophores (84), we will present a method based on information from SILCS as described in section 3.2. The workflow for generation of a SILCS-based pharmacophore model (73, 74) is illustrated in Figure 2.\n• None Similar to docking VS, the desired binding site needs to be defined.\n• None GFE FragMaps from SILCS are used as input into the SILCS-Pharm code (73, 74) to generate pharmacophore models. GFE cutoffs for FragMaps are used to define the sizes of related pharmacophore features and can be determined by visualizing FragMaps in a program such as VMD (85) and adjusting the contour value, as defined by the energy, to get well separated, local FragMap regions. If the chosen GFE contour values are too high there will be many bulky features while contour values that are too low lead to few or no pharmacophore features for VS.\n• None During generation of the pharmacophore by the SILCS-Pharm program FragMap voxels within the defined GFE cutoffs will be clustered into intermediate SILCS features and then converted into standard pharmacophore features. The final generated pharmacophore models or hypotheses are ranked by the sum of all the feature GFEs in the model for a given number of features. More favorable GFE scores typically indicate a more effective model for use in VS as the GFE defines the strength of functional group binding obtained from the SILCS simulation. It is suggested that the most GFE favorable SILCS-Pharm model with four features can be used for VS based on tests in our lab (74).\n• None Pharmacophore VS software such as Pharmer (52) or MOE (56) is then used to filter compounds in a database based on the selected SILCS-Pharm model. RMSD score, which represent the accordance between features in the pharmacophore model with related functional groups in a query compound, can be used to rank the final compound list.\n• None As mentioned above, multiple, low energy conformations for each compound in the database should be pre-generated before pharmacophore VS as ligand flexibility is not included in the posing algorithm. Programs such as Open Babel (58) can be used for this purpose. 100–200 conformations for each ligand should be enough according to our in-house tests.\n• None Once ligands are selected based on RMSD, alternate methods may be used to rank the ligands in a method referred to as consensus scoring (86). For example, SILCS ligand grid free energy (LGFE) scores (67) can be used to re-rank the list to give a free energy based ranking. The final compound list for experimental testing can be obtained by consensus scoring considering both RMSD and LGFE scores to maximize the hit potential (68). SILCS-Pharm workflow for pharmacophore based VS. The protocol starts from the SILCS simulation on the target (i), then FragMaps are generated (ii) and pharmacophore models are derived based on FragMaps (iii). The pharmacophore is then used in VS against a compound database (iv) that contains multiple conformations of each compound from which hit compounds are identified (v) and further tested in bioassays (vi). Once lead compounds are identified from experiments, LBDD methods can be utilized to start to develop an SAR or find more hit compounds. Of these, the similarity search method is the most straightforward and rapid approach (87). It can search for compounds that are chemically or physiochemically similar to the input compound, as described below. This approach may also be used as lead validation, as a compound that has multiple analogs with biological activity from which SAR can be developed is appropriate for further studies (88).\n• None Prepare the query compound in a format the program doing similarity search can recognize. The program MOE (58) has good similarity searching capabilities.\n• None Choose the types of fingerprint used to define the compounds in the database. The fingerprint of a molecule refers to a collection of descriptors such as structural, physical, or chemical properties that are used to define the molecule (79). Structural fingerprints, for example BIT MACCS (89) encodes information such as the presence of specific types of atoms, bonds, or rings in the molecule and can be used to identify compounds that are structurally similar to the lead, facilitating SAR development, and may have improved binding affinity (88). Physiochemical fingerprints such as MPMFP (90) encodes properties such as the free energy of solvation, polarity and molecular weight and can be used to identify compounds with dissimilar structures but similar physiochemical properties. This approach may help to identify novel hits that have activity but with a different chemical scaffold as compared to the lead compound, a process referred to as “lead hopping.” Such compounds could represent novel intellectual property (IP).\n• None Choose a similarity comparison method and do the similarity search against an in silico database. To quantify the extent of similarity between two molecules, various similarity metrics (91) are available such as the commonly used Tanimoto coefficient (92). Such metrics allow for giant databases to be rapidly screened. Compounds that are more similar to the query compound will have higher coefficients, such that the cutoff for the coefficient can be varied to select a desired number of similar compounds for testing. With the BIT MACCS fingerprints, a compound with a TC of 0.85 or higher (over a range of 0 to 1) is likely to have biological activity similar to that of the parent, query compound. When multiple hits for a specific bacterial target with activity data are available, structure-activity relationship (SAR) models can be developed and used to predict new compounds with improved activity (93). LBDD SAR models use regression methods to relate a set of descriptors of the lead series of compounds to their activities. The developed regression model can then be used to quantitatively predict the activity of the modified compounds (93). The descriptors can be physical or chemical properties of compounds or even geometric parameters that are representative for the spatial distributions of important functional groups in the compounds, i.e. pharmacophore features. Knowledge of the relationship of these properties to activity (i.e. SAR) can be used by the medicinal chemist to qualitatively design new, synthetically-accessible compounds that can be quantitatively evaluated. When developing SAR using pharmacophore descriptors, the appropriate conformations of the compounds that are responsible for the biological activity must be used. Here we illustrate the development of SAR using our in-house developed conformationally sampled pharmacophore (CSP) protocol (94, 95).\n• None Langevin dynamics based MD simulations are conducted for all known hit compounds. Aqueous solvation effects of the simulated compounds can be included using explicit solvent or are treated using an implicit solvation model such as the generalized Born continuum solvent model (96). Simulations should be performed for a minimum of 10 ns with the sampling of conformations of the ligand checked for convergence. If sampling is not adequate, the simulations should be extended or conducted using enhanced sampling methods, such as Temperature or Hamiltonian Replica Exchange methods (97). Snapshots are typically saved every 0.2 ps for analysis.\n• None Pharmacophore points, which are representative of well-conserved functional groups common in the hit compounds, such as aromatic ring centroid and hydrogen bond donor/acceptor atoms, are identified. Distances and angles between these pharmacophore points are measured throughout the trajectories from which probability distributions are obtained.\n• None Analysis can be performed on 1- (1D) or 2-dimensional (2D) probability distributions. 1D distributions involve, for example, a distance between two important functional groups or the angle between 3 groups. 2D distributions can be between all possible distance or angle pairs. The 1D or 2D distributions are recorded for each hit compound. One hit compound, usually the most active compound, is selected as reference. To quantify the extent of similarity of the distributions, the overlap coefficients (OC) between the probability distributions of the reference compound and other compounds are calculated (95).\n• None OCs are then used as independent variables in multiple regression analyses to fit the experimental activities. Different combinations of OCs for the various 1D and 2D pharmacophore probability distribution are regressed to identify those that yield the best correlation with the experimental data. For large training sets of compounds, multiple SAR models can be developed (95). The active compounds are usually divided into training and test set compounds with only the training set used for the SAR development, with the test set used to filter out the best SAR model. In studies of the opioids for a given set of compounds, CSP SAR models have been developed for both mu and delta efficacies (95, 98), allowing for identification of a compound that is both a mu agonist and a delta antagonist that may be of lower tolerance than opioids currently used in the clinic (99).\n• None The regression model can be extended by the inclusion of physiochemical properties such as polar solvent accessibility, MW among others (100, 101).\n• None The best CSP-SAR model can then be used to calculate predicted activities of query compounds and suggest the most potential compounds for further experimental tests. Ideally, multiple models are available for different activities allowing for both desirable and undesirable characteristics to be designed into the compounds, as done above with the opioids. In an ongoing study as the number of compounds for which biological activity is available increases the CSP model should be reevaluated to improve its predictability. Free energy perturbation (FEP) is a higher level, computationally demanding method with increased accuracy (see Note 5) that may be used to quantify the binding free energy change related to a modification in a compound (102). To save computational time, the single step FEP (SSFEP) may be applied (103). The approach uses a pre-computed MD simulation of the hit compound-target complex from which the free energy difference due to small, single non-hydrogen atom modifications (e.g. aromatic –H to –Cl or –OH) can be rapidly evaluated (103). This is in contrast to the need for many simulations in which the chemical modification is introduced in standard FEP methods (102). SSFEP has the ability to give rapid predictions of binding affinity changes related to modifications and, thus, is quite useful for lead optimization (104). The method may be applied using the following protocol with most simulations packages.\n• None Run five 10 ns MD simulations of the hit compound-target complex and of the hit compound alone in solution.\n• None For the chemical modification of the hit compound build in the modification onto the compounds with all other coordinates in the ligand and the remainder of the system identical to those from the original MD simulation.\n• None Evaluate the interaction energy of the hit compound with the full environment for both the initial, unmodified and modified states for the simulations in the presence of the target and hit compound alone in solution.\n• None Calculate the free energy difference, ΔG, in the presence of the protein and in aqueous solution based on the free energy perturbation formula (105) or the Bennett acceptance ratio (BAR) as described elsewhere (106). The difference in the free energy differences in the presence of the protein and in aqueous solution yields the overall free energy difference, ΔΔG, due to the chemical modification. The utility of the SSFEP approach is that the ΔΔG values for many modifications may be rapidly evaluated as the same trajectories from the original MD simulations of the hit compound are used in each case. This approach may be of use during the fine tuning of ligand affinity or specificity for a target or as required to improve physiochemical and pharmacokinetic properties without significantly altering desirable properties such as affinity.\n\n1 Conformational flexibility of molecules is a very important feature no matter if it is a small ligand or a large protein. Thus conformational sampling of a protein or ligand that produces an ensemble of biological meaningful conformations is necessary either for SBDD or for LBDD. The CADD methods presented in the chapter such as SILCS for SBDD or CSP for LBDD take this issue into account and thus have advantages over other CADD methods that only rely on single crystal structure or limited ligand conformations. 2 MD simulation is an efficient way to generate conformational ensembles. For larger system, more advanced MD techniques can be employed to enhance the sampling efficiency such as replica exchange methods. The protocols developed in our lab such as Hamiltonian replica exchange with biasing potentials (107) and replica exchange with concurrent solute scaling and Hamiltonian biasing in one dimension (108) are efficient replica exchange methods for use to enhance the MD efficiency. However, with all MD based methods the user must perform careful analysis to assure that the conformational ensemble is adequately converged for effective use in CADD. 3 Protonation states of titratable residues at the targeted binding site and in the ligand being studied are quite important when setting up the CADD calculations. For example, different protonation states of histidine residues can offer different hydrogen bonding types to potential ligands. Available experimental observations and known complex structures are useful to determine the correct protonation state of protein residue upon ligand binding. Software such as Reduce can assign the most appropriate protonation state based on environment. Constant pH MD simulation (109) where protonation state of titratable residue can change during the simulation may also be useful. With respect to ligands, many computational tools for prediction of ionization state are available, though common sense by the user is often adequate to deal with the most common ionizable groups such as carboxylates. 4 For VS, consensus scoring can be used instead of a single scoring scheme to rank hit compounds to allow more diversity of the identified compounds (86). For example, in our SILCS-Pharm protocol, LGFE and RMSD are used together to rank compounds that pass our pharmacophore model filtering. Additional scoring metrics can include the DOCK or AUTODOCK scores (49, 50), or the average interaction energies from MD simulations, with many other variations available. 5 In the ligand optimization stage of CADD, as only a few compounds are under consideration, accuracy rather than computational efficiency is usually pursued. This means more sophisticated binding affinity evaluation methods should be used. These include the free energy methods such as SSFEP or the SILCS based LGFE scoring discussed above. 6 When constructing the final list of compound for experimental assays from VS, in addition to the binding score, drug likeness can be another criterion to further filter the list. Potential bioavailability of a compound is often judged by the Lipinski’s rule of five (RO5) (82). The 4-dimensional bioavailability (4D-BA) descriptor (83) is a scalar term derived from the four criteria in RO5 and thus facilitates the selection of potential bioavailable compounds in an automatic fashion. Pan assay interference compounds (PAINS) filter (110) can also be used to remove compounds that are likely to interfere in experimental screening techniques mainly through potential reactivity leading to false positives."
    },
    {
        "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10819513",
        "document": "Identifying and developing a novel therapeutic agent can be an exhaustive and expensive endeavor in the expansive realm of drug discovery, where biology converges with chemistry. Historically, this journey relied on serendipitous discoveries or traditional trial-and-error methodologies, often consuming decades and substantial resources without a guaranteed outcome. The late 20th century heralded a transformative epoch for this field with the introduction of Computer-Aided Drug Design (CADD), which blends the intricate complexities of biological systems with the predictive power of computational algorithms and the development of chemical as well as biological-data-curated databases [1]. The core principle underpinning CADD are the utilization of computer algorithms on chemical and biological data to simulate and predict how a drug molecule will interact with its target—usually a protein or DNA sequence in the biological system [2]. This can range from understanding the drug’s molecular structure or target and predicting how the drug will bind to forecasting the pharmacological effects and potential side effects. CADDs birth was facilitated by two crucial advancements: the blossoming field of structural biology, which unveiled the three-dimensional architectures of biomolecules, and the exponential growth in computational power, which made it feasible to perform complex simulations in relatively shorter timeframes [3]. One of the earliest and most celebrated applications of CADD was in the design of the anti-influenza drug Zanamivir. This process showcased the potential of this approach to significantly truncate the drug discovery timeline [4]. At its core, CADD is subdivided into two main categories: structure-based drug design (SBDD) and ligand-based drug design (LBDD) [5]. SBDD leverages knowledge of the three-dimensional structure of the biological target, aiming to understand how potential drugs can fit and interact with it. In contrast, LBDD does not require knowledge of the target structure but instead focuses on known drug molecules and their pharmacological profiles to design new drug candidates (Figure 1) [6]. Conventional pathways in structure-based drug design (SBDD) and ligand-based drug design (LBDD) employ distinct methodologies. SBDD centers on target biomolecule structures, while LBDD relies on known ligand characteristics [7]. The rise of CADD is synonymous with the paradigm shift in drug discovery, where the process transitioned from being largely empirical to becoming more rational and targeted [8]. However, as with any scientific methodology, CADD has challenges. While predicting the behavior of biological systems solely based on computer simulations, it is important to acknowledge the inherent pitfalls. For instance, consider the hypothetical scenario where a computer simulation accurately models the biochemical interactions between a receptor and its target. However, if the simulation lacks crucial real-world data on external environmental factors or unexpected biological responses, the predictions may deviate significantly from the actual outcomes. These models, while sophisticated, often require experimental validation to ascertain their predictions [9,10]. In conclusion, CADD signifies the harmonious blend of biology and technology, aiming to expedite drug discovery. While it has already made significant strides in the field, its full potential is yet to be realized as newer computational methods and an increased understanding of biological systems come to the fore.\n\nDelineating the Array of Techniques in Computer-Aided Drug Design Computer-Aided Drug Design (CADD) is a powerful and interdisciplinary field that plays a pivotal role in modern drug discovery. It combines computational techniques with biological knowledge to identify and optimize potential drug candidates. This integration of diverse methodologies contributes to the versatility and effectiveness of CADD in the pharmaceutical industry. The vastness and versatility of CADD arise from the plethora of techniques and methodologies that underpin this field. This field’s effectiveness is rooted in its diverse methodologies, ranging from molecular modeling to predicting drug metabolism. Within CADD, adherence to Lipinski’s rule is paramount for achieving optimal oral drug likeliness, where compounds ideally minimize violations of criteria such as molecular weight, lipophilicity, hydrogen bond donors, and acceptors. This strategic integration of CADD principles and adherence to drug-likeness criteria collectively accelerates and refines the drug discovery process, showcasing the versatility and impactful role of CADD in various fields and the pharmaceutical industry (Figure 2) [11,12]. Elements of processing biomolecular data for CADD (image sourced from the European Research Council website [13]). Molecular Modeling: At the heart of CADD lies molecular modeling, which encompasses a wide range of computational techniques used to model or mimic the behavior of molecules. This involves creating three-dimensional models of molecular structures, often of proteins and ligands. This technique provides insights into molecules’ structural and functional attributes, facilitating a deeper understanding of how potential drugs might behave within the biological system [14]. It enables researchers to visualize and analyze the interactions between drug candidates and their target proteins, aiding in the design and optimization of potential drugs. Recently developed AI/ML-driven tools like AlphaFold2 [15], trTosetta [16,17], Robetta [18], RoseTTA Fold [19], ESMFold [20], and OmegaFold [21] have accelerated protein structure prediction by many folds [22]. Methods like molecular dynamics (MD) simulations can forecast the time-dependent behavior of molecules, capturing their motions and interactions over time through various tools like Gromacs [23], ACEMD [24], and OpenMM [25,26] (Table 1). Homology Modeling/Comparative Modeling: Create a 3D model of the target protein using a homologous protein’s empirically confirmed structure as a guide. Ab Initio Modeling: Build a 3D model of the target protein by sampling the protein’s conformational space without using experimental data. Threading: Build a 3D model of the target protein by aligning the protein sequence with the sequences of proteins of known structure. Hybrid Modeling: Combine two or more modeling approaches to improve the accuracy of the predicted structure. Molecular Dynamics: Simulate the behavior of the protein over time using classical or quantum mechanics. Knowledge-based methods: Use existing knowledge about protein structure and function to predict the structure of the target protein. Template-free methods: Build a 3D model of the target protein without using templates or homologous proteins. Fragment-assembly methods: Build a 3D model of the target protein by assembling fragments of known protein structures. Docking and Virtual Screening: Docking involves predicting the orientation and position of a drug molecule when it binds to its target protein. It estimates the binding affinity between the drug and its target, which is crucial in drug design [27]. Utilizing advanced tools such as AutoDock Vina [28], AutoDock GOLD [29], Glide [30], DOCK [31], LigandFit [32], and SwissDock [33], researchers can predict binding affinities and orientations with precision (Table 2). Conversely, virtual screening, a complementary approach, involves sifting through vast compound libraries to identify potential drug candidates [34]. Tools like DOCK [26], LigandFit [27], and ChemBioServer [35] facilitate this process, rapidly evaluating interactions and identifying compounds with high binding affinities. DOCK is renowned for structure-based drug design; LigandFit integrates into the Schrödinger suite; and ChemBioServer is an online platform for efficient virtual screening. The synergy of these docking and virtual screening tools enhances the accuracy of predictions, contributing significantly to the identification of promising drug candidates in the complex landscape of computational drug design. Many researchers, like Pinzi and Sohoo, have extensively discussed using these tools, showcasing their implementation in advancing the field of computational drug design [36,37]. Predicting the binding affinities and orientations of ligands. Fast, accurate, and easy to use. May not be as accurate for complex systems. Predicting the binding affinities and orientations of ligands, especially for flexible ligands. Requires a license and can be expensive. Predicting the binding affinities and orientations of ligands. Accurate and integrated with other Schrödinger tools. Requires the Schrödinger suite, which can be expensive. Predicting the binding affinities and orientations of ligands and performing virtual screening. It is versatile and can be used for both docking and virtual screening. Can be slower than other tools. Predicting the binding affinities and orientations of ligands. Easy to use and integrated with other Schrödinger tools. May not be as accurate for complex systems. Predicting the binding affinities and orientations of ligands. Easy to use and accessible online. May not be as accurate for complex systems. Quantitative Structure-Activity Relationship (QSAR): QSAR modeling explores the relationship between the chemical structure of molecules and their biological activities. Through statistical methods, QSAR models can predict the pharmacological activity of new compounds based on their structural attributes, enabling chemists to make informed modifications to enhance a drug’s potency or reduce its side effects [38,39]. In a research endeavor by Luo et al., the Similarity Ensemble Approach (SEA) served as a pivotal tool employed to gauge the precision of k-nearest neighbors (kNN) Quantitative Structure-Activity Relationship (QSAR) models. These models were systematically constructed for known ligands associated with individual G Protein-Coupled Receptor (GPCR) targets to unveil active and inactive molecules [40]. Meanwhile, a separate investigation by Raj et al. focused on developing QSAR models for 50 compounds exhibiting anti-HIV activity utilizing the molecular field analysis method. The findings underscored the critical role of electrostatic and steric interactions in influencing the anti-HIV activity of the compounds [41]. Furthermore, a novel approach was adopted in another study leveraging a deep neural network by Nigsch et al., in conjunction with QSAR models, to analyze a diverse collection of 1000 chemicals known for their anti-cancer activity. According to this study, integrating the QSAR approach with deep learning techniques proved advantageous, enabling the identification of critical structural characteristics that significantly contributed to the compounds’ anti-cancer efficacy [42]. Pharmacophore Modeling: A pharmacophore is a spatial arrangement of essential features in a molecule necessary for its pharmacological activity. Pharmacophore modeling is a fundamental component of contemporary drug discovery, involving the identification of spatial arrangements of essential molecular features crucial for a molecule’s pharmacological activity. This approach is potent for medicinal chemists, enabling the rational design of novel compounds with optimized pharmacological properties [43]. For example, pharmacophore modeling has proven instrumental in kinase inhibitors. Zhang et al. utilized pharmacophore modeling to discern essential features in active kinase inhibitors, including hydrogen bond donors, acceptors, and hydrophobic regions. The identified pharmacophore elements provided valuable guidance for designing novel compounds, resulting in improved selectivity and potency against specific kinases implicated in disease pathways [44]. Similarly, pharmacophore modeling has been applied to design ligands targeting G protein-coupled receptors (GPCRs). Fidom et al. exemplified this application by elucidating the spatial arrangement of features crucial for GPCR binding, such as aromatic interactions and hydrogen bonding [45,46]. The resulting pharmacophore models facilitated the development of ligands with enhanced affinity and selectivity for specific GPCRs involved in diverse therapeutic areas. In summary, the strategic use of pharmacophore modeling enables systematic analysis of the essential features contributing to a molecule’s pharmacological activity. This knowledge enhances understanding of ligand-receptor interactions and empowers researchers to rationally design compounds with enhanced efficacy and reduced side effects, shaping the future landscape of pharmaceutical research. Prediction of Drug Metabolism and Pharmacokinetics (DMPK): The ultimate success of a drug is not solely determined by its ability to bind to its target. Its metabolic stability, solubility, and how it is distributed in the body (pharmacokinetics) play pivotal roles. CADD offers tools that can predict the DMPK properties of compounds, allowing researchers to anticipate and address potential issues related to drug metabolism, bioavailability, and potential drug-drug interactions [47]. Novo Drug Design: Unlike other methods that modify existing molecules, de novo drug design creates new drug molecules from scratch. This technique leverages computational algorithms to generate new molecular structures that fit specific criteria, opening the door to many novel drug candidates [48]. In summary, the techniques embedded within CADD provide an integrated, multi-faceted approach to drug discovery. By offering a suite of tools that spans molecular modeling to drug metabolism prediction, CADD ensures that drug candidates are potent and selective and have optimal pharmacokinetic and safety profiles.\n\n3. Integration of Machine Learning and AI in CADD 3.1. Machine Learning and AI: The New Vanguard in Drug Discovery The technological renaissance that defines the 21st century has borne witness to the meteoric rise of Machine Learning (ML) and Artificial Intelligence (AI). These computational realms, known for their data-driven decision-making capabilities, have begun to significantly influence the sphere of Computer-Aided Drug Design (CADD), reshaping the contours of drug discovery [49]. Machine Learning, a subset of AI, hinges on algorithms that can learn patterns from vast data sets without being explicitly programmed for specific tasks (Figure 3) [50]. In drug discovery, ML has been instrumental in predicting molecular properties, understanding drug-receptor interactions, and forecasting biological responses based on chemical structures. Techniques such as deep learning, which uses neural networks modeled after the human brain, show immense potential for predicting complex drug-related outcomes with remarkable accuracy [51]. Predicting Drug-Drug Interactions: One of the challenges in drug discovery is understanding how a new drug might interact with other medications a patient might be taking. ML algorithms can process large databases of known drug-drug interactions to predict potential harmful combinations for novel compounds [52]. Drug Repurposing: Drug repurposing involves finding new therapeutic applications for existing drugs. By analyzing vast datasets, Machine Learning can identify potential new targets for existing medications, thus saving both time and costs associated with traditional drug discovery [53]. Generative Adversarial Networks (GANs) in Drug Design: GANs are a form of AI where two neural networks (a generator and a discriminator) are trained in tandem. The generator creates molecular structures while the discriminator evaluates them. Over time, the generator becomes adept at creating feasible and potentially bioactive molecular structures, which can be synthesized and tested in the lab [54]. Predictive Toxicology: One of the primary reasons drug candidates fail in clinical trials is unforeseen toxicity. ML models can help predict potential adverse effects by analyzing historical data on drug-induced toxicities, thus filtering out potentially toxic compounds early in the discovery process. Furthermore, utilizing descriptors like molecular weight, lipophilicity, and electronic properties, QSAR models predict toxicological effects by correlating a molecule’s structure with its potential toxicity. Additional descriptors, such as solubility, metabolic stability, and identification of toxicophores, provide comprehensive insights, facilitating early hazard identification and prioritization of compounds for experimental testing in computational toxicology. The integration of AI and ML into CADD signifies more than just the adoption of new technologies. It represents a paradigm shift from traditional hypothesis-driven research to data-driven discovery, leveraging the power of big data and computational prowess to inform decision-making at every step of drug discovery [55]. However, while these technologies promise a revolution in drug discovery, challenges persist. Issues such as data quality, interpretability of AI models, and the need for experimental validation continue to be focal areas of attention in this integration [56]. In essence, the synergy of ML, AI, and CADD sets the stage for a new era in drug discovery. An era characterized by increased efficiency, reduced costs, and the rapid delivery of effective therapeutics to patients in need.\n\nUnderstanding the Obstacles: The Roadblocks in Computer-Aided Drug Design While CADD offers unparalleled advantages in expediting and refining drug discovery, it is crucial to recognize its inherent challenges. A notable obstacle is the scarcity of experts proficient in AI/ML within CADD. Initiatives like specialized training programs and targeted recruitment are crucial; for example, organizations like in-silico Medicine are pioneering efforts to bridge this gap, fostering a skilled workforce capable of harnessing advanced computational techniques for drug discovery. Addressing these limitations can lead to better strategies and pave the way for more effective drug discovery workflows [57]. Accuracy of Predictive Models: In CADD, a major challenge lies in ensuring the accuracy of computational models, given that molecular dynamics simulations, docking scores, and machine learning predictions all rely on theoretical models. These models may not fully capture the intricate nuances of biological systems. To enhance accuracy, it is essential to delve into the intricacies of scoring algorithms [58]. Scoring algorithms in drug discovery are pivotal for predicting the binding affinity between molecules and their targets. To ensure their accuracy, it is imperative to actively mitigate the risk of false positives and negatives. This involves meticulous calibration of scoring parameters, the incorporation of diverse molecular descriptors, and continuous validation against experimental data. For instance, refining docking scores through rigorous validation against known binding affinities can enhance the reliability of predictions. By optimizing the balance between sensitivity and specificity, researchers can bolster confidence in scoring algorithms, reducing the likelihood of inaccuracies in drug discovery predictions [59,60,61]. Data Quality and Quantity: The predictions made by CADD tools are only as good as the data they are trained on. The predictions are likely inaccurate if the underlying data are of poor quality or insufficient. The lack of curated, high-quality datasets, especially in the context of machine learning in drug discovery, is a recurring challenge [62]. Removing outliers and ensuring consistent data formatting can refine molecular interaction datasets, minimizing inaccuracies and bolstering the reliability of computational models. Additionally, implementing standardized experimental protocols, such as consistent assay conditions and endpoint measurements, further contributes to improved data quality in CADD, ensuring robust and dependable results. Over-reliance on Computational Predictions: While CADD is a powerful tool, over-reliance on its predictions without subsequent experimental validation can lead to misguided efforts. Balancing computational predictions with experimental evidence is essential for a successful drug discovery [63]. Time and Computational Cost: Some advanced CADD techniques, especially those involving extensive molecular dynamics simulations or intricate machine learning models, require vast computational resources. The associated costs, both in terms of time and infrastructure, can be prohibitive for some research groups [64]. Representing Molecular Flexibility: Most biological molecules, including potential drug compounds and their target proteins, are highly flexible. Accurately representing this flexibility, especially in techniques like molecular docking, is challenging and can significantly impact the results of CADD studies [65]. Interpretability of AI Models: As AI and machine learning models become more complex, their predictions become more challenging to interpret. This ‘black-box’ nature of AI models can make it challenging to understand why a particular compound is predicted to be active or how its structure might be optimized [66]. Despite these challenges, the potential benefits of CADD in drug discovery are immense. By acknowledging these limitations and continually striving to address them through innovation and research, CADD will remain at the forefront of modern drug discovery, shaping the future of therapeutics.\n\n10. The Future of CADD: Emerging Technologies and Innovations 10.1. Charting the Horizon: Navigating the Next Frontiers of Computer-Aided Drug Design The transformative influence of CADD on drug discovery is beyond dispute. However, like any evolving discipline, the future holds new challenges and unparalleled opportunities. Harnessing cutting-edge technologies and paradigms can unlock an era where drug discovery is faster, more precise, and more patient-centric [103]. Traditional computing faces limitations in handling complex drug design problems. Quantum computing, with its ability to control and compute information radically differently, may revolutionize molecular modeling and simulations, enabling the exploration of vast molecular spaces in mere seconds [104]. Immersive technologies can provide researchers with an intuitive understanding of molecular structures and interactions. Through AR/VR, drug design can become a more tactile and visual endeavor, enhancing molecular modeling and collaborative efforts [105]. Machine learning, notably deep learning, is rapidly becoming integral to CADD. Neural networks, with their ability to recognize patterns from vast datasets, can predict drug interaction toxicity and suggest novel drug compounds [106]. As genomic sequencing becomes more commonplace, CADD tools that cater to individual genetic profiles will gain prominence. This will foster an era of genuinely personalized drugs tailored to an individual’s genetic makeup [107]. Open-source and collaborative platforms can democratize drug discovery. By harnessing the collective intelligence of the global scientific community, these platforms can accelerate the drug discovery process and integrate diverse expertise [108]. As environmental concerns come to the fore, integrating principles of green chemistry into CADD can result in drug synthesis processes that are both efficient and environmentally benign [109]. In the grand vista of drug discovery, the future of CADD shines bright. Embracing innovations and pushing the boundaries of technology will enhance the discipline and promise a better healthcare future for all. In a progressively interconnected world, the role of collaborative networks and open-source platforms in CADD cannot be overstated. These entities amplify the collective intellectual prowess of researchers worldwide, allowing for a swift, democratic, and cost-efficient drug discovery process [110]. Traditional drug discovery often demands vast resources, making it an exclusive venture. Open-source platforms democratize this, allowing researchers to contribute and access advanced CADD tools [111] irrespective of their affiliations. Initiatives like the Open-Source Drug Discovery (OSDD) project for tuberculosis exemplify this global commitment [112]. Crowdsourcing platforms in CADD harness the power of global intellect. Challenges posted on these platforms lead to diverse solution pathways, many of which might be non-traditional yet highly effective [113]. Open-source platforms ensure that CADD tools are continually improved. Community-driven tools are updated frequently based on user feedback and the latest scientific advancements [114,115,116]. In an age characterized by collaboration and open access, collaborative networks and open-source platforms in CADD emerge as beacons of hope. They underline the belief that in unity lies strength, and in shared knowledge lies the promise of a healthier tomorrow. 10.3. Drawing Lines in the Digital Sand: Navigating the Ethical and Regulatory Labyrinths of Computer-Aided Drug Design In the exhilarating race of drug discovery through CADD, the underlying ethical and regulatory considerations provide crucial checkpoints. Ensuring these digital methodologies hasten drug discovery and preserving the highest ethical standards becomes paramount [117]. With the increased utilization of patient data in personalized medicine, ensuring data privacy are paramount. Regulations like the General Data Protection Regulation (GDPR) guide the collecting, storing, and processing of personal data in research, imposing stringent data protection requirements [118]. Defining IP rights can become murky as CADD veers towards more collaborative and open-source models. Balancing between open-access and proprietary claims ensures researchers and institutions obtained due credit [119]. AI-driven methodologies in CADD can sometimes inherit biases present in their training data. Ensuring that these models are transparent, interpretable, and unbiased becomes essential for ethical drug discovery [120]. Reproducibility, a cornerstone of scientific rigor, must be confirmed in CADD. Ensuring consistent results across different computational settings is pivotal [121] with increasingly complex algorithms and models. While CADD can predict potential drug candidates, the transition to in vivo testing, especially on animals, brings its own set of ethical concerns. Regulatory bodies provide guidelines on minimizing animal testing and ensuring humane conditions [122]. For a drug to reach the market, it is not enough for it to be discovered through CADD; regulatory bodies must accept and validate these methodologies. Collaborations between CADD scientists and regulatory authorities can streamline this acceptance process [123]. In conclusion, while CADD offers transformative potential in drug discovery, it is essential to navigate the process with ethical integrity and in compliance with existing regulations. As the adage goes, with great power comes great responsibility, and in the realm of CADD, this holds especially true. 10.4. A Glimpse into the Horizon: Envisioning the Next Epoch of Computer-Aided Drug Design The ever-evolving realm of CADD continues to offer promise and innovation. However, as with any cutting-edge field, it is fraught with challenges and uncertainties. Looking forward, it is essential to pinpoint potential trajectories and hurdles that might shape the next generation of drug discovery [124]. As we stand on the brink of a quantum revolution, the potential for quantum computers to optimize molecular simulations and improve drug design methodologies is immense. They promise speed and precision previously deemed unattainable [104]. The continued evolution of AI promises more sophisticated drug discovery models. Deep learning models that can simulate protein folding or predict drug-target interactions with increased accuracy are on the horizon [125]. With advancements in genomics, proteomics, and metabolomics, integrating this vast and varied data into CADD will allow for a more holistic approach to drug design, considering intricate biological systems [81]. As the volume of biomedical data explodes, standardizing this data to ensure consistency and reliability in CADD methodologies becomes a significant challenge [126]. The ecological footprint of drug development cannot be ignored. Future CADD models might need to incorporate sustainability metrics, ensuring that drug discovery does not come at an environmental cost [127]. As AI becomes more prominent in drug discovery, ethical concerns about machine autonomy, transparency in algorithmic decisions, and potential biases become more pronounced [128]. In essence, the future of CADD is an intricate tapestry of innovation, challenges, and ethical considerations. By preemptively addressing these challenges and harnessing new technologies, CADD can continue revolutionizing drug discovery, ensuring better health outcomes for all.\n\n11.1. Forging Synergy: When the Computational Meets the Experimental in Drug Design As the chasm between experimental biology and computational methodologies in drug design narrows, the symbiosis between these disciplines offers unparalleled potential. While CADD provides the tools to forecast and simulate, experimental data acts as both the foundation and the validator of these predictions [129]. While CADD can predict a myriad of drug properties, these remain theoretical until experimentally verified. Experimental results offer evidence of drug efficacy, metabolism, and safety, among other characteristics [130]. Experimental data does not just validate CADD predictions; it also enriches them. This data are invaluable when a predicted molecule does not yield the expected results in the lab. It informs subsequent design iterations, leading to a more refined and likely successful candidate [131]. Molecular dynamics simulations can predict how molecules will behave over time. Yet, experimental techniques like X-ray crystallography or nuclear magnetic resonance (NMR) provide snapshots of these molecules, which can validate or recalibrate these simulations [132]. Experimental results from high-throughput screenings, assays, and other methodologies provide a wealth of data. This data can be mined using AI and other CADD tools to uncover patterns, relationships, or potential drug candidates that might be overlooked [133]. With a growing database of experimental results, the predictive models used in CADD can be trained more effectively. This integration helps continually refine the accuracy of CADD models, making them more reliable over time [134]. While CADD offers tools to navigate the complex maze of biological systems, real-world experimental data provides the actual map. Together, they offer a more straightforward path to successful drug candidates [135]. In sum, the confluence of experimental data and CADD are more than just complementary; it is synergistic. Drug discovery becomes more robust, efficient, and accurate by fostering a more intimate relationship between these domains. 11.2. Shaping the Drug Designers of Tomorrow: The Essentiality of CADD in Modern Education The realm of drug discovery, rife with promise, demands cutting-edge technology and well-equipped minds to wield it. As CADD emerges as a linchpin in the drug discovery landscape, it underscores the urgency of integrating CADD training into contemporary education [136]. While traditional chemistry and biology programs emphasize foundational knowledge, introducing CADD modules can offer students early exposure to the computational aspects of drug design. Such foundational exposure can spark interest and cultivate the next generation of drug discoverers [137]. Universities worldwide are realizing the importance of specialized courses focusing solely on CADD. These courses amalgamate computational methodologies, biology, and drug pharmacology, producing experts capable of spearheading drug discovery ventures [138]. The volatile, evolving nature of CADD mandates professionals to be in a perpetual state of learning. Workshops, online courses, and conferences focusing on the latest CADD methodologies are indispensable for professionals to stay abreast of [139]. Drug design is a symphony of various disciplines. Ensuring that CADD training is not siloed but integrates elements of biology, chemistry, AI, and even ethics is crucial. A holistic, multidisciplinary approach produces well-rounded professionals [140]. Encouragingly, many institutions offer research opportunities focused on CADD for postgraduates and early-career scientists. These platforms allow hands-on experience, bridging the gap between theory and real-world applications [141]. The pharmaceutical and biotech industries have a vested interest in the proficiency of CADD professionals. Collaboration between academia and industry can drive curriculum development, ensuring it aligns with the real-world demands of drug discovery [142]. Conclusively, as the tower of drug discovery leans more on CADD, training proficient individuals becomes paramount. An investment in education is an investment in a healthier, brighter future.\n\nThe rapid progression of CADD, coupled with its integral role in recent drug discoveries, prompts us to ponder the trajectory of this discipline and the challenges it is poised to encounter [143]. With quantum computers inching closer to practical applications, their potential impact on CADD is enormous. Quantum algorithms can drastically reduce the time required for molecular simulations, thereby accelerating drug discovery manifolds [104]. While AI and machine learning have already entrenched themselves in CADD, the proliferation of deep learning models promises even more precise predictions. These models, trained on vast datasets, might eventually surpass traditional simulation methods in accuracy [144]. With advances in biology, previously deemed ‘undruggable’ targets are now within CADDs crosshairs. This shift demands that CADD evolve and devise strategies to engage with these challenging targets [145]. As CADD and AI models start playing more prominent roles in determining drug viability, ethical questions about trustworthiness, bias in predictions, and accountability will arise. Addressing these concerns will be paramount [146]. With genomics, proteomics, and metabolomics offering a deluge of biological data, CADDs future lies in efficiently harnessing this data. Integrating multi-omics data can provide a holistic view of biological systems, facilitating better drug design [147]. Furthermore, in addition to extensive datasets, the execution of numerous simulation processes in AI/ML necessitates high-specification hardware. The computational demands of AI/ML algorithms, such as deep learning models, require robust hardware configurations with powerful processors, ample memory, and efficient GPUs to handle complex computations effectively. Access to high-performance hardware is crucial for optimizing the training and inference phases of AI/ML systems, ensuring the timely and accurate processing of tasks. As collaborative efforts become more common, ensuring the privacy and security of shared data becomes critical. Developing protocols and standards for data sharing without compromising data security will be pivotal [148]. The environmental footprint of drug discovery, especially with energy-intensive computational methods, cannot be ignored. Future CADD methodologies must be sustainable, considering drug efficacy and environmental impact [149]. In essence, while the future of CADD radiates promise, it is not without its challenges. Navigating this labyrinth will necessitate a fusion of technological prowess, ethical considerations, and a commitment to sustainable practices.\n\nPersonalized medicine, often interchangeable with precision medicine, seeks to customize healthcare by tailoring decisions and practices to the individual patient. Integrating CADD with personalized medicine stands to revolutionize treatment paradigms [157]. The completion of the Human Genome Project has provided a detailed genetic blueprint. Leveraging this information, CADD can help design drugs targeting specific genetic mutations or variants associated with diseases [158]. By integrating genetic, epigenetic, and proteomic data, CADD tools can forecast a patient’s likely response to a drug. This facilitates the administration of therapies that are most likely efficacious while minimizing adverse effects [159]. In some rare diseases caused by particular genetic mutations, CADD offers the possibility of creating drugs tailored for individual patients, an approach that would be the pinnacle of personalized medicine. Biomarkers are vital in personalized medicine, providing measurable indicators of disease states. CADD aids in the discovery of drugs that can modulate these biomarkers, leading to personalized therapeutic solutions. As electronic health records become more prevalent, integrating this real-world data with CADD models can provide insights into drug performance in diverse populations, allowing for more individualized therapy recommendations. The prospects of personalized medicine via CADD are exciting, but they come with ethical dilemmas, especially regarding data privacy and potential inequalities in access to tailored treatments. In sum, CADDs intersection with personalized medicine promises treatments optimized for each patient, transcending the one-size-fits-all approach. A new era of healthcare beckons by harnessing the power of computational tools in sync with individual data. Often called theranostics, this approach leverages CADD to develop drugs alongside diagnostic tests that determine a patient’s suitability for the treatment. This ensures the right drug reaches the right patient at the right time. Personalized medicine is greatly enhanced by patient-derived models like organoids or patient-derived xenografts. CADD can use data from these models to simulate drug responses, allowing individualized therapy adjustments [160]. Cancer epitomizes the need for personalized medicine, given the heterogeneity in tumors, even within the same cancer type. CADD tools can analyze tumor genomic data to identify druggable targets unique to each patient’s cancer profile [161]. As wearable technology becomes increasingly sophisticated, capturing diverse health metrics and integrating this data with CADD models can fine-tune drug recommendations based on real-time patient status [162]. While the prospects of CADD-driven personalized medicine are revolutionary, the associated costs are a concern. Ensuring these tailored treatments are economically viable and accessible to all, regardless of socio-economic status, is a pressing challenge [163]. Integrating CADD with personalized medicine could redefine treatment regimens, ensuring patients receive interventions tailored to their unique genetic and physiological profiles. But as with all transformative advances, balancing innovation with ethics, accessibility, and cost remains pivotal."
    },
    {
        "link": "https://sciencedirect.com/book/9780128223123/molecular-docking-for-computer-aided-drug-design",
        "document": ""
    },
    {
        "link": "https://mdpi.com/1424-8247/17/1/22",
        "document": "Identifying and developing a novel therapeutic agent can be an exhaustive and expensive endeavor in the expansive realm of drug discovery, where biology converges with chemistry. Historically, this journey relied on serendipitous discoveries or traditional trial-and-error methodologies, often consuming decades and substantial resources without a guaranteed outcome. The late 20th century heralded a transformative epoch for this field with the introduction of Computer-Aided Drug Design (CADD), which blends the intricate complexities of biological systems with the predictive power of computational algorithms and the development of chemical as well as biological-data-curated databases [ 1 ]. The core principle underpinning CADD are the utilization of computer algorithms on chemical and biological data to simulate and predict how a drug molecule will interact with its target—usually a protein or DNA sequence in the biological system [ 2 ]. This can range from understanding the drug’s molecular structure or target and predicting how the drug will bind to forecasting the pharmacological effects and potential side effects. CADDs birth was facilitated by two crucial advancements: the blossoming field of structural biology, which unveiled the three-dimensional architectures of biomolecules, and the exponential growth in computational power, which made it feasible to perform complex simulations in relatively shorter timeframes [ 3 ]. One of the earliest and most celebrated applications of CADD was in the design of the anti-influenza drug Zanamivir. This process showcased the potential of this approach to significantly truncate the drug discovery timeline [ 4 ]. At its core, CADD is subdivided into two main categories: structure-based drug design (SBDD) and ligand-based drug design (LBDD) [ 5 ]. SBDD leverages knowledge of the three-dimensional structure of the biological target, aiming to understand how potential drugs can fit and interact with it. In contrast, LBDD does not require knowledge of the target structure but instead focuses on known drug molecules and their pharmacological profiles to design new drug candidates ( Figure 1 ) [ 6 ]. The rise of CADD is synonymous with the paradigm shift in drug discovery, where the process transitioned from being largely empirical to becoming more rational and targeted [ 8 ]. However, as with any scientific methodology, CADD has challenges. While predicting the behavior of biological systems solely based on computer simulations, it is important to acknowledge the inherent pitfalls. For instance, consider the hypothetical scenario where a computer simulation accurately models the biochemical interactions between a receptor and its target. However, if the simulation lacks crucial real-world data on external environmental factors or unexpected biological responses, the predictions may deviate significantly from the actual outcomes. These models, while sophisticated, often require experimental validation to ascertain their predictions [ 9 10 ]. In conclusion, CADD signifies the harmonious blend of biology and technology, aiming to expedite drug discovery. While it has already made significant strides in the field, its full potential is yet to be realized as newer computational methods and an increased understanding of biological systems come to the fore.\n\nDelineating the Array of Techniques in Computer-Aided Drug Design Computer-Aided Drug Design (CADD) is a powerful and interdisciplinary field that plays a pivotal role in modern drug discovery. It combines computational techniques with biological knowledge to identify and optimize potential drug candidates. This integration of diverse methodologies contributes to the versatility and effectiveness of CADD in the pharmaceutical industry. The vastness and versatility of CADD arise from the plethora of techniques and methodologies that underpin this field. This field’s effectiveness is rooted in its diverse methodologies, ranging from molecular modeling to predicting drug metabolism. Within CADD, adherence to Lipinski’s rule is paramount for achieving optimal oral drug likeliness, where compounds ideally minimize violations of criteria such as molecular weight, lipophilicity, hydrogen bond donors, and acceptors. This strategic integration of CADD principles and adherence to drug-likeness criteria collectively accelerates and refines the drug discovery process, showcasing the versatility and impactful role of CADD in various fields and the pharmaceutical industry ( Figure 2 ) [ 11 12 ]. Molecular Modeling: At the heart of CADD lies molecular modeling, which encompasses a wide range of computational techniques used to model or mimic the behavior of molecules. This involves creating three-dimensional models of molecular structures, often of proteins and ligands. This technique provides insights into molecules’ structural and functional attributes, facilitating a deeper understanding of how potential drugs might behave within the biological system [ 14 ]. It enables researchers to visualize and analyze the interactions between drug candidates and their target proteins, aiding in the design and optimization of potential drugs. Recently developed AI/ML-driven tools like AlphaFold2 [ 15 ], trTosetta [ 16 17 ], Robetta [ 18 ], RoseTTA Fold [ 19 ], ESMFold [ 20 ], and OmegaFold [ 21 ] have accelerated protein structure prediction by many folds [ 22 ]. Methods like molecular dynamics (MD) simulations can forecast the time-dependent behavior of molecules, capturing their motions and interactions over time through various tools like Gromacs [ 23 ], ACEMD [ 24 ], and OpenMM [ 25 26 ] ( Table 1 ). Docking and Virtual Screening: Docking involves predicting the orientation and position of a drug molecule when it binds to its target protein. It estimates the binding affinity between the drug and its target, which is crucial in drug design [ 27 ]. Utilizing advanced tools such as AutoDock Vina [ 28 ], AutoDock GOLD [ 29 ], Glide [ 30 ], DOCK [ 31 ], LigandFit [ 32 ], and SwissDock [ 33 ], researchers can predict binding affinities and orientations with precision ( Table 2 ). Conversely, virtual screening, a complementary approach, involves sifting through vast compound libraries to identify potential drug candidates [ 34 ]. Tools like DOCK [ 26 ], LigandFit [ 27 ], and ChemBioServer [ 35 ] facilitate this process, rapidly evaluating interactions and identifying compounds with high binding affinities. DOCK is renowned for structure-based drug design; LigandFit integrates into the Schrödinger suite; and ChemBioServer is an online platform for efficient virtual screening. The synergy of these docking and virtual screening tools enhances the accuracy of predictions, contributing significantly to the identification of promising drug candidates in the complex landscape of computational drug design. Many researchers, like Pinzi and Sohoo, have extensively discussed using these tools, showcasing their implementation in advancing the field of computational drug design [ 36 37 ]. Quantitative Structure-Activity Relationship (QSAR): QSAR modeling explores the relationship between the chemical structure of molecules and their biological activities. Through statistical methods, QSAR models can predict the pharmacological activity of new compounds based on their structural attributes, enabling chemists to make informed modifications to enhance a drug’s potency or reduce its side effects [ 38 39 ]. In a research endeavor by Luo et al., the Similarity Ensemble Approach (SEA) served as a pivotal tool employed to gauge the precision of k-nearest neighbors (kNN) Quantitative Structure-Activity Relationship (QSAR) models. These models were systematically constructed for known ligands associated with individual G Protein-Coupled Receptor (GPCR) targets to unveil active and inactive molecules [ 40 ]. Meanwhile, a separate investigation by Raj et al. focused on developing QSAR models for 50 compounds exhibiting anti-HIV activity utilizing the molecular field analysis method. The findings underscored the critical role of electrostatic and steric interactions in influencing the anti-HIV activity of the compounds [ 41 ]. Furthermore, a novel approach was adopted in another study leveraging a deep neural network by Nigsch et al., in conjunction with QSAR models, to analyze a diverse collection of 1000 chemicals known for their anti-cancer activity. According to this study, integrating the QSAR approach with deep learning techniques proved advantageous, enabling the identification of critical structural characteristics that significantly contributed to the compounds’ anti-cancer efficacy [ 42 ]. Pharmacophore Modeling: A pharmacophore is a spatial arrangement of essential features in a molecule necessary for its pharmacological activity. Pharmacophore modeling is a fundamental component of contemporary drug discovery, involving the identification of spatial arrangements of essential molecular features crucial for a molecule’s pharmacological activity. This approach is potent for medicinal chemists, enabling the rational design of novel compounds with optimized pharmacological properties [ 43 ]. For example, pharmacophore modeling has proven instrumental in kinase inhibitors. Zhang et al. utilized pharmacophore modeling to discern essential features in active kinase inhibitors, including hydrogen bond donors, acceptors, and hydrophobic regions. The identified pharmacophore elements provided valuable guidance for designing novel compounds, resulting in improved selectivity and potency against specific kinases implicated in disease pathways [ 44 ]. Similarly, pharmacophore modeling has been applied to design ligands targeting G protein-coupled receptors (GPCRs). Fidom et al. exemplified this application by elucidating the spatial arrangement of features crucial for GPCR binding, such as aromatic interactions and hydrogen bonding [ 45 46 ]. The resulting pharmacophore models facilitated the development of ligands with enhanced affinity and selectivity for specific GPCRs involved in diverse therapeutic areas. In summary, the strategic use of pharmacophore modeling enables systematic analysis of the essential features contributing to a molecule’s pharmacological activity. This knowledge enhances understanding of ligand-receptor interactions and empowers researchers to rationally design compounds with enhanced efficacy and reduced side effects, shaping the future landscape of pharmaceutical research. Prediction of Drug Metabolism and Pharmacokinetics (DMPK): The ultimate success of a drug is not solely determined by its ability to bind to its target. Its metabolic stability, solubility, and how it is distributed in the body (pharmacokinetics) play pivotal roles. CADD offers tools that can predict the DMPK properties of compounds, allowing researchers to anticipate and address potential issues related to drug metabolism, bioavailability, and potential drug-drug interactions [ 47 ]. Novo Drug Design: Unlike other methods that modify existing molecules, de novo drug design creates new drug molecules from scratch. This technique leverages computational algorithms to generate new molecular structures that fit specific criteria, opening the door to many novel drug candidates [ 48 ]. In summary, the techniques embedded within CADD provide an integrated, multi-faceted approach to drug discovery. By offering a suite of tools that spans molecular modeling to drug metabolism prediction, CADD ensures that drug candidates are potent and selective and have optimal pharmacokinetic and safety profiles.\n\nUnderstanding the Obstacles: The Roadblocks in Computer-Aided Drug Design While CADD offers unparalleled advantages in expediting and refining drug discovery, it is crucial to recognize its inherent challenges. A notable obstacle is the scarcity of experts proficient in AI/ML within CADD. Initiatives like specialized training programs and targeted recruitment are crucial; for example, organizations like in-silico Medicine are pioneering efforts to bridge this gap, fostering a skilled workforce capable of harnessing advanced computational techniques for drug discovery. Addressing these limitations can lead to better strategies and pave the way for more effective drug discovery workflows [ 57 ]. 60, Accuracy of Predictive Models: In CADD, a major challenge lies in ensuring the accuracy of computational models, given that molecular dynamics simulations, docking scores, and machine learning predictions all rely on theoretical models. These models may not fully capture the intricate nuances of biological systems. To enhance accuracy, it is essential to delve into the intricacies of scoring algorithms [ 58 ]. Scoring algorithms in drug discovery are pivotal for predicting the binding affinity between molecules and their targets. To ensure their accuracy, it is imperative to actively mitigate the risk of false positives and negatives. This involves meticulous calibration of scoring parameters, the incorporation of diverse molecular descriptors, and continuous validation against experimental data. For instance, refining docking scores through rigorous validation against known binding affinities can enhance the reliability of predictions. By optimizing the balance between sensitivity and specificity, researchers can bolster confidence in scoring algorithms, reducing the likelihood of inaccuracies in drug discovery predictions [ 59 61 ]. Data Quality and Quantity: The predictions made by CADD tools are only as good as the data they are trained on. The predictions are likely inaccurate if the underlying data are of poor quality or insufficient. The lack of curated, high-quality datasets, especially in the context of machine learning in drug discovery, is a recurring challenge [ 62 ]. Removing outliers and ensuring consistent data formatting can refine molecular interaction datasets, minimizing inaccuracies and bolstering the reliability of computational models. Additionally, implementing standardized experimental protocols, such as consistent assay conditions and endpoint measurements, further contributes to improved data quality in CADD, ensuring robust and dependable results. Over-reliance on Computational Predictions: While CADD is a powerful tool, over-reliance on its predictions without subsequent experimental validation can lead to misguided efforts. Balancing computational predictions with experimental evidence is essential for a successful drug discovery [ 63 ]. Time and Computational Cost: Some advanced CADD techniques, especially those involving extensive molecular dynamics simulations or intricate machine learning models, require vast computational resources. The associated costs, both in terms of time and infrastructure, can be prohibitive for some research groups [ 64 ]. Representing Molecular Flexibility: Most biological molecules, including potential drug compounds and their target proteins, are highly flexible. Accurately representing this flexibility, especially in techniques like molecular docking, is challenging and can significantly impact the results of CADD studies [ 65 ]. Interpretability of AI Models: As AI and machine learning models become more complex, their predictions become more challenging to interpret. This ‘black-box’ nature of AI models can make it challenging to understand why a particular compound is predicted to be active or how its structure might be optimized [ 66 ]. Despite these challenges, the potential benefits of CADD in drug discovery are immense. By acknowledging these limitations and continually striving to address them through innovation and research, CADD will remain at the forefront of modern drug discovery, shaping the future of therapeutics.\n\n5. Experimental Validation in CADD: From In-Silico to the Lab Bench At the crossroads of drug discovery, Computer-Aided Drug Design (CADD) outputs demand rigorous experimental validation to ensure their biological and therapeutic relevance. A drug’s true potential can only be ascertained through this synergy between the computational and experimental realms [ 67 ]. No matter how advanced, computational predictions are inherently rooted in theoretical models. While these models can approximate biological systems, discrepancies always exist. Experimental validation serves as the crucible, determining whether a predicted molecule has genuine therapeutic promise or is merely a computational artifact [ 68 ]. After the CADD process identifies potential drug candidates, biochemical assays often serve as the first validation step. Such assays measure the interaction between the proposed drug molecule and its intended target protein, offering insights into binding affinities and possible mechanisms of action [ 69 ]. Cell-based assays are employed to further understand a drug’s biological relevance. These tests assess how a compound affects cellular functions, allowing researchers to ascertain its potential efficacy and toxicity in a more complex, biologically relevant setting [ 70 ]. Before any drug candidate reaches human trials, its efficacy, safety, and pharmacokinetic properties must be investigated in vivo. Animal models serve this purpose, providing a more comprehensive understanding of how a drug will behave in a living organism [ 71 ]. Techniques such as X-ray crystallography and NMR spectroscopy can provide atomic-level details of the interaction between a drug and its target. Such insights can validate computational predictions, refine drug design strategies, and offer mechanistic understandings of drug action [ 72 ]. Often, experimental validation reveals unexpected outcomes or unanticipated challenges. Rather than being a linear process, drug discovery often involves iterations between CADD predictions and experimental testing, leading to refined models and better drug candidates [ 73 ]. Furthermore, to ensure robust and validated results in CADD, the initial imperative is to employ software acknowledged for its accuracy in scoring results in simulations and predictions. Utilizing well-validated tools, such as AutoDock Vina or Schrödinger Suite, establishes a foundation of reliability and precision in the computational models, laying the groundwork for more dependable and meaningful insights in CADD simulations and predictions. In essence, while CADD provides a powerful arsenal of tools to guide and expedite drug discovery, the proof of a drug’s worth always rests in the experimental realm. This synergy between computation and experimentation forms the backbone of modern drug discovery, ensuring that only the most promising compounds transition from the digital domain to the bedside.\n\n10. The Future of CADD: Emerging Technologies and Innovations 10.1. Charting the Horizon: Navigating the Next Frontiers of Computer-Aided Drug Design The transformative influence of CADD on drug discovery is beyond dispute. However, like any evolving discipline, the future holds new challenges and unparalleled opportunities. Harnessing cutting-edge technologies and paradigms can unlock an era where drug discovery is faster, more precise, and more patient-centric [ 103 ]. Traditional computing faces limitations in handling complex drug design problems. Quantum computing, with its ability to control and compute information radically differently, may revolutionize molecular modeling and simulations, enabling the exploration of vast molecular spaces in mere seconds [ 104 ]. Immersive technologies can provide researchers with an intuitive understanding of molecular structures and interactions. Through AR/VR, drug design can become a more tactile and visual endeavor, enhancing molecular modeling and collaborative efforts [ 105 ]. Machine learning, notably deep learning, is rapidly becoming integral to CADD. Neural networks, with their ability to recognize patterns from vast datasets, can predict drug interaction toxicity and suggest novel drug compounds [ 106 ]. As genomic sequencing becomes more commonplace, CADD tools that cater to individual genetic profiles will gain prominence. This will foster an era of genuinely personalized drugs tailored to an individual’s genetic makeup [ 107 ]. Open-source and collaborative platforms can democratize drug discovery. By harnessing the collective intelligence of the global scientific community, these platforms can accelerate the drug discovery process and integrate diverse expertise [ 108 ]. As environmental concerns come to the fore, integrating principles of green chemistry into CADD can result in drug synthesis processes that are both efficient and environmentally benign [ 109 ]. In the grand vista of drug discovery, the future of CADD shines bright. Embracing innovations and pushing the boundaries of technology will enhance the discipline and promise a better healthcare future for all. In a progressively interconnected world, the role of collaborative networks and open-source platforms in CADD cannot be overstated. These entities amplify the collective intellectual prowess of researchers worldwide, allowing for a swift, democratic, and cost-efficient drug discovery process [ 110 ]. Traditional drug discovery often demands vast resources, making it an exclusive venture. Open-source platforms democratize this, allowing researchers to contribute and access advanced CADD tools [ 111 ] irrespective of their affiliations. Initiatives like the Open-Source Drug Discovery (OSDD) project for tuberculosis exemplify this global commitment [ 112 ]. 115, Crowdsourcing platforms in CADD harness the power of global intellect. Challenges posted on these platforms lead to diverse solution pathways, many of which might be non-traditional yet highly effective [ 113 ]. Open-source platforms ensure that CADD tools are continually improved. Community-driven tools are updated frequently based on user feedback and the latest scientific advancements [ 114 116 ]. In an age characterized by collaboration and open access, collaborative networks and open-source platforms in CADD emerge as beacons of hope. They underline the belief that in unity lies strength, and in shared knowledge lies the promise of a healthier tomorrow. 10.3. Drawing Lines in the Digital Sand: Navigating the Ethical and Regulatory Labyrinths of Computer-Aided Drug Design In the exhilarating race of drug discovery through CADD, the underlying ethical and regulatory considerations provide crucial checkpoints. Ensuring these digital methodologies hasten drug discovery and preserving the highest ethical standards becomes paramount [ 117 ]. With the increased utilization of patient data in personalized medicine, ensuring data privacy are paramount. Regulations like the General Data Protection Regulation (GDPR) guide the collecting, storing, and processing of personal data in research, imposing stringent data protection requirements [ 118 ]. Defining IP rights can become murky as CADD veers towards more collaborative and open-source models. Balancing between open-access and proprietary claims ensures researchers and institutions obtained due credit [ 119 ]. AI-driven methodologies in CADD can sometimes inherit biases present in their training data. Ensuring that these models are transparent, interpretable, and unbiased becomes essential for ethical drug discovery [ 120 ]. Reproducibility, a cornerstone of scientific rigor, must be confirmed in CADD. Ensuring consistent results across different computational settings is pivotal [ 121 ] with increasingly complex algorithms and models. While CADD can predict potential drug candidates, the transition to in vivo testing, especially on animals, brings its own set of ethical concerns. Regulatory bodies provide guidelines on minimizing animal testing and ensuring humane conditions [ 122 ]. For a drug to reach the market, it is not enough for it to be discovered through CADD; regulatory bodies must accept and validate these methodologies. Collaborations between CADD scientists and regulatory authorities can streamline this acceptance process [ 123 ]. In conclusion, while CADD offers transformative potential in drug discovery, it is essential to navigate the process with ethical integrity and in compliance with existing regulations. As the adage goes, with great power comes great responsibility, and in the realm of CADD, this holds especially true. 10.4. A Glimpse into the Horizon: Envisioning the Next Epoch of Computer-Aided Drug Design The ever-evolving realm of CADD continues to offer promise and innovation. However, as with any cutting-edge field, it is fraught with challenges and uncertainties. Looking forward, it is essential to pinpoint potential trajectories and hurdles that might shape the next generation of drug discovery [ 124 ]. As we stand on the brink of a quantum revolution, the potential for quantum computers to optimize molecular simulations and improve drug design methodologies is immense. They promise speed and precision previously deemed unattainable [ 104 ]. The continued evolution of AI promises more sophisticated drug discovery models. Deep learning models that can simulate protein folding or predict drug-target interactions with increased accuracy are on the horizon [ 125 ]. With advancements in genomics, proteomics, and metabolomics, integrating this vast and varied data into CADD will allow for a more holistic approach to drug design, considering intricate biological systems [ 81 ]. As the volume of biomedical data explodes, standardizing this data to ensure consistency and reliability in CADD methodologies becomes a significant challenge [ 126 ]. The ecological footprint of drug development cannot be ignored. Future CADD models might need to incorporate sustainability metrics, ensuring that drug discovery does not come at an environmental cost [ 127 ]. As AI becomes more prominent in drug discovery, ethical concerns about machine autonomy, transparency in algorithmic decisions, and potential biases become more pronounced [ 128 ]. In essence, the future of CADD is an intricate tapestry of innovation, challenges, and ethical considerations. By preemptively addressing these challenges and harnessing new technologies, CADD can continue revolutionizing drug discovery, ensuring better health outcomes for all.\n\n11.1. Forging Synergy: When the Computational Meets the Experimental in Drug Design As the chasm between experimental biology and computational methodologies in drug design narrows, the symbiosis between these disciplines offers unparalleled potential. While CADD provides the tools to forecast and simulate, experimental data acts as both the foundation and the validator of these predictions [ 129 ]. While CADD can predict a myriad of drug properties, these remain theoretical until experimentally verified. Experimental results offer evidence of drug efficacy, metabolism, and safety, among other characteristics [ 130 ]. Experimental data does not just validate CADD predictions; it also enriches them. This data are invaluable when a predicted molecule does not yield the expected results in the lab. It informs subsequent design iterations, leading to a more refined and likely successful candidate [ 131 ]. Molecular dynamics simulations can predict how molecules will behave over time. Yet, experimental techniques like X-ray crystallography or nuclear magnetic resonance (NMR) provide snapshots of these molecules, which can validate or recalibrate these simulations [ 132 ]. Experimental results from high-throughput screenings, assays, and other methodologies provide a wealth of data. This data can be mined using AI and other CADD tools to uncover patterns, relationships, or potential drug candidates that might be overlooked [ 133 ]. With a growing database of experimental results, the predictive models used in CADD can be trained more effectively. This integration helps continually refine the accuracy of CADD models, making them more reliable over time [ 134 ]. While CADD offers tools to navigate the complex maze of biological systems, real-world experimental data provides the actual map. Together, they offer a more straightforward path to successful drug candidates [ 135 ]. In sum, the confluence of experimental data and CADD are more than just complementary; it is synergistic. Drug discovery becomes more robust, efficient, and accurate by fostering a more intimate relationship between these domains. 11.2. Shaping the Drug Designers of Tomorrow: The Essentiality of CADD in Modern Education The realm of drug discovery, rife with promise, demands cutting-edge technology and well-equipped minds to wield it. As CADD emerges as a linchpin in the drug discovery landscape, it underscores the urgency of integrating CADD training into contemporary education [ 136 ]. While traditional chemistry and biology programs emphasize foundational knowledge, introducing CADD modules can offer students early exposure to the computational aspects of drug design. Such foundational exposure can spark interest and cultivate the next generation of drug discoverers [ 137 ]. Universities worldwide are realizing the importance of specialized courses focusing solely on CADD. These courses amalgamate computational methodologies, biology, and drug pharmacology, producing experts capable of spearheading drug discovery ventures [ 138 ]. The volatile, evolving nature of CADD mandates professionals to be in a perpetual state of learning. Workshops, online courses, and conferences focusing on the latest CADD methodologies are indispensable for professionals to stay abreast of [ 139 ]. Drug design is a symphony of various disciplines. Ensuring that CADD training is not siloed but integrates elements of biology, chemistry, AI, and even ethics is crucial. A holistic, multidisciplinary approach produces well-rounded professionals [ 140 ]. Encouragingly, many institutions offer research opportunities focused on CADD for postgraduates and early-career scientists. These platforms allow hands-on experience, bridging the gap between theory and real-world applications [ 141 ]. The pharmaceutical and biotech industries have a vested interest in the proficiency of CADD professionals. Collaboration between academia and industry can drive curriculum development, ensuring it aligns with the real-world demands of drug discovery [ 142 ]. Conclusively, as the tower of drug discovery leans more on CADD, training proficient individuals becomes paramount. An investment in education is an investment in a healthier, brighter future.\n\nThe rapid progression of CADD, coupled with its integral role in recent drug discoveries, prompts us to ponder the trajectory of this discipline and the challenges it is poised to encounter [ 143 ]. With quantum computers inching closer to practical applications, their potential impact on CADD is enormous. Quantum algorithms can drastically reduce the time required for molecular simulations, thereby accelerating drug discovery manifolds [ 104 ]. While AI and machine learning have already entrenched themselves in CADD, the proliferation of deep learning models promises even more precise predictions. These models, trained on vast datasets, might eventually surpass traditional simulation methods in accuracy [ 144 ]. With advances in biology, previously deemed ‘undruggable’ targets are now within CADDs crosshairs. This shift demands that CADD evolve and devise strategies to engage with these challenging targets [ 145 ]. As CADD and AI models start playing more prominent roles in determining drug viability, ethical questions about trustworthiness, bias in predictions, and accountability will arise. Addressing these concerns will be paramount [ 146 ]. With genomics, proteomics, and metabolomics offering a deluge of biological data, CADDs future lies in efficiently harnessing this data. Integrating multi-omics data can provide a holistic view of biological systems, facilitating better drug design [ 147 ]. Furthermore, in addition to extensive datasets, the execution of numerous simulation processes in AI/ML necessitates high-specification hardware. The computational demands of AI/ML algorithms, such as deep learning models, require robust hardware configurations with powerful processors, ample memory, and efficient GPUs to handle complex computations effectively. Access to high-performance hardware is crucial for optimizing the training and inference phases of AI/ML systems, ensuring the timely and accurate processing of tasks. As collaborative efforts become more common, ensuring the privacy and security of shared data becomes critical. Developing protocols and standards for data sharing without compromising data security will be pivotal [ 148 ]. The environmental footprint of drug discovery, especially with energy-intensive computational methods, cannot be ignored. Future CADD methodologies must be sustainable, considering drug efficacy and environmental impact [ 149 ]. In essence, while the future of CADD radiates promise, it is not without its challenges. Navigating this labyrinth will necessitate a fusion of technological prowess, ethical considerations, and a commitment to sustainable practices.\n\nPersonalized medicine, often interchangeable with precision medicine, seeks to customize healthcare by tailoring decisions and practices to the individual patient. Integrating CADD with personalized medicine stands to revolutionize treatment paradigms [ 157 ]. The completion of the Human Genome Project has provided a detailed genetic blueprint. Leveraging this information, CADD can help design drugs targeting specific genetic mutations or variants associated with diseases [ 158 ]. By integrating genetic, epigenetic, and proteomic data, CADD tools can forecast a patient’s likely response to a drug. This facilitates the administration of therapies that are most likely efficacious while minimizing adverse effects [ 159 ]. In some rare diseases caused by particular genetic mutations, CADD offers the possibility of creating drugs tailored for individual patients, an approach that would be the pinnacle of personalized medicine. Biomarkers are vital in personalized medicine, providing measurable indicators of disease states. CADD aids in the discovery of drugs that can modulate these biomarkers, leading to personalized therapeutic solutions. As electronic health records become more prevalent, integrating this real-world data with CADD models can provide insights into drug performance in diverse populations, allowing for more individualized therapy recommendations. The prospects of personalized medicine via CADD are exciting, but they come with ethical dilemmas, especially regarding data privacy and potential inequalities in access to tailored treatments. In sum, CADDs intersection with personalized medicine promises treatments optimized for each patient, transcending the one-size-fits-all approach. A new era of healthcare beckons by harnessing the power of computational tools in sync with individual data. Often called theranostics, this approach leverages CADD to develop drugs alongside diagnostic tests that determine a patient’s suitability for the treatment. This ensures the right drug reaches the right patient at the right time. Personalized medicine is greatly enhanced by patient-derived models like organoids or patient-derived xenografts. CADD can use data from these models to simulate drug responses, allowing individualized therapy adjustments [ 160 ]. Cancer epitomizes the need for personalized medicine, given the heterogeneity in tumors, even within the same cancer type. CADD tools can analyze tumor genomic data to identify druggable targets unique to each patient’s cancer profile [ 161 ]. As wearable technology becomes increasingly sophisticated, capturing diverse health metrics and integrating this data with CADD models can fine-tune drug recommendations based on real-time patient status [ 162 ]. While the prospects of CADD-driven personalized medicine are revolutionary, the associated costs are a concern. Ensuring these tailored treatments are economically viable and accessible to all, regardless of socio-economic status, is a pressing challenge [ 163 ]. Integrating CADD with personalized medicine could redefine treatment regimens, ensuring patients receive interventions tailored to their unique genetic and physiological profiles. But as with all transformative advances, balancing innovation with ethics, accessibility, and cost remains pivotal.\n\nIn conclusion, Computer-Aided Drug Design is a transformative catalyst in modern drug discovery, poised at the intersection of biological intricacies and computational prowess. The journey from historical breakthroughs to the contemporary landscape underscores its pivotal role in expediting drug development. However, as CADD charts its future trajectory, challenges emerge, necessitating continual optimization, ethical considerations, and the integration of diverse biological data. Success stories exemplify the tangible impact of CADD on clinical applications, while the infusion of Machine Learning augments predictive capabilities, unveiling new frontiers. Collaborative networks and global initiatives democratize drug discovery, emphasizing the strength of unity. The convergence of personalized medicine offers tailored solutions, albeit with ethical and accessibility challenges. Looking ahead, quantum computing, immersive technologies, and green chemistry promise a paradigm shift demanding a delicate balance between innovation and ethical responsibility. Collaborative platforms and open-source initiatives serve as beacons of hope, emphasizing shared knowledge in a global context. Ethical and regulatory considerations are pivotal in guiding CADDs responsible evolution, especially as it converges with emerging technologies and navigates the complexities of the digital era. The symbiosis of experimental data and CADD enriches drug discovery, highlighting the synergistic relationship between computational predictions and real-world validations. In education, the integration of CADD training becomes essential for shaping proficient individuals capable of navigating the multidisciplinary landscape of drug discovery. As CADD anticipates accuracy, bias mitigation, and sustainability challenges, proactive measures must be taken to ensure responsible and compliant use. In essence, the trajectory of CADD is a journey of innovation, challenges, and ethical considerations, paving the way for a future where drug discovery is faster, more precise, and more patient-centric, ultimately contributing to a healthier tomorrow."
    },
    {
        "link": "https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/66301b6f21291e5d1d0b5e1a/original/molecular-docking-in-drug-discovery-techniques-applications-and-advancements.pdf",
        "document": ""
    },
    {
        "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10392679",
        "document": "The field of drug discovery and design is one of the most vibrant areas of research, with many groups from academic and industrial settings contributing to its advancement. The identification of small molecules binding to protein or RNA targets, and the related structural and functional information is of fundamental importance to modern drug discovery. One way to identify small molecule binders is high throughput screening (HTS), where millions of compounds are tested for the desired biological activity (Martis et al., 2011). However, HTS is time-consuming and expensive (DiMasi et al., 2003; Workman, 2003; Hopkins, 2009). An alternative to HTS is computer-aided drug design (CADD) or virtual HTS (Leelananda and Lindert, 2016). CADD provides the capability of screening millions of compounds virtually. This significantly reduces the number of molecules that need to be tested biochemically. Therefore, this approach can cut cost and accelerate the preliminary stage of drug development. CADD has been successfully applied to numerous disease-related target systems advancing research in treatments for congestive heart failure (Lindert et al., 2015a; Cai et al., 2016; Aprahamian et al., 2017; Coldren et al., 2020), several types of cancer (Lee et al., 2010; Ravindranathan et al., 2010; Chuang et al., 2015; Lee et al., 2019; Fratev et al., 2021; Hantz and Lindert, 2022), and various infectious diseases (Durrant et al., 2011; Zhu et al., 2013; Sinko et al., 2014; Lindert et al., 2015b; de Sousa et al., 2020). In practice, commonly utilised CADD approaches may vary depending on the availability and type of experimental information. The two main methods being employed are structure-based drug design (SBDD) and ligand-based drug design (LBDD). In the structure-based approach, the 3D structures of targets are known. They are generally elucidated using techniques such as X-Ray crystallography, nuclear magnetic resonance spectroscopy and cryogenic electron microscopy and can be further supplemented with molecular dynamic simulations (Salsbury, 2010), computational structure prediction (Dorn et al., 2014; Seffernick and Lindert, 2020) and design (Huang et al., 2016) methods. These structures are then used for generating or screening potential small molecule ligands by predicting interactions with the target. On the other hand, ligand-based approaches are utilised when the 3D structure of the target is unknown. However, LBDD relies on the existence of a large set of ligands whose potency against a biological target of interest is known. Using such information, a correlation between the structures and properties of known ligands and their experimentally determined biological activities can be derived. This structure–activity relationship may in turn be used to design new drugs. Over the last decade, there has been a rapid shift in the field of drug development aimed at improving CADD approaches using a variety of machine learning (ML) techniques (Cao et al., 2018). Recently, others have reviewed the general role of artificial intelligence throughout the drug discovery pipeline (Hessler and Baringhaus, 2018; Patel et al., 2020; Carracedo-Reboredo et al., 2021; Gallego et al., 2021; Paul et al., 2021; Dara et al., 2022). Notably, in this work, we provide an overview of recent applications of deep learning (DL) in CADD methods, with a specific focus on SBDD.\n\nDe novo drug design refers to the exploration of a broad chemical space and creation of new chemical compounds without the need for a starting template and was first introduced by Danziger and Dean (1989). The chemical search space for identifying novel compounds is virtually infinite and sufficiently sampling this space is a primary challenge in de novo drug design. In order to resolve this issue, a set of restraints is typically incorporated. For instance, the physical and chemical properties of a target’s active site are key constraints for ligand design. Structural and chemical information plays an immense role in reducing the search space to prevent the sampling of generations of compounds that might be synthetically unobtainable. Two sampling methods, atom-based and fragment-based, are primarily used for compound generation in de novo drug design (Gillet et al., 1993; Wong et al., 2011; Schneider and Fechner, 2022). In atom-based sampling (Fig. 1a), an initial atom is used as a seed inside the active site of the target. From this seed, a diverse set of compounds are grown by varying the number of atoms and hybridization states of each atom (Nishibata and Itai, 1991). Compounds generated from atom-based sampling have high structural diversity. However, the computational cost in narrowing down the lead compound with the atom-based method can increase exponentially with the size of the compound being designed (Hartenfeller and Schneider, 2011). Alternatively, fragment-based sampling (Fig. 1b) circumvents this issue by exploring a database of fragments. These fragments are then used as a seed in the active site to build the rest of the compound. This approach significantly narrows the chemical search space while maintaining good structural diversity (Böhm, 1992; Gillet et al., 1993; Pearlman and Murcko, 1996; Clark et al., 2022). Illustration of computational de novo drug design. (a) In atom-based drug design, the small molecule is built atom by atom by sampling additions of many different types of atoms. (b) In fragment-based drug design, the small molecule is built by sampling additions of a library of fragments. DL and deep reinforcement learning techniques have been used in numerous cases to improve the performance of de novo drug design. Common DL architectures utilised in de novo drug design include recurrent neural network (RNN), graph neural network (GNN), and graph convolutional neural network (GCNN). RNNs make use of sequential data or time series data and are known for their internal memory which takes information from prior inputs to influence the current input and output. In contrast, traditional deep neural networks assume that inputs and outputs are independent of one another (Dupond, 2019). RNNs have been recently expanded with long short-term memory (LSTM) networks. Introduced by Hochreiter and Schmidhuber (1997), LSTM are used as the building blocks for the layers of a RNN and enable RNNs to remember inputs over a long period of time. This memory is commonly referred to as a gated cell, meaning the cell decides whether to store or delete information based on the importance or weight it assigns to the information. GNNs are a class of artificial neural networks used for processing data that can be depicted as graph structures. GNNs operate on an information diffusion mechanism, meaning a graph is processed by a set of units which are linked according to the graph connectivity. The units exchange information in a pairwise fashion until a stable equilibrium is reached (Scarselli et al., 2009). This behaviour is similar to an RNN as weights are shared in each recurrent step. In contrast to RNNs and GNNs, GCNNs do not share weights between hidden layers. GCNNs generalise classical convolutional neural networks (CNNs) to graph-structured data. In this representation, structural information can be incorporated to model connections among entities and create further insights in the data. GCNNs can utilise the graph structure and aggregate node information from the neighbourhoods in a convolutional fashion (Zhang et al., 2019). Deep reinforcement learning (DRL) combines artificial neural networks with a framework of reinforcement learning (RL) that helps agents (i.e. the algorithm making decisions/actions) learn how to reach their goals. RL considers the problem of an agent learning to make decision based on trial-and-error from rewards or punishments. DRL incorporates DL by allowing the agent to make decisions from unstructured data or domain heuristics. We will review several notable recent examples of these architectures below. Jeon and Kim (2020) created an atom-based de novo method to design novel compounds named Molecule Optimization by Reinforcement Learning and Docking (MORLD). MORLD is a deep generative model that uses binding affinities, obtained from docking simulations, as rewards in the reinforcement learning. In MORLD, a compound is optimised by Molecule Deep Q-Networks (MolDQN) (Zhou et al., 2019), based on reinforcement learning and chemistry domain knowledge. Compounds are optimised in single action steps, meaning either an addition or removal of an atom or bond in a chemically valid manner. This is controlled by the user as atom types for modification need to be specified. The modified molecule is then evaluated with different score functions such as the synthetic accessibility (SA) (Ertl and Schuffenhauer, 2009) and quantitative estimate of drug-likeness (QED) (Bickerton et al., 2012). The SA score is a metric that is used to estimate the ease of synthesis of drug-like molecules. SA score ranges from 1 (easy to synthesise) to 10 (difficult to synthesise). QED provides a quantitative metric for assessing druglikeness of target (and/or predicted) compounds. This value ranges between 0 and 1. A compound QED score of 0 indicates that all properties of that compound are unfavourable and vice-versa. These scores are weighted and used to guide compound modification with MolDQN in the next state. This process is repeated until the compound reaches a terminal state. In the terminal state, the modified compound is docked to the target with QuickVina2 (Alhossary et al., 2015). The resulting docking score is given to MolDQN as a reward and a compound is generated. This is repeated until a specified number of training episodes are reached. MORLD was shown to significantly decrease the time needed to design a novel inhibitor for the target discoidin domain receptor 1 kinase (DDR1) compared to the deep generative model GENTRL (Zhavoronkov et al., 2019). GENTRL took approximately 46 days (with training data) to successfully design a novel inhibitor against DDR1, whereas MORLD achieved the same result in less than 2 days without any training data. MORLD was also successfully able to generate agonists for D dopamine receptor (D4DR) without any initial lead compound information. MORLD can be used in situations where the lead compound is available, or when the lead compound must be identified from virtual screening or from scratch. MORLD has a free web server and source code open to the public. While MORLD is a successful tool for drug design, it has a few limitations: (a) the MORLD algorithm is trained on Q-value. Q-value is the expected future rewards, that is a p-value that has been adjusted for the false discovery rate. For example, a q-value of 2% means that 2% of significant results will result in false positives. This limits the diversification of optimised compounds, (b) as an atom-based model it is unable to explore a large chemical space, and (c) the SA and QED scores are not perfect. Therefore, generated compounds are sometimes chemically inaccurate. Additionally, MORLD is also not a suitable method for drug design when the target protein is disordered or does not have a druggable binding pocket. While a relatively new program, MORLD has been cited in many studies to acknowledge the usefulness of DRL in the field of drug discovery. To date, this algorithm has been applied to only the DDR1 system as mentioned previously. Ma et al. (2021) created the Structure-Based de Novo Molecular Generator (SBMolGen), another DL method for de novo drug design based on Monte Carlo tree search (MCTS) (Browne et al., 2012a, 2012b) and docking simulations. Molecular generation in SBMolGen is done by ChemTS (Yang et al., 2017), and rDock (Ruiz-Carmona et al., 2014) is employed for docking generated compounds to the target. In the SBMolGen methodology, ChemTS first generates compounds with MCTS and a recurrent neural network (RNN) (Kaur and Mohta, 2019). These generated models are then docked using rDock and evaluated. Based on the top scoring model from the docking simulation, SBMolGen reweights the search tree in ChemTS. Additional molecules are then generated by ChemTS with the new weights. By repeating this process with a RNN, SBMolGen is able to generate compounds that are aware of the target it is binding to. The RNN model in SBMolGen was trained on 250,000 molecules from the ZINC database (Irwin et al., 2012). The results of SBMolGen were benchmarked with four target proteins: cyclin-dependent kinase 2 (CDK2), epidermal growth factor receptor erbB1 (EGFR), adenosine A2A receptor (AA2AR), and beta-2 adrenergic receptor (ADRB2). For each target, the generated molecules were evaluated against known actives from the Database of Useful Decoys: Enhanced (DUD-E) (Mysinger et al., 2012). Molecules generated with SBMolGen covered a larger chemical space than the ZINC data set, had an SA score of less than 3.5 and generally many of these compounds had QED scores greater than 0.8. Furthermore, SBMolGen was also able to find compounds that had a better docking score (as compared to known compounds) for all four target proteins. Fragment molecular orbital (FMO) (Kitaura et al., 1999) analysis of the target protein and generated molecules suggested that SBMolGen designed molecules with high binding affinity. Benchmark results of SBMolGen were compared against GENTRL and showed that SBMolGen was able to produce molecules that had better docking scores. While SBMolGen is faster than GENTRL, it is still limited by computation time. In addition, docking scores of SBMolGen were based on SA score, Lipinski’s rule of five (Lipinski et al., 2001) and PubChem (Kim et al., 2021) filters. While these are useful filters, comprehensive drug discovery still requires the optimization of a large number of molecular properties (Winter et al., 2019). SBMolGen has been employed as benchmark in several other method development studies. Li et al. (2021b) created DeepLigBuilder, another atom-based DL method for de novo drug design. The deep generative model in DeepLigBuilder is able to design and optimise the 3D structures of ligands directly inside the binding pocket of the target. This method combines a graph convolutional neural network (L-Net) and MCTS, a common technique in reinforcement learning. L-Net iteratively generates and refines molecules by using a state encoder and policy network. During this stage, the state encoder analyses the state of the molecules and passes it to the policy network. The policy network then decides the type, bond order, coordinates, and the number of new atoms to be added. This combination allows the DeepLigBuilder to generate molecules with high binding affinity within the binding pocket. Benchmarking results indicated that L-Net had an overall validity of 96% compared to G-SchNet (Gebauer et al., 2022) which had an output validity of only 77%. The average RMSD of the generated compounds to optimised structures of the generated models with MMFF94 (Halgren, 1996) was only about 0.61 Å. The QED score was above 0.5 for 83.9% of the generated compounds and it was possible to correctly predict the overall distribution of molecular properties. L-Net was also able to re-create the chemical space of the training set almost identically. While L-Net’s performance was highly remarkable, it struggled to generate ring structures as observed by the high standard deviations for bond lengths and torsion angles for aromatic rings. DeepLigBuilder was also tested to generate 3D structures of compounds (as well as lead optimization of known inhibitors) inside the binding pocket of SARS-CoV-2 main protease. The MCTS (as opposed to random search) in DeepLigBuilder successfully lowered the smina scores (Koes et al., 2013) during lead optimization. Furthermore, DeepLigBuilder was able to generate compounds for SARS-CoV-2 main protease with novel chemical structures and high predicted binding affinity and had a success rate of 78.1%. DeepLigBuilder has been cited in several other studies for its use in generating lead compounds for SARS-CoV-2 as discussed above. However, to date, this algorithm has not been applied to other systems. DeepScaffold is another DL tool for scaffold-based (can also be thought of as fragment-based) drug discovery created by Li et al. (2020). DeepScaffold is able to design compounds from cyclic skeletons, classical molecular scaffolds, and pharmacophore queries. DeepScaffold uses three different methods to enable these different inputs. When the input is a cyclic skeleton, DeepScaffold uses a graph convolutional neural network and variational autoencoder (VAE) (Kingma and Welling, 2014) to complete atom and bond types and generate a scaffold. However, if the input is a molecular scaffold, then the program uses a generative model (Li et al., 2018), from the authors’ previous study, to build the side chains. DeepScaffold can also filter out generated molecules that do not match the input pharmacophore query. DeepScaffold was trained on a dataset that contained 914,464 molecules from the ChEMBL databases with QED larger than 0.5. DeepScaffold was then evaluated for chemical validity and diversity, distribution of molecular properties and side chains. The performance of DeepScaffold to produce chemically valid and unique scaffolds was remarkable with an average of 98.9 and 69.1%, respectively. Molecular properties (molecular weight, solubility, and QED) of the test set molecules against the generated molecules indicate good correlation for scaffolds with more atoms. The model also demonstrated a diversified substitution pattern of side chains and was biased towards adding more atoms to nitrogen atoms. Finally, DeepScaffold model was tested on GPCRs. The model was able to produce chemically valid molecules, however, performed rather poorly to produce unique molecules with increasing structural complexity of scaffolds. The model was also able to reproduce known actives for the GPCRs reasonably well. DeepScaffold has been mentioned in several studies. This algorithm has not yet been applied to other target systems. However, it was referenced for successful scaffold hopping during compound design (Zhang et al., 2022). Additionally, DeepScaffold has one of the highest percentages for validity of generated compounds as compared to other scaffold-based de novo methods (Joshi et al., 2021). MolAICal, created by Bai et al. (2021), is another fragment-based drug design algorithm. The two modules in MolAICal are trained with a DL model on fragments from the food and drug administrations (FDA) approved drugs and drug-like ligands from the ZINC database. MolAICal can directly generate 3D structures of drug-like compounds in the protein pockets. It employs a sequence-based generative model and a GNN for fragment generation and the fragment is then grown stochastically. A total of 21,064 fragments of FDA-approved drugs and 1,060,000 drug-like ligands from the ZINC database were used to train the generative model. When the grown ligand reaches a certain length, a genetic algorithm is employed to optimise the ligand into the protein pocket based on the ligand-receptor binding score. The docking score used by MolAICal is the Vinardo score (Quiroga and Villarreal, 2016) from AutoDock Vina (Trott and Olson, 2010). The Vinardo score is a summation of several energetic terms. These are Gaussian steric attractions, quadratic steric repulsions, Lennard-Jones potentials, electrostatic interactions, hydrophobic interactions, non-hydrophobic interactions, and non-directional hydrogen bonds. Vinardo scores are reported as the Gibbs free energy (ΔG) of binding in the unit of kcal mol−1. Therefore lower Vinardo scores are more favourable. However, in MolAICal, the coefficients of the Vinardo score were optimised based on a set of 2,903 protein-ligand complexes with experimental data from the PDBbind database (Wang et al., 2005). MolAICal was then tested to design drugs for glucagon receptor (GCGR) and SARS-CoV-2 main protease. For both the GCGR and the SARS-CoV-2 main protease, MolAICal was able to generate compounds that were diversified and had good theoretical binding affinity. In a recent study, MolAICal was employed to calculate the molecular synthetic accessibility score of traditional Chinese medicines in attempts to inhibit the SARS-CoV-2 nucleocapsid protein (Chen et al., 2022). In another application, MolAICal was utilised to calculate the relative binding energy of compounds from the ZINC database to inhibit carbonic anhydrase as a treatment for altitude sickness (Ali et al., 2022). Finally, Arús-Pous and co-authors describe a fragment-based method that employs DL architecture to generate scaffolds based on SMILES string (Arús-Pous et al., 2020). In this drug-design architecture, a scaffold is generated through an RNN, that is composed of three LSTM cells (Hochreiter and Schmidhuber, 1997), from a SMILES string. Next, the scaffold is ‘decorated’ with side-chains at each attachment point in the scaffold exhaustively. The generated molecules are then filtered based on drug-like properties such as molecular weight, QED, SA, and/or user-specified properties. The decorator model was trained on a set of 4,211 Dopamine Receptor D2 active modulators. Following the training phase, the authors evaluated their generative model and the model was able to produce a diverse set of scaffolds and obtain a large amount of actives. The architecture implemented by Arús-Pous and co-authors is extremely versatile and can be combined with molecular generative architectures. This work by Arús-Pous and co-authors has been highlighted primarily in reviews of artificial intelligence in drug design. A brief summary of all the previously mentioned methods, along with the respective links to the freely accessible code or webserver, can be found in Table 1. The methods mentioned here capture many of the key areas of how DL has influenced de novo drug design. Nonetheless, this is still an active area of research with novel applications of DL algorithms continuously being developed.\n\nLigand binding sites in proteins (Fig. 2) encode necessary chemical and structural information for the successful execution of SBDD. Correct identification of these binding sites may help in the understanding of protein function and aid in the design of better drug-like small molecules. While these functional sites can be experimentally determined, the time and cost required to perform such experiments are quite significant. In contrast, computational prediction of protein-ligand binding sites is relatively inexpensive and significantly faster, thus expediting the drug development process. Several computational methods have been proposed over the years for structure-based binding site prediction (Huang, 2009; Le Guilloux et al., 2009; Yu et al., 2010; Tsujikawa et al., 2016; Dias et al., 2017; Wu et al., 2018). These methods can be roughly categorised as geometry-based, energy-based, and templated-based (Mylonas et al., 2021). Geometry-based methods analyse the geometry of the molecular surface of a target protein in order to locate surface cavities. Common approaches to find surface cavities within geometry-based methods include grid system scanning (Weisel et al., 2007), probe sphere filling (Yu et al., 2010) and alpha spheres (Le Guilloux et al., 2009). Energy-based binding site prediction methods involve searching for energetically favourable interactions between protein atoms and a chemical probe with the aid of a force field (Kozakov et al., 2015; Tsujikawa et al., 2016). Schrodinger’s SiteMap (Halgren, 2007, 2009) program combines geometry and energy-based properties to identify potential protein–ligand and protein–protein binding sites. Finally, template-based binding site prediction relies on the assumption that proteins that share structural similarity to the target protein also share functional similarity (Dey et al., 2013). One of the first ML-based binding site prediction algorithms was CryptoSite (Cimermancic et al., 2016). CryptoSite uses as support vector machine (SVM) to classify residues belonging to a binding site on a score range of 0 (not likely)–1 (most likely). Efforts in DL-based binding site prediction methods are fairly new and date back to as early as 2017. Illustration of computational binding site prediction. In this methodology, 3D voxels are used to identify regions of the protein as potential binding sites (shown as yellow rectangles in the figure). Next, these sites are ranked from most to least probable for a ligand to bind. Convolutional neural networks (CNNs) are a popular DL architecture for identifying potential binding sites. CNNs are regularised versions of fully connected networks, meaning each neuron in one layer is connected to all neurons in the next layer (Albawi et al., 2017). In a 2D-CNN, the kernel or filter used to extract features from images moves in two directions (x and y) and the input/output data is three-dimensional. 2D-CNNs are typically used on image data. A region-based CNN (R-CNN), is a type of 2D-CNN that implements a selective search algorithm that generates region proposals that are then provided to the CNN architecture to compute the features (Girshick et al., 2014). Region proposals are small regions of the image that potentially contain objects of interest in the input image. The benefit of this method is that the algorithm generates approximately 2,000 region proposals which greatly reduces the computational cost compared to computing CNN features for the entire input image. In a 3D-CNN, the kernel moves in three directions (x, y, and z) and the input/output of the model are four-dimensional. These algorithms are typically used on three-dimensional image data (Hedegaard et al., 2022). A popular 3D-CNN architecture is U-Net (Ronneberger et al., 2015) which has been shown to outperform traditional sliding window convolutional networks. The model provides many advantages over patch-based segmentation approaches as it preserves the full context of the input image through an end-to-end pipeline process for the entire image and requires few training samples. One increasingly popular CNN architecture is ResNet (He et al., 2016). ResNet overcomes the vanishing gradient problem, which occurs when the neural network training algorithm attempts to find weights that bring the loss function to a minimal value. If the network has too many layers, the gradient becomes increasingly small until it disappears and optimization cannot continue. ResNet overcomes this problem by creating a deep residual learning framework. This framework consists of shortcut connections that add intermediate inputs to the output of a group of convolution blocks. The shortcut connections perform identity mapping, allowing for smoother gradient flow and ensures important features are carried until the final layers. We review several notable recent examples of these architectures below. Jiménez et al. (2017) developed the first DL-based protein-ligand binding site prediction algorithm, DeepSite. Based on a 4-layer convolutional neural network (DCNN) architecture, DeepSite treats the target protein as an image on a 3D grid. Each atom within the voxels of the grid is defined by atom-based physicochemical properties obtained from AutoDock 4 (Morris et al., 2009). Next, the entire grid is divided into subgrids in order to defined the local properties of smaller protein areas. These subgrids are then scored by the DCNN as potential sites for ligand binding. The DCNN architecture in DeepSite was trained on 7,622 proteins from the sc-PDB (Desaphy et al., 2015) database. Pocket prediction performance was evaluated by distance and shape of the predicted binding site to that of the real binding site. To accomplish this, the authors make use of two metrics, distance to centre of binding site (DCC) and discretized volumetric overlap (DVO). These metrics were also used to benchmark DeepSite against Fpocket (Le Guilloux et al., 2009) and Concavity (Capra et al., 2009) (binding site prediction algorithms). Based on the binding site analysis metrics, DeepSite was able to provide a more accurate binding site prediction compared to Fpocket and Concavity. Further analysis of DeepSite on proteins with diverse folds in the SCOPe database showed no bias towards any specific protein fold. However, there is still room for improvement as DeepSite does not have descriptors for atom-based polarizability effects or descriptors for water molecules. Since its release, DeepSite has been a highly successful tool and has been referenced in several studies. A few notable applications are its use to study the binding process and interaction of antibiotic drugs and lysozymes (Rial et al., 2022), analysis of the potential interaction mechanism of sweeteners with aroma compounds in passion fruit juice (Xiao et al., 2022), and identifying binding pockets for drug candidates for Hepatocellular carcinoma (predominant subtype of liver cancer) (Tang et al., 2022). FRSite, introduced by Jiang et al. (2019), is another binding site prediction method based on a region-based convolutional neural network (R-CNN), an object-detection DL architecture (Ren et al., 2015). In this methodology, the target protein is treated as a 3D image and the binding site is treated as the object within the image to be detected. Similar to DeepSite, FRSite also treats each atom in the target protein on a grid with eight descriptors from high-throughput molecular dynamics (HTMD) (Doerr et al., 2016). Next, the entire grid is fed into a region proposal network-3D (RPN-3D) module in FRSite to detect potential binding sites. Once these potential binding sites are found by RPN-3D, FRSite uses its classifier to score the binding site. FRSite was trained on the same training dataset as DeepSite. Following the training, FRSite was evaluated for DCC and DVO as well as compared to those of other prediction methods. Compared to Fpocket and DeepSite, FRSite outperformed the other algorithms in predicting more accurate binding sites of proteins. Additionally, FRSite is also able to predict the receptor grid of the binding site (which can be used to estimate the size of the pocket). The authors further showed that the DVO estimated by FRSite was more accurate than that of Fpocket. FRSite has been referenced in several review articles and as benchmark in other method development studies. Stepniewska-Dziubinska et al. (2020) introduced another DL-based binding site prediction method called Kalasanty. Similar to DeepSite, Kalasanty also treats the target protein as a 3D image. However, Kalasanty uses a DL model that is more similar to U-Net (Ronneberger et al., 2015) to treat the problem as a 3D image segmentation problem. In this approach, each featurized segment (by atomic properties) of the protein is assigned a certain probability of being a part of a pocket where a small molecule can dock. The DL model in Kalasanty was trained on 15,860 structures with binding site data from sc-PDB dataset. Prediction results from Kalasanty were then evaluated for correct DCC and DVO as well compared to those from DeepSite. Kalasanty correctly predicted binding sites two times better than DeepSite. Furthermore, Kalasanty had DCC values lower (i.e. closer to centre of the actual binding site) than DeepSite. Additionally, the binding sites predicted by DeepSite were bigger (lower DVO) and not accurately modelled when compared to that of Kalasanty (higher DVO). While Kalasanty is a successful binding site prediction model, the dataset used to train the DL model is based on proteins with deep cavities. Therefore this model may not be suitable for cases where the binding sites are located on flat surfaces. While being a newly released method, Kalasanty has already been subjected to many benchmarks in other method development studies. In addition, relevance of Kalasanty in enzyme engineering has been discussed in this review (Singh et al., 2021). Following the works of DeepSite (Jiménez et al., 2017), FRSite (Jiang et al., 2019), and Kalasanty (Stepniewska-Dziubinska et al., 2020), Mylonas et al. (2021) developed DeepSurf. DeepSurf uses a variant (Dimou et al., 2019) of the DL network architecture known as residual network (ResNet) (He et al., 2016). The authors propose an 18-layer ResNet to which the surface representation of the target protein is passed in the form of a 3D grid with physiochemical features. The network then predicts a binding site score for the surface presenting points. These points are then clustered and ranked to predict the binding site. The network was trained on a set of 15,182 structures from the sc-PDB database. The distance between the predicted and the real binding site centre and other metrics were used to evaluate DeepSurf. Furthermore, DeepSurf was benchmarked against DL-based methods such as DeepSite, FRSite and Kalasanty as well non-ML methods. The authors showed that compared to DeepSurf, Kalasanty was unable to predict binding sites for a large number of proteins. Interestingly, DeepSite with its shallow neural network layer performed similarly to DeepSurf. While the non-ML methods such as Fpocket (geometry-based) and AutoSite (energy-based) (Ravindranath and Sanner, 2016) performed poorly, COFACTOR (template-based) (Roy et al., 2012) performed similarly when compared to DeepSurf. However, all ML methods generally tend to predict a smaller number of binding sites that are closer to the actual binding sites compared to non-ML methods. DeepSurf has been mentioned in several other studies. In most of these cases, DeepSurf was used as benchmark for other method development and/or other reviews. Kandel et al. (2021) developed PUResNet where the protein structure is also treated as a 3D image and the binding site as the object within the image. PUResNet employs variants of two DL architectures, ResNet and U-Net, to address this issue. PUResNet was trained on a set of 5,020 protein structures from the sc-PDB database, and evaluated with DCC, DVO and proportion of ligand inside binding pocket (PLI). The authors also benchmarked their method against Kalasanty, developed by Stepniewska-Dziubinska et al., with a k-fold cross-validation and accuracy test. Briefly, k-fold cross-validation is a method employed to estimate the predictive power of the model on new data. From the benchmarking results, it was observed that PUResNet exhibited slightly better performance during the cross-validation and accuracy test than Kalasanty, despite the fact that PUResNet was trained with only a fraction (1/3) of the dataset used to train Kalasanty. PUResNet has been referenced in several other method development studies as well as other review articles, including a review about pharmacological chaperones for rare diseases (Scafuri et al., 2022). Along with primary binding site prediction, allosteric site prediction has also been an active area of study (Goncearenco et al., 2013; Panjkovich and Daura, 2014; Greener and Sternberg, 2015; Song et al., 2017; Tian et al., 2021). The term allosteric binding site refers to a site other than a protein’s primary (orthosteric) active site where a compound can bind, resulting in conformational and dynamic changes (Srinivasan et al., 2014). The incorporation of these dynamic changes is critical to accurately predict allosteric sites. The main difference between orthosteric and allosteric ligand binding site prediction algorithms is the inclusion of protein dynamics. Additionally, the algorithms may differ based on the dataset that the DL model was trained on, as the quality and robustness of the data significantly impacts the algorithms’ features and results. Here we review BiteNet (Kozlovskii and Popov, 2020), a DL-based method for detecting allosteric sites of target proteins from their dynamic ensembles. Similar to other site prediction methods, this work is also inspired by the applications of DL in computer vision (Islam et al., 2016). In BiteNet, the input structure of the target protein is placed on a grid and processed with a neural network composed of 10 3D convolutional layers. BiteNet then outputs the centre of the predicted allosteric binding sites, assigns scores and identifies the neighbouring residues within 6 Å radius. BiteNet captures protein dynamics through an ensemble of protein structures gathered from molecular dynamic simulations. When the input is an ensemble of protein structures, BiteNet employs three different clustering algorithms to group the predicted binding site centres and their neighbouring residues and ranks the clusters from most to least probable. The convolutional neural network in BiteNet was trained on 5,946 structures from the PDB. To test for efficiency and accuracy, the authors tested BiteNet’s predictive power on the P2X3 receptor of the ATP-gated cation channel family, the epidermal growth factor receptor of the kinase family, and the adenosine A2A receptor of the G-protein coupled receptor family. In all three cases, it was observed that BiteNet was able to detect conformation-specific allosteric binding sites. Furthermore, BiteNet was benchmarked against FPocket (Le Guilloux et al., 2009), MetaPocket (Huang, 2009), DeepSite (Jiménez et al., 2017), SiteHound (Hernandez et al., 2009) and P2Rank (Krivák and Hoksza, 2018) for precision and computational speed on an independent benchmark dataset. In terms of speed and precision, BiteNet outperformed all other methods. BiteNet has been referenced in several other method development studies. From this summary, it is evident that DL-based methods for binding site prediction have continuously outperformed traditional methods. A summary of the DL-based binding site prediction methods reviewed in this article is presented in Table 2. While these methods are highly successful, they are also limited. For instance, the dataset used to train most of these methods ignores noncanonical amino acids and heavy metals. Thus, these models are still not generalised enough to encompass the astronomical complexities in drug discovery and there is still a huge potential for improvement in the future.\n\nIn SBDD, an accurate prediction of protein-ligand binding affinity can facilitate the discovery of novel drug-like compounds (Fig. 3). In order to achieve such a prediction, a small molecule is typically docked to the target protein (Fig. 3a) with a molecular docking program. Then a score function (SF) is employed to predict the protein-ligand binding affinity (Fig. 3b). Such SFs (if accurate and fast) allow researchers to screen a large number of compounds to identify potential drug-like small molecules. Over the years, several SFs have been developed to approximate the interaction energy between protein and ligand (Jones et al., 1997; Jain, 2003; Friesner et al., 2004; Shivakumar et al., 2010; Trott and Olson, 2010; Wang et al., 2015). These can be classified into physics-based, empirical, and knowledge-based SFs. Physics-based methods include scoring functions based on force fields, solvation models and quantum mechanics (QM) methods. In physics-based SFs, interactions between the protein and the ligand are calculated based on terms such as van der Waals interaction, electrostatic interaction, torsion entropy, solvent effects and others. Due to the limited accuracy of these terms, QM methods hybridised with molecular mechanics (QM-MM) methods have also been utilised to address covalent interactions, polarisation and charge transfer processes (Hayik et al., 2010). While QM-MM methods are more accurate, they are also computationally expensive and impractical for screening large number of compounds. Empirical SFs estimate binding affinity by making simplistic approximations to the physics-based energy terms such as hydrogen bonds, hydrophobic effects, steric clashes and other important interactions in a protein-ligand system (Böhm, 1994). These terms are usually associated with weights that are optimised using experimental binding affinity data. Due to the simplicity of the energy terms, empirical score functions are typically less computationally expensive. Knowledge-based SFs are derived based on a statistical analysis of protein-ligand complexes from available 3D structures. In knowledge-based SFs, the frequency of different atom pairs occurring within different distances is assumed to play a key role in the interaction of the atoms. These frequencies are then converted to a distance-dependent potential of mean force (Muegge, 2006). Knowledge-based SFs are also computationally inexpensive and have similar accuracies compared to physics-based methods. However, since knowledge-based SFs do not directly account for experimental binding affinity data, discrepancies between predicted and experimental binding affinity are frequently observed (Stahl and Rarey, 2001). Recently, DL-based SFs have also been developed to predict binding affinity and have been shown to outperform traditional SF methods. These models are usually trained on a large dataset with known protein-ligand binding affinity. In DL-based SFs, feautrizing the protein-ligand system plays a key role in its ability to correctly predict the binding affinity. Here, we briefly summarise some of the DL-based methods that have been successful in predicting protein-ligand binding affinity. Illustration of computational binding affinity prediction. (a) Small molecule is docked into a target protein. (b) The binding site and the small molecule are then characterised with many features in order to predict the binding affinity. The atoms in the small molecule are shown in grey, blue and red for carbon, nitrogen and oxygen, respectively. The carbon, nitrogen, oxygen and sulphur of the binding site residues are shown in blue, purple, red and yellow, respectively. Bonds in the small molecule ligand are shown in black. CNNs also serve as popular architectures in binding affinity prediction. Recent examples include variants of traditional 3D-CNNs and ResNet algorithms. Additional CNN-based architectures developed to solve binding affinity prediction are SqueezeNet (Iandola et al., 2016) and ShuffleNet (Zhang et al., 2018). SqueezeNet is a convolution neural network that utilises a design strategy aimed at reducing the number of parameters. Fewer parameters offer several advantages such as: more efficient distributed training, smaller memory storage requirements, and less communication required for over-the-air model updates. SqueezeNet accomplishes its smaller size via Fire modules which replace traditional 3 × 3 filters (kernels) with 1 × 1 filters. ShuffleNet is a CNN architecture that was designed for mobile devices with very limited computing power. In order to reduce computational cost and maintain accuracy, ShuffleNet employs pointwise group convolution and channel shuffle. Grouped convolution makes use of a multiple kernels (filters) per layer, resulting in multiple channel output per layer. This technique led to wider networks helping learn low- and high-level features (Xie et al., 2017), and it has been shown to improve classification accuracy (Xie et al., 2017). Channel shuffle is an operation that helps information flow across feature channels. We review several notable recent examples of these architectures below. Jiménez et al. (2018) developed K a DL-based method that relies on 3D-convolutional neural network (3D-CNN), to predict protein-ligand binding affinity. The DL architecture in K is designed based on the image classification DL architecture SqueezeNet (Iandola et al., 2016). In K , residues of the target protein at the binding site and the ligand are placed on a grid. Next, each atom of these interacting residues and the ligand are featurized by eight pharmacophoric-like properties. These descriptors were used to train the K network on a training dataset that was composed of 3,767 protein-ligand complexes from the PDBbind (Wang et al., 2005) database. Once the network was trained, K was benchmarked against RF-Score (Ballester and Mitchell, 2010), X-Score (Wang et al., 2002), and CyScore (Cao and Li, 2014) (three other non-DL-based binding affinity prediction methods) for performance and accuracy. Briefly, RF-score is a random forest model trained on a large dataset of molecular interactions. A low RF-score generally corresponds to more favourable interactions. X-Score is an empirical score function based on terms such as van der Waals interactions, hydrogen bonding, deformation penalties, and hydrophobic effects. X-Score values are reported as the Gibbs free energy (ΔG) of binding in the unit of kcal mol−1. Therefore a lower value is indicative of more energetically favourable interactions. Cyscore is another empirical score function that has been optimised based on hydrophobic free energy, van der Waals interaction energy, hydrogen-bond energy and the ligand’s entropy. Cyscore values are comparable to the ΔG (kcal mol−1) of binding. Therefore, a lower Cyscore indicates a higher binding affinity. Compared to other binding affinity prediction methods, K exhibited the lowest root mean square error (RMSE) with respect to the predicted and experimental affinity. The Pearson’s correlation coefficient of K was higher than that of X-Score and CyScore but was very similar to that of RF-Score. Furthermore, when these methods were benchmarked against independent datasets that the network had not seen more, K underperformed compared to RF-Score. Binding affinity predictions from K are reported as the ΔG (kcal mol−1) of binding. K is a highly successful program and has been used as a tool in numerous studies. A few examples where K has been successfully utilised are: to engineer maltooligosaccharide-forming amylases and optimise the biosynthesis of maltooligosaccharides (Ding et al., 2022), to determine the inhibitory properties of antiviral drugs and their structural analogues towards coronavirus (Kalamatianos, 2022), during virtual screening (of lead compounds and analogs) to chemically inhibit processes controlled by RecA protein of Acinetobacter baumannii (Tiwari, 2022) and so forth. While K is a versatile SF for binding affinity predictions, the authors argued that additional exploration of the network architecture and a thorough featurization of the molecular descriptor could further improve the SF’s performance. Pafnucy (Stepniewska-Dziubinska et al., 2018), developed by Stepniewska-Dzuibinska et. al, is another DL model for predicting binding affinity of protein-ligand complexes. Similar to K , Pafnucy’s 3D-CNN also treats the ligand and interacting residues of the target protein as a 3D image. This information is then used to create a 3D grid and each atom in the grid is featurized with 19 atomic properties. The network was trained on experimental dissociation constant values of 11,906 protein-ligand complexes (from the PDBbind database) and then evaluated for its performance using RMSE, mean absolute error (MAE), Pearson correlation coefficient, and standard deviation in regression (SD). Therefore, the predicted values from Pafnucy are comparable to experimental dissociation constants. Furthermore, the prediction power of Pafnucy was benchmarked against binding affinity prediction methods such as X-Score, RF-Score, ChemScore, PLP1, ChemPLP (Verdonk et al., 2003), and G-Score (SYBYL 2022). Briefly, ChemScore is an empirical score function based on simple contact terms, a simplistic model for hydrogen bonds, and a penalty function based on flexibility. PLP1 is also an empirical score function that uses terms to account for hydrogen bonds, van der Waals interactions, repulsion potentials, and piecewise linear potentials to model the steric complementarity between protein and ligand. ChemPLP is another empirical score function that is very similar to PLP1. However, in addition to the terms in PLP1, ChemPLP also considers the distance and angle-dependent hydrogen and metal bonding terms from ChemScore. G-Score is a force-field based score function. It is composed of a protein-ligand complexation term (optimised Lenard-Jones potential), a hydrogen bonding term (approximated) and an internal energy term (approximated). All score values from ChemScore, PLP1, ChemPLP, and G-score are reported as the ΔG (kcal mol−1) of binding, therefore lower scores are considered as more favourable interactions. Pafnucy has been successfully used to study the interaction of the mannose receptor of macrophages with carbohydrate ligands (Zlotnikov and Kudryashova, 2022), as well as has been used as benchmark to develop several newer methods. Pafnucy outperformed all other methods except for RF-Score v3 (Li et al., 2015). However, the observed difference in performance was negligible. Additionally, Pafnucy outperformed all 20 binding affinity prediction methods that were used in the CASF-2013 (Li et al., 2014) benchmark. Besides binding affinity prediction, the authors of Pafnucy argue that their model may also be helpful to guide ligand optimization during molecular docking. Methods that employ DL techniques usually require a large amount of data in order for the method to be properly trained. Thus, limited data could pose a challenge. DeepAtom, a DL method for binding affinity prediction of protein-ligand complexes, developed by Li et al. (2019) addresses this concern with a novel 3D-CNN architecture. The authors of DeepAtom developed an efficient 3D-CNN architecture based on ShuffleNet (Zhang et al., 2018). Briefly, ShuffleNet is an extremely computation-efficient CNN architecture that is designed for cases with limited computational resources. Using this architecture allowed DeepAtom’s network to be designed with fewer trainable parameters, deep layers, and limited training data. Similar to other methods, DeepAtom also treats the protein-ligand complex as a 3D image. In DeepAtom the interacting residues of the target protein (defined from the centre of the ligand) as well as the ligand are voxelized in a grid. The atoms occupying the voxels are feauturized with 12 atomic properties. To evaluate DeepAtom, the authors first trained DeepAtom, Pafnucy, and RF-Score with 3,390 complexes from the PDBbind database. These models were then evaluated with Pearson correlation coefficient, MAE, RMSE and SD. DeepAtom showed significant improvement over Pafnucy and slight improvement over RF-Score in all metrics. Next, the authors re-trained DeepAtom, Pafnucy and RF-Score on a larger training dataset (9,383 complexes) and evaluated all three models for performance. DeepAtom performed significantly better than both Pafnucy and RF-Score. Since the networks in DeepAtom and Pafnucy are based on 3D-CNN, it was observed that a well-designed network architecture (DeepAtom) can yield better predictions with limited data suggesting that effective training in the deep layers results in improvements in the capacity for learning and generalisation. DeepAtom has been employed in several method development studies (as benchmark) and other review articles. One key factor of a successful DL scoring function is being able to train the model on features that are important to protein-ligand binding affinity. While the methods discussed so far focused on features that localise to the ligand binding site, OnionNet, developed by Zheng et al. (2019), takes a different approach. The authors treat the protein-ligand complex as a 2D image with one colour channel and use a 2D-CNN architecture to train the binding affinity prediction model. OnionNet is able to capture both local and non-local interactions between the protein and ligand by grouping contacts between each atom of the ligand to atoms in the protein in a set of 60 distance groups. In this way, OnionNet uses 3,840 features to define local and non-local interaction between the target protein and ligand. The network was trained on 11,906 protein-ligand complexes from the PDBbind database. Therefore, predicted values from OnionNet are comparable to experimental dissociation constants. Following the training, the prediction model was evaluated for accuracy and also compared to other ML and non-ML binding affinity prediction methods. In terms of RMSE and Pearson correlation coefficient, OnionNet outperformed Pafnucy and RF-Score. While K performed slightly better than OnionNet, OnionNet significantly outperformed other non-ML SFs used in the CASF-2013 benchmark. The authors further showed that both the local and non-local interactions contribute significantly to the prediction capability of the model. OnionNet has been utilised as a benchmark during the development of several binding affinity prediction methods. AK-Score developed by Kwon et al. (2020) is yet another binding affinity prediction method that was developed based on the ResNet (He et al., 2016) architecture. Similar to other methods, the protein-ligand system is represented as a 3D image on a grid, with each atom on the grid being defined with eight atomic properties. However, the network architecture uses an ensemble-based approach to perform the binding affinity prediction. AK-Score utilises 20 independently trained networks to make predictions and the average of those values is considered as the final prediction. AK-Scores are reported as the ΔG (kcal mol−1) of binding, therefore a low score corresponds to a more energetically favourable interaction. The networks were trained on 3,772 protein-ligand complexes from the PDBbind database. AK-Score was then evaluated on the CASF-2016 benchmark dataset with Pearson correlation coefficient, Spearman correlation coefficient, Kendall tau, predictive index metrics, MAE, and RSME. AK-Score with ensemble-based networks was benchmarked against AK-Score with a single network and K . These results showed that the AK-Score ensemble network model had a lower MAE and RMSE as well as higher Pearson correlation coefficient, Spearman correlation coefficient, Kendall tau and predictive index as compared to other methods. Furthermore, from feature analysis, it was shown that the excluded volume of atoms, spatial distribution of hydrophobic and aromatic atoms of the ligand and protein, as well as the distribution of hydrogen bond acceptors of the protein were particularly important for the network to accurately determine the binding affinity. Therefore, more accurate calculations of such descriptors may play an important role in improving the ensemble-based network of AK-Score. AK-Score has been reviewed in several reviews and its prediction quality has been compared in other method development studies. Similar to AK-Score, RosENet developed by Hassan-Harrirou et al. (2020), also uses an ensemble-based 3D-CNN. However, RosENet uses a hybrid approach where it employs the 3D-CNN to combine molecular mechanics energies calculated from the Rosetta force field (Alford et al., 2017) with molecular descriptors. Briefly, the Rosetta software suite includes algorithms for computational modelling and analysis of protein structures and has enabled scientific advances in areas such as protein design, enzyme design, molecular docking and structure prediction (Leman et al., 2020). The network architecture of RosENet was developed based on the ResNet architecture. Similar to previous methods, protein-ligand complexes were represented in a 3D grid. Atoms of the protein-ligand complex were featurized with 12 molecular descriptors from HTMD (Doerr et al., 2016) and 6 molecular energies obtained from Rosetta. The network was then trained on a curated dataset of 4,463 protein-ligand complexes obtained from the PDBbind database. Therefore, the predicted values from RosENet are comparable to experimental dissociation constants. To evaluate RosENet with respect to the quality of its predictions, the network was first optimised based on a combination of multiple features and architectures, and then compared to Pafnucy and OnionNet. The authors tested two neural network architectures: ResNet (a deep residual neural network) and SqueezeNet (a smaller network architecture). RosENet showed a lower RMSE with the ResNet architecture than it did with the SqueezeNet architecture. Thus, the authors argue that the network implemented in the SqueezeNet architecture was not deep enough to generalise the molecular mechanics energies that were used to featurize the complex. RosENet was also trained with a smaller number of features, leading to a decrease in the prediction quality, however, this change was considered to be insignificant. Overall, RosENet performed better than Pafnucy and comparably to OnionNet. This suggested that the molecular energies described in RosENet may be able to represent similar local and non-local interactions described by the OnionNet. Finally, the RMSE of RosENet decreased when the average of the five best predictions was used. In future studies, the authors of RosENet propose to improve their molecular features by extracting additional dynamic properties from molecular dynamics simulations. RosENet has been featured in other method development studies and as well as review articles. Accurate prediction of binding affinity is a key step during the drug discovery process. Despite several limitations, recently developed DL-based methods for binding affinity prediction have generally significantly outperformed traditional methods. A summary of such DL-based methods as reviewed in this article can be found in Table 3. These studies indicate the availability of successful in silico tools for rapid and accurate prediction of protein-ligand binding affinity. Additionally, a few ML-based methods (Aggarwal and Koes, 2020; Bao et al., 2021) have also been trained to predict ‘the root mean square deviation (RMSD)’ of a docked structure with reference to the native bound structure. Deep Binding Structure RMSD Prediction model (DeepBSP) (Bao et al., 2021) is one such method that employs a 3D-CNN (same model architecture as K ) to achieve this purpose. DeepBSP exhibited accuracy on predicting RMSD. Furthermore, the docking power of DeepBSP (through model selection with lowest predicted RMSD), outperformed all other scoring functions benchmarked in CASF-2016. While these methods have been successful, this field continues to grow and evolve with new advancements in ML.\n\nOver recent years, ML has become an increasingly popular field of study. Application of these methods to biological problems will only continue to grow as academic and industry users create better tools to predict biomolecular structure, treat disease, and improve public health. In this work, we summarised recent DL applications in three structure-based drug discovery categories: de novo drug design, binding site prediction and binding affinity prediction. In the interest of the reader, we also provide links to the various methods’ publicly available source code or webservers. The use of ML (more recently DL) methods is not limited solely to SBDD, and they have also been applied to all other areas of the drug discovery pipeline such as LBDD (Bahi and Batouche, 2018), lead optimization (de Souza Neto et al., 2020), and assessment of absorption (Shin et al., 2018), metabolism (Wang et al., 2020; Litsa et al., 2021), binding kinetics (De Benedetti and Fanelli, 2018; Mardt et al., 2018; Gao et al., 2019; Nunes-Alves et al., 2020; Feizpour et al., 2021; Obeid et al., 2021; Hoffmann et al., 2022), efficacy (Lin et al., 2018; Benning et al., 2020; Li et al., 2021a; Zhu et al., 2021) and toxicity properties (Ferreira and Andricopulo, 2019; Liu et al., 2019; Shi et al., 2019; Cáceres et al., 2020; Feinberg et al., 2020). While these studies are not covered in this review, interested readers are encouraged to review these recent works as well. With the expansion of large datasets, DL algorithms will only become more accurate, thereby increasing cost-effectiveness and time efficiency. However, a major critique of DL methods is their inherit ‘black-box’ nature. In order for the use of large datasets to be truly successful, we must begin to understand the nature of DL algorithms and the connections they make. Another common pitfall of any ML algorithm is the quality of its training dataset(s). In any biological application, an algorithm may easily become hyper-specific to a certain motif if it is not exposed to an unbiased training set. This lack of generalisation prevents the algorithm from solving real-world problems. Recently the company DeepMind and the Baker research group have released AlphaFold (Jumper et al., 2021) and RoseTTAFold (Baek et al., 2021), DL algorithms for protein structure prediction. These algorithms have been credited with ‘solving the protein folding problem’. Given the impressive prediction capabilities of these tools, it is natural to investigate how relevant such DL-based protein conformations are for drug design. For example, recently the crystal structure of the σ receptor has been reported (Alon et al., 2021). In this study, the authors also screened a large library of compounds to the X-ray structure (through molecular docking), which resulted in the identification of around 130 active compounds. However, the molecular docking of these same hits scored relatively poorly against the AlphaFold model. This suggests that additional efforts are needed to employ predicted structures successfully in drug discovery. Within the last year, the CEO of DeepMind announced the plan of applying their knowledge of artificial intelligence to drug discovery with the formation of a new company called Isomorphic Labs (https://www.isomorphiclabs.com). This year another company, CHARM Therapeutics (https://charmtx.com), in the UK have launched DragonFold. DragonFold is the first DL algorithm to rapidly predict protein-ligand co-folding. While we wait on more details of these developments, they however foreshadow a very promising future for the field of computer-aided drug discovery, as the brightest minds from industry and academia continue to collaborate and improve upon ML applications. While the future of the field appears very promising, we must also keep in mind the ethics of good science. As demonstrated by Urbina et al. (2022), artificial intelligence was utilised to predict the toxicity of small molecules. The algorithm developed by the Collaborations team predicted many compounds with higher toxicity as compared to the nerve agent VX. The team calls for a strengthening of ethical training of students, as well as for artificial intelligence drug discovery companies to create a code of conduct to properly train employees and establish protection measures for their technology."
    },
    {
        "link": "https://frontiersin.org/journals/drug-discovery/articles/10.3389/fddsv.2022.829043/full",
        "document": "94% of researchers rate our articles as excellent or good\n\nLearn more about the work of our research integrity team to safeguard the quality of each article we publish."
    },
    {
        "link": "https://nature.com/articles/s41586-023-05905-z",
        "document": ""
    },
    {
        "link": "https://sciencedirect.com/science/article/pii/S1359644624001508",
        "document": ""
    },
    {
        "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11894378",
        "document": "The world population is growing. The United Nations estimates it will reach almost 10 billion by 2050 and 11 billion by the end of the century when the population curve will reach its maximum (United Nations Department of Economic and Social Affairs, 2018; Rosling et al., 2018). The increase in size will not come from the rise in the population of young people but from the rise in the population of adults and the elderly. This shift in demographics will highlight existing population health problems and bring new health-related challenges to the fore, with age-dependent cancer, neurodegenerative, cardiovascular, and chronic diseases continuing to be the dominant health issues. To address these problems, new drug development strategies will need to be devised that benefit both patients-by prolonging their comparable quality of life and reducing healthcare costs-and pharmaceutical developers by introducing more efficient, rapid, and cost-effective drug discovery and development techniques.\n\nDrug discovery is a long and arduous process with a high risk of failure (Wong et al., 2019, Dowden and Munro, 2019). It takes more than a decade and more than a billion dollars to bring a single drug to market (which means that the total cost to pharma companies is even larger when accounting for failed drugs). The chance of a compound entering the preclinical stage and eventually being FDA-approved has been 1 in 20,000 to 30,000 over the last couple of decades (Yamaguchi et al., 2021). The cost and complexity of drug research have led major pharmaceutical companies to decrease their involvement in certain disease categories, such as cardiovascular and neurological diseases (Dowden and Munro, 2019), or to abandon early research and rely on acquisitions of smaller biotech companies that have drugs in preclinical or early clinical stages of development.\n\nAll these challenges have forced the pharmaceutical industry to accept in silico methods as a means of reducing costs and expediting development. Classical tools, such as molecular dynamics (MD), although offering a high level of accuracy and detailed insights into the behavior of proteins, are too expensive for high-throughput studies and are thus used only for evaluating targets and a small number of compounds. Those limitations opened a space for applying machine learning (ML) in drug development. While it has been used in academia for decades, with occasional excursions in the industry, ML came into the spotlight in recent years with the advancements in large language models (LLMs) and denoising diffusion probabilistic models and their use in computational structural biology. The successes of the AlphaFold and RosettaFold models, along with the subsequent Nobel Prize award to Demis Hassabis and John M. Jumper for protein fold prediction and to David Baker for computational protein design, led many to believe that the majority of structural biology and, relatedly, drug design problems would be easily solved. Those were high hopes because ML, although powerful, has limitations. One of the most significant limitations of ML models is their poor generalization outside of training space, making them strongly dependent on the compositions of the training set. An additional issue is, paradoxically, the simplicity with which it is now possible to implement ML models. Modern, advanced ML libraries (PyTorch and TensorFlow) enable the easy deployment of ML models, often without delving into the details of biological phenomena being analyzed. This can lead to a superficial understanding of the results obtained with an ML model. Furthermore, the “black-box” nature of ML models often creates challenges for their adaptation in medical applications, and drug discovery. To bridge the gap between computational power and complex biological systems, interpretable models are needed.\n\nWith all this in mind, we conceptualize this Research Topic with the idea of presenting research that utilizes ML protocols/architectures but offers a detailed and comprehensive interpretation of observed phenomena.\n\nThe first paper in this Research Topic, by Chen et al., deals with the detection of peptides that can bind major histocompatibility complex (MHC) class-I proteins. The authors designed two Convolutional Neural Network-based methods, ConvM and SpConvM, to tackle the binding prediction problem and conducted a thorough bioinformatics study of the results. They show that their method outperforms the current state-of-the-art, allele-specific method in prioritizing and identifying the most likely binding peptides.\n\nHuang et al. addressed the detection of hydration sites in proteins and the prediction of water molecule positions using ML. This is an important issue in drug design as the analysis conducted prior indicates that the majority of ligand binding sites in protein-ligand structures contain at least one bridging water molecule at the interface. The authors’ two-component (scoring and sampling) model outperformed alternative approaches by a large margin.\n\nThe next paper also deals with peptide classification. Khabaz et al., developed a hierarchical machine-learning model for classifying peptides with antimicrobial activity against S. aureus. Their two-level model first classifies peptides into Anti-Microbial Peptides (AMPs) and non-AMPs. The second level then classifies AMPs as active and inactive against S. aureus. The model uses linguistic and physicochemical properties, which were selected through cross-validation-based feature selection to identify the most important features. The model can be used in drug discovery, peptide design, and functional annotation of peptides.\n\nFaris et al., developed a method for discovering selective inhibitors against JAK1 and JAK3. The method uses QSAR models optimized with multiple linear regression and artificial neural networks (ANN). It enabled the identification of optimal compounds exhibiting both favorable affinity and stability during a 100 ns molecular dynamics trajectory. This approach, developed with the help of ANNs, has demonstrated its capability to predict biological activity and stability.\n\nChomicz et al., used clustering and machine learning protocols to develop a method for antibody grouping using clonotype, sequence, paratope prediction, structure prediction, and embedding information. The authors used advanced methods for fast sequence clustering and language models to cluster paratopes. For structure clustering, they applied an adaptation of AlphaFold2 to model antibodies and a fast greedy algorithm-based tool for similarity estimation. The last layer in their architecture is a self-supervised embedding-based language model. They use it to cluster antibody sequences in the latent space. Their results indicate that novel, ML-based methods offer no advantage over standard sequence-based tools for probe-based binder mining. However, they noticed that the advanced ML methods are useful for epitope binning. Thus the authors conclude that advanced methods are better suited for separating a given dataset, rather than to perform data-mining experiments.\n\nAhmadi et al. developed a machine-learning protocol that uses pharmacophore features to separate true binding ligands from decoys for four protein targets. They first used molecular dynamics simulation to generate pharmacophore feature sets from protein-ligand complex conformations. Then, they applied AI/ML algorithms to reduce the whole set of those features to a much smaller set. They showed that this protocol is effective for true binder prediction while remaining medicinal-chemistry friendly.\n\nThe papers published in this Research Topic focus on leveraging machine learning to analyze biological models, predict molecular behaviors, and aid in drug discovery. They incorporate ML into diverse applications, such as peptide-MHC binding prediction, protein-ligand interaction prediction, antimicrobial peptide classification, and antibody clustering. While also demonstrating how these protocols can identify both small-molecule and antibody binders, providing meaningful biological insights."
    }
]