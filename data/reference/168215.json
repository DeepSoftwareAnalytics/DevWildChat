[
    {
        "link": "https://mathjs.org/docs/expressions/parsing.html",
        "document": "Expressions can be parsed and evaluated in various ways:\n• By creating a parser, , which contains a method and keeps a scope with assigned variables in memory.\n\nMath.js comes with a function to evaluate expressions. Syntax:\n\nFunction accepts a single expression or an array with expressions as the first argument and has an optional second argument containing a with variables and functions. The scope can be a regular JavaScript (recommended), a plain JavaScript , or any custom class that implements the interface with methods , , and . The scope will be used to resolve symbols, and to write assigned variables and functions.\n\nWhen an is used as scope, mathjs will internally wrap it in an interface since the internal functions can only use a interface. In case of custom defined functions like , the scope will be wrapped in a , which reads and writes the function variables (like in this example) from a temporary map, and reads and writes other variables from the original scope. The original scope is never copied, it is only wrapped around when needed.\n\nThe following code demonstrates how to evaluate expressions.\n\nMath.js contains a function which compiles expressions into JavaScript code. This is a shortcut for first parsing and then compiling an expression. The syntax is:\n\nFunction accepts a single expression or an array with expressions as the argument. Function returns an object with a function , which can be executed to evaluate the expression against an (optional) scope:\n\nAn expression needs to be compiled only once, after which the expression can be evaluated repeatedly and against different scopes. The optional scope is used to resolve symbols and to write assigned variables or functions. Parameter can be a regular Object, or Map.\n\nMath.js contains a function to parse expressions into an expression tree. The syntax is:\n\nFunction accepts a single expression or an array with expressions as the argument. Function returns the root node of the tree, which can be successively compiled and evaluated:\n\nThe API of nodes is described in detail on the page Expression trees.\n\nAn expression needs to be parsed and compiled only once, after which the expression can be evaluated repeatedly. On evaluation, an optional scope can be provided, which is used to resolve symbols and to write assigned variables or functions. Parameter is a regular Object or Map.\n\nParsed expressions can be exported to text using , and can be exported to LaTeX using . The LaTeX export can be used to pretty print an expression in the browser with a library like MathJax. Example usage:\n\nIn addition to the static functions and , math.js contains a parser with functions and , which automatically keeps a scope with assigned variables in memory. The parser also contains some convenience functions to get, set, and remove variables from memory.\n\nA parser can be created by:\n\nThe parser contains the following functions:\n• Evaluate an expression. Returns the result of the expression.\n• Retrieve a variable or function from the parser’s scope.\n• Retrieve an object with all defined variables in the parser’s scope.\n• Retrieve a map with all defined variables in the parser’s scope.\n• Remove a variable or function from the parser’s scope.\n• Set a variable or function in the parser’s scope.\n\nThe following code shows how to create and use a parser.\n\nThe scope is a data-structure used to store and lookup variables and functions defined and used by expressions.\n\nIt is passed to mathjs via calls to or .\n\nFor ease of use, it can be a Plain Javascript Object; for safety it can be a plain and for flexibility, any object that has the methods / / / , seen on .\n\nSome care is taken to mutate the same object that is passed into mathjs, so they can collect the definitions from mathjs scripts and expressions.\n\nwill fail if the expression uses a blacklisted symbol, preventing mathjs expressions to escape into Javascript. This is enforced by access to the scope.\n\nFor less reliance on this blacklist, scope can also be a , which allows mathjs expressions to define variables and functions of any name.\n\nFor more, see examples of custom scopes."
    },
    {
        "link": "https://30secondsofcode.org/js/s/math-expression-parser",
        "document": "In previous articles, we've explored how to tokenize math expressions and parsing using Abstract Syntax Trees. This time around, we're revisiting math expressions, to build a robust parsing and evaluation system combining the two techniques, and more.\n\nA context-free grammar (CFG) is a set of rules that define the structure of a language. It consists of a set of terminal symbols, non-terminal symbols, a start symbol, and a set of production rules. The production rules define how the non-terminal symbols can be replaced by a sequence of terminal and non-terminal symbols.\n\nLet's look at an example of a simple CFG, representing basic arithmetic expressions:\n\nThese simple rules allow us to generate expressions like . The start symbol is replaced by , which can be replaced by or . The process continues until we reach terminal symbols like , which is any numeric value, in this example. Rules can also be written like to indicate that can be replaced by .\n\nThe Earley parsing algorithm is a general context-free parsing algorithm, that uses dynamic programming to parse strings according to a CFG. It's a top-down, predictive parser that can handle any CFG, including ambiguous grammars.\n\nThe steps of the algorithm are as follows:\n• Initialization: Create a set of states, each representing a possible parsing state. The initial state is created with the start symbol at position 0. This is the starting point of the parsing process.\n• Prediction: For each state, predict possible next states based on the current state. This is done by looking at the next symbol in the production rule and creating a new state with the dot moved one position to the right.\n• Scanning: For each state, scan the next token in the input string. If the token matches the next symbol in the production rule, create a new state with the dot moved one position to the right.\n• Completion: For each state, complete the state by adding the state that predicted it. This is done by looking at the states that predicted the current state and checking if the dot is at the end of the production rule. If it is, create a new state with the dot moved one position to the right.\n\nInstead of going over the specifics of each step in detail, which I'm sure you can find in many other resources (Wikipedia has a terrific lemma on the topic, that I based the below example on), we'll take a look at the example of parsing the expression . Note that represents the current position in the parsing process. If it's at the end of a production, it means that the production is complete.\n\nNow that we have a basic understanding of the Earley parsing algorithm, let's implement a parser for mathematical expressions in JavaScript. We'll use the algorithm to parse the input string and build an abstract syntax tree (AST) representing the expression.\n\nRules in a CFG are comprised of a left-hand side (LHS) and a right-hand side (RHS). The LHS is a non-terminal symbol, and the RHS is a sequence of terminal and non-terminal symbols. To facilitate this, we'll define a class and a class to represent symbols and rules, respectively.\n\nHaving these two classes, we can now define symbols and rules, like so:\n\nNotice how literal tokens like and need to be defined as their own symbols and are represented as strings. Similarly, numeric values are represented as regular expressions. This allows us to tokenize the input string and match tokens against the rules, as we'll see shortly.\n\nGiven a set of rules, we can now define a context-free grammar (CFG) class to hold the rules and provide methods for working with them. We'll extract the rules, symbols and token matchers when creating the CFG to make them easily accessible.\n\nWe can now create a CFG instance with the rules we defined earlier:\n\nHaving extracted the token matchers from the CFG, we can feed them to a tokenizer that will take an input string and produce an array of tokens. We'll then feed these tokens to the Earley parser to build the AST.\n\nWe can now create a tokenizer instance and tokenize an input string:\n\nBefore we can write our parser, we'll have to define an class, similar to the previous article, so that we can store the parsed tokens in a tree-like structure.\n\nWe'll see in a minute how this ties into the Earley parsing algorithm's results.\n\nFinally, we can implement the Earley parser, which will take the tokens produced by the tokenizer and build the AST using the CFG rules. The parser will return the AST root node, which we'll use later to evaluate the expression.\n\nThat's a lot of code, but it's all necessary to implement the Earley parsing algorithm. We can now instantiate the parser and parse the tokens produced by the tokenizer:\n\nWith the AST in hand, we can now evaluate the expression by traversing the tree and performing the necessary operations. We'll add an to the class, so that each rule can be evaluated, if an evaluator function is provided.\n\nWe'll then update the class to include an method that will traverse the tree and evaluate the nodes using the evaluator functions provided by the rules.\n\nFinally, we can update the rules to include evaluators for the arithmetic operations:\n\nWith the updated rules, we can now evaluate the AST:\n\nOur parser is now capable of parsing and evaluating expressions, based on a context-free grammar. Thus, we can write a more complex grammar to handle mathematical expressions with parentheses, negative numbers and even some well-known math functions, such as , , , and .\n\nIn this article, we took a deep dive into the world of context-free grammars and the Earley parsing algorithm. We implemented a parser for mathematical expressions using these concepts, building an abstract syntax tree (AST) that we could evaluate to get the result of the expression. We also took a quick look at how a more complex grammar could be used to parse and evaluate more complex expressions.\n\nI hope you've learned something, because I sure did! See you in the next one! 👋"
    },
    {
        "link": "https://stackoverflow.com/questions/23325832/parse-arithmetic-expression-with-javascript",
        "document": "The expression you are trying to parse into an abstract syntax tree is a context-free expression. This means that you need a context-free grammar to be able to parse it. So let's create a parser.\n\nTo simplify the parsing we'll separate the lexical analysis phase. Hence the first thing we need is to create a lexer. Luckily there are a lot of handy lexer libraries available. We'll use the this one:\n\nNext we create a parser. We'll use the following implementation of Dijkstra's shunting yard algorithm for parsing:\n\nFinally we create a function as follows:\n\nNow you simply call to get a parsed stream of tokens in postfix notation:\n\nThe advantage of postfix form is that you can easily manipulate it using a stack:\n• Pop and and push onto the stack.\n• Pop and and push onto the stack.\n• Pop and and push onto the stack.\n• Pop and and push onto the stack.\n\nSimilarly it's easy to create the output you want using stacks too. It the same steps. You only push different values back onto the stack for different operations.\n\nEdit 1: I was so bored that I solved the problem for you:\n\nThe output is (as you expect) the string . See the demo: http://jsfiddle.net/d2UYZ/4/\n\nEdit 2: If you need to evaluate the expression you could do that easily too. All you need is a context mapping symbols to values and functions for each operator:\n\nThus the expression becomes which evaluates to . See the demo: http://jsfiddle.net/d2UYZ/6/"
    },
    {
        "link": "https://github.com/ariya/tapdigit",
        "document": "TapDigit is a simple JavaScript implementation of a math expression lexer, parser, and evaluator.\n\nsplits a math expression into a sequence of tokens. This is useful for e.g. an expression editor with color syntax highlighting.\n\nparses an expression and produces the JSON-formatted syntax tree representation thereof.\n\ncomputes the result of an expression. Variables, constants, and functions supported in the expression syntax can be extended via object.\n\nThere is also a simple web page (open ) which demonstrates how it works:\n\nTapDigit is created by @AriyaHidayat. It is distributed under the BSD license."
    },
    {
        "link": "https://tomassetti.me/parsing-in-javascript",
        "document": "This is an article similar to a previous one we wrote: Parsing in Java, so the introduction is the same. Skip to chapter 3 if you have already read it.\n\nIf you need to parse a language, or document, from JavaScript there are fundamentally three ways to solve the problem:\n• use an existing library supporting that specific language: for example a library to parse XML\n• building your own custom parser by hand\n• a tool or library to generate a parser: for example ANTLR, that you can use to build parsers for any language\n\nThe first option is the best for well known and supported languages, like XML or HTML. A good library usually include also API to programmatically build and modify documents in that language. This is typically more of what you get from a basic parser. The problem is that such libraries are not so common and they support only the most common languages. In other cases you are out of luck.\n\nBuilding Your Own Custom Parser By Hand\n\nYou may need to pick the second option if you have particular needs. Both in the sense that the language you need to parse cannot be parsed with traditional parser generators, or you have specific requirements that you cannot satisfy using a typical parser generator. For instance, because you need the best possible performance or a deep integration between different components.\n\nIn all other cases the third option should be the default one, because is the one that is most flexible and has the shorter development time. That is why on this article we concentrate on the tools and libraries that correspond to this option.\n\nNote: text in blockquote describing a program comes from the respective documentation\n\nWe are going to see:\n• tools that can generate parsers usable from JavaScript (and possibly from other languages)\n\nTools that can be used to generate the code for a parser are called parser generators or compiler compiler. Libraries that create parsers are known as parser combinators.\n\nParser generators (or parser combinators) are not trivial: you need some time to learn how to use them and not all types of parser generators are suitable for all kinds of languages. That is why we have prepared a list of the best known of them, with a short introduction for each of them. We are also concentrating on one target language: JavaScript. This also means that (usually) the parser itself will be written in JavaScript.\n\nTo list all possible tools and libraries parser for all languages would be kind of interesting, but not that useful. That is because there will be simple too many options and we would all get lost in them. By concentrating on one programming language we can provide an apples-to-apples comparison and help you choose one option for your project.\n\nUseful Things To Know About Parsers\n\nTo make sure that these list is accessible to all programmers we have prepared a short explanation for terms and concepts that you may encounter searching for a parser. We are not trying to give you formal explanations, but practical ones.\n\nA parser is usually composed of two parts: a lexer, also known as scanner or tokenizer, and the proper parser. Not all parsers adopt this two-steps schema: some parsers do not depend on a lexer. They are called scannerless parsers.\n\nA lexer and a parser work in sequence: the lexer scans the input and produces the matching tokens, the parser scans the tokens and produces the parsing result.\n\nLet’s look at the following example and imagine that we are trying to parse a mathematical operation.\n\nThe lexer scans the text and find ‘4’, ‘3’, ‘7’ and then the space ‘ ‘. The job of the lexer is to recognize that the first characters constitute one token of type NUM. Then the lexer finds a ‘+’ symbol, which corresponds to a second token of type PLUS, and lastly it finds another token of type NUM.\n\nThe parser will typically combine the tokens produced by the lexer and group them.\n\nThe definitions used by lexers or parser are called rules or productions. A lexer rule will specify that a sequence of digits correspond to a token of type NUM, while a parser rule will specify that a sequence of tokens of type NUM, PLUS, NUM corresponds to an expression.\n\nScannerless parsers are different because they process directly the original text, instead of processing a list of tokens produced by a lexer.\n\nIt is now typical to find suites that can generate both a lexer and parser. In the past it was instead more common to combine two different tools: one to produce the lexer and one to produce the parser. This was for example the case of the venerable lex & yacc couple: lex produced the lexer, while yacc produced the parser.\n\nThere are two terms that are related and sometimes they are used interchangeably: parse tree and Abstract SyntaxTree (AST).\n\nConceptually they are very similar:\n• they are both trees: there is a root representing the whole piece of code parsed. Then there are smaller subtrees representing portions of code that become smaller until single tokens appear in the tree\n• the difference is the level of abstraction: the parse tree contains all the tokens which appeared in the program and possibly a set of intermediate rules. The AST instead is a polished version of the parse tree where the information that could be derived or is not important to understand the piece of code is removed\n\nIn the AST some information is lost, for instance comments and grouping symbols (parentheses) are not represented. Things like comments are superfluous for a program and grouping symbols are implicitly defined by the structure of the tree.\n\nA parse tree is a representation of the code closer to the concrete syntax. It shows many details of the implementation of the parser. For instance, usually a rule corresponds to the type of a node. A parse tree is usually transformed in an AST by the user, possibly with some help from the parser generator.\n\nA graphical representation of an AST looks like this.\n\nSometimes you may want to start producing a parse tree and then derive from it an AST. This can make sense because the parse tree is easier to produce for the parser (it is a direct representation of the parsing process) but the AST is simpler and easier to process by the following steps. By following steps we mean all the operations that you may want to perform on the tree: code validation, interpretation, compilation, etc..\n\nIn simple terms is a list of rules that define how each construct can be composed. For example, a rule for an if statement could specify that it must starts with the “if” keyword, followed by a left parenthesis, an expression, a right parenthesis and a statement.\n\nA rule could reference other rules or token types. In the example of the if statement, the keyword “if”, the left and the right parenthesis were token types, while expression and statement were references to other rules.\n\nThe most used format to describe grammars is the Backus-Naur Form (BNF), which also has many variants, including the Extended Backus-Naur Form. The Extended variant has the advantage of including a simple way to denote repetitions. A typical rule in a Backus-Naur grammar looks like this:\n\nThe is usually nonterminal, which means that it can be replaced by the group of elements on the right, . The element could contains other nonterminal symbols or terminal ones. Terminal symbols are simply the ones that do not appear as a anywhere in the grammar. A typical example of a terminal symbol is a string of characters, like “class”.\n\nIn the context of parsers an important feature is the support for left-recursive rules. This means that a rule could start with a reference to itself. This reference could be also indirect.\n\nConsider for example arithmetic operations. An addition could be described as two expression(s) separated by the plus (+) symbol, but an expression could also contain other additions.\n\nThis description also match multiple additions like 5 + 4 + 3. That is because it can be interpreted as expression (5) (‘+’) expression(4+3). And then 4 + 3 itself can be divided in its two components.\n\nThe problem is that this kind of rules may not be used with some parser generators. The alternative is a long chain of expressions that takes care also of the precedence of operators.\n\nSome parser generators support direct left-recursive rules, but not indirect one.\n\nWe care mostly about two types of languages that can be parsed with a parser generator: regular languages and context-free languages. We could give you the formal definition according to the Chomsky hierarchy of languages, but it would not be that useful. Let’s look at some practical aspects instead.\n\nA regular language can be defined by a series of regular expressions, while a context-free one need something more. A simple rule of thumb is that if a grammar of a language has recursive elements it is not a regular language. For instance, as we said elsewhere, HTML is not a regular language. In fact, most programming languages are context-free languages.\n\nUsually to a kind of language correspond the same kind of grammar. That is to say there are regular grammars and context-free grammars that corresponds respectively to regular and context-free languages. But to complicate matters, there is a relatively new (created in 2004) kind of grammar, called Parsing Expression Grammar (PEG). These grammars are as powerful as Context-free grammars, but according to their authors they describe programming languages more naturally.\n\nThe Differences Between PEG and CFG\n\nThe main difference between PEG and CFG is that the ordering of choices is meaningful in PEG, but not in CFG. If there are many possible valid ways to parse an input, a CFG will be ambiguous and thus wrong. Instead with PEG the first applicable choice will be chosen, and this automatically solve some ambiguities.\n\nAnother difference is that PEG use scannerless parsers: they do not need a separate lexer, or lexical analysis phase.\n\nTraditionally both PEG and some CFG have been unable to deal with left-recursive rules, but some tools have found workarounds for this. Either by modifying the basic parsing algorithm, or by having the tool automatically rewrite a left-recursive rule in a non recursive way. Either of these ways has downsides: either by making the generated parser less intelligible or by worsen its performance. However, in practical terms, the advantages of easier and quicker development outweigh the drawbacks.\n\nIf you want to know more about the theory of parsing, you should read A Guide to Parsing: Algorithms and Terminology.\n\nThe basic workflow of a parser generator tool is quite simple: you write a grammar that defines the language, or document, and you run the tool to generate a parser usable from your JavaScript code.\n\nThe parser might produce the AST, that you may have to traverse yourself or you can traverse with additional ready-to-use classes, such Listeners or Visitors. Some tools instead offer the chance to embed code inside the grammar to be executed every time the specific rule is matched.\n\nUsually you need a runtime library and/or program to use the generated parser.\n\nLexer was (it has now been officially archived) a lexer that claimed to be modelled after flex. Though a fairer description would be a short lexer based upon . The documentation seems minimal, with just a few examples, but the whole thing is 147 lines of code, so it is actually comprehensive.\n\nHowever, in a few lines manages to support a few interesting things and it appears to be quite popular and easy to use. One thing is its supports RingoJS, a JavaScript platform on top of the JVM. Another one is the integration with Jison, the Bison clone in JavaScript. If you temper your expectations it can be a useful tool.\n\nThere is no grammar, you just use a function to define the RegExp pattern and the action that should be executed when the pattern is matched. So, it is a cross between a lexer generator and a lexer combinator. You cannot combine different lexer functions, like in a lexer combinator, but the lexer it is only created dynamically at runtime, so it is not a proper lexer generator either.\n\nIt has been abandoned, but the fact that is so small, means that you can take on maintenance with ease, if you found it useful.\n\nLet’s see the tools that generate Context Free parsers.\n\nANTLR is a great parser generator written in Java that can also generate parsers for JavaScript and many other languages. ANTLR is based on an new LL algorithm developed by the author and described in this paper: Adaptive LL(*) Parsing: The Power of Dynamic Analysis (PDF).\n\nIt is quite popular for its many useful features: for instance, version 4 supports direct left-recursive rules. However a real added value of a vast community it is the large amount of grammars available.\n\nIt provides two ways to walk the AST, instead of embedding actions in the grammar: visitors and listeners. The first one is suited when you have to manipulate or interact with the elements of the tree, while the second is useful when you just have to do something when a rule is matched.\n\nThe typical grammar is divided in two parts: lexer rules and parser rules. The division is implicit, since all the rules starting with an uppercase letter are lexer rules, while the ones starting with a lowercase letter are parser rules. Alternatively, lexer and parser grammars can be defined in separate files.\n\nIf you are interested to learn how to use ANTLR, you can look into this giant ANTLR tutorial we have written. If you are ready to become a professional ANTLR developer, you can buy our video course to Build professional parsers and languages using ANTLR. The course is taught using Python, but the source code is also available in JavaScript.\n\nAPG is a recursive-descent parser using a variation of Augmented BNF, that they call Superset Augmented BNF. ABNF is a particular variant of BNF designed to better support bidirectional communications protocol. APG also support additional operators, like syntactic predicates and custom user defined matching functions.\n\nIt can generate parsers in C/C++, Java and JavaScript. Support for the last language seems superior and more up to date: it has a few more features and it is more recently updated. In fact, the documentation says it is designed to have the look and feel of JavaScript RegExp. There is also a separate Python version that is developed on its own.\n\nAn APG grammar is very clean and easy to understand.\n\nDespite the name Jison can also replace flex, so you do not need a separate lexer. Although you can use one or build your own custom lexer. All you need is an object with the functions and . It can best recognize languages described by LALR(1) grammars, though it also has modes for LR(0), SLR(1).\n\nThe generated parser does not require a runtime component, you can use it as a standalone software.\n\nIt has a good enough documentation with a few examples and even a section to try your grammars online. Although at times it relies on the Bison manual to cover the lack of its own documentation.\n\nA Jison grammar can be inputted using a custom JSON format or a Bison-styled one. Both requires you to use embedded actions if you want to do something when a rule is matched. The following example is in the custom JSON format.\n\nIt is very popular and used by many project including CoffeeScript and Handlebars.js.\n\nThe Earley algorithm is designed to easily handle all grammars, including left-recursive and ambiguous ones. Essentially its main advantage it is that it should never catastrophically fail. On the other hand, it could be slower than other parsing algorithms. Nearly itself also is able to detect some ambiguous grammars. It can also reports multiple results in the case of an ambiguous input.\n\nA Nearley grammar is a written in a file that can include custom code. A rule can include an embedded action, which the documentation calls a postprocessing function. You can also use a custom lexer.\n\nAnother interesting feature is that you could build custom tokens. You can define them using a tokenizing library, a literal or a test function. The test function must return true if the text corresponds to that specific token.\n\nNearley include tools for debugging and understanding your parser. For instance, Unparser can automatically generate random strings that are considered correct by your parser. This is useful to test your parser against random noise or even to generate data from a schema (e.g. a random email address). It also include a tool to generate SVG railroad diagrams: a graphical way to represent a grammar.\n\nNearley documentation is a good overview of what is available and there is also a third-party playground to try a grammar online. But you will not find a complete explanation of all the features.\n\nAfter the CFG parsers is time to see the PEG parsers available for JavaScript.\n\nIt also provides easy access to the parse tree nodes.\n\nA Canopy grammar has the neat feature of using actions annotation to use custom code in the parser. In practical terms. you just write the name of a function next to a rule and then you implement the function in your source code.\n\nThe JavaScript file containing the action code.\n\nOhm grammars are defined in a custom format that can be put in a separate file or in a string. However, the parser is generated dynamically and not with a separate tool. In that sense it works like a parser library more than a traditional parser generator.\n\nA helper function to create an AST is included among the extras.\n\nA grammar is completely separated from semantic actions. This simplify portability and readability and allows to support different languages with the same grammar. At the moment Ohm only supports JavaScript, but more languages are planned for the future. The typical grammar is then clean and readable.\n\nSemantic actions can be implemented using a simple API. In practical terms this ends up working like the visitor pattern with the difference that is easier to define more groups of semantic actions.\n\nTo support debugging Ohm has a text trace and (work in progress) graphical visualizer. You can see the graphical visualizer at work and test a grammar in the interactive editor.\n\nThe documentation is not that bad, though you have to go under the directory to find it. There is no tutorial, but there are a few examples and a reference. In particular the documentation suggests reading a well commented Math example.\n\nNote: the development of project PEG.js stopped in 2019. The original developer gave the project to a new maintainer, which then go dark. Peggy is the unofficial successor to PEG.js.\n\nPeggy can work as a traditional parser generator and create a parser with a tool or can generate one using a grammar defined in the code. It supports different module loaders (e.g. AMD, CommonJS, etc.).\n\nPeggy has a neat online editor that allows to write a grammar, test the generated parser and download it.\n\nA Peggy grammar is usually written in a file and can include embedded actions, custom initializing code and semantic predicates. That is to say functions that determine if a specific match is activated or not.\n\nThe documentation is good enough, there are a few example grammars, but there are no official tutorials available. However, the good news is that we made one: A Peggy.js Tutorial. The popularity of the project had led to the development of third-party tools, like one to generate railroad diagrams, and plugins, like one to generate TypeScrypt parsers.\n\nWaxeye can facilitate the creation of an AST by defining nodes in the grammar that will not be included in the generated tree. That is quite useful, but a drawback of Waxeye is that it only generates a AST. In the sense that there is no way to automatically execute an action when you match a node. You have to traverse and execute what you need manually.\n\nOne positive side-effect of this limitation is that grammars are easily readable and clean. They are also independent from any language.\n\nA particular feature of Waxeye is that it provides some help to compose different grammars together and then it facilitates modularity. For instance, you could create a common grammar for identifiers, that are usually similar in many languages.\n\nWaxeye has a great documentation in the form of a manual that explains basic concepts and how to use the tool for all the languages it supports. There are a few example grammars.\n\nWaxeye seems to be maintained, but it is not actively developed.\n\nThey allow you to create a parser simply with JavaScript code, by combining different pattern matching functions, that are equivalent to grammar rules. They are generally considered best suited for simpler parsing needs. Given they are just JavaScript libraries you can easily introduce them into your project: you do not need any specific generation step and you can write all of your code in your favorite editor. Their main advantage is the possibility of being integrated in your traditional workflow and IDE.\n\nIn practice this means that they are very useful for all the little parsing problems you find. If the typical developer encounters a problem, that is too complex for a simple regular expression, these libraries are usually the solution. In short, if you need to build a parser, but you don’t actually want to, a parser combinator may be your best option.\n\nAll libraries are inspired by Parsec. Bennu and Parsimmon are the oldest and Bennu the more advanced between the two. They also all have an extensive documentation, but they have no tutorial. Bennu seems to be maintained, but it is not actively developed.\n\nAn example from the documentation.\n\nParsimmon is the most popular among the three, it is stable .\n\nThe following is a part of the JSON example.\n\nParjs is only a few months old, but it is already quite developed. It also has the advantage of being written in TypeScript.\n\nAll the libraries have good documentation, but Parjs is great: it explains how to use the parser and also how to design good parsers with it. There are a few examples, including the following on string formatting.\n\nThere are also some other interesting libraries related to parsing that are not part of a common category.\n\nThere is one special case that could be managed in more specific way: the case in which you want to parse JavaScript code in JavaScript. Contrary to what we have found for Java and C# there is not a definitive choice: there are many good choices to parse JavaScript.\n\nThe three most popular libraries seems to be: Acorn, and UglifyJS. We are not going to say which one it is best because they all seem to be awesome, updated and well supported (except for Esprima, that slowly died).\n\nOne important difference is that UglifyJS is also a mangler/compressor/beautifier toolkit, which means that it also has many other uses. On the other hand, it is the only one to support only up to the version ECMAScript 5 . Another thing to consider is that only esprima have a documentation worthy of projects of such magnitude .\n\nThere is another interesting parsing tool that does not really fit in more common categories of tools, like parser generators or combinators: Chevrotain, a parsing DSL. A parsing DSL works as a cross between a parser combinator and a parser generator. You define a grammar in JavaScript code directly, but using the (Chevrotain) API and not a standard syntax like EBNF or PEG. You can see some reasons to prefer a parsing DSL rather than a parser generator on their documentation.\n\nChevrotain supports many advanced features typical of parser generators: like semantic predicates, separate lexer and parser and a grammar definition (optionally) separated from the actions. The actions can be implemented using a visitor and thus you can reuse the same grammar for multiple projects.\n\nThe following is a partial JSON example grammar from the documentation. As you can see the syntax is clearer to understand for a developer unexperienced in parsing, but a bit more verbose than a standard grammar.\n\nIt is very fast, faster than any other JavaScript library and can compete with a custom parser written by hand, depending on the JavaScript engine on which it runs on. You can see the numbers and get more details on the benchmark of parsing libraries developed by the author of the library.\n\nIt also supports features useful for debugging like railroad diagram generation and custom error reporting. There are also a few features that are useful for building compiler, interpreters or tools for editors, such as automatic error recovery or syntactic content assist. The last one means that it can suggests the next token given a certain input, so it could be used as the building block for an autocomplete feature.\n\nThis is not all, Chevrotain even makes available its own engine to external use. This means that you can build your own parsing library on top of Chevrotain. For instance, you can create your own format for a grammar and then use the Chevrotain engine to power the parsing. It even gives you for free error checking features, such as detecting ambiguous alternatives, left recursion, etc.\n\nChevrotain has a great and well-organized documentation, with a tutorial, examples grammars and a reference. It also has a neat online editor/playground.\n\nAs we said in the sisters article about parsing in Java and C#, the world of parsers is a bit different from the usual world of programmers. In the case of JavaScript also the language lives in a different world from any other programming language. There is such disparate level of competence between its developers that you could find the best ones working with people that just barely know how to put together a script. And both want to parse things.\n\nSo, for JavaScript there are tools that a bit all over this spectrum. We have serious tools developed by academics for their courses or in the course of their degrees together with much simpler tools. Some of which blur the lines between parser generators and parser combinators. And all of them have their place. A further complication is that while usually parser combinators are reserved for easier uses, with JavaScript it is not always the case. You could find very powerful and complex parser combinators and much easier parser generators.\n\nSo, with JavaScript more than ever we cannot definitely suggest one software over the other. What it is best for a user might not be the best for somebody else. And we all know that the most technically correct solution might not be ideal in real life with all its constraints. So we wanted to share what we have learned on the best options for parsing in JavaScript.\n\nWe would like to thank Shahar Soel for having informed us of Chevrotain and having suggested some needed corrections."
    },
    {
        "link": "https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/pow",
        "document": "is equivalent to the operator, except only accepts numbers.\n\n(and the equivalent ) is the only case where doesn't propagate through mathematical operations — it returns despite the operand being . In addition, the behavior where is 1 and is non-finite (±Infinity or ) is different from IEEE 754, which specifies that the result should be 1, whereas JavaScript returns to preserve backward compatibility with its original behavior.\n\nBecause is a static method of , use it as , rather than as a method of a object you created ( is not a constructor)."
    },
    {
        "link": "https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math",
        "document": "This feature is well established and works across many devices and browser versions. It’s been available across browsers since July 2015 . * Some parts of this feature may have varying levels of support.\n\nThe namespace object contains static properties and methods for mathematical constants and functions. works with the type. It doesn't work with .\n\nUnlike most global objects, is not a constructor. You cannot use it with the operator or invoke the object as a function. All properties and methods of are static. Note: Many functions have a precision that's implementation-dependent. This means that different browsers can give a different result. Even the same JavaScript engine on a different OS or architecture can give different results!\n\nReturns the absolute value of the input. Returns the arccosine of the input. Returns the hyperbolic arccosine of the input. Returns the arcsine of the input. Returns the arctangent of the input. Returns the arctangent of the quotient of its arguments. Returns the hyperbolic arctangent of the input. Returns the cube root of the input. Returns the smallest integer greater than or equal to the input. Returns the number of leading zero bits of the 32-bit integer input. Returns the cosine of the input. Returns the hyperbolic cosine of the input. Returns ex, where x is the argument, and e is Euler's number ( …, the base of the natural logarithm). Returns the largest integer less than or equal to the input. Returns the nearest half precision float representation of the input. Returns the nearest single precision float representation of the input. Returns the square root of the sum of squares of its arguments. Returns the result of the 32-bit integer multiplication of the inputs. Returns the natural logarithm (㏒ ; also, ㏑) of the input. Returns the base-10 logarithm of the input. Returns the natural logarithm (㏒ ; also ㏑) of for the number . Returns the base-2 logarithm of the input. Returns the largest of zero or more numbers. Returns the smallest of zero or more numbers. Returns base to the exponent power (that is, ). Returns the value of the input rounded to the nearest integer. Returns the sign of the input, indicating whether it is positive, negative, or zero. Returns the sine of the input. Returns the hyperbolic sine of the input. Returns the positive square root of the input. Returns the sum of a passed iterable of numbers, avoiding floating point precision loss in intermediate results. Returns the tangent of the input. Returns the hyperbolic tangent of the input. Returns the integer portion of the input, removing any fractional digits.\n\nThe trigonometric functions , , , , , , and expect (and return) angles in radians. Since humans tend to think in degrees, and some functions (such as CSS transforms) can accept degrees, it is a good idea to keep functions handy that convert between the two:\n\nCalculating the height of an equilateral triangle If we want to calculate the height of an equilateral triangle, and we know its side length is 100, we can use the formulae length of the adjacent multiplied by the tangent of the angle is equal to the opposite. In JavaScript, we can do this with the following: We use our function to convert 60 degrees to radians, as expects an input value in radians."
    },
    {
        "link": "https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/sqrt",
        "document": "Because is a static method of , you always use it as , rather than as a method of a object you created ( is not a constructor)."
    },
    {
        "link": "https://mathjs.org/docs/reference/functions.html",
        "document": ""
    },
    {
        "link": "https://w3schools.com/js/js_math.asp",
        "document": "Unlike other objects, the Math object has no constructor.\n\nAll methods and properties can be used without creating a Math object first.\n\nThe syntax for any Math property is : .\n\nJavaScript provides 8 mathematical constants that can be accessed as Math properties:\n\nThe syntax for Math any methods is :\n\nThere are 4 common methods to round a number to an integer:\n\nreturns the value of x rounded up to its nearest integer:\n\nreturns the value of x rounded down to its nearest integer:\n\nreturns if x is negative, null or positive:\n\nreturns the value of x to the power of y:\n\nreturns the absolute (positive) value of x:\n\nreturns the sine (a value between -1 and 1) of the angle x (given in radians).\n\nIf you want to use degrees instead of radians, you have to convert degrees to radians:\n\nreturns the cosine (a value between -1 and 1) of the angle x (given in radians).\n\nIf you want to use degrees instead of radians, you have to convert degrees to radians:\n\nand can be used to find the lowest or highest value in a list of arguments:\n\nThe natural logarithm returns the time needed to reach a certain level of growth:"
    }
]